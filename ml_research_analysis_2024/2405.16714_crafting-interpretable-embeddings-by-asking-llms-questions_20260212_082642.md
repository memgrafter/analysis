---
ver: rpa2
title: Crafting Interpretable Embeddings by Asking LLMs Questions
arxiv_id: '2405.16714'
source_url: https://arxiv.org/abs/2405.16714
tags:
- questions
- qa-emb
- arxiv
- language
- embeddings
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces question-answering embeddings (QA-Emb), a
  method that builds interpretable text embeddings by querying a large language model
  (LLM) with a set of yes/no questions. Each element of the resulting embedding represents
  the answer to a different question asked to the LLM.
---

# Crafting Interpretable Embeddings by Asking LLMs Questions

## Quick Facts
- arXiv ID: 2405.16714
- Source URL: https://arxiv.org/abs/2405.16714
- Reference count: 40
- Outperforms established interpretable baseline by 26% on fMRI voxel prediction

## Executive Summary
This paper introduces question-answering embeddings (QA-Emb), a method for building interpretable text embeddings by querying a large language model with yes/no questions. Each embedding dimension represents the answer to a different question, making the embedding human-inspectable. QA-Emb is applied to predict fMRI voxel responses to language stimuli, where it outperforms traditional interpretable baselines while requiring only 29 questions. The method also shows competitive performance against black-box transformer baselines and can be efficiently distilled into a single model for deployment.

## Method Summary
QA-Emb generates interpretable text embeddings by querying an LLM with a set of yes/no questions, where each embedding dimension represents the answer to one question. For fMRI encoding, questions are optimized using Elastic net to select the most informative subset. The resulting binary embeddings are used as features for ridge regression to predict voxel responses. To improve efficiency, QA-Emb can be distilled into a single transformer model with multiple classification heads that computes all question answers in one feedforward pass.

## Key Results
- QA-Emb outperforms established interpretable baseline by 26% on fMRI voxel prediction
- Achieves competitive performance against black-box transformer baselines with only 29 questions
- Can be effectively distilled into a single model for computational efficiency
- Shows potential for zero-shot adaptation to new tasks via prompting

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** QA-Emb produces interpretable embeddings by converting each yes/no question into a single binary feature.
- **Mechanism:** Each embedding dimension is the binary answer to one yes/no question posed to an LLM, making the embedding directly interpretable as a list of question-answer pairs.
- **Core assumption:** LLMs can answer yes/no questions about text inputs with sufficient accuracy for the downstream task.
- **Evidence anchors:**
  - [abstract]: "Each element of the resulting embedding represents the answer to a different question asked to the LLM."
  - [section]: "Each element of the embedding represents the answer to a different question asked to an LLM, making the embedding human-inspectable."
  - [corpus]: Weak; no explicit quantitative LLM accuracy analysis is provided in the paper, though a separate accuracy study is mentioned in Sec. 5.2.
- **Break condition:** If the LLM cannot answer the chosen questions accurately (e.g., due to ambiguity or domain mismatch), the embedding will misrepresent the input.

### Mechanism 2
- **Claim:** QA-Emb outperforms traditional interpretable baselines while requiring far fewer features.
- **Mechanism:** By selecting semantically relevant yes/no questions, QA-Emb captures high-level linguistic or semantic properties more efficiently than word-level statistics (e.g., Eng1000) or dense embeddings.
- **Core assumption:** A small set of well-chosen questions can represent the information needed for fMRI voxel prediction better than thousands of word co-occurrence features.
- **Evidence anchors:**
  - [abstract]: "QA-Emb significantly outperforms an established interpretable baseline, and does so while requiring very few questions."
  - [section]: "QA-Emb improves performance very quickly as a function of the number of features included, even outperforming the final Eng1000 performance with only 29 questions."
  - [corpus]: Weak; the paper reports improvement but does not provide a full ablation over question sets to prove each question is necessary.
- **Break condition:** If the question selection process fails to capture the key features needed for the task, performance will degrade.

### Mechanism 3
- **Claim:** QA-Emb can be efficiently distilled into a single feedforward model for deployment.
- **Mechanism:** A classification head per question is trained to predict the yes/no answers directly from text, replacing the need for repeated LLM calls.
- **Core assumption:** The relationship between text and question answers can be approximated well by a single transformer model with multiple heads.
- **Evidence anchors:**
  - [abstract]: "Additionally, QA-Emb can be effectively approximated with an efficient model."
  - [section]: "We find that we can drastically reduce this cost by distilling the QA-Emb model into a model that computes the answers to all selected questions in a single feedforward pass."
  - [corpus]: Weak; accuracy of distillation is reported but no ablation of model size or head count is provided.
- **Break condition:** If the distillation model cannot capture the nuances needed for accurate question answering, the embedding quality will drop.

## Foundational Learning

- **Concept:** Ridge regression and its role in fMRI encoding models
  - **Why needed here:** QA-Emb embeddings are used as input features for ridge regression to predict voxel responses; understanding regularization is key to interpreting model performance.
  - **Quick check question:** What effect does increasing the ridge parameter have on the fitted coefficients and model variance?
- **Concept:** fMRI preprocessing (PCA on voxel data, temporal interpolation)
  - **Why needed here:** The input to the regression is not raw voxels but principal components, and temporal smoothing is applied to align embeddings with hemodynamic responses.
  - **Quick check question:** Why is Lanczos resampling used on embeddings before fitting the regression?
- **Concept:** Elastic net for feature selection on QA-Emb questions
  - **Why needed here:** To prune redundant or uninformative questions and create a compact interpretable model.
  - **Quick check question:** How does Elastic net differ from standard Lasso in handling correlated questions?

## Architecture Onboarding

- **Component map:**
  - Input text → LLM (via many calls) → binary embedding (one bit per question) → optional distillation model → ridge regression → voxel predictions
- **Critical path:**
  1. Generate question set using GPT-4 prompts
  2. Answer all questions for all stimuli using an LLM (e.g., Mistral/LLaMA)
  3. Apply Elastic net to select a subset of questions
  4. Fit ridge regression on the selected binary embedding
  5. Evaluate voxel-wise correlation on held-out data
- **Design tradeoffs:**
  - Accuracy vs interpretability: Dense embeddings are more accurate but less interpretable; QA-Emb trades some accuracy for interpretability.
  - Speed vs flexibility: Multiple LLM calls are slow but allow any question set; distillation speeds inference but requires training a separate model.
  - Feature size vs coverage: Fewer questions are easier to interpret but may miss subtle features; more questions increase coverage but risk redundancy.
- **Failure signatures:**
  - Low voxel correlation but high training correlation → overfitting, poor generalization
  - Many selected questions with low individual importance → redundancy in question set
  - Large gap between QA-Emb and distilled QA-Emb performance → distillation model too simple or undertrained
- **First 3 experiments:**
  1. Run QA-Emb with a small hand-crafted question set (e.g., 10 questions) and evaluate voxel correlation to sanity-check pipeline.
  2. Vary the number of time-lagged delays (4, 8, 12) to see impact on encoding performance and choose best delay.
  3. Compare voxel correlation using distilled QA-Emb vs. original multi-LLM QA-Emb to quantify distillation fidelity.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do different discrete optimization approaches compare to the heuristic approach used in QA-Emb for selecting questions?
- Basis in paper: [inferred] from "Future work could explore more directly optimizing the set of learned questions Q in Eq. (1) via improved discrete optimization approaches and constraints."
- Why unresolved: The paper only uses a heuristic approach for question generation and selection. There is no direct comparison to more rigorous discrete optimization methods.
- What evidence would resolve it: A study comparing QA-Emb's performance using various discrete optimization methods (e.g., genetic algorithms, reinforcement learning) for question selection against the heuristic approach, measured on fMRI prediction accuracy and question interpretability.

### Open Question 2
- Question: How does QA-Emb perform on tasks requiring longer context understanding compared to the 10-word contexts used in fMRI experiments?
- Basis in paper: [inferred] from "Note that these tasks are more difficult than the relatively simple questions we answer in the fMRI experiments, especially since the fMRI input lengths are each 10 words, whereas the input lengths for these datasets are over 50 words on average"
- Why unresolved: The paper primarily evaluates QA-Emb on relatively short text contexts (10 words). Its performance on longer contexts is unknown.
- What evidence would resolve it: Experiments applying QA-Emb to tasks with longer input texts (e.g., full articles, multi-paragraph documents) and comparing its performance to baselines on these longer-context tasks.

### Open Question 3
- Question: How does the computational efficiency of QA-Emb compare to other interpretable embedding methods when scaled to very large embedding sizes?
- Basis in paper: [explicit] from "QA-Emb is computationally intensive, requiring d LLM calls to compute an embedding" and "we find that we can dramatically reduce this cost by distilling the QA-Emb model into a single LLM model with many classification heads"
- Why unresolved: While the paper explores distillation for efficiency, it doesn't compare QA-Emb's computational requirements to other interpretable methods at scale.
- What evidence would resolve it: A systematic comparison of computational resources (time, memory) required by QA-Emb (with and without distillation) versus other interpretable embedding methods (e.g., bag-of-words, n-grams) as embedding sizes increase.

## Limitations

- LLM accuracy is not quantitatively validated in the paper; a separate accuracy study is referenced but not integrated.
- The exact question generation and answering prompts are not provided, which could lead to variation in results across different runs or LLMs.
- Distillation fidelity is reported but not extensively analyzed; no ablation over model size or head count is provided.

## Confidence

- **High confidence**: The overall approach of using yes/no question embeddings for interpretable fMRI encoding is valid and well-demonstrated.
- **Medium confidence**: The claim of outperforming established interpretable baselines by 26% is supported by the data, but lacks full ablation studies.
- **Medium confidence**: The distillation approach works, but the extent of performance loss and its dependence on model architecture is not fully characterized.

## Next Checks

1. **LLM Accuracy Validation**: Run a controlled test with known question-answer pairs to measure LLM accuracy and assess its impact on embedding quality.
2. **Prompt Sensitivity Analysis**: Test multiple prompts for question generation and answering to quantify variability in performance and interpretability.
3. **Distillation Ablation Study**: Vary the size and architecture of the distillation model to determine the optimal configuration for maintaining QA-Emb fidelity.