---
ver: rpa2
title: 'The Importance of Online Data: Understanding Preference Fine-tuning via Coverage'
arxiv_id: '2406.01462'
source_url: https://arxiv.org/abs/2406.01462
tags:
- offline
- policy
- reward
- assumption
- rlhf
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper studies the theoretical separation between online reinforcement
  learning from human feedback (RLHF) and offline contrastive methods like Direct
  Preference Optimization (DPO) for fine-tuning language models. The key insight is
  that DPO requires a strong global coverage condition (where the reference policy
  covers the entire policy space), while online RLHF only needs a weaker local KL-ball
  coverage.
---

# The Importance of Online Data: Understanding Preference Fine-tuning via Coverage

## Quick Facts
- **arXiv ID**: 2406.01462
- **Source URL**: https://arxiv.org/abs/2406.01462
- **Reference count**: 15
- **Primary result**: Proves offline DPO requires global coverage while online RLHF only needs local KL-ball coverage; proposes HyPO that outperforms DPO on TL;DR summarization

## Executive Summary
This paper establishes a fundamental theoretical separation between online reinforcement learning from human feedback (RLHF) and offline contrastive methods like Direct Preference Optimization (DPO) for language model fine-tuning. The key insight is that DPO requires strong global coverage conditions where the reference policy covers the entire policy space, while online RLHF only needs weaker local KL-ball coverage. This explains why online methods outperform offline ones when offline preference data lacks diversity. The authors propose Hybrid Preference Optimization (HyPO) that combines offline DPO with online KL regularization, achieving better performance than DPO on GPT-4 win-rate while being more computationally efficient than full online RLHF.

## Method Summary
The authors propose Hybrid Preference Optimization (HyPO), which trains a reward model on offline preference data using a Bradley-Terry loss, then optimizes the policy using both an offline DPO objective (to leverage preference data) and an online KL regularization term (to prevent out-of-distribution responses). The method trains a reference policy via supervised fine-tuning, trains a reward model on preference pairs, then jointly optimizes HyPO using both offline preference pairs and online samples from the reference policy. The approach balances computational efficiency with theoretical guarantees by using offline data for preference optimization and online samples only for KL regularization.

## Key Results
- HyPO outperforms DPO on TL;DR summarization with higher GPT-4 win-rate and lower reverse KL divergence
- The theoretical separation shows DPO requires global coverage while online RLHF only needs local KL-ball coverage
- RLHF and DPO both decrease likelihood of preferred and rejected responses during training when function approximation enables extrapolation

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Offline contrastive methods like DPO require global coverage of the response space to guarantee convergence, while online RLHF only needs local KL-ball coverage.
- Mechanism: Global coverage ensures the reference policy πref assigns non-negligible probability to all responses that any optimal policy might generate. This allows the offline reward model to learn accurate rewards for all relevant responses. Local KL-ball coverage is weaker - it only requires πref to cover policies that stay close in KL divergence, which online RLHF can enforce through its explicit KL regularization term.
- Core assumption: The reference policy πref has sufficient support to cover either the entire response space (for global coverage) or at least all responses within a KL ball of reasonable size (for local coverage).
- Evidence anchors:
  - [abstract] "We prove that a global coverage condition is both necessary and sufficient for offline contrastive methods to converge to the optimal policy, but a weaker partial coverage condition suffices for online RL methods."
  - [section 4.1] Proposition 4.1 constructs a policy π that satisfies the in-distribution reward learning assumption but generates out-of-distribution responses with unbounded reverse KL, causing infinite regret.
  - [corpus] Weak evidence - corpus contains related work on coverage but no direct evidence for this specific mechanism.
- Break condition: If the reference policy has near-zero probability on optimal responses, both methods fail, but DPO fails more catastrophically with infinite reverse KL.

### Mechanism 2
- Claim: The reverse KL regularization in online RLHF prevents the policy from generating out-of-distribution responses, while DPO lacks this constraint.
- Mechanism: Online RLHF directly optimizes the reverse KL term KL(π||πref) using on-policy samples, which bounds how far π can deviate from πref. DPO only enforces KL regularization under the offline data distribution, allowing πdpo to generate responses with πref(y|x) ≈ 0, causing unbounded reverse KL.
- Core assumption: The reward function class for RLHF is bounded (Assumption 4.4), which ensures the reverse KL term is finite.
- Evidence anchors:
  - [section 4.1] "DPO constructs the following implicit reward class with the policy class Π: Rdpo = {β log(π(y|x)/πref(y|x)·Z(x)) | π ∈ Π}"
  - [section 4.3] Lemma 4.1 shows that bounded reward functions ensure KL(πrlhf||πref) ≤ 2R'/β
  - [corpus] Weak evidence - corpus mentions coverage but not this specific mechanism.
- Break condition: If the reward function learned by RLHF is unbounded (e.g., neural network outputs explode), the reverse KL constraint fails.

### Mechanism 3
- Claim: Function approximation enables extrapolation beyond training data, allowing both RLHF and DPO to improve the reference policy despite decreasing likelihood of training examples.
- Mechanism: Linear function approximation with global coverage allows the learned reward function to generalize to responses outside the training data. This enables the policy to increase likelihood of optimal responses not seen during training while decreasing likelihood of sub-optimal responses in the training data.
- Core assumption: The feature map ϕ spans the response space and the training data has global coverage in the function approximation sense.
- Evidence anchors:
  - [section 6] Example 6.1 demonstrates this with linear function approximation where ϕ(y1) = [1,0], ϕ(y2) = [1/2,1/2], ϕ(y3) = [0,1]
  - [section 6.1.1] Synthetic experiment shows both online RL and DPO increase probability of optimal response [1,0,0,...] while decreasing probability of preferred responses
  - [corpus] No direct evidence in corpus for this mechanism.
- Break condition: Without function approximation (e.g., independent actions in multi-armed bandit setting), DPO increases probability of out-of-distribution suboptimal responses instead of the optimal one.

## Foundational Learning

- Concept: Coverage in reinforcement learning
  - Why needed here: The paper's core theoretical contribution relies on understanding different coverage conditions (global vs partial) and how they affect convergence of preference learning algorithms
  - Quick check question: What is the difference between global coverage and local KL-ball coverage in the context of this paper?

- Concept: KL divergence and its forward/reverse variants
  - Why needed here: The paper extensively uses KL divergence to measure policy deviation, with reverse KL requiring on-policy sampling and forward KL only requiring offline samples
  - Quick check question: Why does reverse KL require online samples while forward KL can be computed offline?

- Concept: Bradley-Terry model for pairwise preferences
  - Why needed here: The paper assumes preference data follows the Bradley-Terry model p*(y1 ≻ y2|x) = exp(r*(x,y1))/(exp(r*(x,y1)) + exp(r*(x,y2)))
  - Quick check question: What assumption about preferences does the Bradley-Terry model make that the paper explicitly states?

## Architecture Onboarding

- Component map:
  Offline preference dataset -> Reference policy πref -> Reward model -> Policy optimization (PPO for online RLHF, direct optimization for DPO) -> KL regularization

- Critical path:
  1. Train reward model on offline preference data
  2. Optimize policy with reward model + KL regularization (online) OR directly optimize preference objective (offline)
  3. Evaluate on held-out preference data and compute KL divergence to reference policy

- Design tradeoffs:
  - Online RLHF: Higher computational cost (reward model + value network + multiple PPO steps) but stronger theoretical guarantees and better performance when coverage is limited
  - DPO: Lower computational cost but requires global coverage for convergence and can have unbounded reverse KL
  - HyPO: Balances computational efficiency with theoretical guarantees by using offline data for preference optimization and online samples for KL regularization

- Failure signatures:
  - DPO: Increasing reverse KL divergence, poor performance on out-of-distribution prompts
  - Online RLHF: High computational cost, potential overfitting to reward model
  - Both: Decreasing likelihood of both preferred and rejected responses during training (can be good with function approximation, bad without)

- First 3 experiments:
  1. Compare DPO vs HyPO on TL;DR dataset with GPT-4 win-rate and reverse KL metrics
  2. Test coverage sensitivity by training on preference data with varying diversity levels
  3. Validate function approximation extrapolation by testing on responses outside training distribution

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the separation between online RLHF and offline DPO hold under more realistic assumptions about reward model error bounds?
- Basis in paper: [explicit] The paper shows that DPO requires global coverage while RLHF only needs local KL-ball coverage, but this separation relies on bounded reward model assumptions.
- Why unresolved: The theoretical analysis assumes either exact reward learning or bounded reward model errors, but real-world reward models may have unbounded errors in practice.
- What evidence would resolve it: Empirical studies comparing RLHF and DPO performance across different reward model architectures and error distributions, or theoretical analysis relaxing the bounded error assumptions.

### Open Question 2
- Question: Can the function approximation coverage condition be further relaxed to single-policy coverage for RLHF methods?
- Basis in paper: [inferred] The authors conjecture that single-policy coverage (covering only the optimal policy) might be sufficient for RLHF but not for offline methods, but they couldn't prove this.
- Why unresolved: The authors suspect that reverse KL regularization isn't strong enough to induce pessimism under single-policy coverage, but this remains unproven.
- What evidence would resolve it: Lower bounds showing RLHF failure under single-policy coverage, or algorithms achieving meaningful regret under this weaker condition.

### Open Question 3
- Question: How does the extrapolation behavior change when using different function approximation architectures?
- Basis in paper: [explicit] The paper demonstrates that linear function approximation enables DPO to extrapolate correctly, while without function approximation DPO fails to generalize.
- Why unresolved: The analysis only considers linear function approximation, and it's unclear how more complex architectures like neural networks affect the extrapolation behavior.
- What evidence would resolve it: Experiments comparing DPO performance with different function approximators (linear, neural networks, transformers) on tasks where the optimal actions are not in the training data.

## Limitations

- The theoretical separation between global and local coverage conditions is well-established for linear function approximation, but may not fully extend to neural networks where function approximation properties are less understood
- The experiments focus on a single task (TL;DR summarization), limiting generalizability to other domains
- The computational efficiency claims for HyPO assume the online KL regularization is cheap relative to full online RLHF, which depends on the specific implementation

## Confidence

- **High**: The theoretical results on coverage conditions and their implications for convergence (Section 4)
- **Medium**: The empirical validation on TL;DR summarization showing HyPO outperforms DPO
- **Medium**: The explanation for why RLHF and DPO decrease likelihood of both preferred and rejected responses during training

## Next Checks

1. **Coverage Sensitivity Analysis**: Systematically vary the diversity of preference data and measure how HyPO vs DPO performance degrades, quantifying the coverage threshold where HyPO becomes necessary

2. **Cross-Domain Validation**: Test HyPO on a different task (e.g., dialogue or code generation) to verify the coverage-based theoretical predictions generalize beyond summarization

3. **Function Approximation Robustness**: Experiment with different reward model architectures (linear vs small neural networks vs large models) to test how the coverage requirements change with function approximation capacity