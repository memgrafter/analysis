---
ver: rpa2
title: 'SF-GNN: Self Filter for Message Lossless Propagation in Deep Graph Neural
  Network'
arxiv_id: '2407.02762'
source_url: https://arxiv.org/abs/2407.02762
tags:
- node
- representation
- sf-gnn
- layer
- graph
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a novel perspective on the degradation problem
  in deep Graph Neural Networks (GNNs), attributing it to the interference caused
  by low-quality node representations during message propagation. Unlike the popular
  explanations of over-smoothing or over-squashing, the authors propose a new method
  called SF-GNN to address this issue.
---

# SF-GNN: Self Filter for Message Lossless Propagation in Deep Graph Neural Network

## Quick Facts
- arXiv ID: 2407.02762
- Source URL: https://arxiv.org/abs/2407.02762
- Authors: Yushan Zhu; Wen Zhang; Yajing Xu; Zhen Yao; Mingyang Chen; Huajun Chen
- Reference count: 39
- Primary result: SF-GNN improves performance and delays degradation in deep GNNs by filtering low-quality node representations

## Executive Summary
This paper introduces a novel perspective on deep GNN degradation, attributing it to interference from low-quality node representations during message propagation rather than traditional explanations like over-smoothing or over-squashing. The authors propose SF-GNN, which defines two representations per node (node representation and message representation) and introduces a self-filter module to evaluate and gate node representation quality. Experiments on node classification tasks across both homogeneous and heterogeneous graphs, as well as link prediction tasks on knowledge graphs, demonstrate that SF-GNN outperforms state-of-the-art baseline methods in addressing deep GNN degradation.

## Method Summary
SF-GNN addresses deep GNN degradation by introducing a two-representation design: one for node features (h) and one for message propagation (m). A self-filter module (SFM) evaluates the quality of each node representation using the decoder's output on that representation and decides whether to integrate it into message propagation. This filtering mechanism prevents low-quality node representations from interfering with message flow while maintaining information propagation from neighbors. The approach is applied to both node classification tasks using GAT, GraphSAGE, and Ordered GNN, and link prediction tasks using R-GCN, CompGCN, and NBFNet.

## Key Results
- SF-GNN outperforms state-of-the-art baseline methods on node classification tasks for both homogeneous and heterogeneous graphs
- The method delays degradation and improves performance in deep GNN architectures (up to 10 layers)
- SF-GNN demonstrates effectiveness on link prediction tasks for knowledge graphs using multiple KGE models

## Why This Works (Mechanism)

### Mechanism 1
Low-quality node representations disrupt message propagation and degrade deep GNN performance. SF-GNN introduces two representations per node and uses a self-filter module to conditionally block low-quality representations from propagating, preventing interference with message flow.

### Mechanism 2
The two-representation design enables independent control of feature representation and message propagation. This allows blocking propagation of low-quality node features without interrupting the flow of incoming neighbor messages.

### Mechanism 3
The self-filter module uses decoder-based quality assessment to selectively propagate node representations. It calculates quality scores from decoder output and applies Gumbel-softmax sampling to create a learnable gating mechanism.

## Foundational Learning

- **Graph Neural Networks (GNNs) and message passing**: Why needed - SF-GNN builds on GNN message passing mechanisms. Quick check - What is the core operation in a GNN layer that updates a node's representation based on its neighbors?

- **Over-smoothing and over-squashing in deep GNNs**: Why needed - SF-GNN addresses degradation issues traditionally explained by these phenomena. Quick check - How do over-smoothing and over-squashing differently affect node representations in deep GNNs?

- **Knowledge Graph Embedding (KGE) methods**: Why needed - SF-GNN is evaluated on KGE tasks. Quick check - What is the role of the decoder in a GNN-based KGE model, and how does it score triples?

## Architecture Onboarding

- **Component map**: Node representation (h) -> Self-Filter Module (SFM) -> Message representation (m) -> Neighbor nodes

- **Critical path**: 1) Node receives message representations from neighbors, 2) Node updates its node representation using received messages, 3) SFM evaluates quality of updated node representation, 4) Node representation is gated into message representation based on SFM output, 5) Message representation is propagated to neighbors in next layer

- **Design tradeoffs**: Two representations per node increase memory usage but enable selective filtering; decoder-based quality assessment adds computational overhead but provides task-specific evaluation; Gumbel-softmax sampling enables differentiable gating but introduces stochasticity

- **Failure signatures**: If SFM always outputs 0, no node features are propagated and model degrades to neighbor-only aggregation; if SFM always outputs 1, no filtering occurs and model behaves like standard GNN; if quality assessment is unreliable, filtering may block good representations or allow bad ones

- **First 3 experiments**: 1) Implement SF-GNN on a simple GNN (like GCN) for node classification on Cora dataset, comparing with base GNN, 2) Test SFM behavior by analyzing which nodes get filtered at different layers and how this affects performance, 3) Evaluate stability of SF-GNN as layers increase, comparing performance degradation with base GNN and baseline methods (DGCN, FA, OG)

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions, but several areas remain unresolved based on the analysis.

## Limitations
- The paper lacks theoretical guarantees for the self-filter module's ability to distinguish high-quality from low-quality node representations across different GNN architectures
- Computational overhead introduced by the self-filter module is not quantitatively analyzed, especially for large-scale graphs
- The approach's effectiveness on extremely deep GNN models (20+ layers) remains unexplored

## Confidence
- **High**: SF-GNN improves performance on node classification and link prediction tasks
- **Medium**: Low-quality node representations cause deep GNN degradation (alternative to over-smoothing)
- **Medium**: Two-representation design with self-filter effectively addresses degradation

## Next Checks
1. Conduct ablation studies removing the self-filter module to quantify its specific contribution to performance gains
2. Test SF-GNN on graphs with varying homophily ratios to verify robustness across different graph structures
3. Analyze the correlation between decoder-based quality scores and actual downstream task performance to validate the quality assessment mechanism