---
ver: rpa2
title: Multi-label Learning with Random Circular Vectors
arxiv_id: '2407.05656'
source_url: https://arxiv.org/abs/2407.05656
tags:
- chrr
- vector
- vectors
- circular
- output
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper explores circular vectors for extreme multi-label classification
  (XMC), where each vector component is represented as a complex amplitude. The authors
  propose a novel framework that encodes a set of labels for a data instance using
  a low-dimensional circular vector, enabling direct prediction by the output layer
  of a deep neural network (DNN).
---

# Multi-label Learning with Random Circular Vectors

## Quick Facts
- arXiv ID: 2407.05656
- Source URL: https://arxiv.org/abs/2407.05656
- Authors: Ken Nishida; Kojiro Machi; Kazuma Onishi; Katsuhiko Hayashi; Hidetaka Kamigaito
- Reference count: 15
- This paper proposes a novel framework that encodes labels using low-dimensional circular vectors, achieving up to 99% output layer size reduction while improving performance in extreme multi-label classification.

## Executive Summary
This paper introduces circular vectors for extreme multi-label classification (XMC), where each vector component is represented as a complex amplitude on the unit circle. The authors propose a framework that encodes label sets using low-dimensional circular vectors, enabling direct prediction through a deep neural network's output layer. Through experiments on synthetic and actual XMC datasets, they demonstrate that circular vectors have better label encoding capacity and retrieval ability compared to normal real-valued vectors, while significantly reducing output layer size.

## Method Summary
The method encodes labels as circular vectors where each element is a complex amplitude with unitary magnitude. The model architecture uses fully connected neural networks with two hidden layers, where the output layer directly predicts Cartesian coordinates (cos ϕ, sin ϕ) for each angle, which are then converted to circular vectors using the atan2 function. The framework is trained using cosine similarity loss between predicted and target circular vectors, enabling efficient encoding and retrieval of label information in extreme multi-label classification tasks.

## Key Results
- Circular vectors achieved better label encoding capacity and retrieval ability than real-valued vectors in synthetic experiments
- The proposed method significantly improved task performance compared to previous models using random real-valued vectors
- Output layer size was reduced by up to 99% while maintaining or improving classification performance

## Why This Works (Mechanism)

### Mechanism 1
Circular vectors retain unitary magnitude under superposition, enabling accurate label retrieval even with many encoded labels. Each vector element is a complex amplitude on the unit circle, and superposition uses angle operations that maintain the unitary property. The CHRR similarity operation (1/d Σ cos(ϕj - θj)) is more stable than cosine similarity on normalized real vectors when many labels are encoded.

### Mechanism 2
Circular vectors enable 99% output layer size reduction while maintaining or improving task performance. Each label is encoded as a low-dimensional circular vector (d dimensions) instead of an L-dimensional one-hot vector, with d << L. The output layer predicts Cartesian coordinates (cos ϕ, sin ϕ) for each angle, reducing the number of parameters needed for large label sets.

### Mechanism 3
The atan2-based conversion from Cartesian to polar coordinates handles cyclic feature boundaries better than simple arctangent. Output neurons predict (cos ϕ, sin ϕ) pairs, and atan2 maps these to angles in (-π, π], preserving continuity at the wrap-around boundary and providing smooth gradients for learning.

## Foundational Learning

- Concept: Holographic Reduced Representations (HRR) and circular convolution
  - Why needed here: The paper builds on HRR but replaces real-valued vectors with circular vectors for better label encoding
  - Quick check question: What operation in HRR allows retrieval of a vector from a superposition of bindings?

- Concept: Complex numbers and unit circle representation
  - Why needed here: Circular vectors use complex amplitudes to maintain unitary property under superposition
  - Quick check question: How is a point on the unit circle represented using complex numbers?

- Concept: atan2 function and angle computation
  - Why needed here: The model architecture uses atan2 to convert Cartesian coordinates to angles while handling boundary conditions
  - Quick check question: What is the range of angles produced by atan2, and why is it preferred over arctangent for this task?

## Architecture Onboarding

- Component map:
  Input layer (BoW/XLNet) -> Hidden layers (ReLU) -> Output layer (Cartesian coordinates) -> atan2 conversion -> Circular vector -> Similarity computation -> Loss

- Critical path:
  Forward pass: Input → Hidden layers → Raw outputs → (cos ϕ, sin ϕ) pairs → atan2 → Circular vector → Similarity computation → Loss
  Backward pass: Gradient flows through atan2 and similarity operations to update weights

- Design tradeoffs:
  - Output dimension: 2d for CHRR vs d for HRR - doubles parameters but maintains unitary property
  - Model size: CHRR-Half variant reduces hidden/output size to match HRR parameter count
  - Feature representation: BoW vs XLNet embeddings - impacts performance on large label sets

- Failure signatures:
  - Poor retrieval accuracy suggests insufficient model capacity or learning issues
  - Numerical instability in atan2 conversion indicates (cos ϕ, sin ϕ) pairs not on unit circle
  - Vanishing/exploding gradients in backward pass through atan2

- First 3 experiments:
  1. Verify atan2 conversion: Feed known (cos ϕ, sin ϕ) pairs through model and check recovered angles
  2. Test unitary property: Superimpose multiple circular vectors and verify each element remains unitary
  3. Benchmark similarity stability: Compare variance of similarities for CHRR vs HRR with increasing number of encoded labels

## Open Questions the Paper Calls Out

### Open Question 1
How does the performance of circular vectors compare to real-valued vectors in more complex and larger-scale XMC datasets?
- Basis in paper: The authors state that circular vectors have better label encoding capacity and retrieval ability than normal real-valued vectors, but only demonstrate this on a limited set of XMC datasets
- Why unresolved: The experiments were conducted on a limited number of XMC datasets, and the scalability of circular vectors to larger and more complex datasets remains unexplored
- What evidence would resolve it: Conducting experiments on a broader range of XMC datasets with varying sizes and complexities would provide evidence on the scalability and performance of circular vectors compared to real-valued vectors

### Open Question 2
Can the circular vector framework be extended to other neural network architectures, such as LSTM and Transformer, to improve their performance in XMC tasks?
- Basis in paper: The authors mention the potential of incorporating circular vector systems into other DNN models like LSTM and Transformer, but do not provide experimental results or analysis
- Why unresolved: The paper does not explore the application of circular vectors to other neural network architectures, leaving the potential benefits and limitations unexplored
- What evidence would resolve it: Implementing circular vectors in LSTM and Transformer architectures and evaluating their performance on XMC tasks would provide insights into the generalizability and effectiveness of the circular vector framework

### Open Question 3
How does the proposed circular vector framework compare to the latest state-of-the-art models in XMC, such as APLC-XLNet, LightXML, AttentionXML, and CascadeXML?
- Basis in paper: The authors do not compare their circular vector framework to the latest state-of-the-art models in XMC, focusing instead on traditional methods and HRR
- Why unresolved: The paper does not include a comparison with the latest advancements in XMC, leaving the relative performance and potential improvements of the circular vector framework unknown
- What evidence would resolve it: Conducting experiments comparing the circular vector framework to the latest state-of-the-art models in XMC would provide insights into its competitiveness and potential areas for improvement

## Limitations
- The comparison with standard HRR is limited - no baseline results for real-valued HRR on the same XMC tasks
- The 99% output layer size reduction claim needs more context about what baseline this is measured against
- Synthetic dataset experiments may not fully capture real-world label correlations and distributions

## Confidence
- **High Confidence**: The core mechanism of using circular vectors with atan2 conversion for boundary handling is technically sound and well-explained
- **Medium Confidence**: The experimental results showing performance improvements and compression benefits, though limited to specific datasets and model architectures
- **Low Confidence**: The generalizability of these benefits to other XMC methods and real-world applications without further validation

## Next Checks
1. Implement and benchmark against standard real-valued HRR on the same XMC datasets to establish clear performance differences
2. Test the model on additional XMC datasets with different characteristics (label cardinality, feature dimensions) to assess robustness
3. Evaluate the impact of circular vector dimensionality (d) on both performance and compression ratio to identify optimal configurations