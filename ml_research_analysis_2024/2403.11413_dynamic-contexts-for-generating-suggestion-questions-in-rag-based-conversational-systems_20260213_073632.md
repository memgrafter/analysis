---
ver: rpa2
title: Dynamic Contexts for Generating Suggestion Questions in RAG Based Conversational
  Systems
arxiv_id: '2403.11413'
source_url: https://arxiv.org/abs/2403.11413
tags:
- questions
- dynamic
- contexts
- suggestion
- question
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a dynamic context approach for generating suggestion
  questions in RAG-based conversational systems. The method dynamically selects relevant
  few-shot examples and retrieves contexts based on the user's query to guide question
  generation.
---

# Dynamic Contexts for Generating Suggestion Questions in RAG Based Conversational Systems

## Quick Facts
- **arXiv ID**: 2403.11413
- **Source URL**: https://arxiv.org/abs/2403.11413
- **Reference count**: 19
- **One-line primary result**: Dynamic context approach produces more relevant and answerable suggestion questions (44-46% correct) compared to zero-shot, few-shot, and dynamic few-shot methods in RAG-based conversational systems

## Executive Summary
This paper introduces a dynamic context approach for generating suggestion questions in RAG-based conversational systems. The method dynamically selects relevant few-shot examples and retrieves contexts based on user queries to guide question generation. Manual evaluation demonstrates that the dynamic context approach outperforms baseline methods, achieving 44-46% correct samples. The system adapts to query structure and improves question relevance, though it faces challenges with numerical reasoning tasks.

## Method Summary
The approach uses in-context learning with LLMs to generate suggestion questions when users provide ambiguous queries. It combines dynamic few-shot examples and dynamically retrieved contexts selected based on query similarity using cosine similarity on OpenAI embeddings. The system retrieves 4 most similar contexts and selects 3 relevant few-shot examples to construct prompts that generate 3 suggestion questions per query. No fine-tuning is required, making the approach adaptable to different datasets.

## Key Results
- Dynamic context approach achieves 44-46% correct samples in manual evaluation
- Outperforms zero-shot, few-shot, and dynamic few-shot methods
- Improves question relevance by adapting context and examples to query structure
- Models show confusion with numerical reasoning tasks (e.g., baby age calculations)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Dynamic context selection based on query similarity improves relevance of suggestion questions
- Mechanism: The system retrieves contexts and few-shot examples most similar to the user's query using cosine similarity on OpenAI embeddings, ensuring questions are grounded in relevant information
- Core assumption: Similarity in embedding space correlates with relevance for suggestion question generation
- Evidence anchors:
  - [abstract]: "dynamic context, which includes both dynamic few-shot examples and dynamically retrieved contexts"
  - [section]: "We use OpenAI embeddings to embed the examples and dynamic contexts and cosine similarity to select dynamic examples and contexts"
  - [corpus]: Found 25 related papers with average neighbor FMR=0.54, suggesting moderate similarity in related work
- Break condition: When similarity measure fails to capture semantic relevance, or when relevant contexts are not present in dataset

### Mechanism 2
- Claim: Dynamic few-shot examples adapt question structure to query format
- Mechanism: Instead of using static examples, system selects few-shot examples that match structure and format of current query, allowing model to generate questions in appropriate formats
- Core assumption: Structure of relevant examples influences structure of generated questions
- Evidence anchors:
  - [abstract]: "dynamic few-shot examples... dynamically choosing each example based on user's query"
  - [section]: "Dynamic few-shot diverges from traditional few-shot prompting by dynamically choosing each example based on user's query"
  - [corpus]: The paper cites "Learning When to Retrieve, What to Rewrite, and How to Respond in Conversational QA" which addresses similar contextual adaptation
- Break condition: When dynamic selection fails to find structurally appropriate examples, or when model overfits to example structures

### Mechanism 3
- Claim: RAG-based context retrieval ensures answerable questions
- Mechanism: By retrieving contexts specifically relevant to user's query, system can generate questions guaranteed to be answerable from provided contexts
- Core assumption: Retrieved contexts contain sufficient information to answer generated questions
- Evidence anchors:
  - [abstract]: "dynamically retrieved contexts... to generate suggestion questions"
  - [section]: "To ensure that questions generated by Suggestion Question Generator remain pertinent and answerable, dynamic contexts consist of dynamically retrieved contexts"
  - [corpus]: The paper "Query Suggestion for Retrieval-Augmented Generation via Dynamic In-Context Learning" addresses similar RAG-based approaches
- Break condition: When retrieved contexts are incomplete or when answer spans multiple contexts

## Foundational Learning

- Concept: Retrieval-Augmented Generation (RAG)
  - Why needed here: RAG forms foundation of system, allowing it to retrieve relevant contexts for generating answerable questions
  - Quick check question: How does RAG differ from standard generation in terms of handling knowledge-intensive tasks?

- Concept: In-context learning
  - Why needed here: System relies on in-context learning to generate suggestion questions without fine-tuning, making it adaptable to different datasets
  - Quick check question: What are limitations of in-context learning compared to fine-tuning approaches?

- Concept: Few-shot prompting
  - Why needed here: Few-shot examples guide model in generating appropriate question formats, with dynamic selection adapting to query structure
  - Quick check question: How does dynamic few-shot differ from static few-shot in terms of performance and adaptability?

## Architecture Onboarding

- Component map: User query input → Embedding generation → Context retrieval (RAG) → Dynamic example selection → Prompt assembly → LLM generation → Output validation
- Critical path:
  1. Receive user query
  2. Generate embeddings for query
  3. Retrieve 4 most similar contexts using cosine similarity
  4. Select 3 most relevant dynamic few-shot examples
  5. Assemble prompt with query, contexts, and examples
  6. Generate 3 suggestion questions
  7. Validate output quality

- Design tradeoffs:
  - Static vs. dynamic examples: Static provides consistency but lacks adaptability; dynamic adapts but may introduce variability
  - Number of contexts: More contexts provide better grounding but increase prompt size and cost
  - Retrieval method: Dense retrieval (used) vs. sparse retrieval - dense captures semantic similarity better

- Failure signatures:
  - Consistently poor quality questions → Check retrieval relevance and example quality
  - Questions not matching context → Verify context retrieval and prompt assembly
  - Model confusion with numerical reasoning → Check if query involves age calculations or similar

- First 3 experiments:
  1. Test retrieval quality: Compare top-4 retrieved contexts against ground truth relevance
  2. Validate dynamic selection: Compare dynamic vs. static example performance on sample dataset
  3. Assess prompt order impact: Test different arrangements of query, contexts, and examples in prompt

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does performance of Dynamic Contexts compare to other methods when dealing with numerical reasoning tasks in dataset?
- Basis in paper: [explicit] Paper mentions dataset presents significant challenges in showcasing numerical reasoning capabilities of LLMs and that ChatGPT and GPT-4 displayed confusion over baby's age in generated suggestion questions
- Why unresolved: Paper provides example of models' difficulty with numerical distinction but does not provide comprehensive comparison of Dynamic Contexts' performance on numerical reasoning tasks versus other methods
- What evidence would resolve it: Detailed analysis comparing performance of Dynamic Contexts and other methods on variety of numerical reasoning tasks within dataset, including metrics such as accuracy and precision

### Open Question 2
- Question: What is impact of order of query and contexts in prompt on quality of generated suggestion questions?
- Basis in paper: [explicit] Paper mentions ablation study that altered arrangement of example contexts and queries to assess performance, but found no discernible difference in nature of generated suggestion questions
- Why unresolved: Ablation study did not find significant impact of order, but paper does not explore whether there might be subtle or indirect effect on other aspects of generated questions, such as relevance or clarity
- What evidence would resolve it: Further experiments varying order of query and contexts in prompt, along with comprehensive evaluation of generated suggestion questions across multiple dimensions, such as relevance, clarity, and user satisfaction

### Open Question 3
- Question: How does personalization of suggestion questions based on user history affect performance of Dynamic Contexts approach?
- Basis in paper: [explicit] Paper mentions plans to personalize suggestion questions based on user history in future
- Why unresolved: Paper does not provide any results or analysis of impact of personalization on performance of Dynamic Contexts approach
- What evidence would resolve it: Implementation and evaluation of Dynamic Contexts approach with personalized suggestion questions based on user history, including metrics such as user satisfaction, engagement, and relevance of generated questions

## Limitations

- Small sample size (35 annotated QAS pairs) and subjective manual evaluation create uncertainty about generalizability
- Absence of standardized automated evaluation metrics makes benchmarking difficult
- Lack of detailed implementation specifications (prompt templates, similarity thresholds) affects reproducibility
- Models struggle with numerical reasoning tasks, particularly age calculations and similar mathematical queries

## Confidence

**High Confidence**: Core mechanism of dynamic context selection based on query similarity is well-established in literature and supported by results showing improved relevance of suggestion questions compared to baseline methods.

**Medium Confidence**: Claim that dynamic few-shot examples improve question structure adaptation is supported by manual evaluation but could benefit from more rigorous quantitative analysis across diverse query types.

**Low Confidence**: Assertion that approach handles numerical reasoning tasks well is questionable, as paper explicitly notes difficulties with age calculations and similar numerical queries, suggesting potential limitations in handling mathematical reasoning.

## Next Checks

1. **Automated Evaluation Implementation**: Implement standardized automated metrics (BLEU, ROUGE, semantic similarity scores) to validate manual evaluation results across full dataset of 228 blog posts.

2. **Cross-Dataset Generalization Test**: Test dynamic context approach on different domain (e.g., technical documentation or customer support queries) to assess performance outside baby sleep coaching context.

3. **Ablation Study on Context Selection**: Conduct controlled experiments varying number of retrieved contexts (2, 4, 6) and few-shot examples (1, 2, 3) to identify optimal configurations for different query complexity levels.