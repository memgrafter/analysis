---
ver: rpa2
title: 'OmChat: A Recipe to Train Multimodal Language Models with Strong Long Context
  and Video Understanding'
arxiv_id: '2407.04923'
source_url: https://arxiv.org/abs/2407.04923
tags:
- data
- arxiv
- image
- omchat
- preprint
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: OmChat is a multimodal large language model designed for long context
  and video understanding tasks. It uses dynamic vision encoding to handle images
  of varying resolutions, an active progressive multimodal pretraining strategy to
  scale long context capacity, and high-quality data selection.
---

# OmChat: A Recipe to Train Multimodal Language Models with Strong Long Context and Video Understanding

## Quick Facts
- arXiv ID: 2407.04923
- Source URL: https://arxiv.org/abs/2407.04923
- Reference count: 28
- OmChat supports up to 512K context length and outperforms most open-source models on long context and video understanding benchmarks

## Executive Summary
OmChat is a multimodal large language model designed to handle long context sequences and complex video understanding tasks. The model employs dynamic vision encoding to process images at varying resolutions, active progressive multimodal pretraining to scale long context capacity, and high-quality data selection strategies. OmChat demonstrates strong performance on benchmarks involving multiple images and videos, supporting context lengths up to 512K tokens. The model introduces a unified prompting strategy for complex multimodal inputs and proposes a new benchmark, Temporal Visual Needle in a Haystack, for evaluating long video comprehension.

## Method Summary
OmChat's architecture centers on dynamic vision encoding that adapts to images of varying resolutions, enabling effective processing of diverse visual inputs. The model uses an active progressive multimodal pretraining strategy that incrementally scales context length while maintaining performance. High-quality data selection ensures the model learns from relevant and diverse examples. The training approach combines vision-language pretraining with supervised fine-tuning on carefully curated datasets. A unified prompting strategy allows the model to handle complex multimodal inputs containing multiple images, varying resolutions, and embedded text in a consistent manner.

## Key Results
- Achieves 512K context length support, significantly exceeding typical multimodal model capabilities
- Outperforms most open-source models on benchmarks involving multiple images and videos
- Demonstrates strong performance on the Temporal Visual Needle in a Haystack benchmark for long video comprehension

## Why This Works (Mechanism)
OmChat's success stems from three key innovations: dynamic vision encoding that adapts to varying image resolutions, progressive pretraining that scales context length incrementally, and high-quality data selection. The dynamic vision encoding ensures that visual information is preserved regardless of input resolution, while progressive pretraining prevents catastrophic forgetting when extending context length. The data selection process ensures the model learns from relevant examples that cover the full range of expected use cases. Together, these mechanisms enable OmChat to handle both extremely long text contexts and complex multimodal inputs effectively.

## Foundational Learning

**Dynamic Vision Encoding**
- Why needed: Standard vision encoders struggle with varying image resolutions, leading to loss of visual information
- Quick check: Verify that encoded representations maintain consistent quality across different input resolutions

**Progressive Multimodal Pretraining**
- Why needed: Directly training on long contexts can cause catastrophic forgetting and training instability
- Quick check: Confirm that performance on shorter contexts is maintained as length increases

**High-Quality Data Selection**
- Why needed: Poor data quality can lead to degraded performance and biased learning
- Quick check: Analyze data distribution to ensure coverage of diverse scenarios and edge cases

## Architecture Onboarding

**Component Map**
Vision Encoder -> Dynamic Resolution Adapter -> Context Pooler -> Language Model -> Output Generator

**Critical Path**
Input images → Dynamic vision encoding → Multimodal fusion → Extended context processing → Language model generation

**Design Tradeoffs**
- Higher image resolution improves visual understanding but increases computational cost
- Longer context support requires more memory but enables better comprehension of extended narratives
- Unified prompting simplifies usage but may limit specialized optimizations for specific tasks

**Failure Signatures**
- Degradation in visual understanding when input resolution varies significantly
- Performance drop when context length exceeds training distribution
- Prompt sensitivity when multimodal inputs contain complex combinations of elements

**3 First Experiments**
1. Test dynamic vision encoding with images at extreme resolution differences to verify robustness
2. Evaluate context scaling by gradually increasing input length while monitoring performance metrics
3. Assess unified prompting strategy with inputs combining multiple image types, varying resolutions, and embedded text

## Open Questions the Paper Calls Out
None

## Limitations
- Claims about long context capabilities would benefit from independent verification across diverse use cases
- Performance generalizability to domains not covered in evaluation benchmarks is uncertain
- The 512K context length support, while impressive, requires validation for practical applications

## Confidence

**Model Architecture and Training Approach:** High
**Long Context Performance Claims:** Medium
- The progressive pretraining approach is technically sound, but independent verification is needed
**Video Understanding Capabilities:** Medium
- Strong performance on proposed benchmarks, but real-world applicability requires further testing
**Benchmark Comparisons:** Medium
- Comparisons are well-documented but may not capture all practical use cases

## Next Checks

1. Independent reproduction of the progressive pretraining strategy using publicly available datasets to verify the scaling claims for long context handling

2. Evaluation of OmChat's performance on video comprehension tasks beyond the proposed Temporal Visual Needle in a Haystack benchmark, particularly for real-world applications

3. Testing the unified prompting strategy with complex multimodal inputs that combine multiple image types, varying resolutions, and embedded text to assess practical usability limits