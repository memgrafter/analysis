---
ver: rpa2
title: 'WILT: A Multi-Turn, Memorization-Robust Inductive Logic Benchmark for LLMs'
arxiv_id: '2410.10998'
source_url: https://arxiv.org/abs/2410.10998
tags:
- test
- case
- self
- cases
- reasoning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Large language models (LLMs) struggle with multi-turn reasoning
  tasks that require gathering evidence over multiple interactions, as demonstrated
  by their poor performance on the Wason Inductive Logic Test (WILT). This benchmark
  presents models with a boolean function of three variables and requires them to
  infer the rule by proposing test cases and observing outcomes over up to 30 turns.
---

# WILT: A Multi-Turn, Memorization-Robust Inductive Logic Benchmark for LLMs

## Quick Facts
- arXiv ID: 2410.10998
- Source URL: https://arxiv.org/abs/2410.10998
- Reference count: 40
- Primary result: Large language models achieve only 28% accuracy on multi-turn inductive logic reasoning tasks

## Executive Summary
WILT presents a novel benchmark for evaluating large language models on multi-turn inductive logic reasoning tasks. The benchmark requires models to infer boolean functions of three variables by proposing test cases and observing outcomes over up to 30 turns. Unlike traditional benchmarks, WILT starts each test from a clean slate, preventing memorization-based solutions. The evaluation reveals that even state-of-the-art models struggle significantly with this task, achieving only 28% accuracy overall. The benchmark exposes distinct strengths and weaknesses in models' abilities to gather evidence through test case selection versus deducing final rules from observed data.

## Method Summary
The WILT benchmark evaluates LLMs on 50 hidden boolean functions of three variables (x, y, z) implemented as Python lambda functions. Models propose up to 30 test cases in the form of three-number tuples, observe True/False outcomes, and then submit a final guess as an equivalent Python lambda function. The benchmark is designed to resist memorization by starting each test from a clean slate with only initial instructions. Accuracy is calculated by verifying if final guesses are equivalent to hidden rules using generated test cases. The study evaluates multiple state-of-the-art models including Claude 3.5 Sonnet, GPT-4, and others, measuring both overall accuracy and specific capabilities like hypothesis space reduction efficiency and response complexity.

## Key Results
- State-of-the-art LLMs achieve only 28% accuracy on the WILT benchmark
- Claude 3.5 Sonnet performs best with 14/50 correct answers
- Models frequently repeat test cases and struggle with efficient hypothesis space reduction
- Distinct performance patterns emerge between models' abilities to propose effective test cases versus deducing final rules

## Why This Works (Mechanism)

### Mechanism 1
- Claim: WILT forces models to gather evidence through multiple test cases rather than relying on memorization
- Mechanism: The benchmark starts each test from a clean slate with only initial instructions, preventing models from using pre-learned responses
- Core assumption: The hidden rule is deterministic and changes for each test, making memorization ineffective
- Evidence anchors:
  - [abstract]: "each test starts from a clean slate, with only the initial instructions provided, preventing models from relying on pre-learned responses"
  - [section 2]: "All hidden rules take three numbers and return a boolean. These rules are simple and non-stochastic, so there is no additional value to posing the same test multiple times"
  - [corpus]: Weak - the corpus focuses on multi-turn reasoning but doesn't specifically address memorization resistance

### Mechanism 2
- Claim: WILT distinguishes between models' abilities to narrow hypothesis space versus deduce final rules
- Mechanism: The benchmark requires two distinct capabilities - proposing test cases that efficiently reduce possible rules and making accurate final guesses based on evidence
- Core assumption: These two reasoning capabilities are separable and can be measured independently
- Evidence anchors:
  - [abstract]: "we find that some of them are better on one of aforementioned sub-tasks than the other"
  - [section 4.3]: "To succeed at the WILT task, models must succeed at both gathering evidence (hypothesis reduction) and drawing logical conclusions from evidence (function inversion)"
  - [corpus]: Moderate - several papers discuss multi-turn reasoning but few separate hypothesis generation from deduction

### Mechanism 3
- Claim: WILT reveals limitations in LLM multi-turn reasoning that aren't captured by single-turn benchmarks
- Mechanism: By requiring iterative test case generation and observation, the benchmark exposes "doom loop" behaviors and inefficient hypothesis exploration
- Core assumption: Multi-turn reasoning failures are distinct from single-turn reasoning failures and reveal different model limitations
- Evidence anchors:
  - [abstract]: "Our findings reveal that LLMs struggle with this task, exhibiting distinct strengths and weaknesses"
  - [section 4.1]: "we find that LLMs struggle substantially with this task. We show that despite the test's relative simplicity, most models struggle substantially both to propose good tests and to infer a rule based on available evidence"
  - [corpus]: Moderate - the corpus includes papers on multi-turn reasoning but limited evidence on doom loops

## Foundational Learning

- Concept: Hypothesis space reduction
  - Why needed here: Models must efficiently eliminate impossible rules through test case selection
  - Quick check question: If a model tests (2, 4, 6) and gets True, which of these rules is definitely eliminated: x < y < z, x > y > z, or x = y = z?

- Concept: Inductive vs deductive reasoning
  - Why needed here: Models need inductive reasoning to propose test cases and deductive reasoning to infer final rules
  - Quick check question: Is proposing test case (1, 1, 1) to check if all equal is inductive (generalizing from examples) or deductive (applying a rule)?

- Concept: Confirmation bias in hypothesis testing
  - Why needed here: Models often fall into confirming their current hypothesis rather than trying to falsify it
  - Quick check question: If a model thinks the rule might be x < y < z, should it test more cases where x < y < z or try cases where x > y > z?

## Architecture Onboarding

- Component map: Test harness generates hidden rules → model proposes test cases → system returns True/False → repeat up to 30 times → model submits final guess
- Critical path: Test case proposal → system response → hypothesis update → final guess submission
- Design tradeoffs: Simple rules vs. sufficient complexity, number of test cases vs. computational cost, deterministic vs. probabilistic rules
- Failure signatures: Repeated test cases, late guesses with insufficient evidence, overly complex final rules, failure to explore edge cases
- First 3 experiments:
  1. Run baseline tests with a simple model to verify the test harness correctly generates and validates rules
  2. Test a known-good model on a subset of easy rules to establish performance floor
  3. Implement hypothesis space visualization to debug why models are choosing poor test cases

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What specific architectural changes to LLMs would most improve multi-turn reasoning performance on tasks like WILT?
- Basis in paper: Explicit - The paper identifies a significant gap in LLM performance on multi-turn reasoning tasks and suggests that strong single-turn performance does not transfer to multi-turn settings
- Why unresolved: The paper demonstrates the problem but doesn't explore architectural solutions or modifications that could address the identified deficiencies
- What evidence would resolve it: Comparative studies testing different model architectures (modified attention mechanisms, memory modules, or reasoning-specific training approaches) on WILT and similar multi-turn benchmarks

### Open Question 2
- Question: How does the performance gap between single-turn and multi-turn reasoning vary across different types of reasoning tasks beyond inductive logic?
- Basis in paper: Explicit - The paper notes that existing benchmarks are "overwhelmingly carefully chosen single-turn problems" and suggests the multi-turn reasoning gap may be broader than just WILT
- Why unresolved: The study only examines one specific type of multi-turn reasoning task (inductive logic), leaving open whether similar gaps exist in other reasoning domains
- What evidence would resolve it: Systematic comparison of model performance on equivalent single-turn versus multi-turn versions of various reasoning tasks (causal reasoning, analogical reasoning, etc.)

### Open Question 3
- Question: What is the optimal balance between exploration (proposing new test cases) and exploitation (guessing the rule) in multi-turn reasoning tasks?
- Basis in paper: Explicit - The paper notes that some models "tend to respond with many operators" while others "uses fewer operators" and discusses the importance of both hypothesis space reduction and final deduction
- Why unresolved: The paper observes different strategies among models but doesn't determine which approach is optimal or whether this varies by task difficulty
- What evidence would resolve it: Controlled experiments varying the number of allowed test cases and measuring performance trade-offs across different rule complexities and types

## Limitations

- Rule Space Coverage: The 50 hidden rules may not adequately span the full space of possible three-variable boolean functions
- Equivalence Verification Method: The approach for determining if a model's final guess matches the hidden rule lacks complete specification
- Performance Gap Attribution: The benchmark doesn't isolate whether poor performance stems specifically from multi-turn reasoning versus general reasoning limitations

## Confidence

- WILT effectively prevents memorization: High
- Models show distinct strengths in hypothesis reduction vs function inversion: High
- Multi-turn reasoning represents a significant gap for LLMs: Medium

## Next Checks

1. Implement equivalence verification testing to verify the lambda function comparison method correctly identifies equivalent boolean functions
2. Analyze rule type distribution to verify the 50 hidden rules provide adequate coverage across different reasoning patterns
3. Compare single-turn vs multi-turn performance to isolate whether the performance gap is specifically due to turn-based reasoning or general reasoning ability