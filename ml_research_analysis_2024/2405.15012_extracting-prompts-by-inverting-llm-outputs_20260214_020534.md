---
ver: rpa2
title: Extracting Prompts by Inverting LLM Outputs
arxiv_id: '2405.15012'
source_url: https://arxiv.org/abs/2405.15012
tags:
- prompt
- outputs
- prompts
- user
- swiftui
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: output2prompt is a new black-box prompt extraction method that
  learns to extract prompts by inverting LLM outputs without access to logits or adversarial
  queries. It employs a sparse encoder architecture that reduces memory complexity
  from quadratic to linear in the number of outputs.
---

# Extracting Prompts by Inverting LLM Outputs

## Quick Facts
- arXiv ID: 2405.15012
- Source URL: https://arxiv.org/abs/2405.15012
- Authors: Collin Zhang; John X. Morris; Vitaly Shmatikov
- Reference count: 24
- Key outcome: output2prompt achieves 96.7% cosine similarity in prompt extraction from Llama-2 Chat outputs, outperforming logit2prompt while requiring two orders of magnitude fewer training samples

## Executive Summary
This paper introduces output2prompt, a black-box prompt extraction method that learns to extract prompts by inverting LLM outputs without access to logits or adversarial queries. The method employs a sparse encoder architecture that reduces memory complexity from quadratic to linear in the number of outputs. When applied to Llama-2 Chat (7B) outputs, output2prompt achieves 96.7% cosine similarity in prompt extraction, outperforming logit2prompt (93.5%) while requiring two orders of magnitude fewer training samples. The method transfers across different LLMs with cosine similarities above 92%, successfully extracting both user and system prompts.

## Method Summary
output2prompt is an encoder-decoder model trained to map concatenated LLM outputs back to their original prompts. The method uses a sparse transformer encoder that limits self-attention to individual outputs rather than across all outputs, reducing memory complexity from O(n²l²) to O(nl²). The decoder is a standard autoregressive transformer that generates the extracted prompt token by token. The model is trained using the autoregressive language modeling objective to maximize the likelihood of the original prompt given the concatenated LLM outputs. The approach requires only normal user queries and their corresponding outputs, without needing access to model logits or adversarial queries.

## Key Results
- output2prompt achieves 96.7% cosine similarity on Llama-2 Chat (7B) prompt extraction
- Outperforms logit2prompt (93.5% cosine similarity) while requiring two orders of magnitude fewer training samples
- Successfully transfers across different LLMs with cosine similarities above 92%
- Extracted prompts are semantically equivalent to originals, enabling practical cloning of LLM-based applications

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The sparse encoder reduces memory complexity from quadratic to linear by limiting cross-attention between different LLM outputs
- Mechanism: Instead of full self-attention where each token attends to all tokens across all LLM outputs, the sparse encoder applies self-attention only within each individual output
- Core assumption: Cross-attention between different LLM outputs is not strictly necessary for prompt extraction, as outputs are generally independent
- Evidence anchors:
  - [abstract] "output2prompt employs a new sparse encoding techique" and "reduces memory complexity from quadratic to linear in the number of outputs"
  - [section 3.2] "we hypothesize that little is gained from cross-attention between different sequences yi on the input side of the encoder"

### Mechanism 2
- Claim: Multiple LLM outputs contain sufficient information to reconstruct the original prompt without requiring access to logits
- Mechanism: By concatenating n LLM outputs generated from the same prompt, the model captures diverse aspects of the prompt's semantic content
- Core assumption: The semantic content of LLM outputs is correlated with the prompt content, and multiple outputs provide complementary information
- Evidence anchors:
  - [abstract] "output2prompt only needs outputs of normal user queries" and "two orders of magnitude fewer training samples"
  - [section 3.1] "we conjecture that the difference between the samples provides useful information for prompt extraction"

### Mechanism 3
- Claim: The inversion model transfers across different LLM architectures due to the universal nature of semantic embeddings
- Mechanism: The model learns to map semantic patterns in LLM outputs to prompts, which transfers across models because similar semantic content produces similar embeddings
- Core assumption: Different LLMs produce semantically similar embeddings for similar content, enabling cross-model transfer
- Evidence anchors:
  - [abstract] "output2prompt transfers across different language models (both base and instruction-tuned) with little loss in performance, maintaining cosine similarity above 92"
  - [section 5.2] "Although BLEU scores decrease when output2prompt is applied to others LLMs, cosine similarities remain above 92%"

## Foundational Learning

- Concept: Transformer architecture and attention mechanisms
  - Why needed here: Understanding how sparse attention differs from full attention and why it reduces memory complexity
  - Quick check question: How does the memory complexity of full self-attention scale with sequence length compared to sparse attention?

- Concept: Autoregressive language modeling and next-token prediction
  - Why needed here: Understanding how the inversion model is trained to maximize the likelihood of the prompt given LLM outputs
  - Quick check question: What is the objective function used to train the inversion model, and how does it relate to standard language modeling?

- Concept: Semantic similarity and embedding spaces
  - Why needed here: Understanding why cosine similarity is used as a metric and how semantic similarity enables cross-model transfer
  - Quick check question: How does cosine similarity between embeddings capture semantic similarity between different wordings of the same prompt?

## Architecture Onboarding

- Component map: LLM outputs -> Sparse encoder (limited self-attention) -> Decoder (autoregressive) -> Extracted prompt
- Critical path:
  1. Generate LLM outputs from target prompt
  2. Concatenate outputs and feed to sparse encoder
  3. Pass encoder states to decoder
  4. Generate extracted prompt using greedy decoding
  5. Compare with ground truth using cosine similarity
- Design tradeoffs:
  - Sparse vs. full attention: Memory efficiency vs. potential loss of cross-output information
  - Number of outputs (n): More outputs improve extraction quality but increase memory and computation
  - Model size: Larger models may capture more complex patterns but require more resources
- Failure signatures:
  - Low cosine similarity but high BLEU score: Extracted prompt uses different wording but captures semantics
  - High cosine similarity but low functional equivalence: Semantically similar but prompts produce different outputs
  - Poor performance on longer prompts: Model trained on shorter prompts struggles with longer ones
- First 3 experiments:
  1. Test sparse vs. full attention on a small dataset to verify memory savings and accuracy
  2. Vary the number of LLM outputs (n=2, 4, 8, 16) to find the sweet spot for extraction quality
  3. Test cross-model transfer by training on Llama-2 and testing on GPT-3.5 outputs

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of output2prompt compare to logit2text when the number of available outputs per prompt exceeds 64?
- Basis in paper: [explicit] The paper states that "Performance stops increasing after 64 outputs" and "output2prompt outperforms logit2text when we increase the number of outputs because performance of our extraction model depends on (a) the information contained in the LLM outputs, and (b) its ability to reconstruct the prompt from this information."
- Why unresolved: The paper only evaluates up to 64 outputs for both methods, so it's unclear if the performance gap would widen or narrow with more outputs.
- What evidence would resolve it: A controlled experiment comparing both methods using 128, 256, and 512 outputs per prompt.

### Open Question 2
- Question: What is the maximum prompt length that output2prompt can effectively extract, and how does performance degrade beyond this limit?
- Basis in paper: [explicit] The paper mentions "Our output2prompt model is trained to generated prompts of around 200 tokens" and tests prompts up to 433 tokens, but doesn't explore longer prompts systematically.
- Why unresolved: The paper only tests prompts up to 433 tokens, leaving open questions about performance on enterprise-level prompts or complex technical documentation that may exceed this length.
- What evidence would resolve it: A systematic evaluation of output2prompt on prompts ranging from 500 to 2000 tokens, measuring cosine similarity and BLEU scores at each length.

### Open Question 3
- Question: How does output2prompt perform on prompts that contain deliberate obfuscation or anti-extraction patterns?
- Basis in paper: [inferred] The paper discusses adversarial defenses but doesn't test the model against prompts specifically designed to resist extraction, such as those using variable syntax, encoded information, or dynamic content generation.
- Why unresolved: The paper evaluates on "normal" prompts but doesn't address whether sophisticated prompt engineers could create prompts that are inherently more resistant to extraction.
- What evidence would resolve it: A controlled study where prompts are designed with anti-extraction features (variable syntax, encoded information, dynamic content) and tested against output2prompt.

## Limitations

- Sparse attention mechanism may not capture all relevant cross-output dependencies, though the paper assumes this provides minimal benefit
- Cross-model transfer success is limited to specific model pairs and may not generalize to more architecturally divergent models
- Method's reliance on multiple LLM outputs creates practical constraints for resource-constrained environments and real-time applications

## Confidence

- **High confidence**: Memory complexity reduction claims, basic extraction mechanism (sparse encoder + autoregressive decoder), cross-model transfer > 92% cosine similarity
- **Medium confidence**: 96.7% extraction accuracy on Llama-2 Chat, practical equivalence of extracted prompts, two orders of magnitude fewer training samples
- **Low confidence**: Semantic equivalence of extracted vs original prompts in real applications, performance on adversarially designed prompts, transfer to highly divergent model architectures

## Next Checks

1. **Ablation study on sparse vs full attention**: Train two versions of output2prompt (sparse and full attention) on the same dataset, systematically compare memory usage and extraction quality across varying numbers of LLM outputs (n=2, 4, 8, 16) to quantify the tradeoff between efficiency and accuracy.

2. **Cross-architecture transfer robustness**: Test transfer performance from Llama-2 to more architecturally divergent models like Claude, PaLM, or domain-specific LLMs (BioBERT, CodeLlama) to determine if the 92% cosine similarity threshold holds across fundamentally different architectures.

3. **Adversarial prompt resistance**: Generate prompts designed to be semantically ambiguous or produce highly variable LLM outputs, then test whether output2prompt can reliably extract the original prompt or if the method breaks down when outputs are less correlated with the prompt.