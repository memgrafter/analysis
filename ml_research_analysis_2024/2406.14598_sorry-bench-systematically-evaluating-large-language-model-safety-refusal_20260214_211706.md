---
ver: rpa2
title: 'SORRY-Bench: Systematically Evaluating Large Language Model Safety Refusal'
arxiv_id: '2406.14598'
source_url: https://arxiv.org/abs/2406.14598
tags:
- safety
- unsafe
- refusal
- dataset
- llms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces SORRY-Bench, a systematic benchmark to evaluate
  large language models'' (LLMs) safety refusal behaviors. The authors address three
  limitations in existing safety evaluations: imbalanced coverage of unsafe topics,
  lack of consideration for linguistic variations in prompts, and reliance on computationally
  expensive LLM-as-a-judge methods.'
---

# SORRY-Bench: Systematically Evaluating Large Language Model Safety Refusal

## Quick Facts
- arXiv ID: 2406.14598
- Source URL: https://arxiv.org/abs/2406.14598
- Authors: Tinghao Xie; Xiangyu Qi; Yi Zeng; Yangsibo Huang; Udari Madhushani Sehwag; Kaixuan Huang; Luxi He; Boyi Wei; Dacheng Li; Ying Sheng; Ruoxi Jia; Bo Li; Kai Li; Danqi Chen; Peter Henderson; Prateek Mittal
- Reference count: 40
- Key outcome: Introduces a systematic benchmark for evaluating LLM safety refusal behaviors, addressing imbalanced topic coverage, linguistic variations, and computational costs.

## Executive Summary
This paper introduces SORRY-Bench, a comprehensive benchmark for evaluating large language model safety refusal behaviors. The authors address three key limitations in existing safety evaluations: imbalanced coverage of unsafe topics, lack of consideration for linguistic variations in prompts, and reliance on computationally expensive LLM-as-a-judge methods. They propose a fine-grained 45-class safety taxonomy, create a class-balanced dataset with linguistic augmentations, and develop an efficient automated safety evaluator by fine-tuning a 7B LLM on human judgment data.

## Method Summary
The authors developed SORRY-Bench through a multi-stage process: first curating a fine-grained 45-class safety taxonomy by aggregating and refining prior taxonomies; second, creating a class-balanced dataset of 450 unsafe instructions supplemented with 20 linguistic augmentations; third, collecting 7K+ human annotations to train a fine-tuned 7B LLM safety judge; and finally, evaluating over 50 proprietary and open-source LLMs using this framework to assess safety refusal behaviors across different categories and linguistic variations.

## Key Results
- Evaluated over 50 proprietary and open-source LLMs, finding significant variations in safety refusal behaviors across models and categories
- Claude-2 and Gemini-1.5 exhibit the most refusals, while Mistral models demonstrate higher compliance with unsafe requests
- Fine-tuned 7B LLMs can achieve accuracy comparable to GPT-4 scale LLMs (over 80% human agreement) with lower computational cost
- Linguistic mutations significantly impact safety evaluations, with current models showing inconsistent handling of low-resource languages and uncommon dialects

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Fine-grained 45-class taxonomy enables systematic safety evaluation by unifying inconsistent prior taxonomies.
- **Mechanism:** The authors compile prior safety taxonomies into a 45-class taxonomy, mapping existing datasets to it and iteratively refining to cover previously overlooked categories.
- **Core assumption:** A granular, unified taxonomy better captures safety risks than coarse-grained or inconsistent ones.
- **Evidence anchors:** [abstract]: "existing methods often use coarse-grained taxonomies of unsafe topics, and are over-representing some fine-grained topics." [section]: "Our taxonomy curation method consists of two stages. In the first stage, we aggregate the safety taxonomies from 10 prior safety benchmark datasets... In the second stage, we keep on refining this taxonomy via a human-in-the-loop process."
- **Break condition:** If the taxonomy doesn't adequately cover real-world unsafe topics, or if human-in-the-loop refinement is insufficient to identify new categories.

### Mechanism 2
- **Claim:** Automated fine-tuned 7B LLM judges can achieve accuracy comparable to GPT-4 at lower computational cost.
- **Mechanism:** By collecting 7K+ human annotations and fine-tuning a 7B LLM on this data, the authors create a fast and accurate safety evaluator.
- **Core assumption:** Smaller LLMs can match larger ones on specialized tasks when fine-tuned on sufficient high-quality data.
- **Evidence anchors:** [abstract]: "we show that fine-tuned 7B LLMs can achieve accuracy comparable to GPT-4 scale LLMs, with lower computational cost." [section]: "Our finding suggests that small (7B) LLMs, when fine-tuned on sufficient human annotations, can achieve satisfactory accuracy (over 80% human agreement), comparable with and even surpassing larger scale LLMs (e.g., GPT-4o)."
- **Break condition:** If the human annotation dataset is insufficient, biased, or if the 7B LLM architecture has fundamental limitations for this task.

### Mechanism 3
- **Claim:** Class-balanced dataset with linguistic augmentations ensures comprehensive evaluation of safety refusal across diverse real-world scenarios.
- **Mechanism:** The authors create a balanced dataset with 450 unsafe instructions across 45 categories, then augment with 20 linguistic mutations (writing styles, persuasion techniques, encodings, languages).
- **Core assumption:** Real-world unsafe prompts vary in linguistic characteristics, and evaluation should reflect this diversity.
- **Evidence anchors:** [abstract]: "We supplement SORRY-Bench with 20 diverse linguistic augmentations to systematically examine these effects." [section]: "To ensure equal coverage of these variations, we isolate and decouple prompt-level linguistic patterns... This results in 20 * 450 = 9K additional unsafe instructions, capturing diverse formatting and linguistic features."
- **Break condition:** If the linguistic mutations don't accurately represent real-world variations, or if the class balance doesn't reflect actual distribution of unsafe requests.

## Foundational Learning

- **Concept:** Fine-grained safety taxonomies
  - Why needed here: Coarse-grained taxonomies obscure important safety distinctions and lead to imbalanced evaluation coverage.
  - Quick check question: What is the difference between evaluating at the category level versus the subcategory level in safety benchmarks?

- **Concept:** Human-in-the-loop data annotation
  - Why needed here: Automated classification of unsafe prompts to taxonomy categories is insufficient; human review ensures coverage of overlooked categories.
  - Quick check question: Why might automated classification fail to identify all relevant safety categories in a dataset?

- **Concept:** LLM-as-a-judge methodology
  - Why needed here: Manual evaluation is too slow for large-scale benchmarking; automated judges must balance accuracy and efficiency.
  - Quick check question: What are the key design choices when implementing an LLM as a judge for safety evaluation?

## Architecture Onboarding

- **Component map:** Taxonomy curation system -> Dataset collection pipeline -> Human annotation interface -> LLM judge training system -> Benchmark evaluation engine

- **Critical path:** 1. Curate taxonomy from prior work 2. Map existing datasets to taxonomy 3. Collect class-balanced dataset with augmentations 4. Collect human judgments for training judge 5. Fine-tune small LLM judge 6. Evaluate models using judge

- **Design tradeoffs:** Taxonomy granularity vs. annotation complexity; Dataset size vs. annotation cost; Judge model size vs. evaluation speed; Linguistic augmentations vs. real-world relevance

- **Failure signatures:** Imbalanced results across categories suggest taxonomy issues; Judge agreement far below 80% suggests annotation or training problems; Extreme model variation suggests dataset or judge issues

- **First 3 experiments:** 1. Test taxonomy mapping on a small sample of existing datasets to verify coverage 2. Evaluate judge accuracy on a held-out human annotation subset 3. Run a subset of models on a subset of the dataset to validate the full pipeline

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How would the safety refusal behavior of LLMs change if evaluated on a non-binary harmfulness scale rather than binary compliance/refusal?
- Basis in paper: [explicit] The authors discuss this as a potential limitation and future step, noting that "a more desirable and challenging goal is to quantify the 'actual harmfulness' of model responses at a non-binary scale."
- Why unresolved: The paper focuses on binary-scale safety refusal behaviors because most current safety policies define model behavior binarily, and the authors acknowledge that defining "actual harmfulness" at a non-binary scale is more ambiguous at this moment.
- What evidence would resolve it: Developing and testing a comprehensive non-binary harmfulness evaluation framework for LLM safety, with clear scoring rubrics and human judgment datasets, would resolve this question.

### Open Question 2
- Question: How significant is the data contamination issue for safety benchmarks like SORRY-Bench, and what are the best practices to mitigate it?
- Basis in paper: [explicit] The authors mention data contamination as a potential limitation, stating "our dataset may suffer from data contamination issues" and that "future model developers may (accidentally) include our dataset into their training corpus, and may thus overfit on our benchmark."
- Why unresolved: The paper acknowledges the concern but does not provide empirical evidence on the extent of the problem or propose concrete solutions beyond suggesting the development of a private split of the dataset.
- What evidence would resolve it: Conducting empirical studies on the impact of data contamination on safety benchmark results and developing robust methodologies to prevent or detect contamination would address this question.

### Open Question 3
- Question: How do jailbreaking attacks and defenses integrate with the SORRY-Bench framework, and what is their impact on safety refusal evaluation?
- Basis in paper: [inferred] The authors mention jailbreaking as a limitation, noting that their focus is on "average-case bad users" and that "numerous jailbreaking methods have been proposed to compromise LLM safety, capturing the malicious actions that worst-case adversaries would take."
- Why unresolved: The paper deliberately focuses on average-case scenarios and leaves the integration of jailbreaking attacks and defenses as a future step, recognizing the distinct focuses of average-case vs. worst-case evaluations.
- What evidence would resolve it: Incorporating jailbreaking attacks and defenses into the SORRY-Bench framework and evaluating their impact on safety refusal rates across different LLM models would answer this question.

## Limitations
- Taxonomy constructed from 10 prior datasets may have inherent biases in unsafe topic coverage
- Human annotation dataset of 7K+ judgments may not generalize across different cultural contexts
- Linguistic augmentations represent a curated set rather than comprehensive exploration of real-world jailbreak attempts

## Confidence
- **High confidence:** Systematic approach to taxonomy construction, methodology for creating class-balanced dataset, and basic framework for LLM-as-a-judge evaluation
- **Medium confidence:** Specific 45-class taxonomy effectively captures relevant safety concerns, and human annotation quality is sufficient for reliable judge training
- **Medium confidence:** Linguistic augmentations meaningfully stress-test model safety behaviors across diverse prompt variations
- **Low confidence:** Generalizability of findings to all safety scenarios, particularly in low-resource languages and uncommon dialects

## Next Checks
1. **Cross-cultural validation:** Test the taxonomy and judge on safety datasets from different cultural contexts to assess generalizability beyond the original annotation pool
2. **Temporal robustness test:** Evaluate whether the safety taxonomy and judge maintain effectiveness as new safety threats emerge over time, particularly for categories that were difficult to annotate initially
3. **Real-world deployment audit:** Apply the benchmark to evaluate actual deployed models in production environments, comparing benchmark predictions with observed safety incidents and user reports