---
ver: rpa2
title: 'LaVy: Vietnamese Multimodal Large Language Model'
arxiv_id: '2404.07922'
source_url: https://arxiv.org/abs/2404.07922
tags:
- vietnamese
- language
- arxiv
- lavy
- large
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces LaVy, the first Vietnamese multimodal large
  language model (MLLM), addressing the lack of high-quality multimodal resources
  for Vietnamese. LaVy leverages a LLaVA architecture with a CLIP vision encoder and
  a Vietnamese LLM backbone, trained on a curated dataset of 874K image-text pairs
  and 166K Vietnamese instructions.
---

# LaVy: Vietnamese Multimodal Large Language Model

## Quick Facts
- arXiv ID: 2404.07922
- Source URL: https://arxiv.org/abs/2404.07922
- Authors: Chi Tran; Huong Le Thanh
- Reference count: 4
- First Vietnamese multimodal large language model achieving 45.3% accuracy on OpenViVQA benchmark

## Executive Summary
LaVy introduces the first Vietnamese multimodal large language model (MLLM), addressing the critical gap in high-quality multimodal resources for Vietnamese language. The model combines a CLIP vision encoder with a Vietnamese LLM backbone using LLaVA architecture, trained on a carefully curated dataset of 874K image-text pairs and 166K Vietnamese instructions. LaVy-Bench, a novel benchmark for Vietnamese MLLMs, includes zero-shot visual question answering and in-the-wild tasks. The model demonstrates significant performance improvements over multilingual baselines, achieving 45.3% accuracy on OpenViVQA compared to 27.9% for mBLIP-Bloomz-7B, and scoring 67.2/100 on in-the-wild tasks. However, challenges remain with text-heavy image processing and occasional hallucinations.

## Method Summary
LaVy leverages the LLaVA architecture by integrating a CLIP vision encoder with a Vietnamese language model backbone. The training process utilizes a curated dataset containing 874K image-text pairs and 166K Vietnamese instructions, specifically designed for Vietnamese multimodal understanding. The model is evaluated using LaVy-Bench, a newly introduced benchmark featuring zero-shot visual question answering and in-the-wild tasks that assess real-world Vietnamese multimodal capabilities. The architecture maintains the core LLaVA design while adapting the LLM component for Vietnamese language processing.

## Key Results
- Achieves 45.3% accuracy on OpenViVQA benchmark, outperforming mBLIP-Bloomz-7B's 27.9%
- Scores 67.2/100 on in-the-wild Vietnamese multimodal tasks
- Outperforms multilingual baseline mBLIP-LLaMA2-7B with 45.3% vs 32.7% accuracy on benchmark tasks

## Why This Works (Mechanism)
LaVy's effectiveness stems from the successful adaptation of LLaVA architecture to Vietnamese language processing through specialized training data curation. The combination of CLIP's strong visual feature extraction with a Vietnamese-optimized LLM backbone enables effective cross-modal reasoning. The curated training dataset of nearly one million image-text pairs provides sufficient diversity for learning Vietnamese multimodal patterns. The introduction of LaVy-Bench specifically addresses the evaluation gap for Vietnamese MLLMs, enabling proper assessment of model capabilities in culturally and linguistically relevant contexts.

## Foundational Learning
- **Multimodal learning fundamentals**: Understanding how vision and language models interact through cross-modal attention mechanisms (needed for grasping LaVy's architecture; quick check: verify attention visualization between vision and language components)
- **Vision-language pretraining**: Knowledge of CLIP-like models and their feature extraction capabilities (needed to understand vision encoder role; quick check: examine CLIP embedding dimensions and pooling strategies)
- **Vietnamese NLP challenges**: Awareness of specific linguistic features in Vietnamese that affect model design (needed to appreciate backbone modifications; quick check: review Vietnamese tokenization strategies)
- **Zero-shot learning principles**: Understanding how models generalize to unseen tasks without task-specific training (needed for interpreting LaVy-Bench results; quick check: analyze zero-shot vs few-shot performance differences)
- **Cross-lingual model evaluation**: Methods for comparing multilingual models across different language benchmarks (needed for interpreting performance claims; quick check: verify benchmark translation consistency)
- **Hallucination detection in MLLMs**: Techniques for identifying and measuring model hallucinations in multimodal contexts (needed to understand limitations; quick check: implement hallucination detection metrics)

## Architecture Onboarding

Component Map:
Vietnamese LLM Backbone -> Cross-Modal Attention Layer -> CLIP Vision Encoder

Critical Path:
Image input → CLIP vision encoder → Visual feature extraction → Cross-modal attention fusion → Vietnamese LLM processing → Text generation

Design Tradeoffs:
- Vietnamese-specific optimization vs. multilingual generalization
- Training data curation effort vs. model performance
- Computational resources for large-scale multimodal training vs. practical deployment feasibility
- Zero-shot evaluation capability vs. task-specific fine-tuning potential

Failure Signatures:
- TextQA tasks show particularly low performance (30.8% accuracy)
- Hallucinations occur during complex reasoning tasks
- Performance degradation on text-heavy images
- Potential bias toward certain image types present in training data

First 3 Experiments to Run:
1. Evaluate LaVy on English visual question answering benchmarks to assess cross-lingual transfer capabilities
2. Conduct ablation study removing Vietnamese-specific training data to measure performance impact
3. Test model performance on progressively longer text sequences in images to identify breaking points

## Open Questions the Paper Calls Out
None

## Limitations
- Significant performance gap on TextQA tasks (30.8% accuracy) indicates limitations with text-heavy image processing
- Occasional hallucinations during complex reasoning tasks suggest reliability concerns
- Lack of comparison against state-of-the-art English MLLMs makes relative positioning unclear
- LaVy-Bench dataset quality and difficulty calibration have not been independently verified

## Confidence

High Confidence:
- LaVy achieves state-of-the-art performance among Vietnamese MLLMs on OpenViVQA (45.3% vs 27.9% for mBLIP-Bloomz-7B)
- Architectural claims about using LLaVA framework with Vietnamese LLM backbone are verifiable through model weights and code release

Medium Confidence:
- Claims about LaVy being the first Vietnamese MLLM require verification of the literature, though the paper provides reasonable evidence
- Superiority over mBLIP-LLaMA2-7B (45.3% vs 32.7%) is supported by reported results but depends on LaVy-Bench quality

Low Confidence:
- Claims about general superiority over English MLLMs are not tested
- Practical utility claims for real-world Vietnamese applications lack user study validation
- Hallucination frequency and impact are not quantitatively characterized

## Next Checks
1. **Cross-lingual benchmarking**: Evaluate LaVy against GPT-4V and Gemini on Vietnamese tasks from general MLLM benchmarks to establish relative performance to leading models

2. **LaVy-Bench quality audit**: Conduct inter-annotator agreement studies on the LaVy-Bench dataset and analyze question distribution to verify benchmark reliability and difficulty calibration

3. **Real-world deployment study**: Deploy LaVy in Vietnamese-specific applications (e.g., document processing, visual question answering in Vietnamese educational materials) and measure hallucination rates and user satisfaction through controlled studies