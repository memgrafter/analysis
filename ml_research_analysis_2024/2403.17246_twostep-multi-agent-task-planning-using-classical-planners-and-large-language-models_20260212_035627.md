---
ver: rpa2
title: 'TwoStep: Multi-agent Task Planning using Classical Planners and Large Language
  Models'
arxiv_id: '2403.17246'
source_url: https://arxiv.org/abs/2403.17246
tags:
- pddl
- planning
- goal
- agent
- domain
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper presents a method that uses LLMs to decompose a single-agent
  planning problem into subgoals for multiple agents, each solved with classical PDDL
  planners, to enable faster parallel execution than both single-agent and multi-agent
  PDDL approaches. The approach achieves up to 64.7% faster planning time and 13.2%
  shorter execution length on average compared to multi-agent PDDL, while guaranteeing
  plan success.
---

# TwoStep: Multi-agent Task Planning using Classical Planners and Large Language Models

## Quick Facts
- arXiv ID: 2403.17246
- Source URL: https://arxiv.org/abs/2403.17246
- Authors: David Bai; Ishika Singh; David Traum; Jesse Thomason
- Reference count: 23
- Primary result: LLM-based multi-agent task planning achieves up to 64.7% faster planning time and 13.2% shorter execution length compared to multi-agent PDDL

## Executive Summary
This paper presents TWOSTEP, a method that combines classical planning with large language models (LLMs) for multi-agent task planning. The approach uses LLMs to decompose single-agent planning problems into subgoals for multiple agents, each solved independently with classical PDDL planners. By enabling parallel execution of agent-specific plans, TWOSTEP achieves significant improvements in both planning time and execution efficiency compared to traditional multi-agent PDDL approaches.

The key innovation is leveraging LLM commonsense reasoning to approximate human intuitions for goal decomposition, allowing the system to handle complex multi-agent scenarios without the exponential complexity growth typical of multi-agent PDDL. The method was evaluated across five symbolic planning domains and an embodied environment (AI2THOR), demonstrating consistent improvements over both single-agent and multi-agent baselines.

## Method Summary
TWOSTEP decomposes multi-agent planning problems by using an LLM to generate subgoals in natural language, which are then translated into PDDL goal format for each agent. Each agent independently solves its subgoal using a classical PDDL planner (FAST-DOWNWARD), enabling parallel execution. The approach assumes subgoals can be solved independently and that all agents will eventually achieve their respective goals. The main agent's state is updated based on helper agents' actions, and plans are coordinated through a central execution coordinator.

## Key Results
- TWOSTEP achieves up to 64.7% faster planning time compared to multi-agent PDDL approaches
- Execution length is 13.2% shorter on average compared to multi-agent PDDL
- Maintains high success rates across all tested domains while improving efficiency
- Outperforms both single-agent PDDL and multi-agent PDDL baselines in planning time and execution efficiency

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLM-based subgoal generation enables parallel execution of multi-agent tasks.
- Mechanism: The LLM infers helper subgoals that can run concurrently with the main agent, reducing overall plan length.
- Core assumption: LLMs can identify subgoals that are independent enough to execute in parallel while still contributing to the overall goal.
- Evidence anchors:
  - [abstract] "We combine the strengths of both classical planning and LLMs by approximating human intuitions for multi-agent planning goal decomposition."
  - [section III.a] "We hypothesize that LLMs can infer helper subgoals that enable parallel execution alongside the main agent while assuming all agents will eventually achieve their respective goals."
  - [corpus] Weak evidence - related papers focus on planning with LLMs but don't specifically address multi-agent subgoal decomposition for parallel execution.
- Break condition: If the LLM generates subgoals that require waiting for other agents or conflict with each other, parallel execution efficiency will decrease.

### Mechanism 2
- Claim: Decomposing multi-agent problems into single-agent problems reduces planning time complexity.
- Mechanism: Each agent solves a simpler single-agent PDDL problem rather than a complex multi-agent PDDL problem, reducing the exponential growth in search space.
- Core assumption: The search space complexity grows exponentially with the number of agents in multi-agent PDDL, but remains manageable when decomposed into single-agent problems.
- Evidence anchors:
  - [abstract] "We demonstrate that LLM-based goal decomposition leads to faster planning times than solving multi-agent PDDL problems directly"
  - [section II.C] "agents share the same environment, so the environment-specific state conditions are shared between the agents. However, agent-specific state conditions are unique to each agent."
  - [section IV.B] "initializing another agent increases the number of possible actions at any state, as well as introducing additional state conditions, leading to a much larger search space."
- Break condition: If the subgoals are not truly independent or require extensive coordination, the planning time advantage may disappear.

### Mechanism 3
- Claim: LLM-generated subgoals approximate human expert subgoals in quality.
- Mechanism: The LLM uses in-context examples and commonsense reasoning to generate subgoals similar to those a human expert would specify.
- Core assumption: LLMs can learn from in-context examples to generate subgoals that are functionally equivalent to human-annotated ones.
- Evidence anchors:
  - [section V] "We perform human studies to assess the quality LLM inferred subgoals in TWOSTEP. In the first study, we ask a PDDL expert...to annotate the subgoal in PDDL goal format"
  - [abstract] "Additionally, we find that LLM-based approximations of subgoals result in similar multi-agent execution lengths to those specified by human experts."
  - [corpus] Weak evidence - no direct comparison of LLM vs human subgoal generation quality in related papers.
- Break condition: If the LLM consistently generates subgoals that are less efficient or incorrect compared to human annotations, the approximation claim fails.

## Foundational Learning

- Concept: Classical planning and PDDL syntax
  - Why needed here: Understanding how domains and problems are structured in PDDL is essential for implementing the single-agent planning components
  - Quick check question: What are the two main components of a PDDL planning problem and what does each define?

- Concept: Multi-agent planning complexity
  - Why needed here: Understanding why multi-agent planning becomes exponentially harder helps appreciate the need for decomposition
  - Quick check question: How does the search space grow when adding agents to a PDDL problem?

- Concept: LLM in-context learning
  - Why needed here: The approach relies on LLMs generating subgoals from in-context examples, so understanding this mechanism is crucial
  - Quick check question: How does providing examples in the prompt influence the LLM's generation of subgoals?

## Architecture Onboarding

- Component map:
  LLM subgoal generator (English) -> Subgoal translator (PDDL) -> Single-agent PDDL planner (FAST-DOWNWARD) -> Plan execution coordinator -> State editor for main agent

- Critical path:
  LLM generates English subgoals → Translate to PDDL → Each agent plans independently → Execute plans in parallel → Update main agent state

- Design tradeoffs:
  - LLM inference time vs planning time savings
  - Subgoal independence vs overall plan efficiency
  - Parallel execution vs coordination overhead

- Failure signatures:
  - LLM generates conflicting or dependent subgoals
  - PDDL planner fails to find plan for a subgoal
  - Parallel execution results in deadlocks or inefficiencies

- First 3 experiments:
  1. Test LLM subgoal generation on simple domains (BLOCKSWORLD) with known optimal solutions
  2. Compare planning time for single decomposed problems vs multi-agent PDDL on the same tasks
  3. Measure execution length with 2 agents on tasks where partial parallelism is possible

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the quality of LLM-generated subgoals scale with increasing numbers of agents beyond 4?
- Basis in paper: [explicit] The paper tests up to 4 agents but notes the LLM often decides not to generate more subgoals than 3, even when given the option to.
- Why unresolved: The paper does not explore agent counts beyond 4 or analyze the LLM's decision-making process for subgoal generation limits.
- What evidence would resolve it: Testing with 5+ agents and analyzing LLM-generated subgoals to determine if quality degrades or if there's an optimal agent count.

### Open Question 2
- Question: How sensitive is the approach to variations in domain complexity and structure, particularly when agent-specific conditions are not defined?
- Basis in paper: [inferred] The paper notes that MA PDDL performs poorly on TYREWORLD (no agent-specific conditions) and struggles with TERMES (high complexity), while TWOSTEP works with or without such conditions.
- Why unresolved: The paper doesn't systematically vary domain complexity or agent condition definitions to quantify the impact on TWOSTEP's performance.
- What evidence would resolve it: Testing TWOSTEP across domains with varying levels of agent-specific conditions and complexity metrics to measure performance changes.

### Open Question 3
- Question: What is the impact of LLM inference variability on planning time and execution quality across multiple runs?
- Basis in paper: [explicit] The paper reports TWOSTEP results with standard deviations across 3 runs, noting that even with temperature 0, OpenAI's API is nondeterministic.
- Why unresolved: The paper only shows average results and doesn't analyze the distribution of planning times or execution lengths across runs.
- What evidence would resolve it: Detailed statistical analysis of multiple runs showing variance in planning time, execution length, and success rates, including worst-case scenarios.

## Limitations
- Effectiveness heavily depends on LLM's ability to generate appropriate subgoals, with limited analysis of failure modes
- Human study validation is limited to PDDL expert annotation rather than testing actual task completion quality
- Embodied environment results (AI2THOR) are sparse with only one domain mentioned and no detailed performance breakdown

## Confidence
- **High confidence**: The computational complexity advantage of decomposing multi-agent problems into single-agent problems (Mechanism 2)
- **Medium confidence**: The parallel execution time improvements (Mechanism 1) - supported by empirical results but dependent on LLM subgoal quality
- **Low confidence**: The equivalence of LLM-generated subgoals to human expert subgoals (Mechanism 3) - limited human study scope and no quantitative comparison

## Next Checks
1. **Subgoal Quality Analysis**: Conduct systematic testing of LLM subgoal generation across diverse domains, measuring the percentage of generated subgoals that lead to successful plans versus requiring LLM regeneration, to quantify reliability.

2. **Embodied Environment Stress Testing**: Test the approach on multiple embodied tasks with varying navigation complexities and object manipulation requirements to identify failure modes in the AI2THOR domain that weren't apparent in symbolic domains.

3. **Scalability Assessment**: Evaluate how planning time and execution length scale as the number of agents increases beyond two, particularly focusing on when coordination overhead begins to negate the benefits of parallel execution.