---
ver: rpa2
title: Pre-training data selection for biomedical domain adaptation using journal
  impact metrics
arxiv_id: '2409.02725'
source_url: https://arxiv.org/abs/2409.02725
tags:
- language
- pre-training
- biomedical
- data
- domain
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This study investigated whether selecting pre-training data using\
  \ journal impact metrics improves biomedical domain adaptation. We tested two metrics\u2014\
  h-index and SJR\u2014on subsets of PubMed abstracts and continually pre-trained\
  \ BERT-base models."
---

# Pre-training data selection for biomedical domain adaptation using journal impact metrics

## Quick Facts
- arXiv ID: 2409.02725
- Source URL: https://arxiv.org/abs/2409.02725
- Reference count: 12
- Using journal impact metrics for biomedical pre-training data selection shows no significant performance gains over random selection

## Executive Summary
This study investigates whether journal impact metrics can effectively guide pre-training data selection for biomedical domain adaptation. The authors tested two metrics—h-index and SJR—on subsets of PubMed abstracts, continually pre-training BERT-base models and evaluating them on BLURB benchmark tasks. Despite expectations that high-impact journals might provide higher-quality training data, the results showed no significant performance improvements compared to random selection. Interestingly, pre-training on smaller subsets (50% vs. 25% of data) with the same number of training steps did not consistently reduce performance, suggesting efficient training is possible on reduced datasets.

## Method Summary
The study employed a continual pre-training approach using BERT-base models on PubMed abstracts. Two journal impact metrics were tested: h-index and SJR (SCImago Journal Rank). The researchers created subsets of PubMed abstracts based on these metrics and compared their performance against randomly selected subsets. Models were pre-trained for a fixed number of steps on these subsets, with evaluation conducted using the BLURB benchmark across multiple biomedical NLP tasks. The experimental design included comparisons between different subset sizes (25% vs. 50% of available data) while maintaining constant training steps.

## Key Results
- Journal impact metrics (h-index and SJR) did not improve performance over random selection for biomedical pre-training
- Pre-training on 50% of PubMed abstracts with the same training steps as 25% did not consistently reduce performance
- BERT-base models can be efficiently trained on smaller subsets without significant performance loss

## Why This Works (Mechanism)
The lack of improvement from journal impact metrics suggests that citation-based or reputation-based indicators may not capture the linguistic or domain-specific features most valuable for pre-training language models in the biomedical domain. High-impact journals may publish groundbreaking research, but this doesn't necessarily translate to more useful training signals for language models. The consistency across subset sizes indicates that training dynamics may reach effective convergence with fewer examples than traditionally assumed, possibly due to the redundancy and self-similarity in biomedical abstracts.

## Foundational Learning
- Journal impact metrics (h-index, SJR): Standardized measures of journal influence and quality; needed to quantify publication prestige for data selection
- Continual pre-training: Additional training on domain-specific data after initial model training; needed to adapt general models to biomedical domain
- BLURB benchmark: Standard evaluation suite for biomedical language understanding; needed for consistent task performance measurement
- PubMed abstracts: Large-scale biomedical literature database; needed as source of domain-specific training data
- Subset sampling strategies: Methods for selecting representative training data; needed to test different data selection approaches
- Training step normalization: Fixed training duration across different dataset sizes; needed to ensure fair comparison between conditions

## Architecture Onboarding
**Component Map:** PubMed abstracts -> Journal impact filtering -> BERT-base pre-training -> BLURB evaluation

**Critical Path:** Data selection → Pre-training → Fine-tuning → Evaluation

**Design Tradeoffs:** The study prioritized controlled comparison of data selection methods over maximizing model performance, using fixed training steps rather than convergence-based stopping criteria.

**Failure Signatures:** No improvement from impact metrics would manifest as flat or worse performance curves compared to random selection across all tasks.

**First Experiments:** 1) Replicate with additional journal metrics (CiteScore, SNIP); 2) Test on full-text articles instead of abstracts; 3) Evaluate on clinical/medical tasks beyond BLURB scope

## Open Questions the Paper Calls Out
None

## Limitations
- Only tested English-language PubMed abstracts, limiting generalizability to other languages or document types
- Pre-training conducted on subsets representing only 25-50% of available data, potentially missing full literature diversity
- Exclusive reliance on journal-level metrics without incorporating paper-level quality indicators or text quality assessments

## Confidence
- Journal impact metrics ineffective: Medium
- Smaller subsets maintain performance: High

## Next Checks
1. Replicate experiments using additional journal metrics (CiteScore, Source Normalized Impact per Paper) and individual paper-level quality indicators such as citation counts and author expertise measures

2. Extend evaluation to include domain-specific tasks not covered in BLURB, particularly those requiring deep domain knowledge like clinical decision support or biomedical relation extraction

3. Conduct experiments using full-text articles rather than abstracts to determine if observed patterns hold across different text granularities and content richness levels