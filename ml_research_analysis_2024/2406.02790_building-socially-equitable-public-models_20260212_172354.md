---
ver: rpa2
title: Building Socially-Equitable Public Models
arxiv_id: '2406.02790'
source_url: https://arxiv.org/abs/2406.02790
tags:
- agents
- public
- equitable
- downstream
- objective
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses fairness in public models that serve diverse
  downstream agents with heterogeneous objectives. It proposes an Equitable Objective
  that incorporates downstream agents' decision costs into training, parameterized
  by q to control equity emphasis.
---

# Building Socially-Equitable Public Models

## Quick Facts
- arXiv ID: 2406.02790
- Source URL: https://arxiv.org/abs/2406.02790
- Reference count: 40
- This paper proposes an Equitable Objective that incorporates downstream agents' decision costs into training, achieving significantly improved fairness across heterogeneous agents without sacrificing accuracy.

## Executive Summary
This paper addresses fairness in public models serving diverse downstream agents with heterogeneous objectives. It proposes an Equitable Objective that incorporates downstream agents' decision costs into training, parameterized by q to control equity emphasis. The method uses a policy gradient algorithm applicable to both differentiable and non-differentiable cost functions. Theoretical analysis proves that higher q values lead to more uniform performance distributions across agents. Empirical case studies on carbon-efficient data centers and EV charging show that the proposed EQUITABLE PM achieves significantly lower variance and percentile differences in cost regret distributions compared to traditional approaches that only minimize prediction errors.

## Method Summary
The Equitable Objective minimizes an aggregated, reweighted loss parameterized by q that prioritizes optimization of worse-performing agents during training. The policy gradient algorithm enables training on non-differentiable cost functions by sampling predictions from a probabilistic public model and reformulating the objective as an expectation over the joint distribution of inputs, predictions, and costs. The method is theoretically guaranteed to produce more uniform performance distributions across agents as q increases, and is empirically validated on carbon efficiency and EV charging applications.

## Key Results
- In EV charging case study, variance reduced from 0.0602 to 0.0055 when q+1=10
- C95-C5 percentile difference decreased from 0.7717 to 0.2331 in EV charging with higher q values
- Improved fairness achieved without sacrificing accuracy compared to traditional MSE-only approaches

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The Equitable Objective with hyperparameter q creates performance equity by reweighting downstream agents' costs based on their relative performance.
- Mechanism: Higher q values assign larger relative weights to agents with higher downstream costs, effectively prioritizing optimization of worse-performing agents during training.
- Core assumption: Downstream agents' decision costs are representative of their actual business objectives and can be meaningfully aggregated.
- Evidence anchors: The Equitable Objective is described as minimizing aggregated cost incurred by downstream agents, parameterized by q.
- Break condition: If downstream agents have fundamentally incompatible objectives or if their cost functions cannot be meaningfully aggregated.

### Mechanism 2
- Claim: The policy gradient algorithm enables training on non-differentiable cost functions without requiring accurate cost function models.
- Mechanism: Uses probabilistic public model σθ(ˆy | x) to sample predictions and reformulates the objective as an expectation over the joint distribution of inputs, predictions, and costs.
- Core assumption: The probabilistic model can adequately represent uncertainty in predictions, and sampled predictions provide sufficient coverage for effective gradient estimation.
- Evidence anchors: The paper presents detailed gradient computation using log-probability of sampled predictions weighted by aggregated cost regret.
- Break condition: If the cost landscape is highly discontinuous or if the prediction space is too large for effective sampling.

### Mechanism 3
- Claim: Higher q values lead to more uniform performance distributions across downstream agents.
- Mechanism: Theoretical analysis proves that as q increases, normalized entropy of performance distribution increases (indicating greater uniformity), and variance decreases.
- Core assumption: Cost functions are twice differentiable with positive definite Hessians, and optimal solutions exist and are unique.
- Evidence anchors: Theorems prove that the optimum of Equitable Objective is more equitable and that higher q leads to more uniform distributions.
- Break condition: If cost functions are not sufficiently smooth or if optimal solutions are not unique.

## Foundational Learning

- Concept: Policy gradient methods for reinforcement learning
  - Why needed here: The training algorithm relies on policy gradient methods to handle non-differentiable cost functions common in downstream decision-making tasks.
  - Quick check question: What is the key difference between standard policy gradient and the modified version used in EQUITABLE PM?

- Concept: α-fairness in resource allocation
  - Why needed here: The Equitable Objective draws inspiration from α-fairness, a well-established framework for promoting fairness in resource allocation problems.
  - Quick check question: How does the parameter q in the Equitable Objective relate to the α parameter in α-fairness?

- Concept: Multi-agent decision-making and heterogeneous objectives
  - Why needed here: The setting involves multiple downstream agents with different decision processes and objectives, requiring understanding of how to handle heterogeneous costs and ensure fairness across agents.
  - Quick check question: What challenges arise when trying to optimize a single model for multiple agents with conflicting objectives?

## Architecture Onboarding

- Component map: Public model f -> Downstream agents (M) -> Cost functions costm -> Equitable Objective Lq -> Policy gradient algorithm

- Critical path:
  1. Initialize public model parameters θ
  2. For each training batch, obtain predictions from public model
  3. Compute cost regrets for each agent and sample
  4. Calculate gradient using policy gradient method
  5. Update model parameters
  6. Repeat until convergence

- Design tradeoffs:
  - q parameter: Higher q values promote more equity but may reduce overall accuracy
  - Batch size: Larger batches provide more stable gradient estimates but require more computation
  - Probabilistic vs deterministic model: Probabilistic models enable policy gradient but add complexity

- Failure signatures:
  - High variance in cost regrets across agents despite training
  - Degraded prediction accuracy when q is too high
  - Slow convergence or unstable training with non-differentiable costs

- First 3 experiments:
  1. Implement basic public model training with MSE loss on synthetic data
  2. Add single downstream agent with differentiable cost function and verify gradient computation
  3. Extend to multiple agents and implement Equitable Objective with varying q values to observe equity effects

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions, but based on the analysis and limitations discussed, several important open questions arise regarding the method's generalizability, hyperparameter selection, and performance with conflicting agent objectives.

## Limitations
- Empirical validation is limited to two specific domains (data centers and EV charging), raising questions about generalizability to other domains with different cost structures.
- Computational complexity of the policy gradient approach may become prohibitive for high-dimensional prediction spaces or large numbers of downstream agents.
- Performance heavily depends on the choice of q parameter, but the paper provides limited guidance on how to select this hyperparameter in practice.

## Confidence
- Equitable Objective Effectiveness: Medium
- Policy Gradient Algorithm: Medium
- Fairness-Accuracy Tradeoff: High

## Next Checks
1. **Ablation Study on q Values**: Systematically vary q across a wider range (e.g., q+1 ∈ {2, 5, 10, 20}) and measure the full tradeoff curve between fairness metrics (variance, C95-C5) and prediction accuracy (MSE).

2. **Cross-Domain Evaluation**: Apply the method to at least two additional domains with fundamentally different cost structures (e.g., healthcare treatment recommendation and financial risk assessment) to assess generalizability.

3. **Scalability Analysis**: Evaluate the method's performance as the number of downstream agents increases (e.g., M ∈ {5, 10, 20, 50}) and as the prediction space dimensionality grows, measuring both computational runtime and stability of policy gradient estimates.