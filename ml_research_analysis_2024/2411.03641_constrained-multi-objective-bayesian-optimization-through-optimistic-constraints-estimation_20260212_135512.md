---
ver: rpa2
title: Constrained Multi-objective Bayesian Optimization through Optimistic Constraints
  Estimation
arxiv_id: '2411.03641'
source_url: https://arxiv.org/abs/2411.03641
tags:
- optimization
- bayesian
- constrained
- multi-objective
- lemma
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses constrained multi-objective Bayesian optimization,
  a critical challenge in scientific experiment design where regulatory and safety
  concerns impose additional thresholds on experimental outcomes. The authors propose
  COMBOO, a novel algorithm that balances active learning of level sets defined on
  multiple unknowns with multi-objective optimization within the feasible region.
---

# Constrained Multi-objective Bayesian Optimization through Optimistic Constraints Estimation

## Quick Facts
- arXiv ID: 2411.03641
- Source URL: https://arxiv.org/abs/2411.03641
- Reference count: 40
- Authors: Diantong Li; Fengxue Zhang; Chong Liu; Yuxin Chen
- One-line primary result: COMBOO algorithm achieves competitive hypervolume improvement with reduced constraint violations compared to existing baselines in constrained multi-objective Bayesian optimization

## Executive Summary
This paper addresses constrained multi-objective Bayesian optimization (CMOBO), a critical challenge in scientific experiment design where regulatory and safety concerns impose additional thresholds on experimental outcomes. The authors propose COMBOO, a novel algorithm that balances active learning of level sets defined on multiple unknowns with multi-objective optimization within the feasible region. The core method idea is to use optimistic constraint estimation through upper confidence bounds (UCBs) for constraints and random scalarization of UCBs for objectives, constructing an acquisition function that balances feasibility and optimality. Theoretical analysis provides sample efficiency guarantees and the ability to declare infeasibility, while empirical evidence on synthetic benchmarks and real-world applications (including drug discovery problems) demonstrates COMBOO's effectiveness and efficiency compared to existing baselines.

## Method Summary
COMBOO combines optimistic constraint estimation using upper confidence bounds (UCBs) with random scalarization of multiple objectives. For each iteration, the algorithm samples a random weight vector from the simplex to scalarize the UCBs of all objectives, while using UCBs of constraints to define an optimistic feasible region. This approach enables efficient exploration of the Pareto front within the feasible region while maintaining theoretical guarantees on sample efficiency and the ability to detect infeasibility. The method employs Gaussian process models for both objectives and constraints, and uses gradient-based optimization or discretization to maximize the acquisition function subject to optimistic constraints.

## Key Results
- COMBOO achieves competitive hypervolume improvement compared to state-of-the-art baselines (qNEHVI, qParEGO, MESMOC)
- The algorithm shows reduced constraint violations through optimistic constraint estimation
- Empirical validation demonstrates effectiveness on synthetic benchmarks and real-world drug discovery problems (Caco-2++, ESOL+)
- Theoretical guarantees provide sample efficiency bounds and infeasibility detection capability

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The optimistic constraint estimation using upper confidence bounds (UCBs) for constraints enables efficient identification of the feasible region while maintaining theoretical guarantees.
- Mechanism: By constraining the search space using UCBs of constraints (ugj,t(x) ≥ 0), the algorithm avoids querying likely infeasible points while still exploring the full Pareto front within the feasible region. This creates an optimistic but principled feasibility estimation.
- Core assumption: The Gaussian process models for constraints are accurate enough that UCBs provide meaningful upper bounds on the true constraint values.
- Evidence anchors:
  - [abstract]: "The core method idea is to use optimistic constraint estimation through upper confidence bounds (UCBs) for constraints"
  - [section 4]: "For feasibility, COMBOO intersects potentially feasible regions defined by each constraint to avoid querying likely infeasible points"
  - [corpus]: Weak evidence - no direct corpus support for optimistic constraint estimation in multi-objective settings
- Break condition: If the GP models for constraints are highly uncertain or the kernel hyperparameters are poorly chosen, the UCBs may either be too conservative (missing the true feasible region) or too optimistic (frequently violating constraints).

### Mechanism 2
- Claim: Random scalarization of UCBs for objectives enables efficient multi-objective optimization within the feasible region while maintaining theoretical guarantees.
- Mechanism: The algorithm samples θt uniformly from the simplex and optimizes sθt(Ut(x)) where Ut(x) contains the UCBs of all objectives. This allows the algorithm to explore different trade-offs between objectives while maintaining the UCB property for theoretical analysis.
- Core assumption: The scalarization function provides an unbiased estimator of the hypervolume and maintains the Lipschitz property needed for regret bounds.
- Evidence anchors:
  - [section 4]: "COMBOO uses the Upper Confidence Bound (UCB) from single objective Bayesian optimization and employs random scalarization of multiple UCBs for efficient hypervolume optimization"
  - [section 5.1]: "The bound for cumulative HV regret is provided as follows" showing the theoretical guarantee
  - [corpus]: Moderate evidence - related work on scalarization in multi-objective optimization but limited direct support for UCB-based scalarization
- Break condition: If the number of objectives m is large, the scalarization may become inefficient as it requires sampling from higher-dimensional simplices, and the Lipschitz constant may grow unfavorably.

### Mechanism 3
- Claim: The combination of optimistic feasibility estimation and random scalarization creates a principled trade-off between constraint learning and objective optimization that outperforms existing heuristics.
- Mechanism: By using UCBs for both objectives and constraints, the algorithm maintains an optimistic view of the problem that encourages exploration of promising regions while still respecting constraints. The random scalarization ensures that different trade-offs are explored over time.
- Core assumption: The optimistic approach (using UCBs rather than posterior means) provides better exploration than pessimistic approaches while still maintaining reasonable constraint satisfaction.
- Evidence anchors:
  - [section 6.2]: "The scalarization-based methods, COMBOO and qParEGO, showed faster convergence in simple HV compared to non-scalarization approaches"
  - [section 6.2]: "MESMOC utilizes the posterior samples from the GP model, µh,t, to enforce constraints... In contrast, our method employs UCB, uh,t, to optimistically search for feasible regions"
  - [corpus]: Weak evidence - no direct corpus support for comparing optimistic vs pessimistic constraint handling in multi-objective BO
- Break condition: If the problem has very tight constraints relative to the objective landscape, the optimistic approach may lead to excessive constraint violations that outweigh the benefits of better objective optimization.

## Foundational Learning

- Concept: Gaussian Process regression and confidence bounds
  - Why needed here: The algorithm relies on GP models to estimate both objectives and constraints, and uses confidence bounds (LCB/UCB) to guide exploration
  - Quick check question: Given a GP model with mean µ and variance σ², what is the formula for a 95% upper confidence bound?

- Concept: Multi-objective optimization and hypervolume metrics
  - Why needed here: The algorithm optimizes multiple objectives simultaneously and uses hypervolume as the performance metric for the Pareto front
  - Quick check question: What is the hypervolume indicator and why is it used as a metric for multi-objective optimization?

- Concept: Bayesian optimization acquisition functions
  - Why needed here: The algorithm constructs an acquisition function that balances exploration and exploitation while respecting constraints
  - Quick check question: How does Expected Improvement differ from Upper Confidence Bound in Bayesian optimization?

## Architecture Onboarding

- Component map: GP models for objectives and constraints -> UCB/LCB computation -> Random scalarization module -> Constraint feasibility check -> Acquisition function optimizer -> Infeasibility detection module -> Update module for GP posteriors

- Critical path:
  1. Sample θt from simplex
  2. Compute UCBs for all objectives and constraints
  3. Check if maxx minj ugj,t(x) < 0 (declare infeasibility if true)
  4. Optimize acquisition function subject to constraint UCBs
  5. Evaluate selected point
  6. Update GP models

- Design tradeoffs:
  - Optimistic vs pessimistic constraint handling: optimistic (UCB) explores more but may violate constraints more frequently
  - Random scalarization vs fixed scalarization: random explores different trade-offs but adds stochasticity
  - Continuous vs discrete search space: continuous allows finer optimization but requires more complex theoretical analysis

- Failure signatures:
  - Excessive constraint violations: UCBs may be too optimistic
  - Slow convergence: scalarization may not be exploring the right trade-offs
  - Infeasibility declared incorrectly: GP uncertainty may be too high
  - Poor performance on high-dimensional problems: kernel choice or discretization may be inadequate

- First 3 experiments:
  1. Simple 2D, 2-objective, 1-constraint problem with known feasible region to verify basic functionality
  2. Compare optimistic vs pessimistic constraint handling on a simple problem to quantify the trade-off
  3. Test scalability on synthetic problems with increasing numbers of objectives and constraints

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does COMBOO's performance compare to information-theoretic methods like MESMOC when dealing with high-dimensional molecular datasets beyond Caco-2++ and ESOL+?
- Basis in paper: The paper shows COMBOO's effectiveness on Caco-2++ and ESOL+ datasets but notes MESMOC's conservative feasibility estimation leads to fewer violations but slower hypervolume improvement.
- Why unresolved: The paper only tests on two specific molecular datasets with d=2175 and d=2133 dimensions. No experiments are conducted on higher-dimensional datasets or different molecular representations.
- What evidence would resolve it: Testing COMBOO and MESMOC on additional high-dimensional molecular datasets (e.g., larger fragment-based representations, protein-ligand interactions) with varying numbers of objectives and constraints would reveal if COMBOO maintains its advantage in constraint regret while improving hypervolume compared to MESMOC.

### Open Question 2
- Question: Can the theoretical convergence guarantees for COMBOO be extended to continuous search spaces with non-stationary kernel functions?
- Basis in paper: The paper provides theoretical analysis for both discrete and continuous compact search spaces but assumes stationary kernels like RBF and Matérn. The proof for continuous spaces requires 4th-differentiable kernels.
- Why unresolved: The paper only analyzes convergence for specific stationary kernels (RBF, Matérn) and doesn't explore non-stationary kernels that might better capture changing landscape characteristics in real-world optimization problems.
- What evidence would resolve it: Developing theoretical bounds for COMBOO using non-stationary kernels (e.g., deep kernels, neural network-based kernels) and proving convergence guarantees under these more general assumptions would demonstrate the algorithm's robustness to different kernel choices.

### Open Question 3
- Question: What is the impact of the random scalarization parameter θ on COMBOO's exploration-exploitation trade-off in highly constrained regions of the search space?
- Basis in paper: The paper uses random scalarization from S^{m-1} but doesn't analyze how the distribution of θ affects the algorithm's ability to find Pareto-optimal solutions near constraint boundaries.
- Why unresolved: While the paper demonstrates COMBOO's effectiveness through empirical results, it doesn't provide theoretical analysis of how different scalarization strategies (e.g., deterministic vs. random, different distributions over S^{m-1}) affect convergence rates or solution quality in constrained regions.
- What evidence would resolve it: Conducting ablation studies comparing COMBOO with different scalarization strategies (uniform, weighted, adaptive) and analyzing the resulting hypervolume, constraint violation, and constraint regret would reveal optimal scalarization approaches for different problem characteristics.

## Limitations
- Reliance on accurate GP uncertainty estimates for both objectives and constraints
- Assumes Lipschitz continuity of objectives and constraints, which may not hold in all practical applications
- Random scalarization approach may become inefficient as the number of objectives increases

## Confidence
- Theoretical analysis for sample efficiency and infeasibility detection: High
- Empirical validation on synthetic benchmarks: Medium
- Empirical validation on real-world drug discovery problems: Medium

## Next Checks
1. Test COMBOO on high-dimensional problems (d > 10) to evaluate scalability and identify performance degradation points
2. Conduct ablation studies comparing optimistic vs pessimistic constraint handling on problems with varying constraint tightness
3. Implement and validate the infeasibility detection mechanism on problems with known infeasible regions to verify theoretical guarantees in practice