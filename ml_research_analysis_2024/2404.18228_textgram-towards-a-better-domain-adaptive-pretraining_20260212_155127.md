---
ver: rpa2
title: 'TextGram: Towards a better domain-adaptive pretraining'
arxiv_id: '2404.18228'
source_url: https://arxiv.org/abs/2404.18228
tags:
- data
- selection
- corpus
- language
- dataset
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of efficient domain-adaptive
  pretraining for large language models while minimizing computational cost and carbon
  footprint. The authors propose TextGram, a novel data selection method that combines
  n-gram frequency analysis with TextRank-based graph ranking to select domain-relevant
  data from large out-of-domain corpora.
---

# TextGram: Towards a better domain-adaptive pretraining

## Quick Facts
- arXiv ID: 2404.18228
- Source URL: https://arxiv.org/abs/2404.18228
- Reference count: 17
- Primary result: TextGram achieves 91.02% F1-score on IMDb sentiment classification, outperforming baseline methods while reducing pretraining data by 75%

## Executive Summary
This paper addresses the challenge of efficient domain-adaptive pretraining for large language models while minimizing computational cost and carbon footprint. The authors propose TextGram, a novel data selection method that combines n-gram frequency analysis with TextRank-based graph ranking to select domain-relevant data from large out-of-domain corpora. Their approach first identifies top in-domain n-grams, uses these to filter and combine sentences from both in-domain and out-of-domain data, then applies paraphrase mining to build a similarity graph that PageRank uses to rank and select sentences. Experiments using BERT pretraining on RealNews corpus followed by fine-tuning on IMDb sentiment classification show TextGram achieves 91.02% F1-score, outperforming baseline methods while reducing pretraining data requirements by 75% without sacrificing model performance.

## Method Summary
TextGram is a data selection method for domain-adaptive pretraining that combines n-gram frequency analysis with TextRank-based graph ranking. The method identifies top in-domain n-grams, uses them to filter and combine sentences from both in-domain and out-of-domain data, then applies paraphrase mining to build a similarity graph. PageRank ranks the sentences by importance, and the top-ranked sentences from the out-domain are selected for pretraining. The approach was evaluated by pretraining BERT on a filtered RealNews corpus and fine-tuning on IMDb sentiment classification, demonstrating superior performance while reducing data requirements by 75%.

## Key Results
- TextGram achieves 91.02% F1-score on IMDb sentiment classification
- Outperforms baseline methods: Random Selection (90.73%), N-Grams (90.97%), Perplexity (90.99%), Cross-Entropy (90.77%), and TextRank (90.47%)
- Reduces pretraining data requirements by 75% (from 1M to 250K examples) without sacrificing model performance

## Why This Works (Mechanism)

### Mechanism 1
- Claim: TextGram improves domain adaptation by combining in-domain n-gram filtering with graph-based sentence ranking
- Mechanism: First, TextGram identifies top in-domain n-grams and uses them to select sentences from both in-domain and out-domain data. These sentences are then used to build a similarity graph via paraphrase mining, and PageRank ranks the sentences by importance. The top-ranked sentences from the out-domain are selected for pretraining
- Core assumption: In-domain n-grams are strong indicators of domain relevance, and graph-based ranking can effectively surface domain-aligned sentences
- Evidence anchors:
  - [abstract] "Their approach first identifies top in-domain n-grams, uses these to filter and combine sentences from both in-domain and out-domain data, then applies paraphrase mining to build a similarity graph that PageRank uses to rank and select sentences."
  - [section] "we start by selecting the topk in-domain sentences based on the highest frequencies of n-grams calculated from the in-domain corpus... we perform paraphrase mining on this combined set of sentences to determine similarity scores for various pairs of sentences... we create a sparse matrix using the similarity scores, where the nodes represent sentences and the edges indicate the scores between the sentences."
- Break condition: If in-domain n-grams poorly represent domain concepts or the similarity graph fails to capture meaningful sentence relationships, ranking quality drops

### Mechanism 2
- Claim: Domain-adaptive pretraining with TextGram reduces computational cost and carbon footprint
- Mechanism: By selecting a small subset of highly relevant out-domain data (75% reduction from 1M to 250K examples), TextGram allows pretraining on less data while maintaining or improving downstream performance, thus lowering compute and energy usage
- Core assumption: High-quality domain-specific data can replace large volumes of generic data without sacrificing model accuracy
- Evidence anchors:
  - [abstract] "demonstrate that TextGram provides superior domain adaptation while reducing pretraining data requirements by 75% (from 1M to 250K examples) without sacrificing model performance."
  - [section] "Selecting important data reduces the space overhead and the substantial amount of time required to pre-train the model while maintaining constant accuracy."
- Break condition: If the selected subset lacks diversity or is too small, the model may underfit or suffer from negative transfer

### Mechanism 3
- Claim: TextGram outperforms baseline data selection methods (Random, N-Grams, Perplexity, Cross-Entropy, TextRank) in domain-adaptive pretraining
- Mechanism: TextGram integrates n-gram frequency analysis into the TextRank pipeline, creating a more adaptive selection process that considers in-domain data during ranking, leading to better downstream classification performance
- Core assumption: Adaptive methods that incorporate in-domain signals during data selection yield better domain alignment than non-adaptive or generic methods
- Evidence anchors:
  - [abstract] "Experiments using BERT pretraining on RealNews corpus followed by fine-tuning on IMDb sentiment classification show TextGram achieves 91.02% F1-score, outperforming baseline methods including Random Selection (90.73%), N-Grams (90.97%), Perplexity (90.99%), Cross-Entropy (90.77%), and TextRank (90.47%)."
  - [section] "We show that the proposed strategy works better compared to other selection methods."
- Break condition: If the n-gram selection or similarity scoring is noisy or biased, TextGram's advantage over baselines diminishes

## Foundational Learning

- Concept: Bidirectional Encoder Representations from Transformers (BERT)
  - Why needed here: TextGram builds upon BERT as the base model for pretraining and fine-tuning; understanding BERT's architecture and pretraining objectives is essential
  - Quick check question: What is the main pretraining objective used in BERT, and how does it help the model learn word context?

- Concept: Masked Language Modeling (MLM)
  - Why needed here: MLM is the pretraining strategy used after data selection; knowing how it works helps in understanding how the model learns from the selected corpus
  - Quick check question: In MLM, what portion of the input text is masked, and what does the model learn to do?

- Concept: PageRank algorithm
  - Why needed here: TextGram uses PageRank to rank sentences in the similarity graph; understanding its mechanics is crucial for grasping how sentence importance is determined
  - Quick check question: How does the PageRank algorithm assign scores to nodes in a graph, and what does the damping factor represent?

## Architecture Onboarding

- Component map:
  Input corpora -> Tokenization -> n-gram extraction -> In-domain n-gram analysis -> Out-domain corpus combination -> Paraphrase mining -> Graph construction -> PageRank ranking -> Data selection -> BERT pretraining -> Fine-tuning -> Evaluation

- Critical path:
  Input corpora → n-gram filtering → sentence combination → similarity scoring → graph ranking → selected data → BERT pretraining → fine-tuning → evaluation

- Design tradeoffs:
  - Data reduction vs. model performance: Aggressive selection may save compute but risks underfitting
  - In-domain signal strength: Over-reliance on n-grams may miss nuanced domain relevance
  - Graph complexity: Larger graphs increase computation but may improve ranking quality

- Failure signatures:
  - Low F1-score on fine-tuning despite reduced data: Indicates poor data selection
  - High variance in results: Suggests instability in n-gram or similarity scoring
  - Long preprocessing time: May mean inefficiency in similarity computation

- First 3 experiments:
  1. Compare TextGram's F1-score to Random Selection on the same dataset split
  2. Measure computational time and energy usage reduction when using TextGram vs. full corpus pretraining
  3. Evaluate the impact of varying the number of top in-domain n-grams on downstream task performance

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions in the text provided.

## Limitations
- The paper lacks specific implementation details for critical components like paraphrase mining and PageRank parameter tuning
- Evaluation is limited to a single domain pair (news→movie reviews), raising questions about generalizability
- Computational cost reduction claims depend on implementation efficiency, which isn't benchmarked against standard libraries

## Confidence
- High confidence: The core TextGram methodology (n-gram filtering + graph ranking) is clearly described and produces consistent results with stated baselines
- Medium confidence: The computational efficiency claims are supported by data reduction numbers but lack direct energy/cost measurements
- Medium confidence: The superiority over baselines is demonstrated with clear F1-score improvements, though the margin over some baselines is narrow

## Next Checks
1. Cross-domain validation: Test TextGram on a different domain pair (e.g., medical→legal) to assess generalizability beyond news→movie reviews
2. Ablation study: Systematically remove n-gram filtering or PageRank ranking to quantify each component's contribution to the 91.02% F1-score
3. Computational profiling: Measure actual training time, GPU hours, and energy consumption for TextGram vs. full corpus pretraining to validate the 75% efficiency claim with real metrics