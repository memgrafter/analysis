---
ver: rpa2
title: 'SEABO: A Simple Search-Based Method for Offline Imitation Learning'
arxiv_id: '2402.03807'
source_url: https://arxiv.org/abs/2402.03807
tags:
- uni00000013
- uni00000011
- uni00000048
- seabo
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: SEABO addresses the challenge of offline imitation learning without
  access to reward signals by proposing a simple search-based method that annotates
  unlabeled trajectories using expert demonstrations. The core idea is to measure
  the distance between each unlabeled transition and its nearest neighbor in the expert
  trajectory using a KD-tree, assigning larger rewards to transitions closer to the
  expert data.
---

# SEABO: A Simple Search-Based Method for Offline Imitation Learning

## Quick Facts
- arXiv ID: 2402.03807
- Source URL: https://arxiv.org/abs/2402.03807
- Authors: Jiafei Lyu, Xiaoteng Ma, Le Wan, Runze Liu, Xiu Li, Zongqing Lu
- Reference count: 38
- Primary result: SEABO achieves competitive or better performance than using ground-truth rewards on D4RL datasets

## Executive Summary
SEABO addresses offline imitation learning without reward signals by using a simple search-based method that annotates unlabeled trajectories using expert demonstrations. The core idea is to measure the distance between each unlabeled transition and its nearest neighbor in the expert trajectory using a KD-tree, assigning larger rewards to transitions closer to expert data. This unsupervised approach can be combined with any offline RL algorithm and requires only a single expert trajectory. Experiments show SEABO often surpasses prior reward learning and offline IL methods while being computationally efficient.

## Method Summary
SEABO converts offline imitation learning into a nearest-neighbor search problem where rewards are assigned based on proximity to expert demonstrations. The algorithm builds a KD-tree from expert demonstrations and queries each unlabeled transition to find its nearest neighbor in the expert trajectory. The distance between the transition and its nearest neighbor is transformed into a reward signal via a squashing function: r = α exp(-β × d / |A|). This approach works with any offline RL algorithm and can handle expert demonstrations with only observations by querying using (s, s') pairs instead of (s, a, s').

## Key Results
- SEABO achieves normalized scores comparable to or better than using ground-truth rewards on D4RL locomotion and AntMaze tasks
- The method outperforms prior reward learning approaches (OTR, ORIL, UDS) and offline IL methods (DemoDICE, PWIL) across multiple benchmarks
- SEABO is computationally efficient, requiring only O(df log |De|) time complexity for KD-tree operations

## Why This Works (Mechanism)

### Mechanism 1
- Claim: SEABO works because it converts offline imitation learning into a nearest-neighbor search problem, where rewards are assigned based on how close a transition is to expert demonstrations.
- Mechanism: The algorithm builds a KD-tree from expert demonstrations and queries each unlabeled transition to find its nearest neighbor in the expert trajectory. The distance between the transition and its nearest neighbor is then transformed into a reward signal via a squashing function: r = α exp(-β × d / |A|).
- Core assumption: Transitions close to expert demonstrations are near-optimal and should receive higher rewards, while distant transitions are suboptimal and should receive lower rewards.
- Evidence anchors:
  - [abstract]: "SEABO allocates a larger reward to the transition that is close to its closest neighbor in the expert demonstration, and a smaller reward otherwise"
  - [section]: "we hypothesize that the transition is near-optimal if it lies close to the expert trajectory, hence larger reward ought to be assigned to it, and vice versa"
  - [corpus]: "Found 25 related papers... Top related titles: PROF: An LLM-based Reward Code Preference Optimization Framework for Offline Imitation Learning"
- Break condition: If the expert demonstration is not representative of optimal behavior, or if the state-action space has regions where proximity doesn't correlate with optimality, the reward assignment becomes unreliable.

### Mechanism 2
- Claim: SEABO is computationally efficient because it leverages KD-tree search instead of training neural networks for reward estimation.
- Mechanism: KD-tree construction has complexity O(df log |De|) where df is feature dimension, and nearest neighbor queries are fast. This avoids the computational overhead of training reward models or discriminators.
- Core assumption: KD-tree provides sufficient accuracy for nearest neighbor search in the state-action space, and the distance metric captures meaningful similarity.
- Evidence anchors:
  - [abstract]: "SEABO is computationally efficient and easy to implement, requiring only a single expert trajectory"
  - [section]: "SEABO is computationally efficient since there is only a single expert trajectory, and the time complexity of KD-tree gives O(df log |De|)"
  - [corpus]: Weak - no direct evidence about computational efficiency compared to alternatives
- Break condition: In very high-dimensional spaces or with complex state representations, KD-tree search may become inefficient or less accurate.

### Mechanism 3
- Claim: SEABO generalizes well because it works with different base offline RL algorithms and can handle expert demonstrations with only observations.
- Mechanism: The reward annotation process is independent of the RL algorithm, so any offline RL method can use SEABO rewards. When expert demonstrations contain only observations, SEABO can still query using (s, s') pairs.
- Core assumption: The distance metric and squashing function can adapt to different task domains and reward scales without requiring extensive hyperparameter tuning.
- Evidence anchors:
  - [abstract]: "SEABO can be combined with any existing offline RL algorithm to acquire a meaningful policy from the static offline dataset"
  - [section]: "SEABO is easy to implement and can be combined with any offline RL algorithm"
  - [corpus]: Weak - no direct evidence about generalization across different RL algorithms
- Break condition: If the base RL algorithm has specific requirements for reward structure that conflict with SEABO's reward distribution, performance may degrade.

## Foundational Learning

- Concept: Nearest neighbor search algorithms (KD-tree, Ball-tree, HNSW)
  - Why needed here: SEABO fundamentally relies on finding the closest expert demonstration for each unlabeled transition to determine reward magnitude
  - Quick check question: What is the time complexity of building a KD-tree versus performing a nearest neighbor query?

- Concept: Reward shaping and squashing functions
  - Why needed here: The raw distance between transitions needs to be transformed into meaningful reward signals that RL algorithms can use effectively
  - Quick check question: Why does SEABO divide the distance by |A| in the squashing function?

- Concept: Offline reinforcement learning and distribution shift
  - Why needed here: Understanding why offline IL is challenging helps explain why SEABO's simple approach can be effective despite avoiding complex distribution matching
  - Quick check question: What is distribution shift in offline RL, and how does it typically affect policy learning?

## Architecture Onboarding

- Component map: Expert trajectory → KD-tree construction → Unlabeled transition query → Distance calculation → Squashing function → Reward assignment → Base offline RL algorithm → Policy optimization
- Critical path: The sequence from KD-tree construction through reward assignment is critical; any failure here propagates to poor policy learning
- Design tradeoffs: Using KD-tree provides speed but may sacrifice accuracy in high dimensions; using only one expert trajectory limits coverage but keeps computation low
- Failure signatures: Poor performance on tasks where proximity doesn't correlate with optimality; failure when expert demonstrations are not representative; poor results with high-dimensional state spaces
- First 3 experiments:
  1. Run SEABO with a single expert trajectory on a simple D4RL task (like halfcheetah-medium-v2) and compare reward distributions to ground truth
  2. Test SEABO with different distance metrics (Euclidean vs. custom metrics) on the same task
  3. Evaluate SEABO with different base RL algorithms (IQL vs. TD3 BC) on the same dataset to verify algorithm independence

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does SEABO's performance scale with the dimensionality of the state-action space, particularly in high-dimensional visual domains like raw pixel inputs?
- Basis in paper: [inferred] The paper mentions SEABO can work with state-only regimes and visual inputs if preprocessed, but does not extensively evaluate high-dimensional visual domains or discuss scaling challenges.
- Why unresolved: The paper only evaluates SEABO on low-dimensional state representations from D4RL. High-dimensional visual inputs are mentioned as a future direction but not experimentally validated.
- What evidence would resolve it: Comprehensive experiments on visual RL benchmarks like Atari or DMControl Suite, comparing SEABO with and without visual preprocessing, would clarify performance scaling.

### Open Question 2
- Question: What is the impact of different distance metrics (beyond Euclidean) on SEABO's performance, and how can the choice be automated or adapted per task?
- Basis in paper: [explicit] The paper uses Euclidean distance by default but notes that different distance metrics can be used in SEABO, leaving the choice to future work.
- Why unresolved: The paper does not compare alternative distance metrics or propose a method to select them automatically.
- What evidence would resolve it: Systematic comparison of SEABO with various distance metrics (e.g., Mahalanobis, cosine similarity) across multiple tasks, along with a method to select the optimal metric per task, would resolve this.

### Open Question 3
- Question: How does SEABO's performance change when the expert demonstrations are suboptimal or diverse, rather than the highest-return trajectory as used in the experiments?
- Basis in paper: [inferred] The paper uses the highest-return trajectory as the expert demonstration but does not explore the impact of suboptimal or diverse expert data.
- Why unresolved: The experiments assume access to near-optimal expert trajectories, which may not be realistic in practice.
- What evidence would resolve it: Experiments with SEABO using diverse or suboptimal expert trajectories, and comparing performance against using the highest-return trajectory, would clarify robustness to expert quality.

## Limitations
- SEABO's effectiveness depends heavily on the assumption that proximity in state-action space correlates with optimality, which may not hold in tasks with complex dynamics
- The method could struggle with high-dimensional observations or when optimal trajectories have significant variation
- The choice of distance metric and squashing function hyperparameters (α, β) may require task-specific tuning for optimal performance

## Confidence
**High Confidence**: The core mechanism of using KD-tree nearest neighbor search to assign rewards based on distance to expert demonstrations is well-specified and experimentally validated. The computational efficiency claims are supported by the O(df log |De|) complexity analysis.

**Medium Confidence**: The claim that SEABO generalizes across different offline RL algorithms is supported by experiments with IQL and TD3 BC, but broader testing with other algorithms would strengthen this claim. The assertion that SEABO works with expert demonstrations containing only observations is demonstrated but not extensively validated.

**Low Confidence**: The paper's comparison to state-of-the-art methods is limited to specific benchmarks. The performance claims in more complex or high-dimensional tasks remain uncertain without additional empirical validation.

## Next Checks
1. Test SEABO with additional offline RL algorithms beyond IQL and TD3 BC (e.g., CQL, BRAC) on the same D4RL tasks to verify the claimed algorithm independence.

2. Evaluate SEABO on tasks with image-based observations or very high-dimensional state spaces to assess whether KD-tree search remains effective when the state dimension increases significantly.

3. Systematically vary the quality and coverage of expert demonstrations (using random trajectories, suboptimal experts, partial demonstrations) to determine how sensitive SEABO's performance is to the quality of the expert data.