---
ver: rpa2
title: 'CoGS: Causality Constrained Counterfactual Explanations using goal-directed
  ASP'
arxiv_id: '2407.08179'
source_url: https://arxiv.org/abs/2407.08179
tags:
- state
- states
- causal
- rules
- algorithm
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The CoGS framework generates counterfactual explanations from rule-based
  machine learning models by incorporating causal dependencies between features. It
  casts the problem as a planning task, finding a path from an undesired outcome state
  to a desired one through realistic interventions that respect causal rules.
---

# CoGS: Causality Constrained Counterfactual Explanations using goal-directed ASP

## Quick Facts
- arXiv ID: 2407.08179
- Source URL: https://arxiv.org/abs/2407.08179
- Authors: Sopam Dasgupta; JoaquÃ­n Arias; Elmer Salazar; Gopal Gupta
- Reference count: 40
- Key outcome: CoGS generates causally consistent counterfactual explanations efficiently by incorporating causal dependencies between features using s(CASP) ASP planning

## Executive Summary
CoGS is a framework that generates counterfactual explanations for rule-based machine learning models by incorporating causal dependencies between features. It treats the problem as a planning task, finding paths from undesired to desired outcome states through realistic interventions that respect causal rules. Using the goal-directed s(CASP) ASP system, CoGS symbolically computes these interventions, producing actionable counterfactuals rather than potentially implausible ones that ignore feature dependencies. Experiments on German credit, Adult income, and Car Evaluation datasets demonstrate efficient execution times in milliseconds while maintaining causal consistency.

## Method Summary
CoGS generates counterfactual explanations by casting the problem as a planning task where states are feature-value pairs and transitions are actions that change feature values. The framework uses s(CASP) ASP system to symbolically compute paths from undesired to desired outcomes while respecting causal dependencies learned by FOLD-SE. It employs two types of actions: direct actions that change single feature values, and causal actions that change related features to indirectly achieve target changes. The method iteratively searches for minimal path lengths, starting from single changes and increasing until a valid counterfactual path is found. FOLD-SE automatically learns both decision rules and causal dependency rules, which users verify before counterfactual generation.

## Key Results
- CoGS generates counterfactual explanations in milliseconds for German credit, Adult income, and Car Evaluation datasets
- The framework finds paths that flip negative outcomes to positive ones by suggesting feasible changes while respecting causal constraints
- CoGS produces more realistic, actionable counterfactuals compared to methods that ignore feature dependencies

## Why This Works (Mechanism)

### Mechanism 1
- Claim: CoGS uses s(CASP) to symbolically compute counterfactual paths that respect causal dependencies.
- Mechanism: The s(CASP) system treats the counterfactual problem as a planning task, where each state is a set of feature-value pairs and transitions correspond to actions that change feature values while maintaining causal consistency.
- Core assumption: The causal rules learned by FOLD-SE accurately capture true dependencies between features and are verified by users or domain experts.
- Evidence anchors:
  - [abstract] "It uses the goal-directed s(CASP) ASP system to symbolically compute these interventions."
  - [section 2] "s(CASP) computes dual rules... allowing negated queries and constructing alternate worlds/states that lead to counterfactual explanations while considering causal dependencies."
  - [corpus] Weak evidence - no direct mention of s(CASP) in related papers, suggesting this is a novel contribution.

### Mechanism 2
- Claim: Direct and causal actions ensure generated counterfactuals are realistic and actionable.
- Mechanism: Direct actions change a single feature value directly, while causal actions change related features to indirectly achieve the desired target feature change, maintaining causal consistency throughout the path.
- Core assumption: Feature dependencies identified by FOLD-SE are complete enough to cover all necessary causal relationships for generating valid counterfactuals.
- Evidence anchors:
  - [section 3] "CoGS employs two kinds of actions: 1) Direct Actions: directly changing a feature value, and 2) Causal Actions: changing other features to cause the target feature to change."
  - [section 4.1] "Causal Actions are interdependent, governed by C."
  - [corpus] Moderate evidence - related work discusses causal compliance but CoGS specifically implements both action types.

### Mechanism 3
- Claim: The planning formulation with path length constraints ensures minimal and causally consistent counterfactuals.
- Mechanism: By iteratively increasing path length from minimal (1) and using constraints, CoGS finds the shortest path from initial state to goal state that respects causal rules.
- Core assumption: The state space is finite and manageable, allowing complete search within reasonable time bounds.
- Evidence anchors:
  - [section 4.1] "CoGS has the ability to specify the path length for candidate solutions... Starting with a minimal path length of 1, CoGS can identify solutions requiring only a single change."
  - [section 5] "Note that the execution time for finding the counterfactuals is also reported. While we have only shown specific paths in Table 1, our CoGS methodology can generate all possible paths from an original instance to a counterfactual."
  - [corpus] Moderate evidence - execution times reported in milliseconds suggest feasibility for small to medium datasets.

## Foundational Learning

- Concept: Answer Set Programming (ASP) basics
  - Why needed here: CoGS uses ASP to encode the planning problem and causal constraints symbolically.
  - Quick check question: What is the difference between stable models and answer sets in ASP?

- Concept: Rule-based machine learning (FOLD-SE)
  - Why needed here: FOLD-SE generates both decision rules and causal dependency rules used by CoGS.
  - Quick check question: How does FOLD-SE handle numerical vs categorical features differently?

- Concept: Causal reasoning fundamentals
  - Why needed here: Understanding cause-effect relationships is essential for generating realistic counterfactuals.
  - Quick check question: What is the difference between correlation and causation in the context of feature dependencies?

## Architecture Onboarding

- Component map: Input datasets -> FOLD-SE (learns rules) -> CoGS (generates counterfactuals) -> s(CASP) (computes paths) -> Output counterfactual explanations
- Critical path:
  1. User provides initial instance and verifies causal rules
  2. CoGS constructs state space and causal constraints
  3. s(CASP) searches for minimal path satisfying constraints
  4. CoGS returns counterfactual path and validates it
  5. System reports execution metrics
- Design tradeoffs:
  - Grounding vs. goal-directed execution: s(CASP) avoids grounding for efficiency
  - Rule verification vs. automation: Manual verification ensures accuracy but reduces scalability
  - Path completeness vs. computational cost: Longer paths provide more options but increase search time
- Failure signatures:
  - No counterfactual found: Likely missing causal rules or unsatisfiable constraints
  - Very long execution time: State space too large or constraints too complex
  - Generated path violates common sense: Causal rules incorrect or incomplete
- First 3 experiments:
  1. German credit dataset with known causal dependencies - verify basic functionality
  2. Adult income dataset with complex causal rules - test scalability and constraint handling
  3. Car evaluation dataset with no causal dependencies - validate behavior when FOLD-SE finds no causal rules

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the plausibility constraints for immutable or restricted feature values (like age or credit score) be automatically verified rather than relying on human input?
- Basis in paper: [explicit] The paper mentions that plausibility constraints are introduced to respect restrictions on feature values, but states that human verification is currently used for causal dependencies learned by RBML algorithms.
- Why unresolved: The paper acknowledges that automatic verification using commonsense knowledge is a research direction but does not provide a concrete methodology for implementing it.
- What evidence would resolve it: A proposed algorithm or framework that automatically validates plausibility constraints using domain knowledge or logical inference, with experimental validation on real datasets.

### Open Question 2
- Question: How does the performance of CoGS scale with increasing feature dimensionality and complexity of causal dependencies in the dataset?
- Basis in paper: [inferred] The paper demonstrates CoGS on datasets with 4-7 features and reports execution times in milliseconds, but does not explore scalability to high-dimensional datasets with complex causal structures.
- Why unresolved: The experiments are limited to relatively small datasets, and there is no analysis of computational complexity or performance degradation with larger, more complex datasets.
- What evidence would resolve it: Empirical studies showing CoGS execution times and path-finding success rates on datasets with varying numbers of features and complex causal dependencies, along with complexity analysis.

### Open Question 3
- Question: Can CoGS be extended to handle non-binary classification tasks or continuous outcome variables?
- Basis in paper: [explicit] The paper focuses on binary classification scenarios where outcomes are flipped from negative to positive, with no discussion of multi-class or regression problems.
- Why unresolved: The current formulation of the counterfactual generation problem and the goal state definition are specific to binary outcomes, with no indication of how they would generalize.
- What evidence would resolve it: A modified version of the CoGS framework that handles multi-class classification or regression tasks, with experimental validation showing successful counterfactual generation for non-binary outcomes.

## Limitations
- Scalability concerns: Performance on large, high-dimensional datasets with complex causal dependencies remains untested
- Manual verification requirement: Causal rules need user verification, limiting full automation
- Binary classification focus: Framework designed for binary outcomes, not multi-class or regression tasks

## Confidence
- High: CoGS can generate causally consistent counterfactual explanations using s(CASP) ASP planning
- Medium: Scalability to larger, more complex datasets
- Low: Comparison with baseline methods for generating more realistic counterfactuals

## Next Checks
1. Test CoGS on a larger, real-world dataset (e.g., from healthcare or finance) with hundreds of features to evaluate scalability and identify performance bottlenecks.
2. Conduct user studies to validate whether the generated counterfactuals are perceived as realistic and actionable by domain experts, and compare against baseline methods.
3. Introduce controlled noise or errors in the causal rules to assess how robust the framework is to incorrect dependency information and whether it can detect implausible counterfactuals.