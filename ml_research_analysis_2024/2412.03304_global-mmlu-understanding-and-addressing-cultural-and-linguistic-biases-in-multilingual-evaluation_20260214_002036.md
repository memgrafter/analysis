---
ver: rpa2
title: 'Global MMLU: Understanding and Addressing Cultural and Linguistic Biases in
  Multilingual Evaluation'
arxiv_id: '2412.03304'
source_url: https://arxiv.org/abs/2412.03304
tags:
- languages
- cultural
- language
- knowledge
- gid00163
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Global-MMLU addresses cultural and linguistic biases in multilingual
  evaluations by annotating MMLU questions for cultural, geographic, and dialect-specific
  knowledge. It releases improved translations across 42 languages, combining professional
  and community annotations with machine translation.
---

# Global MMLU: Understanding and Addressing Cultural and Linguistic Biases in Multilingual Evaluation

## Quick Facts
- arXiv ID: 2412.03304
- Source URL: https://arxiv.org/abs/2412.03304
- Authors: Shivalika Singh, Angelika Romanou, Clémentine Fourrier, David I. Adelani, Jian Gang Ngui, Daniel Vila-Suero, Peerat Limkonchotiwat, Kelly Marchisio, Wei Qi Leong, Yosephine Susanto, Raymond Ng, Shayne Longpre, Wei-Yin Ko, Sebastian Ruder, Madeline Smith, Antoine Bosselut, Alice Oh, Andre F. T. Martins, Leshem Choshen, Daphne Ippolito, Enzo Ferrante, Marzieh Fadaee, Beyza Ermis, Sara Hooker
- Reference count: 40
- Primary result: 28% of MMLU questions require culturally sensitive knowledge, with 84.9% of geographic questions focusing on North America or Europe

## Executive Summary
Global-MMLU addresses critical cultural and linguistic biases in multilingual evaluations by annotating MMLU questions for cultural, geographic, and dialect-specific knowledge. The dataset improves translations across 42 languages through a combination of professional and community annotations with machine translation. Results show significant model ranking changes between culturally sensitive and culturally agnostic subsets, with an average of 5.7 rank changes and 7.3 position shifts on culturally sensitive data. The work demonstrates that human-translated data is crucial for reliable evaluation, especially for low-resource languages which show 98% higher variability compared to high-resource languages.

## Method Summary
The researchers annotated 1,785 MMLU questions for cultural sensitivity, identifying questions requiring Western-centric cultural, geographic, or dialect knowledge. They improved translations using human verification to minimize translation artifacts, particularly for low-resource languages. The dataset includes culturally sensitive (CS) and culturally agnostic (CA) subsets, enabling balanced evaluation. They evaluated 14 state-of-the-art multilingual models on these subsets across different resource levels, analyzing rank changes and performance variability.

## Key Results
- 28% of MMLU questions require culturally sensitive knowledge, with 54.7% of these being geographic questions
- Model rankings change significantly between CS and CA subsets, averaging 5.7 rank changes and 7.3 position shifts
- Low-resource languages show 98% higher standard deviation in performance compared to high-resource languages
- 84.9% of geographic questions focus on North America or Europe, highlighting Western-centric bias

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Cultural and geographic biases in MMLU significantly distort model rankings when evaluated on multilingual data.
- Mechanism: Questions requiring Western-centric cultural, geographic, or dialect knowledge (28% of MMLU) lead to overfitting on culturally sensitive (CS) subsets, causing models to rank differently compared to culturally agnostic (CA) subsets.
- Core assumption: Model performance is sensitive to the cultural context embedded in evaluation data.
- Evidence anchors:
  - [abstract] "Rankings of model evaluations change depending on whether they are evaluated on the full portion or the subset of questions annotated as culturally sensitive, showing the distortion to model rankings when blindly relying on translated MMLU."
  - [section 2.2] "Our analysis reveals that 28% of MMLU requires CS knowledge... geographic knowledge emerges as the most frequently tagged bias, representing 54.7% of all CS questions."
  - [corpus] Weak - no direct evidence for mechanism impact on model rankings.

### Mechanism 2
- Claim: Human-translated datasets provide more reliable evaluations than machine-translated ones, especially for low-resource languages.
- Mechanism: Machine translation introduces artifacts (translationese) that distort meaning and clarity, leading to unreliable performance measurements. Human verification mitigates this risk.
- Core assumption: Translation quality directly impacts evaluation accuracy.
- Evidence anchors:
  - [abstract] "translation often introduces artifacts that can distort the meaning or clarity of questions in the target language."
  - [section 3.1] "We rely as much as possible on human-verified translations to ensure that the translations are reliable and minimize the biases introduced, specifically translationese."
  - [section 4.2] "models generally perform better on human-translated data for high-resource languages."

### Mechanism 3
- Claim: Low-resource languages show higher variability in model performance, especially on culturally sensitive tasks.
- Mechanism: Limited training data for low-resource languages leads to weaker model generalization and greater sensitivity to cultural nuances.
- Core assumption: Resource availability correlates with model robustness across cultural contexts.
- Evidence anchors:
  - [abstract] "Performance degrades on low-resource languages with higher variability."
  - [section 4.2] "Low-resource languages exhibit significantly higher standard deviations... These represent increases of 98% and 75% compared to high-resource languages."
  - [corpus] Weak - no direct evidence for mechanism's impact on low-resource languages.

## Foundational Learning

- Concept: Cultural bias in evaluation datasets
  - Why needed here: Understanding how cultural context affects model performance is critical for developing fair and accurate multilingual evaluations.
  - Quick check question: Why might a question about US history be problematic for global model evaluation?

- Concept: Translation artifacts and translationese
  - Why needed here: Recognizing how machine translation can distort meaning helps explain why human verification is necessary for reliable evaluations.
  - Quick check question: What are potential issues with relying solely on machine-translated evaluation data?

- Concept: Resource-based language categorization
  - Why needed here: Understanding how language resource availability affects model performance is key to interpreting evaluation results across different language groups.
  - Quick check question: How might low-resource languages differ in evaluation performance compared to high-resource languages?

## Architecture Onboarding

- Component map: Cultural annotation → Translation improvement → Model evaluation → Analysis of CS/CA performance differences
- Critical path: Annotate MMLU for cultural sensitivity → Translate and verify translations → Evaluate models on CS and CA subsets → Analyze rank changes and performance variability
- Design tradeoffs: Using machine translation for broad coverage vs. human translation for quality; balancing CS/CA subsets for fair evaluation vs. maintaining subject diversity
- Failure signatures: Large rank changes between CS and CA subsets indicate cultural bias; high variability in low-resource languages suggests insufficient training data
- First 3 experiments:
  1. Compare model performance on CS vs CA subsets for a single high-resource language.
  2. Analyze translation quality differences between machine and human-translated samples.
  3. Evaluate model rank stability across different resource levels (high, mid, low).

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of models on culturally-sensitive (CS) datasets compare to culturally-agnostic (CA) datasets when evaluated on languages with no available human-translated data?
- Basis in paper: [explicit] The paper shows that low-resource languages exhibit higher variability in model performance on both CS and CA datasets, with larger performance gaps between human-translated and machine-translated data. However, it does not specifically analyze performance differences between CS and CA datasets in the absence of human-translated data.
- Why unresolved: The paper focuses on comparing model performance across different resource levels and dataset types (human-translated vs. machine-translated) but does not isolate the impact of dataset type (CS vs. CA) on languages without human-translated data.
- What evidence would resolve it: Evaluations of model performance on CS and CA datasets for low-resource languages using only machine-translated data, comparing accuracy, variability, and model rankings.

### Open Question 2
- Question: To what extent do cultural biases in MMLU affect model performance on tasks requiring logical reasoning or abstract problem-solving?
- Basis in paper: [inferred] The paper identifies that 28% of MMLU questions require culturally-sensitive knowledge, with a significant portion focusing on Western-centric concepts. However, it does not specifically analyze how these biases impact tasks requiring logical reasoning or abstract problem-solving.
- Why unresolved: The paper provides insights into the prevalence of cultural biases but does not examine their impact on specific types of tasks, such as those requiring logical reasoning or abstract problem-solving.
- What evidence would resolve it: Analysis of model performance on CS and CA subsets for tasks involving logical reasoning or abstract problem-solving, comparing accuracy and error patterns.

### Open Question 3
- Question: How do the rankings of models trained on culturally diverse data compare to those trained on predominantly Western-centric data when evaluated on CS datasets?
- Basis in paper: [explicit] The paper demonstrates that model rankings change significantly between culturally-sensitive and culturally-agnostic subsets, with larger shifts observed on CS datasets. However, it does not compare the performance of models trained on culturally diverse data versus those trained on Western-centric data.
- Why unresolved: The paper evaluates model performance on CS and CA subsets but does not investigate the impact of training data diversity on model performance in culturally-sensitive contexts.
- What evidence would resolve it: Comparative evaluations of models trained on culturally diverse data and those trained on Western-centric data on CS datasets, analyzing accuracy, variability, and ranking changes.

## Limitations

- The cultural sensitivity annotations rely heavily on subjective judgments by annotators, which may introduce inconsistencies across languages and cultures.
- The dataset's reliance on a subset of MMLU questions (1,785 out of 2,850) for annotation may not capture the full spectrum of cultural biases present in the original dataset.
- The evaluation of only 14 models limits the generalizability of the findings across the broader landscape of multilingual models.

## Confidence

**High Confidence**: The observation that model rankings change significantly between culturally sensitive and culturally agnostic subsets is well-supported by the data, showing an average of 5.7 rank changes and 7.3 position shifts. The finding that low-resource languages exhibit higher performance variability (98% and 75% increases in standard deviation compared to high-resource languages) is also strongly supported by the results.

**Medium Confidence**: The claim that human-translated datasets provide more reliable evaluations than machine-translated ones is supported by performance differences, but the analysis is limited to 14 languages with human translations. The assertion that 28% of MMLU questions require culturally sensitive knowledge is based on the annotation process but may not be exhaustive.

**Low Confidence**: The mechanism explaining why low-resource languages show higher variability in culturally sensitive tasks is speculative and lacks direct empirical support. The analysis does not fully explore alternative explanations for performance differences across resource levels.

## Next Checks

1. **Cross-annotator reliability test**: Conduct inter-annotator agreement analysis across multiple annotator pairs to quantify the consistency of cultural sensitivity annotations and identify potential sources of bias in the annotation process.

2. **Expanded model evaluation**: Evaluate a broader range of multilingual models (at least 20 additional models) across the CS and CA subsets to determine if the observed rank changes and performance patterns hold across a more diverse model population.

3. **Temporal validation of translations**: Re-evaluate model performance on Global-MMLU after one year using updated machine translation systems to assess whether improvements in translation quality reduce the need for human-verified translations and mitigate cultural bias artifacts.