---
ver: rpa2
title: Learning Complex Word Embeddings in Classical and Quantum Spaces
arxiv_id: '2412.13745'
source_url: https://arxiv.org/abs/2412.13745
tags:
- complex
- word
- quantum
- skip-gram
- which
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'The paper develops and evaluates methods for training complex-valued
  word embeddings, extending the classical Skip-gram model. It introduces two approaches:
  replacing real values with complex numbers, and using parameterised quantum circuits
  (PQCs) to generate normalised complex vectors.'
---

# Learning Complex Word Embeddings in Classical and Quantum Spaces

## Quick Facts
- arXiv ID: 2412.13745
- Source URL: https://arxiv.org/abs/2412.13745
- Reference count: 12
- Key outcome: Complex-valued word embeddings trained using a two-stage process (classical Skip-gram followed by PQC fitting) perform competitively with classical real-valued baselines while enabling quantum hardware inference

## Executive Summary
This paper develops methods for training complex-valued word embeddings by extending the classical Skip-gram model. The authors introduce two approaches: replacing real values with complex numbers, and using parameterized quantum circuits (PQCs) to generate normalized complex vectors. A key technical contribution is an efficient C implementation that enables training on large corpora with vocabularies exceeding 400k words. Experimental results on standard semantic similarity datasets show that complex embeddings perform competitively with classical baselines, and PQCs fitted to these embeddings achieve identical performance. This demonstrates that PQCs can perfectly reproduce arbitrary complex word embeddings when given sufficient expressive capacity.

## Method Summary
The paper extends the Skip-gram with negative sampling model to complex-valued embeddings by replacing the real-valued inner product with the absolute value of the complex inner product squared (Fidelity). This maintains the semantic similarity learning objective while leveraging complex geometry. The two-stage training process first uses an optimized C implementation to train complex embeddings on large corpora, then fits PQCs to the normalized embeddings. The PQCs, being unitary transformations, can represent any normalized complex vector state. This approach scales with vocabulary size rather than corpus size, making it practical for large-scale quantum embedding training.

## Key Results
- Complex embeddings trained with the C implementation achieve correlation scores of 0.67 on WordSim353, competitive with classical baselines
- PQCs fitted to complex embeddings achieve identical performance, demonstrating perfect reproduction capability
- The two-stage approach scales efficiently, enabling training on a 3.8B-word corpus with 426k vocabulary size
- Different PQC ansatze and depths show varying performance, with more expressive circuits achieving better results

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The complex-valued Skip-gram model maintains competitive performance by replacing real-valued embeddings with complex numbers while preserving the core distributional hypothesis.
- Mechanism: The model replaces the real-valued inner product with a scaled absolute value of the complex inner product squared, which maps to [0,1] and can be treated as a probability. This maintains the semantic similarity learning objective while leveraging complex geometry.
- Core assumption: The complex inner product's absolute value squared (Fidelity) preserves the relative similarity relationships between word pairs.
- Evidence anchors:
  - [abstract]: "complex embeddings perform competitively with classical real-valued baselines"
  - [section]: "we find that, while training the PQCs directly tends to harm performance, the quantum word embeddings from the two-stage process perform as well as the classical Skip-gram embeddings"
  - [corpus]: No direct evidence found in corpus; this is inferred from the paper's experimental results
- Break condition: If the complex geometry fundamentally alters the similarity relationships captured by the distributional hypothesis, performance would degrade.

### Mechanism 2
- Claim: Parameterized Quantum Circuits (PQCs) can perfectly reproduce arbitrary complex word embeddings when given sufficient expressive capacity.
- Mechanism: PQCs, being unitary transformations, can represent any normalized complex vector state. The two-stage training process (classical complex Skip-gram followed by PQC fitting) allows learning in reduced space first, then mapping to quantum representation.
- Core assumption: PQCs with sufficient depth and appropriate ansatze can represent the full space of complex word embeddings.
- Evidence anchors:
  - [abstract]: "PQCs fitted to the output of the C implementation achieve identical performance to the original complex embeddings"
  - [section]: "we find that with an expressive ansatz we are able to fit the vectors perfectly"
  - [corpus]: Weak evidence; corpus mentions related quantum embedding work but doesn't validate this specific claim
- Break condition: If the ansatz is not sufficiently expressive or the PQC depth is inadequate, perfect reproduction would fail.

### Mechanism 3
- Claim: The two-stage training approach scales efficiently with vocabulary size rather than corpus size, enabling large-scale quantum embedding training.
- Mechanism: First stage trains complex embeddings using highly optimized C implementation (scales with corpus size). Second stage fits PQCs to learned embeddings (scales with vocabulary size only). This decouples the computationally expensive corpus processing from the PQC training.
- Core assumption: The vocabulary size is much smaller than the corpus size, making the second stage computationally feasible.
- Evidence anchors:
  - [abstract]: "This enables a highly scalable route to learning embeddings in complex spaces which scales with the size of the vocabulary rather than the size of the training corpus"
  - [section]: "the second fitting stage only scales with the size of the vocabulary, and so the substantial increase in training time from introducing the PQCs is not a barrier to using large training sets"
  - [corpus]: No direct evidence found; this is a design claim based on computational complexity analysis
- Break condition: If vocabulary size becomes comparable to corpus size, the scaling advantage disappears.

## Foundational Learning

- Concept: Skip-gram with negative sampling
  - Why needed here: This is the foundation model being extended to complex and quantum spaces
  - Quick check question: What is the difference between Skip-gram with negative sampling and the original Skip-gram model?

- Concept: Complex vector spaces and inner products
  - Why needed here: The core mathematical operation changes from real inner products to complex inner products
  - Quick check question: How does the absolute value of a complex inner product squared relate to probability?

- Concept: Parameterized Quantum Circuits (PQCs)
  - Why needed here: PQCs are the mechanism for generating quantum word embeddings
  - Quick check question: Why do unitary transformations preserve normalization in quantum states?

## Architecture Onboarding

- Component map: C implementation of complex Skip-gram -> PyTorch fitting module -> Ansatz selection -> Evaluation pipeline
- Critical path: 1. Train complex embeddings using C implementation 2. Normalize embeddings 3. Fit PQCs to normalized embeddings 4. Evaluate on semantic similarity datasets
- Design tradeoffs:
  - Embedding dimension vs. qubit count (logarithmic relationship)
  - Ansatz expressivity vs. circuit depth and training stability
  - Vocabulary size vs. computational resources for PQC fitting
- Failure signatures:
  - Loss plateaus during PQC fitting (insufficient expressivity)
  - Correlation drops significantly (quantum encoding doesn't preserve semantic relationships)
  - Training time explodes (incorrect scaling assumption)
- First 3 experiments:
  1. Run the C implementation on a small corpus (WikiText-103) with vocabulary size ~100k, embedding dimension 64, and verify correlation on WordSim353
  2. Take the output embeddings and fit PQCs with 3 layers of Ansatz 5, verify perfect reconstruction of embeddings
  3. Scale up to the 3.8B-word corpus with vocabulary size 426k, compare performance with classical baseline

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the quantum word embeddings trained in this work provide any performance advantage over classical embeddings for downstream NLP tasks?
- Basis in paper: Explicit - "whether any of this increased expressivity results in improved performance in practice, however, remains to be seen"
- Why unresolved: The paper only evaluates embeddings on semantic similarity tasks, not on actual NLP tasks like classification or generation
- What evidence would resolve it: Experiments applying the quantum embeddings to real NLP tasks and comparing performance to classical baselines

### Open Question 2
- Question: What is the minimum number of qubits required for the quantum embeddings to outperform classical embeddings?
- Basis in paper: Explicit - "We have found that, with e.g. 3 layers of A5, and 5,000 iterations of training, we can fit the vectors perfectly"
- Why unresolved: The paper uses 6 qubits for complex embeddings but doesn't explore the performance trade-off with fewer qubits
- What evidence would resolve it: Systematic experiments with varying numbers of qubits (e.g., 2, 4, 6, 8) to find the threshold where quantum embeddings become competitive

### Open Question 3
- Question: Can the quantum embeddings capture hierarchical relationships between words better than classical embeddings?
- Basis in paper: Explicit - "One potential advantage of using complex-valued embeddings is in learning hierarchical relationships"
- Why unresolved: The paper only evaluates on similarity and relatedness datasets, not on hierarchical relations
- What evidence would resolve it: Evaluation on datasets specifically designed to test hierarchical word relationships (e.g., hypernymy/hyponymy datasets)

## Limitations
- The two-stage training approach, while enabling scalability, means direct training of PQCs on large corpora remains computationally prohibitive
- Evaluation focuses only on semantic similarity tasks, leaving open questions about downstream task performance
- Claims of "identical" performance between PQCs and original complex embeddings may not account for small numerical differences

## Confidence

**High Confidence:** The core claim that complex-valued Skip-gram maintains competitive performance with real-valued baselines is well-supported by experimental results across multiple datasets (WordSim353, MEN, RG-65, SCWS). The demonstration that PQCs can perfectly reproduce complex embeddings when fitted post-hoc is also strongly supported.

**Medium Confidence:** The scalability claims regarding the two-stage approach are logically sound but rely on computational complexity arguments rather than empirical validation across diverse corpus sizes. The performance differences between different PQC ansatze and depths are demonstrated but could benefit from more systematic exploration.

**Low Confidence:** The assertion that PQCs fitted to complex embeddings achieve "identical" performance may be overstated, as small numerical differences could exist that weren't captured in the evaluation metrics used.

## Next Checks

1. **Downstream Task Validation:** Evaluate the learned quantum embeddings on downstream NLP tasks (text classification, sentiment analysis) to assess practical utility beyond semantic similarity.

2. **Scaling Benchmark:** Systematically test the two-stage approach across varying corpus sizes and vocabulary sizes to empirically validate the claimed scaling advantages.

3. **Expressivity Analysis:** Conduct a more thorough analysis of different PQC ansatze and depths to identify the minimum requirements for perfect embedding reproduction, and test performance with noisy quantum hardware simulations.