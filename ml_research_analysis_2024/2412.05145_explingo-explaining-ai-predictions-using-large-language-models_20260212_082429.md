---
ver: rpa2
title: 'Explingo: Explaining AI Predictions using Large Language Models'
arxiv_id: '2412.05145'
source_url: https://arxiv.org/abs/2412.05145
tags:
- narratives
- narrative
- explanation
- explanations
- prompt
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper presents Explingo, a system that uses Large Language
  Models (LLMs) to transform traditional ML explanations (like SHAP values) into human-readable
  narratives. The system includes two components: a Narrator that generates the narratives
  and a Grader that automatically evaluates their quality using metrics like accuracy,
  completeness, fluency, and conciseness.'
---

# Explingo: Explaining AI Predictions using Large Language Models

## Quick Facts
- arXiv ID: 2412.05145
- Source URL: https://arxiv.org/abs/2412.05145
- Reference count: 32
- Primary result: LLMs can generate human-readable narratives from SHAP explanations with automated quality evaluation

## Executive Summary
This paper presents Explingo, a system that uses Large Language Models (LLMs) to transform traditional ML explanations (like SHAP values) into human-readable narratives. The system includes two components: a Narrator that generates the narratives and a Grader that automatically evaluates their quality using metrics like accuracy, completeness, fluency, and conciseness. Experiments show that with a small number of human-labeled and bootstrapped exemplar narratives, the system can generate high-quality narratives that achieve high scores across all metrics. The approach addresses the challenge of making ML explanations more accessible to users by converting technical explanations into natural language descriptions, while maintaining evaluation rigor through automated grading.

## Method Summary
Explingo uses LLMs to convert structured ML explanations (specifically SHAP values) into natural language narratives. The system consists of two main components: a Narrator that generates narratives from explanations using exemplar prompts, and a Grader that automatically evaluates narrative quality using the same LLM technology. The Grader assesses narratives across four metrics - accuracy, completeness, fluency, and conciseness - by comparing them to the original explanations. The system employs bootstrapping techniques to generate high-quality exemplar narratives that improve the Narrator's performance over time.

## Key Results
- LLMs can generate high-quality narratives from SHAP explanations that achieve high scores across accuracy, completeness, fluency, and conciseness metrics
- Bootstrapped exemplars significantly improve narrative accuracy and completeness compared to hand-written examples alone
- The automated Grader provides reliable quality assessment, reducing the need for manual evaluation
- A small number of exemplars (3-5) is sufficient for the Narrator to generate coherent, accurate narratives

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLMs can reliably transform structured XAI explanations (e.g., SHAP values) into natural language narratives.
- Mechanism: The Narrator subsystem prompts an LLM with the raw explanation, its format, and a short context, then generates a fluent narrative. Because the LLM is given structured, domain-specific exemplars, it can align its output to the desired style.
- Core assumption: LLMs, when guided by a small set of hand-written exemplar narratives, can generalize to new explanations while preserving both accuracy and fluency.
- Evidence anchors:
  - [abstract] "Our experiments demonstrate that LLMs can generate high-quality narratives that achieve high scores across all metrics..."
  - [section VI] "Base Prompts... We found that the Narrator was able to generate reasonably complete and accurate narratives with these base prompts..."
  - [corpus] Weak evidence: the related papers focus on other LLM/XAI integration strategies but do not directly validate narrative generation from SHAP.
- Break condition: If exemplars are too few or too stylistically diverse, the LLM may hallucinate or fail to preserve critical information.

### Mechanism 2
- Claim: An LLM-based Grader can automatically assess narrative quality on accuracy, completeness, fluency, and conciseness.
- Mechanism: The Grader prompts an independent LLM with the original explanation, the generated narrative, and a rubric. The LLM returns a score for each metric. This avoids manual grading and scales across domains.
- Core assumption: LLMs can reliably compare a narrative to its source explanation and assign consistent, repeatable scores without human intervention.
- Evidence anchors:
  - [abstract] "...and a Grader that automatically evaluates their quality using metrics like accuracy, completeness, fluency, and conciseness."
  - [section V] "Our G RADER automatically evaluates the quality of the narrative by prompting an independent LLM..."
  - [corpus] Weak evidence: the related papers do not discuss automated LLM grading of XAI narratives.
- Break condition: In complex domains (e.g., mushroom toxicity) the Grader may over-emphasize safety or misunderstand nuanced language, leading to inaccurate grades.

### Mechanism 3
- Claim: Bootstrapping few-shot exemplars improves both accuracy and fluency by creating high-scoring narrative templates that the Narrator can follow.
- Mechanism: DSPy's BootstrapFewShot function generates new exemplars that achieve perfect or near-perfect scores on the Grader metrics. These exemplars are then added to the prompt, reducing the risk of noisy or incomplete hand-written examples.
- Core assumption: High-scoring exemplar narratives serve as better templates for the LLM, improving consistency across generated narratives.
- Evidence anchors:
  - [section VI] "Bootstrapped Few-Shot... we used DSPy’s BootstrapFewShot to create more exemplar narratives that score highly according to our G RADER."
  - [section VII] "Bootstrapped exemplars contribute more to generating accurate and complete narratives..."
  - [corpus] No direct evidence; this is a novel experimental contribution.
- Break condition: If bootstrapped exemplars are too rigid or miss edge cases, the Narrator may fail on out-of-distribution explanations.

## Foundational Learning

- Concept: SHAP (SHapley Additive exPlanations) values and their interpretation.
  - Why needed here: The Narrator and Grader operate directly on SHAP explanations, so understanding feature contributions and their signs is essential for accuracy and completeness grading.
  - Quick check question: Given the SHAP explanation (size, 120, -120), what does the negative value indicate about this feature's impact on the prediction?

- Concept: Few-shot learning and exemplar-based prompting in LLMs.
  - Why needed here: The system relies on a small set of exemplar narratives to steer the LLM's style and structure; understanding how to curate and format these is key to good results.
  - Quick check question: If you have three exemplar narratives with different lengths and styles, how would you decide which to include in the prompt to optimize fluency without hurting accuracy?

- Concept: Automated evaluation using LLM graders.
  - Why needed here: The Grader replaces manual scoring, so understanding rubric design and LLM prompt tuning is crucial for reliable metric assignment.
  - Quick check question: If the Grader consistently scores a metric lower than expected, what prompt adjustments could you make to improve alignment with human judgment?

## Architecture Onboarding

- Component map:
  - SHAP Explanation -> Narrator (LLM with exemplars) -> Narrative
  - SHAP Explanation -> Grader (LLM with rubric) -> Scores (Accuracy, Completeness, Fluency, Conciseness)

- Critical path:
  1. Generate SHAP explanation from model prediction.
  2. Parse explanation into consistent format.
  3. Pass explanation, format, context, and exemplars to Narrator.
  4. Receive narrative output.
  5. Pass explanation, narrative, and rubric to Grader.
  6. Use Grader scores for filtering, tuning, or guardrails.

- Design tradeoffs:
  - More exemplars → better style matching but higher chance of confusion or hallucination.
  - Higher Grader weight on accuracy → fewer accepted narratives but more trustworthy outputs.
  - Smaller LLM models → faster, cheaper, but lower accuracy/completeness scores.

- Failure signatures:
  - Low accuracy score → narrative missing key feature values or misstates contribution direction.
  - Low completeness score → narrative omits one or more features from explanation.
  - Low fluency score → narrative does not match exemplar style or is grammatically awkward.
  - Low conciseness score → narrative is overly verbose relative to number of features.

- First 3 experiments:
  1. Test Narrator with zero exemplars on a simple dataset (e.g., Ames Housing) and evaluate accuracy, completeness, fluency, conciseness.
  2. Add one hand-written exemplar and re-run; compare metric shifts, especially fluency.
  3. Add bootstrapped exemplars (e.g., 2-3) and re-run; check if accuracy/completeness improve while maintaining fluency gains.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can we ensure narrative explanations remain accurate when dealing with comparative terms like "larger" or "smaller" that require distributional context?
- Basis in paper: [explicit] The paper notes that narratives using comparative terms may be incorrectly scored as inaccurate by the G RADER because it lacks information about feature distributions.
- Why unresolved: The G RADER currently grades narratives without access to broader data distributions, making it impossible to validate relative terms.
- What evidence would resolve it: Developing a method to pass statistical context (like mean values or distributions) to the grader and testing whether this improves accuracy scores for comparative narratives.

### Open Question 2
- Question: What is the optimal balance between hand-written and bootstrapped exemplars for different domain types and user preferences?
- Basis in paper: [explicit] The paper found that a mix of exemplars performed best but noted that users in different domains may value metrics differently and wish to modify exemplar numbers accordingly.
- Why unresolved: The experiments used a fixed ratio and number of exemplars, but the paper acknowledges this may not be optimal across all use cases.
- What evidence would resolve it: Systematic experiments varying the ratio and number of exemplars across diverse domains while measuring impact on all four metrics (accuracy, completeness, fluency, conciseness).

### Open Question 3
- Question: Can smaller, local LLM models achieve comparable narrative generation quality to larger API-based models after appropriate finetuning?
- Basis in paper: [explicit] The paper found that a 7B parameter model achieved good style scores but poor accuracy/completeness, suggesting finetuning may be needed.
- Why unresolved: The paper only tested out-of-the-box performance on a small model and did not attempt finetuning approaches.
- What evidence would resolve it: Comparing narrative quality metrics between finetuned smaller models and larger models across diverse datasets to determine if finetuning closes the performance gap.

## Limitations
- The Grader's reliability is untested in high-stakes domains where safety or bias is critical (e.g., medical diagnosis or criminal justice).
- There is no validation against human judgment for the final narrative outputs, so perceived quality may differ from automated scores.
- Exemplar quality and quantity trade-offs are not fully explored; too few or too diverse exemplars may lead to degraded performance.

## Confidence
- **High**: LLMs can generate human-readable narratives from SHAP explanations when guided by exemplars.
- **Medium**: The Grader reliably assigns scores across all four metrics without human oversight.
- **Low**: Bootstrapped exemplars consistently improve both accuracy and completeness without side effects.

## Next Checks
1. Compare Grader scores to human evaluations on a held-out set of narratives to measure alignment.
2. Test the Narrator and Grader on a safety-critical domain (e.g., medical imaging) to assess performance under high stakes.
3. Conduct ablation studies varying the number and diversity of exemplars to find the optimal balance for accuracy and fluency.