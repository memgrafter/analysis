---
ver: rpa2
title: 'The Economic Implications of Large Language Model Selection on Earnings and
  Return on Investment: A Decision Theoretic Model'
arxiv_id: '2405.17637'
source_url: https://arxiv.org/abs/2405.17637
tags:
- costs
- earnings
- cost
- business
- project
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a decision-theoretic framework to evaluate
  large language models (LLMs) based on their economic impact, including earnings
  and return on investment (RoI). The authors propose models that consider variables
  such as cost per token, probability of success, and gains or losses from using LLMs.
---

# The Economic Implications of Large Language Model Selection on Earnings and Return on Investment: A Decision Theoretic Model

## Quick Facts
- arXiv ID: 2405.17637
- Source URL: https://arxiv.org/abs/2405.17637
- Authors: Geraldo Xexéo; Filipe Braida; Marcus Parreiras; Paulo Xavier
- Reference count: 10
- This paper introduces a decision-theoretic framework to evaluate large language models (LLMs) based on their economic impact, including earnings and return on investment (RoI).

## Executive Summary
This paper presents a decision-theoretic framework for evaluating large language models (LLMs) based on their economic impact, including earnings and return on investment (RoI). The authors propose models that consider variables such as cost per token, probability of success, and gains or losses from using LLMs. They find that superior accuracy can justify higher costs through greater earnings but not necessarily larger RoI. Sensitivity analysis shows that predicted gains and losses, along with success probabilities, most impact model outcomes. The framework helps companies align LLM investments with strategic financial objectives.

## Method Summary
The authors develop decision-theoretic models to evaluate LLMs by calculating expected earnings and RoI using probability-weighted outcomes. The framework incorporates transaction-specific variables including cost per token, probability of success, gains, and losses. Sensitivity analysis is performed using partial derivatives and Sobol variance-based indices to identify which parameters most influence outcomes. The model treats each transaction as a probabilistic outcome with gains, losses, and costs, allowing comparison between higher-accuracy, higher-cost models and lower-cost, lower-accuracy alternatives.

## Key Results
- Superior accuracy can justify higher costs through greater earnings but not necessarily larger RoI
- Sensitivity analysis reveals that predicted gains and losses, along with success probabilities, most impact model outcomes
- Cost per token dominates RoI when transaction size is large; accuracy dominates when transaction size is small

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Decision-theoretic RoI models align LLM selection with financial outcomes.
- Mechanism: The model treats each transaction as a probabilistic outcome with gains (G), losses (L), and costs (C). By calculating expected earnings and RoI using probability-weighted outcomes, it captures the trade-off between higher-accuracy, higher-cost models versus lower-cost, lower-accuracy ones.
- Core assumption: Probabilities of success and failure, and the magnitudes of gains/losses, are known or estimable.
- Evidence anchors:
  - [abstract]: "use a decision-theoretic approach to compare the financial impact of different LLMs, considering variables such as the cost per token, the probability of success in the specific task, and the gain and losses associated with LLMs use."
  - [section 3]: "the expected value for the utility of an states is: E[u(s)]≡E[f] :=∑ pi ui"
- Break condition: If probability estimates are inaccurate or if gains/losses are highly volatile, the model becomes unreliable.

### Mechanism 2
- Claim: Sensitivity analysis reveals which parameters most influence RoI and earnings.
- Mechanism: By computing partial derivatives of the earnings and RoI equations with respect to each variable, the model quantifies how sensitive outcomes are to changes in G, L, P, C, and T. Sobol variance-based indices then validate these sensitivities globally.
- Core assumption: The functional forms are sufficiently smooth and differentiable for local analysis; Sobol indices assume probabilistic distributions of inputs.
- Evidence anchors:
  - [section 4.2.1]: "partial derivatives for E[E] are: ∂E[E]/∂G = P, ∂E[E]/∂L = P−1, ∂E[E]/∂C =−T, ∂E[E]/∂T =−C, ∂E[E]/∂P = G + L."
  - [section 4.2.2]: "Sobol sensitivity analysis... using the SALib package... found that P is clearly the most important factor in the first order analysis."
- Break condition: If the model is highly nonlinear or if parameter ranges are very wide, local derivatives may mislead.

### Mechanism 3
- Claim: Cost per token dominates RoI when transaction size is large; accuracy dominates when transaction size is small.
- Mechanism: The model separates fixed costs (development) from variable costs (C per token × T tokens). For small T, the product CT is negligible, so earnings ≈ GP − L(1−P). For large T, CT dominates, making RoI ≈ (GP − L(1−P))/(CT).
- Core assumption: C is known per LLM; T is under project control.
- Evidence anchors:
  - [section 4.2]: "if G and L are large and T small, the factor CT will be small and E[E] will be mainly a function of GP−L(1−P)... For small G and L, and larger T, compression could be effective."
  - [section 2.3]: "the cost per token value, C, is usually very small... For example, for 1,000,000 tokens, the OpenAI prices in May 2024 for tokens in the input were US$5.00 for gpt-4o and 0.50 for gpt-3.5-turbo-0125."
- Break condition: If transaction sizes vary widely across use cases, a single model may mislead.

## Foundational Learning

- Concept: Decision theory and expected utility
  - Why needed here: The framework relies on computing expected values over probabilistic outcomes.
  - Quick check question: What is the expected earnings formula if P(success)=0.9, G=$10, L=$1, CT=$0.01?
    - Answer: E[E] = 0.9×10 − 0.1×1 − 0.01 = 9.0 − 0.1 − 0.01 = $8.89.

- Concept: Sensitivity analysis (local derivatives + global Sobol indices)
  - Why needed here: To identify which variables most influence financial outcomes.
  - Quick check question: If ∂E[E]/∂P = G+L, what does that imply about the impact of improving accuracy?
    - Answer: Each 1% improvement in P increases earnings by (G+L) dollars.

- Concept: Binary classification confusion matrix and cost-sensitive learning
  - Why needed here: Section 5 models true positives, false positives, etc., with distinct gains/losses.
  - Quick check question: In a binary task, if TP=0.2, FP=0.05, FN=0.1, TN=0.65, G=$5, LFP=$2, LFN=$3, CT=$0.001, what is E[E]?
    - Answer: E[E] = (5−0.001)×0.2 − (3+0.001)×0.1 − (2+0.001)×0.05 = 0.9979 − 0.3001 − 0.1001 ≈ $0.5977.

## Architecture Onboarding

- Component map: Input data (tokens, cost per token, probabilities) -> Decision-theoretic equations (earnings, RoI) -> Sensitivity analysis module -> Comparative rankings

- Critical path:
  1. Gather cost per token for candidate LLMs
  2. Estimate or measure success probabilities and transaction sizes
  3. Compute earnings and RoI for each LLM
  4. Run sensitivity analysis to confirm robustness
  5. Select model with best trade-off for business goals

- Design tradeoffs:
  - Accuracy vs. cost: Higher accuracy LLMs cost more per token but may reduce loss exposure
  - Granularity vs. complexity: Including all cost components increases realism but complicates the model
  - Static vs. dynamic: Fixed-parameter models are simpler; dynamic models are more realistic but require more data

- Failure signatures:
  - Over-reliance on local sensitivity when model is highly nonlinear
  - Ignoring probability estimation error when G and L are large
  - Assuming uniform transaction sizes when real usage varies widely

- First 3 experiments:
  1. **Toy cost comparison**: Compare two LLMs with fixed G=$10, L=$1, P1=0.95/C=$0.00001 vs. P2=0.80/C=$0.0000005. Verify RoI ranking matches intuition.
  2. **Sensitivity sweep**: Vary P from 0.75 to 1.0 and plot E[E] and E[R] for each LLM to confirm ∂E[E]/∂P = G+L.
  3. **Transaction size impact**: Fix P, G, L; vary T from 100 to 128,000 tokens. Observe when the higher-cost LLM becomes uneconomical.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of LLMs vary across different industries and business models when evaluated using the proposed decision-theoretic framework?
- Basis in paper: [inferred] The paper suggests that different industries and business models have unique operational variables and challenges that can affect the adoption and success of LLMs.
- Why unresolved: The paper does not provide specific case studies or empirical data from different industries to validate the framework's applicability across diverse business contexts.
- What evidence would resolve it: Empirical studies or case analyses from various industries applying the decision-theoretic framework to evaluate LLMs, showing variations in performance and ROI across different business models.

### Open Question 2
- Question: What are the long-term financial implications of integrating LLMs into business operations, considering factors such as periodic fine-tuning and maintenance expenses?
- Basis in paper: [explicit] The paper mentions the need for a comprehensive evaluation of LLMs, including periodic fine-tuning and maintenance expenses, to provide a more accurate assessment of the total cost of ownership.
- Why unresolved: The paper does not provide a detailed analysis of the long-term financial implications, including the costs and benefits over time.
- What evidence would resolve it: Longitudinal studies or financial models that project the costs and benefits of LLM integration over several years, including periodic updates and maintenance.

### Open Question 3
- Question: How do different prompt strategies and input sizes impact the probability of success and overall ROI of LLM applications in business tasks?
- Basis in paper: [explicit] The paper discusses the impact of input size on the probability of success and suggests that modeling the probability of success as a function of input size and prompt strategies could enhance the framework.
- Why unresolved: The paper does not provide empirical data or models that quantify the impact of different prompt strategies and input sizes on LLM performance and ROI.
- What evidence would resolve it: Experimental studies or simulations that test various prompt strategies and input sizes, measuring their impact on the probability of success and ROI in specific business tasks.

## Limitations
- The model's sensitivity to parameter estimation, particularly when gains and losses are large relative to costs
- Reliance on accurate probability estimation and well-characterized transaction sizes
- Binary classification extension requires detailed confusion matrix data that may not be readily available

## Confidence
- Theoretical framework: High confidence
- Practical applicability: Medium confidence
- Parameter estimation: Low confidence

## Next Checks
1. **Parameter Sensitivity Validation**: Conduct a comprehensive sensitivity analysis varying all input parameters across realistic ranges to identify which combinations of G, L, P, and T values would lead to different LLM selection decisions.

2. **Real-World Application Test**: Apply the framework to a specific business case with actual cost data and measured performance metrics from at least two different LLM providers to validate the practical utility of the model.

3. **Probability Estimation Method Comparison**: Compare the impact of different probability estimation approaches (historical performance, cross-validation, expert judgment) on the final model selection to quantify the uncertainty introduced by this critical parameter.