---
ver: rpa2
title: Improving SMOTE via Fusing Conditional VAE for Data-adaptive Noise Filtering
arxiv_id: '2405.19757'
source_url: https://arxiv.org/abs/2405.19757
tags:
- samples
- latent
- oversampling
- class
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the problem of improving synthetic minority
  oversampling technique (SMOTE) for imbalanced classification. The core method idea
  is to enhance SMOTE by fusing it with a conditional variational autoencoder (VAE)
  that learns a customized latent space reflecting class labels and classification
  difficulty.
---

# Improving SMOTE via Fusing Conditional VAE for Data-adaptive Noise Filtering

## Quick Facts
- **arXiv ID**: 2405.19757
- **Source URL**: https://arxiv.org/abs/2405.19757
- **Reference count**: 26
- **Primary result**: SMOTE-CLS achieves highest average ranking (2.08) in AUPRC across 12 benchmark datasets

## Executive Summary
This paper proposes SMOTE-CLS, a method to enhance the Synthetic Minority Oversampling Technique (SMOTE) by integrating it with a conditional variational autoencoder (VAE). The approach systematically identifies and filters out noisy or problematic samples before applying SMOTE, using a customized latent space that encodes both class labels and classification difficulty. The method demonstrates significant improvements in imbalanced classification performance, particularly in terms of AUPRC and AUC metrics, compared to existing SMOTE variants and deep learning-based approaches.

## Method Summary
SMOTE-CLS follows a three-step process: (1) customizing the latent space using a conditional VAE that incorporates class labels and classification difficulty information, (2) filtering out noise samples based on density estimation in the latent space using kernel density estimation (KDE), and (3) applying SMOTE to the filtered minority samples in the original data space. The method uses KNN-based misclassification error to identify "easy" and "hard" samples within each class, trains a conditional VAE with a Gaussian mixture prior that separates these difficulty groups in latent space, and then filters out low-density samples before oversampling with SMOTE.

## Key Results
- SMOTE-CLS achieves the highest average ranking of 2.08 out of 12 benchmark datasets in terms of Area Under the Precision-Recall Curve (AUPRC)
- The method demonstrates superior performance compared to existing SMOTE variants and deep learning-based approaches
- SMOTE-CLS shows robustness to label noise and outperforms other methods in high-dimensional settings based on MNIST experiments

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Customizing latent space with conditional VAE separates minority and majority samples based on both class label and classification difficulty
- Mechanism: The method relabels samples into four groups (easy-major, hard-major, easy-minor, hard-minor) using KNN misclassification error, then trains a conditional VAE with a Gaussian mixture prior that places each group in distinct regions of latent space
- Core assumption: The classification difficulty as measured by KNN error is a meaningful and stable indicator of whether a sample is prototypical or an outlier within its class
- Evidence anchors:
  - [abstract] "Our approach systematically quantifies the density of data points in a low-dimensional latent space using the VAE, simultaneously incorporating information on class labels and classification difficulty."
  - [section] "Inspired by Han et al. (2005) and Sorscher et al. (2022) identifying a sample difficulty, we categorize the samples into two distinct groups again: 'easy sample' and 'hard sample' based on their local information..."
  - [corpus] Weak evidence: No corpus papers directly validate the difficulty-based relabeling strategy

### Mechanism 2
- Claim: Density-based filtering in the customized latent space removes noise samples that would otherwise corrupt SMOTE interpolation
- Mechanism: After training the conditional VAE, KDE is applied separately to easy-minor and hard-minor latent clusters; samples with density below threshold τ are excluded before applying SMOTE
- Core assumption: Noise samples appear as low-density outliers in the latent space, distinct from genuine minority samples
- Evidence anchors:
  - [abstract] "Then, the data points potentially degrading the augmentation are systematically excluded..."
  - [section] "We estimate the density of hard and easy-minority samples by KDE to consider the distributional property of each group... Using threshold τ to filter out noise, we select minor latent variables whose densities are larger than τs in each group."
  - [corpus] Weak evidence: No corpus papers directly validate this two-stage (difficulty-aware + density-based) filtering

### Mechanism 3
- Claim: Applying SMOTE directly in data space on filtered minority samples preserves local data structure better than generating in latent space
- Mechanism: Instead of generating synthetic samples by decoding from latent space (as in CVAE-based methods), SMOTE interpolates between filtered real minority samples in original feature space
- Core assumption: Real minority samples contain richer and more accurate local structure than can be captured by VAE decoding, especially under limited data
- Evidence anchors:
  - [abstract] "Then, the data points potentially degrading the augmentation are systematically excluded, and the neighboring observations are directly augmented on the data space."
  - [section] "Finally, we employ SMOTE directly to the filtered observations, not latent variables."
  - [corpus] Assumption: No direct corpus evidence; inferred from contrast with CVAE-based methods in the paper

## Foundational Learning

- Concept: Variational Autoencoder (VAE) architecture and evidence lower bound (ELBO) optimization
  - Why needed here: The method builds on conditional VAE to learn a structured latent space that encodes class and difficulty information
  - Quick check question: What are the two main terms in the VAE ELBO, and what do they represent?

- Concept: Kernel Density Estimation (KDE) and bandwidth selection
  - Why needed here: KDE is used to estimate the density of minority samples in latent space for noise filtering
  - Quick check question: How does Scott's rule determine the bandwidth in KDE, and why is it important for density estimation?

- Concept: SMOTE algorithm and nearest neighbor interpolation
  - Why needed here: SMOTE is the final oversampling step applied to filtered minority samples
  - Quick check question: How does SMOTE generate synthetic samples, and what parameter controls the amount of oversampling?

## Architecture Onboarding

- Component map: Data preprocessing → KNN-based difficulty relabeling → Conditional VAE training → KDE-based density estimation → Noise filtering → SMOTE oversampling → Downstream classifier training
- Critical path: Difficulty relabeling → Conditional VAE → Density-based filtering → SMOTE
- Design tradeoffs:
  - Using KNN for difficulty labeling is simple but sensitive to parameter K and noise
  - KDE in latent space requires enough samples per cluster; otherwise density estimates are unreliable
  - SMOTE in data space avoids VAE reconstruction errors but may not extrapolate well beyond observed minority samples
- Failure signatures:
  - Poor latent space separation (difficulty groups overlapping) → noise filtering fails
  - Too aggressive density threshold → filtered set too small → SMOTE cannot generate enough samples
  - Too lenient density threshold → noise not removed → degraded classification
- First 3 experiments:
  1. Verify KNN difficulty labeling by visualizing KNN confusion matrix on a small dataset
  2. Train conditional VAE and plot latent space colored by true label and difficulty to check separation
  3. Apply KDE-based filtering with different thresholds and measure impact on minority sample count and downstream AUPRC

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How robust is the filtering algorithm to different choices of prior distributions in the VAE?
- Basis in paper: [explicit] The authors investigate the effects of different prior distributions in Section 4.2 and Appendix 5, but further systematic evaluation across diverse datasets is needed
- Why unresolved: The paper only provides limited visual examples of robustness to prior distributions, without extensive quantitative analysis or testing on a wider range of datasets and scenarios
- What evidence would resolve it: A comprehensive study varying the prior distribution parameters across multiple datasets, measuring the impact on filtering performance and downstream classification accuracy

### Open Question 2
- Question: Can the SMOTE-CLS method be effectively extended to multi-class imbalanced classification problems?
- Basis in paper: [explicit] The authors acknowledge this as a limitation in the conclusion, stating that customizing the latent space based on the confusion matrix is challenging for multi-class scenarios
- Why unresolved: The paper focuses exclusively on binary classification, and the proposed method's extension to multi-class problems requires further development and validation
- What evidence would resolve it: A successful implementation and evaluation of SMOTE-CLS on multi-class imbalanced datasets, demonstrating improved performance compared to existing multi-class oversampling techniques

### Open Question 3
- Question: How does the choice of the classifier fη in the VAE impact the quality of the latent space and the effectiveness of the filtering process?
- Basis in paper: [explicit] The authors compare the use of a multilayer perceptron versus XGBoost as fη in Appendix 5, showing improved performance with XGBoost
- Why unresolved: While the comparison between MLP and XGBoost provides some insight, a more comprehensive study exploring various classifier choices and their impact on different types of data (e.g., tabular, image, text) is needed
- What evidence would resolve it: A systematic evaluation of SMOTE-CLS using different classifiers for fη across multiple datasets and data types, measuring the impact on latent space quality, filtering performance, and downstream classification accuracy

## Limitations

- The method's performance heavily depends on the quality of KNN-based difficulty labeling, which is sensitive to parameter K and dataset characteristics
- Density-based filtering assumes noise samples are low-density outliers in latent space, but this may not hold for all noise types or when minority class samples are too sparse
- The approach requires careful hyperparameter tuning (VAE architecture, KDE bandwidth, density threshold), with no systematic sensitivity analysis provided

## Confidence

- **High**: SMOTE-CLS improves AUPRC/AUC metrics compared to baseline SMOTE on tested datasets (empirically verified)
- **Medium**: The three-step process (difficulty labeling → VAE → density filtering) logically connects to improved performance (mechanistically sound but limited empirical validation)
- **Low**: Claims about robustness to label noise and superiority in high-dimensional settings (based on single experiment)

## Next Checks

1. Conduct ablation studies to isolate the contribution of each component (difficulty labeling, VAE, density filtering) to overall performance
2. Test the method on additional high-dimensional datasets to validate claims about performance in such settings
3. Evaluate sensitivity to hyperparameter choices (K in KNN, KDE bandwidth, density threshold) through systematic grid search and analysis of variance