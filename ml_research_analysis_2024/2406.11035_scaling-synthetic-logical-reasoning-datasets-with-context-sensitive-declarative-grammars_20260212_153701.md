---
ver: rpa2
title: Scaling Synthetic Logical Reasoning Datasets with Context-Sensitive Declarative
  Grammars
arxiv_id: '2406.11035'
source_url: https://arxiv.org/abs/2406.11035
tags:
- language
- reasoning
- generation
- logical
- natural
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work introduces Unigram, a declarative framework for generating
  synthetic reasoning datasets using context-sensitive grammars. The approach generates
  first-order logic problems by binding two languages (simplified English and TPTP
  theorem-proving language) through flexible rules and semantic constraints.
---

# Scaling Synthetic Logical Reasoning Datasets with Context-Sensitive Declarative Grammars

## Quick Facts
- arXiv ID: 2406.11035
- Source URL: https://arxiv.org/abs/2406.11035
- Authors: Damien Sileo
- Reference count: 12
- Primary result: Achieved state-of-the-art accuracy on FOLIO logic dataset, surpassing GPT-4 by 12%

## Executive Summary
This paper introduces Unigram, a declarative framework for generating synthetic logical reasoning datasets using context-sensitive grammars. The approach generates first-order logic problems by binding two languages (simplified English and TPTP theorem-proving language) through flexible rules and semantic constraints. By leveraging an existing FOL solver instead of proof generation algorithms, the method avoids biases from specific proof traces while enabling multilingual generation. Experiments show that fine-tuning DeBERTa models on the generated dataset achieves state-of-the-art accuracy on the FOLIO human-authored logic dataset, surpassing GPT-4 (with or without external solvers) by 12%.

## Method Summary
The method uses Unigram, a declarative framework that generates synthetic first-order logic reasoning problems by binding English and TPTP languages through context-sensitive rules. Unlike previous approaches that generate problems via proof-tree algorithms, Unigram uses an existing FOL solver (Vampire) to validate satisfiability and determine entailment/contradiction/neutral labels. The framework includes enhanced logical modeling with explicit finite/open domains, comprehensive quantifiers, and improved predicate verbalization. Generated problems are structured as PREMISE, HYPOTHESIS pairs for natural language inference tasks, and DeBERTa models are fine-tuned on the synthetic data for downstream evaluation.

## Key Results
- Fine-tuned DeBERTa models on Unigram-generated data achieved SOTA accuracy on FOLIO human-authored logic dataset
- Outperformed GPT-4 with external solvers by 12% absolute accuracy
- Demonstrated effectiveness on multiple downstream tasks including WANLI, ConTRoL, and Fragments

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Declarative generation using an existing FOL solver avoids bias from specific proof traces
- Mechanism: Instead of generating problems by simulating proof steps (forward inference), the framework directly generates logical formulas and uses a solver to determine entailment/contradiction/neutral relationships
- Core assumption: Solver-based validation provides cleaner logical relationships than proof-trace-based generation
- Evidence anchors:
  - [abstract] "Previous work used domain-specific proof generation algorithms, which biases reasoning toward specific proof traces"
  - [section 3.1] "Previous NLI-style FOL reasoning datasets... generate examples using proof generators that are based on the axioms of FOL. This requires domain-specific generation code and introduces unwanted complexity"
  - [corpus] Weak evidence - only 5 corpus papers found, none directly addressing proof trace bias

### Mechanism 2
- Claim: Context-sensitive rules binding multiple languages enable richer linguistic variety and reasoning patterns
- Mechanism: Unigram rules specify type signatures and surface form realizers for both logical code (TPTP) and English, with constraints to ensure validity
- Core assumption: Multimodal binding through functions (not just context-free grammars) allows more expressive problem generation
- Evidence anchors:
  - [section 3.2] "We propose Unigram, a simpler, more generic method to generate problems with multilingual grammars where rules bind multiple surface form realization templates"
  - [section 4] "We use Unigram to enrich FOL problem generation while also avoiding ambiguity"
  - [corpus] Weak evidence - corpus neighbors don't directly address context-sensitive multilingual generation

### Mechanism 3
- Claim: Explicit finite/open domain modeling with equality handling enables more realistic reasoning scenarios
- Mechanism: The framework introduces explicit domain quantifiers ("everyone in the room") and handles FOL with equality, enabling induction problems
- Core assumption: Realistic domain modeling improves transfer to human-authored problems
- Evidence anchors:
  - [section 4] "We explicitly mention the domain when using the quantifiers... We can then quantify over the room (everyone in the room) or anywhere (everyone anywhere)"
  - [section 5.2] "FOLIO (Han et al., 2022) contains human-written FOL problems"
  - [corpus] No direct evidence - corpus doesn't address domain modeling in synthetic datasets

## Foundational Learning

- First-order logic with equality
  - Why needed here: The framework explicitly handles FOL with equality for domain modeling and predicate relationships
  - Quick check question: Can you explain the difference between open and finite domains in FOL and why equality matters for domain quantification?

- Context-sensitive grammars and formal language theory
  - Why needed here: Unigram uses context-sensitive rules (not just context-free) to bind multiple languages and apply constraints
  - Quick check question: What's the key difference between context-free and context-sensitive grammars, and why does this matter for generating valid FOL problems?

- Natural language inference framework
  - Why needed here: Problems are structured as PREMISE, HYPOTHESIS pairs with entailment/contradiction/neutral labels
  - Quick check question: How does the NLI framework map to logical relationships in FOL, and what's the difference between neutral and non-entailment examples?

## Architecture Onboarding

- Component map: Unigram Rule system -> Generation algorithm -> FOL Solver (Vampire) -> NLI labeling -> DeBERTa NLI model training
- Critical path: Rule definition → Problem generation → Solver validation → NLI labeling → Model training
- Design tradeoffs:
  - Declarative vs. proof-tree generation: Simpler implementation but relies on solver performance
  - Context-sensitivity vs. maintainability: More expressive but harder to debug
  - Domain modeling vs. naturalness: More realistic but potentially less natural language
- Failure signatures:
  - Low diversity in generated problems (constraint issues)
  - Solver timeouts or incorrect labels (solver reliability)
  - Poor transfer to human-authored datasets (domain mismatch)
- First 3 experiments:
  1. Generate 100 problems with basic Unigram rules and verify solver labels match manual inspection
  2. Compare diversity metrics (predicate variety, sentence structure) between Unigram and LogicNLI generation
  3. Fine-tune a small DeBERTa model on 1k generated examples and test on FOLIO validation set

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the declarative generation approach scale to more complex logical systems beyond first-order logic with equality?
- Basis in paper: [explicit] The paper mentions future work on extending Unigram to planning, constraint satisfaction, and modal logic, suggesting current limitations to FOL
- Why unresolved: The paper only demonstrates Unigram's effectiveness on first-order logic problems and does not provide evidence for its performance on more complex logical systems
- What evidence would resolve it: Experiments showing successful generation of datasets for planning problems, constraint satisfaction problems, or modal logic formulas using Unigram would demonstrate scalability to more complex logical systems

### Open Question 2
- Question: How does the performance of Unigram-generated datasets compare to human-authored datasets in terms of capturing nuanced logical reasoning?
- Basis in paper: [explicit] The paper shows that Unigram-FOL outperforms previous synthetic datasets on the FOLIO human-authored logic dataset, but does not directly compare the quality of generated vs. human-authored problems
- Why unresolved: While the paper demonstrates improved performance on downstream tasks, it does not investigate whether the generated problems capture the same level of nuanced logical reasoning as human-authored problems
- What evidence would resolve it: A detailed analysis comparing the logical complexity, diversity of reasoning patterns, and real-world applicability of Unigram-generated problems versus human-authored problems would provide insights into the quality of the generated datasets

### Open Question 3
- Question: What is the impact of using different theorem provers on the quality and diversity of the generated datasets?
- Basis in paper: [explicit] The paper mentions compatibility with multiple theorem provers (Vampire, Z3, Prover9) but only uses Vampire for proof annotations in the experiments
- Why unresolved: The paper does not explore how the choice of theorem prover affects the generation process, the types of problems generated, or the resulting dataset quality
- What evidence would resolve it: Experiments generating datasets using different theorem provers and comparing the resulting problem diversity, proof complexity, and downstream task performance would reveal the impact of theorem prover choice on dataset quality

## Limitations

- The framework's reliance on an external FOL solver introduces potential bottlenecks and solver-specific biases that weren't thoroughly investigated
- Context-sensitive rules may make grammar maintenance and debugging more complex compared to simpler generation approaches
- Limited evidence on whether the generated problems capture the same level of nuanced logical reasoning as human-authored problems

## Confidence

- High confidence: The declarative framework implementation and basic generation pipeline are well-specified and reproducible
- Medium confidence: Claims about avoiding proof-trace bias and achieving SOTA on FOLIO, as these depend on solver behavior and lack direct comparative analysis
- Low confidence: Claims about specific linguistic variety improvements from context-sensitive rules, as diversity metrics aren't systematically reported

## Next Checks

1. Generate parallel datasets using both Unigram and a simple proof-tree generator, then measure logical consistency rates and diversity metrics to directly test the bias-avoidance claim
2. Conduct ablation studies removing equality handling and explicit domain modeling to quantify their contribution to FOLIO performance improvements
3. Test model generalization by creating a held-out subset of FOLIO with unseen predicate patterns and evaluating transfer from synthetic data with varying domain modeling complexity