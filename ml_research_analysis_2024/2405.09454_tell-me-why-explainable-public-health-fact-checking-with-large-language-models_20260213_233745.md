---
ver: rpa2
title: 'Tell Me Why: Explainable Public Health Fact-Checking with Large Language Models'
arxiv_id: '2405.09454'
source_url: https://arxiv.org/abs/2405.09454
tags:
- claim
- task
- explanation
- amber
- few-shot
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a comprehensive evaluation of explainable fact-checking
  using large language models (LLMs) for verifying public health claims and generating
  explanations. The study examines zero/few-shot prompting and parameter-efficient
  fine-tuning across open and closed-source models for both isolated and joint veracity
  prediction and explanation generation tasks.
---

# Tell Me Why: Explainable Public Health Fact-Checking with Large Language Models

## Quick Facts
- arXiv ID: 2405.09454
- Source URL: https://arxiv.org/abs/2405.09454
- Reference count: 25
- Key outcome: GPT-4 excels in zero-shot fact-checking; open-source models close the gap with fine-tuning; joint task explanations outperform isolated ones

## Executive Summary
This paper presents a comprehensive evaluation of explainable fact-checking using large language models (LLMs) for verifying public health claims and generating explanations. The study examines zero/few-shot prompting and parameter-efficient fine-tuning across open and closed-source models for both isolated and joint veracity prediction and explanation generation tasks. A dual evaluation approach combining automatic metrics (ROUGE, NLI-based coherence metrics) and human evaluation is employed. Automatic evaluation shows GPT-4 excels in zero-shot scenarios, while open-source models narrow the gap in few-shot and fine-tuning settings, with Mistral-7B achieving the best macro-F1 of 72.0. Human evaluation reveals GPT-4 performs best overall for explanation quality, though few-shot Llama-70B and Vicuna-13B show strong performance.

## Method Summary
The study evaluates LLMs on the PUBHEALTH dataset (12,243 claims) using zero-shot, few-shot, and parameter-efficient fine-tuning approaches. Three tasks are defined: veracity prediction (4-way classification), explanation generation, and a joint task combining both. Open-source models (Llama-70B, Vicuna-13B, Mistral-7B) are compared against GPT-4. Contexts are summarized to 350 words before processing. Evaluation uses automatic metrics (accuracy, F1, ROUGE scores) and human evaluation across 7 criteria including claim repetition, internal consistency, and external consistency. The methodology systematically compares isolated versus joint task performance and examines how model size and training approach affect both prediction accuracy and explanation quality.

## Key Results
- GPT-4 outperforms all models in zero-shot settings for both veracity prediction and explanation generation
- Open-source models (especially Mistral-7B) achieve competitive performance in few-shot and fine-tuning settings, with Mistral-7B achieving 72.0 macro-F1
- Joint task explanations are consistently rated higher quality than isolated task explanations in human evaluation
- Gold standard explanations in PUBHEALTH dataset show significant weaknesses compared to LLM-generated explanations, particularly on "Unproven" claims

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Zero-shot GPT-4 outperforms open-source models in both veracity prediction and explanation generation
- Mechanism: Large language models trained on diverse web data can perform zero-shot fact-checking through pattern matching and contextual understanding
- Core assumption: Pre-training data includes sufficient public health information and fact-checking patterns
- Evidence anchors:
  - [abstract] "Our automatic evaluation indicates that, within the zero-shot scenario, GPT-4 emerges as the standout performer"
  - [section] "In the zero-shot setting, the closed-source models clearly outperform the open-source models"
  - [corpus] Weak - corpus contains 5 related papers but none directly compare zero-shot performance of GPT-4 vs open models
- Break condition: If pre-training data lacks domain-specific knowledge or contains biased/misinformative patterns that affect judgment

### Mechanism 2
- Claim: Joint task explanations are higher quality than isolated task explanations
- Mechanism: When models must both predict and explain, they engage in deeper reasoning that produces more coherent and realistic explanations
- Core assumption: The cognitive load of joint prediction+explanation forces models to process context more thoroughly
- Evidence anchors:
  - [section] "explanations generated in the context of the joint task tend to be of higher quality than those generated for the explanation task alone"
  - [section] "In the joint task, we solely input the claim and context, prompting the models to predict the veracity label and provide reasons for their prediction"
  - [corpus] Weak - no direct corpus evidence supporting this specific mechanism
- Break condition: If the model can predict correctly without deep understanding, or if explanation generation becomes superficial when combined with prediction

### Mechanism 3
- Claim: Parameter-efficient fine-tuning bridges the performance gap between closed and open-source models
- Mechanism: Fine-tuning adapts open-source models to domain-specific patterns while preserving pre-trained knowledge
- Core assumption: The PUBHEALTH dataset contains representative patterns that can be learned through fine-tuning
- Evidence anchors:
  - [section] "open-source models demonstrate their capacity to not only bridge the performance gap but, in some instances, surpass GPT-4" in few-shot and fine-tuning contexts
  - [section] "This trend persists across both veracity prediction and explanation generation tasks"
  - [corpus] Weak - corpus mentions "JustiLM: Few-shot Justification Generation" but doesn't directly support PEFT effectiveness
- Break condition: If the dataset is too small or noisy for effective fine-tuning, or if catastrophic forgetting occurs

## Foundational Learning

- Concept: Natural Language Inference (NLI) and coherence metrics
  - Why needed here: Used to evaluate explanation quality without relying solely on ROUGE (which struggles with abstractive explanations)
  - Quick check question: What's the difference between Strong Global Coherence and Weak Global Coherence in explanation evaluation?

- Concept: Few-shot and zero-shot prompting techniques
  - Why needed here: The study compares these learning paradigms across multiple models and tasks
  - Quick check question: How does few-shot prompting differ from parameter-efficient fine-tuning in terms of model adaptation?

- Concept: Abstractive vs extractive summarization
  - Why needed here: The study uses abstractive methods for explanation generation, which is more challenging to evaluate
  - Quick check question: Why might abstractive explanations be more difficult to evaluate automatically than extractive ones?

## Architecture Onboarding

- Component map: Context summarization → Claim + context → Model (isolated veracity/prediction/justification or joint prediction+justification) → Automatic metrics (accuracy/F1/ROUGE) and Human evaluation (7 criteria)

- Critical path: For end-to-end fact-checking, the sequence is: context summarization → claim + context → model prediction/justification. The most computationally intensive step is typically context summarization, especially for long articles.

- Design tradeoffs: Zero-shot offers broad generalization but lower performance; fine-tuning improves performance but requires computational resources and risks overfitting; joint task provides better explanations but may reduce prediction accuracy; human evaluation is thorough but expensive and subjective.

- Failure signatures: Claim repetition and internal repetition indicate models are not generating genuine explanations; extra information suggests training data leakage; missing information indicates insufficient context processing; poor coherence scores reveal explanation quality issues.

- First 3 experiments:
  1. Run zero-shot GPT-4 on a small sample to establish baseline performance and identify patterns in errors
  2. Test different few-shot configurations (1-12 shots) on validation set to find optimal shot count for each model
  3. Compare ROUGE vs NLI-based coherence metrics on same sample to understand metric reliability differences

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What specific aspects of gold standard explanations in the PUBHEALTH dataset contribute to their lower quality scores compared to LLM-generated explanations?
- Basis in paper: [explicit] The paper states that gold explanations exhibit significant weaknesses compared to LLM-generated explanations, with a 50% MAE for the Extra Information criterion versus 38% for the worst performing models.
- Why unresolved: The paper identifies this issue but doesn't analyze the specific linguistic or structural characteristics that make gold explanations problematic.
- What evidence would resolve it: A detailed comparative linguistic analysis of gold versus LLM-generated explanations, identifying patterns such as redundancy, lack of coherence, or missing contextual information.

### Open Question 2
- Question: How do different NLI models' performance on the PUBHEALTH dataset relate to their ability to capture explanation quality nuances?
- Basis in paper: [explicit] The paper uses four different NLI models (Decomposable Attention, RoBERTa-SNLI, RoBERTa-MNLI, and Roberta-L-(S+M+A)NLI-FEVER) and reports varying coherence metric results.
- Why unresolved: The paper doesn't analyze why certain NLI models perform better than others for explanation evaluation or what aspects of explanation quality each model captures best.
- What evidence would resolve it: Correlation analysis between NLI model performance and specific explanation quality dimensions (coherence, relevance, completeness) across multiple datasets.

### Open Question 3
- Question: What causes the observed difficulty in generating explanations for "Unproven" claims, and how can this be systematically addressed?
- Basis in paper: [explicit] The paper notes that some models generate unrelated text when handling "Unproven" claims, and the gold explanations also score poorly on this class.
- Why unresolved: The paper identifies this pattern but doesn't investigate whether it's due to dataset bias, model limitations, or inherent difficulty in explaining uncertainty.
- What evidence would resolve it: Comparative analysis of model behavior on "Unproven" claims versus other classes, identifying whether the issue stems from insufficient training examples, ambiguous context, or model architecture limitations.

## Limitations
- Results are primarily limited to the PUBHEALTH dataset, which contains health-related claims from PolitiFact
- Human evaluation involves subjective judgments that may not fully capture explanation quality across diverse contexts
- The study uses 350-word context summaries, which may affect model performance compared to full-length articles

## Confidence
- **High Confidence**: GPT-4's superior performance in zero-shot settings; effectiveness of parameter-efficient fine-tuning; identification of weaknesses in gold standard explanations
- **Medium Confidence**: Joint task explanations being consistently higher quality; specific model rankings across all tasks and settings; effectiveness of automatic coherence metrics as proxies for human evaluation

## Next Checks
1. Cross-domain validation: Test the same models and approaches on fact-checking datasets from different domains (e.g., political claims, scientific misinformation) to assess generalizability beyond health claims.

2. Long-context evaluation: Evaluate model performance with full-length articles rather than 350-word summaries to determine if context truncation affects explanation quality and veracity prediction accuracy.

3. Multi-round fact-checking: Implement a system where initial model predictions are used to generate follow-up questions or additional context requests, then re-evaluate the models to assess iterative improvement capabilities.