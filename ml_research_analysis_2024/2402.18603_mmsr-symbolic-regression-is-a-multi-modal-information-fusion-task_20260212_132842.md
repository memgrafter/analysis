---
ver: rpa2
title: 'MMSR: Symbolic Regression is a Multi-Modal Information Fusion Task'
arxiv_id: '2402.18603'
source_url: https://arxiv.org/abs/2402.18603
tags:
- data
- expression
- symbolic
- learning
- feature
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: MMSR treats symbolic regression as a multi-modal problem, using
  data and expression skeletons as different modalities. It employs SetTransformer
  for data encoding, a skeleton encoder for expression encoding, and a decoder for
  feature fusion and expression generation.
---

# MMSR: Symbolic Regression is a Multi-Modal Information Fusion Task

## Quick Facts
- arXiv ID: 2402.18603
- Source URL: https://arxiv.org/abs/2402.18603
- Authors: Yanjie Li; Jingyi Liu; Weijun Li; Lina Yu; Min Wu; Wenqiang Li; Meilan Hao; Su Wei; Yusong Deng
- Reference count: 40
- Primary result: Achieves state-of-the-art results on multiple datasets including SRBench with R² scores above 0.98 and lower expression complexity than baselines.

## Executive Summary
MMSR treats symbolic regression as a multi-modal problem, using data points and expression skeletons as different modalities. The model employs SetTransformer for data encoding, a skeleton encoder for expression encoding, and a decoder for feature fusion and expression generation. Contrastive learning aligns data and skeleton features to improve feature fusion. The model is trained with cross-entropy loss, MSE loss for constants, and contrastive loss, achieving superior performance on multiple benchmarks.

## Method Summary
MMSR treats symbolic regression as a pure multi-modal problem where data points and expression skeletons are different modalities. The model uses SetTransformer to encode unordered data points, a skeleton encoder for expression tokens, and a decoder with cross-attention for feature fusion. Contrastive learning is introduced during training to align the modalities by bringing matching data-skeleton pairs close in embedding space while pushing non-matching pairs apart. The model is trained jointly with cross-entropy loss for skeleton tokens, MSE loss for constant values, and contrastive loss for alignment.

## Key Results
- Achieves state-of-the-art results on SRBench with R² scores above 0.98
- Demonstrates lower expression complexity than baseline methods
- Shows that joint training of contrastive loss with other losses outperforms separate training strategies

## Why This Works (Mechanism)

### Mechanism 1
- Claim: MMSR achieves state-of-the-art results by treating symbolic regression as a multi-modal problem with aligned data and skeleton features.
- Mechanism: The model encodes data points using SetTransformer and expression skeletons using a skeleton encoder, then fuses them via cross-attention. Contrastive learning aligns the modalities so matching features are close in embedding space.
- Core assumption: Data and skeleton representations can be meaningfully embedded in a shared feature space and that proximity in this space correlates with correctness.
- Evidence anchors:
  - [abstract]: "treat the SR problem as a pure multi-modal problem, and contrastive learning is also introduced in the training process for modal alignment to facilitate later modal feature fusion."
  - [section 3.2.4]: "we align the features of the data obtained by the data point feature extraction extractor with the skeleton features obtained by the skeleton feature extractor."
  - [corpus]: Weak support—no corpus neighbors discuss contrastive learning for symbolic regression; the closest is "FDRMFL: Multi-modal Federated Feature Extraction Model Based on Information Maximization and Contrastive Learning," but that is for regression, not SR.
- Break condition: If the embedding spaces are incompatible or contrastive alignment does not correlate with task success, the alignment fails.

### Mechanism 2
- Claim: Joint training of contrastive loss with cross-entropy and MSE loss leads to better performance than separate training.
- Mechanism: Contrastive loss is optimized simultaneously with reconstruction losses, allowing feature extractors and fusion modules to adapt to each other during training.
- Core assumption: The feature extraction and feature fusion modules are interdependent and benefit from simultaneous adaptation.
- Evidence anchors:
  - [section 3.3]: "we adopt the strategy of training contrastive learning loss and other losses at the same time, which only needs one-step training, instead of training contrastive learning loss first and then training other losses."
  - [section 4.4.5]: "From the diagram above, we can intuitively see the difference between the Joint training strategy and the Separate training strategy... The Joint training strategy trains the three modules together, while the Separate training strategy trains the two encoders and then the decoder."
  - [corpus]: No corpus neighbors explicitly compare joint vs. separate training for multi-modal fusion.
- Break condition: If the gradients from different losses interfere destructively or the modules become overfit to each other's noise.

### Mechanism 3
- Claim: Direct constant encoding in the loss function improves accuracy compared to using placeholder tokens.
- Mechanism: Constants are encoded as (mantissa, exponent) pairs and included in the MSE loss, enabling the model to predict their values during training.
- Core assumption: The model can learn to predict precise numerical values during training rather than just placeholders.
- Evidence anchors:
  - [section 3.2.2]: "to solve the problem that expressions may exhibit high token-level similarities but are suboptimal for equation-specific objectives such as fitting accuracy, which is caused by only using the cross-entropy loss... we encode the expression symbol sequence and the constant sequence separately."
  - [section 4.4.4]: "Introduction of constant encoding facilitates a one-to-one mapping between data instances and their corresponding expressions, effectively reducing the ambiguity of multiple data sets correlating to the same symbolic sequence."
  - [corpus]: No corpus neighbors address constant encoding in symbolic regression transformers.
- Break condition: If the constant prediction task is too difficult or the numerical instability in loss computation dominates training.

## Foundational Learning

- Concept: Multi-modal representation learning with contrastive alignment.
  - Why needed here: Symbolic regression inputs (data points) and outputs (expression skeletons) are fundamentally different modalities; without alignment, the decoder cannot fuse them effectively.
  - Quick check question: What loss function ensures that matched data-skeleton pairs are close in embedding space while mismatched pairs are far apart?

- Concept: Set-based permutation invariance for unordered data.
  - Why needed here: Input data points have no inherent order, so the encoder must be permutation invariant to avoid spurious correlations.
  - Quick check question: Which architecture in MMSR guarantees permutation invariance for the data encoder?

- Concept: Joint multi-loss optimization with balanced weighting.
  - Why needed here: Cross-entropy, MSE for constants, and contrastive loss have different scales and gradients; balancing them is crucial for stable training.
  - Quick check question: How do the hyperparameters λ1, λ2, λ3 in MMSR control the relative influence of each loss component?

## Architecture Onboarding

- Component map: Data Encoder (SetTransformer) -> Skeleton Encoder (Transformer) -> Decoder (Transformer with cross-attention) -> Expression Generation
- Critical path:
  1. Encode data → SetTransformer → F_data
  2. Encode skeleton → SkeletonEncoder → F_skel
  3. Apply contrastive loss to align F_data and F_skel
  4. Fuse via cross-attention in decoder → fused features
  5. Generate expression tokens and constants autoregressively
  6. Compute all three losses and backpropagate jointly
- Design tradeoffs:
  - Joint vs. separate training: Joint allows better feature fusion but increases training instability risk
  - Constant encoding vs. placeholder: Direct encoding improves accuracy but requires careful numerical scaling
  - SetTransformer vs. other encoders: SetTransformer ensures permutation invariance but may be slower than simpler pooling
- Failure signatures:
  - High contrastive loss but low reconstruction loss: Misalignment not improving prediction
  - Low contrastive loss but poor R²: Alignment achieved but irrelevant to task
  - NaN or exploding gradients in MSE loss: Numerical instability in constant prediction
- First 3 experiments:
  1. Train with only cross-entropy loss; measure skeleton accuracy and R²; expect poor constant fitting
  2. Train with cross-entropy + MSE loss but no contrastive loss; measure alignment and performance; expect lower accuracy than full model
  3. Train with full loss; compare to ablations; confirm joint training and alignment improve both skeleton and constant prediction

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does MMSR perform on noisy real-world datasets compared to synthetic ones?
- Basis in paper: [inferred] The paper mentions "anti-noise performance is poor" but doesn't provide detailed experiments on real-world noisy data.
- Why unresolved: The paper only tests on synthetic data and mentions noise issues without addressing them experimentally.
- What evidence would resolve it: Testing MMSR on real-world noisy datasets and comparing performance with other symbolic regression methods.

### Open Question 2
- Question: Can MMSR handle variable input dimensionality or is it limited to fixed input sizes?
- Basis in paper: [inferred] The paper mentions needing to specify maximum number of support variables in advance and can't expand once training is complete.
- Why unresolved: The paper doesn't explore dynamic input handling or adaptive architectures for variable input dimensions.
- What evidence would resolve it: Demonstrating MMSR's performance on datasets with varying input dimensions or developing an adaptive architecture.

### Open Question 3
- Question: How does MMSR's performance scale with extremely large datasets (e.g., 100M+ samples)?
- Basis in paper: [explicit] The paper shows improvement up to 10M samples but doesn't explore scaling beyond this point.
- Why unresolved: The paper only tests up to 10M samples and mentions performance continues to improve but doesn't provide data on larger scales.
- What evidence would resolve it: Training and evaluating MMSR on datasets of 100M+ samples to determine performance saturation point.

## Limitations
- The paper lacks detailed ablation studies on the exact architectural specifications of the skeleton encoder
- Performance on real-world noisy datasets is not thoroughly evaluated despite mentioning anti-noise performance issues
- The model requires specifying maximum input dimensionality in advance and cannot handle variable input sizes

## Confidence
- **High Confidence**: MMSR achieves state-of-the-art results on multiple datasets with R² scores above 0.98. This is directly supported by experimental results.
- **Medium Confidence**: Joint training strategy outperforms separate training for multi-modal feature fusion. While ablation studies support this, the mechanism by which joint training enables better feature fusion is not fully explained.
- **Medium Confidence**: Contrastive learning alignment improves feature fusion and downstream performance. The correlation between alignment quality and task success is inferred but not directly validated.

## Next Checks
1. Measure the cosine similarity between aligned data-skeleton pairs versus mismatched pairs during training to verify contrastive alignment correlates with task success
2. Systematically vary the skeleton encoder architecture (number of layers, attention heads) while keeping other components constant to test architectural sensitivity
3. Test MMSR on expression skeletons not present in the training distribution to measure generalization to novel expression patterns