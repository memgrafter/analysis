---
ver: rpa2
title: Scaling Laws for Multilingual Language Models
arxiv_id: '2410.12883'
source_url: https://arxiv.org/abs/2410.12883
tags:
- language
- loss
- sampling
- ratios
- family
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a novel scaling law for multilingual language
  models by introducing the hypothesis that the test cross-entropy loss for each language
  family is determined solely by its own sampling ratio, independent of other languages
  in the training mixture. This insight simplifies the complexity of multilingual
  scaling and makes the analysis scalable to an arbitrary number of languages.
---

# Scaling Laws for Multilingual Language Models

## Quick Facts
- arXiv ID: 2410.12883
- Source URL: https://arxiv.org/abs/2410.12883
- Reference count: 40
- Key outcome: A novel scaling law for multilingual language models that predicts optimal sampling ratios across model scales, validated on 23 languages across 5 families

## Executive Summary
This paper proposes a novel scaling law for multilingual language models by introducing the hypothesis that test cross-entropy loss for each language family depends solely on its own sampling ratio, independent of other languages in the training mixture. This simplification enables the derivation of a power-law relationship linking performance with dataset size, model size, and sampling ratios. The authors show that optimal sampling ratios derived from small models (85M parameters) generalize effectively to models several orders of magnitude larger (1.2B parameters), offering a resource-efficient approach for multilingual LM training at scale.

## Method Summary
The authors train decoder-only Transformer models ranging from 85M to 1.2B parameters on 23 languages across 5 language families using CommonCrawl data and an in-house English dataset. They vary sampling ratios for each family (Germanic, Romance, Slavic, Indic, Sino-Tibetan) and measure family-level cross-entropy losses. Using a power-law formulation Li(pi) = L⋆i · p−γi, they fit decay rates γi and derive optimal sampling ratios that minimize total loss. The key insight is that γi remains invariant to model size N and dataset size D, allowing optimal ratios to be computed from small models and applied to larger scales.

## Key Results
- Test cross-entropy loss for each language family is determined solely by its own sampling ratio, independent of other languages
- Power-law relationship Li(pi) = L⋆i · p−γi captures the decay of loss with sampling ratio, with γi invariant to model and dataset scales
- Optimal sampling ratios derived from 85M parameter models generalize effectively to 1.2B parameter models

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Test cross-entropy loss for each language family is determined solely by its own sampling ratio, independent of other languages in the training mixture.
- Mechanism: The authors hypothesize that minimal cross-family transfer occurs during multilingual pretraining, allowing each language family's performance to be modeled as a function of its own sampling ratio alone.
- Core assumption: Language families are sufficiently distinct linguistically that most knowledge transfer happens within families rather than across families.
- Evidence anchors:
  - [abstract]: "we shift the focus from individual languages to language families. We introduce and validate a hypothesis that the test cross-entropy loss for each language family is determined solely by its own sampling ratio, independent of other languages in the mixture."
  - [section 3.2]: "We use a controlled experiment to test our hypothesis... In Figure 2 (left), we observe that with a fixed sampling ratio, the Romance loss does not noticeably change regardless of the combination ratios of Germanic and Slavic data."
  - [corpus]: Found 25 related papers (using 8). Average neighbor FMR=0.532, average citations=0.0. Evidence for cross-lingual transfer patterns is mixed in related work, with some studies supporting family-level independence while others highlight transfer effects.
- Break condition: If significant cross-family transfer occurs (e.g., between closely related language families), the independence assumption breaks down and the scaling law loses predictive accuracy.

### Mechanism 2
- Claim: The power-law relationship between loss and sampling ratio follows the form Li(pi) = L⋆i · p−γi, where γi remains invariant to model size N and dataset size D.
- Mechanism: The authors extend existing monolingual scaling laws to multilingual settings by treating sampling ratio as analogous to relative dataset size, capturing diminishing returns in performance improvement.
- Core assumption: The exponent γi characterizing the decay rate of loss with sampling ratio is constant across different model and dataset scales.
- Evidence anchors:
  - [section 3.3]: "The choice of power-law formulation follows naturally from previous studies on scaling laws, which demonstrate that the loss exhibits a power-law behavior with respect to dataset size (Kaplan et al., 2020)."
  - [section 3.4]: "In Figure 4, we observe that given a fixed D, γi(N, D) does not change as N varies, confirming its independence from N. Similarly, the slope remains constant when N is fixed and D changes, verifying that γi(N, D) is also independent of D."
  - [corpus]: Weak evidence - related work focuses on scaling laws but doesn't extensively validate invariance of decay rates across scales.
- Break condition: If γi varies significantly with model size or dataset size, the simplified power-law form becomes inaccurate and predictions degrade.

### Mechanism 3
- Claim: Optimal sampling ratios derived from small models (85M parameters) generalize effectively to models several orders of magnitude larger (1.2B parameters).
- Mechanism: The invariance of decay rates γi across model scales allows optimal sampling ratios to be computed from small, resource-efficient models and applied to large-scale training.
- Core assumption: The relative importance of language families remains consistent across model scales, so optimal ratios computed at small scale remain near-optimal at large scale.
- Evidence anchors:
  - [abstract]: "Our experiments show that the optimal sampling ratios derived from small models (85M parameters) generalize effectively to models that are several orders of magnitude larger (1.2B parameters)."
  - [section 4.2]: "From Table 5, the sampling ratios from the 85M model continue to outperform all other baselines. Furthermore, the total loss is comparable to the one achieved by the optimal sampling ratios fitted specifically for the 1.2B model."
  - [corpus]: No direct evidence found in corpus - this appears to be a novel empirical finding specific to this paper.
- Break condition: If the relative importance of language families changes dramatically with scale, or if very large models exhibit different transfer patterns, the generalization may fail.

## Foundational Learning

- Concept: Scaling laws for neural language models
  - Why needed here: The paper builds on established scaling laws (Kaplan et al., 2020; Hoffmann et al., 2022) to extend them to multilingual settings.
  - Quick check question: What is the general form of the Chinchilla scaling law for monolingual models?

- Concept: Cross-lingual transfer in multilingual models
  - Why needed here: Understanding how knowledge transfers between languages is crucial for the hypothesis about family-level independence.
  - Quick check question: What factors typically influence the amount of cross-lingual transfer between two languages?

- Concept: Power-law relationships and diminishing returns
  - Why needed here: The paper uses power-law formulations to capture how increasing data for a language family yields diminishing performance improvements.
  - Quick check question: How does the exponent in a power-law relationship relate to the concept of diminishing returns?

## Architecture Onboarding

- Component map: Data preparation (23 languages, 5 families) -> Model training (85M-1.2B parameters) -> Loss measurement (family-level cross-entropy) -> Power-law fitting (Li(pi) = L⋆i · p−γi) -> Optimal ratio derivation -> Generalization validation
- Critical path: Train models with different sampling ratios → measure family-level losses → fit power-law relationships → derive optimal ratios → validate generalization to larger models
- Design tradeoffs: The authors choose language families over individual languages to balance granularity with tractability, accepting some loss of detail for computational feasibility
- Failure signatures: If cross-family transfer is significant, losses will vary with joint training mixtures rather than being family-independent; if γi varies with scale, optimal ratios won't generalize; if tokenizer vocabularies are imbalanced, raw losses may misrepresent true performance
- First 3 experiments:
  1. Train a small multilingual model (85M) with various family sampling ratios and measure family-level cross-entropy losses.
  2. Validate the independence hypothesis by fixing one family's ratio while varying others and checking if that family's loss remains stable.
  3. Fit the power-law relationship Li(pi) = L⋆i · p−γi for each family and verify the invariance of γi across different model sizes.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the multilingual scaling law perform for language families with very low-resource languages or highly imbalanced data within the family?
- Basis in paper: [inferred] The paper notes that language families are less affected by data limitations compared to individual languages, but does not explicitly test this scenario.
- Why unresolved: The experimental setup uses a fixed number of languages per family, and does not explore the effects of highly imbalanced data or extremely low-resource languages within families.
- What evidence would resolve it: Testing the scaling law on language families with significant data imbalances or adding very low-resource languages to existing families would show how robust the law is under these conditions.

### Open Question 2
- Question: Can the proposed scaling law be extended to cross-lingual tasks beyond language modeling, such as cross-lingual transfer learning or multilingual machine translation?
- Basis in paper: [explicit] The paper focuses on decoder-only language models for general-purpose tasks and does not explore cross-lingual transfer learning or NMT specifically.
- Why unresolved: While the paper establishes a foundation for multilingual scaling laws, it does not investigate whether the same principles apply to other cross-lingual tasks or model architectures.
- What evidence would resolve it: Applying the scaling law to encoder-decoder models or testing it on cross-lingual transfer learning benchmarks would determine its applicability beyond decoder-only LMs.

### Open Question 3
- Question: How sensitive are the optimal sampling ratios to changes in the underlying data distribution, such as shifts in language prevalence or quality of data across families?
- Basis in paper: [inferred] The paper assumes fixed data distributions and does not explore how changes in data quality or language prevalence affect the derived optimal ratios.
- Why unresolved: The experimental setup uses static data distributions, and does not account for dynamic changes in language prevalence or data quality over time.
- What evidence would resolve it: Recomputing optimal ratios with varying data distributions or simulating shifts in language prevalence would show how robust the ratios are to changes in the underlying data.

## Limitations

- The independence hypothesis may not hold for closely related language families with significant cross-family transfer
- The scaling law is validated on only 23 languages across 5 families, limiting generalizability
- The paper focuses on cross-entropy loss rather than downstream task performance across language families

## Confidence

- **High Confidence**: The power-law formulation Li(pi) = L⋆i · p−γi and its empirical validation across different model sizes and dataset sizes
- **Medium Confidence**: The independence hypothesis that test cross-entropy loss for each language family depends only on its own sampling ratio
- **Medium Confidence**: The generalization of optimal sampling ratios from small (85M) to large (1.2B) models

## Next Checks

1. Test the independence hypothesis with more linguistically related language families (e.g., Romance languages together) to identify boundary conditions where cross-family transfer becomes significant.

2. Validate the scaling law predictions on a held-out set of languages not included in the original 23, to assess generalizability to unseen language families.

3. Evaluate downstream task performance (rather than just cross-entropy loss) when using optimal sampling ratios derived from the scaling law, to confirm that perplexity-based optimization translates to practical performance gains.