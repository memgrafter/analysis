---
ver: rpa2
title: Causality extraction from medical text using Large Language Models (LLMs)
arxiv_id: '2407.10020'
source_url: https://arxiv.org/abs/2407.10020
tags:
- extraction
- data
- medical
- language
- causality
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study explores the use of Large Language Models (LLMs) for
  extracting causal relations from medical texts, specifically Clinical Practice Guidelines
  (CPGs) for gestational diabetes. The research compares the performance of BERT variants
  (BioBERT, DistilBERT, BERT) with LLMs (GPT-4 and LLAMA2) for this task.
---

# Causality extraction from medical text using Large Language Models (LLMs)

## Quick Facts
- arXiv ID: 2407.10020
- Source URL: https://arxiv.org/abs/2407.10020
- Reference count: 40
- BioBERT outperforms LLMs for causality extraction from medical texts with F1-score of 0.72

## Executive Summary
This study compares the effectiveness of Large Language Models (LLMs) versus traditional fine-tuned models for extracting causal relationships from Clinical Practice Guidelines (CPGs) for gestational diabetes. The research evaluates BioBERT, DistilBERT, BERT, GPT-4, and LLAMA2 on a newly annotated dataset of causal statements. Results show that BioBERT achieves the highest performance with an average F1-score of 0.72, while GPT-4 and LLAMA2 show similar but less consistent performance. The study also releases a new annotated dataset marking the first such resource for this domain.

## Method Summary
The study fine-tuned BERT variants (BioBERT, DistilBERT, BERT) on annotated CPGs for gestational diabetes, training BioBERT for 16 epochs, DistilBERT for 18 epochs, and BERT for 20 epochs. For LLMs, GPT-4 was tested with zero, four, six, eight, ten, and twenty-shot prompting, while LLAMA2 was fine-tuned using HuggingFace autotrain. The models were evaluated using token-level and phrase-level F1-scores, Jaccard similarity, and cosine similarity. A new annotated dataset of 35 cause/effect relationships was created from seven medical society guidelines.

## Key Results
- BioBERT achieved the highest average F1-score of 0.72 for causality extraction
- GPT-4 and LLAMA2 showed similar performance but with less consistency
- LLAMA2 failed to generate predictions for 20-35% of data
- Performance of GPT-4 did not improve beyond 10-shot prompting
- The study releases a new annotated dataset of causal statements from CPGs

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Fine-tuning BioBERT on clinical practice guidelines yields higher F1-score than GPT-4 or LLAMA2 for causality extraction
- Mechanism: Domain-specific pre-training on biomedical corpora allows BioBERT to capture medical language patterns and causal relationships more effectively than general-purpose LLMs
- Core assumption: The causal statements in gestational diabetes guidelines follow consistent syntactic and semantic patterns that BioBERT has been pre-trained to recognize
- Evidence anchors:
  - [abstract] "BioBERT outperformed other models, including the Large Language Models, with an average F1-score of 0.72"
  - [section] "BioBERT gave us an average F1 score of 0.61, and BERT gave an average F1 score of 0.60"
  - [corpus] Corpus shows average neighbor FMR=0.477, suggesting moderate domain relevance but no strong causality-specific datasets
- Break condition: If the medical text contains novel causal expressions not present in the biomedical training corpus, or if the guidelines use highly variable syntax

### Mechanism 2
- Claim: Prompt size beyond 10 shots does not improve GPT-4 performance on causality extraction
- Mechanism: GPT-4's attention mechanism and context window have sufficient capacity to learn the causality extraction task from 10 examples, and additional examples provide diminishing returns
- Core assumption: The task of identifying cause/effect relationships in medical text is sufficiently simple that 10 examples provide adequate demonstration
- Evidence anchors:
  - [section] "The results indicate that the performance of GPT-4 does not increase with an increase in the prompt size beyond 10"
  - [section] "With GPT-4, we got an average F1 score of 0.39 with four-shot prompting" and "The F1-scores are computed by comparing the gold labels with the predicted labels"
  - [corpus] No corpus evidence available for GPT-4 performance saturation point
- Break condition: If the task complexity increases or the examples need to cover more diverse causal patterns

### Mechanism 3
- Claim: LLAMA2 shows promise but fails to generate predictions for 20-35% of data
- Mechanism: The open-source nature and smaller parameter count of LLAMA2 allows it to learn causal patterns from fine-tuning, but its generation mechanism may have difficulty producing output for certain input patterns
- Core assumption: The fine-tuning process can transfer LLAMA2's general language understanding to the specific task of causal extraction
- Evidence anchors:
  - [abstract] "LLAMA2 shows promise, in that an average F1 score of 76% was obtained on subset for which it made predictions"
  - [section] "LLAMA2 did not generate predictions for 20-35+% of data"
  - [corpus] No corpus evidence available for LLAMA2's generation failures
- Break condition: If the proportion of non-generated predictions increases with dataset size or complexity

## Foundational Learning

- Concept: BERT transformer architecture with attention mechanisms
  - Why needed here: Understanding how BERT variants capture causal relationships through self-attention is crucial for interpreting model performance differences
  - Quick check question: How does the attention mechanism in BERT help identify relationships between cause and effect phrases in medical text?

- Concept: Fine-tuning vs. few-shot prompting strategies
  - Why needed here: The paper compares fine-tuned BioBERT with few-shot prompted GPT-4, requiring understanding of when each approach is appropriate
  - Quick check question: What are the key differences in how fine-tuning and few-shot prompting adapt pre-trained models to new tasks?

- Concept: Causality extraction evaluation metrics (F1-score, Jaccard similarity)
  - Why needed here: The paper uses multiple evaluation metrics due to the challenges of comparing token-level predictions, requiring understanding of their strengths and limitations
  - Quick check question: Why might Jaccard similarity be preferred over traditional F1-score for evaluating phrase-level causality extraction?

## Architecture Onboarding

- Component map:
  - Data preprocessing: PDF conversion → sentence tokenization → causal sentence extraction → phrase extraction
  - Model components: BERT variants (BioBERT, DistilBERT, BERT) + LLMs (GPT-4, LLAMA2)
  - Evaluation pipeline: Token-level vs. phrase-level comparison → F1-score, Jaccard similarity, cosine similarity
  - Training infrastructure: Fine-tuning on GPU (BioBERT 16 epochs, DistilBERT 18 epochs, BERT 20 epochs) + HuggingFace autotrain for LLAMA2

- Critical path:
  1. Data annotation and preprocessing
  2. Model selection and configuration
  3. Fine-tuning/few-shot prompting
  4. Evaluation metric selection
  5. Result analysis and comparison

- Design tradeoffs:
  - Token-level vs. phrase-level evaluation: Token-level provides fine-grained comparison but requires exact matches, while phrase-level is more robust but loses detail
  - Fine-tuning vs. few-shot prompting: Fine-tuning requires more data and compute but can achieve higher performance, while few-shot is faster but may have lower ceiling
  - Model size vs. performance: Larger models (LLAMA2 70B) may capture more complex patterns but are harder to fine-tune and deploy

- Failure signatures:
  - BioBERT underperformance: Indicates the causal patterns in medical text differ significantly from biomedical training data
  - GPT-4 hallucination: Suggests the model is generating labels beyond the input scope, requiring phrase-level rather than token-level approaches
  - LLAMA2 non-generation: Indicates the fine-tuning data or prompt format is insufficient for the model to produce output consistently

- First 3 experiments:
  1. Compare BioBERT fine-tuning with different epoch counts (8, 12, 16) to find optimal training duration
  2. Test GPT-4 with varying prompt formats (phrase-level vs. token-level, different example counts) to identify best approach
  3. Fine-tune LLAMA2 with different dataset sizes (50%, 75%, 100% of available data) to determine data requirements for consistent generation

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What specific architectural improvements or fine-tuning strategies could enhance the performance of LLMs like GPT-4 and LLAMA2 for causality extraction tasks in medical texts?
- Basis in paper: [explicit] The paper mentions that GPT-4 and LLAMA2 show similar performance but less consistency compared to BioBERT, and LLAMA2's predictions did not include labels for 20-35% of the data.
- Why unresolved: The paper identifies performance issues with LLMs but does not explore potential solutions or strategies to address these limitations.
- What evidence would resolve it: Conducting experiments with different fine-tuning strategies, such as varying the number of epochs, adjusting learning rates, or incorporating domain-specific pre-training, and comparing their impact on the performance of GPT-4 and LLAMA2 for causality extraction.

### Open Question 2
- Question: How does the performance of causality extraction models change when applied to a larger and more diverse dataset of Clinical Practice Guidelines across various medical domains?
- Basis in paper: [explicit] The study uses a dataset of Clinical Practice Guidelines for gestational diabetes and notes the potential for future work with more extensive data.
- Why unresolved: The current study is limited to a specific domain (gestational diabetes), and the generalizability of the models to other medical domains is not explored.
- What evidence would resolve it: Expanding the annotated dataset to include Clinical Practice Guidelines from various medical specialties and evaluating the performance of causality extraction models across these diverse datasets.

### Open Question 3
- Question: What are the key challenges in achieving consistent and reliable causality extraction from medical texts, and how can these challenges be addressed through advancements in NLP techniques?
- Basis in paper: [inferred] The paper highlights issues such as the inconsistency of LLM predictions and the need for more realistic tests to bridge the gap between training and real-world data.
- Why unresolved: The paper identifies challenges but does not delve into potential solutions or advancements in NLP techniques that could mitigate these issues.
- What evidence would resolve it: Investigating and developing advanced NLP techniques, such as improved context understanding, better handling of ambiguous terms, and enhanced semantic similarity measures, and testing their effectiveness in improving causality extraction from medical texts.

## Limitations

- Limited dataset scope focusing only on gestational diabetes Clinical Practice Guidelines
- GPT-4 and LLAMA2 showed inconsistent performance with LLAMA2 failing to generate predictions for 20-35% of data
- Insufficient detail on prompt formats for GPT-4 testing, making reproduction challenging

## Confidence

- High Confidence: BioBERT outperforming other BERT variants (0.72 vs 0.61-0.60 F1-score)
- Medium Confidence: Comparison between BioBERT and LLMs based on single dataset
- Low Confidence: Conclusions about optimal prompt sizes for GPT-4 due to limited exploration

## Next Checks

1. Test the same models on a larger, more diverse set of clinical guidelines covering multiple medical conditions to assess generalizability beyond gestational diabetes.

2. Systematically vary the prompt formats and shot counts for GPT-4 to identify the optimal configuration and verify the reported performance saturation point.

3. Conduct error analysis on the 20-35% of cases where LLAMA2 failed to generate predictions to identify specific patterns or input characteristics that trigger this behavior, and test whether fine-tuning with balanced classes or alternative architectures resolves the issue.