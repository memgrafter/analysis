---
ver: rpa2
title: Corpus-Steered Query Expansion with Large Language Models
arxiv_id: '2402.18031'
source_url: https://arxiv.org/abs/2402.18031
tags:
- csqe
- query
- llms
- documents
- bm25
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the limitations of large language model (LLM)-generated
  query expansions, which can suffer from hallucinations, outdated information, and
  lack of long-tail knowledge due to reliance on intrinsic parametric knowledge. The
  authors propose Corpus-Steered Query Expansion (CSQE), which leverages the relevance
  assessing capability of LLMs to identify relevant documents and extract pivotal
  sentences from the corpus to expand the query.
---

# Corpus-Steered Query Expansion with Large Language Models

## Quick Facts
- arXiv ID: 2402.18031
- Source URL: https://arxiv.org/abs/2402.18031
- Reference count: 12
- Primary result: CSQE outperforms KEQE and ContrieverFT without training, achieving 47.2 mAP on TREC DL19

## Executive Summary
This paper addresses the limitations of LLM-generated query expansions that suffer from hallucinations, outdated information, and lack of long-tail knowledge. The authors propose Corpus-Steered Query Expansion (CSQE), which leverages the relevance assessing capability of LLMs to identify relevant documents and extract pivotal sentences from the corpus for query expansion. CSQE combines corpus-originated expansions with LLM-knowledge empowered expansions to improve relevance prediction between queries and target documents.

## Method Summary
CSQE uses GPT-3.5-Turbo to first retrieve top-k documents using BM25, then identifies relevant documents and extracts key sentences from them. The expanded query combines both corpus-originated information (from identified documents) and LLM-knowledge empowered expansions. The approach requires no training and can be applied to both sparse retrieval models like BM25 and dense retrieval models like Contriever.

## Key Results
- BM25+CSQE achieves 47.2 mAP on TREC DL19 vs 45.0 for BM25+KEQE and 41.7 for ContrieverFT
- CSQE outperforms state-of-the-art supervised ContrieverFT model without any training
- Effective on both high-resource web search datasets (TREC DL19/DL20) and low-resource BEIR datasets
- Robust to queries outside GPT-4's knowledge cutoff, as demonstrated on NovelEval dataset

## Why This Works (Mechanism)
CSQE addresses the fundamental problem that LLMs rely on parametric knowledge which can be outdated, hallucinated, or lack long-tail information. By grounding query expansion in actual corpus documents, CSQE ensures that expansions are relevant to the specific retrieval task and corpus at hand. The method leverages the LLM's ability to assess document relevance and extract key information, effectively combining the strengths of corpus-based and knowledge-based expansion approaches.

## Foundational Learning
1. **Query Expansion Fundamentals** - Why needed: Core IR technique to improve retrieval effectiveness; Quick check: Understanding how expanded queries can capture user intent better than original queries.
2. **LLM Hallucination Problem** - Why needed: Explains why pure LLM-based expansion fails; Quick check: Recognizing that LLMs can generate plausible but incorrect information.
3. **Relevance Assessment in IR** - Why needed: Critical for identifying which documents to use for expansion; Quick check: Understanding how relevance feedback improves retrieval.
4. **Zero-Shot Learning** - Why needed: CSQE requires no training, making it broadly applicable; Quick check: Ability to apply methods without dataset-specific fine-tuning.

## Architecture Onboarding

**Component Map:** BM25 -> GPT-3.5-Turbo (relevance assessment) -> Document selection -> GPT-3.5-Turbo (sentence extraction) -> Query expansion -> Retrieval

**Critical Path:** Query → BM25 retrieval → Top-k documents → GPT relevance assessment → Document selection → Key sentence extraction → Expanded query → Final retrieval

**Design Tradeoffs:** Zero-shot approach vs. supervised methods (no training required but may be less optimal than fine-tuned models); corpus-based grounding vs. pure LLM knowledge (more reliable but potentially less creative); simplicity vs. potential for more complex expansion strategies.

**Failure Signatures:** 
- Poor retrieval quality in initial BM25 step leading to irrelevant expansions
- GPT hallucinations during relevance assessment or sentence extraction
- Over-expansion diluting the original query intent
- Performance degradation on domains significantly different from training data

**First Experiments:**
1. Run KEQE baseline on TREC DL19 to establish baseline performance
2. Implement CSQE with fixed k=10 and compare to KEQE on same dataset
3. Test CSQE on one BEIR dataset (e.g., Scifact) to evaluate cross-domain performance

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions, but based on the experimental setup, several questions arise: How does CSQE perform with different retrieval models beyond BM25 and Contriever? What is the optimal number of documents (k) to retrieve for effective expansion? How sensitive is CSQE to the specific prompt engineering choices? How does computational cost scale with corpus size?

## Limitations
- Performance may be sensitive to LLM choice and prompt engineering details
- Computational efficiency and scalability to very large corpora not explored
- Limited investigation of optimal parameters like number of top-k documents
- Reliance on GPT-3.5-Turbo means potential cost and API availability constraints

## Confidence
- **High confidence**: Superior performance over baselines across multiple datasets; effectiveness on low-resource BEIR datasets; zero-shot capability without training
- **Medium confidence**: Robustness to queries outside GPT-4's knowledge cutoff; generalizability to other retrieval models
- **Low confidence**: Computational efficiency and scalability to large-scale corpora; sensitivity to prompt engineering choices

## Next Checks
1. Conduct prompt sensitivity analysis by testing CSQE with variations in prompt structure and number of top-k documents retrieved
2. Evaluate CSQE's computational efficiency and retrieval quality on larger corpora like MS MARCO
3. Extend NovelEval testing to include queries spanning multiple years post-GPT-4 release to rigorously test temporal robustness