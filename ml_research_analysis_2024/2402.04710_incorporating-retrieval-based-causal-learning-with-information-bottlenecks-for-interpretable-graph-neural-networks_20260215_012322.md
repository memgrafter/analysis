---
ver: rpa2
title: Incorporating Retrieval-based Causal Learning with Information Bottlenecks
  for Interpretable Graph Neural Networks
arxiv_id: '2402.04710'
source_url: https://arxiv.org/abs/2402.04710
tags: []
core_contribution: This paper introduces RC-GNN, a novel interpretable graph neural
  network framework that addresses the trade-off between GNN explanation accuracy
  and prediction performance. The core innovation is the integration of retrieval-based
  causal learning with Graph Information Bottleneck (GIB) theory, which semi-parametrically
  retrieves crucial subgraphs detected by GIB and compresses them via a causal module.
---

# Incorporating Retrieval-based Causal Learning with Information Bottlenecks for Interpretable Graph Neural Networks

## Quick Facts
- arXiv ID: 2402.04710
- Source URL: https://arxiv.org/abs/2402.04710
- Reference count: 24
- Key result: 32.71% higher precision on real-world explanation scenarios compared to state-of-the-art methods

## Executive Summary
This paper introduces RC-GNN, a novel interpretable graph neural network framework that addresses the trade-off between GNN explanation accuracy and prediction performance. The core innovation is the integration of retrieval-based causal learning with Graph Information Bottleneck (GIB) theory, which semi-parametrically retrieves crucial subgraphs detected by GIB and compresses them via a causal module. This approach enables the framework to achieve both reliable explanations and generalizable predictions. The method consistently outperforms state-of-the-art baselines on four graph explanation benchmarks, achieving 32.71% higher precision on real-world explanation scenarios. Moreover, the learned explanations significantly improve GNN classification performance, with average accuracy increases of 5.46% over existing interpretable GNN models.

## Method Summary
RC-GNN employs a semi-parametric paradigm that combines retrieval-based subgraph selection with causal compression learning. The framework first uses a GNN encoder to generate node representations, then retrieves sufficient explanations from candidate graphs sharing the same predicted label. A causal module, comprising separate causal and trivial GNN components, compresses these explanatory subgraphs while minimizing mutual information between the subgraph and input graph. Contrastive learning removes correlation between graph representations and trivial representations, producing disentangled causal and trivial features that improve both explanation quality and prediction accuracy.

## Key Results
- Achieves 32.71% higher precision on real-world explanation scenarios compared to state-of-the-art methods
- Improves GNN classification accuracy by an average of 5.46% over existing interpretable GNN models
- Consistently outperforms baselines on four graph explanation benchmarks (BA-3Motif, MUTAG, Benzene, MutagenicityV2, CYP3A4)

## Why This Works (Mechanism)

### Mechanism 1
The retrieval-based causal learning framework achieves better explanation accuracy by capturing shared subgraph patterns across graphs with the same label. The framework semi-parametrically retrieves crucial subgraphs from a candidate set of graphs with the same predicted label, ensuring that the explanations capture common motif patterns. This works because graphs of the same class typically share common motif patterns that are causally relevant to the prediction.

### Mechanism 2
The causal module improves explanation precision by minimizing mutual information between the input graph and compressed subgraph. The framework uses backdoor adjustment to remove confounding effects, ensuring the compressed subgraph contains minimal information about the input graph while preserving causal information. Shielding the GNN from the confounder (input graph minus causal subgraph) is key to exploiting causal subgraphs.

### Mechanism 3
The disentangled representations from separate causal and trivial GNN modules improve both explanation quality and prediction performance. Two separate GNNs encode causal and trivial subgraphs into disentangled representations, with contrastive learning removing correlation between graph representations and trivial representations. This enables the causal framework to optimize the compression of explanatory subgraphs.

## Foundational Learning

- Concept: Graph Information Bottleneck (GIB) theory
  - Why needed here: Provides the theoretical foundation for identifying maximally informative yet compressed subgraphs
  - Quick check question: How does GIB balance maximizing information about the label while minimizing information about the input graph?

- Concept: Causal theory and backdoor adjustment
  - Why needed here: Enables removal of confounding effects to identify truly causal subgraphs rather than spurious correlations
  - Quick check question: What is the backdoor adjustment formula and how does it relate to causal inference in graphs?

- Concept: Contrastive learning for mutual information maximization
  - Why needed here: Provides a practical method to minimize mutual information between input graph and compressed subgraph
  - Quick check question: How does minimizing contrastive loss relate to maximizing mutual information between variables?

## Architecture Onboarding

- Component map: Input graph → GNN encoder → Subgraph retrieval module → Causal module (causal GNN + trivial GNN) → Linear classifier → Output predictions + Explanations
- Critical path: Graph encoding → Subgraph retrieval → Causal compression → Final prediction
- Design tradeoffs: Parametric vs non-parametric explanation methods; single vs dual GNN architecture; supervised vs unsupervised subgraph retrieval
- Failure signatures: Low precision in explanations (poor subgraph retrieval); poor prediction performance (failed causal disentanglement); high computational cost (inefficient subgraph matching)
- First 3 experiments:
  1. Verify subgraph retrieval module correctly identifies common motifs across same-label graphs
  2. Test causal module's ability to separate causal and trivial features through ablation
  3. Measure improvement in both explanation quality and prediction accuracy compared to baseline methods

## Open Questions the Paper Calls Out

### Open Question 1
How does the performance of RC-GNN vary with different values of the subgraph size parameter K in the retrieval module? The paper mentions that the retrieval module aims to find a subgraph GS with K nodes to maximize I(G'S, GS), but does not explore the impact of varying K on the model's performance.

### Open Question 2
Can the retrieval-based causal learning approach be extended to handle dynamic graphs where the structure changes over time? The current framework is designed for static graph classification tasks. The paper does not discuss how it could be adapted for dynamic graphs.

### Open Question 3
How sensitive is RC-GNN to the choice of the threshold t for determining qualified counterparts in the retrieval process? The paper mentions that the candidate graph G' needs to have a normalized similarity score above threshold t (i.e., t=0.4) to be considered a qualified counterpart.

### Open Question 4
How does the computational complexity of RC-GNN compare to post-hoc explanation methods as the size of the graph and dataset increases? The paper claims that RC-GNN outperforms post-hoc methods but does not provide a detailed complexity analysis.

## Limitations
- Computational complexity of subgraph retrieval may become prohibitive for large-scale graphs
- Effectiveness depends on the assumption that same-label graphs share common causal subgraphs
- Limited ablation studies on hyperparameter sensitivity, particularly regarding the number of retrieved subgraphs

## Confidence
- High Confidence: Overall effectiveness in improving both explanation quality and prediction accuracy
- Medium Confidence: Specific mechanism of how the causal module disentangles causal and trivial features
- Medium Confidence: Scalability claims, given limited discussion of computational complexity

## Next Checks
1. Conduct scalability experiments on larger graph datasets to assess computational efficiency and performance degradation
2. Perform systematic ablation studies varying the number of retrieved subgraphs and contrastive learning parameters
3. Test the framework on datasets where same-label graphs may not share common structural patterns to evaluate robustness of the retrieval assumption