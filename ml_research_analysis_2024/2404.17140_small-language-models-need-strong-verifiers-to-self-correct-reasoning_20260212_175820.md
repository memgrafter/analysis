---
ver: rpa2
title: Small Language Models Need Strong Verifiers to Self-Correct Reasoning
arxiv_id: '2404.17140'
source_url: https://arxiv.org/abs/2404.17140
tags:
- step
- self-correction
- reasoning
- answer
- correct
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces SCORE, a novel pipeline to generate self-correction
  data from small language models and fine-tune them to become self-correcting reasoners.
  The method leverages correct solutions as hints to guide small LMs in critiquing
  their incorrect responses, then uses the filtered critiques for supervised fine-tuning
  of the refiner.
---

# Small Language Models Need Strong Verifiers to Self-Correct Reasoning

## Quick Facts
- arXiv ID: 2404.17140
- Source URL: https://arxiv.org/abs/2404.17140
- Reference count: 36
- This paper introduces SCORE, a novel pipeline to generate self-correction data from small language models and fine-tune them to become self-correcting reasoners, achieving an average of 14.6% improvement with a strong verifier.

## Executive Summary
This paper introduces SCORE, a novel pipeline that enables small language models to self-correct their reasoning by leveraging correct solutions as hints to guide critique generation. The method generates critique-correction data from small LMs, then fine-tunes them to become self-correcting reasoners. Experiments on five datasets spanning math and commonsense reasoning show improved self-correction abilities, with notable performance gains when paired with a strong GPT-4-based verifier. However, limitations are identified when using a weak self-verifier for determining when to correct.

## Method Summary
SCORE is a pipeline that generates self-correction data from small language models and fine-tunes them to become self-correcting reasoners. It leverages correct solutions as hints to guide small LMs in critiquing their incorrect responses, then uses the filtered critiques for supervised fine-tuning of the refiner. The method involves sampling multiple solutions per question, using correct solutions to generate critiques for incorrect ones, filtering these critiques, and fine-tuning the model to generate both critiques and corrections.

## Key Results
- SCORE achieves an average of 14.6% improvement in self-correction performance when paired with a strong GPT-4-based verifier
- Performance gains are notable on GSM8K (12.6%), CommonsenseQA (15.2%), and MATH (21.4%) datasets
- Small LMs like Gemma-7B can surpass larger models like LLaMA-13B after self-correction training

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Correct solutions serve as effective hints to guide critique generation for incorrect responses.
- **Mechanism**: By providing a correct solution alongside an incorrect one, the model can compare steps and identify where reasoning diverges, making critique generation more tractable.
- **Core assumption**: The model can effectively compare reasoning steps between two solutions when given both.
- **Evidence anchors**:
  - [abstract]: "First, we leverage correct solutions to guide the model in critiquing their incorrect responses."
  - [section]: "We find it easier for the LM to generate critiques using correct solutions as hints, as the model only needs to compare the different steps between these two solutions and justify the correct ones."
- **Break condition**: The model fails to identify key differences between correct and incorrect reasoning paths, or the correct solution itself contains subtle errors that mislead the critique.

### Mechanism 2
- **Claim**: Separating verification and refinement into distinct modules reduces training difficulty and allows independent parameterization.
- **Mechanism**: By specializing each module for one task (verification or refinement), the model can focus on mastering one skill rather than both simultaneously.
- **Core assumption**: Models can specialize effectively when trained on focused tasks rather than combined tasks.
- **Evidence anchors**:
  - [section]: "Decoupling (SELF-)VERIFY and SELF-REFINE brings two major advantages... First, we can freely parameterize each module... Second, it reduces the difficulty of training each module, since the model only needs to specialize in one kind of ability."
- **Break condition**: The model cannot generalize its specialized verification or refinement skills to handle combined tasks during inference.

### Mechanism 3
- **Claim**: Strong verifiers are essential to unlock the refinement potential of small language models.
- **Mechanism**: A strong verifier accurately identifies incorrect responses, triggering refinement only when needed, while a weak verifier either over-corrects or under-corrects, limiting improvement.
- **Core assumption**: The refinement module performs well when given accurate signals about when correction is needed.
- **Evidence anchors**:
  - [abstract]: "Our experimental results show improved self-correction abilities... with notable performance gains when paired with a strong GPT-4-based verifier, though limitations are identified when using a weak self-verifier."
  - [section]: "Using the same fine-tuned refiner, the final accuracies vary a lot among different verifiers... a weak self-verifier can only bring minor improvements, if not misguiding the refiner."
- **Break condition**: The verifier produces false positives (correct answers marked as incorrect) or false negatives (incorrect answers marked as correct), leading to wasted refinement attempts or missed opportunities.

## Foundational Learning

- **Concept**: Chain-of-thought reasoning
  - Why needed here: The method relies on sampling diverse reasoning chains and fine-tuning on correct ones, following rejection sampling principles.
  - Quick check question: Can you explain how chain-of-thought prompting differs from direct answer generation and why it's useful for reasoning tasks?

- **Concept**: Supervised fine-tuning with structured outputs
  - Why needed here: The refiner is trained to generate both critiques and corrections in a specific format, requiring understanding of structured output fine-tuning.
  - Quick check question: How would you modify a standard cross-entropy loss to handle the masked tokens in the critique-correction generation task?

- **Concept**: Verification vs. generation in language models
  - Why needed here: The work explicitly separates verification (determining if an answer is correct) from generation (producing the answer), which is a key architectural decision.
  - Quick check question: What are the key differences in model architecture and training data between a verifier and a generator for reasoning tasks?

## Architecture Onboarding

- **Component map**: Question → Base LM (initial solution) → Verifier (check correctness) → Refiner (if incorrect) → Final answer

- **Critical path**: Question → Base LM (initial solution) → Verifier (check correctness) → Refiner (if incorrect) → Final answer

- **Design tradeoffs**:
  - Single vs. multi-iteration refinement: Single iteration is faster but may miss deeper errors; multiple iterations could improve accuracy but increase latency
  - Rule-based vs. learned filtering: Rule-based is faster and more interpretable but may miss nuanced quality issues; learned filtering could be more accurate but requires additional training data
  - Using vs. not using correct solutions as hints during inference: Hints help during training data collection but aren't available during actual use

- **Failure signatures**:
  - Low improvement with strong verifier: Refiner may not be learning effective critique-generation or correction skills
  - High false positive rate in verification: Verifier is too aggressive, causing unnecessary refinement attempts
  - Poor transfer to new datasets: Model may be overfitting to training data patterns rather than learning generalizable self-correction skills

- **First 3 experiments**:
  1. Test critique generation with and without correct solution hints on a small validation set to verify the hint mechanism
  2. Compare single-step vs. multi-step refinement on a held-out dev set to understand iteration benefits
  3. Evaluate the impact of different filtering thresholds on the quality and quantity of training data

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of SCORE change when using stronger or weaker base language models?
- Basis in paper: The paper primarily focuses on LLaMA-2-13B-chat and Gemma-7B-it, but mentions that SCORE could be applied to various datasets whose reasoning could be expressed in a step-by-step format. It also notes that Gemma-7B's accuracy surpasses LLaMA-13B after self-correction, suggesting Gemma is more effective at learning self-correction skills from SCORE fine-tuning.
- Why unresolved: The paper only tests SCORE on two specific base models. The effectiveness of SCORE on a wider range of base models, including those with different sizes and architectures, is not explored.
- What evidence would resolve it: Experimenting with SCORE on a diverse set of base models and comparing the results would provide insights into how the performance scales with different model sizes and architectures.

### Open Question 2
- Question: Can SCORE be effectively combined with other fine-tuning methods, such as rejection sampling fine-tuning, to further improve reasoning performance?
- Basis in paper: The paper mentions that SCORE could be combined with oversample-then-rerank, an orthogonal approach to further improve reasoning. It also shows that SCORE is complementary to RFT (rejection sampling fine-tuning) by demonstrating improved performance when using RFT as the base model.
- Why unresolved: While the paper shows that SCORE can be combined with RFT, it does not explore the potential of combining SCORE with other fine-tuning methods. The effectiveness of such combinations on reasoning performance is not investigated.
- What evidence would resolve it: Experimenting with SCORE combined with various fine-tuning methods and comparing the results would provide insights into the synergistic effects of these combinations on reasoning performance.

### Open Question 3
- Question: How does the performance of SCORE scale with the size of the fine-tuning dataset?
- Basis in paper: The paper investigates the data-efficiency for refiner fine-tuning and finds that the performance plateaus with a weak self-verifier, but benefits from more fine-tuning data when paired with strong verifiers (oracle labels or gpt-4).
- Why unresolved: While the paper shows that more fine-tuning data benefits performance with strong verifiers, it does not explore the scaling behavior in detail. The relationship between dataset size and performance, especially with weak verifiers, is not fully understood.
- What evidence would resolve it: Conducting experiments with varying sizes of fine-tuning datasets and analyzing the performance trends, particularly with weak verifiers, would provide insights into the scaling behavior of SCORE.

## Limitations
- Heavy dependence on strong external verifiers (GPT-4) raises questions about practical deployment costs
- Critique filtering mechanisms rely on rule-based approaches that may not generalize well to more complex reasoning tasks
- The ablation study comparing SCORE with existing self-correction methods is limited

## Confidence

**High Confidence**:
- Small LMs can be fine-tuned to perform self-correction using SCORE
- SCORE improves self-correction performance when paired with strong verifiers
- Separating verification and refinement modules provides practical training advantages

**Medium Confidence**:
- SCORE's performance gains are primarily due to the critique-correction data generation approach
- The specific filtering rules used are optimal for generating high-quality training data
- The method generalizes well across diverse reasoning tasks

**Low Confidence**:
- SCORE would maintain its performance advantage with weaker but more practical verifiers
- The critique-correction approach is superior to alternative self-correction methods in all scenarios

## Next Checks
1. **Verifier Strength Sensitivity Analysis**: Systematically test SCORE with verifiers of varying strengths (including fine-tuned small models) to establish the minimum verifier capability required for SCORE to outperform baseline methods like RFT.

2. **Cross-Domain Generalization**: Apply SCORE-trained models to reasoning tasks from domains not represented in the original training data (e.g., legal reasoning, scientific analysis) to assess whether the self-correction skills transfer beyond math and commonsense reasoning.

3. **Extended Refinement Iterations**: Implement and evaluate multi-step refinement where the output of one refinement round becomes the input for another, measuring whether this cascades improvements or leads to compounding errors, particularly with weaker verifiers.