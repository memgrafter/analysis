---
ver: rpa2
title: Topological safeguard for evasion attack interpreting the neural networks'
  behavior
arxiv_id: '2402.07480'
source_url: https://arxiv.org/abs/2402.07480
tags:
- detector
- neurons
- adversarial
- activation
- attribute
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a novel evasion attack detector that leverages
  topological information from neural network behavior graphs. The method constructs
  graphs representing classifier behavior, where nodes are neurons and edges are weighted
  by activation values.
---

# Topological safeguard for evasion attack interpreting the neural networks' behavior

## Quick Facts
- arXiv ID: 2402.07480
- Source URL: https://arxiv.org/abs/2402.07480
- Reference count: 38
- Detects evasion attacks on neural networks using topological graph features with 88.32% FGSM, 96.9% BIM, and 95.73% PGD accuracy

## Executive Summary
This paper introduces a novel evasion attack detector that leverages topological information from neural network behavior graphs. The method constructs graphs representing classifier behavior, where nodes are neurons and edges are weighted by activation values. New neuron attributes—impact, influence, input proportion, and specialization—are computed to enrich the dataset. A Graph Convolutional Network (GCN) is then trained to distinguish adversarial from legitimate inputs. Evaluated on breast histopathology images with FGSM, BIM, and PGD attacks, the detector achieves 88.32%, 96.9%, and 95.73% accuracy respectively—outperforming existing auxiliary-model detectors.

## Method Summary
The method constructs behavior graphs from neural network activations where neurons are nodes and edges represent activation weights. Four neuron attributes (impact, influence, input proportion, specialization) are computed to enrich the graph representation. A GCN-based detector is trained on these graphs to classify inputs as adversarial or legitimate. The approach is validated on breast histopathology images under FGSM, BIM, and PGD attacks, demonstrating superior performance compared to existing auxiliary-model detectors.

## Key Results
- Achieves 88.32% accuracy detecting FGSM attacks
- Achieves 96.9% accuracy detecting BIM attacks  
- Achieves 95.73% accuracy detecting PGD attacks
- Outperforms existing auxiliary-model detection methods

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Topological information in neural network behavior graphs captures discriminative patterns between adversarial and legitimate inputs.
- Mechanism: The classifier's behavior graph encodes neuron activation flows and topological connectivity. Adversarial inputs induce anomalous activation patterns that are detectable by a GCN trained on these graphs.
- Core assumption: The topology of the classifier contains essential information about whether an evasion attack occurs.
- Evidence anchors:
  - [abstract] "the literature shows that the targeted model’s topology contains essential information about if the evasion attack occurs."
  - [section 3.1] Definition of Behavior Graph explicitly ties neuron activation values to weighted edges between neurons.
  - [corpus] Weak — related works focus on graph representations of DNNs but not specifically on evasion attack detection via topology.
- Break condition: If adversarial attacks can be crafted to mimic legitimate activation topologies, the topological features may lose discriminative power.

### Mechanism 2
- Claim: The four neuron attributes (Impact, Influence, Input Proportion, Specialization) enrich the graph representation with semantic information about neuron behavior.
- Mechanism: These attributes are computed per neuron per input, adding local node features that a GCN can leverage for classification.
- Core assumption: These attributes provide orthogonal and non-redundant information about the classifier's behavior under attack.
- Evidence anchors:
  - [section 3.2] Detailed definitions and interpretations of each attribute.
  - [section 5.2] Experimental ablation study shows impact and influence are most informative, but all contribute.
  - [corpus] Weak — no corpus neighbors explicitly discuss these attributes for evasion detection.
- Break condition: If adversarial attacks systematically nullify these attributes (e.g., by keeping neuron activations unchanged), the enrichment becomes ineffective.

### Mechanism 3
- Claim: GCN architecture is optimal for learning from classifier behavior graphs because it respects local topological structure.
- Mechanism: GCN layers aggregate information from neighboring neurons according to the learned topology, allowing the model to detect localized activation anomalies.
- Core assumption: The classifier's fully-connected structure and activation flow can be meaningfully modeled as a graph for GCN processing.
- Evidence anchors:
  - [section 3.4] Selection of GCN based on its ability to capture topology.
  - [section 4] Experiment uses GCN-based detector with stated architecture.
  - [corpus] Weak — neighbors discuss GCNs for other tasks (e.g., fake news detection, robustness certificates) but not evasion attack detection.
- Break condition: If the classifier's activation dynamics are too sparse or the graph becomes too dense, GCN may fail to learn useful representations.

## Foundational Learning

- Concept: Graph Convolutional Networks (GCNs)
  - Why needed here: GCNs are chosen because they can learn from topological structure of the classifier behavior graph, which is central to the detection method.
  - Quick check question: What is the key difference between GCNs and standard CNNs in terms of input representation?

- Concept: Evasion attacks (FGSM, BIM, PGD)
  - Why needed here: The detector is specifically designed to distinguish these adversarial inputs from legitimate ones.
  - Quick check question: How do FGSM, BIM, and PGD differ in the way they craft adversarial examples?

- Concept: Behavior graphs for neural networks
  - Why needed here: The core input representation for the detector is a graph where nodes are neurons and edges are weighted by activation values.
  - Quick check question: In a behavior graph, what do the nodes and edges represent?

## Architecture Onboarding

- Component map: Preprocess images -> Build behavior graph -> Compute node attributes -> Train GCN detector -> Classify
- Critical path: Preprocess images → build behavior graph → compute neuron attributes → train GCN detector → classify
- Design tradeoffs:
  - GCN vs. MLP: GCN preserves topology but is more complex; MLP ignores structure but is simpler.
  - Attribute selection: More attributes may improve detection but risk overfitting or redundancy.
  - Architecture depth: Deeper GCN may capture more complex patterns but risks overfitting on small datasets.
- Failure signatures:
  - Low accuracy on any attack type (e.g., <80%) suggests attribute or model inadequacy.
  - High training accuracy but low test accuracy indicates overfitting.
  - Loss not converging suggests hyperparameter or architecture issues.
- First 3 experiments:
  1. Train detector on FGSM dataset only; evaluate accuracy and loss curves.
  2. Ablation: Train with only impact and influence attributes; compare to full feature set.
  3. Swap GCN for simple MLP baseline; compare detection performance.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How would the detector's performance change with different graph neural network architectures beyond GCN?
- Basis in paper: [explicit] "In the future, this detector can be improved by testing different architectures of the presented graph convolutional network."
- Why unresolved: The paper only tests one GCN architecture and acknowledges that optimization is needed.
- What evidence would resolve it: Comparative experiments testing alternative GNN architectures (Graph Attention Networks, GraphSAGE, etc.) on the same evasion detection task.

### Open Question 2
- Question: How does the input proportion attribute's informativeness vary with different activation functions in the target model?
- Basis in paper: [explicit] "The input proportion attribute has to be mentioned... it can give more or less information according to the activation function used in the targeted model."
- Why unresolved: The experiment only tested sigmoid activation, which limits the attribute's information content, and the paper acknowledges this limitation.
- What evidence would resolve it: Experiments testing the same detection framework on models using ReLU, Leaky ReLU, or other activation functions that generate more zero-valued activations.

### Open Question 3
- Question: Can the topological detection approach be extended to identify other types of attacks beyond evasion attacks?
- Basis in paper: [explicit] "Even this defense method can be implemented in detecting other threats that generate anomalies in the activation values... such as poisoning and trojaning attacks."
- Why unresolved: The paper only validates the approach on evasion attacks and suggests this as future work without experimental evidence.
- What evidence would resolve it: Empirical testing of the detector on poisoning and trojaning attacks using the same topological feature extraction methodology.

## Limitations
- Limited validation to breast histopathology images; generalizability to other domains unknown
- Only tested on three attack types (FGSM, BIM, PGD); unknown performance against other attack methods
- No evaluation of adaptive attacks where adversaries are aware of the detection method

## Confidence

- **High confidence**: The GCN architecture is appropriate for learning from graph-structured data representing neural network behavior
- **Medium confidence**: The four neuron attributes meaningfully enrich the graph representation for attack detection
- **Medium confidence**: Topological information in classifier behavior graphs provides discriminative power between adversarial and legitimate inputs

## Next Checks

1. **Cross-domain validation**: Test the detector on ImageNet or CIFAR-10 with the same attack types to assess generalizability beyond histopathology
2. **Adversarial robustness evaluation**: Conduct an adaptive attack where the adversary is aware of the topological detection method and attempts to craft attacks that preserve legitimate-like activation patterns
3. **Ablation on neuron attributes**: Systematically remove each of the four attributes in isolation to quantify their individual contributions to detection accuracy