---
ver: rpa2
title: 'Optimisation Strategies for Ensuring Fairness in Machine Learning: With and
  Without Demographics'
arxiv_id: '2411.09056'
source_url: https://arxiv.org/abs/2411.09056
tags:
- fairness
- data
- supp
- learning
- page
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This thesis addresses fairness in machine learning, focusing on\
  \ two main challenges: learning from imbalanced data and ensuring fairness without\
  \ access to sensitive demographic attributes. For imbalanced data, the thesis introduces\
  \ two novel fairness notions\u2014subgroup fairness and instantaneous fairness\u2014\
  in the context of forecasting linear dynamic systems."
---

# Optimisation Strategies for Ensuring Fairness in Machine Learning: With and Without Demographics

## Quick Facts
- arXiv ID: 2411.09056
- Source URL: https://arxiv.org/abs/2411.09056
- Authors: Quan Zhou
- Reference count: 0
- Primary result: Novel fairness formulations for imbalanced time-series data and demographic-blind bias repair using optimal transport

## Executive Summary
This thesis addresses two fundamental fairness challenges in machine learning: learning from imbalanced data and ensuring fairness without access to sensitive demographic attributes. The work introduces two novel fairness notions—subgroup fairness and instantaneous fairness—for forecasting linear dynamic systems, extending group fairness concepts to time-series problems. Additionally, it proposes a group-blind bias-repair framework using optimal transport that achieves demographic parity without requiring individual-level sensitive attribute information. The methodologies are validated on benchmark datasets including COMPAS and Adult Census Income.

## Method Summary
The thesis develops two main approaches: (1) Non-commutative polynomial optimization (NCPOP) for fairness-constrained learning of linear dynamic systems, solving fairness objectives via semidefinite programming (SDP) relaxations with global optimality guarantees under the Archimedean assumption; and (2) Group-blind bias repair using optimal transport to align feature distributions between privileged and unprivileged groups without requiring individual sensitive attributes. The methods are implemented using the NPA and TSSOS hierarchies for NCPOP, and Dykstra's algorithm with Bregman projections for optimal transport.

## Key Results
- Subgroup fairness and instantaneous fairness notions successfully address under-representation bias in forecasting linear dynamic systems
- Group-blind bias repair achieves demographic parity without individual-level sensitive attribute values through population-level distributional alignment
- Empirical validation on COMPAS and Adult Census Income datasets demonstrates improved fairness metrics while maintaining predictive performance

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Non-commutative polynomial optimisation enables global convergence in fairness-constrained learning of linear dynamic systems without assuming hidden state dimension.
- **Mechanism:** The formulation transforms fairness-constrained learning into a non-commutative polynomial optimisation problem (NCPOP), which can be solved using a hierarchy of semidefinite programming (SDP) relaxations. The NCPOP framework handles operator-valued decision variables and guarantees global convergence under the Archimedean assumption.
- **Core assumption:** The underlying linear dynamic system is observable and the SDP relaxations converge to the global optimum.
- **Evidence anchors:**
  - [abstract] "The approach leverages non-commutative polynomial optimization to solve fairness-constrained learning problems with global optimality guarantees."
  - [section 3.2] "For any observable linear system... under the Archimedean assumption, there is a convex optimisation problem whose objective function value is at most ϵ away from Equations (3.1) subject to (3.2–3.3)."
- **Break condition:** The Archimedean assumption fails (system becomes unstable) or the SDP relaxations do not converge within computational limits.

### Mechanism 2
- **Claim:** Group-blind bias repair using optimal transport can achieve demographic parity without requiring individual-level sensitive attribute values.
- **Mechanism:** A projection map is computed using population-level distributions of features for privileged and unprivileged groups. This map aligns the feature distributions of both groups via optimal transport, achieving group parity without needing individual sensitive attribute values for computation or usage.
- **Core assumption:** Source data are an unbiased representation of the population, allowing population-level distributions to be estimated from the source data.
- **Evidence anchors:**
  - [abstract] "This method aligns feature distributions between privileged and unprivileged groups without requiring individual-level sensitive attribute information, achieving demographic parity through population-level distributional adjustments."
  - [section 5.3.1] "We require only the population-level information regarding feature distributions for both privileged and unprivileged groups, as well as the assumptions that source data are collected via unbiased sampling from the population."
- **Break condition:** The unbiased sampling assumption is violated (source data are biased) or the sensitive attribute has more than two classes (current framework limitation).

### Mechanism 3
- **Claim:** Two natural notions of fairness in forecasting (subgroup fairness and instantaneous fairness) extend group fairness to time-series problems and address under-representation bias.
- **Mechanism:** Subgroup fairness equalises the sum of losses across subgroups weighted by trajectory cardinality. Instantaneous fairness equalises the instantaneous loss across all subgroups and times. These notions are formulated as min-max optimisation problems and solved using NCPOP techniques.
- **Core assumption:** The forecasting problem can be modelled as learning from observations of an unknown dynamical system with an unobserved state.
- **Evidence anchors:**
  - [abstract] "These notions extend group fairness concepts to time-series problems, aiming to equalize prediction accuracy across subgroups."
  - [section 4.1] "To address under-representation bias in the training of a forecasting model, it is natural to seek a notion of fairness that captures the overall behaviour across all subgroups, while taking into account the varying amounts of training data for the individual subgroups."
- **Break condition:** The linear dynamical system assumption is violated (non-linear dynamics) or the min-max optimisation becomes intractable for large-scale problems.

## Foundational Learning

- **Concept:** Linear dynamical systems and observability
  - Why needed here: The thesis focuses on fairness in forecasting linear dynamic systems, so understanding the basic model (state evolution, observation equations) and observability is crucial for following the formulations.
  - Quick check question: What is the observability matrix, and why is the observability assumption important for system identification?

- **Concept:** Non-commutative polynomial optimisation and semidefinite programming
  - Why needed here: The thesis uses NCPOP and SDP relaxations to solve the fairness-constrained learning problems. Understanding these optimisation techniques is essential for grasping how the algorithms work and their convergence properties.
  - Quick check question: How does NCPOP differ from commutative polynomial optimisation, and what is the role of SDP relaxations in solving NCPOP problems?

- **Concept:** Fairness definitions (group fairness, individual fairness, demographic parity, equal opportunity)
  - Why needed here: The thesis proposes new fairness notions and applies them to specific fairness problems. Familiarity with existing fairness definitions is necessary to understand the context and motivation for the new approaches.
  - Quick check question: What is the difference between demographic parity and equal opportunity, and how do these group fairness definitions relate to the new notions of subgroup fairness and instantaneous fairness?

## Architecture Onboarding

- **Component map:** Data preprocessing -> Model formulation (LDS, fairness objectives, NCPOP) -> Optimisation solver (SDP relaxations, NPA/TSSOS hierarchies) -> Performance evaluation -> Comparison with baselines

- **Critical path:**
  1. Generate or load biased training data with under-representation bias
  2. Formulate the fairness-constrained learning problem as NCPOP
  3. Solve the NCPOP using SDP relaxations within the NPA or TSSOS hierarchy
  4. Extract the learned model parameters and evaluate performance on fairness metrics and accuracy
  5. Compare results with baseline methods and existing approaches

- **Design tradeoffs:**
  - Computational complexity vs. global optimality guarantees (NCPOP vs. local optimisation methods)
  - Runtime vs. accuracy (first-order vs. higher-order SDP relaxations)
  - Bias repair vs. data distortion (total repair vs. partial repair schemes)
  - Fairness metric choice vs. application context (subgroup fairness vs. instantaneous fairness)

- **Failure signatures:**
  - SDP relaxations fail to converge or yield infeasible solutions (check Archimedean assumption, model formulation)
  - Runtime becomes prohibitive for large-scale problems (check sparsity exploitation, moment order)
  - Fairness metrics improve but accuracy significantly degrades (check bias repair vs. data distortion tradeoff)
  - Results differ substantially from baselines (check implementation correctness, hyperparameter tuning)

- **First 3 experiments:**
  1. Reproduce the biased training data generation and fairness-constrained learning on a simple LDS example to verify the basic framework works.
  2. Apply the group-blind bias repair method to synthetic data with known sensitive attribute distributions to verify demographic parity is achieved without individual-level attribute values.
  3. Compare the subgroup fairness and instantaneous fairness formulations on the COMPAS dataset to demonstrate the effectiveness of the new fairness notions in addressing under-representation bias.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can fairness in machine learning be achieved when sensitive attributes are unavailable, particularly in cases where proxies are correlated with these attributes?
- Basis in paper: [explicit] The paper discusses the challenge of unavailable sensitive attributes and proposes a group-blind bias-repair framework using optimal transport to align feature distributions without requiring individual-level sensitive attribute information.
- Why unresolved: While the proposed method shows promise, the paper acknowledges limitations, such as only allowing one binary sensitive attribute and the need for population-level information on feature distributions for both groups.
- What evidence would resolve it: Empirical validation of the method on diverse datasets with multiple sensitive attributes and in scenarios where population-level information is limited or unavailable.

### Open Question 2
- Question: What are the trade-offs between fairness and accuracy in machine learning models, and how can these trade-offs be quantified and minimized?
- Basis in paper: [explicit] The paper mentions the well-identified trade-offs between fairness and accuracy in machine learning, and explores the trade-off between bias repair and data distortion using the Adult Census Income dataset.
- Why unresolved: The paper provides some insights into the trade-offs, but a comprehensive understanding of the relationship between fairness and accuracy, and methods to quantify and minimize these trade-offs, remains an open question.
- What evidence would resolve it: Development of theoretical frameworks and empirical studies that quantify the trade-offs between fairness and accuracy across different machine learning tasks and datasets.

### Open Question 3
- Question: How can fairness be ensured in machine learning models that are used in dynamic environments where data collection and decision-making processes are constantly evolving?
- Basis in paper: [inferred] The paper discusses the dynamic nature of data collection and the potential for feedback loops and selection bias, but does not explicitly address how to ensure fairness in such dynamic environments.
- Why unresolved: Ensuring fairness in dynamic environments is a complex challenge that requires ongoing monitoring and adaptation of machine learning models, as well as consideration of the long-term consequences of decisions made by these models.
- What evidence would resolve it: Development of methods for continuous monitoring and evaluation of fairness in machine learning models, and the creation of frameworks for adapting these models to changing environments while maintaining fairness.

## Limitations
- Strong assumptions required: The Archimedean assumption for NCPOP convergence and the unbiased sampling assumption for bias repair may not hold in practice
- Computational scalability: SDP relaxations grow exponentially with problem size, limiting application to large-scale problems
- Model restrictions: Linear dynamical system assumption may be too restrictive for complex real-world forecasting problems

## Confidence
- High confidence: The mathematical formulations for subgroup fairness and instantaneous fairness are well-defined and rigorously derived. The optimal transport framework for bias repair without demographics is theoretically sound.
- Medium confidence: Empirical validation on benchmark datasets demonstrates feasibility, but results may not generalize to all application domains. The computational efficiency claims require further testing on larger-scale problems.
- Low confidence: The global optimality guarantees depend heavily on the Archimedean assumption, which is difficult to verify in practice. The unbiased sampling assumption for bias repair is rarely satisfied in real-world scenarios.

## Next Checks
1. Test the bias repair framework on datasets with known sampling bias to quantify performance degradation when the unbiased sampling assumption is violated.
2. Implement a non-linear dynamical system variant of the fairness formulations to assess the limitation of the linear assumption.
3. Scale the NCPOP formulations to larger time windows and higher-dimensional systems to empirically evaluate computational complexity growth and identify practical limits.