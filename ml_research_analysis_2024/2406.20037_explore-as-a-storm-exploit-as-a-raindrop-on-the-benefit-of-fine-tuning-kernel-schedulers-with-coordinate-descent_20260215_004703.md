---
ver: rpa2
title: 'Explore as a Storm, Exploit as a Raindrop: On the Benefit of Fine-Tuning Kernel
  Schedulers with Coordinate Descent'
arxiv_id: '2406.20037'
source_url: https://arxiv.org/abs/2406.20037
tags:
- search
- ansor
- kernel
- droplet
- kernels
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces DPAnsor, a new kernel scheduling methodology
  that combines Ansor's exploration phase with Droplet Search's exploitation phase.
  The key idea is to first use Ansor to explore different kernel optimization spaces,
  then apply Droplet Search to the best schedule found.
---

# Explore as a Storm, Exploit as a Raindrop: On the Benefit of Fine-Tuning Kernel Schedulers with Coordinate Descent

## Quick Facts
- arXiv ID: 2406.20037
- Source URL: https://arxiv.org/abs/2406.20037
- Reference count: 11
- This paper introduces DPAnsor, combining Ansor's exploration with Droplet Search's exploitation to outperform Ansor in both kernel quality and search time

## Executive Summary
This paper presents DPAnsor, a novel kernel scheduling methodology that combines Ansor's exploration phase with Droplet Search's exploitation phase. The approach addresses the limitations of both methods by using Ansor to identify promising kernel optimization spaces, then applying Droplet Search's coordinate descent to optimize within those spaces. The methodology has been approved for integration into both Ansor and MetaSchedule in TVM, demonstrating practical applicability beyond academic research.

## Method Summary
The methodology first runs Ansor with a limited exploration budget (K samples) to identify the best kernel sketch, then applies Droplet Search's coordinate descent to optimize parameters within that sketch. The approach uses K=300 samples for exploration and N=100 for exploitation, compared against Ansor's baseline of 10,000 samples. DPAnsor is evaluated across 20 deep learning models on four architectures (AMD Ryzen 7, NVIDIA A100, NVIDIA RTX 3080, ARM A64FX), measuring kernel quality and search time.

## Key Results
- DPAnsor achieves better kernels than Ansor with 10,000 samples using only 300 samples, with speedups of 1.02x to 1.59x
- Search time is reduced by factors of 1.18x to 1.25x compared to Ansor
- Performance advantage increases with model size, as Ansor's fixed budget is spread thinner across more layers
- The approach has been approved for integration into both Ansor and MetaSchedule in TVM

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Droplet Search's coordinate descent outperforms Ansor's evolutionary search within a fixed kernel space
- Mechanism: Coordinate descent exploits local convexity and parameter sensitivity, while evolutionary search explores broadly but ignores neighborhood structure
- Core assumption: The kernel optimization space contains locally convex regions where gradient-like descent is effective
- Evidence anchors:
  - [abstract] "Droplet Search's coordinate descent approach is 'hardware centric', while Ansor's genetic algorithm is 'input centric'"
  - [section 2.2] "Droplet Search seeks the optimal configuration of an optimization template by determining a descent direction along the objective function"

### Mechanism 2
- Claim: Combining Ansor's exploration with Droplet Search's exploitation yields better kernels in less time than Ansor alone
- Mechanism: Ansor finds promising kernel spaces (sketches) quickly; Droplet Search then finds optimal parameters within that space efficiently
- Core assumption: The best kernel space identified by Ansor contains the global optimum for that model
- Evidence anchors:
  - [abstract] "By applying this approach to the first 300 kernels that Ansor generates, we usually obtain better kernels in less time than if we let Ansor analyze 10,000 kernels"
  - [section 2.3] "We choose the best schedule (annotated sketch), considering the running time of the end-to-end model"

### Mechanism 3
- Claim: DPAnsor's performance advantage increases with model size
- Mechanism: Larger models have more layers, so Ansor's fixed budget of 10,000 trials is spread thinner, making exploration less effective; Droplet Search can then improve results more significantly
- Core assumption: Ansor allocates trials to layers in a way that benefits smaller models more than larger ones
- Evidence anchors:
  - [section 3.3] "The larger the model, the less samples DPAnsor needs to observe to outperform Ansor, if Ansor uses a budget of 10,000 samples"
  - [section 3.3] "If we consider DenseNet201, which has 113 layers, then Ansor allocates, on average, 104/113 trials per layer"

## Foundational Learning

- Concept: Kernel scheduling and optimization spaces
  - Why needed here: Understanding how different kernel implementations form optimization spaces is fundamental to grasping why the combined approach works
  - Quick check question: What is the difference between a sketch and an annotation in kernel scheduling?

- Concept: Search algorithms (exploration vs exploitation)
  - Why needed here: The paper contrasts Ansor's exploration with Droplet Search's exploitation, so understanding these search paradigms is crucial
  - Quick check question: How does coordinate descent differ from genetic algorithms in terms of search strategy?

- Concept: Statistical significance testing
  - Why needed here: Droplet Search uses t-tests to determine if new kernels are faster, and the paper reports p-values for performance comparisons
  - Quick check question: Why does Droplet Search run each kernel three times before comparing speeds?

## Architecture Onboarding

- Component map:
  Ansor (exploration) -> Droplet Search (exploitation) -> TVM infrastructure (integration)

- Critical path:
  1. Ansor explores kernel spaces with limited budget (e.g., 300 trials)
  2. Best sketch is selected based on end-to-end model performance
  3. Droplet Search optimizes parameters within that sketch until convergence
  4. Results are benchmarked against Ansor with full budget

- Design tradeoffs:
  - Exploration budget vs exploitation quality: Higher exploration budget may find better sketches but reduces time savings
  - Convergence criteria: Stricter criteria may find better kernels but increase search time
  - Hardware awareness: The approach benefits from hardware-specific optimizations but may be less portable

- Failure signatures:
  - Poor kernel quality: Likely due to insufficient exploration budget or bad initial sketch selection
  - Excessive search time: Droplet Search not converging quickly enough, possibly due to poor initial sketch
  - Inconsistent results: Stochastic elements in both Ansor and Droplet Search may cause variability

- First 3 experiments:
  1. Run DPAnsor with K=100, N=25 on a small model (e.g., AlexNet) and compare kernel quality and search time to baseline Ansor
  2. Test DPAnsor with K=300, N=100 on a medium model (e.g., ResNet18) across different architectures
  3. Benchmark DPAnsor against PyTorch and TensorFlow on a GPU for a large model (e.g., DenseNet201)

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the combined approach (DPAnsor) scale with larger models that have more than 100 layers?
- Basis in paper: [explicit] The paper mentions that for very large models, Ansor becomes faster due to longer convergence time for Droplet Search, but does not provide detailed analysis of scalability beyond DenseNet201 (113 layers).
- Why unresolved: The evaluation only covers models up to 113 layers, and the paper does not explore how the approach performs with significantly larger models or different model architectures.
- What evidence would resolve it: Experimental results showing DPAnsor performance on models with 200+ layers, comparing search time and kernel quality against Ansor with varying budgets.

### Open Question 2
- Question: How sensitive is DPAnsor's performance to the choice of exploration budget (K) and exploitation budget (N)?
- Basis in paper: [explicit] The paper explores different values of K (1-1000) and N (300), but does not systematically analyze the sensitivity of performance to these parameters across different architectures and models.
- Why unresolved: While the paper provides results for specific K and N values, it does not investigate how performance varies with different combinations of these parameters or if there are optimal values that depend on model characteristics.
- What evidence would resolve it: Comprehensive experiments varying both K and N systematically across different model types and architectures, analyzing performance trade-offs and identifying optimal parameter ranges.

### Open Question 3
- Question: How does DPAnsor perform on non-ML workloads or different types of tensor computations?
- Basis in paper: [inferred] The paper focuses exclusively on deep learning models from the ONNX model zoo, without exploring other types of tensor computations or general-purpose computational workloads.
- Why unresolved: The evaluation is limited to neural network models, leaving open questions about the approach's effectiveness on other tensor-intensive applications like scientific computing, image processing, or custom computational kernels.
- What evidence would resolve it: Experiments applying DPAnsor to a diverse set of tensor computations beyond ML models, including benchmarks from scientific computing, image processing, and custom kernel suites, comparing performance against Ansor and other autotuning approaches.

## Limitations

- The approach depends heavily on Ansor's initial sketch quality - if exploration fails to identify a good sketch, exploitation cannot compensate
- The method assumes the best sketch from Ansor contains the global optimum, which may not hold for highly non-convex optimization spaces
- Hardware-specific nature of Droplet Search's coordinate descent may limit portability across different architectures

## Confidence

High confidence in the core claim that combining Ansor's exploration with Droplet Search's exploitation yields better kernel quality and faster search times. The experimental results across multiple models and architectures are robust, with speedups ranging from 1.02x to 1.59x.

Medium confidence concerns about the scalability of this approach to extremely large models or different search spaces beyond TVM's current kernel optimization domain.

## Next Checks

1. Test DPAnsor's performance degradation when Ansor's exploration budget is reduced below the proposed 300 trials threshold to quantify the minimum viable exploration requirement.

2. Evaluate the approach on non-standard architectures (e.g., mobile GPUs, custom accelerators) to assess hardware portability and identify architecture-specific failure modes.

3. Implement a variant where Droplet Search can escape from local optima by occasionally switching to broader exploration, measuring whether this hybrid approach maintains speed advantages while improving worst-case performance.