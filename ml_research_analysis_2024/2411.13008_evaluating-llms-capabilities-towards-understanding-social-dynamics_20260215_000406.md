---
ver: rpa2
title: Evaluating LLMs Capabilities Towards Understanding Social Dynamics
arxiv_id: '2411.13008'
source_url: https://arxiv.org/abs/2411.13008
tags:
- social
- llms
- language
- arxiv
- understanding
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This study investigates whether large language models (LLMs) can
  understand and explain social dynamics, specifically cyberbullying and anti-cyberbullying
  behaviors on social media. The research compares various LLMs (GPT-2, Llama-2, ChatGPT)
  across three dimensions: language understanding, directionality detection, and behavior
  classification.'
---

# Evaluating LLMs Capabilities Towards Understanding Social Dynamics

## Quick Facts
- arXiv ID: 2411.13008
- Source URL: https://arxiv.org/abs/2411.13008
- Reference count: 40
- Key outcome: LLMs struggle with semantic comprehension of informal social media language, showing near-random performance in cyberbullying detection despite fine-tuning

## Executive Summary
This study investigates whether large language models (LLMs) can understand and explain social dynamics, specifically cyberbullying and anti-cyberbullying behaviors on social media. The research compares various LLMs (GPT-2, Llama-2, ChatGPT) across three dimensions: language understanding, directionality detection, and behavior classification. Through experiments involving paraphrasing, fine-tuning, and prompt engineering, the study finds that while LLMs show promise in understanding directionality after fine-tuning, they struggle with semantic comprehension of informal social media language. The models perform near-random chance in detecting cyberbullying and anti-bullying instances, highlighting the need for larger informal social language datasets and more robust evaluation methods for social context understanding in LLMs.

## Method Summary
The study employs a multi-phase approach to evaluate LLM capabilities in social dynamics understanding. Researchers use Instagram and 4chan datasets with labeled cyberbullying/anti-bullying instances and directionality information. The methodology includes two-phase fine-tuning using LoRA: Phase 1 for structural understanding with WikiTableQuestions dataset, followed by Phase 2 for social understanding using 4chan data. Models are evaluated through constrained generation with prompt templates, testing paraphrasing quality, directionality identification, and cyberbullying/anti-bullying classification using semantic similarity, BLEU, ROUGE, Jaccard scores, and standard classification metrics.

## Key Results
- Fine-tuned LLMs show improved directionality detection compared to zero-shot prompting
- LLMs perform near-random chance (50-50%) in cyberbullying/anti-bullying classification tasks
- Paraphrasing quality varies significantly across models, with high semantic similarity but low edit distance indicating verbatim reproduction issues

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Fine-tuning LLMs with structured reasoning data (WikiTableQuestions) before social media data improves directional understanding
- Mechanism: Pre-training on structured reasoning tasks builds general reasoning capabilities that transfer to social media directionality detection
- Core assumption: Structural reasoning abilities are generalizable across domains and can be transferred through fine-tuning
- Evidence anchors:
  - [section] "We use the WikiTableQuestions [30] dataset to infuse structural intelligence into the model"
  - [section] "Phase 2 of the PEFT process aims to improve the social understanding of under-privileged LLMs"
  - [corpus] Weak - no corpus data directly supporting this specific mechanism
- Break condition: When the structured reasoning data is too dissimilar from social media context, transfer fails

### Mechanism 2
- Claim: Prompt engineering with exemplars guides generation toward expected formats but doesn't improve semantic understanding
- Mechanism: Exemplars provide templates that shape output structure, but LLMs don't fundamentally improve their understanding
- Core assumption: LLMs can follow format patterns without gaining deeper comprehension
- Evidence anchors:
  - [section] "we only see a significant change in generation statistics for the 7B variant of Llama-2"
  - [section] "exemplars may be insufficient to improve language model understanding, as they only guide the generation process rather than improving the model's inherent understanding"
  - [section] "LLMs show promising prospects for learning directionality when they are trained to do so by fine-tuning"
- Break condition: When exemplars are ambiguous or contradictory, models fail to generate coherent responses

### Mechanism 3
- Claim: Constrained generation templates enable extraction of categorical labels while preventing deviation from expected responses
- Mechanism: Structured prompts with specific output formats force models to provide analyzable responses
- Core assumption: LLMs can follow constrained output formats when properly prompted
- Evidence anchors:
  - [section] "We employ templating of the procedural generation through constraints in the generation process"
  - [section] "Fig. 1 shows one of the sample prompts used in our analysis"
  - [section] "constrained generation for our analysis"
- Break condition: When models refuse to follow constraints due to content policies or misunderstanding

## Foundational Learning

- Concept: Semantic similarity vs. edit distance trade-offs
  - Why needed here: Understanding why high semantic similarity doesn't guarantee good paraphrasing quality
  - Quick check question: If two texts have high semantic similarity but high edit distance, what does this indicate about the model's understanding?

- Concept: Transfer learning through fine-tuning
  - Why needed here: Explains why PEFT improves directionality detection
  - Quick check question: What is the key difference between zero-shot prompting and fine-tuning in terms of model capabilities?

- Concept: Language model limitations with informal text
  - Why needed here: Explains why LLMs struggle with social media discourse
  - Quick check question: Why might LLMs trained on formal text corpora perform poorly on informal social media language?

## Architecture Onboarding

- Component map: Data preprocessing -> Prompt engineering -> Model selection (GPT-2, Llama-2 variants, ChatGPT) -> Constrained generation -> Evaluation metrics (BLEU, ROUGE, Jaccard, semantic similarity, Levenshtein ratio)
- Critical path: Fine-tuning -> Directional understanding testing -> Cyberbullying detection testing -> Evaluation
- Design tradeoffs: Model size vs. performance vs. computational cost; zero-shot vs. fine-tuning vs. prompt engineering approaches
- Failure signatures: High semantic similarity with low edit distance (verbatim reproduction); gibberish generation (high edit distance, low semantic similarity); random performance in classification tasks
- First 3 experiments:
  1. Test paraphrasing quality across all models with and without exemplars using semantic similarity and edit distance metrics
  2. Implement Phase 1 PEFT with WikiTableQuestions and measure directional understanding improvement
  3. Implement Phase 2 PEFT with social media data and test cyberbullying/anti-bullying classification performance

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do different fine-tuning strategies (LoRA vs full fine-tuning) impact LLMs' ability to understand informal social media language compared to formal text?
- Basis in paper: [explicit] The paper discusses using LoRA for fine-tuning and mentions the need for larger informal social language datasets
- Why unresolved: The study only used LoRA fine-tuning and compared base vs fine-tuned models, but did not explore different fine-tuning strategies or their relative effectiveness
- What evidence would resolve it: Direct comparison of LoRA vs full fine-tuning performance on informal social media language understanding tasks, with systematic evaluation of parameter efficiency and performance trade-offs

### Open Question 2
- Question: What specific linguistic features or patterns in social media text most challenge LLMs' semantic understanding capabilities?
- Basis in paper: [inferred] The paper shows LLMs struggle with semantic comprehension of informal social media language but doesn't analyze which specific features cause the most difficulty
- Why unresolved: The study identifies the general problem but doesn't perform detailed linguistic analysis of what makes social media text particularly challenging for LLMs
- What evidence would resolve it: Controlled experiments isolating different linguistic features (slang, abbreviations, sarcasm, context-dependent meanings, etc.) to determine which most impact model performance

### Open Question 3
- Question: How does the performance of LLMs on cyberbullying detection tasks compare to traditional machine learning approaches when trained on equivalent datasets?
- Basis in paper: [explicit] The paper finds LLMs perform near-random chance on cyberbullying detection and mentions the need for larger informal language datasets
- Why unresolved: The study only tested LLMs and didn't benchmark against traditional approaches or analyze what dataset sizes might be needed for effective detection
- What evidence would resolve it: Head-to-head comparison of LLM vs traditional ML approaches on cyberbullying detection across various dataset sizes, with analysis of performance scaling with data quantity

## Limitations

- Dataset Generalization Concerns: Findings based on small, specific datasets (2,000 Instagram comments, 600 4chan threads) may not represent full social media diversity
- Evaluation Metric Adequacy: Automated metrics may not capture nuanced comprehension required for social dynamics understanding
- Model Capability Confounding: Difficulty isolating performance differences due to architecture vs. training data vs. fine-tuning effectiveness

## Confidence

**High Confidence**: The finding that fine-tuning improves directionality detection is well-supported through clear performance improvements demonstrating established transfer learning principles.

**Medium Confidence**: The claim that LLMs struggle with informal social media language semantics is reasonable but could benefit from more diverse testing and larger datasets.

**Low Confidence**: The assertion that prompt engineering with exemplars doesn't improve semantic understanding is based on limited evidence from one model variant without comprehensive testing across different approaches.

## Next Checks

1. **Dataset Expansion and Diversity Test**: Replicate the study using 5-10Ã— larger social media datasets from multiple platforms (Twitter, Reddit, Facebook) with diverse demographic representation to determine if observed limitations persist with more comprehensive training data.

2. **Human Evaluation Validation**: Conduct blind human evaluations comparing LLM-generated responses to ground truth social media posts, measuring human raters' ability to distinguish between human and AI-generated social discourse understanding to validate automated metric findings.

3. **Controlled Transfer Learning Experiment**: Isolate the impact of structured reasoning pre-training by conducting a controlled experiment comparing models fine-tuned only on social media data versus those with structured reasoning pre-training, while holding all other variables constant to definitively establish the transfer mechanism's contribution.