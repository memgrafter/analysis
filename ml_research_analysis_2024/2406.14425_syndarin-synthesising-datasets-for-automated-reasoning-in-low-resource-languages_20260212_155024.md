---
ver: rpa2
title: 'SynDARin: Synthesising Datasets for Automated Reasoning in Low-Resource Languages'
arxiv_id: '2406.14425'
source_url: https://arxiv.org/abs/2406.14425
tags:
- dataset
- arxiv
- language
- question
- armenian
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces SynDARin, a novel method for generating QA
  datasets in low-resource languages by leveraging parallel English-Armenian Wikipedia
  paragraphs, generating English QA pairs via LLM, and validating translations to
  maintain quality. The method produces a 1.2K-sample Armenian QA dataset, validated
  to be 70% filtered of poor-quality translations and 98% answerable for English generation.
---

# SynDARin: Synthesising Datasets for Automated Reasoning in Low-Resource Languages

## Quick Facts
- **arXiv ID**: 2406.14425
- **Source URL**: https://arxiv.org/abs/2406.14425
- **Reference count**: 28
- **Primary result**: A novel pipeline generates 1.2K Armenian QA samples from parallel Wikipedia, validated to be 70% filtered of poor translations and 98% answerable for English generation; LLM benchmarks show sub-70% accuracy, confirming nontrivial reasoning challenge.

## Executive Summary
This paper introduces SynDARin, a method for generating synthetic QA datasets in low-resource languages by leveraging parallel English-Armenian Wikipedia paragraphs, generating English QA pairs via LLM, and validating translations to maintain quality. The approach circumvents manual annotation and translation bias, producing a 1.2K-sample Armenian QA dataset with high answerability and nontrivial reasoning difficulty, as evidenced by benchmark results showing state-of-the-art LLMs achieving sub-70% accuracy.

## Method Summary
The SynDARin pipeline mines parallel English-Armenian Wikipedia paragraphs using length and view count filtering, generates English MC questions via GPT-4 with substring validation, translates QA pairs to Armenian using Google Translate API, and validates translations via fuzzy substring and semantic similarity matching. The method produces a high-quality dataset by filtering out samples below specified thresholds, ensuring both answerability and alignment with the source content.

## Key Results
- **Dataset quality**: 1.2K Armenian QA samples, 70% filtered for poor-quality translations, 98% answerable for English generation.
- **Benchmark performance**: State-of-the-art LLMs achieve sub-70% accuracy, confirming the dataset's nontrivial nature.
- **Method effectiveness**: Successfully circumvents manual annotation and translation bias, offering a scalable pipeline for low-resource language QA dataset creation.

## Why This Works (Mechanism)
### Mechanism 1
- Claim: Parallel paragraph mining using length and view count filtering ensures content alignment between English and Armenian Wikipedia texts.
- Mechanism: By rejecting paragraph pairs whose token count difference exceeds a threshold and whose articles have low view/edit counts, the method preserves naturally aligned, human-curated text pairs.
- Core assumption: Introductory Wikipedia paragraphs on the same topic have similar length and are sufficient for alignment without full translation.
- Evidence anchors:
  - [abstract] "We utilize parallel content mining to obtain human-curated paragraphs between English and the target language."
  - [section] "we found that filtering out the paragraph pairs based on their relative view count and the number of tokens, i.e. length, is sufficient."
  - [corpus] Weak: No direct quantitative validation of alignment quality beyond threshold usage.
- Break condition: If topics diverge structurally or length similarity no longer correlates with semantic alignment, the filter fails.

### Mechanism 2
- Claim: Synthetic question generation via LLM with substring validation produces high-quality, answerable questions.
- Mechanism: The LLM generates MC questions with answers explicitly mentioned in the paragraph; only those where the answer substring is present are retained.
- Core assumption: LLM-generated questions will be answerable if the answer string appears verbatim in the paragraph.
- Evidence anchors:
  - [abstract] "We use the English data as context to generate synthetic multiple-choice (MC) question-answer pairs, which are automatically translated and further validated for quality."
  - [section] "Following Lewis et al. (2021); Agrawal et al. (2023), which have shown successful question-generation results for the English language."
  - [corpus] Weak: No explicit validation that substring presence ensures answerability beyond English human evaluation.
- Break condition: If LLM generates questions where the substring is present but contextually ambiguous, or the answer is misleading, the validation passes incorrectly.

### Mechanism 3
- Claim: Fuzzy substring matching plus semantic similarity filtering reduces hallucinated or mistranslated QA pairs in the target language.
- Mechanism: After translation, only pairs where the translated answer substring is found in the Armenian paragraph and has high semantic similarity are kept.
- Core assumption: Morphological variations in Armenian still allow substring and semantic matching to identify correct answers.
- Evidence anchors:
  - [abstract] "Samples below a certain threshold, F(ai, PArm) ≤ KFuzz and cos(M(ai), M(PArm)) ≤ KSim are filtered out."
  - [section] "To mitigate the inconsistencies introduced during the translation process, we save only the samples where the translated answer ai ∈ KArm is contained within and semantically related to the paragraph PArm."
  - [corpus] Weak: No direct analysis of false positive/negative rates for the fuzzy+semantic filter.
- Break condition: If semantic embeddings are misaligned for low-resource languages or fuzzy matching fails on morphological complexity, incorrect pairs may pass.

## Foundational Learning
- **Named Entity Recognition (NER) and entity typing**
  - Why needed here: To analyze question diversity and ensure the generated questions cover varied entity types (PERSON, LOC, ORG, etc.).
  - Quick check question: What entity type would the question "Who won the Nobel Prize in Physics in 2020?" most likely refer to?
- **Fuzzy string matching and semantic embeddings**
  - Why needed here: For validating translated QA pairs by matching answer substrings and checking semantic similarity.
  - Quick check question: If the Armenian answer is "Գրեմմի կենդանի", which fuzzy matching metric would you use to compare it against "Grammy Awards"?
- **Cross-lingual transfer and zero-shot prompting**
  - Why needed here: To evaluate multilingual LLMs without language-specific fine-tuning, using few-shot or zero-shot prompts.
  - Quick check question: How many in-context examples would you provide in a few-shot prompt for an Armenian QA task?

## Architecture Onboarding
- **Component map**: Parallel data mining -> QA generation -> Translation + validation -> Dataset creation -> LLM benchmarking
- **Critical path**:
  1. Mine parallel paragraphs (en, hy)
  2. Generate 10 QA pairs per paragraph
  3. Filter for substring presence
  4. Translate QA pairs
  5. Validate translation with fuzzy+semantic match
  6. Assemble final dataset
  7. Benchmark LLMs
- **Design tradeoffs**:
  - High recall vs. high precision in paragraph mining (tight thresholds reduce recall)
  - Generation speed vs. quality (more examples increase diversity but cost more API calls)
  - Translation quality vs. speed (Google API is fast but may hallucinate)
- **Failure signatures**:
  - Too few parallel paragraphs → empty dataset
  - High validation rejection → possible translation errors or mismatched content
  - Benchmark results near random → dataset may be too hard or LLMs lack cross-lingual reasoning
- **First 3 experiments**:
  1. Run mining on 10 sample articles, verify length/filtering outputs.
  2. Generate QA for one paragraph, inspect substring filtering manually.
  3. Translate one QA pair, test fuzzy and semantic match thresholds.

## Open Questions the Paper Calls Out
### Open Question 1
- Question: What is the optimal threshold for fuzzy substring matching (F) and semantic similarity (cos(M(ai), M(PArm))) in the translation validation pipeline to maximize the balance between filtering poor-quality translations and preserving high-quality ones?
- Basis in paper: [inferred] The paper mentions using thresholds KFuzz and KSim for filtering, but does not explore optimal values.
- Why unresolved: The paper does not provide a systematic study of how different threshold values affect the quality and quantity of the final dataset.
- What evidence would resolve it: A comprehensive ablation study varying KFuzz and KSim, measuring the impact on dataset size, quality (as assessed by human evaluation), and model performance.

### Open Question 2
- Question: How does the proposed method perform when applied to other low-resource languages with different linguistic characteristics (e.g., agglutinative languages, languages with different scripts)?
- Basis in paper: [explicit] The paper states that the method has only been tested for Armenian and suggests extending the study to more languages.
- Why unresolved: The paper's results are specific to Armenian and do not address the generalizability of the method to other languages.
- What evidence would resolve it: Applying the method to a diverse set of low-resource languages and comparing the results in terms of dataset quality, diversity, and model performance.

### Open Question 3
- Question: What is the impact of using different large language models (LLMs) for question generation on the diversity and quality of the generated questions?
- Basis in paper: [explicit] The paper uses GPT-4 for question generation but does not compare it with other LLMs.
- Why unresolved: The paper does not explore whether the choice of LLM affects the characteristics of the generated dataset.
- What evidence would resolve it: A comparison of datasets generated using different LLMs, assessing the diversity of question types, answerability, and alignment with the source paragraphs.

## Limitations
- **Translation quality dependency**: The pipeline's effectiveness relies heavily on the quality of machine translation for low-resource languages, which may introduce errors or hallucinations.
- **Generalizability concerns**: The method's performance on other low-resource languages with different linguistic characteristics (e.g., agglutinative languages) remains untested.
- **Topic and style bias**: The reliance on parallel Wikipedia content may introduce biases in topic coverage and writing style, limiting dataset diversity.

## Confidence
- **High confidence**: The English QA generation mechanism and substring validation are well-established from prior work and produce high answerability (98%). The pipeline's modular design and clear filtering steps are reproducible.
- **Medium confidence**: The synthetic dataset's nontriviality is supported by LLM benchmark results (sub-70% accuracy), but the specific thresholds and quality control steps for Armenian translation need empirical validation.
- **Low confidence**: The method's effectiveness for other low-resource languages and its ability to avoid topic/style biases are speculative without further experiments.

## Next Checks
1. **Cross-lingual robustness test**: Apply the pipeline to another low-resource language (e.g., Swahili or Bengali) and compare dataset quality and LLM performance to the Armenian results.
2. **Translation quality audit**: Manually annotate a sample of translated QA pairs to measure false positive/negative rates in the fuzzy+semantic matching filter and identify common failure modes.
3. **Topic bias analysis**: Use topic modeling (e.g., BERTopic) to quantify the diversity and representativeness of mined Wikipedia paragraphs and ensure coverage beyond introductory content.