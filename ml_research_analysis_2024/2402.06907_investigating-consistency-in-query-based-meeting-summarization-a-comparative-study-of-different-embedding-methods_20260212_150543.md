---
ver: rpa2
title: 'Investigating Consistency in Query-Based Meeting Summarization: A Comparative
  Study of Different Embedding Methods'
arxiv_id: '2402.06907'
source_url: https://arxiv.org/abs/2402.06907
tags:
- meeting
- summarization
- arxiv
- which
- query
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigates how different embedding methods impact
  query-based meeting summarization consistency. Inspired by the QMSum benchmark,
  the authors build a Locate-then-Summarize framework using three embedding variants
  (BERT, RoBERTa, ELECTRA) to extract relevant spans, which are then summarized using
  BART variants.
---

# Investigating Consistency in Query-Based Meeting Summarization: A Comparative Study of Different Embedding Methods

## Quick Facts
- arXiv ID: 2402.06907
- Source URL: https://arxiv.org/abs/2402.06907
- Authors: Chen Jia-Chen; Guillem Senabre; Allane Caron
- Reference count: 40
- Key outcome: RoBERTa embedding achieves highest ROUGE-L (~16.6) among tested locators, but all lag far behind gold span performance (~26.5), indicating span extraction accuracy as key bottleneck.

## Executive Summary
This study investigates how different embedding methods impact query-based meeting summarization consistency using a Locate-then-Summarize framework. The authors compare BERT, RoBERTa, and ELECTRA embeddings for span extraction, followed by BART-based summarization. Their experiments show that fine-tuning summarizers on the QMSum dataset improves ROUGE-L scores by 65.7% for gold spans. While RoBERTa achieves the best ROUGE-L (~16.6) among locators, all tested methods significantly underperform gold span extraction (~26.5), highlighting the need for improved span prediction accuracy.

## Method Summary
The study employs a Locate-then-Summarize framework using the QMSum dataset with 1808 query-summary pairs across 232 meetings. Three embedding variants (BERT, RoBERTa, ELECTRA) are used in a CNN-based locator model to predict <START, END> span indices for query-relevant content. These spans are then summarized using BART variants, with models fine-tuned on QMSum. The system incorporates auxiliary inputs (meeting length and cosine similarity) to reduce out-of-range predictions. Evaluation uses ROUGE metrics to assess summary quality and consistency with gold spans.

## Key Results
- Fine-tuning summarizers on QMSum improves ROUGE-L scores by 65.7% for gold spans
- RoBERTa embedding achieves highest ROUGE-L (~16.6) among tested locators
- All locator methods significantly underperform gold span extraction (~26.5 ROUGE-L)
- Out-of-range span predictions remain a challenge despite auxiliary input mitigation

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Fine-tuning summarizers on QMSum improves ROUGE-L scores by ~65.7% for gold spans.
- Mechanism: Domain-specific fine-tuning adapts general-purpose models to the domain-specific language and structure of meeting transcripts, increasing overlap between generated and reference summaries.
- Core assumption: The QMSum dataset captures domain-specific patterns not present in general summarization datasets.
- Evidence anchors: [abstract] "fine-tuning summarizers on the QMSum dataset improves ROUGE-L scores by 65.7% for gold spans"
- Break condition: If domain shift between pre-training corpus and QMSum is too large, fine-tuning may overfit or fail to generalize.

### Mechanism 2
- Claim: Adding meeting length and cosine similarity as auxiliary inputs improves locator span predictions and reduces out-of-range errors.
- Mechanism: Auxiliary features provide explicit global context about meeting size and relevance signal, preventing locator from predicting spans outside valid ranges.
- Core assumption: Locator cannot infer meeting length or similarity implicitly from input representation alone.
- Evidence anchors: [section] "we find that the predicted results are quite unreasonable, some of <START, END> are even out of the range of meeting length...we decide to concatenate the meeting length and similarity value"
- Break condition: If similarity function or length encoding is noisy or poorly scaled, auxiliary features could mislead locator.

### Mechanism 3
- Claim: RoBERTa embedding yields highest ROUGE-L (~16.6) among three tested locators.
- Mechanism: RoBERTa's improved pretraining objectives and training data produce better contextualized representations for multi-domain, multi-party nature of meetings.
- Core assumption: RoBERTa's architectural and training differences over BERT and ELECTRA translate into better span extraction for meetings.
- Evidence anchors: [abstract] "Across locators, RoBERTa achieves the best ROUGE-L (~16.6)"
- Break condition: If meeting transcripts share characteristics with domains where BERT or ELECTRA excel, ranking could reverse.

## Foundational Learning

- Concept: Transformer-based embeddings and their contextualization
  - Why needed here: Model relies on BERT, RoBERTa, and ELECTRA to represent utterances and queries before span prediction.
  - Quick check question: How does self-attention in transformers differ from static embeddings like word2vec?

- Concept: Max-pooling for sentence representation
  - Why needed here: Locator averages word embeddings to represent utterances and applies max-pooling to compress CNN outputs.
  - Quick check question: What is the effect of max-pooling vs mean-pooling on preserving salient token features?

- Concept: ROUGE metric variants (R-1, R-2, R-L)
  - Why needed here: ROUGE scores are primary evaluation metric; R-L focuses on longest common subsequence, matching word order.
  - Quick check question: Why might R-L be preferred over R-1/R-2 for assessing summary coherence?

## Architecture Onboarding

- Component map: Preprocessor → Embedding (BERT/RoBERTa/ELECTRA) → CNN encoder → Concat (utterance + query + length + similarity) → MLP → <START,END> span → Extractor → BART summarizer → ROUGE evaluation
- Critical path: Embedding → CNN → MLP → Span prediction → Summary generation
- Design tradeoffs:
  - Span-level vs full-meeting summarization: Span extraction reduces input length but may miss cross-span context
  - Fine-tuning vs frozen embeddings: Fine-tuning improves consistency but risks overfitting on small QMSum
  - Embedding choice: RoBERTa yields better spans but may be slower; BERT is lighter but weaker
- Failure signatures:
  - Out-of-range spans: Locator fails to constrain predictions to valid meeting indices
  - Low ROUGE-L with gold spans: Summarizer cannot reconstruct query-relevant content
  - Large gap between gold and located spans: Locator accuracy is the bottleneck
- First 3 experiments:
  1. Run locator with each embedding variant on validation set; record out-of-range rate and ROUGE-L on gold spans
  2. Fine-tune BART on QMSum; compare ROUGE-L with and without fine-tuning on gold spans
  3. Compare span coverage and summary coherence when using random vs learned locator spans

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do different embedding methods impact consistency of query-based meeting summarization beyond current scope of ROUGE metrics?
- Basis in paper: [explicit] Paper investigates impact of different embedding methods on consistency and suggests ROUGE scores alone may not be sufficient for evaluating consistency
- Why unresolved: Study primarily uses ROUGE scores but acknowledges these may not comprehensively capture consistency between summaries and queries
- What evidence would resolve it: Conducting further studies using alternative metrics for consistency evaluation and comparing results with ROUGE scores

### Open Question 2
- Question: What are specific causes of out-of-range span predictions in locator model, and how can they be mitigated?
- Basis in paper: [explicit] Paper identifies predicted indexes sometimes fall out of range of meeting length, affecting locator's performance
- Why unresolved: Paper does not provide detailed analysis of root causes or propose specific solutions
- What evidence would resolve it: Analyzing locator's predictions in detail to identify patterns leading to out-of-range spans and developing targeted prevention strategies

### Open Question 3
- Question: How does fine-tuning summarizer models on QMSum dataset specifically improve consistency of generated summaries?
- Basis in paper: [explicit] Paper reports fine-tuning improves ROUGE scores by 65.7% for gold spans, indicating enhancement in summary consistency
- Why unresolved: Paper does not provide detailed explanation of how fine-tuning contributes to improved consistency or specific mechanisms involved
- What evidence would resolve it: Conducting experiments comparing performance of fine-tuned and non-fine-tuned models on various aspects of summary quality

## Limitations
- Relatively small QMSum dataset (1808 query-summary pairs) may limit generalizability across different meeting domains
- Span extraction approach may miss important cross-span context essential for comprehensive summaries
- Study focuses on three specific embedding variants without exploring other potentially effective models or newer architectures

## Confidence
- Fine-tuning improves ROUGE-L scores: **High** - directly supported by experimental results
- Auxiliary inputs reduce out-of-range predictions: **Medium** - based on observed improvements but lacking ablation
- RoBERTa yields best ROUGE-L among tested locators: **Medium** - consistent with results but not rigorously validated

## Next Checks
1. **Ablation study on fine-tuning**: Compare BART performance on QMSum using different pre-training approaches to isolate contribution of domain adaptation

2. **Cross-domain robustness testing**: Evaluate Locate-then-Summarize framework on meeting datasets from different domains to assess generalizability beyond QMSum

3. **Alternative span prediction strategies**: Implement and compare attention-based or transformer-only locators against current CNN+MLP approach to determine if architectural changes could reduce gold-vs-located span performance gap