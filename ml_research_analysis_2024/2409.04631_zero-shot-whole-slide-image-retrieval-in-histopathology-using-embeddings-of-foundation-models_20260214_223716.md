---
ver: rpa2
title: Zero-Shot Whole Slide Image Retrieval in Histopathology Using Embeddings of
  Foundation Models
arxiv_id: '2409.04631'
source_url: https://arxiv.org/abs/2409.04631
tags:
- carcinoma
- cell
- yottixel
- table
- adenocarcinoma
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'The study evaluated zero-shot whole slide image retrieval using
  embeddings from foundation models in histopathology. Five models were tested: DenseNet,
  UNI, Virchow, and GigaPath (patch-based) and GigaPath WSI (whole-slide).'
---

# Zero-Shot Whole Slide Image Retrieval in Histopathology Using Embeddings of Foundation Models

## Quick Facts
- arXiv ID: 2409.04631
- Source URL: https://arxiv.org/abs/2409.04631
- Reference count: 0
- Primary result: Zero-shot whole slide image retrieval using foundation model embeddings achieved F1 scores of 27% to 42% for top-5 retrieval across 23 organs and 117 cancer subtypes, indicating current models are not yet clinically adequate.

## Executive Summary
This study evaluates zero-shot whole slide image retrieval in histopathology using embeddings from five foundation models: DenseNet, UNI, Virchow, GigaPath (patch-based), and GigaPath WSI (whole-slide). Using the Yottixel search engine and the TCGA dataset of 11,444 WSIs across 23 organs and 117 cancer subtypes, the study found moderate retrieval performance with top-5 macro-average F1 scores ranging from 27% (DenseNet) to 42% (UNI, Virchow, GigaPath). The results suggest that current foundation models, despite being trained on large histopathology datasets, are not yet adequate for clinical whole-slide retrieval tasks without further refinement.

## Method Summary
The study employs a patch-based pipeline where WSIs are segmented into color regions, and 2% of representative patches are selected at 224x224 resolution. Five pre-trained foundation models generate embeddings for these patches, which are then aggregated using a median-of-minimums strategy into a single WSI embedding. The Yottixel search engine performs retrieval using Hamming distance. Models are evaluated in a zero-shot manner without fine-tuning, measuring macro-average F1 scores for top-1, majority of top-3, and majority of top-5 retrieval accuracy across TCGA's 23 organs and 117 cancer subtypes.

## Key Results
- Top-5 retrieval F1 scores ranged from 27% (DenseNet) to 42% (UNI, Virchow, GigaPath)
- Aggregation method impacts performance: GigaPath-WSI showed improvements but was computationally prohibitive
- Zero-shot retrieval performance remains insufficient for clinical application across all tested models

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Zero-shot retrieval performance is limited because current foundation models are not trained end-to-end for WSI-level classification, only for patch-level representation.
- Mechanism: The Yottixel system uses a patch-based pipeline: WSIs are segmented into color regions, representative patches are selected, and embeddings are computed per patch. The median-of-minimums strategy aggregates these into a single WSI embedding. This design forces models optimized for patch-level tasks to perform WSI-level retrieval without task-specific adaptation.
- Core assumption: Patch-level embeddings capture sufficient global context for accurate WSI retrieval without further aggregation or fine-tuning.
- Evidence anchors:
  - [abstract] reports F1 scores of 27% to 42% for top-5 retrieval, indicating low performance.
  - [section] explains Yottixel's median-of-minimums scheme and that models are evaluated in a "zero-shot" manner without altering embeddings or training classifiers.
- Break condition: If aggregation strategies (e.g., mean, attention, or end-to-end fine-tuning) consistently outperform median-of-minimums, the current assumption fails.

### Mechanism 2
- Claim: The semantic gap persists because the embedding space learned by general-purpose or even histopathology-specific models does not align with clinically relevant diagnostic subtypes.
- Mechanism: Models like UNI, Virchow, and GigaPath are pretrained on large-scale histopathology datasets, but their embedding objectives (self-supervised learning) prioritize generic visual similarity over clinically meaningful distinctions. Retrieval accuracy suffers when the embedding geometry does not reflect the nuanced differences between cancer subtypes.
- Core assumption: Self-supervised pretraining on large histopathology datasets produces embeddings that are semantically aligned with clinical diagnostic criteria.
- Evidence anchors:
  - [section] states foundation models "hold great promise in addressing the long-standing semantic gap problem," yet results show "current performance ... is not yet adequate for clinical application."
  - [section] notes that "self-supervised learning" was used in training UNI, Virchow, and GigaPath.
- Break condition: If embeddings from these models cluster clinically similar cases tightly and clinically distinct cases far apart in retrieval benchmarks, the assumption holds; otherwise, it fails.

### Mechanism 3
- Claim: Computational constraints necessitate patch-based analysis, but this introduces information loss that degrades retrieval performance compared to holistic WSI analysis.
- Mechanism: The Yottixel pipeline selects only 2% of representative patches at 224x224 resolution due to hardware limits. This sparse sampling may miss critical diagnostic features present in other regions, leading to lower F1 scores.
- Core assumption: A small, fixed percentage of representative patches is sufficient to represent the diagnostic content of a WSI.
- Evidence anchors:
  - [section] explains that 2% of patches are used instead of the 5% default, and that "computational burden" is reduced.
  - [section] notes GigaPath WSI results are delayed "due to the significant computational resources required for processing."
- Break condition: If increasing patch sampling or resolution consistently improves retrieval performance, the current assumption is invalid.

## Foundational Learning

- Concept: Zero-shot learning
  - Why needed here: The study evaluates models without fine-tuning, relying solely on pre-trained embeddings for retrieval.
  - Quick check question: What is the difference between zero-shot and few-shot learning in the context of histopathology embeddings?

- Concept: Self-supervised learning
  - Why needed here: Models like UNI, Virchow, and GigaPath use self-supervised pretraining (DINOv2, masked autoencoders) to learn representations without labeled data.
  - Quick check question: How does self-supervised pretraining differ from supervised pretraining in foundation models for histopathology?

- Concept: Aggregation strategies for WSI embeddings
  - Why needed here: The median-of-minimums method aggregates patch embeddings into a single WSI vector; understanding alternatives is key to improving performance.
  - Quick check question: What are common aggregation methods for patch-level embeddings in WSI analysis, and how might they affect retrieval accuracy?

## Architecture Onboarding

- Component map: Yottixel (search engine) -> Patching module -> Foundation model (UNI/Virchow/GigaPath) -> Embedding aggregation -> Hamming distance search
- Critical path: Patch extraction -> Embedding generation -> Aggregation -> Similarity search
- Design tradeoffs: Patch sampling rate vs. computational cost; embedding size vs. storage; zero-shot vs. fine-tuned performance
- Failure signatures: Low F1 scores; poor retrieval of rare subtypes; high variance across organs
- First 3 experiments:
  1. Increase patch sampling from 2% to 5% and measure F1 score change.
  2. Replace median-of-minimums with mean or attention-based aggregation and compare retrieval accuracy.
  3. Fine-tune a foundation model on a subset of TCGA subtypes and evaluate zero-shot vs. fine-tuned performance.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What specific architectural or training modifications to foundation models could improve their zero-shot WSI retrieval performance beyond the current 27-42% F1 scores?
- Basis in paper: [explicit] The paper notes that current foundation models show low performance (27-42% F1) and that "significant refinements and alternative strategies are still needed to achieve reliable and accurate cancer tissue subtyping through image search and retrieval"
- Why unresolved: The study only tested existing foundation models without exploring architectural modifications or alternative training approaches specifically designed for WSI retrieval tasks
- What evidence would resolve it: Comparative studies testing modified architectures (e.g., attention mechanisms for whole-slide context, hierarchical feature extraction) or specialized training objectives designed for WSI retrieval tasks, showing measurable improvements in F1 scores

### Open Question 2
- Question: How would alternative WSI representation methods (beyond patch-based approaches) affect retrieval performance for whole slide images?
- Basis in paper: [explicit] The paper states "The challenge of representing an entire WSI with a single vector remains unresolved in computational pathology" and discusses hardware constraints forcing patch-based approaches
- Why unresolved: The study relied on patch-based methods and aggregation strategies due to computational limitations, but advances in hardware and model capacity could enable direct WSI processing
- What evidence would resolve it: Direct comparison of retrieval performance using new methods that process entire WSIs (e.g., hierarchical transformers, multi-resolution encoders) versus traditional patch-based approaches on the same dataset

### Open Question 3
- Question: What aggregation strategies for patch embeddings would optimize WSI-level representation learning for retrieval tasks?
- Basis in paper: [explicit] The paper mentions "The aggregation technique used in the GigaPath-WSI model shows some improvements" but was limited by computational resources, and questions remain about optimal WSI representation
- Why unresolved: Current aggregation methods (median-of-minimums, simple averaging) were used without systematic evaluation of alternative strategies for combining patch embeddings into meaningful WSI representations
- What evidence would resolve it: Comparative evaluation of different aggregation methods (attention-based pooling, clustering-based aggregation, transformer-based fusion) on retrieval performance metrics across multiple WSI datasets

## Limitations
- Computational and methodological constraints: Fixed 2% patch sampling and median-of-minimums aggregation may not capture all diagnostic features
- Generalizability concerns: Performance may vary on external datasets or rare subtypes; zero-shot approach may not be clinically viable
- Implementation barriers: Exact Yottixel codebase and hardware requirements for GigaPath WSI processing are not provided

## Confidence
- High Confidence: Reported retrieval F1 scores (27%-42%) and zero-shot evaluation methodology are well-supported
- Medium Confidence: Patch-level embeddings and aggregation strategies are plausible limiting factors but require further validation
- Low Confidence: Assumption that self-supervised pretraining produces clinically aligned embeddings is not directly tested

## Next Checks
1. Replace median-of-minimums aggregation with mean or attention-based methods and measure F1 score impact
2. Increase patch sampling from 2% to 5% and evaluate retrieval performance changes
3. Fine-tune a foundation model on a subset of TCGA subtypes and compare zero-shot vs. fine-tuned accuracy