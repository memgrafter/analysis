---
ver: rpa2
title: 'CORE: Towards Scalable and Efficient Causal Discovery with Reinforcement Learning'
arxiv_id: '2401.16974'
source_url: https://arxiv.org/abs/2401.16974
tags:
- causal
- learning
- variables
- core
- policy
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces CORE, a deep reinforcement learning approach
  for causal discovery that learns to sequentially reconstruct causal graphs while
  performing informative interventions. CORE addresses the scalability and generalization
  challenges in causal discovery by learning a policy that operates on partially observable
  Markov decision processes, jointly optimizing intervention selection and graph estimation
  through a dual Q-learning setup.
---

# CORE: Towards Scalable and Efficient Causal Discovery with Reinforcement Learning

## Quick Facts
- arXiv ID: 2401.16974
- Source URL: https://arxiv.org/abs/2401.16974
- Reference count: 40
- Authors: Andreas W. M. Sauter; Nicolò Botteghi; Erman Acar; Aske Plaat

## Executive Summary
CORE introduces a deep reinforcement learning approach for causal discovery that learns to sequentially reconstruct causal graphs while performing informative interventions. The method addresses scalability and generalization challenges by jointly optimizing intervention selection and graph estimation through a dual Q-learning setup. CORE demonstrates successful generalization to unseen causal structures of up to 10 variables, achieving lower structural Hamming distances than state-of-the-art methods while requiring only 15 data samples per graph.

## Method Summary
CORE frames causal discovery as a sequential decision-making problem within a partially observable Markov decision process. The agent learns to select interventions and update the causal graph structure through a dual Q-network architecture - one network for intervention actions and another for structural updates. The method uses dense rewards based on edge manipulations rather than sparse terminal rewards, enabling more efficient learning. CORE is trained on synthetic structural causal models with known structures and evaluated on held-out test sets, demonstrating superior performance in both structure estimation accuracy and sample efficiency compared to existing approaches.

## Key Results
- CORE achieves lower structural Hamming distances than state-of-the-art methods on synthetic causal discovery tasks
- The method successfully generalizes to unseen causal structures of up to 10 variables
- CORE requires only 15 data samples per graph while maintaining high accuracy
- The approach scales to larger graphs where previous methods become computationally infeasible

## Why This Works (Mechanism)

### Mechanism 1
- Claim: CORE learns to perform informative interventions by jointly optimizing intervention selection and graph estimation.
- Mechanism: The dual Q-learning setup uses two separate networks - one for intervention actions and one for structural update actions - with a dense reward signal based on edge manipulations.
- Core assumption: Interventions that lead to better structural updates will receive higher rewards, driving the policy to learn informative interventions.
- Evidence anchors:
  - [abstract]: "CORE learns to sequentially reconstruct causal graphs from data while learning to perform informative interventions."
  - [section 3.2]: "we consider the SHD as a natural candidate for our reward function" and the dense reward formulation based on edge manipulations.
  - [corpus]: Weak - related work focuses on causal discovery but doesn't explicitly discuss this dual optimization mechanism.
- Break condition: If the reward signal becomes too sparse or uninformative, the policy may fail to learn which interventions are most beneficial.

### Mechanism 2
- Claim: CORE generalizes to unseen causal structures by learning a policy that doesn't impose a specific algorithm for causal discovery.
- Mechanism: By training on a diverse set of synthetic SCMs with known structures, the policy learns generalizable patterns rather than memorizing specific structures.
- Core assumption: The training data distribution sufficiently covers the space of possible causal structures the policy will encounter during testing.
- Evidence anchors:
  - [abstract]: "CORE generalizes to unseen graphs and efficiently uncovers causal structures."
  - [section 4.1]: "We train our model on a training set of SCMs with known causal structures and evaluate it on SCMs with causal structures that were not seen during training."
  - [section 6.1]: Discussion of transferability limitations when testing on function classes different from training.
- Break condition: If the test function class is too different from training (e.g., linear vs interaction), performance degrades significantly.

### Mechanism 3
- Claim: CORE scales to larger graphs by imposing additional structure on the policy and using more efficient reward formulations.
- Mechanism: Separate networks for intervention and structural decisions, dense rewards, and direct history input (rather than LSTM) enable efficient learning even as graph size increases.
- Core assumption: The computational overhead of separate networks and dense rewards is outweighed by the benefits in learning efficiency.
- Evidence anchors:
  - [section 4.3]: "Addressing the structural actions and the intervention actions with separate networks makes learning the corresponding Q-functions more efficient."
  - [section 4.3]: "Representing the reward densely instead of a summary at the end of each episode often improves performance."
  - [section 3.3]: Description of the dual Q-network architecture.
- Break condition: As graph size grows beyond 10 nodes, the action space may become too large even with masking, or the training data may not cover the expanded structure space.

## Foundational Learning

- Concept: Partially Observable Markov Decision Processes (POMDPs)
  - Why needed here: The causal discovery process involves partial observability - the agent only sees observations from the current SCM, not the full state.
  - Quick check question: How does the agent maintain state information across steps when it can't directly observe the SCM state?

- Concept: Temporal Difference Learning and Deep Q-Networks
  - Why needed here: The agent needs to learn value functions for state-action pairs to make sequential decisions about interventions and structure updates.
  - Quick check question: What role does the target network play in stabilizing the Q-learning process?

- Concept: Structural Hamming Distance (SHD)
  - Why needed here: SHD provides a metric for evaluating how close the estimated causal graph is to the true graph, which is essential for the reward function.
  - Quick check question: How does the dense reward formulation based on edge manipulations relate to the overall SHD between estimated and true graphs?

## Architecture Onboarding

- Component map:
  - Environment: Generates SCMs with known causal structures
  - Agent: Dual Q-networks (intervention Q-in and structure Q-st)
  - Observation: Sampled values from current SCM's post-interventional distribution
  - Action space: Multi-discrete - intervention targets + structural operations
  - Reward: Dense signal based on edge additions/removals vs ground truth

- Critical path:
  1. Sample new SCM from training set at episode start
  2. Agent observes current state (history + sample)
  3. Agent selects intervention and structural actions
  4. Environment applies intervention, generates new sample
  5. Reward computed based on structural update
  6. Agent updates Q-networks via temporal difference learning
  7. Repeat until episode ends

- Design tradeoffs:
  - Separate networks for intervention vs structure: Better representation but more parameters
  - Dense vs sparse rewards: Faster learning but potentially noisier signal
  - Direct history input vs LSTM: Simpler architecture but may miss long-term dependencies

- Failure signatures:
  - Agent keeps performing same interventions without structural progress
  - Q-values become NaN or explode during training
  - Performance plateaus early without approaching ground truth SHD

- First 3 experiments:
  1. Run with random policy (no learning) to establish baseline SHD
  2. Train on small graphs (3-4 nodes) and verify convergence to near-zero SHD
  3. Test generalization by evaluating on held-out graphs of same size as training

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of CORE change when applied to causal discovery in real-world datasets with hidden confounders and soft interventions?
- Basis in paper: [inferred] The paper acknowledges limitations in the presence of unobserved confounders and the assumption of being able to intervene on any variable with any target value.
- Why unresolved: The current implementation of CORE does not address the challenges posed by hidden confounders or soft interventions, which are common in real-world scenarios.
- What evidence would resolve it: Testing CORE on real-world datasets known to have hidden confounders and soft interventions, and comparing its performance to other methods that can handle such complexities.

### Open Question 2
- Question: Can CORE be adapted to handle larger causal graphs, such as those with thousands of variables, without significant loss in accuracy or computational efficiency?
- Basis in paper: [explicit] The paper notes that many applications involve up to 5000 variables, but CORE is currently limited to graphs with up to 10 variables.
- Why unresolved: The scalability of CORE to larger graphs is not demonstrated, and it is unclear how the method would perform or if it would require significant modifications.
- What evidence would resolve it: Scaling up CORE to handle graphs with thousands of variables and evaluating its performance and computational requirements in comparison to existing methods.

### Open Question 3
- Question: How does the choice of function class during training affect CORE's ability to generalize to different types of causal relationships in unseen data?
- Basis in paper: [explicit] The paper discusses the impact of training on different function classes (linear, linear + noise, interaction) and how it affects CORE's performance on unseen graphs.
- Why unresolved: The transferability of CORE across various function classes is not fully understood, and it is unclear how to choose the optimal function class for training to maximize generalizability.
- What evidence would resolve it: Conducting extensive experiments with CORE trained on a wide range of function classes and testing its performance on diverse real-world datasets to identify the most effective training strategy.

## Limitations

- CORE's performance degrades when tested on function classes different from those used during training, indicating limited transferability across function class complexity
- The method currently cannot handle hidden confounders or soft interventions, which are common in real-world scenarios
- Computational requirements are substantial, with 90 million training steps needed for 10-variable graphs

## Confidence

**High Confidence**: The core claim that CORE can perform causal discovery through reinforcement learning is well-supported by experimental results showing consistent improvement in SHD scores over baseline methods. The technical implementation details, including the dual Q-network architecture and POMDP formulation, are clearly specified and theoretically sound.

**Medium Confidence**: The generalization claims are supported by held-out test sets, but the transferability limitations to different function classes indicate that generalization may be more limited than initially suggested. The comparison with state-of-the-art methods is comprehensive within the synthetic domain, but real-world applicability remains to be demonstrated.

**Low Confidence**: The scalability claims beyond 10 variables are extrapolations not directly tested in the paper. The computational efficiency claims relative to existing methods may not hold for substantially larger graphs or different computational environments.

## Next Checks

1. **Function Class Transferability Test**: Systematically vary the function class complexity (e.g., linear → quadratic → interaction terms) in both training and testing to map the boundaries of CORE's generalization capabilities and identify the minimal function complexity required for robust performance.

2. **Real-World Dataset Evaluation**: Apply CORE to established benchmark datasets with known causal structures (e.g., ALARM network) to validate performance beyond synthetic data and assess sensitivity to noise and confounding present in real-world measurements.

3. **Training Efficiency Optimization**: Compare CORE's learning curves against alternative reward formulations (e.g., terminal-only vs dense rewards) and architectural modifications (e.g., LSTM vs direct history input) to identify the most computationally efficient configuration that maintains or improves performance.