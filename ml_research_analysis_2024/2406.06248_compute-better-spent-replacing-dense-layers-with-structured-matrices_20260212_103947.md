---
ver: rpa2
title: 'Compute Better Spent: Replacing Dense Layers with Structured Matrices'
arxiv_id: '2406.06248'
source_url: https://arxiv.org/abs/2406.06248
tags:
- dense
- learning
- matrices
- structured
- compute
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work investigates replacing dense matrices with structured
  matrices in foundation models to improve computational efficiency. The authors systematically
  explore various structured matrix types (low-rank, convolution, Kronecker, Monarch,
  Tensor-Train, and a novel Block Tensor-Train) and identify crucial initialization
  and learning rate scaling differences between structured and dense matrices.
---

# Compute Better Spent: Replacing Dense Layers with Structured Matrices

## Quick Facts
- arXiv ID: 2406.06248
- Source URL: https://arxiv.org/abs/2406.06248
- Authors: Shikai Qiu, Andres Potapczynski, Marc Finzi, Micah Goldblum, Andrew Gordon Wilson
- Reference count: 39
- Primary result: Block Tensor-Train matrices achieve better scaling laws than dense matrices with 3.8× less compute on ImageNet

## Executive Summary
This work systematically investigates replacing dense matrices with structured matrices in foundation models to improve computational efficiency. The authors explore various structured matrix types including low-rank, convolution, Kronecker, Monarch, Tensor-Train, and a novel Block Tensor-Train (BTT), identifying crucial initialization and learning rate scaling differences between structured and dense matrices. They propose a structure-aware learning rate scaling technique based on Maximal Update Parameterization (μP) that enables effective training of structured layers. Through extensive experiments on CIFAR-10/100 and ImageNet, they show that structured matrices can achieve better scaling laws than dense matrices, with BTT achieving the best performance across all tested structured matrix types.

## Method Summary
The authors propose replacing dense linear layers with structured matrices that maintain computational efficiency while preserving expressiveness. They systematically evaluate six structured matrix families, with their novel Block Tensor-Train (BTT) achieving superior scaling laws. The key innovation is structure-aware learning rate scaling based on μP theory, which accounts for the fact that structured matrices have smaller underlying parameter tensors than their output dimensions. The methodology includes careful initialization scaling and weight normalization for BTT layers to prevent activation growth in transformer models.

## Key Results
- BTT achieves exponentially lower training loss than dense matrices on CIFAR-10/100 with augmentation
- On ImageNet-1k, BTT matches dense ViT-S/32 performance with 3.8× less compute
- Weight normalization is critical for stabilizing training with BTT layers in transformer models
- Structured matrices without parameter sharing (BTT, low-rank, convolution, Monarch) consistently outperform those with sharing (Kronecker, Tensor-Train)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Structure-aware learning rates are essential for training structured matrices effectively.
- Mechanism: Structured matrices have smaller underlying parameter tensors than their output dimensions, requiring scaled learning rates per component to maximize feature learning.
- Core assumption: Spectral norm considerations apply independently to each dense component within a structured matrix.
- Evidence anchors:
  - [abstract]: "Using insights from the Maximal Update Parameterization, we determine the optimal scaling for initialization and learning rates of these unconventional layers."
  - [section]: "We show that structured layers often require drastically different learning rates and initialization scales compared to their dense counterparts, because their underlying trainable parameter matrices tend to be much smaller in size than the width of the layer."
  - [corpus]: No direct evidence - corpus focuses on other structured matrix approaches but doesn't address learning rate scaling specifically.

### Mechanism 2
- Claim: Parameter sharing in structures like Kronecker and Tensor-Train leads to worse scaling laws than structures with parameters equal to FLOPs.
- Mechanism: Parameter sharing reduces the number of independent parameters per unit compute, limiting expressiveness growth as compute increases.
- Core assumption: The intrinsic dimension of the data manifold theory doesn't fully explain the observed structure-dependence of scaling exponents.
- Evidence anchors:
  - [section]: "Structures that do not share parameters are more flexible per unit of compute, and consistently achieve better scaling laws."
  - [section]: "Parameters equal FLOPs leads to better scaling laws than those with parameter sharing (Kronecker and TT) under-perform dense matrices in our experiments."
  - [corpus]: No direct evidence - corpus doesn't compare parameter sharing effects on scaling laws.

### Mechanism 3
- Claim: BTT achieves better scaling laws than dense matrices by trading compute per dimension for more dimensions.
- Mechanism: By reducing compute per dimension through lower rank or fewer blocks, BTT can increase model width while maintaining total compute, creating more expressive models.
- Core assumption: The optimal compute per dimension is significantly less than the dimension itself for the tasks studied.
- Evidence anchors:
  - [section]: "Less compute per dimension is more compute-efficient on CIFAR-10. BTT with a lower rank achieves lower train error per FLOP."
  - [section]: "Similarly, in Figure 5b, Monarch matrices with more blocks and higher sparsity are more compute-efficient."
  - [corpus]: No direct evidence - corpus doesn't discuss the compute per dimension trade-off.

## Foundational Learning

- Concept: Maximal Update Parameterization (μP)
  - Why needed here: μP provides the theoretical foundation for scaling initialization and learning rates as models increase in width, which is critical for structured matrices.
  - Quick check question: Why does μP require smaller initialization variance when input dimension is larger than output dimension compared to conventional methods?

- Concept: Spectral norm and its role in feature learning
  - Why needed here: Understanding spectral norm is crucial for determining appropriate learning rates for structured matrices, as the analysis decomposes structured layers into dense components.
  - Quick check question: How does the spectral norm of a weight matrix relate to the size and update rate of its output features?

- Concept: Structured matrix decompositions (Kronecker, Tensor-Train, Monarch)
  - Why needed here: These decompositions form the basis of the alternative linear layers studied, and understanding their computational properties is essential for implementation and optimization.
  - Quick check question: What is the computational complexity difference between dense matrix-vector multiplication and its structured counterparts?

## Architecture Onboarding

- Component map:
  Linear Operator abstractions (CoLA) -> BTT implementation with rank and core parameters -> Weight normalization -> Structure-aware learning rate scaling -> CIFAR-10/100 and ImageNet datasets

- Critical path:
  1. Implement BTT matrix operations using Linear Operator abstractions
  2. Apply structure-aware learning rate scaling per μP
  3. Train models with BTT layers and weight normalization
  4. Measure scaling laws by varying model width
  5. Compare performance against dense baselines

- Design tradeoffs:
  - Higher BTT rank increases expressiveness but reduces compute efficiency
  - More cores in BTT can improve expressiveness but increase memory cost
  - Weight normalization prevents divergence but adds parameters
  - Structure-aware learning rates require component decomposition but enable effective training

- Failure signatures:
  - Vanishing or exploding activations during training
  - Poor performance despite correct implementation (likely learning rate issue)
  - Memory overflow with high-rank or multi-core BTT
  - Runtime inefficiencies for small matrix sizes

- First 3 experiments:
  1. Implement 2-core BTT with rank 1 and verify matrix-vector multiplication correctness
  2. Train a simple MLP on CIFAR-10 with BTT layers using structure-aware learning rates
  3. Compare scaling laws of BTT vs dense MLPs by varying width and measuring training loss

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the theoretical basis for when structured matrices will have better scaling laws than dense matrices, and can this be predicted from data and model characteristics?
- Basis in paper: [inferred] The paper mentions this is an exciting direction for future work, noting that their findings are empirical and a theoretical understanding is needed.
- Why unresolved: The paper relies on empirical observations of scaling laws without providing a theoretical framework to predict when structured matrices will outperform dense matrices.
- What evidence would resolve it: A mathematical model or theorem that relates data manifold properties, model architecture, and matrix structure to predict scaling law improvements.

### Open Question 2
- Question: How does the optimal compute per dimension (ξ) scale with model width and task characteristics?
- Basis in paper: [explicit] The paper discusses that the optimal ξ is likely non-trivial and task-dependent, showing different ξ values are more compute-efficient on CIFAR-10 for BTT and Monarch.
- Why unresolved: While the paper demonstrates that ξ impacts compute efficiency, it doesn't provide a general rule for how to set ξ as a function of width and task.
- What evidence would resolve it: Empirical studies across diverse tasks showing how optimal ξ varies with width and task properties, potentially leading to a predictive model.

### Open Question 3
- Question: Does the advantage of structured matrices diminish or change at larger scales, particularly for language modeling?
- Basis in paper: [explicit] The paper notes that while BTT offers significant improvement in image classification, it provides less improvement in language modeling compared to dense matrices.
- Why unresolved: The paper's experiments are limited to relatively small-scale models, and it's unclear if the observed trends hold as models and datasets scale up.
- What evidence would resolve it: Large-scale experiments (e.g., GPT-3 sized models) comparing structured vs. dense matrices across various tasks to see if the advantage persists or changes.

## Limitations
- Generalization across tasks may be limited as experiments focus primarily on image classification benchmarks
- Architecture-specific findings may not transfer to domains where dense layers aren't the primary computational bottleneck
- Computational regime considerations suggest structured matrices may be less beneficial when using pre-trained dense models
- Theoretical gaps exist in explaining the connection between parameter sharing and scaling laws

## Confidence
- **High confidence**: The structure-aware learning rate scaling methodology is well-grounded in Maximal Update Parameterization theory
- **Medium confidence**: The claim that BTT achieves better scaling laws than dense matrices is supported by extensive experiments but depends on task and training setup
- **Medium confidence**: The observation that parameter sharing leads to worse scaling laws is consistent across experiments but lacks complete theoretical explanation
- **Low confidence**: The assertion that BTT is "superior" to all other structured matrices in every context is overstated

## Next Checks
1. Evaluate BTT and other structured matrices on non-vision tasks including language modeling and tabular data regression to assess cross-task generalization
2. Investigate whether structured matrices can be effectively integrated into pre-trained dense models through fine-tuning
3. Measure actual wall-clock time and memory bandwidth usage for largest models to determine if theoretical FLOP reductions translate to practical speedups on memory-bound hardware