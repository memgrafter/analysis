---
ver: rpa2
title: 'EPIC: Efficient Position-Independent Caching for Serving Large Language Models'
arxiv_id: '2410.15332'
source_url: https://arxiv.org/abs/2410.15332
tags:
- tokens
- caching
- attention
- epic
- first
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "EPIC introduces LegoLink, a new algorithm for position-independent\
  \ caching in large language model serving. Unlike existing approaches that require\
  \ quadratic recomputation or dynamic sparsity, LegoLink recomputes only the first\
  \ k tokens of each immutable chunk (k \u2264 32), reducing complexity to O(kN) and\
  \ relying on static sparsity."
---

# EPIC: Efficient Position-Independent Caching for Serving Large Language Models

## Quick Facts
- arXiv ID: 2410.15332
- Source URL: https://arxiv.org/abs/2410.15332
- Authors: Junhao Hu, Wenrui Huang, Weidong Wang, Haoyi Wang, Tiancheng Hu, Qin Zhang, Hao Feng, Xusheng Chen, Yizhou Shan, Tao Xie
- Reference count: 24
- Key outcome: LEGO introduces LegoLink, a new algorithm for position-independent caching in large language model serving

## Executive Summary
EPIC introduces LegoLink, a novel algorithm for position-independent caching in large language model serving. Unlike existing approaches that require quadratic recomputation or dynamic sparsity, LegoLink recomputes only the first k tokens of each immutable chunk (k ≤ 32), reducing complexity to O(kN) and relying on static sparsity. This mitigates the "attention sink" effect at chunk beginnings, preserving accuracy with minimal computation. Evaluated across six datasets and three models, EPIC achieves up to 8× TTFT improvement and 7× throughput gains over state-of-the-art methods, with accuracy losses under 7%. The system integrates with vLLM and supports OpenAI-compatible caching APIs.

## Method Summary
EPIC's LegoLink algorithm addresses position-independent caching by recomputing only the first k tokens (k ≤ 32) of each immutable chunk rather than a percentage of all tokens. This approach reduces computational complexity from O(N²) to O(kN) while eliminating the runtime overhead associated with dynamic sparsity selection. The algorithm mitigates the "attention sink" effect where initial tokens of independently compiled chunks absorb disproportionate attention, by allowing these tokens to recognize their non-initial positions through recomputation. The system integrates with vLLM and supports explicit cache ID-based indexing for better user control and reduced indexing overhead.

## Key Results
- Achieves up to 8× improvement in Time-To-First-Token (TTFT) compared to state-of-the-art methods
- Provides 7× throughput gains while maintaining accuracy losses under 7%
- Supports longer context lengths (50K tokens) compared to CacheBlend (35K tokens)
- Eliminates runtime overhead by using static sparsity instead of dynamic sparsity

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LegoLink reduces attention sink by recomputing the first k tokens of each chunk (except the first), allowing these tokens to recognize non-initial positions.
- Mechanism: Initial tokens of each chunk absorb disproportionate attention when compiled independently (position IDs start at zero), blocking subsequent tokens from accessing relevant context. By recomputing these initial tokens, they are exposed to the full context and lose their attention sink behavior.
- Core assumption: The attention sink phenomenon is primarily caused by position ID reset at chunk boundaries and can be mitigated by recomputing the affected tokens.
- Evidence anchors:
  - [abstract]: "LegoLink recomputes only the first k tokens of each immutable chunk (k ≤ 32), reducing complexity to O(kN) and relying on static sparsity. This mitigates the 'attention sink' effect at chunk beginnings, preserving accuracy with minimal computation."
  - [section]: "The initial tokens of each chunk disproportionately absorb attention—a phenomenon known as 'attention sink' (Xiao et al., 2024). LegoLink recomputes k (k ≤ 32) initial tokens of each chunk (except the first chunk), allowing these to recognize their non-initial positions and crippling their attention-sink ability."

### Mechanism 2
- Claim: Static attention sparsity in LegoLink eliminates runtime overhead compared to dynamic sparsity in CacheBlend.
- Mechanism: CacheBlend selects tokens for recomputation dynamically during the link step by comparing attention maps, incurring substantial runtime overhead (16.3% to 63.56% of TTFT). LegoLink predefines the recomputed tokens (first k tokens of each chunk) before runtime, eliminating this overhead.
- Core assumption: The set of tokens that need recomputation can be predetermined based on chunk structure rather than computed dynamically.
- Evidence anchors:
  - [abstract]: "LegoLink relies on static attention sparsity, which selects the tokens to recompute beforehand, further improving performance."
  - [section]: "Figure 10 shows that the runtime overhead of CacheBlend takes around 16.3% to 63.56% of Time-To-First-Token (TTFT)."

### Mechanism 3
- Claim: LegoLink achieves O(kN) complexity instead of O(N²) by limiting recomputation to k tokens per chunk rather than a percentage of all tokens.
- Mechanism: CacheBlend recomputes 15% of all tokens in the prompt, maintaining O(N²) complexity. LegoLink recomputes only the first k tokens of each chunk, where k is constant per chunk and independent of total prompt length N, reducing complexity to O(kN).
- Core assumption: Recomputing only the initial tokens of each chunk is sufficient to mitigate accuracy loss, and k remains small relative to N.
- Evidence anchors:
  - [abstract]: "LegoLink reduces recomputation complexity to O(kN) ~ O(N), where k ≪ N and increases with the number of immutable chunks instead of N."
  - [section]: "LegoLink reduces recomputation complexity to O(kN) ~ O(N), where k ≪ N and increases with the number of immutable chunks instead of N."

## Foundational Learning

- Concept: Transformer attention mechanism and KV cache
  - Why needed here: Understanding how KV vectors are computed and reused is fundamental to grasping position-independent caching
  - Quick check question: What is the computational complexity of standard attention and how does it scale with sequence length?

- Concept: Position IDs and their role in attention
  - Why needed here: The position-independent caching challenge arises because standard attention depends on absolute position IDs
  - Quick check question: How do position IDs affect the attention computation between tokens in different chunks?

- Concept: Attention sparsity and its implications
  - Why needed here: Both CacheBlend and LegoLink leverage attention sparsity, but in different ways (dynamic vs static)
  - Quick check question: What does it mean that "only a small subset of tokens significantly influence attention computation"?

## Architecture Onboarding

- Component map: KVCompile -> KVCache -> KVLink -> Scheduler -> Model backend
- Critical path:
  1. Compile step: User submits immutable chunks → KVCompile processes chunks → KV vectors stored in KVCache → cache IDs returned
  2. Link step: User submits request with cache IDs → KVLink retrieves and concatenates KV vectors → LegoLink recomputes initial tokens → decode stage proceeds

- Design tradeoffs:
  - Explicit vs implicit caching: EPIC uses explicit cache IDs for better user control and reduced indexing overhead
  - Static vs dynamic sparsity: LegoLink uses static sparsity for lower runtime overhead but requires understanding of attention sink patterns
  - Compile-time vs runtime overhead: LegoLink-0 shifts all overhead to compile time, achieving zero runtime overhead

- Failure signatures:
  - Out-of-memory errors during link step (CacheBlend problem)
  - Accuracy degradation due to insufficient recomputation
  - Cache ID mismatches or missing KV vectors
  - Excessive compile-time overhead with LegoLink-0

- First 3 experiments:
  1. Compare TTFT of LegoLink variants (LegoLink-2, LegoLink-16, LegoLink-32) on a simple multi-document QA task
  2. Measure runtime overhead percentage for CacheBlend vs LegoLink on the same workload
  3. Test accuracy preservation across different model architectures (Mistral, Llama, Yi) using the synchronous workload

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does LegoLink's static attention sparsity selection strategy perform across different model architectures and training recipes?
- Basis in paper: [explicit] The paper states that LegoLink relies on static attention sparsity to select tokens for recomputation beforehand, but does not provide detailed analysis of how this strategy generalizes across different model architectures.
- Why unresolved: The paper only evaluates three specific models (Mistral 7B, Llama 3.1 8B, Yi Coder 9B) and does not explore how the static sparsity selection would perform on other architectures or training recipes.
- What evidence would resolve it: Systematic evaluation of LegoLink across a diverse set of model architectures, including different attention mechanisms, layer configurations, and training approaches, would provide evidence of its generalizability.

### Open Question 2
- Question: What is the optimal value of k (number of initial tokens to recompute per chunk) for different task types and model sizes?
- Basis in paper: [explicit] The paper mentions that increasing the number of recomputed tokens in LegoLink yields diminishing accuracy gains, but does not provide a systematic analysis of how to determine the optimal k value.
- Why unresolved: The paper only tests a few specific values of k (2, 4, 8, 16, 32) and does not provide guidance on how to select the optimal value for different scenarios.
- What evidence would resolve it: A comprehensive study that varies k across different task types, model sizes, and chunk sizes would help establish guidelines for selecting the optimal value.

### Open Question 3
- Question: How does the performance of LegoLink scale with extremely long prompts (e.g., 100K+ tokens) in real-world scenarios?
- Basis in paper: [inferred] The paper shows that LegoLink supports longer context lengths compared to CacheBlend (50K vs 35K tokens), but does not evaluate performance at extremely long prompt lengths.
- Why unresolved: The paper's evaluation focuses on prompts up to 50K tokens, but real-world applications may require handling much longer prompts.
- What evidence would resolve it: Performance evaluation of LegoLink with prompts exceeding 100K tokens, including analysis of memory usage, computation time, and accuracy degradation, would provide insights into its scalability.

## Limitations
- Static sparsity assumption may not hold across all workloads with varying attention patterns
- Evaluation focuses on synchronous workloads, may not reflect real-world bursty traffic patterns
- Limited exploration of optimal k values across different task types and model architectures

## Confidence

- **High confidence**: The O(kN) complexity advantage of LegoLink over CacheBlend's O(N²) approach is well-supported by the algorithm design and theoretical analysis. The 8× TTFT improvement and 7× throughput gains are directly measurable outcomes from the experiments.
- **Medium confidence**: The accuracy preservation claims (under 7% loss) are supported by experimental results but may be sensitive to specific model architectures and task types. The effectiveness of static sparsity over dynamic sparsity is demonstrated but could vary with different attention patterns.
- **Low confidence**: The generalization of results to models not evaluated in the study, and the system's behavior under bursty traffic conditions rather than the continuous synchronous workload tested.

## Next Checks
1. **Attention pattern variability test**: Evaluate LegoLink's performance when attention sparsity patterns vary significantly across different prompt types to validate the static sparsity assumption.
2. **Bursty workload evaluation**: Test the system under realistic traffic patterns with request bursts and idle periods to assess cache hit rates and performance under non-continuous usage.
3. **Cross-model generalization**: Apply LegoLink to additional model architectures (e.g., Gemma, Qwen) not evaluated in the original study to verify the claimed accuracy preservation across diverse model families.