---
ver: rpa2
title: Theoretical Corrections and the Leveraging of Reinforcement Learning to Enhance
  Triangle Attack
arxiv_id: '2411.12071'
source_url: https://arxiv.org/abs/2411.12071
tags:
- attack
- tarl
- adversarial
- imagenet
- attacks
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses limitations in Triangle Attack (TA), a state-of-the-art
  decision-based black-box adversarial attack, by introducing TARL (Triangle Attack
  with Reinforcement Learning). The authors identify that TA's angle update algorithm
  is not universally optimal across different decision boundary shapes, leading to
  suboptimal adversarial examples or failure to find them.
---

# Theoretical Corrections and the Leveraging of Reinforcement Learning to Enhance Triangle Attack

## Quick Facts
- arXiv ID: 2411.12071
- Source URL: https://arxiv.org/abs/2411.12071
- Reference count: 40
- Primary result: TARL achieves comparable or better attack success rates than TA while using half the queries (500 vs 1000)

## Executive Summary
This paper addresses limitations in Triangle Attack (TA), a state-of-the-art decision-based black-box adversarial attack, by introducing TARL (Triangle Attack with Reinforcement Learning). The authors identify that TA's angle update algorithm is not universally optimal across different decision boundary shapes, leading to suboptimal adversarial examples or failure to find them. TARL addresses this by implementing a Q-learning algorithm that adaptively adjusts the learned angle based on historical query data, allowing the agent to learn optimal values for different boundary characteristics.

The method was evaluated on ImageNet and CIFAR-10 datasets across 9 models including VGG-16, ResNet variants, Vision Transformers, Vision Mamba, and a Diffusion defense model. TARL achieved comparable or better attack success rates than TA while using half the queries (500 vs 1000). Under tight perturbation budgets, TARL maintained competitive performance, with the performance gap narrowing as perturbation budgets increased. The results demonstrate TARL's superior query efficiency while maintaining robustness across diverse models and perturbation constraints.

## Method Summary
TARL introduces a Q-learning algorithm to improve Triangle Attack by adaptively adjusting the learned angle parameter. The method constructs a Q-table to store expected cumulative rewards for each possible action (increasing or decreasing the angle) in each state (current angle value). An epsilon-greedy strategy balances exploration and exploitation when selecting the next angle value. The attack generates candidate adversarial examples by constructing triangles in the DCT-transformed frequency space and uses binary search to find optimal parameters. Historical data of angle values, L2 distances, and success/failure outcomes are used to update the Q-table, allowing the agent to learn optimal angle values for different decision boundary characteristics.

## Key Results
- TARL achieved comparable or better attack success rates than TA while using half the queries (500 vs 1000)
- TARL maintained competitive performance under tight perturbation budgets, with the performance gap narrowing as budgets increased
- The method was evaluated across 9 models including CNNs, Vision Transformers, Vision Mamba, and defense models on ImageNet and CIFAR-10 datasets

## Why This Works (Mechanism)

### Mechanism 1
- Claim: TARL uses Q-learning to adaptively adjust the learned angle α in Triangle Attack based on historical query data
- Mechanism: TARL implements a Q-learning algorithm where the agent learns optimal α values by storing Q-values for state-action pairs in a Q-table. The states represent different α values and actions represent whether to increase or decrease α. The reward is set to be 1/l2 if the resulting image is adversarial, and 0 if not.
- Core assumption: The optimal α value varies depending on the characteristics of different decision boundary shapes, and historical data can inform the selection of better α values
- Evidence anchors:
  - [abstract] "TARL addresses this by implementing a Q-learning algorithm that adaptively adjusts the learned angle based on historical query data"
  - [section] "To address the problems with the current Triangle Attack method outlined in Section 2.2, we revised the alpha update algorithm and introduce TARL. TARL implement a Q-learning algorithm to train the agent to find the optimal value of alpha based on historical observation data"
  - [corpus] Weak corpus support - the related papers focus on general black-box attacks but don't specifically discuss Q-learning for angle optimization
- Break condition: If the decision boundary characteristics don't correlate with optimal α values, or if historical data doesn't provide useful patterns for learning

### Mechanism 2
- Claim: TARL achieves comparable or better attack success rates while using half the queries (500 vs 1000)
- Mechanism: By learning optimal α values through Q-learning rather than using a fixed update rule, TARL converges faster to effective adversarial examples. The agent explores different α values and learns from historical data, making educated guesses that converge to optimal points more efficiently.
- Core assumption: The query efficiency improvement comes from better α selection that leads to faster convergence to adversarial examples
- Evidence anchors:
  - [abstract] "TARL achieved comparable or better attack success rates than TA while using half the queries (500 vs 1000)"
  - [section] "TARL is able to take into consideration previously explored alpha values and corresponding l2 distances when suggesting the next alpha, making it more query-efficient"
  - [corpus] Weak corpus support - related papers discuss query efficiency but not specifically the trade-off between query count and attack success rate
- Break condition: If the overhead of maintaining and querying the Q-table outweighs the benefits of better α selection, or if the exploration phase requires too many queries

### Mechanism 3
- Claim: TARL maintains competitive performance under tight perturbation budgets
- Mechanism: TARL's adaptive α adjustment allows it to find adversarial examples that are closer to the decision boundary (smaller perturbations) while still maintaining high success rates. The Q-learning approach helps navigate complex decision boundaries more effectively than fixed update rules.
- Core assumption: Adaptive α selection helps find adversarial examples with smaller perturbations, which is crucial under tight perturbation budgets
- Evidence anchors:
  - [abstract] "Under tight perturbation budgets, TARL maintained competitive performance, with the performance gap narrowing as perturbation budgets increased"
  - [section] "According to Proposition 1 in the paper [37], 'With the same angle β, a smaller angle α makes it easier to find an adversarial example while a larger angle α leads to smaller perturbation'"
  - [corpus] Weak corpus support - related papers don't specifically address perturbation budget constraints
- Break condition: If the decision boundaries are too complex for Q-learning to effectively navigate, or if the perturbation constraints are so tight that even optimal α selection cannot find valid adversarial examples

## Foundational Learning

- Concept: Q-learning and reinforcement learning fundamentals
  - Why needed here: TARL's core innovation relies on using Q-learning to adaptively adjust parameters based on historical data
  - Quick check question: What is the difference between Q-learning and other reinforcement learning approaches like policy gradients?

- Concept: Decision-based black-box adversarial attacks
  - Why needed here: Understanding the threat model and attack methodology is crucial for implementing TARL
  - Quick check question: How does a decision-based attack differ from score-based or transfer attacks?

- Concept: Triangle Attack methodology and limitations
  - Why needed here: TARL builds upon Triangle Attack, so understanding its mechanism and limitations is essential
  - Quick check question: What is the role of the learned angle α in Triangle Attack, and why was it identified as a limitation?

## Architecture Onboarding

- Component map:
  - Q-table stores expected cumulative rewards for state-action pairs
  - Epsilon-greedy strategy balances exploration and exploitation when selecting next α values
  - DCT transformation converts images to frequency space for triangle construction
  - Binary search finds optimal β values for triangle construction
  - Historical data recorder tracks α values, l2 distances, and success/failure outcomes

- Critical path:
  1. Initialize Q-table with all possible α values
  2. Generate initial adversarial example using binary search
  3. For each query:
     - Sample 2D subspace using DCT
     - Find optimal β using binary search
     - Use epsilon-greedy strategy to select next α based on Q-table
     - Generate candidate adversarial example
     - Record outcome and update Q-table
  4. Return best adversarial example found

- Design tradeoffs:
  - Q-table size vs. granularity of α values
  - Exploration rate vs. exploitation efficiency
  - Query budget allocation between exploration and exploitation
  - Complexity of Q-table update rule vs. learning speed

- Failure signatures:
  - Q-table convergence to suboptimal α values
  - Exploration phase consuming too many queries
  - Poor correlation between historical data and optimal α values
  - Failure to adapt to different decision boundary characteristics

- First 3 experiments:
  1. Verify Q-table initialization and basic update mechanism with synthetic data
  2. Test epsilon-greedy strategy with known optimal α values to validate learning
  3. Compare TARL performance against TA on a simple model with predictable decision boundaries

## Open Questions the Paper Calls Out

The paper doesn't explicitly call out open questions, but based on the limitations and scope of the work, several important questions remain unanswered:

1. How does TARL's performance scale when the decision boundary has highly irregular or discontinuous shapes compared to smoother boundaries?

2. What is the theoretical limit of TARL's query efficiency reduction when applied to models with extremely high dimensionality?

3. How does TARL perform when the perturbation budget is extremely tight (RMSE C < 0.01) compared to TA's performance in the same regime?

## Limitations

- The paper lacks detailed hyperparameter specifications for the Q-learning algorithm, which could substantially impact reproducibility and performance
- The evaluation primarily focuses on attack success rate and query efficiency, but doesn't extensively analyze the quality of adversarial examples beyond L2 distance metrics
- Claims about TARL's performance under tight perturbation budgets are based on limited comparative analysis without statistical significance verification

## Confidence

- **High Confidence**: The theoretical foundation of using Q-learning for adaptive angle selection is sound and well-explained
- **Medium Confidence**: The experimental results showing TARL's improved query efficiency are promising but limited to specific datasets and models
- **Low Confidence**: The claims about TARL's performance under tight perturbation budgets are based on limited comparative analysis

## Next Checks

1. **Hyperparameter Sensitivity Analysis**: Conduct experiments varying the Q-learning hyperparameters (learning rate, discount factor, exploration rate) to determine their impact on TARL's performance and identify optimal settings.

2. **Cross-Architecture Generalization Test**: Evaluate TARL on a diverse set of novel architectures not included in the original evaluation to assess its generalization capabilities.

3. **Statistical Significance Verification**: Perform statistical tests comparing TARL and TA performance across multiple runs to verify that observed improvements are statistically significant rather than due to random variation.