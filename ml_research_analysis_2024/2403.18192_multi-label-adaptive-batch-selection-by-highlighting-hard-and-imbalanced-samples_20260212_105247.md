---
ver: rpa2
title: Multi-Label Adaptive Batch Selection by Highlighting Hard and Imbalanced Samples
arxiv_id: '2403.18192'
source_url: https://arxiv.org/abs/2403.18192
tags:
- batch
- selection
- loss
- multi-label
- adaptive
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of class imbalance in multi-label
  classification, which can bias deep learning models towards majority labels. The
  authors propose an adaptive batch selection algorithm that prioritizes hard samples
  associated with minority labels.
---

# Multi-Label Adaptive Batch Selection by Highlighting Hard and Imbalanced Samples

## Quick Facts
- arXiv ID: 2403.18192
- Source URL: https://arxiv.org/abs/2403.18192
- Reference count: 40
- This paper proposes an adaptive batch selection algorithm that prioritizes hard samples associated with minority labels in multi-label classification.

## Executive Summary
This paper addresses the problem of class imbalance in multi-label classification, which can bias deep learning models towards majority labels. The authors propose an adaptive batch selection algorithm that prioritizes hard samples associated with minority labels. The method uses the multi-label binary cross-entropy loss to assess sample difficulty and assigns selection probabilities based on a rank-based approach with quantization. A variant of the method also considers label correlations. Experiments on 13 benchmark datasets with five different multi-label deep learning models show that the adaptive batch selection method converges faster and achieves better performance than random batch selection, particularly for datasets with high imbalance or many labels.

## Method Summary
The method implements adaptive batch selection for multi-label classification by calculating binary cross-entropy losses for each sample, applying imbalance weights to emphasize minority labels, quantizing losses to smooth selection probabilities, and using these probabilities to select training batches. The approach includes a warm-up phase, employs the Adam optimizer with specific hyperparameters, and uses 5-fold cross-validation across 13 benchmark datasets with five different multi-label deep learning models (C2AE, MPVAE, PACA, CLIF, and DELA). A chain-based variant further leverages label correlations to improve batch composition.

## Key Results
- Adaptive batch selection converges faster than random batch selection across all datasets and models
- The method significantly improves performance for embedding-based and sophisticated neural network models
- Chain-based selection variant further enhances performance when label correlations are strong
- Method shows particular effectiveness on datasets with high imbalance or many labels

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The adaptive batch selection prioritizes hard samples associated with minority labels to mitigate class imbalance bias.
- Mechanism: It uses multi-label binary cross-entropy loss to assess sample difficulty and assigns selection probabilities based on a rank-based approach with quantization.
- Core assumption: Higher loss samples are harder to classify and more informative for model training.
- Evidence anchors:
  - [abstract] "instances associated with minority labels tend to induce greater losses"
  - [section] "The connection between the higher loss and minority labels offers valuable insight: similar to higher loss samples, instances associated with minority labels are more crucial for training models."
  - [corpus] Weak - no direct evidence from corpus papers about loss-minority label relationship.
- Break condition: If the assumption that higher loss always correlates with minority labels breaks down, the prioritization strategy becomes ineffective.

### Mechanism 2
- Claim: Quantization index-based probability assignment smooths the selection probability distribution and reduces sensitivity to small loss changes.
- Mechanism: It limits the rank value within the range of n using a quantization function Q(ℓi) = ⌈ℓi/∆⌉, where ∆ is the adaptive quantization step size.
- Core assumption: Small differences in loss values should not lead to disproportionate shifts in sample selection probabilities.
- Evidence anchors:
  - [section] "the ranking-based method for sample selection probability tends to magnify small differences, resulting in significant (disproportionate) shifts in rankings and selection probabilities."
  - [section] "By adopting quantization index-based probability assignment, batch selection becomes insensitive to the small loss change and results in a smoother overall probability distribution."
  - [corpus] No direct evidence - this is a novel contribution not present in related papers.
- Break condition: If quantization causes loss of critical information about sample difficulty, model performance may degrade.

### Mechanism 3
- Claim: The chain-based selection strategy exploits label correlations to improve batch selection quality.
- Mechanism: It selects seed samples based on selection probability, identifies labels most closely related to the seed label, and defines a new probability set for subsequent selections.
- Core assumption: Label correlations provide additional information that can improve batch composition, especially for underrepresented labels.
- Evidence anchors:
  - [section] "It is well-known that label correlations are significant in multi-label learning. Label correlations can provide additional information, especially when some labels have insufficient training samples."
  - [section] "By considering label correlation, we propose a chain-based adaptive batch selection method."
  - [corpus] No direct evidence - this is a novel contribution not present in related papers.
- Break condition: If label correlations are weak or misleading in the dataset, the chain-based strategy may introduce noise rather than improvement.

## Foundational Learning

- Concept: Multi-label classification and class imbalance
  - Why needed here: The paper addresses class imbalance in multi-label data where minority labels are underrepresented in mini-batches.
  - Quick check question: What is the difference between macro-F and micro-F metrics in multi-label classification?

- Concept: Loss functions and sample difficulty
  - Why needed here: The method uses binary cross-entropy loss to assess sample difficulty and prioritize harder samples.
  - Quick check question: How does binary cross-entropy loss differ from other loss functions like asymmetric loss?

- Concept: Batch selection strategies
  - Why needed here: The paper compares adaptive batch selection with random batch selection and other heuristic methods.
  - Quick check question: What is the key difference between hard sample mining and adaptive batch selection?

## Architecture Onboarding

- Component map: Multi-label dataset -> Deep learning model (C2AE, MPVAE, PACA, CLIF, DELA) -> Adaptive module (sample difficulty assessment, probability assignment, batch selection) -> Improved model performance with faster convergence

- Critical path:
  1. Warm-up phase to stabilize initial random initialization
  2. Calculate sample losses using binary cross-entropy
  3. Apply imbalance weights to losses
  4. Quantize losses to smooth probability distribution
  5. Assign selection probabilities based on quantized ranks
  6. Select batches using these probabilities
  7. Train model with selected batches

- Design tradeoffs:
  - Higher selection pressure (se) improves minority label focus but may cause overfitting
  - Larger batch sizes reduce computation but may miss minority label samples
  - Chain-based selection adds complexity but leverages label correlations

- Failure signatures:
  - Model performance plateaus despite adaptive selection (possibly due to quantization losing critical information)
  - Convergence becomes unstable (possibly due to incorrect imbalance weighting)
  - No improvement over random selection (possibly due to weak correlation between loss and minority labels)

- First 3 experiments:
  1. Compare convergence curves of adaptive vs random batch selection on a small, imbalanced dataset
  2. Test sensitivity to selection pressure (se) parameter on a validation set
  3. Evaluate chain-based vs standard adaptive selection on a dataset with known label correlations

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal value of selection pressure (se) for adaptive batch selection across different multi-label datasets?
- Basis in paper: [explicit] The paper explores the impact of varying se values on adaptive batch selection, noting that higher se values can emphasize the selection of hard minority samples but may also lead to overfitting. It mentions that se may not expedite the convergence of training losses but can improve evaluation metrics by batch setups.
- Why unresolved: The paper does not provide a definitive answer on the optimal se value, indicating that the relationship between se and model performance is complex and may vary depending on the dataset and the specific evaluation metric.
- What evidence would resolve it: A comprehensive study comparing the performance of adaptive batch selection across a wide range of multi-label datasets with varying levels of class imbalance and label cardinality, using different se values, would help determine the optimal se value for different scenarios.

### Open Question 2
- Question: How does the adaptive batch selection method perform with different batch sizes, and is there an optimal batch size for this method?
- Basis in paper: [explicit] The paper investigates the performance of the adaptive batch selection method with batch sizes of 256 and 512, finding that it consistently outperforms random batch selection across both batch sizes.
- Why unresolved: While the paper shows that the adaptive batch selection method is effective with different batch sizes, it does not provide a definitive answer on whether there is an optimal batch size for this method or how the performance scales with batch size.
- What evidence would resolve it: A systematic study evaluating the performance of the adaptive batch selection method across a wide range of batch sizes, including smaller and larger sizes, would help determine the optimal batch size and how the method's performance scales with batch size.

### Open Question 3
- Question: How does the adaptive batch selection method compare to other existing batch selection methods in terms of computational efficiency and model performance?
- Basis in paper: [inferred] The paper introduces a novel adaptive batch selection method for multi-label classification and demonstrates its effectiveness in improving model performance and convergence compared to random batch selection. However, it does not provide a direct comparison with other existing batch selection methods.
- Why unresolved: While the paper shows the effectiveness of the proposed method, it does not provide a comprehensive comparison with other existing batch selection methods, leaving the question of how it compares in terms of computational efficiency and model performance unanswered.
- What evidence would resolve it: A direct comparison of the proposed adaptive batch selection method with other existing batch selection methods, such as Online Hard Example Mining (OHEM) and Adaptive Batch Selection (Ada-Boundary), in terms of computational efficiency and model performance on various multi-label datasets, would help answer this question.

## Limitations

- The empirical correlation between higher loss and minority label samples is assumed but not rigorously validated across all datasets
- The quantization mechanism's sensitivity threshold is set heuristically without theoretical justification
- The chain-based selection variant's effectiveness depends on label correlation strength, which varies significantly across datasets

## Confidence

- High confidence: Faster convergence rates with adaptive selection compared to random
- Medium confidence: Improved performance metrics for embedding-based and sophisticated models
- Low confidence: Generalizability of chain-based selection variant across all dataset types

## Next Checks

1. Conduct ablation studies to isolate the contribution of each mechanism (loss-based selection, quantization, chain-based approach)
2. Test the method on additional multi-label datasets with varying imbalance levels and label correlation structures
3. Perform theoretical analysis of the quantization threshold selection to provide principled guidance rather than heuristic values