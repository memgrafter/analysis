---
ver: rpa2
title: 'Harnessing the Power of Multiple Minds: Lessons Learned from LLM Routing'
arxiv_id: '2405.00467'
source_url: https://arxiv.org/abs/2405.00467
tags:
- llms
- routing
- arxiv
- each
- performance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigates LLM routing, where input queries are directed
  to the single most suitable LLM from a diverse pool. The approach treats routing
  as both a classification and clustering task, using multi-label and separate classifiers,
  as well as KMeans clustering with TF-IDF and RoBERTa embeddings.
---

# Harnessing the Power of Multiple Minds: Lessons Learned from LLM Routing

## Quick Facts
- arXiv ID: 2405.00467
- Source URL: https://arxiv.org/abs/2405.00467
- Authors: KV Aditya Srivatsa; Kaushal Kumar Maurya; Ekaterina Kochmar
- Reference count: 34
- Primary result: Routing models perform similarly to or slightly lower than top individual LLMs, likely due to small training data.

## Executive Summary
This study investigates LLM routing, where input queries are directed to the single most suitable LLM from a diverse pool. The approach treats routing as both a classification and clustering task, using multi-label and separate classifiers, as well as KMeans clustering with TF-IDF and RoBERTa embeddings. Experiments with 7 open-source LLMs on GSM8K and MMLU benchmarks show that while the theoretical upper bounds for routing are higher than individual model performance, the proposed routing model in practice performs similarly to or slightly lower than top individual LLMs, likely due to small training data. The routing model consistently achieves inference latency comparable to or lower than single LLMs, highlighting its potential for efficient LLM utilization.

## Method Summary
The method involves generating 10 responses per LLM per query, applying majority voting to extract a single answer, and using the resulting binary accuracy as a label to train the routing classifier. The routing classifier is a RoBERTa fine-tuned model that takes the query text and outputs a probability distribution over all candidate LLMs. The routing policies include ArgMax, Random, Prediction, and Sorted Pred, each using confidence scores differently. Additionally, clustering queries by similarity using TF-IDF or RoBERTa embeddings and KMeans is explored, where each cluster is associated with the LLM that achieves the highest accuracy on that cluster in the training set.

## Key Results
- Routing models perform similarly to or slightly lower than top individual LLMs due to small training data.
- The routing model consistently achieves inference latency comparable to or lower than single LLMs.
- Clustering approach shows instability, with the best-performing LLM for a cluster in training not remaining optimal for the same cluster in testing.

## Why This Works (Mechanism)

### Mechanism 1
Routing based on majority-voting label aggregation can identify which LLMs are capable of solving which queries. The paper generates 10 responses per LLM per query, applies majority voting (MAJ@10) to extract a single answer, and uses the resulting binary accuracy as a label to train the routing classifier. The core assumption is that majority voting over 10 samples per LLM per query yields a reliable ground truth for whether that LLM can solve the query.

### Mechanism 2
A multi-label classifier trained on query-to-LLM capability pairs can predict, with confidence scores, which LLMs in the pool can solve a given query. The routing classifier (RoBERTa fine-tuned) takes the query text and outputs a probability distribution over all candidate LLMs; these probabilities are used as confidence scores to select the single best LLM under different policies (ArgMax, Random, Prediction, Sorted Pred). The core assumption is that the classifier can learn to generalize from the small training set to unseen queries, and the confidence scores are well-calibrated.

### Mechanism 3
Clustering queries by similarity and assigning each cluster to its best-performing LLM can improve routing efficiency. Queries are embedded using TF-IDF or RoBERTa, clustered with KMeans, and each cluster is associated with the LLM that achieves the highest accuracy on that cluster in the training set; at inference, queries are routed to the LLM assigned to their cluster. The core assumption is that the best-performing LLM for a cluster in training remains the best for the same cluster in test, and clusters are sufficiently homogeneous.

## Foundational Learning

- **Majority voting over multiple LLM generations**: Why needed here? LLM outputs are stochastic; a single generation may be wrong even if the model is generally capable of solving the query. Quick check: If 10 generations from an LLM yield 7 correct and 3 incorrect answers for a query, what is the majority-vote label?
- **Multi-label classification and confidence score calibration**: Why needed here? A query may be solvable by more than one LLM, so the routing model must predict a set of capable LLMs and rank them by confidence. Quick check: If a classifier outputs [0.9, 0.4, 0.1] for three LLMs, which one is selected under ArgMax policy?
- **Clustering with embeddings and KMeans for query similarity**: Why needed here? Grouping similar queries allows assigning a single best LLM per group, reducing the need for per-query classification. Quick check: If two queries have cosine similarity >0.8 in RoBERTa embedding space, should they be placed in the same cluster?

## Architecture Onboarding

- **Component map**: Data pipeline: GSM8K/MMLU → LLM sampling (10 generations each) → answer extraction → majority-vote labeling. Training pipeline: RoBERTa fine-tuned on query text → multi-label output → confidence scores. Routing policies: ArgMax, Random, Prediction, Sorted Pred (each uses confidence scores differently). Clustering branch: TF-IDF/Roberta embeddings → KMeans → cluster→LLM mapping.
- **Critical path**: Query → RoBERTa classifier → confidence scores → routing policy → selected LLM → inference.
- **Design tradeoffs**: Small training data → simpler models (few RoBERTa layers) to avoid overfitting. Latency vs accuracy: selecting one LLM reduces latency vs ensembling all. Cluster size vs granularity: more clusters improve fit but risk sparsity.
- **Failure signatures**: All queries routed to same LLM → classifier overfit. Cluster assignments change drastically between train/test → clustering unstable. Confidence scores not well-calibrated → policy selection fails.
- **First 3 experiments**: 1) Run with only the two best-performing LLMs (gemma-7b, metamath-7b) to see if classifier can beat top-1 baseline. 2) Compare ArgMax vs Random policy on same classifier to test if confidence scores help. 3) Run clustering with both TF-IDF and RoBERTa embeddings to measure stability of cluster assignments.

## Open Questions the Paper Calls Out

### Open Question 1
How does increasing the size of the training data affect the performance of the LLM routing model? The paper mentions that the small size of the training data is a limitation and may lead to suboptimal performance of the routing model. Experiments comparing routing model performance with different sizes of training data, demonstrating the relationship between training data size and routing accuracy, would resolve this question.

### Open Question 2
Can the routing model effectively handle LLMs with very different capabilities? The paper notes that the proposed model works well with equally capable LLMs but is not yet effective enough for LLMs that have very different capabilities. Experiments using LLMs with a wide range of capabilities to evaluate the routing model's performance and identify limitations in handling diverse models would resolve this question.

### Open Question 3
How can the inference latency issue be mitigated when frequently switching between LLMs? The paper mentions that frequent switches between LLMs necessitate loading most of them into memory, posing a limited memory issue. Investigation of memory-efficient approaches or techniques to reduce the memory footprint when loading and switching between LLMs during inference would resolve this question.

## Limitations
- Small training dataset (9k GSM8K, 15k MMLU) may lead to overfitting and poor generalization.
- Clustering approach shows instability, with the best-performing LLM for a cluster in training not remaining optimal for the same cluster in testing.
- The routing model performs similarly to or slightly lower than top individual LLMs, likely due to small training data and classifier performance bottlenecks.

## Confidence

- **High**: The routing model consistently achieves inference latency comparable to or lower than single LLMs, demonstrating the efficiency benefits of selecting a single model rather than ensembling.
- **Medium**: The claim that routing classifiers can predict which LLMs are capable of solving which queries is moderately supported, but the practical performance being "similar to or slightly lower than top individual LLMs" suggests significant limitations in the approach.
- **Low**: The theoretical upper bounds for routing being higher than individual model performance is calculated but not empirically validated in the routing experiments.

## Next Checks

1. **Robustness to Training Data Size**: Systematically vary the training dataset size (e.g., 25%, 50%, 75%, 100% of available data) and measure routing accuracy to quantify the impact of small training data on classifier performance and identify the minimum viable training set size.

2. **Label Quality Assessment**: For a subset of queries where majority voting is ambiguous (e.g., answers distributed 4-3-3 or similar), manually verify the ground truth labels and measure how many queries in the training set have unreliable majority-vote labels that could mislead the routing classifier.

3. **Cross-Dataset Generalization**: Test the routing classifier trained on GSM8K on a completely different mathematical reasoning dataset (e.g., MATH or Ape210K) to evaluate whether the classifier can generalize beyond the specific distribution of its training data, or if it merely memorizes dataset-specific patterns.