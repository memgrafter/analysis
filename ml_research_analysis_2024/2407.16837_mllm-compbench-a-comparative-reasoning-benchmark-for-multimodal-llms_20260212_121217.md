---
ver: rpa2
title: 'MLLM-CompBench: A Comparative Reasoning Benchmark for Multimodal LLMs'
arxiv_id: '2407.16837'
source_url: https://arxiv.org/abs/2407.16837
tags:
- images
- image
- which
- right
- left
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces MLLM-CompBench, a new benchmark for evaluating
  multimodal large language models' (MLLMs) ability to perform comparative reasoning
  across eight types of relativity (attribute, existence, state, emotion, temporality,
  spatiality, quantity, quality) using 39.8K image pairs from 14 diverse domains.
  The benchmark uses horizontally concatenated image pairs with questions about relative
  comparisons between them.
---

# MLLM-CompBench: A Comparative Reasoning Benchmark for Multimodal LLMs

## Quick Facts
- **arXiv ID:** 2407.16837
- **Source URL:** https://arxiv.org/abs/2407.16837
- **Reference count:** 40
- **Key outcome:** New benchmark reveals current MLLMs struggle with comparative reasoning, achieving only 74.7% accuracy on 39.8K image pairs across 8 relativity types

## Executive Summary
This paper introduces MLLM-CompBench, a comprehensive benchmark designed to evaluate multimodal large language models' (MLLMs) ability to perform comparative reasoning across eight types of relativity: attribute, existence, state, emotion, temporality, spatiality, quantity, and quality. The benchmark uses horizontally concatenated image pairs with questions about relative comparisons between them. When evaluated on recent MLLMs including GPT-4V, Gemini-Pro, and LLaVA-1.6, the models showed notable shortcomings particularly in existence, spatiality, and quantity relativity tasks, with top models achieving only 74.7% accuracy overall. The paper also presents analyses of error patterns and explores fine-tuning approaches, finding that while fine-tuning helps with temporal reasoning, it provides limited improvement for quantity comparisons.

## Method Summary
MLLM-CompBench evaluates MLLMs using 39.8K image pairs from 14 diverse domains, each paired with questions about eight types of relativity (attribute, existence, state, emotion, temporality, spatiality, quantity, quality). Images are horizontally concatenated as input, with questions including multiple-choice answer options. The benchmark is evaluated on GPT-4V, Gemini-Pro, LLaVA-1.6, and VILA-1.5 using official APIs or source code, with accuracy measured as exact match to ground-truth answers. Fine-tuning experiments explore improvements for specific relativity types using LoRA on LLaVA-1.6.

## Key Results
- Current MLLMs achieve only 74.7% accuracy on comparative reasoning tasks
- Models show significant weaknesses in Existence (77.6%), Spatiality (76.8%), and Quantity (69.5%) relativity types
- Fine-tuning LLaVA-1.6 improves temporal reasoning but has limited impact on quantity comparisons
- GPT-4o and Gemini1.5-Pro show improvements over predecessors but still struggle with spatiality and quantity tasks

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Horizontal image concatenation enables direct comparative reasoning by providing spatial context for left-right comparisons
- **Mechanism:** By placing two relevant images side-by-side horizontally, the model can use spatial relationships (left vs right) as explicit anchors for answering comparative questions
- **Core assumption:** The model's vision encoder can effectively process concatenated images and maintain spatial awareness of which image is on the left vs right
- **Evidence anchors:**
  - [abstract] "we concatenate two images horizontally (i.e., left and right) as the visual input"
  - [section] "We observe that current MLLMs face challenges in answering relative questions in COMP BENCH (see Table 2)"
  - [corpus] Weak - corpus lacks specific studies on horizontal concatenation effects
- **Break condition:** If the vision encoder cannot maintain spatial relationships in concatenated inputs, or if models rely too heavily on absolute positioning rather than relative comparison

### Mechanism 2
- **Claim:** Multiple choice question format with answer options enables automated evaluation while testing fine-grained discrimination
- **Mechanism:** Providing answer options (e.g., "left/right", "first/second", multiple choices for existence) constrains model responses while still requiring nuanced visual understanding to select correct option
- **Core assumption:** The model can effectively use provided options to guide reasoning without simply memorizing patterns
- **Evidence anchors:**
  - [abstract] "we include the possible answers as options in the questions"
  - [section] "A question is answered correctly if the model prediction exactly matches the ground-truth answer"
  - [corpus] Weak - corpus lacks comparative studies of different evaluation formats
- **Break condition:** If models learn to exploit option patterns rather than genuine visual reasoning, or if option format biases responses

### Mechanism 3
- **Claim:** Diverse relativity types (8 categories across 14 domains) provide comprehensive assessment of comparative reasoning capabilities
- **Mechanism:** By covering visual attributes, existence, state, emotion, temporality, spatiality, quantity, and quality across varied visual domains, the benchmark tests multiple dimensions of comparative reasoning
- **Core assumption:** Different relativity types require distinct reasoning capabilities that collectively represent comprehensive comparative reasoning
- **Evidence anchors:**
  - [abstract] "eight dimensions of relative comparison: visual attribute, existence, state, emotion, temporality, spatiality, quantity, and quality"
  - [section] "Our results reveal notable shortcomings in their comparative abilities, especially in Existence, Spatiality, and Quantity Relativity"
  - [corpus] Moderate - corpus shows other works focus on narrower subsets (Kazemi et al. focuses on math/physics/logic)
- **Break condition:** If certain relativity types are redundant or if model performance on one type doesn't generalize to others

## Foundational Learning

- **Concept:** Visual attribute comparison (size, color, texture, shape, pattern)
  - **Why needed here:** Forms the basis for recognizing relative differences between objects
  - **Quick check question:** Given two circles, one red and one blue, can you identify which is larger without measuring?

- **Concept:** Spatial reasoning and depth perception
  - **Why needed here:** Essential for understanding spatiality relativity (which object is closer/further)
  - **Quick check question:** If two identical objects appear different sizes in images, can you determine which is closer to the camera?

- **Concept:** Temporal reasoning and sequence understanding
  - **Why needed here:** Critical for temporality relativity (which event happened first)
  - **Quick check question:** Given two frames from a video, can you determine which shows an earlier moment in the sequence?

## Architecture Onboarding

- **Component map:** Vision encoder (CLIP) → Bridge module → LLM → Answer generation
- **Critical path:** Image pair concatenation → Vision encoding → Question processing → Comparative reasoning → Answer selection
- **Design tradeoffs:** Horizontal concatenation vs. separate processing vs. attention-based fusion; multiple choice vs. open-ended answers
- **Failure signatures:** Random guessing on existence questions suggests object recognition issues; systematic errors on spatial questions indicate depth perception problems
- **First 3 experiments:**
  1. Test baseline accuracy on simple attribute comparisons (size/color) to establish lower bound
  2. Evaluate spatial reasoning by comparing performance on depth-based vs. proximity-based questions
  3. Measure impact of answer format (multiple choice vs. open-ended) on model accuracy

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do the newly released MLLMs (GPT-4o and Gemini1.5-Pro) perform on the existence, spatiality, and quantity relativity tasks compared to their predecessors?
- Basis in paper: [explicit] The paper evaluates GPT-4o and Gemini1.5-Pro on MLLM-CompBench and finds that while both models show significant improvements over their predecessors, they still struggle with spatiality and quantity tasks.
- Why unresolved: While the paper provides some insights into the performance of the new models, a detailed analysis of their strengths and weaknesses across different relativity types is needed to fully understand their capabilities.
- What evidence would resolve it: A comprehensive evaluation of the new models on all eight relativity types, including detailed error analysis and comparison with human performance, would provide a clearer picture of their strengths and weaknesses.

### Open Question 2
- Question: Can fine-tuning strategies be further optimized to improve MLLMs' performance on quantity and existence relativity tasks?
- Basis in paper: [inferred] The paper finds that fine-tuning LLaVA-1.6 on temporal data significantly improves its performance, but has limited impact on quantity tasks. This suggests that current fine-tuning approaches may not be optimal for all relativity types.
- Why unresolved: The paper only explores a limited set of fine-tuning strategies and does not investigate alternative approaches or hyperparameter tuning that could potentially improve performance on challenging tasks.
- What evidence would resolve it: Experimenting with different fine-tuning strategies, such as using different datasets, architectures, or hyperparameter settings, could provide insights into how to optimize MLLMs for quantity and existence relativity tasks.

### Open Question 3
- Question: What are the key architectural or training differences between MLLMs that lead to variations in their performance on different relativity types?
- Basis in paper: [inferred] The paper observes that different MLLMs exhibit varying levels of performance across different relativity types, suggesting that their architectures or training approaches may play a role in their capabilities.
- Why unresolved: The paper does not delve into the specific architectural or training differences that contribute to these performance variations, leaving open questions about how to design MLLMs that excel across all relativity types.
- What evidence would resolve it: A detailed analysis of the architectures and training strategies of different MLLMs, along with ablation studies to identify the key factors influencing their performance on different relativity types, would provide valuable insights.

## Limitations

- Benchmark relies on horizontal image concatenation which may not fully capture complex spatial relationships needed for certain relativity types
- Multiple-choice format may constrain model responses and potentially underestimate capabilities in open-ended settings
- Dataset construction for Existence relativity involves subjective judgments about object presence and similarity thresholds that could introduce bias

## Confidence

- **High confidence**: Overall finding that current MLLMs show notable shortcomings in comparative reasoning across most relativity types (74.7% average accuracy)
- **Medium confidence**: Specific claim about horizontal concatenation being an effective method for comparative reasoning
- **Low confidence**: Assertion that fine-tuning provides limited improvement for quantity comparisons

## Next Checks

1. **Ablation study on image concatenation methods**: Compare performance using horizontal concatenation versus separate image processing with attention mechanisms or prompt-based image ordering to determine if spatial arrangement is truly critical for comparative reasoning success.

2. **Cross-dataset generalization test**: Evaluate models on MLLM-CompBench after training on alternative comparative reasoning datasets (like Kazemi et al. or MMMU) to determine if observed limitations are dataset-specific or reflect fundamental model capabilities.

3. **Open-ended response evaluation**: Repeat the benchmark using open-ended question answering rather than multiple choice to assess whether the format constraint artificially limits model performance, particularly for complex relativity types like spatiality and quantity.