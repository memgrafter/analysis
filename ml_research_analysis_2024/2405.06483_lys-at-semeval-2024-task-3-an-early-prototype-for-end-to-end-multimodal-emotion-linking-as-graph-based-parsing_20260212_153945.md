---
ver: rpa2
title: 'LyS at SemEval-2024 Task 3: An Early Prototype for End-to-End Multimodal Emotion
  Linking as Graph-Based Parsing'
arxiv_id: '2405.06483'
source_url: https://arxiv.org/abs/2405.06483
tags:
- emotion
- linguistics
- multimodal
- decoder
- computational
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a graph-based end-to-end system for multimodal
  emotion cause analysis in conversations. The approach uses a transformer-based encoder
  to contextualize text, image, and audio inputs, combined with a graph-based decoder
  to model cause-effect relations between triggered emotions.
---

# LyS at SemEval-2024 Task 3: An Early Prototype for End-to-End Multimodal Emotion Linking as Graph-Based Parsing

## Quick Facts
- arXiv ID: 2405.06483
- Source URL: https://arxiv.org/abs/2405.06483
- Reference count: 17
- Ranked 7th out of 15 submissions with weighted strict F-score of 6.77 on official SemEval 2024 Task 3 Subtask 1

## Executive Summary
This paper presents a graph-based end-to-end system for multimodal emotion cause analysis in conversations, using a transformer-based encoder with a graph-based decoder to model cause-effect relations between triggered emotions. The system ranked 7th out of 15 submissions in the official SemEval 2024 Task 3 Subtask 1 (text-only) with a weighted strict F-score of 6.77, and achieved post-evaluation F-scores of 20.43 (text-only), 23.36 (text+audio), and 22.17 (text+image) for Subtask 2. The approach predicts emotions, cause-effect relations, and spans in cause utterances, with span prediction significantly impacting performance. Future work will explore smaller, more efficient models for full fine-tuning.

## Method Summary
The system frames emotion-cause analysis as graph-based parsing, using a transformer encoder to contextualize text, image, and audio inputs, combined with a graph-based decoder to model cause-effect relations between triggered emotions. For text, BERT produces utterance embeddings; for images, ViT processes frames through an LSTM; for audio, wav2vec processes waveforms through an LSTM. The decoder uses two biaffine classifiers to compute adjacency scores and emotion labels for edges between utterance pairs, plus a span attention module to identify triggering words. Only the BERT encoder is fine-tuned during training due to resource constraints.

## Key Results
- Ranked 7th out of 15 submissions in official SemEval 2024 Task 3 Subtask 1 (text-only) with weighted strict F-score of 6.77
- Post-evaluation results for Subtask 2 (multimodal): F-scores of 20.43 (text-only), 23.36 (text+audio), and 22.17 (text+image)
- Span prediction significantly impacts performance, with removal increasing F-score from 20.43 to 23.36
- System predicts emotions, cause-effect relations, and spans in cause utterances

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Graph-based decoding from dependency parsing enables modeling emotion-cause relations as directed edges between utterances
- Mechanism: Encoder produces utterance embeddings; decoder applies two biaffine classifiers to compute adjacency scores (cause→effect) and emotion labels for each edge
- Core assumption: Emotion-cause relations can be represented as a directed labeled graph where utterances are nodes and emotions are dependency labels
- Evidence anchors:
  - [abstract] "graph-based methods from dependency parsing to identify causal emotion relations"
  - [section 3.1] "the task can be cast as the estimation of the adjacent matrix of G, similarly to syntactic...dependency parsing"
  - [corpus] No direct evidence; this is a novel application of parsing techniques to emotion analysis
- Break condition: If emotion-cause relations are not well-represented as pairwise directed edges, or if non-local dependencies exist beyond utterance pairs

### Mechanism 2
- Claim: Fine-tuning only the text encoder while freezing multimodal encoders yields better performance than full multimodal fine-tuning
- Mechanism: Text encoder (BERT) is fine-tuned end-to-end with decoder; visual (ViT) and audio (wav2vec) encoders are frozen and only their outputs are projected through trainable LSTMs
- Core assumption: Text modality contains more discriminative information for this task than audio or visual modalities
- Evidence anchors:
  - [section 3.2] "we chose to fine-tune only BERT during training...based on our empirical observation of superior results when learning from text"
  - [section 5] "text-only F-score (20.43) vs multimodal with frozen encoders (11.49)"
  - [corpus] No direct evidence; this is a resource-driven design choice not compared against full fine-tuning
- Break condition: If audio/visual modalities contain critical information not captured in text, or if task requires full multimodal integration

### Mechanism 3
- Claim: Span prediction significantly improves model performance by focusing on specific words triggering emotions
- Mechanism: Span attention module computes binary mask S over words in cause utterance using one-head attention with effect embeddings as keys/values
- Core assumption: Only specific spans within cause utterances trigger emotions in effect utterances
- Evidence anchors:
  - [section 3.1] "predict the adjacent matrix of G with a span that covers the specific words from Ui that trigger the emotion εj"
  - [section 5] "span prediction significantly impacts performance: removing span prediction increases F-score from 20.43 to 23.36"
  - [corpus] No direct evidence; this design choice lacks comparison against models without span prediction
- Break condition: If emotions are triggered by discourse-level context rather than specific word spans, or if span boundaries are ambiguous

## Foundational Learning

- Concept: Dependency parsing as graph-based structured prediction
  - Why needed here: The task is framed as predicting a dependency graph where edges represent emotion-cause relations
  - Quick check question: How do biaffine classifiers compute edge scores and labels in graph-based dependency parsing?

- Concept: Multimodal fusion strategies in neural networks
  - Why needed here: The system combines text, image, and audio modalities through separate encoders and concatenation
  - Quick check question: What are the tradeoffs between early fusion, late fusion, and cross-modal attention for multimodal tasks?

- Concept: Span prediction and attention mechanisms
  - Why needed here: The span attention module identifies specific words within utterances that trigger emotions
  - Quick check question: How does one-head attention with effect embeddings as keys/values produce binary span masks?

## Architecture Onboarding

- Component map:
  Text encoder (BERT) → utterance CLS embeddings; Visual encoder (ViT) → LSTM → visual embedding; Audio encoder (wav2vec) → LSTM → audio embedding; Decoder (biaffine classifiers) → adjacency matrix + emotion tensor; Span attention module → binary span tensor

- Critical path: BERT → decoder biaffine → adjacency scores → emotion labels; BERT + decoder effect embeddings → span attention → span masks

- Design tradeoffs:
  - Fine-tuning only BERT vs full multimodal fine-tuning: computational efficiency vs potential performance loss
  - Graph-based vs sequence-based decoding: structured prediction vs simpler classification
  - Span prediction vs utterance-level prediction: precision vs computational complexity

- Failure signatures:
  - Low precision, high recall: model predicts too many cause-effect relations
  - Span masks all ones/zeros: attention mechanism fails to localize triggering words
  - Frozen multimodal encoders underperform: modality-specific fine-tuning needed

- First 3 experiments:
  1. Remove span prediction module and compare F-scores
  2. Fine-tune all encoders (BERT, ViT, wav2vec) vs only BERT
  3. Replace biaffine decoder with simpler MLP classifier for ablation

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How would different multimodal fusion strategies (early vs. late fusion) impact the performance of the system on the emotion-cause analysis task?
- Basis in paper: [inferred] The paper mentions that their multimodal encoder concatenates unimodal representations before passing them to the decoder, but does not explore alternative fusion strategies or compare their effectiveness.
- Why unresolved: The authors acknowledge resource limitations prevented exhaustive hyperparameter searches and focused only on one fusion approach for multimodal inputs.
- What evidence would resolve it: Systematic comparison of early fusion (concatenating before transformer layers), late fusion (fusing decoder outputs), and cross-modal attention mechanisms, measuring impact on weighted F-scores for both subtasks.

### Open Question 2
- Question: What is the specific contribution of span prediction to the overall performance, and how would removing it affect precision-recall trade-offs?
- Basis in paper: [explicit] The authors note that span prediction significantly impacts performance and that removing it while keeping text inputs increases F-score from 20.43 to a notably higher value.
- Why unresolved: While the authors observe improved F-scores without span prediction, they don't provide detailed analysis of how precision and recall are affected separately, or whether this trade-off is consistent across different emotion types.
- What evidence would resolve it: Ablation studies showing separate precision and recall values with and without span prediction, stratified by emotion type, and analysis of whether conservative predictions (high precision, lower recall) are preferable for practical applications.

### Open Question 3
- Question: How would fine-tuning all multimodal components (BERT, ViT, and wav2vec) simultaneously affect performance compared to the current approach of only fine-tuning BERT?
- Basis in paper: [explicit] The authors note their multimodal system only fine-tuned BERT while freezing pretrained weights for ViT and wav2vec, observing that full fine-tuning was computationally prohibitive but would likely improve results.
- Why unresolved: Resource constraints prevented the authors from testing their hypothesis about full fine-tuning, leaving open questions about whether the observed performance gap between text-only and multimodal approaches is primarily due to insufficient fine-tuning of visual and audio components.
- What evidence would resolve it: Comparative experiments with full fine-tuning of all components versus partial fine-tuning, measuring the relative contribution of each modality when all components are adapted to the specific task rather than relying on general-purpose pretraining.

## Limitations
- Low evaluation scores (6.77 for official text-only task) indicate the task remains challenging and current approach has significant room for improvement
- Frozen multimodal encoders without fine-tuning may limit ability to capture modality-specific nuances
- Span prediction mechanism shows counterintuitive results, with removal improving performance from 20.43 to 23.36

## Confidence
- Mechanism 1 (Graph-based decoding): Medium confidence - novel application of parsing techniques but low evaluation scores suggest limited effectiveness
- Mechanism 2 (Frozen multimodal encoders): Low confidence - primarily driven by computational constraints without systematic comparison
- Mechanism 3 (Span prediction): Low confidence - counterintuitive performance suggests current implementation may be introducing noise

## Next Checks
1. Ablation study on multimodal encoders: Compare performance when fine-tuning all encoders (BERT, ViT, wav2vec) versus freezing multimodal encoders
2. Alternative span prediction strategies: Replace current span attention module with simpler alternatives and compare performance
3. Extended evaluation on different conversational contexts: Test system on conversations with varying emotional intensity, speaker relationships, and discourse structures