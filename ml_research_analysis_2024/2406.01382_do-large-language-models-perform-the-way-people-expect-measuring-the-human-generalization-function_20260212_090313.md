---
ver: rpa2
title: Do Large Language Models Perform the Way People Expect? Measuring the Human
  Generalization Function
arxiv_id: '2406.01382'
source_url: https://arxiv.org/abs/2406.01382
tags:
- human
- questions
- question
- people
- belief
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a framework for evaluating large language
  models (LLMs) based on human beliefs about their capabilities. The authors collect
  a dataset of 19K human generalizations across 79 tasks, showing that humans make
  structured, sparse generalizations about LLM performance.
---

# Do Large Language Models Perform the Way People Expect? Measuring the Human Generalization Function

## Quick Facts
- arXiv ID: 2406.01382
- Source URL: https://arxiv.org/abs/2406.01382
- Authors: Keyon Vafa; Ashesh Rambachan; Sendhil Mullainathan
- Reference count: 36
- Key outcome: This paper introduces a framework for evaluating large language models (LLMs) based on human beliefs about their capabilities, showing that while larger models perform better in low-stakes settings, they can induce overconfidence in high-stakes settings where mistakes are costly, leading to worse deployed performance.

## Executive Summary
This paper introduces a framework for evaluating large language models based on human beliefs about their capabilities rather than raw performance metrics. The authors collect 19K human generalizations across 79 tasks, demonstrating that humans make structured, sparse generalizations about LLM performance. They develop NLP models that predict belief changes with 0.81 AUC, finding that simpler models like BERT outperform larger LLMs. The key insight is that larger models' superior raw capabilities don't always translate to better human-aligned deployment decisions, particularly in high-stakes settings where misaligned beliefs can lead to overconfidence and worse outcomes.

## Method Summary
The authors collect human belief change data through Prolific surveys using MMLU and BBH questions, implementing a bandit sampling approach to efficiently identify non-sparse task pairs where humans update their beliefs. They fine-tune BERT to predict belief changes from concatenated question pairs with correctness indicators, using an ensemble of 5 models. Eight LLMs (including GPT-4, Llama-2 variants, Alpaca, Mistral, GPT-3.5) are evaluated zero-shot on the same questions, and their alignment with human generalizations is measured using weighted generalized accuracy and binary cross-entropy, with BERT predictions serving as estimates of human belief changes.

## Key Results
- Human generalizations about LLM performance are sparse and structured, with BERT achieving 0.81 AUC in predicting belief changes compared to 0.60 for simpler baselines
- Larger LLMs like GPT-4 can perform worse in high-stakes settings due to misalignment with human generalization functions, leading to overconfidence in deployment decisions
- Simpler NLP models like BERT outperform larger, more recent LLMs in predicting human generalizations, suggesting efficacy doesn't strictly scale with model size or complexity

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Human generalizations about LLM performance are sparse and structured, making them predictable.
- Mechanism: Humans only update beliefs when LLM performance on one task provides information about another task. Most task pairs are unrelated, leading to sparse belief changes.
- Core assumption: Human belief updates follow consistent patterns that can be captured by NLP models.
- Evidence anchors:
  - [abstract]: "We show that the human generalization function can be predicted using NLP methods: people have consistent structured ways to generalize."
  - [section 4]: "The best model has an AUC of 0.81, compared to the simplest non-text baseline with an AUC of 0.60."
  - [corpus]: Weak - corpus papers focus on different alignment aspects, not human generalization sparsity.
- Break condition: If human belief updates become completely unstructured or if all task pairs become related.

### Mechanism 2
- Claim: LLM alignment with human generalizations affects deployment performance, especially in high-stakes settings.
- Mechanism: Humans deploy models based on their beliefs about capabilities. If LLM performance doesn't align with these beliefs, deployment decisions become suboptimal, particularly when mistakes are costly.
- Core assumption: Human deployment decisions directly depend on their beliefs about LLM capabilities.
- Evidence anchors:
  - [abstract]: "more capable models (e.g. GPT-4) can do worse on the instances people choose to use them for, exactly because they are not aligned with the human generalization function."
  - [section 5]: "When the cost of mistakes is high, more capable models can perform worse on the instances people choose to use them for because they are not aligned with the human generalization function."
  - [corpus]: Moderate - related papers discuss alignment but focus on different aspects like participatory feedback or anthropomorphic expectations.
- Break condition: If human deployment decisions become completely independent of their beliefs about LLM capabilities.

### Mechanism 3
- Claim: Simpler NLP models like BERT outperform larger LLMs in predicting human generalizations.
- Mechanism: BERT captures the structured patterns in human belief updates better than larger models, possibly because it focuses on relevant features without being distracted by other capabilities.
- Core assumption: The structure needed for predicting human generalizations is present in simpler models and lost in larger ones.
- Evidence anchors:
  - [section 4]: "BERT outperforms larger and more recent language models, suggesting that efficacy in predicting human generalization does not strictly scale with size or complexity."
  - [section 4]: "The models which contain the most structure for predicting generalization are actually some of the simpler and smaller ones (e.g. BERT)."
  - [corpus]: Weak - corpus papers don't address model size vs. generalization prediction performance.
- Break condition: If larger models with different architectures or prompting strategies outperform BERT.

## Foundational Learning

- Concept: Sparse human generalization function
  - Why needed here: Understanding that humans only update beliefs for related task pairs is crucial for designing effective data collection and modeling approaches.
  - Quick check question: Why did the authors use a bandit approach for data collection instead of random sampling?

- Concept: Model alignment with human expectations
  - Why needed here: The core contribution is showing how misalignment between LLM capabilities and human expectations affects deployment performance.
  - Quick check question: What happens to GPT-4's performance when humans become more risk-averse in deployment decisions?

- Concept: Task delegation and human-AI interaction
  - Why needed here: The framework assumes humans delegate tasks to LLMs based on their beliefs, which is fundamental to the evaluation approach.
  - Quick check question: How does this framework differ from traditional supervised learning evaluation?

## Architecture Onboarding

- Component map: Survey data collection pipeline (Prolific integration) -> BERT fine-tuning for belief change prediction -> LLM evaluation framework for alignment measurement -> Bandit algorithm for efficient data collection

- Critical path:
  1. Collect human generalization data via surveys
  2. Train BERT model on belief change prediction
  3. Evaluate LLM alignment using weighted generalized accuracy
  4. Analyze results across different risk tolerance levels

- Design tradeoffs:
  - Using synthetic LLM responses vs. real interactions
  - Choosing between different NLP models for prediction
  - Balancing data collection efficiency vs. representativeness

- Failure signatures:
  - BERT model shows poor AUC on held-out data
  - LLM alignment results show no variation across models
  - Survey responses show no belief changes for any task pairs

- First 3 experiments:
  1. Test BERT vs. Llama-2 on held-out survey data
  2. Evaluate GPT-4 alignment at different risk tolerance levels
  3. Compare human generalization of humans vs. LLMs on same task pairs

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do human generalization functions vary across individuals, and what factors contribute to this heterogeneity?
- Basis in paper: [inferred] The authors note that they study the human generalization function "in the aggregate" and suggest that "the true function may differ between humans" as a limitation of their study.
- Why unresolved: The paper does not collect data from individual humans across multiple interactions, making it difficult to model individual-level generalization functions.
- What evidence would resolve it: Collecting longitudinal data from individual participants across multiple interactions with LLMs, along with demographic and psychological measures, would allow modeling of individual differences in generalization functions.

### Open Question 2
- Question: How does the human generalization function change over time as people gain more experience with LLMs?
- Basis in paper: [explicit] The authors state "the human generalization function may change over time as people understand the capabilities of LLMs better" and suggest collecting examples over time to assess these changes.
- Why unresolved: The paper only collects cross-sectional data at a single point in time, not tracking how individual generalization patterns evolve with experience.
- What evidence would resolve it: Repeated longitudinal surveys of the same participants at multiple timepoints as they interact with increasingly capable LLMs would reveal how generalization functions adapt over time.

### Open Question 3
- Question: How do explanations of LLM decision-making processes affect human generalization patterns and deployment decisions?
- Basis in paper: [inferred] The authors discuss related work on explainable AI (XAI) and note it would be "interesting to explore how such interventions affect human generalizations and deployment decisions" but do not test this empirically.
- Why unresolved: The study focuses on raw LLM outputs without providing any interpretability tools or explanations that might help humans better understand model capabilities.
- What evidence would resolve it: A controlled experiment comparing human generalization patterns and deployment decisions when shown LLM outputs with and without explanations/rationales would reveal the impact of interpretability interventions.

## Limitations
- Framework assumes human deployment decisions are solely based on beliefs about LLM capabilities, potentially oversimplifying real-world contexts where cost, latency, and policies also influence decisions
- Study uses synthetic LLM responses rather than real human-LLM interactions, which may not fully capture actual belief formation complexity
- Generalization patterns may be task-specific to MMLU and BBH datasets used, limiting broader applicability

## Confidence
- Human generalization predictability (High): Multiple experiments show consistent patterns with BERT achieving 0.81 AUC, supported by the sparse structure of human belief updates.
- Model alignment impact on deployment (Medium): Results show correlation between misalignment and worse deployed performance, but real-world deployment decisions may involve additional factors.
- BERT superiority over larger models (Medium): Clear performance differences exist, but the specific reasons for BERT's advantage need further investigation with different model architectures.

## Next Checks
1. Test framework on additional task domains beyond MMLU and BBH to verify generalization of human belief patterns
2. Conduct real human-LLM interaction studies to compare synthetic vs. actual belief formation
3. Evaluate different prompting strategies and model architectures to better understand BERT's performance advantage