---
ver: rpa2
title: Do Language Models Care About Text Quality? Evaluating Web-Crawled Corpora
  Across 11 Languages
arxiv_id: '2403.08693'
source_url: https://arxiv.org/abs/2403.08693
tags:
- language
- corpora
- corpus
- languages
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The study investigates the impact of corpus quality on language
  model performance across 11 European languages. It evaluates four web-crawled corpora
  (CC100, MaCoCu, mC4, and OSCAR) through human annotation and automatic evaluation.
---

# Do Language Models Care About Text Quality? Evaluating Web-Crawled Corpora Across 11 Languages

## Quick Facts
- arXiv ID: 2403.08693
- Source URL: https://arxiv.org/abs/2403.08693
- Reference count: 0
- Primary result: CC100 outperforms other corpora despite lower human-perceived quality

## Executive Summary
This study investigates whether corpus quality affects language model performance across 11 European languages. The researchers evaluate four web-crawled corpora (CC100, MaCoCu, mC4, and OSCAR) through human annotation and automatic evaluation using XLM-RoBERTa models. Surprisingly, they find that CC100, which received the lowest quality rating from human annotators, actually achieves the best performance on downstream tasks including POS tagging, NER, COPA, and CB. This challenges the assumption that higher-quality corpora are necessary for better language model performance.

## Method Summary
The researchers employed a two-pronged evaluation approach. First, they conducted human annotation of corpus quality by randomly sampling 200 paragraphs per corpus-language combination and rating them on a 5-point scale from "perfect" to "completely unusable." Second, they trained XLM-RoBERTa models using continued pretraining on each corpus and evaluated them on four downstream tasks. The training was performed for five languages (Albanian, Croatian, Icelandic, Serbian, Slovenian) across the four corpora, with 100,000 training steps and evaluation every 1,000 steps.

## Key Results
- CC100 outperforms all other corpora on downstream tasks despite receiving the lowest human quality ratings
- No significant correlation exists between human-perceived corpus quality and language model performance
- Corpus size appears to matter more than quality for these evaluation tasks
- MaCoCu and OSCAR contain higher proportions of publishable text, while mC4 shows significant quality issues

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Human-perceived corpus quality does not correlate with downstream LM performance.
- Mechanism: Quality assessments focus on publishable text proportion, but LMs learn effectively from imperfect running text.
- Core assumption: Models trained on imperfect running text achieve comparable downstream performance to those trained on high-quality text.
- Evidence anchors:
  - [abstract] "we find that CC100 actually obtains the best performance, while OSCAR is the worst performing corpus"
  - [section 4.2] "MaCoCu and OSCAR contain paragraphs that are most often at least running text"
  - [section 5.1] "Continuing training a multi-lingual language model on a specific language of interest is a simple and (relatively) cheap way of improving performance"

### Mechanism 2
- Claim: Corpus size and diversity may matter more than human-perceived quality.
- Mechanism: Larger corpora with more unique data provide better LM training despite quality issues.
- Core assumption: Model performance scales primarily with data volume rather than quality for these tasks.
- Evidence anchors:
  - [section 5.1] "CC100 actually obtains the best performance, even ahead of the combined model"
  - [section 5.1] "we do not find any indication that the quality of the data significantly influences performance"
  - [section 5.1] "the only corpus that is noticeably smaller for all languages is OSCAR"

### Mechanism 3
- Claim: Continued training on XLM-R benefits from additional exposure to similar language patterns.
- Mechanism: Even imperfect text from CC100 (used in XLM-R pretraining) reinforces learned representations.
- Core assumption: Model benefits from seeing more examples of language patterns, even imperfect ones.
- Evidence anchors:
  - [section 5.1] "we improve on the XLM-R baseline in virtually all settings"
  - [section 5.1] "CC100 corpus was in fact already used for pretraining XLM-R"
  - [section 5.1] "we can be reasonably sure that this did not negatively affect the results of the CC100 corpus"

## Foundational Learning

- Concept: Corpus evaluation methodology
  - Why needed here: Understanding how quality was assessed determines what conclusions can be drawn
  - Quick check question: What are the five annotation categories and what do they represent?

- Concept: Continued pretraining vs. training from scratch
  - Why needed here: The study uses continued training, which affects results interpretation
  - Quick check question: What are the advantages of continued training over training from scratch?

- Concept: Downstream task evaluation
  - Why needed here: Performance is measured on specific tasks, not overall language understanding
  - Quick check question: Which four tasks were used for evaluation and what do they measure?

## Architecture Onboarding

- Component map:
  - Data pipeline: Four corpora (CC100, MaCoCu, mC4, OSCAR) → random paragraph sampling → human annotation
  - Training pipeline: XLM-R base model → continued training on each corpus/language combination → fine-tuning on downstream tasks
  - Evaluation pipeline: Trained models → fine-tuning → performance measurement on POS, NER, COPA, CB

- Critical path:
  - Corpus quality annotation → continued training of XLM-R → fine-tuning on downstream tasks → performance comparison

- Design tradeoffs:
  - Human annotation provides quality insights but doesn't correlate with performance
  - Using existing XLM-R model enables efficient training but may bias toward CC100
  - Controlling for data size reveals size matters more than quality, but doesn't capture all factors

- Failure signatures:
  - If models fail to converge during continued training
  - If downstream task performance is significantly worse than baseline
  - If manual quality assessment shows clear differences but automatic evaluation does not

- First 3 experiments:
  1. Run manual annotation on a small sample from each corpus to verify quality assessment methodology
  2. Train XLM-R on a single corpus/language combination for 10,000 steps to verify continued training setup
  3. Fine-tune trained model on POS tagging task to verify downstream evaluation pipeline

## Open Questions the Paper Calls Out
None explicitly stated in the paper.

## Limitations
- Manual quality assessment was performed on a relatively small sample (200 paragraphs per corpus-language combination)
- Evaluation focuses on 11 European languages, limiting generalizability to other language families
- Downstream tasks (POS, NER, COPA, CB) represent a limited set of language understanding challenges
- Continued training approach means all models benefit from XLM-R's pretraining on CC100 data

## Confidence

**High Confidence**: The finding that CC100 outperforms other corpora in downstream task performance is well-supported by the experimental results. The methodology for continued training and evaluation is clearly specified and reproducible.

**Medium Confidence**: The conclusion that human-perceived corpus quality does not correlate with downstream performance is supported but should be interpreted cautiously. The manual quality assessment, while systematic, covers a small fraction of the total data and focuses on paragraph-level quality rather than document-level coherence or topical diversity.

**Low Confidence**: The broader implication that corpus curation quality is unimportant for LM training is not directly supported by this study. The results show that for the specific setup (continued training on XLM-R with four specific downstream tasks), quality doesn't matter, but this may not extend to other training paradigms or task types.

## Next Checks

1. **Extended Quality Assessment**: Conduct manual annotation on a larger sample size (1000+ paragraphs per corpus-language) and include document-level coherence evaluation to verify whether the initial quality assessment holds at scale.

2. **Training-from-Scratch Comparison**: Train language models from scratch on each corpus to determine whether the lack of quality correlation persists when models don't benefit from pretraining on CC100 data.

3. **Task Diversity Expansion**: Evaluate models on additional downstream tasks including document summarization, long-form generation, and domain-specific classification to test whether quality effects emerge in tasks beyond the current evaluation set.