---
ver: rpa2
title: Bayesian Preference Elicitation with Language Models
arxiv_id: '2403.05534'
source_url: https://arxiv.org/abs/2403.05534
tags:
- user
- preferences
- open
- questions
- option
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: OPEN combines language models (LMs) with Bayesian Optimal Experimental
  Design (BOED) to elicit human preferences through natural language interaction.
  The approach uses an LM to extract domain-relevant features and verbalize BOED-selected
  pairwise comparison queries, enabling principled yet flexible preference learning
  in complex, open-ended domains.
---

# Bayesian Preference Elicitation with Language Models

## Quick Facts
- arXiv ID: 2403.05534
- Source URL: https://arxiv.org/abs/2403.05534
- Authors: Kunal Handa; Yarin Gal; Ellie Pavlick; Noah Goodman; Jacob Andreas; Alex Tamkin; Belinda Z. Li
- Reference count: 33
- Primary result: OPEN achieves TIDA of 0.49 in user studies vs 0.37-0.40 for baselines

## Executive Summary
This paper presents OPEN, a framework that combines language models (LMs) with Bayesian Optimal Experimental Design (BOED) to elicit human preferences through natural language interaction. The approach uses an LM to extract domain-relevant features and verbalize BOED-selected pairwise comparison queries, enabling principled yet flexible preference learning in complex, open-ended domains. In user studies on content recommendation, OPEN significantly outperformed both LM-only and BOED-only baselines, demonstrating that explicit uncertainty modeling via BOED is crucial for informative queries, while natural language verbalization is essential for user comprehension and accurate preference specification.

## Method Summary
OPEN integrates language models with Bayesian Optimal Experimental Design to elicit human preferences through natural language interaction. The method uses an LM to extract domain-relevant features from items being compared, then applies BOED to select maximally informative pairwise comparison queries. These queries are verbalized into natural language using the same LM, creating a closed loop where the LM both extracts features and generates questions. The framework models preferences as a Gaussian process over the LM-extracted features, allowing it to handle complex, high-dimensional preference spaces while maintaining uncertainty quantification through the BOED framework.

## Key Results
- OPEN achieved a Time-Integrated Delta Accuracy (TIDA) of 0.49 in user studies, significantly outperforming LM-only (0.37) and BOED-only (0.40) baselines
- The combination of LM-based feature extraction with BOED query selection was essential for superior performance
- Natural language verbalization of queries was crucial for user comprehension and accurate preference specification

## Why This Works (Mechanism)
OPEN works by combining the complementary strengths of language models and Bayesian optimal experimental design. Language models provide rich, domain-specific feature representations extracted through zero-shot prompts, capturing the semantic nuances of complex items like movies or books. Bayesian Optimal Experimental Design then uses these features to quantify uncertainty and select queries that maximize information gain about the user's underlying preference function. The LM also verbalizes these queries into natural language, making them comprehensible to users while preserving the underlying information-theoretic properties. This creates a feedback loop where uncertainty quantification drives query selection, and natural language communication enables accurate preference elicitation.

## Foundational Learning

**Bayesian Optimal Experimental Design (BOED)**: A framework for selecting experiments that maximize expected information gain about unknown parameters. Why needed: BOED provides principled uncertainty quantification for query selection. Quick check: Can compute expected information gain using the posterior entropy formula.

**Gaussian Process Preference Learning**: Models preferences as a distribution over functions using a GP prior. Why needed: Handles complex, non-linear preference relationships while maintaining uncertainty estimates. Quick check: Can compute posterior distribution over preferences given pairwise comparisons.

**Language Model Feature Extraction**: Uses LMs to extract semantic features from items through prompt engineering. Why needed: Provides rich, domain-relevant representations without manual feature engineering. Quick check: Can extract consistent feature vectors across item pairs using zero-shot prompts.

**Query Verbalization**: Converts structured queries into natural language using LMs. Why needed: Makes information-theoretic queries comprehensible to human users. Quick check: Can generate clear pairwise comparison questions from feature-level queries.

## Architecture Onboarding

**Component Map**: LM Feature Extractor -> BOED Query Selector -> LM Query Verbalizer -> User Preference Observation -> GP Preference Model -> (feedback to BOED)

**Critical Path**: The query selection loop: LM extracts features → BOED selects next query → LM verbalizes query → user provides preference → GP updates preference model → (repeat)

**Design Tradeoffs**: Using LM for both feature extraction and verbalization trades computational efficiency for consistency, while zero-shot prompting avoids fine-tuning but may limit feature quality in specialized domains.

**Failure Signatures**: Poor feature extraction leads to uninformative queries; overly complex verbalizations confuse users; insufficient uncertainty quantification causes premature convergence to incorrect preferences.

**First Experiments**:
1. Test feature extraction consistency by comparing vectors for semantically similar items
2. Validate query informativeness by measuring posterior uncertainty reduction
3. Assess verbalization clarity through user comprehension tests

## Open Questions the Paper Calls Out
None

## Limitations
- Evaluation was limited to a single content recommendation domain (movies and books)
- User study sample size was relatively small, limiting generalizability claims
- Zero-shot LM feature extraction may not generalize well to specialized or multilingual domains

## Confidence

**High confidence**: Theoretical framework combining BOED with LM-based feature extraction is mathematically sound
**Medium confidence**: Empirical results showing superior performance are robust for tested domain but generalizability uncertain
**Medium confidence**: Claims about natural language verbalization's importance are supported by qualitative observations

## Next Checks

1. Conduct cross-domain validation by testing OPEN in diverse preference elicitation scenarios (e.g., travel planning, product configuration, medical treatment selection) to assess generalizability
2. Perform ablation studies systematically varying the number of LM features, query complexity, and verbalization approaches to quantify their relative contributions to performance
3. Scale up user studies with larger, more diverse participant pools and include longitudinal studies to evaluate preference stability over extended interaction sequences