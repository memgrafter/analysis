---
ver: rpa2
title: Hard-label based Small Query Black-box Adversarial Attack
arxiv_id: '2403.06014'
source_url: https://arxiv.org/abs/2403.06014
tags:
- attack
- adversarial
- attacks
- methods
- example
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the challenge of hard-label black-box adversarial
  attacks, which are more practical but less efficient than soft-label attacks due
  to limited access to the target model's output. The proposed method, SQBA, combines
  transfer-based and query-based approaches by leveraging gradients from a pre-trained
  surrogate model to guide the search for adversarial examples.
---

# Hard-label based Small Query Black-box Adversarial Attack

## Quick Facts
- arXiv ID: 2403.06014
- Source URL: https://arxiv.org/abs/2403.06014
- Reference count: 40
- The paper proposes SQBA, achieving up to 5x higher attack success rates than state-of-the-art methods at small query budgets (100-250 queries).

## Executive Summary
This paper addresses the challenge of hard-label black-box adversarial attacks, which are more practical but less efficient than soft-label attacks due to limited access to the target model's output. The proposed method, SQBA, combines transfer-based and query-based approaches by leveraging gradients from a pre-trained surrogate model to guide the search for adversarial examples. Experiments on CIFAR-10 and ImageNet datasets show that SQBA achieves significantly higher attack success rates compared to state-of-the-art methods, especially at small query budgets (100-250 queries). For instance, SQBA achieved 82% success rate on ResNet-18 with 1000 queries, significantly outperforming baselines like HSJA (60.5%) and Sign-Opt (32.1%). The method also generalizes well across different model architectures and datasets, demonstrating its effectiveness in practical black-box attack scenarios.

## Method Summary
SQBA integrates transfer-based and query-based approaches for hard-label black-box adversarial attacks. It uses gradients from a pre-trained surrogate model to estimate a favorable search direction, then refines this with Monte Carlo sampling to estimate the hard-label gradient, guiding the search more efficiently toward adversarial examples. The method employs a dual-gradient approach where the surrogate model provides an initial gradient direction, and the query-based gradient estimation refines this direction using finite differences. SQBA also includes a multi-gradient selection mechanism to choose the most effective initial direction from multiple surrogate gradients, and an adaptive switching mechanism to prevent local minima by transitioning between surrogate-guided and pure hard-label optimization based on line search progress.

## Key Results
- SQBA achieved 82% attack success rate on ResNet-18 with 1000 queries, outperforming HSJA (60.5%) and Sign-Opt (32.1%)
- At small query budgets (100-250 queries), SQBA showed up to 5x higher success rates compared to state-of-the-art methods
- The method demonstrated strong generalization across different model architectures and datasets, including CIFAR-10 and ImageNet

## Why This Works (Mechanism)

### Mechanism 1
- Claim: SQBA combines surrogate gradient guidance with hard-label optimization to reduce query count in black-box attacks.
- Mechanism: SQBA uses gradients from a pre-trained surrogate model to estimate a favorable search direction, then refines this with Monte Carlo sampling to estimate the hard-label gradient, guiding the search more efficiently toward adversarial examples.
- Core assumption: The surrogate model's gradient direction is a useful prior that remains correlated with the target model's true gradient, especially near the decision boundary.
- Evidence anchors:
  - [abstract]: "Experiments show the proposed method significantly improves the query efficiency of the hard-label based black-box attack across various target model architectures."
  - [section]: "SQBA integrates the transfer based attack to take advantage of the gradients generated from a surrogate model, and applies gradient-free optimisation introduced in [8]."
  - [corpus]: Weak overlap with related work; no direct evidence of gradient correlation maintenance.
- Break condition: If the surrogate and target models diverge significantly, the initial gradient direction becomes misleading, and the search stalls or diverges.

### Mechanism 2
- Claim: Multi-gradient selection from surrogate model mitigates gradient transferability failures.
- Mechanism: Multiple gradients are sampled from surrogate predictions at different distances from the original input; the one most perpendicular to the surrogate's decision boundary and closest to the input is chosen as the initial direction.
- Core assumption: The direction of steepest ascent in surrogate loss becomes increasingly perpendicular to the surrogate's decision boundary as the point moves away from the original input.
- Evidence anchors:
  - [section]: "The iterate approximate gradient, therefore, can be chosen from the calculated gradient vectors such that ηi≥0.2 and H(˜x + 2δµi) = 1."
  - [corpus]: No direct supporting citations; this is inferred from the method description.
- Break condition: If the target model's decision boundary is vastly different, the perpendicular direction in surrogate space may not correspond to a useful adversarial direction in target space.

### Mechanism 3
- Claim: Adaptive switching between surrogate-guided and pure hard-label gradients prevents local minima.
- Mechanism: A binary weight β is initially set to use the surrogate gradient; when the search progress stalls (indicated by a small step size), β switches to favor pure hard-label Monte Carlo sampling.
- Core assumption: A small step size in line search indicates being trapped in a local minimum, necessitating a switch to a more exploratory gradient.
- Evidence anchors:
  - [section]: "The proposed method begins with β= 1 meaning∇Hw(x′t) is solely used to approximate iterate gradients. When∇Hw(x′t) is indicated to be in a local minima, β is then switched to 0..."
  - [corpus]: No supporting literature cited; this is an internal design choice.
- Break condition: If the line search never produces a small step size even in a local minimum, the switch never occurs, and progress stalls.

## Foundational Learning

- Concept: Zeroth-order optimization
  - Why needed here: Hard-label attacks lack gradient information, so numerical estimation of gradients via finite differences or random sampling is required.
  - Quick check question: What is the key difference between zeroth-order and first-order optimization in the context of black-box attacks?

- Concept: Adversarial transferability
  - Why needed here: SQBA leverages gradients from a surrogate model, relying on the assumption that adversarial examples transfer across models trained on the same task.
  - Quick check question: Under what conditions does adversarial transferability tend to break down?

- Concept: Hard-label vs soft-label outputs
  - Why needed here: SQBA operates under hard-label constraints (only class predictions available), making the optimization problem non-smooth and requiring specialized methods.
  - Quick check question: Why is it more challenging to perform gradient estimation with only hard-label feedback compared to probability distributions?

## Architecture Onboarding

- Component map:
  Surrogate model inference -> gradient computation (Dual Gradient Method) -> Multi-gradient selection module -> Monte Carlo sampling -> Line search + boundary tuning -> Binary search for proximity

- Critical path:
  1. Compute initial surrogate gradient
  2. Select best multi-gradient candidate
  3. Estimate hard-label gradient
  4. Combine gradients with adaptive weight β
  5. Perform line search to update example
  6. Apply binary search for proximity

- Design tradeoffs:
  - Using surrogate gradients speeds convergence but risks misdirection if transferability fails.
  - Multi-gradient selection adds computation but improves robustness to poor surrogate gradients.
  - Adaptive β switching introduces complexity but prevents stagnation in local minima.

- Failure signatures:
  - Low attack success rate despite many queries -> surrogate-target mismatch.
  - Oscillating or slow progress in line search -> poor gradient estimates.
  - Query budget exhausted before reaching boundary -> inadequate exploration or bad initial direction.

- First 3 experiments:
  1. Run SQBA with a perfect surrogate (same architecture as target) to measure upper performance bound.
  2. Test SQBA with a mismatched surrogate (different architecture) to measure sensitivity to transferability gaps.
  3. Disable multi-gradient selection to quantify its contribution to attack success.

## Open Questions the Paper Calls Out
None

## Limitations
- The paper's claims rely heavily on the assumption that surrogate gradients remain meaningfully correlated with target model gradients in hard-label settings, which is not empirically validated beyond the presented attack success rates.
- The adaptive switching mechanism for β is introduced without theoretical justification for why line search step size reliably indicates local minima.
- The multi-gradient selection process, while intuitively appealing, lacks comparative ablation studies to quantify its individual contribution to performance gains.

## Confidence
- High confidence: SQBA's overall architecture and integration of surrogate gradients with query-based optimization is sound and well-described
- Medium confidence: The specific performance improvements (5x query efficiency) are credible but depend on unverified transferability assumptions
- Low confidence: The theoretical basis for the multi-gradient selection criterion and adaptive β switching mechanism

## Next Checks
1. Perform ablation studies isolating the contributions of multi-gradient selection, adaptive β switching, and surrogate gradient guidance to quantify their individual impact on attack success rates
2. Measure and report surrogate-target gradient correlation (cosine similarity) across different model pairs and input regions to empirically validate the transferability assumption
3. Test SQBA's robustness to surrogate model quality by systematically varying surrogate-target architectural similarity and measuring performance degradation