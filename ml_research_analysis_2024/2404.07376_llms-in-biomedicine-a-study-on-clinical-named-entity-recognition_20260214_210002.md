---
ver: rpa2
title: 'LLMs in Biomedicine: A study on clinical Named Entity Recognition'
arxiv_id: '2404.07376'
source_url: https://arxiv.org/abs/2404.07376
tags:
- arxiv
- entity
- preprint
- llms
- biomedical
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper explores the use of Large Language Models (LLMs) for
  clinical Named Entity Recognition (NER) tasks in biomedicine. The authors investigate
  strategies to enhance LLM performance, focusing on prompt engineering, in-context
  learning, and the integration of external biomedical knowledge.
---

# LLMs in Biomedicine: A study on clinical Named Entity Recognition

## Quick Facts
- arXiv ID: 2404.07376
- Source URL: https://arxiv.org/abs/2404.07376
- Reference count: 12
- Key outcome: DiRAG method improves zero-shot clinical NER performance, achieving an F1 score increase of 5.6% on the I2B2 dataset and 0.5% on the NCBI-disease dataset

## Executive Summary
This paper explores the use of Large Language Models (LLMs) for clinical Named Entity Recognition (NER) tasks in biomedicine. The authors investigate strategies to enhance LLM performance, focusing on prompt engineering, in-context learning, and the integration of external biomedical knowledge. They demonstrate that carefully designed prompts, particularly the TANL format, significantly improve performance. The selection of relevant in-context examples using the KATE method, with biomedical-specific encoders, further boosts results. The proposed method, DiRAG, which leverages a medical knowledge base like UMLS, substantially improves zero-shot clinical NER performance, achieving an F1 score increase of 5.6% on the I2B2 dataset and 0.5% on the NCBI-disease dataset. These findings highlight the potential of LLMs for clinical NER when combined with appropriate techniques and external knowledge sources.

## Method Summary
The paper proposes a comprehensive approach to enhance LLM performance for clinical NER tasks. The method consists of three main components: prompt engineering, in-context learning, and knowledge integration. For prompt engineering, the authors explore different formats, with the TANL format showing the most promise. In-context learning is improved through the KATE method, which selects relevant examples using biomedical-specific encoders. The DiRAG method integrates external biomedical knowledge from sources like UMLS to further boost performance. The approach is evaluated on two datasets: I2B2 and NCBI-disease, demonstrating significant improvements in zero-shot clinical NER performance.

## Key Results
- DiRAG method improves zero-shot clinical NER performance, achieving an F1 score increase of 5.6% on the I2B2 dataset
- The TANL prompt format significantly improves performance compared to other formats
- KATE method for in-context learning, using biomedical-specific encoders, enhances results
- Integration of external biomedical knowledge from sources like UMLS further boosts performance

## Why This Works (Mechanism)
The paper's approach works by leveraging the strengths of LLMs while addressing their limitations in the biomedical domain. Prompt engineering with the TANL format provides a clear structure for the model to understand the task. In-context learning with KATE-selected examples allows the model to learn from relevant instances, improving its ability to generalize. The integration of external biomedical knowledge through DiRAG compensates for the model's potential lack of domain-specific information, enabling it to make more accurate predictions in the clinical context.

## Foundational Learning

1. **Clinical Named Entity Recognition (NER)**: Identifying and classifying medical entities in clinical text.
   - Why needed: Essential for extracting valuable information from electronic health records and medical literature.
   - Quick check: Ability to identify and classify entities like diseases, symptoms, and treatments in clinical text.

2. **Prompt Engineering**: Designing effective prompts to guide LLM behavior.
   - Why needed: LLMs require specific instructions to perform tasks accurately.
   - Quick check: Understanding how different prompt formats affect model performance.

3. **In-context Learning**: Teaching models through examples within the prompt.
   - Why needed: Allows models to learn new tasks without fine-tuning.
   - Quick check: Ability to select and use relevant examples to improve model performance.

4. **Knowledge Integration**: Incorporating external knowledge into LLM inference.
   - Why needed: Enhances model performance by providing domain-specific information.
   - Quick check: Understanding how external knowledge sources like UMLS can be integrated into the inference process.

## Architecture Onboarding

**Component Map**: Prompt Engineering -> In-context Learning (KATE) -> Knowledge Integration (DiRAG) -> LLM Inference

**Critical Path**: The critical path for the DiRAG method involves: 1) Preparing the prompt with the TANL format, 2) Selecting in-context examples using KATE, 3) Integrating relevant biomedical knowledge, and 4) Feeding the augmented prompt to the LLM for inference.

**Design Tradeoffs**: The main tradeoff is between performance improvement and computational overhead. While the DiRAG method significantly enhances performance, it requires additional computation for knowledge integration and example selection. This may impact inference speed and resource requirements.

**Failure Signatures**: Potential failures may occur if the selected in-context examples are not truly representative of the target domain or if the integrated knowledge is outdated or irrelevant. Additionally, the method may struggle with entities not well-covered in the external knowledge base.

**First Experiments**:
1. Test the DiRAG method on additional clinical NER datasets to assess generalizability across different medical domains and entity types.
2. Evaluate the computational overhead and inference time of the DiRAG method compared to baseline approaches.
3. Conduct an ablation study to quantify the individual contributions of prompt engineering, in-context learning, and knowledge integration to the overall performance improvements.

## Open Questions the Paper Calls Out
None

## Limitations
- Evaluation based on two datasets (I2B2 and NCBI-disease) may not fully represent the diversity of clinical NER tasks
- Performance improvements show variation across different prompting strategies and knowledge integration methods
- DiRAG method requires access to comprehensive medical knowledge bases like UMLS, which may not be universally available or up-to-date

## Confidence

**Major Claims and Confidence Levels:**
- **LLMs can effectively perform clinical NER with appropriate prompt engineering (High confidence)**: The results consistently show that well-designed prompts significantly improve performance across different datasets.
- **In-context learning with KATE-selected examples enhances performance (Medium confidence)**: While improvements are demonstrated, the method's effectiveness may depend on the specific biomedical encoders used and the quality of example selection.
- **Integration of external biomedical knowledge substantially improves zero-shot performance (High confidence)**: The DiRAG method shows clear benefits, particularly for the I2B2 dataset, though the impact varies between datasets.

## Next Checks

1. Test the proposed methods on additional clinical NER datasets (e.g., n2c2, CADEC) to assess generalizability across different medical domains and entity types.
2. Evaluate the computational overhead and inference time of the DiRAG method compared to baseline approaches, as the integration of external knowledge may impact practical deployment.
3. Conduct an ablation study to quantify the individual contributions of prompt engineering, in-context learning, and knowledge integration to the overall performance improvements.