---
ver: rpa2
title: 'Confidence Under the Hood: An Investigation into the Confidence-Probability
  Alignment in Large Language Models'
arxiv_id: '2405.16282'
source_url: https://arxiv.org/abs/2405.16282
tags:
- confidence
- certain
- answer
- certainty
- alignment
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces the concept of Confidence-Probability Alignment
  to evaluate how well a language model's verbalized certainty matches its internal
  confidence as measured by token probabilities. The authors construct prompts that
  elicit both an answer and a self-assessed certainty level, then compute Spearman
  correlations between these measures.
---

# Confidence Under the Hood: An Investigation into the Confidence-Probability Alignment in Large Language Models

## Quick Facts
- arXiv ID: 2405.16282
- Source URL: https://arxiv.org/abs/2405.16282
- Reference count: 15
- Key outcome: GPT-4 shows strongest confidence-probability alignment (average Spearman's ρ of 0.42) across five datasets, outperforming other models

## Executive Summary
This paper introduces the concept of Confidence-Probability Alignment to evaluate how well a language model's verbalized certainty matches its internal confidence as measured by token probabilities. The authors construct prompts that elicit both an answer and a self-assessed certainty level, then compute Spearman correlations between these measures. Across five datasets, GPT-4 shows the strongest alignment, outperforming other models like InstructGPT-3 + RLHF, GPT-3, Zephyr-7B, and Microsoft's Phi-2. The study also examines the impact of temperature on certainty variability and finds GPT-4's certainty is more stable. Additionally, they analyze how verbalized certainty and internal confidence relate to answer correctness, finding higher confidence generally corresponds with higher accuracy in GPT-4.

## Method Summary
The study evaluates confidence-probability alignment by constructing structured prompts that elicit both model answers and self-assessed certainty levels. For each multiple-choice question, the model generates responses with token probabilities extracted, then uses a Confidence Querying Prompt (CQP) with third-person perspective, option contextualization, and Likert scale to obtain verbalized certainty. Spearman correlation coefficients are computed between internal confidence (token probabilities) and verbalized certainty across five datasets and six models. The approach quantifies alignment by measuring how well the model's internal confidence signals correspond to its explicit confidence statements.

## Key Results
- GPT-4 demonstrates strongest confidence-probability alignment with average Spearman's ρ of 0.42 across datasets
- Structured prompting techniques (third-person perspective, option contextualization, Likert scale) improve alignment
- Temperature affects verbalized certainty variability, with GPT-4 showing more stable certainty across temperature settings
- Higher verbalized certainty generally corresponds with higher answer accuracy in GPT-4

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Confidence-Probability Alignment measures the correlation between a model's internal token-level confidence and its verbally expressed certainty.
- Mechanism: The study constructs prompts that elicit both an answer and a self-assessed certainty level, then computes Spearman correlations between these measures.
- Core assumption: Higher token probabilities correspond to higher model confidence, and verbalized certainty accurately reflects the model's true confidence.
- Evidence anchors:
  - [abstract] "We introduce the concept of Confidence-Probability Alignment, that connects an LLM's internal confidence, quantified by token probabilities, to the confidence conveyed in the model's response when explicitly asked about its certainty."
  - [section 2.3] "We define verbalized certainty as the language model's explicit expression of its confidence level in its own answer to a given question."
  - [corpus] Found 25 related papers; average neighbor FMR=0.509. Direct Confidence Alignment: Aligning Verbalized Confidence with Internal Confidence In Large Language Models is highly related.
- Break condition: If token probabilities do not accurately represent internal confidence (e.g., due to tokenization issues or model architecture), the alignment measure becomes unreliable.

### Mechanism 2
- Claim: Structured prompting techniques improve the alignment between internal and verbalized confidence.
- Mechanism: The study uses a Confidence Querying Prompt (CQP) with third-person perspective, option contextualization, and a Likert scale to elicit verbalized certainty.
- Core assumption: The CQP design effectively encourages the model to introspect and provide accurate confidence assessments.
- Evidence anchors:
  - [section 2.3] "The CQP is presented as follows: A language model was asked: {question} Options were: {choices_text}. The model 's answer was: {response_text}. Analyse its answer given other options. How certain are you of the model 's answer?"
  - [section 5.2] "In Table 4 we analyzed the individual and combined impacts of three proposed prompting techniques: Simulation of Third-Person Perspective (TTP), Option Contextualization (OC), and Likert Scale Utilization (LSU)."
  - [corpus] Annotation-Efficient Universal Honesty Alignment explores training-based calibration, suggesting prompting alone may have limitations.
- Break condition: If the model does not engage with the prompt structure as intended, or if the prompt design does not generalize across tasks, the alignment may not improve.

### Mechanism 3
- Claim: GPT-4 exhibits stronger confidence-probability alignment compared to other models due to its architecture and training.
- Mechanism: The study finds GPT-4 consistently outperforms other models (InstructGPT-3, GPT-3, Zephyr-7B, Phi-2) across multiple datasets.
- Core assumption: GPT-4's architecture and training methodology (including RLHF) lead to better alignment between internal and verbalized confidence.
- Evidence anchors:
  - [abstract] "Notably, among the models analyzed, OpenAI's GPT-4 showed the strongest confidence-probability alignment, with an average Spearman's $\hatρ$ of 0.42, across a wide range of tasks."
  - [section 4] "Here, GPT-4 consistently outperforms its counterparts across every dataset. Specifically, it registered the highest coefficient on the QASC datasets of nearly 0.5, indicative of a moderate correlation."
  - [corpus] Does Alignment Tuning Really Break LLMs' Internal Confidence? suggests calibration can degrade, but GPT-4's RLHF may mitigate this.
- Break condition: If GPT-4's architecture or training does not inherently improve alignment, or if the datasets used do not capture the full range of model capabilities, the observed superiority may not hold.

## Foundational Learning

- Concept: Spearman's rank correlation coefficient
  - Why needed here: Used to measure the alignment between internal confidence and verbalized certainty.
  - Quick check question: What is the key difference between Spearman's and Pearson's correlation coefficients?

- Concept: Token probabilities and log probabilities
  - Why needed here: Internal confidence is quantified using token probabilities derived from log probabilities or logits.
  - Quick check question: How are token probabilities calculated from log probabilities in GPT-family models?

- Concept: Reinforcement Learning from Human Feedback (RLHF)
  - Why needed here: GPT-4's training includes RLHF, which may contribute to its stronger confidence-probability alignment.
  - Quick check question: How does RLHF differ from standard language model training?

## Architecture Onboarding

- Component map: Confidence Querying Prompt (CQP) -> Token Probability Extraction -> Spearman Correlation Computation -> Alignment Evaluation
- Critical path: 1) Construct structured prompts with questions and answer options. 2) Obtain model responses and extract token probabilities. 3) Use CQP to elicit verbalized certainty. 4) Compute Spearman correlation between internal confidence and verbalized certainty.
- Design tradeoffs: The study focuses on multiple-choice questions for controlled confidence elicitation, but this may limit generalizability to open-ended tasks. The use of Spearman correlation allows for non-linear relationships but may miss other alignment patterns.
- Failure signatures: Poor alignment may indicate issues with prompt design, tokenization, or model architecture. Very low or negative correlations suggest the model's internal confidence does not correspond to its verbalized certainty.
- First 3 experiments:
  1. Replicate the alignment evaluation on a held-out dataset to verify consistency.
  2. Test the impact of different temperature settings on alignment to understand model sensitivity.
  3. Apply the CQP to a different model architecture (e.g., PaLM) to assess generalizability.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal temperature parameter for balancing verbalized certainty stability and model performance across different datasets?
- Basis in paper: [inferred] The paper analyzes temperature's impact on verbalized certainty variability in GPT-4, showing that higher temperatures increase variability, with datasets like OpenBookQA and Riddle Sense being most sensitive.
- Why unresolved: The study only examines temperature's effect on variability, not its optimal setting for balancing stability and performance. Different datasets show varying sensitivities, suggesting no universal optimal temperature exists.
- What evidence would resolve it: Systematic experiments testing different temperature values across multiple datasets while measuring both certainty stability metrics and task performance metrics (accuracy, F1 score, etc.) to identify Pareto-optimal temperature settings.

### Open Question 2
- Question: How do Confidence-Probability Alignment scores translate to real-world reliability in high-stakes applications?
- Basis in paper: [explicit] The authors note that understanding alignment is crucial for reliability in high-stakes areas like healthcare, law, and education, but do not empirically validate this connection.
- Why unresolved: The paper establishes correlation metrics between internal and verbalized confidence but does not demonstrate how these translate to actual reliability or error rates in practical applications.
- What evidence would resolve it: Empirical studies applying models with different alignment scores to real-world tasks, measuring actual error rates, user trust levels, and decision-making outcomes compared to alignment metrics.

### Open Question 3
- Question: Can model architectures be optimized specifically to improve Confidence-Probability Alignment rather than general performance?
- Basis in paper: [inferred] GPT-4 shows the best alignment but the authors suggest this may be due to training methodology improvements rather than architectural optimizations targeting alignment specifically.
- Why unresolved: The study observes correlation between alignment and model versions but doesn't investigate whether architectural changes could directly target alignment improvement.
- What evidence would resolve it: Ablation studies comparing models with identical training data and objectives but different architectural choices (attention mechanisms, layer depth, etc.) while measuring alignment scores, or training architectures with alignment as an explicit optimization objective.

## Limitations
- Tokenization effects on confidence measurement may artificially inflate or deflate correlation values
- Prompting methodology constraints limit generalizability to open-ended reasoning tasks
- Dataset representativeness may not capture full spectrum of LLM capabilities and failure modes

## Confidence
- **High confidence**: GPT-4 demonstrates superior confidence-probability alignment compared to other models across multiple datasets
- **Medium confidence**: Structured prompting significantly improves confidence-probability alignment
- **Medium confidence**: Higher verbalized certainty correlates with higher answer accuracy in GPT-4

## Next Checks
1. **Ablation study of prompting techniques**: Systematically remove each component of the Confidence Querying Prompt to quantify individual contributions to alignment improvement
2. **Cross-task generalization test**: Apply the confidence-probability alignment evaluation to open-ended reasoning tasks to assess whether observed patterns hold beyond multiple-choice questions
3. **Temporal stability analysis**: Evaluate how confidence-probability alignment changes over time with continued model usage and fine-tuning