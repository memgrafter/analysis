---
ver: rpa2
title: 'CCI3.0-HQ: a large-scale Chinese dataset of high quality designed for pre-training
  large language models'
arxiv_id: '2410.18505'
source_url: https://arxiv.org/abs/2410.18505
tags:
- dataset
- chinese
- cci3
- quality
- performance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces CCI3.0-HQ, a high-quality 500GB Chinese dataset
  designed for pre-training large language models. The authors address the challenge
  of limited high-quality Chinese datasets by developing a two-stage hybrid filtering
  pipeline that enhances data quality through safety filtering, text extraction, deduplication,
  and advanced quality assessment using a 0.5B parameter classifier trained on 140k
  annotated samples.
---

# CCI3.0-HQ: a large-scale Chinese dataset of high quality designed for pre-training large language models

## Quick Facts
- arXiv ID: 2410.18505
- Source URL: https://arxiv.org/abs/2410.18505
- Reference count: 40
- A 500GB high-quality Chinese dataset for LLM pre-training that outperforms existing Chinese datasets on 10 benchmarks

## Executive Summary
This paper introduces CCI3.0-HQ, a high-quality 500GB Chinese dataset designed for pre-training large language models. The authors address the challenge of limited high-quality Chinese datasets by developing a two-stage hybrid filtering pipeline that enhances data quality through safety filtering, text extraction, deduplication, and advanced quality assessment using a 0.5B parameter classifier trained on 140k annotated samples. Experimental results demonstrate that models trained on CCI3.0-HQ outperform those trained on other Chinese datasets (SkyPile, WanjuanV1, and CCI3.0) across 10 benchmarks in a zero-shot setting, achieving superior average scores in both English and Chinese tasks. The proposed quality classifier also achieves the highest F1 score (0.73 macro average) for Chinese web data classification, surpassing existing classifiers.

## Method Summary
The method employs a two-stage hybrid filtering pipeline to create CCI3.0-HQ. The first stage (Fundamental Processing) includes safety filtering, text extraction, deduplication, and basic quality assessment using a simple model score. The second stage (High-Quality Processing) uses Qwen2-72B-Instruct to identify high-quality samples, which are then used to train a compact 0.5B quality classifier. This classifier is applied to filter the entire CCI3.0 dataset, producing the final CCI3.0-HQ dataset. The pipeline effectively distills the quality assessment capabilities of the larger model into a more efficient classifier while maintaining high data quality standards.

## Key Results
- CCI3.0-HQ achieves superior performance on 10 benchmarks in zero-shot setting compared to CCI3.0, SkyPile, and WanjuanV1
- The quality classifier achieves a 0.73 macro average F1 score for Chinese web data classification, surpassing existing classifiers
- Models trained on CCI3.0-HQ show improved average scores across both English and Chinese tasks
- The dataset and classifier are open-sourced to support further advancements in Chinese language model development

## Why This Works (Mechanism)

### Mechanism 1
The two-stage hybrid filtering pipeline improves dataset quality by first removing unsafe/low-quality content and then refining with a high-precision classifier. The pipeline starts with Fundamental Processing (safety filtering, text extraction, deduplication, heuristic quality filtering) to remove obviously bad content, then High-Quality Processing applies a Qwen2-72B-instruct-based annotation and trains a 0.5B classifier to filter for high-quality educational content.

### Mechanism 2
Training a compact 0.5B classifier on 140k annotated samples effectively distills the quality scoring ability of the larger Qwen2-72B-instruct model. Instead of using Qwen2-72B-instruct to score the entire corpus, a smaller BGE-M3-based classifier is trained on annotated samples and then applied to filter the full dataset, reducing cost while maintaining quality.

### Mechanism 3
The open-sourced CCI3.0-HQ classifier achieves superior F1 scores for Chinese web data classification, improving the overall quality of the dataset. The trained 0.5B classifier achieves a macro average F1 score of 0.73 on the benchmark, outperforming existing classifiers, which enhances the quality of the filtered dataset.

## Foundational Learning

- Data curation and filtering for large language models: Why needed here - The paper's entire approach relies on systematically removing low-quality and unsafe content while retaining high-quality data for LLM training. Quick check question: What are the key steps in the two-stage filtering pipeline, and why is each necessary?

- Quality classification and evaluation metrics (F1 score, precision, recall): Why needed here - The effectiveness of the filtering process is measured using F1 scores and other classification metrics, and the classifier is compared against existing models. Quick check question: How is the classifier's performance evaluated, and what does a macro average F1 score of 0.73 mean?

- Zero-shot evaluation of LLMs: Why needed here - The paper evaluates the models trained on different datasets using zero-shot performance on various benchmarks to assess the impact of dataset quality. Quick check question: What does zero-shot evaluation mean, and why is it used to compare the performance of models trained on different datasets?

## Architecture Onboarding

- Component map: Raw data collection → Fundamental Processing (safety filtering, text extraction, deduplication, heuristic filtering) → CCI3.0 dataset → High-Quality Processing (Qwen2-72B-instruct annotation, classifier training) → CCI3.0-HQ dataset → LLM training → Evaluation

- Critical path: Raw data collection → Fundamental Processing → High-Quality Processing → CCI3.0-HQ dataset creation → LLM training → Zero-shot evaluation

- Design tradeoffs: Using a large model (Qwen2-72B-instruct) for annotation is accurate but expensive; training a smaller classifier is cheaper but may lose some nuance. The choice of annotation method (FineWeb-edu vs. DCLM) affects the type of high-quality content retained.

- Failure signatures: If the classifier performs poorly on the benchmark, the dataset quality will suffer. If the safety filters are too aggressive, the dataset may be too small or lack diversity. If the model is not properly trained, the zero-shot evaluation may not reflect the true performance of the dataset.

- First 3 experiments:
  1. Run the fundamental processing pipeline on a small subset of the data to check for obvious issues with safety filtering, deduplication, and text extraction.
  2. Test the high-quality classifier on a held-out validation set to ensure it generalizes well and achieves the expected F1 score.
  3. Train a small-scale model (e.g., 0.5B parameters) on the filtered dataset and evaluate its zero-shot performance on a subset of benchmarks to confirm the dataset's quality.

## Open Questions the Paper Calls Out

### Open Question 1
How does CCI3.0-HQ perform when used to train larger models (e.g., 1B+ parameters) compared to smaller models like the 0.5B parameter model used in the experiments? The paper evaluates CCI3.0-HQ using a 0.5B parameter model but does not explore its performance with larger models, which are more commonly used in practice.

### Open Question 2
How does the quality classifier perform on non-Chinese web data, and can it be adapted for multilingual use? The classifier is specifically trained and evaluated on Chinese web data, but the paper does not discuss its performance or adaptability for other languages.

### Open Question 3
What is the long-term impact of using CCI3.0-HQ on model robustness and generalization to real-world tasks? The paper evaluates CCI3.0-HQ using benchmark tasks in a zero-shot setting but does not assess its impact on real-world applications or long-term model behavior.

## Limitations

- The two-stage filtering pipeline is computationally intensive and may exclude potentially useful content
- The 0.73 macro-F1 score for the classifier is based on a limited test set
- Zero-shot evaluation doesn't test for generalization to downstream tasks

## Confidence

- High confidence: Dataset creation methodology and filtering pipeline are clearly described and reproducible
- Medium confidence: Classifier performance metrics and comparison with existing models
- Medium confidence: Zero-shot evaluation results showing dataset quality improvements

## Next Checks

1. Test the 0.5B classifier on a temporally separated test set to assess performance stability over time
2. Quantify the diversity loss from safety filtering by comparing n-gram distributions before and after filtering
3. Compare the two-stage pipeline's computational cost against a single-stage approach using only the 0.5B classifier