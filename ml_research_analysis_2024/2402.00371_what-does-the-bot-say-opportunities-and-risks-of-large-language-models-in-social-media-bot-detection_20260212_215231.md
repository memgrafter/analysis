---
ver: rpa2
title: What Does the Bot Say? Opportunities and Risks of Large Language Models in
  Social Media Bot Detection
arxiv_id: '2402.00371'
source_url: https://arxiv.org/abs/2402.00371
tags:
- uni00000013
- user
- detection
- llms
- social
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work explores the opportunities and risks of large language
  models (LLMs) in social media bot detection. For opportunities, a mixture-of-heterogeneous-experts
  framework is proposed, leveraging LLMs to analyze diverse user information modalities
  (metadata, text, and network structure) either via in-context learning or instruction
  tuning, with ensemble voting for final predictions.
---

# What Does the Bot Say? Opportunities and Risks of Large Language Models in Social Media Bot Detection

## Quick Facts
- **arXiv ID**: 2402.00371
- **Source URL**: https://arxiv.org/abs/2402.00371
- **Reference count**: 38
- **Primary result**: Instruction-tuned LLMs outperform state-of-the-art bot detection baselines by up to 9.1% F1-score, while LLM-guided manipulations can reduce detector performance by up to 29.6%.

## Executive Summary
This work investigates both opportunities and risks of large language models in social media bot detection. The authors propose a mixture-of-heterogeneous-experts framework that leverages LLMs to analyze diverse user information modalities (metadata, text, and network structure) either through in-context learning or instruction tuning, achieving state-of-the-art performance. Conversely, they introduce LLM-guided manipulation strategies that can significantly degrade bot detector performance, highlighting the dual-use nature of this technology.

## Method Summary
The authors explore LLMs for bot detection using a mixture-of-heterogeneous-experts framework with five modality-specific predictors (metadata, text, meta+text, structure-random, structure-attention) that can be used via in-context learning or instruction tuning, ensembled via majority voting. For risks, they develop LLM-guided manipulation strategies that rewrite bot content and alter structural features to evade detection, including four textual strategies (zero-shot, few-shot, classifier guidance, text attribute) and three structural strategies (add/remove/combine neighbor).

## Key Results
- Instruction tuning on 1,000 annotated examples produces LLMs that outperform state-of-the-art baselines by up to 9.1% F1-score
- LLM-guided manipulation strategies can reduce bot detector performance by up to 29.6% and harm calibration
- The mixture-of-heterogeneous-experts framework analyzing metadata, text, and network structure achieves superior performance through ensemble voting

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Instruction tuning on 1,000 annotated examples produces LLMs that outperform state-of-the-art bot detection baselines by up to 9.1% F1-score.
- Mechanism: Instruction tuning adapts pretrained LLMs to follow specific instructions for bot detection tasks, enabling them to capture nuanced patterns of bot behavior beyond simple data artifacts.
- Core assumption: The nuances of bot accounts are beyond simple data artifacts and require model adaptation and reasoning.
- Evidence anchors:
  - [abstract]: "instruction tuning on merely 1,000 annotated examples produces specialized LLMs that outperform state-of-the-art baselines by up to 9.1% on both datasets"
  - [section 4.1]: "while in-context learning abilities are attributed to pretraining data (Min et al., 2022b) and LLMs have seen social media texts (Dodge et al., 2021), the nuances of bot accounts are beyond simple data artifacts and would need model adaptation and reasoning"
- Break condition: If the training data contains insufficient or biased examples, or if the instruction format fails to capture relevant bot detection patterns.

### Mechanism 2
- Claim: LLM-guided manipulation strategies can reduce the performance of existing bot detectors by up to 29.6% and harm calibration.
- Mechanism: LLMs are used to iteratively refine bot content (text and structural features) based on feedback from external classifiers or by imitating genuine user patterns, making bots less distinguishable from humans.
- Core assumption: LLMs can effectively generate realistic paraphrases and suggest strategic follow/unfollow actions that evade detection.
- Evidence anchors:
  - [abstract]: "LLM-guided manipulation strategies could significantly bring down the performance of existing bot detectors by up to 29.6% and harm the calibration and reliability of bot detection systems"
  - [section 4.2]: "Classifier guidance is the most successful among textual manipulations. On average, classifier guidance achieved a 6.0% and 3.2% drop in accuracy and F1-score"
- Break condition: If LLMs fail to preserve malicious content during rewriting or if manipulation strategies become predictable to detectors.

### Mechanism 3
- Claim: A mixture-of-heterogeneous-experts framework that analyzes diverse user information modalities separately and ensembles predictions achieves state-of-the-art performance.
- Mechanism: Different LLM predictors analyze metadata, text, and network structure independently, with majority voting to combine predictions, capturing that not all modalities of a bot account are malicious.
- Core assumption: Different user information modalities contain complementary information for bot detection, and ensemble methods can leverage this complementarity.
- Evidence anchors:
  - [abstract]: "we propose a mixture-of-heterogeneous-experts framework to tackle the diverse user information"
  - [section 2.1]: "different user information modalities are separately analyzed with LLMs while majority voting is conducted to ensemble uni-modality predictions"
- Break condition: If individual modality predictors are weak or if ensemble voting fails to resolve conflicting predictions effectively.

## Foundational Learning

- Concept: In-context learning
  - Why needed here: Understanding how LLMs can perform bot detection tasks using examples provided in the prompt without parameter updates.
  - Quick check question: How does in-context learning differ from fine-tuning, and what are its limitations for specialized tasks like bot detection?

- Concept: Instruction tuning
  - Why needed here: Grasping how adapting LLMs with labeled examples improves their ability to follow specific instructions for bot detection.
  - Quick check question: What is the role of the {instruction, input, output} triplet in instruction tuning, and why is it effective for this task?

- Concept: Ensemble methods
  - Why needed here: Understanding how combining predictions from multiple models (each analyzing different user information modalities) can improve overall performance.
  - Quick check question: Why might majority voting be a suitable method for combining predictions from different modality-specific LLM predictors?

## Architecture Onboarding

- Component map: Five LLM predictors (metadata, text, meta+text, structure-random, structure-attention) used via in-context learning or instruction tuning, with ensemble layer for final predictions. For risks: text manipulation components (zero-shot, few-shot, classifier guidance, text attribute) and structural manipulation components (add neighbor, remove neighbor, combine neighbor).
- Critical path: For opportunities: input user information → five modality-specific LLM predictors → majority voting → bot/human classification. For risks: target bot information → LLM-guided manipulation → evasion of detection.
- Design tradeoffs: Using LLMs provides superior performance but requires significant computational resources and is only instruction-tuned on 1,000 examples, unlike lightweight baselines trained on larger datasets. Ensemble methods improve performance but add complexity.
- Failure signatures: Poor performance on in-context learning suggests LLMs need instruction tuning for this task. Large drops in performance after manipulation indicate successful evasion strategies. Calibration issues suggest overconfidence in predictions.
- First 3 experiments:
  1. Evaluate LLM-based detectors with in-context learning vs. instruction tuning on the same dataset.
  2. Test the effectiveness of different LLM-guided manipulation strategies (text vs. structural) on existing detectors.
  3. Analyze the calibration of LLM-based detectors before and after manipulation to assess reliability.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How effective would LLM-based bot detectors be on social media platforms other than Twitter/X, such as TikTok, Reddit, or Facebook, given the differences in platform structure, user behavior, and available metadata?
- Basis in paper: [explicit] The authors note that their experiments focus primarily on Twitter/X due to data availability, but express interest in expanding to other platforms in future work.
- Why unresolved: The current study is limited to Twitter/X data, and the authors acknowledge that different social media platforms have distinct characteristics that could impact the effectiveness of LLM-based detection methods.
- What evidence would resolve it: Experiments applying the proposed LLM-based detection framework to datasets from other social media platforms, comparing performance to platform-specific baselines and analyzing the impact of platform-specific features and user behaviors.

### Open Question 2
- Question: How do the social biases inherent in language models impact the fairness and potential for false positives in LLM-based bot detection, particularly for underrepresented communities or users who post in languages other than English?
- Basis in paper: [explicit] The authors discuss the known social biases in language models and hypothesize that LLM-based bot detectors might underserve certain users and communities due to these biases.
- Why unresolved: While the authors acknowledge the potential for bias, they do not empirically investigate the fairness implications of their proposed methods or provide concrete evidence of disparate impact.
- What evidence would resolve it: A comprehensive fairness analysis of the proposed LLM-based detection methods, including measuring performance disparities across different demographic groups, languages, and content styles, and exploring bias mitigation techniques.

### Open Question 3
- Question: How does the performance of LLM-based bot detectors change over time as social media bot accounts evolve and adapt their strategies to evade detection, and what is the long-term sustainability of this approach?
- Basis in paper: [explicit] The authors acknowledge that social media bot accounts are constantly evolving and express a desire to test their methods with more up-to-date data.
- Why unresolved: The current study uses datasets collected in and before 2022, and the authors note the challenge of obtaining access to more recent data due to platform restrictions.
- What evidence would resolve it: Longitudinal studies tracking the performance of LLM-based bot detectors on evolving bot accounts over time, ideally with access to recent social media data, and exploring methods for continuously updating and adapting the models to keep pace with evolving bot strategies.

## Limitations

- The study relies on Twitter-specific datasets, raising questions about generalizability to other social platforms with different user behaviors and content characteristics.
- Instruction tuning uses only 1,000 examples per dataset, which may not capture the full diversity of bot behaviors across different attack vectors.
- Structural manipulation strategies depend on follower/following relationships that may not be equally accessible across all platforms or API endpoints.

## Confidence

*High Confidence*: The finding that instruction-tuned LLMs outperform baselines by up to 9.1% F1-score is well-supported with clear experimental results across two datasets and multiple LLM architectures. The observation that LLM-guided manipulations can reduce detector performance by up to 29.6% is also robustly demonstrated.

*Medium Confidence*: The effectiveness of specific manipulation strategies and the superiority of the mixture-of-heterogeneous-experts framework are supported, but could benefit from additional ablation studies and cross-platform validation.

*Low Confidence*: The generalizability of these findings to real-world deployment scenarios, where bot operators may have limited access to follower/following data or where detection systems employ multiple defensive mechanisms simultaneously.

## Next Checks

1. **Cross-platform validation**: Test the proposed LLM-based detectors and manipulation strategies on non-Twitter datasets (e.g., Reddit, Instagram) to assess generalizability across different social media ecosystems with varying content types and interaction patterns.

2. **Defense mechanism evaluation**: Implement and evaluate defensive strategies against the identified manipulation attacks, such as detecting LLM-generated text patterns or monitoring abnormal structural changes, to understand the practical limitations of these evasion techniques.

3. **Real-world deployment analysis**: Conduct a longitudinal study measuring the effectiveness of both detection and evasion strategies over time, accounting for adaptive responses from both bot operators and detection system maintainers, including cost-benefit analysis of implementing such LLM-based systems at scale.