---
ver: rpa2
title: 'MedVH: Towards Systematic Evaluation of Hallucination for Large Vision Language
  Models in the Medical Context'
arxiv_id: '2407.02730'
source_url: https://arxiv.org/abs/2407.02730
tags:
- medical
- lvlms
- tasks
- hallucination
- image
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces MedVH, the first benchmark for evaluating
  hallucinations in large vision-language models (LVLMs) in medical contexts. The
  dataset evaluates hallucinations through five tasks assessing multimodal understanding
  and long-text generation, including wrongful image detection, "none of the above"
  selection, clinically incorrect question handling, false confidence justification,
  and medical report generation.
---

# MedVH: Towards Systematic Evaluation of Hallucination for Large Vision Language Models in the Medical Context

## Quick Facts
- arXiv ID: 2407.02730
- Source URL: https://arxiv.org/abs/2407.02730
- Authors: Zishan Gu; Changchang Yin; Fenglin Liu; Ping Zhang
- Reference count: 6
- Primary result: First benchmark for evaluating hallucinations in large vision-language models in medical contexts

## Executive Summary
This paper introduces MedVH, a novel benchmark designed to systematically evaluate hallucinations in large vision-language models (LVLMs) within medical contexts. The dataset evaluates five distinct hallucination types through tasks that assess multimodal understanding and long-text generation capabilities. Experiments with seven state-of-the-art LVLMs reveal that medical LVLMs, despite superior performance on standard medical tasks, exhibit more hallucinations than general LVLMs. The characterization score metric introduced balances hallucination resistance with medical knowledge utilization, highlighting significant improvement opportunities before real-world deployment.

## Method Summary
MedVH is constructed through a comprehensive process involving expert consultation, medical knowledge base extraction, and systematic data generation. The benchmark evaluates hallucinations through five tasks: wrongful image detection, "none of the above" selection, clinically incorrect question handling, false confidence justification, and medical report generation. The characterization score metric is introduced to balance hallucination resistance with medical knowledge utilization, providing a more nuanced evaluation framework than simple accuracy metrics.

## Key Results
- Medical LVLMs exhibit more hallucinations than general LVLMs despite better performance on standard medical tasks
- CheXagent achieves 0.978 accuracy on wrongful image detection but only 0.244 on clinically incorrect questions
- Characterization score reveals significant room for improvement in domain-specific fine-tuned models
- Benchmark effectively identifies inconsistencies in model robustness across different hallucination types

## Why This Works (Mechanism)
The benchmark's effectiveness stems from its systematic approach to identifying specific hallucination patterns in medical contexts. By focusing on multimodal understanding and long-text generation, MedVH captures hallucinations that arise from the complex interplay between visual and textual information processing. The characterization score metric provides a balanced evaluation that accounts for both hallucination resistance and medical knowledge utilization, preventing models from simply avoiding hallucinations by refusing to answer medical questions.

## Foundational Learning
- Multimodal understanding: Essential for processing both visual and textual medical information simultaneously; quick check: model can correctly identify anatomical structures in medical images while interpreting associated text
- Hallucination detection: Critical for identifying false information generation in medical contexts; quick check: model flags medically impossible scenarios or contradicts established medical knowledge
- Medical knowledge utilization: Necessary for applying domain expertise while avoiding hallucinated content; quick check: model provides accurate medical explanations without fabricating information
- Confidence calibration: Important for distinguishing between certain and uncertain medical predictions; quick check: model appropriately expresses uncertainty when faced with ambiguous medical cases
- Long-text generation: Required for producing coherent medical reports without accumulating hallucinations; quick check: generated reports maintain consistency and medical accuracy throughout

## Architecture Onboarding

**Component Map:** Data Generation -> Task Definition -> Model Evaluation -> Characterization Scoring -> Benchmark Analysis

**Critical Path:** The most critical sequence involves data generation through expert consultation and medical knowledge base extraction, followed by task definition that captures diverse hallucination types, leading to model evaluation and characterization scoring that produces actionable insights about hallucination patterns.

**Design Tradeoffs:** The benchmark prioritizes systematic evaluation of specific hallucination types over comprehensive coverage, potentially missing some subtle hallucination patterns. The focus on five distinct tasks provides clear evaluation metrics but may oversimplify the complex nature of medical hallucinations.

**Failure Signatures:** Models may exhibit high accuracy on straightforward hallucination detection while failing on clinically incorrect questions, indicating inconsistent robustness. Some models might avoid hallucinations by refusing to answer medical questions, artificially inflating hallucination resistance scores while demonstrating poor medical knowledge utilization.

**First Experiments:** 
1. Test wrongful image detection task with CheXagent to establish baseline hallucination detection capabilities
2. Evaluate "none of the above" selection task across all seven LVLMs to compare general versus medical model performance
3. Assess medical report generation task with clinically incorrect questions to identify knowledge gaps and hallucination patterns

## Open Questions the Paper Calls Out
None

## Limitations
- Benchmark focuses on five specific hallucination types, potentially missing other forms of medical misinformation
- Human annotation process may introduce subjective biases, particularly in clinically judgmental tasks
- Comparison between medical and general LVLMs assumes comparable training conditions that may not account for domain-specific optimization differences

## Confidence

**High Confidence:**
- Medical LVLMs exhibit more hallucinations than general LVLMs despite superior task performance

**Medium Confidence:**
- Characterization score effectively balances hallucination resistance with medical knowledge utilization
- Benchmark comprehensiveness given the specific hallucination types evaluated

## Next Checks
1. External validation with additional medical LVLMs and human expert assessment to verify benchmark accuracy and relevance
2. Expansion of hallucination categories to include more subtle forms of medical misinformation and contextual errors
3. Comparative analysis of MedVH results with real-world clinical deployment data to assess practical significance of identified hallucinations