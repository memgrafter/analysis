---
ver: rpa2
title: 'Alice in Wonderland: Simple Tasks Showing Complete Reasoning Breakdown in
  State-Of-the-Art Large Language Models'
arxiv_id: '2406.02061'
source_url: https://arxiv.org/abs/2406.02061
tags:
- uni00000010
- uni00000015
- uni00000013
- uni00000048
- problem
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Large language models (LLMs) exhibit severe reasoning and generalization
  breakdowns when tested on a simple common-sense math problem (Alice in Wonderland,
  AIW). Despite claims of strong reasoning capabilities, all state-of-the-art models,
  including GPT-4 and Claude 3 Opus, perform poorly, with low average correct response
  rates and strong performance fluctuations across variations that do not change problem
  structure or difficulty.
---

# Alice in Wonderland: Simple Tasks Showing Complete Reasoning Breakdown in State-Of-the-Art Large Language Models

## Quick Facts
- arXiv ID: 2406.02061
- Source URL: https://arxiv.org/abs/2406.02061
- Authors: Marianna Nezhurina; Lucia Cipolina-Kun; Mehdi Cherti; Jenia Jitsev
- Reference count: 40
- Large language models exhibit severe reasoning and generalization breakdowns when tested on simple common-sense math problems, despite claims of strong reasoning capabilities.

## Executive Summary
This study reveals that state-of-the-art large language models, including GPT-4 and Claude 3 Opus, show complete reasoning breakdowns on a simple common-sense math problem despite their purported strong reasoning capabilities. The models exhibit low average correct response rates and strong performance fluctuations across variations that preserve problem structure and difficulty. Control experiments rule out low-level issues like language parsing or arithmetic failures, indicating the breakdown stems from fundamental reasoning deficits. The models also demonstrate overconfidence in wrong solutions, producing persuasive confabulations that mislead users about solution quality.

## Method Summary
The study tests large language models on the Alice in Wonderland (AIW) problem and its variations, using multiple prompt types including STANDARD, THINKING, and RESTRICTED prompts. Models are evaluated on both the original problem and control problems (AIW Light versions) that isolate specific operations. Performance is measured through correct response rates and sensitivity to problem-irrelevant variations. The evaluation pipeline includes problem template generation, prompt formatting, model inference, response parsing, statistical analysis, and visualization of performance fluctuations.

## Key Results
- All tested SOTA models, including GPT-4 and Claude 3 Opus, perform poorly on AIW variations with low average correct response rates
- Strong performance fluctuations occur across natural variations that preserve problem structure and difficulty
- Standard interventions like chain-of-thought prompting and multi-step self-verification fail to improve performance
- Models exhibit overconfidence in wrong solutions, producing persuasive confabulations

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The AIW problem exposes generalization deficits by using natural number variations that do not change problem structure or difficulty.
- Mechanism: Models are tested on multiple instances of the same logical problem where only the instantiated numbers differ. If generalization is intact, performance should be consistent across all variations. Strong performance fluctuations indicate sensitivity to irrelevant perturbations, revealing lack of robustness.
- Core assumption: Problem variations preserve both problem structure and difficulty exactly; any performance difference is therefore due to model generalization deficits rather than task complexity changes.
- Evidence anchors: [abstract] "strong performance fluctuations on natural variations in problem template that do not change either problem structure or its difficulty at all", [section] "we measure sensitivity of models to those variations, observing strong performance fluctuations"
- Break condition: If variations accidentally alter difficulty (e.g., by using extremely large numbers that introduce numerical instability), the mechanism would break.

### Mechanism 2
- Claim: Control AIW Light problems rule out low-level parsing or arithmetic failures as the cause of breakdown.
- Mechanism: By constructing simplified versions that isolate specific operations (e.g., basic family structure handling, elementary arithmetic), we can verify whether the model can execute required low-level operations. Success on controls implies failures on the main problem stem from higher-level reasoning deficits.
- Core assumption: If models can solve AIW Light problems (which require the same low-level operations as AIW) but fail on AIW, then the issue is not with basic operations but with integrating them into a coherent solution.
- Evidence anchors: [abstract] "By testing models on further control problems with similar form, we rule out that breakdown might be rooted in minor low-level issues like natural language or numbers parsing", [section] "AIW Light problems keep problem template close to the original, changing only the final question part such that the posed modified question tests particular operations"
- Break condition: If control problems are too simple and do not actually require the same operations as the main problem, the mechanism would break.

### Mechanism 3
- Claim: Overconfidence and confabulations in wrong answers reveal a disconnect between model certainty and actual reasoning capability.
- Mechanism: Models often provide persuasive-sounding explanations for incorrect answers, expressing high confidence. This indicates the model is generating plausible text without genuine understanding, misleading users about solution quality.
- Core assumption: The presence of confident, explanation-like text accompanying wrong answers is evidence that the model is confabulating rather than reasoning correctly.
- Evidence anchors: [abstract] "We also observe strong overconfidence in the wrong solutions, expressed in form of plausible sounding explanation-like confabulations", [section] "we observe strong overconfidence in the solution quality across the tested models"
- Break condition: If confident explanations sometimes accompany correct answers, overconfidence alone would not be a reliable signature of breakdown.

## Foundational Learning

- Concept: Controlled problem variation
  - Why needed here: To isolate and measure model sensitivity to problem-irrelevant changes, revealing generalization deficits.
  - Quick check question: If a model solves variation 1 with 100% accuracy but variation 2 with 20% accuracy, what does this indicate about its generalization?

- Concept: Low-level operation isolation
  - Why needed here: To distinguish between failures in basic operations (parsing, arithmetic) versus higher-level reasoning failures.
  - Quick check question: If a model succeeds on AIW Light Arithmetic Siblings but fails on AIW original, what can we conclude about the source of its failure?

- Concept: Performance fluctuation analysis
  - Why needed here: To quantify robustness by measuring variance in correct response rates across variations, beyond just average accuracy.
  - Quick check question: Why might two models with the same average correct response rate across variations differ in their generalization capability?

## Architecture Onboarding

- Component map: Problem template generation -> Prompt formatting -> Model inference -> Response parsing -> Statistical aggregation -> Visualization of fluctuations
- Critical path: Template generation → Prompt formatting → Model inference → Response parsing → Statistical aggregation → Visualization of fluctuations
- Design tradeoffs: Using natural numbers keeps problems human-readable but may introduce accidental difficulty changes; restrictive prompts reduce computation but may disadvantage models; automated parsing simplifies evaluation but may misclassify complex responses
- Failure signatures: Strong performance fluctuations across variations indicate lack of robustness; consistently low correct rates suggest fundamental reasoning deficits; high confidence in wrong answers signals confabulation
- First 3 experiments:
  1. Run AIW original variations 1-4 with STANDARD prompt on a selected model; record correct rates per variation
  2. Run AIW Light Arithmetic Siblings with THINKING prompt; verify model can perform basic arithmetic
  3. Run AIW Light Family with THINKING prompt; verify model grasps basic family relations

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can we design AIW-like benchmarks that are both immune to training data leakage and capable of providing diverse, scalable problem instances through combinatorial generation?
- Basis in paper: [inferred]
- Why unresolved: The paper demonstrates that even simple AIW problems can be solved by models that have likely been exposed to similar data during training. However, it also suggests that variations built into problem templates could offer a path to scalable benchmarks. The challenge lies in ensuring these benchmarks are robust against memorization while still providing meaningful, diverse instances.
- What evidence would resolve it: A benchmark suite based on AIW-like templates that consistently exposes model weaknesses across variations, even for models trained on large web-scale datasets, would demonstrate its effectiveness in measuring true generalization.

### Open Question 2
- Question: What are the underlying mechanisms that lead to the observed overconfidence and confabulation behaviors in LLMs when confronted with problems they cannot solve?
- Basis in paper: [explicit]
- Why unresolved: The paper provides anecdotal evidence of models exhibiting overconfident tones and generating persuasive explanations for incorrect answers. However, the root causes of these behaviors—whether they stem from training data, model architecture, or optimization objectives—remain unclear.
- What evidence would resolve it: A systematic study analyzing the correlation between training data, model architecture, and the frequency/intensity of overconfident/confabulatory responses across a range of tasks would shed light on the underlying mechanisms.

### Open Question 3
- Question: How can we effectively measure and compare model robustness to problem structure-preserving variations, and what unified metrics best capture both average performance and sensitivity to such variations?
- Basis in paper: [explicit]
- Why unresolved: The paper introduces a unified robustness score (R) that combines average correct response rates with entropy-based measures of performance fluctuations. However, its effectiveness and generalizability across different problem types and model classes remain to be fully validated.
- What evidence would resolve it: A comprehensive evaluation of the R score across diverse problem sets and model architectures, demonstrating its ability to consistently rank models by robustness and correlate with human judgments of model reliability, would establish its validity.

## Limitations

- The evaluation relies on a single problem type, raising questions about whether results generalize to other reasoning tasks
- Performance measurement depends on response parsing accuracy, which could misclassify complex model outputs
- The study focuses on closed-ended questions with single correct answers, potentially missing aspects of reasoning that involve multiple valid solutions

## Confidence

High confidence in: (1) The observation that SOTA models fail on AIW variations despite claims of strong reasoning capabilities; (2) The demonstration that standard interventions like chain-of-thought prompting do not improve performance; (3) The finding that models exhibit overconfidence and confabulation when providing wrong answers

Medium confidence in: (1) The conclusion that observed failures stem specifically from generalization deficits rather than other factors like numerical instability or prompt sensitivity; (2) The claim that AIW Light controls adequately isolate low-level operation failures from higher-level reasoning deficits; (3) The assertion that problem variations preserve difficulty exactly

## Next Checks

1. Test model performance across a broader range of problem types beyond the AIW template to determine whether observed generalization failures are specific to this problem structure or indicative of more general reasoning deficits

2. Systematically vary numerical parameters in the AIW problem (e.g., extremely large numbers, edge cases like zero or negative values) to determine whether performance fluctuations stem from numerical processing limitations rather than reasoning deficits

3. Implement blinded human evaluation of model explanations accompanying wrong answers to quantify the persuasiveness of confabulations and determine whether they consistently mislead human evaluators about solution quality