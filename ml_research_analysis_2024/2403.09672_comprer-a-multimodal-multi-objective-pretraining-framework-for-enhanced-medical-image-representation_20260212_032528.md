---
ver: rpa2
title: 'COMPRER: A Multimodal Multi-Objective Pretraining Framework for Enhanced Medical
  Image Representation'
arxiv_id: '2403.09672'
source_url: https://arxiv.org/abs/2403.09672
tags:
- comprer
- medical
- learning
- contrastive
- image
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces COMPRER, a novel multi-modal, multi-objective
  pretraining framework designed to enhance medical image representation for improved
  diagnostic and prognostic capabilities. The framework combines fundus imaging and
  carotid ultrasound, two complementary modalities for cardiovascular health assessment,
  through a series of contrastive and predictive learning objectives.
---

# COMPRER: A Multimodal Multi-Objective Pretraining Framework for Enhanced Medical Image Representation

## Quick Facts
- arXiv ID: 2403.09672
- Source URL: https://arxiv.org/abs/2403.09672
- Authors: Guy Lutsker; Hagai Rossman; Nastya Godiva; Eran Segal
- Reference count: 12
- One-line primary result: COMPRER achieves superior cardiovascular condition prediction using multimodal contrastive learning from fundus images and carotid ultrasound

## Executive Summary
COMPRER introduces a novel multi-modal, multi-objective pretraining framework that enhances medical image representation by combining fundus imaging and carotid ultrasound through contrastive and predictive learning objectives. The framework leverages transformer-based architectures and self-supervised learning techniques to capture complex patterns across different imaging modalities and time points. By integrating multimodal contrastive loss, temporal contrastive loss, reconstruction loss, and predictive loss, COMPRER demonstrates superior performance in predicting cardiovascular conditions compared to existing models with larger parameter counts and more extensive training data.

## Method Summary
COMPRER uses DINOV2-Base ViT encoders for fundus and carotid ultrasound images, combined with linear projection heads for contrastive embeddings, MLP predictor heads for medical measure regression, and a transposed CNN decoder for image reconstruction. The framework employs AdamW optimizer with learning rate 3×10⁻⁴, weight decay 0.5, and StepLR scheduler, training on 8 NVIDIA A40 GPUs for 4 days with batch size 9 per GPU. The method integrates multimodal contrastive learning, temporal contrastive learning across visits, bilateral fundus contrastive loss, reconstruction loss, and predictive loss to create robust medical image representations.

## Key Results
- Achieves higher Area Under the Curve (AUC) scores in evaluating medical conditions compared to baseline models
- Maintains favorable performance on external out-of-distribution UK Biobank dataset
- Introduces novel Top-K metric for assessing contrastive learning performance in medical imaging

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Multi-modal contrastive learning aligns representations across fundus images and carotid ultrasound by maximizing similarity for paired samples
- Mechanism: CLIP-style loss computed between embeddings from fundus (f) and carotid (c) image encoders, encouraging shared latent space where related images are close together
- Core assumption: Paired fundus and carotid images correspond to same patient and condition
- Evidence anchors: Contrastive loss function defined as Lcontr(u,v) = -1/N Σᵢ log(exp(sim(ui,vi)/τ) / Σⱼ exp(sim(ui,vj)/τ)), multimodal loss consolidates information across different imaging modalities
- Break condition: If image pairs are not truly aligned or correspond to different patients, contrastive loss will push unrelated features together

### Mechanism 2
- Claim: Multi-visit contrastive loss captures temporal patterns by contrasting embeddings from same patient at different visits
- Mechanism: Same contrastive loss applied to fundus images across visits (fᵗ vs fᵗ') and carotid images across visits (cᵗ vs cᵗ')
- Core assumption: Temporal differences between visits encode meaningful disease progression signals
- Evidence anchors: Lcontr_fv = Lcontr_CLIP(fᵗ, fᵗ') and Lcontr_cv = Lcontr_CLIP(cᵗ, cᵗ'), temporal loss imparts ability to discern patterns over time
- Break condition: If visit-to-visit changes are dominated by noise or acquisition artifacts, contrastive signal will be unreliable

### Mechanism 3
- Claim: Reconstruction loss ensures model preserves spatial structure in latent representation
- Mechanism: Transposed convolutional decoder takes compressed embeddings and attempts to reconstruct original images, penalizing loss of fine detail through MSE
- Core assumption: Latent space must retain sufficient information to accurately reproduce input images
- Evidence anchors: Transposed CNN comprising transposed convolutional layers with GELU activations reconstructs original images from latent embeddings, regularization effect introduced
- Break condition: If decoder is too weak or latent space too compressed, reconstruction loss will plateau and stop enforcing structure preservation

## Foundational Learning

- Concept: Self-supervised learning (SSL) via contrastive objectives
  - Why needed here: Avoids reliance on scarce labeled medical data while still forcing model to learn semantically meaningful embeddings
  - Quick check question: In CLIP-style contrastive learning, what is the role of the temperature τ in the loss function?

- Concept: Multi-task/multi-objective optimization
  - Why needed here: Combines complementary signals (modality alignment, temporal consistency, reconstruction, prediction) to improve generalization over single-objective pretraining
  - Quick check question: Why might adding more objectives not always improve performance? What trade-off must be managed?

- Concept: Transformer-based vision encoders with pretrained weights
  - Why needed here: Leverages large-scale pretraining (DINOV2) to initialize rich visual features, reducing amount of medical-domain data needed for effective fine-tuning
  - Quick check question: What advantage does a linear projection head provide after the ViT backbone in contrastive learning?

## Architecture Onboarding

- Component map: Images → DINOV2-Base ViT encoders → Linear projection heads → Contrastive loss + Prediction loss + Reconstruction loss → Parameter update
- Critical path: Forward pass: Images → ViT encoders → projection heads → contrastive loss + prediction loss + reconstruction loss → backward pass → parameter update
- Design tradeoffs: Multi-objective training increases representational richness but also computational cost and risk of conflicting gradients; reconstructing full images adds strong regularizer but may be unnecessary if only downstream prediction is needed; using DINOV2 weights speeds convergence but may bias model toward ImageNet-style features
- Failure signatures: If contrastive losses dominate, embeddings may collapse to trivial solutions; if reconstruction loss dominates, embeddings may overfit to pixel-level details; if prediction losses dominate, model may ignore cross-modal alignment
- First 3 experiments: 1) Train with only multimodal contrastive loss to verify basic cross-modal alignment; 2) Add reconstruction loss to confirm spatial structure preservation; 3) Add all losses together and evaluate whether Top-K contrastive metrics improve over ablation

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does COMPRER's performance scale with addition of more imaging modalities beyond fundus images and carotid ultrasound?
- Basis in paper: [explicit] Paper mentions HPP dataset contains over 20 distinct data modalities and suggests future iterations could incorporate additional modalities, but does not provide empirical results
- Why unresolved: Study only evaluated two modalities, no data on how model would perform with more diverse data sources
- What evidence would resolve it: Conducting experiments with additional modalities and comparing performance metrics to current two-modality setup

### Open Question 2
- Question: What are specific mechanisms by which temporal contrastive learning objective contributes to improved prognostic performance?
- Basis in paper: [explicit] Paper suggests temporal contrastive learning provides insights into future events within longitudinal data, potentially contributing to superior prognostic performance
- Why unresolved: While paper hypothesizes importance of temporal information, does not provide detailed analysis of how objective specifically enhances prognostic capabilities
- What evidence would resolve it: Detailed ablation studies isolating effects of temporal contrastive learning on prognostic tasks, possibly through visualizations or feature importance analyses

### Open Question 3
- Question: How does model's performance vary across different demographic groups, and what biases might exist in its predictions?
- Basis in paper: [inferred] Paper emphasizes importance of dataset diversity for model generalization and mentions need to mitigate bias, but does not provide demographic breakdown of performance or bias analysis
- Why unresolved: Study does not report on model performance across different demographic subgroups, leaving potential biases and disparities unexplored
- What evidence would resolve it: Evaluating model performance on stratified demographic groups and conducting bias audits to identify and address disparities in predictions

## Limitations
- Absence of direct empirical validation for multi-objective design choices, with ablation studies not systematically isolating contribution of each loss component
- Novel temporal contrastive loss lacks comparison against simpler temporal modeling approaches
- External validation on UK Biobank not fully characterized in terms of domain shift magnitude or potential confounding factors

## Confidence
- High confidence: Basic feasibility of multi-modal contrastive learning between fundus and carotid images, given strong empirical support from CLIP-style approaches
- Medium confidence: Specific multi-objective combination of losses proposed, as synergistic effects are plausible but not rigorously isolated
- Medium confidence: Reconstruction loss contribution, as this design choice is less common in contrastive medical imaging literature and its necessity is not clearly demonstrated

## Next Checks
1. **Ablation study with strict orthogonality**: Train models with only one loss active at a time, then systematically combine pairs to quantify marginal gains and potential interference between objectives
2. **Temporal consistency validation**: Evaluate whether temporal contrastive loss actually captures disease progression by testing on dataset with known longitudinal disease trajectories, comparing against baseline using only cross-sectional features
3. **Top-K metric benchmarking**: Compare proposed Top-K contrastive evaluation metric against established retrieval metrics (Recall@K, NDCG) on standard multi-modal retrieval benchmark to validate its effectiveness for measuring representation quality