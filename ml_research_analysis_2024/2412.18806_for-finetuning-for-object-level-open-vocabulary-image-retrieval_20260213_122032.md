---
ver: rpa2
title: 'FOR: Finetuning for Object Level Open Vocabulary Image Retrieval'
arxiv_id: '2412.18806'
source_url: https://arxiv.org/abs/2412.18806
tags:
- clip
- retrieval
- image
- vision
- novel
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: FOR addresses the problem of object-centric open-vocabulary image
  retrieval by introducing a fine-tuning framework that adapts a pre-trained CLIP
  model to a target dataset while maintaining open vocabulary capabilities. The core
  method involves a specialized SUM-CLIP head that generates multiple embeddings per
  image through learnable queries and decoder layers, combined with a multi-objective
  training approach using both supervised and pseudo-label losses.
---

# FOR: Finetuning for Object Level Open Vocabulary Image Retrieval

## Quick Facts
- arXiv ID: 2412.18806
- Source URL: https://arxiv.org/abs/2412.18806
- Authors: Hila Levi; Guy Heller; Dan Levi
- Reference count: 40
- Primary result: FOR improves open-vocabulary object retrieval accuracy by up to 8 mAP@50 points on novel categories compared to Cluster-CLIP

## Executive Summary
FOR introduces a fine-tuning framework that adapts CLIP for object-centric open-vocabulary image retrieval while maintaining the model's vision-language association capabilities. The method employs a specialized SUM-CLIP head that generates multiple embeddings per image through learnable queries and decoder layers, combined with a multi-objective training approach using both supervised and pseudo-label losses. This design enables efficient retrieval without expensive post-processing while preserving CLIP's pre-trained visual-textual alignment. FOR achieves state-of-the-art performance on three benchmark datasets (COCO, LVIS, and nuImages), demonstrating effectiveness in both fully supervised and semi-supervised settings.

## Method Summary
FOR fine-tunes a pre-trained CLIP model for object-centric open-vocabulary retrieval by replacing CLIP's global query with N learnable queries processed through decoder layers before the cross-attention module. The method employs a dual training objective: supervised loss on base categories and pseudo-label loss using ImageNet-21K categories assigned through Cluster-CLIP embeddings. By freezing CLIP's linear layers and training only the SUM-CLIP head, the approach maintains the pre-trained vision-language alignment while adapting to target dataset objects. The framework generates exactly N embeddings per image, making it directly suitable for large-scale retrieval without additional post-processing.

## Key Results
- FOR achieves 8 mAP@50 improvement on novel categories compared to Cluster-CLIP on COCO dataset
- Demonstrates effectiveness in semi-supervised settings, requiring only 1-10% labeled data for competitive performance
- Outperforms Dense-CLIP and Cluster-CLIP on all three benchmark datasets while maintaining computational efficiency

## Why This Works (Mechanism)

### Mechanism 1
SUM-CLIP generates multiple object-representative embeddings while preserving CLIP's vision-language alignment by replacing the single global query with N learnable queries processed through decoder layers before the cross-attention module. This allows capturing multiple objects while keeping CLIP's key, value, and output layers frozen to maintain pre-trained visual-textual associations.

### Mechanism 2
Multi-objective training with supervised and pseudo-label losses prevents open vocabulary catastrophic forgetting by fine-tuning SUM-CLIP on base categories while the pseudo-label loss compensates for novel categories using ImageNet-21K classes assigned through Cluster-CLIP embeddings. This dual supervision maintains open vocabulary capabilities during fine-tuning.

### Mechanism 3
Learnable queries enable efficient object summarization without expensive post-processing by producing exactly N embeddings per image (vs. Dense-CLIP's K >> N or Cluster-CLIP's need for K-means clustering), making it directly suitable for large-scale retrieval frameworks without additional computational overhead.

## Foundational Learning

- **Dual-encoder architecture for cross-modal retrieval**
  - Why needed: Enables efficient offline indexing of visual embeddings and online text query processing, separating computationally expensive vision encoding from rapid retrieval operations
  - Quick check: How does the dual-encoder design enable separation of offline and online processing compared to joint encoders?

- **Vision-language pre-training and alignment**
  - Why needed: CLIP's pre-training on web-scale image-caption data creates a shared embedding space where visual and textual representations can be directly compared using cosine similarity for retrieval
  - Quick check: Why is maintaining CLIP's vision-language association important during fine-tuning, and how does freezing certain layers help achieve this?

- **Semi-supervised learning with pseudo-labels**
  - Why needed: Allows leveraging large amounts of unlabeled data through pseudo-label generation from a fixed reference model (Cluster-CLIP), extending supervision beyond the limited labeled dataset categories
  - Quick check: What are the risks of using pseudo-labels for training, and how does the confidence threshold help mitigate these risks?

## Architecture Onboarding

- **Component map**: Image → CLIP backbone (frozen) → SUM-CLIP head (trainable) → N embeddings → Index; Query text → Text encoder (frozen) → Text embeddings → Retrieval index
- **Critical path**: Image → CLIP backbone → SUM-CLIP head → N embeddings → Index; Query text → Text encoder → Query embedding → Nearest neighbor search → Retrieved images
- **Design tradeoffs**: SUM-CLIP vs. Dense-CLIP vs. Cluster-CLIP (computational efficiency vs. accuracy vs. fine-tuning capability); number of learnable queries N (retrieval accuracy vs. storage/computation); freezing vs. unfreezing CLIP layers (stability vs. adaptability)
- **Failure signatures**: Catastrophic forgetting of novel categories (supervised loss too dominant); poor retrieval accuracy on base categories (pseudo-label loss too dominant or noisy); slow retrieval (too many embeddings per image or inefficient indexing)
- **First 3 experiments**:
  1. Compare SUM-CLIP with varying numbers of learnable queries (5, 25, 50) on COCO validation set to find optimal balance between accuracy and efficiency
  2. Test different freezing strategies for CLIP's linear layers (all frozen, only query/key frozen, all unfrozen) to evaluate impact on base vs. novel category performance
  3. Evaluate semi-supervised performance with different fractions of labeled data (1%, 5%, 10%) to measure effectiveness of pseudo-label loss in data-efficient scenarios

## Open Questions the Paper Calls Out

### Open Question 1
How does FOR's performance scale with increasing dataset size and query diversity in real-world applications? The paper mentions FOR's effectiveness in interactive large-scale retrieval but doesn't extensively test performance across vastly different dataset sizes or query types. Systematic evaluation across datasets with varying scales and diverse query types would reveal performance scaling characteristics and limitations.

### Open Question 2
What is the impact of FOR's pseudo-labeling strategy on rare and emerging categories in dynamic datasets? While the paper shows effectiveness on rare categories in LVIS, it doesn't explore dynamic datasets where new categories emerge over time. Testing on datasets where new categories are introduced after initial training would reveal how well the pseudo-labeling strategy adapts to evolving visual concepts.

### Open Question 3
How does FOR compare to retrieval methods that incorporate additional modalities (audio, temporal information) for object-centric search? The paper focuses exclusively on visual-text retrieval using CLIP-based embeddings without exploring multimodal extensions. Comparative evaluation against multimodal retrieval systems would reveal whether the visual-text focus limits its applicability in multimodal contexts.

## Limitations

- Evaluation limited to three object detection datasets (COCO, LVIS, nuImages) with similar characteristics, raising questions about generalizability to broader retrieval scenarios
- Pseudo-label quality and its impact on learning for novel categories is not thoroughly validated, though acknowledged as a potential failure point
- Computational efficiency claims lack comprehensive runtime comparisons or memory usage analysis across different dataset scales

## Confidence

- **High Confidence**: The overall retrieval performance improvements on benchmark datasets are well-supported by quantitative results (mAP@50 metrics across multiple datasets and experimental conditions)
- **Medium Confidence**: The mechanism of how SUM-CLIP generates multiple object-representative embeddings while preserving CLIP's alignment is theoretically sound but lacks extensive ablation studies or theoretical analysis
- **Medium Confidence**: The claim that pseudo-labels prevent open vocabulary catastrophic forgetting is supported by empirical results but the pseudo-label quality and assignment process could benefit from more rigorous validation

## Next Checks

1. **Ablation on Pseudo-Label Quality**: Conduct an analysis measuring the accuracy of pseudo-labels assigned to images and correlate this with retrieval performance, particularly for novel categories

2. **Scalability Testing**: Evaluate FOR on a larger-scale, more diverse image retrieval dataset (e.g., Flickr30k or a web-scale image collection) to test the claimed efficiency and performance benefits beyond the current benchmark datasets

3. **Real-World Retrieval Task**: Test FOR on a practical open-vocabulary retrieval task with natural language queries and diverse image content, moving beyond the object detection-style queries used in the current evaluation