---
ver: rpa2
title: 'Tangram: Benchmark for Evaluating Geometric Element Recognition in Large Multimodal
  Models'
arxiv_id: '2408.13854'
source_url: https://arxiv.org/abs/2408.13854
tags:
- geometric
- lmms
- tangram
- diagram
- elements
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Tangram is a benchmark for evaluating geometric element recognition
  in large multimodal models (LMMs). It contains 1,080 geometric diagrams from primary
  and secondary school exams, competitions, and textbooks, each with four questions
  resulting in 4,320 visual-question-answer pairs.
---

# Tangram: Benchmark for Evaluating Geometric Element Recognition in Large Multimodal Models

## Quick Facts
- **arXiv ID**: 2408.13854
- **Source URL**: https://arxiv.org/abs/2408.13854
- **Reference count**: 19
- **Key outcome**: LMMs achieve only 53.0% accuracy on geometric element recognition vs. human experts at 99.5%

## Executive Summary
Tangram is a benchmark for evaluating geometric element recognition in large multimodal models (LMMs). It contains 1,080 geometric diagrams from primary and secondary school exams, competitions, and textbooks, each with four questions resulting in 4,320 visual-question-answer pairs. The benchmark focuses on counting geometric elements like letters, circles, triangles, and line segments, making it a "simple yet challenging" task for LMMs. When evaluated on Tangram, 13 prominent LMMs including GPT-4o, Claude 3.5 Sonnet, and various open-source models achieved a top accuracy of only 53.0%, significantly lower than human performance (93.6% for students, 99.5% for experts).

## Method Summary
Tangram evaluates geometric element recognition by presenting LMMs with diagrams from real educational materials and asking them to count specific elements. The benchmark uses zero-shot evaluation with optional chain-of-thought prompting, comparing results against human performance from middle school students and math experts. Models are evaluated on their ability to count letters, circles, triangles, and line segments across 1,080 diagrams, with performance measured across three difficulty levels based on element count.

## Key Results
- 13 prominent LMMs including GPT-4o, Claude 3.5 Sonnet achieved only 53.0% top accuracy
- Human experts scored 99.5% accuracy while middle school students scored 93.6%
- LMMs struggled particularly with overlapping elements (triangles, line segments) and performed worse on solid geometry diagrams
- Performance improvements from chain-of-thought prompting were limited

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Tangram's design isolates geometric element recognition as a distinct task to reveal LMMs' fundamental perception limitations
- Mechanism: By focusing on counting simple geometric elements rather than complex reasoning, the benchmark exposes failures in basic visual comprehension that are masked in higher-level reasoning tasks
- Core assumption: Accurate element recognition is a prerequisite for successful geometric reasoning
- Evidence anchors:
  - [abstract] "Unlike existing benchmarks that emphasize higher-level cognition and reasoning, Tangram focuses on understanding geometric elements, requiring models to perform a 'simple yet challenging' counting task"
  - [section] "We argue that accurately understanding the elements within a geometric diagram is a crucial prerequisite for effective reasoning"
  - [corpus] FMR scores show related work focuses on higher-level reasoning, making Tangram's focus unique (0.5252 avg FMR)
- Break condition: If LMMs can accurately count overlapping elements, the mechanism would be invalidated

### Mechanism 2
- Claim: Tangram's use of real-world educational materials creates a fair and uncontaminated evaluation
- Mechanism: By sourcing diagrams from actual exams, competitions, and textbooks rather than synthetic data, the benchmark ensures ecological validity and prevents data contamination
- Core assumption: Real educational materials provide representative geometric diagrams that models haven't been specifically trained on
- Evidence anchors:
  - [section] "Tangram is a novel benchmark with all questions newly constructed as visual question-answer pairs. This approach effectively prevents data leakage"
  - [section] "These diagrams are then filtered based on the criterion that each must contain identifiable geometric elements"
  - [corpus] No evidence of similar contamination prevention in related benchmarks
- Break condition: If models show unusually high performance, it might indicate data leakage or overrepresentation in training data

### Mechanism 3
- Claim: Fine-grained annotations enable detailed failure analysis across different geometric elements and difficulty levels
- Mechanism: By categorizing diagrams into three difficulty levels and providing element-specific accuracy metrics, the benchmark reveals which types of geometric elements and complexity levels are most challenging for models
- Core assumption: Different geometric elements and complexity levels pose distinct challenges that can be isolated and measured
- Evidence anchors:
  - [section] "We categorize the diagrams in Tangram into three difficulty levels: Easy, Medium and Hard, based on the total number of elements in each diagram"
  - [section] "The benchmark requires models to count the points, triangles, circles, and line segments in the diagrams"
  - [section] "Each diagram in Tangram is annotated with a complexity level that corresponds to its associated question"
- Break condition: If element-specific performance is uniform across all categories, the mechanism would be weakened

## Foundational Learning

- Concept: Visual perception vs. reasoning distinction
  - Why needed here: Understanding why Tangram focuses on element recognition rather than reasoning helps engineers grasp the benchmark's unique contribution
  - Quick check question: Why might a model perform well on geometric reasoning tasks while failing at basic element counting?

- Concept: Contamination prevention in benchmark design
  - Why needed here: The novel aspect of Tangram's data collection approach requires understanding how to create uncontaminated evaluation datasets
  - Quick check question: How does using real educational materials rather than synthetic data help prevent contamination?

- Concept: Difficulty scaling in visual tasks
  - Why needed here: The three-tier difficulty system requires understanding how visual complexity affects model performance
  - Quick check question: What makes overlapping elements particularly challenging for LMMs compared to isolated elements?

## Architecture Onboarding

- Component map: Data collection pipeline → Element annotation → Difficulty classification → Question generation → Model evaluation → Failure analysis
- Critical path: Data collection → Element annotation → Difficulty classification → Question generation → Model evaluation → Failure analysis
- Design tradeoffs: The choice between synthetic vs. real educational materials trades potential control for ecological validity; the three-tier difficulty system trades granularity for complexity in analysis
- Failure signatures: Consistent underperformance on overlapping elements, sharp accuracy drops with increasing complexity, and element-specific weaknesses indicate fundamental perception limitations rather than reasoning failures
- First 3 experiments:
  1. Compare performance on isolated vs. overlapping elements within the same difficulty level
  2. Test whether element recognition improves with increased model size for different geometric element types
  3. Evaluate whether chain-of-thought prompting helps with complex vs. simple elements differently

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What architectural modifications would most effectively improve LMMs' ability to recognize overlapping geometric elements?
- Basis in paper: Explicit - The paper identifies that LMMs struggle particularly with recognizing overlapping elements like triangles and line segments, and that this difficulty increases with diagram complexity.
- Why unresolved: The paper demonstrates the problem but doesn't propose or test specific architectural solutions beyond noting that larger model sizes don't uniformly help with complex elements.
- What evidence would resolve it: Comparative experiments testing different architectural approaches (attention mechanisms, visual encoders, geometric-specific modules) on Tangram's overlapping element recognition tasks.

### Open Question 2
- Question: How do different types of geometric diagrams (plane vs. solid) require different visual processing strategies in LMMs?
- Basis in paper: Explicit - The paper shows that models perform significantly worse on solid geometry diagrams compared to plane geometry, suggesting different processing challenges.
- Why unresolved: While the performance difference is documented, the paper doesn't investigate what specific visual processing capabilities are lacking for solid geometry.
- What evidence would resolve it: Ablation studies isolating the visual features/models needed for solid vs. plane geometry, potentially revealing if 3D reasoning, depth perception, or other capabilities are the bottleneck.

### Open Question 3
- Question: Is there an optimal model size for geometric element recognition, or does performance scale continuously with size?
- Basis in paper: Explicit - The paper shows performance generally improves with model size for some elements but notes that larger models don't necessarily help with more complex geometric shapes.
- Why unresolved: The paper only tests a limited range of model sizes and observes mixed results, but doesn't systematically explore the relationship between size and performance across all element types.
- What evidence would resolve it: Extensive scaling experiments across multiple orders of magnitude in model size, tracking performance on different geometric element types to identify any optimal size or inflection points.

## Limitations
- Dataset availability uncertainty: The Tangram benchmark dataset is marked as "will be released soon" but current accessibility is unclear
- Cultural specificity: Benchmark focuses exclusively on Chinese educational materials, potentially limiting generalizability
- Evaluation methodology: Reliance on GPT-4o for answer extraction may introduce evaluation bias

## Confidence
- High confidence: The performance gap between LMMs (53.0% accuracy) and human experts (99.5% accuracy) is substantial and well-documented
- Medium confidence: The claim that LMMs struggle specifically with overlapping elements is supported but requires independent verification with different evaluation methodologies
- Medium confidence: The assertion that Tangram represents a unique contribution by focusing on basic geometric perception rather than complex reasoning is reasonable but difficult to verify without comprehensive analysis of all related benchmarks

## Next Checks
1. **Independent Dataset Verification**: Request access to the actual Tangram dataset to verify the reported performance metrics and test whether the same accuracy patterns emerge with different evaluation methodologies
2. **Cross-Cultural Validation**: Test whether the observed performance gaps persist when evaluating LMMs on geometric diagrams from educational systems outside China, to assess the benchmark's generalizability
3. **Evaluation Methodology Audit**: Replicate the experiments using multiple answer extraction models (not just GPT-4o) to determine if the performance gaps are consistent across different evaluation approaches