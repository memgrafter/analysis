---
ver: rpa2
title: Optimizing example selection for retrieval-augmented machine translation with
  translation memories
arxiv_id: '2405.15070'
source_url: https://arxiv.org/abs/2405.15070
tags:
- pour
- translation
- exemples
- nous
- dans
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses optimizing example selection for retrieval-augmented
  neural machine translation using translation memories. It focuses on improving the
  upstream retrieval step for a fixed multi-Levenshtein Transformer model that edits
  multiple examples to produce translations.
---

# Optimizing example selection for retrieval-augmented machine translation with translation memories

## Quick Facts
- **arXiv ID**: 2405.15070
- **Source URL**: https://arxiv.org/abs/2405.15070
- **Reference count**: 0
- **Primary result**: Coverage-based retrieval using Levenshtein distance (λ=0.5) slightly outperforms Maximum Marginal Relevance baseline with BLEU scores of 49.4 (test-0.4) and 62.7 (test-0.6)

## Executive Summary
This paper addresses optimizing example selection for retrieval-augmented neural machine translation using translation memories. It focuses on improving the upstream retrieval step for a fixed multi-Levenshtein Transformer model that edits multiple examples to produce translations. The core method uses submodular function theory to formalize coverage-based retrieval, introducing a smoothed weighted coverage function with parameter λ to balance coverage and relevance. Experiments with English-French translation across six domains compare different coverage metrics (bag-of-words, n-grams, Levenshtein distance) and λ values. Results show the Levenshtein-based coverage (DL) generally outperforms others, with λ=0.5 achieving the best average BLEU scores, slightly surpassing the Maximum Marginal Relevance baseline.

## Method Summary
The method formalizes retrieval-augmented machine translation as a submodular coverage maximization problem. It introduces a smoothed weighted coverage function with parameter λ that balances coverage of source sentence aspects against relevance of individual examples. The greedy algorithm selects k examples by maximizing this function, where coverage can be measured using bag-of-words (SDM), n-grams (NGM), or Levenshtein distance (DL). The multi-Levenshtein Transformer model then edits these examples non-autoregressively to produce translations. The system uses BM25 for initial candidate filtering (100 candidates), followed by coverage-based selection, with evaluation across English-French translation tasks using six different domains.

## Key Results
- Levenshtein-based coverage (DL) outperforms bag-of-words and n-gram approaches for example selection
- Parameter λ=0.5 achieves optimal balance between coverage and relevance, yielding average BLEU scores of 49.4 (test-0.4) and 62.7 (test-0.6)
- Coverage-based retrieval slightly outperforms Maximum Marginal Relevance baseline
- Coverage-based selection tends to select longer examples, making joint editing more challenging

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Submodular functions provide a principled way to optimize coverage-based example selection for translation memories
- Mechanism: The smoothed weighted coverage function with parameter λ balances the trade-off between coverage and relevance by exponentially decaying the contribution of already-covered aspects
- Core assumption: Maximizing coverage of source sentence aspects leads to better translation quality when examples are edited by the downstream model
- Evidence anchors:
  - [abstract] "We rely on the theory of submodular functions and explore new algorithms to optimize this coverage"
  - [section] "We rely on the theory of submodular functions to formalize the RPS based on coverage maximization"
  - [corpus] Weak evidence - corpus analysis shows no direct correlation between submodularity application and downstream performance gains
- Break condition: The assumption that source coverage translates to target coverage breaks down due to linguistic divergences between languages

### Mechanism 2
- Claim: Levenshtein distance-based coverage metric (DL) outperforms bag-of-words and n-gram approaches for example selection
- Mechanism: DL captures character-level alignment information that better represents which source tokens can be directly copied from examples during the edit-based translation process
- Core assumption: The multi-Levenshtein Transformer model benefits most from examples containing directly copyable source tokens
- Evidence anchors:
  - [section] "Results show the Levenshtein-based coverage (DL) generally outperforms others"
  - [section] "The DL score captures the notion of how many source tokens can be copied"
  - [corpus] No corpus evidence provided for DL superiority over other metrics
- Break condition: When source sentences contain high lexical variation or syntactic divergence, the copy-based assumption becomes invalid

### Mechanism 3
- Claim: The parameter λ in the smoothed coverage function controls the trade-off between coverage and relevance
- Mechanism: λ = 0 maximizes coverage by selecting diverse examples, while λ = 1 maximizes relevance by selecting the most similar examples; intermediate values balance these objectives
- Core assumption: There exists an optimal λ value that balances coverage diversity with individual example relevance for translation quality
- Evidence anchors:
  - [section] "With this new definition, an aspect n already covered continues to contribute to the calculation of f, but with a coefficient that decreases exponentially"
  - [section] "λ = 0.5 achieves the best average BLEU scores"
  - [corpus] No corpus evidence provided for λ optimization across different domains
- Break condition: When domain characteristics vary significantly, the optimal λ may differ substantially, making a single value suboptimal

## Foundational Learning

- Submodularity theory
  - Why needed here: Provides the mathematical foundation for formalizing coverage-based example selection as an optimization problem
  - Quick check question: What is the diminishing returns property that defines submodular functions?

- Edit-based neural machine translation
  - Why needed here: The downstream model (multi-Levenshtein Transformer) relies on editing examples, making example quality crucial for performance
  - Quick check question: How does the Levenshtein Transformer model differ from standard autoregressive translation models?

- Information retrieval fundamentals
  - Why needed here: The upstream retrieval step requires understanding of similarity metrics and candidate filtering strategies
  - Quick check question: What is the difference between lexical similarity (BM25) and semantic similarity measures?

## Architecture Onboarding

- Component map:
  - Translation memory: Stores source-target sentence pairs
  - Retrieval system: Filters candidates using BM25, then applies coverage-based selection
  - Coverage function: Calculates smoothed weighted coverage with parameter λ
  - Multi-Levenshtein Transformer: Edit-based model that translates using selected examples

- Critical path: Source sentence → BM25 filtering → Coverage optimization → Example selection → Multi-Levenshtein translation

- Design tradeoffs:
  - Coverage vs. relevance: Higher coverage may include less relevant examples
  - Example length: Longer examples provide more context but are harder to edit jointly
  - Computational cost: Coverage calculation requires pairwise comparisons with all candidates

- Failure signatures:
  - Poor BLEU scores despite high coverage: Indicates source-target coverage assumption is violated
  - Long inference times: Suggests coverage calculation is too expensive for the candidate set size
  - Domain-specific performance drops: Indicates λ value or coverage metric is suboptimal for certain domains

- First 3 experiments:
  1. Compare DL-0 vs DL-1 on a small domain to validate coverage vs relevance tradeoff
  2. Test different λ values (0.1, 0.5, 0.9) on test-0.6 to find optimal balance
  3. Implement MMR baseline and compare against DL-0.5 to validate improvement claims

## Open Questions the Paper Calls Out
- How can the selection of translation examples be optimized jointly with the translation model during training rather than as a separate retrieval step?

## Limitations
- Performance gains vary significantly across domains (49.4 BLEU on test-0.4 vs 62.7 on test-0.6)
- Lacks corpus-level validation showing correlation between coverage metrics and actual edit opportunities
- Critical implementation details for DL metric and alignment algorithm are underspecified

## Confidence
- **High confidence**: Submodular framework and greedy algorithm are mathematically sound with approximation guarantees
- **Medium confidence**: Levenshtein-based coverage superiority demonstrated but lacks explanatory corpus evidence
- **Low confidence**: Optimal λ=0.5 value may be domain-specific rather than universally applicable

## Next Checks
1. Perform ablation studies varying test set composition to quantify sensitivity of optimal λ to domain characteristics
2. Conduct corpus-level analysis correlating coverage metric values with actual edit distances computed by the multi-Levenshtein model
3. Implement and compare multiple alignment algorithms for the DL metric to determine which implementation details most influence performance