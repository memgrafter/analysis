---
ver: rpa2
title: 'Llama Guard 3 Vision: Safeguarding Human-AI Image Understanding Conversations'
arxiv_id: '2411.10414'
source_url: https://arxiv.org/abs/2411.10414
tags:
- llama
- guard
- vision
- classification
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Llama Guard 3 Vision is a multimodal safety classifier designed
  to moderate human-AI conversations involving images. It extends Llama Guard to handle
  both visual and textual content, supporting prompt and response classification using
  the MLCommons taxonomy of 13 hazard categories.
---

# Llama Guard 3 Vision: Safeguarding Human-AI Image Understanding Conversations

## Quick Facts
- **arXiv ID**: 2411.10414
- **Source URL**: https://arxiv.org/abs/2411.10414
- **Reference count**: 7
- **Primary result**: Llama Guard 3 Vision outperforms GPT-4o and GPT-4o mini on internal benchmarks, achieving higher F1 scores and lower false positive rates, particularly in response classification.

## Executive Summary
Llama Guard 3 Vision is a multimodal safety classifier designed to moderate human-AI conversations involving images. It extends Llama Guard to handle both visual and textual content, supporting prompt and response classification using the MLCommons taxonomy of 13 hazard categories. The model is fine-tuned on Llama 3.2-Vision with a hybrid dataset of human-generated and synthetic samples, augmented with text-only safety data for improved generalization. Llama Guard 3 Vision demonstrates superior performance compared to GPT-4o models on internal benchmarks, particularly excelling in response classification while showing differential robustness against adversarial attacks.

## Method Summary
Llama Guard 3 Vision is fine-tuned on Llama 3.2-Vision using a hybrid dataset containing 22,500 prompt-image pairs and 40,034 prompt-response-image samples. The training incorporates human-generated and synthetic samples, augmented with text-only safety data. Images are preprocessed by rescaling to 4 chunks of 560×560 pixels. The model is trained with sequence length 8192, learning rate 1×10^-5, and 3600 training steps using supervised fine-tuning with data augmentation techniques including dropping random categories and shuffling category indices.

## Key Results
- Llama Guard 3 Vision outperforms GPT-4o and GPT-4o mini on internal benchmarks, achieving higher F1 scores and lower false positive rates
- The model demonstrates stronger performance in response classification compared to prompt classification
- Under adversarial testing, the model shows 82% misclassification rate for prompts under high perturbation PGD attacks but only 27% for responses under unbounded attacks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Llama Guard 3 Vision achieves higher F1 scores and lower false positive rates than GPT-4o models, especially in response classification.
- Mechanism: The model is fine-tuned on a hybrid dataset of human-generated and synthetic samples, augmented with text-only safety data, and optimized for multimodal (text and image) prompts and text responses. It primarily relies on model responses for classification while effectively ignoring prompt-based attacks.
- Core assumption: The hybrid dataset and fine-tuning process provide sufficient coverage and generalization across the 13 hazard categories of the MLCommons taxonomy.
- Evidence anchors:
  - [abstract] "Llama Guard 3 Vision outperforms GPT-4o and GPT-4o mini on internal benchmarks, achieving higher F1 scores and lower false positive rates, particularly in response classification."
  - [section] "Table 1 shows that Llama Guard 3 Vision outperforms GPT-4o and GPT-4o mini in terms of F1 score, specifically in response classification."
- Break condition: If the dataset does not adequately cover edge cases or if the model overfits to the training distribution, performance may degrade on unseen or adversarial inputs.

### Mechanism 2
- Claim: Llama Guard 3 Vision exhibits robust behavior in response classification, primarily relying on model responses while effectively ignoring prompt-based attacks.
- Mechanism: The model is more resilient when classifying responses (27% misclassification under unbounded PGD attack) compared to prompt classification (up to 82% misclassification under high perturbation).
- Core assumption: The response classification task is inherently more robust to image-based adversarial attacks than prompt classification.
- Evidence anchors:
  - [abstract] "Adversarial robustness testing shows that while it is vulnerable to PGD image attacks on prompts (up to 82% misclassification at high perturbation), it remains more resilient when classifying responses (27% misclassification under unbounded attack)."
- Break condition: If attackers develop more sophisticated methods that can manipulate both prompts and responses simultaneously, the robustness advantage may diminish.

### Mechanism 3
- Claim: Llama Guard 3 Vision is reasonably robust in response classification against GCG text attacks, assuming that the attacker is only allowed to modify the prompt.
- Mechanism: The model maintains a lower misclassification rate (30% vs. 16% baseline) when the attacker can only modify the prompt, compared to when the attacker can also modify the agent response (75% misclassification).
- Core assumption: The response classification task is inherently more robust to text-based adversarial attacks than prompt classification when the attacker cannot directly manipulate the agent response.
- Evidence anchors:
  - [abstract] "GCG text attacks are also more effective against prompt classification (72% success) than response classification (30% success)."
- Break condition: If attackers gain the ability to manipulate the agent response or develop more sophisticated text-based attacks, the robustness advantage may diminish.

## Foundational Learning

- **Concept: Multimodal safety classification**
  - Why needed here: The model needs to understand and classify both visual and textual content for safety purposes.
  - Quick check question: Can you explain the difference between prompt classification and response classification in the context of multimodal safety?

- **Concept: Adversarial robustness**
  - Why needed here: Understanding the model's vulnerabilities to various attack types (PGD for images, GCG for text) is crucial for deployment and improvement.
  - Quick check question: What are the key differences between PGD and GCG attacks, and how do they affect the model's performance?

- **Concept: MLCommons hazard taxonomy**
  - Why needed here: The model is trained to predict safety labels based on this taxonomy, so understanding its categories is essential for proper use and evaluation.
  - Quick check question: Can you list and briefly describe three categories from the MLCommons hazard taxonomy used in this work?

## Architecture Onboarding

- **Component map**: Llama Guard 3 Vision is built on top of the Llama 3.2 11B vision model, fine-tuned for multimodal safety classification. It includes a vision encoder that rescales images into 4 chunks of 560 × 560 pixels each, and a text processing component for handling prompt and response classification tasks.

- **Critical path**: The critical path for safety classification involves: 1) receiving multimodal input (text and image), 2) processing the image through the vision encoder, 3) combining the image and text representations, 4) applying the fine-tuned safety classification model, and 5) outputting a safety label (safe/unsafe) along with violated categories if applicable.

- **Design tradeoffs**: The model prioritizes response classification robustness over prompt classification, which may lead to higher false positive rates in prompt classification. It also uses a single image per prompt, which may limit its ability to handle more complex multimodal inputs.

- **Failure signatures**: Common failure modes include misclassification of prompts containing ambiguous or complex images, vulnerability to image-based adversarial attacks (PGD) on prompts, and susceptibility to text-based adversarial attacks (GCG) when the attacker can modify both prompts and responses.

- **First 3 experiments**:
  1. Evaluate the model's performance on a held-out test set using the MLCommons hazard taxonomy, comparing F1 scores and false positive rates for prompt and response classification.
  2. Test the model's robustness against PGD image attacks on prompts with varying perturbation budgets, measuring the percentage of harmful prompts misclassified as safe.
  3. Assess the model's vulnerability to GCG text attacks on both prompts and responses, comparing misclassification rates when the attacker can modify only the prompt versus both the prompt and response.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does Llama Guard 3 Vision's performance compare to other proprietary multimodal safety classifiers when evaluated on the same benchmark?
- Basis in paper: [explicit] The paper compares Llama Guard 3 Vision to GPT-4o and GPT-4o mini but does not compare it to other proprietary models.
- Why unresolved: The paper focuses on internal benchmarking and does not include comparisons to other commercial safety classifiers.
- What evidence would resolve it: Comparative evaluation of Llama Guard 3 Vision against other proprietary safety classifiers using the same MLCommons taxonomy and benchmark.

### Open Question 2
- Question: What specific techniques could be employed to further improve Llama Guard 3 Vision's robustness against PGD image attacks beyond the current l∞ perturbation bounds?
- Basis in paper: [inferred] The paper mentions PGD attacks and their effectiveness but does not explore specific robustness enhancement techniques beyond general recommendations.
- Why unresolved: The paper acknowledges the vulnerability to PGD attacks but does not detail concrete methods to improve robustness.
- What evidence would resolve it: Implementation and evaluation of specific adversarial training techniques or defensive distillation methods to improve resistance to PGD attacks.

### Open Question 3
- Question: How does Llama Guard 3 Vision perform in multilingual settings beyond English, particularly for languages with different visual and textual characteristics?
- Basis in paper: [explicit] The paper explicitly states that Llama Guard 3 Vision is optimized for English and only supports one image at the moment.
- Why unresolved: The paper does not provide data on performance in multilingual contexts or with multiple images.
- What evidence would resolve it: Comprehensive evaluation of Llama Guard 3 Vision's performance across multiple languages and image configurations, including non-Latin scripts and culturally diverse imagery.

## Limitations
- Evaluation is based entirely on internal Meta benchmarks with no public or third-party datasets reported
- The exact distribution of the hybrid training set (human-generated vs. synthetic) is unspecified
- Adversarial attack methodologies are referenced but not fully detailed, limiting reproducibility

## Confidence
- **High Confidence**: Llama Guard 3 Vision outperforms GPT-4o models on internal benchmarks in F1 score and false positive rate, particularly for response classification
- **Medium Confidence**: The model demonstrates differential robustness between prompt and response classification against adversarial attacks
- **Low Confidence**: The claim that the hybrid dataset and fine-tuning process provide sufficient coverage across all 13 MLCommons hazard categories

## Next Checks
1. Test Llama Guard 3 Vision on publicly available multimodal safety datasets to verify whether internal benchmark performance translates to external data
2. Implement and test against a broader range of adversarial attack methods beyond PGD and GCG
3. Obtain or reconstruct the training dataset composition to analyze balance between human-generated and synthetic samples across all 13 hazard categories