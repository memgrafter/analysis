---
ver: rpa2
title: 'Diversity Drives Fairness: Ensemble of Higher Order Mutants for Intersectional
  Fairness of Machine Learning Software'
arxiv_id: '2412.08167'
source_url: https://arxiv.org/abs/2412.08167
tags:
- fairness
- fairhome
- intersectional
- software
- methods
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: FairHOME addresses the challenge of intersectional fairness in
  ML software, which requires fairness across subgroups defined by multiple protected
  attributes. The method generates diverse input mutants representing different subgroups
  for each input instance using higher order mutation across multiple protected attributes,
  inspired by social science theories that highlight the benefits of diversity in
  decision-making.
---

# Diversity Drives Fairness: Ensemble of Higher Order Mutants for Intersectional Fairness of Machine Learning Software

## Quick Facts
- arXiv ID: 2412.08167
- Source URL: https://arxiv.org/abs/2412.08167
- Reference count: 40
- FairHOME enhances intersectional fairness by 47.5% on average, 9.6 percentage points higher than current best methods

## Executive Summary
FairHOME addresses intersectional fairness in ML software by generating diverse input mutants representing different subgroups for each input instance using higher order mutation across multiple protected attributes. Unlike conventional ensemble methods that combine predictions from different models, FairHOME combines predictions for the original input and its mutants, all generated by the same ML model. This approach achieves significant fairness improvements with minimal performance degradation, outperforming seven state-of-the-art fairness improvement methods across 24 decision-making tasks.

## Method Summary
FairHOME generates mutants by applying higher order mutation to protected attributes in input instances, creating representations of all possible subgroup combinations. The original ML model makes predictions on both the original input and all mutants, which are then aggregated using ensemble strategies (majority vote, averaging, or weighted averaging). This process enhances fairness by incorporating diverse subgroup perspectives without requiring model retraining, making it applicable to deployed ML software.

## Key Results
- FairHOME improved intersectional fairness by 47.5% on average across six fairness metrics
- Outperformed existing methods by 9.6 percentage points on average
- Achieved fairness gains with only 0.1% to 2.7% performance reduction

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Generating mutants representing diverse subgroups broadens decision-making perspectives and reduces bias
- Mechanism: Higher order mutation creates mutants for all subgroup combinations, and aggregating predictions reduces over-representation of privileged groups
- Core assumption: Diversity in input representation leads to fairer outcomes
- Evidence anchors: Abstract mentions broadening perspectives; Section III-B discusses exploring input domain exhaustively
- Break condition: If model is too sensitive to input variations or predictions are uncorrelated with protected attributes

### Mechanism 2
- Claim: Combining predictions from same model for diverse inputs achieves fairness without retraining
- Mechanism: Same trained model predicts on original input and mutants, ensemble provides balanced decision
- Core assumption: Well-trained model makes consistent predictions across subgroups
- Evidence anchors: Abstract contrasts with conventional ensemble methods; Section III-A describes output ensemble
- Break condition: If model is heavily biased such that aggregation doesn't overcome discrimination

### Mechanism 3
- Claim: Ensemble strategies provide robustness to biased predictions
- Mechanism: Majority vote, averaging, and weighted averaging aggregate predictions to mitigate bias influence
- Core assumption: Different strategies effectively combine predictions to reduce bias
- Evidence anchors: Section III-C introduces three ensemble strategies; Section V-E shows similar fairness improvements
- Break condition: If ensemble strategies don't effectively reduce bias or introduce new unfairness

## Foundational Learning

- Concept: Higher order mutation
  - Why needed here: Generates mutants representing all possible combinations of protected attribute values for diversity
  - Quick check question: How does higher order mutation differ from simple mutation, and why is it important for intersectional fairness?

- Concept: Ensemble learning
  - Why needed here: Combines predictions from same model for original input and mutants to achieve fairness without retraining
  - Quick check question: What are advantages and disadvantages of using same model for ensemble versus different models?

- Concept: Fairness metrics (WC-SPD, WC-AOD, WC-EOD, AC-SPD, AC-AOD, AC-EOD)
  - Why needed here: Measures intersectional fairness across subgroups defined by multiple protected attributes
  - Quick check question: What is difference between worst-case and average-case intersectional fairness metrics, and when is each more appropriate?

## Architecture Onboarding

- Component map: Input → Mutation → Prediction → Aggregation → Decision
- Critical path: Original input undergoes mutation to generate mutants → Original model predicts on all inputs → Ensemble strategy aggregates predictions → Final fair decision
- Design tradeoffs:
  - Simplicity vs. fairness: Majority vote is simpler but may be less effective than weighted averaging
  - Performance vs. fairness: Aggregating predictions may reduce accuracy but improve fairness
  - Coverage vs. efficiency: Generating all possible mutants ensures coverage but increases computational cost
- Failure signatures:
  - Decreased performance: If model is too sensitive to input variations
  - Increased bias: If model is heavily biased and aggregation doesn't overcome discrimination
  - Computational overhead: If generating and processing all mutants is too expensive
- First 3 experiments:
  1. Apply FairHOME to simple binary classification task with two protected attributes and evaluate fairness improvements using WC-SPD and AC-SPD
  2. Compare effectiveness of different ensemble strategies on dataset with known bias
  3. Analyze impact of mutating non-protected features correlated with protected attributes on fairness and performance

## Open Questions the Paper Calls Out

The paper doesn't explicitly call out open questions, but several remain unresolved:
- Scalability to datasets with more than two protected attributes
- Optimal ensemble strategy selection criteria
- Performance on real-world deployed systems versus benchmark datasets
- Trade-off between mutant diversity and computational efficiency

## Limitations

- Computational overhead of generating all possible mutants may become prohibitive with many protected attributes or subgroups
- Limited exploration of scenarios where protected attributes are correlated with non-protected features
- Generalizability to continuous protected attributes and datasets with more than two protected attributes remains untested

## Confidence

**High confidence**: Core mechanism of generating mutants and aggregating predictions is well-defined and theoretically sound. Evaluation methodology using standard benchmarks and multiple fairness metrics provides strong empirical support.

**Medium confidence**: Claim that FairHOME achieves fairness improvements with minimal performance degradation is supported by experiments, but trade-off analysis could be more comprehensive across different model types and domains.

**Low confidence**: Generalizability to scenarios with more than two protected attributes or continuous protected attributes is not explored, limiting applicability to complex real-world situations.

## Next Checks

1. Evaluate FairHOME's performance and computational requirements on datasets with three or more protected attributes to assess scalability limitations

2. Systematically test FairHOME on datasets where protected attributes are strongly correlated with non-protected features to understand impact on fairness improvements

3. Apply FairHOME to multiple types of ML models (decision trees, neural networks, SVMs) beyond tested ones to verify fairness improvements are not model-specific