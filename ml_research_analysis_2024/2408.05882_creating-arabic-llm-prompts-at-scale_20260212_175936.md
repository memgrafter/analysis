---
ver: rpa2
title: Creating Arabic LLM Prompts at Scale
arxiv_id: '2408.05882'
source_url: https://arxiv.org/abs/2408.05882
tags:
- prompts
- datasets
- fine
- arabic
- translation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper presents two methods for creating Arabic instruction-following
  prompts at scale: (1) generating prompts from 78 existing Arabic NLP datasets using
  the PromptSource tool, resulting in 67.4 million prompts covering 67 different tasks,
  and (2) translating and filtering English prompt datasets (PromptSource and Super-NaturalInstructions)
  using machine translation quality estimation, yielding 20 million high-quality prompts.
  The authors fine-tuned the base Qwen2 7B model using 800K and 8M of their newly
  created prompts, and showed that both fine-tuned models significantly outperformed
  the publicly available instruction-tuned Qwen2 7B, as well as other baseline models
  including AceGPT-Instruct 7B, Jais-chat 13B, and Llama3-Instruct 8B.'
---

# Creating Arabic LLM Prompts at Scale

## Quick Facts
- arXiv ID: 2408.05882
- Source URL: https://arxiv.org/abs/2408.05882
- Authors: Abdelrahman El-Sheikh; Ahmed Elmogtaba; Kareem Darwish; Muhammad Elmallah; Ashraf Elneima; Hassan Sawaf
- Reference count: 0
- Primary result: Fine-tuned Qwen2 7B outperforms much larger Llama3 70B on Arabic prompts using 8M task-specific prompts

## Executive Summary
This paper presents two methods for creating Arabic instruction-following prompts at scale: generating prompts from 78 existing Arabic NLP datasets using PromptSource (67.4 million prompts), and translating/ filtering English prompt datasets (20 million high-quality prompts). The authors fine-tune the base Qwen2 7B model using these prompts and demonstrate significant performance improvements over both the original instruction-tuned Qwen2 7B and other baseline models including AceGPT-Instruct 7B, Jais-chat 13B, and Llama3-Instruct 8B. The fine-tuned Qwen2 7B model using 8M prompts achieved an average ROUGE-L score of 0.224, slightly outperforming the much larger Llama3-Instruct 70B (0.221).

## Method Summary
The authors developed two complementary approaches to create Arabic prompts at scale. First, they used the PromptSource tool to generate natural language prompts from 78 publicly available Arabic NLP datasets, creating 67.4 million prompts covering 67 different tasks. Second, they translated and filtered English prompt datasets (PromptSource and Super-NaturalInstructions) using machine translation quality estimation, retaining only high-quality translations. The base Qwen2 7B model was fine-tuned using LoRA with 800K and 8M prompts from these combined datasets, achieving significant performance improvements over baseline models.

## Key Results
- Fine-tuned Qwen2 7B with 8M prompts achieved average ROUGE-L score of 0.224, outperforming Llama3-Instruct 70B (0.221)
- Both fine-tuned models significantly outperformed the publicly available instruction-tuned Qwen2 7B (0.143) and other baselines
- Fine-tuning Qwen2 7B with 800K prompts achieved average ROUGE-L score of 0.184, already showing significant improvement
- The approach demonstrates that task-specific Arabic prompts enable smaller models to match or exceed performance of much larger instruction-tuned models

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Translating English prompt datasets to Arabic and filtering via reference-free MT quality estimation produces high-quality Arabic prompts.
- Mechanism: Opus-MT neural model translates English prompts, COMET-QE evaluates translation quality per sentence, and prompts with all sentences scoring above 0.7 are retained.
- Core assumption: Reference-free MT quality estimation correlates well with human judgment for Arabic translation quality.
- Evidence anchors:
  - [abstract] "we translate both PromptSource and Super-NaturalInstructions, and we use state-of-the-art referenceless machine translation quality estimation (Rei et al., 2023) to retain only prompts where all the sentences that make up the prompt have a minimum quality score."
  - [section 2.2] "the pipeline incorporates the COMET-QE reference-free MT evaluation model (Rei et al., 2023), which has been shown to correlate well with human judgments."
  - [corpus] Weak evidence for Arabic-specific MT quality correlation; no cited Arabic MT QE studies found.
- Break condition: If COMET-QE fails to correlate with human judgment for Arabic, the translation quality threshold would filter out too many or too few prompts.

### Mechanism 2
- Claim: Fine-tuning a base LLM with task-specific Arabic prompts significantly improves performance over general instruction-tuned models.
- Mechanism: Low-Rank Adaptation (LoRA) freezes base Qwen2 parameters and trains small adaptation matrices on 800K or 8M Arabic task prompts.
- Core assumption: LoRA fine-tuning with task-specific prompts transfers sufficient Arabic instruction-following capability.
- Evidence anchors:
  - [abstract] "We show that fine tuning an open 7 billion parameter large language model, namely base Qwen2 7B, enables it to outperform a state-of-the-art 70 billion parameter instruction tuned model, namely Llama3 70B, in handling Arabic prompts."
  - [section 3.4] "fine tuning Qwen2 7B model using our carefully curated dataset of 800k samples led to statistically significant improvement over the Qwen2-Instruct 7B with respective average ROUGE-L scores of 0.184 and 0.143."
  - [corpus] No corpus evidence; assumes LoRA efficacy without validation.
- Break condition: If LoRA adaptation fails to capture task-specific patterns, performance gains will not materialize despite training data size.

### Mechanism 3
- Claim: Creating natural language prompts from existing Arabic NLP datasets using PromptSource yields high task coverage and volume.
- Mechanism: 78 Arabic datasets are used to generate 2-8 diverse prompt templates each, covering 67 tasks, producing 67.4 million prompts.
- Core assumption: PromptSource's template system effectively maps structured NLP tasks to natural language instructions.
- Evidence anchors:
  - [abstract] "The first methods entails automatically translating existing prompt datasets from English, such as PromptSource and Super-NaturalInstructions, and then using machine translation quality estimation to retain high quality translations only. The second method involves creating natural language prompts on top of existing Arabic NLP datasets."
  - [section 2.1] "We created natural language prompts for 78 publicly available Arabic datasets using the PromptSource sourcing tool. From 78 datasets, we generated 67,488,303 prompts."
  - [corpus] No corpus evidence; assumes PromptSource template diversity is sufficient.
- Break condition: If prompt templates fail to cover the full task space or introduce bias, downstream LLM performance will be inconsistent.

## Foundational Learning

- Concept: Reference-free machine translation quality estimation (MTQE).
  - Why needed here: To filter translated English prompts without needing human-annotated reference translations.
  - Quick check question: What metric does COMET-QE use to evaluate translation quality without references?

- Concept: Low-Rank Adaptation (LoRA) fine-tuning.
  - Why needed here: To efficiently fine-tune large LLMs on task-specific data without full parameter updates.
  - Quick check question: How does LoRA reduce the number of trainable parameters compared to full fine-tuning?

- Concept: PromptSource template system.
  - Why needed here: To systematically convert structured NLP datasets into natural language instruction prompts.
  - Quick check question: What are the two main components of a PromptSource prompt template?

## Architecture Onboarding

- Component map: Sentence splitter -> Opus-MT translator -> COMET-QE evaluator -> Manual verifier -> Prompt dataset loader -> LoRA adapter -> Qwen2 7B base -> Fine-tuning loop -> Test prompts -> LLM -> ROUGE-L scorer -> Dataset scorer

- Critical path:
  1. Generate Arabic prompts from existing datasets (manual + translation)
  2. Split into train/validation/test sets
  3. Fine-tune Qwen2 7B with LoRA using sampled training sets
  4. Evaluate on held-out test set across 67 tasks

- Design tradeoffs:
  - Translation quality threshold (0.7) vs. prompt volume: higher threshold reduces noise but lowers total prompt count
  - LoRA rank and alpha: higher values increase adaptation capacity but risk overfitting
  - Prompt template diversity vs. consistency: more templates increase coverage but may reduce prompt uniformity

- Failure signatures:
  - Low ROUGE-L on test set despite high training ROUGE-L -> overfitting
  - High variance across tasks -> uneven prompt quality or task coverage gaps
  - Sudden performance drop after fine-tuning -> catastrophic forgetting or noisy training data

- First 3 experiments:
  1. Run COMET-QE on a small sample of translated prompts and manually verify quality correlation.
  2. Fine-tune Qwen2 7B with 10K prompts from one dataset and test on that dataset only.
  3. Compare ROUGE-L scores of translated vs. manually created prompts on the same task.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal balance between translated and native Arabic prompts for achieving the best model performance?
- Basis in paper: [inferred] The paper describes two methods for creating Arabic prompts (translation-based and native prompt creation) and shows that both methods contribute to improved model performance, but does not explore the optimal ratio between them.
- Why unresolved: The authors fine-tuned their model using a combined dataset from both methods without analyzing whether one source is more effective than the other or what the ideal proportion should be.
- What evidence would resolve it: Systematic experiments comparing model performance when trained exclusively on translated prompts, exclusively on native prompts, and various combinations of both.

### Open Question 2
- Question: How does the quality estimation threshold (0.7) impact the overall effectiveness of the translated prompts and could a lower threshold yield better results?
- Basis in paper: [explicit] The authors mention using a threshold of 0.7 for filtering translations, noting this "enhances reliability" but acknowledging it "ensures that only high-quality translated prompts are retained."
- Why unresolved: The paper does not explore how varying this threshold would affect downstream model performance, nor does it justify why 0.7 was chosen as the optimal threshold.
- What evidence would resolve it: Experiments training models with prompts filtered at different quality thresholds (e.g., 0.5, 0.6, 0.7, 0.8) to measure performance trade-offs between quantity and quality.

### Open Question 3
- Question: How well do the fine-tuned models generalize to unseen Arabic dialects and regional variations not represented in the training data?
- Basis in paper: [inferred] The paper mentions that some datasets involve "dialect classification" but does not discuss whether the training data adequately represents the diversity of Arabic dialects or how the model performs on underrepresented dialects.
- Why unresolved: The evaluation focuses on aggregate performance across tasks without analyzing dialect-specific performance or testing on dialect-specific benchmarks.
- What evidence would resolve it: Testing the fine-tuned models on benchmark datasets representing different Arabic dialects (Egyptian, Levantine, Gulf, Maghrebi, etc.) and measuring performance variations across dialects.

## Limitations
- Limited Arabic-specific validation of COMET-QE quality estimation model, with no cited studies demonstrating its effectiveness for Arabic translation quality assessment
- No systematic quality assessment of the 67.4 million automatically generated prompts from 78 datasets, assuming consistent quality across all tasks
- Does not explore optimal balance between translated and native Arabic prompts or test model performance on underrepresented Arabic dialects

## Confidence

**Major Uncertainties:**
The primary uncertainty lies in the Arabic-specific validation of the COMET-QE reference-free MT quality estimation model. While the authors claim it "correlates well with human judgments," no specific Arabic MTQE studies or validation results are cited or presented.

**Confidence Labels:**
- High confidence: The general methodology of using LoRA fine-tuning with task-specific prompts is well-established and the observed performance improvements over baselines are substantial and statistically significant.
- Medium confidence: The claim that reference-free MTQE works effectively for Arabic translation quality assessment is plausible but lacks direct Arabic-specific validation evidence.
- Low confidence: The assumption that the 67.4 million automatically generated prompts from 78 datasets maintain consistent quality across all tasks, given no systematic quality assessment of the PromptSource-generated prompts is reported.

## Next Checks
1. Conduct a controlled study evaluating COMET-QE scores against human judgments specifically for Arabic prompt translation quality to verify the 0.7 threshold's appropriateness.
2. Perform ablation studies comparing model performance when fine-tuned with different prompt volumes (e.g., 100K vs 800K vs 8M) to determine optimal training size and identify potential overfitting patterns.
3. Systematically evaluate ROUGE-L score variance across the 67 tasks to identify which task categories show the largest performance gaps and investigate whether prompt template quality correlates with task performance.