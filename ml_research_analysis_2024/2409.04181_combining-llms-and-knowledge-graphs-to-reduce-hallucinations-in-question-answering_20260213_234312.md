---
ver: rpa2
title: Combining LLMs and Knowledge Graphs to Reduce Hallucinations in Question Answering
arxiv_id: '2409.04181'
source_url: https://arxiv.org/abs/2409.04181
tags:
- which
- question
- cypher
- what
- query
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The authors address hallucinations in biomedical question answering
  by integrating LLMs with a knowledge graph and introducing a query checker that
  validates and corrects Cypher queries. Using the LangChain framework, their method
  generates queries from natural language, checks them against the KG schema, and
  executes them on a 2-hop subgraph of PrimeKG.
---

# Combining LLMs and Knowledge Graphs to Reduce Hallucinations in Question Answering

## Quick Facts
- arXiv ID: 2409.04181
- Source URL: https://arxiv.org/abs/2409.04181
- Reference count: 40
- Primary result: Query checker improves LLM-generated Cypher accuracy by 14.95 percentage points, with GPT-4 Turbo achieving 90% accuracy

## Executive Summary
This paper addresses hallucinations in biomedical question answering by integrating large language models with knowledge graphs through a query-checking mechanism. The authors introduce a three-stage validation system that checks and corrects LLM-generated Cypher queries against the PrimeKG schema before execution. Using the LangChain framework, their approach combines validated queries with retrieval-augmented generation to ground answers in verified KG facts. The system achieves 90% query accuracy with GPT-4 Turbo and demonstrates that query validation can significantly improve LLM performance on structured query generation tasks.

## Method Summary
The authors developed a KGQA pipeline using LangChain that generates Cypher queries from natural language questions, validates them through a three-stage query checker (syntax node checker, node checker, and relation checker), and executes them on a 2-hop subgraph of PrimeKG. The query checker automatically corrects common schema and direction errors in near-miss queries. Results are then used for RAG-based answer generation. The system was evaluated on 50 biomedical questions using multiple LLMs including GPT-4 Turbo and Llama 3.3:70b, testing zero-shot, one-shot, and few-shot prompting strategies.

## Key Results
- GPT-4 Turbo achieved 90% query accuracy on biomedical questions
- Query checker corrected 172 of 999 erroneous queries, providing 14.95 percentage point accuracy uplift
- Few-shot prompting significantly improved Llama 3.3:70b performance but had minimal impact on other models
- System includes a user-friendly web interface and custom benchmark dataset

## Why This Works (Mechanism)

### Mechanism 1
LLM-generated Cypher queries can be syntactically and semantically validated against a KG schema before execution, reducing hallucinations. The query checker validates queries in three stages: syntax node checker ensures return clauses reference valid node names and types, node checker validates node types against the KG schema, relation checker verifies relationship direction and type compatibility. The checker can fix near-miss errors without breaking intent. This works because LLMs often produce near-correct queries differing only in node type, relationship direction, or return attribute syntax. The mechanism fails when LLM's query structure is fundamentally wrong (e.g., wrong path pattern, missing join).

### Mechanism 2
Integrating structured KG data with LLMs via retrieval-augmented generation grounds answers in verified facts, lowering hallucination risk. Validated Cypher queries retrieve exact answer sets from the KG, which are then used to generate natural-language responses, ensuring every claim maps to a known KG triple. This assumes the KG is authoritative and complete for the queried domain, allowing answers to be reconstructed from its subgraph without external inference. The approach fails when KG incompleteness or missing relevant triples means valid answers cannot be retrieved, forcing fallback to hallucination.

### Mechanism 3
Prompt engineering, particularly few-shot examples, can substantially improve LLM accuracy on KG query generation. Providing structured examples in prompts teaches the model the desired Cypher pattern and schema constraints, leading to fewer generation errors. This works because the LLM's reasoning ability can be guided by demonstration, and errors are mostly pattern-level rather than fundamental understanding. The approach fails if the LLM lacks sufficient reasoning depth, as even good examples may not prevent structural mistakes.

## Foundational Learning

- **Knowledge Graph schema and node/relationship typing**: Understanding KG schema is essential for the query checker to validate node types and relationship directions; engineers must map natural-language entities to schema elements. Quick check: Given a schema with node types {Drug, Disease, Gene} and relations {contraindicates}, can you map "multiple sclerosis" to the correct node type and verify a relation direction?

- **Cypher query syntax and structure**: Engineers must understand Cypher to read and debug generated queries, as the query checker needs to parse and validate LLM outputs. Quick check: Identify the error in `MATCH (d:pathway {name:"multiple sclerosis"})-[:contraindication]->(dr:drug) RETURN dr;` and correct it.

- **Retrieval-augmented generation (RAG) workflow**: Understanding how query results feed back into answer generation clarifies why grounding reduces hallucinations. Quick check: In a RAG pipeline, where do the facts originate and how are they combined with LLM output?

## Architecture Onboarding

- **Component map**: User query → LLM (Cypher generator) → Query checker (validator/corrector) → Neo4jGraph (KG executor) → RAG (answer generator) → Web UI
- **Critical path**: LLM generation → query checker → KG execution → answer synthesis
- **Design tradeoffs**: Checker improves accuracy but adds latency; open-source models cheaper but less accurate; few-shot prompts improve quality but increase prompt size
- **Failure signatures**: Checker cannot fix wrong path patterns; missing KG triples cause empty results; overly long prompts exceed context limits
- **First 3 experiments**:
  1. Run the pipeline on a single sample question and inspect the uncorrected vs corrected Cypher output
  2. Benchmark accuracy with and without the query checker on 5 hand-picked questions
  3. Compare LLM accuracy with zero-shot vs few-shot prompting on the same 5 questions

## Open Questions the Paper Calls Out

### Open Question 1
Does integrating more advanced semantic matching (e.g., fuzzy or contextual entity resolution) into the LLM-to-Cypher pipeline improve accuracy compared to exact matching under a fixed schema? The paper explicitly notes that exact matching was chosen "by design" to isolate reasoning-to-Cypher under a fixed schema, and adding semantic or fuzzy matching would change the construct to alias coverage. This remains unresolved because the current study fixed the schema and matching method to isolate reasoning; expanding to semantic matching would change the evaluation construct. Evidence would require a controlled experiment comparing exact vs. semantic matching on the same fixed schema, measuring query accuracy uplift.

### Open Question 2
Can prompt engineering or few-shot learning bridge the performance gap between proprietary and open-source LLMs in generating accurate Cypher queries? The authors found that GPT-4 Turbo and GPT-5 consistently outperformed open-source models, and few-shot prompting notably improved Llama 3:70b performance but did not close the gap. This remains unresolved because the study tested prompt strategies but did not find sufficient improvement to match proprietary model accuracy. Evidence would require systematic testing of advanced prompt engineering techniques (e.g., chain-of-thought, role-playing) across multiple open-source models to determine if performance can match proprietary models.

### Open Question 3
How does the performance of the LLM-based KGQA pipeline change when scaling from a 2-hop subgraph to the full PrimeKG or other larger biomedical knowledge graphs? The experiments used a reduced 2-hop subgraph of PrimeKG for efficiency, with no evaluation on the full graph or alternative KGs. This remains unresolved because the study focused on a controlled subset to isolate reasoning; real-world performance on larger graphs is unknown. Evidence would require benchmarking the same pipeline on progressively larger subgraphs and the full PrimeKG, measuring query accuracy and execution efficiency.

## Limitations
- Query checker cannot fix all error types, particularly when LLM's conceptual understanding is fundamentally wrong (e.g., wrong path pattern, missing join conditions)
- KG completeness assumption is critical but potentially fragile - system cannot produce correct answers if relevant triples are missing
- Limited generalizability beyond biomedical domain and PrimeKG, as evaluation was restricted to 50 questions on a single KG subgraph

## Confidence

**High Confidence**: The mechanism by which Cypher query validation and correction reduces hallucinations through grounding in KG schema, supported by quantitative results showing 90% query accuracy for GPT-4 Turbo and measurable accuracy uplift from the query checker.

**Medium Confidence**: The claim that few-shot prompting significantly improves Llama 3.3:70b accuracy, while other models show minimal improvement, as this finding is based on a single comparison and the underlying reasons for model-specific differences are not explored.

**Low Confidence**: The generalizability of the approach beyond biomedical domain and PrimeKG, as the evaluation is limited to 50 questions on a single KG subgraph without testing on other domains or knowledge graph structures.

## Next Checks

1. **Error Classification Analysis**: Manually categorize the 999 erroneous queries into types (syntax errors, wrong node types, incorrect relationships, wrong path patterns, etc.) to determine what proportion the query checker can and cannot fix.

2. **KG Completeness Impact**: Measure how many questions with incorrect or empty results are due to missing triples in the KG versus incorrect queries, to quantify the relative contribution of KG incompleteness to overall performance.

3. **Cross-Domain Generalization**: Test the complete pipeline (LLM + query checker + KG) on a non-biomedical knowledge graph with a different schema to assess whether the approach generalizes beyond the evaluated domain.