---
ver: rpa2
title: 'The Visual Experience Dataset: Over 200 Recorded Hours of Integrated Eye Movement,
  Odometry, and Egocentric Video'
arxiv_id: '2404.18934'
source_url: https://arxiv.org/abs/2404.18934
tags:
- https
- page
- investigation
- arxiv
- vision
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The Visual Experience Dataset (VEDB) introduces over 240 hours
  of egocentric video paired with gaze and head-tracking data from 58 observers aged
  6-49, recorded across 717 sessions. This dataset addresses limitations in existing
  visual experience datasets by providing integrated gaze, head movement, and egocentric
  video data, enabling researchers to study natural scene statistics, gaze behavior,
  and head-eye coordination during everyday activities.
---

# The Visual Experience Dataset: Over 200 Recorded Hours of Integrated Eye Movement, Odometry, and Egocentric Video

## Quick Facts
- arXiv ID: 2404.18934
- Source URL: https://arxiv.org/abs/2404.18934
- Reference count: 20
- Over 240 hours of egocentric video paired with gaze and head-tracking data from 58 observers aged 6-49

## Executive Summary
The Visual Experience Dataset (VEDB) introduces over 240 hours of egocentric video paired with gaze and head-tracking data from 58 observers aged 6-49, recorded across 717 sessions. This dataset addresses limitations in existing visual experience datasets by providing integrated gaze, head movement, and egocentric video data, enabling researchers to study natural scene statistics, gaze behavior, and head-eye coordination during everyday activities. The dataset includes fine-grained annotations of tasks and environments, making it suitable for training and evaluating deep neural networks for scene recognition and human activity detection. VEDB's unique features include high-resolution egocentric video, robust head-tracking via VI-SLAM, and gaze data with median errors under 2 degrees for 63 hours of content. The dataset is publicly available with privacy protections and offers a rich resource for studying visual perception and behavior in naturalistic settings.

## Method Summary
The dataset was collected using a custom hardware setup including Pupil Labs Core eye-tracking system, FLIR Chameleon3 world camera, and Intel RealSense T265 tracking camera. Each session consists of synchronized first-person video, binocular eye videos, and tracking camera data including odometry and IMU. The data underwent preprocessing including pupil detection, gaze calibration using thin-plate spline regression, head-tracking transformation via VI-SLAM, and privacy protection through face blurring. The dataset is publicly available through open science platforms with different access levels for sensitive data.

## Key Results
- Over 240 hours of egocentric video combined with gaze- and head-tracking data
- 58 observers ranging from 6-49 years old across 717 sessions
- Gaze data with median errors under 2 degrees for 63 hours of content
- 124 scene categories annotated using the Places database

## Why This Works (Mechanism)

### Mechanism 1
Integrating egocentric video with gaze and head-tracking data enables fine-grained study of visual experience statistics. By combining video streams from a world camera with synchronized eye-tracking and head-tracking data, the dataset captures not just what is visible, but what is actually attended and how head movements interact with visual exploration.

### Mechanism 2
Large, diverse sampling across observers, tasks, and environments mitigates dataset bias and enables generalizable findings. Recruiting 58 observers aged 6-49 across 717 sessions and 124 scene categories ensures that the dataset captures a broad distribution of visual experiences.

### Mechanism 3
Privacy-preserving preprocessing (blurring faces, de-identification) enables public release without violating participant privacy. Applying Gaussian blur to detected faces and limiting access to sensitive data balances scientific utility with ethical obligations.

## Foundational Learning

- **Simultaneous Localization and Mapping (SLAM)**: The dataset uses a VI-SLAM system (Intel RealSense T265) to track head position and orientation in real-world coordinates. Quick check: What are the two main outputs of a SLAM system that are relevant for analyzing visual experience?
- **Thin-Plate Spline Regression**: Used to calibrate gaze by mapping pupil positions to gaze locations in the world camera view. Quick check: Why might a non-linear calibration method like thin-plate splines be preferred over linear regression for gaze mapping?
- **Scene Category Classification**: The dataset annotates scene locations using the Places database, enabling studies of how scene type influences gaze and head behavior. Quick check: How does the distribution of scene categories in the dataset affect the generalizability of models trained on it?

## Architecture Onboarding

- **Component map**: World Camera (FLIR Chameleon3) → High-resolution egocentric video; Eye Cameras (Pupil Labs Core) → Binocular eye video for pupil detection and gaze tracking; Tracking Camera (Intel RealSense T265) → Odometry, IMU, and VI-SLAM pose estimation; Acquisition Software → Synchronized recording and compression; Preprocessing Pipeline → Pupil detection, calibration, validation, head-tracking transformation; Privacy Pipeline → Face detection and blurring
- **Critical path**: Recording → Synchronization → Calibration → Validation → Privacy filtering → Distribution
- **Design tradeoffs**: High-resolution video improves scene statistics but increases storage and processing demands; Broad observer age range increases diversity but introduces variability in gaze calibration stability; Privacy blurring protects identities but may reduce face detection performance in downstream tasks
- **Failure signatures**: Low pupil detection confidence → Calibration failures or drift; Missing synchronization timestamps → Inability to align gaze with scene events; Incomplete scene annotations → Gaps in task/environment context
- **First 3 experiments**: 1) Verify synchronization accuracy by cross-correlating timestamps across all three camera streams in a sample session; 2) Assess gaze calibration stability by computing drift over time in passive vs. active sessions; 3) Test face blurring efficacy by running a face detection algorithm on blurred vs. original frames to measure recall and precision

## Open Questions the Paper Calls Out

### Open Question 1
How does the frequency and distribution of different object categories in egocentric vision change across different age groups and developmental stages? The dataset includes observers aged 6-49 years old, enabling age-related comparisons, but does not analyze age-related differences in visual experience.

### Open Question 2
What are the specific environmental and task conditions that lead to the highest gaze tracking accuracy or failure rates? While error rates are reported, the specific conditions causing failures are not systematically identified.

### Open Question 3
How does the spatial distribution of fixated objects (faces, hands, etc.) relate to functional organization in higher-level visual cortex? The case study shows spatial distribution of faces and hands relative to fixation but does not explore the neural implications.

## Limitations

- Self-selected nature of recordings may not fully capture all visual experiences
- Privacy protections effectiveness across diverse demographics and occlusions not independently validated
- Specific performance benchmarks for deep neural network training not provided

## Confidence

- **High** confidence in dataset's core value for providing integrated gaze, head, and video data at scale
- **Medium** confidence in claimed applications for scene recognition and activity detection, pending further validation studies
- **Medium** confidence in privacy protections, as efficacy depends on face detection performance across varied conditions

## Next Checks

1. Conduct an independent audit of face blurring efficacy across demographic groups and occlusion scenarios.
2. Validate gaze calibration stability over extended recording periods by comparing drift between passive and active sessions.
3. Benchmark deep neural network performance on scene recognition tasks using VEDB against other established datasets to quantify its added value.