---
ver: rpa2
title: 'Derivative-Free Optimization via Finite Difference Approximation: An Experimental
  Study'
arxiv_id: '2411.00112'
source_url: https://arxiv.org/abs/2411.00112
tags:
- uni00000013
- algorithm
- uni00000014
- gradient
- spsa
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper conducts an experimental comparison of derivative-free
  optimization (DFO) methods that use finite difference approximation for gradient
  estimation. Two classical approaches - Kiefer-Wolfowitz (KW) and Simultaneous Perturbation
  Stochastic Approximation (SPSA) - use only two function evaluations per iteration
  to conserve samples but suffer from imprecise gradient estimates, requiring diminishing
  step sizes and resulting in slow convergence.
---

# Derivative-Free Optimization via Finite Difference Approximation: An Experimental Study

## Quick Facts
- arXiv ID: 2411.00112
- Source URL: https://arxiv.org/abs/2411.00112
- Reference count: 8
- Key outcome: Batch-based finite difference estimators (Cor-CFD) outperform classical KW and SPSA algorithms in both low-dimensional and high-dimensional settings by providing more accurate gradient estimates through sample recycling and correlation exploitation.

## Executive Summary
This paper presents an experimental comparison of derivative-free optimization methods that use finite difference approximation for gradient estimation. The study contrasts classical approaches like Kiefer-Wolfowitz (KW) and Simultaneous Perturbation Stochastic Approximation (SPSA), which use only two function evaluations per iteration, with batch-based finite difference estimators that use more samples for improved accuracy. The experiments demonstrate that when an efficient batch-based estimator (Cor-CFD) is applied, the resulting gradient descent algorithm generally shows better performance compared to classical methods across various test scenarios.

## Method Summary
The paper conducts experimental comparisons between classical two-sample derivative-free methods (KW and SPSA) and batch-based finite difference estimators. The Cor-CFD algorithm uses R pilot perturbations to estimate optimal perturbation parameters through bootstrap and regression techniques, then generates correlated samples by adjusting their location and scale based on estimated expectations and standard deviations. This approach reduces variance and bias compared to classical two-sample estimators. The study tests these methods on both low-dimensional (1D functions like x⁴ and -100cos(πx/100)) and high-dimensional (d=64) optimization problems, using stochastic Armijo line search and adaptive batch size strategies.

## Key Results
- Cor-CFD-based gradient descent algorithms demonstrate better performance than KW and SPSA in tested scenarios
- Batch-based finite difference estimators enable the use of constant step sizes, avoiding the slow convergence of diminishing step size approaches
- Cor-CFD's sample recycling strategy provides computational efficiency while maintaining accuracy through correlation exploitation

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Cor-CFD achieves better performance by using batch-based finite difference estimators to obtain more accurate gradient estimates
- Mechanism: Cor-CFD uses R pilot perturbations to estimate optimal perturbation parameter through bootstrap and regression, then generates correlated samples by adjusting location and scale based on estimated expectations and standard deviations
- Core assumption: Correlation structure among samples can be exploited to reduce estimation error while maintaining computational feasibility
- Evidence anchors: Abstract states "when an efficient batch-based FD estimator is applied, its corresponding gradient descent algorithm generally shows better performance compared to classical KW and SPSA algorithms"

### Mechanism 2
- Claim: Batch-based gradient estimation enables use of constant step sizes, avoiding slow convergence
- Mechanism: Multiple samples provide accurate gradient estimates, allowing trust in gradient direction for larger, constant step sizes rather than diminishing ones
- Core assumption: More accurate gradient estimates allow for stable optimization with larger step sizes without compromising convergence
- Evidence anchors: Abstract states "batch-based finite difference estimators use more samples to achieve more accurate gradient estimates, enabling the use of constant step sizes"

### Mechanism 3
- Claim: Cor-CFD's sample recycling strategy provides computational efficiency while maintaining accuracy
- Mechanism: After estimating optimal perturbation using pilot samples, Cor-CFD recycles samples by adjusting location and scale based on estimated expectation and standard deviation
- Core assumption: Sample recycling through appropriate transformations preserves statistical properties needed for accurate gradient estimation
- Evidence anchors: Section 2.3 describes "recycle them by adjusting their location and scale based on the expectation and standard deviation"

## Foundational Learning

- Concept: Finite difference approximation for gradient estimation
  - Why needed here: The paper relies on understanding how finite difference methods approximate gradients when closed-form derivatives are unavailable, particularly the trade-off between sample efficiency and estimation accuracy
  - Quick check question: What is the fundamental trade-off between using two samples per dimension versus using batch samples for gradient estimation in finite difference methods?

- Concept: Stochastic approximation and convergence theory
  - Why needed here: The paper discusses different convergence requirements for algorithms using diminishing versus constant step sizes, requiring understanding of stochastic optimization foundations
  - Quick check question: Why do KW and SPSA algorithms require diminishing step sizes while batch-based methods can use constant step sizes?

- Concept: Bootstrap and regression techniques for parameter estimation
  - Why needed here: Cor-CFD uses bootstrap and least-squares regression to estimate unknown constants in optimal perturbation parameter, central to its methodology
  - Quick check question: How does the bootstrap technique help in estimating the optimal perturbation parameter when it depends on unknown constants like Var[ε(xk)] and µ(3)(xk)?

## Architecture Onboarding

- Component map: Sample generation -> Pilot perturbation estimation -> Optimal perturbation calculation -> Gradient estimation -> Step size selection -> Parameter update -> Repeat

- Critical path: Sample generation → Pilot perturbation estimation → Optimal perturbation calculation → Gradient estimation → Step size selection → Parameter update → Repeat

- Design tradeoffs:
  - Sample efficiency vs accuracy: More samples per iteration improves gradient estimates but reduces iterations possible
  - Computational overhead vs estimation quality: Bootstrap and regression steps add computation but significantly improve gradient estimation
  - Batch size growth: Gradually increasing batch size helps maintain accuracy as optimization progresses but requires careful tuning

- Failure signatures:
  - Slow convergence despite accurate gradients: Indicates step size selection issues or poor batch size scaling
  - Oscillatory behavior: Suggests gradient estimation instability or inappropriate perturbation parameters
  - High variance in results across runs: Points to insufficient sample sizes or poor bootstrap estimation

- First 3 experiments:
  1. Implement Cor-CFD with synthetic 1D function (e.g., µ(x) = x⁴) and compare convergence against KW algorithm with varying noise levels
  2. Test Cor-CFD on the Schittkowski function (3.1) with d=64 dimensions, comparing against optimized SPSA parameters
  3. Vary batch size growth strategy and measure impact on convergence speed and stability across different noise regimes

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal dynamic strategy for adjusting batch size nk based on optimization progress and noise level?
- Basis in paper: [explicit] The paper mentions "the setting is not optimal because nk depends on the true gradient ∇µ(xk) and noise level Var[ϵ(xk)]" and suggests this as future work
- Why unresolved: The paper uses a fixed formula nk = ⌊(n0 + k)/R⌋ × R which doesn't adapt to changing conditions during optimization
- What evidence would resolve it: Experimental comparison of different adaptive batch size strategies showing improved convergence rates compared to current static approach

### Open Question 2
- Question: How do Cor-CFD-based methods perform on non-convex optimization problems with multiple local minima?
- Basis in paper: [inferred] The paper only tests on convex or single-minimum problems and doesn't explore complex landscapes
- Why unresolved: All test functions used have relatively simple optimization landscapes, leaving open how the method performs in more challenging scenarios
- What evidence would resolve it: Experimental results comparing Cor-CFD to other methods on benchmark non-convex problems with multiple local minima

### Open Question 3
- Question: What are the theoretical convergence rates of Cor-CFD-based optimization algorithms?
- Basis in paper: [explicit] The paper states "The theory presented in Hu and Fu (2024) may offer some valuable guidance for this direction" suggesting this is currently lacking
- Why unresolved: While the paper demonstrates empirical performance, it doesn't provide theoretical guarantees on convergence rates
- What evidence would resolve it: Mathematical proofs establishing convergence rates under various noise conditions and problem structures

## Limitations

- Experimental comparisons are limited to specific test functions and noise regimes, raising questions about generalizability to broader problem classes
- The Cor-CFD algorithm's computational overhead from bootstrap sampling and regression steps is not thoroughly quantified relative to its performance gains
- Theoretical convergence guarantees for batch-based finite difference estimators under non-convex landscapes remain underdeveloped

## Confidence

- **High confidence**: The fundamental mechanism that batch-based finite difference estimators provide more accurate gradient estimates than two-sample estimators
- **Medium confidence**: The claim that Cor-CFD's sample recycling strategy provides significant computational efficiency while maintaining accuracy
- **Low confidence**: The assertion that the performance advantages of Cor-CFD over KW and SPSA are consistent across all tested scenarios without exception

## Next Checks

1. Replicate experiments with additional test functions: Test Cor-CFD on a broader set of benchmark problems including non-smooth functions and functions with multiple local optima to verify robustness claims.

2. Measure computational overhead: Quantify the runtime overhead of Cor-CFD's bootstrap and regression steps relative to the improved convergence speed to assess true computational efficiency.

3. Sensitivity analysis of batch size scaling: Systematically vary the batch size growth rate and measure its impact on convergence stability and final solution quality across different noise regimes.