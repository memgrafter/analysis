---
ver: rpa2
title: 'LoTa-Bench: Benchmarking Language-oriented Task Planners for Embodied Agents'
arxiv_id: '2402.08178'
source_url: https://arxiv.org/abs/2402.08178
tags:
- task
- find
- table
- alfred
- examples
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces LoTa-Bench, a benchmark system for evaluating
  language-oriented task planners for embodied agents. It addresses the challenge
  of comparing planner performance due to the lack of standardized evaluation frameworks.
---

# LoTa-Bench: Benchmarking Language-oriented Task Planners for Embodied Agents

## Quick Facts
- arXiv ID: 2402.08178
- Source URL: https://arxiv.org/abs/2402.08178
- Reference count: 33
- Primary result: Introduces LoTa-Bench, a benchmark system for evaluating LLM-based language-oriented task planners for embodied agents

## Executive Summary
This paper presents LoTa-Bench, a novel benchmark system designed to evaluate language-oriented task planners for embodied agents. The core challenge addressed is the lack of standardized evaluation frameworks for comparing planner performance. The proposed solution leverages large language models (LLMs) to generate step-by-step plans for home-service tasks, with performance measured by task success rates. Experiments on two datasets (ALFRED and W AH-NL) demonstrate that larger LLMs like GPT-3 achieve higher success rates (up to 21.36% on ALFRED), and enhancements like in-context example selection and replanning show potential for improved performance. LoTa-Bench aims to accelerate the development of LLM-based task planners by providing a reproducible evaluation framework.

## Method Summary
The benchmark uses LLM-based task planners that generate step-by-step plans through in-context learning. Planners construct prompts combining a prefix, in-context examples, and natural language instructions, then use the LLM to generate skills sequentially until completion. The method employs two datasets (ALFRED with AI2-THOR simulator, and W AH-NL with VirtualHome simulator) and evaluates performance using task success rates and subgoal success rates. The approach tests various LLM models of different sizes and explores enhancements like semantic similarity-based example selection and replanning strategies.

## Key Results
- Larger LLM models generally achieve higher task success rates (GPT-3 175B: 21.36% success on ALFRED)
- Semantic similarity sampling for in-context examples significantly improves performance (LLaMA 1 65B: 43.25% subgoal success rate on W AH-NL)
- Task success rates increase with model size, though with some exceptions (GPT-J 6B outperformed GPT-NeoX 20B)
- The benchmark framework successfully enables reproducible evaluation of LLM-based task planners

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The LLM-based task planner achieves reasonable performance because it leverages the model's ability to generate step-by-step plans via in-context learning, using a prompt structure that combines a prefix, examples, and the task instruction.
- Mechanism: The planner constructs a prompt with a fixed prefix describing the robot's role, multiple in-context examples pairing instructions with skill sequences, and the new instruction. The LLM generates the next skill by scoring all possible skills, selecting the highest probability one, and appending it to the prompt for the next step. This autoregressive process continues until the "done" skill is chosen or a maximum length is reached.
- Core assumption: The LLM's in-context learning is sufficiently effective to generalize from the provided examples to unseen instructions without requiring additional training.
- Evidence anchors:
  - [abstract] "The core method involves using large language models (LLMs) to generate step-by-step plans for home-service tasks, with performance measured by task success rates."
  - [section 3] "Our baseline task planner leverages the in-context learning capabilities of large language models (LLMs)..."
  - [corpus] Weak evidence; no direct neighbor studies cited.
- Break condition: If the in-context examples are not representative or the prompt length exceeds model limits, performance degrades significantly.

### Mechanism 2
- Claim: Larger LLM models generally achieve higher task success rates because they have more parameters to encode procedural knowledge and better generalization from examples.
- Mechanism: The experiments show a positive correlation between model size and success rate (e.g., GPT-3 175B achieving 21.36% success on ALFRED vs. smaller models). Larger models can better understand complex instructions and plan longer, more accurate sequences.
- Core assumption: Model size directly correlates with the ability to understand and execute complex task planning.
- Evidence anchors:
  - [section 5.2] "Overall, task success rates increased with the size of the language model..."
  - [section 5.2] "GPT-3 (text-davinci-003) showed the best success rate of 21.36% on ALFRED..."
  - [corpus] Weak evidence; no direct neighbor studies cited.
- Break condition: If the task complexity exceeds the model's reasoning capacity, or if the examples are not sufficiently diverse, increasing size may not help.

### Mechanism 3
- Claim: Semantic similarity-based in-context example selection improves performance by providing more relevant planning examples to the LLM.
- Mechanism: Instead of random sampling, examples are selected based on similarity scores between the input instruction and the instruction in the training set, computed using Sentence BERT. This strategy selects examples that are most contextually relevant, improving the LLM's ability to generate accurate plans.
- Core assumption: The similarity between the input instruction and training examples is a good proxy for the relevance of the planning example.
- Evidence anchors:
  - [section 6.1] "Semantic Similarity Sampling... aims to select the most relevant planning examples..."
  - [section 6.1] "Semantic Similarity Sampling led to significant performance gains; notably, the LLaMA 1 65B model achieved a subgoal success rate of 43.25%..."
  - [corpus] Weak evidence; no direct neighbor studies cited.
- Break condition: If the similarity metric does not capture the true relevance of the examples, or if the training set lacks diversity, this method may not improve performance.

## Foundational Learning

- Concept: In-context learning
  - Why needed here: The entire task planning approach relies on the LLM's ability to understand and execute tasks based solely on the prompt, without additional training.
  - Quick check question: What is the difference between in-context learning and fine-tuning, and why is in-context learning chosen here?

- Concept: Prompt engineering
  - Why needed here: The performance of the LLM-based planner is highly dependent on how the prompt is structured, including the prefix, examples, and instruction.
  - Quick check question: What are the key components of the prompt used in this benchmark, and how do they contribute to the planner's performance?

- Concept: Embodied AI and task planning
  - Why needed here: Understanding the context of the problem domain is crucial for evaluating the planner's performance and limitations.
  - Quick check question: What are the key challenges in embodied task planning, and how does this benchmark address them?

## Architecture Onboarding

- Component map:
  - LLM-based Task Planner -> Simulator (AI2-THOR or VirtualHome) -> Evaluation Metrics (Task Success Rates, Subgoal Success Rates)

- Critical path:
  1. Construct prompt with prefix, examples, and instruction
  2. Use LLM to generate next skill
  3. Execute skill in simulator
  4. Update prompt with executed skill
  5. Repeat until task completion or maximum steps
  6. Evaluate performance against goal conditions

- Design tradeoffs:
  - Model size vs. inference speed: Larger models achieve better performance but are slower
  - Number of examples vs. prompt length: More examples can improve performance but may exceed model limits
  - Random vs. semantic example selection: Semantic selection improves performance but requires additional computation

- Failure signatures:
  - Action planning failures: LLM generates incorrect or irrelevant skills
  - Object selection failures: LLM selects the wrong object for an action
  - Absence of visual grounding: LLM fails to account for object visibility or location
  - Lack of physical understanding: LLM generates plans that violate physical constraints

- First 3 experiments:
  1. Baseline evaluation with different LLM models and sizes to establish performance benchmarks
  2. Impact of the number of in-context examples on performance to optimize prompt construction
  3. Semantic similarity-based example selection to improve the relevance of in-context examples

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of LLM-based task planners scale with model size, and are there diminishing returns for very large models?
- Basis in paper: [explicit] The paper observes that task success rates generally increase with model size but notes exceptions (e.g., GPT-J 6B outperforming GPT-NeoX 20B).
- Why unresolved: The experiments tested a limited range of model sizes, and the relationship between model size and performance is not fully characterized, especially for extremely large models.
- What evidence would resolve it: A comprehensive study testing a wider range of model sizes, including very large models, to establish the scaling relationship and identify any points of diminishing returns.

### Open Question 2
- Question: How effective are different in-context example selection strategies, and can they be further optimized?
- Basis in paper: [explicit] The paper explores three strategies (Random Sampling, Task-Specific Sampling, and Semantic Similarity Sampling) and finds that Semantic Similarity Sampling performs best.
- Why unresolved: The experiments only test a limited number of strategies and do not explore more sophisticated methods for example selection or consider the impact of example order within the prompt.
- What evidence would resolve it: A thorough investigation of various in-context example selection strategies, including those that consider example order and semantic relationships, to identify the most effective approach.

### Open Question 3
- Question: How does the ability to handle ambiguous or incomplete user instructions impact the performance of LLM-based task planners?
- Basis in paper: [explicit] The paper mentions that ambiguous or incorrect user instructions contribute to planning failures.
- Why unresolved: The experiments use well-defined instructions and do not explicitly test the planner's ability to handle ambiguity or request clarification.
- What evidence would resolve it: Experiments that systematically introduce ambiguity or incompleteness into user instructions and evaluate the planner's ability to resolve or request clarification.

## Limitations
- The LLM-based planner lacks visual grounding and physical understanding, leading to potential planning errors when objects are not visible or reachable
- Performance metrics may not fully capture the planner's ability to handle real-world complexity and uncertainty
- The evaluation is constrained by specific simulator environments and may not generalize to more diverse or dynamic settings

## Confidence
- High confidence: The core methodology of using LLM-based in-context learning for task planning is well-established and the experimental setup is clearly defined
- Medium confidence: The performance improvements from larger models and semantic similarity-based example selection are demonstrated, but the study does not explore alternative prompt engineering strategies or fine-tuning approaches that could yield better results
- Low confidence: The study does not address the scalability of the approach to more complex tasks or larger environments, and the impact of the planner's limitations on real-world deployment is not fully explored

## Next Checks
1. Conduct ablation studies to quantify the impact of visual grounding and physical reasoning modules on planner performance
2. Evaluate the planner's robustness to environmental changes and task variations by testing on out-of-distribution scenarios
3. Investigate the scalability of the approach by benchmarking on more complex tasks and larger environments to assess generalization capabilities