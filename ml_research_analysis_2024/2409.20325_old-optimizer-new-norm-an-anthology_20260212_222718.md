---
ver: rpa2
title: 'Old Optimizer, New Norm: An Anthology'
arxiv_id: '2409.20325'
source_url: https://arxiv.org/abs/2409.20325
tags:
- norm
- descent
- page
- steepest
- step
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper shows that popular deep learning optimizers like Adam,
  Shampoo, and Prodigy can be understood as variants of steepest descent under specific
  norms. By disabling their exponential moving averages, each optimizer is shown to
  solve a steepest descent problem under a particular norm: Adam under the max-of-max
  norm, Shampoo under the spectral norm, and Prodigy as sign descent with automatic
  step size adjustment.'
---

# Old Optimizer, New Norm: An Anthology

## Quick Facts
- arXiv ID: 2409.20325
- Source URL: https://arxiv.org/abs/2409.20325
- Reference count: 35
- This paper shows that popular deep learning optimizers like Adam, Shampoo, and Prodigy can be understood as variants of steepest descent under specific norms.

## Executive Summary
This paper provides a unified theoretical framework for understanding popular deep learning optimizers by showing they are equivalent to steepest descent under specific operator norms when their exponential moving averages are disabled. The authors demonstrate that Adam performs sign gradient descent under the max-of-max norm, Shampoo solves a spectral norm problem, and Prodigy implements sign descent with automatic step size adjustment. This insight opens a new design space where different operator norms can be assigned to different layers based on their role in the network, potentially improving training stability and speed.

## Method Summary
The paper analyzes three popular optimizers (Adam, Shampoo, Prodigy) by theoretically proving they are equivalent to steepest descent under specific norms when their exponential moving averages are disabled. The authors systematically derive these equivalences by examining the update rules of each optimizer and showing how they reduce to steepest descent problems under particular induced operator norms. They then propose a modular norm framework where different layers can be assigned different norms based on their functional role in the network, creating a new design space for optimizer development.

## Key Results
- Adam without EMA reduces to sign gradient descent under the max-of-max (ℓ1→ℓ∞) norm
- Shampoo without accumulation projects gradients to semi-orthogonal matrices, equivalent to steepest descent under the spectral norm
- Prodigy without EMA implements sign descent with dynamic step size adjustment based on "escape velocity"
- The modular norm framework allows different layers to use different operator norms based on their network role

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Disabling EMA transforms Adam into sign gradient descent under the max-of-max norm
- Mechanism: The Adam update rule without EMA reduces to dividing the gradient by its squared magnitude, which equals sign descent. This sign descent is equivalent to steepest descent under the infinity norm on the flattened weight vector, which coincidentally equals the max-of-max norm across layers.
- Core assumption: The exponential moving average components can be set to zero without affecting the underlying optimization behavior.
- Evidence anchors:
  - [abstract] "after switching off exponential moving averages, each method is equivalent to steepest descent under a particular norm"
  - [section] "If we switch off EMA by setting β1=β2=0, the Adam updates reduce to just sign gradient descent"
  - [corpus] No direct corpus evidence found
- Break condition: If the layer-wise structure of the network significantly deviates from the assumed matrix organization, or if EMA components are essential for numerical stability rather than just smoothing.

### Mechanism 2
- Claim: Shampoo without accumulation is equivalent to steepest descent under the maximum spectral norm
- Mechanism: The Shampoo update without accumulation projects the gradient matrix to the closest semi-orthogonal matrix in Frobenius norm, which is equivalent to steepest descent under the spectral norm when aggregated across layers.
- Core assumption: The accumulation of gradient products (L and R matrices) can be disabled without affecting the core optimization principle.
- Evidence anchors:
  - [abstract] "after switching off exponential moving averages, each method is equivalent to steepest descent under a particular norm"
  - [section] "Shampoo without accumulation projects the gradient matrix to the closest semi-orthogonal matrix in Frobenius norm"
  - [corpus] No direct corpus evidence found
- Break condition: If the spectral norm does not capture the geometry of the loss surface for the specific network architecture, or if the projection to semi-orthogonal matrices becomes numerically unstable.

### Mechanism 3
- Claim: Prodigy without EMA implements sign descent with automatic step size adjustment based on "escape velocity"
- Mechanism: Prodigy uses a dynamic step size rule that increases multiplicatively until the weights escape the linearization of the loss around the initial weights, at which point it stops increasing.
- Core assumption: The step size adjustment mechanism based on the angle between gradient and weight difference effectively approximates the optimal escape velocity.
- Evidence anchors:
  - [abstract] "after switching off exponential moving averages, each method is equivalent to steepest descent under a particular norm"
  - [section] "Prodigy without EMA is sign descent—an example of steepest descent—with a particular mechanism for warming up the step size"
  - [corpus] No direct corpus evidence found
- Break condition: If the assumptions about dense gradients and small weight changes relative to initialization break down, or if the multiplicative increase in step size overshoots the optimal value.

## Foundational Learning

- Concept: Induced operator norms
  - Why needed here: The paper's central insight is that different optimizers implicitly use different induced operator norms on weight matrices, and understanding this allows for more principled optimizer design.
  - Quick check question: Given a matrix M and norms ||·||_a and ||·||_b, how would you compute the induced operator norm ||M||_a→b?

- Concept: Steepest descent under different norms
  - Why needed here: The paper reframes multiple optimizers as variants of steepest descent under specific norms, providing a unified framework for understanding and designing optimization algorithms.
  - Quick check question: What is the solution to the steepest descent problem arg min_∆w g^T∆w + λ/2||∆w||² under the ℓ∞ norm?

- Concept: Modular norm and its properties
  - Why needed here: The modular norm generalizes the specific norms used by Adam and Shampoo, providing a design space for creating new optimizers by assigning different norms to different layers.
  - Quick check question: How does the modular norm combine individual layer norms, and what is its corresponding steepest descent update rule?

## Architecture Onboarding

- Component map: Norm selection per layer -> Aggregation of norms across layers -> Step size computation -> Parameter update mechanism
- Critical path: The most critical path is the norm selection and aggregation step, as this determines the geometry of the optimization problem. Incorrect norm choices can lead to poor convergence or even divergence, regardless of the step size selection.
- Design tradeoffs: Choosing norms that are computationally tractable (like ℓ1→ℓp and ℓp→ℓ8 norms) versus norms that better capture the geometry of the problem; using simple aggregation methods (like max) versus more complex ones that might better preserve information; balancing the need for theoretical guarantees with practical considerations like memory usage.
- Failure signatures: Poor convergence or divergence; sensitivity to learning rate despite theoretical guarantees; unexpected behavior when scaling to larger models; performance degradation when changing network architecture or dataset characteristics.
- First 3 experiments:
  1. Implement steepest descent under different induced operator norms (spectral, ℓ1→ℓ2, ℓ2→ℓ∞) on a simple linear regression problem and compare convergence rates.
  2. Create a modular norm optimizer that assigns different norms to linear and embedding layers, and test it on a small NLP task.
  3. Implement the Shampoo update without accumulation using the Newton-Schulz iteration method described in the appendix, and compare its performance to the full Shampoo optimizer.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the max-of-max norm (ℓ1→ℓ8 operator norm) have any special relationship to neural network training beyond being coincidentally equivalent to Adam's behavior?
- Basis in paper: [explicit] The authors note that "taking the weight space to be Rn equipped with the simple infinity norm seems to 'throw away' the fact that the weight space of a neural network is built in a structured way out of layers of matrices" and ask "Why does the ℓ1 to ℓ8 induced operator norm rear its head? What does it have to do with deep learning?"
- Why unresolved: The paper shows the equivalence between Adam and sign descent under the max-of-max norm but doesn't explain why this particular norm would be beneficial for neural network training specifically.
- What evidence would resolve it: Empirical studies comparing different operator norms across various network architectures and tasks, or theoretical analysis connecting the ℓ1→ℓ8 norm to specific properties of neural network optimization landscapes.

### Open Question 2
- Question: How should practitioners actually choose which operator norms to assign to different layers in practice?
- Basis in paper: [explicit] The authors propose that "one should consider the role that layer plays in the neural network" and suggest different norms for linear vs embedding layers, but acknowledge "it's natural to ask how one should assign norms to layers" and "there are so many norms to choose from!"
- Why unresolved: While the paper provides some guidance (e.g., RMS to RMS for linear layers, ℓ1 to RMS for embeddings), it doesn't provide a systematic methodology for norm selection or empirical validation of these choices.
- What evidence would resolve it: Systematic ablation studies testing different norm assignments across diverse architectures, or development of principled criteria for norm selection based on layer properties.

### Open Question 3
- Question: What is the precise role of exponential moving averages (EMA) in these optimizers, and can we design better smoothing mechanisms?
- Basis in paper: [explicit] The authors state that "nailing down the precise role of EMA is perhaps still an open problem" and suggest EMA "can then be thought of as 'smoothing out' the algorithm, or making it more robust to mini-batch noise."
- Why unresolved: While the paper shows that removing EMA reveals the underlying steepest descent structure, it doesn't characterize what EMA contributes beyond this smoothing effect or explore alternative smoothing mechanisms.
- What evidence would resolve it: Rigorous analysis of EMA's impact on convergence rates, robustness to noise, and generalization; comparison with alternative smoothing techniques like momentum or adaptive step size schedules.

### Open Question 4
- Question: Can the modular norm framework be extended to handle non-linear layers and complex architectures like transformers?
- Basis in paper: [inferred] The paper focuses on linear and embedding layers, which have straightforward operator norm interpretations, but modern architectures contain non-linear operations (activations, attention mechanisms) that don't fit neatly into the matrix norm framework.
- Why unresolved: The authors demonstrate the modular norm for linear layers but don't address how to handle non-linearities or more complex architectural patterns that dominate modern deep learning.
- What evidence would resolve it: Extension of the modular norm framework to handle non-linear layers, or empirical studies showing improved performance when applying different norms to various components of complex architectures.

## Limitations
- The analysis assumes EMA components are primarily for smoothing and can be disabled without affecting optimization behavior
- Practical implementations involve approximations (particularly for Shampoo's matrix operations) that may deviate from ideal behavior
- Limited empirical validation of specific norm assignment recommendations across diverse architectures

## Confidence
- **High**: Mathematical equivalence of disabled-EMA optimizers to steepest descent under specific norms
- **Medium**: The general principle that norm selection affects optimization geometry
- **Low**: Specific recommendations for norm assignment across layers without empirical validation

## Next Checks
1. Implement each optimizer with disabled EMA and measure training stability compared to original versions, monitoring for numerical issues.
2. Create a modular norm optimizer that systematically tests different norm assignments across multiple architectures and tasks to identify performance patterns.
3. Compare convergence rates of disabled-EMA optimizers against their original versions and standard SGD on simple convex problems where theoretical predictions can be validated.