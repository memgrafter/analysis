---
ver: rpa2
title: Testing Uncertainty of Large Language Models for Physics Knowledge and Reasoning
arxiv_id: '2411.14465'
source_url: https://arxiv.org/abs/2411.14465
tags:
- answer
- responses
- question
- rate
- error
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper examines uncertainty and accuracy trade-offs in large
  language models (LLMs) when answering physics questions. The authors evaluate four
  LLMs - Llama3.1-8B, Mixtral-8x7B, Mistral-7B, and GPT-3.5-turbo - on a dataset of
  823 physics multiple-choice questions across five difficulty levels.
---

# Testing Uncertainty of Large Language Models for Physics Knowledge and Reasoning

## Quick Facts
- arXiv ID: 2411.14465
- Source URL: https://arxiv.org/abs/2411.14465
- Reference count: 36
- Primary result: Most LLMs show a bell-shaped accuracy-uncertainty relationship, with higher accuracy correlating with lower uncertainty, though this pattern breaks for complex reasoning questions.

## Executive Summary
This paper investigates the relationship between uncertainty and accuracy in large language models (LLMs) when answering physics multiple-choice questions. The authors evaluate four LLMs - Llama3.1-8B, Mixtral-8x7B, Mistral-7B, and GPT-3.5-turbo - on a dataset of 823 physics questions across five difficulty levels. By prompting each model 20 times per question and measuring response entropy, they identify patterns of reliable versus hallucinated responses. The study reveals that while most models exhibit a bell-shaped distribution between accuracy and uncertainty, this relationship is not universal and becomes more asymmetric for complex reasoning questions. GPT-3.5-turbo shows the highest response diversity while Mixtral-8x7B provides the most consistent answers.

## Method Summary
The study uses the mlphys101 dataset containing 823 university-level physics multiple-choice questions with five answer options each. Questions are categorized into five complexity levels from simple definitions to multi-step reasoning. Four LLMs are tested using three-shot prompting with temperature=0.7. Each model is prompted 20 times per question, and Shannon entropy is calculated from the response distributions to measure uncertainty. Accuracy is determined by comparing responses to ground truth answers. The accuracy-uncertainty relationship is analyzed through 2D histograms, and response consistency is evaluated across models.

## Key Results
- Most models exhibit a bell-shaped accuracy-uncertainty distribution, with higher accuracy correlating with lower uncertainty
- GPT-3.5-turbo shows the highest response diversity while Mixtral-8x7B provides the most consistent answers
- The accuracy-uncertainty trade-off becomes more asymmetric for complex reasoning questions compared to knowledge retrieval tasks
- Larger models generally provide more consistent answers but still exhibit concerning rates of high-certainty hallucinations

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLM accuracy correlates with response entropy following a bell-shaped distribution, where higher accuracy corresponds to lower uncertainty for most questions.
- Mechanism: When LLMs are confident in their answers, they tend to generate consistent responses across multiple trials, resulting in low entropy. As uncertainty increases, the distribution of responses becomes more varied, increasing entropy. However, at very high uncertainty, models may randomly select answers, which can sometimes accidentally be correct, creating the bell-shaped pattern.
- Core assumption: The entropy measure accurately captures the model's internal uncertainty about its responses.
- Evidence anchors:
  - [abstract]: "most models provide accurate replies in cases where they are certain, but this is by far not a general behavior. The relationship between accuracy and uncertainty exposes a broad horizontal bell-shaped distribution"
  - [section 3]: "The 2D histograms of '1-Accuracy' (or Error Rate) versus Entropy for all models exhibit a similar bell-shaped distribution"
- Break condition: The bell-shaped relationship breaks down for questions requiring complex reasoning, where the relationship becomes asymmetric with high certainty for incorrect answers.

### Mechanism 2
- Claim: Model size and architecture influence the consistency of responses, with larger models generally showing lower entropy distributions.
- Mechanism: Larger models have more parameters and training data, allowing them to develop more consistent internal representations for specific questions. This leads to more stable output patterns across multiple trials, resulting in lower entropy scores.
- Core assumption: Model parameter count and architecture directly impact response consistency.
- Evidence anchors:
  - [abstract]: "GPT-3.5-turbo shows the highest response diversity, while Mixtral-8x7B provides the most consistent answers"
  - [section 3]: "Mixtral-8x7B-Instruct-v0.1 (being the largest model in use for our analysis) demonstrates a majority of entries in the lowest entropy bin"
- Break condition: If the training data for larger models contains conflicting information about physics concepts, this could actually increase entropy rather than decrease it.

### Mechanism 3
- Claim: Question complexity level affects the accuracy-uncertainty trade-off, with reasoning questions showing more asymmetric relationships than knowledge retrieval tasks.
- Mechanism: Simple knowledge retrieval questions have clear, unambiguous answers that models can confidently retrieve from their training data. Complex reasoning questions require multi-step logic and synthesis, which models handle less reliably, leading to high-certainty hallucinations.
- Core assumption: Question complexity can be reliably categorized and affects model reasoning differently.
- Evidence anchors:
  - [abstract]: "The asymmetry between accuracy and uncertainty intensifies as the questions demand more logical reasoning of the LLM agent, while the same relationship remains sharp for knowledge retrieval tasks"
  - [section 2.1]: Questions classified into five categories from simple definitions to multi-step reasoning
- Break condition: If the categorization of question complexity is inaccurate or if models have been specifically trained on reasoning tasks, this relationship may not hold.

## Foundational Learning

- Concept: Entropy as a measure of uncertainty in discrete probability distributions
  - Why needed here: The paper uses Shannon entropy to quantify the diversity of model responses across multiple trials, which is central to understanding the accuracy-uncertainty relationship
  - Quick check question: If a model answers a question with the same answer 20 times out of 20 trials, what is the entropy of its responses?

- Concept: Bell-shaped (normal) distribution properties
  - Why needed here: The accuracy-uncertainty relationship follows a bell-shaped curve, requiring understanding of how such distributions behave and what they indicate
  - Quick check question: In a bell-shaped accuracy-uncertainty curve, where would you expect to find questions where the model is both highly accurate and highly certain?

- Concept: Few-shot prompting and temperature settings in LLMs
  - Why needed here: The study uses three-shot prompting with temperature=0.7, which affects response diversity and must be understood to interpret the entropy measurements
  - Quick check question: How would increasing the temperature parameter affect the entropy of model responses?

## Architecture Onboarding

- Component map: Dataset loading -> LLM API integration -> Few-shot prompting system -> Response collection loop (20 trials) -> Entropy calculation -> Accuracy calculation -> Visualization/Analysis
- Critical path: For each question, prompt all models 20 times, calculate entropy for each model's responses, compare against ground truth accuracy, and aggregate results for analysis
- Design tradeoffs: Using 20 trials per question balances computational cost against statistical reliability. Three-shot prompting was chosen after testing zero-shot, one-shot, and two-shot approaches, representing a tradeoff between prompt engineering effort and model performance
- Failure signatures: High entropy with low accuracy indicates model uncertainty; low entropy with low accuracy indicates high-certainty hallucinations; inconsistent results across different prompting styles suggest sensitivity to prompt engineering
- First 3 experiments:
  1. Replicate the entropy calculation for a single question across all four models to verify the consistency measurement
  2. Test the bell-shaped relationship on a small subset of questions by manually calculating accuracy-entropy pairs
  3. Verify the few-shot prompting approach by comparing zero-shot, one-shot, two-shot, and three-shot performance on a few questions

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do different prompting strategies affect the accuracy-uncertainty trade-off in LLMs for physics reasoning tasks?
- Basis in paper: [inferred] The paper notes that "response accuracy is highly dependent on the prompting context and style used" and suggests that the high diversity observed in GPT-3.5-turbo might be due to prompting issues, but this requires validation through additional testing and comparisons to other datasets.
- Why unresolved: The study used a fixed few-shot prompting approach and observed significant differences in model performance, particularly for GPT-3.5-turbo, but did not systematically explore how different prompting strategies might affect these results.
- What evidence would resolve it: Systematic testing of multiple prompting strategies (zero-shot, few-shot with varying shot counts, chain-of-thought prompting, etc.) across all models on the same physics dataset, comparing the resulting accuracy-uncertainty distributions.

### Open Question 2
- Question: What are the underlying mechanisms causing the bell-shaped distribution between accuracy and uncertainty in LLM responses?
- Basis in paper: [explicit] The authors note "a bell-shaped distribution between accuracy and uncertainty" but provide only theoretical explanations for specific cases (2-5 response types) without explaining why this general pattern emerges across different models and question complexities.
- Why unresolved: While the paper provides mathematical explanations for specific response patterns (2-5 distinct responses), it doesn't explain the fundamental reason for the observed bell-shaped relationship across all scenarios and models.
- What evidence would resolve it: Analysis of model internal representations and attention patterns during question answering, combined with theoretical work on how probability distributions over responses relate to model confidence and accuracy.

### Open Question 3
- Question: How does model size specifically influence the reliability and hallucination patterns in physics reasoning tasks?
- Basis in paper: [explicit] The authors observe that "Mixtral-8x7B-Instruct-v0.1 (being the largest model in use for our analysis) demonstrates a majority of entries in the lowest entropy bin" and provides "answers with minimal (if not vanishing) diversity," while GPT-3.5-turbo shows the highest diversity.
- Why unresolved: The paper identifies size-related differences but doesn't isolate whether these are due to model architecture, training data differences, or other factors, nor does it establish whether larger models are consistently more reliable across all physics question types.
- What evidence would resolve it: Comparative analysis of multiple models across a broader range of sizes (including smaller and larger models than tested) on standardized physics benchmarks, controlling for training data and architecture differences.

## Limitations

- The study uses only multiple-choice questions with five options, limiting generalizability to open-ended physics problems or other question formats
- The entropy calculation treats all response variations equally without considering semantic similarity between answers, potentially overestimating uncertainty for semantically equivalent responses
- The accuracy-uncertainty relationship is analyzed through aggregated 2D histograms, which may mask question-specific variations in the relationship

## Confidence

**High confidence**: The finding that GPT-3.5-turbo shows higher response diversity than smaller models is well-supported by the entropy measurements and aligns with the expected relationship between model size and response consistency.

**Medium confidence**: The bell-shaped accuracy-uncertainty relationship is supported by the aggregated 2D histograms, but the paper acknowledges that this pattern is not universal and breaks down for complex reasoning questions.

**Low confidence**: The asymmetry intensification for reasoning questions is based on qualitative observations from the histograms rather than quantitative analysis of the reasoning vs. knowledge retrieval question subsets.

## Next Checks

1. **Question-level accuracy-uncertainty analysis**: Instead of aggregating across all questions, analyze the accuracy-uncertainty relationship separately for each of the five complexity levels to verify whether the asymmetry pattern holds when controlling for question complexity.

2. **Semantic entropy refinement**: Implement a semantic similarity measure (e.g., cosine similarity on embeddings) to group semantically equivalent responses before calculating entropy, which would provide a more accurate measure of true uncertainty versus surface-level variation.

3. **Prompt sensitivity testing**: Systematically vary the few-shot examples, prompt format, and temperature parameter across a subset of questions to quantify how sensitive the accuracy-uncertainty relationship is to prompt engineering choices.