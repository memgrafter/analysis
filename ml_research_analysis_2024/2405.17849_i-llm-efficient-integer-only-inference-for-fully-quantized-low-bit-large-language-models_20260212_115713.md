---
ver: rpa2
title: 'I-LLM: Efficient Integer-Only Inference for Fully-Quantized Low-Bit Large
  Language Models'
arxiv_id: '2405.17849'
source_url: https://arxiv.org/abs/2405.17849
tags:
- quantization
- integer-only
- llms
- arxiv
- non-linear
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of efficient integer-only inference
  for large language models (LLMs) by proposing I-LLM, a novel post-training quantization
  (PTQ) framework. The primary obstacle to integer-only quantization for LLMs is the
  large fluctuation of activations across channels and tokens in both linear and non-linear
  operations.
---

# I-LLM: Efficient Integer-Only Inference for Fully-Quantized Low-Bit Large Language Models

## Quick Facts
- arXiv ID: 2405.17849
- Source URL: https://arxiv.org/abs/2405.17849
- Authors: Xing Hu; Yuan Cheng; Dawei Yang; Zhihang Yuan; Jiangyong Yu; Chen Xu; Sifan Zhou
- Reference count: 40
- One-line primary result: I-LLM achieves comparable accuracy to floating-point baseline and outperforms non-integer quantization methods for low-bit LLM inference

## Executive Summary
I-LLM introduces a novel post-training quantization framework enabling efficient integer-only inference for large language models. The approach addresses the primary challenge of large activation fluctuations across channels and tokens in LLMs through three key innovations: Fully-Smooth Block-Reconstruction for inter-channel variation harmonization, Dynamic Integer-only MatMul for handling inter-token variations, and integer-only approximations for non-linear operators using bit shifting. Experiments demonstrate that I-LLM can operate at W4A4 with negligible accuracy loss, bridging the gap between low-bit integer-only quantization and LLMs while enabling deployment on edge devices without floating-point capabilities.

## Method Summary
I-LLM is a post-training quantization framework that enables integer-only inference for fully-quantized low-bit LLMs. The method consists of three main components: (1) Fully-Smooth Block-Reconstruction (FSBR) learns smoothing coefficients to harmonize inter-channel variations of activations and weights, (2) Dynamic Integer-only MatMul (DI-MatMul) employs dynamic quantization to handle inter-token variations that static quantization cannot address, and (3) integer-only approximations for non-linear operators (DI-ClippedSoftmax, DI-Exp, and DI-Norm) using bit shifting operations. The framework is evaluated on LLaMA and LLaMA2 models using WikiText2 and C4 datasets for perplexity, along with zero-shot accuracy on PIQA, ARC, BoolQ, HellaSwag, and Winogrande benchmarks.

## Key Results
- I-LLM achieves W4A4 quantization with negligible loss of accuracy compared to floating-point baseline
- Outperforms non-integer quantization methods on both perplexity and zero-shot accuracy metrics
- Enables efficient deployment of LLMs on edge devices without floating-point capabilities

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Large fluctuation of activations across channels and tokens in both linear and non-linear operations is the primary obstacle to integer-only quantization for LLMs.
- Mechanism: FSBR smooths inter-channel variations by learning smoothing coefficients for all possible equivalent smoothing transformations at the channel level, effectively balancing all suitable activation-activation and activation-weight pairs.
- Core assumption: Activation disparities across channels are the main source of quantization error in LLMs.
- Evidence anchors:
  - [abstract] "we identify the primary obstacle to integer-only quantization for LLMs lies in the large fluctuation of activations across channels and tokens in both linear and non-linear operations."
  - [section 3.2] "To mitigate the issue of non-linear layer activations in LLMs being affected by channel and token differences...we propose Fully-Smooth Block-Reconstruction (FSBR)."
  - [corpus] No direct evidence for this specific claim in neighboring papers, but related work on channel-wise smoothing exists.
- Break condition: If activation distributions are already uniform across channels, FSBR would provide no benefit.

### Mechanism 2
- Claim: Dynamic quantization on input and output through full-integer matrix multiplication can handle inter-token variations that static quantization cannot.
- Mechanism: DI-MatMul enables dynamic quantization by employing integer-only operations to quantize inputs and outputs dynamically based on runtime statistics rather than calibration set values.
- Core assumption: Token-wise activation variations are too large for static quantization to handle effectively.
- Evidence anchors:
  - [section 3.3] "Traditional static quantization methods...often falter when encountering input beyond the calibration set. In contrast, DI-MatMul is designed to proactively recognize and adapt to the diverse range of input data."
  - [abstract] "to alleviate degradation caused by inter-token variations, we introduce a novel approach called Dynamic Integer-only MatMul (DI-MatMul)."
  - [corpus] No direct evidence for this specific claim in neighboring papers, but related work on dynamic quantization exists.
- Break condition: If token-wise variations are minimal, the overhead of dynamic quantization would outweigh its benefits.

### Mechanism 3
- Claim: Integer-only approximations for non-linear operators can replace complex math calculations while maintaining accuracy.
- Mechanism: DI-ClippedSoftmax, DI-Exp, and DI-Norm use bit shifting and integer arithmetic to approximate exponential, softmax, and normalization functions without floating-point operations.
- Core assumption: Non-linear functions can be accurately approximated using integer operations with minimal precision loss.
- Evidence anchors:
  - [section 3.4] "we design DI-ClippedSoftmax, DI-Exp, and DI-Norm, which utilize bit shift to execute non-linear operators efficiently while maintaining accuracy."
  - [abstract] "we design DI-ClippedSoftmax, DI-Exp, and DI-Normalization, which utilize bit shift to execute non-linear operators efficiently while maintaining accuracy."
  - [corpus] No direct evidence for this specific claim in neighboring papers, but related work on integer-only non-linear operations exists.
- Break condition: If the approximation error exceeds acceptable thresholds for model accuracy.

## Foundational Learning

- Concept: Uniform quantization and dequantization
  - Why needed here: The paper relies on understanding how floating-point values are mapped to integer representations and back
  - Quick check question: How does the choice of quantization step size affect the precision and range of representable values?

- Concept: Dyadic arithmetic and bit shifting
  - Why needed here: The integer-only operations rely heavily on dyadic number representations and bit shift operations for efficient computation
  - Quick check question: Why is representing quantization steps as dyadic numbers (mI/2kI) beneficial for integer-only computation?

- Concept: Activation function approximation
  - Why needed here: The paper introduces integer-only approximations for non-linear functions like softmax and exponential
  - Quick check question: What are the trade-offs between polynomial approximation and piecewise linear approximation for non-linear functions?

## Architecture Onboarding

- Component map:
  - FSBR (Fully-Smooth Block-Reconstruction) -> DI-MatMul (Dynamic Integer-only MatMul) -> DI-ClippedSoftmax/DI-Exp/DI-Norm (integer-only non-linear approximations)

- Critical path:
  1. FSBR reconstruction during PTQ phase
  2. Dynamic quantization parameter calculation in DI-MatMul
  3. Non-linear function approximation using DI-Exp and DI-ClippedSoftmax

- Design tradeoffs:
  - Static vs dynamic quantization: Static is faster but less accurate for varying inputs; dynamic is more accurate but adds runtime overhead
  - Precision vs efficiency: Lower bit-width improves efficiency but may hurt accuracy
  - Approximation accuracy vs computational cost: More accurate approximations require more operations

- Failure signatures:
  - Accuracy degradation: Could indicate insufficient smoothing in FSBR or poor approximation in non-linear functions
  - Runtime overhead: Could indicate inefficient dynamic quantization implementation
  - Numerical instability: Could indicate issues with integer overflow or underflow in bit-shifting operations

- First 3 experiments:
  1. Validate FSBR effectiveness: Compare perplexity with and without FSBR on a small LLM
  2. Test dynamic quantization: Measure accuracy improvement of DI-MatMul vs static quantization on varying input distributions
  3. Benchmark non-linear approximations: Compare accuracy of DI-ClippedSoftmax vs floating-point softmax across different input ranges

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does I-LLM's performance compare when deployed on specialized hardware like ARM Cortex-M or Google Edge TPU compared to NVIDIA A6000 GPU?
- Basis in paper: [explicit] The paper states "Looking ahead, we are committed to deploying I-LLM on specialized hardware and cloud platforms, with the aim of achieving even greater acceleration performance" but does not provide quantitative results for this deployment.
- Why unresolved: The paper only reports results from NVIDIA A6000 GPU experiments and acknowledges this as a limitation, stating they have not yet obtained quantitative results for deployment on specialized hardware.
- What evidence would resolve it: Quantitative latency and throughput measurements of I-LLM on edge processors like ARM Cortex-M, GreenWaves GAP-9, or Google's Edge TPU, comparing them to the A6000 GPU results.

### Open Question 2
- Question: Can I-LLM be extended to computer vision tasks and maintain similar accuracy and efficiency benefits as observed in LLMs?
- Basis in paper: [explicit] The paper states "Another limitation is that we only focused on natural language models, however, it would be interesting to explore how I-LLM performs in computer vision tasks."
- Why unresolved: The authors explicitly identify this as an unexplored area and leave it for future work, indicating no experiments have been conducted on computer vision models.
- What evidence would resolve it: Experiments applying I-LLM to vision transformers (ViTs) or convolutional neural networks (CNNs), measuring perplexity/accuracy and comparing against existing integer-only quantization methods for these architectures.

### Open Question 3
- Question: What is the optimal value for the clipping coefficient c in DI-ClippedSoftmax across different LLM architectures and bit-width settings?
- Basis in paper: [explicit] The paper shows in Table 5 that the choice of clipping value affects model accuracy, selecting c=15 for their experiments, but this is presented as a hyperparameter tuning result rather than a theoretically derived value.
- Why unresolved: The paper only provides empirical results for one model (LLaMA-7B) at W4A4 and W6A6 settings, without exploring how this parameter varies with model size, architecture, or quantization bit-width.
- What evidence would resolve it: A systematic study across different LLM families (OPT, LLaMA, LLaMA2, LLaMA3) and bit-width configurations (W4A4, W6A6, W8A8) to identify optimal clipping values and potential patterns or theoretical justification for these values.

## Limitations

- Limited empirical validation scope: Evaluation primarily focused on perplexity metrics and limited zero-shot accuracy benchmarks
- Hardware-specific optimization claims: Lacks concrete measurements of inference speedups on specific hardware platforms
- Scalability and generalization concerns: Only evaluated on LLaMA and LLaMA2 models without testing significantly larger models or different LLM architectures

## Confidence

- High Confidence: Technical feasibility of integer-only approximations for non-linear functions (DI-ClippedSoftmax, DI-Exp, DI-Norm) using bit shifting operations
- Medium Confidence: Effectiveness of Fully-Smooth Block-Reconstruction (FSBR) in harmonizing inter-channel variations
- Medium Confidence: Dynamic Integer-only MatMul (DI-MatMul) approach for handling inter-token variations
- Low Confidence: Claim that I-LLM achieves "negligible loss of accuracy" at W4A4 quantization

## Next Checks

1. **Ablation study on component contributions**: Conduct controlled experiments isolating the impact of FSBR, DI-MatMul, and each integer-only non-linear approximation on overall accuracy by testing configurations with only one component active at a time and measuring the degradation relative to the full I-LLM system.

2. **Hardware performance benchmarking**: Implement I-LLM on representative integer-only hardware accelerators (e.g., ARM Ethos-U, Google Edge TPU) and measure actual inference latency, power consumption, and memory bandwidth usage, comparing these metrics against floating-point implementations and other quantization methods to validate the claimed efficiency benefits.

3. **Cross-architecture generalization test**: Evaluate I-LLM on LLM architectures beyond LLaMA and LLaMA2, including different attention mechanisms (e.g., Mamba, RWKV) and varying model sizes to test whether the inter-channel and inter-token variation smoothing techniques generalize across diverse architectural patterns or are specific to the tested models.