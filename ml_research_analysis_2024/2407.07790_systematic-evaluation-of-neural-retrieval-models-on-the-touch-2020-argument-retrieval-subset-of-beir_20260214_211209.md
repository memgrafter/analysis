---
ver: rpa2
title: "Systematic Evaluation of Neural Retrieval Models on the Touch\xE9 2020 Argument\
  \ Retrieval Subset of BEIR"
arxiv_id: '2407.07790'
source_url: https://arxiv.org/abs/2407.07790
tags:
- retrieval
- touch
- neural
- https
- argument
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This study investigates why neural retrieval models underperform\
  \ BM25 on the Touch\xE9 2020 argument retrieval task. Through black-box evaluation,\
  \ corpus denoising, and post-hoc relevance judgments, the authors find that short,\
  \ non-argumentative documents and missing judgments bias evaluation against neural\
  \ models."
---

# Systematic Evaluation of Neural Retrieval Models on the Touch√© 2020 Argument Retrieval Subset of BEIR

## Quick Facts
- arXiv ID: 2407.07790
- Source URL: https://arxiv.org/abs/2407.07790
- Authors: Nandan Thakur; Luiz Bonifacio; Maik Fr√∂be; Alexander Bondarenko; Ehsan Kamalloo; Martin Potthast; Matthias Hagen; Jimmy Lin
- Reference count: 40
- Primary result: Neural retrieval models underperform BM25 on Touch√© 2020 due to short, non-argumentative documents and missing judgments

## Executive Summary
This study investigates why neural retrieval models underperform BM25 on the Touch√© 2020 argument retrieval task, which is part of the BEIR benchmark. Through black-box evaluation, corpus denoising, and post-hoc relevance judgments, the authors find that short, non-argumentative documents and missing judgments bias evaluation against neural models. By removing documents under 20 words and adding post-hoc judgments, neural model effectiveness improves by up to 0.52 nDCG@10, but BM25 remains most effective. Theoretical analysis using retrieval axioms reveals that neural models violate the document length normalization axiom LNC2, explaining BM25's robustness. The findings highlight the need for improved training strategies and data quality in argument retrieval.

## Method Summary
The study conducts black-box evaluation of neural retrieval models (TAS-B, Contriever, DRAGON+, SPLADEv2, CITADEL+) on the Touch√© 2020 argument retrieval task without retraining. The method involves three main components: corpus denoising by removing documents under 20 words, post-hoc relevance judgments for unjudged documents in the top-10 results, and theoretical analysis using retrieval axioms (specifically LNC2). Effectiveness is measured using nDCG@10 and hole@ùëò metrics. The study compares BM25 against neural models before and after denoising, and analyzes the impact of adding post-hoc judgments on model rankings.

## Key Results
- Neural models retrieve more short, non-argumentative documents than BM25, biasing evaluation against them
- Adding post-hoc judgments increases neural model effectiveness by up to 0.52 nDCG@10
- BM25 almost perfectly agrees with the LNC2 axiom (>99% agreement), while neural models substantially violate it
- Despite improvements, BM25 remains the most effective model for argument retrieval on Touch√© 2020

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Neural models underperform BM25 on Touch√© 2020 because they retrieve shorter, often non-argumentative documents.
- Mechanism: Short documents (<20 words) are frequently non-relevant in argument retrieval; neural models are biased toward these due to lexical overlap and embedding proximity, while BM25's length normalization mitigates this.
- Core assumption: Many short documents in Touch√© 2020 are not valid arguments (no premises), and relevance depends on argumentative quality, not just lexical overlap.
- Evidence anchors:
  - [abstract] "short, non-argumentative documents... bias evaluation against neural models"
  - [section 4.1] "quite a few of the neural models' results are unjudged in the Touch√© 2020 data... many of the short Touch√© passages are not argumentative and thus non-relevant per se"
  - [corpus] Short document spike at 20‚Äì30 words in Touch√© 2020 distribution (Figure 4)
- Break condition: If Touch√© 2020 corpus is cleaned of short non-arguments or neural models are trained with explicit length regularization.

### Mechanism 2
- Claim: Missing relevance judgments create unfair penalization for neural models.
- Mechanism: Unjudged documents retrieved by neural models are treated as non-relevant, lowering their scores; denoising and adding post-hoc judgments reveals true effectiveness.
- Core assumption: The original Touch√© 2020 judgments have selection bias (unjudged top results are often relevant), and neural models retrieve more diverse, unjudged documents than BM25.
- Evidence anchors:
  - [abstract] "missing judgments complicate fair comparison... augmenting the unjudged data with post-hoc judgments"
  - [section 4.4] "over 78% of the judgment pairs were judged relevant (with 48% highly relevant and 30% relevant), indicating that many 'relevant' documents are retrieved by models but unjudged originally in Touch√© 2020"
  - [corpus] Over 50% of top-10 results are unjudged in original data (hole@10 >50%)
- Break condition: If judgment process is exhaustive or unjudged results are filtered out pre-evaluation.

### Mechanism 3
- Claim: Neural models violate document length normalization axiom LNC2, explaining BM25's robustness.
- Mechanism: LNC2 states that self-concatenation of a document should not lower its relevance score; neural models map all documents to fixed-size vectors, ignoring true length, while BM25 explicitly normalizes for document length.
- Core assumption: The LNC2 axiom captures an important property for argument retrieval where longer documents tend to have more argumentative content.
- Evidence anchors:
  - [abstract] "Theoretical analysis using retrieval axioms reveals that neural models violate the document length normalization axiom LNC2, explaining BM25's robustness"
  - [section 4.5] "BM25 almost perfectly agrees with the LNC2 axiom (agreement above 99%), whereas neural models substantially violate LNC2"
  - [corpus] Touch√© 2020 has wide document length variance, with many short documents (Figure 4)
- Break condition: If neural models are modified to include length-aware components or trained with length-aware objectives.

## Foundational Learning

- Concept: Document length normalization in BM25
  - Why needed here: Explains why BM25 is robust to short, non-argumentative documents in Touch√© 2020.
  - Quick check question: What is the role of parameter b in BM25, and how does it affect document length normalization?

- Concept: Axiomatic analysis in information retrieval
  - Why needed here: Provides theoretical grounding for empirical findings about model weaknesses.
  - Quick check question: What does the LNC2 axiom require, and why might it be especially important for argument retrieval?

- Concept: Relevance judgment bias and selection bias
  - Why needed here: Explains why post-hoc judgments can significantly change model effectiveness scores.
  - Quick check question: How does the hole@10 metric reflect selection bias in a test collection?

## Architecture Onboarding

- Component map: Retrieval pipeline -> document corpus -> retrieval models (BM25, dense, sparse, multi-vector) -> evaluation (nDCG@10, hole@10) -> denoising -> post-hoc judgments -> axiomatic analysis
- Critical path: Corpus -> denoising (remove short docs) -> retrieval model inference -> evaluation with post-hoc judgments
- Design tradeoffs: Using length-based heuristics for denoising vs. argument classification; adding post-hoc judgments vs. relying on existing judgments
- Failure signatures: Low nDCG@10 correlated with high proportion of short, unjudged documents; neural models underperforming BM25 on argument retrieval tasks
- First 3 experiments:
  1. Filter Touch√© 2020 corpus to remove documents <20 words and re-run all models, compare nDCG@10
  2. Add post-hoc relevance judgments for unjudged top-10 results and re-evaluate models
  3. Apply DocT5query expansion to short documents and evaluate change in neural model effectiveness

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can neural retrieval models be trained to be more robust against short, non-argumentative documents in argument retrieval tasks?
- Basis in paper: [explicit] The paper identifies that neural models retrieve shorter, non-argumentative documents in Touch√© 2020 and suggests that improved training strategies could address this issue.
- Why unresolved: The authors propose exploring regularization objectives that penalize short documents but do not implement or evaluate such training approaches.
- What evidence would resolve it: Experimental results comparing standard-trained neural models with models trained using document length regularization on argument retrieval tasks, showing effectiveness improvements.

### Open Question 2
- Question: To what extent does the document length normalization axiom LNC2 explain the performance gap between BM25 and neural models across different IR tasks beyond argument retrieval?
- Basis in paper: [explicit] The paper finds that all neural models violate LNC2 on Touch√© 2020 while BM25 agrees with it, suggesting this may explain BM25's superior performance on this task.
- Why unresolved: The study only examines LNC2 violation on one dataset (Touch√© 2020) and does not test whether this axiom violation generalizes to other IR tasks where neural models underperform.
- What evidence would resolve it: Systematic axiomatic analysis of LNC2 agreement across multiple IR datasets where neural models underperform, correlating axiom violation rates with effectiveness gaps.

### Open Question 3
- Question: How does the quality and completeness of relevance judgments affect the relative performance of neural versus lexical retrieval models in information retrieval evaluation?
- Basis in paper: [explicit] The authors show that adding post-hoc judgments to fill unjudged results substantially improves neural model effectiveness on Touch√© 2020, suggesting evaluation biases may disadvantage neural approaches.
- Why unresolved: The study only demonstrates this effect on one dataset with one type of post-hoc judgment process; it remains unclear whether this is a general phenomenon across IR tasks.
- What evidence would resolve it: Comparative evaluation of neural and lexical models across multiple IR datasets with varying levels of judgment completeness, measuring how post-hoc judgments affect relative performance rankings.

## Limitations
- Denoising approach may inadvertently filter out valid but concise arguments by removing all documents under 20 words
- Post-hoc judgment process introduces potential subjectivity that could affect the magnitude of observed improvements
- Theoretical analysis using retrieval axioms provides useful insights but doesn't establish causation for the observed performance differences

## Confidence

- High confidence: BM25's superior performance on Touch√© 2020 and the negative impact of short, non-argumentative documents on neural models
- Medium confidence: The extent to which missing judgments bias evaluation (based on the relatively small number of post-hoc judgments added)
- Low confidence: The theoretical explanation via LNC2 axiom violation as the primary cause of neural model underperformance (correlation doesn't prove causation)

## Next Checks

1. Conduct ablation studies testing different document length thresholds (15, 25, 30 words) to verify the 20-word cutoff is optimal
2. Perform inter-annotator agreement analysis on post-hoc judgments to quantify judgment reliability
3. Test whether length-aware neural architectures (e.g., adding length features to dense retrievers) can match BM25's performance on Touch√© 2020