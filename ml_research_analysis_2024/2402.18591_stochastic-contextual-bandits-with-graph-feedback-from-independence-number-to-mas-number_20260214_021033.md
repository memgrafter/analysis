---
ver: rpa2
title: 'Stochastic contextual bandits with graph feedback: from independence number
  to MAS number'
arxiv_id: '2402.18591'
source_url: https://arxiv.org/abs/2402.18591
tags:
- feedback
- graph
- bound
- bandits
- regret
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper studies stochastic contextual bandits with graph feedback,
  where taking an action reveals rewards for neighboring actions in a feedback graph
  across all contexts. The key result is a regret lower bound $\Omega(\sqrt{\betaM(G)T})$,
  where $\betaM(G)$ is a graph-theoretic quantity that interpolates between the independence
  number $\alpha(G)$ and maximum acyclic subgraph number $m(G)$ as the number of contexts
  $M$ varies.
---

# Stochastic contextual bandits with graph feedback: from independence number to MAS number

## Quick Facts
- arXiv ID: 2402.18591
- Source URL: https://arxiv.org/abs/2402.18591
- Reference count: 40
- Key outcome: Regret lower bound $\Omega(\sqrt{\beta_M(G)T})$ for stochastic contextual bandits with graph feedback, where $\beta_M(G)$ interpolates between independence number $\alpha(G)$ and MAS number $m(G)$ as contexts increase

## Executive Summary
This paper studies stochastic contextual bandits with graph feedback, where taking an action reveals rewards for neighboring actions in a feedback graph across all contexts. The key result is a regret lower bound $\Omega(\sqrt{\beta_M(G)T})$, where $\beta_M(G)$ is a graph-theoretic quantity that interpolates between the independence number $\alpha(G)$ and maximum acyclic subgraph number $m(G)$ as the number of contexts $M$ varies. For many contexts ($M \geq m(G)$), this shows the MAS number $m(G)$ characterizes the statistical complexity, contrasting with multi-armed bandits where $\alpha(G)$ is the fundamental quantity. The paper also provides algorithms achieving near-optimal regret for important classes of feedback graphs, including transitively closed graphs with applications in auctions and inventory control.

## Method Summary
The paper establishes regret lower bounds through information-theoretic arguments using self-avoiding context sequences and upper bounds through arm elimination algorithms. The key technical innovation is the graph-theoretic quantity $\beta_M(G)$ that interpolates between $\alpha(G)$ and $m(G)$. For self-avoiding contexts, the algorithm uses a layering technique with sequential game I to choose exploration sets that dominate active action sets. For general contexts, it employs a different sequential game II. The algorithms rely on polynomial-time subroutines for solving sequential games on graphs, though exact computational details are sketched.

## Key Results
- Establishes $\Omega(\sqrt{\beta_M(G)T})$ regret lower bound for contextual bandits with graph feedback
- Shows $\beta_M(G)$ interpolates between $\alpha(G)$ and $m(M)$ as number of contexts $M$ increases
- Provides polynomial-time algorithms achieving $\tilde{O}(\sqrt{\beta_M(G)T})$ for self-avoiding contexts
- Achieves $\tilde{O}(\sqrt{\min\{\beta_M(G), m(G)\}T})$ for general contexts
- Demonstrates applications to transitively closed graphs with implications for auctions and inventory control

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The regret lower bound scales as $\Omega(\sqrt{\beta_M(G)T})$ where $\beta_M(G)$ interpolates between independence number $\alpha(G)$ and MAS number $m(G)$ as contexts increase
- Mechanism: The proof constructs hard instances using $M$ independent sets $I_1,...,I_M$ with $I_i \to I_j$ forbidden for $i<j$. This forces the learner to explore each $I_c$ under context $c$, but with $M$ contexts the acyclic structure (captured by MAS number) becomes the limiting factor
- Core assumption: The context sequence is self-avoiding (no jumping back to previous contexts)
- Evidence anchors:
  - [abstract] "Interestingly, $\beta_M(G)$ interpolates between $\alpha(G)$ (the independence number of the graph) and $m(G)$ (the maximum acyclic subgraph number) as the number of contexts $M$ varies"
  - [section 2] "The proof uses the definition (2) of $\beta_M(G)$ to construct $M$ independent sets $I_1,...,I_M$ such that $I_i \to I_j$ for $i<j$"
- Break condition: If contexts can jump back (non-self-avoiding), the exploration in earlier contexts can provide information for later contexts, breaking the lower bound argument

### Mechanism 2
- Claim: For self-avoiding contexts, there exists a polynomial-time algorithm achieving $\tilde{O}(\sqrt{\beta_M(G)T})$ regret
- Mechanism: Uses arm elimination with layering technique. Each layer $\ell$ maintains active sets $A_{c,\ell}$ where all actions have been observed $\ell$ times. On each layer, solves a sequential game to find a small set of actions to explore that dominate the active set
- Core assumption: The feedback graph is strongly observable and contexts are self-avoiding
- Evidence anchors:
  - [section 3.2] "Algorithm 1: Arm elimination algorithm for self-avoiding contexts" with detailed description
  - [section 3.2] "The algorithm relies on the well-known idea of arm elimination"
- Break condition: If graph is not strongly observable, some actions cannot be observed even when their neighbors are played, breaking the dominance requirement

### Mechanism 3
- Claim: For general (non-self-avoiding) contexts, achieves $\tilde{O}(\sqrt{\min\{\beta_M(G), m(G)\}T})$ regret using a different sequential game
- Mechanism: Similar layering approach but uses sequential game II where the adversary can choose contexts adaptively and the learner picks one action at a time. This captures the more challenging case where exploration in one context can help future contexts
- Core assumption: General feedback graph with possibly adaptive context sequences
- Evidence anchors:
  - [section 3.3] "Algorithm 2: Arm elimination under general contexts" with detailed description
  - [section 3.1] "The second sequential game is motivated by bandit learning with an arbitrary context sequence"
- Break condition: The gap between $\beta_M(G)$ and $\beta_M(G)$ in the general case remains an open problem when the graph contains long paths

## Foundational Learning

- Concept: Graph-theoretic quantities (independence number, MAS number, dominating number)
  - Why needed here: These quantities characterize the statistical complexity of learning under graph feedback. The paper shows how these quantities change as the number of contexts varies
  - Quick check question: What is the difference between $\alpha(G)$ and $m(G)$ for a directed graph? Can you construct an example where they differ significantly?

- Concept: Sequential games on graphs
  - Why needed here: The algorithms reduce the contextual bandit problem to solving sequential games where the learner chooses actions to explore while the adversary chooses active sets. Understanding these games is crucial for implementing the algorithms
  - Quick check question: In the sequential game I, why must the learner choose a dominating set $D_c \subseteq A_c$? What would happen if they chose a smaller set?

- Concept: Layering technique in bandit algorithms
  - Why needed here: The algorithms use layers to gradually increase the accuracy of reward estimates. Each layer ensures actions have been observed enough times for concentration bounds to hold
  - Quick check question: How does the layering technique ensure that confidence bounds hold with high probability across all layers and contexts?

## Architecture Onboarding

- Component map:
  - Graph analysis module: Computes $\beta_M(G)$, $\beta_M(G)$, $m(G)$ for given feedback graph
  - Sequential game solver: Solves games I and II for given graph and $M$
  - Layering manager: Maintains active sets $A_{c,\ell}$ and tracks observation counts
  - Exploration controller: Chooses actions based on sequential game solutions
  - Confidence bound calculator: Computes (5) for arm elimination

- Critical path: Graph analysis → Sequential game solution → Layering setup → Context reception → Action selection → Observation update → Confidence bound update

- Design tradeoffs:
  - Exact vs approximate sequential game solutions: Exact is NP-hard but approximate gives $O(\log|V|)$ factor increase
  - Number of layers: More layers give better concentration but increase computational cost
  - Exploration vs exploitation balance: More exploration gives better estimates but higher regret

- Failure signatures:
  - Regret higher than theoretical bound: Likely issue with sequential game solution or layering
  - Confidence bounds failing: Possibly incorrect observation counting or too few layers
  - Algorithm not polynomial time: Using exact NP-hard subroutines instead of approximations

- First 3 experiments:
  1. Implement graph analysis module and test on simple graphs (undirected, directed acyclic, transitively closed)
  2. Implement sequential game solver I and verify against known values for small graphs
  3. Build complete algorithm for self-avoiding contexts and test on synthetic problems with known $\beta_M(G)$

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Is the MAS number $m(G)$ the fundamental complexity measure for contextual bandits with graph feedback when $T$ is small ($T < \beta_M(G)^3$)?
- Basis in paper: [explicit] The paper establishes a lower bound requiring $T \geq \beta_M(G)^3$, but notes this is not an artifact of the analysis and the optimal regret becomes fundamentally different for smaller $T$.
- Why unresolved: The paper only provides bounds for large $T$ and defers the small $T$ case to future work, acknowledging it becomes a mixture of $\sqrt{T}$ and $T^{2/3}$ terms like in multi-armed bandits.
- What evidence would resolve it: A complete characterization of the minimax regret for contextual bandits with graph feedback across all values of $T$, similar to the recent multi-armed bandits result.

### Open Question 2
- Question: What is the exact minimax value of sequential game II (Definition 2) for general context sequences?
- Basis in paper: [explicit] The paper introduces two sequential games and provides upper bounds for both, but notes that the minimax value of sequential game II is not characterized tightly in general.
- Why unresolved: The paper shows an example where the current upper bound $\beta_{dom}(G,M)$ is loose, and identifies the challenge of non-greedy approaches in choosing vertices when the context sequence is not self-avoiding.
- What evidence would resolve it: A tight characterization of the minimax value $U^*_2(G,M)$ that matches the lower bound $\Omega(\sqrt{\beta_M(G)T})$ when $T \geq \beta_M(G)^3$.

### Open Question 3
- Question: Does the UCB algorithm achieve optimal regret for contextual bandits with graph feedback, or is the MAS number $m(G)$ fundamental even for this algorithm?
- Basis in paper: [explicit] The paper notes that UCB algorithms analyzed for both multi-armed and contextual bandits only achieve regret upper bound $\tilde{O}(\sqrt{m(G)T})$, and this remains an interesting open question.
- Why unresolved: Current UCB analyses do not exploit the structure that could lead to better regret bounds, and it's unknown if forced exploration is necessary to achieve the tighter bounds.
- What evidence would resolve it: A proof that UCB (or a variant) achieves regret $\tilde{O}(\sqrt{\beta_M(G)T})$ for general contexts, or a lower bound showing this is impossible and $m(G)$ is indeed fundamental.

## Limitations
- The gap between $\beta_M(G)$ and $\beta_M(G)$ for general contexts remains unresolved when graphs contain long paths
- Lower bound proof relies heavily on self-avoiding context sequences, limiting generalizability
- Algorithms require polynomial-time subroutines for solving sequential games, which may be computationally expensive for large graphs
- Focus on stochastic rewards does not address adversarial or non-stationary settings

## Confidence
High confidence: The main theoretical results establishing $\Omega(\sqrt{\beta_M(G)T})$ lower bounds and the interpolation between $\alpha(G)$ and $m(G)$ are rigorously proven with detailed constructions. Medium confidence: The algorithmic implementations and their polynomial-time guarantees, as some computational details are sketched rather than fully specified. Low confidence: The gap between $\beta_M(G)$ and $\beta_M(G)$ for general contexts remains unresolved.

## Next Checks
1. Implement the sequential game solvers for small graphs and verify they produce optimal or near-optimal exploration sets for known examples.
2. Simulate the arm elimination algorithm on transitively closed graphs with $M \geq m(G)$ contexts and verify the regret scales as $O(\sqrt{m(G)T})$.
3. Test the algorithm on graphs where $\alpha(G) \neq m(G)$ (e.g., directed cycles) to confirm the transition from independence number to MAS number as contexts increase.