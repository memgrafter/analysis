---
ver: rpa2
title: 'HoME: Hierarchy of Multi-Gate Experts for Multi-Task Learning at Kuaishou'
arxiv_id: '2408.05430'
source_url: https://arxiv.org/abs/2408.05430
tags:
- expert
- experts
- home
- gate
- multi-task
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper addresses stability issues in industrial-scale multi-task
  Mixture-of-Experts (MoE) systems, specifically expert collapse, expert degradation,
  and expert underfitting. The authors propose HoME, a Hierarchy of Multi-Gate Experts
  model, with three key innovations: expert normalization and Swish activation to
  balance expert outputs, a hierarchy mask mechanism to model task relatedness and
  maximize shared-expert efficiency, and feature-gate and self-gate mechanisms to
  ensure sparse-task experts receive appropriate gradients.'
---

# HoME: Hierarchy of Multi-Gate Experts for Multi-Task Learning at Kuaishou

## Quick Facts
- arXiv ID: 2408.05430
- Source URL: https://arxiv.org/abs/2408.05430
- Reference count: 40
- Primary result: Hierarchy of Multi-Gate Experts model that addresses stability issues in industrial-scale MoE systems

## Executive Summary
HoME (Hierarchy of Multi-Gate Experts) is a novel multi-task learning architecture designed to address stability issues in industrial-scale Mixture-of-Experts systems. The model introduces three key innovations: expert normalization with Swish activation, a hierarchy mask mechanism to model task relatedness, and feature-gate/self-gate mechanisms to ensure sparse-task experts receive appropriate gradients. The system was evaluated on Kuaishou's short-video platform, demonstrating significant improvements over strong baselines across all tasks.

## Method Summary
The paper proposes a hierarchical multi-gate expert system that addresses common stability issues in industrial-scale MoE systems through three innovations: expert normalization and Swish activation to balance expert outputs, a hierarchy mask mechanism to model task relatedness and maximize shared-expert efficiency, and feature-gate and self-gate mechanisms to ensure sparse-task experts receive appropriate gradients. The architecture maintains task-specific performance while leveraging shared representations across related tasks.

## Key Results
- Average improvements of 0.52% GAUC across all tasks in offline experiments
- 0.636% increase in play-time in online A/B tests
- Outperforms strong baselines across all evaluated tasks
- Demonstrates effectiveness in real-world deployment at Kuaishou's scale

## Why This Works (Mechanism)
The model addresses MoE stability issues through a hierarchical approach that balances expert utilization while maintaining task-specific performance. The expert normalization and Swish activation prevent expert collapse by ensuring stable output distributions. The hierarchy mask mechanism leverages task relatedness to maximize shared-expert efficiency, reducing redundancy. The feature-gate and self-gate mechanisms ensure that even sparsely activated experts receive sufficient gradient information, preventing underfitting.

## Foundational Learning

**Mixture-of-Experts (MoE)**: Why needed - Enables specialized processing for different input types while maintaining parameter efficiency; Quick check - Verify that gate mechanisms properly route inputs to appropriate experts

**Expert Collapse**: Why needed - Prevents concentration of all inputs to few experts, ensuring balanced utilization; Quick check - Monitor expert activation patterns across different input distributions

**Task-Relatedness Modeling**: Why needed - Leverages shared representations across related tasks to improve efficiency; Quick check - Validate hierarchy mask assignments through task correlation analysis

**Sparse Task Handling**: Why needed - Ensures experts handling rare tasks still receive adequate training signals; Quick check - Verify gradient flow to sparse-task experts during training

## Architecture Onboarding

Component Map: Input -> Gate Networks -> Expert Pool -> Hierarchy Mask -> Output Heads

Critical Path: Input features flow through gate networks that determine expert routing, pass through the expert pool with hierarchy masks applied, and produce task-specific outputs through specialized heads.

Design Tradeoffs: Balances between shared and task-specific parameters, prioritizes stability over maximum parameter efficiency, and maintains backward compatibility with existing MoE systems.

Failure Signatures: Expert collapse manifests as uniform gate activations, degradation shows as decreasing task performance despite increasing training loss, and underfitting appears as stagnant sparse-task performance.

First Experiments:
1. Baseline MoE with expert normalization comparison
2. Single-task MoE vs multi-task HoME performance
3. Gate sparsity analysis across different task combinations

## Open Questions the Paper Calls Out
None specified in the paper.

## Limitations
- Proprietary dataset prevents independent verification of reported improvements
- Aggregate GAUC improvements don't reveal individual task trade-offs
- Limited reporting on statistical significance and duration of online A/B tests
- Missing analysis of potential negative impacts on non-measured business metrics

## Confidence
- **High confidence**: Theoretical framework for addressing MoE stability issues is well-established
- **Medium confidence**: Reported offline improvements of 0.52% GAUC due to proprietary dataset
- **Medium confidence**: Online A/B test results showing 0.636% play-time increase with limited reporting

## Next Checks
1. Conduct ablation studies to isolate the contribution of each innovation (expert normalization, hierarchy masks, feature-gate mechanisms) to overall performance
2. Implement controlled experiments with different task relatedness patterns to validate the hierarchy mask mechanism's effectiveness across varying task structures
3. Perform long-term online monitoring to assess potential delayed effects or degradation patterns that may emerge after extended deployment of HoME systems