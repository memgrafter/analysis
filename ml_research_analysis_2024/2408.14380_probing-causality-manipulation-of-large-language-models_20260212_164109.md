---
ver: rpa2
title: Probing Causality Manipulation of Large Language Models
arxiv_id: '2408.14380'
source_url: https://arxiv.org/abs/2408.14380
tags:
- llms
- causality
- knowledge
- causal
- layer
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work proposes a novel hierarchical probing method to investigate
  large language models' (LLMs) internal manipulation of causality. The method provides
  different shortcuts via retrieval-augmented generation (RAG) and in-context learning
  (ICL) to guide models on a designed causality classification task.
---

# Probing Causality Manipulation of Large Language Models

## Quick Facts
- arXiv ID: 2408.14380
- Source URL: https://arxiv.org/abs/2408.14380
- Authors: Chenyang Zhang; Haibo Tong; Bin Zhang; Dongyu Zhang
- Reference count: 37
- Primary result: LLMs can detect causality-related entities and recognize direct causal relationships, but lack specialized cognition for causality, primarily relying on global semantic understanding rather than systematic causal reasoning.

## Executive Summary
This work proposes a novel hierarchical probing method to investigate how large language models internally manipulate causality. The method provides different shortcuts via retrieval-augmented generation (RAG) and in-context learning (ICL) to guide models on a designed causality classification task. Experiments on GPT-4, GPT-3.5, ChatGLM, and MedChatGLM show that while LLMs can detect causality-related entities and recognize direct causal relationships, they lack specialized cognition for causality and primarily rely on global semantic understanding rather than systematic causal reasoning.

## Method Summary
The method involves constructing a causality classification dataset from the CMedCausal medical dataset by applying three different actions to create positive and negative instances. A hierarchical probing framework is implemented with three layers of augmentation: no augmentation (Layer 1), original CMedCausal passages (Layer 2), and medical knowledge from DiseaseKG (Layer 3). The RAG retrieval pipeline chunks and encodes knowledge passages for similarity search. Prompt templates are designed with simple and advanced versions incorporating chain-of-thought reasoning. Models are evaluated using F1-score and Matthews correlation coefficient (MCC) to measure performance on the binary classification task.

## Key Results
- LLMs demonstrate ability to detect causality-related entities and recognize direct causal relationships
- Models lack specialized cognition for causality, primarily relying on global semantic understanding
- Performance varies across models and actions, with global semantics being more important than entity recognition for classification
- Back-translation experiments show models fail to recognize causal entities in heterogeneous forms

## Why This Works (Mechanism)

### Mechanism 1
Hierarchical probing with controlled shortcuts (RAG + ICL) reveals how LLMs manipulate causality internally. By systematically adding shortcuts (Layer 1: no augmentation, Layer 2: gold-standard passage, Layer 3: medical KG knowledge), we can observe whether LLMs improve classification performance. If performance increases with a shortcut, that shortcut corresponds to a capability the model lacks natively.

### Mechanism 2
Action 2 (global causation disturbing) creates more statistically distinguishable negative instances, making classification easier based on global semantics rather than causal understanding. By shuffling all entities in the sentence, Action 2 creates negative instances that are statistically very different from positive instances (higher PPL difference).

### Mechanism 3
Back-translation preserves causal entities while changing their form, revealing whether models can recognize causal entities in heterogeneous representations. Layer 2.5 uses back-translated knowledge from Layer 2. If performance drops compared to Layer 2, it indicates models rely on specific representations of causal entities rather than understanding their meaning across different forms.

## Foundational Learning

- **Causal reasoning vs. statistical association**
  - Why needed here: Understanding the difference between models that rely on statistical patterns versus those that understand causal relationships is fundamental to interpreting the probing results.
  - Quick check question: If a model classifies "Eating causes health" as positive but "Health causes eating" as negative, is it using causal reasoning or statistical association?

- **Retrieval Augmented Generation (RAG)**
  - Why needed here: The hierarchical probing method uses RAG to provide different levels of knowledge augmentation to the models, which is central to the experimental design.
  - Quick check question: What is the difference between Layer 1 (no augmentation), Layer 2 (original CMedCausal), and Layer 3 (Universal Medical-KG) in terms of the knowledge provided to the model?

- **In-Context Learning (ICL)**
  - Why needed here: The probing method uses ICL through prompts to guide the models' behavior, which is essential for understanding how the experimental results were obtained.
  - Quick check question: How does the simple prompt differ from the advanced prompt in terms of the guidance provided to the model?

## Architecture Onboarding

- **Component map:** Dataset construction (CMedCausal base + Actions 1-3) -> Hierarchical probing layers (1-3 + back-translation) -> Retrieval pipeline (chunking, encoding, FAISS search) -> Prompt templates (simple vs advanced) -> Evaluation metrics (F1, MCC) -> Model zoo (GPT-4, GPT-3.5, ChatGLM, MedChatGLM, BERT)

- **Critical path:** Dataset construction → Model selection → Hierarchical probing with RAG/ICL → Performance evaluation → Analysis of results to infer causal manipulation capabilities

- **Design tradeoffs:** Using back-translation preserves entities but may introduce noise; using FAISS for retrieval is efficient but may miss relevant knowledge; using MCC alongside F1 provides more robust evaluation but is less interpretable

- **Failure signatures:** If all models perform at chance level (MCC ≈ 0), the probing method may not be effective; if performance doesn't vary across layers, the shortcuts may not be meaningful; if back-translation doesn't preserve entities, Layer 2.5 won't provide useful insights

- **First 3 experiments:**
  1. Run baseline evaluation (Layer 1, simple prompt) on all models to establish native performance
  2. Add Layer 2 augmentation (original CMedCausal) with simple prompt to test if gold-standard knowledge improves performance
  3. Add Layer 2.5 (back-translated knowledge) to test whether models can recognize causal entities across different linguistic forms

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do specific training strategies (like RLHF vs supervised learning) differentially impact LLMs' ability to recognize and reason about causality in medical contexts?
- Basis in paper: The paper notes that GPT-4 and GPT-3.5 use RLHF training strategy, which makes their answer results more similar to human beings and improves their ability to discuss causal problems to a certain extent, while BERT is trained on supervised datasets.
- Why unresolved: The paper only provides a brief comparison of training strategies' effects on causality reasoning without detailed analysis of how different strategies impact specific aspects of causal understanding.
- What evidence would resolve it: Comparative experiments isolating the effects of different training strategies on causality tasks, with ablation studies on specific RLHF components and their impact on causal reasoning performance.

### Open Question 2
- Question: Can LLMs develop systematic causal reasoning abilities beyond statistical associations, and what architectural modifications would enable this?
- Basis in paper: The paper concludes that LLMs lack specialized cognition for causality, merely treating causal entities as part of the global semantic of the sentence, and rely on linguistic order and positions to demonstrate causality rather than systematic causal reasoning.
- Why unresolved: The paper identifies the limitation but doesn't explore potential architectural changes or training methods that could help LLMs move beyond statistical associations to develop true causal reasoning capabilities.
- What evidence would resolve it: Experiments testing modified architectures or training approaches designed to enhance causal reasoning, comparing their performance against standard LLMs on complex causality tasks.

### Open Question 3
- Question: How does the effectiveness of different knowledge augmentation strategies (RAG vs ICL) vary across model sizes and domains in causality reasoning tasks?
- Basis in paper: The paper uses both RAG and ICL to provide shortcuts for models, but notes that performance varies across models and actions, with different models showing different sensitivities to knowledge augmentation.
- Why unresolved: The paper provides results showing variation but doesn't deeply analyze why certain augmentation strategies work better for specific model types or how this varies across different domains beyond medical contexts.
- What evidence would resolve it: Systematic experiments varying knowledge augmentation methods across multiple model sizes and domains, with detailed analysis of which aspects of augmentation are most effective for different model types.

## Limitations

- The correlation between performance differences across hierarchical layers and actual internal causal manipulation capabilities may not be causal
- The back-translation mechanism relies on the assumption that back-translation preserves causal entities while changing their form - this preservation may not hold consistently
- Dataset construction artifacts from the three actions may influence model performance in unintended ways

## Confidence

High confidence: LLMs show some ability to recognize causal entities and direct causal relationships, but primarily rely on global semantic understanding rather than systematic causal reasoning.

Medium confidence: Action 2 creates more statistically distinguishable negative instances that make classification easier based on global semantics rather than causal understanding.

Low confidence: Back-translation preserves causal entities while changing their form, allowing us to test entity recognition across heterogeneous representations.

## Next Checks

1. **PPL Correlation Validation**: Measure the correlation between PPL differences across the three actions and model performance. Run a controlled experiment where models are explicitly trained to distinguish based on PPL differences versus causal understanding, then compare performance patterns with the current results.

2. **Back-translation Entity Preservation**: Conduct a systematic evaluation of back-translation quality by comparing entity recognition accuracy before and after back-translation. Use multiple translation APIs and languages to test whether the preservation assumption holds consistently.

3. **Knowledge-Augmentation Ablation**: Design an ablation study where models are given partial knowledge augmentation (e.g., only causes or only effects from Layer 2) to test whether performance improvements truly reflect causal understanding or simply better access to relevant information.