---
ver: rpa2
title: Using Combinatorial Optimization to Design a High quality LLM Solution
arxiv_id: '2405.13020'
source_url: https://arxiv.org/abs/2405.13020
tags:
- solution
- each
- design
- factors
- combination
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces a combinatorial optimization-based methodology
  for designing high-quality LLM solutions by identifying key factors influencing
  solution quality, including prompt types, LLM inputs, generation parameters, and
  design alternatives. The approach uses combinatorial optimization to create a small
  test plan that covers all desired factor interactions, enabling efficient evaluation
  of the design space when manual evaluation is time-consuming.
---

# Using Combinatorial Optimization to Design a High quality LLM Solution

## Quick Facts
- arXiv ID: 2405.13020
- Source URL: https://arxiv.org/abs/2405.13020
- Authors: Samuel Ackerman; Eitan Farchi; Rami Katan; Orna Raz
- Reference count: 2
- Primary result: Combinatorial optimization methodology reduces LLM design space from ~200,000 to ~12 combinations while identifying dominant factors

## Executive Summary
This paper presents a combinatorial optimization-based methodology for designing high-quality LLM solutions by identifying key factors influencing solution quality and creating a small test plan that covers all desired factor interactions. The approach is particularly valuable when manual evaluation is time-consuming, as it reduces the design space from hundreds of thousands of combinations to a manageable number while maintaining coverage of important interactions. The methodology combines expert knowledge infusion with automated assistance for defining and covering the design space, enabling efficient identification of dominant factors through statistical analysis including ANOVA and logistic regression.

## Method Summary
The methodology involves identifying factors governing LLM solution quality (prompts, inputs, generation parameters, design alternatives), then applying combinatorial optimization to create a small subset P that ensures all desired interactions occur. Each element of P is developed into a complete benchmark configuration, evaluated (typically by human annotators), and analyzed statistically to identify significant factors and the best solution pipeline. The approach leverages the sparsity-of-effects principle to focus on low-order interactions while reducing the evaluation burden from potentially hundreds of thousands of combinations to around a dozen.

## Key Results
- Combinatorial optimization reduces ~200,000 possible parameter combinations to ~12 combinations while maintaining coverage of key factor interactions
- Statistical analysis (ANOVA and logistic regression) can identify the most influential factors with minimal data from the optimized test plan
- The methodology enables expert knowledge infusion while providing automated coverage of the design space, making it especially applicable when design and evaluation of each benchmark is time-consuming

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Combinatorial optimization reduces the design space from ~200,000 combinations to ~12 combinations while maintaining coverage of key factor interactions
- Mechanism: The method applies k-coverage (typically k=2 or k=3) to ensure that all important combinations of factor values are represented in the small test plan, leveraging the sparsity-of-effects principle that suggests only low-order interactions significantly impact solution quality
- Core assumption: The sparsity-of-effects principle holds true for LLM solution quality - that is, only a small subset of factors and their low-order interactions determine solution quality
- Evidence anchors:
  - [abstract] "combinatorial optimization is used to create a small subset P that ensures all desired interactions occur in P"
  - [section 3] "The sparsity-of-effects principle... tells us that we should expect the quality of the design of the LLM solution to be dominated by low order one, two or three interactions of parameters"
  - [corpus] No direct evidence found in corpus about sparsity-of-effects principle for LLM solutions; this is a general experimental design principle
- Break condition: If the sparsity-of-effects principle doesn't hold for the specific LLM use case, or if high-order interactions are critical, the optimization will miss important combinations and the reduced plan will be insufficient

### Mechanism 2
- Claim: Statistical analysis (ANOVA and logistic regression) can identify the most influential factors with minimal data
- Mechanism: By collecting scores on the small optimized test plan (P), statistical tests can determine which factors significantly affect solution quality and estimate their relative influence, allowing identification of the best parameter combination
- Core assumption: The small sample of combinations in P provides sufficient statistical power to detect significant effects and interactions
- Evidence anchors:
  - [abstract] "Statistical analysis, including ANOVA and logistic regression, is applied to evaluate parameter effects and identify the best solution pipeline"
  - [section 8] "we conduct a statistical analysis that has two main objectives: 1. Evaluate whether the average scores between various combinations are statistically significantly different"
  - [corpus] No direct evidence found in corpus about statistical analysis effectiveness for LLM optimization; this is standard statistical methodology
- Break condition: If the number of combinations in P is too small relative to the number of factors, or if the data is noisy, statistical tests may lack power to detect true effects or may produce false positives

### Mechanism 3
- Claim: The methodology enables expert knowledge infusion while providing automated coverage of the design space
- Mechanism: Domain experts identify relevant factors and define which interactions matter, then combinatorial optimization automatically generates the test plan that covers these interactions, combining human expertise with computational efficiency
- Core assumption: Domain experts can correctly identify the relevant factors and interactions that should be covered
- Evidence anchors:
  - [abstract] "The methodology allows infusion of expert knowledge, provides automated assistance for defining and covering the design space"
  - [section 2] "Identifying the factors that govern the LLM solution quality enables the infusion of subject matter expert knowledge"
  - [corpus] No direct evidence found in corpus about expert knowledge infusion effectiveness for LLM design
- Break condition: If experts miss critical factors or incorrectly specify which interactions matter, the optimization will create a test plan that misses important combinations, leading to suboptimal solutions

## Foundational Learning

- Concept: Combinatorial Design and Coverage
  - Why needed here: The paper relies on creating small test plans that maintain coverage of important factor interactions - understanding how combinatorial designs achieve this is essential
  - Quick check question: If you have 5 factors with 2-3 levels each, how many combinations would you need to test to ensure all 2-way interactions are covered using a combinatorial design approach?

- Concept: Statistical Significance Testing
  - Why needed here: The methodology uses statistical tests to determine if differences in solution quality between parameter combinations are meaningful rather than due to chance
  - Quick check question: What is the difference between a p-value and a confidence interval, and how would each be used in determining if one LLM parameter configuration is significantly better than another?

- Concept: Logistic Regression for Binary Outcomes
  - Why needed here: The paper uses logistic regression to model how factor levels influence the probability of a "good" vs "bad" solution quality outcome
  - Quick check question: In logistic regression, what does the odds ratio tell you about a factor's effect on the outcome, and how would you interpret a significant interaction term?

## Architecture Onboarding

- Component map: Factor Identification -> Combinatorial Optimization -> Benchmark Generation -> Evaluation -> Statistical Analysis -> Solution Selection
- Critical path:
  1. Expert identifies factors and their levels
  2. Combinatorial optimization generates test plan P
  3. Each p in P is developed into a complete benchmark
  4. Benchmarks are evaluated (human or automated)
  5. Statistical analysis identifies significant factors and best combination
  6. Optimal solution is selected and deployed

- Design tradeoffs:
  - Number of combinations in P vs. coverage completeness: More combinations provide better coverage but require more evaluation effort
  - Type of statistical test vs. assumptions: Different tests have different assumptions about data distribution and variance
  - Human vs. automated evaluation: Human evaluation provides nuanced quality assessment but is time-consuming and subjective
  - Depth of interaction coverage (k=2 vs k=3) vs. computational complexity: Higher-order coverage requires more combinations

- Failure signatures:
  - Suboptimal solution selection: If the combinatorial design missed important interactions or statistical analysis lacked power
  - Inconsistent results: If human evaluation is inconsistent or benchmarks are not properly implemented
  - Missing critical factors: If experts failed to identify all relevant factors during the identification phase
  - Overfitting: If the analysis identifies spurious correlations rather than true causal relationships

- First 3 experiments:
  1. Implement a simple 2-factor combinatorial design with 2 levels each (4 total combinations) to verify the optimization and coverage mechanics work correctly
  2. Run a full factorial on a small subset of factors to establish ground truth for statistical analysis accuracy
  3. Implement the full methodology on a toy LLM problem (e.g., prompt optimization for a simple task) to validate the end-to-end pipeline before applying to production use cases

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the methodology be extended to handle continuous interaction effects between factors rather than just discrete levels?
- Basis in paper: [explicit] The paper mentions that the approach can be applied to "continuous values as well, though in the running example the values are discrete" and uses discrete levels for factors like temperature and maxNewToken
- Why unresolved: The paper focuses on discrete factor levels and doesn't explore how continuous interactions would be handled or whether the combinatorial optimization approach needs modification for continuous spaces
- What evidence would resolve it: Experimental results showing the methodology's effectiveness with continuous factor values, or theoretical framework for extending the approach to continuous interactions

### Open Question 2
- Question: How does the proposed methodology compare to AutoML approaches in terms of solution quality when evaluation time is not a constraint?
- Basis in paper: [explicit] The paper states the approach "can also be used as a baseline to compare and validate an autoML approach" and is "especially applicable when the design and evaluation of each benchmark in P is time-consuming"
- Why unresolved: The paper positions the approach as complementary to AutoML for time-constrained scenarios but doesn't provide empirical comparisons when time is not a limiting factor
- What evidence would resolve it: Head-to-head comparison studies between the combinatorial optimization approach and AutoML methods across various problem domains with different evaluation time constraints

### Open Question 3
- Question: What is the optimal sample size per parameter combination needed to achieve statistically significant results while minimizing evaluation effort?
- Basis in paper: [explicit] The paper mentions "a sample of 20-30 code examples is a good number by the Central limit theorem" but also shows simulation results with 30 samples per combination
- Why unresolved: The paper provides a general guideline but doesn't systematically explore the relationship between sample size, statistical power, and evaluation effort across different problem domains and factor spaces
- What evidence would resolve it: Empirical studies examining statistical significance levels achieved with different sample sizes across multiple problem domains, providing concrete guidelines for optimal sample size selection

## Limitations
- The sparsity-of-effects assumption for LLM solutions is a critical unproven hypothesis that may not hold for complex LLM systems with non-linear interactions
- The methodology heavily depends on expert judgment for factor identification, which may introduce bias or miss important parameters
- The effectiveness of statistical analysis on small sample sizes (12 combinations) for detecting true effects in high-dimensional LLM parameter spaces is questionable

## Confidence
- Medium: The combinatorial optimization framework itself - well-established methodology with proven track record in experimental design
- Medium: The claim about reducing 200,000 combinations to 12 while maintaining coverage - depends on correct implementation and the sparsity assumption
- Low: The effectiveness of statistical analysis on small samples for identifying truly optimal LLM configurations - limited empirical validation in this specific domain

## Next Checks
1. **Ground Truth Validation**: Apply the methodology to a small-scale problem where full factorial testing is feasible, then compare the combinatorial optimization results against the ground truth to verify coverage completeness and accuracy of factor identification.

2. **Sparsity Assumption Test**: Systematically test whether LLM solution quality is actually dominated by low-order interactions by comparing results from k=2 coverage versus k=3 coverage on the same problem, measuring if higher-order interactions significantly impact outcomes.

3. **Statistical Power Analysis**: Conduct a formal power analysis to determine the minimum number of evaluations needed per combination to reliably detect effect sizes typical in LLM optimization, then verify the 12-combination approach meets these requirements.