---
ver: rpa2
title: From Real to Cloned Singer Identification
arxiv_id: '2407.08647'
source_url: https://arxiv.org/abs/2407.08647
tags:
- singer
- singers
- identification
- these
- music
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates singer identification methods for detecting
  cloned voices in music. Three embedding models are trained using singer-level contrastive
  learning, where positive pairs consist of segments with vocals from the same singers.
---

# From Real to Cloned Singer Identification

## Quick Facts
- arXiv ID: 2407.08647
- Source URL: https://arxiv.org/abs/2407.08647
- Reference count: 0
- Key outcome: Singer identification models perform well on real singers but significantly deteriorate when classifying cloned versions, especially models trained on mixtures.

## Executive Summary
This paper investigates singer identification methods for detecting cloned voices in music. Three embedding models are trained using singer-level contrastive learning, where positive pairs consist of segments with vocals from the same singers. The models differ in their input types: mixtures, vocals, or both. While all models perform well on real singers across multiple datasets, their performance significantly deteriorates when classifying cloned versions of singers, with mixture models showing the largest drop. The findings highlight the need to understand biases in singer identification systems and develop more robust approaches for voice deepfake detection.

## Method Summary
The study trains three transformer-based embedding models using singer-level contrastive learning with NT-Xent loss. Models are trained on 30,025 singers and fine-tuned on 7500 singers for classification. The key difference is input type: mixture models use full tracks, vocal models use separated vocal stems, and hybrid models use both. Data preprocessing involves filtering tracks with at least 75% vocal segments for training and 50% for evaluation. Vocal stems are extracted using Demucs source separation. Models are evaluated on real singers using FMA, MTG, and a closed dataset, and on cloned singers using a separate dataset of 67 singers.

## Key Results
- All three models achieve high accuracy (80-90% top-1) on real singer identification across multiple datasets
- Performance drops significantly when classifying cloned voices, especially for mixture models
- Vocal models outperform mixture models by a few percentage points on real singer identification
- Hybrid models show slightly better performance than mixture models, suggesting benefits from diverse input training

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Contrastive learning at singer level captures vocal identity features that transfer well to real singer identification
- Mechanism: Positive pairs from same singer's segments force model to learn identity-specific embeddings; negatives from different singers encourage discrimination
- Core assumption: Vocal segments contain enough singer-specific information to distinguish identity when paired appropriately
- Evidence anchors: "positive pairs consist of segments with vocals from the same singers whilst negatives come from different singers", "These pairs are drawn on the fly from different songs of the same singer"

### Mechanism 2
- Claim: Vocal stem input provides better singer discrimination than mixtures because it isolates the identity-relevant signal
- Mechanism: By removing instrumental interference, the model focuses learning on vocal characteristics that define singer identity
- Core assumption: Instrumental information adds noise rather than useful context for singer identification
- Evidence anchors: "The Vocal model outperforms the models that use mixtures as an input by a few percentage points", "separating vocals from the rest of the track clearly helps our models disambiguate singers between each other"

### Mechanism 3
- Claim: Training on both mixtures and vocals (Hybrid) provides robustness that balances pure vocal and context-aware identification
- Mechanism: Random sampling from both input types creates embeddings that can handle varied real-world scenarios
- Core assumption: Real-world deployment will encounter both separated and mixed audio, requiring flexible models
- Evidence anchors: "For the third... these segments are randomly sampled from either; the following positive pairings are possible during the contrastive learning task: vocal-vocal, vocal-mixture, mixture-vocal, and mixture-mixture", "The Hybrid model, which samples both mixtures and vocal stems during pre-training, is slightly better-performing than the mixture model"

## Foundational Learning

- Concept: Contrastive learning and triplet loss
  - Why needed here: The paper relies on NT-Xent loss for singer-level discrimination; understanding this is essential for grasping how embeddings are trained
  - Quick check question: What is the key difference between NT-Xent loss and triplet loss, and why might NT-Xent be preferred here?

- Concept: Music source separation
  - Why needed here: The study compares models using mixtures vs vocal stems, making source separation technology crucial to understanding the experimental setup
  - Quick check question: How does Demucs [27,28] separate vocals from mixtures, and what are its limitations for this application?

- Concept: Embedding evaluation metrics (top-1, top-5 accuracy)
  - Why needed here: The paper reports identification performance using these metrics; understanding them is necessary to interpret results
  - Quick check question: In a 1000-class singer identification task, what does a top-5 accuracy of 80% tell us about the model's performance?

## Architecture Onboarding

- Component map: Input preprocessing → Mel-spectrogram extraction → Transformer encoder → Projection head → Contrastive loss → Classifier head (frozen encoder)
- Critical path: Data loading → Spectrogram computation → Model forward pass → Loss computation → Backpropagation → Validation evaluation
- Design tradeoffs: Mixtures vs vocals (source separation cost vs identification accuracy), model complexity vs inference speed, training data diversity vs overfitting risk
- Failure signatures: Degraded performance on genres with heavy vocal effects, poor generalization to unseen singers, sensitivity to instrumental context
- First 3 experiments:
  1. Train baseline Mixture model on 100 singers, evaluate on FMA validation set
  2. Train Vocal model on same data, compare top-1 accuracy to Mixture baseline
  3. Train Hybrid model, evaluate whether it outperforms both Mixture and Vocal models on real singer identification

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can singer identification systems be made more robust to voice deepfakes and cloned voices?
- Basis in paper: The paper demonstrates that current models significantly deteriorate in performance when classifying cloned versions of singers, especially for models trained on mixtures
- Why unresolved: The paper highlights the need to understand biases in singer identification systems and suggests that models bias singers towards certain types of instrumentals, struggling to classify them in altered contexts
- What evidence would resolve it: Evidence could include developing and testing new models that incorporate more diverse training data, including cloned voices, and evaluating their performance on real and synthetic voices

### Open Question 2
- Question: What are the specific effects of vocal effects and musical context on singer identification performance?
- Basis in paper: The paper mentions that genres like Hip-hop, Pop, and Electronic, which use effects such as reverb and vocoder on singing voices, show lower performance in singer identification
- Why unresolved: The paper suggests that future works should aim to lessen the gap between genres, but does not provide specific methods or evidence to address this issue
- What evidence would resolve it: Evidence could include experiments that isolate and test the impact of specific vocal effects and musical contexts on singer identification performance, and developing methods to mitigate these effects

### Open Question 3
- Question: How can singer identification systems be adapted to handle singers with long discographies and evolving singing styles?
- Basis in paper: The paper notes that singers with more songs and longer careers are harder to classify, possibly due to changes in style, mixing effects, or singing voice over time
- Why unresolved: The paper suggests that future works should design systems that are more robust to singing voice evolution, but does not provide specific solutions or evidence
- What evidence would resolve it: Evidence could include developing models that incorporate temporal information and changes in singing style, and testing their performance on singers with varying discographies and career lengths

## Limitations

- The cloned voice dataset is much smaller (67 singers) than the real singer datasets (7500 singers), limiting generalizability
- The paper doesn't explore whether performance drop on cloned voices could be mitigated through architectural changes
- The analysis of why mixture models perform worse on cloned voices is speculative without controlled experiments

## Confidence

- **High confidence**: All models maintain strong performance on real singers (top-1 accuracy 80-90% on closed dataset)
- **Medium confidence**: Vocal models outperform mixture models, but lacks ablation studies to isolate exact mechanisms
- **Medium confidence**: Performance drops significantly on cloned voices, but doesn't conclusively establish if this reflects fundamental limitation or fixable bias

## Next Checks

1. Cross-synthesis evaluation: Test models on cloned voices generated by different synthesis methods to determine if performance degradation correlates with synthesis technique or is consistent across methods

2. Instrumental context ablation: Create controlled experiments where instrumental stems are progressively reintroduced to vocal-only models to quantify the exact contribution of instrumental information to identification performance

3. Embedding similarity analysis: Compute and compare cosine similarities between original and cloned voice embeddings to determine if the performance drop reflects fundamental changes in vocal identity representation or surface-level acoustic differences