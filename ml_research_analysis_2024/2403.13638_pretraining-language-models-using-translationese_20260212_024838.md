---
ver: rpa2
title: Pretraining Language Models Using Translationese
arxiv_id: '2403.13638'
source_url: https://arxiv.org/abs/2403.13638
tags:
- data
- synthetic
- language
- clean
- tasks
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The authors show that pre-training language models on translationese
  data filtered by small TinyLMs is almost as effective as training on clean monolingual
  data, with only 0.87% drop on NLU and 2.35% on NLG tasks. They train TinyLMs on
  clean low-resource corpora and use them to filter synthetic translations, showing
  that this approach closes much of the data gap between English and low-resource
  languages.
---

# Pretraining Language Models Using Translationese

## Quick Facts
- **arXiv ID**: 2403.13638
- **Source URL**: https://arxiv.org/abs/2403.13638
- **Authors**: Meet Doshi; Raj Dabre; Pushpak Bhattacharyya
- **Reference count**: 40
- **Key outcome**: Pre-training on filtered translationese data achieves only 0.87% drop in NLU and 2.35% drop in NLG tasks compared to clean monolingual data.

## Executive Summary
This paper proposes using translationese data filtered by small TinyLMs as an effective alternative to clean monolingual data for pretraining language models, particularly for low-resource languages. The authors demonstrate that this approach can achieve performance very close to models trained on clean data, with only minor relative drops in both natural language understanding and generation tasks. They also show that adding a small amount of clean data after synthetic pretraining can further close the performance gap. The method is validated across multiple Indic languages and scales to larger models like Llama-3-8B, suggesting its practical applicability for low-resource language modeling.

## Method Summary
The authors generate synthetic translationese data by translating English documents into target languages using IndicTrans2, then filter this data using TinyLMs trained on clean monolingual corpora based on perplexity scores. These filtered synthetic datasets are used to pretrain language models, which are then fine-tuned on downstream tasks. The approach is tested on both small models (28M-85M parameters) and scaled to larger models (Gemma-2B, Llama-3-8B). The filtering process aims to remove low-quality synthetic text while retaining fluent translationese, and the authors evaluate the effectiveness through NLU and NLG benchmarks across multiple Indic languages.

## Key Results
- Pre-training on filtered translationese data results in only 0.87% relative performance drop on NLU tasks and 2.35% on NLG tasks compared to clean data pretraining.
- Adding 10% clean data after synthetic pretraining further reduces the performance gap with clean data baselines.
- The approach scales effectively to larger models like Llama-3-8B while maintaining competitive performance.
- Source language choice impacts translationese quality, with macaronically rich languages like Hindi producing better synthetic data than less mixed languages like Gujarati.

## Why This Works (Mechanism)

### Mechanism 1
TinyLMs trained on clean data can effectively filter low-quality synthetic text by using perplexity as a quality score. Documents with high perplexity scores are likely to contain translationese artifacts and are filtered out, while documents with low perplexity scores are retained as high-quality synthetic data.

### Mechanism 2
Fine-tuning on a small amount of clean data after pretraining on synthetic data helps correct domain drift and re-aligns the model's distribution toward natural text patterns, effectively closing the performance gap with clean data pretraining.

### Mechanism 3
Languages with high intra-script code-mixing (like Hindi) produce better translationese because of higher lexical overlap with the target language, resulting in more natural translations with fewer artifacts compared to less mixed languages like Gujarati.

## Foundational Learning

- **Concept**: Perplexity as a proxy for document quality
  - Why needed here: Used to score and filter synthetic documents based on how "natural" they appear to a TinyLM.
  - Quick check question: What does a high perplexity score indicate about a document's fluency?

- **Concept**: Scaling laws for compute-optimal training
  - Why needed here: Guides the number of tokens used for pretraining models to avoid under/over-training.
  - Quick check question: What happens if you train a model with fewer tokens than the scaling law recommends?

- **Concept**: Back-translation and forward-translation in synthetic data generation
  - Why needed here: Explains the broader context of how synthetic data is typically created for NLP tasks.
  - Quick check question: How does forward translation differ from back translation in synthetic data generation?

## Architecture Onboarding

- **Component map**: TinyLM (filter) → Synthetic corpus → Main LM (pretrain) → Downstream fine-tuning
- **Critical path**: Generate translationese → Filter with TinyLM → Pretrain LM → Fine-tune on downstream tasks
- **Design tradeoffs**: Small TinyLMs are fast but may misclassify complex documents; larger models improve accuracy but increase cost.
- **Failure signatures**: Poor downstream performance indicates inadequate filtering; high variance in perplexity scores suggests TinyLM uncertainty.
- **First 3 experiments**:
  1. Train a TinyLM on clean Hindi data and use it to filter a small synthetic Hindi corpus generated from English; measure perplexity distribution.
  2. Pretrain a base LM on filtered synthetic data and compare NLU task performance to a clean data baseline.
  3. Add 10% clean data to the synthetic-pretrained model and re-evaluate to measure the closing of the performance gap.

## Open Questions the Paper Calls Out

### Open Question 1
How do the performance characteristics of translationese-filtered synthetic data vary across different scales of language models, particularly for models larger than 100B parameters? The study only demonstrates effectiveness up to Llama-3-8B, leaving scalability to extremely large models untested.

### Open Question 2
What are the long-term effects of using translationese data on the linguistic diversity and cultural nuances captured by language models? The study focuses on short-term performance metrics without addressing potential long-term impacts on linguistic and cultural richness.

### Open Question 3
How does the choice of source language for generating translationese data affect the quality and effectiveness of the synthetic data for different target languages? The study only examines a limited set of source and target language pairs, and the underlying reasons for performance differences are not fully explored.

## Limitations

- The reliance on perplexity-based filtering may not capture all dimensions of translationese quality and could introduce bias against certain types of natural text.
- The performance improvements are primarily measured against small baseline models, and effectiveness at scale for production-grade models remains to be fully established.
- The approach assumes consistent quality in translationese generation across different source languages, though observed differences suggest potential variability.

## Confidence

- **High Confidence**: Filtered translationese data can effectively substitute for clean monolingual data in pretraining, achieving minimal performance drops.
- **Medium Confidence**: Adding 10% clean data significantly closes the performance gap, though optimal proportions may vary.
- **Medium Confidence**: Hindi produces better translationese than Gujarati due to macaronic properties, but this may be specific to the Indic language context.
- **Low Confidence**: Scalability to much larger models and effectiveness across diverse language families remains speculative.

## Next Checks

1. Conduct an ablation study testing different TinyLM sizes and perplexity thresholds on the same synthetic corpus to measure their impact on downstream performance.

2. Train TinyLMs on one language and use them to filter synthetic data for another language to test cross-lingual generalization of the filtering approach.

3. After pretraining on filtered synthetic data, fine-tune models on clean data for extended periods to measure how quickly they recover performance compared to clean data baselines.