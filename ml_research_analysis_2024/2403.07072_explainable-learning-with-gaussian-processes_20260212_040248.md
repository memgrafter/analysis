---
ver: rpa2
title: Explainable Learning with Gaussian Processes
arxiv_id: '2403.07072'
source_url: https://arxiv.org/abs/2403.07072
tags:
- attributions
- attribution
- attri
- feature
- gaussian
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper develops a principled approach to feature attribution
  in Gaussian process regression (GPR) models using integrated gradients. The authors
  show that attributions under model uncertainty follow a Gaussian process distribution,
  quantifying the uncertainty arising from model uncertainty.
---

# Explainable Learning with Gaussian Processes

## Quick Facts
- arXiv ID: 2403.07072
- Source URL: https://arxiv.org/abs/2403.07072
- Authors: Kurt Butler; Guanchao Feng; Petar M. Djuric
- Reference count: 18
- Primary result: Develops principled approach to feature attribution in GPR models using integrated gradients, showing attributions follow a Gaussian process distribution that quantifies model uncertainty

## Executive Summary
This paper addresses the challenge of explainable machine learning in Gaussian process regression (GPR) models by developing a principled feature attribution framework using integrated gradients. The authors show that when integrated gradients are applied to GPR models, the resulting attributions themselves follow a Gaussian process distribution, naturally quantifying uncertainty arising from model uncertainty. They derive exact closed-form expressions for attributions using common GPR kernels like squared exponential and automatic relevance detection, demonstrating both superior accuracy and computational efficiency compared to existing approximation methods. The framework is validated through theoretical analysis and experiments on both synthetic and real-world datasets, including medical and industrial applications.

## Method Summary
The authors extend integrated gradients (IG) to GPR models by deriving exact expressions for feature attributions that preserve the Gaussian process properties of the original model. The method assumes the GPR model has a mean function in C¹(RD) and a covariance function in C²(RD×RD), under which the IG attributions also follow a GP distribution. Closed-form solutions are derived for squared exponential (SE) and automatic relevance detection squared exponential (ARD-SE) kernels, showing these exact expressions are more accurate and computationally efficient than numerical approximations like the right-hand rule. The approach quantifies attribution uncertainty through the variance of the attribution GP, enabling confidence-aware feature importance measures.

## Key Results
- IG attributions in GPR models preserve Gaussianity, following a GP distribution that quantifies attribution uncertainty
- Exact closed-form expressions for attributions are derived for SE and ARD-SE kernels, outperforming numerical approximations
- The framework demonstrates versatility and robustness through experiments on simulated data and real-world datasets (breast cancer prognosis, Taipei housing, wine quality)
- Attributions sum to prediction differences (completeness property) and uncertainty scales appropriately with model confidence

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Integrated gradients preserve Gaussianity of attributions in GPR models
- Mechanism: The attribution operator (computing IG attributions) is linear, and linear operations preserve Gaussianity of random processes
- Core assumption: The GPR model has a mean function in C¹(RD) and a covariance function in C²(RD×RD)
- Evidence anchors:
  - [abstract]: "When using integrated gradients as an attribution method, we show that the attributions of a GPR model also follow a Gaussian process distribution"
  - [section]: "Theorem 1 (Integrated gradients preserve Gaussianity) If F ∼ GP (m, k), where we assume m ∈ C1(RD) and k ∈ C2(RD × RD), then the IG attributions follow a GP"
  - [corpus]: Found 25 related papers, but no direct evidence of this specific mechanism preservation claim
- Break condition: If the GPR mean or covariance functions lack sufficient differentiability (C¹ or C² respectively), the Gaussianity preservation fails

### Mechanism 2
- Claim: Exact expressions for attributions are more accurate and computationally efficient than approximations
- Mechanism: The paper derives closed-form expressions for common GPR kernels (SE, ARD-SE) that directly compute attributions without numerical integration
- Core assumption: The kernel functions admit tractable analytical forms for the required integrals
- Evidence anchors:
  - [abstract]: "We also show that, when applicable, the exact expressions for GPR attributions are both more accurate and less computationally expensive than the approximations currently used in practice"
  - [section]: "Using the attribution GP, we gain the ability to measure the confidence of each attribution"
  - [corpus]: Weak evidence - only 25 related papers found, none specifically comparing exact vs approximate GPR attributions
- Break condition: For kernels without tractable analytical forms, exact expressions may not exist and numerical approximations become necessary

### Mechanism 3
- Claim: Attributions inherit uncertainty from the GPR model, providing confidence quantification
- Mechanism: Since attributions follow a GP distribution, their variance at any point quantifies uncertainty arising from model uncertainty
- Core assumption: The GPR posterior predictive distribution has non-zero variance
- Evidence anchors:
  - [abstract]: "which quantifies the uncertainty in attribution arising from uncertainty in the model"
  - [section]: "We also derive exact expressions for the attributions of various common GPR models"
  - [corpus]: Weak evidence - related papers focus on GPR uncertainty but not specifically on attribution uncertainty
- Break condition: If the GPR model has zero variance (perfectly certain predictions), attribution uncertainty would also be zero

## Foundational Learning

- Concept: Gaussian Process Regression fundamentals
  - Why needed here: The paper builds on GPR theory to derive attribution expressions
  - Quick check question: What are the two key functions that define a Gaussian process?

- Concept: Integrated gradients attribution method
  - Why needed here: IG is the specific attribution method extended to GPR models
  - Quick check question: What are the seven axioms that uniquely define IG attributions?

- Concept: Feature attribution and completeness property
  - Why needed here: The paper works within the feature attribution framework where attributions must sum to prediction difference
  - Quick check question: How is the completeness property expressed mathematically for feature attributions?

## Architecture Onboarding

- Component map: GPR model with kernel functions (SE, ARD-SE) -> Attribution operator implementing IG method -> Closed-form derivation engine for specific kernels -> Numerical integration module for comparison -> Experiment validation framework

- Critical path: GPR model → Attribution computation → Uncertainty quantification → Validation experiments

- Design tradeoffs:
  - Exact expressions vs numerical approximations (accuracy vs generality)
  - Different kernel choices (computational complexity vs expressiveness)
  - Baseline selection (sensitivity vs interpretability)

- Failure signatures:
  - Attribution uncertainty zero despite model uncertainty
  - Attributions not summing to prediction difference
  - Numerical integration failing to converge to exact results

- First 3 experiments:
  1. Verify attributions sum to prediction difference for simple synthetic data
  2. Compare exact vs numerical integration attributions on SE kernel
  3. Test uncertainty scaling with distance from baseline in simulated heteroscedastic data

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What are the practical limitations of applying GPR-based feature attributions in real-time applications, and how can computational efficiency be improved?
- Basis in paper: [explicit] The paper discusses the computational expense of exact GPR attributions and the need for approximations like RFGPs.
- Why unresolved: The paper mentions that exact GPR attributions are more computationally efficient than approximations but does not provide a detailed analysis of real-time application scenarios or optimization techniques.
- What evidence would resolve it: Experimental comparisons of exact and approximate GPR attributions in real-time settings, along with optimization strategies for computational efficiency.

### Open Question 2
- Question: How does the choice of baseline in integrated gradients affect the interpretability of GPR attributions, and are there principled methods to select the optimal baseline?
- Basis in paper: [explicit] The paper acknowledges that feature attributions are sensitive to the choice of baseline and mentions the use of the average feature vector as a simple approach.
- Why unresolved: The paper does not explore the impact of different baseline selection strategies on the interpretability of GPR attributions or propose methods to determine the optimal baseline.
- What evidence would resolve it: Comparative studies of different baseline selection methods and their effects on attribution interpretability, along with guidelines for selecting the optimal baseline.

### Open Question 3
- Question: Can the principles of GPR-based feature attributions be extended to other non-parametric Bayesian models, such as Dirichlet processes or hierarchical Bayesian models?
- Basis in paper: [inferred] The paper focuses on GPR but discusses the preservation of Gaussianity under linear operators, which could potentially be applied to other models.
- Why unresolved: The paper does not explore the applicability of GPR-based attribution principles to other non-parametric Bayesian models or provide theoretical extensions.
- What evidence would resolve it: Theoretical derivations and experimental validations of feature attribution methods for other non-parametric Bayesian models, demonstrating the generalization of GPR-based principles.

## Limitations
- Exact expressions are only derived for SE and ARD-SE kernels; applicability to other kernels remains uncertain
- Limited empirical validation with only three real-world datasets, potentially constraining generalizability
- The comparison with existing approximations is not comprehensive across different kernel types
- Assumes sufficient differentiability (C¹ and C²) of GPR mean and covariance functions, which may not hold for all kernel choices

## Confidence

- High confidence in the theoretical framework preserving Gaussianity through integrated gradients (Mechanism 1)
- Medium confidence in computational efficiency claims due to limited empirical comparison (Mechanism 2)
- Medium confidence in uncertainty quantification validity across diverse scenarios (Mechanism 3)

## Next Checks

1. Test the attribution GP framework with non-differentiable kernels (e.g., Matérn kernels) to verify the C¹/C² assumptions are necessary
2. Benchmark exact vs numerical attributions across a broader set of kernel families beyond SE and ARD-SE
3. Evaluate attribution uncertainty behavior on synthetic data with controlled heteroscedasticity patterns