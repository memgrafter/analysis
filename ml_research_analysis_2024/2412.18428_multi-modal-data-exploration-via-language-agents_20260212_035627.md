---
ver: rpa2
title: Multi-Modal Data Exploration via Language Agents
arxiv_id: '2412.18428'
source_url: https://arxiv.org/abs/2412.18428
tags:
- data
- image
- task
- system
- m2ex
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "The paper addresses the challenge of querying heterogeneous multi-modal\
  \ data (structured tables, text, images) using natural language, a problem that\
  \ remains largely unexplored. M\xB2EX proposes an LLM-based agentic framework that\
  \ decomposes complex queries into sub-tasks, orchestrates modality-specific experts,\
  \ and executes them in an efficient directed acyclic task graph (DAG)."
---

# Multi-Modal Data Exploration via Language Agents

## Quick Facts
- arXiv ID: 2412.18428
- Source URL: https://arxiv.org/abs/2412.18428
- Reference count: 23
- Primary result: M²EX outperforms state-of-the-art multi-modal exploration systems in accuracy (up to 42% improvement) and latency (up to 51% reduction) on datasets ArtWork, RotoWire, and EHRXQA

## Executive Summary
M²EX is an LLM-based agentic framework for querying heterogeneous multi-modal data (structured tables, text, images) using natural language. The system decomposes complex queries into sub-tasks, orchestrates modality-specific experts, and executes them in an efficient directed acyclic task graph (DAG). By leveraging smart planning, parallel execution, and self-debugging mechanisms, M²EX achieves superior performance in both accuracy and various performance metrics including query latency, API costs, and planning efficiency compared to state-of-the-art systems like CAESURA and NeuralSQL.

## Method Summary
M²EX uses an LLM-based agentic framework to decompose natural language questions into subtasks such as text-to-SQL generation and image analysis, orchestrating modality-specific experts in an efficient query plan. The system constructs a directed acyclic task graph (DAG) to enable parallel execution of independent tasks while maintaining correct dependencies. After each expert tool executes, it validates its output and performs selective re-planning if errors are detected, avoiding full workflow restarts. The framework supports zero-shot cross-domain generalization using a single prompt set without in-context examples, and incorporates memory state to track intermediate results for iterative exploration and error correction.

## Key Results
- Achieved up to 42% higher answer accuracy than CAESURA and NeuralSQL on ArtWork, RotoWire, and EHRXQA datasets
- Reduced query latency by up to 51% through parallel execution and selective re-planning
- Demonstrated 7% to 13% improvement in planning efficiency across all datasets while maintaining or improving accuracy

## Why This Works (Mechanism)

### Mechanism 1
M²EX achieves superior accuracy by decomposing complex multi-modal queries into parallelizable sub-tasks organized in a Directed Acyclic Graph (DAG), enabling independent execution without unnecessary intermediate dependencies. The planner uses LLM-based reasoning to break down the user query into atomic sub-tasks (e.g., text-to-SQL, image analysis) and builds a DAG where nodes are (tool, args) pairs. Edges encode data dependencies, and parallel execution minimizes latency.

**Core assumption**: Sub-tasks can be isolated without losing semantic intent, and LLM can correctly identify parallelizable branches.

### Mechanism 2
Self-debugging with selective re-planning reduces latency and token usage by avoiding full workflow restarts. After each expert tool executes, it validates its output. If the output fails validation, only the affected sub-DAG is re-planned and re-executed rather than the entire workflow.

**Core assumption**: Errors are localized and do not cascade to unrelated sub-tasks.

### Mechanism 3
Zero-shot cross-domain generalization allows M²EX to perform well on unseen datasets without in-context examples. The LLM planner uses a single prompt set to decompose and plan queries across domains (ArtWork, RotoWire, EHRXQA), without requiring dataset-specific tuning or examples.

**Core assumption**: The LLM's reasoning generalizes well enough to handle domain-specific semantics and tool orchestration.

## Foundational Learning

- **Concept**: Directed Acyclic Graphs (DAGs)
  - Why needed here: DAGs model task dependencies and enable parallel execution while preventing circular dependencies that would cause infinite loops.
  - Quick check question: Can you explain why a DAG is preferred over a linear workflow for multi-modal query execution?

- **Concept**: Tool orchestration via LLM reasoning
  - Why needed here: LLMs dynamically select and sequence expert models (text-to-SQL, VQA, plotting) based on query intent, enabling flexible multi-modal handling.
  - Quick check question: How does LLM-based tool selection differ from static rule-based pipelines?

- **Concept**: Self-debugging and selective re-planning
  - Why needed here: Localized error correction reduces computational cost and latency by avoiding full workflow restarts.
  - Quick check question: What distinguishes selective re-planning from full re-planning in terms of performance impact?

## Architecture Onboarding

- **Component map**: User Query → Agent Core (LLM) → Planner → DAG → Task Executors (tools) → Memory State → Final Answer
- **Critical path**: 1. Query decomposition (planning) 2. DAG construction 3. Parallel execution of independent tasks 4. Validation and selective re-planning if needed 5. Result synthesis
- **Design tradeoffs**: Parallelism vs. dependency correctness (over-parallelizing can corrupt results if dependencies are missed); Zero-shot generalization vs. domain accuracy (no in-context examples speeds deployment but may reduce precision on specialized tasks); Token efficiency vs. reasoning depth (shorter prompts reduce cost but may limit complex reasoning)
- **Failure signatures**: DAG construction errors (tasks execute out of order or in parallel when sequential execution is required); Tool selection failures (wrong model called for task); Image analysis errors (especially for categorical outputs in medical datasets); Re-planning loops (selective re-planning fails to converge if errors cascade)
- **First 3 experiments**: 1. Single-modal query test: Run a text-only query on ArtWork to validate text2SQL and data preparation pipelines. 2. Multi-modal parallel execution: Execute a query requiring image analysis and SQL in parallel; verify DAG correctness. 3. Error injection test: Force an image analysis error and confirm selective re-planning re-executes only the affected sub-DAG.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can M²EX's performance be improved when handling categorical outputs in image analysis tasks, given that errors in this area are nearly three times higher than those in binary outputs?
- Basis in paper: The error analysis shows that 36 out of 49 errors in EHRXQA are due to inaccurate image analysis in categorical tasks, while only 13 errors are linked to binary outputs.
- Why unresolved: The paper identifies the M3AE model as the bottleneck but does not explore alternative image processing approaches or improvements to the visual pipeline.
- What evidence would resolve it: Experimental results comparing M²EX's performance with alternative image models or enhanced visual reasoning techniques, particularly for categorical outputs.

### Open Question 2
- Question: Can M²EX's planning efficiency be further optimized to handle larger datasets and additional modalities like video, while maintaining accuracy and reducing latency?
- Basis in paper: The paper mentions future work will focus on planning efficiency and scaling to larger datasets and new modalities, but does not provide concrete solutions or experiments.
- Why unresolved: The current implementation is tested on reduced datasets due to hardware limitations, and the paper does not explore strategies for scaling up.
- What evidence would resolve it: Performance benchmarks of M²EX on larger datasets and with video data, demonstrating maintained or improved accuracy and efficiency.

### Open Question 3
- Question: How does the choice of language model (e.g., GPT-4o vs. other LLMs) impact M²EX's performance in text interpretation and overall accuracy?
- Basis in paper: The paper restricts experiments to GPT-4o for comparability with recent studies, but acknowledges that further improvements in nuanced language understanding are needed.
- Why unresolved: The paper does not explore the impact of different LLMs on M²EX's performance, particularly in text analysis and SQL generation.
- What evidence would resolve it: Comparative experiments using different LLMs within M²EX, measuring accuracy, latency, and cost across various tasks and datasets.

## Limitations

- Performance degradation on categorical image analysis tasks, with errors nearly three times higher than binary outputs, due to limitations in the M3AE model
- Zero-shot cross-domain generalization may struggle with specialized domains requiring domain-specific knowledge without in-context examples
- Current implementation tested on reduced datasets due to hardware constraints, limiting scalability validation

## Confidence

- **High confidence**: DAG-based parallel execution framework and its impact on latency reduction
- **Medium confidence**: Accuracy improvements over baselines (methodology sound but limited dataset scope)
- **Low confidence**: Zero-shot cross-domain generalization claims (insufficient evidence across diverse domains)

## Next Checks

1. **Domain generalization test**: Apply M²EX to a medical dataset (e.g., MIMIC-IV) without in-context examples to evaluate zero-shot performance on specialized terminology
2. **Error cascade analysis**: Systematically inject sequential dependencies in multi-modal workflows to test whether selective re-planning can identify root causes in cascading failures
3. **Token efficiency audit**: Measure actual token consumption during planning vs. execution phases to validate claimed efficiency gains over baseline systems