---
ver: rpa2
title: Graph is all you need? Lightweight data-agnostic neural architecture search
  without training
arxiv_id: '2405.01306'
source_url: https://arxiv.org/abs/2405.01306
tags:
- graph
- neural
- architectures
- nasgraph
- architecture
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proposes NASGraph, a training-free and data-agnostic
  neural architecture search (NAS) method that converts neural architectures into
  directed acyclic graphs (DAGs) and uses graph-theoretic measures, particularly average
  degree, as proxy metrics to rank architectures. By treating inputs to neural components
  as graph nodes and determining connectivity based on forward propagation outputs,
  the method avoids the computational cost of training candidate models.
---

# Graph is all you need? Lightweight data-agnostic neural architecture search without training

## Quick Facts
- **arXiv ID**: 2405.01306
- **Source URL**: https://arxiv.org/abs/2405.01306
- **Reference count**: 40
- **Primary result**: NASGraph achieves competitive ranking correlations on NAS-Bench-101, NAS-Bench-201, Micro TransNAS-Bench-101, and NDS benchmarks using graph-theoretic measures without training

## Executive Summary
NASGraph is a training-free and data-agnostic neural architecture search (NAS) method that converts neural architectures into directed acyclic graphs (DAGs) and uses graph-theoretic measures, particularly average degree, as proxy metrics to rank architectures. By treating inputs to neural components as graph nodes and determining connectivity based on forward propagation outputs, the method avoids the computational cost of training candidate models. Experiments show NASGraph achieves competitive Spearman and Kendall ranking correlations across multiple NAS benchmarks, outperforming or matching existing training-free methods. The average degree metric exhibits the lowest bias toward operations among compared methods. Combining NASGraph's graph measure with data-dependent metrics like jacob_cov further improves ranking accuracy. NASGraph runs efficiently on CPUs and maintains stable rankings across different random initializations, demonstrating its effectiveness for lightweight, data-agnostic NAS.

## Method Summary
NASGraph converts neural architectures into DAGs by performing forward propagation with an all-ones input, where edges are created between nodes only if the corresponding channel's output is non-zero. The method computes graph measures (average degree, density, resilience parameter, wedge count) from these DAGs and uses them as proxy metrics to rank architectures. The approach is training-free, data-agnostic, and computationally lightweight, running efficiently on CPUs. The core insight is that edge presence encodes non-zero output contributions from forward propagation, making average degree a proxy for architectural expressiveness. The method also demonstrates that combining graph measures with data-dependent metrics like jacob_cov improves ranking accuracy by capturing complementary aspects of architecture quality.

## Key Results
- NASGraph achieves Spearman correlations up to 0.85 on CIFAR-10/CIFAR-100 when combined with jacob_cov
- Average degree metric shows the lowest bias toward operations among graph measures
- Method runs efficiently on CPUs without requiring training data
- Stable rankings across different random initializations

## Why This Works (Mechanism)

### Mechanism 1
The average degree of the DAG proxy correlates with neural network performance because edge presence encodes non-zero output contributions from forward propagation. NASGraph maps each channel in each layer to a node; an edge is created only if the corresponding channel's output is non-zero when the input channel is activated with an all-ones vector. Thus, a higher average degree reflects richer inter-channel signal flow, which is indicative of expressive architectures. The core assumption is that ReLU ensures non-negative outputs; zero output signals a useless connection, while any positive output indicates a meaningful link. This mapping is bijective and stable under random initialization.

### Mechanism 2
Data-agnostic operation of NASGraph avoids bias from dataset-specific trends, leading to more stable rankings across benchmarks. Since NASGraph uses only the architecture's own forward pass on an all-ones input, no training data or label is required. This eliminates dataset-induced performance variance and keeps the proxy metric invariant to data distribution. The core assumption is that the forward pass with an all-ones input sufficiently probes the network's structure to yield discriminative scores; no dataset-dependent gradient or Jacobian information is needed.

### Mechanism 3
Combining avg_deg with jacob_cov improves ranking because they capture complementary aspects of architecture quality: structural connectivity vs gradient flow. avg_deg reflects the static graph connectivity derived from forward propagation, while jacob_cov measures sensitivity of outputs to inputs via Jacobians. Their rank-sum balances static expressiveness with dynamic adaptability. The core assumption is that these two metrics are not highly correlated and thus provide orthogonal information; their combination yields a more accurate proxy than either alone.

## Foundational Learning

- **Directed acyclic graph (DAG) representation of neural networks**: Why needed - NASGraph explicitly converts neural architectures into DAGs; understanding node-edge mapping is essential for implementing the conversion. Quick check - How does the paper determine whether to place an edge between two nodes in the DAG?
- **ReLU activation properties**: Why needed - The edge creation rule depends on ReLU's non-negativity; zero output implies no edge, positive output implies edge. Quick check - What would happen to the graph conversion if ReLU were replaced with a linear activation?
- **Graph measures (average degree, density, resilience, wedge count)**: Why needed - These metrics are used to rank architectures; choosing the right measure affects ranking quality. Quick check - Which graph measure yielded the highest Spearman correlation on NAS-Bench-101?

## Architecture Onboarding

- **Component map**: Neural architecture -> Forward pass with all-ones input -> DAG conversion -> Graph measure computation -> Architecture ranking
- **Critical path**: 1) Parse architecture definition (cell/module structure) 2) For each block, run forward pass with masked inputs to compute channel-wise outputs 3) Create edges where output ≠ all-zeros 4) Build DAG and compute chosen graph measure 5) Rank architectures and select top candidate(s)
- **Design tradeoffs**: Surrogate model depth vs accuracy (fewer cells/channels speeds up conversion but may reduce proxy fidelity); Edge direction handling (treating DAG as undirected for avg_deg simplifies computation but may lose directional nuance); Batch dimension usage (using channel count as batch size enables parallel computation but deviates from standard BN semantics)
- **Failure signatures**: All architectures map to identical DAGs (conversion logic bug, e.g., missing mask application); Very low ranking correlation (metric not discriminative; try a different graph measure); High variance across random seeds (insufficient probing; consider averaging over multiple initializations)
- **First 3 experiments**: 1) Convert a simple CNN from NAS-Bench-201 to DAG and manually verify edge creation logic 2) Compare avg_deg vs density rankings on a small subset of architectures 3) Test the impact of surrogate model depth (1 vs 3 cells) on ranking stability

## Open Questions the Paper Calls Out

- **How do graph-theoretic measures from NASGraph compare to gradient-based methods on more complex search spaces like NAS-Bench-1Shot1 or DARTS?**: The paper shows competitive results on NAS-Bench-101, NAS-Bench-201, and Micro TransNAS-Bench-101 but does not test on more complex or larger search spaces like NAS-Bench-1Shot1 or DARTS. Testing NASGraph on these benchmarks and comparing Spearman/Kendall correlations to gradient-based NAS methods would resolve this.
- **Can the average degree metric be further improved by incorporating node centrality measures like betweenness or closeness centrality?**: The paper uses average degree and mentions other graph measures but does not explore centrality-based measures. Evaluating the ranking performance of betweenness or closeness centrality on NAS benchmarks and comparing to average degree would resolve this.
- **What is the impact of graph conversion sensitivity to specific neural architecture designs, such as skip connections or parallel branches?**: The authors discuss how skip connections affect the frequency distribution in operation preference but do not analyze how the graph conversion handles complex architectural patterns. Analyzing the graph representations of architectures with different skip connection patterns or parallel branches and assessing ranking stability across these designs would resolve this.

## Limitations
- Performance on more complex search spaces beyond established benchmarks remains untested
- Limited sensitivity analysis with only three random seeds per configuration
- No comparison with state-of-the-art training-based NAS methods on the same benchmarks
- Theoretical stability of DAG conversion across diverse architecture families unverified

## Confidence

- **High confidence**: The experimental methodology for computing ranking correlations (Spearman's ρ and Kendall's τ) is standard and reproducible. The results on established NAS benchmarks are credible and directly comparable to existing methods.
- **Medium confidence**: The claim that average degree is the least biased graph measure toward operations is supported by the data but relies on comparisons only with three other graph measures. The specific ranking of graph measures may vary with different architecture sets.
- **Low confidence**: The paper's core claim—that graph-theoretic measures can effectively rank neural architectures without training—rests on several untested assumptions about the stability of DAG conversion across diverse architecture families and the method's performance on real-world NAS scenarios.

## Next Checks
1. Verify edge creation logic by manually checking DAG conversion for a simple CNN architecture from NAS-Bench-201
2. Reproduce Spearman correlation results on NAS-Bench-101 using average degree metric
3. Test ranking stability by computing correlations across different random initializations (5+ seeds)