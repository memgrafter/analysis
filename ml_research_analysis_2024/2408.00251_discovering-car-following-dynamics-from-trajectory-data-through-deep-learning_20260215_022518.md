---
ver: rpa2
title: Discovering Car-following Dynamics from Trajectory Data through Deep Learning
arxiv_id: '2408.00251'
source_url: https://arxiv.org/abs/2408.00251
tags:
- expression
- data
- car-following
- variable
- expressions
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study applies deep learning techniques to discover the governing
  mathematical expressions of car-following dynamics from trajectory data. The proposed
  VIS-enhanced DSR framework integrates deep symbolic regression with variable interaction
  selection to find interpretable and parsimonious expressions.
---

# Discovering Car-following Dynamics from Trajectory Data through Deep Learning

## Quick Facts
- arXiv ID: 2408.00251
- Source URL: https://arxiv.org/abs/2408.00251
- Authors: Ohay Angah; James Enouen; Xuegang; Ban; Yan Liu
- Reference count: 15
- This study applies deep learning techniques to discover the governing mathematical expressions of car-following dynamics from trajectory data.

## Executive Summary
This paper introduces a VIS-enhanced deep symbolic regression (DSR) framework that discovers interpretable mathematical expressions governing car-following dynamics from trajectory data. The method integrates variable interaction selection (VIS) with deep symbolic regression to identify strong variable interactions and use them as prior knowledge for guiding expression exploration. The framework includes two penalty terms in the reward function: a complexity penalty to encourage parsimonious expressions and a variable interaction penalty to focus on physically meaningful variable combinations. Experiments on simulated car-following data demonstrate the method's ability to recover target expressions accurately under noise-free conditions and up to moderate noise levels.

## Method Summary
The proposed framework combines deep symbolic regression with variable interaction selection to discover governing equations of car-following dynamics. VIS first identifies strong variable interactions by training a deep neural network on trajectory data and measuring interaction strengths using the Archipelago method based on Hessian approximation. These interactions guide DSR, which uses an LSTM neural network to sample mathematical tokens according to a reward function that includes complexity and variable interaction penalties. The framework is tested on simulated car-following data generated using the Krauss model with varying noise levels, comparing performance against standard DSR and DSR with genetic programming assistance.

## Key Results
- The VIS-enhanced DSR framework recovers target expressions accurately under noise-free conditions
- Performance remains effective up to moderate noise levels (around 8%)
- The method outperforms standard DSR and DSR with GP assistance in running time and expression parsimony while maintaining accuracy

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The VIS-enhanced DSR framework discovers interpretable car-following models by combining variable interaction selection with deep symbolic regression.
- Mechanism: VIS identifies strong variable interactions from trajectory data, providing prior knowledge to guide DSR toward physically meaningful expressions. This reduces search space and improves interpretability.
- Core assumption: Variable interactions in car-following data can be reliably detected through neural network training and Archipelago interaction strength measurement.
- Evidence anchors:
  - [abstract] "We propose an expression exploration framework based on deep symbolic regression (DSR) integrated with a variable intersection selection (VIS) method to find variable combinations that encourage interpretable and parsimonious mathematical expressions."
  - [section] "In VIS, a deep neural network is first trained to fit the data... The interaction strengths of variables... can be measured by the Archipelago... which uses the approximation of the Hessian of f_dnn(X)"
  - [corpus] Weak evidence - corpus focuses on trajectory prediction, not model discovery

### Mechanism 2
- Claim: Two penalty terms in the reward function enable efficient discovery of parsimonious expressions.
- Mechanism: Complexity penalty normalizes expression complexity to encourage simpler models, while variable interaction penalty focuses search on recommended combinations. This balances accuracy with interpretability.
- Core assumption: Parsimony and physical interpretability can be encoded through mathematical penalty terms in the reward function.
- Evidence anchors:
  - [abstract] "two penalty terms are added to improve the reward function: (i) a complexity penalty to regulate the complexity of the explored expressions to be parsimonious, and (ii) a variable interaction penalty to encourage the expression exploration to focus on variable combinations that can best describe the data."
  - [section] "We redesign the original reward function in Petersen et al. [2019] to encourage the LSTM to (i) explore expressions consisting of the recommended variable combinations generated by VIS, and (ii) explore expressions with reasonable expression complexity."
  - [corpus] No direct evidence - corpus papers focus on prediction, not model discovery

### Mechanism 3
- Claim: Risk-seeking policy gradient training enables efficient exploration of symbolic expressions.
- Mechanism: LSTM neural network samples mathematical tokens according to probabilities updated by top-performing cases rather than average performance, accelerating discovery of high-reward expressions.
- Core assumption: Top-performing cases provide better learning signals than average cases for symbolic regression problems.
- Evidence anchors:
  - [section] "We use an LSTM neural network to sample tokens. The LSTM is trained through the risk-seeking policy gradient algorithm proposed by Petersen et al. [2019], which updates the LSTM weights through the top-performing cases (instead of the average-performing cases as normally done)."
  - [abstract] "We propose an expression exploration framework based on deep symbolic regression (DSR)"
  - [corpus] No direct evidence - corpus focuses on trajectory prediction models, not symbolic regression

## Foundational Learning

- Concept: Symbolic regression for discovering governing equations
  - Why needed here: Traditional car-following models are expressed as mathematical equations; symbolic regression can discover similar interpretable expressions from data
  - Quick check question: What distinguishes symbolic regression from standard regression techniques?

- Concept: Variable interaction selection using neural networks
  - Why needed here: Identifying which variables interact strongly helps constrain the search space for meaningful car-following models
  - Quick check question: How does the Archipelago method measure interaction strength between variables?

- Concept: Risk-seeking policy gradient optimization
  - Why needed here: Standard gradient methods may converge to local optima; risk-seeking approach explores more aggressively for better expressions
  - Quick check question: Why might updating weights based on top-performing cases be better than average cases for this problem?

## Architecture Onboarding

- Component map: Trajectory data -> VIS neural network training -> Interaction strength calculation -> Recommended variable combinations -> DSR with LSTM -> Token sampling -> Expression evaluation -> Reward calculation -> LSTM weight update
- Critical path: Trajectory data → VIS neural network training → Interaction strength calculation → Recommended variable combinations → DSR with LSTM → Token sampling → Expression evaluation → Reward calculation → LSTM weight update
- Design tradeoffs: Expressiveness vs. interpretability (complex models fit better but are harder to understand), exploration vs. exploitation (search broadly vs. refine known good solutions), accuracy vs. parsimony (penalties for complexity may reduce fit quality)
- Failure signatures: Slow convergence (poor reward function tuning), overly complex expressions (weak complexity penalty), incorrect variable combinations (poor VIS performance), failure to recover target expressions (insufficient search space or constraints)
- First 3 experiments:
  1. Run VIS on clean Krauss model data to verify interaction detection works (should identify {vf(t), vl(t), sf(t), ds(t)} as key variables)
  2. Test DSR-only on noise-free data to establish baseline performance without VIS guidance
  3. Run VIS-DSR-GP with β=0.15 on clean data to verify improved convergence over DSR-only and DSR-GP

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the reward function be designed to effectively balance accuracy, interpretability, parsimony, and generalizability when learning governing equations from trajectory data?
- Basis in paper: [explicit] The authors discuss the need for balancing multiple objectives in learning governing equations and suggest defining specific measures for interpretability, parsimony, and generalizability, but acknowledge this is challenging.
- Why unresolved: The paper recognizes the importance of balancing these objectives but does not provide a concrete solution or reward function design that achieves this balance effectively.
- What evidence would resolve it: A well-defined reward function that incorporates metrics for accuracy, interpretability, parsimony, and generalizability, along with experimental results demonstrating its effectiveness in learning interpretable and generalizable governing equations.

### Open Question 2
- Question: How can the proposed framework be extended to handle traffic data with multiple vehicle classes and varying parameters across classes?
- Basis in paper: [inferred] The authors mention that real-world traffic streams often contain multiple vehicle classes with different parameters, and their initial testing shows this can be challenging for the proposed learning method.
- Why unresolved: The paper does not provide a solution or detailed discussion on how to handle the complexities introduced by multiple vehicle classes and varying parameters.
- What evidence would resolve it: An extension of the framework that can effectively learn governing equations from multimodal traffic data with multiple vehicle classes, along with experimental results demonstrating its performance on such data.

### Open Question 3
- Question: How can advanced deep learning techniques, such as large language models, be leveraged to improve the efficiency and quality of expression search in discovering governing equations?
- Basis in paper: [explicit] The authors suggest that leveraging the interaction and memory capabilities of large language models could enhance search efficiency and expression quality, but note that this needs further exploration and improvement.
- Why unresolved: The paper does not provide a concrete implementation or detailed discussion on how to integrate large language models into the framework for equation discovery.
- What evidence would resolve it: An implementation of the framework that incorporates large language models for expression search, along with experimental results demonstrating improved efficiency and quality of discovered governing equations compared to the current approach.

## Limitations

- The framework's performance degrades significantly with noise levels exceeding 8%, where discovered expressions become increasingly complex and deviate from target structure.
- The study relies on simulated data using the Krauss model rather than real-world trajectory data, which may not capture full complexity of actual driving behaviors.
- The effectiveness of penalty terms requires careful tuning, as improper values could either oversimplify models or prevent convergence entirely.

## Confidence

**High confidence** in the core methodology of combining VIS with DSR for symbolic regression, supported by clear mathematical formulations and established techniques from referenced literature.

**Medium confidence** in the practical effectiveness across different noise levels and data conditions. While results show good performance up to moderate noise (8%), the framework's robustness to higher noise levels and real-world data remains uncertain.

**Low confidence** in the scalability to more complex car-following scenarios involving additional variables, nonlinear interactions, or multi-conditional models.

## Next Checks

1. **Noise robustness test**: Evaluate the framework on data with noise levels ranging from 0% to 20% in 2% increments to identify the precise breaking point where performance degrades substantially.

2. **Real-world data validation**: Apply the framework to empirical trajectory data from actual highway driving scenarios to assess whether the simulated data assumptions hold.

3. **Generalization to alternative models**: Test the framework on trajectory data generated from different car-following models (e.g., Gipps, IDM, or Helly models) to verify whether the VIS-DSR approach can discover their governing equations.