---
ver: rpa2
title: 'NAP^2: A Benchmark for Naturalness and Privacy-Preserving Text Rewriting by
  Learning from Human'
arxiv_id: '2406.03749'
source_url: https://arxiv.org/abs/2406.03749
tags:
- privacy
- information
- human
- rewriting
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces NAP2, a benchmark for naturalness and privacy-preserving
  text rewriting that uses human-inspired strategies like deletion and obscuring of
  sensitive information. The dataset was constructed using both crowdsourced and LLM-generated
  rewrites based on PERSONA-CHAT, with manual curation for evaluation and GPT-4 for
  synthetic data augmentation.
---

# NAP^2: A Benchmark for Naturalness and Privacy-Preserving Text Rewriting by Learning from Human

## Quick Facts
- arXiv ID: 2406.03749
- Source URL: https://arxiv.org/abs/2406.03749
- Reference count: 40
- Primary result: Introduces NAP2 benchmark for privacy-preserving text rewriting using human-inspired deletion and obscuring strategies, achieving 93.81% privacy preservation with T5-BASE model.

## Executive Summary
This paper introduces NAP2, a benchmark for naturalness and privacy-preserving text rewriting that uses human-inspired strategies like deletion and obscuring of sensitive information. The dataset was constructed using both crowdsourced and LLM-generated rewrites based on PERSONA-CHAT, with manual curation for evaluation and GPT-4 for synthetic data augmentation. Experiments show that a T5-BASE model trained on NAP2 achieves 93.81% privacy preservation (PRIVACY_NLI), outperforming state-of-the-art methods like DP-based models. Human evaluation confirms the high naturalness and utility of the rewrites, and the synthetic data improves performance by 7%. The work highlights the effectiveness of human-inspired rewriting strategies and provides a new direction for privacy-preserving text generation.

## Method Summary
The NAP2 benchmark uses PERSONA-CHAT as source data and creates a manually curated dataset (895 instances) with human rewrites using deletion and obscuring strategies. Synthetic rewrites are generated by GPT-4 (3,900 instances) for training augmentation. The T5-BASE model is fine-tuned on this combined dataset. Evaluation uses PRIVACY_NLI (NLI-based privacy metric), ROUGE scores for semantic relevance, and human evaluation for naturalness. The method also includes a DP-SGD variant for comparison with differential privacy approaches.

## Key Results
- T5-BASE model trained on NAP2 achieves 93.81% privacy preservation (PRIVACY_NLI)
- Human evaluation confirms high naturalness and utility of rewrites
- Synthetic data augmentation improves model performance by 7%
- Outperforms DP-based methods and zero-shot LLMs on privacy preservation while maintaining naturalness

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The NAP2 benchmark improves naturalness and privacy preservation by training models on human-inspired deletion and obscuring strategies.
- Mechanism: The model learns to identify sensitive information in text and either remove it (deletion) or replace it with more general terms (obscuring), preserving the grammatical structure and coherence of the original text.
- Core assumption: Human rewriting strategies can be effectively captured and generalized by machine learning models.
- Evidence anchors:
  - [abstract]: "Compared to the prior works on anonymization, the human-inspired approaches result in more natural rewrites and offer an improved balance between privacy protection and data utility."
  - [section]: "Both strategies aim to make rewritten texts as natural as possible such that i) they do not raise the awareness of potential attackers that rewrites are sanitized; and ii) downstream applications can directly process such natural rewrites without fine-tuning their models for any unnatural parts of texts."
  - [corpus]: The corpus includes human-authored rewrites using both strategies, providing examples for the model to learn from.
- Break condition: If the model fails to generalize beyond the specific examples in the corpus, it may not effectively handle new types of sensitive information or contexts.

### Mechanism 2
- Claim: The use of synthetic data augmentation improves the model's performance by increasing the diversity and size of the training data.
- Mechanism: GPT-4 generates synthetic rewrites based on the manual corpus, providing additional examples for the model to learn from and improving its ability to generalize to new inputs.
- Core assumption: Synthetic data generated by a powerful language model can effectively supplement human-authored data.
- Evidence anchors:
  - [abstract]: "Incorporation of such synthetic data improves the T5-BASE model trained on human curated data by 7% in terms of privacy preservation."
  - [section]: "The resulting dataset is used to augment the training set of the manually created corpus to mitigate the data scarcity issue."
  - [corpus]: The synthetic data is generated based on the manual corpus, ensuring consistency in the rewriting strategies.
- Break condition: If the synthetic data is not diverse enough or does not accurately reflect the nuances of human rewriting, it may not improve the model's performance.

### Mechanism 3
- Claim: The novel privacy metric PRIVACY_NLI effectively measures the privacy preservation of the rewrites by using a Natural Language Inference model to determine if a rewrite entails the personal information.
- Mechanism: The NLI model evaluates the semantic relationship between the rewrite and the personal information, providing a quantitative measure of privacy preservation.
- Core assumption: The NLI model can accurately capture the semantic entailment between the rewrite and the personal information.
- Evidence anchors:
  - [abstract]: "We propose a novel metric, called PRIVACY_NLI, by using the ROBERTA model trained on the MNLI corpus, to infer to what degree it is possible to infer personal information in personas."
  - [section]: "As the NLI model classifies a pair of input texts into entailed, contradicted, or neutral, we adopt P (entailed|x, p) as the score of privacy_leakage, e. Hence, we consider PRIVACY_NLI as 1- privacy_leakage, denoting the privacy preserved by our method."
  - [corpus]: The metric is validated against human judgments, demonstrating its effectiveness in measuring privacy preservation.
- Break condition: If the NLI model is not accurate enough or does not generalize well to new types of personal information, it may not provide an accurate measure of privacy preservation.

## Foundational Learning

- Concept: Natural Language Inference (NLI)
  - Why needed here: NLI is used to evaluate the semantic relationship between the rewrite and the personal information, providing a measure of privacy preservation.
  - Quick check question: What are the three possible outcomes of an NLI model, and how are they used to measure privacy preservation?

- Concept: Differential Privacy (DP)
  - Why needed here: DP provides a theoretical framework for protecting privacy by adding noise to the data, ensuring that individual records cannot be identified.
  - Quick check question: How does DP differ from the human-inspired strategies used in NAP2, and what are the advantages and disadvantages of each approach?

- Concept: Text Rewriting Strategies
  - Why needed here: Understanding the different strategies for rewriting text (deletion, obscuring) is crucial for implementing and evaluating the NAP2 benchmark.
  - Quick check question: What are the key differences between deletion and obscuring, and how do they impact the naturalness and privacy preservation of the rewrites?

## Architecture Onboarding

- Component map: Input utterance and personal information -> Rewriting model (T5-BASE) -> Rewritten utterance with preserved naturalness and privacy -> Evaluation (PRIVACY_NLI metric and human evaluation)

- Critical path: Input utterance and personal information -> Rewriting model processes input and generates rewrite -> PRIVACY_NLI metric evaluates privacy preservation -> Human evaluation assesses naturalness and utility

- Design tradeoffs: Privacy vs. naturalness: Balancing the need to protect sensitive information with the desire to maintain the grammatical structure and coherence of the original text. Synthetic vs. human data: Using synthetic data to augment the training set, but ensuring that it accurately reflects the nuances of human rewriting.

- Failure signatures: Low PRIVACY_NLI score: Indicates that the rewrite does not effectively protect sensitive information. Poor human evaluation scores: Suggests that the rewrite is not natural or does not preserve the utility of the original text.

- First 3 experiments:
  1. Evaluate the performance of the T5-BASE model on the manually curated corpus, comparing it to human rewrites.
  2. Assess the impact of synthetic data augmentation on the model's performance, comparing it to models trained only on human data.
  3. Validate the effectiveness of the PRIVACY_NLI metric by comparing it to human judgments on a held-out test set.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the PRIVACY_NLI metric perform across different NLI backbone models and training datasets?
- Basis in paper: [explicit] The paper compares PRIVACY_NLI using DEBERTA and ROBERTA models trained on different corpora (mixed entailment corpus and MNLI), showing varying alignment with human evaluation (SPRIVACY).
- Why unresolved: The paper only evaluates two NLI models. Performance may vary significantly with other NLI architectures or training approaches, potentially affecting the reliability of PRIVACY_NLI as a universal evaluation metric.
- What evidence would resolve it: Comprehensive evaluation of PRIVACY_NLI using a wider range of NLI models (e.g., BERT variants, ELECTRA, DeBERTa-v3) trained on diverse entailment datasets would establish its robustness and generalizability.

### Open Question 2
- Question: What is the impact of dataset size on the performance of T5-BASE models trained on NAP2 for privacy-preserving rewriting?
- Basis in paper: [inferred] The paper notes that NAP2 is "somewhat limited in scope" due to budget constraints, with only 895 manually curated instances. It also mentions that incorporating synthetic data improves performance by 7%.
- Why unresolved: The paper doesn't systematically explore how performance scales with dataset size, particularly whether the synthetic data augmentation fully compensates for the small human-curated dataset or if there's a point of diminishing returns.
- What evidence would resolve it: Controlled experiments training T5-BASE models on varying proportions of human-curated and synthetic data (e.g., 100%, 75%, 50%, 25% human data with proportional synthetic augmentation) would reveal the optimal balance and scalability limits.

### Open Question 3
- Question: How do LLM-based naturalness judgments compare to human evaluations across different types of rewriting errors?
- Basis in paper: [explicit] The paper finds that LLM-as-judge (using GPT-4o) generally aligns well with human naturalness ratings for high-performing models but overestimates naturalness for low-performing systems like FLAIR-SCRUBBING.
- Why unresolved: The analysis doesn't distinguish between different types of unnaturalness (e.g., grammatical errors, semantic incoherence, lack of fluency) or examine whether LLMs are systematically biased toward certain error patterns.
- What evidence would resolve it: A detailed error analysis categorizing unnatural rewrites by type, comparing human and LLM judgments for each category, would reveal whether LLMs have systematic blind spots in detecting specific naturalness issues.

## Limitations
- Generalization to real-world scenarios: The benchmark's effectiveness on broader, more diverse text domains remains uncertain.
- Robustness of PRIVACY_NLI metric: The NLI-based privacy metric relies on the accuracy and generalization of the underlying ROBERTA model.
- Synthetic data quality and diversity: The exact nature of GPT-4's output diversity and its alignment with human rewriting patterns is not detailed.

## Confidence
- High confidence: The core methodology of using human-inspired deletion and obscuring strategies, and the effectiveness of the NAP2 benchmark in improving privacy preservation and naturalness.
- Medium confidence: The generalizability of the results to other text domains and the long-term robustness of the PRIVACY_NLI metric.
- Low confidence: The optimal balance between synthetic and human data for training, and the potential biases introduced by synthetic data augmentation.

## Next Checks
1. Cross-domain evaluation: Test the T5-BASE model trained on NAP2 on a diverse set of text domains (e.g., medical records, legal documents, social media posts) to assess its generalizability and robustness.
2. Adversarial testing of PRIVACY_NLI: Conduct adversarial testing by generating challenging rewrite-personal information pairs that are designed to fool the NLI model, and evaluate its ability to correctly identify privacy leakage.
3. Human evaluation on synthetic data: Perform a thorough human evaluation of the synthetic data generated by GPT-4, comparing it to human-authored rewrites in terms of naturalness, privacy preservation, and diversity.