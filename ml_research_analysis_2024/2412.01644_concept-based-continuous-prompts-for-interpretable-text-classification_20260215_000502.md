---
ver: rpa2
title: Concept Based Continuous Prompts for Interpretable Text Classification
arxiv_id: '2412.01644'
source_url: https://arxiv.org/abs/2412.01644
tags:
- prompts
- describe
- concepts
- shot
- continuous
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a method for interpreting continuous prompts
  by decomposing them into human-readable concepts. The key idea is to demonstrate
  that continuous prompts can always be decomposed into a concept embedding matrix
  and a coefficient matrix, which can be found and optimized.
---

# Concept Based Continuous Prompts for Interpretable Text Classification

## Quick Facts
- arXiv ID: 2412.01644
- Source URL: https://arxiv.org/abs/2412.01644
- Authors: Qian Chen; Dongyang Li; Xiaofeng He
- Reference count: 40
- This paper proposes a method for interpreting continuous prompts by decomposing them into human-readable concepts.

## Executive Summary
This paper addresses the interpretability challenge of continuous prompt tuning by proposing a framework that decomposes continuous prompts into human-readable concepts. The method demonstrates that continuous prompts can be represented as linear combinations of column vectors from the FFN outer layer, which correspond to semantic patterns that can be described with concepts. By employing GPT-4o to generate concept pools and using submodular optimization to select discriminative concepts, the framework achieves similar performance to original P-tuning and word-based approaches while providing more interpretable results.

## Method Summary
The framework generates candidate concepts using GPT-4o, then applies submodular optimization to select a subset that maximizes diversity and coverage. The selected concepts are embedded using a text encoder, and a coefficient matrix is learned to approximate the original continuous prompt matrix through a two-loss optimization framework combining KL divergence and log probability terms. The method is evaluated on three text classification datasets (SST-2, IMDB, AGNews) using BERT-Large and GPT-2-Medium, showing comparable performance to continuous prompts while providing interpretable concept-based explanations.

## Key Results
- Concept-CP achieves similar accuracy to P-tuning and word-based approaches across three datasets
- The framework provides interpretable explanations by selecting 5-20 concepts per class
- Concept correlation scores demonstrate alignment between learned coefficients and token attribution methods
- Performance is robust across few-shot and full-shot settings

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Continuous prompts can be decomposed into human-readable concepts through linear combinations of column vectors from the FFN outer layer.
- Mechanism: The transformer's output is a linear combination of column vectors (pm) from the FFN outer weight matrix W2. Each pm corresponds to a shallow or semantic pattern that can be described with concepts. By finding concept embeddings C and coefficient matrix Q such that CQ approximates the original prompt matrix P, we create interpretable explanations.
- Core assumption: Each column vector pm in the FFN outer layer corresponds to a meaningful semantic pattern that can be aligned with human-readable concepts.
- Evidence anchors:
  - [abstract]: "we demonstrate that a corresponding concept embedding matrix and a coefficient matrix can always be found to replace the prompt embedding matrix"
  - [section 3]: "Notice that all the terms preceding pm are scalars, so the final output can be viewed as a linear combination of pm. Based on findings of Geva et al. (2021), each pm corresponds to a shallow or semantic pattern, which can be described with concepts"
  - [corpus]: Weak - The corpus contains related work on interpretable embeddings but no direct evidence for this specific FFN decomposition mechanism.

### Mechanism 2
- Claim: Submodular optimization can select discriminative and representative concepts from a large candidate pool.
- Mechanism: The framework generates candidate concepts using GPT-4o, then applies submodular optimization to select a subset that maximizes diversity (Ld) and coverage (Lc). The diversity term encourages informative concepts while coverage ensures representative aspects of each class are included.
- Core assumption: Concept selection can be formulated as a submodular optimization problem where diversity and coverage functions satisfy diminishing returns property.
- Evidence anchors:
  - [abstract]: "we employ GPT-4o to generate a concept pool and choose potential candidate concepts that are discriminative and representative using a novel submodular optimization algorithm"
  - [section 5.2]: "We can measure the two properties above with a function F : 2^|Cy| → R. A submodular function satisfies the diminishing property... we propose the following monotonic submodular function to select Cy from Sy"
  - [corpus]: Moderate - The corpus includes work on submodular optimization for document summarization and facility location, supporting the general approach but not the specific application to concept selection.

### Mechanism 3
- Claim: The two-loss framework (Lf + Ll) ensures explanation fidelity while maintaining model performance.
- Mechanism: Lf (KL divergence between original and decomposed model outputs) ensures the decomposed explanation stays consistent with the original continuous prompts, while Ll (negative log likelihood) ensures performance retention. The weighted combination μLf + Ll balances interpretability and accuracy.
- Core assumption: The decomposed concept representation can maintain the original model's decision-making behavior while providing interpretable concepts.
- Evidence anchors:
  - [section 5.3]: "We require our explanations to stay consistent with original continuous prompts while maintaining the performance. Based on prior studies (Yeh et al., 2019; Ju et al., 2023), we construct two loss terms to meet these requirements"
  - [abstract]: "Experiments demonstrate that our framework can achieve similar results as the original P-tuning and word-based approaches using only a few concepts while providing more plausible results"
  - [corpus]: Weak - The corpus does not contain evidence for this specific two-loss approach for concept decomposition.

## Foundational Learning

- Concept: Linear algebra and matrix decomposition
  - Why needed here: The core mechanism relies on decomposing the prompt embedding matrix P into CQ, which requires understanding matrix factorization and linear combinations.
  - Quick check question: Can you explain why the Frobenius norm is used to measure the decomposition error ||CQ - P||_F?

- Concept: Submodular optimization
  - Why needed here: The concept selection process uses submodular functions with diminishing returns property to efficiently select representative concepts.
  - Quick check question: What property of submodular functions allows greedy algorithms to achieve near-optimal solutions?

- Concept: Transformer architecture and FFN layers
  - Why needed here: The mechanism depends on understanding how transformer outputs are computed as linear combinations of FFN column vectors.
  - Quick check question: How does the residual connection in the transformer block affect the final output computation?

## Architecture Onboarding

- Component map:
  GPT-4o concept generator → Candidate pool creation → Submodular optimizer → Concept selection → BERT/GPT-2 encoder → Concept embedding initialization → Coefficient matrix Q → Combined loss function (Lf + Ll) → Final concept-based model

- Critical path:
  1. Generate candidate concepts using GPT-4o
  2. Select discriminative concepts via submodular optimization
  3. Initialize concept embeddings and coefficients
  4. Train using combined loss function
  5. Generate explanations using top-k concepts

- Design tradeoffs:
  - Number of concepts per class vs. interpretability (more concepts = better coverage but more noise)
  - λ parameter in submodular function vs. diversity/coverage balance
  - μ parameter in loss function vs. fidelity/performance trade-off
  - GPT-4o generation quality vs. computational cost

- Failure signatures:
  - High variance in accuracy across different shot settings
  - Low concept correlation with attribution methods
  - Explanations that don't align with input content
  - Performance degradation compared to continuous prompts

- First 3 experiments:
  1. Verify concept decomposition works by checking ||CQ - P||_F is small on validation data
  2. Test submodular optimization by comparing selected concepts' diversity and coverage scores
  3. Validate explanation quality by computing concept correlation with token attribution methods

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the choice of submodular optimization function parameters (λ, ψ, and ϕ) affect the fidelity and interpretability of the concept decomposition?
- Basis in paper: [explicit] The paper mentions that λ controls the weights of diversity and coverage terms in the submodular optimization function, and ψ and ϕ are used to measure differences and similarities between concepts, respectively.
- Why unresolved: The paper does not provide an analysis of how varying these parameters impacts the quality of the concept decomposition or the performance of the model.
- What evidence would resolve it: Conducting experiments with different values of λ, ψ, and ϕ, and analyzing the resulting concept sets and model performance would provide insights into their effects.

### Open Question 2
- Question: Can the concept decomposition framework be extended to more complex tasks beyond text classification, such as question answering or summarization?
- Basis in paper: [inferred] The paper focuses on text classification tasks, but the concept decomposition framework is based on the general idea of decomposing continuous prompts into human-readable concepts, which could potentially be applied to other NLP tasks.
- Why unresolved: The paper does not explore the applicability of the framework to other tasks, and it is unclear how the concept generation and selection process would need to be adapted.
- What evidence would resolve it: Implementing the framework for other tasks and evaluating its performance and interpretability compared to existing methods would demonstrate its generalizability.

### Open Question 3
- Question: How does the length of the continuous prompt tokens (Nq) affect the trade-off between interpretability and performance?
- Basis in paper: [explicit] The paper mentions that experiments were conducted with different values of Nq, but the results and analysis are limited to specific cases.
- Why unresolved: The paper does not provide a comprehensive analysis of how varying Nq impacts the concept decomposition and model performance.
- What evidence would resolve it: Conducting experiments with a wider range of Nq values and analyzing the resulting concept sets and model performance would provide insights into the trade-off.

## Limitations
- The theoretical mechanism assumes FFN column vectors correspond to describable semantic patterns without direct validation
- Framework evaluated only on three text classification datasets using limited model architectures
- Significant computational overhead from two-stage approach not characterized
- Concept generation quality depends on GPT-4o without systematic analysis of variability

## Confidence

**High confidence** in empirical results showing Concept-CP achieves comparable accuracy to baselines across three datasets with sound methodology and appropriate ablation studies.

**Medium confidence** in theoretical mechanism of prompt decomposition via FFN layer analysis - mathematically coherent but relies on untested assumptions about semantic pattern correspondence.

**Low confidence** in generalizability beyond tested settings - limited analysis of failure modes, concept quality variability, or performance on different tasks.

## Next Checks

1. **Prompt decomposition validation**: Generate synthetic prompt matrices with known concept structure, apply the decomposition method, and measure reconstruction error ||CQ - P||_F. Compare with random baseline to verify that the decomposition captures meaningful structure rather than fitting noise.

2. **Concept quality analysis**: For each dataset, compute the average cosine similarity between GPT-4o-generated concepts and actual input texts. Analyze whether high-performing concept sets have systematically different concept quality metrics than low-performing ones. This would validate whether concept generation quality drives performance.

3. **Ablation of submodular parameters**: Systematically vary λ in the submodular function and measure its impact on both accuracy and concept correlation. Plot accuracy vs. diversity/coverage scores to identify whether the theoretical trade-off manifests empirically and whether there are optimal operating points.