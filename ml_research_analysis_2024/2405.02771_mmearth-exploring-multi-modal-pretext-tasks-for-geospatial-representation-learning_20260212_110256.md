---
ver: rpa2
title: 'MMEarth: Exploring Multi-Modal Pretext Tasks For Geospatial Representation
  Learning'
arxiv_id: '2405.02771'
source_url: https://arxiv.org/abs/2405.02771
tags:
- data
- tasks
- learning
- sentinel-2
- modalities
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces MMEarth, a large-scale multi-modal dataset
  for Earth observation, and proposes a Multi-Pretext Masked Autoencoder (MP-MAE)
  approach for pretraining models on satellite imagery. The MP-MAE extends masked
  autoencoders with multiple pretext tasks using aligned pixel-level and image-level
  modalities.
---

# MMEarth: Exploring Multi-Modal Pretext Tasks For Geospatial Representation Learning

## Quick Facts
- arXiv ID: 2405.02771
- Source URL: https://arxiv.org/abs/2405.02771
- Reference count: 40
- Key outcome: Multi-modal pretext tasks on satellite imagery outperform ImageNet pretraining and single-modal domain-specific pretraining for Earth observation tasks

## Executive Summary
This paper introduces MMEarth, a large-scale multi-modal dataset for Earth observation, and proposes a Multi-Pretext Masked Autoencoder (MP-MAE) approach for pretraining models on satellite imagery. The MP-MAE extends masked autoencoders with multiple pretext tasks using aligned pixel-level and image-level modalities. Experiments show that pretraining with multi-modal pretext tasks on MMEarth outperforms both ImageNet pretraining and domain-specific pretraining on Sentinel-2 images, especially for linear probing and few-shot learning. The learned representations demonstrate improved label and parameter efficiency, making them particularly valuable for global-scale applications with limited labeled data.

## Method Summary
The method involves adapting the ConvNeXt V2 architecture to medium-resolution satellite images by reducing patch size from 32x32 to 16x16 pixels and avoiding early downsampling. The MP-MAE model is trained on the MMEarth dataset using random masking of input patches and separate decoders for each pretext task. The model learns to reconstruct masked patches from multiple aligned modalities including Sentinel-2 optical, Sentinel-1 SAR, elevation, and landcover data. Training is performed for 200 epochs with a base learning rate of 1.5e-4 and batch size of 4096, with evaluation on downstream tasks including image classification and semantic segmentation using both fine-tuning and linear probing approaches.

## Key Results
- Multi-modal pretext tasks improve linear probing performance compared to single-modal approaches
- MP-MAE outperforms both ImageNet pretraining and domain-specific pretraining on Sentinel-2 images
- Pretraining with multi-modal pretext tasks provides better label efficiency in few-shot learning scenarios
- Domain-specific architectural modifications are crucial for adapting ConvNeXt V2 to satellite imagery

## Why This Works (Mechanism)

### Mechanism 1
Multi-modal pretext tasks improve representation quality by requiring the model to extract semantic features that generalize across different data types. The MP-MAE model is trained to reconstruct masked patches from multiple aligned modalities (e.g., Sentinel-2 optical, Sentinel-1 SAR, elevation, landcover). Each reconstruction task forces the encoder to learn semantic features that are useful for predicting diverse targets, leading to richer representations. The core assumption is that semantic features needed to predict one modality are also useful for downstream tasks. Evidence shows that multi-pretext tasks improved both fine-tuning and linear probing performance across all downstream tasks.

### Mechanism 2
Domain-specific pretraining with medium-resolution satellite images and appropriate patch sizes is crucial for adapting the ConvNeXt V2 architecture to Earth observation data. The paper adjusts the ConvNeXt V2 encoder by reducing the patch size from 32x32 to 16x16 pixels and avoiding early downsampling. This preserves spatial information important for medium-resolution satellite images and pixel-level downstream tasks like semantic segmentation. The core assumption is that the inductive bias of CNNs (spatial locality) is more important for satellite images than efficiency gains from early downsampling. Evidence shows that domain-specific pretraining improved fine-tuning and linear probing performance across all datasets compared to pretraining on ImageNet.

### Mechanism 3
Multi-modal pretext tasks improve label efficiency, especially in few-shot learning scenarios, by providing a richer supervisory signal during pretraining. The model learns to predict diverse targets from optical images, which requires understanding complex relationships in the data. This leads to representations that generalize better to new tasks with limited labeled data. The core assumption is that semantic features learned to predict one modality are transferable to other tasks. Evidence shows that models pretrained with multi-modal pretext tasks outperformed models trained on only Sentinel-2 or only ImageNet in few-shot linear probing scenarios.

## Foundational Learning

- **Concept**: Masked Autoencoders (MAE)
  - **Why needed here**: MAEs are the backbone of the MP-MAE approach, providing a framework for self-supervised learning by reconstructing masked patches
  - **Quick check question**: What is the key difference between MAE and traditional autoencoders, and why is this important for efficient training?

- **Concept**: Multi-task learning
  - **Why needed here**: MP-MAE extends MAE by incorporating multiple pretext tasks using different modalities, requiring an understanding of how to combine losses and balance tasks
  - **Quick check question**: How does the uncertainty-based loss weighting help handle tasks with different levels of noise or difficulty?

- **Concept**: Domain adaptation
  - **Why needed here**: The paper adapts the ConvNeXt V2 architecture to medium-resolution satellite images, requiring an understanding of how architectural choices affect performance on different data domains
  - **Quick check question**: Why is reducing the patch size from 32x32 to 16x16 pixels crucial for adapting the ConvNeXt V2 encoder to Sentinel-2 images?

## Architecture Onboarding

- **Component map**: Input image → random masking → encoder → latent representation → multiple decoders → loss computation → gradient update
- **Critical path**: Input image → random masking → encoder → latent representation → multiple decoders → loss computation → gradient update
- **Design tradeoffs**:
  - Using ConvNeXt V2 vs. Vision Transformers: ConvNeXt V2 is fully convolutional, making it more suitable for downstream tasks with varying input sizes
  - Pixel-level vs. image-level pretext tasks: Combining both types of tasks improves performance, but may increase computational cost
  - Uncertainty weighting vs. equal weighting: Uncertainty weighting provides insights into task importance but may not always improve performance
- **Failure signatures**:
  - Poor reconstruction quality: May indicate issues with the encoder architecture or loss function
  - Overfitting to pretraining tasks: May occur if the pretext tasks are too easy or the model is too large relative to the dataset
  - Catastrophic forgetting: May occur if fine-tuning is not done carefully, causing the model to forget the useful representations learned during pretraining
- **First 3 experiments**:
  1. Train MP-MAE on MMEarth with only the Sentinel-2 reconstruction task to establish a baseline
  2. Add one pixel-level pretext task (e.g., elevation reconstruction) and compare performance on a downstream task
  3. Add one image-level pretext task (e.g., biome classification) and compare performance on the same downstream task

## Open Questions the Paper Calls Out

### Open Question 1
How does the performance of MP-MAE vary with different numbers of pretext tasks and modalities? Is there an optimal subset of modalities that balances performance and computational efficiency? The study focused on comparing multi-pretext tasks to single-pretext tasks and did not systematically explore the effect of using subsets of modalities or varying the number of tasks.

### Open Question 2
How does the pretraining performance of MP-MAE change with varying image sizes and patch sizes? Is there an optimal balance between input image size and patch size for learning effective representations? The study primarily focused on adapting the patch size for the specific case of MMEarth and did not systematically investigate the effect of varying image and patch sizes on pretraining outcomes.

### Open Question 3
How does the choice of loss aggregation strategy (equal weighting vs. task uncertainty weighting) impact the final representations learned by MP-MAE? Are there scenarios where one strategy is preferable over the other? The study only presents a brief comparison of the two strategies without delving into the nuances of their impact on the learned representations or exploring their performance in different scenarios.

## Limitations
- The exact contribution of each modality and pretext task combination is not fully explored
- Dataset curation process may introduce selection bias that partially contributes to performance gains
- The magnitude of improvement in label efficiency varies across tasks and could be influenced by dataset-specific factors

## Confidence
- High confidence: Architectural modifications for adapting ConvNeXt V2 to satellite imagery are well-justified and show consistent performance improvements
- Medium confidence: Multi-modal pretext tasks improve label efficiency in few-shot learning scenarios, but improvement magnitude varies across tasks
- Medium confidence: Uncertainty-weighted loss function provides useful insights into task importance, but experimental evidence for its effectiveness is limited

## Next Checks
1. Ablation study: Systematically remove each pretext task (one at a time) to quantify the individual contribution of each modality to the final performance, and test whether certain combinations of tasks are more effective than others

2. Cross-dataset generalization: Evaluate the pretrained models on a completely different satellite dataset (e.g., from a different sensor or geographic region) to test whether the learned representations generalize beyond the MMEarth dataset

3. Temporal stability analysis: Test whether models pretrained on historical data maintain their performance advantage when fine-tuned on temporally shifted data, as Earth observation data can change significantly over time due to seasonal variations and environmental changes