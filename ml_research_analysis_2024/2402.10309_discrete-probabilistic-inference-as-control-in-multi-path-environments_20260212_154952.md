---
ver: rpa2
title: Discrete Probabilistic Inference as Control in Multi-path Environments
arxiv_id: '2402.10309'
source_url: https://arxiv.org/abs/2402.10309
tags:
- soft
- learning
- state
- maxent
- where
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper establishes novel equivalences between Generative Flow
  Networks (GFlowNets) and maximum entropy Reinforcement Learning (MaxEnt RL) objectives
  for sampling from discrete distributions. The key contributions are: 1) Extending
  reward correction methods to guarantee that the optimal MaxEnt RL policy induces
  a distribution proportional to the target Gibbs distribution, regardless of the
  MDP structure.'
---

# Discrete Probabilistic Inference as Control in Multi-path Environments

## Quick Facts
- arXiv ID: 2402.10309
- Source URL: https://arxiv.org/abs/2402.10309
- Reference count: 40
- Key outcome: Establishes novel equivalences between Generative Flow Networks (GFlowNets) and maximum entropy Reinforcement Learning (MaxEnt RL) objectives for sampling from discrete distributions

## Executive Summary
This paper establishes theoretical connections between two approaches for discrete probabilistic inference: Generative Flow Networks (GFlowNets) and maximum entropy Reinforcement Learning (MaxEnt RL). The authors prove that with appropriate reward correction, the optimal MaxEnt RL policy induces a distribution proportional to the target Gibbs distribution regardless of the MDP structure. They demonstrate that common GFlowNet objectives (Trajectory Balance and Modified Detailed Balance) are equivalent to MaxEnt RL objectives (Path Consistency Learning and Soft Q-Learning with policy parametrization). The work provides a unified framework enabling transfer of techniques between these two approaches for probabilistic inference over discrete and structured spaces.

## Method Summary
The paper develops reward correction methods that ensure the terminating state distribution of MaxEnt RL policies matches target Gibbs distributions. Three main equivalences are proven: 1) With corrected rewards, the optimal MaxEnt RL policy achieves the desired terminating state distribution; 2) Trajectory Balance loss in GFlowNets is equivalent to Path Consistency Learning in MaxEnt RL; 3) Modified Detailed Balance in GFlowNets is equivalent to a variant of Soft Q-Learning with policy parametrization. Empirical validation is performed on three domains: factor graphs, Bayesian network structure learning, and phylogenetic tree generation, demonstrating similar performance across equivalent algorithms.

## Key Results
- Reward correction guarantees optimal MaxEnt RL policy induces distribution proportional to target Gibbs distribution
- Trajectory Balance loss in GFlowNets is equivalent to Path Consistency Learning in MaxEnt RL (up to reward correction)
- Modified Detailed Balance in GFlowNets is equivalent to Soft Q-Learning with policy parametrization
- Empirical results show similar performance of equivalent methods across three discrete domains

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Reward correction guarantees the terminating state distribution matches the target Gibbs distribution regardless of MDP structure.
- Mechanism: The reward function is adjusted with a backward transition probability term that compensates for multiple trajectories leading to the same terminating state, ensuring the optimal policy's terminating state distribution matches the desired Gibbs distribution.
- Core assumption: The backward transition probability PB(s|s') is a valid distribution over parent states of s' in the MDP's state transition graph.
- Evidence anchors:
  - [abstract]: "Extending reward correction methods to guarantee that the optimal MaxEnt RL policy induces a distribution proportional to the target Gibbs distribution, regardless of the MDP structure."
  - [section]: Theorem 3.1 proves that with corrected reward satisfying (8), the terminating state distribution associated with the optimal policy satisfies π*(x) ∝ exp(−E(x)/α).
  - [corpus]: Missing direct evidence; closest related work is Mohammadpour et al. (2024) on reward correction, but lacks proof details.
- Break condition: If the backward transition probability PB is not properly normalized or doesn't form a valid distribution over parent states.

### Mechanism 2
- Claim: Path Consistency Learning (PCL) and Subtrajectory Balance (SubTB) objectives are equivalent under reward correction.
- Mechanism: Both objectives enforce consistency between policy/value functions and policy/flow functions respectively, and with the corrected reward, their residuals become proportional with a simple correspondence between parameterizations.
- Core assumption: The reward function satisfies the corrected form in (9) where energy only appears at the end of trajectories.
- Evidence anchors:
  - [abstract]: "Proving that the widely-used Trajectory Balance loss in GFlowNets is equivalent to Path Consistency Learning in MaxEnt RL, up to a reward correction."
  - [section]: Proposition 3.2 establishes LPCL(θ, ϕ) = α²LSubTB(θ, ϕ) with πθ(s'|s) = PθF(s'|s) and Vϕsoft(s) = αlogFϕ(s).
  - [corpus]: Weak evidence; no direct mentions of PCL-SubTB equivalence in neighbor papers.
- Break condition: If intermediate rewards are present in the MDP, requiring a different reward correction form than (9).

### Mechanism 3
- Claim: Soft Q-Learning with policy parametrization is equivalent to Modified Detailed Balance in GFlowNets.
- Mechanism: When all states are terminating, Soft Q-Learning can be expressed as a function of policy parameters, and this objective becomes proportional to Modified Detailed Balance under the same reward correction.
- Core assumption: All states in the MDP are valid terminating states (S ≡ X) with intermediate rewards defined by E(s) - E(s') + αlogPB(s|s').
- Evidence anchors:
  - [abstract]: "Introducing a variant of Soft Q-Learning depending directly on a policy, and showing its equivalence to the Modified Detailed Balance loss in GFlowNets."
  - [section]: Proposition 3.3 shows Lπ-SQL(θ) = α²LM-DB(θ) with πθ(s'|s) = PθF(s'|s) under the all-terminating states assumption.
  - [corpus]: Missing direct evidence; no neighbor papers discuss this specific equivalence.
- Break condition: If the MDP contains non-terminating states or the reward structure doesn't allow the intermediate reward decomposition.

## Foundational Learning

- Concept: Maximum Entropy Reinforcement Learning (MaxEnt RL)
  - Why needed here: MaxEnt RL provides the theoretical foundation for sampling from Gibbs distributions through stochastic policies that maximize both expected return and policy entropy.
  - Quick check question: How does the entropy term in the MaxEnt RL objective (equation 3) affect the optimal policy compared to standard RL?

- Concept: Generative Flow Networks (GFlowNets)
  - Why needed here: GFlowNets provide an alternative approach to sampling from distributions over discrete structured spaces by learning flow-conserving policies.
  - Quick check question: What are the flow-matching conditions (equation 6) that a GFlowNet policy must satisfy to sample proportionally to the target distribution?

- Concept: Reward Shaping and Correction
  - Why needed here: Reward correction is the key technique that bridges MaxEnt RL and GFlowNets, ensuring the terminating state distribution matches the target regardless of MDP structure.
  - Quick check question: How does the reward correction term αlogPB(st|st+1) in equation (8) compensate for multiple trajectories leading to the same terminating state?

## Architecture Onboarding

- Component map: MDP structure (states S, actions A, initial s0, terminal sf) -> Reward functions (sparse, corrected, intermediate) -> Algorithms (PCL/SQL/SAC or TB/DB/SubTB) -> Policy representations (direct policy, Q-function, flow function) -> Backward transition probability PB

- Critical path: Build MDP → Define reward correction → Implement algorithm (PCL/SQL or TB/DB) → Train with target network → Evaluate terminating state distribution

- Design tradeoffs:
  - Separate vs shared networks for policy and value/flow functions (Dueling vs Unified architectures)
  - Reward correction at trajectory level vs transition level
  - All-terminating states assumption vs general MDP structure

- Failure signatures:
  - Policy collapse to deterministic behavior (insufficient entropy regularization)
  - High variance in training (improper reward correction or exploration)
  - Distribution mismatch (incorrect backward transition probability PB)
  - Instability with SAC (sensitivity to hyperparameters)

- First 3 experiments:
  1. Implement simple chain MDP with sparse rewards to verify basic PCL vs TB equivalence
  2. Add backward transition probability PB to create DAG structure and test reward correction effectiveness
  3. Compare SQL vs DB on all-terminating states MDP with intermediate rewards

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the reward correction method generalize to continuous state and action spaces in the context of GFlowNets?
- Basis in paper: [explicit] The paper conjectures that the reward correction method can be extended to continuous spaces, similar to extensions of GFlowNets to continuous spaces (Li et al., 2023, Lahlou et al., 2023).
- Why unresolved: While the paper suggests this generalization, it does not provide a formal proof or empirical validation in continuous settings.
- What evidence would resolve it: A formal extension of the reward correction theorem to continuous spaces, accompanied by empirical results demonstrating its effectiveness in continuous GFlowNet applications.

### Open Question 2
- Question: How does the choice of backward transition probability PB affect the convergence and performance of GFlowNets in practice?
- Basis in paper: [explicit] The paper mentions that Mohammadpour et al. (2024) introduced a correction depending on the number of trajectories to a state, which can be learned by solving a second MaxEnt RL problem, and that this corresponds to a particular choice of backward transition probability PB(s|s') = n(s)/n(s').
- Why unresolved: While the paper discusses different choices of PB, it does not provide a comprehensive empirical comparison of their effects on convergence and performance.
- What evidence would resolve it: An empirical study comparing the performance of GFlowNets using different backward transition probability choices on a variety of tasks, with a focus on convergence speed and final performance.

### Open Question 3
- Question: Can the equivalence between MaxEnt RL and GFlowNets be extended to more complex environments, such as those with stochastic transitions or partial observability?
- Basis in paper: [inferred] The paper focuses on deterministic MDPs and does not address stochastic or partially observable environments. However, it mentions that some works have attempted to generalize GFlowNets to stochastic environments (Bengio et al., 2023, Pan et al., 2023b).
- Why unresolved: The paper does not explore the theoretical or empirical implications of extending the equivalence to more complex environments.
- What evidence would resolve it: A theoretical analysis of how the reward correction and objective equivalences generalize to stochastic and partially observable environments, along with empirical results demonstrating their effectiveness in such settings.

## Limitations

- The theoretical equivalence relies on specific reward correction mechanisms that may not generalize to all MDP structures
- Empirical validation is limited to controlled experimental settings across three discrete domains
- The backward transition probability PB(s|s') is a critical component whose choice affects convergence but lacks comprehensive empirical comparison

## Confidence

- Theoretical equivalence proofs: Medium-High
- Empirical validation across domains: Medium
- Practical significance of reward correction: Low-Medium

## Next Checks

1. Implement a comprehensive ablation study removing the reward correction term to quantify its impact on distribution matching accuracy across all three domains
2. Test the theoretical equivalence on a larger set of synthetic MDPs with varying DAG structures and multiple paths to terminating states
3. Evaluate the sensitivity of each algorithm to hyperparameters, particularly the entropy regularization coefficient α, to assess practical robustness