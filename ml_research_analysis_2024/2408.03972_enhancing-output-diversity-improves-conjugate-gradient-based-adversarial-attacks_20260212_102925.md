---
ver: rpa2
title: Enhancing Output Diversity Improves Conjugate Gradient-based Adversarial Attacks
arxiv_id: '2408.03972'
source_url: https://arxiv.org/abs/2408.03972
tags:
- reacg
- adversarial
- robust
- than
- attack
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper proposes ReACG, a method to enhance adversarial attacks\
  \ by increasing output diversity. ReACG modifies the Auto Conjugate Gradient (ACG)\
  \ attack by rescaling the coefficient \u03B2(k) and optimizing step size control."
---

# Enhancing Output Diversity Improves Conjugate Gradient-based Adversarial Attacks

## Quick Facts
- arXiv ID: 2408.03972
- Source URL: https://arxiv.org/abs/2408.03972
- Reference count: 0
- Primary result: ReACG reduces robust accuracy by 0.4–0.9% vs APGD and 0.1–0.4% vs ACG

## Executive Summary
This paper introduces ReACG, a method that enhances adversarial attacks by increasing output diversity. The authors modify the Auto Conjugate Gradient (ACG) attack by rescaling the coefficient β(k) and optimizing step size control through multi-objective checkpoint optimization. Experiments on 30 robust models trained on CIFAR-10, CIFAR-100, and ImageNet datasets demonstrate that ReACG outperforms state-of-the-art attacks APGD and ACG, with lower robust accuracy by approximately 0.4 to 0.9% and 0.1 to 0.4%, respectively.

## Method Summary
ReACG is an adversarial attack method based on the Auto Conjugate Gradient (ACG) attack, which modifies search direction and step size control. The method automatically adjusts the coefficient β(k) to prevent premature convergence and uses multi-objective optimization to find appropriate checkpoint parameters that control step size decay. ReACG was tested on 30 robust models from the RobustBench benchmark across CIFAR-10, CIFAR-100, and ImageNet datasets, using parameters p1=0.43, q=0.24, qmin=0.08, and N=100 iterations with CW/DLR loss as the objective function.

## Key Results
- ReACG achieves lower robust accuracy compared to APGD (0.4–0.9% improvement) and ACG (0.1–0.4% improvement)
- ReACG shows higher CTC (second likely prediction) diversity, with fewer images having #CTC=1 and more having #CTC≥2
- Particularly strong performance on ImageNet models with large numbers of classification classes

## Why This Works (Mechanism)

### Mechanism 1
Rescaling β(k) prevents premature convergence to small search steps, maintaining search diversity. When |β(k)| exceeds the average ratio of gradient to conjugate gradient magnitude, the method substitutes a normalized version of β(k) to avoid stagnation. This forces the search direction to vary more across iterations. Break condition: If the rescaling threshold is set too high, useful conjugate gradient scaling may be discarded, reducing attack efficiency.

### Mechanism 2
Multi-objective checkpoint optimization improves step size control, increasing the distance between search points. Instead of using fixed checkpoint parameters, ReACG searches for p1, q, and qmin that minimize robust accuracy while maximizing loss. This yields checkpoints that force more frequent step-size reductions, keeping the search in higher-loss regions longer. Break condition: Over-aggressive checkpoint spacing may cause step decay before meaningful gradient progress, stalling the attack.

### Mechanism 3
Increasing the number of distinct CTCs encountered during attack correlates with higher misclassification rates. By increasing the distance between consecutive search points, ReACG samples a broader region of the output space, exposing more misclassified classes before settling on the final adversarial example. Break condition: If the model's decision boundary is very flat, CTC variation may not translate into lower robust accuracy.

## Foundational Learning

- Concept: Conjugate gradient method and its use in adversarial attacks.
  - Why needed here: ReACG is built on ACG, which itself is inspired by conjugate gradient updates; understanding the update equations is essential to grasp why β(k) scaling matters.
  - Quick check question: In ACG, what role does β(k) play in the search direction update?

- Concept: Multi-objective optimization for hyperparameter search.
  - Why needed here: ReACG uses Optuna to tune checkpoint parameters; knowing how multi-objective Optuna works explains why robust accuracy and loss are both optimized.
  - Quick check question: What is the effect of using a multi-objective loss that returns (∞, -∞) when checkpoints < 4?

- Concept: CTC (second likely prediction) and its use as a diversity metric.
  - Why needed here: ReACG claims higher CTC diversity improves attack performance; understanding CTC helps interpret experimental results.
  - Quick check question: How is #CTC computed from the sequence of CTCs during an attack?

## Architecture Onboarding

- Component map: Input preprocessing -> Gradient computation (g(k)) -> Conjugate gradient scaling (β(k)) -> Search direction (s(k)) -> Step-size control (η(k)) -> Projection onto feasible set -> Loss evaluation
- Critical path: Gradient computation → β(k) rescaling decision → s(k) update → η(k) step control → projection → loss check
- Design tradeoffs: Rescaling β(k) adds a conditional branch but reduces redundant steps; checkpoint optimization increases tuning time but improves final robustness
- Failure signatures: If β(k) rescaling never triggers, the attack behaves like ACG; if checkpoints are too sparse, step decay may never occur
- First 3 experiments:
  1. Run ReACG with β(k) rescaling disabled; compare robust accuracy to ACG baseline
  2. Vary the rescaling threshold multiplier; measure impact on attack success rate
  3. Fix checkpoints to ACG values but keep β(k) rescaling; isolate contribution of each component

## Open Questions the Paper Calls Out

### Open Question 1
How does the performance of ReACG compare to other state-of-the-art adversarial attacks, such as Square Attack or AutoAttack, on ImageNet models with a large number of classification classes?
- Basis in paper: [explicit] The paper states that ReACG demonstrated particularly promising results for ImageNet models with a large number of classification classes.
- Why unresolved: The paper only compared ReACG to APGD and ACG, and did not evaluate its performance against other state-of-the-art attacks.
- What evidence would resolve it: Conducting experiments to compare ReACG's performance with other state-of-the-art attacks, such as Square Attack or AutoAttack, on ImageNet models with a large number of classification classes.

### Open Question 2
What is the impact of modifying the coefficient β(k) on the convergence speed of ReACG compared to ACG?
- Basis in paper: [explicit] The paper mentions that ReACG modifies the coefficient β(k) based on the ratio of the gradient to the conjugate gradient, which leads to an increase in the distance between two consecutive search points and enhances output diversity.
- Why unresolved: The paper does not provide a detailed analysis of the convergence speed of ReACG compared to ACG.
- What evidence would resolve it: Conducting experiments to measure and compare the convergence speed of ReACG and ACG on various models and datasets.

### Open Question 3
How does the choice of the distance function (e.g., Euclidean distance or uniform distance) affect the performance of ReACG?
- Basis in paper: [inferred] The paper focuses on ℓ∞ attacks that use d(u, v) = ∥u − v∥∞ as a distance function, but it does not explore the impact of other distance functions on ReACG's performance.
- Why unresolved: The paper does not provide an analysis of the impact of different distance functions on ReACG's performance.
- What evidence would resolve it: Conducting experiments to evaluate the performance of ReACG using different distance functions, such as Euclidean distance or uniform distance, on various models and datasets.

## Limitations
- The empirical claim that output diversity (measured by CTC count) causally improves attack success lacks ablation studies isolating diversity from other factors like step-size control
- The rescaling rule for β(k) is heuristic; no theoretical analysis justifies the threshold choice or its impact on convergence guarantees
- The Optuna multi-objective search is constrained to a narrow parameter space; performance gains might not generalize to other robust models or attack budgets

## Confidence

**High** - Quantitative results showing ReACG outperforms ACG and APGD on 30 models (robust accuracy drops of 0.1–0.9%)

**Medium** - Causal claim that increasing inter-step distance improves diversity; correlation is shown but mechanism is not fully isolated

**Low** - Theoretical justification for β(k) rescaling; the method is described as "empirical" without convergence bounds

## Next Checks

1. Run ReACG with diversity-maximizing components (CTC sampling) disabled to measure the isolated effect of search distance on robust accuracy
2. Test β(k) rescaling with multiple threshold multipliers on a held-out robust model to assess sensitivity and overfitting to the chosen value
3. Apply ReACG to non-robust models to determine whether the diversity gain translates to models with smooth decision boundaries