---
ver: rpa2
title: 'AIME: AI System Optimization via Multiple LLM Evaluators'
arxiv_id: '2410.03131'
source_url: https://arxiv.org/abs/2410.03131
tags:
- aime
- evaluation
- arxiv
- code
- optimization
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper addresses the problem of AI system optimization using
  text-based feedback loops. It identifies a critical issue: single LLM evaluators
  tend to miss errors in generated code, leading to suboptimal performance.'
---

# AIME: AI System Optimization via Multiple LLM Evaluators

## Quick Facts
- arXiv ID: 2410.03131
- Source URL: https://arxiv.org/abs/2410.03131
- Reference count: 30
- Primary result: Single LLM evaluators miss errors in generated code; multiple specialized evaluators improve detection and success rates

## Executive Summary
AIME addresses the critical limitation of single LLM evaluators in AI system optimization: their tendency to miss errors in generated code, leading to suboptimal performance. The paper proposes using multiple independent LLM evaluators, each specializing in a specific role (e.g., correctness, logic, syntax), and concatenating their evaluations to improve error detection. Experiments on LeetCodeHard and HumanEval datasets demonstrate up to 62% higher error detection and 16% higher success rates compared to single-evaluator methods. The choice of evaluators and their roles significantly impacts overall performance.

## Method Summary
AIME uses multiple independent LLM evaluators with role-specific prompts (syntax, logic, correctness, readability, runtime, redundancy) to optimize code generation through iterative feedback loops. The method implements TextGrad framework with GPT-4o, running 10 optimization iterations per problem. Each evaluator provides independent assessment based on its specialized role, and their outputs are concatenated to form comprehensive feedback. The system compares against Single-Eval baseline using Success Rate (SR), Completion Rate (CR), and Error Detection Rate (EDR) metrics on LeetCodeHard (39 problems) and HumanEval (first 20 problems) datasets.

## Key Results
- AIME achieves up to 62% higher error detection rate compared to single evaluator methods
- 16% higher relative absolute error (RAE) over Single-Eval on LeetCodeHard dataset
- Different combinations of evaluator roles show up to 12% difference in success rate

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Multiple independent LLM evaluators each specializing in one criterion reduce the suboptimality gap compared to a single evaluator handling all criteria.
- **Mechanism:** Distributing evaluation roles across multiple evaluators reduces cognitive load and avoids conflation of criteria. Concatenating outputs approximates linear combination of evaluation distributions.
- **Core assumption:** Aggregation function behaves linearly, allowing error distributions to combine additively.
- **Evidence anchors:** Theorem 1 bounds suboptimality using total variation distance; related works on role-specific evaluators support decomposition approach.
- **Break condition:** Significant role overlap or nonlinear aggregation invalidates theoretical guarantee.

### Mechanism 2
- **Claim:** Increasing the number of evaluators improves error detection rate even when all share the same role.
- **Mechanism:** Independent sampling from same evaluation policy yields diverse outputs; probability of at least one detecting error increases with K.
- **Core assumption:** Evaluator outputs are independent conditional on input code and prompt.
- **Evidence anchors:** Figure 5 shows EDR increases from 1→3→6 correctness-only evaluators; 62% EDR improvement reported.
- **Break condition:** Highly correlated outputs (same prompt, same temperature) yield diminishing returns.

### Mechanism 3
- **Claim:** AIME is more robust to adversarial or biased evaluators than Single-Eval.
- **Mechanism:** Multiple evaluators dilute effect of single biased one; at least one honest evaluator compensates.
- **Core assumption:** At least one evaluator is unbiased and its output can compensate for biased ones.
- **Evidence anchors:** Figure 2 shows RAE metric comparison under adversarial correctness evaluator.
- **Break condition:** All evaluators adversarially biased on same criterion collapses robustness.

## Foundational Learning

- **Concept:** Total variation distance (dTV) as metric for distribution divergence.
  - **Why needed here:** Theorem 1 uses dTV to bound suboptimality between optimal and approximated evaluation distributions.
  - **Quick check question:** Given two discrete distributions P and Q over {0,1}, what is dTV(P,Q)?

- **Concept:** Linear additivity assumption for aggregation functions.
  - **Why needed here:** Proof assumes g(e₁,e₂)=αe₁+(1−α)e₂ to show cancellation in ∆₂ term.
  - **Quick check question:** If e₁=0.7, e₂=0.3, and α=0.5, what is g(e₁,e₂) under linear additivity?

- **Concept:** Role decomposition in prompt engineering.
  - **Why needed here:** AIME assigns each evaluator distinct role to avoid conflation.
  - **Quick check question:** List three distinct evaluation criteria useful for code generation tasks.

## Architecture Onboarding

- **Component map:** Initial prompt generator (π₀) → Code generator (πθ) → K role-specific evaluators (πk, k=1…K) → Concatenation aggregator → Feedback generator (πf) → Next-prompt generator (πx) → repeat
- **Critical path:** Initial prompt → Code generator → K evaluators → Concatenation → Feedback → Next prompt → repeat
- **Design tradeoffs:** More evaluators → higher error detection but higher latency and token cost; fewer evaluators → faster but risk missing errors; role overlap vs. specialization tradeoff
- **Failure signatures:** Low EDR despite multiple evaluators → correlated outputs or insufficient role diversity; degraded SR/CR with more evaluators → over-partitioning leading to context loss; high latency → token budget per evaluator too large
- **First 3 experiments:**
  1. Compare EDR of AIME (K=3, distinct roles) vs Single-Eval on LeetCodeHard with eval temperature=0
  2. Vary K=1,3,6 with all evaluators set to "correctness" role; measure EDR and SR
  3. Introduce adversarial evaluator (always claims correctness) and measure RAE for both methods

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** What is the optimal number and combination of evaluators for maximizing error detection rate in code generation tasks?
- **Basis in paper:** [explicit] Paper demonstrates 6 evaluators outperform single evaluator, with different role combinations showing up to 12% success rate differences
- **Why unresolved:** Paper shows 6 evaluators with all roles performs best but doesn't systematically explore all possible combinations
- **What evidence would resolve it:** Comprehensive grid search over all possible combinations of evaluators (1-6 evaluators, all role subsets) with statistical analysis

### Open Question 2
- **Question:** How does AIME perform on non-code generation tasks that require multi-criteria evaluation?
- **Basis in paper:** [inferred] Paper acknowledges limitation to code generation and suggests future work could extend to molecule optimization or text generation
- **Why unresolved:** Paper only evaluates on code generation tasks (LeetCodeHard and HumanEval)
- **What evidence would resolve it:** Experiments applying AIME to at least 3-5 diverse tasks (mathematical reasoning, question answering, molecule optimization) with comparative performance metrics

### Open Question 3
- **Question:** What is the optimal weighting strategy for combining evaluations from multiple LLM evaluators?
- **Basis in paper:** [explicit] Paper uses uniform weighting through concatenation and suggests future research could investigate weighting and aggregation methods
- **Why unresolved:** Paper uses simple concatenation without exploring weighted combinations or learned aggregation strategies
- **What evidence would resolve it:** Experiments comparing different aggregation methods (weighted combinations, learned models, LLM-based summarization) against baseline concatenation approach

## Limitations
- Theoretical guarantees rely on idealized assumptions about linear aggregation and evaluator independence that may not hold in practice
- Empirical validation scope is limited to two coding datasets with GPT-4o, performance on other domains or model families unknown
- Role specification lacks precision in exact prompt templates and evaluation criteria boundaries

## Confidence

- **High confidence:** AIME's architecture using multiple evaluators with role specialization is sound and follows established LLM evaluation patterns
- **Medium confidence:** 62% EDR improvement is robust across experimental conditions, though absolute performance depends on evaluator configuration
- **Low confidence:** Theoretical suboptimality bound holds under ideal conditions but may not reflect practical performance when independence assumptions break

## Next Checks

1. **Test role independence empirically.** Run ablation studies varying K=1,3,6 with all evaluators set to "correctness" role to measure EDR gains purely from independent sampling, then compare against K=3 with distinct roles to isolate role specialization effect.

2. **Validate robustness to adversarial evaluators.** Systematically introduce biased evaluators (e.g., always claiming correctness) and measure RAE degradation for both AIME and Single-Eval to confirm robustness claim holds when at least one evaluator remains honest.

3. **Characterize role overlap effects.** Design experiments where evaluator roles intentionally overlap (e.g., syntax + logic combined) and measure tradeoff between redundancy and robustness to determine optimal role specialization boundaries.