---
ver: rpa2
title: Recurrent Complex-Weighted Autoencoders for Unsupervised Object Discovery
arxiv_id: '2405.17283'
source_url: https://arxiv.org/abs/2405.17283
tags:
- phase
- syncx
- complex-valued
- object
- grouping
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a recurrent autoencoder architecture, SynCx,
  for unsupervised object discovery. SynCx uses complex-valued weights to process
  complex-valued activations at every layer, iteratively updating phase maps to reconstruct
  input images.
---

# Recurrent Complex-Weighted Autoencoders for Unsupervised Object Discovery

## Quick Facts
- **arXiv ID**: 2405.17283
- **Source URL**: https://arxiv.org/abs/2405.17283
- **Reference count**: 40
- **Primary result**: SynCx achieves ARI scores of 0.89, 0.82, and 0.59 on Tetrominoes, dSprites, and CLEVR datasets respectively, outperforming or matching synchrony-based baselines

## Executive Summary
This paper introduces SynCx, a recurrent complex-weighted autoencoder for unsupervised object discovery. Unlike prior synchrony-based models that require additional mechanisms like χ-binding or contrastive training, SynCx achieves binding through simple matrix-vector products using complex-valued weights and activations. The model iteratively updates phase maps while keeping magnitudes clamped to the input image, converging to globally consistent object assignments. SynCx demonstrates strong performance on synthetic datasets, showing particular robustness in separating similarly colored objects without supervision.

## Method Summary
SynCx is a recurrent autoencoder that processes complex-valued activations and weights through iterative phase updates. The model consists of an encoder that compresses the input through complex convolutions with stride 2, creating a representational bottleneck, followed by a decoder that reconstructs the image using nearest-neighbor upsampling and complex convolutions. At each iteration, the decoder's phase output becomes the next input phase while magnitudes remain clamped to the original image. The model is trained with MSE loss on magnitude across multiple iterations, and object assignments are extracted by clustering phase components from the decoder's penultimate layer using k-means.

## Key Results
- Outperforms or matches state-of-the-art synchrony-based baselines on Tetrominoes (ARI: 0.89), dSprites (ARI: 0.82), and CLEVR (ARI: 0.59)
- More robust grouping performance, particularly separating similarly colored objects without supervision
- Shows less reliance on color cues compared to baselines like RF
- Iterative structure allows propagation of local constraints into globally consistent phase assignments

## Why This Works (Mechanism)

### Mechanism 1
Iterative phase updates allow spatially local constraints to propagate into globally consistent object assignments. The model initializes phases randomly and uses recurrent updates where decoder output phases become next input phases. Complex-valued weights enable binding via matrix-vector products without additional mechanisms. Local convolutions ensure spatial locality of constraints that propagate over iterations toward fixed-point convergence.

### Mechanism 2
The representational bottleneck forces the model to use phase to encode statistical regularities, compressing features into higher-order object representations. By reducing spatial resolution, the model must compress the input image. Complex-valued activations enable this compression by encoding relationships (via phase) among lower-order features into higher-order features. This specialization emerges from the need to compress regularities.

### Mechanism 3
Complex-valued weights allow constructive/destructive interference between activations, enabling richer feature-phase configurations without extra binding mechanisms. Complex multiplication naturally produces interference effects where matrix-vector products encode feature configurations through phase relationships. This inherent interference captures relevant phase relationships for binding, eliminating the need for additional gating or contrastive training.

## Foundational Learning

- **Complex numbers and complex algebra**: The model's weights and activations are complex-valued, and binding occurs through complex matrix-vector products. *Quick check*: What is the result of multiplying two complex numbers in polar form? (Answer: magnitudes multiply, phases add)
- **Autoencoder architecture and bottleneck layers**: SynCx uses a fully convolutional autoencoder with a bottleneck to force compression and phase-based encoding. *Quick check*: What happens to spatial resolution as you pass through encoder layers with stride > 1? (Answer: Resolution decreases, creating a bottleneck)
- **Unsupervised object discovery and evaluation with ARI**: The task is to discover objects without supervision and measure success via Adjusted Rand Index. *Quick check*: What does an ARI score of 1 indicate? (Answer: Perfect match between predicted and ground-truth object assignments)

## Architecture Onboarding

- **Component map**: Input magnitude clamped, phase randomly initialized -> Encoder (complex convolutions with stride 2) -> Bottleneck (small spatial map, high-dimensional complex features) -> Decoder (nearest-neighbor upsampling, complex convolutions) -> Output (reconstructed magnitude, phase for next iteration)
- **Critical path**: 1) Random phase initialization, 2) Forward pass through encoder/decoder (N times), 3) Backpropagation of MSE loss across iterations, 4) Phase update from decoder output to next input, 5) Convergence to fixed point
- **Design tradeoffs**: More iterations → better convergence but higher compute; Wider bottleneck → more capacity but less phase specialization; Complex weights → richer interference but higher parameter count
- **Failure signatures**: Phase maps show no specialization toward objects; Reconstruction MSE low but ARI low (model memorizes); Phase updates diverge or oscillate over iterations
- **First 3 experiments**: 1) Train with 1 iteration vs. 3 iterations to see effect of recurrence, 2) Remove bottleneck (no stride) to test necessity of compression, 3) Replace complex weights with real weights to test necessity of interference

## Open Questions the Paper Calls Out

### Open Question 1
How does phase initialization distribution (von-Mises vs uniform) quantitatively impact the model's ability to discover objects across various datasets? The paper shows von-Mises initialization yields better ARI scores but doesn't analyze why or test varying concentration parameters.

### Open Question 2
Can SynCx's binding mechanism generalize to more complex naturalistic scenes beyond synthetic datasets? The paper focuses on synthetic datasets and notes synchrony-based models struggle with naturalistic images, but doesn't test on real-world datasets.

### Open Question 3
How sensitive is SynCx's grouping performance to the choice of layer from which object assignments are extracted? The paper extracts from the penultimate decoder layer but doesn't explore impact of choosing different layers or whether deeper layers consistently provide better object-centric representations.

## Limitations
- Exact architectural specifications (layer counts, kernel sizes, channel dimensions) are not fully specified
- Sufficiency of theoretical justification for phase-based object binding remains unclear
- Reliance on complex-valued weight interactions whose interpretability is not fully established

## Confidence

- Mechanism 1 (Phase propagation): **Medium** - Supported by ablation studies and visualizations, but lacks theoretical convergence guarantees
- Mechanism 2 (Bottleneck compression): **Low** - No direct evidence provided that bottlenecks in complex-valued autoencoders lead to phase specialization for objects
- Mechanism 3 (Complex interference): **Medium** - Demonstrated empirically but no comparative analysis against real-valued baselines with similar capacity

## Next Checks
1. Conduct ablation studies removing the representational bottleneck to quantify its contribution to ARI performance versus model capacity effects
2. Implement and compare against a real-valued autoencoder with identical architecture to isolate benefits of complex-valued operations
3. Analyze phase convergence dynamics across iterations using phase difference metrics between consecutive steps to verify fixed-point behavior