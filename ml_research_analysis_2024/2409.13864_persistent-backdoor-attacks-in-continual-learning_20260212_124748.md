---
ver: rpa2
title: Persistent Backdoor Attacks in Continual Learning
arxiv_id: '2409.13864'
source_url: https://arxiv.org/abs/2409.13864
tags:
- backdoor
- task
- attack
- tasks
- attacks
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This work identifies that backdoor attacks lose effectiveness
  in continual learning (CL) settings as model parameters evolve and erase embedded
  triggers. To address this, the authors propose two persistent backdoor attacks:
  Blind Task Backdoor, which modifies loss computation across all tasks, and Latent
  Task Backdoor, which embeds triggers into the most stable neurons of a single task.'
---

# Persistent Backdoor Attacks in Continual Learning

## Quick Facts
- arXiv ID: 2409.13864
- Source URL: https://arxiv.org/abs/2409.13864
- Reference count: 40
- Key outcome: Proposed attacks achieve >90% ASR across six CL algorithms while evading SentiNet and I-BAU defenses

## Executive Summary
This paper addresses the fundamental challenge that backdoor attacks lose effectiveness in continual learning (CL) settings as model parameters evolve and erase embedded triggers. The authors propose two persistent backdoor attacks: Blind Task Backdoor, which modifies loss computation across all tasks, and Latent Task Backdoor, which embeds triggers into the most stable neurons of a single task. Experiments on three datasets with six CL algorithms and two architectures demonstrate both attacks maintain high attack success rates while preserving classification accuracy, even against existing defenses like SentiNet and I-BAU.

## Method Summary
The paper proposes two persistent backdoor attacks for continual learning environments. Blind Task Backdoor subtly alters loss computation without direct training control by introducing penalty coefficients to balance attack success with classification accuracy across all tasks. Latent Task Backdoor embeds triggers into the most stable neurons of a single task, identified using Fisher importance matrix analysis to find neurons with minimal parameter variation across task sequences. The attacks are evaluated on SplitMNIST, PermutedMNIST, and SplitCIFAR10 datasets using six CL algorithms (SI, EWC, XdG, LwF, DGR, A-GEM) with CNN and ResNet18 architectures.

## Key Results
- Both attacks achieve over 90% attack success rate while maintaining high classification accuracy
- Blind Task Backdoor shows robust performance across all six CL algorithms
- Latent Task Backdoor demonstrates effectiveness with minimal neuron overlap (6.4% maximum) between tasks
- Attacks successfully evade existing defenses including SentiNet and I-BAU
- Effectiveness demonstrated across static, dynamic, physical, and text-based triggers

## Why This Works (Mechanism)

### Mechanism 1
Embedding triggers in stable neurons prevents backdoor erasure during CL updates. The model identifies neurons with low parameter variation across tasks using Fisher importance matrix. These neurons maintain consistent values even as other parameters change, creating a persistent trigger location. Core assumption: Neuron stability correlates with importance for task performance and can be measured reliably via Fisher diagonal. Break condition: If task distributions change fundamentally, requiring previously stable neurons to adapt significantly.

### Mechanism 2
Modified loss computation preserves backdoor across all tasks by constraining parameter changes. Blind Task Backdoor adds penalty terms to loss function that balance attack success with classification accuracy across all tasks. The constraint ensures previous task loss doesn't increase beyond threshold. Core assumption: Multi-objective optimization can find Pareto optimal balance between backdoor persistence and clean performance. Break condition: If regularization strength is too high, attack may fail; if too low, clean accuracy degrades.

### Mechanism 3
Neuron overlap across tasks is minimal, enabling task-specific persistence. Different tasks rely on predominantly disjoint sets of neurons. By embedding triggers in task-specific stable neurons, the backdoor remains effective for its target task while being invisible to others. Core assumption: CL algorithms distribute task representations across different neuron populations with minimal overlap. Break condition: If tasks share significant neuron populations, backdoor interference may occur.

## Foundational Learning

- **Concept**: Catastrophic forgetting in neural networks
  - Why needed here: Understanding why CL algorithms exist and why backdoor attacks fail without persistence mechanisms
  - Quick check question: What happens to a model's performance on task A when it's trained on task B without any CL mechanism?

- **Concept**: Parameter variation measurement and analysis
  - Why needed here: Quantifying neuron stability and layer-wise changes is fundamental to the attack design
  - Quick check question: How would you measure the variation of a parameter across multiple training tasks?

- **Concept**: Multi-objective optimization and Pareto optimality
  - Why needed here: Blind backdoor requires balancing attack success, clean accuracy, and persistence constraints
  - Quick check question: What is the difference between a single-objective and multi-objective optimization problem?

## Architecture Onboarding

- **Component map**: Trigger embedding module -> Neuron selection component -> Loss modification system -> Model training -> Backdoor activation
- **Critical path**: Trigger embedding → Neuron selection → Loss modification → Model training → Backdoor activation
- **Design tradeoffs**: BTB offers broader applicability but requires loss function access; LTB is more stealthy but task-specific
- **Failure signatures**: ASR drops below 80% after 2-3 tasks; ACC degradation exceeds 10%; neuron stability measurements fail
- **First 3 experiments**:
  1. Validate neuron stability measurement on simple CNN with SplitMNIST
  2. Test BTB with static trigger on single CL algorithm (LwF)
  3. Compare LTB vs BTB on ResNet18 with SplitCIFAR10 across multiple algorithms

## Open Questions the Paper Calls Out

### Open Question 1
How does the proposed persistent backdoor attack perform against adaptive defenses that actively monitor neuron stability across tasks? The paper demonstrates that stable neurons identified using the Diagonal Fisher Matrix remain effective for backdoor persistence, but doesn't evaluate defenses that could specifically target this mechanism. What evidence would resolve it: Experiments showing attack success rates against defenses that actively track and regularize neuron stability metrics during continual learning would clarify this.

### Open Question 2
What is the relationship between the percentage of stable neurons selected for trigger embedding and the persistence of the backdoor across different continual learning algorithms? The paper notes that replay-based algorithms require a higher percentage of neurons (90%) compared to regularization-based algorithms (70%) for effective trigger embedding. What evidence would resolve it: A comprehensive study varying the percentage of stable neurons selected across all CL algorithms would reveal optimal thresholds and their theoretical underpinnings.

### Open Question 3
How do persistent backdoor attacks affect the energy efficiency and computational overhead of continual learning systems during inference? The paper focuses on attack efficacy and defense evasion but doesn't address the practical implications of backdoor persistence on system resources. What evidence would resolve it: Empirical measurements of energy consumption and computational latency for backdoored versus clean models during inference across various CL scenarios would provide this insight.

## Limitations
- Neuron stability mechanism lacks theoretical guarantees about functional importance preservation across task sequences
- Multi-objective optimization approach has limited theoretical grounding for the specific constraint formulation
- Task-specific neuron isolation based on single-dataset overlap analysis may not generalize to high task similarity scenarios

## Confidence
- Neuron stability persistence mechanism: Medium confidence - validated through empirical neuron variation analysis but lacks theoretical guarantees
- Loss modification optimization approach: Low confidence - heuristic constraint formulation with limited ablation studies
- Task-specific neuron isolation: Medium confidence - supported by single-dataset overlap analysis but needs broader validation

## Next Checks
1. **Neuron stability transferability test**: Apply the neuron selection methodology to CL scenarios with deliberately similar tasks (e.g., CIFAR10→CIFAR100) to evaluate whether stable neurons remain task-specific or become shared across related tasks.

2. **Constraint sensitivity analysis**: Systematically vary the regularization coefficients (λ, µ, γ) in the Blind Task Backdoor formulation to quantify the robustness of the Pareto-optimal balance and identify failure thresholds.

3. **Cross-algorithm stability validation**: Test the neuron stability measurements and subsequent LTB effectiveness across a broader range of CL algorithms beyond the six tested, particularly those using different regularization strategies.