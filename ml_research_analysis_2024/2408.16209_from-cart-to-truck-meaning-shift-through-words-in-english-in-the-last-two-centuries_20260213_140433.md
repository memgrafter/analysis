---
ver: rpa2
title: 'From cart to truck: meaning shift through words in English in the last two
  centuries'
arxiv_id: '2408.16209'
source_url: https://arxiv.org/abs/2408.16209
tags:
- word
- words
- embeddings
- concepts
- electricity
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study uses diachronic word embeddings to trace how words representing
  the same concepts have changed over time from 1800 to 2000. The method involves
  training word2vec skip-gram models per decade, cleaning zeroed embeddings, and aligning
  them using orthogonal Procrustes to the 1990s.
---

# From cart to truck: meaning shift through words in English in the last two centuries

## Quick Facts
- **arXiv ID:** 2408.16209
- **Source URL:** https://arxiv.org/abs/2408.16209
- **Reference count:** 40
- **Primary result:** Diachronic word embeddings reveal how concepts like "truck" were once linked to "cart," or "aircraft" to "ship," uncovering societal and technological evolution.

## Executive Summary
This study uses diachronic word embeddings to trace how words representing the same concepts have changed over time from 1800 to 2000. The method involves training word2vec skip-gram models per decade, cleaning zeroed embeddings, and aligning them using orthogonal Procrustes to the 1990s. It identifies temporal analogies showing how concepts like "truck" were once linked to "cart," or "aircraft" to "ship," revealing societal and technological evolution. Results show coal and steam as early analogues to modern energy terms, and transport terms shifting from animal-drawn vehicles to modern machinery. Computing terms moved from mathematics and science to digital concepts. The study highlights language adaptation but notes limitations due to corpus bias, coarse temporal granularity, and potential model biases, stressing the need for expert interpretation.

## Method Summary
The study uses pre-trained word2vec skip-gram embeddings from the Google Books N-Gram corpus, trained per decade from 1800s to 1990s. Zeroed embeddings are removed, and each decade's embeddings are aligned to the 1990s using orthogonal Procrustes. For each concept and decade, the top N most similar words are retrieved using cosine similarity. Temporal analogies are identified by comparing these neighbor sets across decades to reveal how different words represented the same concept over time.

## Key Results
- Transport terms shifted from "cart," "carriage," "coach" to "truck," "car," "bus" over two centuries
- Energy terms evolved from "coal," "steam," "iron" to "nuclear," "solar," "fuel"
- Computing terms moved from "mathematics," "science," "engineering" to "digital," "virtual," "online"
- Entertainment shifted from "phonograph," "piano," "music" to "film," "television," "radio"

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Diachronic word embeddings trained per decade capture semantic shifts in a stable vector space that allows meaningful comparison across time.
- Mechanism: Word2vec skip-gram learns dense vector representations where similar words are closer in embedding space. Training per decade produces distinct but comparable spaces. Orthogonal Procrustes alignment minimizes rotation between decade embeddings and a reference decade (1990s), enabling cross-decade analogy computation.
- Core assumption: Word embeddings preserve relative similarity structure across different corpora and time periods.
- Evidence anchors:
  - [abstract] "using diachronic word embeddings trained using word2vec with skipgram and aligning them using orthogonal Procrustes"
  - [section] "Word embedding techniques, such as word2vec (Mikolov et al., 2013), were employed to map words to dense vectors. These techniques are recognized for their ability to generate embeddings in which words with similar meanings exhibit similar vector representations"
- Break condition: If embeddings drift so far that relative similarities are lost, or if alignment introduces excessive distortion, temporal analogies will be meaningless.

### Mechanism 2
- Claim: Temporal analogies reveal onomasiological shifts by showing which words are most similar to a target word's modern embedding in each historical decade.
- Mechanism: For each decade, the algorithm retrieves the top N nearest neighbors of a concept's embedding in that decade's space. Comparing these neighbor sets across decades reveals how different words represented the same underlying concept over time.
- Core assumption: Nearest neighbor retrieval in embedding space is a valid proxy for identifying synonymous or analogous terms across time.
- Evidence anchors:
  - [abstract] "identify shifts in energy, transport, entertainment, and computing domains"
  - [section] "For each concept, we identified the top N words most similar to an embedding at each period"
- Break condition: If the embedding space is too sparse, or if temporal granularity is too coarse, similar words may not be captured or may be incorrectly grouped.

### Mechanism 3
- Claim: The method can uncover societal and technological evolution by linking historical terminology to modern analogues through embedding similarity.
- Mechanism: By interpreting the nearest neighbors of modern concepts in past decades, the method infers historical equivalents or related terms, revealing how technology, culture, or society shaped word usage.
- Core assumption: Historical word usage patterns are encoded in the embeddings and can be interpreted by modern readers with contextual knowledge.
- Evidence anchors:
  - [abstract] "revealing connections between language and societal changes"
  - [section] "Our analysis highlights language adaptation to technology, society, and perception changes"
- Break condition: If the training corpus is heavily biased (e.g., toward scientific literature), the retrieved neighbors may not reflect general usage, leading to misleading conclusions.

## Foundational Learning

- Concept: Word embeddings and distributional semantics
  - Why needed here: The entire method depends on representing words as dense vectors where similarity reflects semantic relatedness. Understanding how embeddings are trained and how similarity is computed is essential for interpreting results.
  - Quick check question: What property of word embeddings allows us to find words with similar meanings using cosine similarity?

- Concept: Orthogonal Procrustes alignment
  - Why needed here: The method requires aligning embeddings from different decades so that comparisons are meaningful. Procrustes finds the rotation matrix that best aligns two sets of vectors in a least-squares sense.
  - Quick check question: Why is orthogonal (rotation-only) alignment preferred over general affine transformations in this context?

- Concept: Onomasiology vs. semasiology
  - Why needed here: The study explicitly takes an onomasiological approach (studying how different words represent the same concept over time), which is less common than semasiological studies. Understanding this distinction is key to interpreting the results.
  - Quick check question: What is the main difference between studying how a word's meaning changes over time (semasiology) versus how different words represent the same concept over time (onomasiology)?

## Architecture Onboarding

- Component map:
  - Data layer: Google N-Grams corpus, split by decade (1800-2000)
  - Model layer: Word2vec skip-gram trained per decade
  - Cleaning layer: Remove zeroed embeddings, align with orthogonal Procrustes
  - Query layer: similar_by_vector function (Gensim) to retrieve nearest neighbors
  - Analysis layer: Interpret temporal analogies across domains

- Critical path:
  1. Load and clean decade embeddings
  2. Align all decade embeddings to a reference decade (1990s)
  3. For each concept and decade, retrieve top N similar words
  4. Compare neighbor sets across decades to identify shifts

- Design tradeoffs:
  - Decadal granularity vs. capturing fine-grained shifts (trade coarse-grained trends for potential noise)
  - Skip-gram vs. contextual embeddings (faster, more stable vs. richer context but resource-heavy)
  - Alignment to 1990s vs. another decade (modern relevance vs. historical anchoring)

- Failure signatures:
  - Too few words remain after cleaning (corpus sparsity)
  - Aligned embeddings show high distortion (alignment failure)
  - Neighbor lists are dominated by stop words or corpus artifacts (bias or poor embeddings)

- First 3 experiments:
  1. Train and align embeddings for a small set of clearly related words (e.g., "car", "automobile", "vehicle") to verify alignment quality.
  2. Compare nearest neighbors for a word across adjacent decades to check for gradual change.
  3. Manually inspect neighbor lists for a known historical concept (e.g., "telegraph") to validate onomasiological interpretation.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How would using contextual embeddings like BERT instead of static word2vec embeddings affect the quality and granularity of detected semantic shifts?
- Basis in paper: [inferred] The paper mentions that while word2vec skip-gram embeddings were used due to availability and ease of training, more advanced contextual embedding models like BERT are now accessible. However, the utilization of these newer models comes with a trade-off between their enhanced capabilities and the available resources.
- Why unresolved: The paper did not explore the use of contextual embeddings like BERT, which could potentially provide more nuanced representations of word meanings in different contexts.
- What evidence would resolve it: Conducting the same study using BERT or other contextual embeddings and comparing the results with the word2vec-based findings could reveal whether contextual embeddings provide better granularity in detecting semantic shifts.

### Open Question 2
- Question: How would reducing the time interval granularity from decades to years or even shorter periods impact the detection of semantic shifts?
- Basis in paper: [explicit] The paper states that the embeddings used were trained in discrete ten-year periods, and the granularity of these timeframes might not adequately capture nuanced shifts in meaning. It suggests that a potential approach to addressing this limitation is to repeat the study with finer time intervals, which could unveil more subtle changes.
- Why unresolved: The paper used embeddings trained on ten-year intervals, which may have missed more subtle semantic shifts that could occur within shorter time periods.
- What evidence would resolve it: Training embeddings on shorter time intervals (e.g., yearly or even monthly) and comparing the detected semantic shifts with those found using the decade-level embeddings could reveal whether finer granularity leads to more accurate and detailed results.

### Open Question 3
- Question: How do the biases in the Google Books N-Gram corpus, particularly its skew towards scientific literature, impact the detected semantic shifts and their interpretation?
- Basis in paper: [explicit] The paper mentions that the utilized word embeddings were trained on the Google Books N-Gram corpus, which is known to possess a bias towards scientific literature. It emphasizes the importance of recognizing that this dataset may not offer a fully representative or randomly sampled reflection of the entire English language.
- Why unresolved: The paper acknowledges the potential bias in the corpus but does not explore how this bias might influence the detected semantic shifts or their interpretation.
- What evidence would resolve it: Comparing the detected semantic shifts using embeddings trained on different corpora (e.g., a more balanced corpus or one focused on a specific domain) could help assess the impact of corpus bias on the results and interpretations.

## Limitations
- Corpus bias toward scientific literature may skew detected semantic shifts away from general language use
- Decadal granularity is coarse, potentially missing finer-grained semantic shifts
- Alignment to 1990s may introduce distortions, especially for earlier centuries with different vocabulary and usage

## Confidence
- **High**: The mechanism of using orthogonal Procrustes to align decade embeddings is mathematically sound and well-established in the literature
- **Medium**: The onomasiological approach of retrieving nearest neighbors to track conceptual evolution is valid, but interpretation requires careful contextual knowledge
- **Medium**: The examples provided (e.g., "truck" → "cart", "aircraft" → "ship") are plausible, but broader validation across more concepts is needed

## Next Checks
1. Validate alignment quality by computing reconstruction error (Frobenius norm) after Procrustes alignment and checking if it remains low across all decades
2. Test robustness by repeating the analysis with a different reference decade (e.g., 1980s) and comparing results to assess sensitivity to alignment choice
3. Manually inspect neighbor lists for a set of known historical concepts (e.g., "telegraph", "horse", "phonograph") to verify that retrieved terms are historically accurate analogues