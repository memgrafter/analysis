---
ver: rpa2
title: 'Large Language Models as Conversational Movie Recommenders: A User Study'
arxiv_id: '2404.19093'
source_url: https://arxiv.org/abs/2404.19093
tags:
- user
- users
- recommendations
- recommendation
- recommender
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a user study evaluating the effectiveness of
  large language models (LLMs) as conversational movie recommenders. The study involved
  160 active users interacting with an LLM-based chatbot interface (LLMRec) in three
  different recommendation scenarios (birthday, long trip, niche).
---

# Large Language Models as Conversational Movie Recommenders: A User Study

## Quick Facts
- arXiv ID: 2404.19093
- Source URL: https://arxiv.org/abs/2404.19093
- Reference count: 40
- Primary result: LLM-based conversational recommenders excel at explainability but struggle with personalization, diversity, and trustworthiness compared to traditional systems.

## Executive Summary
This paper presents a user study evaluating large language models as conversational movie recommenders through a chatbot interface (LLMRec). The study involved 160 active users interacting with an LLM-based system across three recommendation scenarios, comparing experiences with a classic recommender system. Results show LLMs excel in explainability and creating interactive experiences, but struggle with personalization, diversity, novelty, and trustworthiness. Different personalized prompting techniques (zero-shot, one-shot, few-shot) did not significantly impact perceived recommendation quality, while user consumption history played a significant role in satisfaction.

## Method Summary
The study recruited 160 active users from an online movie recommender system, randomly assigning them to one of three prompting techniques (zero-shot, one-shot, few-shot). Users interacted with a Gradio chatbot interface powered by Llama2-7b-Chat across three recommendation scenarios (birthday, long trip, niche), providing feedback through surveys after each interaction. The evaluation framework compared LLMRec to a traditional recommender system on multiple dimensions including personalization, diversity, novelty, and explainability. Conversation logs and survey responses were analyzed using statistical tests and qualitative coding.

## Key Results
- LLMs provide superior explainability compared to traditional recommenders, with users appreciating natural language justifications for recommendations
- Different prompting techniques (zero-shot, one-shot, few-shot) showed no significant impact on user-perceived recommendation quality
- User consumption history significantly influences satisfaction, with heavy raters finding LLM recommendations less personalized and novel
- User context provided during conversations strongly correlates with recommendation satisfaction

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLMs provide strong explainability through natural language justifications
- Mechanism: The model is explicitly instructed to give explanations including predicted ratings and popularity levels
- Core assumption: Training on large text corpora enables coherent explanation generation
- Evidence anchors:
  - [abstract] "Our results also indicate that different personalized prompting techniques do not significantly affect user-perceived recommendation quality"
  - [section 4.1.2] "P368 also enjoyed getting explanations on why they got certain recommendations"

### Mechanism 2
- Claim: User satisfaction depends heavily on contextual information provided
- Mechanism: The model leverages in-context learning from user-provided examples and preferences
- Core assumption: Specific examples and clear requirements lead to better recommendations
- Evidence anchors:
  - [section 4.3.1] "when users provided specific information such as previously watched movies, expressed their preferences...their satisfaction with recommendations tended to be higher"
  - [section 4.3.2] "A key finding was that the main factor contributing to user satisfaction with recommendations was the context provided by users"

### Mechanism 3
- Claim: Historical consumption patterns influence perception of recommendation quality
- Mechanism: Users with different consumption levels have different expectations
- Core assumption: Heavy raters have more refined tastes and higher expectations
- Evidence anchors:
  - [section 4.2] "Using ANOVA, we found statistically significant differences among these groups for several metrics"
  - [section 4.2] "Heavy raters, however, were less satisfied with LLMRec, reporting it was less personalized"

## Foundational Learning

- **Concept**: In-context learning
  - Why needed here: Understanding how LLMs learn from examples within prompts is crucial for designing effective recommendation strategies
  - Quick check question: How does providing more examples in the prompt affect the quality of recommendations in few-shot vs zero-shot settings?

- **Concept**: User satisfaction metrics in recommender systems
  - Why needed here: The study uses multiple dimensions to evaluate recommendations, requiring understanding of multi-dimensional user satisfaction
  - Quick check question: Why might users rate explainability higher but personalization lower in LLM-based recommendations?

- **Concept**: Statistical significance testing
  - Why needed here: The study uses ANOVA and Tukey's HSD tests to determine if differences between groups are meaningful
  - Quick check question: What does it mean when the study says "different prompting techniques do not significantly impact user-perceived recommendation quality"?

## Architecture Onboarding

- **Component map**: User Interface -> LLM Model -> Prompt Engineering Layer -> Evaluation Framework -> Database Integration
- **Critical path**: 1) User logs in and provides context, 2) System generates personalized prompt, 3) LLM processes request and generates recommendations, 4) User provides feedback through survey, 5) System logs interaction for analysis
- **Design tradeoffs**: Model size vs. response time (7B parameter model chosen for balance), prompt complexity vs. consistency (different prompting techniques tested), user context collection vs. user effort (tradeoff between quality and friction)
- **Failure signatures**: High conversation length with low satisfaction scores, repeated requests for similar recommendations, user testing system boundaries rather than seeking recommendations, low explainability scores despite high satisfaction
- **First 3 experiments**: 1) Test impact of different prompt structures on recommendation quality, 2) Measure correlation between user context provided and recommendation satisfaction, 3) Compare recommendation quality across different user consumption patterns (light vs heavy raters)

## Open Questions the Paper Calls Out

- **Open Question 1**: How do different user demographics (age, gender, education level) affect user satisfaction with LLM-based recommenders?
  - Basis in paper: [inferred] The paper found that users who rated more movies had different satisfaction levels
  - Why unresolved: The study only collected basic demographic information but did not analyze how these factors influenced satisfaction
  - What evidence would resolve it: Analyzing user satisfaction scores across different demographic groups

- **Open Question 2**: How does the length of conversation with LLMRec impact the quality and personalization of recommendations over time?
  - Basis in paper: [explicit] The paper found a negative relationship between conversation length and user satisfaction
  - Why unresolved: While the study identified this relationship, it did not explore how conversation length affects recommendation quality over time
  - What evidence would resolve it: Analyzing quality and personalization at different conversation stages

- **Open Question 3**: How do different prompting techniques (e.g., chain-of-thought, tree-of-thought) affect user satisfaction compared to zero-shot, one-shot, and few-shot prompting?
  - Basis in paper: [explicit] The paper tested three techniques but found no significant differences
  - Why unresolved: The study did not explore more advanced prompting techniques that could potentially improve satisfaction
  - What evidence would resolve it: Comparing satisfaction across different prompting techniques including chain-of-thought and tree-of-thought

## Limitations

- The study relies on 160 active users from a single movie recommender platform, limiting generalizability to casual movie viewers or different platforms
- Only the Llama2-7b-Chat model was evaluated, preventing insights about how different LLM architectures or sizes might perform
- Self-reported survey responses may be influenced by social desirability bias, novelty effects, or participants' desire to provide helpful feedback

## Confidence

**High Confidence Claims**:
- LLMRec demonstrates superior explainability compared to traditional recommenders
- Users who provide more context receive better recommendations
- Heavy raters consistently rate LLM recommendations lower than light/medium raters

**Medium Confidence Claims**:
- Different prompting techniques do not significantly impact recommendation quality
- LLMRec struggles with personalization, diversity, and novelty compared to traditional systems
- User conversation patterns significantly influence recommendation outcomes

**Low Confidence Claims**:
- The specific quantitative differences between LLMRec and MovieRec performance metrics
- The exact magnitude of improvement from providing context versus minimal information
- Long-term user satisfaction and adoption potential of LLM-based recommenders

## Next Checks

1. **Cross-platform validation**: Replicate the study with users from multiple movie recommendation platforms and different demographic backgrounds to assess generalizability beyond MovieRec's active user base.

2. **Model architecture comparison**: Test the same evaluation framework with different LLM sizes (smaller models like 1B-3B parameters and larger models like 13B-70B parameters) to determine if performance improvements scale with model size.

3. **Longitudinal user study**: Conduct a follow-up study tracking the same users over 3-6 months to measure whether initial satisfaction differences persist and whether users actually consume the recommended movies, providing behavioral validation beyond self-reported metrics.