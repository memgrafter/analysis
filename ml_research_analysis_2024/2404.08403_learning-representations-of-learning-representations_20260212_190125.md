---
ver: rpa2
title: Learning representations of learning representations
arxiv_id: '2404.08403'
source_url: https://arxiv.org/abs/2404.08403
tags:
- learning
- iclr
- machine
- representations
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study presents the ICLR dataset containing abstracts and metadata
  from all ICLR submissions from 2017-2024. The authors benchmark various sentence
  transformer models on kNN classification accuracy using this dataset.
---

# Learning representations of learning representations

## Quick Facts
- arXiv ID: 2404.08403
- Source URL: https://arxiv.org/abs/2404.08403
- Reference count: 12
- Primary result: TF-IDF bag-of-words representation outperforms most modern sentence transformer models on kNN classification accuracy using the ICLR abstract dataset

## Executive Summary
This study introduces the ICLR dataset containing abstracts and metadata from all ICLR submissions from 2017-2024. The authors benchmark various sentence transformer models on kNN classification accuracy using this dataset, finding that classic TF-IDF bag-of-words representation surprisingly outperforms most modern sentence transformers. The dataset also enables analysis of trends in machine learning research over time, revealing improvements in gender balance and shifts in research topics. The results suggest that substantially improving upon TF-IDF remains an open challenge for the NLP community.

## Method Summary
The authors assembled the ICLR dataset by extracting abstracts and metadata from all ICLR submissions between 2017-2024 using the OpenReview API. They evaluated multiple sentence transformer models (SBERT, SciNCL, SPECTER2, and others) alongside TF-IDF using kNN classification accuracy (k=10, 10-fold cross-validation). The embeddings were generated using pre-trained models, and performance was compared to establish a benchmark for abstract representation. The dataset also enabled qualitative analysis of research trends through keyword extraction and manual labeling of topics.

## Key Results
- TF-IDF bag-of-words representation outperforms most dedicated sentence transformer models in kNN classification accuracy on the ICLR abstract dataset
- The three models specifically trained to represent scientific abstracts (SciNCL, SPECTER2, SPECTER2-L) all had lower kNN accuracy than TF-IDF
- Top performing language models (Cohere, OpenAI) barely outperform TF-IDF, suggesting substantial improvement remains an open NLP challenge

## Why This Works (Mechanism)

### Mechanism 1
TF-IDF performs competitively because abstracts are concise and topic-specific, reducing the vocabulary complexity that modern transformers are designed to handle. Abstracts contain focused, technical language with high-frequency domain-specific terms. TF-IDF captures these key terms effectively, and the limited vocabulary size in abstracts reduces the need for contextual embeddings that transformers provide. The core assumption is that the abstract length and domain specificity make word frequency patterns sufficient for capturing semantic meaning. Break Condition: If applied to longer documents with broader vocabulary or more nuanced semantic relationships, TF-IDF would likely underperform compared to transformer-based methods.

### Mechanism 2
The kNN classification task may not fully leverage the strengths of transformer-based contextual embeddings. kNN classification in high-dimensional space measures local neighborhood similarity, which TF-IDF captures reasonably well for abstract-level representations. Transformers excel at capturing long-range dependencies and nuanced context that kNN may not fully exploit. The core assumption is that kNN classification is not the optimal evaluation metric for sentence transformer representations in this specific dataset. Break Condition: If the evaluation metric were changed to something like semantic textual similarity or downstream task performance, transformer models might show stronger performance.

### Mechanism 3
The dataset's consistency and standardization across years may reduce the advantage of pre-trained transformer models. ICLR abstracts follow similar formatting and technical writing conventions. This consistency means the vocabulary and structure don't vary significantly enough to require the generalization capabilities that transformers provide over simpler models. The core assumption is that the dataset's homogeneity reduces the need for complex contextual understanding. Break Condition: If the dataset included more diverse document types or writing styles, transformer models would likely show greater advantage.

## Foundational Learning

- Concept: Vector Space Models
  - Why needed here: Understanding how TF-IDF creates vector representations and how kNN operates in this space is fundamental to interpreting the results
  - Quick check question: What does each dimension represent in a TF-IDF vector space for document representations?

- Concept: Sentence Transformer Architectures
  - Why needed here: Knowing how models like SBERT, SciNCL, and SPECTER2 differ in their training objectives and architectural choices helps explain their varying performance
  - Quick check question: How does the training objective (e.g., contrastive loss vs. masked language modeling) affect the final sentence embeddings?

- Concept: Dimensionality Reduction Techniques
  - Why needed here: Understanding t-SNE and SVD helps interpret the 2D visualizations and why certain representations perform better after dimensionality reduction
  - Quick check question: Why might SVD with L2 normalization improve kNN accuracy compared to raw TF-IDF?

## Architecture Onboarding

- Component Map:
  - OpenReview API -> Metadata extraction -> Keyword-based labeling -> Train/test split
  - TF-IDF (scikit-learn), Sentence Transformers (Hugging Face), Commercial APIs (Cohere, OpenAI) -> Embedding Generation
  - kNN classification (scikit-learn) with 10-fold cross-validation -> Evaluation
  - t-SNE embedding (openTSNE) -> 2D Visualization

- Critical Path:
  1. Dataset assembly and labeling (prerequisite for all experiments)
  2. Embedding generation for each model
  3. kNN evaluation and comparison
  4. t-SNE visualization for qualitative analysis

- Design Tradeoffs:
  - TF-IDF: Fast, interpretable, but lacks semantic understanding
  - Sentence Transformers: Better semantic capture but computationally expensive
  - Commercial APIs: State-of-the-art performance but black-box and costly
  - kNN evaluation: Simple and relevant for visualization but may not capture all model strengths

- Failure Signatures:
  - Poor kNN accuracy: Model not capturing relevant semantic distinctions
  - Unstable t-SNE visualizations: Inappropriate perplexity or learning rate parameters
  - Inconsistent results across runs: Random seed issues or non-deterministic components

- First 3 Experiments:
  1. Baseline TF-IDF evaluation: Generate TF-IDF vectors and measure kNN accuracy to establish the performance floor
  2. Single sentence transformer comparison: Test one model (e.g., SBERT) to verify the implementation and evaluation pipeline
  3. Dimensionality reduction impact: Apply SVD to TF-IDF and compare kNN accuracy to understand the effect of dimensionality reduction

## Open Questions the Paper Calls Out

### Open Question 1
What specific architectural modifications or training strategies could significantly improve sentence transformer performance beyond TF-IDF on the ICLR dataset? Basis: The authors state that "substantially outperforming TF-IDF representation remains an open NLP challenge" and that "the top performing language models barely outperform TF-IDF" on their kNN benchmark. Why unresolved: While the paper shows TF-IDF performs surprisingly well, it doesn't explore what architectural innovations or training approaches might bridge the gap between bag-of-words and transformer-based representations. What evidence would resolve it: Empirical results demonstrating a sentence transformer model that achieves at least 10% improvement in kNN accuracy over TF-IDF on the ICLR dataset using novel architectural or training innovations.

### Open Question 2
How do sentence transformer representations compare to TF-IDF when evaluated on downstream tasks beyond kNN classification, such as clustering quality or semantic search? Basis: The authors focus primarily on kNN classification accuracy but note that "the kNN graph quality is the only metric relevant for our application" while acknowledging that "modern benchmarks evaluate embedding models using various metrics". Why unresolved: The paper establishes kNN as a relevant metric for their specific application but doesn't explore whether TF-IDF's advantage extends to other evaluation paradigms that might be more relevant for different use cases. What evidence would resolve it: Comparative evaluation of TF-IDF vs. top-performing sentence transformers across multiple downstream tasks (clustering metrics, retrieval quality, semantic search relevance) on the ICLR dataset.

### Open Question 3
What are the specific linguistic or domain-specific features of machine learning abstracts that make TF-IDF so competitive, and can these be explicitly incorporated into transformer architectures? Basis: The authors find that "bag-of-words representation outperforms most dedicated sentence transformer models" and speculate about why complex transformer models don't significantly outperform simple word count representations. Why unresolved: The paper identifies the surprising performance of TF-IDF but doesn't analyze what aspects of ML abstracts (technical vocabulary, structured format, specific linguistic patterns) contribute to this phenomenon. What evidence would resolve it: Linguistic analysis identifying key features where TF-IDF excels on ML abstracts, followed by targeted architectural modifications to transformer models that explicitly incorporate these features.

## Limitations

- The kNN classification task may not fully capture the strengths of transformer-based models, particularly their ability to handle semantic nuance and long-range dependencies
- The dataset's consistency across years (standardized ICLR abstracts) may artificially favor simpler bag-of-words approaches
- Conclusions about broader trends in ML research are based on keyword extraction and manual labeling, which may introduce bias

## Confidence

- **High Confidence**: TF-IDF's competitive performance on kNN classification accuracy (directly measured and reproducible)
- **Medium Confidence**: Interpretation that this represents a "substantially open challenge" for the NLP community (requires broader context and alternative evaluation metrics)
- **Low Confidence**: Conclusions about broader trends in ML research (based on keyword extraction and manual labeling, which may introduce bias)

## Next Checks

1. Replicate the kNN classification results using alternative evaluation metrics such as semantic textual similarity scores or downstream task performance (e.g., information retrieval precision@K) to verify if TF-IDF's advantage holds across different assessment methods.

2. Test the models on a more diverse corpus of scientific abstracts from multiple venues to determine if the dataset's consistency artificially inflates TF-IDF's performance relative to transformer models.

3. Conduct ablation studies by varying abstract lengths and vocabulary complexity to identify the break points where transformer models begin to outperform TF-IDF, helping to understand the conditions under which each approach excels.