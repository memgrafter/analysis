---
ver: rpa2
title: How to Evaluate Reward Models for RLHF
arxiv_id: '2410.14872'
source_url: https://arxiv.org/abs/2410.14872
tags:
- reward
- arenahard
- alpacaeval
- preference
- human
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of efficiently evaluating reward
  models for Reinforcement Learning from Human Feedback (RLHF) without expensive end-to-end
  LLM training. The authors propose Preference Proxy Evaluations (PPE), a benchmark
  that evaluates reward models using crowdsourced human preference data and verifiable
  correctness datasets across 12 domains.
---

# How to Evaluate Reward Models for RLHF

## Quick Facts
- arXiv ID: 2410.14872
- Source URL: https://arxiv.org/abs/2410.14872
- Reference count: 33
- Primary result: Accuracy on human preference data predicts downstream RLHF success with 77% Pearson correlation

## Executive Summary
This paper addresses the challenge of evaluating reward models for Reinforcement Learning from Human Feedback (RLHF) without expensive end-to-end LLM training. The authors propose Preference Proxy Evaluations (PPE), a benchmark that evaluates reward models using crowdsourced human preference data and verifiable correctness datasets across 12 domains. Through a controlled experiment where different reward models are used to RLHF a base LLM, they demonstrate that accuracy on human preference data is the strongest predictor of downstream RLHF success. The PPE benchmark is fully open-sourced with 113,836 preference pairs and 81,760 correctness-labeled responses.

## Method Summary
The authors propose Preference Proxy Evaluations (PPE), a benchmark that evaluates reward models using crowdsourced human preference data and verifiable correctness datasets. They conduct a controlled experiment where different reward models are used to RLHF a base Llama-3.1-8B-Instruct model using Direct Preference Optimization (DPO). The trained models are then evaluated on Chatbot Arena, with the key finding being that accuracy on human preference data achieves 77% Pearson correlation with post-RLHF performance. The benchmark covers 12 domains and includes both human preference pairs (113,836) and correctness-labeled responses (81,760) from 5 static benchmarks.

## Key Results
- Accuracy on human preference data is the strongest predictor of downstream RLHF success (77% Pearson correlation)
- Lower-bound performance metrics are more predictive than average or upper-bound metrics
- Correctness metrics have complementary predictive power to human preference metrics
- PPE benchmark is fully open-sourced with comprehensive evaluation datasets

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Accuracy on human preference data is the strongest predictor of downstream RLHF success
- Mechanism: Granular measurement of human preference prediction captures the fine-grained alignment needed for successful RLHF training
- Core assumption: Human preference accuracy correlates with the ability to guide LLM training toward human-desired outputs
- Evidence anchors: 77% Pearson correlation with post-RLHF performance
- Break condition: If human preferences don't align with actual quality metrics or if reward models overfit to specific preference patterns

### Mechanism 2
- Claim: Lower-bound performance metrics are more predictive than average or upper-bound metrics
- Mechanism: RLHF training exploits weaknesses in reward models, so robust performance across all domains is essential
- Core assumption: RLHF training will find and exploit any weaknesses in the reward model
- Evidence anchors: Lower quantile aggregation showed higher correlation with downstream Arena Scores
- Break condition: If RLHF training doesn't exploit weaknesses or if the model learns to avoid problematic regions

### Mechanism 3
- Claim: Correctness metrics have complementary predictive power to human preference metrics
- Mechanism: Correctness provides an objective signal that captures aspects of quality not reflected in human preferences alone
- Core assumption: Expert humans would prefer correct answers over incorrect ones regardless of style
- Evidence anchors: Correctness and human preference do not necessarily align
- Break condition: If correctness doesn't correlate with human preference or if benchmarks are poorly constructed

## Foundational Learning

- Concept: Pearson correlation coefficient
  - Why needed here: Used to measure the relationship between reward model metrics and downstream RLHF performance
  - Quick check question: If a reward model has 0.8 Pearson correlation with human preference, what does this mean about the relationship between its performance and RLHF success?

- Concept: Reinforcement Learning from Human Feedback (RLHF)
  - Why needed here: The paper evaluates reward models specifically for their effectiveness in RLHF pipelines
  - Quick check question: What are the key components of an RLHF pipeline and how does the reward model fit into this architecture?

- Concept: Best-of-K evaluation
  - Why needed here: Used to measure how well reward models can select the best response from multiple samples
  - Quick check question: How does increasing K affect the evaluation of a reward model's ability to identify high-quality responses?

## Architecture Onboarding

- Component map: Reward model → PPE evaluation (human preference + correctness metrics) → DPO training → Chatbot Arena evaluation → correlation analysis
- Critical path: Reward model → PPE benchmark evaluation → DPO training → Chatbot Arena evaluation → correlation analysis
- Design tradeoffs: Using DPO vs PPO (DPO is offline but may have different reward model requirements), using 32 samples for K (balances cost vs exploration space)
- Failure signatures: Low correlation between PPE metrics and Arena scores, reward models overfitting to specific domains, benchmarks not representing real-world distributions
- First 3 experiments:
  1. Evaluate a new reward model on PPE and check correlation with existing results
  2. Test different K values in best-of-K evaluation to find optimal balance
  3. Compare DPO vs PPO training with same reward models to understand algorithm-specific requirements

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How would different RLHF algorithms (like PPO vs DPO) affect the correlation between reward model metrics and downstream LLM performance?
- Basis in paper: Explicit - The paper notes they used DPO for cost reasons and acknowledges this may affect reward model requirements compared to online algorithms like PPO.
- Why unresolved: The controlled experiment only tested DPO with one base model, limiting generalizability across different RLHF methods.
- What evidence would resolve it: Conducting similar correlation studies with PPO and other RLHF algorithms using multiple base models would reveal if the observed correlations are algorithm-specific.

### Open Question 2
- Question: Does measuring lower-bound performance in reward models consistently predict better RLHF outcomes across different domains and tasks?
- Basis in paper: Explicit - The paper found that lower quantile aggregation showed higher correlation with downstream Arena Scores than average or upper-bound metrics.
- Why unresolved: The observation was made on a single RLHF experiment and specific metrics; broader validation is needed.
- What evidence would resolve it: Testing this hypothesis across multiple RLHF runs with different reward models and tasks would confirm if this pattern holds universally.

### Open Question 3
- Question: How does reward model performance on non-English prompts correlate with RLHF success for multilingual applications?
- Basis in paper: Explicit - The paper collected human preference data in over 121 languages but didn't analyze cross-lingual correlations.
- Why unresolved: The correlation analysis focused on overall performance without examining language-specific patterns.
- What evidence would resolve it: Analyzing correlation coefficients separately for different language groups would reveal if reward models need language-specific evaluation for multilingual RLHF.

### Open Question 4
- Question: What is the optimal sampling strategy (K value) for best-of-K evaluation in reward model assessment?
- Basis in paper: Explicit - The paper used K=32 for best-of-K curves but noted this choice balances inference costs with exploration space.
- Why unresolved: The choice of K was somewhat arbitrary and may affect the reliability of reward model evaluation.
- What evidence would resolve it: Systematically testing different K values and their correlation with downstream performance would identify the most predictive sampling strategy.

### Open Question 5
- Question: How does benchmark leakage affect the validity of PPE's correlation with RLHF outcomes over time?
- Basis in paper: Explicit - The authors acknowledge benchmark leakage as a real possibility and note that the human preference dataset can be updated.
- Why unresolved: The current correlation analysis is based on a fixed dataset, and the impact of evolving benchmarks is unknown.
- What evidence would resolve it: Re-running the correlation study with updated preference datasets over time would reveal how benchmark leakage affects predictive validity.

## Limitations

- The correlation was measured on a specific Llama-3.1-8B-Instruct model using DPO training with a fixed set of 9 reward models, limiting generalizability
- The PPE benchmark covers 12 domains but may not fully represent the diversity of real-world applications where RLHF is deployed
- The paper acknowledges benchmark leakage as a possibility that could affect the validity of correlations over time

## Confidence

**High confidence** in the correlation finding (77% Pearson) and the general framework of using proxy evaluations for reward models. The experimental setup was controlled and the results are statistically significant.

**Medium confidence** in the specific finding that accuracy on human preference data is the strongest predictor, as this is based on one experimental run with specific hyperparameters.

**Low confidence** in the completeness of the correctness metrics' predictive power, as the paper notes that correctness and human preference do not necessarily align.

## Next Checks

1. **Generalizability test**: Apply the same PPE evaluation framework to different base models (e.g., Llama-3.1-70B, GPT-4 derivatives) and RLHF algorithms (PPO vs DPO) to verify the 77% correlation holds across different architectures and training methods.

2. **Domain coverage audit**: Analyze which of the 12 PPE domains contribute most to the correlation signal and test whether adding or removing specific domains significantly impacts the predictive power, helping identify potential blind spots in the benchmark.

3. **Failure case analysis**: Systematically examine instances where high PPE scores don't predict good RLHF outcomes to identify systematic biases or limitations in the proxy evaluation approach, potentially leading to benchmark improvements.