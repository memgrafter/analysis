---
ver: rpa2
title: 'SOK-Bench: A Situated Video Reasoning Benchmark with Aligned Open-World Knowledge'
arxiv_id: '2405.09713'
source_url: https://arxiv.org/abs/2405.09713
tags:
- knowledge
- reasoning
- situated
- commonsense
- question
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces SOK-Bench, a novel benchmark designed to
  evaluate situated open-world commonsense reasoning in videos. Unlike previous benchmarks
  focused on factual or situated reasoning, SOK-Bench requires models to understand
  and apply both situated knowledge (observable entities, relations, and processes
  from videos) and general knowledge to solve problems.
---

# SOK-Bench: A Situated Video Reasoning Benchmark with Aligned Open-World Knowledge

## Quick Facts
- arXiv ID: 2405.09713
- Source URL: https://arxiv.org/abs/2405.09713
- Reference count: 40
- Primary result: Introduces SOK-Bench, a novel benchmark for situated open-world commonsense reasoning in videos that requires integrating both situated and general knowledge.

## Executive Summary
SOK-Bench is a novel benchmark designed to evaluate situated open-world commonsense reasoning in videos. Unlike previous benchmarks focused on factual or situated reasoning, SOK-Bench requires models to understand and apply both situated knowledge (observable entities, relations, and processes from videos) and general knowledge to solve problems. The benchmark consists of 44K questions and 10K situations with instance-level annotations. To create this dataset, the authors propose an automatic and scalable generation method using combinations of LLMs and MLLMs, extracting situated knowledge from videos, extending to open-world knowledge, and generating question-answer pairs through iterative dialogues and self-prompting. When evaluated on recent mainstream large vision-language models, the results show significant room for improvement, highlighting the research value of SOK-Bench.

## Method Summary
The authors propose an automatic and scalable generation method using combinations of LLMs and MLLMs to create SOK-Bench. The pipeline extracts situated knowledge from videos, extends to open-world knowledge, and generates question-answer pairs through iterative dialogues and self-prompting, followed by manual review for quality assurance. The process involves extracting observable situated entities, relations, and processes from videos using MLLMs to generate situated knowledge graphs, compiling relevant commonsense knowledge exhaustively using LLMs to create general knowledge graphs, aligning the content of situations with commonsense knowledge to reveal underlying logical connections and implications, and formulating questions and answers by integrating the gathered information.

## Key Results
- The benchmark consists of 44K questions and 10K situations with instance-level annotations.
- Recent mainstream large vision-language models show significant room for improvement on SOK-Bench.
- The automatic generation method using combinations of LLMs and MLLMs is scalable and reduces reliance on manual annotation.

## Why This Works (Mechanism)

### Mechanism 1
The automatic generation method using combinations of LLMs and MLLMs is scalable and reduces reliance on manual annotation. The pipeline extracts situated knowledge from videos, extends to open-world knowledge, and generates question-answer pairs through iterative dialogues and self-prompting, followed by manual review for quality assurance. Core assumption: LLMs and MLLMs can be prompted effectively to generate high-quality, structured knowledge representations (graphs and rationales) that align with human reasoning. Evidence anchors: [abstract], [section], and corpus findings showing related works but weak direct evidence on LLM-generated knowledge graph quality.

### Mechanism 2
Situated commonsense reasoning requires integrating both situated knowledge (from video) and general knowledge beyond visible content. The benchmark explicitly links QAs to three aligned knowledge graphs (Situated, General, and Situated Commonsense) so models must combine observable facts with external common sense to answer correctly. Core assumption: Real-world reasoning tasks cannot be solved from video content alone; models need external world knowledge. Evidence anchors: [abstract] and [section], with related works confirming need for external knowledge in VQA.

### Mechanism 3
Few-shot self-prompting improves specificity and concreteness of situated commonsense knowledge generation. By feeding previously generated situated commonsense examples back into the LLM prompt, the model learns to produce more detailed, video-specific knowledge rather than generic responses. Core assumption: LLMs can iteratively refine their outputs when given targeted examples that match the desired granularity and context. Evidence anchors: [section] with claims of 97% concrete knowledge generation after two iterations.

## Foundational Learning

- Concept: Graph-based knowledge representation
  - Why needed here: The benchmark relies on structured Situated, General, and Situated Commonsense Knowledge Graphs to encode entities, attributes, relations, and reasoning paths.
  - Quick check question: Can you describe how a node in the Situated Knowledge Graph might link to an edge in the General Knowledge Graph?

- Concept: Multimodal prompting (LLM + MLLM collaboration)
  - Why needed here: LLMs generate textual knowledge and questions; MLLMs extract visual attributes and temporal relations from video frames.
  - Quick check question: What role does an MLLM play in extracting object attributes that the LLM alone cannot derive?

- Concept: Iterative refinement via self-prompting
  - Why needed here: Few-shot self-prompting allows the model to iteratively improve the specificity of situated commonsense knowledge by using its own prior outputs as examples.
  - Quick check question: Why might two iterations be enough for high-quality situated commonsense knowledge, according to the paper?

## Architecture Onboarding

- Component map: Raw video clips + step-by-step description annotations -> MLLM stage (object/action recognition + attribute/relationship extraction) -> LLM stage 1 (Situated Knowledge Graph generation) -> LLM stage 2 (General Knowledge Graph generation) -> LLM stage 3 (Few-shot Self-Prompting for Situated Commonsense Knowledge Graph) -> QA generation (template-based or LLM-based) -> Human review -> 44K QAs, 10K videos, aligned knowledge graphs, rationales

- Critical path: Video -> SKG (MLLM + LLM collaboration) -> SKG + seed nodes -> GKG (LLM) -> SKG + GKG -> SCKG via Few-shot Self-Prompting (LLM) -> Knowledge graphs -> QAs (template or LLM generation) -> QAs + graphs -> Human validation

- Design tradeoffs: Automation vs. manual quality control (hybrid approach trades scalability for accuracy), bottom-up template QA vs. top-down LLM QA (templates ensure alignment but are less diverse), number of self-prompting iterations (two yield high specificity, more may introduce noise).

- Failure signatures: Low parsing rate of generated JSON graphs, human validation flags >10% of QAs as invalid, LLM outputs become repetitive without few-shot examples, MLLM fails to extract relevant visual attributes.

- First 3 experiments: 1) Run 10 videos through full pipeline and measure parsing success rate and human validation pass rate. 2) Compare QA quality between bottom-up template generation and top-down LLM generation on same video set. 3) Test impact of adding/removing few-shot self-prompting by generating SCKGs with and without this step and measuring specificity via human judgment.

## Open Questions the Paper Calls Out

### Open Question 1
How does the Few-Shot Self-Prompting technique specifically improve the generation of situated commonsense knowledge compared to traditional few-shot prompting methods? Basis: [explicit] The paper introduces Few-Shot Self-Prompting but does not explain how it differs from traditional methods. Why unresolved: Paper mentions technique and benefits but lacks detailed comparison. What evidence would resolve it: Comparative study showing performance differences with metrics like specificity and relevance.

### Open Question 2
What are the limitations of using large language models (LLMs) and multimodal large language models (MLLMs) for generating high-quality situated commonsense knowledge graphs, and how can these limitations be addressed? Basis: [inferred] Paper discusses use of LLMs and MLLMs but doesn't address potential drawbacks. Why unresolved: Focus on proposed methods without discussing limitations. What evidence would resolve it: Analysis of performance including error rates, inconsistencies, biases, and proposed solutions.

### Open Question 3
How does the proposed benchmark for situated open-world commonsense reasoning in videos compare to existing benchmarks in terms of evaluating the true understanding and reasoning capabilities of AI models? Basis: [explicit] Paper introduces SOK-Bench highlighting focus on dynamic, open-world contexts. Why unresolved: No direct comparison with existing benchmarks. What evidence would resolve it: Comparative study evaluating performance on SOK-Bench versus other benchmarks with metrics like accuracy and reasoning depth.

## Limitations
- The evaluation shows significant room for improvement in current models but doesn't establish whether the benchmark itself is too difficult or whether the generation pipeline introduces biases.
- The human validation process is described but not quantified in terms of coverage or inter-annotator agreement.
- Ablation studies focus on internal pipeline components rather than comparing final benchmark quality against human-created alternatives.

## Confidence

- Mechanism 1 (LLM/MLLM generation pipeline): Medium confidence - Well-specified approach but actual quality and consistency of automatically generated knowledge graphs remains largely unmeasured.
- Mechanism 2 (Integration of situated and general knowledge): High confidence - Architectural separation clearly defined and aligns with established research needs.
- Mechanism 3 (Few-shot self-prompting): Medium confidence - 97% specificity rate supported by human testing but lacks detail on sample size and evaluation criteria.

## Next Checks

1. Quality audit of generated knowledge graphs: Sample 100 videos from the benchmark and independently evaluate the completeness, accuracy, and alignment of their Situated, General, and Situated Commonsense Knowledge Graphs against human annotations.

2. Human vs. automatic QA comparison: Create a small human-annotated subset of 100 question-answer pairs for the same videos and compare difficulty, diversity, and reasoning requirements against automatically generated pairs.

3. Cross-model generalization test: Evaluate not just "mainstream" LLMs/MLLMs but also smaller, task-specialized models on the benchmark to determine whether performance gaps are due to model scale or the inherent difficulty of the situated reasoning task.