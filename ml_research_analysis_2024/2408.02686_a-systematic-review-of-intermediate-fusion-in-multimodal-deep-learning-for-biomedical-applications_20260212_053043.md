---
ver: rpa2
title: A Systematic Review of Intermediate Fusion in Multimodal Deep Learning for
  Biomedical Applications
arxiv_id: '2408.02686'
source_url: https://arxiv.org/abs/2408.02686
tags:
- fusion
- data
- multimodal
- latexit
- modalities
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This systematic review analyzes intermediate fusion techniques
  in multimodal deep learning for biomedical applications. We examined 54 articles
  covering diverse data types including imaging, tabular, text, audio, video, and
  time series.
---

# A Systematic Review of Intermediate Fusion in Multimodal Deep Learning for Biomedical Applications

## Quick Facts
- **arXiv ID**: 2408.02686
- **Source URL**: https://arxiv.org/abs/2408.02686
- **Reference count**: 40
- **Primary result**: Comprehensive analysis of intermediate fusion techniques in biomedical multimodal deep learning, introducing formal notation and identifying key research gaps.

## Executive Summary
This systematic review examines 54 peer-reviewed articles on intermediate fusion in multimodal deep learning for biomedical applications. The review introduces a formal notation system to categorize fusion methods based on what is fused, when fusion occurs, how many times it happens, and how it is performed. Analysis reveals that concatenation dominates fusion approaches (82% of single fusion methods), bimodal imaging-tabular combinations are most common, and the choice of unimodal modules critically determines fusion performance. The review identifies key research gaps including limited modality combinations, suboptimal marginal representation dimensions, and insufficient consideration of multimodal fusion strategies.

## Method Summary
The study employed a PRISMA-based systematic literature review across PubMed, IEEE Xplore, Scopus, and Google Scholar databases, searching for multimodal deep learning articles in biomedical applications. The review process included comprehensive database searches with synonyms for multimodal learning, biomedical applications, and intermediate fusion, followed by screening using defined inclusion/exclusion criteria. Selected articles were then categorized and analyzed according to a newly introduced notation system for fusion methods, documenting modalities, architectures, fusion operations, and experimental configurations.

## Key Results
- Concatenation-based fusion dominates the field, appearing in 82% of single fusion approaches across reviewed studies
- Bimodal datasets combining imaging and tabular data represent the most common modality combination in biomedical applications
- The design and architecture of unimodal modules significantly impact overall fusion performance, with modality-specific feature extraction being critical

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Intermediate fusion balances modality preservation and interaction better than early or late fusion.
- Mechanism: Intermediate fusion extracts modality-specific features through separate unimodal modules, then fuses these representations at a feature level, allowing for both intra-modal feature extraction and inter-modal interaction.
- Core assumption: The intermediate representation captures sufficient modality-specific information before fusion.
- Evidence anchors:
  - [abstract]: "Intermediate fusion, striking a balance, integrates data at the feature extraction stage, allowing for a more effective combination of modality-specific features."
  - [section]: "This method is particularly beneficial in biomedical applications where different data types, like imaging and genomic information, need to interact closely to produce meaningful insights."
  - [corpus]: Weak - no corpus evidence specifically on intermediate vs early/late fusion performance.
- Break condition: If modality-specific features lose critical information during unimodal processing, the fusion becomes ineffective.

### Mechanism 2
- Claim: Deep learning models excel at capturing complex, nonlinear relationships between modalities at intermediate fusion.
- Mechanism: Deep learning models process each modality through specialized unimodal networks, extracting high-level features that can then be combined through fusion operations to capture inter-modal relationships.
- Core assumption: Deep learning models can effectively extract meaningful features from each modality before fusion.
- Evidence anchors:
  - [abstract]: "By leveraging deep learning's ability in feature extraction and representation learning, intermediate fusion methods can effectively bridge the gap between these diverse data sources."
  - [section]: "In intermediate fusion, deep learning models are particularly capable at entangling the complex, nonlinear relationships that often exist between different modalities in biomedical data."
  - [corpus]: Weak - corpus focuses on multimodal learning but not specifically on intermediate fusion's advantages.
- Break condition: If deep learning models fail to extract meaningful features from one or more modalities, the fusion becomes ineffective.

### Mechanism 3
- Claim: Feature extraction before fusion improves model performance by creating compact, informative representations.
- Mechanism: Unimodal modules transform raw data into condensed feature representations (marginal representations) that are then fused, reducing dimensionality while preserving critical information.
- Core assumption: Feature extraction can effectively condense information without losing critical details.
- Evidence anchors:
  - [section]: "The power of intermediate fusion lies in its ability to capture both intra- and inter-modal relationships inherent in multimodal data."
  - [section]: "Extracting these marginal representations, facilitated by a global loss derived from fused features, allows for adaptability in relation to other modalities."
  - [corpus]: Weak - corpus doesn't specifically address feature extraction benefits in intermediate fusion.
- Break condition: If feature extraction loses critical information or creates representations that are too abstract to be useful for fusion.

## Foundational Learning

- Concept: Understanding different types of data fusion (early, late, intermediate)
  - Why needed here: The paper contrasts intermediate fusion with early and late fusion, so understanding these differences is crucial for grasping the paper's contributions.
  - Quick check question: What is the key difference between early fusion and intermediate fusion in terms of when modality information is combined?

- Concept: Deep learning architectures (CNNs, RNNs, Transformers)
  - Why needed here: The paper extensively discusses different deep learning architectures used as unimodal modules, which are fundamental to understanding the fusion process.
  - Quick check question: Why are CNNs particularly well-suited for imaging data in multimodal deep learning applications?

- Concept: Feature extraction and dimensionality reduction
  - Why needed here: The paper emphasizes how unimodal modules transform raw data into feature representations, which is central to the intermediate fusion approach.
  - Quick check question: How does feature extraction before fusion help address the curse of dimensionality in biomedical datasets?

## Architecture Onboarding

- Component map: Modalities → Unimodal Modules → Fusion Module → Multimodal Module → Target
- Critical path: Raw data → unimodal feature extraction → fusion operation → multimodal processing → prediction
- Design tradeoffs: Simpler fusion (concatenation) vs. complex fusion (attention, calibration); equal vs. unequal marginal representation dimensions
- Failure signatures: Poor performance on missing modalities; overfitting to dominant modalities; failure to capture inter-modal relationships
- First 3 experiments:
  1. Implement simple concatenation-based fusion with two imaging modalities and compare to unimodal baselines
  2. Test attention-based fusion with tabular and imaging data to capture modality importance
  3. Experiment with different marginal representation dimensions to find optimal feature space sizes

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What are the optimal dimensions for marginal representations in multimodal fusion when processing heterogeneous biomedical data?
- Basis in paper: [explicit] The review identifies that most studies use equal dimensions for marginal representations without clear justification, and notes this may be suboptimal given different modalities contain varying information densities.
- Why unresolved: There is no systematic methodology for determining optimal dimensions, and the current practice of using equal dimensions lacks theoretical grounding for the diverse nature of biomedical data.
- What evidence would resolve it: Empirical studies comparing performance across different dimensionality configurations for specific modality combinations, with analysis of information retention and computational efficiency.

### Open Question 2
- Question: How can intermediate fusion architectures be designed to effectively handle missing modalities in real-world biomedical applications?
- Basis in paper: [explicit] The review notes that only 5 out of 54 articles address missing modalities, and most current approaches use simple concatenation-based fusion that fails with incomplete data.
- Why unresolved: Current architectures lack inherent robustness to missing modalities, and the complexity of biomedical data makes it challenging to develop generalizable solutions for handling incomplete instances.
- What evidence would resolve it: Development and validation of fusion architectures that maintain performance with varying degrees of missing data, along with comparative studies of different robustness strategies.

### Open Question 3
- Question: What fusion strategies beyond concatenation can effectively capture inter-modal relationships in biomedical data while maintaining computational efficiency?
- Basis in paper: [explicit] The review shows concatenation dominates (82% of single fusion approaches) despite its limitations, and identifies tensor operations, attention mechanisms, and other methods as underexplored alternatives.
- Why unresolved: The field shows a clear bias toward unimodal feature extraction over sophisticated multimodal fusion strategies, leaving critical aspects of inter-modal relationship modeling unexplored.
- What evidence would resolve it: Comparative performance studies of alternative fusion strategies across diverse biomedical tasks, demonstrating both effectiveness in capturing relationships and practical computational considerations.

## Limitations

- Findings primarily based on published literature rather than empirical validation of fusion methods, potentially reflecting publication bias
- Systematic search may have missed relevant grey literature or preprints that could affect completeness of analysis
- Proposed future research directions lack empirical validation or pilot studies demonstrating their potential impact

## Confidence

- **High Confidence**: The formalization of intermediate fusion notation and the systematic categorization of existing methods are well-supported by the reviewed literature corpus.
- **Medium Confidence**: The identification of research gaps and limitations in current approaches is supported by literature patterns but may be influenced by publication bias.
- **Low Confidence**: The proposed future research directions, while logical extensions of identified gaps, lack empirical validation or pilot studies demonstrating their potential impact.

## Next Checks

1. Conduct a focused empirical study comparing different fusion strategies (concatenation, attention, calibration) on the same biomedical dataset to validate claims about their relative effectiveness.
2. Perform a broader literature search including preprints and grey literature to assess whether the identified research gaps persist when considering a more comprehensive corpus.
3. Develop and benchmark a minimal working implementation of the proposed notation system across diverse multimodal biomedical datasets to verify its practical applicability and uncover any overlooked fusion patterns.