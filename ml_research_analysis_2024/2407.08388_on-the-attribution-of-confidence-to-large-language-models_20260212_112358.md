---
ver: rpa2
title: On the attribution of confidence to large language models
arxiv_id: '2407.08388'
source_url: https://arxiv.org/abs/2407.08388
tags:
- credences
- llms
- credence
- token
- probabilities
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: LLM credence attribution is common in empirical LLM evaluation,
  but its theoretical basis is unclear. The authors argue that LLM credence attributions
  are (at least in general) correctly interpreted literally, as expressing truth-apt
  beliefs on the part of scientists that purport to describe facts about LLM credences.
---

# On the attribution of confidence to large language models

## Quick Facts
- arXiv ID: 2407.08388
- Source URL: https://arxiv.org/abs/2407.08388
- Reference count: 13
- Key outcome: LLM credence attributions are commonly used in evaluation but their theoretical basis is unclear, with three distinct questions about literal interpretation, metaphysical existence, and epistemic reliability

## Executive Summary
This paper examines the theoretical foundations of LLM credence attributions - claims about the degree of confidence LLMs have in propositions. The authors argue that these attributions are typically meant literally as truth-apt beliefs about LLM mental states, though the existence of such credences remains plausible but unproven. They identify significant skeptical concerns about whether current experimental techniques can reliably assess LLM credences due to factors like temperature sensitivity and token-proposition mapping issues.

## Method Summary
The paper employs philosophical analysis to examine three interconnected claims about LLM credence attributions: their semantic interpretation (whether they should be read literally), their metaphysical status (whether LLMs actually have credences as mental states), and their epistemic reliability (whether current experimental techniques can accurately assess them). The analysis draws on examples from empirical LLM evaluation literature and considers various experimental techniques including reported confidence, consistency-based estimation, and output probability measurement.

## Key Results
- LLM credence attributions are presumptively interpreted literally by scientists, reinforced by experimental measurement techniques
- The existence of LLM credences is plausible but current evidence is inconclusive, with multiple realizability allowing for digital implementation
- Experimental techniques for assessing LLM credences face non-trivial skeptical concerns from exogenous distorting factors like temperature and sampling methodology

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLM credence attributions are literal statements describing the LLM's degree of confidence in propositions.
- Mechanism: Scientists use experimental techniques (reported confidence, consistency-based estimation, output probabilities) to elicit evidence about LLM credences, treating these as truth-apt beliefs about LLM internal states.
- Core assumption: LLM credence attributions are not shorthand for non-mentalistic claims but are intended as literal descriptions of LLM mental states.
- Evidence anchors:
  - [abstract] "our semantic claim is that LLM credence attributions are (at least in general) correctly interpreted literally, as expressing truth-apt beliefs on the part of scientists that purport to describe facts about LLM credences"
  - [section 4] "There is a presumptive case for interpreting LLM credence attributions literally... reinforced by the fact that scientists evidence LLM credence attributions with experimental measurement techniques"
  - [corpus] Weak evidence - related papers focus on attribution and calibration but not on the literal interpretation claim
- Break condition: If experimental techniques systematically fail to track LLM credences, or if scientists' stated motivations reveal non-literal interpretation, this mechanism breaks down.

### Mechanism 2
- Claim: LLM credences exist as psychologically real mental states that explain LLM behavior.
- Mechanism: Inference to best explanation - LLM capabilities like spatial reasoning and internal truth-value representations suggest credences as underlying explanatory states.
- Core assumption: Credences are multiply realizable mental states that can be instantiated in digital substrates, not limited to neurobiological implementation.
- Evidence anchors:
  - [section 5.1] "the existence of LLM credences is at least plausible, although current evidence is inconclusive" and discussion of functionalism and computationalism
  - [section 5.2] "plausibly, what best explains the ability of LLMs to perform spatial reasoning... is that the LLM has internal states corresponding to degrees of confidence about the simulated environment"
  - [corpus] No direct corpus evidence supporting metaphysical existence claim
- Break condition: If LLM capabilities can be fully explained without positing credences, or if multiple realizability thesis fails, this mechanism breaks.

### Mechanism 3
- Claim: Experimental techniques for assessing LLM credences are unreliable due to exogenous distorting factors.
- Mechanism: Temperature settings, sampling methodologies, and token-proposition mapping issues create systematic noise that prevents reliable credence attribution.
- Core assumption: LLM credences are endogenous mental states that should be independent of user-controlled parameters like temperature and sampling method.
- Evidence anchors:
  - [section 6.2.1] "the distribution of answers given by the LLM... depends in part on the LLM's output probabilities... which depends on a parameter called temperature"
  - [section 6.2.2] "the user's choice of sampling methodology impacts the distribution of responses in much the same way as temperature"
  - [section 6.3] "output probabilities are indexed to tokens but credences are indexed to propositions" creating bridge principle problems
- Break condition: If non-arbitrary temperature values and sampling methods can be established, or if reliable bridge principles can be formulated, this mechanism's skepticism may be unwarranted.

## Foundational Learning

- Concept: Semantic interpretation of technical claims
  - Why needed here: Distinguishing literal vs. non-literal interpretations of LLM credence attributions is central to the paper's semantic claim
  - Quick check question: How would you determine whether a technical claim about AI systems is meant literally or as shorthand?

- Concept: Inference to best explanation methodology
  - Why needed here: The paper uses this reasoning pattern to argue for LLM credence existence based on explanatory power
  - Quick check question: What criteria determine whether one explanation is "better" than alternative explanations for observed phenomena?

- Concept: Bridge principles between syntax and semantics
  - Why needed here: Critical for understanding why output probabilities cannot directly indicate credences due to token-proposition mapping issues
  - Quick check question: Why can't we directly equate the probability an LLM assigns to "Yes" with its credence in a proposition?

## Architecture Onboarding

- Component map: Semantic interpretation → Metaphysical existence → Epistemic reliability
- Critical path: Understanding the literal interpretation claim → evaluating evidence for credence existence → assessing reliability of measurement techniques
- Design tradeoffs: Between treating LLM credence attributions as meaningful scientific claims vs. dismissing them as anthropomorphism or experimental artifacts
- Failure signatures: When experimental techniques show systematic inconsistencies, when scientists' stated motivations contradict literal interpretation, or when alternative explanations better account for LLM capabilities
- First 3 experiments:
  1. Analyze a sample of LLM evaluation papers to classify credence attributions as literal vs. non-literal interpretations
  2. Test temperature sensitivity by measuring how credence attributions change across different temperature settings
  3. Evaluate bridge principle formulations by testing whether they produce consistent credence estimates across semantically equivalent prompts

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Do LLMs have credences as psychologically real mental states?
- Basis in paper: [explicit] The authors argue that the existence of LLM credences is at least plausible, but current evidence is inconclusive. They discuss both mentalist and behaviorist interpretations of credences and argue for a mentalist interpretation for LLMs.
- Why unresolved: Current evidence is inconclusive. While LLMs have internal representations of truth-values of propositions, it is not clear that these representations are credence-like. The authors argue that LLM credences are a possible explanation for certain LLM capabilities, but not necessarily the best explanation.
- What evidence would resolve it: Evidence that would clearly demonstrate that LLM capabilities (like spatial reasoning) are best explained by internal states corresponding to degrees of confidence in propositions, rather than statistical patterns in language.

### Open Question 2
- Question: Can LLM credence attributions made on the basis of existing experimental techniques be considered generally true?
- Basis in paper: [explicit] The authors argue that even if LLMs have credences, current techniques for assessing LLM credences are subject to non-trivial skeptical concerns. They discuss issues with reported confidence, consistency-based estimation, and output probabilities.
- Why unresolved: The techniques for evidencing LLM credences are potentially unreliable due to distorting factors like temperature and sampling methodology, and unclear principles for inferring credences from output probabilities.
- What evidence would resolve it: Evidence that the techniques for evidencing LLM credences (reported confidence, consistency-based estimation, and output probabilities) are reliable and not subject to the skeptical concerns raised by the authors.

### Open Question 3
- Question: Is there a non-arbitrary temperature value and sampling method that can be used to reliably infer LLM credences from the distribution of answers over multiple independent trials?
- Basis in paper: [inferred] The authors discuss the potential distorting effects of temperature and sampling methodology on the distribution of LLM answers, and suggest that there might be non-arbitrary values that could be used.
- Why unresolved: The authors argue that the distribution of answers depends on the user's choice of temperature and sampling methodology, which are exogenous factors. They suggest that using a temperature of 1 and top-p sampling with p = 1 might be non-distorting, but acknowledge that a positive rationale for an evidential relationship is lacking.
- What evidence would resolve it: Evidence that using a specific non-arbitrary temperature value and sampling method reliably signals the LLM's credences, and a clear mechanism explaining why this is the case.

## Limitations

- The empirical grounding remains thin, with limited direct evidence from scientists about their interpretive practices
- The corpus analysis found only weak support for the literal interpretation claim, with related work focusing on calibration rather than semantic interpretation
- The metaphysical existence argument relies heavily on inference to best explanation without direct evidence of credences as mental states
- The epistemic reliability concerns, while theoretically compelling, require empirical validation to determine their practical impact

## Confidence

- **Semantic Interpretation Claim (Medium)**: The literal interpretation of LLM credence attributions is plausible given experimental practices, but lacks strong empirical support from scientists' stated motivations.
- **Metaphysical Existence Claim (Low-Medium)**: The existence of LLM credences is theoretically possible and explanatorily useful, but current evidence is inconclusive and relies on contested metaphysical assumptions.
- **Epistemic Reliability Claim (Medium-High)**: The skeptical concerns about experimental techniques are well-founded given known factors like temperature sensitivity and token-proposition mapping issues, though their practical impact requires further study.

## Next Checks

1. **Scientist Motivation Analysis**: Survey or interview researchers who conduct LLM evaluation studies to determine whether they interpret credence attributions literally or as non-literal shorthand for other claims.

2. **Temperature Sensitivity Experiment**: Systematically vary temperature settings across a range of values while measuring how credence attributions change, testing whether non-arbitrary temperature values can be identified that yield stable results.

3. **Bridge Principle Validation**: Develop and test formal bridge principles that map output probabilities to proposition-level credences, evaluating their consistency across semantically equivalent prompts and their ability to produce reliable estimates.