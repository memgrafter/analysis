---
ver: rpa2
title: Quantifying the Capabilities of LLMs across Scale and Precision
arxiv_id: '2405.03146'
source_url: https://arxiv.org/abs/2405.03146
tags:
- arxiv
- reasoning
- performance
- tasks
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study comprehensively evaluates the impact of model scale
  and quantization on the performance of two major open-source LLM families, Llama
  2-Chat and Mistral, across diverse tasks including reasoning, hallucination detection,
  misinformation detection, and natural language understanding. The evaluation spans
  model sizes from 7B to 70B parameters and precision levels from 4-bit to 32-bit.
---

# Quantifying the Capabilities of LLMs across Scale and Precision

## Quick Facts
- arXiv ID: 2405.03146
- Source URL: https://arxiv.org/abs/2405.03146
- Authors: Sher Badshah; Hassan Sajjad
- Reference count: 40
- This study comprehensively evaluates the impact of model scale and quantization on the performance of two major open-source LLM families, Llama 2-Chat and Mistral, across diverse tasks including reasoning, hallucination detection, misinformation detection, and natural language understanding.

## Executive Summary
This paper presents a comprehensive evaluation of how model scale and quantization precision affect the performance of large language models across diverse tasks. The study examines Llama 2-Chat and Mistral families ranging from 7B to 70B parameters at 4-bit, 8-bit, FP16, and FP32 precision levels. Results demonstrate that larger models generally outperform smaller ones across most tasks, with the critical finding that larger models show exceptional resilience to quantization, maintaining high accuracy even at 4-bit precision for numerous tasks. This suggests that deploying larger 4-bit models can be more beneficial than using smaller models at higher precision under similar memory constraints. However, the benefits of scale are not uniform across all tasks, and quantization impacts vary across different model architectures.

## Method Summary
The study evaluates two major open-source LLM families (Llama 2-Chat and Mistral) across parameter sizes from 7B to 70B using zero-shot evaluation on diverse tasks including summarization, machine translation, sentiment analysis, reasoning, and misinformation detection. Models are tested at 4-bit, 8-bit, FP16, and FP32 precision levels using LLM.int8() for 8-bit quantization and QLoRA with NF4 for 4-bit quantization. The evaluation framework uses standardized metrics (ROUGE-1, ChrF+, F1, accuracy) with human evaluation for reasoning, hallucination, and misinformation detection tasks. Sampling strategies vary from 30-660 test samples per dataset with batch sizes of 8 for 7B models and 2 for larger models.

## Key Results
- Larger models consistently outperform smaller ones across most tasks, demonstrating the importance of scale
- Larger models exhibit exceptional resilience to precision reduction, maintaining high accuracy even at 4-bit quantization for numerous tasks
- Benefits of scale are not uniform across all tasks, with some showing only marginal or no improvements with increased model size

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Larger models encode information more redundantly, so 4-bit quantization preserves essential patterns while discarding fine-grained noise.
- Mechanism: Parameter redundancy allows compression without destroying task-relevant representations.
- Core assumption: Task performance depends on coarse-grained features more than high-precision details.
- Evidence anchors:
  - [abstract] "larger models show exceptional resilience to precision reduction and can maintain high accuracy even at 4-bit quantization for numerous tasks"
  - [section] "the larger Llama 2-Chat 70B consistently outperforms their smaller counterparts in achieving higher scores"
- Break condition: If task requires precise numerical reasoning or fine-grained distinctions (e.g., Letter String Analogies), quantization degrades performance.

### Mechanism 2
- Claim: Architectural efficiency differences (grouped query attention, sliding window) allow smaller models like Mistral to perform closer to larger models at lower precision.
- Mechanism: Specialized attention mechanisms reduce parameter interactions that are sensitive to quantization noise.
- Core assumption: Efficiency gains can offset lack of scale for some tasks.
- Evidence anchors:
  - [abstract] "the impact of quantization on performance varies across different model architectures and tasks"
  - [section] "Mistral 7B achieved almost identical performance across all precision levels"
- Break condition: If the task benefits strongly from scale (e.g., complex reasoning), efficiency gains plateau.

### Mechanism 3
- Claim: Knowledge density in larger models creates robustness to quantization, while smaller models lack sufficient redundancy.
- Mechanism: Dense knowledge embeddings provide multiple paths to retrieve correct answers even after compression.
- Core assumption: Factual and commonsense knowledge is more redundantly encoded in larger models.
- Evidence anchors:
  - [abstract] "larger models are more truthful" and "larger models in both model families exhibit comparable performance in 4 and 8-bit quantization"
  - [section] "larger models are better at detecting scientific misinformation" while smaller models are more sensitive to quantization
- Break condition: If knowledge is sparse or task-specific, quantization erases unique patterns needed for correct retrieval.

## Foundational Learning

- Concept: Model scale vs performance scaling laws
  - Why needed here: To understand why scale improves performance but not uniformly across tasks
  - Quick check question: What does the "emergent abilities" concept imply about the relationship between scale and task performance?

- Concept: Post-training quantization (PTQ) methods and trade-offs
  - Why needed here: To grasp how 4-bit and 8-bit quantization works and its impact on model behavior
  - Quick check question: How does LLM.int8() differ from QLoRA in handling outlier submatrices?

- Concept: Zero-shot evaluation methodology
  - Why needed here: To interpret why no examples are given during testing and what that reveals about model capabilities
  - Quick check question: Why might zero-shot performance underestimate a model's true potential compared to few-shot?

## Architecture Onboarding

- Component map:
  - Llama 2-Chat family: 7B, 13B, 70B parameters with supervised fine-tuning + RLHF
  - Mistral family: 7B (grouped query attention + sliding window) and Mixtral 8×7B (MoE)
  - Quantization backends: LLM.int8() for 8-bit, QLoRA with NF4 for 4-bit
  - Evaluation framework: Zero-shot prompts, human evaluation for reasoning/hallucination, automated metrics for NLU

- Critical path: Load quantized model → run zero-shot prompt → collect outputs → evaluate with task-specific metric → aggregate across tasks/precision levels

- Design tradeoffs:
  - Memory vs. accuracy: 4-bit saves ~75% memory but may lose performance on precision-sensitive tasks
  - Scale vs. quantization: Larger models tolerate more compression; smaller models may need higher precision
  - Architecture vs. scale: Efficient designs (Mistral) can outperform naive scaling in some settings

- Failure signatures:
  - Sudden accuracy drop at 4-bit on tasks requiring fine-grained distinctions
  - Performance inversion where smaller model outperforms larger at low precision (scale limit reached)
  - Inconsistent behavior across similar tasks (indicating task-specific sensitivity)

- First 3 experiments:
  1. Run 7B and 70B models on a simple classification task at FP16 and 4-bit to observe quantization tolerance baseline
  2. Test spatial reasoning (SpartQA) at multiple precisions to identify precision-sensitive domains
  3. Compare Mistral 7B vs Mixtral 8×7B on commonsense QA to assess architectural efficiency gains

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do different quantization techniques beyond 4-bit and 8-bit (e.g., 2-bit, 3-bit) impact the performance of LLMs across various tasks?
- Basis in paper: [inferred] The paper focuses on 4-bit and 8-bit quantization but mentions the potential for exploring other precision levels.
- Why unresolved: The study did not investigate quantization levels below 4-bit, leaving a gap in understanding the impact of even lower precision on model performance.
- What evidence would resolve it: Experiments comparing model performance across a range of quantization levels (e.g., 2-bit, 3-bit, 4-bit, 8-bit, 16-bit) on various tasks would provide insights into the trade-offs between model size, precision, and performance.

### Open Question 2
- Question: How does the choice of quantization technique (e.g., LLM.int8(), QLoRA) influence the performance of LLMs at different scales and across various tasks?
- Basis in paper: [explicit] The paper mentions using LLM.int8() for 8-bit quantization and QLoRA for 4-bit quantization but does not compare their performance.
- Why unresolved: The study did not compare the performance of different quantization techniques, leaving uncertainty about which method is most effective for different model scales and tasks.
- What evidence would resolve it: Experiments comparing the performance of different quantization techniques (e.g., LLM.int8(), QLoRA, GPTQ) across various model scales and tasks would provide insights into the optimal quantization method for different scenarios.

### Open Question 3
- Question: How do the findings on the impact of scale and quantization on LLM performance generalize to other model architectures (e.g., encoder-decoder models, multimodal models)?
- Basis in paper: [inferred] The paper focuses on decoder-only models (Llama 2-Chat and Mistral) but does not explore other model architectures.
- Why unresolved: The study's findings may not be applicable to other model architectures, leaving uncertainty about the generalizability of the results.
- What evidence would resolve it: Experiments evaluating the impact of scale and quantization on a diverse range of model architectures (e.g., encoder-decoder models, multimodal models) would provide insights into the generalizability of the findings.

## Limitations

- The evaluation focuses primarily on English-language tasks, limiting conclusions about multilingual performance under quantization
- Task selection bias may affect generalizability as results may not extend to specialized domains like code generation or medical diagnosis
- The study uses zero-shot evaluation only, which may underestimate the true capabilities of both large and small models compared to fine-tuned approaches

## Confidence

**High confidence**: The finding that larger models maintain higher accuracy at 4-bit quantization for most tasks is well-supported by consistent results across multiple evaluation datasets.

**Medium confidence**: The claim that deploying larger 4-bit models is generally preferable to smaller high-precision models depends heavily on specific task requirements and memory constraints.

**Low confidence**: The precise quantification of how much redundancy larger models possess that enables quantization tolerance remains speculative.

## Next Checks

1. **Task-specific precision sensitivity analysis**: Conduct a systematic ablation study on a subset of tasks to identify the exact precision threshold where performance degrades for different model sizes.

2. **Cross-lingual generalization test**: Evaluate the same model families and quantization levels on multilingual benchmarks (e.g., XNLI, WikiLingua) to determine if scale-precision trade-offs observed in English extend to other languages.

3. **Fine-tuning comparison**: Compare zero-shot results with few-shot and fine-tuned performance for both large and small models at various precision levels to reveal whether scale advantages persist when models can adapt to specific tasks.