---
ver: rpa2
title: Aligning Pre-trained Models for Spoken Language Translation
arxiv_id: '2411.18294'
source_url: https://arxiv.org/abs/2411.18294
tags:
- connector
- speech
- encoder
- how2
- translation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a novel approach to end-to-end speech translation
  (ST) by aligning frozen pre-trained automatic speech recognition (ASR) and machine
  translation (MT) models using a small connector module (STE connector). The connector
  bridges the modality gap between speech and text by transforming ASR encoder embeddings
  into the latent representation space of the MT encoder.
---

# Aligning Pre-trained Models for Spoken Language Translation

## Quick Facts
- **arXiv ID:** 2411.18294
- **Source URL:** https://arxiv.org/abs/2411.18294
- **Reference count:** 13
- **Primary result:** End-to-end speech translation using frozen pre-trained ASR and MT models aligned by a small connector, achieving comparable performance to conventional ST systems with <5% trainable parameters

## Executive Summary
This paper introduces a novel approach to end-to-end speech translation (ST) by aligning frozen pre-trained automatic speech recognition (ASR) and machine translation (MT) models using a small connector module. The connector bridges the modality gap between speech and text by transforming ASR encoder embeddings into the latent representation space of the MT encoder. Experiments on the How2 English-Portuguese dataset demonstrate that this alignment approach is viable and scalable, achieving comparable results to conventional end-to-end ST systems while using significantly fewer trainable parameters. The STE connector outperforms the Q-Former, particularly for longer input sequences, and scaling up foundation models universally improves translation results. Additionally, the connectors demonstrate domain adaptation capabilities, significantly improving translation performance for out-of-domain MT models.

## Method Summary
The approach involves aligning frozen pre-trained ASR and MT models using connector modules that bridge the modality gap between speech and text representations. Two architectures are proposed: ECD (Encoder-Connector-Decoder) and ECED (Encoder-Connector-Encoder-Decoder). Two connector types are explored: Q-Former, which uses fixed-length mappings with cross-attention, and STE connector, which handles variable-length sequences through convolutional subsampling. The foundation models remain frozen during training, with only the connector parameters being updated using cross-entropy loss. Experiments are conducted on the How2 English-Portuguese dataset using various pre-trained models including E-Branchformer and Whisper for ASR, and MarianMT and T5 for MT.

## Key Results
- The STE connector outperforms the Q-Former, particularly for longer input sequences (>20 seconds)
- Scaling up foundation ASR and MT models universally improves translation results across both connector types
- Domain adaptation is successful, with T5's translation performance significantly improving when aligned with the How2-adapted ASR model
- The approach uses <5% of the size of the aligned models for trainable parameters, demonstrating high parameter efficiency

## Why This Works (Mechanism)
The approach works by leveraging the rich representations learned by pre-trained foundation models while minimizing the need for extensive task-specific training. The connector modules act as modality bridges, transforming speech embeddings into text-compatible representations without requiring fine-tuning of the large foundation models. This preserves the general knowledge captured during pre-training while adapting to the specific ST task. The STE connector's advantage for longer sequences stems from its ability to handle variable-length inputs through convolutional subsampling, avoiding the fixed-dimensional bottleneck of the Q-Former.

## Foundational Learning
- **Modality gap bridging**: The fundamental challenge of connecting speech and text representations requires specialized connectors that can transform between different feature spaces - critical for any multimodal alignment task
- **Cross-modal attention mechanisms**: The Q-Former uses cross-attention to map speech embeddings to text space, while the STE connector uses convolutional subsampling - both approaches need to preserve semantic information during transformation
- **Frozen foundation models**: Keeping large pre-trained models frozen preserves their general knowledge while reducing computational cost - requires careful design of the connector to handle representation mismatches
- **Variable vs. fixed-length mappings**: The trade-off between Q-Former's fixed-dimensional bottleneck and STE's flexible handling of sequence length affects performance on diverse utterance lengths
- **Domain adaptation through alignment**: The ability to improve out-of-domain MT performance by aligning with in-domain ASR suggests the connector learns task-relevant transformations beyond simple modality bridging

## Architecture Onboarding

**Component Map:**
ASR Encoder -> Connector (Q-Former/STE) -> MT Encoder/Decoder -> Translation Output

**Critical Path:**
Speech input → ASR Encoder → Connector transformation → MT Decoder → Translated text

**Design Tradeoffs:**
- **Fixed vs. variable length**: Q-Former uses fixed query count (128) while STE handles variable lengths via subsampling - affects performance on long utterances
- **Parameter efficiency vs. performance**: Both connectors use <5% parameters of foundation models, but STE shows better performance especially for longer sequences
- **Modality preservation**: Connectors must preserve semantic information while transforming representations - balance between compression and fidelity

**Failure Signatures:**
- Performance degradation on longer utterances indicates connector bottleneck issues
- Minimal domain adaptation gains suggest poor representation alignment or domain mismatch
- High variance in results across different input lengths points to connector sensitivity

**3 First Experiments to Run:**
1. Train baseline ECD with Q-Former (6 layers, dmodel 256) on How2 to establish baseline performance
2. Replace Q-Former with STE connector and evaluate performance improvement, particularly on longer utterances
3. Test domain adaptation by aligning out-of-domain T5 with in-domain ASR and measuring MT improvement

## Open Questions the Paper Calls Out
None

## Limitations
- Performance gains from scaling foundation models may not generalize to all language pairs or domains beyond How2 English-Portuguese
- Domain adaptation results with out-of-domain T5 are promising but limited to a single case study
- The approach's effectiveness for extremely low-resource scenarios (below 17 hours) remains unexplored

## Confidence
- **High confidence**: Basic alignment methodology and connector training procedure are well-specified and reproducible
- **Medium confidence**: Performance improvements from scaling foundation models and domain adaptation capabilities, as these are demonstrated but on limited datasets
- **Low confidence**: Generalization of results to other language pairs, domains, and resource scenarios not covered in experiments

## Next Checks
1. **Input length sensitivity**: Conduct comprehensive experiments across multiple utterance length bins (short: <10s, medium: 10-30s, long: >30s) to validate the STE connector's consistent advantage over Q-Former for variable-length inputs

2. **Cross-lingual generalization**: Test the alignment approach with different language pairs (e.g., English-German, English-Chinese) to assess whether performance gains from scaling foundation models transfer across typologically diverse languages

3. **Extreme low-resource scenario**: Evaluate the approach with sub-10-hour training data to determine the minimum viable dataset size and identify potential performance degradation patterns when foundation models are trained on limited in-domain data