---
ver: rpa2
title: 'Ask-before-Plan: Proactive Language Agents for Real-World Planning'
arxiv_id: '2406.12639'
source_url: https://arxiv.org/abs/2406.12639
tags:
- clarification
- agent
- user
- planning
- city
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a new task called Proactive Agent Planning,
  where language agents must predict clarification needs, invoke external tools to
  gather information, and generate a plan based on both conversation and environment
  interaction. To address this, the authors propose the Clarification-Execution-Planning
  (CEP) framework consisting of three specialized agents for clarification, execution,
  and planning.
---

# Ask-before-Plan: Proactive Language Agents for Real-World Planning

## Quick Facts
- arXiv ID: 2406.12639
- Source URL: https://arxiv.org/abs/2406.12639
- Reference count: 40
- Key outcome: CEP framework achieves 99.4% clarification accuracy and 100% well-formed tool calls on Ask-before-Plan dataset

## Executive Summary
This paper introduces Proactive Agent Planning, a new task where language agents must predict when clarification is needed, interact with external tools to gather missing information, and generate comprehensive plans. The authors propose the Clarification-Execution-Planning (CEP) framework, a three-agent system specialized for clarification, tool execution, and planning. The framework includes trajectory tuning for fine-tuning clarification and execution agents, and a memory recollection mechanism for optimizing dynamic execution. Experiments on the newly constructed Ask-before-Plan dataset demonstrate significant performance improvements over baseline methods across all three subtasks.

## Method Summary
The CEP framework consists of three specialized agents: a clarification agent that predicts need and generates questions, an execution agent that interacts with tools (static or dynamic), and a planning agent that generates JSON-formatted travel plans. The approach uses trajectory tuning to fine-tune the clarification and execution agents on task-specific data, and implements memory recollection to optimize the dynamic execution agent's reasoning. The method is evaluated on the Ask-before-Plan dataset, which contains 1,000 training and 1,000 testing samples built on TravelPlanner with ambiguous user instructions requiring clarification.

## Key Results
- Clarification accuracy: 99.4% (micro) and 98.2% (macro)
- Well-formed tool call rates: 100% (micro) and 99.3% (macro)
- Delivery rates: 98.8% (micro) and 64.3% (macro)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Trajectory tuning enables specialized agents to handle ambiguous user instructions more effectively than prompting alone
- Mechanism: Fine-tuning the clarification and execution agents on trajectory sequences (Ct-1, Et-1) aligns them with the specific task structure of predicting clarification needs and generating valid tool calls
- Core assumption: LLM-based agents benefit from task-specific fine-tuning when handling multi-step reasoning and tool interactions
- Evidence anchors:
  - [abstract]: "We introduce the trajectory tuning scheme for the clarification agent and static execution agent"
  - [section 4.1]: "To resolve these issues and align the current language-based agents with our task, we propose to use the trajectory (Ct-1, Et-1) to fine-tune the clarification agent and the execution agent"
  - [corpus]: Weak evidence - no direct citation, but trajectory tuning concept aligns with recent work on instruction tuning for specialized tasks
- Break condition: If trajectory data is insufficient or noisy, fine-tuning may not generalize beyond training distribution

### Mechanism 2
- Claim: Memory recollection prevents redundant self-reflection in dynamic execution, reducing inference time and improving performance
- Mechanism: Reusing self-reflective feedback from previous turns in the same conversation eliminates repeated generation of identical rationales
- Core assumption: Similar types of exceptions occur across conversation turns with shared clarified details
- Evidence anchors:
  - [abstract]: "we propose the memory recollection mechanism to optimize the memory utility for the execution agent in long-context reasoning"
  - [section 4.2]: "the memory recollection mechanism to reuse self-reflective feedback from previous turns"
  - [corpus]: Weak evidence - no direct citation, but concept aligns with memory optimization techniques in multi-turn reasoning systems
- Break condition: If execution agent encounters diverse exceptions requiring unique reflections, memory reuse may become ineffective

### Mechanism 3
- Claim: Multi-agent specialization (clarification, execution, planning) outperforms monolithic approaches by dividing complex reasoning into manageable subtasks
- Mechanism: Each agent focuses on its specialized role, with clarification agent identifying ambiguity, execution agent handling tool interactions, and planning agent generating final plans based on clarified information
- Core assumption: Complex planning tasks benefit from modular decomposition into specialized components
- Evidence anchors:
  - [abstract]: "we propose a novel multi-agent framework, Clarification-Execution-Planning (CEP), which consists of three agents specialized in clarification, execution, and planning"
  - [section 4.3]: "Given the conversation Ct of turn t, the execution agent first generates the whole interaction chain Et"
  - [corpus]: Weak evidence - no direct citation, but multi-agent decomposition aligns with established software engineering principles
- Break condition: If communication overhead between agents exceeds benefits of specialization, or if agents cannot effectively coordinate

## Foundational Learning

- Concept: Topological sort for dependency management in indefinite details
  - Why needed here: Ensures clarification questions are asked in an order that respects dependencies between different types of missing information
  - Quick check question: If a travel plan requires both destination and budget information, and budget depends on destination, in what order should clarification questions be asked?

- Concept: Tool learning with function call generation
  - Why needed here: Agents must interact with external APIs to gather information needed for clarification and planning
  - Quick check question: What makes a tool call "well-formed" - is it just correct syntax, or must parameters also be valid for the specific context?

- Concept: JSON plan generation with structured output
  - Why needed here: Planning agent must produce machine-readable travel plans that satisfy multiple constraints
  - Quick check question: What are the key fields required in the JSON plan format, and why is structured output preferred over natural language?

## Architecture Onboarding

- Component map:
  - Clarification Agent -> Execution Agent -> Planning Agent
  - Memory Bank (for dynamic execution)
  - Trajectory Tuner (for fine-tuning)

- Critical path:
  1. Execution agent generates tool interaction chain
  2. Clarification agent predicts need and generates question
  3. User provides clarification
  4. Process repeats until all details clarified
  5. Planning agent generates final plan

- Design tradeoffs:
  - Static vs dynamic execution: Static is faster but less flexible; dynamic handles exceptions but has higher complexity
  - Memory recollection vs full reflection: Memory reduces redundancy but may miss novel situations
  - Multi-agent vs monolithic: Better specialization but higher coordination overhead

- Failure signatures:
  - Clarification agent: Incorrect need prediction, irrelevant questions
  - Execution agent: Invalid tool calls, API errors, timeout
  - Planning agent: Invalid JSON, constraint violations, missing fields

- First 3 experiments:
  1. Baseline: Direct prompting without fine-tuning or memory mechanisms
  2. Ablation: Remove memory recollection from dynamic execution
  3. Scaling: Test with larger model sizes (e.g., LLaMA-3-8B vs Mistral-7B)

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the CEP framework perform on other domains beyond travel planning?
- Basis in paper: [inferred] The paper states "The effectiveness of our proposed framework, CEP, has not been verified on other benchmarks" and mentions they are the first to introduce this problem
- Why unresolved: The authors acknowledge they haven't tested their framework on other domains due to lack of suitable datasets and limited computing resources
- What evidence would resolve it: Testing CEP on other planning domains like event planning, educational course scheduling, or medical appointment planning to validate generalizability

### Open Question 2
- Question: What is the optimal number of clarification turns before the agent should attempt to proceed with planning?
- Basis in paper: [inferred] The paper discusses multiple clarification turns but doesn't establish a threshold for when clarification becomes counterproductive
- Why unresolved: The paper shows that excessive clarification can introduce noise into the planning process (e.g., affecting "minimum nights stay" constraint) but doesn't quantify the optimal balance
- What evidence would resolve it: Controlled experiments varying the number of clarification turns and measuring planning success rates and constraint satisfaction

### Open Question 3
- Question: How does real-time user simulation compare to the static simulation used in this study?
- Basis in paper: [explicit] The paper states "To simplify the evaluation on the clarification subtask, we employ static user simulation rather than real-time user simulation for dialogues"
- Why unresolved: The authors chose static simulation to focus on evaluating clarification question generation without user simulation interference, but acknowledge this as a limitation
- What evidence would resolve it: Comparative studies using both static and dynamic user simulation to measure differences in agent performance and user satisfaction

## Limitations
- Limited generalizability to non-travel planning domains due to lack of diverse benchmark datasets
- Static user simulation may not capture real-world user behavior and response patterns
- Heavy reliance on trajectory tuning data quality for effective fine-tuning of specialized agents

## Confidence

- High Confidence: The effectiveness of multi-agent specialization for complex planning tasks (99.4% clarification accuracy, 100% well-formed tool calls)
- Medium Confidence: The memory recollection mechanism's ability to optimize dynamic execution agent performance
- Low Confidence: Generalization of results to non-travel planning domains and scalability to significantly larger model sizes

## Next Checks

1. **Domain Transfer Test**: Evaluate CEP framework on non-travel planning tasks (e.g., home renovation planning, event organization) to assess domain generalizability of the trajectory tuning approach

2. **Memory Mechanism Analysis**: Conduct ablation studies comparing memory recollection with full self-reflection across different conversation lengths to quantify memory utility gains

3. **Baseline Comparison Gap**: Implement and test additional baseline methods including more recent LLMs (GPT-4, Claude) and traditional planning algorithms to better contextualize CEP's performance advantages