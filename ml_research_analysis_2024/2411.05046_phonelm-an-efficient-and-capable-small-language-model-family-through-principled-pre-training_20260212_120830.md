---
ver: rpa2
title: PhoneLM:an Efficient and Capable Small Language Model Family through Principled
  Pre-training
arxiv_id: '2411.05046'
source_url: https://arxiv.org/abs/2411.05046
tags:
- uni00000013
- phonelm
- uni00000014
- uni00000048
- https
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work introduces PhoneLM, a family of small language models
  (SLMs) designed for on-device deployment on smartphones. The core principle behind
  PhoneLM is to optimize the model architecture for runtime efficiency on target hardware
  before pre-training, which is a departure from existing approaches that prioritize
  capability over efficiency.
---

# PhoneLM:an Efficient and Capable Small Language Model Family through Principled Pre-training

## Quick Facts
- arXiv ID: 2411.05046
- Source URL: https://arxiv.org/abs/2411.05046
- Authors: Rongjie Yi; Xiang Li; Weikai Xie; Zhenyan Lu; Chenghua Wang; Ao Zhou; Shangguang Wang; Xiwen Zhang; Mengwei Xu
- Reference count: 40
- One-line primary result: PhoneLM achieves state-of-the-art speed-capability trade-offs among SLMs with similar parameter sizes, running significantly faster than comparable models on smartphone hardware.

## Executive Summary
PhoneLM is a family of small language models (SLMs) specifically designed for efficient on-device deployment on smartphones. The key innovation is optimizing the model architecture for runtime efficiency on target hardware before pre-training, rather than optimizing post-training. This approach leverages the observation that runtime speed is more sensitive to model architecture than final accuracy. The models demonstrate superior speed-capability trade-offs, running significantly faster than comparable models on smartphone hardware while maintaining competitive accuracy on zero-shot tasks.

## Method Summary
PhoneLM uses an exhaustive ahead-of-pretraining architecture search on smartphone hardware to identify optimal configurations, followed by pre-training on open datasets and fine-tuning for specific tasks. The methodology involves testing various SLM configurations (100M and 200M parameters) on target hardware to find architectures that maximize throughput without significant accuracy loss. Models are pre-trained on large corpora (1.1T and 1.5T tokens for 0.5B and 1.5B parameter variants respectively) using open datasets, then fine-tuned for instruction following and function calling capabilities. The approach incorporates specific optimizations like ReLU activation and pre-quantized positional embeddings to enhance runtime efficiency on smartphone NPUs.

## Key Results
- PhoneLM models achieve state-of-the-art speed-capability trade-offs among SLMs with similar parameter sizes
- The 0.5B parameter PhoneLM runs at 33.9 tokens/second on Xiaomi 14, significantly faster than comparable models
- PhoneLM demonstrates competitive accuracy on 7 zero-shot tasks while maintaining superior runtime performance
- The models support Android intent invocation through function calling capabilities fine-tuned on DroidCall dataset

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Optimizing SLM architecture for runtime efficiency before pre-training yields better speed-capability trade-offs than post-training optimization.
- Mechanism: Runtime speed is more sensitive to model architecture than final accuracy, allowing for architecture search to identify configurations that maximize speed without significant accuracy loss.
- Core assumption: The cost of pre-training SLMs for each device is amortized by deploying SLM as a system-level service.
- Evidence anchors: [abstract] "runtime speed is more sensitive to model architecture than final accuracy"; [section] "we test a bunch of SLMs with 100M and 200M parameters using various configurations... We then compare their loss on the same validation dataset..."
- Break condition: If pre-training cost is not amortized by system-level deployment, the ahead-of-pretraining approach may not be cost-effective.

### Mechanism 2
- Claim: Using ReLU activation function improves runtime efficiency on smartphones compared to SiLU or GELU.
- Mechanism: ReLU is more efficient to compute on smartphones, especially for NPUs that specialize in integer calculations, and it brings more sparsity to the FFN structure, facilitating inference acceleration.
- Core assumption: The hardware architecture of smartphones, particularly NPUs, favors integer-based operations over floating-point operations.
- Evidence anchors: [section] "PhoneLM uses a rather legacy activation type ReLU... There are two reasons. First, calculating the ReLU operator on a smartphone is more efficient than the SiLU operator..."
- Break condition: If smartphone hardware evolves to favor floating-point operations or if SiLU/GELU implementations become more efficient, this mechanism may no longer hold.

### Mechanism 3
- Claim: Pre-quantized positional embeddings improve runtime efficiency on smartphones.
- Mechanism: Quantizing the values of sin and cos of RoPE to INT8 reduces computational load while introducing minimal loss in accuracy, accelerating the quantization calculation on smartphones.
- Core assumption: The fixed cosine function without outliers can be effectively quantized to INT8 without significant accuracy degradation.
- Evidence anchors: [section] "In order to accelerate the quantization calculation on smartphones, we quantize the values of sin and cos of RoPE to INT8 because it is a fixed cosine function without outliers..."
- Break condition: If the quantization process introduces significant accuracy loss or if smartphone hardware becomes more efficient at handling floating-point operations, this mechanism may no longer be beneficial.

## Foundational Learning

- Concept: Transformer architecture and its components (attention mechanisms, feed-forward networks, activation functions).
  - Why needed here: Understanding the transformer architecture is crucial for comprehending how PhoneLM is designed and optimized for runtime efficiency.
  - Quick check question: What are the key components of a transformer architecture and how do they contribute to the model's performance?

- Concept: Hardware-software co-design and optimization.
  - Why needed here: PhoneLM's design is guided by the principle of optimizing the architecture for specific hardware before pre-training, highlighting the importance of hardware-software co-design.
  - Quick check question: How does optimizing a model's architecture for specific hardware characteristics improve its runtime efficiency?

- Concept: Quantization and its impact on model performance and efficiency.
  - Why needed here: PhoneLM uses pre-quantized positional embeddings to improve runtime efficiency on smartphones, demonstrating the role of quantization in model optimization.
  - Quick check question: What is quantization and how does it affect a model's performance and computational efficiency?

## Architecture Onboarding

- Component map: PhoneLM uses a vanilla transformer decoder architecture with specific hyperparameters optimized for smartphone hardware, including hidden size, intermediate hidden size, heads, layers, vocabulary size, and context length.
- Critical path: The critical path involves architecture search on smartphone hardware to identify the most efficient configuration, followed by pre-training on open datasets, and finally fine-tuning for specific tasks.
- Design tradeoffs: The tradeoff between model size and runtime efficiency, as well as the tradeoff between accuracy and computational cost when using quantization techniques.
- Failure signatures: If the architecture search does not adequately consider the target hardware characteristics, the resulting model may not achieve the desired runtime efficiency. Additionally, if quantization introduces significant accuracy loss, the model's performance may suffer.
- First 3 experiments:
  1. Test the inference speed of different model configurations on the target smartphone hardware to identify the most efficient architecture.
  2. Evaluate the accuracy of the optimized model on a validation dataset to ensure that the runtime efficiency gains do not come at the cost of significant accuracy loss.
  3. Test the quantized positional embeddings on the smartphone hardware to verify that they improve runtime efficiency without introducing significant accuracy degradation.

## Open Questions the Paper Calls Out
No open questions were explicitly called out in the provided content.

## Limitations
- The architecture search methodology lacks transparency regarding the specific search space and selection criteria
- Runtime performance claims are limited to a single smartphone model (Xiaomi 14 with Snapdragon 8Gen3) and may not generalize
- Efficiency gains from ReLU activation and pre-quantized positional embeddings lack detailed ablation studies demonstrating individual contributions

## Confidence

**High Confidence**: The core claim that runtime speed is more sensitive to model architecture than final accuracy is supported by experimental evidence showing significant throughput variations across different configurations with similar validation loss.

**Medium Confidence**: The assertion that PhoneLM achieves state-of-the-art speed-capability trade-offs among SLMs with similar parameter sizes is credible but limited by the evaluation scope and hardware specificity.

**Low Confidence**: The claims about specific architectural choices (ReLU, pre-quantized positional embeddings) being optimal for smartphone efficiency lack sufficient comparative analysis against alternative approaches.

## Next Checks

1. **Hardware Platform Generalization**: Test PhoneLM variants on multiple smartphone architectures (e.g., MediaTek, Samsung Exynos) to verify that the efficiency gains are not specific to Snapdragon processors.

2. **Ablation Study of Architectural Choices**: Conduct controlled experiments isolating the contributions of ReLU activation and pre-quantized positional embeddings to determine their individual impact on runtime efficiency and accuracy.

3. **Long-tail Performance Analysis**: Evaluate PhoneLM on edge cases and adversarial prompts to identify potential failure modes where the efficiency optimizations might compromise capability, particularly in scenarios requiring complex reasoning or handling of rare tokens.