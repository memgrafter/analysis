---
ver: rpa2
title: 'SK-VQA: Synthetic Knowledge Generation at Scale for Training Context-Augmented
  Multimodal LLMs'
arxiv_id: '2406.19593'
source_url: https://arxiv.org/abs/2406.19593
tags:
- context
- dataset
- image
- sk-vqa
- synthetic
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces SK-VQA, the largest synthetic multimodal
  dataset for knowledge-based visual question answering (KB-VQA), containing over
  2 million visual question-answer pairs paired with context documents. The dataset
  is generated using GPT-4 to create context documents and QA pairs for images from
  diverse sources including LAION, Wikipedia, and synthetic COCO-Counterfactuals images.
---

# SK-VQA: Synthetic Knowledge Generation at Scale for Training Context-Augmented Multimodal LLMs

## Quick Facts
- **arXiv ID**: 2406.19593
- **Source URL**: https://arxiv.org/abs/2406.19593
- **Reference count**: 40
- **Primary result**: Largest synthetic multimodal dataset (2M+ samples) for knowledge-based VQA with enhanced generalization capabilities

## Executive Summary
This paper introduces SK-VQA, a synthetic multimodal dataset containing over 2 million visual question-answer pairs paired with context documents. Generated using GPT-4, the dataset overcomes limitations of existing KB-VQA datasets by not requiring Wikipedia-linked images and enabling more diverse question types. Human evaluation confirmed high quality with 77% accuracy on sampled pairs. Extensive experiments demonstrate that SK-VQA serves as both a challenging benchmark and effective training resource, with models trained on it showing enhanced generalization in context-aware VQA and multimodal RAG settings while outperforming existing datasets in out-of-domain performance.

## Method Summary
The SK-VQA dataset is created through a fully automated synthetic data generation pipeline using GPT-4 to produce context documents and multiple QA pairs for images from diverse sources including LAION, Wikipedia, and synthetic COCO-Counterfactuals. The approach generates both context and QA pairs in a single inference step, conditioned on the task requirements. Two filtering stages (IR and CAP) remove context documents that reference images or lack explicit answers. The dataset is publicly available and supports training context-augmented multimodal LLMs, with fine-tuning procedures specified for models like LLaVA-7B and PaliGemma-3B using specific hyperparameters.

## Key Results
- SK-VQA contains over 2 million visual question-answer pairs with context documents, making it the largest synthetic KB-VQA dataset
- Models trained on SK-VQA achieved significant zero-shot improvements on InfoSeek and Enc-VQA while outperforming other datasets on ViQuAE
- Human evaluation confirmed 77% accuracy on sampled QA pairs, demonstrating high quality of synthetic generation
- Filtered subsets (SK-VQAIR and SK-VQAIR+CAP) retained 64-83% of data while improving focus and training efficiency

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Synthetic context generation conditioned on QA creation produces more diverse and relevant context documents
- Mechanism: By generating both context documents and QA pairs in a single GPT-4 inference step, the context generation is explicitly conditioned on the task of creating questions that require reasoning over both the image and context
- Core assumption: GPT-4's powerful language capabilities allow it to generate contextually relevant knowledge that is specifically tailored for question creation
- Evidence anchors:
  - [abstract]: "Unlike existing datasets which are limited to images that can be linked to Wikipedia passages, our dataset encompasses a broader range of images, features a more diverse set of question types, and possesses richer linguistic style"
  - [section 3.1]: "we construct SK-VQA using a fully automated synthetic multimodal data generation approach which utilizes a strong foundation model (GPT-4) to produce relevant context documents and multiple QA pairs for a given image"
  - [corpus]: Weak evidence - corpus focuses on related RAG work rather than synthetic data generation mechanisms

### Mechanism 2
- Claim: Diverse image sources improve model generalization across different knowledge domains
- Mechanism: By using images from LAION, Wikipedia, and synthetic COCO-Counterfactuals, the dataset covers a broader range of topics including art, fashion, sports, events, and music rather than just entity-specific knowledge
- Core assumption: Models trained on diverse data sources will generalize better to unseen domains than those trained on homogeneous datasets
- Evidence anchors:
  - [section 4.3]: "Unlike previous KB-VQA datasets which have focused primarily on entity-specific knowledge, our dataset spans a broader range of topics such as art, fashion, sports, events, and music"
  - [section 5.3]: "models trained on our SK-VQA dataset achieve significant zero-shot improvements on both InfoSeek and Enc-VQA while outperforming the models trained on the other two datasets on ViQuAE"
  - [corpus]: Moderate evidence - related work shows importance of diverse data but not specifically for RAG systems

### Mechanism 3
- Claim: Filtering techniques can improve performance while reducing dataset size
- Mechanism: Two-stage filtering removes context documents that directly reference images (IR filtering) and those where answers aren't present in context (CAP filtering), creating focused subsets
- Core assumption: Context documents that don't reference the image and contain explicit answers provide cleaner training signals for RAG systems
- Evidence anchors:
  - [section 3.2-3.3]: "we create a filtered subset of our dataset which excludes these cases by identifying the presence of the words picture, photo, image, or painting in the generated context document"
  - [section 5.5]: "For data from LAION, SK-VQAIR retains 64% of the original data, while SK-VQAIR+CAP retains 40%. For data from Wikipedia, SK-VQAIR retains 83%, and SK-VQAIR+CAP retains 50%"
  - [corpus]: Weak evidence - corpus doesn't discuss filtering strategies in synthetic data generation

## Foundational Learning

- Concept: Multimodal retrieval-augmented generation (RAG) systems
  - Why needed here: Understanding how context documents are used to augment multimodal question answering is fundamental to appreciating why SK-VQA was created
  - Quick check question: What is the key difference between standard VQA and knowledge-based VQA in terms of required external knowledge?

- Concept: Synthetic data generation for training multimodal models
  - Why needed here: The paper's core contribution relies on understanding how synthetic data can be used to train models that require context-augmented generation
  - Quick check question: Why might synthetic data generation be preferable to using only naturally occurring data for training context-augmented MLLMs?

- Concept: Question diversity and vocabulary analysis
  - Why needed here: The paper emphasizes how SK-VQA achieves superior question diversity compared to existing datasets, which is crucial for understanding its effectiveness
  - Quick check question: How does question diversity in a training dataset relate to model generalization performance?

## Architecture Onboarding

- Component map: GPT-4 → Context/Document Generation → QA Pair Generation → Dataset Filtering → Model Training → Evaluation
- Critical path: Image input → GPT-4 prompt generation → Context + QA extraction → Filtering → Training data creation
- Design tradeoffs: Scale vs. quality (generating 2M+ samples vs. ensuring high accuracy), diversity vs. domain specificity (multiple image sources vs. focused knowledge domains)
- Failure signatures: Low semantic evaluation scores, high hallucination rates in context documents, poor out-of-domain generalization despite good in-domain performance
- First 3 experiments:
  1. Test GPT-4 generation pipeline with 100 images to validate context relevance and QA quality
  2. Apply filtering techniques to a small subset to measure impact on data quality and quantity
  3. Fine-tune a small MLLM on filtered SK-VQA subset and evaluate on existing KB-VQA datasets to verify generalization claims

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal ratio of real to synthetic data for training context-augmented multimodal LLMs?
- Basis in paper: Inferred from the paper's discussion of synthetic data generation and its application to training MLLMs, as well as references to related work that has explored data ratios (Howard et al., 2022; Le et al., 2024).
- Why unresolved: The paper focuses on the creation and evaluation of SK-VQA, a synthetic dataset, but does not explore the optimal balance between real and synthetic data for training.
- What evidence would resolve it: Experiments comparing the performance of models trained on varying ratios of real to synthetic data, using metrics such as out-of-domain generalization and performance on KB-VQA tasks.

### Open Question 2
- Question: How do different filtering techniques (IR and CAP) impact the performance of models trained on SK-VQA across various image sources?
- Basis in paper: Explicit discussion of IR and CAP filtering techniques and their application to different subsets of the dataset (SK-VQAIR and SK-VQAIR+CAP).
- Why unresolved: While the paper presents some ablation studies on filtering techniques, it does not comprehensively explore the impact of these filters on model performance across all image sources (LAION, Wikipedia, COCO-Counterfactuals).
- What evidence would resolve it: Detailed experiments comparing model performance on different image source subsets (LAION, Wikipedia, COCO-Counterfactuals) when trained on data filtered by various combinations of IR and CAP techniques.

### Open Question 3
- Question: Can the quality of synthetic data generated by LLaVA-34B be improved to match or exceed that of GPT-4 for training context-augmented multimodal LLMs?
- Basis in paper: Explicit mention of experiments using LLaVA-34B for data generation and the authors' observation that a significant proportion of questions generated by LLaVA-34B were invalid.
- Why unresolved: The paper does not explore potential methods to improve the quality of data generated by LLaVA-34B, such as prompt engineering, fine-tuning, or other techniques.
- What evidence would resolve it: Experiments demonstrating improved data quality from LLaVA-34B through various techniques, validated by human evaluation or downstream task performance.

## Limitations
- Exact filtering criteria implementation details are not fully specified, making exact reproduction challenging
- The 77% human evaluation accuracy may not capture edge cases across the full 2M+ dataset
- The synthetic data generation pipeline may need recalibration as GPT-4 capabilities evolve over time

## Confidence
- **High Confidence**: Claims about dataset scale (2M+ samples), diversity of image sources, and general effectiveness of synthetic data generation for KB-VQA training
- **Medium Confidence**: Claims about superiority over existing datasets in both in-domain and out-of-domain performance, particularly regarding the specific magnitude of improvements
- **Low Confidence**: Long-term stability claims about synthetic data generation pipeline and complete robustness to GPT-4 capability changes

## Next Checks
1. **Implementation Verification**: Re-implement the IR and CAP filtering criteria exactly as described and measure the filtering rate on a sample of 1000 images to verify alignment with the reported 64% (LAION) and 83% (Wikipedia) retention rates

2. **Temporal Robustness Test**: Generate a small synthetic dataset (100 images) using both current GPT-4 and an older version of GPT-4 to measure variance in context document quality and QA pair accuracy

3. **Domain-Specific Performance Analysis**: Conduct detailed ablation studies on each image source (LAION, Wikipedia, COCO-Counterfactuals) to identify which sources contribute most to out-of-domain generalization versus in-domain performance