---
ver: rpa2
title: 'Scalable Bayesian Inference in the Era of Deep Learning: From Gaussian Processes
  to Deep Neural Networks'
arxiv_id: '2404.19157'
source_url: https://arxiv.org/abs/2404.19157
tags:
- posterior
- prior
- linearised
- linear
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This thesis develops scalable methods to equip neural networks
  with model uncertainty, addressing the challenge of Bayesian inference in large
  neural networks. The core idea is to leverage the linearised Laplace approximation,
  which transforms Bayesian inference in neural networks into Bayesian inference in
  conjugate Gaussian-linear models.
---

# Scalable Bayesian Inference in the Era of Deep Learning: From Gaussian Processes to Deep Neural Networks

## Quick Facts
- arXiv ID: 2404.19157
- Source URL: https://arxiv.org/abs/2404.19157
- Authors: Javier Antoran
- Reference count: 27
- Key outcome: This thesis develops scalable methods to equip neural networks with model uncertainty, addressing the challenge of Bayesian inference in large neural networks. The core idea is to leverage the linearised Laplace approximation, which transforms Bayesian inference in neural networks into Bayesian inference in conjugate Gaussian-linear models. This allows us to borrow ideas from deep learning to make probabilistic methods more scalable. We use stochastic gradient descent (SGD) to perform posterior sampling in linear models and their convex duals: Gaussian processes. We also resolve incompatibilities between the classical linearised Laplace model evidence and modern deep learning practices, such as stochastic optimisation, early stopping, and normalisation layers. Our sample-based EM algorithm for hyperparameter learning in linearised neural networks allows us to scale to modern architectures and datasets, such as ResNet-50 on Imagenet. We apply these methods to uncertainty estimation and experimental design for computed tomography (CT) image and volume reconstruction, achieving state-of-the-art results.

## Executive Summary
This thesis addresses the challenge of Bayesian inference in large neural networks by developing scalable methods that leverage the linearised Laplace approximation. The key insight is that linearising a neural network at its trained weights creates a tractable Gaussian-linear model whose posterior uncertainty approximates the NN's uncertainty. By transforming Bayesian inference in neural networks into Bayesian inference in conjugate Gaussian-linear models, we can borrow ideas from deep learning to make probabilistic methods more scalable. We use stochastic gradient descent (SGD) to perform posterior sampling in linear models and their convex duals: Gaussian processes. We also resolve incompatibilities between the classical linearised Laplace model evidence and modern deep learning practices, such as stochastic optimisation, early stopping, and normalisation layers. Our sample-based EM algorithm for hyperparameter learning in linearised neural networks allows us to scale to modern architectures and datasets, such as ResNet-50 on Imagenet. We apply these methods to uncertainty estimation and experimental design for computed tomography (CT) image and volume reconstruction, achieving state-of-the-art results.

## Method Summary
The thesis develops scalable methods to equip neural networks with model uncertainty by leveraging the linearised Laplace approximation. This approximation transforms Bayesian inference in neural networks into Bayesian inference in conjugate Gaussian-linear models. The key steps are: 1) Train a neural network to obtain weights ṽ, 2) Compute the Jacobian J(ṽ) using forward/backward automatic differentiation, 3) Build a linear model h(w) = g(ṽ) + J(ṽ)(w - ṽ), 4) Choose a prior precision A (isotropic, layerwise, or g-prior), and 5) Perform Bayesian inference on h(w) to get the posterior. The posterior covariance acts as an uncertainty estimate. To scale to large models, the thesis uses SGD for GP posterior sampling, sample-based EM for linear model inference, and matrix-free linear algebra (Jacobian-vector products only). The thesis also resolves incompatibilities between the classical linearised Laplace model evidence and modern deep learning practices, such as stochastic optimisation, early stopping, and normalisation layers.

## Key Results
- The linearised Laplace approximation transforms Bayesian inference in neural networks into Bayesian inference in conjugate Gaussian-linear models, enabling scalable uncertainty estimation.
- The thesis resolves incompatibilities between the classical linearised Laplace model evidence and modern deep learning practices, such as stochastic optimisation, early stopping, and normalisation layers.
- The sample-based EM algorithm for hyperparameter learning in linearised neural networks allows scaling to modern architectures and datasets, such as ResNet-50 on Imagenet.
- The methods achieve state-of-the-art results in uncertainty estimation and experimental design for computed tomography (CT) image and volume reconstruction.

## Why This Works (Mechanism)
### Mechanism 1
- Claim: Linearising a neural network at its trained weights creates a tractable Gaussian-linear model whose posterior uncertainty approximates the NN's uncertainty.
- Mechanism: The first-order Taylor expansion around the trained weights maps the NN to an affine function with Jacobian basis. Bayesian inference on this linear model (with Gaussian prior) yields closed-form posterior mean and covariance. The mean matches the NN output; the covariance acts as uncertainty estimate.
- Core assumption: The linearisation point is a mode of the NN's loss (or at least close to one).
- Evidence anchors:
  - [abstract] "We leverage the linearised Laplace approximation to equip pre-trained neural networks with the uncertainty estimates provided by their tangent linear models."
  - [section] "MacKay (1992a) resolves this by introducing an additional approximation: a local linearisation of the neural network function around v⋆."
  - [corpus] Weak/no direct evidence in corpus.
- Break condition: If the linearisation point is far from a loss mode, the linear model poorly approximates the NN; errorbars become unreliable.

### Mechanism 2
- Claim: The linear model's Jacobian features provide a data-dependent basis that captures the NN's learned function structure.
- Mechanism: The NN's Jacobian at the linearisation point acts as a feature expansion. This basis is not arbitrary—it reflects how small weight perturbations affect outputs for the specific data the NN was trained on. Bayesian inference with this basis yields uncertainty estimates that respect the NN's inductive biases.
- Core assumption: The NN's Jacobian is full rank and spans the relevant function space.
- Evidence anchors:
  - [abstract] "This turns the problem of Bayesian inference in neural networks into one of Bayesian inference in conjugate Gaussian-linear models."
  - [section] "The key observation, made by Khan et al. (2019b), is that the GGN-Laplace posterior matches the true posterior of the tangent linear model h."
  - [corpus] Weak/no direct evidence in corpus.
- Break condition: If the Jacobian basis is rank-deficient or the NN is too deep/nonlinear, the linear approximation fails.

### Mechanism 3
- Claim: The model evidence (marginal likelihood) of the linear model selects hyperparameters that balance fit and complexity, avoiding overfitting.
- Mechanism: The evidence integrates out the linear model's weights, rewarding priors that concentrate on parameter settings fitting the data well. This automatic Occam's razor favours simpler models unless data strongly supports complexity.
- Core assumption: The linear model is a good surrogate for the NN; evidence maximisation in the linear model corresponds to good NN uncertainty.
- Evidence anchors:
  - [abstract] "We resolve these and construct a sample-based EM algorithm for scalable hyperparameter learning with linearised neural networks."
  - [section] "The marginal likelihood of of some model M1 differs from the regular likelihood in that the model parameters have been marginalised out."
  - [corpus] Weak/no direct evidence in corpus.
- Break condition: If the linear model is a poor surrogate, evidence maximisation may overfit or underfit the NN's uncertainty.

## Foundational Learning
- Concept: Gaussian processes as the function-space dual of Gaussian linear regression.
  - Why needed here: The thesis repeatedly uses the duality between weight-space linear models and GPs to design scalable inference algorithms (e.g., pathwise conditioning, inducing points).
  - Quick check question: Can you write the posterior mean of a GP in both weight-space and function-space forms?

- Concept: Pathwise conditioning for GP posterior sampling.
  - Why needed here: Chapter 4 uses pathwise conditioning to recast GP inference as optimisation, enabling SGD-based sampling without explicit covariance matrices.
  - Quick check question: How does the pathwise form (f|Y)(·) = f(·) + K(·)Xα differ from the standard mean-covariance form?

- Concept: The linearised Laplace approximation and its connection to the neural tangent kernel.
  - Why needed here: Chapters 5-6 rely on linearising NNs to obtain tractable Bayesian inference; understanding the NTK connection clarifies when linearisation is accurate.
  - Quick check question: When does the linearised Laplace posterior match the NTK GP posterior?

## Architecture Onboarding
- Component map:
  - Pre-trained NN -> Jacobian basis -> Linear model -> Bayesian inference (exact or approx.) -> Uncertainty estimates
  - Evidence maximisation (exact or sample-based) -> Prior precision tuning -> Improved uncertainty calibration
  - SGD for GP posterior sampling, Sample-based EM for linear model inference, Matrix-free linear algebra (Jacobian-vector products only)

- Critical path:
  1. Train NN to obtain weights ṽ.
  2. Compute Jacobian J(ṽ) (via forward/backward AD).
  3. Build linear model h(w) = g(ṽ) + J(ṽ)(w - ṽ).
  4. Choose prior precision A (isotropic, layerwise, or g-prior).
  5. Perform Bayesian inference on h(w) to get posterior.
  6. Use posterior covariance as uncertainty estimate.

- Design tradeoffs:
  - Exact vs approximate inference: Exact is intractable for large models; approximations (KFAC, sampling) trade accuracy for scalability.
  - Prior choice: Isotropic is simple but may under/over-regularise; layerwise or g-prior adapt to network structure but add hyperparameters.
  - Linearisation point: Trained weights are convenient but not necessarily a loss mode; using the linear model's mode may improve evidence.

- Failure signatures:
  - Overestimated uncertainty: Linear model poorly approximates NN; evidence maximisation overfits.
  - Underestimated uncertainty: Prior too tight; linearisation too crude.
  - Slow hyperparameter convergence: Poor evidence approximation; too few posterior samples.

- First 3 experiments:
  1. Linearise a small MLP on MNIST; compare exact Laplace uncertainty to NN ensemble uncertainty.
  2. Apply sample-based EM to a LeNet on CIFAR-10; evaluate calibration vs MAP/KFAC.
  3. Use linearised DIP for CT reconstruction; compare uncertainty estimates to MC dropout.

## Open Questions the Paper Calls Out
### Open Question 1
- Question: How does the choice of prior precision (regularizer) affect the uncertainty calibration of linearised Laplace, particularly in the context of modern deep learning architectures with normalization layers?
- Basis in paper: [explicit] Section 5.2 discusses the sensitivity of linearised Laplace to prior precision and introduces the layerwise prior and diagonal g-prior to address this issue.
- Why unresolved: The paper shows empirical improvements with these priors but doesn't provide a rigorous theoretical justification for their effectiveness.
- What evidence would resolve it: A theoretical analysis explaining why the layerwise prior and diagonal g-prior are effective in addressing the scaling invariance issue introduced by normalization layers.

### Open Question 2
- Question: How can we efficiently scale hyperparameter learning for Gaussian processes and linearised neural networks beyond the effective dimension-based MacKay update?
- Basis in paper: [explicit] Section 6.2.1 discusses the MacKay update for hyperparameter learning but notes its limitations for hyperparameters beyond the prior precision.
- Why unresolved: The MacKay update is limited to diagonal prior precisions and doesn't generalize to other hyperparameters like kernel lengthscales or noise variance.
- What evidence would resolve it: Development of a more general hyperparameter learning method that scales to large-scale problems and can handle various types of hyperparameters.

### Open Question 3
- Question: How can we leverage the uncertainty estimates from linearised DIP for adaptive experimental design in computed tomography?
- Basis in paper: [explicit] Section 7.5 introduces the concept of using linearised DIP for experimental design but focuses on synthetic data and doesn't explore its application to real-world CT data.
- Why unresolved: The paper demonstrates the potential of linearised DIP for adaptive design but doesn't investigate its performance on real-world CT data or compare it to other experimental design methods.
- What evidence would resolve it: Application of linearised DIP-based experimental design to real-world CT datasets and comparison with other methods in terms of reconstruction quality and radiation dose reduction.

## Limitations
- The linearised Laplace approximation relies on first-order Taylor expansion, which may poorly capture highly nonlinear regimes in deep networks.
- The thesis acknowledges that linearisation accuracy depends on the trained weights being near a loss mode, but empirical validation of this assumption across diverse architectures is limited.
- The sample-based EM algorithm's convergence properties and sensitivity to initialisation require further study.

## Confidence
- **High confidence**: The theoretical connection between linearised Laplace approximation and Gaussian-linear models; the use of SGD for GP posterior sampling.
- **Medium confidence**: Scalability claims for ResNet-50 on ImageNet; effectiveness of linearised DIP for CT reconstruction.
- **Low confidence**: Generalisability of uncertainty estimates to out-of-distribution data; robustness of sample-based EM to hyperparameter initialisation.

## Next Checks
1. Compare linearised Laplace uncertainty estimates against MC dropout and deep ensembles on a diverse set of benchmarks (e.g., CIFAR-10-C, ImageNet-A) to assess calibration and robustness.
2. Evaluate the sensitivity of sample-based EM to prior initialisation and learning rates by systematically varying these hyperparameters and measuring evidence convergence.
3. Test the linearisation accuracy by computing the Frobenius norm of the difference between NN outputs and their linear approximations across different layers and network depths.