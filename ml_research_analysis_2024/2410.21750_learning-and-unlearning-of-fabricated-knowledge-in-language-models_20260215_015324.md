---
ver: rpa2
title: Learning and Unlearning of Fabricated Knowledge in Language Models
arxiv_id: '2410.21750'
source_url: https://arxiv.org/abs/2410.21750
tags:
- arxiv
- facts
- language
- knowledge
- fact
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates how long new knowledge lasts in language
  models during continued training. The authors created a dataset called "Outlandish"
  with knowledge-conflicting facts and their mundane/random variants to probe LM memory
  retention.
---

# Learning and Unlearning of Fabricated Knowledge in Language Models

## Quick Facts
- arXiv ID: 2410.21750
- Source URL: https://arxiv.org/abs/2410.21750
- Authors: Chen Sun; Nolan Andrew Miller; Andrey Zhmoginov; Max Vladymyrov; Mark Sandler
- Reference count: 28
- Key outcome: This paper investigates how long new knowledge lasts in language models during continued training using knowledge-conflicting facts (KCFs)

## Executive Summary
This paper investigates the persistence and removal of fabricated knowledge in language models through controlled injection of knowledge-conflicting facts (KCFs). The authors demonstrate that KCFs are remembered for tens of thousands of training steps, significantly longer than mundane or randomly jumbled facts. They show that KCFs occupy a "sweet spot" in novelty that makes them particularly memorable and prone to inappropriate priming effects. The paper introduces a novel multi-step sparse update procedure that can largely erase KCFs while preserving the model's main task training ability.

## Method Summary
The authors created the "Outlandish" dataset containing knowledge-conflicting facts (KCFs) and their mundane/random variants. They finetuned PALM-8B models on various datasets (Alpaca, Flan, SuperGlue) while regularly injecting KCFs into the training data. Memory retention was measured by tracking next-token prediction accuracy and perplexity at keyword positions. For unlearning, they applied a multi-step sparse update procedure that zeroes out bottom-k% of parameter updates by gradient magnitude to erase KCF effects while maintaining task performance.

## Key Results
- Knowledge-conflicting facts are remembered for tens of thousands of training steps, significantly longer than mundane or randomly jumbled facts
- KCFs can inappropriately "prime" hallucinations on unrelated prompts more than other fact types
- A simple multi-step sparse update procedure can largely erase KCFs while preserving main task training performance
- KCFs occupy a sweet spot in novelty between consistency and randomness for optimal memorability

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Knowledge-conflicting facts (KCFs) occupy a "sweet spot" in novelty between consistency and randomness, leading to optimal memorability.
- Mechanism: When injected facts are too consistent with world knowledge, they are treated as expected and learned normally. When too random, they are treated as noise. KCFs create sufficient "surprise" to be flagged for encoding while still being structured enough to be retained.
- Core assumption: Language models have an implicit novelty-detection mechanism that differentially weights unexpected but structured information.
- Evidence anchors:
  - [abstract] "facts that conflict with common knowledge are remembered for tens of thousands of training steps, while prompts not conflicting with common knowledge (mundane), as well as scrambled prompts (randomly jumbled) are both forgotten much more rapidly"
  - [section 4.2] "these results indicate that the new facts that were the easiest to inject into LMs, and the most enduring, were facts that occupied a sweet spot in the spectrum of novelty between total consistency and total randomness"
  - [corpus] Weak: related papers focus on unlearning rather than initial memorability mechanisms
- Break condition: If the model's novelty detection mechanism is disabled or overridden by other training signals, the sweet spot effect would disappear.

### Mechanism 2
- Claim: KCFs cause inappropriate "priming" effects on logically unrelated prompts sharing the same objects.
- Mechanism: The model encodes associations from KCFs that become active during inference, even when contextually inappropriate. This creates spurious connections between learned facts and unrelated reasoning.
- Core assumption: Language models form associative memories that can be triggered by shared lexical elements regardless of logical context.
- Evidence anchors:
  - [abstract] "knowledge-conflicting facts can 'prime' how the language model hallucinates on logically unrelated prompts much more than these two extremes of full consistency and full randomness"
  - [section 4.2] "the sentence shown in Fig. 4e uses the tokens '79' to denote the knowledge-conflicting fact... Following finetuning, the tokens '7' and '9' together was then recruited to describe the running speed of mammals"
  - [corpus] Weak: related work focuses on unlearning rather than priming mechanisms
- Break condition: If the model's attention mechanisms were modified to be more context-sensitive and less associative, priming effects would be reduced.

### Mechanism 3
- Claim: Multi-step sparse updates can erase poisoned facts while preserving task performance.
- Mechanism: KCF memories are stored sparsely in parameter space. By sparsifying the cumulative gradient updates (removing bottom k% by magnitude), the specific parameters storing KCFs can be zeroed out while leaving the majority of task-relevant parameters intact.
- Core assumption: The parameter space has sufficient redundancy that removing small-magnitude updates doesn't significantly impact task performance.
- Evidence anchors:
  - [abstract] "impacts of knowledge-conflicting facts in LMs, though they can be long lasting, can be largely erased by novel application of multi-step sparse updates, even while the training ability of the model is preserved"
  - [section 4.3] "zeroing out the bottom 90% of the KCF parameter updates by gradient magnitude during training on the poison fact still retained memory of the poison fact but zeroing out the top 20% of the KCF parameter updates totally erased next token prediction"
  - [section 4.3] "At 85% sparsification (green line), the KCF has been nearly entirely erased while finetuning had been largely unaffected"
- Break condition: If KCF memories were distributed across all parameters rather than sparsely localized, sparsification would harm task performance.

## Foundational Learning

- Concept: Catastrophic forgetting
  - Why needed here: The paper investigates how long injected facts persist during continued training, which relates to how neural networks balance new learning with preserving existing knowledge.
  - Quick check question: What mechanism allows language models to retain some knowledge while forgetting other knowledge during continued training?

- Concept: Sparse representations in neural networks
  - Why needed here: The sparsification technique relies on the assumption that memories are stored in sparse subsets of parameters, which is a fundamental property being exploited.
  - Quick check question: Why does removing small-magnitude parameter updates often preserve task performance in deep neural networks?

- Concept: Associative memory and priming
  - Why needed here: The priming effect demonstrates how language models form and retrieve associations that can be triggered inappropriately, which is central to understanding the KCF behavior.
  - Quick check question: How do language models form associations between lexical elements that can be triggered even in logically inappropriate contexts?

## Architecture Onboarding

- Component map: Input data (Alpaca, Flan, SuperGlue) -> Model (Transformer-based PALM-8B) -> Memory injection (KCFs) -> Evaluation (next-token accuracy/perplexity) -> Unlearning (sparse updates)
- Critical path: Data injection → Model training → Memory retention evaluation → Unlearning procedure → Performance validation
- Design tradeoffs:
  - Memory vs. generalization: More persistent memories may lead to better recall but worse adaptation
  - Sparsity vs. completeness: Higher sparsification removes more poisoned content but risks task degradation
  - Frequency vs. duration: More frequent KCF injection leads to longer retention but requires more training
- Failure signatures:
  - Complete loss of KCF retention indicates novelty detection failure
  - Task performance degradation after sparsification indicates insufficient redundancy
  - Absence of priming effects indicates associative memory failure
- First 3 experiments:
  1. Inject a KCF into a frozen model and measure initial perplexity/accuracy to establish baseline encoding
  2. Vary KCF presentation frequency during training and measure retention duration
  3. Apply sparsification at different thresholds and measure both KCF erasure and task performance impact

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal level of novelty for knowledge injection that maximizes memorability without causing excessive hallucinations?
- Basis in paper: [explicit] The paper identifies a "sweet spot" in fact novelty between consistency and randomness where knowledge-conflicting facts (KCFs) are most enduring, but also cause more priming of hallucinations than mundane or random facts.
- Why unresolved: While the paper demonstrates this sweet spot exists and characterizes its effects, it doesn't provide a precise quantification of what constitutes the optimal novelty level for different applications or model sizes.
- What evidence would resolve it: Systematic experiments varying the degree of fact novelty across multiple model sizes and finetuning tasks, measuring both memorability and hallucination propensity to identify the optimal point.

### Open Question 2
- Question: How does the sparsification procedure affect the retention of beneficial knowledge versus harmful knowledge?
- Basis in paper: [inferred] The paper demonstrates that multi-step sparse updates can erase KCFs while preserving task performance, but doesn't explore whether this affects retention of legitimate knowledge.
- Why unresolved: The sparsification method is shown to work for erasing poisoned facts, but its broader impact on knowledge retention and model capabilities is unknown.
- What evidence would resolve it: Experiments testing the sparsification procedure on models containing both beneficial and harmful knowledge, measuring retention of legitimate facts and overall model performance.

### Open Question 3
- Question: What are the underlying mechanisms that make knowledge-conflicting facts more memorable than mundane or random facts?
- Basis in paper: [explicit] The paper observes that KCFs are more memorable than other fact types but doesn't explain the underlying mechanisms.
- Why unresolved: While the paper characterizes the phenomenon, it doesn't investigate the neural or computational mechanisms that make KCFs more memorable.
- What evidence would resolve it: Mechanistic interpretability studies examining the neural representations and attention patterns associated with KCFs versus other fact types, potentially using techniques like activation patching or causal tracing.

## Limitations

- The sparsity-based unlearning method may not generalize well to larger models or different architectures
- The Outlandish dataset construction is relatively small-scale (5 base facts with 200 variants each), limiting generalizability
- The priming effect demonstrations are qualitative rather than quantitative

## Confidence

**High Confidence**: The basic observation that knowledge-conflicting facts are retained longer than consistent or random facts during continued training is well-supported by the experimental results across multiple datasets.

**Medium Confidence**: The novelty-detection mechanism explanation for why KCFs are particularly memorable is plausible but not directly tested. The mechanistic claims about parameter sparsity and associative priming are reasonable but require more rigorous validation.

**Low Confidence**: The generalization of the sparsification technique to other model sizes and tasks is not established. The specific numerical thresholds (85% sparsification) may not transfer to different settings.

## Next Checks

1. **Cross-model validation**: Apply the KCF injection and retention experiments to smaller and larger transformer architectures (e.g., 1B and 30B parameter models) to test whether the novelty-detection sweet spot generalizes across scales.

2. **Quantitative priming analysis**: Design controlled experiments that measure the magnitude and statistical significance of priming effects using multiple fact types and prompt variations, including control conditions to isolate associative from contextual effects.

3. **Sparsity threshold robustness**: Systematically vary the sparsification threshold (e.g., 50%, 70%, 90%, 95%) and measure both KCF erasure effectiveness and task performance degradation across different finetuning objectives to establish more robust operational guidelines.