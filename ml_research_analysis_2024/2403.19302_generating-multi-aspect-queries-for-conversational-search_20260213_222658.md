---
ver: rpa2
title: Generating Multi-Aspect Queries for Conversational Search
arxiv_id: '2403.19302'
source_url: https://arxiv.org/abs/2403.19302
tags:
- mq4cs
- query
- user
- queries
- retrieval
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proposes generating multiple aspect-specific queries
  for complex conversational queries to improve retrieval. It introduces MQ4CS, a
  framework that uses an LLM to generate up to 5 queries per utterance, retrieves
  documents for each query separately, and fuses the results.
---

# Generating Multi-Aspect Queries for Conversational Search

## Quick Facts
- arXiv ID: 2403.19302
- Source URL: https://arxiv.org/abs/2403.19302
- Reference count: 40
- Key outcome: Multi-aspect query generation improves conversational search retrieval, with up to 45% gains using optimal query counts

## Executive Summary
This paper addresses the challenge of retrieving relevant documents in conversational search by generating multiple aspect-specific queries for complex user utterances. The proposed MQ4CS framework uses large language models to decompose a single conversational query into up to five queries, each targeting a different facet of the information need. By retrieving documents for each query independently and fusing the results, the approach ensures broader coverage of relevant passages that might be missed by traditional single-query methods.

Experiments on five major conversational search datasets (CAsT 19/20/22, iKAT 23, and TopiOCQA) demonstrate significant improvements over state-of-the-art single-query methods, with gains of up to 9.7% in nDCG@3 and 10.6% in MRR. The paper also introduces MASQ, a dataset of multi-aspect queries, and performs oracle analysis revealing that optimal per-utterance query count can lead to up to 45% improvement, highlighting the potential for adaptive query generation.

## Method Summary
MQ4CS uses a large language model (GPT-4 or LLaMA) to generate 1-5 multi-aspect queries per conversational utterance, representing different facets of the information need. The framework first generates a response to the user utterance using conversation context and persona (if available), then prompts the LLM to generate multiple queries targeting distinct aspects. Each query undergoes BM25 retrieval followed by Cross-Encoder reranking. The final ranking is produced by fusing results from all queries using interleaving or reranking with the LLM-generated answer.

## Key Results
- MQ4CS outperforms state-of-the-art single-query methods by up to 9.7% in nDCG@3 and 10.6% in MRR across five conversational search datasets
- Oracle analysis shows that selecting the optimal number of queries per utterance (ϕ*) can improve nDCG@3 by up to 45% compared to fixed ϕ
- More than 65% of utterances exhibit better performance using multi-aspect generated queries compared to single-query approaches
- The average optimal query count (ϕ*) varies by dataset: 2.37 for iKAT 23, 2.12 for CAsT 22, 1.96 for CAsT 20, and 1.29 for TopiOCQA

## Why This Works (Mechanism)

### Mechanism 1
Multi-aspect query generation improves retrieval by capturing distinct facets of complex user information needs. The LLM decomposes a single conversational utterance into multiple queries, each targeting a different aspect of the information need. Retrieval is performed independently for each query, and results are fused, ensuring coverage of diverse relevant passages that a single query might miss. This works because complex utterances often require evidence from multiple sources, and a single query cannot effectively represent all facets.

### Mechanism 2
Using the LLM's internal knowledge and reasoning to generate queries without fine-tuning leads to strong performance. The framework relies on zero-shot prompting of GPT-4 or LLaMA to generate multi-aspect queries and responses, leveraging the model's pre-existing world knowledge and reasoning capacity. This approach assumes that large language models possess sufficient world knowledge to accurately model user information needs and generate effective queries.

### Mechanism 3
Oracle analysis reveals that optimal per-utterance query count (ϕ*) significantly improves performance, highlighting the need for adaptive query generation. For each utterance, the number of generated queries (ϕ) is fixed, but oracle analysis shows that selecting ϕ* (the optimal number per utterance) can improve nDCG@3 by up to 45%. This demonstrates that the complexity of user utterances varies, and a fixed ϕ is suboptimal for capturing all relevant information.

## Foundational Learning

- **Conversational information seeking (CIS)**: Understanding the main challenge in conversational search compared to ad-hoc search, where user utterances depend on prior dialogue turns and may be ambiguous.

- **Query rewriting**: Learning how multi-aspect query generation differs from standard single query rewriting, extending the idea of making utterances self-contained by generating multiple queries per utterance.

- **Ranked list fusion**: Understanding common strategies for fusing multiple ranked lists in information retrieval, crucial for combining results from multiple generated queries.

## Architecture Onboarding

- **Component map**: LLM response generation (AG) -> Multi-aspect query generation (QG) -> Retrieval for each query -> Reranking -> Ranked list fusion
- **Critical path**: LLM response generation → Multi-aspect query generation → Retrieval for each query → Reranking → Ranked list fusion
- **Design tradeoffs**: Fixed ϕ vs. adaptive ϕ (simpler vs. better performance), single LLM call vs. multiple calls (efficiency vs. control), interleaving vs. reranking fusion (speed vs. quality)
- **Failure signatures**: Poor query diversity, over-generation of queries for simple utterances, latency issues with multiple queries
- **First 3 experiments**: 1) Run MQ4CS with ϕ=1 on CAsT 19 to confirm baseline performance, 2) Run MQ4CS with ϕ=5 on iKAT 23 comparing to LLM4CS, 3) Perform oracle analysis on utterance subsets

## Open Questions the Paper Calls Out

### Open Question 1
What is the optimal number of multi-aspect queries (φ) to generate for a given user utterance? The paper mentions oracle analysis showing significant gains (up to 45%) when using optimal φ, but does not propose a method to determine φ dynamically per utterance. A per-turn φ selection model that predicts the optimal number based on utterance complexity would resolve this.

### Open Question 2
How do different LLMs compare in generating multi-aspect queries, and what biases exist in their outputs? The paper uses GPT-4 and LLaMA 3.1 but does not analyze bias in query generation or compare multiple LLMs systematically. A comprehensive study comparing multiple LLMs on query diversity, bias metrics, and retrieval impact would address this.

### Open Question 3
How does the interleaving fusion strategy compare to reranking fusion in terms of retrieval quality and efficiency? The paper proposes two fusion methods but does not explore alternative fusion strategies or provide ablation studies on fusion hyperparameters. Ablation studies comparing fusion strategies across datasets would resolve this.

## Limitations

- The framework requires multiple LLM calls (response generation and query generation), increasing computational cost compared to single-query methods
- The paper does not address how to efficiently determine the optimal number of queries per utterance in real-time systems
- Performance may be limited in specialized domains where the LLM lacks sufficient world knowledge, as the approach relies on zero-shot prompting without fine-tuning

## Confidence

- **Core mechanism**: High - Consistent performance improvements across five diverse datasets support the effectiveness of multi-aspect query generation
- **LLM query generation quality**: Medium - Good results demonstrated, but lacks detailed analysis of query diversity or quality metrics
- **Practical applicability of oracle findings**: Low - Adaptive ϕ selection remains theoretical without implementation of a selection model

## Next Checks

1. Implement an adaptive query count selection model that predicts optimal ϕ per utterance and measure real-world performance impact versus fixed ϕ
2. Conduct ablation studies varying fusion strategies (interleaving vs. reranking) and first-stage retrieval methods (BM25 vs. learned sparse retrieval)
3. Test the framework on specialized domains (e.g., medical or technical search) where LLM world knowledge may be limited