---
ver: rpa2
title: 'Assessing Large Language Models for Online Extremism Research: Identification,
  Explanation, and New Knowledge'
arxiv_id: '2408.16749'
source_url: https://arxiv.org/abs/2408.16749
tags:
- extremist
- online
- post
- extremism
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This study evaluates two AI approaches\u2014BERT and GPT models\u2014\
  for detecting and classifying online extremist content. BERT, a traditional supervised\
  \ learning model, showed strong performance with large training datasets but struggled\
  \ with limited data and transfer learning between far-right and far-left content."
---

# Assessing Large Language Models for Online Extremism Research: Identification, Explanation, and New Knowledge

## Quick Facts
- arXiv ID: 2408.16749
- Source URL: https://arxiv.org/abs/2408.16749
- Authors: Beidi Dong; Jin R. Lee; Ziwei Zhu; Balassubramanian Srinivasan
- Reference count: 25
- Primary result: GPT models outperform BERT in zero-shot extremist content classification while requiring no labeled training data

## Executive Summary
This study evaluates two AI approaches—BERT and GPT models—for detecting and classifying online extremist content. BERT, a traditional supervised learning model, showed strong performance with large training datasets but struggled with limited data and transfer learning between far-right and far-left content. In contrast, GPT models demonstrated superior performance in zero-shot settings, achieving higher accuracy without requiring labeled training data. The study also tested four prompt engineering techniques for GPT, finding that more detailed prompts generally improved performance, though overly complex instructions could hinder results. GPT 4 outperformed GPT 3.5 in classifying far-right content, while GPT 3.5 excelled with far-left content. These findings highlight the potential of large language models for efficient and effective online extremism detection, offering practical advantages over traditional supervised methods. Future research should focus on optimizing prompt strategies and understanding model sensitivities to enhance classification accuracy.

## Method Summary
The researchers collected Twitter data using far-right and far-left keywords on May 28th, 2023, creating a manually labeled dataset of 542 tweets. They trained BERT models using supervised learning on varying training set sizes and evaluated performance on test sets. For GPT models, they employed zero-shot classification using four different prompt engineering techniques: naïve prompt, role-playing, professional definition, and combined approach. The study compared binary classification (extremist vs. non-extremist) and multi-class element classification across both model types, measuring precision, recall, and F1 scores.

## Key Results
- GPT models achieved superior classification accuracy compared to BERT in zero-shot settings without requiring labeled training data
- BERT performance improved significantly with larger training datasets but showed poor transfer learning between far-right and far-left content
- GPT 4 outperformed GPT 3.5 in classifying far-right extremist posts, while GPT 3.5 excelled at classifying far-left content
- Prompt engineering significantly impacted GPT performance, with overly complex instructions sometimes impairing results

## Why This Works (Mechanism)

### Mechanism 1
- Claim: GPT models achieve superior classification accuracy compared to BERT in zero-shot settings due to their ability to leverage pre-trained knowledge without requiring task-specific labeled data.
- Mechanism: GPT models use prompt engineering to guide their pre-trained understanding of language toward classification tasks. Unlike BERT, which relies on supervised learning from labeled training data, GPT can interpret natural language instructions and context embedded in prompts to make classification decisions.
- Core assumption: The pre-trained language knowledge in GPT models is sufficient to capture the nuances of extremist content classification without task-specific fine-tuning.
- Evidence anchors:
  - [abstract]: "GPT models demonstrated superior performance in zero-shot settings, achieving higher accuracy without requiring labeled training data"
  - [section]: "The essence of such zero-shot method with LLMs lies in the design of the prompt"
  - [corpus]: Weak - corpus shows related work on BERT and LLM approaches but doesn't directly address zero-shot performance differences
- Break condition: If the classification task requires highly specialized domain knowledge not well-represented in the pre-training corpus, GPT's zero-shot performance would degrade.

### Mechanism 2
- Claim: More detailed prompts generally improve GPT classification performance, but overly complex instructions can impair results.
- Mechanism: Prompt engineering provides contextual guidance that helps GPT models interpret the task correctly. Adding definitional frameworks, role assignments, and professional terminology enhances model understanding, but excessive complexity introduces confusion by overwhelming the model's processing capacity.
- Core assumption: There exists an optimal level of prompt detail that balances guidance with clarity, beyond which performance degrades.
- Evidence anchors:
  - [section]: "Prompt 3 (role-playing) and Prompt 4 (professional definition) produced better results... suggesting that more detailed information and tailored instruction enhance performance"
  - [section]: "Providing GPT models with multiple layers of context and instruction may have impaired and confused their classification performance"
  - [corpus]: Weak - corpus shows related work on prompt engineering but not specifically on complexity-performance tradeoffs
- Break condition: If prompt complexity exceeds the model's context window or the user's ability to construct coherent instructions, performance will suffer.

### Mechanism 3
- Claim: Different GPT model versions have unique sensitivities to what they consider extremist, leading to version-specific classification behaviors.
- Mechanism: GPT models trained on different datasets or with different architectural parameters develop distinct sensitivities to language patterns. GPT 4 shows heightened sensitivity to discriminatory racial and ethnic discourse, while GPT 3.5 takes a more neutral stance, resulting in version-specific false positive/negative patterns.
- Core assumption: The training data and architectural differences between GPT versions create systematic differences in how they interpret extremist content.
- Evidence anchors:
  - [section]: "GPT 3.5 performed better at classifying far-left extremist posts, while GPT 4 performed better at classifying far-right extremist posts"
  - [section]: "GPT 4 appeared to be more sensitive to discriminatory racial and ethnic discourse, while GPT 3.5 took a more neutral stance"
  - [corpus]: Weak - corpus shows related work on model comparisons but not specifically on sensitivity differences
- Break condition: If model updates or fine-tuning significantly alter the sensitivity patterns, the observed version-specific behaviors would change.

## Foundational Learning

- Concept: Zero-shot learning
  - Why needed here: Understanding how GPT models can perform classification without task-specific training data is fundamental to the paper's comparison with supervised BERT models
  - Quick check question: What is the key difference between zero-shot and few-shot learning approaches in language models?

- Concept: Prompt engineering
  - Why needed here: The paper's main contribution involves testing different prompt strategies to optimize GPT performance for extremist content classification
  - Quick check question: How does adding role-playing context to a prompt potentially improve model performance on specialized tasks?

- Concept: Transfer learning limitations
  - Why needed here: The paper demonstrates that BERT models struggle with knowledge transfer between far-right and far-left content, which is crucial for understanding when traditional approaches fail
  - Quick check question: Why might training a BERT model on far-right data perform poorly when tested on far-left content?

## Architecture Onboarding

- Component map: Twitter data collection -> Manual labeling -> BERT training and evaluation -> GPT zero-shot classification with prompt engineering -> Performance comparison and sensitivity analysis
- Critical path: Data collection → Manual labeling → BERT baseline establishment → GPT prompt engineering experiments → Performance analysis → Sensitivity comparison across model versions
- Design tradeoffs: BERT offers high accuracy with sufficient labeled data but requires extensive manual labeling effort and shows poor transfer learning. GPT eliminates labeling requirements but introduces version-specific sensitivities and prompt complexity challenges.
- Failure signatures: BERT failures manifest as high variance in performance metrics and poor transfer learning. GPT failures appear as version-specific false positive/negative patterns and degraded performance with overly complex prompts.
- First 3 experiments:
  1. Replicate BERT binary classification with small training samples (12-24 posts) to confirm performance degradation observed in the paper
  2. Test GPT 3.5 with Prompt 1 (naïve) versus Prompt 3 (role-playing) on the same test set to verify prompt complexity effects
  3. Compare GPT 3.5 and GPT 4 performance on far-left content using Prompt 3 to reproduce the version sensitivity findings

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What specific prompt engineering techniques beyond those tested would optimize LLM performance for extremism detection, and at what point does added detail become counterproductive?
- Basis in paper: [explicit] The paper states "Future research should explore various prompt engineering techniques to determine the optimal level of information or detail for maximum performance" and found that overly complex prompts may impair performance.
- Why unresolved: The study only tested four prompts and identified a threshold where added detail becomes counterproductive, but did not systematically explore what lies beyond that threshold.
- What evidence would resolve it: A systematic study testing a broader range of prompt variations, including varying levels of detail, context, and instruction types, to map the relationship between prompt complexity and classification performance.

### Open Question 2
- Question: How do domain-specific LLMs compare to general-purpose LLMs like GPT for extremism detection and classification tasks?
- Basis in paper: [explicit] The paper notes as a limitation that "the GPT models used in our study were not domain specific but trained on general corpus or knowledge" and suggests "Future research would benefit from exploring the differences in classification outcomes using LLM models that are domain specific."
- Why unresolved: The study used only general-purpose GPT models without comparing them to models specifically trained on extremism-related data.
- What evidence would resolve it: Comparative studies testing both general-purpose and domain-specific LLMs on the same extremism datasets to measure differences in accuracy, false positive rates, and classification capabilities.

### Open Question 3
- Question: What are the optimal training data strategies for BERT models in extremism detection, and how can transfer learning be improved between different ideological categories?
- Basis in paper: [explicit] The paper found that BERT performance improves with larger training datasets but struggles with transfer learning between far-right and far-left content, and suggests this is a significant limitation.
- Why unresolved: While the study tested various training data sizes and combined datasets, it did not explore advanced training strategies like adversarial training, multi-task learning, or curriculum learning that might improve transfer capabilities.
- What evidence would resolve it: Experiments testing advanced training methodologies and transfer learning techniques to determine optimal approaches for improving BERT performance across different extremist ideologies.

## Limitations

- Data generalization: Findings are based on Twitter data from a single date, raising questions about applicability to other platforms, time periods, or evolving extremist discourse patterns.
- Manual labeling subjectivity: Specific definitions and inter-rater reliability measures for manual labeling are not detailed, potentially affecting ground truth quality.
- Prompt sensitivity boundaries: The study identifies prompt complexity effects but doesn't fully characterize optimal prompt design boundaries across different extremist content domains.

## Confidence

- High confidence: BERT's superior performance with large labeled datasets versus its limitations with small samples and transfer learning.
- Medium confidence: GPT's zero-shot performance advantages over BERT, dependent on specific extremist content characteristics and prompt engineering quality.
- Medium confidence: Version-specific sensitivities of GPT 3.5 versus GPT 4, based on single-point comparisons that may reflect temporal or contextual factors.

## Next Checks

1. **Temporal validation**: Replicate the classification task using Twitter data collected at different time periods (pre- and post-May 2023) to assess whether GPT's superior zero-shot performance remains consistent as extremist discourse evolves.

2. **Cross-platform testing**: Apply the same BERT and GPT approaches to extremist content from alternative platforms (e.g., Reddit, Telegram) to determine if the observed performance patterns hold across different social media ecosystems.

3. **Prompt optimization study**: Systematically vary prompt complexity parameters (number of instructions, definition depth, context layers) to identify the precise inflection point where additional complexity begins degrading GPT performance, and test whether this threshold differs across extremist content types.