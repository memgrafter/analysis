---
ver: rpa2
title: Maximally Permissive Reward Machines
arxiv_id: '2408.08059'
source_url: https://arxiv.org/abs/2408.08059
tags:
- reward
- planning
- task
- agent
- plan
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces maximally permissive reward machines (MPRMs)
  that leverage sets of partial-order plans to provide greater flexibility in reinforcement
  learning. Unlike previous approaches using single plans, MPRMs allow agents to learn
  optimal policies by encoding all possible ways to achieve a task.
---

# Maximally Permissive Reward Machines

## Quick Facts
- arXiv ID: 2408.08059
- Source URL: https://arxiv.org/abs/2408.08059
- Reference count: 30
- Key outcome: MPRMs allow agents to learn optimal policies by encoding all possible ways to achieve a task, achieving higher rewards than single-plan approaches at the cost of slower convergence.

## Executive Summary
This paper introduces maximally permissive reward machines (MPRMs) that leverage sets of partial-order plans to provide greater flexibility in reinforcement learning. Unlike previous approaches using single plans, MPRMs allow agents to learn optimal policies by encoding all possible ways to achieve a task. The authors prove that policies learned using MPRMs are at least as good as those from single-plan approaches and present experimental results on three tasks in CRAFT WORLD showing MPRM-trained agents achieve higher rewards than single-plan counterparts. While convergence is slower with MPRMs, they enable learning optimal policies when the planning domain is adequate for the goal.

## Method Summary
The approach uses partial-order planning to generate all possible plans for achieving a task, then constructs a reward machine that encodes all prefixes of these plans. The agent learns using Q-learning on the product of the MDP and MPRM. The key innovation is Construction 1, which builds the maximally permissive reward machine from the set of all partial-order plans, allowing the agent to follow any valid plan sequence and still receive appropriate rewards.

## Key Results
- MPRM-trained agents achieve higher rewards than single-plan RM agents in all three CRAFT WORLD tasks
- Convergence is slower with MPRMs, particularly in the gold-or-gem task
- Theoretical proof shows MPRM optimal policies have expected discounted future reward â‰¥ any single-plan RM optimal policy
- MPRMs enable agents to choose optimal plans based on current state and resources

## Why This Works (Mechanism)

### Mechanism 1
- Claim: MPRMs allow agents to learn optimal policies by encoding all possible ways to achieve a task, not just one plan.
- Mechanism: MPRM construction uses the set of all partial-order plans for a goal, where each plan represents a different valid sequence of abstract actions. The reward machine state space is built from all prefixes of these plans, enabling the agent to follow any valid plan and still get rewarded.
- Core assumption: The planning domain is adequate, meaning it captures all relevant fluents and transitions needed to achieve the goal.
- Evidence anchors:
  - [abstract] "We prove that learning using such 'maximally permissive' reward machines results in higher rewards than learning using RMs based on a single plan."
  - [section] "We prove that the expected discounted future reward of optimal policies learned using an MPRM is greater than or equal to that obtained from optimal policies learned using a RM synthesized from any single partial-order plan."
- Break condition: If the planning domain is not adequate, the MPRM may miss critical fluents or transitions, causing the agent to learn suboptimal policies.

### Mechanism 2
- Claim: MPRMs outperform single-plan reward machines because they provide more flexibility in choosing the best plan based on current state.
- Mechanism: The MPRM state space includes all possible prefixes of all plans. During learning, the agent can dynamically choose which plan to follow based on its current position and available resources, without being locked into a single sequence.
- Core assumption: The MDP has enough variation in starting states or resource availability that different plans are optimal in different contexts.
- Evidence anchors:
  - [section] "In the first map for the bridge task, if the agent starts from a location in the upper half of the map it is more convenient to build an iron bridge, while in the lower half of the map it is more convenient to build a rope bridge."
  - [section] "This illustrates that the MPRM agent is able to choose the 'correct' plan for the agent's position."
- Break condition: In environments where all plans are equally good regardless of starting state, the flexibility advantage disappears.

### Mechanism 3
- Claim: MPRMs converge more slowly than single-plan RMs due to the increased state space complexity.
- Mechanism: The MPRM state space is exponentially larger than single-plan RMs because it includes all prefixes of all plans. This means the agent needs more samples to learn Q-values for all state-action pairs.
- Core assumption: Sample complexity increases with state space size, and the learning algorithm is not specialized for large state spaces.
- Evidence anchors:
  - [section] "However, in all experiments, the agent trained with the maximally permissive RM converges more slowly than the other agents (particularly in the gold-or-gem task)."
  - [section] "Intuitively, increasing the flexibility of the RM trades solution quality for sample complexity."
- Break condition: If the planning domain is very small or the learning algorithm is highly efficient, the convergence penalty might be negligible.

## Foundational Learning

- Concept: Markov Decision Processes (MDPs)
  - Why needed here: The underlying environment in which the agent learns is modeled as an MDP, with states, actions, rewards, and transition probabilities.
  - Quick check question: What are the four components of an MDP, and how do they relate to the agent's learning process?

- Concept: Partial-order planning
  - Why needed here: The paper uses partial-order plans instead of sequential plans to provide flexibility in action ordering while still ensuring the goal is achieved.
  - Quick check question: How does a partial-order plan differ from a sequential plan, and why does this difference matter for reward machine construction?

- Concept: Reward machines
  - Why needed here: Reward machines are used to encode non-Markovian rewards and task structure, allowing the agent to learn complex behaviors.
  - Quick check question: What is the key advantage of using reward machines over traditional reward functions in reinforcement learning?

## Architecture Onboarding

- Component map:
  Planning domain -> Algorithm 1 -> MPRM construction -> RL algorithm -> CRAFT WORLD environment

- Critical path:
  1. Define planning domain for the task
  2. Run Algorithm 1 to get all partial-order plans
  3. Construct MPRM using Construction 1
  4. Train agent with Q-learning on the product MDP
  5. Evaluate learned policy

- Design tradeoffs:
  - MPRMs provide better solution quality but require more training samples
  - Using all partial-order plans gives maximum flexibility but may be computationally expensive
  - The approach requires a planning domain, which may not always be available

- Failure signatures:
  - Agent fails to learn any policy: Planning domain may be inadequate or MDP too complex
  - Agent learns suboptimal policy: Planning domain may be missing critical fluents or transitions
  - Training takes too long: Planning domain may have too many actions, creating too many plans

- First 3 experiments:
  1. Implement Algorithm 1 and verify it produces correct partial-order plans for a simple domain
  2. Build MPRM for a simple task and verify it accepts all valid action sequences
  3. Train agent on a simple MDP with MPRM and verify it learns better than single-plan RM

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the sample complexity of learning with MPRMs compare to learning with RMs based on sampled subsets of plans, as opposed to the complete set of all partial-order plans?
- Basis in paper: [explicit] The paper notes that learning with MPRMs converges more slowly than with single-plan RMs, and mentions future work on using top-k planning to sample diverse subsets of plans.
- Why unresolved: The paper does not provide experimental results comparing MPRMs to RMs based on sampled plan subsets, only mentioning this as future work.
- What evidence would resolve it: Experiments comparing learning performance (reward, convergence speed) between MPRMs and RMs based on various sampling strategies (e.g., random sampling, top-k diverse plans) on the same tasks.

### Open Question 2
- Question: Can the MPRM approach be effectively extended to environments with continuous state and action spaces?
- Basis in paper: [explicit] The paper mentions that MPRMs are applicable to continuous environments but notes that learning in such environments is more challenging and leaves this for future work.
- Why unresolved: The paper only discusses theoretical applicability to continuous spaces, without providing empirical results or implementation details for such environments.
- What evidence would resolve it: Successful application of MPRMs to continuous control tasks (e.g., robotic manipulation, locomotion) with quantitative comparisons to baseline RL methods.

### Open Question 3
- Question: How does the performance of MPRMs compare to option-based approaches that use abstract actions derived from plans?
- Basis in paper: [explicit] The paper mentions future work on investigating option-based approaches similar to [12], where abstract actions in plans are implemented as options.
- Why unresolved: The paper does not provide any experimental results or theoretical analysis comparing MPRMs to option-based methods.
- What evidence would resolve it: Experiments comparing learning performance (reward, convergence speed, policy quality) between MPRMs and option-based methods using the same planning domains and tasks.

## Limitations

- The approach requires an adequate planning domain, but the paper doesn't empirically test what happens when the domain is inadequate
- Computational cost of generating all partial-order plans could become prohibitive for complex domains
- Slower convergence represents a significant tradeoff that may limit practical applicability in time-sensitive scenarios

## Confidence

**High confidence**: The theoretical proof that MPRMs provide at least as good expected discounted future reward as single-plan RMs (Section 3). The mathematical formulation is rigorous and the logic is sound given the stated assumptions.

**Medium confidence**: The experimental results showing MPRMs achieve higher rewards in CRAFT WORLD. While the results are consistent across three tasks, the sample size is limited and the environments are relatively simple grid-based tasks.

**Medium confidence**: The claim that slower convergence is an acceptable tradeoff for better solution quality. This is presented as an inherent limitation but the magnitude of the slowdown relative to the performance gain is not quantified in a way that would help practitioners make informed decisions.

## Next Checks

1. **Adequacy sensitivity test**: Systematically remove critical fluents from the planning domain in one task and measure how MPRM performance degrades compared to single-plan RMs. This would validate the assumption that domain adequacy is necessary for MPRM benefits.

2. **Plan enumeration scalability**: Measure the time and memory required to generate all partial-order plans for increasingly complex domains to quantify the computational overhead of Algorithm 1.

3. **Sample complexity quantification**: Plot learning curves for both MPRM and single-plan RM agents on the same task, measuring not just final reward but also the number of samples required to reach 90% of optimal performance. This would provide concrete data on the convergence tradeoff.