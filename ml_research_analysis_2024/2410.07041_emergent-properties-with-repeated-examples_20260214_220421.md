---
ver: rpa2
title: Emergent properties with repeated examples
arxiv_id: '2410.07041'
source_url: https://arxiv.org/abs/2410.07041
tags:
- training
- examples
- data
- budget
- two-set
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper investigates the impact of training data repetition
  on transformer model performance. The authors propose two key insights: (1) for
  a fixed training budget, models trained on smaller datasets with more repetitions
  often outperform models trained on larger, less-repeated datasets, and (2) a "two-set
  training" approach, where a small subset of examples is repeated more frequently
  during training, significantly improves learning speed and performance.'
---

# Emergent properties with repeated examples

## Quick Facts
- arXiv ID: 2410.07041
- Source URL: https://arxiv.org/abs/2410.07041
- Authors: FranÃ§ois Charton; Julia Kempe
- Reference count: 40
- Key outcome: Two-set training with data repetition significantly improves transformer performance on mathematical tasks compared to single-set training with the same data budget

## Executive Summary
This paper investigates how data repetition affects transformer model performance, challenging the conventional wisdom that diverse datasets with minimal repetition are optimal. The authors propose that for a fixed training budget, smaller datasets with more repetitions can outperform larger, less-repeated datasets. They introduce a "two-set training" approach where a small subset of examples is repeated more frequently during training, demonstrating significant improvements in learning speed and final performance across three mathematical tasks: GCD computation, modular multiplication, and matrix eigenvalue calculation.

## Method Summary
The authors conduct controlled experiments comparing single-set training (uniform data sampling) against two-set training (one subset repeated more frequently) on mathematical tasks. They maintain a fixed training budget while varying dataset size and repetition rates. The two-set approach involves creating two distinct data subsets: a small "core" set that appears frequently in training batches and a larger "background" set that appears less often. This is implemented through custom data loaders that oversample the core set. Experiments span three mathematical domains with varying complexity, measuring both learning speed (early stopping performance) and final accuracy on held-out test sets.

## Key Results
- Two-set training with 50,000 examples repeated 3,000 times achieved 69/100 correct predictions on GCD test set versus 27/100 with single-set training on same budget
- For modular multiplication, two-set training enabled learning a task that single-set training could not solve at all
- Two-set training allowed smaller models to learn larger eigenvalue problems that single-set training could not handle

## Why This Works (Mechanism)
The paper demonstrates that data repetition can be beneficial for learning in transformers, particularly when combined with diverse background examples. The two-set training approach appears to work by allowing models to first learn patterns from repeated examples before generalizing to more diverse cases. Random selection of the repeated subset performs best, and mixing repeated and non-repeated examples in mini-batches is crucial. The mechanism likely involves both memorization of repeated examples and pattern recognition that transfers to unseen cases, though the exact learning dynamics remain partially unclear.

## Foundational Learning
- Transformer architecture fundamentals - Why needed: Understanding attention mechanisms and positional encoding is crucial for grasping how models process repeated versus diverse examples
- Quick check: Can you explain how self-attention works and why positional encoding matters for sequence tasks?

- Mathematical reasoning in neural networks - Why needed: The experiments focus on mathematical computation tasks where symbolic reasoning differs from pattern recognition in natural data
- Quick check: What distinguishes learning arithmetic operations from learning language patterns in neural networks?

- Curriculum learning concepts - Why needed: The two-set approach implicitly creates a curriculum where models see easier/repeated examples before diverse ones
- Quick check: How does presenting simpler examples first potentially benefit model training compared to uniform sampling?

## Architecture Onboarding

Component map: Data Loader -> Training Loop -> Transformer Model -> Loss Function -> Optimizer -> Validation Metrics

Critical path: The two-set data loader creates mini-batches with oversampled core examples, which feed into the transformer training loop. The model learns from these mixed batches, with loss computed on both repeated and diverse examples. This mixed training signal appears essential for the two-set effect.

Design tradeoffs: The authors balance between memorization (from repeated examples) and generalization (from diverse examples). Too much repetition risks overfitting, while too little loses the benefits. The random selection of core examples versus curated selection represents another tradeoff between simplicity and potential performance gains from intelligent curation.

Failure signatures: Single-set training fails to learn certain mathematical tasks entirely, particularly modular multiplication. When batch mixing is removed (pure repetition vs pure diversity), the two-set advantage disappears. Models trained with excessive repetition on too-small datasets show poor generalization despite high training accuracy.

First experiments:
1. Replicate the GCD comparison between single-set and two-set training with identical data budgets
2. Test different core set sizes (10k, 50k, 100k) to find optimal repetition ratio
3. Compare random versus curated selection of repeated examples for a specific task

## Open Questions the Paper Calls Out
None

## Limitations
- Experiments focus on narrow mathematical tasks which may not generalize to complex, diverse problem domains
- Fixed training budget constraint may not reflect practical scenarios where data availability rather than compute is limiting
- The mechanism behind why certain problems benefit more from repetition than others remains partially unclear

## Confidence

High confidence in empirical observations: The experimental results are clearly presented with quantitative comparisons between single-set and two-set training approaches. The reproducibility of the mathematical task setups provides strong evidence for the reported performance differences.

Medium confidence in generalization claims: While the mathematical task results are robust, extending these findings to broader machine learning applications requires additional validation. The claim that repetition can be beneficial for learning, while supported by these results, needs testing across diverse problem domains.

Low confidence in theoretical explanation: The paper provides empirical evidence for the benefits of repetition but offers limited theoretical justification for why the two-set training approach works, particularly for tasks that single-set training cannot solve at all.

## Next Checks

1. Test the two-set training approach on natural language processing tasks and computer vision problems to assess generalizability beyond mathematical computations.

2. Conduct ablation studies varying the ratio of repeated to non-repeated examples across different problem complexities to identify optimal repetition rates for various task types.

3. Implement follow-up experiments with curriculum learning approaches where the repeated subset is strategically selected based on difficulty progression rather than random selection, to determine if intelligent curation can outperform random repetition.