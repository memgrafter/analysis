---
ver: rpa2
title: Tackling the Dynamicity in a Production LLM Serving System with SOTA Optimizations
  via Hybrid Prefill/Decode/Verify Scheduling on Efficient Meta-kernels
arxiv_id: '2412.18106'
source_url: https://arxiv.org/abs/2412.18106
tags:
- attention
- prefill
- performance
- length
- tokens
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: XY-Serve is a production-grade LLM serving system that addresses
  dynamic workloads in LLM inference through an abstraction mechanism that decomposes
  computations into hardware-friendly, fine-grained meta primitives. The system introduces
  token-wise scheduling that batches tokens in chunks and processes them through workload
  decomposition, computation task reordering, and meta kernels.
---

# Tackling the Dynamicity in a Production LLM Serving System with SOTA Optimizations via Hybrid Prefill/Decode/Verify Scheduling on Efficient Meta-kernels

## Quick Facts
- arXiv ID: 2412.18106
- Source URL: https://arxiv.org/abs/2412.18106
- Reference count: 40
- Key outcome: Up to 89% end-to-end throughput improvement on Ascend NPUs with 14.6% faster GEMM kernels and 21.5% faster attention kernels

## Executive Summary
XY-Serve is a production-grade LLM serving system that addresses dynamic workloads in LLM inference through an abstraction mechanism that decomposes computations into hardware-friendly, fine-grained meta-primitives. The system introduces token-wise scheduling that batches tokens in chunks and processes them through workload decomposition, computation task reordering, and meta kernels. For attention, XY-Serve proposes a meta-kernel that computes the matmul-softmax-matmul pattern with architectural-aware tile sizes, while for GEMM it introduces a virtual padding scheme that adapts to dynamic shape changes using highly efficient GEMM primitives with fixed tile sizes. Experimental results show up to 89% end-to-end throughput improvement compared to current baselines on Ascend NPUs.

## Method Summary
XY-Serve implements a production-grade LLM serving system that tackles dynamic workloads through token-wise scheduling and workload decomposition into hardware-friendly meta-primitives. The system uses a Token-Table to represent Linear operators as single entries and a Task-Table for efficient task reordering. It introduces SmoothGEMM with virtual padding instead of physical padding for arbitrary matrix shapes, and Meta-Attention that supports arbitrary shapes and mask structures through a cube-vector orchestration pipeline. The system supports various optimization techniques including APC, SplitFuse, and speculative decoding while maintaining high hardware utilization through efficient scheduling and task management.

## Key Results
- Up to 89% end-to-end throughput improvement on Ascend NPUs compared to baselines
- 14.6% faster GEMM kernels through virtual padding scheme with selective read/write operations
- 21.5% faster attention kernels with mask-aware computation and cube-vector orchestration

## Why This Works (Mechanism)

### Mechanism 1
- Claim: XY-Serve achieves up to 89% end-to-end throughput improvement by decomposing dynamic workloads into hardware-friendly meta-primitives.
- Mechanism: The system uses token-wise scheduling to batch tokens in chunks, then decomposes these into fine-grained computational tasks (workload decomposition), reorders them for balanced core utilization (task reordering), and processes them using optimized meta-kernels for attention and GEMM operations.
- Core assumption: AI accelerators with tile-based programming models can achieve higher efficiency when dynamic shapes are transformed into fixed tile sizes with virtual padding rather than physical padding.
- Evidence anchors:
  - [abstract]: "The core idea is an abstraction mechanism that smooths out the workload variability by decomposing computations into unified, hardware-friendly, fine-grained meta primitives."
  - [section 3.2.2]: "In the Token-Table, each Linear operator corresponds to a single entry... The tokenNum equals the total number of tokens in the currently scheduled chunk. Tiling is performed on the result matrix of dimensions tokenNum × nLen, where each tiling block corresponds to a submatrix of the result."
  - [corpus]: No direct evidence found for this specific mechanism in corpus papers. The related work focuses on attention optimization and serving systems but doesn't detail this specific decomposition approach.
- Break condition: If the virtual padding mechanism introduces significant memory overhead or if the tile size optimization doesn't generalize across different model architectures and sequence lengths.

### Mechanism 2
- Claim: Meta-attention achieves 21.5% faster attention kernels by supporting arbitrary shapes and mask structures while maintaining hardware efficiency.
- Mechanism: The system uses a radix tree for efficient K/V cache management, enables token-wise prefix matching for APC, and processes speculative decoding masks row-by-row to support arbitrary spec lengths. It employs a cube-vector orchestration pipeline that overlaps QK, softmax, and SV computations.
- Core assumption: The mask-aware computation strategy that skips invalid regions in attention scores can maintain correctness while significantly reducing computation.
- Evidence anchors:
  - [section 4.2.2]: "To fully utilize the sparsity in the attention mask and skip redundant computations, we adopt a mask-aware strategy that significantly enhances efficiency. As displayed in Fig. 11, in the attention computation, once the query dimension coordinate index qLen and the kvLen of K/V are known, any data corresponding to positions after qLen + kvLen in each row of the attention score matrix is invalid."
  - [section 4.1.1]: "The core requirement for supporting both prefix reuse and Chunked Prefill is that the attention module must be capable of handling arbitrary K/V cache lengths and performing token-wise K/V cache reuse. To achieve this, we use a radix tree to efficiently manage the K/V cache."
  - [corpus]: No direct evidence found for this specific mask-aware computation approach in corpus papers. Related work mentions attention optimization techniques but doesn't detail this specific implementation.
- Break condition: If the radix tree management introduces significant overhead for cache lookups or if the mask-aware computation strategy becomes too complex to maintain correctness across all speculative decoding algorithms.

### Mechanism 3
- Claim: SmoothGEMM achieves 14.6% faster GEMM kernels by supporting arbitrary matrix shapes through virtual padding rather than physical padding.
- Mechanism: The system replaces physical padding in global memory with virtual padding on-chip, combined with selective read/write mechanisms. It focuses on fixed shapes (multiples of tiling size) and uses offline profiling to optimize task allocation strategies like swizzling for each supported shape.
- Core assumption: Fixed-shape optimizations can be applied to dynamic workloads by transforming arbitrary shapes into the nearest supported fixed shape, with virtual padding eliminating the overhead of physical padding.
- Evidence anchors:
  - [section 5.1]: "As illustrated in Fig. 12(a), the dimension M in GEMM is intrinsically related to the number of tokens... To mitigate these issues, we replace the physical padding in global memory with virtual padding on the chip, combined with the selective read and write mechanisms shown in Fig. 12(a)."
  - [section 5.3]: "Given our focus on fixed shapes, such as multiples of the tiling size, and operating under budget constraints, we narrow the range of token sizes we handle... This enables us to perform offline customization and optimization for these specific shapes."
  - [corpus]: No direct evidence found for this specific virtual padding approach in corpus papers. Related work mentions GEMM optimization but doesn't detail this specific implementation.
- Break condition: If the offline profiling becomes too time-consuming for new model architectures or if the fixed-shape approach cannot adequately handle extreme cases of dynamic shapes.

## Foundational Learning

- Concept: Tile-based programming models in AI accelerators
  - Why needed here: Understanding how AI accelerators like Ascend NPUs process data in fixed-size tiles is crucial for grasping why virtual padding is more efficient than physical padding for dynamic shapes.
  - Quick check question: What is the typical tile size used in cube-based AI accelerators, and how does this constraint affect matrix multiplication operations?

- Concept: Speculative decoding algorithms (sequence-based vs tree-based)
  - Why needed here: The system must support various speculative decoding algorithms that generate different mask structures, requiring understanding of how these algorithms work and their performance characteristics.
  - Quick check question: How do sequence-based and tree-based speculative decoding algorithms differ in their mask generation and acceptance rate characteristics?

- Concept: Automatic Prefix Caching (APC) and PagedAttention
  - Why needed here: These optimization techniques introduce dynamicities in K/V cache management that the system must handle efficiently, requiring understanding of their mechanisms and performance implications.
  - Quick check question: How does PagedAttention fragment K/V cache access patterns compared to traditional continuous access, and what are the performance implications?

## Architecture Onboarding

- Component map: User request → APC module (prefix matching) → Token-wise Scheduling (chunk formation) → Workload Decomposition (token table creation) → Task Reordering (task table generation) → Meta-Attention/SmoothGEMM execution → Output generation. The most performance-critical path is the Meta-Attention execution for attention-heavy workloads.

- Critical path: User request → APC module (prefix matching) → Token-wise Scheduling (chunk formation) → Workload Decomposition (token table creation) → Task Reordering (task table generation) → Meta-Attention/SmoothGEMM execution → Output generation. The most performance-critical path is the Meta-Attention execution for attention-heavy workloads.

- Design tradeoffs: The system trades implementation complexity for performance gains by introducing multiple abstraction layers (token table, task table) and supporting various optimization techniques (APC, SD, SplitFuse) simultaneously. The choice between physical padding (simpler) and virtual padding (more complex but more efficient) represents a key tradeoff.

- Failure signatures: Performance degradation occurs when chunk sizes become too small (underutilizing hardware), when task reordering fails to balance load across cores, or when virtual padding introduces unexpected memory overhead. System stability issues may arise from incorrect radix tree management in K/V cache operations.

- First 3 experiments:
  1. Measure MFU and MBU for Meta-Attention with varying prefix lengths (0-4k tokens) to validate the performance improvement claims under different APC scenarios.
  2. Benchmark SmoothGEMM with different token count combinations across P/D/V stages to verify the virtual padding mechanism's effectiveness compared to physical padding.
  3. Test end-to-end performance with different QPS rates (4, 8, 16, 32) on the nightly benchmarks to validate the 89% throughput improvement claim under realistic workloads.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the virtual padding scheme scale to different AI accelerator architectures beyond tile-based models like Ascend?
- Basis in paper: [explicit] "While currently implemented on Ascend NPUs [34,35], these techniques can be applied to other AI platforms as well" and "we believe the approach can be readily applicable to SIMT architectures as well"
- Why unresolved: The paper mentions applicability to other architectures but does not provide empirical validation or theoretical framework for how virtual padding would adapt to SIMT or other non-tile-based architectures
- What evidence would resolve it: Performance comparison of virtual padding on multiple architectures (SIMT, systolic arrays, etc.) showing comparable efficiency gains and demonstrating the mechanism's adaptability

### Open Question 2
- Question: What is the optimal budget length for token-wise scheduling in production workloads with varying P/D/V ratios?
- Basis in paper: [explicit] "The scheduling system selects a fixed-budget length of tokens from the scheduling queues to form chunks" but the paper does not empirically determine the optimal budget length
- Why unresolved: The paper mentions budget length as a parameter but does not provide guidelines or empirical results showing how different budget lengths affect performance across different workload patterns
- What evidence would resolve it: Systematic evaluation showing achieved QPS, TTFT, and TBT metrics across different budget lengths (e.g., 32, 64, 128, 256 tokens) for various workload mixes

### Open Question 3
- Question: How does XY-Serve's performance compare to other state-of-the-art serving systems (e.g., Orca, SGLang) in disaggregated deployment scenarios?
- Basis in paper: [inferred] The paper discusses disaggregated deployment in Section 2.3 and mentions "disaggregated LLMs" but only compares against Ascend-vLLM and GPU implementations
- Why unresolved: While the paper shows strong performance in integrated deployments, it does not evaluate the system's effectiveness in disaggregated setups where prefill and decode are separated
- What evidence would resolve it: End-to-end comparison of disaggregated deployment performance (achieved QPS, latency metrics) against other serving systems that support disaggregated LLM inference

## Limitations

- Virtual padding mechanism requires careful tuning of tile sizes and may not generalize well to extreme dynamic shape scenarios
- Complexity of supporting multiple speculative decoding algorithms and optimization techniques simultaneously introduces significant engineering overhead
- Performance improvements measured on Ascend NPUs may not translate directly to other hardware platforms (GPUs, other NPUs)

## Confidence

**High Confidence:** The fundamental approach of decomposing dynamic workloads into hardware-friendly meta-primitives is well-grounded in established optimization techniques. The claim of 89% end-to-end throughput improvement is supported by systematic experimental validation across multiple QPS rates.

**Medium Confidence:** The specific implementation details of virtual padding and mask-aware computation strategies show strong theoretical foundations but lack extensive empirical validation across diverse workloads. The performance improvements for individual components (14.6% for GEMM, 21.5% for attention) are based on controlled experiments that may not fully capture real-world serving variability.

**Low Confidence:** The scalability claims for handling increasingly complex optimization scenarios (multiple speculative decoding algorithms, various prefix lengths) are based on limited experimental data. The long-term maintenance implications of the complex abstraction layers are not addressed.

## Next Checks

1. **MFU/MBU Validation Under Variable Prefix Lengths:** Conduct controlled experiments measuring MAC Utilization (MFU) and Memory Bandwidth Utilization (MBU) for Meta-Attention kernels across prefix lengths from 0 to 4096 tokens. This will validate whether the 21.5% performance improvement holds consistently across different APC scenarios and identify potential bottlenecks in the cube-vector orchestration pipeline.

2. **Cross-Hardware Performance Portability:** Implement a baseline version of the virtual padding mechanism on GPU hardware and compare performance against the Ascend NPU results. This will determine whether the optimization strategies are hardware-agnostic or require significant reimplementation for different accelerator architectures.

3. **Long-term Stability Under Mixed Workloads:** Design a stress test that runs the system continuously for 48+ hours with mixed workload patterns (varying token counts, prefix lengths, and optimization techniques) while monitoring memory usage, cache hit rates, and task scheduling overhead. This will reveal potential degradation modes and resource leakage issues not visible in short-term benchmarks.