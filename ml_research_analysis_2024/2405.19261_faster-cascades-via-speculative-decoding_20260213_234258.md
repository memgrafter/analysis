---
ver: rpa2
title: Faster Cascades via Speculative Decoding
arxiv_id: '2405.19261'
source_url: https://arxiv.org/abs/2405.19261
tags:
- speculative
- speccascade
- specdecode
- large
- deferral
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper presents a new method that combines two approaches
  to efficient language model inference: cascades and speculative decoding. Cascades
  defer to a larger model only for "hard" inputs, while speculative decoding drafts
  tokens from a small model and verifies them in parallel with a large model.'
---

# Faster Cascades via Speculative Decoding

## Quick Facts
- arXiv ID: 2405.19261
- Source URL: https://arxiv.org/abs/2405.19261
- Authors: Harikrishna Narasimhan; Wittawat Jitkrittum; Ankit Singh Rawat; Seungyeon Kim; Neha Gupta; Aditya Krishna Menon; Sanjiv Kumar
- Reference count: 40
- One-line primary result: Speculative cascades combine cascades and speculative decoding to achieve better cost-quality trade-offs than either approach alone

## Executive Summary
This paper presents a new method that combines two approaches to efficient language model inference: cascades and speculative decoding. Cascades defer to a larger model only for "hard" inputs, while speculative decoding drafts tokens from a small model and verifies them in parallel with a large model. The authors propose a way to implement cascade deferral rules using speculative execution, creating "speculative cascades." They derive the theoretically optimal deferral rule for this setup and propose a practical plug-in estimator. Experiments with Gemma and T5 models on various language tasks show that speculative cascades provide better cost-quality trade-offs than both standard cascades and speculative decoding baselines, with token-specific variants performing particularly well.

## Method Summary
The paper introduces speculative cascades, a method that implements cascade deferral rules through speculative execution. The approach uses a smaller model to draft tokens auto-regressively while a larger model verifies them in parallel to decide whether to defer. The authors derive the theoretically optimal deferral rule, which depends on both expected loss difference and TV distance between models, and propose a practical plug-in estimator. They also introduce token-specific deferral rules that consider each token individually rather than comparing distribution modes globally. The method is evaluated on various language tasks using Gemma and T5 models, showing improved cost-quality trade-offs compared to baselines.

## Key Results
- Speculative cascades achieve better cost-quality trade-offs than both standard cascades and speculative decoding baselines
- Token-specific deferral rules outperform global deferral rules by comparing individual token quality against verifier distribution
- The theoretically optimal deferral rule includes a cost term proportional to TV distance between models
- Experiments demonstrate consistent improvements across WMT translation, summarization, and question-answering tasks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Speculative cascades combine the quality trade-offs of cascades with the faster execution of speculative decoding.
- Mechanism: The paper implements cascade deferral rules through speculative execution, where a smaller model drafts tokens auto-regressively and a larger model verifies them in parallel to decide whether to defer.
- Core assumption: The target distribution that interleaves the drafter's and verifier's distributions can be approximated through speculative sampling while maintaining quality.
- Evidence anchors:
  - [abstract] "We leverage the best of both these approaches by designing new speculative cascading techniques that implement their deferral rule through speculative execution."
  - [section 4.2] "Our proposal is to then invoke the speculative sampling procedure in Algorithm 4 with Tδ as the target distribution function."
- Break condition: If the parallel verification step becomes a bottleneck, the latency advantage disappears.

### Mechanism 2
- Claim: The theoretically optimal deferral rule for speculative cascades depends on both expected loss difference and TV distance between models.
- Mechanism: The optimal rule defers when the expected loss from the smaller model exceeds that of the larger model plus a cost term proportional to their TV distance.
- Core assumption: TV distance between the two models' distributions is a good proxy for the cost of deferral in terms of rejection rate.
- Evidence anchors:
  - [section 4.3] "The minimizer of equation 8 is of the form: r*(x<t) = 1 ⇐ ⇒ Ev∼P(·|x<t) [ℓ(v, qt)] > Ev∼P(·|x<t) [ℓ(v, pt)] + α · DTV(pt, qt)."
  - [section 4.3] "When pt and qt are similar, the rejection rate for qt is low, and hence the deferral decision will depend largely on which of the two models yields a lower expected loss."
- Break condition: If the plug-in estimator poorly approximates the optimal rule due to model miscalibration, performance degrades.

### Mechanism 3
- Claim: Token-specific deferral rules improve upon global deferral by considering each token individually.
- Mechanism: Instead of comparing max probabilities, the token-specific rule compares the quality of each candidate token against the verifier's distribution, accepting tokens that are sufficiently likely under the verifier.
- Core assumption: The quality of a sampled token can be better assessed by comparing it directly to the verifier's probability rather than comparing distribution modes.
- Evidence anchors:
  - [section 4.4] "To alleviate this problem, we propose the use of token-specific deferral rules r : Vt−1 × V → {0, 1} that use both the prefix x<t and a candidate token v to provide a binary decision."
  - [section 4.4] "This target distribution closely mimics qt(·) on tokens that the deferral rule deems to be of acceptable quality, and defers to pt(·) otherwise."
- Break condition: If the token-specific rule becomes too conservative, it may defer too often, losing the efficiency advantage.

## Foundational Learning

- Concept: Cascading models
  - Why needed here: The paper builds on the cascade framework where a smaller model handles "easy" inputs and defers to a larger model for "hard" ones.
  - Quick check question: What determines whether a cascade defers to the larger model in standard implementations?

- Concept: Speculative decoding
  - Why needed here: The paper uses speculative decoding's parallel verification mechanism to implement cascade deferral rules more efficiently.
  - Quick check question: How does speculative decoding accept or reject draft tokens during verification?

- Concept: TV (Total Variation) distance
  - Why needed here: The optimal deferral rule for speculative cascades includes a cost term proportional to the TV distance between the two models.
  - Quick check question: What does the TV distance between two probability distributions measure?

## Architecture Onboarding

- Component map:
  - Drafter model (smaller) -> Verifier model (larger) -> Deferral rule module -> Speculative sampling engine

- Critical path:
  1. Generate draft tokens from smaller model
  2. Run verifier in parallel on all draft prefixes
  3. Apply deferral rule to accept/reject each token
  4. Roll back to first rejected token and continue

- Design tradeoffs:
  - Block size γ vs latency: Larger blocks reduce overhead but increase rollback cost
  - Deferral strictness α vs quality: Stricter rules defer more often, improving quality but reducing speed
  - Token-specific vs global rules: Token-specific rules are more precise but computationally heavier

- Failure signatures:
  - High rejection rate indicates poor draft model quality or overly strict deferral
  - Low quality despite high deferral rate suggests suboptimal deferral rule
  - Latency approaching sequential cascade indicates verification bottleneck

- First 3 experiments:
  1. Implement basic speculative cascade with Chow's rule and verify it matches standard cascade quality
  2. Compare token-specific vs global deferral rules on a small dataset
  3. Sweep α parameter to find optimal quality-latency tradeoff point

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the theoretical relationship between the optimal deferral rule for speculative cascades (Lemma 4) and the optimal deferral rule for sequential cascades (Lemma 1)? Specifically, can we derive conditions under which these two optimal rules coincide?
- Basis in paper: [explicit] The paper derives both optimal deferral rules and shows they differ due to the different objectives (sequential vs. speculative).
- Why unresolved: While the paper characterizes both optimal rules, it doesn't establish when they might be equivalent or how they relate to each other theoretically.
- What evidence would resolve it: A mathematical proof showing conditions under which the two optimal rules produce the same deferral decisions, or empirical evidence demonstrating when they differ significantly.

### Open Question 2
- Question: How would the proposed speculative cascade framework extend to more than two models (e.g., a small, medium, and large model)? What would be the optimal deferral strategy in this case?
- Basis in paper: [inferred] The paper focuses exclusively on two-model cascades, but mentions in the conclusions that extending to more than two models would be an interesting direction for future work.
- Why unresolved: The paper only considers two-model cascades, leaving the generalization to multiple models unexplored.
- What evidence would resolve it: A formal extension of the speculative cascade framework to handle multiple models, along with experimental results comparing multi-model speculative cascades to traditional cascades and speculative decoding.

### Open Question 3
- Question: What is the impact of model calibration on the effectiveness of the plug-in estimators used in the speculative cascade deferral rules?
- Basis in paper: [explicit] The paper mentions that the efficacy of plug-in estimators depends on how closely the individual models approximate the ground-truth distribution and relies on models being calibrated.
- Why unresolved: While the paper acknowledges calibration as a factor, it doesn't empirically investigate how varying degrees of model calibration affect the performance of speculative cascades.
- What evidence would resolve it: Controlled experiments varying model calibration (e.g., through temperature scaling) and measuring the resulting performance of speculative cascades with different deferral rules.

## Limitations

- The theoretical analysis assumes perfect knowledge of expected loss under the true distribution, but practical implementations must estimate this from model predictions
- The paper does not address potential distributional shift between training and inference conditions that could affect deferral rule performance
- Memory overhead from maintaining parallel execution paths is not discussed, though this could be significant for long sequences

## Confidence

- High Confidence: The core mechanism of implementing cascade deferral through speculative execution is well-established and the theoretical characterization of the optimal deferral rule is sound
- Medium Confidence: The practical effectiveness of plug-in estimators and token-specific variants, as these depend heavily on model calibration and the quality of probability estimates
- Medium Confidence: The claimed latency improvements, as these depend on specific hardware configurations and implementation details not fully disclosed

## Next Checks

1. **Calibration Analysis**: Systematically evaluate the calibration of drafter and verifier models across all experimental setups to quantify the impact on plug-in estimator accuracy and deferral rule performance.

2. **Memory and Resource Profiling**: Measure the actual memory overhead and computational resources required for speculative cascades compared to standard cascades, particularly for the token-specific variants with larger block sizes.

3. **Cross-Domain Validation**: Test speculative cascades on non-language tasks (e.g., vision or multimodal tasks) where the assumptions about expected loss computation may not hold, to assess the method's broader applicability.