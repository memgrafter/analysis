---
ver: rpa2
title: Crowdsourced Multilingual Speech Intelligibility Testing
arxiv_id: '2403.14817'
source_url: https://arxiv.org/abs/2403.14817
tags:
- speech
- test
- intelligibility
- crowdsourced
- were
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The study addresses the need for rapid, scalable, and multilingual
  speech intelligibility testing in the context of generative audio algorithms. It
  proposes a crowdsourced adaptation of the Diagnostic Rhyme Test (DRT), leveraging
  large-scale, low-cost participant pools while maintaining reliability through pre-screening,
  validation mechanisms, and per-file scoring.
---

# Crowdsourced Multilingual Speech Intelligibility Testing

## Quick Facts
- arXiv ID: 2403.14817
- Source URL: https://arxiv.org/abs/2403.14817
- Reference count: 0
- One-line primary result: Crowdsourced multilingual DRT testing yields reliable intelligibility scores across five languages with strong correlation to laboratory results.

## Executive Summary
This study presents a scalable, multilingual approach to speech intelligibility testing using crowdsourced Diagnostic Rhyme Tests (DRT). The authors address the limitations of traditional laboratory-based methods by implementing a crowdsourced version of DRT with rigorous pre-screening and validation mechanisms. The approach enables rapid, low-cost evaluation of generative audio algorithms across multiple languages while maintaining reliability through careful participant filtering and per-file scoring strategies.

## Method Summary
The method involves adapting the Diagnostic Rhyme Test for multilingual crowdsourcing, using pre-screening criteria (language proficiency, hearing ability, headphone use, digit-in-noise test) and validation mechanisms to ensure data quality. DRT word lists were developed in five languages (English, Spanish, French, German, Mandarin Chinese) and recorded as audio stimuli. Participants completed tests via Qualtrics, with results aggregated per audio file (~20 responses each) rather than per listener. Intelligibility scores were calculated using the standard DRT formula P(c) = (R - W)/(R + W) × 100, where R is correct responses and W is incorrect responses.

## Key Results
- Crowdsourced DRT testing achieved strong correlation with laboratory results (Pearson's r = 0.93 in Experiment 2)
- Multilingual word lists and audio recordings were successfully developed and validated across five languages
- Per-file scoring approach yielded consistent and meaningful relative intelligibility scores across codec conditions
- Crowdsourced absolute scores were lower than laboratory scores but showed more pronounced differences between codecs

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Crowdsourced testing can yield reliable intelligibility scores if pre-screening and post-validation are rigorously applied.
- Mechanism: By filtering participants based on language proficiency, hearing status, headphone use, and a digit-in-noise test, then discarding trials with <80% validation accuracy, the study ensures that the remaining responses are from capable listeners under reasonable conditions.
- Core assumption: Large sample sizes can compensate for uncontrolled environmental variation.
- Evidence anchors:
  - [abstract] "pre-screening, validation mechanisms, and per-file scoring"
  - [section] "Only submissions that fulfilled the following criteria... had solved more than 80% of the validation questions correctly"
  - [corpus] Weak; no corpus neighbor directly discusses pre-screening in crowdsourced speech testing.

### Mechanism 2
- Claim: Using a closed-set diagnostic rhyme test (DRT) instead of transcription tasks reduces ambiguity and memory effects in crowdsourced conditions.
- Mechanism: Participants choose between two alternatives for each word pair, avoiding spelling errors and the need to hold long sentences in memory, which is harder to control in uncontrolled environments.
- Core assumption: DRT items are acoustically interpretable and sensitive to phoneme-level degradations that matter for speech codec evaluation.
- Evidence anchors:
  - [section] "rhyme tests do not include semantic clues... may be more sensitive to short-term (phoneme-level) degradations"
  - [section] "Due to the larger number of judgements per file... we calculated the scores for each audio sample rather than across participant trials"
  - [corpus] No direct match; corpus lacks neighbors discussing DRT vs. transcription in crowdsourcing.

### Mechanism 3
- Claim: Calculating intelligibility per audio file (rather than per listener trial) leverages large response counts to improve score stability and granularity.
- Mechanism: Averaging across ~20 responses per file smooths out individual listener variability and enables per-word or per-phoneme analysis.
- Core assumption: Responses are conditionally independent given the audio condition.
- Evidence anchors:
  - [section] "we calculated the scores for each audio sample rather than across participant trials"
  - [section] "We report the mean and 95% confidence interval (CI95)"
  - [corpus] Weak; corpus does not contain explicit discussion of per-file scoring strategies.

## Foundational Learning

- Concept: Diagnostic Rhyme Test (DRT) structure and purpose
  - Why needed here: The entire study relies on DRT as the intelligibility metric; understanding its closed-set design is essential to interpreting results.
  - Quick check question: What is the main advantage of DRT over transcription tasks in crowdsourcing?

- Concept: Crowdsourcing quality control methods (pre-screening, validation questions)
  - Why needed here: These mechanisms are the study's core reliability strategy; without them, results would be noisy.
  - Quick check question: Which two criteria were used to filter out low-quality participant submissions?

- Concept: Statistical comparison of crowdsourced vs. laboratory results
  - Why needed here: The study's validity claim rests on correlation with controlled experiments; knowing how to interpret t-tests and Pearson correlations is necessary.
  - Quick check question: What correlation coefficient indicated strong consistency in Experiment 2?

## Architecture Onboarding

- Component map: Pre-screening -> Digit-in-noise test -> Practice run -> Main test blocks -> Validation check -> Score aggregation -> Statistical analysis
- Critical path: Pre-screening → Digit-in-noise test → Practice run → Main test blocks → Validation check → Score aggregation → Statistical analysis
- Design tradeoffs:
  - Larger sample sizes vs. cost and time
  - Closed-set format vs. ecological validity
  - Per-file averaging vs. loss of listener-level detail
- Failure signatures:
  - Low validation accuracy rates → suspect attention or hearing issues
  - Wide CI95 → insufficient sample size or high variability
  - Inconsistent relative scores across conditions → potential design flaw
- First 3 experiments:
  1. Replicate Experiment 1: Compare expert vs. crowdsourced Spanish NB PCMU results.
  2. Run Experiment 2 within-crowd: Test-retest consistency with same participants.
  3. Conduct Experiment 3 codec comparison: English AMR-WB vs. AMR-NB using same test set.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do different crowdsourcing platforms (e.g., Prolific vs. Amazon Mechanical Turk) compare in terms of data quality and participant reliability for speech intelligibility testing?
- Basis in paper: [explicit] The authors note that initial test results indicate considerable differences in response quality across different crowdsourcing platforms, such as Prolific and Amazon Mechanical Turk, which they intend to investigate further.
- Why unresolved: The paper only mentions the intention to investigate further but does not provide detailed comparative results or analysis.
- What evidence would resolve it: Comparative studies analyzing response quality, participant reliability, and consistency metrics across multiple crowdsourcing platforms for speech intelligibility tests.

### Open Question 2
- Question: What is the impact of adding noise to speech intelligibility tests in a crowdsourced environment, and how does it affect the psychometric curve?
- Basis in paper: [explicit] The authors mention that they did not report the effect of adding noise to shift the assessment into the steep part of the psychometric curve, but initial experiments show the expected effect of lowering the intelligibility threshold and increasing the test's sensitivity to differences between treatments.
- Why unresolved: The paper indicates that this aspect was not reported in the current work, and the authors intend to demonstrate this in follow-up work.
- What evidence would resolve it: Detailed experimental results showing the impact of noise addition on intelligibility scores, psychometric curve shifts, and sensitivity to treatment differences in crowdsourced settings.

### Open Question 3
- Question: How do absolute intelligibility scores obtained from crowdsourced tests compare to those from laboratory tests, and what factors contribute to any observed differences?
- Basis in paper: [explicit] The authors observe that crowdsourced absolute mean scores are lower than laboratory scores, and the difference between codecs is more pronounced in crowdsourced tests.
- Why unresolved: While the paper notes the differences, it does not fully explore the underlying factors contributing to these discrepancies or provide a comprehensive analysis of the causes.
- What evidence would resolve it: In-depth analysis identifying specific factors (e.g., headphone quality, environmental noise, participant attention) that contribute to differences in absolute scores between crowdsourced and laboratory tests.

## Limitations
- The study does not report cross-validation with independent datasets to verify generalizability of the per-file scoring method
- Adaptation of DRT to languages other than English may introduce cultural or phonological biases not accounted for in validation
- Reliance on large sample sizes assumes participant pool remains representative across different deployment contexts

## Confidence
- High confidence: The correlation between crowdsourced and laboratory results (Pearson's r = 0.93 in Experiment 2) and the consistency of relative intelligibility scores across codec conditions
- Medium confidence: The effectiveness of the pre-screening and validation mechanisms, as the study reports their use but does not provide detailed analysis of their impact on data quality
- Low confidence: The assumption that per-file scoring is universally superior to per-listener aggregation, as this is not directly tested against alternative scoring strategies

## Next Checks
1. Replicate Experiment 2 with a different participant pool to test the robustness of crowdsourced test-retest consistency
2. Conduct a cross-language validation study to ensure the DRT word lists are equally sensitive to phoneme-level degradations across all five languages
3. Compare the per-file scoring method against per-listener aggregation in a controlled experiment to quantify trade-offs in stability and granularity