---
ver: rpa2
title: 'DIALIGHT: Lightweight Multilingual Development and Evaluation of Task-Oriented
  Dialogue Systems with Large Language Models'
arxiv_id: '2401.02208'
source_url: https://arxiv.org/abs/2401.02208
tags:
- systems
- dialogue
- evaluation
- system
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: DIALIGHT is a toolkit for developing and evaluating multilingual
  task-oriented dialogue (ToD) systems. It enables systematic comparisons between
  fine-tuned PLM-based systems and zero-shot/ ICL-based LLM systems.
---

# DIALIGHT: Lightweight Multilingual Development and Evaluation of Task-Oriented Dialogue Systems with Large Language Models

## Quick Facts
- arXiv ID: 2401.02208
- Source URL: https://arxiv.org/abs/2401.02208
- Reference count: 16
- Primary result: DIALIGHT enables systematic comparison between fine-tuned PLM and ICL-based LLM systems, showing PLMs achieve higher accuracy/coherence while LLMs excel in response diversity and likeability but struggle with task-specific instruction adherence and multilingual output generation.

## Executive Summary
DIALIGHT is a toolkit for developing and evaluating multilingual task-oriented dialogue (ToD) systems that enables systematic comparisons between fine-tuned PLM-based systems and zero-shot/instruction-conditional learning (ICL)-based LLM systems. The toolkit features a secure web interface for human evaluation at both utterance and dialogue levels, and a microservice-based backend for scalability. Evaluations on the Multi3WOZ dataset show that while PLM fine-tuning leads to higher accuracy and coherence, LLM-based systems excel in producing diverse and likeable responses. However, LLMs face significant challenges in task-specific instruction adherence and multilingual output generation, highlighting areas for future research.

## Method Summary
The paper presents DIALIGHT, a toolkit that compares fine-tuned PLM-based systems (mT5small and mT5large) with ICL-based LLM systems (GPT-3.5, LLaMA2, and OpenChat-3.5) for multilingual task-oriented dialogue. The toolkit uses the Multi3WOZ dataset covering Arabic, English, French, and Turkish. Fine-tuned systems are trained on task-specific data for dialogue state tracking and response generation, while LLM systems use in-context learning with English prompts. The toolkit features automatic evaluation metrics (JGA, BLEU, ROUGE, METEOR, Inform Rate, Success Rate) and a microservice-based backend for scalable human evaluation.

## Key Results
- PLM fine-tuning achieves higher accuracy and coherence compared to ICL-based LLM systems
- LLM-based systems excel in producing diverse and likeable responses
- LLMs struggle significantly with task-specific instruction adherence and generating outputs in languages other than English

## Why This Works (Mechanism)

### Mechanism 1
- Claim: FT-based systems outperform ICL-based systems in multilingual task-oriented dialogue because fine-tuning on task-specific data provides better instruction adherence and linguistic alignment.
- Mechanism: Fine-tuning adjusts the model parameters specifically for the task and language, whereas ICL relies on pre-existing generalization patterns that may not align with task-specific constraints or linguistic norms in non-English languages.
- Core assumption: The fine-tuning process with task-specific data and language-specific training improves both the task performance and multilingual generation capabilities.
- Evidence anchors:
  - [abstract] "PLM fine-tuning leads to higher accuracy and coherence"
  - [section 4] "substituting the predicted dialogue states resulted in a marginal increase" vs. large improvement when substituting utterances
  - [corpus] Weak correlation (avg_neighbor_fmr=0.5227) with only 25 related papers suggests this is not a well-explored area
- Break condition: If fine-tuning data is too limited or biased, the performance gap may narrow or reverse, especially if the LLM has strong multilingual pretraining.

### Mechanism 2
- Claim: LLM-based systems excel in producing diverse and likeable responses because they leverage broad pretraining data and instruction-following capabilities.
- Mechanism: Large language models trained on diverse internet-scale data can generate more varied and engaging responses, and their instruction-following capabilities can produce outputs that align better with human preferences in terms of informativeness and helpfulness.
- Core assumption: The diversity and likeability come from the model's pretraining on varied conversational data rather than task-specific optimization.
- Evidence anchors:
  - [abstract] "LLM-based systems excel in producing diverse and likeable responses"
  - [section 5.3] "The ICL-based system generates more diverse responses and exhibits a more favourable personality"
  - [corpus] Weak evidence (no citations for related papers) suggests this finding is preliminary
- Break condition: If the task requires strict adherence to structured formats or specific linguistic norms, the diversity advantage may become a liability.

### Mechanism 3
- Claim: The microservice architecture enables efficient and scalable evaluation of dialogue systems by allowing independent scaling of different components.
- Mechanism: By separating each task model (DST, RG, etc.) into independent services, the system can scale components independently based on demand, and stateless design allows multiple instances to handle requests in parallel.
- Core assumption: The overhead of inter-service communication is outweighed by the benefits of independent scaling and the ability to handle multiple instances of the same model.
- Evidence anchors:
  - [section 5.2] "Our human evaluation tool is architected using a microservice design" and "each service operates independently"
  - [section 5.2] "enables a single model to be concurrently shared by multiple systems" and "load balancing by distributing a set of queries across multiple instances"
  - [corpus] No direct evidence; this appears to be an architectural design choice
- Break condition: If the overhead of managing multiple services and network communication becomes significant, or if the task models are too tightly coupled, the benefits may diminish.

## Foundational Learning

- Concept: Task-oriented dialogue system pipeline (DST → Database → RG)
  - Why needed here: Understanding this pipeline is crucial for implementing and modifying the DIALIGHT system, as it forms the backbone of how user inputs are processed and responses are generated.
  - Quick check question: What are the three main components of a task-oriented dialogue system and how do they interact?

- Concept: Fine-tuning vs. in-context learning
  - Why needed here: DIALIGHT explicitly compares these two paradigms, so understanding their differences is essential for interpreting the results and making informed decisions about which approach to use.
  - Quick check question: What is the key difference between fine-tuning and in-context learning, and how does this difference impact performance in task-oriented dialogue?

- Concept: Automatic evaluation metrics (JGA, BLEU, ROUGE, METEOR, Inform Rate, Success Rate)
  - Why needed here: These metrics are used throughout the paper to evaluate system performance, so understanding what they measure is crucial for interpreting the results and comparing different approaches.
  - Quick check question: What does each of these automatic evaluation metrics measure, and what are their strengths and limitations in the context of task-oriented dialogue?

## Architecture Onboarding

- Component map: Web Interface -> Model Connector -> DST Service, RG Service, Database -> Web Interface
- Critical path: 1. User submits dialogue through web interface 2. Request routed through Model Connector to appropriate task model service 3. Task model processes dialogue and generates response 4. Response and evaluation data stored in database 5. Results displayed to user through web interface
- Design tradeoffs:
  - Microservices vs. monolithic architecture: Microservices enable independent scaling and easier maintenance but introduce network overhead and complexity
  - Fine-tuning vs. in-context learning: Fine-tuning provides better task-specific performance but requires labeled data, while in-context learning is more flexible but may struggle with task adherence
  - Automatic vs. human evaluation: Automatic metrics are faster and more scalable but may not correlate well with human judgments, while human evaluation is more reliable but slower and more resource-intensive
- Failure signatures:
  - High latency or timeouts: Could indicate overloaded services or network issues in microservice architecture
  - Low automatic evaluation scores: May indicate poor task-specific performance, especially for in-context learning approaches
  - Inconsistent human evaluation results: Could suggest issues with the web interface or data collection process
  - Container deployment failures: May indicate issues with Docker configuration or resource constraints
- First 3 experiments:
  1. Deploy the FT-mT5small system on a single dialogue to verify the basic pipeline functionality
  2. Compare the FT-mT5small and ICL-GPT-3.5 systems on a small set of dialogues to observe performance differences
  3. Deploy the microservice architecture with multiple instances of a task model to test load balancing and scalability

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do LLMs perform in multilingual dialogue state tracking compared to PLMs when prompts are provided in the target language rather than English?
- Basis in paper: [inferred] The paper notes that LLMs struggle with generating outputs in languages other than English when prompts are provided in English, suggesting this might improve with language-specific prompts.
- Why unresolved: The paper only tested prompts in English, leaving the impact of language-specific prompts unexplored.
- What evidence would resolve it: Conducting experiments with prompts provided in the target languages (Arabic, French, Turkish) and comparing the performance of LLMs and PLMs in dialogue state tracking.

### Open Question 2
- Question: What is the impact of incorporating training examples in the prompts for LLMs in task-oriented dialogue systems?
- Basis in paper: [explicit] The paper mentions that including training examples in prompts for LLMs adversely affects system performance, but this was not thoroughly investigated due to high costs.
- Why unresolved: The paper did not conduct a hyperparameter search for the number of ICL examples due to the high costs associated with such an experiment.
- What evidence would resolve it: Performing a systematic study varying the number and quality of training examples in the prompts for LLMs and measuring the impact on system performance.

### Open Question 3
- Question: How can task-oriented dialogue systems be modernized to better align with the inherent pretraining of LLMs and improve their performance?
- Basis in paper: [explicit] The paper suggests that the misalignment between task requirements for TOD and the inherent pretraining of LLMs leads to system failures, indicating a need for critical reevaluation of current design choices.
- Why unresolved: The paper identifies this as a potential area for future research but does not provide specific solutions or experimental results.
- What evidence would resolve it: Developing and testing new architectural designs or training methodologies for TOD systems that leverage the strengths of LLMs, and evaluating their performance against current systems.

## Limitations

- Limited language coverage (four languages) constrains generalizability of findings
- Zero-shot nature of ICL-based systems means performance may vary significantly with different prompting strategies
- Focus on single dataset (Multi3WOZ) may not capture full range of task-oriented dialogue challenges

## Confidence

- **High Confidence**: The architectural claims about microservice design enabling scalability and the observed performance advantages of fine-tuned systems in accuracy and coherence metrics.
- **Medium Confidence**: The comparative claims about LLM-based systems producing more diverse and likeable responses, as these rely on subjective human evaluations that may vary across different user groups.
- **Low Confidence**: The mechanism explaining why fine-tuning specifically improves multilingual performance, as the paper provides limited linguistic analysis of the observed differences.

## Next Checks

1. **Cross-dataset validation**: Test both system paradigms on additional multilingual task-oriented dialogue datasets (e.g., M2M-GPT, MASSIVE) to verify if the performance patterns hold across different data distributions and domains.

2. **Prompt engineering impact study**: Systematically vary the in-context learning prompts for LLM-based systems (changing few-shot examples, prompt templates, and instructions) to quantify the sensitivity of performance to prompt design.

3. **Human evaluation replication**: Conduct a second round of human evaluations with different annotators and dialogue topics to verify the consistency of findings regarding response diversity, likeability, and instruction adherence.