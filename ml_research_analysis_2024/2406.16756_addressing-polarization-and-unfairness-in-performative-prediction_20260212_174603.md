---
ver: rpa2
title: Addressing Polarization and Unfairness in Performative Prediction
arxiv_id: '2406.16756'
source_url: https://arxiv.org/abs/2406.16756
tags:
- fair
- loss
- fairness
- group
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the unfairness and polarization issues of
  performative stable (PS) solutions in performative prediction (PP). The authors
  show that PS solutions can lead to severe polarization and performance disparities
  among demographic groups, and that conventional fairness interventions often fail
  to achieve both stability and fairness in PP settings.
---

# Addressing Polarization and Unfairness in Performative Prediction

## Quick Facts
- **arXiv ID**: 2406.16756
- **Source URL**: https://arxiv.org/abs/2406.16756
- **Reference count**: 40
- **Primary result**: Conventional fairness interventions fail in performative prediction due to stability disruption; novel fairness mechanisms provably ensure both stability and fairness

## Executive Summary
This paper addresses the critical problem of polarization and unfairness in performative stable (PS) solutions within performative prediction (PP) frameworks. The authors demonstrate that conventional fairness interventions like group loss variance penalties and distributionally robust optimization (DRO) often fail to achieve both stability and fairness because they disrupt the convergence properties required for PS solutions. To resolve this, the paper proposes novel fairness mechanisms that provably ensure both stability and fairness by carefully designing convex penalty terms that preserve the mathematical properties needed for convergence.

## Method Summary
The paper proposes three fairness mechanisms within a Fair-RRM framework: group-level penalty (GLP), sample-level penalty (SLP), and sample re-weighting (RW). These mechanisms add convex fairness terms to the objective function that maintain strong convexity and joint smoothness properties required for convergence. The iterative algorithm updates model parameters using fair objectives at each step while accounting for how model deployment affects data distribution through retention dynamics. The key innovation is ensuring that fairness penalties preserve the mathematical conditions needed for convergence to stable solutions.

## Key Results
- PS solutions can lead to severe polarization and performance disparities among demographic groups
- Conventional fairness interventions (group loss variance penalty, DRO) fail under performative prediction due to convergence issues
- Proposed fairness mechanisms provably achieve both stability and fairness under mild conditions
- Empirical results validate theoretical findings on synthetic and real datasets (Credit data, MNIST)

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Conventional fairness interventions fail under performative prediction because they disrupt stability of iterative algorithms
- **Mechanism**: Fairness penalties that are not convex or jointly smooth can cause iterative algorithms to diverge or oscillate instead of converging to stable solutions
- **Core assumption**: Fairness penalty must preserve strong convexity and joint smoothness properties required for convergence
- **Evidence anchors**: Examples 4.2-4.3 demonstrate non-convergence with group loss variance and DRO with χ² distance
- **Break condition**: When fairness penalty introduces non-convexity or violates joint smoothness condition

### Mechanism 2
- **Claim**: Novel fairness mechanisms achieve both stability and fairness by preserving convexity
- **Mechanism**: Proposed group-level, sample-level, and re-weighting methods add convex fairness terms maintaining strong convexity and joint smoothness
- **Core assumption**: Fairness penalty must be convex and jointly smooth with respect to model parameters
- **Evidence anchors**: Section 4.3 shows convergence of Fair-RRM with proposed fair updates; Section 5 provides theoretical analysis
- **Break condition**: When fairness mechanism strength ρ is too large, violating convergence condition ϵ(1 + β̃/γ) < 1

### Mechanism 3
- **Claim**: Fairness mechanism strength ρ controls tradeoff between fairness improvement and global loss
- **Mechanism**: Increasing ρ assigns more weight to disadvantaged groups/samples, reducing group-wise loss disparity while maintaining stability
- **Core assumption**: Relationship between ρ and fairness improvement is monotonic under retention dynamics
- **Evidence anchors**: Theorem 5.4 proves group loss disparity is non-increasing in ρ; Section 6 shows empirical results
- **Break condition**: When ρ approaches maximum value where ϵ(1 + β̃max/γ) = 1, potentially violating convergence

## Foundational Learning

- **Concept**: Performative prediction (PP) framework
  - **Why needed here**: Understanding how model deployment affects data distribution is crucial for recognizing why conventional fairness methods fail
  - **Quick check question**: What is the key difference between standard supervised learning and performative prediction in terms of data distribution?

- **Concept**: Strong convexity and joint smoothness
  - **Why needed here**: These mathematical properties are essential for proving convergence of iterative algorithms to stable solutions
  - **Quick check question**: Why must the fairness penalty term preserve strong convexity and joint smoothness for convergence?

- **Concept**: Distributionally robust optimization (DRO)
  - **Why needed here**: Understanding DRO helps recognize why it fails in PP settings and how proposed sample re-weighting differs
  - **Quick check question**: How does the proposed sample re-weighting method differ from standard DRO in terms of computational efficiency and convergence?

## Architecture Onboarding

- **Component map**: Data distribution module -> Fairness mechanism module -> Iterative optimization module -> Evaluation module
- **Critical path**:
  1. Initialize data distribution D(0) and model parameters θ(0)
  2. At each iteration t:
     - Compute fair objective Lfair(θ;D(t-1), ρ)
     - Update model parameters θ(t) = argminθ Lfair(θ;D(t-1), ρ)
     - Update data distribution D(t) = T(θ(t);D(t-1))
  3. Check convergence criteria
  4. Evaluate fairness metrics at Fair-PS solution
- **Design tradeoffs**:
  - Choice of fairness mechanism: Group-level penalty requires sensitive attribute access, while sample-level penalty and re-weighting do not
  - Strength of fairness ρ: Higher values improve fairness but may risk convergence; must stay below ρmax
  - Deployment schema: Conventional, k-delayed, or delayed schemas affect convergence rate but not Fair-PS solution
- **Failure signatures**:
  - Non-convergence: Model parameters oscillate or diverge instead of stabilizing
  - Fairness degradation: Group-wise loss disparity increases instead of decreasing with higher ρ
  - Sensitivity issues: Small changes in initial conditions lead to large variations in final fairness metrics
- **First 3 experiments**:
  1. Gaussian mean estimation with two groups: Test convergence and fairness improvement with varying ρ
  2. Credit data with strategic behavior: Validate effectiveness on real-world data with retention dynamics
  3. MNIST classification with MLP: Verify fairness mechanisms work in non-convex settings with deep learning models

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: How does choice of fairness penalty (group-level vs. sample-level) impact convergence rate and stability in non-convex settings?
- **Basis in paper**: The paper mentions Fair-RRM and Fair-RERM convergence results are derived under mild conditions but doesn't cover non-convex settings
- **Why unresolved**: Theoretical analysis assumes convexity of loss function and fairness penalties, but deep learning models are often non-convex
- **What evidence would resolve it**: Empirical studies comparing convergence rates and stability of Fair-PS solutions using different fairness penalties in non-convex settings

### Open Question 2
- **Question**: How do different retention dynamics (delayed vs. conventional) affect fairness and stability of Fair-PS solutions?
- **Basis in paper**: The paper discusses impact of different deployment schemas on convergence rates but doesn't explore their impact on fairness and stability in depth
- **Why unresolved**: Paper provides theoretical analysis on convergence rates under different schemas but not on fairness impact
- **What evidence would resolve it**: Empirical studies comparing fairness and stability of Fair-PS solutions under different retention dynamics

### Open Question 3
- **Question**: Can proposed fairness mechanisms be extended to handle more complex fairness metrics beyond group-wise loss disparity?
- **Basis in paper**: The paper focuses on group-wise loss disparity and participation disparity as fairness metrics
- **Why unresolved**: Paper doesn't explore applicability to other fairness metrics like equal opportunity or demographic parity
- **What evidence would resolve it**: Theoretical analysis and empirical studies demonstrating effectiveness for other fairness metrics

## Limitations
- Convergence analysis relies on strong convexity and joint smoothness assumptions that may not hold in practical deep learning applications
- Retention dynamics assumption may not capture complex real-world feedback loops between model deployment and data distribution
- Effectiveness depends critically on choosing appropriate ρ values, but systematic guidance for this choice is limited

## Confidence

**High**: Conventional fairness methods fail in PP due to non-convexity and joint smoothness violations (Section 4.2 examples)

**Medium**: Proposed fairness mechanisms achieve both stability and fairness under theoretical conditions (Theorems 5.1-5.4)

**Medium**: Fairness-fairness tradeoff can be controlled via parameter ρ (Theorem 5.4 and empirical results)

## Next Checks
1. Test convergence of fairness mechanisms on non-convex neural network architectures beyond MLP (e.g., CNNs for MNIST)
2. Conduct ablation studies varying fairness mechanism strength ρ across multiple orders of magnitude to identify breaking points
3. Evaluate sensitivity of Fair-PS solutions to initial conditions and data distribution perturbations