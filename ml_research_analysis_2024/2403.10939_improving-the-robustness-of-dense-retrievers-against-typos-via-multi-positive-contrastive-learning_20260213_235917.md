---
ver: rpa2
title: Improving the Robustness of Dense Retrievers Against Typos via Multi-Positive
  Contrastive Learning
arxiv_id: '2403.10939'
source_url: https://arxiv.org/abs/2403.10939
tags:
- contrastive
- learning
- retrieval
- query
- queries
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of improving dense retriever robustness
  against typos in queries. Current methods often fail to fully utilize multiple typoed
  query variants available during training.
---

# Improving the Robustness of Dense Retrievers Against Typos via Multi-Positive Contrastive Learning

## Quick Facts
- arXiv ID: 2403.10939
- Source URL: https://arxiv.org/abs/2403.10939
- Authors: Georgios Sidiropoulos; Evangelos Kanoulas
- Reference count: 23
- Primary result: Multi-positive contrastive learning improves dense retriever robustness against typos compared to single-positive methods

## Executive Summary
This paper addresses the problem of improving dense retriever robustness against typos in queries. Current methods often fail to fully utilize multiple typoed query variants available during training. The core idea is to employ multi-positive contrastive learning, which leverages all available typoed queries as positive samples simultaneously, instead of treating them as single positives. Experiments on two datasets (MS MARCO and DL-Typo) show that this approach improves robustness against typos compared to single-positive contrastive learning methods. Specifically, the multi-positive approach yielded improvements in MRR@10 and R@1000 metrics on both datasets, with some improvements being statistically significant.

## Method Summary
The paper proposes using multi-positive contrastive learning for dense retrievers to improve robustness against typos. Instead of sampling a single typoed variant as positive per update, the method simultaneously employs all available typoed variants of a query as positives. The multi-positive loss computes similarity across all positives, encouraging the model to align the anchor with all its typoed variants. The approach was evaluated on MS MARCO and DL-Typo datasets with synthetically generated typos, comparing DR+DL, DR+CL, DR+ST+DL and their multi-positive variants against baselines.

## Key Results
- Multi-positive contrastive learning consistently outperforms single-positive counterpart across different numbers of typoed variants (K=1,10,20,30,40)
- Improvements observed in MRR@10 and R@1000 metrics on both MS MARCO and DL-Typo datasets
- Some improvements were statistically significant, demonstrating robustness of the approach

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Using multiple typoed variants as positives simultaneously improves contrastive learning by focusing on what makes the anchor and all positives similar.
- Mechanism: Multi-positive contrastive loss computes similarity across all positives, encouraging the model to align the anchor with all its typoed variants rather than just one randomly sampled variant per update.
- Core assumption: The available typoed variants are all semantically equivalent to the original query and should be treated as equally relevant.
- Evidence anchors:
  - [abstract] "we argue that all available positives can be used at the same time and employ contrastive learning that supports multiple positives"
  - [section 2.1] "Given a query q, instead of sampling a different typoed variant q′ from a set Q′ at each update, we propose simultaneously employing all typoed variants"
  - [corpus] Weak evidence - no direct corpus neighbors discussing multi-positive contrastive learning specifically
- Break condition: If typoed variants introduce semantic drift or become too dissimilar from the original query, treating them as positives could harm learning.

### Mechanism 2
- Claim: Replacing single-positive contrastive loss with multi-positive variant in robustifying subtasks yields improvements in robustness against typos.
- Mechanism: The multi-positive variant aggregates gradients from multiple positive pairs simultaneously, providing richer supervision signals than single-positive variants that only consider one positive per anchor.
- Core assumption: The computational overhead of multi-positive loss is acceptable and the GPU memory constraints can be managed.
- Evidence anchors:
  - [abstract] "employing multi-positive contrastive learning on the robustifying subtask yields improvements in robustness against using contrastive learning with a single positive"
  - [section 2.2] "we propose that for the query retrieval task, given a passage p, we can have a set of relevant queries consisting of the typo-free query and its typoed variations"
  - [corpus] Weak evidence - no direct corpus neighbors discussing computational tradeoffs of multi-positive vs single-positive
- Break condition: If the number of positives becomes too large relative to available GPU memory, the batch size must be reduced, potentially harming training stability.

### Mechanism 3
- Claim: Multi-positive contrastive learning consistently outperforms single-positive counterpart regardless of the number of positives.
- Mechanism: As shown in Table 2, DR+DL+STM consistently outperforms DR+DL+ST across different augmentation sizes (K=1,10,20,30,40), suggesting the multi-positive approach is robust to the number of available typoed variants.
- Core assumption: The improvements scale linearly with the number of positives and are not saturated at lower values of K.
- Evidence anchors:
  - [section 4] "Table 2 unveils that our multi-positive variant consistently outperforms the original model for the different numbers of typoed variants per query"
  - [section 3] "we obtain typo variations for each typo-free query via a synthetic typo generation model and repeat the typo generation process 10 times"
  - [corpus] Weak evidence - no direct corpus neighbors discussing scaling behavior of multi-positive contrastive learning
- Break condition: If the number of positives becomes very large, the softmax normalization in Equation 2 may become numerically unstable or the gradient magnitudes may explode.

## Foundational Learning

- Concept: Contrastive learning fundamentals (anchor-positive-negative triplets)
  - Why needed here: The paper builds upon contrastive learning framework and extends it from single-positive to multi-positive setting
  - Quick check question: What is the key difference between Equation 1 (single-positive) and Equation 2 (multi-positive) in terms of gradient computation?

- Concept: Dense retrieval architecture (dual-encoder models)
  - Why needed here: The paper applies multi-positive contrastive learning to dense retrievers that use BERT-based query and passage encoders
  - Quick check question: In a dual-encoder architecture, what are the two separate encoders learning to map into a common embedding space?

- Concept: Data augmentation for robustness
  - Why needed here: The paper uses typo generation to create augmented training data and employs multi-positive contrastive learning on this augmented data
  - Quick check question: What types of transformations does the typo generator apply to create typoed queries from original queries?

## Architecture Onboarding

- Component map: Query Encoder -> Embedding -> Multi-positive Contrastive Loss -> Gradient -> Update; Passage Encoder -> Embedding -> Multi-positive Contrastive Loss -> Gradient -> Update
- Critical path: Query/Passage → Encoder → Embedding → Contrastive Loss → Gradient → Update; Multi-positive loss requires computing similarity across all positives simultaneously before backpropagation
- Design tradeoffs: Multi-positive vs single-positive: richer supervision vs higher memory/computation; Number of typoed variants (K): more positives vs memory constraints; Batch size: larger batches for multi-positive vs GPU memory limits
- Failure signatures: Training instability: likely from numerical issues in softmax with many positives; No improvement over baseline: possibly from semantic drift in typoed variants; Memory errors: batch size too large for multi-positive computation
- First 3 experiments: 1) Compare DR+CL (single-positive) vs DR+CLM (multi-positive) on MS MARCO typoed queries; 2) Test different values of K (1, 10, 20, 30, 40) for DR+DL+STM vs DR+DL+ST; 3) Evaluate on DL-Typo dataset to verify robustness across different evaluation sets

## Open Questions the Paper Calls Out
The paper does not explicitly call out any open questions in the text provided.

## Limitations
- Evaluation relies primarily on synthetic typos generated by TextAttack, which may not fully capture real-world typo distributions
- Computational overhead of multi-positive contrastive learning is not thoroughly analyzed
- Study does not explore semantic drift that might occur when typoed variants become too dissimilar from original queries
- Weighting hyperparameters for combining multiple loss components are set through manual tuning without systematic analysis

## Confidence
- **High confidence**: The core claim that multi-positive contrastive learning outperforms single-positive approaches on typoed queries is well-supported by experimental results across two datasets and multiple metrics
- **Medium confidence**: The scalability claim that improvements hold across different numbers of typoed variants (K=1,10,20,30,40) is demonstrated but without exploring higher values or saturation points
- **Medium confidence**: The computational feasibility claim is supported by successful training runs but lacks detailed analysis of memory usage and runtime overhead compared to single-positive variants

## Next Checks
1. **Semantic drift analysis**: Measure semantic similarity between original queries and typoed variants using embedding similarity or lexical overlap. Flag any variants where semantic drift exceeds a threshold (e.g., >0.3 cosine distance from original), as these could harm learning.

2. **Scaling behavior investigation**: Systematically evaluate the performance and memory usage as K increases beyond 40 (e.g., K=50, 100) to identify potential saturation points or computational bottlenecks in the multi-positive contrastive loss.

3. **Real-world typo evaluation**: Supplement synthetic typos with a small set of human-annotated real typos from search logs to validate that improvements generalize beyond the synthetic typo distribution used in training.