---
ver: rpa2
title: 'HGRN2: Gated Linear RNNs with State Expansion'
arxiv_id: '2404.07904'
source_url: https://arxiv.org/abs/2404.07904
tags:
- linear
- state
- hgrn2
- recurrent
- attention
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: HGRN2 addresses the limited recurrent state size in HGRN by introducing
  an outer-product-based state expansion mechanism that enlarges the state without
  adding parameters. This expansion enables hardware-efficient training through a
  linear attention interpretation.
---

# HGRN2: Gated Linear RNNs with State Expansion

## Quick Facts
- arXiv ID: 2404.07904
- Source URL: https://arxiv.org/abs/2404.07904
- Reference count: 25
- HGRN2 consistently outperforms HGRN1 across language modeling, image classification, and Long Range Arena benchmarks

## Executive Summary
HGRN2 addresses the limited recurrent state size in HGRN by introducing an outer-product-based state expansion mechanism that enlarges the state without adding parameters. This expansion enables hardware-efficient training through a linear attention interpretation. Experiments show HGRN2 consistently outperforms HGRN1 across language modeling, image classification, and Long Range Arena benchmarks. The 3B HGRN2 model slightly outperforms Mamba and LLaMa Architecture Transformer in controlled language modeling experiments and achieves competitive downstream performance with fewer training tokens than open-source 3B models.

## Method Summary
HGRN2 is a gated linear RNN that expands the recurrent state using outer products, replacing element-wise products with outer products to increase state size from Rd to Rd×d without adding parameters. The model employs a multi-head variant to reduce computational complexity from O(BNd²) to O(BNd²/H). Training leverages hardware-efficient linear attention algorithms through its linear attention interpretation. The architecture maintains gating mechanisms (input, forget, output gates) while expanding the state through diagonalized forget gates and matrix dot products.

## Key Results
- Consistently outperforms HGRN1 across language modeling, image classification, and Long Range Arena benchmarks
- 3B HGRN2 model slightly outperforms Mamba and LLaMa Architecture Transformer in controlled language modeling experiments
- Achieves competitive downstream performance with fewer training tokens than open-source 3B models

## Why This Works (Mechanism)

### Mechanism 1
The outer product-based state expansion mechanism effectively increases the recurrent state size without introducing additional parameters. By replacing element-wise products with outer products, HGRN2 expands the state to a higher-dimensional matrix (from Rd to Rd×d). This expanded state can capture more complex patterns and dependencies without increasing the parameter count, as the expansion is achieved through a non-parametric operation.

### Mechanism 2
The linear attention interpretation of HGRN2 enables hardware-efficient training. The recurrence in HGRN2 can be viewed as a form of linear attention, where the state expansion is achieved through an outer product. This allows leveraging efficient linear attention algorithms and hardware optimizations (e.g., tensor cores) during training, mitigating the computational overhead typically associated with state expansion.

### Mechanism 3
The multi-head variant of HGRN2 reduces computational complexity while maintaining the benefits of state expansion. By introducing multiple heads, the effective state size becomes d²/H, where H is the number of heads. This reduces the computational complexity from O(BNd²) to O(BNd²/H), making it feasible to scale the state expansion ratio without incurring prohibitive computational costs.

## Foundational Learning

- **Linear Attention**: Understanding linear attention is crucial for grasping the hardware-efficient training aspect of HGRN2 and its similarity to other efficient models like GLA. Quick check: How does linear attention differ from traditional softmax attention, and what are the computational benefits?

- **Gated Recurrent Neural Networks (GRNNs)**: HGRN2 builds upon the HGRN architecture, which is a type of GRNN. Understanding the gating mechanisms and their role in recurrent networks is essential for comprehending HGRN2's improvements. Quick check: What is the purpose of the forget gate in a GRNN, and how does it differ from the input gate?

- **Outer Product**: The outer product is the key operation that enables state expansion in HGRN2. Understanding its properties and how it differs from element-wise products is crucial for grasping the mechanism behind HGRN2's improved expressiveness. Quick check: How does the outer product of two vectors differ from their element-wise product, and what are the implications for the resulting matrix?

## Architecture Onboarding

- **Component map**: Input layer -> HGRN2 layer (input gate, forget gate, output gate, outer product expansion) -> Output layer

- **Critical path**:
  1. Input projection and gating
  2. State expansion through outer product
  3. Recurrence with diagonalized forget gate and matrix dot product
  4. Output projection through output gate and matrix-vector multiplication
  5. (Optional) Multi-head processing for reduced complexity

- **Design tradeoffs**:
  - State expansion vs. computational complexity: Increasing the expansion ratio improves expressiveness but also increases computational cost
  - Single head vs. multi-head: Single head provides maximum expressiveness but is computationally expensive, while multi-head reduces complexity at the cost of some expressiveness
  - Linear attention interpretation vs. traditional RNN: The linear attention form enables hardware-efficient training but may introduce some approximation errors compared to traditional RNNs

- **Failure signatures**:
  - Degraded performance on long-range dependency tasks: May indicate that the state expansion is insufficient or that the multi-head variant is reducing expressiveness too much
  - Increased training time or memory usage: May indicate that the expansion ratio is too high or that the hardware-efficient training algorithms are not being effectively utilized
  - Instability during training: May indicate issues with the gating mechanisms or the diagonalization of the forget gate

- **First 3 experiments**:
  1. Ablation study on expansion ratio: Train HGRN2 with different expansion ratios (e.g., 4, 8, 16, 32) on a language modeling task and evaluate the impact on perplexity and computational cost
  2. Comparison with HGRN1: Train both HGRN1 and HGRN2 with similar parameter counts on the same task and compare their performance to validate the benefits of state expansion
  3. Multi-head variant evaluation: Train HGRN2 with different numbers of heads and evaluate the tradeoff between computational complexity and performance to determine the optimal configuration

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: What is the theoretical upper limit of state expansion ratio (n) before diminishing returns in HGRN2's performance?
- **Basis in paper**: The paper shows performance gains plateau after n=128 in Figure 2, suggesting a practical limit exists
- **Why unresolved**: The experiments only tested up to n=128, leaving uncertainty about whether further expansion could still provide benefits or if there's a fundamental barrier
- **What evidence would resolve it**: Additional experiments testing expansion ratios beyond 128 (e.g., 256, 512) across multiple tasks to determine if performance plateaus, declines, or continues improving

### Open Question 2
- **Question**: How does HGRN2's performance compare to hybrid models that combine linear attention with local window attention?
- **Basis in paper**: The paper notes that linear attention models historically underperform softmax attention, and recent improvements have combined linear attention with local attention, but doesn't compare HGRN2 to such hybrids
- **Why unresolved**: While HGRN2 shows strong performance, it's unclear if it matches or exceeds models that strategically combine different attention mechanisms
- **What evidence would resolve it**: Direct controlled experiments comparing HGRN2 against models like RetNet or other hybrid architectures on identical benchmarks

### Open Question 3
- **Question**: What is the computational complexity trade-off between increasing HGRN2's state size versus increasing its depth (number of layers)?
- **Basis in paper**: The paper shows both state expansion and increased parameters improve performance, but doesn't analyze the relative efficiency of these approaches
- **Why unresolved**: The paper demonstrates both strategies work but doesn't quantify which provides better performance-per-parameter or performance-per-FLOP
- **What evidence would resolve it**: Systematic experiments varying both state size and depth independently while measuring performance, parameter count, and training/inference time

## Limitations

- The computational complexity of O(BNd²) for the single-head variant could become prohibitive for large models or sequences
- The effectiveness of leveraging existing linear attention kernels depends heavily on implementation details not detailed in the paper
- Downstream evaluation is limited to commonsense reasoning tasks, not assessing performance on scientific text, code, or multilingual datasets

## Confidence

- **High confidence**: The core claim that HGRN2 consistently outperforms HGRN1 across multiple benchmarks is well-supported by experimental results
- **Medium confidence**: The claim about achieving competitive performance with fewer training tokens is supported by controlled experiments but limited to a subset of the Pile dataset
- **Low confidence**: The claim that HGRN2 "slightly outperforms" Mamba and LLaMa Architecture Transformer is based on a single metric with small margins that may not be statistically significant

## Next Checks

1. **Ablation study on expansion ratio**: Conduct comprehensive ablation study evaluating impact of different expansion ratios (e.g., n=4, 8, 16, 32) on language modeling performance and computational cost

2. **Multi-head variant analysis**: Evaluate multi-head variant with varying numbers of heads (e.g., H=1, 2, 4, 8) to assess impact on performance and computational complexity

3. **Broader downstream evaluation**: Extend downstream evaluation to include additional tasks and domains such as scientific text, code, and multilingual datasets to assess generalizability of performance improvements