---
ver: rpa2
title: 'Towards a clinically accessible radiology foundation model: open-access and
  lightweight, with automated evaluation'
arxiv_id: '2403.08002'
source_url: https://arxiv.org/abs/2403.08002
tags:
- llav
- a-rad
- image
- report
- radiology
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work presents LLaVA-Rad, a lightweight and open-source multimodal
  foundation model designed to address the challenges of deploying state-of-the-art
  medical AI in clinical settings. The key innovation is a modular training approach
  that leverages pre-trained image and text models, with a focus on a lightweight
  adapter to align modalities.
---

# Towards a clinically accessible radiology foundation model: open-access and lightweight, with automated evaluation

## Quick Facts
- arXiv ID: 2403.08002
- Source URL: https://arxiv.org/abs/2403.08002
- Reference count: 40
- Achieved SOTA performance on radiology tasks with lightweight design (29.4 F1-RadGraph, 38.1 ROUGE-L on MIMIC-CXR)

## Executive Summary
This paper presents LLaVA-Rad, a lightweight multimodal foundation model designed to make advanced medical AI clinically accessible. The model achieves state-of-the-art performance on radiology tasks while maintaining computational efficiency suitable for real-world deployment. LLaVA-Rad uses a modular approach combining pre-trained vision and language models with a lightweight adapter, trained on 697K chest X-ray images paired with radiology reports. The authors introduce CheXprompt, a GPT-4-based evaluation metric that correlates with expert assessments, addressing the challenge of evaluating factual correctness in generated medical reports.

## Method Summary
LLaVA-Rad employs a modular architecture that leverages pre-trained BiomedCLIP-CXR for image encoding and Vicuna-7B-v1.5 for text generation, with a lightweight MLP adapter to align the modalities. The model is trained on a large dataset of 697K chest X-ray image-report pairs from seven diverse sources, with GPT-4 used to synthesize reports from structured labels and translate reports from other languages. For evaluation, the authors introduce CheXprompt, a GPT-4-based metric that assesses factual correctness by identifying specific error types in generated reports.

## Key Results
- Achieved SOTA performance on standard radiology benchmarks (MIMIC-CXR, Open-I) with F1-RadGraph of 29.4 and ROUGE-L of 38.1
- Outperformed much larger models including GPT-4V and Med-PaLM M (84B) on cross-modal retrieval and report generation tasks
- Demonstrated computational efficiency with inference times of 0.7s per sample on a single V100 GPU and fine-tuning capabilities

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Using a modular approach with domain-specific pre-trained models and a lightweight adapter improves data efficiency and model performance in clinical radiology tasks.
- **Mechanism:** The model leverages pre-trained vision and text encoders (BiomedCLIP-CXR and Vicuna-7B-v1.5) and focuses on training a lightweight MLP to align image features with the language model's word embedding space. This modular approach allows the model to benefit from large-scale pre-training while focusing computational resources on the alignment task.
- **Core assumption:** Pre-trained models on general domains can be effectively adapted to specialized biomedical tasks with minimal fine-tuning.
- **Evidence anchors:**
  - [abstract]: "We adopt a modular approach by incorporating state-of-the-art pre-trained models for image and text modalities, and focusing on training a lightweight adapter to ground each modality to the text embedding space."
  - [section]: "Our intuition for designing LLaVA-Rad is that a lightweight, specialized SMM can be efficiently developed by decomposing training into unimodal pretraining on individual modalities followed by lightweight cross-modal learning focusing on a small adapter to ground a non-text modality to the text embedding space."
- **Break condition:** If the pre-trained models are not sufficiently aligned with the biomedical domain, the lightweight adapter may not effectively bridge the modality gap.

### Mechanism 2
- **Claim:** GPT-4 can be effectively used to synthesize radiology reports from structured labels and translate reports, enhancing the training dataset.
- **Mechanism:** GPT-4 is used to generate synthetic reports based on annotated image labels for datasets that lack free-text reports. It also translates reports from other languages into English, ensuring linguistic consistency across the training data.
- **Core assumption:** GPT-4 can generate clinically accurate and relevant reports that are consistent with the ground-truth labels.
- **Evidence anchors:**
  - [abstract]: "For training, we assemble a large dataset comprising 697 thousand radiology image-report pairs from 7 diverse sources. Some data sources only contain structured labels of key findings, in which case we use GPT-4 to synthesize the report based on the ground-truth labels."
  - [section]: "Since CXR images are often published with a limited number of associated findings or image labels instead of a complete report, we used GPT-4 to synthesize a report based on annotated image labels."
- **Break condition:** If GPT-4 generates reports that are not clinically accurate or relevant, the model may learn incorrect associations between images and reports.

### Mechanism 3
- **Claim:** CheXprompt, a GPT-4-based evaluation metric, provides a more accurate assessment of factual correctness in generated radiology reports compared to traditional metrics.
- **Mechanism:** GPT-4 is used to evaluate the factual correctness of generated reports by identifying specific error types, such as false positive findings, omission of findings, and incorrect locations/positions of findings. The evaluation is designed to be consistent with expert radiologist assessments.
- **Core assumption:** GPT-4 can accurately identify and categorize errors in radiology reports, providing a reliable assessment of factual correctness.
- **Evidence anchors:**
  - [abstract]: "For evaluation, we propose CheXprompt, a GPT-4-based metric for factuality evaluation, and demonstrate its parity with expert evaluation."
  - [section]: "We thus explore the utility of an LLM-based evaluation system, which has shown success in other domains [32, 48, 15]. Specifically, we employ GPT-4 as an evaluator to count how often the generated report contains errors in each of the following six categories, as per a previous study [53]."
- **Break condition:** If GPT-4's evaluation is not consistent with expert radiologist assessments, the metric may not accurately reflect the factual correctness of generated reports.

## Foundational Learning

- **Concept:** Multimodal learning and alignment of image and text representations.
  - **Why needed here:** The model needs to understand the relationship between chest X-ray images and their corresponding radiology reports to generate accurate and relevant reports.
  - **Quick check question:** How does the model align image features with the word embedding space of the language model?

- **Concept:** Data augmentation and synthetic data generation using large language models.
  - **Why needed here:** The model requires a large and diverse dataset to learn the complex relationship between chest X-ray images and their corresponding reports. GPT-4 is used to generate synthetic reports from structured labels and translate reports from other languages.
  - **Quick check question:** How does GPT-4 generate synthetic reports that are clinically accurate and relevant?

- **Concept:** Evaluation of factual correctness in generated reports using language models.
  - **Why needed here:** Traditional metrics may not accurately assess the factual correctness of generated radiology reports. GPT-4 is used to identify specific error types and provide a more reliable assessment of factual correctness.
  - **Quick check question:** How does GPT-4 identify and categorize errors in radiology reports?

## Architecture Onboarding

- **Component map:** BiomedCLIP-CXR → MLP adapter → Vicuna-7B-v1.5
- **Critical path:** Image encoder → MLP adapter → Text encoder → Generated report
- **Design tradeoffs:** Using a modular approach with pre-trained models allows for efficient training but may limit the model's ability to learn complex relationships between images and reports. Using GPT-4 for data augmentation and evaluation can enhance the training dataset and provide a more accurate assessment of factual correctness but may introduce biases or errors.
- **Failure signatures:** Poor alignment between image and text representations may result in inaccurate or irrelevant generated reports. Errors in GPT-4-generated reports may lead to incorrect associations between images and reports. Inconsistencies between GPT-4's evaluation and expert radiologist assessments may indicate limitations in the CheXprompt metric.
- **First 3 experiments:**
  1. Evaluate the alignment between image and text representations using cross-modal retrieval tasks.
  2. Assess the accuracy of GPT-4-generated reports by comparing them to ground-truth reports.
  3. Validate the CheXprompt metric by comparing its evaluations to those of expert radiologists.

## Open Questions the Paper Calls Out
None

## Limitations
- Reliance on GPT-4 for both data synthesis and evaluation introduces potential biases and limits reproducibility
- Limited clinical validation beyond benchmark performance metrics
- Data quality assumptions about GPT-4's ability to generate clinically accurate reports from structured labels

## Confidence
- **High confidence**: The modular architecture design and technical implementation details (Mechanisms 1 and 3)
- **Medium confidence**: The performance claims relative to larger models (based on benchmark metrics)
- **Medium confidence**: The effectiveness of GPT-4 for data synthesis (Mechanism 2, due to limited validation details)

## Next Checks
1. Conduct a radiologist study comparing GPT-4-synthesized reports against ground truth reports to verify clinical accuracy and identify systematic biases.
2. Test CheXprompt's consistency by having multiple expert radiologists independently evaluate the same generated reports to validate the claimed correlation.
3. Evaluate LLaVA-Rad on a held-out external dataset not used in training to assess real-world generalization beyond benchmark performance.