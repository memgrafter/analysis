---
ver: rpa2
title: 'iGAiVA: Integrated Generative AI and Visual Analytics in a Machine Learning
  Workflow for Text Classification'
arxiv_id: '2409.15848'
source_url: https://arxiv.org/abs/2409.15848
tags:
- data
- recall
- text
- ieee
- class
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a visual analytics-driven approach to improve
  machine learning models for text classification by using large language models to
  generate targeted synthetic training data. The method identifies data deficiencies
  through visual analytics techniques such as t-SNE, PCA, RBF heatmaps, and tag-treemaps,
  then uses these insights to guide synthetic data generation.
---

# iGAiVA: Integrated Generative AI and Visual Analytics in a Machine Learning Workflow for Text Classification

## Quick Facts
- arXiv ID: 2409.15848
- Source URL: https://arxiv.org/abs/2409.15848
- Reference count: 40
- Improved class T13 recall from 18% to 31% and overall model performance by up to 9.5% for specific classes

## Executive Summary
This paper presents iGAiVA, an integrated approach that combines visual analytics (VA) and generative AI to improve text classification models. The method identifies data deficiencies through VA techniques and uses these insights to guide targeted synthetic data generation via large language models (LLMs). The approach was demonstrated on a real-world ticketing system dataset with 39,100 messages across 15 classes, achieving significant improvements in model recall for underperforming classes. The authors also developed a software tool that integrates these techniques into an iterative machine learning workflow.

## Method Summary
The iGAiVA approach uses four VA techniques (t-SNE, PCA, RBF heatmaps, and tag-treemaps) to visualize data distributions and identify classification errors. These visualizations reveal patterns of poor performance and data imbalance, which guide the selection of specific text examples as inputs to an LLM (GPT-3.5 API) for generating targeted synthetic training data. The synthetic data is merged with the original training set, and the model is retrained and evaluated. This iterative process continues until performance targets are met, with cache management to handle multiple synthetic datasets.

## Key Results
- Increased class T13 recall from 18% to 31%
- Improved overall model performance by up to 9.5% for specific classes
- Demonstrated effectiveness on 39,100-message ticketing system dataset across 15 classes

## Why This Works (Mechanism)

### Mechanism 1
Visual analytics reduces the search space for LLM-based synthetic data generation by enabling targeted hypothesis formation. Analysts use t-SNE, PCA, RBF heatmaps, and tag-treemap visualizations to identify regions of poor model performance and data imbalance, guiding synthetic data generation to high-impact areas.

### Mechanism 2
RBF heatmaps combined with PCA scatter plots enable quantitative assessment of error regions, guiding synthetic data placement. RBF heatmaps estimate recall error rates across PCA feature space, revealing high-error regions even where data points are sparse.

### Mechanism 3
Tag-treemap visualizations enable keyword-level comparison of text clusters, supporting semantic-based synthetic data generation. Tag-treemap combines tag clouds with treemap layout to show keyword statistics of different text subsets, identifying semantic gaps to target for synthetic data generation.

## Foundational Learning

- **Dimensionality reduction (t-SNE, PCA)**: Reduces high-dimensional text features to 2D visualizations, making it possible to identify clusters and patterns related to classification performance. *Quick check*: What is the key difference between t-SNE and PCA in terms of preserving data relationships?

- **Radial Basis Function interpolation**: Estimates error rates across continuous feature space, enabling targeting of synthetic data generation in unsampled regions. *Quick check*: How does RBF interpolation differ from nearest-neighbor estimation in handling sparse data regions?

- **Tag cloud and treemap visualization**: Shows keyword statistics of different text subsets, enabling semantic-level comparison to identify gaps for synthetic data generation. *Quick check*: What advantage does combining tag clouds with treemaps offer over using either visualization alone?

## Architecture Onboarding

- **Component map**: Visual Analytics Engine -> LLM Integration Module -> Data Management Layer -> ML Model Interface
- **Critical path**: Visualize training data with VA techniques → Select examples from identified regions → Generate synthetic data using LLM → Merge synthetic data with training data → Retrain model and evaluate → Repeat until performance targets are met
- **Design tradeoffs**: Trades computational cost of generating multiple visualizations against precision of targeted synthetic data generation; uses cache storage for rapid iteration but requires memory management
- **Failure signatures**: Performance plateaus despite iterations (visual patterns no longer correlate with errors, LLM generates redundant data, cache overflow, or overfitting to synthetic patterns)
- **First 3 experiments**:
  1. Run complete workflow on small dataset with 2-3 classes to verify all components interact correctly
  2. Generate synthetic data for one poorly performing class and verify recall improvement
  3. Test cache management by generating multiple synthetic datasets and confirming older datasets are properly purged when cache limit is reached

## Open Questions the Paper Calls Out

### Open Question 1
What is the long-term impact of targeted synthetic data generation on overall model robustness across changing organizational structures? The paper demonstrates improvements on a single ticketing system dataset but doesn't investigate how these improvements hold up as organizational structures evolve over time or across different ticketing systems.

### Open Question 2
What is the optimal balance between human effort in visual analytics versus automated synthetic data generation for different class sizes and complexity levels? The authors describe extensive manual visual analysis processes and iterative refinement, but don't quantify the efficiency trade-offs or identify thresholds where automation becomes more effective.

### Open Question 3
How do different parameter settings for the LLM (temperature, max tokens, frequency penalty, etc.) affect the quality and diversity of generated synthetic data for different types of text classification tasks? The authors mention specific parameters but don't explore how different settings affect resulting model performance or characteristics of generated data.

## Limitations
- Reliance on correlation between visual patterns and actual model errors, which is not empirically validated across different datasets
- RBF interpolation may introduce significant uncertainty in high-dimensional text feature spaces
- Tag-treemap approach may not capture complex semantic features where context extends beyond simple keyword matching

## Confidence
- **Medium**: Overall framework integration and workflow design are well-specified and reproducible
- **Low**: Effectiveness of visual analytics in accurately identifying data deficiencies that lead to classification errors
- **Medium**: Targeted synthetic data generation approach, though quality and diversity of generated data is not thoroughly evaluated
- **Low**: Generalizability of results beyond the specific ticketing system dataset used

## Next Checks
1. Apply the iGAiVA approach to at least two additional text classification datasets with different characteristics to assess generalizability
2. Conduct an ablation study systematically removing each visual analytics technique to quantify their individual contributions
3. Perform human evaluation or automated semantic similarity analysis to assess whether LLM-generated synthetic data maintains semantic coherence with real data and effectively addresses identified deficiencies