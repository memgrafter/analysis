---
ver: rpa2
title: Revisiting the Robust Generalization of Adversarial Prompt Tuning
arxiv_id: '2405.11154'
source_url: https://arxiv.org/abs/2405.11154
tags:
- adversarial
- prompt
- pre-trained
- robustness
- tuning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This work addresses the problem of over-fitting in adversarial\
  \ prompt tuning, which limits the model\u2019s ability to generalize on both clean\
  \ and adversarial scenarios. The core method idea is to propose an adaptive Consistency-guided\
  \ Adversarial Prompt Tuning (CAPT) framework that utilizes multi-modal prompt learning\
  \ to enhance the alignment of image and text features for adversarial examples and\
  \ leverage the strong generalization of pre-trained CLIP to guide the model, enhancing\
  \ its robust generalization on adversarial examples while maintaining its accuracy\
  \ on clean ones."
---

# Revisiting the Robust Generalization of Adversarial Prompt Tuning

## Quick Facts
- arXiv ID: 2405.11154
- Source URL: https://arxiv.org/abs/2405.11154
- Authors: Fan Yang; Mingxuan Xia; Sangzhou Xia; Chicheng Ma; Hui Hui
- Reference count: 40
- Primary result: CAPT achieves 16.91% accuracy and 7.51% robustness improvements over state-of-the-art methods on ImageNet across different shot settings

## Executive Summary
This paper addresses the critical problem of overfitting in adversarial prompt tuning, which limits model performance on both clean and adversarial examples. The authors propose an adaptive Consistency-guided Adversarial Prompt Tuning (CAPT) framework that uses multi-modal prompt learning to enhance alignment between image and text features for adversarial examples. By leveraging the strong generalization of pre-trained CLIP and introducing an adaptive consistency objective, CAPT significantly outperforms existing methods while maintaining accuracy on clean inputs. The framework demonstrates excellent performance across in-distribution and out-of-distribution experiments.

## Method Summary
CAPT is an adaptive consistency-guided adversarial prompt tuning framework that addresses overfitting in vision-language models. It employs learnable multi-modal prompts in both image and text encoders to improve feature alignment for adversarial examples. The method uses an adaptive consistency loss that balances learning from the frozen CLIP model and the fine-tuned model, preventing overfitting while improving generalization. Training involves generating adversarial examples using PGD, extracting multi-modal features, and optimizing a combined loss that includes cross-entropy, adaptive consistency-guided loss, and KL divergence regularization for consistency between clean and adversarial inputs.

## Key Results
- Achieves 16.91% improvement in accuracy and 7.51% improvement in robustness on ImageNet dataset averaged across different shots
- Demonstrates excellent performance in both in-distribution and out-distribution experiments
- Outperforms state-of-the-art methods including HEP, PAFT, AVP, and APT across 14 diverse datasets

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Multi-modal prompt learning improves alignment between visual and textual features for adversarial examples, enhancing robustness.
- Mechanism: Learnable prompt vectors in both image and text encoders (deep transformer layers) learn to align features across modalities, compensating for distributional shift from adversarial perturbations.
- Core assumption: Feature alignment gap between visual and textual features for adversarial examples is a primary source of misclassification.
- Evidence anchors: Multi-modal prompt learning proposed to enhance alignment for adversarial examples; related works focus on prompt tuning but don't explicitly anchor alignment improvements to multi-modal prompts.
- Break condition: If feature alignment problem is secondary to other adversarial vulnerabilities (model architecture, input preprocessing), this mechanism may have limited impact.

### Mechanism 2
- Claim: Adaptive consistency loss balances learning from frozen CLIP and fine-tuned model, preventing overfitting while improving generalization.
- Mechanism: Adaptive weight (αcons) dynamically adjusts based on reliability of predictions from frozen CLIP model, allowing fine-tuned model to learn task-specific features while staying aligned with generalizable CLIP features.
- Core assumption: Overfitting occurs because model becomes too specialized to training set's adversarial distribution, losing CLIP's generalization.
- Evidence anchors: Adaptive weighting strategy designed based on model reliability on clean examples; related works mention adaptive weighting but not explicitly tied to overfitting prevention in adversarial contexts.
- Break condition: If overfitting is due to factors unrelated to consistency between clean/adversarial inputs (data scarcity, model capacity), this mechanism may not fully address the issue.

### Mechanism 3
- Claim: Consistency between clean and adversarial inputs improves robustness without sacrificing clean accuracy.
- Mechanism: TRADES-like loss term (KL divergence between clean and adversarial predictions) encourages model to learn representations stable across input variations.
- Core assumption: Model predicting similarly for clean and adversarial inputs will be more robust while retaining clean accuracy.
- Evidence anchors: KL divergence regularization added to ensure consistency between clean and adversarial inputs; TRADES known method but novel application to prompt tuning in this context.
- Break condition: If clean-adversarial consistency is not dominant factor in robustness (dependent on input preprocessing or model architecture), this mechanism may have limited impact.

## Foundational Learning

- Concept: Adversarial training
  - Why needed here: CAPT uses adversarial training during prompt tuning to improve robustness against adversarial examples.
  - Quick check question: What is the goal of adversarial training in the context of CAPT?

- Concept: Multi-modal feature alignment
  - Why needed here: CAPT improves alignment between visual and textual features for adversarial examples to enhance robustness.
  - Quick check question: How does multi-modal prompt learning contribute to feature alignment in CAPT?

- Concept: KL divergence
  - Why needed here: CAPT uses KL divergence to measure consistency between clean and adversarial predictions, as well as between fine-tuned and frozen CLIP models.
  - Quick check question: What is the role of KL divergence in the adaptive consistency loss of CAPT?

## Architecture Onboarding

- Component map: Pre-trained CLIP (frozen) -> Learnable multi-modal prompts -> Adaptive consistency loss -> Adversarial training
- Critical path: 1) Generate adversarial examples using PGD 2) Extract multi-modal features from prompt-tuned and frozen CLIP models 3) Compute cross-entropy loss on clean examples 4) Compute adaptive consistency-guided loss 5) Update learnable multi-modal prompts using combined loss
- Design tradeoffs: Tradeoff between robustness and accuracy (adaptive consistency loss balances objectives), computational cost (multi-modal prompting and adversarial training increase complexity), model complexity (learnable prompts and adaptive weights increase complexity)
- Failure signatures: Overfitting (model loses generalization), underfitting (fails to learn task-specific features), instability (adaptive consistency loss not properly tuned)
- First 3 experiments: 1) Verify multi-modal prompt learning improves feature alignment for adversarial examples vs uni-modal prompting 2) Test impact of adaptive consistency loss on overfitting by comparing CAPT with/without frozen CLIP guidance 3) Evaluate robustness-accuracy tradeoff by ablating TRADES-like consistency term

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the adaptive consistency weight αcons impact the trade-off between accuracy and robustness during different stages of training?
- Basis in paper: The paper introduces an adaptive weighting strategy that dynamically adjusts the weight based on the model's reliability, aiming to balance consistency between the frozen CLIP and the fine-tuning model.
- Why unresolved: The paper does not provide a detailed analysis of how the adaptive weight evolves during training or its specific impact on the model's performance at different training stages.
- What evidence would resolve it: Empirical analysis showing the evolution of αcons over training epochs and its correlation with changes in accuracy and robustness metrics.

### Open Question 2
- Question: How does the multi-modal prompt tuning in CAPT compare to uni-modal approaches in terms of computational efficiency and scalability to larger models?
- Basis in paper: The paper suggests that multi-modal prompt tuning enhances alignment between visual and textual features, but does not compare its computational efficiency or scalability to uni-modal methods.
- Why unresolved: The paper focuses on the effectiveness of multi-modal prompts but does not provide a comparative analysis of computational resources or scalability.
- What evidence would resolve it: Benchmarking studies comparing the training time, memory usage, and performance of CAPT with uni-modal approaches on models of varying sizes.

### Open Question 3
- Question: What is the impact of different perturbation budgets (ε) on the effectiveness of CAPT in various downstream tasks?
- Basis in paper: The paper evaluates CAPT under different perturbation budgets (ε=1/255 and 4/255) and shows its superiority, but does not explore a wider range of ε values or their impact on different tasks.
- Why unresolved: The paper limits its evaluation to two specific perturbation budgets, leaving the impact of other ε values unexplored.
- What evidence would resolve it: Experiments testing CAPT across a broader range of perturbation budgets and analyzing its performance across diverse downstream tasks.

### Open Question 4
- Question: How does CAPT perform under different types of adversarial attacks beyond PGD, such as gradient-free or query-based attacks?
- Basis in paper: The paper uses PGD attacks for both training and evaluation, but does not explore other attack methods that might reveal different vulnerabilities.
- Why unresolved: The paper's focus on PGD attacks limits understanding of CAPT's robustness against other attack types.
- What evidence would resolve it: Testing CAPT against various adversarial attack methods and comparing its robustness to other defense mechanisms.

## Limitations

- Uncertainty about whether improvements generalize beyond CLIP-ViT-B/32 backbone used in experiments
- Computational overhead of multi-modal prompt tuning not thoroughly analyzed for practical deployment
- Adaptive consistency mechanism relies on assumptions about frozen CLIP model's reliability that aren't empirically validated

## Confidence

- **High confidence**: Empirical improvements over baselines (16.91% accuracy, 7.51% robustness gains) well-supported by experimental results across multiple datasets and shot settings
- **Medium confidence**: Mechanism explanations for how multi-modal alignment and adaptive consistency prevent overfitting are plausible but not conclusively proven
- **Medium confidence**: Claims about in-distribution and out-of-distribution generalization superiority are supported by experiments but would benefit from additional dataset diversity

## Next Checks

1. Conduct ablation studies systematically disabling each mechanism (multi-modal prompting, adaptive consistency, TRADES-like loss) to quantify individual contributions to reported gains
2. Test CAPT's performance when initialized from larger CLIP backbones (ViT-L/14) to verify scalability and determine if overfitting patterns change with model capacity
3. Analyze the evolution of feature alignment metrics during training to empirically validate whether claimed alignment improvements between visual and textual features for adversarial examples actually occur and correlate with performance gains