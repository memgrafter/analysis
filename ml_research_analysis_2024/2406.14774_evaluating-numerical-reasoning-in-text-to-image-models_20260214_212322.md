---
ver: rpa2
title: Evaluating Numerical Reasoning in Text-to-Image Models
arxiv_id: '2406.14774'
source_url: https://arxiv.org/abs/2406.14774
tags:
- number
- task
- image
- images
- prompt
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a comprehensive evaluation of numerical reasoning
  in text-to-image generative models. The authors introduce GECKO NUM, a benchmark
  consisting of 1386 text prompts, 52,721 generated images, and 479,570 human annotations,
  designed to systematically test models' abilities in exact number generation, approximate
  number generation, and reasoning about partial quantities.
---

# Evaluating Numerical Reasoning in Text-to-Image Models

## Quick Facts
- arXiv ID: 2406.14774
- Source URL: https://arxiv.org/abs/2406.14774
- Reference count: 40
- Best model (DALL-E 3) achieves 45.2% accuracy on exact number generation and 48.7% on approximate number generation

## Executive Summary
This paper presents a comprehensive evaluation of numerical reasoning capabilities in text-to-image generative models. The authors introduce GECKO NUM, a benchmark consisting of 1386 text prompts, 52,721 generated images, and 479,570 human annotations, designed to systematically test models' abilities in exact number generation, approximate number generation, and reasoning about partial quantities. The evaluation reveals that even the most advanced models have only rudimentary numerical skills, with DALL-E 3 achieving only 45.2% accuracy on exact number generation tasks. The study demonstrates that current models struggle particularly with generating images containing more than 4 objects of the same type and understanding linguistic quantifiers like "a few," "many," and "no."

## Method Summary
The authors created GECKO NUM, a benchmark to evaluate numerical reasoning in text-to-image models across three tasks: exact number generation, approximate number generation, and reasoning about partial quantities. They generated images using seven text-to-image models (DALL-E 3, Imagen variants, and Muse variants) with five different seeds each. Human annotators then evaluated the generated images using three task-specific templates: counting objects, selecting the best description, and answering generated questions. The annotations were processed to calculate accuracy scores for each model on each task and prompt type, allowing for comprehensive analysis of performance patterns across different model families and prompt types.

## Key Results
- DALL-E 3 achieved 45.2% accuracy on exact number generation and 48.7% on approximate number generation, outperforming other models
- All models showed a consistent bias toward generating 4 objects when prompted for "a few," suggesting this is the learned interpretation
- Models struggled with prompts containing zero objects (e.g., "a watermelon with no seeds") and linguistic quantifiers like "many" and "a few"

## Why This Works (Mechanism)
Unknown: The paper does not provide a detailed explanation of the underlying mechanisms that explain why models perform well or poorly on numerical reasoning tasks.

## Foundational Learning
- **Numerical reasoning in text-to-image models** - Understanding how models interpret and generate numerical information from text prompts is crucial for evaluating their reasoning capabilities
- **Human annotation methodology** - Systematic collection of human judgments is necessary to establish ground truth for evaluating model outputs in subjective tasks
- **Benchmark design** - Creating comprehensive evaluation frameworks that cover different aspects of a capability (exact, approximate, partial quantities) provides a complete picture of model performance
- **Prompt engineering** - The way numerical information is expressed in prompts (exact numbers, linguistic quantifiers, partial quantities) significantly affects model performance
- **Cross-cultural numerical interpretation** - Different cultures may interpret numerical expressions differently, affecting the generalizability of results
- **Vision-language model evaluation** - Understanding how visual generation models compare to language models on counting tasks reveals important differences in their capabilities

## Architecture Onboarding

**Component Map:** Prompt generation -> Image generation (7 models) -> Human annotation collection -> Accuracy calculation -> Analysis

**Critical Path:** Text prompt → Image generation → Human evaluation → Accuracy score

**Design Tradeoffs:** The authors chose to use human annotations rather than automated metrics, sacrificing scalability for accuracy and reliability in evaluating subjective numerical reasoning tasks.

**Failure Signatures:** Models consistently fail on prompts with more than 4 objects, struggle with zero-quantity prompts, and show systematic biases in interpreting linguistic quantifiers.

**3 First Experiments:**
1. Generate images for all 1386 prompts using DALL-E 3 with five different seeds and collect human annotations for exact number generation task
2. Compare model performance on prompts with exact numbers (1-10) versus linguistic quantifiers ("a few," "many," "no")
3. Evaluate whether fine-tuning PaLIGemma on GECKO NUM improves its performance on both GECKO NUM and TallyQA benchmarks

## Open Questions the Paper Calls Out
### Open Question 1
- Question: What specific architectural or training modifications would be necessary to enable text-to-image models to reliably generate images with more than 10 objects of the same type?
- Basis in paper: [explicit] The paper demonstrates that even the best models (DALL-E 3) struggle with generating more than 4 objects accurately, and all models tend to overestimate or underestimate numbers beyond small quantities.
- Why unresolved: The paper shows the problem exists but doesn't explore potential solutions or modifications to address this limitation.
- What evidence would resolve it: A study comparing different model architectures, training procedures, or fine-tuning techniques specifically designed to improve large-number generation accuracy, with quantitative evaluation on a benchmark like GECKO NUM.

### Open Question 2
- Question: How does the quality and diversity of training data affect a model's ability to understand and generate approximate quantities and zero objects?
- Basis in paper: [explicit] The paper shows that models struggle with linguistic quantifiers like "a few," "many," and "no," and that DALL-E 3 consistently fails to generate images with zero objects (e.g., "a watermelon with no seeds").
- Why unresolved: The paper doesn't investigate whether the training data distribution or quality is a limiting factor in these capabilities.
- What evidence would resolve it: An ablation study comparing model performance on approximate quantities and zero after training on datasets with varying proportions of prompts containing these concepts.

### Open Question 3
- Question: What is the relationship between a model's ability to generate correct counts and its ability to answer visual counting questions (VQA)?
- Basis in paper: [explicit] The paper demonstrates that fine-tuning PaLIGemma on GECKO NUM improves its performance on both GECKO NUM and TallyQA, suggesting some relationship between these tasks.
- Why unresolved: The paper only shows correlation through transfer learning experiments, not establishing a causal relationship or understanding the underlying mechanisms.
- What evidence would resolve it: A comprehensive study directly comparing the counting abilities of text-to-image models and vision-language models on the same dataset, with analysis of where they succeed and fail differently.

## Limitations
- Results rely on human annotations which introduce potential subjectivity and cultural biases in interpreting numerical quantities
- Evaluation focuses on English-language prompts and Western cultural contexts, limiting generalizability to other languages and cultures
- Benchmark covers only three types of numerical reasoning tasks, potentially missing other important aspects of numerical understanding
- Performance differences may be influenced by varying training data and optimization objectives across evaluated models

## Confidence
- Model performance findings: **High** - Results are consistent across multiple evaluation methods and human annotators
- Benchmark utility claims: **Medium** - Supported by preliminary results but requires broader validation
- Cultural generalizability: **Low** - Limited testing across different cultural contexts and languages

## Next Checks
1. Test GECKO NUM across additional cultural contexts and languages to verify cross-cultural generalizability of results
2. Evaluate the benchmark's effectiveness with other types of vision-language models beyond text-to-image generators
3. Conduct controlled experiments to isolate the impact of training data composition versus architectural differences on numerical reasoning performance