---
ver: rpa2
title: 'MARS: Unleashing the Power of Variance Reduction for Training Large Models'
arxiv_id: '2411.10438'
source_url: https://arxiv.org/abs/2411.10438
tags:
- gradient
- training
- variance
- learning
- mars
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes MARS, a unified optimization framework that
  incorporates variance reduction into adaptive gradient methods. MARS leverages a
  scaled stochastic recursive momentum technique combined with preconditioned gradient
  updates to improve training efficiency for large models.
---

# MARS: Unleashing the Power of Variance Reduction for Training Large Models

## Quick Facts
- arXiv ID: 2411.10438
- Source URL: https://arxiv.org/abs/2411.10438
- Reference count: 16
- This paper proposes MARS, a unified optimization framework that incorporates variance reduction into adaptive gradient methods, achieving improved training efficiency for large models.

## Executive Summary
This paper introduces MARS (Momentumized Adaptive Recursive Second-order), a unified optimization framework that combines variance reduction techniques with adaptive gradient methods. The authors propose three instances of MARS based on AdamW, Lion, and Shampoo, demonstrating improved training efficiency for large language models. Through experiments on GPT-2 models, MARS consistently outperforms AdamW in terms of validation loss and downstream task performance while maintaining better token efficiency.

## Method Summary
MARS leverages a scaled stochastic recursive momentum (STORM) technique combined with preconditioned gradient updates. The framework computes gradients twice per step, using the difference between consecutive gradients to reduce variance without requiring full gradient computations. Three variants are introduced: MARS-AdamW with diagonal preconditioning, MARS-Lion with sign preconditioning, and MARS-Shampoo with matrix preconditioning. The method incorporates gradient clipping to maintain numerical stability and uses exponential moving averages for momentum.

## Key Results
- MARS achieved a validation loss of 2.53 on GPT-2 large compared to 2.56 for AdamW
- Improved downstream task accuracy on Hellaswag to 44.20% compared to 42.31% for AdamW
- Demonstrated better token efficiency through reduced computational steps while maintaining convergence

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Variance reduction via STORM momentum reduces stochastic gradient variance without requiring full gradient computations.
- Mechanism: The STORM update formula adds a gradient correction term that cancels common noise across consecutive steps, improving the gradient estimate quality.
- Core assumption: Parameters don't change drastically between consecutive steps, making the gradient difference term meaningful for variance reduction.
- Evidence anchors: [abstract] mentions "scaled stochastic recursive momentum technique"; [section 2] shows the gradient correction term formula; related papers focus on matrix variants rather than this specific variance reduction approach.

### Mechanism 2
- Claim: Preconditioning with second-order information accelerates convergence by adapting learning rates per parameter.
- Mechanism: The Hessian approximation matrix Ht transforms gradients to account for different curvature in parameter space, improving step directions.
- Core assumption: The preconditioning matrix accurately approximates the local geometry of the loss landscape.
- Evidence anchors: [abstract] mentions "preconditioned gradient methods"; [section 3.1] discusses minimizing second-order Taylor expansion; related paper "MARS-M" suggests matrix preconditioning variants exist.

### Mechanism 3
- Claim: Gradient clipping prevents explosion of the variance-reduced gradient estimator, maintaining training stability.
- Mechanism: The ect variable applies norm-based clipping to the combined stochastic gradient and correction term before momentum averaging.
- Core assumption: The scaled gradient correction term can occasionally produce large values that need clipping for numerical stability.
- Evidence anchors: [section 3.1] explicitly mentions gradient clipping on ct; [section 3.2.1] discusses clipping-by-norm; related papers don't discuss clipping in variance reduction context.

## Foundational Learning

- Concept: Stochastic gradient descent and its convergence properties
  - Why needed here: Understanding why variance reduction matters requires knowing that standard SGD has O(ε−4) gradient complexity
  - Quick check question: What is the gradient complexity of standard SGD for finding an ε-approximate first-order stationary point?

- Concept: Second-order optimization and Hessian matrix properties
  - Why needed here: The paper builds preconditioning on Hessian approximations, so understanding their role in Newton's method is crucial
  - Quick check question: How does the Newton update formula differ from standard gradient descent, and why is it more efficient?

- Concept: Exponential moving averages and momentum in optimization
  - Why needed here: The STORM momentum and the vt updates both rely on EMA, which affects how information is aggregated over time
  - Quick check question: What happens to the effective window of historical information as β approaches 1 in an EMA?

## Architecture Onboarding

- Component map: Sample data → Compute gradient twice → Apply variance reduction → Clip → Momentum update → Preconditioning → Parameter update
- Critical path: The variance reduction step (ct computation) is most critical - it directly affects training stability and efficiency
- Design tradeoffs:
  - Computational cost: Computing gradients twice per step vs. improved convergence
  - Memory: Storing momentum variables vs. performance gains
  - Hyperparameters: Tuning γt and preconditioning parameters vs. simplicity
- Failure signatures:
  - Training instability: Likely from improper γt or clipping threshold
  - Slow convergence: May indicate poor preconditioning matrix choice or insufficient variance reduction
  - Memory issues: Excessive momentum storage for large models
- First 3 experiments:
  1. Compare training loss curves with γt = 0 (AdamW baseline) vs. γt = 0.025 (recommended) on GPT-2 small
  2. Test gradient clipping threshold sensitivity by varying the 1.0 threshold in the ect computation
  3. Validate preconditioning matrix choice by comparing AdamW-style diagonal vs. Shampoo-style full matrix on a medium-sized model

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the scaling parameter γt affect the convergence rate and final performance of MARS across different model sizes and datasets?
- Basis in paper: The paper states "In practice,γt is often set between0 and1" and provides experimental results comparing different γ-schedules on GPT-2 small.
- Why unresolved: The paper only tests a limited set of γ values (0.025, 0.05) and schedules on one model size.
- What evidence would resolve it: Systematic experiments varying γ across multiple model sizes and datasets, potentially using adaptive methods to adjust γ during training.

### Open Question 2
- Question: Can MARS be effectively combined with other variance reduction techniques beyond STORM, such as SVRG or SARAH, to further improve training efficiency?
- Basis in paper: The paper presents MARS as a unified framework that "accommodates all existing full matrix or diagonal Hessian approximations" but only explores STORM-based implementations.
- Why unresolved: The paper focuses exclusively on STORM momentum and doesn't investigate whether other variance reduction techniques could be integrated into the MARS framework.
- What evidence would resolve it: Experimental comparisons of MARS variants using different variance reduction techniques on the same benchmarks, measuring both convergence speed and final performance.

### Open Question 3
- Question: What is the theoretical convergence guarantee of MARS for non-convex optimization, and how does it compare to existing adaptive methods like AdamW?
- Basis in paper: While the paper mentions that STORM achieves "nearly optimal gradient complexity ofO(ε−3) for non-convex and smooth optimization problems," it doesn't provide specific convergence analysis for MARS.
- Why unresolved: The paper focuses on empirical validation but doesn't provide theoretical convergence guarantees for the MARS framework.
- What evidence would resolve it: Formal convergence analysis proving gradient complexity bounds for MARS under various preconditioning schemes, with comparison to theoretical guarantees for AdamW and other adaptive methods.

## Limitations
- Experiments were limited to models up to 1.5B parameters, leaving scalability questions for truly massive models
- Only tested on a single dataset (OpenWebText) and model architecture (GPT-2), limiting generalizability claims
- Computational overhead of computing gradients twice per step may offset efficiency gains for certain hardware configurations

## Confidence

- **High confidence**: The mathematical derivation of the variance reduction technique and its integration with preconditioning methods is sound and well-explained
- **Medium confidence**: The experimental results showing improved validation loss and downstream task performance are reproducible based on provided implementation details
- **Low confidence**: Claims about general applicability across diverse model architectures and datasets beyond the GPT-2/OpenWebText combination

## Next Checks

1. Test MARS on vision transformer models (e.g., ViT) on ImageNet to verify cross-domain applicability and compare against established optimizers like AdamW and Lion
2. Conduct ablation studies varying the gradient correction parameter γt across a wider range (0.001 to 0.1) to determine optimal settings for different model sizes
3. Measure wall-clock training time and GPU memory consumption for MARS vs AdamW on models of increasing scale (125M to 13B parameters) to quantify practical efficiency trade-offs