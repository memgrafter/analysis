---
ver: rpa2
title: Multimodal Metadata Assignment for Cultural Heritage Artifacts
arxiv_id: '2406.00423'
source_url: https://arxiv.org/abs/2406.00423
tags:
- text
- classification
- class
- classifier
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper presents a multimodal classification approach for cultural
  heritage artifacts, specifically silk fabrics. The authors develop a system that
  predicts missing metadata (such as production place, time period, technique, and
  material) using three modalities: images, text descriptions, and tabular data.'
---

# Multimodal Metadata Assignment for Cultural Heritage Artifacts

## Quick Facts
- arXiv ID: 2406.00423
- Source URL: https://arxiv.org/abs/2406.00423
- Reference count: 40
- Primary result: Multimodal approach achieves 74.2% average F1 score vs 55.6% for best single modality

## Executive Summary
This paper presents a multimodal classification system for predicting missing metadata of cultural heritage artifacts, specifically silk fabrics from European museums. The system combines image, text, and tabular data modalities using a late fusion approach where individual classifiers are trained separately then combined. The authors introduce a novel dataset of over 28,000 records with multiple images and multilingual text descriptions, addressing class imbalance through focal loss. The multimodal approach significantly outperforms single-modality methods and integrates predictions into a knowledge graph for visualization through the ADASilk exploratory search engine.

## Method Summary
The authors employ a late fusion approach where three modality-specific classifiers are trained independently: a CNN with ResNet backbone for images, a multilingual transformer (XLM-Roberta) for text descriptions, and Gradient Tree Boosting for tabular metadata. Each classifier uses focal loss to handle class imbalance, focusing training on hard examples from underrepresented classes. The individual modality predictions are then combined using another Gradient Tree Boosting model. The system predicts four metadata properties (production place, time period, technique, material) across a dataset of 28,000 silk fabric records from 12 museums, with text descriptions in English, Spanish, French, and Catalan.

## Key Results
- Multimodal classifier achieves 74.2% average F1 score across all tasks
- Best single-modality performance is 55.6% F1 score
- Focal loss improves classification of underrepresented classes
- Knowledge graph integration enables rich representation of predictions with provenance tracking

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Focal loss improves classification of underrepresented classes by focusing training on hard examples.
- Mechanism: Penalizes easy-to-classify examples less and emphasizes harder examples, which are likely from minority classes.
- Core assumption: Hard examples correspond to underrepresented classes in imbalanced datasets.
- Evidence anchors:
  - [abstract] "use the focal loss to handle class imbalance"
  - [section] "We adapt the training of the network weights such that hard training examples get a higher impact on the weight updates"
  - [corpus] Weak: no direct corpus evidence on focal loss effectiveness in cultural heritage
- Break condition: If hard examples are not from underrepresented classes, focal loss may not help.

### Mechanism 2
- Claim: Multimodal fusion outperforms single modalities by combining complementary information.
- Mechanism: Late fusion combines predictions from image, text, and tabular classifiers, allowing each modality to specialize while the fusion model leverages all available information.
- Core assumption: Each modality captures different aspects of the data, and combining them improves overall accuracy.
- Evidence anchors:
  - [abstract] "multimodal approach providing the best results"
  - [section] "multimodal classifier very significantly outperforms all single modality classifiers (74.2% vs 55.6%)"
  - [corpus] Weak: no direct corpus evidence on late fusion in cultural heritage
- Break condition: If modalities are redundant or noisy, fusion may not improve results.

### Mechanism 3
- Claim: Knowledge graph integration enables rich representation and visualization of predictions.
- Mechanism: Predictions are stored as RDF triples using PROV ontology, allowing tracking of provenance, confidence scores, and integration with existing metadata.
- Core assumption: Knowledge graphs provide a standardized way to represent complex relationships and metadata.
- Evidence anchors:
  - [abstract] "leveraged specific data models and taxonomy in a Knowledge Graph to create the dataset and to store classification results"
  - [section] "predictions can be properly represented within our data model with appropriate classes and properties"
  - [corpus] Weak: no direct corpus evidence on KG integration in cultural heritage
- Break condition: If knowledge graph structure is too rigid or complex, integration may be difficult.

## Foundational Learning

- Concept: Convolutional Neural Networks (CNNs) for image classification
  - Why needed here: Images of silk fabrics need to be classified into categories like material, technique, etc.
  - Quick check question: What is the role of convolutional layers in a CNN?

- Concept: Transformer-based models for text classification
  - Why needed here: Text descriptions of silk fabrics need to be classified into the same categories as images.
  - Quick check question: How does a transformer model handle multilingual text?

- Concept: Knowledge graphs and ontologies
  - Why needed here: Predictions need to be integrated with existing metadata in a structured way.
  - Quick check question: What is the difference between a taxonomy and an ontology?

## Architecture Onboarding

- Component map: Image classifier (CNN+ResNet) -> Text classifier (XLM-R) -> Tabular classifier (XGBoost) -> Multimodal classifier (XGBoost) -> Knowledge graph (RDF+PROV)
- Critical path: Image/text/tabular classification → Multimodal fusion → Knowledge graph integration
- Design tradeoffs:
  - Multitask vs single-task learning: Multitask may improve performance but increases complexity
  - Focal loss vs other imbalance handling: Focal loss focuses on hard examples but may be sensitive to label noise
  - Late fusion vs early fusion: Late fusion allows each modality to specialize but may miss early interactions
- Failure signatures:
  - Poor performance on underrepresented classes: Check focal loss implementation and label distribution
  - Inconsistent predictions across modalities: Check for label noise and modality-specific preprocessing
  - Knowledge graph integration issues: Check ontology mapping and RDF triple generation
- First 3 experiments:
  1. Train image classifier with and without focal loss, compare F1 scores on validation set
  2. Train text classifier with and without focal loss, compare F1 scores on validation set
  3. Train multimodal classifier with different combinations of modalities, compare F1 scores on validation set

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can self-supervised learning approaches improve metadata prediction performance in cultural heritage domains where labeled data is scarce?
- Basis in paper: [explicit] The authors note that supervised data scarcity is likely to remain a challenge and suggest that future work should lean more heavily into self-supervised learning.
- Why unresolved: The paper does not explore or implement any self-supervised learning techniques, focusing instead on supervised approaches with limited labeled data.
- What evidence would resolve it: Implementation and evaluation of self-supervised pre-training methods on cultural heritage datasets, comparing their performance against purely supervised approaches.

### Open Question 2
- Question: What is the impact of label noise on multimodal classification performance, and how can it be mitigated?
- Basis in paper: [explicit] The authors identify label noise as a potential issue, particularly in the material and technique tasks, and hypothesize it may interact negatively with the focal loss.
- Why unresolved: The paper does not systematically analyze or address label noise beyond acknowledging its presence.
- What evidence would resolve it: Experiments comparing classification performance with and without label noise, using techniques like label smoothing or robust loss functions.

### Open Question 3
- Question: How does the fusion of predictions from multiple modalities compare to direct joint learning of all modalities?
- Basis in paper: [inferred] The authors use a late fusion approach and suggest exploring different approaches to integrating predictions from multiple modalities.
- Why unresolved: The paper only implements late fusion and does not compare it to other fusion strategies like early or intermediate fusion.
- What evidence would resolve it: Implementation and evaluation of different fusion strategies (early, intermediate, late) on the same dataset, comparing their performance and computational efficiency.

## Limitations
- Dataset composition is limited to silk fabrics from European collections, limiting generalizability
- Lack of comparative analysis against other class imbalance techniques beyond weight rescaling
- Late fusion approach may miss benefits from early or hybrid fusion strategies that capture cross-modal interactions earlier

## Confidence
*High Confidence:* The overall multimodal classification framework and its superior performance over single-modality approaches (74.2% vs 55.6% F1 score) are well-supported by the experimental results. The knowledge graph integration methodology and the ADASilk visualization system are clearly described and technically sound.

*Medium Confidence:* The specific implementation details of the focal loss variants used for both image and text classifiers are somewhat sparse, making exact reproduction challenging. The paper mentions that focal loss helps with class imbalance but doesn't provide detailed analysis of how different gamma parameters affect performance across various minority classes.

*Low Confidence:* The scalability claims for the system are not empirically validated. While the authors mention the potential for larger datasets, no experiments demonstrate performance degradation or computational requirements as dataset size increases significantly beyond the current 28,000 records.

## Next Checks
1. **Cross-domain generalization test:** Apply the trained multimodal classifier to artifacts from different cultural heritage domains (e.g., pottery, paintings) to assess how well the model generalizes beyond silk fabrics, measuring F1 score drops across different artifact categories.

2. **Focal loss parameter sensitivity analysis:** Systematically vary the gamma parameter in focal loss for both image and text classifiers across underrepresented classes, documenting how different values affect minority class F1 scores versus overall performance.

3. **Fusion strategy comparison:** Implement and evaluate early fusion and hybrid fusion approaches alongside the current late fusion method, comparing computational efficiency and classification accuracy across all four metadata properties to determine if earlier fusion could capture complementary information more effectively.