---
ver: rpa2
title: 'REValueD: Regularised Ensemble Value-Decomposition for Factorisable Markov
  Decision Processes'
arxiv_id: '2401.08850'
source_url: https://arxiv.org/abs/2401.08850
tags:
- revalued
- learning
- decqn
- ensemble
- performance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of scaling deep reinforcement
  learning to high-dimensional discrete action spaces, specifically factorizable Markov
  Decision Processes (FMDPs). The proposed method, REValueD, leverages value-decomposition
  to independently learn utility values for each sub-action space, reducing the action
  space complexity.
---

# REValueD: Regularised Ensemble Value-Decomposition for Factorisable Markov Decision Processes

## Quick Facts
- arXiv ID: 2401.08850
- Source URL: https://arxiv.org/abs/2401.08850
- Reference count: 40
- Primary result: REValueD outperforms DecQN and BDQ in humanoid and dog tasks with many sub-action spaces in the DeepMind Control Suite.

## Executive Summary
This paper addresses the challenge of scaling deep reinforcement learning to high-dimensional discrete action spaces in factorizable Markov Decision Processes (FMDPs). The proposed method, REValueD, combines value-decomposition with an ensemble of critics and a regularisation loss to mitigate overestimation bias, control target variance, and address credit assignment issues. REValueD demonstrates superior performance compared to DecQN and BDQ in challenging tasks from the DeepMind Control Suite, particularly in humanoid and dog tasks with many sub-action spaces.

## Method Summary
REValueD leverages value-decomposition to independently learn utility values for each sub-action space, reducing the action space complexity. To mitigate the increased target variance introduced by this decomposition, REValueD employs an ensemble of critics. Additionally, a regularisation loss is introduced to address the credit assignment issue, where exploratory actions in one dimension can negatively impact the value of optimal actions in other dimensions. The method is compared against DecQN and BDQ baselines on discretized versions of DeepMind Control Suite and MetaWorld tasks.

## Key Results
- REValueD outperforms DecQN and BDQ in humanoid and dog tasks with many sub-action spaces.
- Ablation studies confirm the importance of both the ensemble and regularisation components.
- REValueD demonstrates robustness to increasing sub-action space sizes.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The DecQN decomposition reduces overestimation bias but increases target variance.
- Mechanism: By decomposing Q-values into mean utilities across sub-actions, the max operator operates on smaller, bounded utility ranges, reducing the bias from function approximation error. However, averaging introduces variance because the max over multiple independent uniform noise sources is noisier than a single max.
- Core assumption: Function approximation errors are uniformly distributed and independent across sub-actions.
- Evidence anchors:
  - [abstract] states "whilst it curtails the over-estimation bias inherent to Q-learning algorithms, it amplifies target variance."
  - [section 4] provides Theorem 1 proving both effects mathematically.
  - [corpus] lacks direct comparison studies but cites foundational Q-learning overestimation literature.
- Break condition: If sub-actions are correlated or errors are not uniformly distributed, the variance reduction from ensembles may not hold.

### Mechanism 2
- Claim: Ensembling critics reduces target variance without affecting bias.
- Mechanism: Multiple critics provide independent estimates of the utility mean; averaging them reduces variance by a factor of 1/K while preserving the expected value.
- Core assumption: Each critic's noise is independent and identically distributed.
- Evidence anchors:
  - [section 4] Theorem 2 shows Var(Z_ens) = 1/K Var(Z_dec).
  - [section 5] experiments with varying ensemble sizes confirm performance gains.
  - [corpus] cites prior ensemble use for exploration and variance reduction.
- Break condition: If critics share parameters or training data causes correlation, variance reduction will be less effective.

### Mechanism 3
- Claim: Regularization loss mitigates credit assignment issues by discouraging large utility changes when rewards are influenced by other sub-actions.
- Mechanism: The Huber loss between target utilities and current utilities is weighted by |δi|; large TD errors imply sub-action influence from others, so weights increase to stabilize updates.
- Core assumption: Large |δi| indicates sub-action interference rather than true utility improvement.
- Evidence anchors:
  - [abstract] explains "exploratory actions in one dimension can have a negative influence on the value of optimal actions in other dimensions."
  - [section 4] Equation (4.4) defines the regularization with adaptive weights.
  - [section 5] ablation studies show performance drops without regularization in complex tasks.
- Break condition: If the weighting function misclassifies exploratory effects as interference, optimal sub-actions may be under-updated.

## Foundational Learning

- Concept: Q-learning overestimation bias
  - Why needed here: Understanding bias reduction motivates the DecQN decomposition.
  - Quick check question: What causes overestimation bias in Q-learning with function approximation?
- Concept: Value decomposition in multi-agent RL
  - Why needed here: The paper leverages MARL paradigms for single-agent FMDPs.
  - Quick check question: How does centralized training with decentralized execution apply to FMDPs?
- Concept: Ensemble methods in RL
  - Why needed here: Ensembles are used to control variance introduced by decomposition.
  - Quick check question: What is the variance reduction factor when averaging K independent estimators?
- Concept: Credit assignment problem
  - Why needed here: Regularization addresses interference between sub-actions.
  - Quick check question: Why is credit assignment harder in FMDPs than standard MDPs?

## Architecture Onboarding

- Component map:
  - Input: State vector
  - Utility networks: K critics × N sub-action utility heads
  - Ensemble mean: Average utilities across critics for target computation
  - Regularization: Huber loss between current and target utilities, weighted by |δi|
  - Loss: Sum of DecQN TD loss and weighted regularization loss
- Critical path:
  1. Sample batch from replay buffer
  2. Forward pass through K critics to get utilities
  3. Compute ensemble mean utilities
  4. Calculate TD targets using ensemble mean
  5. Compute DecQN loss and regularization loss
  6. Backpropagate combined loss
  7. Update target networks with Polyak averaging
- Design tradeoffs:
  - Ensemble size K: Larger K reduces variance but increases compute and memory.
  - Regularization weight β: Higher β stabilizes but may slow learning.
  - Exploration strategy: ϵ-greedy with critic sampling vs. full ensemble averaging.
- Failure signatures:
  - High variance in returns: Ensemble too small or regularization too weak.
  - Slow learning: Regularization too strong or ensemble too large.
  - Instability: Sub-actions interfering; increase β or adjust weighting function.
- First 3 experiments:
  1. Compare DecQN vs. DecQN+Ensemble on walker-run task; check variance reduction.
  2. Vary ensemble size K=1,3,5,10; measure performance and training stability.
  3. Toggle regularization (β=0 vs β=0.5) in humanoid-stand; assess credit assignment effects.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the choice of value-decomposition (mean vs. sum) impact the performance of REValueD in FMDPs with varying levels of noise or uncertainty in the reward signal?
- Basis in paper: [explicit] The paper analyzes the theoretical bias-variance tradeoff of DecQN's mean decomposition vs. a sum decomposition, showing that the sum decomposition increases overestimation bias compared to the mean. It also compares DecQN with a sum decomposition to DecQN and REValueD empirically.
- Why unresolved: The theoretical analysis and empirical comparison only consider deterministic environments. The impact of the decomposition choice on performance in stochastic environments is not investigated.
- What evidence would resolve it: Empirical results comparing the performance of REValueD using the mean vs. sum decomposition in stochastic FMDPs with varying levels of reward noise.

### Open Question 2
- Question: How does the size of the ensemble (K) in REValueD affect the exploration-exploitation tradeoff in FMDPs with sparse rewards or delayed rewards?
- Basis in paper: [explicit] The paper analyzes how the ensemble size affects performance in DM Control Suite tasks, finding that larger ensembles generally improve performance in more complex tasks. It also mentions that some target variance can be useful for exploration.
- Why unresolved: The analysis only considers tasks with relatively dense rewards. The impact of ensemble size on exploration-exploitation tradeoff in tasks with sparse or delayed rewards is not investigated.
- What evidence would resolve it: Empirical results comparing the performance of REValueD with different ensemble sizes in FMDPs with sparse or delayed rewards, measuring both final performance and sample efficiency.

### Open Question 3
- Question: How does the weighting function in the regularisation loss of REValueD affect its performance in FMDPs with different levels of sub-action space interdependencies?
- Basis in paper: [explicit] The paper introduces a regularisation loss to mitigate the credit assignment issue in FMDPs, where exploratory actions in one dimension can negatively impact the value of optimal actions in other dimensions. It uses an exponential weighting function based on the TD error.
- Why unresolved: The paper only uses one specific weighting function and does not investigate the impact of different functional forms on performance in FMDPs with varying levels of sub-action space interdependencies.
- What evidence would resolve it: Empirical results comparing the performance of REValueD with different weighting functions (e.g., quadratic, linear) in FMDPs with varying levels of sub-action space interdependencies, measured by the correlation between sub-actions' utilities.

## Limitations
- REValueD's performance may be sensitive to the ensemble size (K) and regularisation coefficient (β), requiring extensive hyperparameter tuning.
- The effectiveness of the regularisation loss in mitigating credit assignment issues relies on the assumption that large TD errors indicate sub-action interference rather than true utility improvement.
- The decomposition of Q-values into mean utilities across sub-actions may not be optimal for all FMDPs, especially those with correlated sub-actions or non-uniform function approximation errors.

## Confidence

- Claim: REValueD reduces overestimation bias while controlling target variance.
  - Confidence: Medium
- Claim: The ensemble of critics reduces target variance.
  - Confidence: High
- Claim: The regularisation loss mitigates credit assignment issues.
  - Confidence: Medium

## Next Checks
1. Investigate the sensitivity of REValueD to the ensemble size (K) and regularisation coefficient (β) by conducting a grid search over these hyperparameters and evaluating the performance across different tasks.
2. Examine the effectiveness of the regularisation loss in mitigating credit assignment issues by comparing REValueD's performance with and without regularisation on tasks with varying levels of sub-action interference.
3. Explore alternative value decomposition methods, such as factorized Q-functions or hierarchical Q-learning, and compare their performance to REValueD on a range of FMDPs to assess the generalizability of the proposed approach.