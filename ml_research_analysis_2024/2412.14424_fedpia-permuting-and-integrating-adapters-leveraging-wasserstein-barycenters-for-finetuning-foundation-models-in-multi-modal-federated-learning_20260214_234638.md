---
ver: rpa2
title: FedPIA -- Permuting and Integrating Adapters leveraging Wasserstein Barycenters
  for Finetuning Foundation Models in Multi-Modal Federated Learning
arxiv_id: '2412.14424'
source_url: https://arxiv.org/abs/2412.14424
tags:
- adapter
- client
- adapters
- fedpia
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of fine-tuning large Vision-Language
  Models (VLMs) in multi-modal Federated Learning (FL) settings, particularly in healthcare,
  where data privacy regulations limit data sharing. The authors propose FedPIA, a
  novel framework that improves the naive combination of FL and Parameter-Efficient
  Fine-Tuning (PEFT) by introducing Permutation and Integration of Adapters in both
  the server and clients using Wasserstein Barycenters.
---

# FedPIA -- Permuting and Integrating Adapters leveraging Wasserstein Barycenters for Finetuning Foundation Models in Multi-Modal Federated Learning

## Quick Facts
- arXiv ID: 2412.14424
- Source URL: https://arxiv.org/abs/2412.14424
- Authors: Pramit Saha; Divyanshu Mishra; Felix Wagner; Konstantinos Kamnitsas; J. Alison Noble
- Reference count: 12
- Primary result: Achieved 3.89% and 5.18% mean improvement for VQA tasks and outperformed full fine-tuning in some scenarios

## Executive Summary
This paper addresses the challenge of fine-tuning large Vision-Language Models (VLMs) in multi-modal Federated Learning (FL) settings, particularly in healthcare, where data privacy regulations limit data sharing. The authors propose FedPIA, a novel framework that improves the naive combination of FL and Parameter-Efficient Fine-Tuning (PEFT) by introducing Permutation and Integration of Adapters in both the server and clients using Wasserstein Barycenters. This approach bridges the gap between adapters trained on diverse data distributions and tasks, enhancing knowledge integration. FedPIA was evaluated across five FL task settings using 48 medical image datasets and two VLM backbones (ViLT and ALBEF), demonstrating consistent performance improvements over state-of-the-art PEFT-FL baselines.

## Method Summary
FedPIA introduces a novel framework for fine-tuning large VLMs in federated learning settings by leveraging Parameter-Efficient Fine-Tuning (PEFT) techniques with adapter layers. The key innovation is the Permutation and Integration of Adapters (PIA) mechanism that operates at both server and client levels. At the server, client adapters are aligned using Wasserstein Barycenters to match the global adapter's neuron order before integration. At the client level, the global adapter is aligned to the local adapter based on activation similarity before combining them. The framework uses dynamic integration with exponential weighting to stabilize the aggregation of aligned adapters, addressing the challenge of diverse data distributions and tasks across clients.

## Key Results
- Achieved overall mean improvement of 3.89% and 5.18% for Visual Question Answering tasks compared to state-of-the-art PEFT-FL baselines
- Outperformed full fine-tuning in some scenarios, demonstrating the effectiveness of the parameter-efficient approach
- Demonstrated consistent performance improvements across 48 medical image datasets and five different FL task settings
- Showed robustness to various heterogeneity conditions including heterogeneous client data, models, and tasks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Permutation via Wasserstein Barycenters aligns diverse adapter neurons before integration, reducing misalignment in parameter space.
- Mechanism: In the server, each client adapter's neurons are permuted to match the global adapter's neuron order using Wasserstein Barycenters. This creates a one-to-one correspondence in parameter space, enabling effective averaging. The cost metric is the Euclidean distance between incoming weight vectors of adapter neurons.
- Core assumption: Adapter neurons are permutation-invariant—meaning their order does not affect the model's output as long as the connections remain consistent.
- Evidence anchors:
  - [abstract] "This layerwise permutation helps to bridge the gap in the parameter space of local and global adapters before integration."
  - [section] "Owing to the permutation invariance property, these adapters lack one-to-one correspondence, which is crucial for effective information fusion."
  - [corpus] Weak: The cited corpus papers focus on Wasserstein Barycenters for distribution averaging, but not specifically for neural adapter permutation. This is a novel application.
- Break condition: If adapter neurons are not permutation-invariant (e.g., due to non-symmetric operations), the alignment would be invalid and could degrade performance.

### Mechanism 2
- Claim: Client-level PIA aligns global adapter to client-specific adapter using activation similarity, improving adaptation to local data distributions.
- Mechanism: In each client, the global adapter is permuted to match the local adapter by computing the permutation matrix based on neuron activation similarity over a batch of samples. This aligns neurons that produce similar outputs, not just similar weights.
- Core assumption: Neurons with similar activations for a given input are functionally equivalent and should be aligned for better integration.
- Evidence anchors:
  - [abstract] "Furthermore, in order to better integrate client-specific and client-agnostic knowledge in the clients, we permute the weights of the global adapter in each client and bring it closer to client-specific adapter in the weight space before combining them."
  - [section] "In other words, neurons across local and global adapters in each client would be considered similar if they yield similar activations for a given instance."
  - [corpus] Weak: No direct corpus evidence for activation-based adapter alignment. This is a novel mechanism.
- Break condition: If activation patterns are highly dynamic or non-stationary across batches, the alignment may become unstable and hurt convergence.

### Mechanism 3
- Claim: Dynamic integration using Wasserstein Barycenters with an exponential weighting term stabilizes the aggregation of aligned adapters.
- Mechanism: After permutation, client adapters are integrated into the global adapter using a weighted sum where weights depend on the Wasserstein distance between the aligned adapters. The exponential term (exp(-γ||fW_k - WG||²)) downweights adapters that are farther from the current global adapter, improving stability.
- Core assumption: Adapters closer in parameter space after permutation should contribute more to the global adapter to prevent instability.
- Evidence anchors:
  - [section] "The aligned K adapters are then integrated dynamically to form the global adapter as: fW_G = (1/K) Σ_k fW_k exp(-γ||fW_k - WG||²)"
  - [corpus] Weak: The corpus discusses Wasserstein Barycenters for distribution averaging, but not specifically with exponential weighting for neural adapters. This dynamic weighting is a novel stabilization mechanism.
- Break condition: If γ is too large, the integration becomes overly conservative and may ignore useful diverse information; if too small, instability may return.

## Foundational Learning

- Concept: Wasserstein Barycenters and Earth Mover's Distance
  - Why needed here: Used to compute optimal permutations and weighted averages of adapter parameters in a geometry-aware way.
  - Quick check question: What is the key difference between averaging parameters directly vs. using Wasserstein Barycenters?

- Concept: Permutation Invariance in Neural Networks
  - Why needed here: Justifies why rearranging neurons (with consistent connections) doesn't change model behavior, enabling alignment.
  - Quick check question: Under what conditions are neural network parameters considered permutation-invariant?

- Concept: Federated Learning and Data Heterogeneity
  - Why needed here: The framework operates in a setting where clients have non-IID data and limited resources, requiring parameter-efficient and privacy-preserving methods.
  - Quick check question: How does data heterogeneity across clients affect the convergence of federated learning?

## Architecture Onboarding

- Component map:
  - Clients: Local adapter (trained on client data) -> Global adapter (received from server) -> PIA module (for alignment and integration)
  - Server: Client adapters (received) -> PIA module (for permutation and integration) -> Global adapter (broadcast to clients)
  - PIA Module: Permutation computation (Wasserstein Barycenter) -> Alignment -> Dynamic integration

- Critical path:
  1. Clients train local adapters on local data
  2. Clients send local adapters to server
  3. Server permutes and integrates adapters into global adapter
  4. Server sends global adapter to clients
  5. Clients permute global adapter to match local adapter
  6. Clients integrate and train on local data

- Design tradeoffs:
  - Permutation adds computational overhead but enables better integration
  - Activation-based alignment is more adaptive but requires extra forward passes
  - Dynamic integration with exponential weighting adds stability but introduces a hyperparameter (γ)

- Failure signatures:
  - If permutation is incorrect, adapters may not align properly, leading to degraded performance
  - If activation-based alignment is unstable, client training may oscillate
  - If γ is mis-tuned, integration may be too conservative or too unstable

- First 3 experiments:
  1. Implement and test permutation alignment on a simple two-client setup with synthetic data to verify neuron correspondence
  2. Compare performance with and without client-level PIA activation alignment on a single client to measure adaptation benefit
  3. Test dynamic integration with different γ values on a small federated setup to find the optimal balance between stability and diversity

## Open Questions the Paper Calls Out

The paper does not explicitly call out specific open questions, but several important areas for future research emerge from the work:

- The optimal balance between server-level and client-level permutation/integration operations in terms of communication efficiency and model performance
- The impact of different Wasserstein distance metrics on alignment quality and convergence compared to alternative permutation strategies
- The framework's performance in federated learning scenarios with heterogeneous model architectures across clients

## Limitations

- The effectiveness of activation-based alignment depends heavily on the stability of activation patterns across batches, which may not hold in highly dynamic or non-stationary data distributions
- The exponential weighting term γ is a critical hyperparameter that significantly affects performance, with sensitivity across different task types and data distributions not extensively explored
- Computational overhead from permutation computation using Wasserstein Barycenters may impact practicality in resource-constrained federated learning environments

## Confidence

- **High Confidence**: The mechanism of server-level permutation and integration using Wasserstein Barycenters to align diverse adapter neurons is well-grounded in the permutation invariance property of neural network parameters and has strong empirical support from the experimental results
- **Medium Confidence**: The client-level activation-based alignment and integration mechanism shows promise in improving adaptation to local data distributions, but its effectiveness may vary depending on data characteristics and requires further validation across diverse scenarios
- **Medium Confidence**: The overall performance improvements reported (3.89% and 5.18% for VQA tasks) are significant, but the extent of improvement may be task-specific and dependent on the degree of data heterogeneity and the choice of hyperparameters

## Next Checks

1. **Activation Alignment Stability Test**: Implement a controlled experiment where activation-based alignment is tested on a single client with varying batch compositions to assess the stability of alignment across different data distributions and its impact on convergence.

2. **Hyperparameter Sensitivity Analysis**: Conduct a comprehensive grid search over the exponential weighting term γ and other critical hyperparameters (e.g., learning rate, batch size) to quantify their impact on performance across different task types and data heterogeneity levels.

3. **Cross-Domain Generalization Test**: Evaluate FedPIA on a non-medical vision-language dataset (e.g., COCO, Flickr30k) to assess its generalizability beyond the medical domain and identify any domain-specific limitations or adaptations needed.