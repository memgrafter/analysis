---
ver: rpa2
title: Spiking Convolutional Neural Networks for Text Classification
arxiv_id: '2406.19230'
source_url: https://arxiv.org/abs/2406.19230
tags:
- snns
- neural
- networks
- accuracy
- time
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a "conversion + fine-tuning" approach for training
  spiking neural networks (SNNs) for text classification tasks. The method involves
  converting a pre-trained convolutional neural network (CNN) to an SNN and then fine-tuning
  the SNN using surrogate gradients.
---

# Spiking Convolutional Neural Networks for Text Classification

## Quick Facts
- arXiv ID: 2406.19230
- Source URL: https://arxiv.org/abs/2406.19230
- Reference count: 30
- Primary result: Proposed "conversion + fine-tuning" approach achieves comparable accuracy to CNNs with lower energy consumption for text classification.

## Executive Summary
This paper introduces a novel "conversion + fine-tuning" method for training spiking neural networks (SNNs) on text classification tasks. The approach combines conversion of pre-trained CNNs to SNNs with fine-tuning using surrogate gradients, while encoding pre-trained word embeddings as spike trains. Experimental results on six text classification datasets demonstrate that the proposed SNNs achieve accuracy comparable to traditional CNNs while consuming significantly less energy. The SNNs also show improved robustness against adversarial attacks compared to their CNN counterparts.

## Method Summary
The method involves two main steps: conversion and fine-tuning. First, a tailored CNN is trained on text classification tasks using positive-valued word embeddings, ReLU activations, and average pooling. This CNN is then converted to an SNN by duplicating its architecture and weights, with the addition of a spike generator for input embeddings. The converted SNN is subsequently fine-tuned using surrogate gradients (Fast-Sigmoid) with backpropagation through time. The spike encoding of word embeddings preserves semantic information from large-scale pre-training while enabling SNN processing.

## Key Results
- SNNs achieve comparable accuracy to CNNs on six text classification datasets while consuming significantly less energy.
- The "conversion + fine-tuning" approach bridges the accuracy gap between SNNs and traditional deep learning models for NLP tasks.
- SNNs demonstrate improved robustness against adversarial attacks compared to CNNs due to their inherent temporal variability and spike non-differentiability.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Conversion + fine-tuning bridges SNNs and DNNs for text classification.
- Mechanism: Pre-trained CNNs are converted to SNNs by duplicating architecture and weights, then fine-tuned using surrogate gradients. This combines the accuracy of conversion-based approaches with the efficiency of spike-based training.
- Core assumption: The initial conversion preserves most of the learned decision boundaries, making fine-tuning sufficient for performance recovery.

### Mechanism 2
- Claim: Pre-trained word embeddings are encoded as spike trains to leverage prior knowledge.
- Mechanism: Word embeddings are normalized, shifted to positive values, and converted to Poisson spike trains. This allows SNNs to benefit from large-scale pre-training without losing semantic information.
- Core assumption: The temporal variability introduced by Poisson encoding does not significantly harm the learned semantic relationships.

### Mechanism 3
- Claim: SNNs are more robust to adversarial attacks due to spike non-differentiability and temporal variability.
- Mechanism: Random Poisson spike generation introduces inherent noise tolerance, making it harder for adversarial perturbations to consistently mislead the model.
- Core assumption: Temporal variability masks targeted adversarial patterns that rely on precise activation values.

## Foundational Learning

- Concept: Spiking neuron dynamics (leaky integrate-and-fire).
  - Why needed here: SNNs rely on spike-based computation rather than continuous activations.
  - Quick check question: What happens when the membrane potential exceeds the threshold in an LIF neuron?

- Concept: Surrogate gradient backpropagation.
  - Why needed here: Enables gradient-based learning in non-differentiable spiking systems.
  - Quick check question: How does the Fast-Sigmoid surrogate approximate the Heaviside step function?

- Concept: Conversion from DNN to SNN.
  - Why needed here: Provides a good initialization for SNN training.
  - Quick check question: What architectural changes are required when tailoring a CNN for SNN conversion?

## Architecture Onboarding

- Component map: Tailored CNN (input: positive word embeddings, ReLU, avg-pooling) -> Spike generator (Poisson encoding of embeddings) -> LIF neurons (integrate-and-fire dynamics) -> Surrogate gradient layer (Fast-Sigmoid backward pass) -> Ensemble layer (multiple neurons per class for robustness)
- Critical path: 1. Train tailored CNN → convert to SNN → fine-tune with surrogate gradients. 2. At inference: embeddings → spike train → LIF propagation → ensemble voting.
- Design tradeoffs: More time steps → higher accuracy but more energy; Higher threshold → fewer spikes but possible accuracy drop; Dropout during training → better generalization, but slightly slower convergence.
- Failure signatures: Accuracy drops sharply if embeddings are not shifted to positive values; Performance collapses if the decay rate β is not set to 1.0 after conversion; Overfitting if too many neurons per class are used.
- First 3 experiments: 1. Train tailored CNN with positive embeddings; compare to original CNN accuracy. 2. Convert to SNN; measure accuracy without fine-tuning. 3. Fine-tune SNN; compare energy consumption vs. baseline CNN.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can SNNs be pre-trained using masked language modeling on large text datasets to potentially close the performance gap with transformer-based models?
- Basis in paper: [explicit] The paper mentions this as a potential future direction, stating "It would be interesting to see if we can pre-train SNNs unsupervisedly using masked language modeling with a large collection of text data in the future."
- Why unresolved: Current SNNs lag behind ANNs in accuracy on existing datasets designed for continuous values, and there's no established method for pre-training SNNs on text data.
- What evidence would resolve it: Development and evaluation of a masked language modeling pre-training approach for SNNs, demonstrating improved performance on downstream text classification tasks compared to SNNs trained only with the proposed conversion + fine-tuning method.

### Open Question 2
- Question: Can new datasets specifically designed for SNNs, such as those from event-based cameras or biological nervous systems recordings, enable SNNs to outperform ANNs on language tasks?
- Basis in paper: [inferred] The paper suggests that new datasets compatible with SNNs' properties (temporal/spiking nature) could be more suitable and challenging for SNNs than current continuous-valued datasets.
- Why unresolved: Existing benchmarks were created for ANNs, potentially limiting SNNs' performance due to information loss during conversion to spike trains.
- What evidence would resolve it: Creation and benchmarking of SNN-specific language datasets, showing SNNs achieving superior performance compared to ANNs on these tasks.

### Open Question 3
- Question: What is the optimal trade-off between energy efficiency and accuracy when adjusting the membrane threshold Uthr in SNNs for text classification?
- Basis in paper: [explicit] The paper shows that increasing Uthr can reduce active neurons (spikes) by ~50% with minimal accuracy loss, suggesting significant energy savings are possible.
- Why unresolved: While the paper demonstrates this relationship, the optimal Uthr value likely varies across different datasets, architectures, and hardware implementations.
- What evidence would resolve it: Systematic evaluation of Uthr's impact on energy consumption and accuracy across diverse text classification tasks and SNN architectures, identifying generalizable guidelines for setting this parameter.

## Limitations
- Lack of specific implementation details for spike encoding of word embeddings, particularly for Chinese datasets.
- Missing methodology for adversarial attack evaluation and energy consumption calculations.
- No established method for pre-training SNNs on text data using masked language modeling.

## Confidence
- High Confidence: The core "conversion + fine-tuning" approach for training SNNs is well-supported by the methodology described and experimental results on multiple datasets.
- Medium Confidence: The encoding of word embeddings as spike trains is plausible but lacks implementation details, reducing confidence in this specific claim.
- Low Confidence: Claims about robustness to adversarial attacks and energy consumption are not well-supported due to missing methodological details.

## Next Checks
1. Implement and test the proposed method for converting word embeddings to spike trains, ensuring that semantic information is preserved. Compare classification accuracy with and without this encoding.
2. Calculate energy consumption using the same methodology as the paper, considering the number of spikes and inference time steps. Compare these results with the reported energy savings.
3. Design and execute adversarial attacks similar to those described in the paper. Evaluate the robustness of the SNNs compared to CNNs under these attack conditions.