---
ver: rpa2
title: 'QE-EBM: Using Quality Estimators as Energy Loss for Machine Translation'
arxiv_id: '2410.10228'
source_url: https://arxiv.org/abs/2410.10228
tags:
- translation
- data
- energy
- quality
- loss
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes using quality estimation models as energy-based
  loss functions for training neural machine translation models. The key idea is to
  leverage the gradients from quality estimation scores, which RL methods cannot exploit,
  to improve translation quality through energy-based training.
---

# QE-EBM: Using Quality Estimators as Energy Loss for Machine Translation

## Quick Facts
- arXiv ID: 2410.10228
- Source URL: https://arxiv.org/abs/2410.10228
- Authors: Gahyun Yoo; Jay Yoon Lee
- Reference count: 17
- Primary result: QE-EBM outperforms supervised fine-tuning and RL baselines for low-resource machine translation, achieving up to 2.5 BLEU and 7.1 COMET-KIWI improvements on English-to-Mongolian translation

## Executive Summary
This paper introduces QE-EBM, a novel approach that uses quality estimation (QE) models as trainable energy loss functions for neural machine translation (NMT). The key insight is that QE scores contain gradient information that reinforcement learning cannot exploit, providing more informative training signals for improving translation quality. The method is particularly effective for low-resource language pairs, where it significantly outperforms both supervised fine-tuning and traditional RL approaches like REINFORCE and PPO. The approach includes two variants: QE-STATIC with fixed energy model parameters and QE-DYNAMIC with dynamically updated parameters using contrastive learning.

## Method Summary
The QE-EBM method treats quality estimation scores as energy functions (Eθ(x,y) = -s(x,y)) and backpropagates through them to train NMT models. It uses MBART as the NMT model and COMET-KIWI as the QE model, with a multi-task setup combining cross-entropy loss for labeled data and energy loss for unlabeled data. Two variants are explored: QE-STATIC maintains fixed energy model parameters, while QE-DYNAMIC fine-tunes the QE model using contrastive learning with NCE loss before each NMT update. The approach also incorporates data sampling techniques including filtering labeled data based on QE scores and nearest-neighbor retrieval to reduce gradient mismatch between labeled and unlabeled batches.

## Key Results
- QE-EBM achieves 2.5 BLEU, 7.1 COMET-KIWI, 5.3 COMET, and 6.4 XCOMET improvements over supervised baseline for English-to-Mongolian translation
- The method outperforms both supervised fine-tuning and RL baselines (REINFORCE, PPO) across multiple low-resource language pairs
- QE-DYNAMIC variant shows superior performance compared to QE-STATIC, particularly for languages with very limited parallel data

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Energy-based training with QE scores provides more informative gradients than scalar rewards from RL
- Mechanism: QE scores are treated as energy functions Eθ(x,y) = -s(x,y), allowing gradients to flow back through NMT parameters and capture token-level contributions to translation quality
- Core assumption: QE score gradients contain actionable information for improving translation quality
- Evidence anchors: [abstract] "reinforcement learning cannot exploit the gradients with respect to the QE score"
- Break condition: If QE model gradients are noisy or misaligned with actual translation quality improvements

### Mechanism 2
- Claim: Fine-tuning the QE model (QE-DYNAMIC) keeps the energy landscape aligned with improving NMT outputs
- Mechanism: QE model updated via contrastive learning with NCE loss, distinguishing gold translations from NMT-generated ones
- Core assumption: As NMT quality improves, QE model needs updating to maintain relevant guidance
- Evidence anchors: [section] "In QE-DYNAMIC, the loss net is fine-tuned with contrastive loss before each task net update"
- Break condition: If contrastive learning doesn't improve QE model discriminability or destabilizes NMT training

### Mechanism 3
- Claim: Data sampling techniques improve gradient quality by reducing distribution mismatch
- Mechanism: Filtering selects high-quality parallel data based on QE scores; NN retrieval pairs similar source sentences to reduce conflicting gradients
- Core assumption: Heterogeneous labeled/unlabeled data can create conflicting gradients
- Evidence anchors: [section] "This is expected to reduce the mismatch in gradient direction"
- Break condition: If filtering removes too much data or NN retrieval creates artificial correlations

## Foundational Learning

- Concept: Energy-based models and their relationship to probability distributions
  - Why needed here: Understanding how QE scores function as energy functions and their relationship to training objectives
  - Quick check question: How does minimizing an energy function differ from maximizing a probability in terms of gradient flow?

- Concept: Contrastive learning and noise-contrastive estimation (NCE)
  - Why needed here: QE-DYNAMIC uses NCE loss to update the QE model, distinguishing gold from generated translations
  - Quick check question: What is the mathematical difference between standard cross-entropy and NCE loss?

- Concept: Straight-through estimator (STE) for gradient approximation
  - Why needed here: STE is used to backpropagate through the discrete sampling process in NMT
  - Quick check question: How does STE approximate gradients through non-differentiable operations?

## Architecture Onboarding

- Component map: Source sentence → NMT generation → QE scoring → Energy loss → NMT parameter updates
- Critical path: Source → MBART → COMET-KIWI → Energy loss → Parameter updates
- Design tradeoffs:
  - QE-DYNAMIC vs QE-STATIC: Dynamic updates improve alignment but increase training time and complexity
  - STE approximation vs exact gradients: STE enables training but introduces approximation error
  - Data filtering vs data retention: Filtering improves quality but may reduce diversity
- Failure signatures:
  - Training instability: Oscillating or exploding gradients
  - Degraded translation quality: BLEU/COMET scores decrease despite training
  - Memory issues: Running out of GPU memory during contrastive QE updates
- First 3 experiments:
  1. Run QE-STATIC with filtering only on a low-resource language pair to establish baseline improvements
  2. Add NN retrieval to QE-STATIC to test combined data sampling benefits
  3. Switch to QE-DYNAMIC with all techniques to compare against static variant

## Open Questions the Paper Calls Out

- How does the performance of QE-EBM compare to other energy-based methods for text generation beyond residual energy-based models?
- What is the impact of different data sampling techniques on the performance of QE-EBM for low-resource languages?
- How does the choice of quality estimation model affect the performance of QE-EBM?
- What are the computational and memory requirements of QE-EBM compared to RL methods?
- How does the performance of QE-EBM scale with increasing dataset size for high-resource languages?

## Limitations
- The approach requires substantial computational resources due to joint training of NMT and QE models
- Empirical validation of the core hypothesis (QE gradients provide more informative signals than RL rewards) lacks direct corpus evidence
- Individual contributions of dynamic QE updates and data sampling techniques to overall improvements are not clearly isolated through ablation studies

## Confidence

- High confidence: The basic feasibility of using QE scores as energy functions for NMT training
- Medium confidence: The superiority of QE-EBM over RL baselines for low-resource translation
- Low confidence: The specific contributions of individual components (dynamic QE updates, data sampling techniques) to the overall improvements

## Next Checks

1. Run ablation studies on low-resource language pairs to isolate the individual contributions of data sampling techniques (filtering vs nearest-neighbor retrieval) to performance improvements
2. Compare training stability and convergence speed between QE-STATIC and QE-DYNAMIC variants on identical hardware and training budgets
3. Test whether the QE-EBM approach transfers benefits to other QE model architectures beyond COMET-KIWI to validate the general applicability of the energy-based framework