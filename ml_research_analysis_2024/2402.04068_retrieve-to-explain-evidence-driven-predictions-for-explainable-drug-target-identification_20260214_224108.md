---
ver: rpa2
title: 'Retrieve to Explain: Evidence-driven Predictions for Explainable Drug Target
  Identification'
arxiv_id: '2402.04068'
source_url: https://arxiv.org/abs/2402.04068
tags:
- evidence
- mask
- answer
- language
- shapley
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "Retrieve to Explain (R2E) is a semi-parametric retrieval-based\
  \ language model that scores and ranks possible answers to a research question based\
  \ on evidence retrieved from a document corpus. R2E uses Shapley values to attribute\
  \ each answer\u2019s score to its supporting evidence, enabling transparent explanations."
---

# Retrieve to Explain: Evidence-driven Predictions for Explainable Drug Target Identification

## Quick Facts
- **arXiv ID:** 2402.04068
- **Source URL:** https://arxiv.org/abs/2402.04068
- **Reference count:** 40
- **Primary result:** R2E matches literature-based models and surpasses genetics-based approaches for drug target identification with AUROC of 0.633-0.638

## Executive Summary
Retrieve to Explain (R2E) is a semi-parametric retrieval-based language model that scores and ranks possible answers to research questions based on evidence retrieved from a document corpus. The model uses Shapley values to attribute each answer's score to its supporting evidence, enabling transparent explanations for human-in-the-loop decision making. By representing answers only in terms of their supporting evidence with the answer itself masked, R2E allows incorporation of new evidence without retraining. When predicting drug target efficacy in clinical trials, R2E matches literature-based models and surpasses a widely-used genetics-based approach, with AUROC of 0.633. Performance improves further to 0.638 AUROC when auditing explanations using GPT-4 to remove false positive evidence.

## Method Summary
R2E combines a Retriever module that uses a masked language model to embed query and evidence passages and retrieve k most similar passages for each answer, with a Reasoner module that takes the query and retrieved evidence, combines them using convolutional layers and a set transformer, and outputs a binary probability. The model represents each answer only in terms of its supporting evidence, with the answer itself masked, allowing Shapley value attribution to explain how each piece of evidence contributes to the final prediction. The semi-parametric architecture enables incorporation of new evidence without retraining by simply adding it to the retrieval corpus. R2E was evaluated on drug target identification tasks using clinical trial outcomes data, biomedical literature, and genetics associations templated into natural language.

## Key Results
- R2E achieves AUROC of 0.633 for clinical trial outcome prediction, matching literature-based models and surpassing genetics-based approaches
- Performance improves to 0.638 AUROC when auditing explanations using GPT-4 to remove false positive evidence
- The model successfully ranks drug targets by relevance to user queries, with MRR and hits@10 metrics demonstrating effective retrieval and reasoning

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Representing answers only in terms of evidence allows transparent attribution of predictions to supporting evidence.
- Mechanism: The R2E architecture masks the answer entity in all passages, making the evidence itself the feature space. This enables Shapley value attribution to explain how each piece of evidence contributes to the final score.
- Core assumption: The model can effectively learn to rank answers based solely on their supporting evidence, without direct access to the answer entity itself.
- Evidence anchors:
  - [abstract]: "The architecture represents each answer only in terms of its supporting evidence, with the answer itself masked."
  - [section]: "In particular, the feature space is comprised of the evidence itself, allowing the use of feature attribution methods to estimate the contribution of each piece of evidence to the final prediction."
- Break condition: If the model cannot learn meaningful representations from evidence alone, or if the evidence masking prevents effective learning of answer-entity relationships.

### Mechanism 2
- Claim: Retrieval-based inference allows incorporation of new evidence without retraining.
- Mechanism: By retrieving evidence at inference time and representing answers only through this evidence, R2E can incorporate new evidence simply by adding it to the retrieval corpus, without needing to retrain model parameters.
- Core assumption: The retrieval mechanism can effectively find relevant evidence for new answers, and the model can reason from this evidence to make predictions.
- Evidence anchors:
  - [abstract]: "The architecture also allows incorporation of new evidence without retraining, including non-textual data modalities templated into natural language."
  - [section]: "In addition to clarifying the prediction to the user, we show that this evidence-oriented approach allows model predictions to be updated without retraining by modifying the corpus, such as updating it with new evidence."
- Break condition: If the retrieval mechanism fails to find relevant evidence for new answers, or if the model cannot effectively reason from newly added evidence.

### Mechanism 3
- Claim: Templating structured data into natural language allows multimodal reasoning.
- Mechanism: R2E can incorporate structured data (like genetics associations) by converting it into natural language sentences, which can then be retrieved and used as evidence like any other text.
- Core assumption: The model can effectively reason from templated natural language representations of structured data, and these representations capture the relevant information.
- Evidence anchors:
  - [section]: "Since R2E can generate a score for every answer in the answer set, it is particularly applicable in human-in-the-loop scenarios where many potential hypotheses should be prioritized for user review."
  - [section]: "We assessed R2E's ability to reason from genetics data by generating a sentence for every row in the genetics dataset... The genetics corpus was given to the R2E Retriever alone and in combination with the year-split biomedical literature."
- Break condition: If the templating process fails to capture relevant information from structured data, or if the model cannot effectively reason from these templated representations.

## Foundational Learning

- Concept: Shapley value attribution
  - Why needed here: To provide transparent explanations of how each piece of evidence contributes to the final prediction, which is critical for human-in-the-loop decision making in drug discovery.
  - Quick check question: What does a positive Shapley value for a piece of evidence indicate about its contribution to the model's prediction?

- Concept: Retrieval-augmented generation
  - Why needed here: To incorporate large amounts of external evidence at inference time, allowing the model to leverage both learned parameters and retrieved information for predictions.
  - Quick check question: How does retrieval-augmented generation differ from traditional parametric models in terms of knowledge incorporation?

- Concept: Cloze-style question answering
  - Why needed here: To frame scientific research questions in a way that allows multiple potential answers (drug targets) to be scored and ranked based on evidence.
  - Quick check question: What is the advantage of using cloze-style questions for scientific hypothesis generation compared to traditional question answering?

## Architecture Onboarding

- Component map: Retriever (MLM) -> Reasoner (set transformer) -> Prediction with Shapley values
- Critical path: Query → Retriever → Reasoner → Prediction with Shapley values
  The retriever and reasoner must work together effectively to produce accurate predictions, and the explanation module must accurately attribute predictions to evidence.
- Design tradeoffs:
  - Retrieval-based vs parametric: Retrieval allows incorporation of new evidence without retraining but adds computational overhead
  - Evidence masking: Enables transparent attribution but may make learning more challenging
  - Multimodal templating: Allows incorporation of structured data but requires careful templating to preserve relevant information
- Failure signatures:
  - Poor retrieval quality: Evidence may not be relevant to the query, leading to incorrect predictions
  - Ineffective evidence representation: The model may not learn to effectively represent answers based on evidence alone
  - Explanation inaccuracy: Shapley values may not accurately reflect the true contribution of evidence to predictions
- First 3 experiments:
  1. Test retrieval quality by examining retrieved evidence for a sample of queries
  2. Validate explanation quality by comparing Shapley values to human expert annotations
  3. Test multimodal reasoning by comparing performance on genetics data with and without templating into natural language

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does R2E performance scale with model size?
- Basis in paper: [inferred] ... The paper mentions that the Retriever and Reasoner architectures were relatively small (10 and 2 million parameters respectively) and suggests that improvements could be expected with model scaling.
- Why unresolved: The paper does not explore the impact of increasing model size on R2E performance.
- What evidence would resolve it: Experiments comparing R2E performance with different model sizes (e.g., 10M, 100M, 1B parameters) on the same tasks.

### Open Question 2
- Question: How does R2E handle entities with multiple surface forms or synonyms?
- Basis in paper: [explicit] ... The paper mentions using entity linking to ground surface forms of genes and proteins to the same entity under a 1:1 assumption, but does not discuss how this handles multiple surface forms or synonyms.
- Why unresolved: The paper does not provide details on how R2E handles entities with multiple surface forms or synonyms beyond the 1:1 assumption.
- What evidence would resolve it: Experiments comparing R2E performance with and without handling of multiple surface forms or synonyms for entities.

### Open Question 3
- Question: How does R2E's performance compare to other explainable models for drug target identification?
- Basis in paper: [explicit] ... The paper compares R2E to a genetics-based baseline and a non-explainable literature-based model, but does not compare to other explainable models for drug target identification.
- Why unresolved: The paper does not explore how R2E's performance and explainability compare to other explainable models specifically designed for drug target identification.
- What evidence would resolve it: Experiments comparing R2E to other explainable models (e.g., knowledge graph-based models, other retrieval-based models with explanations) on the same drug target identification tasks.

## Limitations
- The model's AUROC of 0.633-0.638 for clinical trial prediction, while competitive with literature-based approaches, remains modest and indicates substantial room for improvement.
- The reliance on GPT-4 for evidence auditing introduces a dependency on a black-box model whose performance characteristics are not fully characterized in this context.
- The masking strategy that enables transparency may also limit the model's ability to learn direct answer-entity relationships, potentially constraining performance.

## Confidence
- **High confidence**: The core mechanism of using Shapley values for evidence attribution is technically sound and well-established. The semi-parametric architecture for incorporating new evidence without retraining is clearly implemented.
- **Medium confidence**: The clinical trial outcome prediction results are promising but require external validation on additional datasets to confirm generalizability. The GPT-4 auditing approach shows utility but its effectiveness depends on the reliability of the underlying LLM.
- **Low confidence**: The long-term stability of evidence-based representations when corpora evolve significantly, and the scalability of the approach to entirely new domains beyond drug discovery.

## Next Checks
1. **Cross-domain validation**: Test R2E on a completely different scientific domain (e.g., materials science or climate research) to verify the approach's generalizability beyond biomedical applications.
2. **Temporal robustness analysis**: Evaluate how model performance degrades or improves when training on increasingly older literature (5+ year gaps) to assess the retrieval mechanism's resilience to knowledge drift.
3. **Human-AI collaboration study**: Conduct a controlled experiment comparing expert predictions with and without R2E explanations to quantify the actual utility of Shapley-based evidence attribution in decision-making workflows.