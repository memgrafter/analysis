---
ver: rpa2
title: Unsupervised hard Negative Augmentation for contrastive learning
arxiv_id: '2401.02594'
source_url: https://arxiv.org/abs/2401.02594
tags:
- negative
- augmentation
- learning
- paraphrasing
- tf-idf
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Unsupervised hard Negative Augmentation (UNA),
  a method that generates synthetic negative instances for contrastive learning by
  leveraging term frequency-inverse document frequency (TF-IDF) scores to replace
  important terms in sentences. Experiments show that models trained with UNA improve
  semantic textual similarity (STS) task performance, achieving Spearman correlation
  of 0.7767 with BERT and 0.7820 with RoBERTa.
---

# Unsupervised hard Negative Augmentation for contrastive learning

## Quick Facts
- arXiv ID: 2401.02594
- Source URL: https://arxiv.org/abs/2401.02594
- Authors: Yuxuan Shu; Vasileios Lampos
- Reference count: 40
- Primary result: Improves semantic textual similarity (STS) task performance using TF-IDF-guided term replacement for hard negative augmentation

## Executive Summary
This paper introduces Unsupervised hard Negative Augmentation (UNA), a method that generates synthetic negative instances for contrastive learning by leveraging TF-IDF scores to replace important terms in sentences. The approach improves semantic textual similarity task performance by creating hard negative samples that force models to learn finer semantic distinctions. Experiments demonstrate state-of-the-art results on the SICK Relatedness task when combining UNA with paraphrasing, while ablation studies confirm the effectiveness of the TF-IDF-driven term replacement strategy.

## Method Summary
UNA uses TF-IDF scores to identify semantically important terms in sentences and generates negative samples by replacing these terms with alternatives of similar TF-IDF magnitude. The method is integrated into contrastive learning frameworks like SimCSE, where negative samples are created during self-supervised pre-training. For each training batch, UNA is applied once every 5 batches to maintain training stability while providing sufficient hard negative examples. The approach is compatible with different backbone models (BERT and RoBERTa) and provides a straightforward way to improve downstream STS tasks without requiring additional labeled data.

## Key Results
- Improves STS task performance with Spearman correlation of 0.7767 with BERT and 0.7820 with RoBERTa
- Achieves state-of-the-art results on SICK Relatedness task when combined with paraphrasing augmentation
- Ablation studies confirm TF-IDF-driven term replacement strategy is effective for generating quality negative samples

## Why This Works (Mechanism)

### Mechanism 1
- Claim: TF-IDF-guided term replacement selects semantically meaningful words for replacement
- Mechanism: TF-IDF scores identify words that are rare (high TF-IDF) and therefore carry more semantic weight. Replacing these words with alternatives of similar TF-IDF magnitude creates negative samples that are structurally similar but semantically different
- Core assumption: Words with high TF-IDF scores are semantically more important than common words in determining sentence meaning
- Evidence anchors:
  - [abstract]: "UNA uses TF-IDF scores to ascertain the perceived importance of terms in a sentence and then produces negative samples by replacing terms with respect to that"
  - [section 3]: "Terms with a higher degree of substance (i.e. words that are less common) are on average more important in determining the meaning of a sentence"
  - [corpus]: Weak - no corpus-level evidence provided for this specific TF-IDF importance claim

### Mechanism 2
- Claim: Replacement within TF-IDF score vicinity preserves syntactic structure while changing semantics
- Mechanism: By sampling replacement words within a radius of the original word's maximum TF-IDF score, the method maintains sentence structure and fluency while creating meaningful semantic differences
- Core assumption: Words with similar TF-IDF scores in the corpus tend to play similar syntactic roles
- Evidence anchors:
  - [section 3]: "We determine a radius of r terms (below and above the reference maximum TF-IDF score of term i) and sample a replacement term with respect to their maximum TF-IDF scores"
  - [abstract]: "generate negative samples by replacing terms with respect to that [TF-IDF]"
  - [corpus]: Weak - no corpus-level evidence provided for syntactic role preservation claim

### Mechanism 3
- Claim: Hard negative samples improve contrastive learning by forcing the model to learn finer semantic distinctions
- Mechanism: Generating negative samples that are semantically close but different creates "hard" negatives that force the model to learn more discriminative features rather than relying on superficial differences
- Core assumption: Hard negatives are more effective than random negatives for contrastive learning objectives
- Evidence anchors:
  - [section 2.2]: "Hard negative samples, which are negative samples that are challenging for the model to differentiate from the anchor instance, have proven to be effective in contrastive learning"
  - [abstract]: "Our experiments demonstrate that models trained with UNA improve the overall performance in semantic textual similarity tasks"
  - [corpus]: Weak - no corpus-level evidence provided for hard negative effectiveness claim

## Foundational Learning

- Concept: TF-IDF (Term Frequency-Inverse Document Frequency)
  - Why needed here: Forms the basis for identifying important terms to replace and selecting appropriate replacement candidates
  - Quick check question: How does TF-IDF score change when a word appears in many documents versus few documents?

- Concept: Contrastive Learning (InfoNCE loss)
  - Why needed here: The training framework that benefits from hard negative samples generated by UNA
  - Quick check question: What is the relationship between the temperature hyperparameter τ and the strength of negative sampling?

- Concept: Sentence Embedding and Semantic Similarity
  - Why needed here: The downstream task being improved by better pre-training with UNA
  - Quick check question: How does cosine similarity between sentence embeddings relate to semantic similarity scores?

## Architecture Onboarding

- Component map: Pre-training → TF-IDF representation generation → UNA augmentation → contrastive loss → evaluation on STS tasks
- Critical path: TF-IDF vectorization → term replacement probability calculation → replacement term sampling → negative sample generation → contrastive loss computation
- Design tradeoffs: TF-IDF vocabulary size vs. computational efficiency; replacement radius vs. semantic distance; augmentation frequency vs. training stability
- Failure signatures: Poor STS performance despite training; TF-IDF matrix too sparse; replacement terms don't make grammatical sense; model converges too quickly or not at all
- First 3 experiments:
  1. Verify TF-IDF matrix generation on sample corpus and inspect term importance rankings
  2. Test UNA on a small batch to ensure it generates grammatically correct negative samples with appropriate semantic distance
  3. Run ablation study comparing UNA with random replacement to validate TF-IDF importance for term selection

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does UNA perform on languages other than English?
- Basis in paper: [explicit] The paper explicitly states that experiments focused on English language datasets and that findings may not provide constructive directions for other languages with different characteristics compared to English.
- Why unresolved: The paper does not provide any experimental results or analysis for languages other than English.
- What evidence would resolve it: Experimental results of UNA on non-English datasets, ideally covering a diverse range of languages with different linguistic characteristics.

### Open Question 2
- Question: How does the performance of UNA change with longer documents or text constructs beyond single sentences?
- Basis in paper: [inferred] The paper mentions that the application of TF-IDF on single sentences might provide quite dataset-specific results, implying potential limitations with longer text.
- Why unresolved: The paper only evaluates UNA on sentence-level tasks and does not explore its effectiveness on longer text constructs.
- What evidence would resolve it: Experimental results of UNA on document-level or paragraph-level tasks, comparing performance with sentence-level tasks.

### Open Question 3
- Question: What is the optimal frequency for applying UNA during training, and how does it affect model performance?
- Basis in paper: [explicit] The paper conducts experiments to find the optimal frequency of applying UNA during training, settling on once every 5 batches.
- Why unresolved: While the paper provides results for a specific frequency, it does not explore the full range of possible frequencies or their impact on different tasks.
- What evidence would resolve it: A comprehensive study of UNA performance across various frequencies of application, ideally including analysis of task-specific optimal frequencies.

### Open Question 4
- Question: How does UNA compare to other negative sampling strategies in contrastive learning beyond the ones mentioned in the paper?
- Basis in paper: [explicit] The paper compares UNA to some baseline augmentation strategies but does not exhaustively compare it to all possible negative sampling methods in the literature.
- Why unresolved: The paper only provides a limited comparison and does not explore the full landscape of negative sampling techniques.
- What evidence would resolve it: A systematic comparison of UNA against a wide range of negative sampling strategies in contrastive learning, including both hard and easy negative sampling methods.

## Limitations
- Reliance on TF-IDF as proxy for semantic importance may not capture context-dependent meaning or domain-specific nuances
- Assumes high TF-IDF words are semantically important, which may not hold across all domains or languages
- Augmentation strategy only replaces single terms rather than considering multi-word expressions or contextual relationships

## Confidence

- **High Confidence**: Claims about improved STS performance over baselines (0.7767 vs 0.7637 for BERT, 0.7820 vs 0.7716 for RoBERTa)
- **Medium Confidence**: Claims about TF-IDF effectiveness for identifying semantically important terms for replacement
- **Medium Confidence**: Claims about compatibility with different backbone models (BERT and RoBERTa)
- **Low Confidence**: Claims about state-of-the-art results without comparison to recent methods beyond SimCSE

## Next Checks

1. Test UNA's performance when applied more or less frequently than every 5 batches to determine optimal augmentation frequency across different corpus sizes and training dynamics
2. Evaluate whether replacing multi-word expressions or considering word dependencies improves performance compared to the current single-term replacement strategy
3. Compare UNA's effectiveness against alternative term importance measures (e.g., attention weights, part-of-speech tagging, or semantic role labeling) to validate TF-IDF as the optimal choice