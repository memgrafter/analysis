---
ver: rpa2
title: 'Teaching LLMs at Charles University: Assignments and Activities'
arxiv_id: '2407.19798'
source_url: https://arxiv.org/abs/2407.19798
tags:
- students
- course
- language
- llms
- were
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper describes the development and implementation of a new
  course on large language models (LLMs) taught at Charles University. The course
  includes practical assignments where students experiment with LLM inference for
  weather report generation and machine translation using various open-source models.
---

# Teaching LLMs at Charles University: Assignments and Activities

## Quick Facts
- arXiv ID: 2407.19798
- Source URL: https://arxiv.org/abs/2407.19798
- Reference count: 4
- Course provides practical assignments and activities for teaching LLMs with publicly available materials

## Executive Summary
This paper describes a new course on large language models taught at Charles University that combines practical assignments with classroom activities to provide hands-on experience with LLM inference and critical thinking about their applications and ethics. Students experiment with open-source models for weather report generation and machine translation, using a unified API to access various architectures without specialized hardware. The course includes quizzes, group discussions on LLM understanding and ethics, and a "best paper" session where students evaluate recent research papers, all designed to deepen understanding of LLM capabilities, limitations, and broader implications.

## Method Summary
The course uses pre-trained open-source LLMs accessed through a text-generation-webui API, allowing students to experiment with different models and decoding parameters for weather report generation and machine translation tasks. Students work in teams to craft prompts, adjust sampling parameters (temperature, top-k, top-p, beam size), and submit outputs through a web application with leaderboards. Classroom activities include quizzes on factual knowledge, discussions on LLM understanding and ethics, and role-playing as a best-paper committee to critically evaluate recent research papers. All materials and assignments are publicly available for other educators.

## Key Results
- Students gained hands-on experience with multiple LLM models through practical assignments
- Classroom discussions covered both technical questions about LLM understanding and broader ethical considerations
- The "best paper" activity developed students' critical assessment skills for AI research
- Course materials and assignments are publicly available as a resource for other educators

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Hands-on LLM inference assignments improve student understanding of practical LLM capabilities and limitations.
- Mechanism: By providing direct access to multiple LLM models through a unified API, students can experiment with different architectures and parameters, leading to experiential learning about model behaviors and constraints.
- Core assumption: Direct interaction with models is more effective than theoretical instruction alone for understanding LLM capabilities.
- Evidence anchors:
  - [section]: "For each task, we ran instances of different models on our GPU cluster with an API provided by the text-generation-webui package. The API allowed the students to access and configure the models without the need to access specialized hardware or rely on commercial platforms."
  - [section]: "Besides choosing the appropriate prompts, the teams experimented with various decoding parameters, including the sampling temperature, the k and p parameters for top-k and top-p sampling, and the beam size."
  - [corpus]: Found 25 related papers, but none directly address the effectiveness of hands-on LLM assignments for learning. Weak corpus evidence.

### Mechanism 2
- Claim: Structured classroom discussions enhance critical thinking about LLM capabilities and ethics.
- Mechanism: Group discussions on complex topics like LLM understanding and ethics force students to articulate positions, consider counterarguments, and engage with philosophical thought experiments.
- Core assumption: Peer discussion facilitates deeper understanding than passive learning methods.
- Evidence anchors:
  - [section]: "Discussion is an effective method for teaching non-technical topics. In the final session on this course, we focused on two primary areas. The first area involves the question of whether LLMs can truly understand language."
  - [section]: "Here, students discussed environmental and labor issues related to training LLMs (e.g. Bender et al., 2021) and the broader challenges associated with the development and deployment of language technologies (e.g. Jørgensen and Søgaard, 2023)."
  - [corpus]: Weak corpus evidence for the specific claim about discussion effectiveness in LLM courses, though general education literature supports discussion-based learning.

### Mechanism 3
- Claim: Research paper evaluation activities develop critical assessment skills for AI research.
- Mechanism: Role-playing as a best-paper committee requires students to deeply engage with research papers, identify strengths and weaknesses, and defend their evaluations through structured debate.
- Core assumption: Evaluating research papers in a structured, social context improves students' ability to critically assess AI research.
- Evidence anchors:
  - [section]: "One of the goals we set for the course was to teach the students to responsibly assess the quality and trustworthiness of recent research papers. We organized an activity where the students role-played a best-paper committee, partially inspired by the Role-Playing Paper-Reading Seminars (Jacobson and Raffel, 2021)."
  - [section]: "Then, the students re-grouped by their assigned article, where they discussed the paper again and nominated an advocate for and against it. Then, the advocates presented their final one-minute speeches."
  - [corpus]: No direct corpus evidence for this specific pedagogical approach in LLM courses, though the referenced Jacobson and Raffel work provides some support for paper-reading seminars.

## Foundational Learning

- Concept: Prompt engineering
  - Why needed here: Students need to understand how to effectively communicate with LLMs through well-crafted prompts to achieve desired outputs.
  - Quick check question: What is the difference between zero-shot and few-shot prompting, and when might each be more effective?

- Concept: Decoding strategies (temperature, top-k, top-p, beam search)
  - Why needed here: Students must understand how different decoding parameters affect the creativity, diversity, and quality of LLM outputs.
  - Quick check question: How does increasing the temperature parameter affect the distribution of token probabilities during sampling?

- Concept: Evaluation metrics for NLP tasks
  - Why needed here: Students need to understand how to assess the quality of LLM outputs using appropriate metrics for different tasks.
  - Quick check question: What are the advantages and limitations of using BLEU versus BERTScore for evaluating machine translation quality?

## Architecture Onboarding

- Component map: Lecture sessions -> Hands-on assignments (weather generation, machine translation) -> Classroom activities (quizzes, discussions, paper reading) -> Web interface for submissions and leaderboards
- Critical path: Ensuring students have functional access to LLM inference APIs before assignment deadlines
- Design tradeoffs: Trades depth for breadth by covering many LLM topics superficially rather than deeply exploring a few topics
- Failure signatures: Students struggling with prompt engineering, insufficient GPU resources, students lacking NLP fundamentals
- First 3 experiments:
  1. Set up the text-generation-webui API with one model (e.g., Mistral 7B) and verify basic inference functionality through a simple prompt.
  2. Implement the Character F-score calculation and verify it works with known test cases for the machine translation assignment.
  3. Deploy the quiz web application with a simple multiple-choice question and test the QR code functionality for student access.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How effective are LLM-based assignments and classroom activities in improving student understanding of LLMs compared to traditional lecture-based approaches?
- Basis in paper: [inferred] The paper describes various LLM-based assignments and classroom activities implemented in a new course, but does not provide quantitative data on their effectiveness compared to traditional methods.
- Why unresolved: The paper focuses on describing the course materials and activities rather than evaluating their impact on student learning outcomes. No comparison with traditional teaching methods is made.
- What evidence would resolve it: A controlled study comparing student performance and understanding in courses using LLM-based assignments/activities versus traditional lecture-based approaches would provide evidence. Pre- and post-course assessments of student knowledge and skills could be used to measure learning gains.

### Open Question 2
- Question: What are the optimal strategies for scaling up LLM-based assignments to accommodate larger class sizes while maintaining educational quality and resource efficiency?
- Basis in paper: [explicit] The paper mentions potential issues with scaling up LLM-based assignments for larger numbers of students, particularly regarding access to computing resources and availability of teaching assistants.
- Why unresolved: While the paper acknowledges scalability concerns, it does not provide concrete solutions or guidelines for scaling up LLM-based assignments effectively.
- What evidence would resolve it: Empirical studies comparing different scaling strategies (e.g., cloud-based solutions, resource scheduling, peer assessment) in terms of their impact on educational quality, resource usage, and student satisfaction would provide insights into optimal scaling approaches.

### Open Question 3
- Question: How can LLM-based assignments be designed to effectively address ethical considerations and potential biases in LLM development and use?
- Basis in paper: [explicit] The paper mentions that ethical considerations were discussed in the course, including environmental and labor issues related to LLM training and broader challenges associated with language technology development and deployment.
- Why unresolved: While ethical considerations were mentioned, the paper does not provide specific details on how LLM-based assignments can be designed to address these issues or measure their impact on student awareness and understanding of AI ethics.
- What evidence would resolve it: Development and evaluation of LLM-based assignments specifically designed to highlight ethical considerations and potential biases, along with pre- and post-assignment surveys to measure changes in student awareness and attitudes towards AI ethics, would provide insights into effective approaches.

## Limitations
- No formal assessment of learning outcomes or student performance metrics
- Limited discussion of potential challenges when scaling the course to larger cohorts
- Insufficient detail on how students with varying technical backgrounds were supported

## Confidence
- Medium: Claims about hands-on assignments improving understanding - supported by the detailed assignment design but lacking comparative effectiveness data
- Medium: Claims about discussion-based learning enhancing critical thinking - reasonable based on pedagogical literature but not specifically validated for LLM courses
- Low: Claims about the best-paper committee activity developing research evaluation skills - novel approach with minimal supporting evidence from corpus or course evaluation

## Next Checks
1. Administer pre- and post-course surveys measuring student confidence and understanding of LLM concepts, with statistical analysis of improvement
2. Conduct controlled comparison between students who completed hands-on assignments versus those who only received theoretical instruction
3. Implement the course with a larger cohort (>50 students) and document resource usage patterns, technical support needs, and any modifications required for scalability