---
ver: rpa2
title: Large Language Model as a Teacher for Zero-shot Tagging at Extreme Scales
arxiv_id: '2406.09288'
source_url: https://arxiv.org/abs/2406.09288
tags:
- labels
- training
- label
- document
- extreme
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces LMTX, a novel framework for Extreme Zero-shot
  Multi-label Text Classification (EZ-XMC) that leverages a Large Language Model (LLM)
  as a teacher to guide the training of a lightweight bi-encoder. The approach addresses
  the challenge of training robust models in zero-shot scenarios without annotated
  data, where existing methods rely on suboptimal pseudo labels.
---

# Large Language Model as a Teacher for Extreme Zero-shot Tagging

## Quick Facts
- **arXiv ID**: 2406.09288
- **Source URL**: https://arxiv.org/abs/2406.09288
- **Reference count**: 34
- **Key outcome**: Achieves up to 37% improvement in P@1 on AmazonCat-13K compared to state-of-the-art methods

## Executive Summary
This paper introduces LMTX, a framework that uses a Large Language Model (LLM) as a teacher to guide training of a lightweight bi-encoder for Extreme Zero-shot Multi-label Text Classification (EZ-XMC). The approach addresses the challenge of training robust models without annotated data by leveraging LLM to identify high-quality pseudo labels from a predefined label set. LMTX achieves state-of-the-art performance across five benchmark datasets while maintaining computational efficiency by using the LLM only during training, not inference.

## Method Summary
LMTX employs a bi-encoder architecture with DistilBERT-based shared weights to embed both documents and labels into a common space. During training, an LLM teacher evaluates the relevance between documents and a shortlist of candidate labels retrieved via ANNS, selecting pseudo-positive labels for triplet loss training. The framework uses curriculum-based learning where the bi-encoder's improvement leads to better label retrieval, which enables the LLM to make more accurate selections in subsequent cycles. At inference, a lightweight bi-encoder performs MIPS-based retrieval without LLM involvement, achieving sublinear time complexity.

## Key Results
- Achieves up to 37% improvement in P@1 on AmazonCat-13K compared to state-of-the-art methods
- Establishes new state-of-the-art performance in EZ-XMC across five benchmark datasets
- Demonstrates computational efficiency by requiring less training data and faster inference than LLM-based approaches

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: The LLM teacher improves label-instance alignment compared to document-derived pseudo-labels.
- **Mechanism**: LLM evaluates semantic relevance between document and each candidate label, selecting only those that truly match the document content.
- **Core assumption**: LLM's zero-shot reasoning capability can accurately judge label relevance better than heuristic document segmentation.
- **Evidence anchors**: LLM-based pseudo-label selection outperforms document segmentation methods.
- **Break condition**: If LLM becomes noisy or biased, it may select irrelevant labels, degrading model performance.

### Mechanism 2
- **Claim**: Curriculum learning from LLM feedback improves bi-encoder training quality over time.
- **Mechanism**: As bi-encoder improves, ANNS retrieves more relevant label candidates, which LLM refines further, creating better training pairs in each cycle.
- **Core assumption**: Bi-encoder improvement leads to better shortlist quality, which enables LLM to make better selections.
- **Evidence anchors**: Performance monitoring on development set determines training cycles.
- **Break condition**: If bi-encoder plateaus early, curriculum benefits may not materialize.

### Mechanism 3
- **Claim**: Separating training (LLM-guided) from inference (bi-encoder only) achieves both accuracy and efficiency.
- **Mechanism**: LLM used only during training to generate high-quality pseudo labels, then lightweight bi-encoder handles all inference without LLM.
- **Core assumption**: Quality pseudo labels from LLM can be effectively transferred to bi-encoder training.
- **Evidence anchors**: Sublinear inference complexity achieved through MIPS indexing.
- **Break condition**: If pseudo labels don't transfer well, bi-encoder may underperform despite training with LLM guidance.

## Foundational Learning

- **Concept**: Extreme Multi-label Text Classification (XMC)
  - **Why needed here**: LMTX operates in XMC setting with potentially millions of labels, requiring understanding of label sparsity and retrieval challenges.
  - **Quick check question**: In XMC, if a document has 5 relevant labels out of 100,000 possible labels, what is the precision at k=5 if the model ranks all 5 correctly?

- **Concept**: Bi-encoder architecture with shared weights
  - **Why needed here**: LMTX uses bi-encoder to embed both documents and labels into same space for efficient similarity search.
  - **Quick check question**: Why does LMTX use shared weights for document and label encoders instead of separate encoders?

- **Concept**: Triplet loss for embedding learning
  - **Why needed here**: LMTX trains bi-encoder using triplet loss with pseudo positive labels from LLM and in-batch negatives.
  - **Quick check question**: In triplet loss, what is the role of the margin parameter γ, and what happens if it's set too large?

## Architecture Onboarding

- **Component map**: Input documents → Bi-encoder (DistilBERT) → ANNS index → LLM teacher → Triplet loss → Trained bi-encoder → MIPS index for inference
- **Critical path**: Document → Bi-encoder → ANNS shortlist → LLM relevance check → Pseudo positive label → Triplet loss update
- **Design tradeoffs**: LLM accuracy vs computational cost during training; shortlist size vs retrieval quality vs LLM evaluation cost
- **Failure signatures**: Poor performance if LLM selects irrelevant labels; slow training if shortlist too large; model collapse if negative sampling strategy flawed
- **First 3 experiments**:
  1. Test bi-encoder with ground truth labels to establish upper bound performance
  2. Vary shortlist size (5, 10, 15) to find optimal tradeoff between LLM evaluation cost and label quality
  3. Compare different LLM teachers (WizardLM, Llama2, Vicuna) to identify best teacher model for label selection

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: How does the performance of LMTX scale when using larger training datasets compared to the 30k subset used in experiments?
- **Basis in paper**: The paper mentions that LMTX was trained on subsets of large datasets (30k documents each) and notes potential for further improvement with larger training sets, but does not provide experimental evidence for this claim.
- **Why unresolved**: The paper only evaluates LMTX on reduced subsets of large datasets to manage computational costs, leaving the performance potential on full datasets unexplored.
- **What evidence would resolve it**: Experiments comparing LMTX performance on full versus subset versions of the same datasets, showing how precision and recall metrics change with increased training data.

### Open Question 2
- **Question**: What is the impact of different curriculum learning strategies on LMTX's performance, beyond the simple epoch-based progression described?
- **Basis in paper**: The paper mentions that LMTX employs a curriculum-based method that dynamically adjusts based on relevance feedback from the LLM, but does not explore alternative curriculum strategies.
- **Why unresolved**: The paper only describes one approach to curriculum learning without comparing it to other potential strategies like difficulty-based progression or adaptive sampling rates.
- **What evidence would resolve it**: Comparative experiments testing LMTX with different curriculum learning strategies, measuring how each affects convergence speed and final performance metrics.

### Open Question 3
- **Question**: How does LMTX's performance compare when using list-wise versus point-wise prompts for the LLM teacher model?
- **Basis in paper**: The paper mentions that the current LLM pseudo-labeling approach relies on point-wise feedback, which is time-consuming, suggesting that alternative prompting strategies could be explored.
- **Why unresolved**: The paper uses point-wise prompts for the LLM but does not investigate whether list-wise prompts could provide more efficient or accurate label selection.
- **What evidence would resolve it**: Direct comparison experiments using both point-wise and list-wise prompting approaches with the same datasets, measuring both performance differences and computational efficiency.

### Open Question 4
- **Question**: What is the optimal shortlist size for LMTX across different datasets and how does it vary with dataset characteristics?
- **Basis in paper**: The paper provides an analysis of shortlist size sensitivity for EURLex-4K dataset, finding optimal performance at size 10, but does not explore this across all datasets or examine dataset-dependent variations.
- **Why unresolved**: While the paper identifies optimal shortlist size for one dataset, it does not systematically investigate how this parameter should be tuned for different datasets with varying characteristics.
- **What evidence would resolve it**: Systematic experiments varying shortlist size across all datasets, correlating optimal sizes with dataset characteristics like label space size, average labels per document, and label distribution patterns.

## Limitations

- Framework performance heavily depends on LLM's zero-shot reasoning capability, which may vary across domains and languages
- Computational cost of repeated LLM evaluations during training may become prohibitive at truly extreme scales
- Performance relies on predefined label set, limiting applicability to scenarios with dynamically emerging labels

## Confidence

- **High confidence**: The bi-encoder architecture design and separation of training/inference are well-established techniques
- **Medium confidence**: The curriculum learning approach based on LLM feedback is novel but relies on assumptions about correlation between bi-encoder improvement and label shortlist quality
- **Medium confidence**: The claim of 37% improvement over baselines is based on specific experimental conditions and may not generalize across all XMC datasets

## Next Checks

1. **Cross-domain validation**: Test LMTX on non-English datasets and domains with different label characteristics (e.g., scientific literature, legal documents) to assess generalization beyond current benchmarks
2. **Ablation study on LLM dependency**: Systematically evaluate how LMTX performance degrades when using simpler zero-shot classifiers instead of LLM for label selection, to quantify actual contribution of LLM reasoning
3. **Scalability analysis**: Measure training time and memory usage as label set size scales from 10K to 1M labels, identifying exact point where LLM evaluation costs become prohibitive