---
ver: rpa2
title: Multi-Facet Blending for Faceted Query-by-Example Retrieval
arxiv_id: '2412.01443'
source_url: https://arxiv.org/abs/2412.01443
tags:
- facet
- facets
- fable
- document
- retrieval
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces FaBle, a multi-facet blending method that
  improves faceted query-by-example (QBE) retrieval by decomposing and recomposing
  documents into facet-specific units. FaBle leverages large language models to generate
  similar and dissimilar facet fragments, then recombines them into synthetic training
  pairs without requiring pre-defined facet labels.
---

# Multi-Facet Blending for Faceted Query-by-Example Retrieval

## Quick Facts
- arXiv ID: 2412.01443
- Source URL: https://arxiv.org/abs/2412.01443
- Authors: Heejin Do; Sangwon Ryu; Jonghwi Kim; Gary Geunbae Lee
- Reference count: 19
- This paper introduces FaBle, a multi-facet blending method that improves faceted query-by-example (QBE) retrieval by decomposing and recomposing documents into facet-specific units.

## Executive Summary
This paper introduces FaBle, a multi-facet blending method that improves faceted query-by-example (QBE) retrieval by decomposing and recomposing documents into facet-specific units. FaBle leverages large language models to generate similar and dissimilar facet fragments, then recombines them into synthetic training pairs without requiring pre-defined facet labels. Applied to scientific paper abstract retrieval using only 1,000 documents, FaBle outperforms or matches models trained on over 1.3 million citation-labeled examples, with notable gains on the challenging method facet (up to +7.6% NDCG@20). The method also generalizes well to educational exam item retrieval, demonstrating domain-agnostic effectiveness. A new benchmark dataset, FEIR, is introduced for this domain. FaBle shows that modular augmentation can enhance fine-grained retrieval in low-resource settings.

## Method Summary
FaBle is a three-stage pipeline that uses large language models to decompose documents into facet-specific summaries, generate similar and dissimilar facet fragments, and recombine them into synthetic training pairs. The method employs LLaMA2-13B for prompt-based facet identification and generation, followed by contrastive learning with triplet loss on the SPECTER model. By leveraging LLM capabilities to create facet-wise relevance-informed document pairs, FaBle eliminates the need for pre-defined facet knowledge or labels while achieving strong performance with minimal training data.

## Key Results
- FaBle achieves up to +7.6% NDCG@20 improvement on the method facet compared to state-of-the-art models
- The method outperforms or matches models trained on over 1.3 million citation-labeled examples using only 1,000 documents
- FaBle demonstrates domain-agnostic effectiveness by generalizing to educational exam item retrieval with the newly introduced FEIR benchmark

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Decomposing documents into facet-specific summaries and generating similar/dissimilar fragments improves retrieval precision.
- Mechanism: The method uses a two-stage prompt approach where a document is first summarized into facet-specific summaries (Sf) using LLaMA2, and then these summaries are used to guide the generation of similar (Cf_sim) and dissimilar (Cf_dis) facet fragments through self-feeding. This allows the model to create more targeted and relevant content for each facet.
- Core assumption: The intrinsic distinguishing capabilities of LLMs can be effectively leveraged to generate facet-specific content that aligns with user intent.
- Evidence anchors:
  - [abstract]: "We automatically decompose documents into facet units and generate (ir)relevant pairs by leveraging LLMs' intrinsic distinguishing capabilities; then, dynamically recomposing the units leads to facet-wise relevance-informed document pairs."
  - [section]: "By prompting the model to summarize a desired facet within the document, the intended facet-distinct information is extracted."
- Break condition: If the LLM fails to accurately identify or generate facet-specific content, the entire process becomes ineffective.

### Mechanism 2
- Claim: Recomposing generated facet fragments into synthetic documents creates effective training pairs for fine-tuning retrieval models.
- Mechanism: The method combines similar and dissimilar facet fragments using a recomposition strategy to form positive and negative document pairs for a query document. This allows the model to learn facet-specific similarities and differences, improving its ability to retrieve relevant documents based on facet conditions.
- Core assumption: Synthetic document pairs created through recomposition can effectively represent facet-wise relevance for training purposes.
- Evidence anchors:
  - [abstract]: "Our modularization eliminates the need for pre-defined facet knowledge or labels."
  - [section]: "Consequently, the triplet pair (Df ;Q, Df +, Df −) is constructed for the query document Df ;Q, conditioned on a target facet f."
- Break condition: If the recomposition strategy fails to create meaningful document pairs, the training process will not benefit from the synthetic data.

### Mechanism 3
- Claim: Hard negative mining enhances the model's ability to distinguish between relevant and irrelevant documents by focusing on challenging negative samples.
- Mechanism: The method uses MiniLM to score the similarity between summarized facets and generated dissimilar facet components, regenerating those with scores between 0.25 and 0.5 to create hard negatives. This ensures that the model is trained on more challenging examples, improving its discriminative power.
- Core assumption: Challenging negative samples lead to better representation learning and improved retrieval performance.
- Evidence anchors:
  - [abstract]: "We explicitly prompt the LLM to create facets of different topics to generate negative (dissimilar) ones for a specific facet."
  - [section]: "We then measure the MiniLM scores for the regenerated facets and identify those below 0.5 as hard negatives."
- Break condition: If the hard negative generation process fails to create sufficiently challenging samples, the model's discriminative power will not improve.

## Foundational Learning

- Concept: Understanding the basics of query-by-example (QBE) retrieval and its limitations.
  - Why needed here: QBE is the fundamental task that this paper aims to improve by introducing faceted QBE. Understanding its limitations helps in appreciating the need for a more fine-grained approach.
  - Quick check question: What is the primary limitation of traditional QBE retrieval that faceted QBE aims to address?

- Concept: Familiarity with large language models (LLMs) and their capabilities in generation tasks.
  - Why needed here: The paper leverages LLMs to decompose documents into facet-specific summaries and generate similar/dissimilar fragments. Understanding LLMs' capabilities is crucial for grasping the method's approach.
  - Quick check question: How do LLMs' intrinsic distinguishing capabilities contribute to the generation of facet-specific content in this method?

- Concept: Knowledge of contrastive learning and its application in representation learning.
  - Why needed here: The paper uses contrastive learning to fine-tune the retrieval model with the synthetic document pairs created through recomposition. Understanding contrastive learning is essential for understanding the training process.
  - Quick check question: How does contrastive learning help in learning facet-specific similarities and differences in this method?

## Architecture Onboarding

- Component map: Document Decomposition -> Facet Generation -> Facet Recomposition -> Model Fine-tuning -> Hard Negative Mining
- Critical path: Document Decomposition → Facet Generation → Facet Recomposition → Model Fine-tuning → Hard Negative Mining
- Design tradeoffs: The method trades off the complexity of manual annotation for the automation of synthetic data generation. It also relies on the capabilities of LLMs, which may introduce variability in the quality of generated content.
- Failure signatures: If the generated facet fragments do not accurately represent the intended facets, the entire process will fail. Similarly, if the recomposition strategy does not create meaningful document pairs, the training process will not benefit.
- First 3 experiments:
  1. Test the accuracy of the document decomposition step by comparing the generated summaries to manually annotated facets.
  2. Evaluate the quality of the generated similar and dissimilar facet fragments by assessing their relevance to the intended facets.
  3. Measure the effectiveness of the recomposition strategy by comparing the synthetic document pairs to manually created ones in terms of facet-wise relevance.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does FaBle's performance scale with increasing numbers of original documents beyond 1,000?
- Basis in paper: [explicit] The paper deliberately limited the initial data to 1,017 documents to validate effectiveness in data-scarce settings, and observed performance improvements with increased data size in experiments.
- Why unresolved: The paper focused on proving effectiveness with minimal data rather than exploring performance at scale, and only tested up to 1,000 documents.
- What evidence would resolve it: Experiments showing NDCG@20 and MAP scores as a function of training data size (e.g., 1K, 5K, 10K, 50K documents) would clarify scalability limits and diminishing returns.

### Open Question 2
- Question: Can FaBle be effectively applied to domains with very different facet structures, such as legal documents or news articles?
- Basis in paper: [explicit] The paper demonstrated FaBle on scientific papers and educational items, but noted it is broadly applicable to domains with distinct facets, and suggested future work on other fields.
- Why unresolved: Only two domains were tested, and the paper did not investigate how well FaBle adapts to domains with more complex or less structured facet hierarchies.
- What evidence would resolve it: Applying FaBle to legal case retrieval or multi-topic news summarization and comparing results to domain-specific baselines would show generalizability limits.

### Open Question 3
- Question: How sensitive is FaBle's performance to the choice of LLM (e.g., using GPT-4 instead of LLaMA2)?
- Basis in paper: [explicit] The paper used LLaMA2-13B for its open access and ease of use, and noted that self-feeding helped target-oriented generation, but did not compare other LLMs.
- Why unresolved: No ablation studies or comparisons with other LLMs (e.g., GPT-3.5, GPT-4) were performed, leaving uncertainty about whether performance gains are tied to model size or prompting strategy.
- What evidence would resolve it: Head-to-head comparisons of FaBle using different LLMs on the same dataset, measuring generation quality and retrieval performance, would isolate the contribution of the model choice.

## Limitations
- Heavy dependence on LLM quality for facet decomposition and generation, which may not hold for highly specialized domains
- Effectiveness relies on the assumption that combining generated fragments preserves semantic coherence, which may not always be true
- Performance could degrade when applied to longer documents or those with more complex facet structures

## Confidence

- **High Confidence**: The core mechanism of using synthetic document pairs for contrastive learning is well-established in the literature, and the reported improvements on NDCG@20 and MAP metrics are supported by the experimental results.
- **Medium Confidence**: The effectiveness of the three-stage pipeline (decomposition, generation, recomposition) is demonstrated on two datasets, but the generalizability to other domains and document types remains to be fully validated.
- **Low Confidence**: The specific prompt templates and hyperparameter choices that led to optimal performance are not fully disclosed, making it difficult to assess the reproducibility and potential variability in results.

## Next Checks

1. **Cross-Domain Validation**: Apply FaBle to a third, previously unseen domain (e.g., medical literature or legal documents) to test its domain-agnostic capabilities and identify any domain-specific limitations.

2. **Ablation Study on LLM Components**: Conduct an ablation study by varying the LLM used for decomposition and generation (e.g., using different model sizes or architectures) to quantify the impact of LLM quality on overall performance.

3. **Human Evaluation of Generated Pairs**: Perform a human evaluation of a sample of the synthetic document pairs to assess the quality and relevance of the generated facets and the effectiveness of the recomposition strategy. This would provide qualitative insights into the method's strengths and weaknesses.