---
ver: rpa2
title: 'Fineweb-Edu-Ar: Machine-translated Corpus to Support Arabic Small Language
  Models'
arxiv_id: '2411.06402'
source_url: https://arxiv.org/abs/2411.06402
tags:
- arabic
- zhang
- wang
- yang
- dataset
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work addresses the scarcity of high-quality Arabic pretraining
  data for small language models by introducing FineWeb-Edu-Ar, a machine-translated
  version of the English FineWeb-Edu dataset. The dataset contains 202B Arabic tokens
  generated using the nllb-200-distilled-600M translation model.
---

# Fineweb-Edu-Ar: Machine-translated Corpus to Support Arabic Small Language Models

## Quick Facts
- arXiv ID: 2411.06402
- Source URL: https://arxiv.org/abs/2411.06402
- Reference count: 18
- Primary result: A 202B token Arabic pretraining dataset created by translating English educational content, enabling development of Arabic small language models

## Executive Summary
This work addresses the scarcity of high-quality Arabic pretraining data for small language models by introducing FineWeb-Edu-Ar, a machine-translated version of the English FineWeb-Edu dataset. The dataset contains 202B Arabic tokens generated using the nllb-200-distilled-600M translation model. The authors conducted a systematic evaluation of 12 English-to-Arabic machine translation models, benchmarking them on translation quality and runtime performance. They selected the Pareto-optimal model that balanced quality and computational efficiency, producing the dataset within 480 GPU-days and approximately 2000 kg CO2eq emissions. FineWeb-Edu-Ar is the largest publicly available machine-translated Arabic dataset, designed to support the development of Arabic small language models.

## Method Summary
The authors translated the deduplicated FineWeb-Edu dataset using nllb-200-distilled-600M, processing text in 200-token chunks without overlap. They benchmarked 12 English-to-Arabic MT models using LLM-as-a-Judge with GPT-4o, selecting the model that balanced translation quality and computational efficiency. The resulting FineWeb-Edu-Ar dataset contains 202B Arabic tokens in JSONL format with metadata, stored on HuggingFace.

## Key Results
- Generated 202B Arabic tokens through machine translation of English educational content
- Selected nllb-200-distilled-600M as the Pareto-optimal translation model balancing quality and efficiency
- Produced the dataset within 480 GPU-days and approximately 2000 kg CO2eq emissions
- Created the largest publicly available machine-translated Arabic pretraining corpus

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Machine translation of high-quality English educational data fills the gap for Arabic pretraining data scarcity.
- Mechanism: High-resource English corpora like FineWeb-Edu contain generic knowledge that can be translated to Arabic, supplementing the limited availability of native Arabic educational data.
- Core assumption: The generic knowledge in English educational texts is largely language-agnostic and valuable for Arabic SLMs.
- Evidence anchors:
  - [abstract]: "A key technique in this space is machine translation (MT), where high quality English text is adapted to a target, comparatively low-resource, language."
  - [section]: "Many languages, including Arabic, suffer from a distinct lack of the same kind of high quality, educational focused, and readily available data that allowed other small language models to flourish."
  - [corpus]: Weak - the paper mentions the dataset is "noisy in the sense of translation inaccuracies" and may not include enough regional facts of Arabic-speaking countries.
- Break condition: If the translated data contains too many inaccuracies or if the knowledge is too culturally specific to English-speaking contexts.

### Mechanism 2
- Claim: Selecting a Pareto-optimal translation model balances quality and computational efficiency for large-scale corpus generation.
- Mechanism: Evaluating multiple English-to-Arabic MT models on translation quality and runtime performance allows choosing a model that provides acceptable quality within computational budget constraints.
- Core assumption: A model exists that offers a good tradeoff between translation quality and computational cost.
- Evidence anchors:
  - [abstract]: "They selected the Pareto-optimal model that balanced quality and computational efficiency, producing the dataset within 480 GPU-days and approximately 2000 kg CO2eq emissions."
  - [section]: "We choose nllb-200-distilled-600M as the best MT model within the computational budget of 500 A100 GPU-days."
  - [corpus]: Weak - the paper doesn't provide detailed evidence of the quality of the selected model beyond its position on the Pareto frontier.
- Break condition: If the computational budget is too constrained to allow for acceptable translation quality, or if the quality requirements are too high for the available computational resources.

### Mechanism 3
- Claim: Using a deduplicated version of the source corpus reduces redundant processing and conserves emissions.
- Mechanism: Processing only the deduplicated version of Fineweb-Edu present in the training set of SmolLM reduces the amount of data to be translated, saving computational resources and emissions.
- Core assumption: The deduplicated version contains sufficient unique and valuable content for pretraining.
- Evidence anchors:
  - [section]: "Furthermore, to reduce redundant processing and conserve emissions, we processed only the deduplicated version of Fineweb-Edu present in the training set of SmolLM, a successful English SLM."
  - [abstract]: Not explicitly mentioned, but implied by the focus on computational efficiency.
  - [corpus]: Weak - the paper doesn't provide evidence that the deduplicated version is sufficient for pretraining purposes.
- Break condition: If the deduplicated version lacks sufficient diversity or coverage of the original corpus, leading to poor pretraining outcomes.

## Foundational Learning

- Concept: Machine Translation Quality Assessment
  - Why needed here: To evaluate the performance of different MT models and select the best one for translating the corpus.
  - Quick check question: How does the LLM-as-a-Judge approach work for assessing translation quality, and what are its limitations?

- Concept: Computational Cost Estimation
  - Why needed here: To estimate the emissions and financial cost of translating the corpus using different MT models.
  - Quick check question: How do you calculate the CO2 emissions and financial cost of training or inference tasks on GPUs?

- Concept: Dataset Properties and Schema
  - Why needed here: To understand the structure and content of the generated dataset for proper usage in pretraining.
  - Quick check question: What information does the JSON schema provide about the dataset, and how can you access the Arabic and English passages?

## Architecture Onboarding

- Component map: FineWeb-Edu (English corpus) -> nllb-200-distilled-600M (MT model) -> Translation pipeline (200-token chunks) -> FineWeb-Edu-Ar (Arabic corpus)

- Critical path: Translate deduplicated FineWeb-Edu using nllb-200-distilled-600M within 480 GPU-days budget while maintaining acceptable translation quality.

- Design tradeoffs: The main tradeoff is between translation quality and computational efficiency. Choosing a higher-quality model may lead to better pretraining outcomes but at a higher computational cost and emissions. Conversely, choosing a more efficient model may reduce costs but potentially compromise quality.

- Failure signatures: Poor translation quality leading to ineffective pretraining, exceeding computational budget or emissions constraints, or the generated dataset lacking sufficient diversity or coverage for pretraining purposes.

- First 3 experiments:
  1. Translate a small sample of the English corpus using the selected MT model and evaluate the translation quality using the LLM-as-a-Judge approach.
  2. Estimate the computational cost and emissions of translating the entire deduplicated corpus using the selected model and compare it with the allocated budget.
  3. Analyze the properties and schema of the generated dataset to ensure it meets the requirements for pretraining Arabic SLMs.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal trade-off between translation quality and computational cost for machine-translated Arabic datasets, and how does this vary with different use cases (e.g., pretraining vs. fine-tuning)?
- Basis in paper: [explicit] The paper discusses selecting the nllb-200-distilled-600M model as a balance between quality and cost, but acknowledges that the optimal balance for foundational models remains open.
- Why unresolved: The paper does not provide empirical evidence on how different translation quality levels impact downstream model performance, leaving the optimal balance unclear.
- What evidence would resolve it: A systematic study comparing model performance (e.g., on MMLU or other benchmarks) trained on datasets with varying translation quality levels would clarify the trade-off.

### Open Question 2
- Question: How do machine-translated Arabic datasets compare to native Arabic datasets in terms of cultural relevance and factual accuracy for Arabic-speaking regions?
- Basis in paper: [explicit] The paper notes that FineWeb-Edu's knowledge domain is English-centric and may lack regional facts of Arabic-speaking countries, suggesting a potential limitation of machine-translated data.
- Why unresolved: The paper does not provide a comparative analysis of cultural relevance or factual accuracy between machine-translated and native Arabic datasets.
- What evidence would resolve it: A study evaluating the cultural and factual accuracy of machine-translated Arabic datasets against native Arabic datasets, possibly using native Arabic speakers as judges, would address this question.

### Open Question 3
- Question: What is the impact of machine translation inaccuracies on the performance of Arabic small language models, and how can these inaccuracies be mitigated?
- Basis in paper: [explicit] The paper mentions that the dataset can be considered "noisy" due to translation inaccuracies but does not explore their impact on model performance or mitigation strategies.
- Why unresolved: The paper does not provide data on how translation errors affect downstream tasks or methods to reduce their impact.
- What evidence would resolve it: Experiments measuring the effect of translation errors on model performance and testing mitigation techniques (e.g., post-editing, filtering) would provide insights into this issue.

## Limitations

- Translation quality remains uncertain with limited direct evidence of the resulting translation quality.
- The dataset may lack Arabic-specific cultural and regional knowledge due to English-centric source content.
- The deduplicated subset may not provide sufficient diversity for robust Arabic SLM pretraining.

## Confidence

**High Confidence**: Computational cost estimates (480 GPU-days, ~2000 kg CO2eq) and MT model selection process based on benchmarking 12 models.

**Medium Confidence**: Claim that FineWeb-Edu-Ar is the largest publicly available machine-translated Arabic dataset; assertion that it addresses Arabic SLM pretraining needs requires empirical validation.

**Low Confidence**: Claim that translated English educational data sufficiently fills the gap for Arabic pretraining data scarcity; lacks direct evidence about how well translated content serves Arabic-specific contexts.

## Next Checks

1. **Translation Quality Assessment**: Conduct comprehensive evaluation using automated metrics (BLEU, COMET) and human evaluation on a stratified sample, comparing against native Arabic educational corpora to quantify noise level and identify systematic translation errors.

2. **Pretraining Impact Study**: Train Arabic SLMs using FineWeb-Edu-Ar and evaluate performance on downstream tasks compared to models trained on native Arabic data or smaller machine-translated datasets, measuring both effectiveness and efficiency gains.

3. **Cultural and Linguistic Coverage Analysis**: Analyze the dataset for representation of Arabic-specific knowledge, dialects, and cultural contexts, identifying gaps where translated English content may be insufficient or inappropriate for Arabic language understanding.