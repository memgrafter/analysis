---
ver: rpa2
title: 'MultiTrust: A Comprehensive Benchmark Towards Trustworthy Multimodal Large
  Language Models'
arxiv_id: '2406.07057'
source_url: https://arxiv.org/abs/2406.07057
tags:
- images
- mllms
- image
- task
- which
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper presents MultiTrust, the first comprehensive benchmark
  for evaluating the trustworthiness of Multimodal Large Language Models (MLLMs).
  The benchmark addresses five key aspects: truthfulness, safety, robustness, fairness,
  and privacy, covering 32 diverse tasks with self-curated datasets.'
---

# MultiTrust: A Comprehensive Benchmark Towards Trustworthy Multimodal Large Language Models

## Quick Facts
- arXiv ID: 2406.07057
- Source URL: https://arxiv.org/abs/2406.07057
- Reference count: 40
- MultiTrust presents first comprehensive benchmark evaluating MLLM trustworthiness across 5 key aspects with 32 tasks and 21 models.

## Executive Summary
This paper introduces MultiTrust, the first comprehensive benchmark for evaluating the trustworthiness of Multimodal Large Language Models (MLLMs). The benchmark systematically assesses trustworthiness across five key aspects: truthfulness, safety, robustness, fairness, and privacy, covering 32 diverse tasks with self-curated datasets. Extensive experiments on 21 modern MLLMs reveal significant trustworthiness issues and risks, particularly in proprietary models' vulnerability to multimodal jailbreaking and adversarial attacks. The study demonstrates that multimodality amplifies inherent risks from base LLMs, with models more prone to disclosing privacy in text and revealing ideological and cultural biases even when paired with irrelevant images.

## Method Summary
MultiTrust evaluates 21 modern MLLMs (4 proprietary, 17 open-source) across 32 diverse tasks covering five trustworthiness aspects. The benchmark uses self-curated datasets including 8 constructed from scratch and 24 adapted or improved from existing sources. Models are evaluated using various metrics including accuracy, toxicity scores, refuse-to-answer rates, and attack success rates. A unified interface is provided to standardize evaluation across different model architectures and training techniques.

## Key Results
- MLLMs exhibit significant trustworthiness issues across truthfulness, safety, robustness, fairness, and privacy dimensions
- Proprietary models show better performance due to safety guardrails but remain vulnerable to multimodal jailbreaking and adversarial attacks
- Multimodal training compromises base LLM alignment, increasing privacy disclosure and bias revelation even with irrelevant visual inputs

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The evaluation strategy covering multimodal risks and cross-modal impacts reveals that multimodality amplifies internal risks from base LLMs.
- Mechanism: By designing tasks that compare text-only performance with multimodal inputs (both relevant and irrelevant), the study isolates the effects of added visual context on base LLM behavior. This allows identification of instability and risk amplification due to multimodality.
- Core assumption: The added visual modality interacts with the base LLM in ways that can destabilize previously stable behaviors.
- Evidence anchors:
  - [abstract] "The multimodality amplifies the internal risks from base LLMs"
  - [section] "we advocate that it is equally important to consider the interaction between modalities"
  - [corpus] Weak - no direct evidence, only related studies on MLLM risks

### Mechanism 2
- Claim: Proprietary models demonstrate better trustworthiness due to safety guardrails and alignment efforts.
- Mechanism: Proprietary models are likely trained with more extensive safety fine-tuning and alignment data, leading to better refusal rates and lower toxicity in responses.
- Core assumption: Safety guardrails and alignment training improve model behavior in risky scenarios.
- Evidence anchors:
  - [abstract] "GPT-4V [107] and Claude3 [8] demonstrate better performance due to their safety guardrails"
  - [section] "proprietary models still struggle with the perception of visually confusing images and are vulnerable to multimodal jailbreaking"
  - [corpus] Weak - only mentions related studies on LLM safety

### Mechanism 3
- Claim: Multimodal training compromises the alignment of base LLMs, increasing trustworthiness risks.
- Mechanism: When base LLMs are fine-tuned with multimodal data, the original safety alignment can be weakened or overwritten, leading to more jailbreaking susceptibility and privacy leaks.
- Core assumption: Fine-tuning on multimodal data can overwrite or weaken safety alignment learned during base LLM training.
- Evidence anchors:
  - [abstract] "MLLMs are more inclined to disclose privacy in text and reveal ideological and cultural biases even when paired with irrelevant images"
  - [section] "multimodal training with LLM significantly compromises its previous trustworthy alignment"
  - [corpus] Weak - only mentions related studies on LLM alignment

## Foundational Learning

- Concept: Multimodal Large Language Models (MLLMs) combine visual and textual data processing capabilities.
  - Why needed here: Understanding MLLMs is crucial for comprehending the trustworthiness challenges introduced by the multimodal nature.
  - Quick check question: What are the key differences between traditional LLMs and MLLMs in terms of input processing and potential risks?

- Concept: Trustworthiness dimensions (truthfulness, safety, robustness, fairness, privacy).
  - Why needed here: These dimensions provide the framework for evaluating and analyzing MLLM trustworthiness comprehensively.
  - Quick check question: Can you list the five primary aspects of trustworthiness evaluated in MultiTrust and briefly explain their significance?

- Concept: Cross-modal impacts and multimodal risks.
  - Why needed here: These concepts are central to the study's evaluation strategy, focusing on how visual inputs affect base LLM behavior and introduce new risks.
  - Quick check question: How do cross-modal impacts differ from multimodal risks in the context of MLLM trustworthiness evaluation?

## Architecture Onboarding

- Component map: MultiTrust benchmark consists of 32 tasks organized under 5 trustworthiness aspects (truthfulness, safety, robustness, fairness, privacy), with datasets curated for each task and metrics defined for evaluation.
- Critical path: 1) Design tasks and datasets, 2) Deploy and configure 21 MLLMs, 3) Run evaluations and collect results, 4) Analyze results and draw conclusions.
- Design tradeoffs: Comprehensive evaluation vs. computational cost; focusing on specific risks vs. broader coverage; using existing datasets vs. creating new ones.
- Failure signatures: Inconsistent results across tasks; models failing to complete evaluations; dataset quality issues; metric calculation errors.
- First 3 experiments:
  1. Run a subset of tasks on a single MLLM to validate task design and data quality.
  2. Compare results across different MLLMs on a single task to check for consistency and identify potential issues.
  3. Perform a small-scale cross-modal impact analysis on a subset of tasks to validate the evaluation strategy.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the multimodality specifically affect the safety mechanisms of base LLMs during multimodal training?
- Basis in paper: [explicit] The paper states that "the multimodal training with LLM significantly compromises its previous trustworthy alignment" and provides examples like MiniGPT-4-Llama2 and mPLUG-Owl2.
- Why unresolved: The paper identifies the phenomenon but does not deeply investigate the specific mechanisms by which multimodal training weakens the safety mechanisms of base LLMs. It would be valuable to understand whether this is due to architectural changes, training data distribution shifts, or other factors.
- What evidence would resolve it: Detailed ablation studies comparing the safety mechanisms of base LLMs before and after multimodal training, or analyses of how specific components of the training process impact safety alignment.

### Open Question 2
- Question: What are the long-term societal impacts of MLLMs' trustworthiness issues, particularly regarding fairness and privacy?
- Basis in paper: [inferred] The paper discusses various trustworthiness issues but does not extensively explore their long-term societal consequences. It mentions potential impacts like "perpetuate social inequalities" and "violate personal rights" but does not provide a comprehensive analysis.
- Why unresolved: While the paper identifies trustworthiness issues, it does not fully explore how these issues might manifest and compound over time in real-world applications, potentially leading to systemic biases or privacy violations.
- What evidence would resolve it: Longitudinal studies tracking the deployment of MLLMs in various sectors, analyzing how trustworthiness issues evolve and impact different demographic groups over extended periods.

### Open Question 3
- Question: How can the evaluation framework be extended to assess trustworthiness in real-time, dynamic environments where MLLMs interact with users and other AI agents?
- Basis in paper: [inferred] The paper focuses on static evaluation of MLLMs but does not address how to assess trustworthiness in dynamic, interactive scenarios. It mentions "agents" and "dynamic environments" briefly in the discussion but does not provide a concrete framework.
- Why unresolved: The current evaluation framework is designed for controlled, static scenarios. Real-world applications of MLLMs often involve dynamic interactions that may reveal new trustworthiness challenges not captured in static evaluations.
- What evidence would resolve it: Development and validation of a dynamic evaluation framework that simulates real-world interactions, including adversarial scenarios, user feedback loops, and evolving contexts.

## Limitations

- The study's findings may not generalize to all existing or future MLLMs as the field is rapidly evolving
- Specific details of dataset curation and preprocessing steps are not fully elaborated, limiting reproducibility
- The benchmark may not encompass all possible scenarios or edge cases that could arise in real-world applications

## Confidence

**High Confidence:**
- MLLMs exhibit significant trustworthiness issues and risks across various aspects
- Proprietary models generally demonstrate better performance due to safety guardrails
- Multimodal training with LLMs compromises the alignment of base LLMs

**Medium Confidence:**
- The multimodality amplifies the internal risks from base LLMs
- The MultiTrust benchmark provides a comprehensive and scalable toolbox for standardized trustworthiness research

**Low Confidence:**
- The specific mechanisms by which multimodal training compromises base LLM alignment are not fully elucidated
- The generalizability of the study's findings to real-world applications is uncertain

## Next Checks

1. Conduct a thorough evaluation of the datasets used in the MultiTrust benchmark to assess their quality, representativeness, and potential biases
2. Perform a generalizability study to evaluate how well MultiTrust results translate to real-world MLLM applications across diverse scenarios
3. Conduct longitudinal analysis of MLLM trustworthiness by periodically re-evaluating the benchmark on updated model versions to track progress and identify emerging risks