---
ver: rpa2
title: Towards Subgraph Isomorphism Counting with Graph Kernels
arxiv_id: '2405.07497'
source_url: https://arxiv.org/abs/2405.07497
tags:
- uni00000010
- uni0000002f
- graph
- kernel
- uni0000003a
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper pioneers the use of graph kernels to approximate subgraph
  isomorphism counting, a P-complete problem. The key idea is to construct Gram matrices
  from graph kernels to implicitly capture structural correlations, rather than relying
  on explicit feature representations.
---

# Towards Subgraph Isomorphism Counting with Graph Kernels

## Quick Facts
- **arXiv ID**: 2405.07497
- **Source URL**: https://arxiv.org/abs/2405.07497
- **Authors**: Xin Liu; Weiqi Wang; Jiaxin Bai; Yangqiu Song
- **Reference count**: 40
- **Primary result**: Graph kernels with neighborhood information extraction achieve state-of-the-art performance on subgraph isomorphism counting for homogeneous data

## Executive Summary
This paper pioneers the application of graph kernels to the #P-complete problem of subgraph isomorphism counting. Instead of explicit feature extraction, the authors construct Gram matrices from graph kernels to implicitly capture structural correlations among substructures. They introduce neighborhood information extraction through modified color assignment algorithms and apply kernel tricks (polynomial and Gaussian) for efficient computation. Experiments demonstrate that this approach achieves superior performance on homogeneous datasets while revealing limitations on heterogeneous data.

## Method Summary
The method treats subgraph isomorphism counting as a regression problem using graph kernels to approximate counts. Gram matrices are constructed from various graph kernels (SP, GR, WLOA, WL, k-WL variants) to capture pairwise similarities between graphs and patterns. Kernel tricks (polynomial and RBF) enable implicit transformations to higher-dimensional spaces. Neighborhood information extraction modifies the WL algorithm by incorporating edge colors into the color histogram. Ridge regression is then trained on the transformed Gram matrices with hyperparameters tuned on validation sets.

## Key Results
- Graph kernels with neighborhood information extraction achieve state-of-the-art performance on homogeneous synthetic datasets
- The approach significantly outperforms naive baselines and neural networks in challenging counting scenarios
- Performance degrades on heterogeneous datasets (IMDB-BINARY, IMDB-MULTI, ENZYMES), where traditional GR kernel remains superior
- Polynomial and RBF kernel tricks enable efficient computation without explicit feature mapping

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Gram matrices constructed from graph kernels implicitly capture structural correlations among substructures, enabling subgraph isomorphism counting without explicit feature extraction
- **Mechanism**: The kernel matrix K stores pairwise similarity scores between graphs, reflecting the alignment of substructures across the dataset. This implicitly encodes combinatorial relationships needed for counting isomorphisms through the reproducing property in implicit Hilbert space
- **Core assumption**: The similarity function preserves enough structural information to distinguish isomorphic from non-isomorphic subgraphs
- **Evidence anchors**: [abstract] mentions exploring polynomial and Gaussian kernel variants; [section] states that structural information can be implicitly captured within the matrix
- **Break condition**: If the kernel function fails to distinguish structurally different subgraphs, the Gram matrix won't encode sufficient information for accurate counting

### Mechanism 2
- **Claim**: Neighborhood Information Extraction (NIE) improves kernel performance by incorporating edge colors into the color histogram
- **Mechanism**: The modified color assignment algorithm adds pairwise colors to the histogram based on edges and latest node colors, creating a more expressive representation that records |VG| + |EG| colors instead of just |VG|
- **Core assumption**: The additional |EG| colors provide discriminative power for distinguishing subgraphs with different local neighborhood structures
- **Evidence anchors**: [section] describes the neighborhood-aware color assignment algorithm and notes NIE's crucial role for homogeneous data
- **Break condition**: When neighborhoods are uniformly distributed, edge colors become uninformative and NIE provides no benefit

### Mechanism 3
- **Claim**: Kernel tricks enable efficient implicit transformations to higher-dimensional spaces
- **Mechanism**: Polynomial kernel uses the identity (g⊤i gj + 1)p = Σk p!/(k!(p-k)!) (g⊤i gj)p-k 1k; RBF kernel uses exp(-∥gi - gj∥²/2σ²) = exp(-Ki,i - 2Ki,j + Kj,j/2σ²) to implicitly map to infinite dimensions
- **Core assumption**: Implicit computation tricks preserve mathematical properties while being computationally efficient
- **Evidence anchors**: [section] describes employing implicit computation tricks for efficient kernel transformations
- **Break condition**: Large polynomial power parameters cause overflow; poorly chosen σ in RBF kernel causes numerical instability

## Foundational Learning

- **Concept**: Graph isomorphism and subgraph isomorphism definitions
  - **Why needed here**: The entire problem formulation and evaluation metrics depend on understanding when two graphs are structurally equivalent
  - **Quick check question**: Given two graphs with the same number of vertices and edges but different labelings, can they still be isomorphic?

- **Concept**: Weisfeiler-Leman (WL) algorithm and its variants
  - **Why needed here**: The paper builds upon WL kernel foundations and extends them with neighborhood information extraction
  - **Quick check question**: Why does the 3-WL algorithm have higher representation power than the standard WL algorithm?

- **Concept**: Reproducing Kernel Hilbert Space (RKHS) and kernel trick
  - **Why needed here**: The paper relies on implicit feature spaces and efficient kernel computations rather than explicit feature extraction
  - **Quick check question**: How does the reproducing property θ(Gi) = Σj Kij θ(Gj) enable working in the implicit Hilbert space?

## Architecture Onboarding

- **Component map**: Data preprocessing -> Kernel computation -> Kernel tricks -> Neighborhood extraction -> Regression -> Evaluation
- **Critical path**: Gram matrix construction → Kernel trick application → Regression training → Evaluation
- **Design tradeoffs**:
  - Explicit vs. implicit feature representation: Gram matrices vs. neural network embeddings
  - Kernel selection: Trade-off between discriminative power and computational complexity
  - Neighborhood extraction: Additional expressiveness vs. increased memory and computation
- **Failure signatures**:
  - Poor performance on heterogeneous data despite good results on homogeneous data
  - Numerical instability when using polynomial kernel with large power parameters
  - Memory overflow when computing high-order k-WL kernels on large graphs
- **First 3 experiments**:
  1. Implement and compare basic WL kernel vs. NIE-WL kernel on a small homogeneous dataset to verify neighborhood information improves performance
  2. Test polynomial vs. RBF kernel tricks on a dataset with known structural patterns to observe overflow and stability issues
  3. Evaluate Ridge regression with different regularization parameters on a validation set to find optimal hyperparameter settings

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: How can we further improve the performance of graph kernels on heterogeneous datasets beyond what is achieved by neighborhood information extraction?
- **Basis in paper**: [explicit] The paper notes that NIE significantly improves performance on homogeneous data but is not as effective on heterogeneous datasets like ENZYMES
- **Why unresolved**: The paper does not explore alternative approaches or modifications to graph kernels that could better handle the complexity of heterogeneous data structures
- **What evidence would resolve it**: Experiments comparing enhanced graph kernel variants against existing methods on heterogeneous datasets

### Open Question 2
- **Question**: Can we develop a theoretical framework to explain the relationship between the representation power of graph kernels and their effectiveness in subgraph isomorphism counting?
- **Basis in paper**: [inferred] The paper discusses the gap between graph isomorphism testing and subgraph isomorphism counting, and how graph kernels can approximate the latter through implicit correlations
- **Why unresolved**: The paper focuses on empirical results and practical implementation, leaving the theoretical underpinnings unexplored
- **What evidence would resolve it**: A formal proof or theoretical model demonstrating the conditions under which graph kernels can accurately approximate subgraph isomorphism counts

### Open Question 3
- **Question**: How can we scale graph kernel methods to handle larger graphs and more complex patterns without running into memory or computational limitations?
- **Basis in paper**: [explicit] The paper mentions that some kernel methods encounter out-of-memory issues on certain datasets
- **Why unresolved**: The paper does not propose or test strategies to mitigate these scalability issues
- **What evidence would resolve it**: Experiments demonstrating the effectiveness of memory-efficient or distributed implementations of graph kernels on large-scale datasets

## Limitations

- Performance degrades significantly on heterogeneous datasets despite good results on homogeneous data
- Scalability challenges with memory limitations for high-order k-WL kernels on large graphs
- Lack of theoretical framework explaining why certain kernel designs work better for counting

## Confidence

- **High confidence**: The basic premise that subgraph isomorphism counting is #P-complete and can be approximated through regression is well-established
- **Medium confidence**: Experimental results showing improved performance on homogeneous data are convincing but limited dataset diversity reduces generality
- **Low confidence**: Claims about implicit correlation capture and specific benefits of NIE lack direct supporting evidence from the corpus

## Next Checks

1. **Ablation study**: Systematically remove neighborhood information extraction from the pipeline and measure performance degradation on homogeneous vs. heterogeneous datasets to quantify the specific contribution of NIE

2. **Numerical stability testing**: Implement polynomial kernel with varying power parameters (p=2, 3, 5, 10) on synthetic datasets and monitor for overflow or underflow conditions, establishing safe operating ranges

3. **Heterogeneous data stress test**: Apply the complete pipeline to additional heterogeneous datasets with known complex structures (e.g., social networks with communities) to evaluate whether the observed limitations on IMDB and ENZYMES generalize to other challenging cases