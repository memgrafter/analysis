---
ver: rpa2
title: 'FACT: Examining the Effectiveness of Iterative Context Rewriting for Multi-fact
  Retrieval'
arxiv_id: '2410.21012'
source_url: https://arxiv.org/abs/2410.21012
tags:
- retrieval
- tasks
- context
- fact
- iterative
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper identifies a "lost-in-the-middle" phenomenon where LLMs
  progressively lose track of critical information during multi-fact retrieval tasks,
  resulting in incomplete or inaccurate retrieval. To address this, the authors introduce
  FACT (Find All Crucial Texts), an iterative context-rewriting method that refines
  context through successive rounds of rewriting, enabling models to capture essential
  facts incrementally.
---

# FACT: Examining the Effectiveness of Iterative Context Rewriting for Multi-fact Retrieval

## Quick Facts
- **arXiv ID:** 2410.21012
- **Source URL:** https://arxiv.org/abs/2410.21012
- **Reference count:** 9
- **Primary result:** Iterative context rewriting significantly improves multi-fact retrieval accuracy (up to 99.9%) but shows limited effectiveness in general-purpose QA tasks.

## Executive Summary
This paper identifies a "lost-in-the-middle" phenomenon where LLMs progressively lose track of critical information during multi-fact retrieval tasks, resulting in incomplete or inaccurate retrieval. To address this, the authors introduce FACT (Find All Crucial Texts), an iterative context-rewriting method that refines context through successive rounds of rewriting, enabling models to capture essential facts incrementally. Experiments show FACT significantly enhances multi-fact retrieval performance across various tasks, achieving up to 99.9% accuracy in some retrieval tasks. However, improvements are less notable in general-purpose QA scenarios. The method demonstrates the need for more resilient long-context retrieval strategies and highlights the limitations of LLMs in multi-fact retrieval.

## Method Summary
FACT (Find All Crucial Texts) is an iterative context-rewriting method designed to address the "lost-in-the-middle" phenomenon in multi-fact retrieval. The approach consists of three core components: a Retrieve function that identifies candidate facts from context, a Rewrite function that refines the context by removing already-retrieved facts, and a Stop function that determines when to halt iterations. The method processes queries through multiple rounds, with each iteration focusing on remaining facts without competition from previously retrieved information. FACT is evaluated across retrieval tasks (RULER, Counting Stars) and QA tasks (NarrativeQA, Qasper, MultiFieldQA, HotpotQA, 2WikiMQA, MuSiQue) using various models including GPT-4o, GPT-4o-mini, Llama-3.1 8B Instruct, and Qwen-2.5 7B Instruct.

## Key Results
- FACT achieves up to 99.9% accuracy in multi-fact retrieval tasks (RULER), compared to 62.5% for direct retrieval baseline
- Performance improvements are task-dependent, with significant gains in retrieval tasks but minimal impact on general-purpose QA scenarios
- Model family differences are pronounced: GPT-4o variants consistently improve with iterative rewriting, while Llama-3.1 and Qwen-2.5 show performance decline
- The "lost-in-the-middle" phenomenon is validated through probe analysis showing systematic accuracy degradation during generation

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Iterative context rewriting progressively refines attention focus by removing already-retrieved facts.
- **Mechanism:** FACT performs successive retrieval rounds where identified facts are removed from context, allowing subsequent rounds to focus on remaining facts without competition for attention.
- **Core assumption:** LLMs lose track of critical information when multiple facts compete for attention in a single-pass retrieval.
- **Evidence anchors:**
  - [abstract] "FACT... refines context through successive rounds of rewriting. This approach enables models to capture essential facts incrementally, which are often overlooked in single-pass retrieval."
  - [section 2] "Conventional retrieval techniques... tend to focus on isolated facts, missing the broader context needed to retrieve all necessary information"
  - [corpus] Weak - corpus doesn't contain direct evidence about iterative rewriting mechanisms
- **Break condition:** If rewriting removes essential context needed for understanding remaining facts, performance may degrade rather than improve.

### Mechanism 2
- **Claim:** Progressive loss of information occurs during generation when tracking multiple facts simultaneously.
- **Mechanism:** As generation progresses, model's internal representations show declining accuracy in tracking previously retrieved facts until recovery near generation end.
- **Core assumption:** Model has limited capacity to maintain multiple fact representations concurrently during generation.
- **Evidence anchors:**
  - [section 2] "as the generation progresses, the model progressively loses information until it recovers at the final few generated tokens"
  - [section 2] "the performance degradation is not due to an overloaded number of key tokens" but rather fundamental constraint in model's capacity
  - [corpus] Weak - corpus lacks evidence about internal representation degradation patterns
- **Break condition:** If model capacity increases sufficiently to maintain multiple fact representations, the "lost-in-the-middle" phenomenon may disappear.

### Mechanism 3
- **Claim:** Iterative rewriting effectiveness varies significantly across model families and task types.
- **Mechanism:** Models trained with retrieval-augmented tasks (GPT-4o family) benefit from iterative refinement, while models lacking such training (Llama-3.1, Qwen-2.5) may experience performance decline.
- **Core assumption:** Training objectives and data distribution determine model's ability to benefit from iterative context modification.
- **Evidence anchors:**
  - [section 4.3] "GPT-4o and GPT-4o-mini consistently improve as the number of rewriting iterations increases" while "Llama-3.1 and Qwen-2.5 show a noticeable performance decline"
  - [section 4.3] "This difference likely comes from training differences: GPT-4o may have been specifically trained on retrieval-augmented tasks"
  - [corpus] Weak - corpus doesn't provide evidence about training differences across model families
- **Break condition:** If iterative rewriting is applied uniformly across all models regardless of their training background, suboptimal performance may result.

## Foundational Learning

- **Concept:** Attention mechanisms and context window limitations
  - Why needed here: Understanding why LLMs struggle with multi-fact retrieval requires grasping how attention mechanisms allocate processing resources across long contexts
  - Quick check question: If an LLM has a 4096 token context window and processes a 3000 token document with 10 facts, how might attention mechanisms prioritize information?

- **Concept:** Retrieval-augmented generation (RAG) and context refinement
  - Why needed here: FACT builds on RAG principles by iteratively refining retrieved context rather than performing single-pass retrieval
  - Quick check question: How does iterative context rewriting differ from standard RAG approaches that perform retrieval once before generation?

- **Concept:** Linear probing for mechanistic analysis
  - Why needed here: The paper uses linear probes to diagnose internal representation quality during generation, revealing the "lost-in-the-middle" phenomenon
  - Quick check question: What does high probe accuracy indicate about a model's internal state when it fails to output correct information?

## Architecture Onboarding

- **Component map:** Query → Retrieve candidate facts → Rewrite context → Repeat until stopping condition
- **Critical path:** The most time-sensitive path is the iterative loop where each iteration requires full context processing. Performance scales linearly with number of iterations.
- **Design tradeoffs:** FACT trades computational overhead (multiple retrieval passes) for improved retrieval accuracy. More iterations generally improve performance but increase latency.
- **Failure signatures:** 
  - Performance plateaus or declines after certain iteration count
  - Models with limited retrieval training show degraded performance with iterative rewriting
  - Task-specific failures where iterative rewriting oversimplifies complex reasoning requirements
- **First 3 experiments:**
  1. Run FACT with 1, 2, 3 iterations on RULER K1V10Q1 task to observe performance curve
  2. Compare FACT performance across Llama-3.1 and GPT-4o models on same task to identify model family differences
  3. Apply FACT to both retrieval tasks and QA tasks to observe task-specific effectiveness variations

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What specific training data and methodology could improve open-source models' performance on multi-fact retrieval tasks?
- Basis in paper: [inferred] The paper notes that GPT-4o models perform better due to potential training on retrieval-augmented tasks, while Llama-3.1 and Qwen-2.5 struggle with retrieved context, suggesting differences in training regimes and architectures.
- Why unresolved: The paper does not provide specific details on how to adapt training data or methodology for open-source models to enhance their multi-fact retrieval capabilities.
- What evidence would resolve it: Experimental results comparing the performance of open-source models before and after being trained on retrieval-augmented tasks, or a detailed analysis of the differences in training data and methodology between open-source and closed-source models.

### Open Question 2
- Question: How can the computational efficiency of FACT be optimized without significantly compromising its accuracy?
- Basis in paper: [explicit] The paper mentions that FACT introduces additional computation due to its iterative nature, which could increase latency in certain applications.
- Why unresolved: The paper suggests mitigating computational overhead by fine-tuning the number of iterations or applying FACT selectively, but does not provide specific strategies or experimental results to support these suggestions.
- What evidence would resolve it: Comparative studies on the trade-off between computational efficiency and accuracy when applying different optimization strategies, such as varying the number of iterations or using selective application of FACT.

### Open Question 3
- Question: What task-adaptive strategies can be developed to dynamically adjust the number of iterations or degree of context rewriting based on specific task characteristics?
- Basis in paper: [explicit] The paper highlights that FACT exhibits significant improvements in multi-fact retrieval tasks but mixed results in general-purpose QA tasks, suggesting the need for task-adaptive strategies.
- Why unresolved: The paper does not provide specific task-adaptive strategies or experimental results to support the development of dynamic adjustment of iterations or context rewriting based on task characteristics.
- What evidence would resolve it: Experimental results comparing the performance of FACT with and without task-adaptive strategies, or a detailed analysis of the impact of different task characteristics on the effectiveness of iterative context rewriting.

## Limitations

- FACT effectiveness varies dramatically across model families, with open-source models showing performance decline rather than improvement
- The method shows minimal impact on general-purpose QA tasks despite significant gains in structured retrieval scenarios
- The "lost-in-the-middle" phenomenon is symptomatically documented but lacks direct mechanistic validation of underlying neural causes

## Confidence

**High Confidence**: The empirical results demonstrating FACT's effectiveness on retrieval tasks (RULER, Counting Stars) with accuracy improvements from 62.5% to 99.9% and 89.3% to 99.3% respectively. The probe-based evidence of "lost-in-the-middle" phenomenon showing systematic accuracy degradation during generation is also well-supported.

**Medium Confidence**: The explanation of why FACT works differently across model families. While the paper attributes differences to training objectives, the causal mechanism linking training data to iterative rewriting effectiveness remains speculative without direct evidence of how different training regimes affect attention mechanisms.

**Low Confidence**: The generalizability of FACT to real-world applications beyond controlled retrieval tasks. The paper's focus on synthetic datasets and structured key-value pairs limits confidence in performance on unstructured, noisy real-world data.

## Next Checks

1. **Mechanistic Validation**: Conduct ablation studies removing different components of the FACT pipeline (retrieve, rewrite, stop functions) to isolate which elements are essential for performance gains, and test whether the observed improvements persist when controlling for iteration count versus specific rewriting strategies.

2. **Cross-Domain Transfer**: Apply FACT to unstructured document retrieval tasks from real-world domains (medical records, legal documents, technical manuals) to assess whether iterative rewriting maintains effectiveness when moving beyond synthetic key-value retrieval to natural language contexts with implicit relationships.

3. **Attention Mechanism Analysis**: Use attention visualization tools to track how attention weights shift across iterations and whether the "lost-in-the-middle" phenomenon correlates with specific attention patterns, providing direct evidence of the proposed mechanism rather than just symptom observation.