---
ver: rpa2
title: 'Piece of Table: A Divide-and-Conquer Approach for Selecting Subtables in Table
  Question Answering'
arxiv_id: '2412.07629'
source_url: https://arxiv.org/abs/2412.07629
tags:
- table
- sub-table
- tables
- columns
- question
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the challenge of applying language models to
  table question answering, particularly when dealing with large tables that exceed
  the token limits of standard transformers. To overcome this, the authors propose
  PieTa (Piece of Table), a divide-and-conquer framework that iteratively divides
  tables into smaller windows, uses language models to select relevant cells within
  each window, and merges these into a subtable.
---

# Piece of Table: A Divide-and-Conquer Approach for Selecting Subtables in Table Question Answering

## Quick Facts
- arXiv ID: 2412.07629
- Source URL: https://arxiv.org/abs/2412.07629
- Reference count: 20
- Primary result: State-of-the-art performance among sub-table-based methods on WikiSQL, improving accuracy by up to 1.5%

## Executive Summary
This paper addresses the challenge of applying language models to table question answering when tables exceed transformer context limits. The authors propose PieTa (Piece of Table), a divide-and-conquer framework that iteratively divides large tables into overlapping windows, uses language models to select relevant cells within each window, and merges these into a subtable. This multi-resolution approach avoids the limitations of long context inputs while capturing dependencies across rows and columns. Evaluated on WikiSQL, PieTa achieved state-of-the-art performance among sub-table-based methods, improving table QA accuracy by up to 1.5% compared to using the original table.

## Method Summary
PieTa is a divide-and-conquer framework that addresses the token length constraints of language models when processing large tables. The method iteratively divides tables into overlapping windows of fixed size (3×3), uses a fine-tuned language model selector to identify relevant cells within each window, and merges these selections through a union operation. The process repeats until convergence, producing a subtable that contains the essential information for answering the question. The approach is evaluated on WikiSQL using OmniTab as the reader model, with Llama3-8B-Instruct fine-tuned as the selector.

## Key Results
- Achieved state-of-the-art performance among sub-table-based methods on WikiSQL
- Improved table QA accuracy by up to 1.5% compared to using the original table
- Demonstrated significantly better precision and recall in sub-table selection compared to existing approaches like ITR and TabSQLify
- Showed stable performance across varying table sizes, including tables up to 1,950 rows

## Why This Works (Mechanism)

### Mechanism 1
Multi-resolution union strategy allows PieTa to sidestep the need for exact sub-table intersection while still capturing relevant dependencies. Instead of identifying a minimal intersection of rows satisfying all conditions (the "gold table"), PieTa takes the union of sub-windows from overlapping partitions. Each sub-window is independently analyzed by an LM selector that marks relevant cells, and the union operation gradually builds a table that is more likely to contain all needed cells without needing to process long sequences. The union of per-window sub-tables contains enough relevant information for the QA reader to infer correct answers, even if it also includes some irrelevant cells.

### Mechanism 2
Dividing tables into overlapping windows of fixed size w×w lets the LM selector operate within manageable context limits while preserving spatial relationships across table boundaries. A sliding window with stride 1 partitions the table into overlapping sub-windows. The LM selector only needs to process w×w cells at a time, well within transformer limits, and the overlap ensures that dependencies spanning rows/columns across window boundaries are not missed. Overlapping windows are sufficient to capture dependencies that span multiple rows or columns; no critical relationship is lost between non-overlapping regions.

### Mechanism 3
Fine-tuning the LM selector to predict explicit cell values (not just row/column indices) improves precision in sub-table selection. The training data includes the cell values for each selected row/column, so the LM learns to reconstruct a small table rather than just mark positions. This richer supervision guides the selector to make more complete and accurate selections. Providing cell values during training gives the LM better signal to distinguish relevant from irrelevant cells, improving generalization.

## Foundational Learning

- **Table linearization and context limits in transformers**: Why needed here - PieTa's core motivation is that large tables exceed token limits; understanding how tables are linearized and why this causes problems is key to grasping the solution. Quick check question: What happens to a table with 1000 cells when fed to a 1024-token transformer without preprocessing?

- **Overlap in sliding-window processing**: Why needed here - The overlapping windows are central to preserving cross-window dependencies; missing this detail leads to misunderstanding why PieTa works. Quick check question: If window size w=3 and stride=1, how many windows will a 5-row table produce?

- **Set union vs. set intersection in table retrieval**: Why needed here - PieTa uses union, not intersection, to build sub-tables; knowing the difference clarifies why the approach is feasible. Quick check question: Given two sub-tables A={rows 1,2} and B={rows 2,3}, what is A∪B and what is A∩B?

## Architecture Onboarding

- **Component map**: Divide module -> Conquer module (LM selector) -> Combination module -> Loop controller -> Reader module
- **Critical path**: 1. Input table + question → Divide module → list of overlapping windows. 2. Each window + question → Conquer module (LM selector) → sub-window table. 3. All sub-window tables → Combination module → merged table. 4. Repeat steps 1-3 until convergence. 5. Final table → Reader → final answer.
- **Design tradeoffs**: Window size w vs. context limit (smaller w → safer context size but more windows, higher compute; larger w → fewer windows but risk of overflow); Overlap stride (stride=1 ensures coverage but doubles window count; stride>1 saves compute but may miss dependencies); Union vs. intersection (union simpler and safer for LM, but can introduce irrelevant cells that may distract the reader).
- **Failure signatures**: No change in table across iterations (either converged or stuck, e.g., selector not finding any new relevant cells); Final sub-table close to original size (union not effective, likely window too large or selector weak); QA accuracy no better than baseline (union includes too much noise, or selector not fine-tuned well).
- **First 3 experiments**: 1. Run with w=2, stride=1 on a small WikiSQL table, inspect per-window selections and union output; check that overlap captures cross-row dependencies. 2. Compare selector outputs trained to output cell values vs. only indices on the same table; measure precision/recall of selected cells. 3. Measure iteration count and table size shrinkage from original to final sub-table on a set of 10 diverse WikiSQL tables; confirm union reduces size but retains gold table cells.

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions, but several emerge from the work: How does PieTa's multi-resolution union-based approach compare to intersection-based approaches for sub-table selection in terms of capturing long-range dependencies? Can the PieTa framework be effectively adapted for table QA tasks beyond WikiSQL, such as those involving numerical reasoning or temporal reasoning? How does the performance of PieTa scale with increasing table sizes beyond those in WikiSQL, particularly for tables with thousands of rows? What is the impact of different window sizes on PieTa's performance, and is there an optimal window size that balances precision and computational efficiency?

## Limitations
- Evaluation is limited to WikiSQL, which may not generalize to more complex table QA datasets with nested queries or multi-table reasoning
- The proposed approach relies on iterative convergence, but the stopping criteria are not clearly defined, potentially leading to inconsistent results across different table structures
- The union-based approach may introduce noise from irrelevant cells, though this trade-off is not thoroughly explored

## Confidence

- **High confidence**: The core mechanism of using overlapping windows to enable LM processing within context limits is sound and well-supported by the design rationale
- **Medium confidence**: The claim of state-of-the-art performance among sub-table-based methods is supported by WikiSQL results, but external validation on diverse datasets would strengthen this claim
- **Medium confidence**: The assertion that cell-value reconstruction during training improves precision is plausible based on the described mechanism, but direct ablation studies comparing index-only vs. value-inclusive training would provide stronger evidence

## Next Checks
1. Test PieTa on a more complex table QA dataset (e.g., WikiTableQuestions or TabFact) to assess generalization beyond WikiSQL's relatively simple queries
2. Conduct ablation studies comparing the union strategy against intersection-based approaches on tables where the gold sub-table is known, measuring both QA accuracy and noise introduction
3. Analyze the iterative convergence behavior across diverse table structures to determine if stopping criteria need to be adaptive based on table characteristics