---
ver: rpa2
title: Multi-hop Question Answering over Knowledge Graphs using Large Language Models
arxiv_id: '2404.19234'
source_url: https://arxiv.org/abs/2404.19234
tags:
- question
- knowledge
- entities
- language
- answering
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper evaluates large language models (LLMs) for multi-hop
  question answering over knowledge graphs (KGs). The study compares information-retrieval
  (IR) and semantic-parsing (SP) approaches using LLMs like GPT-3.5-turbo and GPT-4.
---

# Multi-hop Question Answering over Knowledge Graphs using Large Language Models
## Quick Facts
- arXiv ID: 2404.19234
- Source URL: https://arxiv.org/abs/2404.19234
- Authors: Abir Chakraborty
- Reference count: 6
- Key outcome: This paper evaluates large language models (LLMs) for multi-hop question answering over knowledge graphs (KGs). The study compares information-retrieval (IR) and semantic-parsing (SP) approaches using LLMs like GPT-3.5-turbo and GPT-4. For IR, the model iteratively retrieves and filters relations and entities from subgraphs. For SP, the model identifies entities and predicates, then generates SPARQL queries. Experiments on six datasets (WebQSP, MetaQA-3hop, CWQ, LC-QuAD v1.0/v2.0, KQAPro) show that both IR-LLM and SP-LLM approaches achieve competitive performance, often surpassing traditional supervised methods. Notably, the IR-LLM approach achieves new state-of-the-art results on WebQSP and CWQ, while SP-LLM performs well on LC-QuAD and KQAPro. The study highlights the effectiveness of in-context learning with LLMs for KGQA tasks.

## Executive Summary
This paper evaluates large language models (LLMs) for multi-hop question answering over knowledge graphs (KGs), comparing information-retrieval (IR) and semantic-parsing (SP) approaches. The study employs LLMs like GPT-3.5-turbo and GPT-4 to either iteratively retrieve and filter relations and entities from subgraphs (IR) or identify entities and predicates to generate SPARQL queries (SP). Experiments on six datasets demonstrate that both IR-LLM and SP-LLM approaches achieve competitive performance, often surpassing traditional supervised methods. Notably, the IR-LLM approach achieves new state-of-the-art results on WebQSP and CWQ, while SP-LLM performs well on LC-QuAD and KQAPro, highlighting the effectiveness of in-context learning with LLMs for KGQA tasks.

## Method Summary
The paper evaluates large language models (LLMs) for multi-hop question answering over knowledge graphs (KGs) using two distinct approaches: information-retrieval (IR) and semantic-parsing (SP). For the IR approach, the LLM iteratively retrieves and filters relations and entities from subgraphs, refining its search based on the question context. In contrast, the SP approach involves the LLM identifying entities and predicates within the question, then generating SPARQL queries to extract the relevant information from the KG. Experiments conducted on six datasets, including WebQSP, MetaQA-3hop, CWQ, LC-QuAD v1.0/v2.0, and KQAPro, demonstrate that both IR-LLM and SP-LLM approaches achieve competitive performance, often surpassing traditional supervised methods. Notably, the IR-LLM approach achieves new state-of-the-art results on WebQSP and CWQ, while SP-LLM performs well on LC-QuAD and KQAPro. The study underscores the effectiveness of in-context learning with LLMs for KGQA tasks, offering insights into the potential of these models for complex reasoning over structured data.

## Key Results
- Both IR-LLM and SP-LLM approaches achieve competitive performance, often surpassing traditional supervised methods.
- The IR-LLM approach achieves new state-of-the-art results on WebQSP and CWQ datasets.
- SP-LLM performs well on LC-QuAD and KQAPro datasets, demonstrating the effectiveness of in-context learning with LLMs for KGQA tasks.

## Why This Works (Mechanism)
The effectiveness of LLMs in multi-hop KGQA stems from their ability to leverage in-context learning and natural language understanding. The IR-LLM approach works by iteratively refining its search through the KG, using the LLM's capacity to understand and filter relevant relations and entities based on the question context. This iterative refinement allows the model to navigate complex multi-hop paths efficiently. The SP-LLM approach, on the other hand, leverages the LLM's ability to parse natural language into structured queries, generating SPARQL queries that accurately capture the entities and predicates needed to answer the question. The success of both approaches highlights the versatility of LLMs in handling structured data and complex reasoning tasks, showcasing their potential to outperform traditional methods in KGQA scenarios.

## Foundational Learning
- **Knowledge Graphs (KGs)**: Structured representations of real-world entities and their interrelations. *Why needed*: Provide the structured data foundation for KGQA tasks. *Quick check*: Ensure the KG is well-formed and contains the necessary entities and relations for the questions being asked.
- **Multi-hop Reasoning**: The ability to traverse multiple edges in a KG to connect entities and derive answers. *Why needed*: Essential for answering complex questions that require information from multiple nodes. *Quick check*: Verify that the LLM can correctly identify and traverse the required hops in the KG.
- **In-context Learning**: The ability of LLMs to learn and adapt based on the context provided within the input. *Why needed*: Allows LLMs to dynamically adjust their responses based on the specific question and KG context. *Quick check*: Test the LLM's performance with varying contexts to ensure consistent adaptability.
- **SPARQL Queries**: A query language for retrieving and manipulating data stored in RDF format, commonly used with KGs. *Why needed*: Provides a standardized way to query and extract information from KGs. *Quick check*: Validate that the generated SPARQL queries accurately reflect the intended entities and predicates.

## Architecture Onboarding
**Component Map**: Question -> LLM (IR/SP) -> KG Traversal/SPARQL Generation -> Answer
**Critical Path**: Question parsing -> Entity/Relation identification -> KG traversal/query generation -> Answer extraction
**Design Tradeoffs**: The IR approach trades off query precision for flexibility in navigating the KG, while the SP approach prioritizes structured query generation but may struggle with ambiguous natural language.
**Failure Signatures**: IR may fail with ambiguous or incomplete entity mentions, while SP may struggle with complex or non-standard query structures.
**First Experiments**: 1) Test IR approach on a simple KG with clear entity mentions. 2) Evaluate SP approach on a dataset with well-defined SPARQL structures. 3) Compare both approaches on a dataset with mixed query types.

## Open Questions the Paper Calls Out
None

## Limitations
- Heavy reliance on proprietary LLMs (GPT-3.5-turbo and GPT-4) raises reproducibility and accessibility concerns.
- Evaluation focuses primarily on English-language datasets, limiting generalizability to other languages and cultural contexts.
- Computational efficiency and cost considerations are not extensively addressed, which are critical for practical deployment.

## Confidence
**High Confidence**: The experimental methodology and comparative analysis are well-structured, with clear distinctions between IR-LLM and SP-LLM approaches. The results demonstrate consistent patterns across multiple datasets.

**Medium Confidence**: The generalizability of findings to other KGQA scenarios, particularly those involving dynamic or incomplete knowledge graphs, remains uncertain. The study's focus on specific dataset characteristics may limit broader applicability.

**Low Confidence**: The long-term viability of LLM-based approaches given the rapid evolution of both KG technology and LLM capabilities makes definitive conclusions about sustained performance challenging.

## Next Checks
1. Replicate the experiments using open-source LLMs (e.g., LLaMA, Mistral) to assess the dependency on proprietary models and evaluate cost-performance tradeoffs.
2. Conduct stress testing on datasets with varying graph densities and relation types to identify breaking points for both IR-LLM and SP-LLM approaches.
3. Implement a comprehensive benchmarking framework that includes computational efficiency metrics, cost analysis, and scalability assessments across different KG sizes and query volumes.