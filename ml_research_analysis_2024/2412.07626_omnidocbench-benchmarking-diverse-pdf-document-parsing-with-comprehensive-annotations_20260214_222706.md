---
ver: rpa2
title: 'OmniDocBench: Benchmarking Diverse PDF Document Parsing with Comprehensive
  Annotations'
arxiv_id: '2412.07626'
source_url: https://arxiv.org/abs/2412.07626
tags:
- table
- text
- document
- content
- page
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: OmniDocBench is a novel benchmark for document parsing that addresses
  the lack of comprehensive evaluation in existing methods. It introduces a high-quality
  dataset spanning nine diverse document types with detailed annotations including
  19 layout categories and 14 attribute labels.
---

# OmniDocBench: Benchmarking Diverse PDF Document Parsing with Comprehensive Annotations

## Quick Facts
- **arXiv ID**: 2412.07626
- **Source URL**: https://arxiv.org/abs/2412.07626
- **Reference count**: 40
- **Primary result**: OmniDocBench introduces a novel benchmark for document parsing with 981 high-quality PDF pages across 9 diverse document types, comprehensive annotations including 19 layout categories and 14 attribute labels, and reveals that specialized pipeline tools outperform general VLMs in overall accuracy while VLMs show better generalization on specialized document types.

## Executive Summary
OmniDocBench addresses the critical gap in comprehensive evaluation for document parsing methods by introducing a high-quality dataset spanning nine diverse document types with detailed annotations. The benchmark provides a fair, diverse, and fine-grained evaluation standard that supports multi-level assessment from end-to-end evaluation to task-specific and attribute-based analysis. When evaluated on OmniDocBench, specialized pipeline tools like MinerU and Mathpix demonstrated superior performance in overall accuracy, particularly for Chinese text and tables, while general VLMs showed better generalization capabilities on specialized document types like slides and handwritten notes.

## Method Summary
OmniDocBench is a novel benchmark designed to comprehensively evaluate document parsing methods across diverse document types and attributes. The benchmark consists of 981 high-quality PDF pages across nine document categories, each annotated with 19 layout categories and 14 attribute labels covering text language, background colors, rotation, and table characteristics. The evaluation framework supports three levels of assessment: end-to-end evaluation measuring overall parsing performance, component-specific evaluation analyzing individual tasks like layout detection and text recognition, and attribute-level evaluation examining performance across different document characteristics. The benchmark was used to compare both pipeline-based methods (MinerU, Marker, Mathpix) and end-to-end multimodal methods (GOT-OCR, Nougat, GPT-4o, Qwen2-VL, InternVL2), revealing distinct strengths and weaknesses of each approach across different document types and attributes.

## Key Results
- Specialized pipeline tools (MinerU, Mathpix) outperformed general VLMs in overall accuracy, particularly for Chinese text and table recognition
- VLMs demonstrated superior generalization on specialized document types like slides and handwritten notes despite lower overall accuracy
- Document parsing methods still face significant challenges with complex layouts, rotated text, and diverse document attributes, indicating the need for more robust and adaptable parsing solutions

## Why This Works (Mechanism)
The benchmark's effectiveness stems from its comprehensive annotation scheme that captures both structural layout information and fine-grained attribute details, enabling precise evaluation across multiple dimensions. By spanning nine diverse document types and including detailed attribute labels, OmniDocBench exposes the strengths and weaknesses of different parsing approaches in varied real-world scenarios. The multi-level evaluation framework allows for granular analysis of performance bottlenecks, revealing that specialized tools excel at specific tasks while general VLMs offer broader adaptability across document types.

## Foundational Learning

**Layout Detection**: Understanding document structure through bounding box annotation of text, tables, figures, and other elements. *Why needed*: Forms the foundation for all downstream parsing tasks by identifying document components. *Quick check*: Verify detection mAP scores across different document types to ensure consistent structural understanding.

**Attribute Classification**: Labeling text and table attributes including language, background color, rotation, and frame types. *Why needed*: Enables fine-grained evaluation of how parsing methods handle document variations and complexities. *Quick check*: Examine attribute-level performance metrics to identify specific weaknesses in handling rotated text or colored backgrounds.

**Reading Order Extraction**: Determining the logical sequence of content within multi-column layouts. *Why needed*: Critical for accurate content extraction and understanding document semantics. *Quick check*: Validate normalized edit distance scores for reading order to ensure proper content sequencing across complex layouts.

## Architecture Onboarding

**Component Map**: PDF Document → Preprocessing → Special Component Extraction → Adjacency Search Match → Metric Calculation → Evaluation Results

**Critical Path**: Document preprocessing and special component extraction form the critical path, as errors in these stages propagate through the matching and metric calculation phases, directly impacting evaluation accuracy.

**Design Tradeoffs**: The benchmark balances comprehensiveness with practicality by focusing on nine diverse document types rather than attempting to cover every possible document variation, while the multi-level evaluation framework provides depth without excessive complexity.

**Failure Signatures**: Performance degradation is most pronounced for rotated text (particularly 90° and 270° orientations), complex multi-column layouts, and documents with mixed language content, with pipeline methods struggling more with generalization while VLMs face challenges with precise layout reconstruction.

**First Experiments**:
1. Evaluate text recognition performance on simple single-column documents with standard orientation to establish baseline accuracy
2. Test layout detection mAP on documents with varying column structures to assess structural understanding capabilities
3. Compare formula recognition accuracy between pipeline methods and VLMs on academic papers with complex mathematical content

## Open Questions the Paper Calls Out
None

## Limitations
- **Evaluation Algorithm Ambiguity**: The Adjacency Search Match algorithm is not fully specified, making it difficult to reproduce the exact evaluation methodology and potentially affecting result consistency
- **Preprocessing Variability**: Different preprocessing steps for pipeline methods and VLMs are mentioned but not thoroughly discussed, raising questions about evaluation fairness
- **Language and Layout Biases**: Performance variations across languages and layouts suggest potential biases that may limit generalizability to other document types or languages

## Confidence
**Medium**: The methodology for creating a diverse dataset and comprehensive annotation scheme is sound, but there are significant uncertainties regarding the exact implementation of the evaluation pipeline, particularly the Adjacency Search Match algorithm and preprocessing steps.

## Next Checks
1. Implement the Adjacency Search Match algorithm based on the paper's description and validate it against provided examples to ensure correct matching of ground truth and predictions
2. Develop a standardized preprocessing pipeline that can handle both pipeline method outputs and VLM outputs consistently, ensuring fair comparison across different approaches
3. Test the top-performing methods from OmniDocBench on other established document parsing benchmarks (such as DocBank or PubLayNet) to verify the generalizability of performance rankings and identify any dataset-specific biases