---
ver: rpa2
title: 'PORT: Preference Optimization on Reasoning Traces'
arxiv_id: '2406.16061'
source_url: https://arxiv.org/abs/2406.16061
tags:
- reasoning
- arxiv
- dataset
- preference
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper proposes using preference optimization methods on Chain-of-Thought
  steps to improve mathematical reasoning in language models. Two complementary schemes
  generate rejected answers: weak LLM prompting and digit corruption.'
---

# PORT: Preference Optimization on Reasoning Traces

## Quick Facts
- **arXiv ID:** 2406.16061
- **Source URL:** https://arxiv.org/abs/2406.16061
- **Reference count:** 40
- **Key outcome:** Using preference optimization on Chain-of-Thought steps improves mathematical reasoning, achieving up to 8.47% and 18.73% relative accuracy increases on GSM8K and AQuA-RAT benchmarks without extra annotations.

## Executive Summary
This paper proposes PORT (Preference Optimization on Reasoning Traces) to enhance mathematical reasoning in language models by applying preference optimization methods to Chain-of-Thought steps. The approach uses two schemes for generating rejected answers: weak LLM prompting and digit corruption. Applied to GSM8K and AQuA-RAT benchmarks, PORT leads to significant accuracy improvements for both Falcon2-11B and Mistral-7B models. The method demonstrates robustness across base models and training datasets, with digit corruption proving most effective. Additionally, improved reasoning abilities transfer to non-mathematical tasks like ARC and symbolic reasoning.

## Method Summary
PORT applies preference optimization (specifically DPO) to Chain-of-Thought reasoning steps by constructing preference datasets with rejected answers generated through digit corruption and weak LLM prompting. The process involves supervised fine-tuning on GSM8K rationales followed by DPO fine-tuning using the preference dataset. The approach is implemented using LoRA with specific hyperparameters (rank 64, Î± = 16) and evaluated on multiple benchmarks including GSM8K, AQuA-RAT, ARC-Challenge, and LastLetterConcat.

## Key Results
- Up to 8.47% relative accuracy increase on GSM8K for Falcon2-11B
- Up to 18.73% relative accuracy increase on AQuA-RAT for Mistral-7B
- Transfer learning improvements on ARC (1.4%) and LastLetterConcat (2.1%)
- Digit corruption scheme proves most effective among rejection generation methods

## Why This Works (Mechanism)

### Mechanism 1: Digit Corruption
Digit corruption improves mathematical reasoning by creating minimally perturbed wrong answers that retain structure while introducing errors similar to common mistakes. Each digit in reasoning steps is replaced with a random digit (0-9), generating incorrect but structurally similar reasoning paths that serve as rejected answers. This allows the base language model to learn to avoid digit errors by seeing corrupted examples that differ only in digit values while preserving non-digit structure.

### Mechanism 2: Weak LLM Prompting
Weak LLM prompting generates rejected answers that mimic typical model mistakes, helping the base model avoid similar errors. A smaller "weak" language model generates reasoning steps when prompted with partial reasoning traces, and these generated steps serve as rejected answers. The weak LLM's mistakes represent realistic error patterns that the base model should learn to avoid.

### Mechanism 3: Preference Optimization
Preference optimization methods (DPO, IPO, KTO, ORPO) outperform supervised fine-tuning by explicitly modeling relative preferences between correct and incorrect reasoning paths. The model learns to assign higher likelihood to reasoning steps that lead to correct answers compared to structurally similar incorrect ones, using loss functions that optimize for this preference ordering.

## Foundational Learning

- **Chain-of-Thought (CoT) reasoning**
  - Why needed here: The entire approach relies on generating and evaluating intermediate reasoning steps, not just final answers
  - Quick check question: Can you explain the difference between outcome-based and process-based supervision in language model training?

- **Preference optimization vs supervised fine-tuning**
  - Why needed here: The paper explicitly compares these approaches and claims preference optimization provides better reasoning abilities
  - Quick check question: What is the key difference between DPO's loss function and standard supervised fine-tuning loss?

- **Digit corruption as data augmentation**
  - Why needed here: This is the primary method for generating rejected answers and directly impacts model performance
  - Quick check question: How does digit corruption differ from random noise injection in terms of preserving reasoning structure?

## Architecture Onboarding

- **Component map:** Base model (Falcon2-11B/Mistral-7B) -> CoT dataset (GSM8K/AQuA) -> Preference dataset generator (digit corruption + weak LLM) -> Preference optimization trainer (DPO) -> Evaluation pipeline (GSM8K, AQuA, ARC, LastLetterConcat)

- **Critical path:**
  1. Load CoT dataset and construct SFT dataset (prompt-response pairs)
  2. Generate preference dataset with chosen/rejected reasoning pairs
  3. Apply SFT to initialize model parameters
  4. Apply DPO using preference dataset
  5. Evaluate on downstream tasks

- **Design tradeoffs:**
  - Digit corruption vs weak LLM prompting: Simpler implementation vs potentially more realistic errors
  - Preference dataset size: Larger datasets may improve performance but increase computational cost
  - Base model size: Larger models may achieve better performance but at higher computational expense

- **Failure signatures:**
  - Poor GSM8K performance after SFT but improvement after DPO suggests preference optimization is working
  - No improvement on ARC despite good mathematical performance suggests domain-specific reasoning patterns
  - Overfitting on training dataset indicated by performance drop on AQuA after SFT

- **First 3 experiments:**
  1. Run SFT only on GSM8K dataset and measure performance on all evaluation tasks
  2. Add digit corruption preference optimization and measure performance gains
  3. Compare different weak LLM sizes (Gemma-2B-it vs Llama-7B) for rejected answer generation

## Open Questions the Paper Calls Out

### Open Question 1
How does the choice of base model affect the effectiveness of preference optimization for reasoning tasks? The paper states that the approach is robust across base models, but only uses Falcon2-11B and Mistral-7B, leaving uncertainty about performance with smaller or larger models, or different architectures.

### Open Question 2
What is the optimal mix of digit corruption, weak LLM generation, and human annotations for constructing preference datasets? The paper compares two schemes but does not explore human annotations or more sophisticated rejection generation methods.

### Open Question 3
How does the quality and diversity of the source reasoning dataset affect the generalization of preference optimization to out-of-distribution tasks? The paper only uses GSM8K as the source dataset, leaving uncertainty about how dataset characteristics affect performance on unseen tasks.

## Limitations

- Digit corruption mechanism lacks rigorous quantitative analysis and ablation studies compared to alternative rejected answer generation methods
- Transfer learning claims require careful interpretation as improvements on ARC and LastLetterConcat are relatively modest
- Claims about the mechanism by which digit corruption improves reasoning are largely speculative without empirical validation

## Confidence

- **High confidence:** Core methodology of using preference optimization on CoT steps is technically sound and well-implemented
- **Medium confidence:** Claims about digit corruption being most effective lack comprehensive ablation studies
- **Low confidence:** Mechanisms explaining how digit corruption improves reasoning are largely speculative

## Next Checks

1. **Error Pattern Analysis:** Conduct detailed analysis of errors generated by digit corruption versus weak LLM prompting, comparing distribution of error types and measuring correlation with performance improvements.

2. **Ablation Study on Preference Data Quality:** Create systematically degraded versions of the preference dataset and measure how performance degrades to establish minimum viable preference dataset size and quality requirements.

3. **Cross-Domain Transfer Validation:** Design controlled experiment to test whether improvements on ARC and LastLetterConcat are due to learned reasoning patterns or simple pattern matching by modifying these tasks where surface patterns are preserved but underlying reasoning requirements are changed.