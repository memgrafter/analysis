---
ver: rpa2
title: 'Delving into the Reversal Curse: How Far Can Large Language Models Generalize?'
arxiv_id: '2410.18808'
source_url: https://arxiv.org/abs/2410.18808
tags:
- training
- llms
- answer
- bias
- question
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper examines the "reversal curse" in large language models,
  where models trained on "A is B" fail to generalize to "B is A". Through experiments
  with open-ended question-answering and multiple-choice tests, the authors reveal
  that models can generalize to "B is A" in multiple-choice settings when both A and
  B are presented in context, but only if the training data follows specific structural
  patterns like "[Name] is [Description]".
---

# Delving into the Reversal Curse: How Far Can Large Language Models Generalize?

## Quick Facts
- arXiv ID: 2410.18808
- Source URL: https://arxiv.org/abs/2410.18808
- Authors: Zhengkai Lin; Zhihang Fu; Kai Liu; Liang Xie; Binbin Lin; Wenxiao Wang; Deng Cai; Yue Wu; Jieping Ye
- Reference count: 40
- Models trained on "A is B" fail to generalize to "B is A" in open-ended generation, but can do so in multiple-choice settings when both entities are presented in context.

## Executive Summary
This paper investigates the "reversal curse" in large language models, where models trained on facts of the form "A is B" fail to generalize to the reversed form "B is A" in open-ended generation. Through extensive experiments with multiple model architectures and datasets, the authors demonstrate that this limitation stems from an inherent thinking bias toward using names as starting points for fact recall. Interestingly, models can overcome this limitation in multiple-choice settings when both entities are presented in context, revealing that the issue is not fundamental knowledge retrieval but rather how that knowledge is expressed in free-form generation.

## Method Summary
The authors conduct experiments using synthetic biography datasets with controlled fact structures ("NameIsDescription" vs "DescriptionIsName"), fine-tuning various LLMs including LLaMA2, LLaMA3, Vicuna, and Mistral. They evaluate model performance on both open-ended question-answering (using ROUGE-1 recall) and multiple-choice tests. The methodology includes knowledge injection through finetuning, saliency analysis to examine information flow patterns, and various mitigation attempts including longer training, mix training, and QA finetuning. The experiments are designed to test the hypothesis that models have an inherent thinking bias toward name-first fact recall that affects their ability to apply knowledge bidirectionally.

## Key Results
- Models achieve perfect performance on multiple-choice questions when both entities are presented in context, even when they fail on open-ended questions
- The generalization ability to "B is A" is highly correlated with the structure of facts in training documents, specifically favoring "[Name] is [Description]" patterns
- Attempts to mitigate the thinking bias through extended training, mix training, or QA finetuning show limited success, with performance improvements only on in-domain questions
- Saliency analysis reveals that information flows from name tokens to description tokens, confirming the thinking bias at the token interaction level

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLMs can generalize from "A is B" to "B is A" in multiple-choice settings when both A and B are presented in context.
- Mechanism: When both entities are present in the prompt, the model uses the options as contextual hints to retrieve the correct fact from memory, bypassing the backward recall deficiency that causes the reversal curse in open-ended generation.
- Core assumption: The model's ability to retrieve knowledge is not fundamentally impaired; the issue is expressing that knowledge in free-form generation.
- Evidence anchors:
  - [abstract] "LLMs are able to generalize to 'B is A' when both A and B are presented in the context as in the case of a multiple-choice question"
  - [section 2.2] "LLMs possess the ability to comprehend the identity relationships between people and their descriptions"
- Break condition: If the training data structure doesn't align with the model's thinking bias, even with both entities present, performance drops to random guessing.

### Mechanism 2
- Claim: The structure of training facts ("[Name] is [Description]" vs "[Description] is [Name]") critically affects generalization ability.
- Mechanism: Models develop an inherent thinking bias toward using names as starting points for fact recall during knowledge application. When facts are structured with names first, this bias is reinforced; when descriptions come first, the model struggles to retrieve the corresponding name.
- Core assumption: The pretraining corpus bias toward "[Name] is [Description]" structures shapes the model's internal processing patterns.
- Evidence anchors:
  - [abstract] "This generalization ability is highly correlated to the structure of the fact 'A is B' in the training documents"
  - [section 3.1] "LLMs possess an inherent bias in fact recalling during knowledge application"
  - [corpus] Weak evidence - the corpus analysis shows bias but doesn't directly prove causation
- Break condition: If training data is balanced or reversed, the bias would need to be explicitly addressed through architectural changes or training objectives.

### Mechanism 3
- Claim: The thinking bias is difficult to mitigate through training alone.
- Mechanism: The bias is deeply ingrained from pretraining and represents a fundamental processing preference. Attempts to mitigate it through longer training, mix training, or QA finetuning only improve performance on in-domain questions while failing to generalize to out-of-domain questions.
- Core assumption: The bias reflects a structural preference in the model's architecture rather than a simple pattern that can be unlearned.
- Evidence anchors:
  - [abstract] "The negative impact of this bias on the downstream performance of LLMs can hardly be mitigated through training alone"
  - [section 4] Results showing that extended training and various mitigation strategies fail to improve MCQ performance on the DescriptionIsName subset
- Break condition: If the bias could be mitigated through architectural modifications (e.g., bidirectional training objectives) rather than data structure alone.

## Foundational Learning

- Concept: Knowledge injection and catastrophic forgetting
  - Why needed here: Understanding how LLMs incorporate new facts without losing existing knowledge is crucial for interpreting the experimental results
  - Quick check question: If a model is finetuned on synthetic facts, why doesn't it forget its general capabilities as measured by MMLU?

- Concept: Bidirectional vs unidirectional information flow
  - Why needed here: The reversal curse stems from unidirectional processing, and understanding this distinction is key to grasping why MCQs work differently than open-ended questions
  - Quick check question: How does the autoregressive training objective create a preference for left-to-right information flow?

- Concept: Attention mechanisms and saliency scores
  - Why needed here: The saliency analysis reveals how information flows through the model and demonstrates the thinking bias at the token interaction level
  - Quick check question: What does a high saliency score from a name token to the answer position indicate about the model's processing?

## Architecture Onboarding

- Component map: Transformer layers with attention heads -> MLP layers -> Position embeddings -> LoRA adapters (during finetuning) -> Saliency computation across attention matrices

- Critical path: Training data → Finetuning (LoRA or full parameters) → Knowledge injection → Multiple-choice evaluation → Saliency analysis → Bias validation

- Design tradeoffs: Using synthetic data avoids contamination but may not capture all real-world complexities. LoRA provides efficiency but may limit full capacity adaptation. The choice between open-ended and multiple-choice evaluation affects how well the reversal curse is revealed.

- Failure signatures: Random guessing on MCQs despite perfect open-QA performance indicates thinking bias. Marginal improvement with extended training suggests the bias is structural rather than simply undertrained. Performance differences between NameIsDescription and DescriptionIsName subsets reveal data structure sensitivity.

- First 3 experiments:
  1. Replicate the basic finetuning and MCQ evaluation to confirm the pattern holds across different model sizes
  2. Apply the CoT prompting experiment to verify the thinking bias in external outputs
  3. Compute saliency scores to validate the bias in internal token interactions

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the thinking bias be mitigated through architectural changes to the model rather than training modifications?
- Basis in paper: [inferred] from the failure of various training-based mitigation strategies
- Why unresolved: The paper only explores training-based approaches and acknowledges that the bias persists despite these efforts, but doesn't investigate architectural modifications
- What evidence would resolve it: Experiments testing different model architectures (e.g., bidirectional attention, modified transformer layers) to see if they reduce the thinking bias

### Open Question 2
- Question: How does the thinking bias manifest in autoregressive models trained with objectives other than next-token prediction?
- Basis in paper: [explicit] from the mention of autoregressive-blank-infilling objective experiments
- Why unresolved: The paper only briefly mentions these experiments but doesn't provide detailed results or analysis of how different training objectives affect the bias
- What evidence would resolve it: Comprehensive comparison of thinking bias across models trained with different objectives (next-token prediction, blank-infilling, masked language modeling)

### Open Question 3
- Question: What is the relationship between the thinking bias and the models' ability to understand and apply symmetric relationships in other domains?
- Basis in paper: [inferred] from the discussion of how the bias affects knowledge application in biographical facts and literature data
- Why unresolved: The paper only examines the bias in specific domains (biographical facts, literature) but doesn't explore whether the bias extends to other types of symmetric relationships
- What evidence would resolve it: Experiments testing the models' ability to understand and apply symmetric relationships in various domains (mathematical equations, logical statements, physical laws)

## Limitations

- Synthetic Data Dependence: The entire experimental framework relies on synthetic biographies and facts, which may not capture the full complexity of real-world knowledge structures.
- Pretraining Corpus Analysis Gaps: The paper mentions that pretraining corpora have a "[Name] is [Description]" bias but provides limited direct evidence for this claim.
- Multiple-Choice Task Validity: While MCQs successfully reveal the reversal curse in a controlled setting, the paper doesn't fully address whether this finding translates to practical implications for real-world LLM applications.

## Confidence

**High Confidence**: The core observation that models can generalize "B is A" in multiple-choice settings but fail in open-ended generation is robustly demonstrated across multiple model architectures and datasets.

**Medium Confidence**: The hypothesis that pretraining corpus bias creates an inherent thinking bias toward name-first fact recall is plausible and supported by correlation, but the causal mechanism remains incompletely established.

**Low Confidence**: Claims about the difficulty of mitigating this bias through training alone are based on limited intervention attempts (longer training, mix training, QA finetuning).

## Next Checks

1. **Corpus Causality Experiment**: Conduct controlled pretraining from scratch with balanced "[Name] is [Description]" and "[Description] is [Name]" distributions to definitively establish whether the pretraining corpus structure causes the observed thinking bias.

2. **Cross-Domain Generalization Test**: Evaluate the reversal curse phenomenon on domains beyond biographies (e.g., scientific facts, geographical relationships, historical events) to determine whether the bias is specific to person-description relationships.

3. **Architectural Intervention Validation**: Implement and test architectural modifications specifically designed to address unidirectional information flow, such as bidirectional training objectives or modified attention mechanisms, to assess whether the thinking bias can be mitigated through structural rather than data-driven approaches.