---
ver: rpa2
title: 'SDQ: Sparse Decomposed Quantization for LLM Inference'
arxiv_id: '2406.13868'
source_url: https://arxiv.org/abs/2406.13868
tags:
- quantization
- outliers
- sparse
- sparsity
- compute
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper proposes SDQ (Sparse Decomposed Quantization) for LLM\
  \ inference, a hybrid method combining structured sparsity and quantization to exploit\
  \ both compute and memory efficiency. SDQ decomposes weights into sparse tensors\
  \ and uses structured sparse/low-bit-width compute hardware, achieving 4\xD7 effective\
  \ compute throughput with less than 1% quality drop."
---

# SDQ: Sparse Decomposed Quantization for LLM Inference

## Quick Facts
- arXiv ID: 2406.13868
- Source URL: https://arxiv.org/abs/2406.13868
- Reference count: 19
- Combines structured sparsity and quantization for 4× effective compute throughput with <1% quality drop

## Executive Summary
SDQ introduces a hybrid method that combines structured sparsity and quantization for efficient LLM inference. The approach decomposes weight matrices into sparse tensors and leverages structured sparse/low-bit-width compute hardware to achieve significant performance gains. By extracting local outliers per structured sparse vector rather than using global outlier extraction, SDQ enables efficient computation on sparse hardware while maintaining model quality. Evaluations demonstrate superior performance compared to sparsification-only or quantization-only methods across perplexity and zero-shot accuracy metrics.

## Method Summary
SDQ operates by decomposing weight matrices into sparse components and applying quantization to the decomposed tensors. The method employs local outlier extraction per structured sparse vector, which differs from traditional global outlier approaches. This decomposition enables the use of specialized hardware that supports structured sparsity patterns while maintaining numerical stability through careful outlier management. The hybrid approach exploits both compute efficiency through sparsity and memory efficiency through quantization, achieving higher effective throughput than either technique alone.

## Key Results
- Achieves 4× effective compute throughput compared to dense inference
- Maintains less than 1% quality drop in model performance
- Outperforms sparsification-only and quantization-only methods on OPT and LLaMA models

## Why This Works (Mechanism)
SDQ works by decomposing weight matrices into sparse tensors and leveraging structured sparsity patterns that align with hardware capabilities. The local outlier extraction per structured sparse vector allows the method to identify and preserve important weight values within each sparse block, rather than relying on global statistics that may miss block-specific important values. This approach enables efficient computation on hardware designed for structured sparsity while the quantization component reduces memory bandwidth requirements. The decomposition strategy ensures that the sparse structure is preserved during quantization, allowing both techniques to complement each other rather than competing.

## Foundational Learning
- Structured Sparsity Patterns - Why needed: Enables hardware acceleration for sparse computations; Quick check: Verify supported sparsity patterns match hardware capabilities
- Tensor Decomposition Methods - Why needed: Reduces memory footprint while preserving essential information; Quick check: Validate decomposition quality through reconstruction error
- Local vs Global Outlier Extraction - Why needed: Local extraction better preserves block-specific important values; Quick check: Compare perplexity with different outlier extraction strategies
- Hybrid Quantization Strategies - Why needed: Combines memory efficiency with computational efficiency; Quick check: Measure memory savings vs accuracy trade-off
- Hardware-Software Co-design - Why needed: Optimizes for specific hardware capabilities; Quick check: Profile performance on target hardware architecture

## Architecture Onboarding
Component map: Input -> Decomposer -> Local Outlier Extractor -> Quantizer -> Structured Sparse Compute Hardware -> Output
Critical path: Weight matrix decomposition → outlier extraction → quantization → sparse computation
Design tradeoffs: Local outlier extraction vs global extraction (precision vs simplicity), structured sparsity patterns vs flexibility, quantization levels vs quality preservation
Failure signatures: Quality degradation (>1%) indicates suboptimal decomposition or outlier extraction, throughput below 4× suggests hardware incompatibility
First experiments: 1) Benchmark baseline dense inference performance, 2) Test decomposition quality with different sparsity patterns, 3) Evaluate local vs global outlier extraction impact on perplexity

## Open Questions the Paper Calls Out
None

## Limitations
- Relies on specialized hardware support for structured sparsity, limiting practical deployment scenarios
- Performance gains are hardware-dependent and may not generalize across different architectures
- Focuses on inference-only scenarios without addressing training implications

## Confidence
- High: Decomposition approach and hybrid sparsity-quantization methodology are technically sound
- Medium: Local outlier extraction effectiveness needs broader validation across sparsity patterns
- Medium: Performance improvements are hardware-specific and may not translate universally

## Next Checks
1. Test SDQ on additional LLM architectures beyond OPT and LLaMA to verify generalizability
2. Evaluate the method on hardware platforms with varying levels of structured sparsity support to quantify hardware dependency
3. Conduct ablation studies comparing local versus global outlier extraction strategies across different sparsity levels and model sizes