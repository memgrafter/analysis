---
ver: rpa2
title: 'DiaMond: Dementia Diagnosis with Multi-Modal Vision Transformers Using MRI
  and PET'
arxiv_id: '2410.23219'
source_url: https://arxiv.org/abs/2410.23219
tags:
- diagnosis
- diamond
- multi-modal
- data
- fusion
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: DiaMond addresses the challenge of diagnosing dementia, particularly
  Alzheimer's Disease (AD) and frontotemporal dementia (FTD), by integrating multi-modal
  data from MRI and PET scans. The core innovation is a pure vision Transformer framework
  that uses both self-attention and a novel bi-attention mechanism to effectively
  capture unique and shared features across modalities, enhanced by a multi-modal
  normalization technique (RegBN) to reduce redundant dependencies.
---

# DiaMond: Dementia Diagnosis with Multi-Modal Vision Transformers Using MRI and PET

## Quick Facts
- arXiv ID: 2410.23219
- Source URL: https://arxiv.org/abs/2410.23219
- Authors: Yitong Li; Morteza Ghahremani; Youssef Wally; Christian Wachinger
- Reference count: 38
- Primary result: 92.4% balanced accuracy for AD diagnosis using MRI+PET

## Executive Summary
DiaMond introduces a pure vision Transformer framework for dementia diagnosis that integrates multi-modal MRI and PET imaging data. The core innovation is a bi-attention mechanism that effectively captures both unique and shared features across modalities while reducing redundant dependencies through multi-modal normalization. Evaluated across three distinct datasets, DiaMond achieves state-of-the-art performance in Alzheimer's Disease diagnosis, AD-MCI-CN classification, and differential diagnosis between AD and frontotemporal dementia. The framework demonstrates superior fairness across demographics compared to existing methods.

## Method Summary
The method employs vision Transformers with self-attention and a novel bi-attention mechanism to process MRI and PET scans simultaneously. A multi-modal normalization technique (RegBN) reduces redundant dependencies between modalities. The architecture includes cross-attention between modalities and a Transformer encoder with dual normalization layers. The framework is evaluated using multiple performance metrics including accuracy, balanced accuracy, and area under the curve across three datasets.

## Key Results
- 92.4% balanced accuracy for Alzheimer's Disease diagnosis
- 65.2% accuracy for AD-MCI-CN classification
- 76.5% accuracy for differential diagnosis of AD vs FTD
- Superior fairness across demographic groups compared to existing methods
- Significant performance improvement over traditional CNN-based approaches

## Why This Works (Mechanism)
The framework's success stems from the vision Transformer's ability to capture long-range dependencies in medical imaging data while the bi-attention mechanism enables effective cross-modal feature fusion. The multi-modal normalization (RegBN) addresses the challenge of redundant information between MRI and PET modalities, allowing the model to focus on complementary information. The self-attention layers enable the model to weigh the importance of different regions across both modalities, while cross-attention facilitates explicit information exchange between MRI and PET representations.

## Foundational Learning
1. **Vision Transformers for medical imaging** - Why needed: Traditional CNNs struggle with capturing long-range dependencies in medical scans; Quick check: Verify attention maps highlight relevant anatomical regions
2. **Multi-modal fusion techniques** - Why needed: Combining complementary information from different imaging modalities improves diagnostic accuracy; Quick check: Compare single-modal vs multi-modal performance
3. **Bi-attention mechanisms** - Why needed: Standard attention mechanisms may not effectively handle the complex relationships between different imaging modalities; Quick check: Test with and without bi-attention
4. **Multi-modal normalization** - Why needed: Prevents redundancy and improves stability when combining different imaging sources; Quick check: Monitor training stability with different normalization schemes
5. **Self-attention vs cross-attention** - Why needed: Different attention types serve distinct purposes in feature extraction and fusion; Quick check: Ablation study isolating each attention type
6. **Transformer-based feature extraction** - Why needed: Enables hierarchical representation learning from raw imaging data; Quick check: Compare feature maps at different transformer layers

## Architecture Onboarding

Component map: Input MRI/PET scans → Self-attention layers → Bi-attention mechanism → Multi-modal normalization (RegBN) → Cross-attention → Transformer encoder → Classification head

Critical path: The bi-attention mechanism combined with RegBN normalization represents the core innovation, enabling effective cross-modal feature fusion while reducing redundancy. The vision Transformer backbone provides hierarchical feature extraction, while cross-attention layers facilitate explicit information exchange between modalities.

Design tradeoffs: The framework prioritizes accuracy over computational efficiency, which may limit clinical deployment in resource-constrained settings. The focus on AD and FTD limits generalizability to other dementia types. The reliance on paired MRI/PET data may not be feasible in all clinical settings where only one modality is available.

Failure signatures: Performance degradation may occur with scans from different scanner types or protocols not represented in training data. The model may struggle with early-stage disease where imaging differences are subtle. Demographic biases could emerge if training data lacks diversity.

Three first experiments:
1. Compare performance with single-modal (MRI-only or PET-only) baselines
2. Ablation study removing bi-attention mechanism to quantify its contribution
3. Test on out-of-distribution data with different scanner types or protocols

## Open Questions the Paper Calls Out
None identified in the source material.

## Limitations
- Limited to AD and FTD diagnoses, excluding other dementia types
- Performance dependent on specific preprocessing and augmentation strategies
- Computational requirements may limit clinical deployment in resource-constrained environments
- May not generalize to different imaging protocols or clinical settings

## Confidence
High Confidence: Methodology and performance metrics are well-documented and reproducible, with statistically significant improvements over baselines
Medium Confidence: Fairness analysis lacks detailed demographic breakdown and potential confounding factor investigation
Low Confidence: Clinical interpretability of attention maps and correlation to pathological features remains limited; temporal aspects of disease progression not addressed

## Next Checks
1. External validation on additional, independent datasets with different imaging protocols and scanner types
2. Comparative analysis of computational efficiency and resource requirements against established CNN-based approaches
3. Detailed demographic fairness analysis with breakdown by age, gender, and other relevant demographic variables