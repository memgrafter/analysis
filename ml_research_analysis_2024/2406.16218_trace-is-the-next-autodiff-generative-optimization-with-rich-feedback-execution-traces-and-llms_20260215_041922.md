---
ver: rpa2
title: 'Trace is the Next AutoDiff: Generative Optimization with Rich Feedback, Execution
  Traces, and LLMs'
arxiv_id: '2406.16218'
source_url: https://arxiv.org/abs/2406.16218
tags:
- trace
- feedback
- optimization
- answer
- opto
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Trace, a framework for optimizing computational
  workflows through a generalization of back-propagation. Trace treats workflows as
  computational graphs and uses execution traces to efficiently update heterogeneous
  parameters (e.g., prompts, code, hyperparameters) based on rich feedback.
---

# Trace is the Next AutoDiff: Generative Optimization with Rich Feedback, Execution Traces, and LLMs

## Quick Facts
- **arXiv ID:** 2406.16218
- **Source URL:** https://arxiv.org/abs/2406.16218
- **Reference count:** 40
- **Key outcome:** Trace+OptoPrime achieves state-of-the-art results on BigBench-Hard and LLFBench Meta-World by optimizing computational workflows using execution traces and LLM reasoning

## Executive Summary
This paper introduces Trace, a framework that generalizes back-propagation for optimizing computational workflows with heterogeneous parameters (prompts, code, hyperparameters) using rich feedback. Trace treats workflows as computational graphs and uses execution traces to efficiently update parameters. The authors formalize this as Optimization with Trace Oracle (OPTO) and develop OptoPrime, an LLM-based optimizer that leverages execution traces to solve OPTO problems. Experiments demonstrate superior performance compared to specialized optimizers across numerical optimization, prompt tuning, hyperparameter tuning, and robot control tasks.

## Method Summary
Trace converts computational workflows into DAGs using Python decorators (@trace.node and @trace.bundle). The framework performs a backward pass from output feedback through execution traces to update parameters iteratively. OptoPrime, the LLM-based optimizer, formats execution traces as pseudo-algorithm problems with variable definitions and feedback, then prompts an LLM to suggest parameter changes. The approach generalizes optimization beyond differentiable functions to non-differentiable workflows like LLM prompting and code generation.

## Key Results
- Trace+OptoPrime achieves state-of-the-art performance on BigBench-Hard and LLFBench Meta-World benchmarks
- Outperforms specialized optimizers in numerical optimization, prompt tuning, and hyperparameter tuning tasks
- Successfully optimizes heterogeneous parameters including prompts, code functions, and hyperparameters in robot control applications

## Why This Works (Mechanism)

### Mechanism 1
Execution traces provide richer feedback than scalar loss, enabling more efficient optimization. Trace propagates DAGs recording intermediate computation steps, allowing optimizers to understand how parameters affect outputs through the entire computational path. Core assumption: execution traces contain sufficient information to construct improvement directions from output feedback.

### Mechanism 2
OptoPrime uses LLM reasoning to convert trace feedback into parameter updates. It formats execution traces as pseudo-algorithm problems with variable definitions, function documentation, and feedback, then prompts an LLM to suggest parameter changes. Core assumption: LLMs can effectively reason about execution traces and generate valid parameter updates.

### Mechanism 3
OPTO framework generalizes optimization to heterogeneous parameters and rich feedback. It abstracts optimization as selecting parameters θ, receiving execution trace g and feedback f, then updating parameters based on context ω across diverse domains. Core assumption: rich feedback combined with execution traces provides sufficient information for optimization across domains.

## Foundational Learning

- **Concept:** Computational graphs and DAGs
  - **Why needed here:** Trace represents workflows as computational graphs where nodes are inputs/outputs and edges represent dependencies
  - **Quick check:** Can you draw the DAG for a simple workflow like x = node(5); y = x + 2; z = y * 3?

- **Concept:** Backpropagation and automatic differentiation
  - **Why needed here:** Trace generalizes backpropagation by using execution traces instead of gradients for non-differentiable workflows
  - **Quick check:** How does backpropagation compute gradients through a computational graph?

- **Concept:** LLM-based optimization and prompting
  - **Why needed here:** OptoPrime uses LLMs to reason about execution traces and generate parameter updates
  - **Quick check:** What is the difference between Chain-of-Thought prompting and standard prompting?

## Architecture Onboarding

- **Component map:** User-declared parameters -> Workflow execution -> Trace builds graph -> Backward pass with feedback -> OptoPrime updates parameters -> Repeat
- **Critical path:** User declares parameters → executes workflow → Trace builds graph → backward pass with feedback → OptoPrime updates parameters → repeat
- **Design tradeoffs:** Trace vs. black-box optimization (richer feedback vs. computational overhead), trace granularity (detailed vs. abstract graphs), LLM context window vs. graph complexity
- **Failure signatures:** Poor convergence (insufficient feedback or trace information), syntax errors in generated code (LLM reasoning failure), memory issues (large graphs exceeding LLM context)
- **First 3 experiments:**
  1. Simple numerical optimization: min|x - y*| using synthetic computation graphs
  2. Battleship agent: optimize reason() and act() functions using binary feedback
  3. Traffic light control: tune green light durations using scalar delay feedback

## Open Questions the Paper Calls Out

### Open Question 1
How can we design an efficient optimizer for large-scale computational graphs (with thousands of nodes) in the OPTO framework? The current OptoPrime optimizer struggles with large graphs due to context limitations of LLMs. Designing an efficient optimizer requires new techniques for graph simplification, representation, and potentially a hybrid approach combining LLM reasoning with traditional search algorithms.

### Open Question 2
What are the theoretical conditions under which OPTO problems can be efficiently solved? The paper discusses the generality of OPTO and raises questions about its solvability, suggesting that efficient solutions depend on the context and feedback providing enough information to construct a corrective search direction. Identifying subsets of OPTO that are efficiently solvable is an open question.

### Open Question 3
How can we design adaptive optimizers for general OPTO problems that balance generality and task-specific performance? The paper presents OptoPrime as a first step but acknowledges its limitations. Designing an adaptive optimizer that can handle diverse OPTO problems while maintaining efficiency and task-specific performance requires further research.

## Limitations
- Dependence on LLM reasoning capabilities (context window constraints, reasoning reliability)
- Computational overhead from trace construction and subgraph propagation
- Lack of formal theoretical guarantees for convergence
- Assumption that execution traces provide sufficient information for parameter updates may not hold for all workflows

## Confidence
- **High:** The formalization of OPTO as a generalization of optimization problems; the implementation of Trace as a computational graph library
- **Medium:** The claim that execution traces provide richer feedback than scalar loss; the effectiveness of OptoPrime on the demonstrated tasks
- **Low:** The claim that Trace+OptoPrime consistently outperforms specialized optimizers; the scalability of the approach to large, complex workflows

## Next Checks
1. Test scalability by applying Trace to workflows with 100+ nodes and measure LLM context window limits
2. Compare convergence rates with gradient-based methods on differentiable tasks where gradients are available
3. Evaluate performance on tasks with ambiguous or sparse feedback to test robustness of trace-based reasoning