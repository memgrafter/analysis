---
ver: rpa2
title: 'Leveraging AI Predicted and Expert Revised Annotations in Interactive Segmentation:
  Continual Tuning or Full Training?'
arxiv_id: '2402.19423'
source_url: https://arxiv.org/abs/2402.19423
tags:
- annotations
- revised
- classes
- continual
- tuning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of efficiently leveraging AI
  predictions and expert revisions in interactive segmentation, particularly in medical
  imaging. The authors propose Continual Tuning, a method that combines network design
  and data reuse to iteratively improve AI models.
---

# Leveraging AI Predicted and Expert Revised Annotations in Interactive Segmentation: Continual Tuning or Full Training?

## Quick Facts
- arXiv ID: 2402.19423
- Source URL: https://arxiv.org/abs/2402.19423
- Reference count: 0
- One-line primary result: Continual Tuning achieves 16x speedup compared to full training while maintaining segmentation performance in medical image interactive segmentation.

## Executive Summary
This paper addresses the challenge of efficiently leveraging AI predictions and expert revisions in interactive segmentation, particularly in medical imaging. The authors propose Continual Tuning, a method that combines network design and data reuse to iteratively improve AI models. The approach involves a shared network for all classes, followed by class-specific networks for individual classes. To mitigate catastrophic forgetting, the shared network for previously learned classes is frozen, while only the class-specific network for revised classes is updated. Additionally, a small fraction of data with previous annotations is reused, selected based on importance estimates combining uncertainty and consistency of AI predictions. Experiments on abdominal CT scans demonstrate that Continual Tuning achieves a 16x speed improvement compared to full training from scratch, without compromising performance.

## Method Summary
Continual Tuning is a method for iterative improvement of AI models in interactive segmentation tasks. It uses a hybrid approach combining network architecture design and data reuse strategy. The model consists of a shared backbone network followed by class-specific MLP layers for each organ class. When expert revisions are made to annotations for specific classes, the method freezes the shared network for previously learned classes and updates only the class-specific network for revised classes. Additionally, instead of using only revised annotations, the method combines them with AI-predicted annotations for unchanged classes. Data selection for training is based on importance estimates computed from uncertainty and consistency of AI predictions, allowing selective reuse of a small fraction of data with previous annotations.

## Key Results
- Achieved 16x speedup compared to full training from scratch while maintaining performance
- Mean Dice Similarity Coefficient (DSC) scores of 76.1% and 78.8% for Swin UNETR and U-Net backbones respectively
- Successfully prevented catastrophic forgetting in multi-class segmentation tasks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The class-specific network design prevents catastrophic forgetting by freezing the shared network for previously learned classes.
- Mechanism: The shared network encodes general features across all classes, while class-specific networks handle class-specific features. When fine-tuning on new classes, only the class-specific networks for revised classes are updated, leaving the shared network and other class-specific networks frozen.
- Core assumption: The shared network captures sufficient general features that remain valid across classes, and class-specific networks can be updated independently without affecting the shared network's performance.
- Evidence anchors:
  - [abstract]: "To mitigate forgetting, we freeze the shared network for previously learned classes and only update the class-specific network for revised classes."
  - [section]: "To address the issue of forgetting, we keep the parameters of the shared network for the previously learned classes frozen while exclusively updating the network associated with the revised classes."
- Break condition: If the shared network contains features that are too class-specific or if the class boundaries are not well-separated, updating one class-specific network might inadvertently affect the shared network's ability to serve other classes.

### Mechanism 2
- Claim: Hybrid data continual tuning improves performance by combining AI-predicted annotations for previous classes with expert-revised annotations for new classes.
- Mechanism: Instead of using only revised annotations, the method merges them with AI-predicted annotations for unchanged classes. This maintains the benefit of the original training data while prioritizing the accuracy of revised annotations.
- Core assumption: AI-predicted annotations for previously learned classes are sufficiently accurate to maintain performance when combined with expert-revised annotations for new classes.
- Evidence anchors:
  - [abstract]: "This design enables AI models to efficiently prioritize expert revised annotations without forgetting, due to the use of AI-predicted annotations for previous classes."
  - [section]: "By merging the revised annotations to the AI predicted annotations, the Hybrid Data could be expressed as(C ∗1, C∗2, C∗3, ...C∗m, Cn)"
- Break condition: If AI-predicted annotations for previous classes contain significant errors, they could propagate through the model and degrade overall performance.

### Mechanism 3
- Claim: Selective data reuse based on uncertainty and consistency estimates improves efficiency without compromising performance.
- Mechanism: Instead of using all available data, the method selects a subset of data with previous annotations based on importance scores computed from uncertainty and consistency of AI predictions.
- Core assumption: Data with lower uncertainty and higher consistency are more valuable for training and can be used to approximate the benefits of using all available data.
- Evidence anchors:
  - [abstract]: "The selection of such data relies on the importance estimate of each data. The importance score is computed by combining the uncertainty and consistency of AI predictions."
  - [section]: "The selection for the revision process is based on the uncertainty of the AI predicted annotations."
- Break condition: If the uncertainty and consistency metrics do not accurately reflect the true importance of data points, the selection process could miss critical training examples.

## Foundational Learning

- Concept: Catastrophic forgetting in neural networks
  - Why needed here: The paper addresses this problem directly by designing a network architecture that prevents it.
  - Quick check question: What happens to a neural network's performance on previously learned tasks when it is trained on new tasks without any special techniques?

- Concept: Continual learning and incremental training
  - Why needed here: The proposed method is a continual learning approach that incrementally improves the model using new annotations.
  - Quick check question: How does continual learning differ from standard training when new data becomes available?

- Concept: Uncertainty estimation in deep learning
  - Why needed here: The method uses uncertainty estimates to select important data for training.
  - Quick check question: What are two common approaches to estimate uncertainty in deep learning models?

## Architecture Onboarding

- Component map:
  - Shared backbone network (frozen for previous classes) -> Class-specific MLP layers for each organ class -> Text embedding module for class conditioning -> Data selection module based on uncertainty/consistency

- Critical path:
  1. Input CT scan → Shared backbone → Class-specific MLPs
  2. Expert revision → Uncertainty estimation → Data selection
  3. Fine-tuning → Update only class-specific MLPs for revised classes

- Design tradeoffs:
  - Memory vs. performance: Freezing the shared network saves memory but may limit adaptability
  - Speed vs. accuracy: Selective data reuse speeds up training but might miss important examples
  - Complexity vs. simplicity: Class-specific networks add complexity but prevent catastrophic forgetting

- Failure signatures:
  - Performance degradation on previously learned classes
  - Inconsistent predictions across similar anatomical structures
  - Slow convergence or poor performance on revised classes

- First 3 experiments:
  1. Verify that freezing the shared network prevents catastrophic forgetting by comparing with full fine-tuning
  2. Test the effectiveness of hybrid data by comparing with only revised annotations vs. full dataset
  3. Validate the data selection strategy by comparing performance with different selection thresholds

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the Continual Tuning method perform in terms of computational efficiency and model performance when applied to real-world medical datasets with a larger number of classes and more complex anatomical structures?
- Basis in paper: [inferred] The paper discusses the potential of Continual Tuning for real-world applications but does not provide experimental results on large-scale, complex datasets.
- Why unresolved: The experiments conducted in the paper are limited to a relatively small number of classes and a controlled dataset. The performance of Continual Tuning on larger, more diverse datasets remains unknown.
- What evidence would resolve it: Conducting experiments on large-scale medical datasets with a higher number of classes and more complex anatomical structures, comparing the computational efficiency and model performance of Continual Tuning against other methods.

### Open Question 2
- Question: What is the optimal strategy for selecting data with previous annotations to reuse during the Continual Tuning process, and how does this selection impact the model's performance and efficiency?
- Basis in paper: [explicit] The paper proposes a method for selecting data based on importance estimates combining uncertainty and consistency of AI predictions, but does not explore the impact of different selection strategies.
- Why unresolved: The paper does not investigate the effects of different data selection strategies on the model's performance and efficiency. The optimal strategy for selecting data remains unclear.
- What evidence would resolve it: Conducting experiments comparing the performance and efficiency of Continual Tuning using different data selection strategies, such as random selection, uncertainty-based selection, or consistency-based selection.

### Open Question 3
- Question: How does the Continual Tuning method handle the introduction of new classes or changes in the annotation guidelines over time, and what are the implications for long-term model maintenance and updates?
- Basis in paper: [inferred] The paper discusses the potential of Continual Tuning for long-term learning but does not address the challenges of handling new classes or changes in annotation guidelines.
- Why unresolved: The paper does not explore the challenges of maintaining and updating the model over time, especially when new classes are introduced or annotation guidelines change.
- What evidence would resolve it: Conducting experiments to evaluate the performance of Continual Tuning when new classes are introduced or annotation guidelines are updated, and investigating strategies for efficient model maintenance and updates.

## Limitations
- Evaluation limited to two backbone architectures (Swin UNETR and U-Net) and a specific domain (abdominal CT segmentation)
- 16x speedup claim relative to specific baseline without comparison to other continual learning approaches
- Data selection strategy based on uncertainty and consistency metrics described but not fully validated

## Confidence

- **High Confidence**: The catastrophic forgetting mitigation mechanism through frozen shared networks and class-specific updates is well-supported by ablation studies and comparison with full fine-tuning baselines.
- **Medium Confidence**: The hybrid data reuse strategy shows performance improvements but lacks comprehensive ablation studies to determine optimal mixing ratios between AI predictions and expert revisions.
- **Low Confidence**: The data selection strategy's effectiveness is demonstrated through results but lacks detailed analysis of how different uncertainty thresholds affect performance.

## Next Checks

1. **Cross-architecture validation**: Test the Continual Tuning approach on additional backbone architectures (e.g., SegFormer, TransUNet) to verify generalizability beyond Swin UNETR and U-Net.
2. **Ablation on data selection**: Systematically vary the data selection threshold and compare performance to determine the optimal trade-off between efficiency and accuracy.
3. **Long-term forgetting analysis**: Conduct experiments tracking performance on previously learned classes over multiple revision rounds to quantify any gradual degradation that might not be apparent in single-round evaluations.