---
ver: rpa2
title: 'GNN-VPA: A Variance-Preserving Aggregation Strategy for Graph Neural Networks'
arxiv_id: '2403.04747'
source_url: https://arxiv.org/abs/2403.04747
tags:
- aggregation
- networks
- neural
- learning
- graph
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses signal propagation issues in graph neural networks
  (GNNs), particularly exploding activations during message aggregation. The authors
  propose a variance-preserving aggregation (VPA) function that maintains the same
  expressive power as sum aggregation while improving forward and backward dynamics.
---

# GNN-VPA: A Variance-Preserving Aggregation Strategy for Graph Neural Networks

## Quick Facts
- arXiv ID: 2403.04747
- Source URL: https://arxiv.org/abs/2403.04747
- Reference count: 31
- Primary result: VPA achieves consistent 0.2-0.8% accuracy improvements across GNN architectures on graph classification benchmarks

## Executive Summary
This paper addresses signal propagation issues in graph neural networks, particularly exploding activations during message aggregation. The authors propose a variance-preserving aggregation (VPA) function that maintains the same expressive power as sum aggregation while improving forward and backward dynamics. Experiments on standard graph classification benchmarks show that VPA consistently outperforms traditional aggregation methods across various GNN architectures.

## Method Summary
VPA introduces a normalization factor of 1/√N during aggregation, where N is the number of neighboring nodes. This preserves the variance of aggregated messages, preventing them from exploding or vanishing as they propagate through network layers. The method is implemented as a drop-in replacement for standard aggregation functions in GNN architectures including GIN, GCN, SGC, and GAT. For attention-based models, VPA modifies the attention mechanism by introducing a variance-preserving attention weight normalization.

## Key Results
- GIN+VPA achieves 72.0% accuracy on IMDB-BINARY compared to 71.8% for GIN+SUM
- VPA consistently outperforms sum, mean, and max aggregation across all tested architectures
- VPA shows particular benefits for social network datasets without node features, where it forces learning from network structure alone
- Variance preservation property maintains GNN expressivity while improving learning dynamics

## Why This Works (Mechanism)

### Mechanism 1
- Claim: VPA prevents exploding activations by normalizing neighbor messages with 1/√N
- Mechanism: Scaling each message by 1/√N ensures aggregated message variance remains constant, preventing variance growth with neighbor count
- Core assumption: Messages from neighboring nodes are independent random variables with zero mean and finite variance
- Evidence anchors:
  - [abstract] "The core idea is to use a normalization factor of 1/√N during aggregation, where N is the number of neighboring nodes. This preserves the variance of the aggregated messages, preventing them from exploding or vanishing as they propagate through the network layers."
  - [section 2] "Our key idea is to introduce a new aggregation function which preserves variance, i.e., m_i ~ p_N(0, I). This is possible with the aggregation function L ≡ 1/√N ∑."
- Break condition: Independence assumption breaks when messages are computed from shared node features or become correlated across layers

### Mechanism 2
- Claim: VPA maintains same expressive power as sum aggregation for graph isomorphism testing
- Mechanism: VPA uses f(x) = 1/√|X| ∑x∈X f(x) where f(x) is one-hot encoding, preserving multiset information needed for isomorphism testing
- Core assumption: Number of unique elements in multiset is bounded by constant N, allowing bijective mapping to natural numbers
- Evidence anchors:
  - [section 2] "According to Xu et al. (2019) a prerequisite for maximum expressive power w.r.t. discriminating non-isomorphic graphs is an injective aggregation function, such as SUM aggregation, while MEAN or MAX aggregation results in limited expressivity. A message passing algorithm with VPA has the same expressive power as SUM aggregation, which follows analogously to Xu et al. (2019) from Lemma 2."
- Break condition: When number of unique node features exceeds bounded constant N, injective property may be lost

### Mechanism 3
- Claim: VPA extends to attention mechanisms while preserving variance
- Mechanism: For attention weights c_i, defines y = 1/C ∑c_i z_i where C = √(∑c_i²), maintaining variance of aggregated message
- Core assumption: Attention weights are constants (not random variables) and sum to 1
- Evidence anchors:
  - [section 2] "To apply the concept of variance preservation to attention, we define a constant C := √(∑c_i²) and use the following attention mechanism: y = 1/C ∑c_i z_i. As shown in Lemma 3 this results in a variance-preserving attention mechanism."
- Break condition: If attention weights are learned and become correlated with messages, variance preservation may fail

## Foundational Learning

- Concept: Signal propagation theory in neural networks
  - Why needed here: Understanding how activations propagate through layers is crucial for designing aggregation functions that prevent exploding or vanishing activations
  - Quick check question: What is the main difference between variance preservation in GNNs and traditional normalization techniques like batch normalization?

- Concept: Graph isomorphism and expressive power of GNNs
  - Why needed here: The choice of aggregation function directly impacts the GNN's ability to distinguish non-isomorphic graphs, which is fundamental to graph classification tasks
  - Quick check question: Why does sum aggregation provide more expressive power than mean or max aggregation for graph isomorphism testing?

- Concept: Message passing in graph neural networks
  - Why needed here: VPA is a modification to the message aggregation step, so understanding the message passing framework is essential for implementing and extending VPA
  - Quick check question: In the message passing framework, what are the three main steps involved in updating node embeddings?

## Architecture Onboarding

- Component map:
  GNN layers (GIN, GCN, SGC, GAT) → VPA modification → Classification head
  VPA acts as drop-in replacement for sum, mean, or max aggregation functions
  For attention-based models (GAT), VPA modifies the attention mechanism itself

- Critical path:
  1. Initialize node features
  2. For each layer:
     a. Compute messages from neighboring nodes
     b. Aggregate messages using VPA: m_i = 1/√N ∑m_ij
     c. Update node embeddings: h_i' = ψ(h_i, θ(m_i))
  3. Pool node embeddings for graph-level classification
  4. Apply classification head

- Design tradeoffs:
  - VPA vs. sum: VPA prevents exploding activations but may slightly increase computational overhead due to square root calculation
  - VPA vs. mean: VPA maintains expressivity while mean aggregation loses information about node degrees
  - VPA vs. max: VPA preserves multiset information while max aggregation is not injective

- Failure signatures:
  - If variance preservation is not maintained: Exploding or vanishing activations during training
  - If expressive power is lost: GNN cannot distinguish between non-isomorphic graphs
  - If attention mechanism is not properly modified: Variance preservation in GAT+VPA may fail

- First 3 experiments:
  1. Implement VPA in simple GIN model and compare training dynamics with sum aggregation on small graph classification dataset
  2. Test VPA on graph datasets with and without node features to verify performance on social network data
  3. Extend VPA to GAT and compare performance with standard GAT on benchmark dataset

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does VPA provide advantages for deeper GNNs (more than 5 layers) where exploding/vanishing activations become more severe?
- Basis in paper: [inferred] The paper mentions that VPA is "especially relevant for very deep networks" and that "exploding activations are a main obstacle for efficient training" but only tests shallow networks (5 layers).
- Why unresolved: Experiments were limited to shallow networks where all methods converged without diverging. Benefits for deeper architectures remain untested.
- What evidence would resolve it: Training GNNs with 10+ layers using VPA versus SUM aggregation, measuring training stability, convergence speed, and final performance.

### Open Question 2
- Question: Can VPA be theoretically proven to preserve variance under realistic assumptions about message distributions during training?
- Basis in paper: [explicit] The paper acknowledges that "distributional assumptions to formally show variance preservation might only hold at the time of initialization" and uses this as a simplifying assumption.
- Why unresolved: Theoretical proofs rely on independence assumptions that the authors acknowledge are "too strong" in practice. Real message distributions during training likely violate these assumptions.
- What evidence would resolve it: Mathematical analysis showing variance preservation under more realistic assumptions about message correlations, or empirical measurements of message variance throughout training.

### Open Question 3
- Question: How does VPA perform on graph regression tasks and other graph learning problems beyond graph classification?
- Basis in paper: [inferred] All experiments focus exclusively on graph classification benchmarks. Authors mention VPA could be applied to "many GNN architectures" but don't demonstrate this.
- Why unresolved: Evaluation is limited to specific task type, leaving open whether variance preservation benefits generalize to other graph learning scenarios.
- What evidence would resolve it: Experiments on graph regression, link prediction, and node classification tasks using VPA across multiple GNN architectures.

## Limitations
- Theoretical assumptions of message independence may not hold in deeper networks where messages become correlated
- Limited evaluation to shallow networks (5 layers) despite claims about benefits for very deep networks
- Extension to attention mechanisms assumes weights are constants, while in practice they are learned parameters that may correlate with messages

## Confidence
- Core claim (VPA improves signal propagation while maintaining expressivity): Medium
- Expressive power claim (VPA maintains same power as sum aggregation): High
- Attention extension claim: Low

## Next Checks
1. Test VPA on deeper GNN architectures (8+ layers) to verify variance preservation holds across many layers and identify potential correlation effects
2. Evaluate VPA on graphs with highly variable node degrees to check if √N normalization becomes unstable for nodes with very few or very many neighbors
3. Compare VPA against other variance control techniques like layer normalization or residual connections to isolate specific benefits of aggregation-level normalization