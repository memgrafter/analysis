---
ver: rpa2
title: Towards Truly Zero-shot Compositional Visual Reasoning with LLMs as Programmers
arxiv_id: '2401.01974'
source_url: https://arxiv.org/abs/2401.01974
tags:
- image
- self
- video
- return
- object
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the challenge of compositional visual reasoning
  with large language models (LLMs) as programmers. It introduces a framework that
  improves upon existing approaches by automatically generating in-context examples
  (ICEs) and introducing spatially and temporally abstract routines.
---

# Towards Truly Zero-shot Compositional Visual Reasoning with LLMs as Programmers

## Quick Facts
- arXiv ID: 2401.01974
- Source URL: https://arxiv.org/abs/2401.01974
- Reference count: 40
- Primary result: Improves compositional visual reasoning by automatically generating in-context examples and introducing abstract spatial/temporal routines

## Executive Summary
This paper addresses the challenge of compositional visual reasoning using large language models as programmers. The authors introduce a framework that eliminates the need for human-engineered in-context examples by automatically generating them from few-shot labeled data, and reduces the cognitive burden on LLMs by introducing abstract spatial and temporal reasoning routines. The approach consistently improves performance across multiple compositional question-answering and video temporal reasoning tasks while making the LLM-as-programmers setup more robust and requiring less human engineering.

## Method Summary
The framework combines zero-shot learning with automatically generated in-context examples (ACEs), abstract API routines for spatial and temporal reasoning, and self-tuning mechanisms. It uses a small number of labeled examples to generate code in a zero-shot manner, ranks results by accuracy, and selects the best programs as ACEs for test time. The abstract API provides higher-level spatial and temporal abstractions (like `get_patch_left_of` and `sort_patches_left_to_right`) that reduce the LLM's need for low-level reasoning. When code execution fails due to module sensitivity, the framework automatically adjusts hyperparameters through self-tuning.

## Key Results
- Consistent performance gains across compositional question-answering tasks (GQA, RefCOCO/RefCOCO+) and video temporal reasoning (NExT-QA)
- Eliminates the need for human engineering of in-context examples while maintaining or improving performance
- Reduces the burden on LLMs for spatial and temporal reasoning through abstract routines
- Demonstrates self-tuning capabilities that recover from module failures due to hyperparameter sensitivity

## Why This Works (Mechanism)

### Mechanism 1: ACE Generation
- **Claim:** Automatically generated in-context examples (ACEs) replace hand-engineered ICEs, reducing human labor while maintaining or improving performance.
- **Mechanism:** Runs in zero-shot manner on few labeled examples, generates code for each, ranks by accuracy, and selects best programs to pair with queries as ACEs for test time.
- **Core assumption:** LLM sometimes generates correct programs, and reusing these improves generalization.
- **Evidence anchors:** [abstract], [section] - Weak corpus support
- **Break condition:** If LLM never generates correct programs on few-shot examples, or ranking fails to select useful programs.

### Mechanism 2: Abstract API
- **Claim:** Abstract API routines reduce cognitive load by providing higher-level spatial/temporal abstractions instead of low-level Python primitives.
- **Mechanism:** Introduces routines like `get_patch_left_of`, `sort_patches_left_to_right`, and video event localization functions that encapsulate multiple lines of code into single function calls.
- **Core assumption:** Current LLMs are weak at low-level spatial/temporal reasoning but can use higher-level abstractions effectively.
- **Evidence anchors:** [abstract], [section] - Weak corpus support
- **Break condition:** If LLM cannot use provided abstract routines or routines are not general enough.

### Mechanism 3: Self-tuning
- **Claim:** Self-tuning through dynamic hyperparameter adjustment improves performance when code execution fails due to module sensitivity.
- **Mechanism:** Automatically lowers module thresholds (e.g., object detection) and re-executes code when failures occur, iterating until success or maximum trials.
- **Core assumption:** Certain modules have hyperparameters that significantly affect success rates, and adjusting these dynamically can recover from failures.
- **Evidence anchors:** [abstract], [section] - Weak corpus support
- **Break condition:** If failures are not due to hyperparameter settings or adjusting hyperparameters doesn't resolve issues.

## Foundational Learning

- **Concept:** In-context learning (ICL) in LLMs
  - **Why needed here:** Framework relies on LLM's ability to generate correct code based on examples provided in prompt, without gradient updates.
  - **Quick check question:** What is the difference between zero-shot, few-shot, and in-context learning in LLMs?

- **Concept:** Task decomposition and modular reasoning
  - **Why needed here:** Framework decomposes complex visual reasoning tasks into subtasks solved by individual tools, mimicking human System 2 thinking.
  - **Quick check question:** How does decomposing a task into subtasks help when solving compositional visual reasoning problems?

- **Concept:** Hyperparameter sensitivity in vision models
  - **Why needed here:** Self-tuning mechanism relies on understanding that certain vision model thresholds significantly affect success rates.
  - **Quick check question:** Why do object detection thresholds matter for compositional visual reasoning tasks?

## Architecture Onboarding

**Component map:** LLM code generator -> Abstract API routines -> Vision modules (detection, depth, captioning) -> Execution engine -> Self-debugging/Self-tuning

**Critical path:** Prompt construction with ACEs → Code generation → Abstract API → Vision module execution → Result compilation → Self-correction (if needed)

**Design tradeoffs:** Automatic ACE generation reduces human engineering but depends on LLM's code generation quality; abstract API simplifies reasoning but may not cover all scenarios; self-tuning can recover from failures but adds execution time and complexity.

**Failure signatures:** Compilation errors, wrong return types, module failures due to hyperparameter sensitivity, logical errors in spatial/temporal reasoning.

**First experiments:**
1. Test code generation with different prompt templates on few-shot examples to evaluate ACE quality
2. Validate abstract API coverage by analyzing required operations across test datasets
3. Characterize failure modes by logging all code generation failures and categorizing by type

## Open Questions the Paper Calls Out

- **Open Question 1:** What is the optimal set of spatial and temporal abstract routines for visual reasoning tasks? The paper mentions this is an open research question when discussing the Abstract API, but doesn't systematically explore different combinations.
- **Open Question 2:** How can the few-shot example generation process be improved to eliminate the need for labeled examples entirely? The paper discusses generating ACEs from few-shot examples but mentions future work on eliminating this requirement.
- **Open Question 3:** What mechanisms can be developed to improve the LLM's self-correction capabilities beyond simple hyperparameter tuning? The paper shows limited success with self-debugging and suggests exploring other self-correction methods.

## Limitations

- Framework's reliance on code generation introduces brittleness due to potential compilation errors, wrong return types, or module failures
- Abstract API may not be sufficiently general to cover all compositional reasoning scenarios, limiting applicability
- Self-tuning assumes failures are primarily due to hyperparameter settings, but failures could stem from hallucination, model limitations, or fundamentally incorrect reasoning paths

## Confidence

**High confidence:**
- Observation that existing LLM-as-programmer approaches require human-engineered ICEs and struggle with complex compositional reasoning
- General framework design combining few-shot ACE generation, abstract APIs, and self-correction mechanisms

**Medium confidence:**
- Performance improvements reported across datasets (implementation-dependent)
- Effectiveness of abstract API in reducing cognitive load

**Low confidence:**
- Scalability and generalizability to entirely new domains or more complex compositional tasks
- Robustness of self-tuning when failures are not due to hyperparameter settings

## Next Checks

1. **Prompt template validation:** Systematically test different prompt templates for code generation and self-debugging to determine sensitivity of performance to prompt engineering.

2. **Abstract API coverage analysis:** Conduct systematic analysis of abstract API's coverage across all reasoning types in test datasets to quantify what percentage of required operations are supported.

3. **Failure mode characterization:** Implement comprehensive logging of all code generation failures and categorize them by type to determine primary sources of failure and whether self-tuning addresses the most common ones.