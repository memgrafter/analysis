---
ver: rpa2
title: 'LLMJudge: LLMs for Relevance Judgments'
arxiv_id: '2408.08896'
source_url: https://arxiv.org/abs/2408.08896
tags:
- llms
- relevance
- llmjudge
- test
- retrieval
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The LLMJudge challenge explored using large language models (LLMs)
  to generate relevance judgments for information retrieval evaluation, addressing
  the high cost of human labeling. The challenge used a dataset derived from TREC
  2023 Deep Learning track, with 25 queries and thousands of passages in both development
  and test sets.
---

# LLMJudge: LLMs for Relevance Judgments

## Quick Facts
- arXiv ID: 2408.08896
- Source URL: https://arxiv.org/abs/2408.08896
- Reference count: 5
- Primary result: LLM-generated relevance judgments achieved moderate agreement with human labels (Cohen's kappa 0.8-0.9) while maintaining strong system ranking consistency (high Kendall's tau)

## Executive Summary
The LLMJudge challenge explored using large language models to generate relevance judgments for information retrieval evaluation as a cost-effective alternative to human labeling. Using a dataset from TREC 2023 Deep Learning track with 25 queries and thousands of passages, participants generated 0-3 relevance scores for query-document pairs using various LLMs and prompts. The evaluation revealed that while LLM judgments achieved moderate agreement with human labels, they consistently preserved system ranking order, suggesting LLMs can reliably evaluate retrieval systems even with imperfect label agreement.

## Method Summary
The study used a dataset derived from TREC 2023 Deep Learning track containing 25 queries with thousands of passages (7,224 for development, 4,414 for test). Participants submitted LLM-generated relevance judgments (0-3 scale) for query-document pairs using their chosen models and prompts. Generated judgments were evaluated against hidden human labels using Cohen's kappa for inter-rater agreement and Kendall's tau for system ordering consistency. The challenge received 39 submissions from 7 groups across multiple institutions.

## Key Results
- LLM-generated judgments achieved moderate agreement with human labels (Cohen's kappa around 0.8-0.9)
- Despite varying individual label agreement, LLM approaches maintained strong consistency in system rankings (Kendall's tau consistently high)
- The study provided insights into different LLM approaches for automatic relevance judgment with 39 submissions from 7 groups
- The collected data will be released to support automatic relevance judgment research in information retrieval

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLMs can generate relevance judgments that achieve moderate agreement with human labels while maintaining high consistency in system rankings.
- Mechanism: The LLM generates a 0-3 relevance score for each query-document pair, and these scores correlate strongly with human-generated rankings even when individual labels differ.
- Core assumption: The relative ordering of documents matters more for evaluation than exact label matching.
- Evidence anchors:
  - [section] "Results showed that while participants achieved moderate agreement with human judgments (Cohen's kappa around 0.8-0.9), they exhibited strong consistency in system rankings (Kendall's tau consistently high)"
  - [abstract] "while participants achieved moderate agreement with human judgments (Cohen's kappa around 0.8-0.9), they exhibited strong consistency in system rankings (Kendall's tau consistently high)"
- Break condition: If the LLM's scoring distribution diverges significantly from human scoring patterns, system rankings may become unreliable despite high Kendall's tau.

### Mechanism 2
- Claim: Different LLM approaches and prompts produce varying levels of agreement with human judgments, but all maintain reasonable system ranking consistency.
- Mechanism: The challenge format allows testing multiple LLMs and prompting strategies, revealing that different approaches trade off between label accuracy and ranking consistency.
- Core assumption: Multiple approaches can produce useful judgments even if they don't perfectly match human scores.
- Evidence anchors:
  - [section] "The study included 39 submissions from 7 groups across multiple institutions, providing insights into the effectiveness of different LLM approaches for automatic relevance judgment"
  - [abstract] "the collected data will be released as a package to support automatic relevance judgment research in information retrieval and search"
- Break condition: If prompt engineering or LLM selection doesn't account for domain-specific language patterns in the corpus.

### Mechanism 3
- Claim: Synthetic data generation can scale to build full test collections that produce similar system ordering to real collections.
- Mechanism: By generating both synthetic queries and synthetic judgments, LLMs can create complete evaluation datasets that preserve the relative performance of different retrieval systems.
- Core assumption: The structural properties of synthetic data mirror those of real data sufficiently for evaluation purposes.
- Evidence anchors:
  - [section] "Rahmani et al. [4] have studied fully synthetic test collection using LLMs. In their study, they not only generated synthetic queries but also synthetic judgment to build a full synthetic test collation for retrieval evaluation"
  - [corpus] "Weak evidence - corpus contains related papers on synthetic test collections but no direct validation data for this specific mechanism"
- Break condition: If synthetic data introduces systematic biases that consistently favor or disfavor certain system types.

## Foundational Learning

- Concept: Cohen's kappa for inter-rater agreement
  - Why needed here: To measure how well LLM-generated labels match human labels beyond chance agreement
  - Quick check question: If two labelers always give the same label, what would Cohen's kappa be?

- Concept: Kendall's tau for rank correlation
  - Why needed here: To evaluate whether LLM judgments preserve the relative ordering of systems even if individual labels differ
  - Quick check question: What does a Kendall's tau of 1.0 indicate about system rankings?

- Concept: Relevance judgment scales
  - Why needed here: Understanding the 0-3 scale (irrelevant to perfectly relevant) is essential for interpreting LLM outputs
  - Quick check question: Which score indicates a passage contains the exact answer to a query?

## Architecture Onboarding

- Component map: LLM -> Prompt template -> Query-document pair -> Relevance score (0-3) -> Collection of judgments -> Evaluation metrics (Cohen's kappa, Kendall's tau)
- Critical path: Input query-document -> LLM inference -> Score assignment -> Quality assessment
- Design tradeoffs: Closed-source LLMs (GPT-4) vs fine-tuned open-source LLMs for cost vs performance, single-shot vs few-shot prompting for flexibility vs consistency
- Failure signatures: Low Cohen's kappa with high Kendall's tau suggests systematic scoring differences; high kappa with low tau suggests inconsistent ranking ability
- First 3 experiments:
  1. Test different prompt templates on a small subset and measure Cohen's kappa against human labels
  2. Compare system rankings from different LLM approaches using Kendall's tau on the full dataset
  3. Analyze scoring distribution patterns to identify systematic biases in LLM judgments

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Which specific LLMs can match human-level accuracy in generating relevance judgments, and how do their performances compare across different domains or types of queries?
- Basis in paper: [explicit] The paper explicitly states that it remains unclear which LLMs can match the accuracy of human labelers in generating relevance judgments.
- Why unresolved: The challenge results showed moderate agreement with human judgments but did not definitively identify which LLMs perform best or under what conditions they excel. Different models were submitted but not systematically compared.
- What evidence would resolve it: A comprehensive comparative study across multiple domains, query types, and LLM architectures, with statistical analysis of their agreement with human judgments under controlled conditions.

### Open Question 2
- Question: What specific prompt formulations are most effective for generating reliable relevance judgments across different LLM architectures and datasets?
- Basis in paper: [explicit] The paper explicitly asks which prompts are most effective for the task, and the related work by Thomas et al. showed that LLM performance varies with simple paraphrases of prompts.
- Why unresolved: While the challenge collected submissions with different prompts, the results didn't isolate the impact of prompt design from model choice. The scatter plot in Figure 1 shows performance but doesn't attribute differences to specific prompt features.
- What evidence would resolve it: Systematic ablation studies varying prompt components (format, examples, instructions) across multiple models and datasets, with statistical significance testing.

### Open Question 3
- Question: Do synthetic relevance judgments generated by LLMs contain systematic biases that could affect retrieval system evaluation, and if so, what are their characteristics?
- Basis in paper: [explicit] The paper explicitly asks whether there are biases in synthetically generated data, which remains an open question.
- Why unresolved: The challenge measured agreement with human judgments but didn't analyze potential systematic biases in the LLM-generated labels, such as tendency toward certain score distributions or sensitivity to specific document features.
- What evidence would resolve it: Analysis of LLM judgment distributions compared to human distributions, testing for statistical bias in specific dimensions (document length, query complexity, etc.), and validation on diverse datasets.

### Open Question 4
- Question: How does data leakage affect the quality of LLM-generated relevance judgments, and what measures can prevent it?
- Basis in paper: [explicit] The paper explicitly asks if data leakage affects the quality of generated labels, which remains unexplored in the challenge.
- Why unresolved: The challenge dataset was derived from TREC 2023, but the paper doesn't discuss whether participating LLMs might have been trained on related data, nor does it measure the impact of any such overlap.
- What evidence would resolve it: Controlled experiments comparing judgment quality when models are trained/pretrained on overlapping vs. non-overlapping data, and development of techniques to detect and mitigate leakage effects.

## Limitations
- The study lacks detailed analysis of individual submission strategies, making it difficult to identify which prompting techniques were most effective
- Limited error analysis showing where LLM judgments systematically diverge from human judgments
- Test set contains only 25 queries, which may not provide sufficient statistical power for definitive conclusions

## Confidence
- High confidence: The claim that LLMs can achieve moderate Cohen's kappa agreement (0.8-0.9) with human labels is well-supported by the results presented
- Medium confidence: The assertion that LLMs maintain strong Kendall's tau for system ranking is supported, but the specific mechanisms for achieving this across different approaches require more detailed investigation
- Low confidence: The generalizability of these findings to other domains or larger query sets remains uncertain due to the limited scope of the TREC 2023 dataset used

## Next Checks
1. Conduct error analysis on specific query-document pairs where LLM judgments differ most from human labels to identify systematic patterns in LLM judgment errors
2. Test the same LLM approaches on a larger, more diverse query set to assess generalizability beyond the TREC 2023 Deep Learning track domain
3. Implement and evaluate different prompt engineering strategies (e.g., few-shot vs zero-shot, different instruction templates) to identify which approaches yield the best balance of label accuracy and ranking consistency