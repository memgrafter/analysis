---
ver: rpa2
title: 'In-Context Learning May Not Elicit Trustworthy Reasoning: A-Not-B Errors in
  Pretrained Language Models'
arxiv_id: '2409.15454'
source_url: https://arxiv.org/abs/2409.15454
tags:
- a-not-b
- llms
- reasoning
- replace
- actual
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper investigates whether Large Language Models (LLMs) exhibit
  inhibitory control abilities comparable to human infants, using a modified version
  of the classic A-not-B cognitive task. The authors design text-based multiple-choice
  questions (MCQs) where the correct answer changes from a consistent pattern (Option
  A) to a different one (Option B).
---

# In-Context Learning May Not Elicit Trustworthy Reasoning: A-Not-B Errors in Pretrained Language Models

## Quick Facts
- **arXiv ID**: 2409.15454
- **Source URL**: https://arxiv.org/abs/2409.15454
- **Reference count**: 38
- **Primary result**: State-of-the-art LLMs exhibit A-Not-B errors, showing up to 83.3% accuracy drop when correct answers change from A to B in in-context learning

## Executive Summary
This paper investigates whether large language models possess inhibitory control abilities comparable to human infants by adapting the classic A-not-B cognitive task to text-based reasoning. The authors design multiple-choice questions where the correct answer shifts from a consistent pattern (Option A) to a different one (Option B), revealing that even state-of-the-art models like Llama3-8b make significant errors when this context changes. The study demonstrates that LLMs perform well with in-context learning but fail to suppress previously established response patterns, showing accuracy drops of up to 83.3%. These findings suggest LLMs only possess infant-level cognitive abilities in this regard, raising concerns about their reliability in dynamic reasoning tasks.

## Method Summary
The authors adapt the A-not-B cognitive task to test LLMs by creating two prompting setups: original prompting with varied correct answers across examples, and A-not-B prompting where all examples have correct answer A, then test on questions where answer B is correct. They preprocess four reasoning datasets (MathQA, CommonsenseQA, Winogrande, SciQ) into binary multiple-choice format and run inference on four models (Llama3-8b, Llama3-70b, Qwen1.5-7b, Qwen1.5-72b) with both prompt types. Performance is measured by comparing accuracy between original and A-not-B settings to identify errors. The study also examines factors affecting performance including model size, number of prompt examples, and reasoning task type.

## Key Results
- LLMs show accuracy drops of up to 83.3% when correct answers shift from A to B in in-context learning scenarios
- Smaller models consistently show lower resilience to A-not-B errors compared to larger models across all shots
- Arithmetic reasoning tasks exhibit the most dramatic performance drops, while commonsense reasoning shows more moderate declines
- Better pretraining data quality and larger model sizes can help reduce A-not-B errors

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLMs exhibit A-Not-B errors due to limited inhibitory control, failing to suppress previously established response patterns during in-context learning.
- Mechanism: When presented with multiple-choice questions where the correct answer changes from a consistent pattern (Option A) to a different one (Option B), LLMs continue to select Option A despite the change, similar to human infants' behavior in the original A-Not-B task.
- Core assumption: The observed performance drop (up to 83.3%) is primarily due to the model's inability to inhibit the learned pattern rather than a lack of reasoning ability.
- Evidence anchors:
  - [abstract] "We found that state-of-the-art LLMs (like Llama3-8b) perform consistently well with in-context learning (ICL) but make errors and show a significant drop of as many as 83.3% in reasoning tasks when the context changes trivially."
  - [section] "We found that state-of-the-art LLMs (like Llama3-8b) perform consistently well with in-context learning (ICL) but make errors and show a significant drop of as many as 83.3% in reasoning tasks when the context changes trivially."
  - [corpus] Weak evidence; no direct corpus support for inhibitory control mechanisms in LLMs.

### Mechanism 2
- Claim: Model size and training data quality significantly impact the severity of A-Not-B errors in LLMs.
- Mechanism: Larger models with more parameters and higher-quality training data are better equipped to handle context changes and suppress previously established patterns, resulting in fewer A-Not-B errors.
- Core assumption: The relationship between model size, training data quality, and inhibitory control is causal, with larger models and better data leading to improved performance in A-Not-B scenarios.
- Evidence anchors:
  - [abstract] "Our findings suggest that during the training stage, model size and training data quality are critical factorsâ€”larger models and higher-quality training data can help reduce these errors."
  - [section] "The results show that model size significantly impacts performance, with smaller models consistently showing lower resilience (less accuracy) compared to larger models across all shots."
  - [corpus] Weak evidence; no direct corpus support for the relationship between model size, training data quality, and A-Not-B errors.

### Mechanism 3
- Claim: The type of reasoning task and the number of prompt examples influence the likelihood of A-Not-B errors in LLMs.
- Mechanism: Tasks that require more complex reasoning or have less straightforward logical patterns are more susceptible to A-Not-B errors. Additionally, providing more examples reinforces the established pattern, making it harder for the model to suppress and adapt to context changes.
- Core assumption: The difficulty of the reasoning task and the number of examples directly impact the model's ability to inhibit previously learned patterns and adapt to new contexts.
- Evidence anchors:
  - [abstract] "Moreover, the type of task and the number of prompt examples significantly impact performance; LLMs are more likely to rely on superficial A-Not-B patterns, particularly in reasoning tasks that involve less straightforward logical patterns and memorization, and when the A-Not-B pattern is more strongly reinforced."
  - [section] "In contrast, the commonsense reasoning dataset (CommonsenseQA (Talmor et al., 2019)) shows a more moderate performance drop across all shots, which is consistently less dramatic compared to arithmetic reasoning."
  - [corpus] Weak evidence; no direct corpus support for the relationship between task type, number of examples, and A-Not-B errors.

## Foundational Learning

- Concept: In-context learning (ICL)
  - Why needed here: Understanding ICL is crucial because the A-Not-B errors occur during this process, where the model learns from examples provided in the prompt without updating its parameters.
  - Quick check question: What is the primary difference between in-context learning and traditional fine-tuning?

- Concept: Inhibitory control
  - Why needed here: Inhibitory control is the core cognitive ability being tested in the A-Not-B task, as it involves the model's ability to suppress previously learned patterns and adapt to new contexts.
  - Quick check question: How does inhibitory control relate to the model's ability to handle context changes in the A-Not-B scenario?

- Concept: Multiple-choice question answering (MCQA)
  - Why needed here: The A-Not-B task is implemented using MCQA datasets, making it essential to understand how models approach and answer these types of questions.
  - Quick check question: What are the potential challenges models face when answering MCQA questions, and how might these challenges contribute to A-Not-B errors?

## Architecture Onboarding

- Component map: LLMs (Llama3, Qwen) -> MCQA datasets (MathQA, CommonsenseQA, Winogrande, SciQ) -> Prompting strategy (original vs A-Not-B) -> Performance measurement
- Critical path: 1) Preprocess MCQA datasets to binary format with consistent ground truth answers 2) Create original and A-Not-B prompts 3) Run models on both prompt types 4) Compare performance to identify A-Not-B errors
- Design tradeoffs: Larger models and higher-quality training data reduce errors but require more computational resources; more examples strengthen patterns but make adaptation harder
- Failure signatures: Significant accuracy drop (up to 83.3%) when correct answer changes from consistent pattern A to different answer B
- First 3 experiments:
  1. Run Llama3-8b on original and A-Not-B prompts using MathQA dataset with 3, 5, 10, and 25 examples to observe performance drop
  2. Compare Llama3-8b and Llama3-70b on A-Not-B prompts using CommonsenseQA dataset with 10 examples to investigate model size impact
  3. Test Qwen1.5-7b on A-Not-B prompts using Winogrande dataset with 3, 5, 10, and 25 examples to examine relationship between examples and errors

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can advanced prompting strategies like self-explanation or chain-of-thought consistently mitigate A-not-B errors across different reasoning tasks?
- Basis in paper: [explicit] The paper mentions that self-explanation can help reduce A-not-B errors to some extent, but not enough to eliminate them, particularly in challenging arithmetic reasoning tasks.
- Why unresolved: The study only explores self-explanation in the context of arithmetic reasoning, leaving open the question of whether it would be effective for other reasoning tasks or if other prompting strategies might be more effective.
- What evidence would resolve it: Systematic experiments applying self-explanation and other advanced prompting techniques across a variety of reasoning tasks (e.g., commonsense, causal, scientific) to measure their effectiveness in mitigating A-not-B errors.

### Open Question 2
- Question: How do different pretraining data qualities and quantities influence LLMs' resilience to A-not-B errors?
- Basis in paper: [explicit] The paper suggests that better pretraining data (both in quality and quantity) can enhance LLMs' inhibitory control and reduce A-not-B errors, as evidenced by comparing Llama3 models to Llama2.
- Why unresolved: The study does not provide a detailed analysis of how specific characteristics of pretraining data (e.g., diversity, complexity, domain coverage) correlate with performance on A-not-B tasks.
- What evidence would resolve it: Controlled experiments varying the characteristics of pretraining data and measuring their impact on LLMs' performance in A-not-B scenarios.

### Open Question 3
- Question: Are there fundamental differences in the reasoning processes of LLMs and humans that explain why LLMs are more susceptible to A-not-B errors?
- Basis in paper: [inferred] The paper notes that humans are much more resilient to A-not-B errors than LLMs, suggesting potential differences in reasoning processes.
- Why unresolved: The study does not investigate the underlying mechanisms of reasoning in LLMs and humans, leaving open the question of why LLMs fail where humans succeed.
- What evidence would resolve it: Comparative studies analyzing the reasoning strategies and cognitive processes of LLMs and humans in A-not-B tasks, potentially using techniques like cognitive modeling or neural network interpretability.

## Limitations

- The study relies on artificially constructed binary-choice scenarios that may not accurately represent real-world reasoning failures in LLMs
- The mechanistic explanation for A-not-B errors (limited inhibitory control) is plausible but not definitively proven, with alternative explanations possible
- The study focuses on relatively small-scale models (up to 70B parameters) and does not explore whether larger frontier models might demonstrate different inhibitory control capabilities

## Confidence

- **High confidence**: The empirical observation that LLMs show significant accuracy drops (up to 83.3%) when correct answers shift from A to B patterns in in-context learning scenarios. This finding is directly measurable and reproducible across multiple models and datasets.
- **Medium confidence**: The interpretation that these errors reflect fundamental limitations in inhibitory control analogous to infant cognitive development. While the experimental evidence supports this view, alternative explanations cannot be ruled out without further investigation.
- **Low confidence**: The claim that larger models and higher-quality training data can significantly mitigate A-not-B errors. The paper provides suggestive evidence but does not establish causal relationships or explore the underlying mechanisms.

## Next Checks

1. **Test frontier models**: Evaluate GPT-4, Claude 3, and Gemini Pro on the same A-not-B task to determine whether larger, more capable models exhibit the same pattern of errors. This would clarify whether the phenomenon scales with model size or represents a fundamental limitation across all LLMs.

2. **Analyze failure cases**: Conduct detailed error analysis on model outputs to distinguish between true A-not-B errors (continuing to select A despite context change) and other failure modes such as token preference biases or misinterpretation of the task. This would validate whether the observed accuracy drops genuinely reflect inhibitory control limitations.

3. **Explore architectural interventions**: Test whether architectural modifications such as attention mechanism adjustments, additional context windows, or specialized prompting strategies can reduce A-not-B errors. This would help determine whether the phenomenon represents an inherent limitation or a solvable engineering challenge.