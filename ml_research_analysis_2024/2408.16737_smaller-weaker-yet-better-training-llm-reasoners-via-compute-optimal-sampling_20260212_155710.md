---
ver: rpa2
title: 'Smaller, Weaker, Yet Better: Training LLM Reasoners via Compute-Optimal Sampling'
arxiv_id: '2408.16737'
source_url: https://arxiv.org/abs/2408.16737
tags:
- data
- sampling
- finetuning
- arxiv
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates whether using a weaker but cheaper language
  model for synthetic data generation is more compute-optimal than using a stronger
  but more expensive model under a fixed inference budget. The authors compare data
  generated by Gemma2-9B (weaker, cheaper) and Gemma2-27B (stronger, expensive) across
  coverage, diversity, and false positive rate metrics, finding that the weaker model
  achieves 11% higher coverage and 86% higher diversity but 7% higher false positive
  rate.
---

# Smaller, Weaker, Yet Better: Training LLM Reasoners via Compute-Optimal Sampling

## Quick Facts
- **arXiv ID**: 2408.16737
- **Source URL**: https://arxiv.org/abs/2408.16737
- **Reference count**: 40
- **Primary result**: Models trained on data generated by weaker but cheaper models outperform those trained on data from stronger but more expensive models under a fixed compute budget.

## Executive Summary
This paper investigates whether using a weaker but cheaper language model for synthetic data generation is more compute-optimal than using a stronger but more expensive model under a fixed inference budget. The authors compare data generated by Gemma2-9B (weaker, cheaper) and Gemma2-27B (stronger, expensive) across coverage, diversity, and false positive rate metrics, finding that the weaker model achieves 11% higher coverage and 86% higher diversity but 7% higher false positive rate. They then finetune models in three paradigms - knowledge distillation, self-improvement, and a novel weak-to-strong improvement setup - and consistently find that models trained on weaker model-generated data outperform those trained on stronger model-generated data, with relative gains up to 31.6%. The results hold across multiple benchmarks and even when scaling to state-of-the-art models like Gemini-1.5, suggesting that compute-optimal sampling from weaker models may be a more efficient approach for training strong reasoners.

## Method Summary
The authors generate synthetic solutions using Gemma2-9B (weaker, cheaper) and Gemma2-27B (stronger, expensive) models at compute-matched budgets on MATH and GSM-8K datasets, filtering by final answer correctness. They then finetune Gemma-7B, Gemma2-9B, and Gemma2-27B models using supervised finetuning in three paradigms: knowledge distillation (stronger model learning from weaker), self-improvement (model learning from its own data), and weak-to-strong improvement (stronger model learning from weaker model's data). The finetuning uses batch sizes of 8 or 32 for 600-24000 steps depending on model and budget, with performance measured by pass@1 accuracy on held-out test sets.

## Key Results
- Weaker models achieve 11% higher coverage and 86% higher diversity than stronger models under compute-matched sampling
- Models finetuned on weaker model-generated data consistently outperform those trained on stronger model-generated data, with relative gains up to 31.6%
- Weak-to-strong improvement paradigm shows the strongest gains, outperforming self-improvement in 6 out of 7 comparisons
- Results hold when scaling to state-of-the-art models like Gemini-1.5, with compute-matched sampling from Flash outperforming sampling from Pro

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Compute-optimal sampling from weaker models increases problem coverage and solution diversity more than sampling from stronger models under a fixed budget.
- Mechanism: Under a fixed compute budget, generating more samples from a smaller model yields higher coverage (more unique problems solved) and higher diversity (more unique solutions per problem) compared to fewer samples from a larger model.
- Core assumption: The number of inference tokens needed per solution is roughly model-agnostic, so weaker models can generate proportionally more solutions within the same compute budget.
- Evidence anchors:
  - [abstract] "weaker but cheaper (WC) model versus a stronger but more expensive (SE) model... higher coverage and diversity, but also exhibit higher false positive rates."
  - [section] "Equation 1 indicates that at a fixed sampling budget, for each question we can generate PSE/PWC more samples from WC."
- Break condition: If the cost per solution differs significantly between models due to architectural differences or if the weaker model's solutions are so low quality that they fail to cover meaningful problem space.

### Mechanism 2
- Claim: Finetuning on data generated by weaker models can yield better reasoning performance than finetuning on data from stronger models.
- Mechanism: The increased coverage and diversity in WC-generated data provide a richer training distribution that compensates for higher false positive rates, leading to stronger generalization.
- Core assumption: The training process can learn to distinguish correct reasoning paths even when some false positives are present, and the diversity helps avoid overfitting to narrow solution patterns.
- Evidence anchors:
  - [abstract] "models finetuned on WC-generated data consistently outperform those trained on SE-generated data across multiple benchmarks."
  - [section] "Our findings reveal that models finetuned on WC-generated data consistently outperform those trained on SE-generated data across multiple benchmarks."
- Break condition: If the false positive rate becomes too high relative to the coverage gain, or if the model overfits to the noise in WC data.

### Mechanism 3
- Claim: Weak-to-strong improvement (W2S-I) can enhance strong models more effectively than self-improvement with their own data.
- Mechanism: The stronger model benefits from exposure to diverse reasoning patterns generated by the weaker model, which provides complementary perspectives not captured in its own self-generated data.
- Core assumption: The weaker model's solutions, despite being less refined, contain unique problem-solving approaches that can improve the stronger model's reasoning breadth.
- Evidence anchors:
  - [abstract] "a novel weak-to-strong improvement setup where a weaker LM teaches reasoning to a stronger LM."
  - [section] "finetune the SE model on the WC and SE data... training SE on data from WC corresponds to W2S-I."
- Break condition: If the stronger model's architecture is so advanced that self-generated data is already maximally informative, or if the weaker model's reasoning quality is too low to be beneficial.

## Foundational Learning

- Concept: Compute-optimal sampling and its relationship to model parameters and FLOPs.
  - Why needed here: The core insight relies on understanding how model size affects sampling cost and how to allocate a fixed compute budget across different models.
  - Quick check question: If a 9B model generates 3x more samples than a 27B model under the same compute budget, what is the approximate parameter ratio between them?

- Concept: Coverage and diversity metrics for synthetic data evaluation.
  - Why needed here: These metrics quantify the quality of synthetic data and explain why WC data can be superior despite higher false positive rates.
  - Quick check question: If coverage@1 is 40% for a model and diversity@1 is 1.5, what does this tell you about the model's problem-solving and solution variety?

- Concept: Knowledge distillation, self-improvement, and weak-to-strong improvement paradigms.
  - Why needed here: The paper tests multiple finetuning setups to establish the robustness of the WC advantage across different learning scenarios.
  - Quick check question: In knowledge distillation, what is the relationship between the student and teacher models, and how does this differ from self-improvement?

## Architecture Onboarding

- Component map:
  - Data generation pipeline: Model selection (WC vs SE) → Sampling strategy (compute-matched) → Solution filtering (final answer correctness) → Dataset creation
  - Evaluation pipeline: Coverage/diversity/FPR computation → Supervised finetuning on generated data → Pass@1 accuracy measurement on test sets
  - Model zoo: Gemma2-9B (WC), Gemma2-27B (SE), Gemma-7B (student), Gemini-1.5-Pro/Flash for scaling experiments

- Critical path:
  1. Select WC and SE model pair with known parameter sizes
  2. Generate synthetic solutions for training dataset under compute-matched sampling
  3. Filter solutions by final answer correctness
  4. Finetune target model on filtered data
  5. Evaluate on held-out test sets using pass@1 accuracy

- Design tradeoffs:
  - Higher coverage vs higher false positive rate: More samples from WC increase coverage but also increase noise
  - Sampling budget allocation: Compute-matched vs number-matched sampling significantly affects results
  - Model size vs reasoning quality: Smaller models may generate more diverse solutions but with lower reasoning quality

- Failure signatures:
  - Low coverage improvement despite higher sampling: WC model's solutions are too poor to solve new problems
  - Decreased accuracy after finetuning: False positive rate overwhelms the benefits of increased coverage/diversity
  - No performance difference between WC and SE data: Coverage/diversity gains are minimal or model is already saturated

- First 3 experiments:
  1. Generate synthetic data from Gemma2-9B and Gemma2-27B at low sampling budget, compute coverage/diversity/FPR, and verify WC advantages
  2. Finetune Gemma-7B on both datasets and measure pass@1 accuracy to confirm WC superiority in knowledge distillation setup
  3. Repeat experiment with high sampling budget and evaluate generalization on Functional MATH to test robustness

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do the quality dimensions (coverage, diversity, false positive rate) interact with each other across different sampling budgets and model pairs?
- Basis in paper: [explicit] The paper measures coverage, diversity, and false positive rate for different model pairs (Gemma2-9B vs Gemma2-27B, Gemini-Pro vs Gemini-Flash) at different sampling budgets
- Why unresolved: The paper presents these metrics separately but doesn't analyze their interrelationships or how they trade off against each other
- What evidence would resolve it: A comprehensive analysis showing how changes in one metric affect the others across different model pairs and sampling regimes

### Open Question 2
- Question: What is the optimal balance between sampling compute budget and model parameter size for maximizing reasoning performance?
- Basis in paper: [inferred] The paper shows that compute-matched sampling from weaker models outperforms sampling from stronger models, but doesn't explore the full parameter size-sampling budget space
- Why unresolved: The experiments focus on specific model pairs and sampling ratios, leaving the broader optimization landscape unexplored
- What evidence would resolve it: Systematic experiments varying both model parameter sizes and sampling budgets to identify the optimal combinations

### Open Question 3
- Question: How does the weak-to-strong improvement paradigm perform when scaling to larger models and more complex reasoning tasks?
- Basis in paper: [explicit] The paper introduces the weak-to-strong improvement paradigm and tests it on MATH and GSM-8K datasets with Gemma and Gemini models
- Why unresolved: The experiments are limited to specific model sizes and reasoning datasets, leaving questions about scalability and generalizability
- What evidence would resolve it: Results from applying the paradigm to larger models (e.g., >100B parameters) and more complex reasoning tasks (e.g., multi-step scientific reasoning)

## Limitations
- The compute-matched sampling assumption may not hold precisely across different architectures
- Evaluation relies on a single verification method (Gemini-1.5-Pro) which could introduce bias
- Results are primarily validated on reasoning tasks (MATH, GSM-8K) and may not generalize to other domains
- The precise magnitude of improvements is sensitive to implementation details like sampling temperature and prompt engineering

## Confidence

**High Confidence**: The core finding that weaker models generate more diverse solutions under compute-matched budgets is well-supported by the coverage/diversity metrics and aligns with fundamental scaling laws. The observation that training on WC-generated data improves performance is robust across three different finetuning paradigms and holds across multiple model sizes.

**Medium Confidence**: The weak-to-strong improvement mechanism, while theoretically compelling, has limited empirical validation. The paper demonstrates this in only one setup (Gemma2-9B training on its own vs WC data), and the magnitude of improvement varies significantly across different model scales. The scaling behavior to state-of-the-art models like Gemini-1.5 shows promise but requires more extensive validation.

**Low Confidence**: The claim about computational optimality and the precise 31.6% relative gain figure are sensitive to implementation details like sampling temperature, prompt engineering, and filtering criteria. Small changes in these parameters could substantially affect the measured performance differences.

## Next Checks

1. **Architecture-Specific Cost Validation**: Measure actual inference costs (latency, FLOPs) for both Gemma2-9B and Gemma2-27B across different problem types to verify the proportional scaling assumption. Test whether the compute-matched sampling budget allocation remains optimal under different hardware configurations.

2. **Verification Method Ablation**: Replicate the coverage/diversity/FPR measurements using multiple verification approaches (human evaluation, different strong models, chain-of-thought consistency checks) to assess the robustness of the WC advantages to verification methodology.

3. **Domain Generalization Test**: Apply the WC training approach to non-reasoning tasks like summarization, classification, or code generation to determine whether the coverage/diversity benefits translate to other domains or are specific to mathematical reasoning.