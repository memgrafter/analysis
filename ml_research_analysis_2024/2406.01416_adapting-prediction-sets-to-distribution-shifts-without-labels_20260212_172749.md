---
ver: rpa2
title: Adapting Prediction Sets to Distribution Shifts Without Labels
arxiv_id: '2406.01416'
source_url: https://arxiv.org/abs/2406.01416
tags:
- coverage
- prediction
- test
- distribution
- shifts
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper tackles the problem of maintaining reliable prediction
  sets under distribution shifts when ground truth labels are unavailable at test
  time. The authors introduce two new methods, ECP and EACP, which adjust the score
  function in conformal prediction based on the base model's own uncertainty evaluation.
---

# Adapting Prediction Sets to Distribution Shifts Without Labels

## Quick Facts
- arXiv ID: 2406.01416
- Source URL: https://arxiv.org/abs/2406.01416
- Reference count: 40
- This paper introduces ECP and EACP methods that adapt conformal prediction sets to distribution shifts using only unlabeled test data, improving coverage without labels.

## Executive Summary
This paper addresses the challenge of maintaining reliable prediction sets under distribution shifts when ground truth labels are unavailable at test time. The authors propose two methods, ECP and EACP, that leverage the base model's uncertainty estimation to adapt conformal prediction sets. ECP scales the score function by an entropy quantile computed from unlabeled test data, while EACP further refines this by updating the base model using entropy minimization during test time adaptation. Both methods consistently improve coverage compared to standard baselines across extensive experiments on large-scale datasets and various neural network architectures.

## Method Summary
The paper introduces two entropy-based methods for adapting prediction sets under distribution shifts without labels. ECP (Entropy scaled Conformal Prediction) scales the original conformal score function by the entropy quantile computed from the base model's predictions on test data. EACP (Entropy base-Adapted Conformal Prediction) extends this by first updating the base model using entropy minimization on the unlabeled test data, then applying ECP scaling. Both methods use polynomial scaling functions (linear or quadratic) to adjust prediction set sizes based on the estimated severity of distribution shift, measured through entropy of the model's softmax outputs.

## Key Results
- ECP and EACP significantly improve coverage under distribution shifts compared to standard baselines like SplitCP and NAIVE
- EACP nearly matches the performance of fully supervised oracle methods while using no labels
- The methods perform well across multiple architectures (ResNet-50, ViT variants) and diverse shift types (corruptions, semantic shifts)
- Linear and quadratic scaling functions both work well, with quadratic providing better coverage on severe shifts

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Entropy quantile scaling effectively estimates the inflation factor needed for prediction sets under distribution shift.
- Mechanism: The entropy of the base model's softmax outputs on test data increases with the severity of distribution shift. By scaling the conformal score function by the entropy quantile, the prediction set expands proportionally to the estimated uncertainty.
- Core assumption: Entropy of the base model's predictions is strongly correlated with the magnitude of distribution shift.
- Evidence anchors:
  - [abstract]: "the amount of such inflation is strongly correlated with the magnitude of the distribution shift, through the use of the entropy quantile."
  - [section 3.1]: "Previous works have established the relation between such an entropy notion and the magnitude of the distribution shift, showing that larger shifts are strongly correlated with higher entropy."
  - [corpus]: Weak - corpus contains related work on test-time adaptation but no direct mention of entropy-quantile scaling for conformal prediction.
- Break condition: If entropy becomes uncorrelated with shift magnitude (e.g., in adversarial perturbations designed to fool entropy), the scaling factor would be inappropriate.

### Mechanism 2
- Claim: Entropy minimization during test time adaptation refines the base model's uncertainty estimates and improves set calibration.
- Mechanism: By minimizing the entropy loss on unlabeled test data, the base model updates to be more confident on the shifted distribution, which reduces the entropy quantile and thus shrinks the prediction sets while maintaining coverage.
- Core assumption: Entropy minimization leads to meaningful adaptation without labels and improves the base model's alignment with the test distribution.
- Evidence anchors:
  - [abstract]: "the base model is better suited for the shifted distribution Dtest, which generally improves the quality of the prediction sets built on top of it."
  - [section 3.2]: "the updated base model is better suited for the shifted distribution Dtest, which generally improves the quality of the prediction sets built on top of it."
  - [corpus]: Moderate - corpus includes "Uncertainty-Calibrated Test-Time Model Adaptation without Forgetting" which suggests entropy-based adaptation is studied, but not specifically for conformal prediction.
- Break condition: If the entropy minimization update is unstable or leads to catastrophic forgetting of the original task, adaptation quality degrades.

### Mechanism 3
- Claim: Linear or polynomial scaling of entropy quantile provides an effective way to adjust prediction set sizes based on shift severity.
- Mechanism: The relationship between entropy quantile and the ratio of conformal thresholds (calibration vs. test) can be approximated by polynomial functions, allowing principled scaling beyond linear.
- Core assumption: The log-log relationship between entropy quantile and threshold ratio follows a predictable polynomial pattern that can be estimated.
- Evidence anchors:
  - [section 3.3]: "Figure 3 shows that the optimal polynomial order generally increases with the severity of the distribution shift" and "we will empirically validate that our methods with either linear scaling (denoted by ECP1 / EACP1) or quadratic scaling (denoted by ECP2 / EACP2) perform well in a wide range of settings."
  - [corpus]: Weak - corpus doesn't contain direct evidence about polynomial scaling relationships for conformal prediction.
- Break condition: If the polynomial relationship doesn't hold for new types of distribution shifts (e.g., label distribution shifts), the scaling becomes suboptimal.

## Foundational Learning

- Concept: Conformal prediction and split conformal prediction
  - Why needed here: The entire method builds on split conformal prediction as the base framework for generating prediction sets with coverage guarantees.
  - Quick check question: What is the key difference between standard split conformal prediction and the methods proposed in this paper?

- Concept: Entropy as a measure of model uncertainty
  - Why needed here: The entropy of softmax outputs is used as the core uncertainty metric for scaling prediction sets.
  - Quick check question: How is entropy calculated from a model's softmax outputs, and why is it a good proxy for uncertainty?

- Concept: Test-time adaptation (TTA) and entropy minimization
  - Why needed here: EACP builds on TTA techniques, specifically entropy minimization, to update the base model on unlabeled test data.
  - Quick check question: What is the difference between entropy minimization and other TTA objectives like self-supervised learning?

## Architecture Onboarding

- Component map:
  Base model -> Entropy calculator -> Quantile estimator -> Scaling function -> Score function -> Threshold calculator -> Prediction set generator

- Critical path:
  1. Load pre-trained base model and calibration threshold
  2. For each test batch: compute entropy, update base model (if EACP), compute entropy quantile, scale scores, generate prediction sets
  3. Return prediction sets

- Design tradeoffs:
  - Linear vs. quadratic scaling: Linear provides smaller sets but may under-cover on severe shifts; quadratic provides better coverage but larger sets
  - Batch size for TTA: Larger batches provide more stable entropy estimates but may be computationally expensive
  - Entropy quantile parameter (β): Setting β=1-α provides a simple default but may need tuning for specific applications

- Failure signatures:
  - Coverage consistently below target despite scaling: likely entropy-uncertainty correlation is broken or scaling function is inappropriate
  - Prediction sets become empty: scaling factor too large or base model adaptation failing
  - Set sizes don't vary with uncertainty: entropy calculation or quantile estimation is incorrect

- First 3 experiments:
  1. Verify entropy correlation: Plot entropy vs. true label softmax score on shifted data to confirm expected inverse relationship
  2. Test scaling function: Implement ECP with different scaling functions (linear, quadratic) on a simple distribution shift and observe coverage/set size tradeoff
  3. Validate TTA integration: Compare EACP vs. ECP on a dataset with moderate shift to verify adaptation improves performance

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of ECP and EACP scale with the size of the unlabeled test dataset Dtest, particularly for very small N?
- Basis in paper: [inferred] The methods rely on entropy quantile estimation from the test data, but the paper does not analyze the effect of test dataset size on estimation reliability or method performance.
- Why unresolved: No experiments are shown with varying N, and theoretical analysis of the trade-off between estimation variance and effective scaling is absent.
- What evidence would resolve it: Systematic experiments varying N (e.g., 10, 100, 1000 samples) and theoretical bounds on entropy quantile convergence under distribution shift.

### Open Question 2
- Question: Are there better uncertainty measures than entropy that could be used for scaling in ECP/EACP, and how do they compare empirically?
- Basis in paper: [explicit] The paper mentions exploring alternative uncertainty measures but focuses on entropy, and briefly dismisses softmax variance and 1-max softmax without exhaustive comparison.
- Why unresolved: Only three uncertainty measures are compared, and no ablation studies with other candidates (e.g., mutual information, energy-based scores) are provided.
- What evidence would resolve it: Comprehensive experiments testing multiple uncertainty metrics on the same benchmarks, reporting coverage and set size trade-offs.

### Open Question 3
- Question: Can the scaling function f in ECP/EACP be learned adaptively at test time without label access, and would this improve performance?
- Basis in paper: [explicit] The paper suggests the optimal f depends on the unknown ratio τD/τtest and only explores fixed polynomial scalings.
- Why unresolved: No method is proposed or tested for learning f in an unsupervised manner, leaving open whether adaptive scaling could close the gap to oracle methods further.
- What evidence would resolve it: A proposed algorithm for unsupervised learning of f (e.g., meta-learning style updates) and experimental validation on shifting datasets.

## Limitations

- The methods rely critically on the assumption that entropy correlates with distribution shift severity, which may break down for certain shift types.
- Polynomial scaling assumes a predictable relationship between entropy quantile and threshold ratio that may not generalize to novel shift scenarios.
- The theoretical justification for polynomial scaling remains largely empirical without rigorous mathematical foundation.

## Confidence

**High confidence**: The core claim that entropy scaling improves conformal prediction coverage under covariate shifts is well-supported by extensive experiments across multiple datasets and architectures.

**Medium confidence**: The effectiveness of entropy minimization during test-time adaptation (EACP) is demonstrated but relies on the stability of ETA from referenced work.

**Low confidence**: The theoretical justification for polynomial scaling of entropy quantile remains largely empirical with limited mathematical foundation.

## Next Checks

1. **Break condition testing**: Systematically test ECP/EACP on adversarial perturbations specifically designed to decouple entropy from shift severity to identify the boundaries of method applicability.

2. **Scaling function generalization**: Evaluate the polynomial scaling approach on synthetic distribution shifts with known theoretical properties to validate whether the observed relationships generalize beyond the tested real-world datasets.

3. **Cross-architecture robustness**: Test the methods across a wider range of neural network architectures (RNNs, Transformers with different attention mechanisms) to assess whether the entropy-shift correlation is architecture-dependent.