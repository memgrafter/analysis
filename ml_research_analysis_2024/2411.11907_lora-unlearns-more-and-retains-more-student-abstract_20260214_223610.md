---
ver: rpa2
title: LoRA Unlearns More and Retains More (Student Abstract)
arxiv_id: '2411.11907'
source_url: https://arxiv.org/abs/2411.11907
tags:
- unlearning
- lora
- dataset
- remaining
- computational
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work tackles the challenge of machine unlearning, where a
  model must remove the influence of a specific class while retaining performance
  on remaining classes. The proposed PruneLoRA method combines model pruning with
  Low-Rank Adaptation (LoRA) to achieve efficient and effective unlearning.
---

# LoRA Unlearns More and Retains More (Student Abstract)

## Quick Facts
- arXiv ID: 2411.11907
- Source URL: https://arxiv.org/abs/2411.11907
- Authors: Atharv Mittal
- Reference count: 1
- This work tackles the challenge of machine unlearning, where a model must remove the influence of a specific class while retaining performance on remaining classes.

## Executive Summary
This work introduces PruneLoRA, a method that combines model pruning with Low-Rank Adaptation (LoRA) to achieve efficient and effective unlearning. The approach aims to remove the influence of a specific class from a trained model while maintaining performance on the remaining classes. Experiments on ResNet-50 and Vision Transformer (ViT) models trained on CIFAR-10 demonstrate that PruneLoRA achieves near-perfect unlearning accuracy (UA) and membership inference attack (MIA) efficacy while maintaining high remaining accuracy (RA) and testing accuracy (TA).

## Method Summary
PruneLoRA combines model pruning with Low-Rank Adaptation (LoRA) to achieve efficient and effective unlearning. The method first applies structured L2 pruning with 50% sparsity to targeted layers (convolutional layers for ResNet-50, linear and attention layers for ViT), then adds LoRA adapters to the pruned layers. The model is then fine-tuned on the remaining classes using Adam optimizer with learning rate 1e-3 and cross-entropy loss. The approach is evaluated against Fine-tuning, Pruning + Fine-tuning, and LoRA alone baselines on CIFAR-10 dataset using metrics including UA, MIA-Efficacy, RA, TA, RTE, and GPU memory requirements.

## Key Results
- PruneLoRA achieves near-perfect unlearning accuracy (UA) and membership inference attack (MIA) efficacy
- On ResNet-50, PruneLoRA outperforms other methods in remaining accuracy (RA) and testing accuracy (TA)
- On ViT, PruneLoRA significantly improves RA and TA compared to baselines, except LoRA, while achieving perfect UA
- PruneLoRA reduces computational cost and memory requirements, bridging the gap between exact and approximate unlearning approaches

## Why This Works (Mechanism)

### Mechanism 1
- Claim: PruneLoRA combines model pruning with LoRA to achieve efficient and effective unlearning.
- Mechanism: By pruning the model first to reduce parameter count, then applying LoRA for low-rank updates, the method targets a smaller parameter space for adaptation, lowering computational cost while preserving model performance.
- Core assumption: Low-rank updates via LoRA can sufficiently adjust the pruned model's parameters to unlearn the target class without catastrophic forgetting.
- Evidence anchors:
  - [abstract]: "PruneLoRA method combines model pruning with Low-Rank Adaptation (LoRA) to achieve efficient and effective unlearning."
  - [section]: "We leverage LoRA to selectively modify a subset of the pruned model's parameters, thereby reducing the computational cost, memory requirements and improving the model's ability to retain performance on the remaining classes."

### Mechanism 2
- Claim: Pruning before LoRA reduces the parameter space, making LoRA updates computationally cheaper and more focused.
- Mechanism: Structured L2 pruning with 0.5 sparsity removes 50% of parameters in targeted layers (convolutional in ResNet-50, linear/attention in ViT). LoRA then adapts only the remaining parameters, reducing both runtime and memory usage.
- Core assumption: Pruning does not eliminate critical information needed for the remaining classes, and LoRA can compensate for the reduced capacity.
- Evidence anchors:
  - [section]: "L2 Pruning to prune 50% of the specific layers in each model... After final finetuning... we evaluate the models..."
  - [section]: "LoRA was applied to these layers to enable efficient fine-tuning."

### Mechanism 3
- Claim: LoRA's low-rank adaptation mitigates catastrophic forgetting during unlearning.
- Mechanism: LoRA introduces regularization that limits parameter updates to a low-rank subspace, preventing large deviations from the original model and thus preserving performance on non-target classes.
- Core assumption: Low-rank updates are sufficient to unlearn the target class while maintaining generalization on remaining classes.
- Evidence anchors:
  - [section]: "(Biderman et al. 2024) shows that in the context of LLMs, LoRA provides a form of regularization that mitigates 'forgetting' of the source domain..."
  - [abstract]: "LoRA (Hu et al. 2022) reduces the need for large-scale parameter updates by applying low-rank updates to the model."

## Foundational Learning

- Concept: Model pruning and its impact on model capacity
  - Why needed here: Understanding how pruning affects model performance is crucial for interpreting PruneLoRA's design and results.
  - Quick check question: What happens to a model's accuracy if you prune 50% of its parameters without fine-tuning?

- Concept: Low-Rank Adaptation (LoRA) and low-rank matrix decomposition
  - Why needed here: LoRA's mechanism of updating a small subset of parameters via low-rank matrices is central to PruneLoRA's efficiency.
  - Quick check question: How does LoRA reduce the number of parameters that need to be updated during fine-tuning?

- Concept: Machine unlearning metrics (UA, MIA-Efficacy, RA, TA)
  - Why needed here: Evaluating PruneLoRA requires understanding these metrics and their significance in measuring unlearning effectiveness and retained performance.
  - Quick check question: Why is it important for an unlearning method to achieve high UA while also maintaining high RA and TA?

## Architecture Onboarding

- Component map:
  - Pre-trained model -> Structured L2 pruning (50% sparsity) -> LoRA adapters -> Fine-tuning on remaining classes -> Evaluation

- Critical path:
  1. Load pre-trained model and dataset
  2. Apply structured L2 pruning (50% sparsity) to targeted layers
  3. Add LoRA adapters to pruned layers
  4. Fine-tune on remaining classes for 5/10 epochs
  5. Evaluate unlearning and retained performance metrics

- Design tradeoffs:
  - Pruning level (sparsity): Higher sparsity reduces computational cost but may hurt retained performance
  - LoRA rank: Higher rank increases adaptation capacity but also computational cost
  - Fine-tuning epochs: More epochs may improve performance but increase runtime

- Failure signatures:
  - Low UA/MIA-Efficacy: LoRA updates insufficient to remove target class influence
  - Low RA/TA: Pruning removed critical features for retained classes
  - High RTE/GPU: Pruning/LoRA not effectively reducing computational cost

- First 3 experiments:
  1. Baseline: Prune the model (50% sparsity) and fine-tune without LoRA; measure RA, TA, UA, MIA-Efficacy
  2. LoRA only: Apply LoRA to full model and fine-tune on remaining classes; measure RA, TA, UA, MIA-Efficacy
  3. PruneLoRA: Apply pruning (50% sparsity) then LoRA, fine-tune, and measure all metrics for comparison

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does PruneLoRA perform on extremely large-scale models like GPT-4 or Llama 3 compared to exact unlearning methods?
- Basis in paper: [explicit] The paper states "A promising avenue for future research is the application of this method to Large Language Models (LLMs) and Vision-Language Models (VLMs)" and mentions computational constraints prevented exploring this direction.
- Why unresolved: The paper only evaluated on ResNet-50 and ViT on CIFAR-10, which are relatively small models. The authors explicitly note they were unable to explore the method on LLMs due to computational constraints.
- What evidence would resolve it: Experiments comparing PruneLoRA against exact unlearning (retraining) on LLMs or VLMs, measuring UA, RA, TA, RTE, and GPU memory requirements across different model sizes.

### Open Question 2
- Question: Does the effectiveness of LoRA in PruneLoRA vary significantly across different types of layers (convolutional, attention, linear) or is it consistently effective regardless of layer type?
- Basis in paper: [explicit] The authors note "future work could explore reducing the number of layers to which LoRA is applied" and observed that restricting LoRA to the last attention layer yielded the best results for ViT, suggesting layer-specific effectiveness.
- Why unresolved: The paper only tested LoRA on specific layer types (convolutional for ResNet-50, last attention layer for ViT) but did not systematically study the effectiveness across different layer types or configurations.
- What evidence would resolve it: Comparative experiments applying LoRA to different layer types and positions within the same model architecture, measuring the impact on UA, RA, TA, and computational efficiency.

### Open Question 3
- Question: How does PruneLoRA perform in continual learning scenarios where models must repeatedly unlearn and relearn information over time?
- Basis in paper: [inferred] The "Future Scope" section mentions "Models can also be studied under continual learning contexts, where models repeatedly learn and unlearn information over time" as a potential research direction.
- Why unresolved: The paper only evaluated single unlearning tasks and did not test the method's performance under repeated unlearning/relearning cycles or its ability to handle sequential unlearning tasks.
- What evidence would resolve it: Experiments where PruneLoRA is applied to models undergoing multiple unlearning/relearning cycles, measuring degradation in performance, effectiveness of unlearning, and computational overhead over time.

## Limitations

- Evaluation restricted to CIFAR-10 and two model architectures (ResNet-50 and ViT), limiting generalizability to larger-scale datasets and more diverse model types.
- Unlearning task simplified to removing one class at a time, whereas real-world scenarios may involve forgetting multiple classes or data points.
- Comparison with other unlearning methods limited to three baselines, and no ablation studies provided to isolate contributions of pruning versus LoRA.

## Confidence

- **High confidence**: PruneLoRA achieves near-perfect UA and MIA-Efficacy while maintaining high RA and TA on ResNet-50 and ViT (CIFAR-10).
- **Medium confidence**: PruneLoRA reduces computational cost and memory requirements compared to exact unlearning methods, bridging the gap between exact and approximate unlearning approaches.
- **Low confidence**: PruneLoRA's effectiveness generalizes to other datasets, model architectures, and unlearning scenarios (e.g., forgetting multiple classes).

## Next Checks

1. **Dataset and model generalization**: Evaluate PruneLoRA on larger-scale datasets (e.g., ImageNet) and diverse model architectures (e.g., CNNs, Transformers) to assess its generalizability beyond CIFAR-10 and ResNet-50/ViT.

2. **Multi-class unlearning**: Extend the evaluation to unlearning multiple classes simultaneously, which is a more realistic scenario, to test PruneLoRA's scalability and effectiveness in complex unlearning tasks.

3. **Comparative analysis with state-of-the-art**: Conduct a comprehensive comparison with recent advanced unlearning methods (e.g., TraceHiding, KD-LoRA) on the same datasets and models to benchmark PruneLoRA's performance and identify its relative strengths and weaknesses.