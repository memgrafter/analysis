---
ver: rpa2
title: 'Aviary: training language agents on challenging scientific tasks'
arxiv_id: '2412.21154'
source_url: https://arxiv.org/abs/2412.21154
tags:
- language
- arxiv
- agents
- agent
- sequence
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces Aviary, a gymnasium for training language
  agents on complex scientific tasks. The core method involves framing language agents
  as policies solving language-grounded partially observable Markov decision processes
  (LDPs), implemented as stochastic computation graphs.
---

# Aviary: training language agents on challenging scientific tasks

## Quick Facts
- arXiv ID: 2412.21154
- Source URL: https://arxiv.org/abs/2412.21154
- Authors: Siddharth Narayanan; James D. Braza; Ryan-Rhys Griffiths; Manu Ponnapati; Albert Bou; Jon Laurent; Ori Kabeli; Geemi Wellawatte; Sam Cox; Samuel G. Rodriques; Andrew D. White
- Reference count: 40
- One-line primary result: Language agents backed by open-source, non-frontier LLMs can match and exceed both frontier LLM agents and human experts on multiple scientific tasks at up to 100x lower inference cost.

## Executive Summary
This paper introduces Aviary, a gymnasium for training language agents on complex scientific tasks. The core innovation frames language agents as policies solving language-grounded partially observable Markov decision processes (LDPs), implemented as stochastic computation graphs. The authors develop five environments, including three scientific ones: DNA construct engineering, scientific literature question answering, and protein stability engineering. Through online training and inference-time scaling, they demonstrate that language agents backed by open-source, non-frontier LLMs can match and exceed the performance of frontier LLM agents and human experts on multiple tasks, achieving up to 100x lower inference cost.

## Method Summary
The method formalizes language agents as policies solving language-grounded partially observable Markov decision processes (LDPs), implemented as stochastic computation graphs. The approach involves creating five environments (GSM8K, HOTPOT QA, PaperQA, Molecular Cloning, Protein Stability) and training Llama-3.1-8B-Instruct agents using behavior cloning and expert iteration. The system leverages inference-time compute scaling through majority voting to improve performance. The training procedure combines online exploration with supervised learning from successful trajectories to iteratively improve agent performance on scientific tasks.

## Key Results
- Trained Llama-3.1-8B-Instruct agents achieved 0.89 accuracy on SeqQA and 0.89 accuracy on LitQA2 tasks
- Language agents matched or exceeded human performance and previously reported results while significantly reducing inference costs
- Majority voting provided approximately 20 percentage points of accuracy improvement on both environments
- Demonstrated up to 100x lower inference cost compared to frontier LLM agents

## Why This Works (Mechanism)

### Mechanism 1
Training language agents as policies solving language-grounded partially observable Markov decision processes (LDPs) enables iterative improvement through environment feedback. By formalizing agents as policies that interact with tools via natural language, the system can leverage reinforcement-style learning methods like expert iteration to improve over time. The core assumption is that the environment provides sufficient feedback signals (observations and rewards) to guide policy improvement. Break condition occurs if the environment's feedback is too sparse or ambiguous to guide learning.

### Mechanism 2
Stochastic computation graphs (SCGs) provide a unified framework for representing diverse language agent architectures. SCGs allow both deterministic (e.g., tool selection) and stochastic (e.g., LLM sampling) operations to be composed, enabling modular agent design and training. The core assumption is that language agent architectures can be expressed as directed acyclic graphs with appropriate node types. Break condition occurs if certain agent behaviors (e.g., complex reasoning chains) cannot be adequately represented in SCG form.

### Mechanism 3
Inference-time compute scaling through majority voting can significantly improve agent performance. Sampling multiple trajectories and selecting the consensus answer leverages the stochastic nature of LLMs to find correct solutions that individual samples might miss. The core assumption is that the correct answer appears with sufficient frequency across sampled trajectories. Break condition occurs if the distribution of answers across samples is too uniform or the correct answer is too rare.

## Foundational Learning

- **Concept**: Partially Observable Markov Decision Processes (POMDPs)
  - Why needed here: The LDP formalism extends POMDPs to language-grounded environments, providing the theoretical foundation for agent-environment interaction.
  - Quick check question: How does the partial observability aspect of POMDPs relate to the language-grounded observations in LDPs?

- **Concept**: Stochastic Computation Graphs
  - Why needed here: SCGs provide the mathematical framework for representing and optimizing the various components of language agents.
  - Quick check question: What distinguishes a deterministic node from a stochastic node in an SCG?

- **Concept**: Expert Iteration
  - Why needed here: EI enables the iterative improvement of language agents by combining online exploration with supervised learning from successful trajectories.
  - Quick check question: How does expert iteration differ from standard reinforcement learning approaches?

## Architecture Onboarding

- **Component map**: Environment implementations (GSM8K, HOTPOT QA, PaperQA, Molecular Cloning, Protein Stability) → Agent implementations (based on SCGs) → Training frameworks (behavior cloning, expert iteration) → Policy updates → Improved agent performance
- **Critical path**: Environment → Agent interaction → Feedback collection → Policy update → Improved agent performance
- **Design tradeoffs**: Open-source vs. frontier models (cost vs. capability), single vs. multiple trajectory sampling (speed vs. accuracy), training vs. inference optimization (upfront cost vs. per-task cost)
- **Failure signatures**: Low diversity in tool usage patterns, overfitting to training tasks, inability to generalize to novel problems, high variance in performance across samples
- **First 3 experiments**:
  1. Run a simple GSM8K task with a zero-shot Llama-3.1-8B agent to verify basic environment-agent interaction.
  2. Train a Llama-3.1-8B agent on GSM8K using behavior cloning to establish baseline performance.
  3. Apply majority voting to the trained agent on test tasks to measure inference-time scaling benefits.

## Open Questions the Paper Calls Out

### Open Question 1
How does the performance of language agents scale with the size of the environment state space, particularly for tasks requiring extensive tool usage and memory? The paper demonstrates that language agents can exceed human performance on certain scientific tasks, but the environments used have finite state spaces and limited tool sets. This question is unresolved because the paper does not explore the performance of language agents in environments with large and complex state spaces, which are common in real-world scientific tasks. Experiments comparing the performance of language agents on tasks with varying state space sizes and tool complexity would resolve this question.

### Open Question 2
Can language agents effectively generalize to unseen scientific tasks within the same domain, or do they require task-specific training for each new problem? The paper shows that language agents trained on specific scientific tasks (SeqQA, LitQA2) can match or exceed performance on those tasks, but does not investigate generalization to other tasks within the same domain. This question is unresolved because the paper focuses on demonstrating the capabilities of language agents on specific benchmarks, but does not explore their ability to adapt to new and unseen tasks within the same scientific domain. Experiments testing the performance of language agents on a diverse set of scientific tasks within the same domain, both seen and unseen during training, would assess their generalization capabilities.

### Open Question 3
What is the impact of different inference-time compute scaling strategies (e.g., majority voting, oracle verification) on the performance of language agents, and how do these strategies interact with the complexity of the task and the size of the model? The paper demonstrates the effectiveness of majority voting in improving the performance of language agents, but does not extensively explore the impact of different inference-time scaling strategies or their interaction with task complexity and model size. This question is unresolved because the paper focuses on a specific inference-time scaling strategy (majority voting) and does not systematically compare it to other strategies or investigate how these strategies interact with different task complexities and model sizes. Experiments comparing the performance of language agents using different inference-time compute scaling strategies on tasks with varying complexities and using models of different sizes would understand the optimal strategies for different scenarios.

## Limitations
- Theoretical framework generalizability to diverse scientific domains remains to be extensively validated
- Training data dependency may limit effectiveness on highly complex scientific tasks where successful solutions are rare
- Inference-time scaling practicality may be limited by computational cost and latency implications

## Confidence
- **High Confidence**: The core technical contributions (LDP formalism, SCG implementation, expert iteration methodology) are well-founded and demonstrated across multiple environments
- **Medium Confidence**: The 100x cost reduction claim relative to frontier models is supported by experimental results but may depend heavily on specific implementation details and task characteristics
- **Low Confidence**: The generalizability of these approaches to entirely new scientific domains and the long-term stability of trained agents when deployed in dynamic research environments remain uncertain

## Next Checks
1. Evaluate trained agents on scientific tasks from domains not represented in the training environments to assess generalization capabilities beyond the five environments presented
2. Systematically vary the amount of training data and expert trajectories to determine the minimum requirements for achieving competitive performance, particularly for the scientific tasks
3. Test agent performance when environmental parameters or available tools change over time, simulating real-world research scenarios where tools and data sources evolve