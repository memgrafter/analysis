---
ver: rpa2
title: 'ECoh: Turn-level Coherence Evaluation for Multilingual Dialogues'
arxiv_id: '2407.11660'
source_url: https://arxiv.org/abs/2407.11660
tags:
- response
- responses
- dialogue
- evaluation
- coherence
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents GENRESCOH, a large-scale dataset of over 130k
  multilingual dialogue responses targeting coherence evaluation. The dataset is generated
  using GPT-3.5-Turbo and GPT-4 from XDailyDialog and XPersona sources across English,
  French, German, Italian, and Chinese.
---

# ECoh: Turn-level Coherence Evaluation for Multilingual Dialogues

## Quick Facts
- **arXiv ID**: 2407.11660
- **Source URL**: https://arxiv.org/abs/2407.11660
- **Reference count**: 24
- **Primary result**: ECoh-0.5B-ML achieves 0.945 F1 score, outperforming teacher model GPT-3.5-Turbo at 0.910 F1

## Executive Summary
This paper introduces ECoh, a family of lightweight multilingual dialogue coherence evaluators trained on GENRESCOH, a large-scale dataset of over 130k synthetic dialogue responses across five languages. The approach leverages instruction-tuned QWEN models fine-tuned for binary coherence classification with explanation generation. ECoh demonstrates strong performance, with the smallest 0.5B model outperforming the teacher model GPT-3.5-Turbo while providing higher-quality explanations. The system generalizes well across languages and unseen datasets, addressing the challenge of evaluating dialogue coherence in multilingual contexts.

## Method Summary
The authors generate GENRESCOH using GPT-3.5-Turbo and GPT-4 to create synthetic coherent and incoherent dialogue responses from XDailyDialog and XPersona datasets in English, French, German, Italian, and Chinese. They then fine-tune QWEN 1.5-Chat models (0.5B, 1.8B, 4B parameters) using LoRA with specific hyperparameters (r=8, α=32, dropout=0.1) for binary coherence classification with explanation generation. The models are evaluated using F1 score, Point Biserial Correlation, and GPT-4-based explanation quality assessment, demonstrating that smaller models can achieve competitive performance while providing better explanations than larger baselines.

## Key Results
- ECoh-0.5B-ML achieves 0.945 F1 score, outperforming teacher model GPT-3.5-Turbo (0.910 F1)
- ECoh provides higher-quality explanations than QWEN 1.5-7B-Chat, scoring over 4/5 on average
- 4B multilingual models maintain strong performance across all tested languages
- ECoh generalizes well to unseen datasets (FED-turn, XPersona) while maintaining language coverage

## Why This Works (Mechanism)
The approach succeeds by combining synthetic data generation with model distillation, where smaller QWEN models learn from larger teacher models' outputs on carefully crafted synthetic examples. The LoRA fine-tuning technique allows efficient adaptation of pretrained models to the coherence task while preserving the base model's capabilities. The multilingual training across five languages enables the models to capture cross-lingual coherence patterns, and the explanation generation component provides transparency in predictions, which is critical for dialogue system evaluation.

## Foundational Learning
- **Synthetic Data Generation**: Creating large-scale training data using GPT models - needed because real dialogue coherence annotations are expensive and limited; quick check: verify synthetic data quality through human evaluation
- **LoRA Fine-tuning**: Parameter-efficient adaptation of pretrained models - needed to efficiently train smaller models on specific tasks; quick check: monitor training loss and validation metrics
- **Multilingual Training**: Joint training across multiple languages - needed to enable cross-lingual generalization; quick check: evaluate per-language performance to identify gaps
- **Explanation Generation**: Producing human-readable justifications for predictions - needed for transparency and practical deployment; quick check: measure explanation coherence using BLEU and human evaluation
- **Binary Classification**: Distinguishing coherent from incoherent responses - needed as the core evaluation task; quick check: examine confusion matrices and per-class metrics
- **Teacher-Student Distillation**: Smaller models learning from larger teacher outputs - needed to achieve strong performance with compact models; quick check: compare student and teacher performance on held-out data

## Architecture Onboarding

**Component Map**: GPT-4 (prompt template) -> GENRESCOH dataset -> QWEN 1.5-Chat models -> LoRA fine-tuning -> ECoh evaluators

**Critical Path**: Synthetic data generation → Model fine-tuning → Evaluation → Deployment

**Design Tradeoffs**: The approach trades synthetic data quality for scale, using GPT models to generate 130k+ examples rather than relying on expensive human annotations. This enables multilingual coverage but may introduce generation biases. The choice of small QWEN models (0.5B-4B) prioritizes efficiency and deployment practicality over maximum possible performance, achieving competitive results through effective fine-tuning.

**Failure Signatures**: Poor coherence detection suggests issues with dataset quality or insufficient training diversity. Incoherent explanations indicate problems with the instruction-tuning process or inadequate explanation-focused training. Performance degradation on unseen languages suggests the multilingual training wasn't comprehensive enough. Overfitting manifests as strong GENRESCOH performance but weak external dataset results.

**First Experiments**:
1. Fine-tune a 0.5B QWEN model on GENRESCOH and evaluate F1 score on the validation split
2. Generate explanations for a sample of predictions and compute BLEU score against GPT-4 references
3. Test the fine-tuned model on FED-turn dataset to assess cross-dataset generalization

## Open Questions the Paper Calls Out
None

## Limitations
- Limited external validation across diverse dialogue domains and languages
- Quality metrics rely on GPT-4 judgments which may introduce bias
- Dataset generation inherits potential biases and limitations from GPT-3.5-Turbo and GPT-4
- No analysis of confidence score calibration for practical deployment reliability
- Scalability to larger models and more diverse dialogue contexts remains unexplored

## Confidence
- ECoh model performance on GENRESCOH: High
- Generalization to external datasets: Medium
- Explanation quality claims: Medium
- Dataset generation methodology: Medium

## Next Checks
1. Evaluate ECoh on additional multilingual dialogue datasets beyond FED-turn and XPersona to test true cross-domain generalization
2. Conduct human evaluation of explanation quality across multiple languages to validate GPT-4-based assessments
3. Test model calibration by examining confidence scores on edge cases and analyzing calibration plots for reliability in practical applications