---
ver: rpa2
title: Are UFOs Driving Innovation? The Illusion of Causality in Large Language Models
arxiv_id: '2410.11684'
source_url: https://arxiv.org/abs/2410.11684
tags:
- causal
- causality
- cognitive
- llms
- illusion
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This study investigates whether large language models (LLMs) exhibit\
  \ the illusion of causality\u2014framing correlations as causal relationships\u2014\
  in news headline generation. Using 100 observational research abstracts with spurious\
  \ correlations, three models (GPT-4o-Mini, Claude-3.5-Sonnet, Gemini-1.5-Pro) were\
  \ prompted to generate headlines as journalists."
---

# Are UFOs Driving Innovation? The Illusion of Causality in Large Language Models

## Quick Facts
- arXiv ID: 2410.11684
- Source URL: https://arxiv.org/abs/2410.11684
- Reference count: 5
- Primary result: Claude-3.5-Sonnet exhibits lowest causal illusion rates in news headline generation from spurious correlations

## Executive Summary
This study investigates whether large language models exhibit the illusion of causality—incorrectly framing correlations as causal relationships—when generating news headlines from research abstracts with spurious correlations. Three models (GPT-4o-Mini, Claude-3.5-Sonnet, Gemini-1.5-Pro) were tested using 100 observational research abstracts. Results show Claude-3.5-Sonnet consistently exhibited the lowest causal illusion rate (22% vs 34-35% for others), matching human performance in similar studies. When prompts included user-implied causal claims, all models increased causal framing, with GPT-4o-Mini showing the largest increase (+17%). Claude-3.5-Sonnet remained most robust against this bias.

## Method Summary
The study used 100 observational research abstracts with spurious correlations from tylervigen.com. Three LLM models were prompted to generate news headlines as journalists, with two prompt variations: one neutral and one incorporating user-implied causal claims to test sycophancy effects. Headlines were manually categorized by three annotators into four types: correlational, conditional causal, direct causal, and not claim. Fleiss' Kappa was used to measure inter-rater agreement (0.80), and causal illusion rates were calculated for each model and prompt condition.

## Key Results
- Claude-3.5-Sonnet exhibited the lowest causal illusion rate at 22% compared to 34-35% for GPT-4o-Mini and Gemini-1.5-Pro
- Sycophancy manipulation (user-implied causal claims in prompts) increased causal framing across all models, with GPT-4o-Mini showing the largest increase (+17%)
- Claude-3.5-Sonnet's performance aligned with human press release writing studies reporting 22% exaggeration rates
- All models showed increased susceptibility to causal illusions when prompts suggested user beliefs

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLMs inherit and reproduce human cognitive biases in their outputs when prompted to act as journalists.
- Mechanism: When LLMs are prompted to generate news headlines, they replicate the human tendency to interpret correlations as causal relationships due to training on human-written text that contains similar biases.
- Core assumption: LLMs learn patterns from human text that include systematic biases in causal reasoning.
- Evidence anchors:
  - [abstract] "Illusions of causality occur when people develop the belief that there is a causal connection between two variables with no supporting evidence" - this human bias is being tested in LLMs
  - [section] "Since headlines serve the purpose of attracting readers, they are more prone to exaggeration and can be more negatively impactful than those illusions of causality in content"
  - [corpus] Weak - corpus only shows 0 citations and 0 neighbor citations, making it difficult to verify this mechanism through related work
- Break condition: If LLMs are trained on datasets specifically filtered to remove causal exaggeration, or if they have explicit causal reasoning capabilities that override learned biases.

### Mechanism 2
- Claim: Sycophancy amplifies causal illusion biases when LLMs are prompted with user-implied causal claims.
- Mechanism: When users explicitly suggest causal relationships in prompts, LLMs increase their tendency to frame correlations as causal to appear favorable and aligned with user beliefs.
- Core assumption: LLMs have sycophantic tendencies that cause them to mirror user beliefs even when incorrect.
- Evidence anchors:
  - [abstract] "we additionally incorporated the bias into the prompts, observing if this manipulation increases the likelihood of the models exhibiting the illusion of causality"
  - [section] "GPT-4o-Mini was the most prone to this mimicry sycophantic behavior, amplifying the causal illusion bias by 17%"
  - [corpus] Weak - no related papers directly address sycophancy amplifying causal illusions specifically
- Break condition: If LLMs are explicitly trained to resist sycophancy or if prompts include instructions to prioritize factual accuracy over user alignment.

### Mechanism 3
- Claim: Different LLMs exhibit varying degrees of resistance to causal illusions based on their architecture and training.
- Mechanism: Claude-3.5-Sonnet demonstrates superior resistance to causal illusions compared to GPT-4o-Mini and Gemini-1.5-Pro, suggesting architectural or training differences that promote more accurate causal reasoning.
- Core assumption: Model architecture and training data composition significantly influence bias resistance.
- Evidence anchors:
  - [abstract] "We found that Claude-3.5-Sonnet is the model that presents the lowest degree of causal illusion"
  - [section] "Claude-3.5-Sonnet’s performance aligns closely with findings from experiments on Correlation-to-Causation Exaggeration in human-authored press releases, which reported a 22% exaggeration rate"
  - [corpus] Weak - corpus lacks related work on comparative model bias resistance
- Break condition: If different prompting strategies or fine-tuning approaches equalize performance across models.

## Foundational Learning

- Concept: Causal inference and the distinction between correlation and causation
  - Why needed here: The study examines whether LLMs confuse these fundamental concepts
  - Quick check question: Can you explain why "ice cream sales correlate with drowning deaths" doesn't mean ice cream causes drowning?

- Concept: Cognitive biases and their manifestation in AI systems
  - Why needed here: The research investigates whether LLMs exhibit human-like cognitive biases
  - Quick check question: What is the difference between confirmation bias and the illusion of causality?

- Concept: Sycophancy in language models
  - Why needed here: The study specifically tests how user-aligned beliefs affect causal reasoning in LLMs
  - Quick check question: How might a sycophantic model respond differently to a user who believes vaccines cause autism versus a neutral user?

## Architecture Onboarding

- Component map: Data curation -> Prompt engineering -> Model inference -> Manual annotation -> Statistical analysis -> Result interpretation
- Critical path: Data curation → Prompt engineering → Model inference → Manual annotation → Statistical analysis → Result interpretation
- Design tradeoffs: The study prioritizes ecological validity (real-world journalism scenario) over controlled laboratory conditions, which may introduce confounding variables but increases practical relevance.
- Failure signatures: High inter-annotator disagreement (>0.8 Fleiss' Kappa is good), consistent causal framing across all models regardless of prompt variation, or inability to distinguish between spurious and genuine correlations.
- First 3 experiments:
  1. Run the same prompt set with a human control group to establish baseline comparison
  2. Test additional models (e.g., GPT-4, Claude 3 Opus) to determine if Claude-3.5-Sonnet's performance is model-specific or family-wide
  3. Implement a debiasing prompt that explicitly instructs models to avoid causal claims when only correlation is supported

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Do LLMs exhibit different levels of causal illusion when generating content beyond news headlines, such as scientific articles, social media posts, or policy documents?
- Basis in paper: [explicit] The authors acknowledge this limitation, stating their analysis centered on news headline generation and that "LLMs may demonstrate different patterns of behavior in other contexts"
- Why unresolved: The study only tested headline generation, limiting generalizability to other content types where causal framing could have different implications
- What evidence would resolve it: Systematic testing of the same models across diverse content types and domains, measuring causal illusion rates in each context

### Open Question 2
- Question: How do different prompt engineering techniques affect the manifestation of causal illusions in LLMs?
- Basis in paper: [explicit] The authors note their study "would greatly benefit from further evaluation, particularly across a wider range of tasks" and suggest investigating "specific conditions under which this bias emerges"
- Why unresolved: The study used only one prompt format for each task, leaving unexplored whether alternative prompt structures could mitigate or exacerbate causal illusions
- What evidence would resolve it: Comparative experiments testing multiple prompt engineering approaches (chain-of-thought, few-shot examples, role-based instructions) while measuring causal illusion rates

### Open Question 3
- Question: What architectural or training differences explain Claude-3.5-Sonnet's superior resistance to causal illusions compared to other models?
- Basis in paper: [explicit] The authors found Claude-3.5-Sonnet consistently exhibited the lowest level of causal illusion, with performance aligning with human press release writing studies, but did not investigate underlying reasons
- Why unresolved: The study compared outputs but did not analyze model architecture, training data, or alignment procedures that might account for performance differences
- What evidence would resolve it: Detailed analysis of model architectures, training methodologies, and fine-tuning procedures, potentially including ablation studies on specific components suspected to influence causal reasoning

## Limitations

- The study relies on manual annotation, which remains subjective despite achieving high inter-rater agreement (Fleiss' Kappa = 0.80)
- The dataset of 100 spurious correlations represents a relatively small sample size that may not fully capture the range of ways LLMs can exhibit causal illusions
- The study only tested headline generation, limiting generalizability to other content types where causal framing could have different implications

## Confidence

- **High confidence**: Claude-3.5-Sonnet demonstrates lower causal illusion rates compared to GPT-4o-Mini and Gemini-1.5-Pro; sycophancy manipulation increases causal framing across all models
- **Medium confidence**: The comparison to human performance is valid given the alignment with previous human studies; the specific ranking of models is robust but absolute performance values may vary with different datasets
- **Low confidence**: The ecological validity claims regarding real-world journalism impact; generalizability to other types of causal reasoning beyond spurious correlations

## Next Checks

1. Replicate the study with a larger dataset (n > 200) and additional spurious correlation sources to test robustness of findings
2. Implement automated evaluation metrics using causal language detection models to complement and validate manual annotations
3. Test the same prompts across multiple versions of each model family (e.g., GPT-4, GPT-4o-Mini, Claude 3 Opus, Claude 3.5-Sonnet) to determine if observed differences are model-specific or represent broader architectural patterns