---
ver: rpa2
title: Towards Pareto Optimal Throughput in Small Language Model Serving
arxiv_id: '2404.03353'
source_url: https://arxiv.org/abs/2404.03353
tags:
- serving
- throughput
- memory
- batch
- size
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper explores serving performance of small language models
  (SLMs) through benchmarking experiments that analyze throughput and latency trade-offs
  under different batch sizes. It demonstrates that for small models like OPT-125M
  and OPT-1.3B, the Pareto-optimal throughput is reached within the memory capacity
  of a single GPU, beyond which further batch size increases provide negligible gains.
---

# Towards Pareto Optimal Throughput in Small Language Model Serving

## Quick Facts
- arXiv ID: 2404.03353
- Source URL: https://arxiv.org/abs/2404.03353
- Reference count: 37
- Primary result: Small language models can reach Pareto-optimal throughput within single GPU memory capacity, with model replication improving resource utilization

## Executive Summary
This paper explores serving performance of small language models (SLMs) through benchmarking experiments that analyze throughput and latency trade-offs under different batch sizes. It demonstrates that for small models like OPT-125M and OPT-1.3B, the Pareto-optimal throughput is reached within the memory capacity of a single GPU, beyond which further batch size increases provide negligible gains. System-level analysis using NVIDIA DCGM metrics reveals underutilization of GPU resources, particularly in SM occupancy and activity. The paper also shows that model replication on a single device can significantly improve both throughput and GPU utilization, especially for smaller models. These findings suggest new optimization opportunities for SLM serving, such as strategic GPU resource allocation and model replication strategies.

## Method Summary
The paper benchmarks inference performance of OPT models (125M to 13B parameters) using vLLM with PagedAttention on 1-4 NVIDIA A100 GPUs (40GB each). Experiments use synthetic workloads from the ShareGPT dataset with 500 requests of 512 input tokens and 256 output tokens. The authors systematically increase batch sizes from 1 to maximum possible, measuring throughput, latency, and GPU utilization metrics (SM active, SM occupancy, power usage) using NVIDIA DCGM. They also explore model replication by running multiple instances of the same or different models on a single GPU to evaluate resource utilization improvements.

## Key Results
- Pareto-optimal throughput for small models (OPT-125M, OPT-1.3B) is reached within single GPU memory capacity
- GPU underutilization observed with large batch sizes, particularly in SM occupancy and activity metrics
- Model replication on single device significantly improves throughput and GPU utilization for smaller models
- Optimal batch size varies dramatically by model size: 125M reaches optimum at 64-128 batch size while 13B requires 4096-8192

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Small Language Models (SLMs) can reach Pareto-optimal throughput within a single GPU's memory capacity due to their smaller memory footprint.
- Mechanism: SLMs require significantly less memory than LLMs, allowing them to be batched to the point where memory-IO time overlaps compute time, achieving optimal throughput without needing distributed inference.
- Core assumption: The KV cache size for SLMs is small enough that the maximum beneficial batch size fits within the available GPU memory.
- Evidence anchors:
  - [abstract] "the small memory footprint of SLMs allows for reaching the Pareto-optimal throughput within the resource capacity of a single accelerator"
  - [section] "We observe that the Pareto-optimal throughput with small models is reached within the resource capacity of a single accelerator"
  - [corpus] Weak evidence - related papers focus on compression and efficiency but don't directly validate the single-GPU Pareto optimality claim
- Break Condition: If SLM KV cache requirements grow (e.g., through longer sequences or larger models) such that the optimal batch size exceeds single GPU memory capacity

### Mechanism 2
- Claim: Model replication on a single device can improve both throughput and GPU utilization for SLMs.
- Mechanism: By partitioning GPU memory among multiple instances of the same or different SLMs, replication reduces underutilization observed with large batch sizes on single models, as multiple models can utilize idle SMs and increase overall throughput.
- Core assumption: The GPU has sufficient idle resources (SMs, memory) that can be effectively partitioned among model replicas without significant overhead.
- Evidence anchors:
  - [abstract] "we present an initial set of findings demonstrating how model replication can effectively improve resource utilization for serving SLMs"
  - [section] "Figure 5 shows an improvement in both the throughput and the latency when running multiple instances simultaneously"
  - [corpus] No direct corpus evidence - replication as a serving optimization is not covered in related papers
- Break Condition: If the overhead of managing multiple instances exceeds the benefits, or if memory partitioning leads to suboptimal batch sizes per replica

### Mechanism 3
- Claim: Large batch sizes in SLM inference shift the performance bottleneck from memory-IO bound to compute-bound, creating a throughput frontier.
- Mechanism: As batch size increases, the arithmetic intensity increases, reducing the relative importance of memory transfers. Eventually, compute time dominates, preventing further throughput gains from larger batches.
- Core assumption: The relationship between batch size and arithmetic intensity follows the theoretical model where compute time eventually exceeds memory transfer time.
- Evidence anchors:
  - [abstract] "With large batches compute time grows to be larger the memory-IO time, reaching a Pareto-optimal throughput frontier"
  - [section] "However, as we increase the inner size of the matrix-matrix operations with larger batches, compute might become important"
  - [corpus] Weak evidence - while related papers discuss efficiency and scaling, they don't specifically address the compute/memory-IO trade-off in the context of batch size optimization
- Break Condition: If the model architecture or serving system optimizations (e.g., quantization, sparsity) significantly alter the arithmetic intensity relationship

## Foundational Learning

- Concept: KV Cache and PagedAttention
  - Why needed here: Understanding how intermediate attention results are stored and managed is crucial for comprehending memory requirements and fragmentation issues in SLM serving
  - Quick check question: How does PagedAttention reduce memory fragmentation compared to naive KV cache management?

- Concept: Batching Techniques (Dynamic vs Continuous Batching)
  - Why needed here: Different batching strategies affect how requests are grouped and processed, impacting both throughput and latency in SLM serving scenarios
  - Quick check question: What is the key difference between dynamic batching and continuous batching in terms of request processing granularity?

- Concept: GPU Performance Metrics (SM Activity, Occupancy, Utilization)
  - Why needed here: These metrics provide insight into how well the GPU resources are being utilized during SLM inference, helping identify bottlenecks and optimization opportunities
  - Quick check question: What is the difference between SM occupancy and SM activity, and why is this distinction important for understanding GPU utilization?

## Architecture Onboarding

- Component map:
  Request Interface -> vLLM Engine -> GPU Memory -> CUDA Kernels -> NVIDIA DCGM

- Critical path:
  1. Tokenize input and send HTTP request
  2. vLLM schedules request into batch
  3. Model weights loaded from HBM to on-chip memory
  4. Attention computations performed using KV cache
  5. Results generated and returned to client

- Design tradeoffs:
  - Batch size vs. latency: Larger batches improve throughput but increase latency
  - Memory allocation: More memory per model allows larger batches but reduces replication opportunities
  - Model replication vs. single instance: Replication improves utilization but adds management overhead

- Failure signatures:
  - GPU utilization remains low despite high batch sizes: Indicates underutilization, potential for replication
  - Throughput plateaus while latency continues to increase: Suggests reaching Pareto-optimal frontier
  - Memory allocation errors: Batch size exceeds available GPU memory

- First 3 experiments:
  1. Benchmark baseline throughput and latency for a single SLM with varying batch sizes to identify Pareto-optimal point
  2. Implement model replication with two instances of the same SLM and compare throughput/utilization to baseline
  3. Test heterogeneous SLM serving by running two different small models simultaneously and measuring overall performance

## Open Questions the Paper Calls Out

1. **Optimal batch size across GPU architectures:** The paper only tests on A100 GPUs and doesn't explore how different GPU architectures (H100, L4, etc.) or memory configurations (16GB vs 40GB) affect the optimal batch size and throughput frontier.

2. **Model replication strategies:** The paper uses basic process-level replication without leveraging GPU-specific multi-tenancy features like MPS or MIG that could provide better resource isolation and scheduling.

3. **Heterogeneous request patterns:** All experiments use fixed-length requests (512 input, 256 output), leaving open questions about how variable request lengths affect batch size optimization and model replication effectiveness under realistic workloads.

## Limitations

- Experiments limited to synthetic workloads with fixed input/output token lengths
- Only tested OPT model family, may not generalize to other SLM architectures
- GPU utilization analysis relies on NVIDIA DCGM metrics that may not capture all system-level bottlenecks

## Confidence

**High Confidence Claims:**
- SLMs can reach Pareto-optimal throughput within single GPU memory capacity
- Model replication improves both throughput and GPU utilization for SLMs
- Large batch sizes shift the performance bottleneck from memory-IO to compute-bound

**Medium Confidence Claims:**
- The specific batch size thresholds for reaching Pareto-optimal throughput
- The exact magnitude of improvement from model replication strategies
- The generalizability of findings to other SLM architectures beyond OPT

**Low Confidence Claims:**
- The optimal strategy for heterogeneous SLM serving with different model sizes
- The long-term stability of observed GPU utilization improvements under sustained load
- The applicability of findings to other GPU architectures beyond NVIDIA A100

## Next Checks

1. **Cross-Model Generalization Test:** Repeat the benchmarking experiments with a diverse set of SLM architectures (e.g., LLaMA, Phi, Mistral) to validate whether the Pareto-optimal throughput findings hold across different model designs and tokenization strategies.

2. **Real-World Workload Validation:** Replace the synthetic ShareGPT workload with production traffic patterns from actual serving systems, including varying sequence lengths and batch arrival rates, to assess how the findings translate to operational environments.

3. **Multi-Instance Heterogeneity Analysis:** Conduct systematic experiments with heterogeneous SLM serving by running combinations of different model sizes simultaneously, measuring both individual model performance and overall system throughput to identify optimal resource allocation strategies.