---
ver: rpa2
title: Evaluating the Robustness of Analogical Reasoning in Large Language Models
arxiv_id: '2411.14215'
source_url: https://arxiv.org/abs/2411.14215
tags:
- problems
- humans
- story
- gpt-3
- human
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigates the robustness of analogical reasoning
  in large language models (LLMs) by testing their performance on variants of standard
  analogy tasks. The researchers evaluated GPT models on letter-string analogies,
  digit matrices, and story analogies, using variants designed to test the same abstract
  reasoning abilities but with content unlikely to appear in training data.
---

# Evaluating the Robustness of Analogical Reasoning in Large Language Models

## Quick Facts
- arXiv ID: 2411.14215
- Source URL: https://arxiv.org/abs/2411.14215
- Authors: Martha Lewis; Melanie Mitchell
- Reference count: 40
- Key outcome: LLMs show significant performance drops on analogy task variants while humans maintain consistent accuracy, indicating LLMs lack robust abstract reasoning despite successes on standard benchmarks

## Executive Summary
This study investigates whether large language models truly possess robust analogical reasoning capabilities by testing their performance on standard analogy tasks and carefully designed variants. The researchers evaluated GPT-3, GPT-3.5, and GPT-4 on letter-string analogies, digit matrices, and story analogies using both original problems and variants that test the same abstract reasoning abilities but with content unlikely to appear in training data. While human participants maintained high performance across all variants, GPT models showed dramatic declines, particularly on tasks involving fictional alphabets and alternate blank positions. The study also found that unlike humans, GPT models exhibited answer-order effects and sensitivity to paraphrasing on story analogies, suggesting they rely on superficial textual features rather than abstract structural reasoning.

## Method Summary
The study evaluated GPT models (GPT-3, GPT-3.5, GPT-4) on three types of analogy problems: letter-string analogies, digit matrices, and story analogies. For each domain, researchers created original tasks and variants designed to test the same abstract reasoning abilities while reducing surface similarity to training data. Variants included problems using fictional alphabets for letter-strings, alternate blank positions for digit matrices, and paraphrased versions of stories. Both human participants and GPT models completed these tasks using zero-shot prompting, and performance was compared between original tasks and variants to assess robustness of analogical reasoning.

## Key Results
- GPT models showed sharp performance declines on letter-string analogies using fictional alphabets while human performance remained high
- On digit matrices, GPT accuracy dropped significantly when blank positions were altered, while humans maintained consistent performance
- GPT models exhibited answer-order effects and sensitivity to paraphrasing on story analogies, unlike human participants

## Why This Works (Mechanism)

### Mechanism 1
- Claim: GPT models rely on statistical similarity to training data rather than robust abstract reasoning when solving analogy tasks.
- Mechanism: When presented with a task that is structurally similar to examples seen during training, GPT models retrieve and apply patterns that appear in their pre-training corpus. If the task is altered in a way that reduces surface similarity (e.g., using fictional alphabets or altering blank positions), the models fail to apply the same abstract reasoning, causing performance to drop sharply.
- Core assumption: GPT models learn patterns by encoding surface-level similarities rather than deep structural rules that generalize across variations.
- Evidence anchors:
  - [abstract]: "LLMs often lack the robustness of zero-shot human analogy-making, exhibiting brittleness on most of the variations we tested."
  - [section]: "For letter-string analogies, we find that while the performance of humans remains high for two types of variants we tested, the GPT models' performance declines sharply."
  - [corpus]: Found 25 related papers (using 8). Average neighbor FMR=0.522, average citations=0.0. (Weak corpus support for robustness evaluation studies.)
- Break condition: If GPT models could generate correct answers on all variants using only the abstract rules of the task, this mechanism would be invalidated.

### Mechanism 2
- Claim: Humans maintain consistent performance across task variants because they use abstract reasoning that is invariant to surface-level changes.
- Mechanism: Humans identify the underlying relational structure of an analogy problem and apply the same reasoning process regardless of superficial differences in the stimuli (e.g., letter order, symbol type, or blank position). This abstraction allows them to solve novel variants with the same accuracy as the original task.
- Core assumption: Human analogical reasoning is based on extracting and applying relational structures rather than memorizing or retrieving specific examples.
- Evidence anchors:
  - [abstract]: "On simple letter-string analogies, we find that while the performance of humans remains high for two types of variants we tested, the GPT models' performance declines sharply."
  - [section]: "For digit-matrix problems, we find a similar pattern but only on one out of the two types of variants we tested. On story-analogy problems, we find that, unlike humans, the performance of GPT models are susceptible to answer-order effects..."
  - [corpus]: Weak corpus support for robustness evaluation studies.
- Break condition: If human performance dropped significantly on variants, this mechanism would be invalidated.

### Mechanism 3
- Claim: GPT models are sensitive to superficial textual features (like answer ordering and paraphrasing) rather than the underlying analogical structure in story-based tasks.
- Mechanism: When solving story analogies, GPT models appear to rely on surface-level textual features such as sentence structure similarity and answer position rather than extracting the causal structure that defines the analogy. This leads to answer-order effects and sensitivity to paraphrasing that humans do not exhibit.
- Core assumption: GPT models prioritize textual similarity over structural abstraction when evaluating story analogies.
- Evidence anchors:
  - [abstract]: "On story-based analogy problems, we find that, unlike humans, the performance of GPT models are susceptible to answer-order effects, and that GPT models also may be more sensitive than humans to paraphrasing."
  - [section]: "We find that while humans are not affected by answer order, GPT-4 seems substantially biased by order."
  - [corpus]: Weak corpus support for robustness evaluation studies.
- Break condition: If GPT models performed equally well regardless of answer order or paraphrasing, this mechanism would be invalidated.

## Foundational Learning

- Concept: Abstract vs. concrete reasoning
  - Why needed here: Understanding the difference between surface-level pattern matching and deep structural reasoning is crucial for interpreting why GPT models fail on task variants.
  - Quick check question: If a GPT model solves a digit matrix problem with numbers 1-9 but fails when the same problem uses symbols @, #, $, what type of reasoning is it using?
- Concept: Robustness in AI evaluation
  - Why needed here: Evaluating models only on original tasks can overestimate their capabilities; robustness testing reveals whether performance generalizes to variations.
  - Quick check question: If a model scores 90% on standard tests but 40% on variants requiring the same reasoning, what does this tell us about its actual capability?
- Concept: Counterfactual comprehension checks
  - Why needed here: These checks ensure models understand task instructions and can apply basic operations in the given context before testing more complex reasoning.
  - Quick check question: Why is it important to verify that a model can identify "successor" and "predecessor" in a fictional alphabet before testing it on analogy problems?

## Architecture Onboarding

- Component map: Problem generation -> Human/GPT evaluation -> Data collection -> Statistical analysis -> Comparison of original vs. variant performance
- Critical path: Problem generation → Human/GPT evaluation → Data collection → Statistical analysis → Comparison of original vs. variant performance
- Design tradeoffs: Using zero-shot prompts tests pure reasoning ability but may underestimate capabilities that could be unlocked with better prompting; testing variants reveals robustness but may introduce confounds if not carefully designed.
- Failure signatures: Large performance drops on variants indicate reliance on surface similarity; answer-order effects indicate sensitivity to superficial features; consistent performance across variants indicates robust abstract reasoning.
- First 3 experiments:
  1. Run human and GPT evaluation on original letter-string problems to establish baseline performance
  2. Generate and evaluate variants with permuted alphabets to test robustness to surface changes
  3. Run counterfactual comprehension checks to verify GPT models understand basic operations in fictional alphabets before proceeding to more complex tests

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can we effectively test the robustness of abstract reasoning in LLMs without relying on variations that may introduce confounding factors like counting abilities or numerical pattern recognition?
- Basis in paper: [explicit] The paper discusses how WHL claimed that poor performance on variant problems could be explained by the models' inability to count, rather than a lack of analogical reasoning abilities.
- Why unresolved: The paper shows that while counting may play a role in some cases, it doesn't fully explain the performance differences. More research is needed to isolate abstract reasoning from other cognitive abilities.
- What evidence would resolve it: Developing and testing variants that require abstract reasoning without numerical patterns or counting, and comparing performance on these to the original tasks and numerical variants.

### Open Question 2
- Question: To what extent do current LLMs rely on memorized patterns versus abstract reasoning when solving analogy problems?
- Basis in paper: [inferred] The paper suggests that LLMs may be using "narrow, non-transferable procedures for task solving" rather than robust abstract reasoning.
- Why unresolved: While the paper provides evidence for a lack of robustness, it doesn't definitively prove whether this is due to memorization or a fundamental inability to perform abstract reasoning.
- What evidence would resolve it: Experiments that systematically vary the content of analogy problems while keeping the abstract structure constant, and analyzing the models' ability to generalize across these variations.

### Open Question 3
- Question: Can we develop LLMs with improved metacognitive abilities that would allow them to overcome biases and perform more robust analogical reasoning?
- Basis in paper: [explicit] The paper mentions that humans' abilities for metacognitive deliberation may contribute to their superior performance on abstract analogical reasoning tasks.
- Why unresolved: Current state-of-the-art AI systems largely lack metacognitive abilities, and it's unclear how to effectively incorporate them into LLMs.
- What evidence would resolve it: Developing and testing LLMs with explicit metacognitive components, and comparing their performance on robust analogical reasoning tasks to standard LLMs and humans.

## Limitations
- Focus on zero-shot prompting may underestimate LLM capabilities that could be unlocked with few-shot or fine-tuned approaches
- Potential confounds in variant design may introduce task complexity beyond intended surface-level changes
- Weak corpus support (average neighbor FMR=0.522, average citations=0.0) suggests this is a relatively underexplored area

## Confidence
- **Medium**: Results showing performance drops on variants are well-documented and replicable, but the interpretation that this definitively demonstrates lack of robust abstract reasoning is somewhat limited by the zero-shot constraint and the absence of comparisons with other model architectures or training approaches.

## Next Checks
1. Test the same variants using few-shot prompting to determine if performance improvements are possible with minimal guidance
2. Create a controlled experiment varying only one surface feature at a time to isolate which specific changes cause performance degradation
3. Compare results across different LLM architectures (including open-source models) to determine if the robustness issues are universal or specific to GPT models