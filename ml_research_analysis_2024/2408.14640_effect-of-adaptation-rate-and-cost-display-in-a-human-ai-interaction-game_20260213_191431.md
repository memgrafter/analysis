---
ver: rpa2
title: Effect of Adaptation Rate and Cost Display in a Human-AI Interaction Game
arxiv_id: '2408.14640'
source_url: https://arxiv.org/abs/2408.14640
tags:
- human
- cost
- actions
- action
- adaptation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This study investigates how an AI\u2019s adaptation rate and the\
  \ type of cost feedback displayed to humans affect human behavior in a two-player\
  \ continuous game. The AI adapted its actions using gradient descent under different\
  \ adaptation rates, while human participants optimized their actions based on one\
  \ of two feedback displays: current cost or a localized cost landscape."
---

# Effect of Adaptation Rate and Cost Display in a Human-AI Interaction Game

## Quick Facts
- arXiv ID: 2408.14640
- Source URL: https://arxiv.org/abs/2408.14640
- Reference count: 39
- One-line primary result: AI adaptation rate and cost feedback type significantly influence whether human behavior converges to Nash or Stackelberg equilibria in human-AI games

## Executive Summary
This study investigates how an AI's adaptation rate and the type of cost feedback displayed to humans affect human behavior in a two-player continuous game. The AI adapted its actions using gradient descent under different adaptation rates, while human participants optimized their actions based on one of two feedback displays: current cost or a localized cost landscape. Results show that increasing the AI's adaptation rate shifted human behavior from the Nash equilibrium toward the human-led Stackelberg equilibrium, with faster rates reducing human cost. The localized cost landscape feedback further shifted outcomes toward the Nash equilibrium compared to current cost feedback. Numerical simulations confirmed these trends, validating the game-theoretic predictions.

## Method Summary
The study employed a two-player continuous game where human participants controlled one player and an AI controlled the other. The AI used gradient descent to adapt its actions under five different adaptation rates (α ∈ {0, 0.001, 0.01, 0.1, 1}). Human participants received either scalar current cost feedback (displayed as an expanding/shrinking circle) or localized cost landscape feedback (displayed as a 7×7 grid of shaded dots). Thirty participants per experiment were recruited via Prolific.com and completed 25-second trials with random axis flips. Data was collected at 60 or 24 Hz and analyzed by trimming to the last 5 seconds and examining median actions and cost histograms.

## Key Results
- Increasing AI adaptation rate shifted human behavior from Nash equilibrium toward human-led Stackelberg equilibrium
- Faster AI adaptation rates reduced human cost while increasing AI cost
- Localized cost landscape feedback shifted outcomes further toward Nash equilibrium compared to current cost feedback
- Numerical simulations using a two-point zeroth-order gradient approximation matched human experimental results

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Faster AI adaptation rate causes the human to shift actions from the Nash equilibrium toward the Stackelberg equilibrium.
- Mechanism: The AI's gradient descent update rule with higher α responds more quickly to human moves, making the human's optimal strategy align more with the Stackelberg follower's best response rather than the Nash mutual best response.
- Core assumption: The human observes AI actions and updates their strategy to minimize their own cost in real time, not in anticipation of future AI states.
- Evidence anchors:
  - [abstract] "Results show that increasing the AI’s adaptation rate shifted human behavior from the Nash equilibrium toward the human-led Stackelberg equilibrium, with faster rates reducing human cost."
  - [section 3.3] "At the fastest adaption rate α = 1, the AI implements the AI’s best response to the human’s action."
  - [corpus] Weak; related work focuses on refresh rates and LLMs, not on adaptation-rate-induced equilibrium shifts.
- Break condition: If human updates are not myopic (e.g., if they plan ahead or use historical data), the equilibrium shift may not occur.

### Mechanism 2
- Claim: Localized cost landscape feedback shifts human actions toward the Nash equilibrium compared to single-point cost feedback.
- Mechanism: The heat map gives the human information about the cost gradient in a local neighborhood, enabling better estimation of optimal actions and reducing the strategic advantage of anticipating AI moves.
- Core assumption: Human can interpret the localized cost information to estimate gradients or cost differences.
- Evidence anchors:
  - [abstract] "The localized cost landscape feedback further shifted outcomes toward the Nash equilibrium compared to current cost feedback."
  - [section 3.5] "The shaded dots represented the Human cost at the human-AI joint action, {cH(h, m(t,:)) | h ∈ Bϵ(h(t,:))}, where the AI’s action, m(t,:), was the AI’s current action..."
  - [corpus] Missing; no direct evidence in related papers.
- Break condition: If the human cannot accurately interpret the heat map, the benefit disappears.

### Mechanism 3
- Claim: Human actions in the experiments can be modeled as a two-point zeroth-order gradient approximation.
- Mechanism: Human samples nearby actions (via mouse movement) and infers the gradient direction, updating their action accordingly.
- Core assumption: Human's exploration of the input space is random enough to estimate gradients.
- Evidence anchors:
  - [section 5.1] "the simulated Human agent had adaptation rate η = 0.01... the number of iterations was T = 1000... K was set to K = 10 to simulation the Human’s strategy to test an action near their current action in order to gain cost information to estimate which action to take next."
  - [section 5.2] "We found that simulating the human adaptation as a two-point zeroth-order approximation of the Human agent’s gradient provided similar learning dynamics as our human experiments results."
  - [corpus] Weak; no corpus support for zeroth-order modeling of human learning in games.
- Break condition: If human exploration is biased or deterministic, the gradient approximation fails.

## Foundational Learning

- Concept: Nash equilibrium definition and characterization via first-order conditions.
  - Why needed here: The paper uses Nash equilibrium as a baseline outcome; understanding its definition is essential to interpret the equilibrium shift.
  - Quick check question: In a two-player game, what conditions on the partial derivatives of the cost functions define a differential Nash equilibrium?

- Concept: Stackelberg equilibrium and the leader-follower dynamic.
  - Why needed here: The study contrasts Nash and human-led Stackelberg equilibria; understanding the sequential nature is key.
  - Quick check question: How does the Stackelberg leader’s optimization differ from the Nash player’s when choosing actions?

- Concept: Gradient descent update rules and learning rates.
  - Why needed here: The AI’s adaptation is governed by gradient descent; the rate α directly influences convergence and equilibrium selection.
  - Quick check question: What happens to the AI’s best-response mapping when α = 1 versus α = 0 in a quadratic cost setting?

## Architecture Onboarding

- Component map: Human mouse movements -> h(t) capture -> AI gradient descent update m(t+1) = m(t) - α ∂cM/∂m -> Cost feedback rendering (circle or heat map) -> Data logging -> Post-trial aggregation and visualization

- Critical path: 1. Human moves mouse → h(t) captured. 2. AI updates m(t) via gradient descent. 3. Cost feedback rendered (circle or heat map). 4. Data logged for each timestep. 5. Post-trial aggregation and visualization.

- Design tradeoffs:
  - Fast α → quicker convergence to Stackelberg but may cause oscillations.
  - Slow α → stability but may bias toward Nash.
  - Heat map resolution → more information vs. cognitive load.
  - Sampling rate → more data vs. storage/processing overhead.

- Failure signatures:
  - AI cost near zero but human cost high → possible misalignment in cost parameters.
  - No visible shift between equilibria → adaptation rate too small or human exploration too random.
  - Heat map feedback yields no change vs. scalar → human not interpreting gradient info.

- First 3 experiments:
  1. Run a 2x2 game with α = 0.01 and scalar cost feedback; verify median actions near Stackelberg.
  2. Repeat with α = 0.001; verify shift toward Nash.
  3. Switch to heat map feedback; check if equilibrium shifts further toward Nash at same α.

## Open Questions the Paper Calls Out
- Question: How do different human adaptation strategies (e.g., gradient-free methods vs. gradient-based methods) affect the convergence to game-theoretic equilibria in human-AI interactions?
  - Basis in paper: [explicit] The paper mentions that "simulating the human adaptation as a two-point zeroth-order approximation of the Human agent’s gradient provided similar learning dynamics as our human experiments results," suggesting that different adaptation strategies could be explored.
  - Why unresolved: The paper focuses on a specific adaptation strategy for humans and does not explore other possible strategies.
  - What evidence would resolve it: Conducting experiments with different human adaptation strategies and comparing their convergence to game-theoretic equilibria would provide insights into the impact of these strategies.

- Question: How do different types of feedback information (e.g., cost feedback vs. cost landscape feedback) influence human trust and decision-making in human-AI interactions?
  - Basis in paper: [explicit] The paper compares the effects of cost feedback and cost landscape feedback on human behavior, but does not explore the underlying mechanisms of how these feedback types influence trust and decision-making.
  - Why unresolved: The paper does not delve into the psychological or cognitive aspects of how feedback information affects human trust and decision-making.
  - What evidence would resolve it: Conducting experiments that measure human trust and decision-making processes under different feedback conditions would provide insights into the influence of feedback information.

- Question: How do larger action spaces and more complex cost functions impact the convergence and stability of human-AI interactions?
  - Basis in paper: [inferred] The paper mentions that "future work will address these limitations by studying larger models and studying other classes action spaces," suggesting that the impact of larger action spaces and more complex cost functions is not yet explored.
  - Why unresolved: The paper focuses on a specific cost model and action space, and does not explore the effects of more complex scenarios.
  - What evidence would resolve it: Conducting experiments with larger action spaces and more complex cost functions would provide insights into the impact of these factors on convergence and stability.

## Limitations
- Theoretical mechanism linking adaptation rate to equilibrium selection is plausible but not rigorously proven
- Human decision model is assumed rather than derived from observed behavior
- Lack of specification for exact cost matrix values and display scaling parameters creates reproducibility gaps

## Confidence
- Mechanism 1 (adaptation rate → equilibrium shift): Medium - supported by experimental results but theoretical justification is incomplete
- Mechanism 2 (cost landscape → Nash shift): Medium - empirical evidence present but interpretation mechanism unclear
- Mechanism 3 (zeroth-order gradient approximation): Low - simulation shows similarity but no corpus support for this human modeling approach

## Next Checks
1. Replicate the core finding that α = 0.01 produces Stackelberg-like behavior while α = 0.001 produces Nash-like behavior, using specified cost parameters
2. Test whether the heat map feedback actually improves human gradient estimation by comparing performance to analytical gradient information
3. Validate the zeroth-order gradient approximation by analyzing actual human exploration patterns in the raw trajectory data