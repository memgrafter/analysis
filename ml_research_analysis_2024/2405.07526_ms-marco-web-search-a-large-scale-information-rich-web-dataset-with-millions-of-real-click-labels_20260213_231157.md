---
ver: rpa2
title: 'MS MARCO Web Search: a Large-scale Information-rich Web Dataset with Millions
  of Real Click Labels'
arxiv_id: '2405.07526'
source_url: https://arxiv.org/abs/2405.07526
tags:
- search
- retrieval
- dataset
- information
- train
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: MS MARCO Web Search introduces a large-scale information-rich web
  dataset with millions of real clicked query-document labels, designed to advance
  research in large semantic understanding models, embedding models, and next-generation
  information access systems. The dataset, based on ClueWeb22, contains 10 billion
  web documents and 10 million queries across 93 languages, with rich information
  such as visual representations, HTML structures, and semantic annotations.
---

# MS MARCO Web Search: a Large-scale Information-rich Web Dataset with Millions of Real Click Labels

## Quick Facts
- arXiv ID: 2405.07526
- Source URL: https://arxiv.org/abs/2405.07526
- Reference count: 40
- Large-scale web dataset with millions of real clicked query-document labels

## Executive Summary
MS MARCO Web Search introduces a comprehensive web-scale information retrieval dataset based on ClueWeb22, containing 10 billion web documents and 10 million queries across 93 languages. The dataset features millions of real click labels from Bing search logs, providing rich information including visual representations, HTML structures, and semantic annotations. With 82% novel content in the test set, it addresses key challenges in web retrieval including language and domain biases while enabling research into embedding models, retrieval algorithms, and end-to-end systems.

## Method Summary
The dataset is constructed using ClueWeb22 as the document corpus and sampling query-document clicks from one year of Bing search logs. Documents are processed to extract multimodal features including visual rendering, HTML structure, and semantic annotations. Queries are collected across 93 languages and filtered for quality. The dataset is split into training, development, and test sets with temporal separation to ensure 82% novel content in the test set. Baseline models including DPR, ANCE, and SimANS are evaluated using both exact (KNN) and approximate (ANN) nearest neighbor search algorithms, with performance measured across quality metrics (MRR, recall) and system metrics (throughput, latency).

## Key Results
- Dataset contains 10 billion web documents and 10 million unique queries across 93 languages
- 82% of test query-document pairs are novel content, enabling evaluation of generalization
- SimANS baseline model shows the best performance among tested models
- ANN search algorithms show significant quality drops compared to brute-force search

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Large-scale real clicked query-document labels enable effective training of semantic understanding models.
- Mechanism: Real click data provides dense supervision signals reflecting actual user intent and document relevance, allowing models to learn robust semantic matching beyond keyword features.
- Core assumption: Click patterns accurately represent user satisfaction and document relevance.
- Evidence anchors:
  - [abstract]: "millions of real clicked query-document labels"
  - [section 3.2]: "We sample query-document clicks from one year of Bing search engine's logs."
  - [corpus]: Weak—corpus provides only high-level neighbor titles, no click data details.
- Break condition: If click logs are sparse, noisy, or biased toward popular queries, the model may overfit to clickbait or miss niche intents.

### Mechanism 2
- Claim: Rich multimodal document information (HTML structure, visual rendering, semantic annotations) improves embedding model generalization.
- Mechanism: Models trained on multi-modal inputs learn richer representations, capturing layout, topic tags, and language signals that pure text embeddings miss.
- Core assumption: Multimodal cues correlate with relevance and user satisfaction.
- Evidence anchors:
  - [abstract]: "provides rich information for various kinds of downstream tasks"
  - [section 3.1]: "ClueWeb22 includes about 10 billion high-quality web pages with rich affiliated information, such as visual representation rendered by web browsers, raw HTML structure, clean text, semantic annotations..."
  - [corpus]: Weak—corpus neighbors lack multimodal details.
- Break condition: If multimodal features are noisy or irrelevant, they may distract the model and degrade performance.

### Mechanism 3
- Claim: Temporal train-test split with minimal overlap improves evaluation of model generalization.
- Mechanism: By ensuring test queries/documents are novel in time, the benchmark tests the model's ability to generalize to unseen content rather than memorizing past pairs.
- Core assumption: Web content and user queries evolve over time; models must adapt to new patterns.
- Evidence anchors:
  - [section 3.3.3]: "We minimize the overlap between the train and test sets by splitting the query-document pairs into train and test sets by time... 82% of query-document pairs are novel content in the test set."
  - [abstract]: "82% of test pairs being novel content."
  - [corpus]: Weak—corpus neighbors don't reference temporal splits.
- Break condition: If temporal drift is too large, models trained on old data may perform poorly on new patterns.

## Foundational Learning

- Concept: Understanding embedding-based retrieval and vector similarity search.
  - Why needed here: The dataset is designed to train and evaluate dense retrieval models; knowing how embeddings capture semantic similarity is critical.
  - Quick check question: What is the difference between exact (KNN) and approximate (ANN) nearest neighbor search in high-dimensional spaces?

- Concept: Web-scale data processing and language distribution analysis.
  - Why needed here: The dataset spans 93 languages and 10B documents; engineers must handle skew, multilingual tokenization, and efficient indexing.
  - Quick check question: How does query language skew affect embedding model bias, and what data-centric techniques can mitigate it?

- Concept: Evaluation metrics for retrieval (MRR, recall, latency, throughput).
  - Why needed here: The benchmark evaluates both quality (MRR, recall) and system performance (QPS, latency); understanding trade-offs is essential for system design.
  - Quick check question: Why might a model with higher recall@100 still have lower MRR@10?

## Architecture Onboarding

- Component map: Document Ingestion → ClueWeb22 parser → Multimodal feature extraction → Document embedding model → Query Collection → Bing logs filter → Query embedding model → Training Pipeline → Negative sampling (DPR/ANCE/SimANS) → Model update → Evaluation → Brute-force vs ANN search comparison → Quality + system metrics → Storage → ANN index (DiskANN/SPANN) + Elasticsearch BM25 baseline

- Critical path: Train embedding model → Generate document/query embeddings → Build ANN index → Serve retrieval with latency/throughput constraints.

- Design tradeoffs:
  - Embedding dimensionality vs memory cost vs retrieval accuracy.
  - ANN recall vs latency (tradeoff between DiskANN and SPANN).
  - Training data scale vs overfitting risk (skew mitigation).
  - Multimodal features vs model complexity.

- Failure signatures:
  - Low recall@100 but high MRR@10 → model overfits to top results, misses broader relevant set.
  - High latency despite good QPS → index building or search traversal inefficiency.
  - Language skew in dev set → imbalanced training data.

- First 3 experiments:
  1. Train DPR baseline on 100M docs, evaluate MRR@10 vs recall@100, compare to brute-force KNN.
  2. Build DiskANN index on DPR embeddings, measure recall@10 and QPS, compare to SPANN.
  3. Test temporal split effect: train on 2023 data, evaluate on 2024 novel queries, measure generalization gap.

## Open Questions the Paper Calls Out
None specified in the provided content.

## Limitations
- Potential bias from Bing-specific click data that may not generalize to other search engines
- Performance degradation when using approximate nearest neighbor search compared to exact search
- Language and domain biases due to skew in the ClueWeb22 dataset

## Confidence
- High confidence: Dataset scale and basic construction methodology
- Medium confidence: Improvements from multimodal features and temporal splitting
- Low confidence: Generalizability across different search domains and user populations

## Next Checks
1. Conduct ablation studies to quantify the actual contribution of multimodal features versus text-only embeddings in retrieval performance
2. Test model performance on cross-domain datasets (e.g., academic, news, e-commerce) to assess generalizability beyond web search
3. Analyze click data quality by examining coverage across query types, popularity distributions, and potential position bias effects on labeled relevance judgments