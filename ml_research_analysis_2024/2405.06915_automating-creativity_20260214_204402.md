---
ver: rpa2
title: Automating Creativity
arxiv_id: '2405.06915'
source_url: https://arxiv.org/abs/2405.06915
tags:
- creativity
- genai
- prompt
- market
- space
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper develops a triple prompt-response-reward engineering
  framework to evolve generative AI from generative to creative. The framework comprises
  three models: 1) prompt engineering for developing novel prompts that are objectively,
  individually, or socially novel, 2) response engineering for generating surprising
  outputs that are unfamiliar, unexpected, or unlikely, and 3) reward engineering
  for improving creativity over time by incorporating feedback from AI, creators/managers,
  and/or customers.'
---

# Automating Creativity

## Quick Facts
- arXiv ID: 2405.06915
- Source URL: https://arxiv.org/abs/2405.06915
- Reference count: 9
- Primary result: A triple prompt-response-reward engineering framework to evolve generative AI from generative to creative, enabling incremental to radical innovation

## Executive Summary
This paper introduces a triple prompt-response-reward engineering framework to transform generative AI from simple content generation to genuine creativity. The framework consists of three interconnected models: prompt engineering for developing novel prompts at objective, individual, or social levels; response engineering for generating surprising outputs that are unfamiliar, unexpected, or unlikely; and reward engineering for improving creativity over time through feedback from AI, creators/managers, and customers. The framework enables strategic application of generative AI across various creativity levels from incremental to radical innovation. Two proofs of concept demonstrate the framework's application in developing marketing strategies for Starbucks and iPhone.

## Method Summary
The paper develops a conceptual framework combining three engineering approaches: prompt engineering to generate novel prompts, response engineering to produce surprising outputs, and reward engineering to incorporate multi-source feedback for continuous creativity improvement. The methodology involves applying these three components iteratively to evolve generative AI capabilities from basic generation to creative output. Two case studies with Starbucks and iPhone marketing strategies illustrate the framework's practical application, though specific methodological details and validation metrics are not extensively documented.

## Key Results
- Framework enables evolution of generative AI from generation to creativity
- Three models work synergistically: prompt engineering, response engineering, and reward engineering
- Successfully demonstrated through Starbucks and iPhone marketing strategy development
- Supports creativity at incremental to radical innovation levels

## Why This Works (Mechanism)
The framework works by creating a systematic approach to creativity that mirrors human creative processes. Prompt engineering establishes the foundation by generating novel starting points that are objectively, individually, or socially unique. Response engineering then produces outputs that surprise through unfamiliarity, unexpectedness, or unlikelihood. Finally, reward engineering creates a feedback loop that continuously improves creative outputs by incorporating perspectives from AI systems, human creators/managers, and end customers. This triple engineering approach provides a structured methodology for scaling creative processes while maintaining the novelty and value that characterize genuine creativity.

## Foundational Learning
- Prompt Engineering: Creating novel prompts that are objectively, individually, or socially unique
  - Why needed: Establishes the creative foundation for generating unique outputs
  - Quick check: Are prompts generating outputs that pass novelty tests across different domains?

- Response Engineering: Generating outputs that are unfamiliar, unexpected, or unlikely
  - Why needed: Ensures creative outputs have the "surprise" element that distinguishes them from routine generation
  - Quick check: Can outputs be measured for surprise value using standardized creativity metrics?

- Reward Engineering: Incorporating multi-source feedback to improve creativity over time
  - Why needed: Enables continuous improvement and adaptation of creative outputs
  - Quick check: Are feedback loops effectively improving output quality and novelty?

## Architecture Onboarding

**Component Map:**
Prompt Engineering -> Response Engineering -> Reward Engineering

**Critical Path:**
1. Generate novel prompts (objective/individual/social novelty)
2. Produce surprising outputs (unfamiliar/unexpected/unlikely)
3. Incorporate feedback and improve (AI/creator/customer perspectives)

**Design Tradeoffs:**
- Balancing novelty vs. usefulness in outputs
- Managing conflicting feedback from different sources
- Maintaining creative surprise while ensuring practical applicability

**Failure Signatures:**
- Outputs become formulaic or predictable
- Feedback loops create echo chambers
- Prompts lose novelty over time
- Response engineering produces outputs that are novel but not valuable

**First Experiments:**
1. Test prompt engineering across three domains to verify objective, individual, and social novelty
2. Compare response engineering outputs against baseline generative AI using creativity metrics
3. Implement reward engineering feedback loops with simulated AI/creator/customer feedback

## Open Questions the Paper Calls Out
None identified in the source material.

## Limitations
- Limited empirical validation with only two case studies
- Lack of detailed methodology for measuring creativity metrics
- Abstract description of feedback mechanisms without conflict resolution strategies
- Unclear operational definitions for creativity levels and surprise quantification

## Confidence
- Framework validity: Medium
- Practical implementation: Low
- Creativity measurement: Low
- Business impact: Medium

## Next Checks
1. Conduct controlled experiments comparing outputs generated through this framework against baseline generative AI models using standardized creativity metrics like the Torrance Tests of Creative Thinking across multiple domains.

2. Implement the framework in a real-world creative process with measurable business outcomes over time, documenting the evolution of outputs and user satisfaction.

3. Perform expert evaluation studies where domain specialists assess the novelty, value, and surprise of outputs generated through each component of the triple engineering approach versus traditional methods.