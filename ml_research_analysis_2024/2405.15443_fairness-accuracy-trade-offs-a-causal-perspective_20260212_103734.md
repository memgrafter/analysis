---
ver: rpa2
title: 'Fairness-Accuracy Trade-Offs: A Causal Perspective'
arxiv_id: '2405.15443'
source_url: https://arxiv.org/abs/2405.15443
tags:
- causal
- fairness
- loss
- psel
- excess
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper analyzes the tension between fairness and accuracy from
  a causal perspective. It introduces path-specific excess loss (PSEL) to quantify
  how much a predictor's loss increases when enforcing causal fairness constraints.
---

# Fairness-Accuracy Trade-Offs: A Causal Perspective

## Quick Facts
- arXiv ID: 2405.15443
- Source URL: https://arxiv.org/abs/2405.15443
- Authors: Drago Plecko; Elias Bareinboim
- Reference count: 40
- Key outcome: This paper analyzes the tension between fairness and accuracy from a causal perspective. It introduces path-specific excess loss (PSEL) to quantify how much a predictor's loss increases when enforcing causal fairness constraints. The total excess loss (TEL) is shown to decompose into a sum of PSELs across different causal pathways. The causal fairness/utility ratio (CFUR) summarizes the trade-off between fairness gain and loss in accuracy. A new neural approach (CFCL) is introduced to learn causally-constrained fair predictors. Experiments on real-world datasets show that, from a causal viewpoint, fairness and utility are almost always in tension, contrary to some existing literature. The approach allows quantifying fairness-utility trade-offs for specific causal pathways, enabling more informed decisions about which constraints to impose.

## Executive Summary
This paper provides a causal framework for understanding the trade-off between fairness and accuracy in machine learning. The authors introduce path-specific excess loss (PSEL) as a measure of how much predictive accuracy degrades when enforcing fairness constraints along specific causal pathways. They show that the total excess loss (TEL) can be decomposed into a sum of PSELs, allowing for granular analysis of fairness-utility trade-offs. The causal fairness/utility ratio (CFUR) provides a single metric to compare the efficiency of different causal constraints. A novel neural approach (CFCL) is proposed to learn predictors that satisfy these constraints. Experiments on real-world datasets demonstrate that, from a causal perspective, fairness and utility are typically in tension, contradicting some prior literature that suggested these goals could be aligned.

## Method Summary
The paper introduces a causal framework for analyzing fairness-accuracy trade-offs using path-specific excess loss (PSEL) and the causal fairness/utility ratio (CFUR). The method estimates causal effects between protected attributes, confounders, mediators, and outcomes, then trains neural predictors subject to causal fairness constraints using a Lagrangian formulation (CFCL algorithm). For each causal path subset, the approach computes PSEL (how much accuracy degrades) and TVD (reduction in group disparity), then calculates CFUR as the ratio of disparity reduction to accuracy loss. The total excess loss decomposes into a sum of PSELs across all paths, enabling granular attribution of fairness-utility trade-offs to specific causal mechanisms.

## Key Results
- Path-specific excess loss (PSEL) allows attribution of fairness-utility trade-offs to individual causal pathways (direct, indirect, spurious)
- Total excess loss (TEL) decomposes into a sum of PSELs across causal pathways
- Causal fairness/utility ratio (CFUR) provides a single metric to compare fairness-utility trade-offs across pathways
- Experiments on real-world datasets show that fairness and utility are almost always in tension from a causal viewpoint
- The CFCL neural approach can learn predictors that satisfy complex causal fairness constraints

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The path-specific excess loss (PSEL) allows attribution of fairness-utility trade-offs to individual causal pathways.
- Mechanism: By decomposing the total excess loss (TEL) into a sum of PSELs across causal pathways, the method quantifies how much predictive accuracy degrades when enforcing fairness constraints along specific paths (direct, indirect, spurious).
- Core assumption: Causal effects can be isolated and constrained without unintended amplification bias in non-targeted paths.
- Evidence anchors:
  - [abstract] "We introduce the notion of a path-specific excess loss (PSEL) that captures how much the predictor's loss increases when a causal fairness constraint is enforced."
  - [section] "The total excess loss (TEL), defined as the difference between the loss of predictor fair along all causal pathways vs. an unconstrained predictor, can be decomposed into a sum of more local PSELs."
  - [corpus] Weak - no corpus evidence explicitly supporting the decomposition claim.
- Break condition: If causal paths are confounded or unidentifiable, the decomposition may be invalid or misleading.

### Mechanism 2
- Claim: The causal fairness/utility ratio (CFUR) provides a single metric to compare fairness-utility trade-offs across pathways.
- Mechanism: CFUR is defined as the ratio of reduction in group disparity (TVD) to excess loss incurred, summarizing the efficiency of fairness gains per unit loss for each causal path.
- Core assumption: The reduction in discrimination (measured by TVD) and the increase in loss (measured by PSEL) are both meaningful and comparable for each path.
- Evidence anchors:
  - [abstract] "the causal fairness/utility ratio, defined as the ratio of the reduction in discrimination vs. the excess in the loss from constraining a causal pathway."
  - [section] "we can quantify the fairness/utility trade-off through a causal lens... based on the excess loss and the reduction in disparity resulting from constraining a causal path to zero."
  - [corpus] Weak - corpus does not discuss CFUR specifically.
- Break condition: If the loss function or disparity metric is inappropriate for the domain, CFUR values may mislead decision-making.

### Mechanism 3
- Claim: The neural CFCL algorithm can learn predictors that satisfy complex causal fairness constraints.
- Mechanism: CFCL reformulates causal fairness constraints as a Lagrangian optimization problem, enabling gradient-based learning of predictors that meet fairness requirements on held-out data.
- Core assumption: Causal effects can be estimated and enforced via differentiable constraints in a neural network training loop.
- Evidence anchors:
  - [abstract] "a new neural approach (CFCL) is introduced to learn causally-constrained fair predictors."
  - [section] "We introduce a new neural approach for causally-constrained fair learning... a novel Lagrangian formulation of the optimization problem."
  - [corpus] Weak - no corpus evidence on CFCL or Lagrangian fairness training.
- Break condition: If causal effect estimates are unstable or biased, the learned predictor may violate fairness constraints in practice.

## Foundational Learning

- Concept: Structural Causal Models (SCMs) and causal diagrams
  - Why needed here: The entire fairness-utility analysis relies on identifying and constraining causal pathways between protected attributes and outcomes.
  - Quick check question: Can you explain how a backdoor path differs from a direct causal path in an SCM?

- Concept: Path-specific causal effects (natural direct, indirect, spurious)
  - Why needed here: The method quantifies how much each type of causal effect contributes to both predictive accuracy and group disparities.
  - Quick check question: What is the difference between the natural direct effect and the natural indirect effect?

- Concept: Shapley value attribution
  - Why needed here: The method uses Shapley values to fairly attribute total excess loss to individual causal pathways across all possible decompositions.
  - Quick check question: Why might averaging PSELs across all paths be preferable to assigning blame to a single path?

## Architecture Onboarding

- Component map:
  - Data preprocessing -> Causal effect estimation -> Fair predictor learning (CFCL) -> PSEL/TVD computation -> CFUR calculation -> Analysis/visualization

- Critical path:
  1. Estimate causal effects from data.
  2. Train fair predictors for each causal path subset using CFCL.
  3. Compute PSEL and TVD for each predictor.
  4. Calculate CFUR and decompose TEL.
  5. Interpret results and select constraints.

- Design tradeoffs:
  - Tradeoff between computational cost (training multiple neural networks) and granularity of analysis.
  - Tradeoff between strictness of causal assumptions and applicability to real-world data.
  - Tradeoff between using more granular path-specific effects vs. simplicity of direct/indirect/spurious decomposition.

- Failure signatures:
  - PSEL values close to zero for all paths may indicate weak causal effects or model misspecification.
  - CFUR values near zero or undefined may suggest either no loss incurred or no disparity reduction.
  - High variance in PSEL/TVD estimates across bootstrap samples may indicate unstable causal effect estimation.

- First 3 experiments:
  1. Linear synthetic dataset with known causal structure; verify PSEL decomposition matches analytical solution.
  2. Real-world dataset (e.g., Census); compute CFUR for direct, indirect, and spurious effects; compare with baseline fairness methods.
  3. Ablation study: Remove one causal effect at a time; measure impact on CFUR and TEL; assess sensitivity to constraint choice.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the path-specific excess loss (PSEL) decomposition be extended to settings with continuous protected attributes or non-binary mediator variables?
- Basis in paper: [explicit] The paper focuses on binary X and assumes a specific causal structure (SFM), but discusses generalizing to path-specific effects in Appendix F
- Why unresolved: The current formulation relies on discrete path decompositions and binary attribute assumptions. Real-world applications often involve continuous or multi-category variables
- What evidence would resolve it: Theoretical proofs extending PSEL decomposition to continuous variables, or experimental results showing consistent decomposition patterns with continuous attributes

### Open Question 2
- Question: How does the choice of loss function (beyond MSE/AUROC) affect the causal fairness-utility ratio (CFUR) and the observed fairness-utility trade-offs?
- Basis in paper: [inferred] The paper uses RMSE and 1-AUROC as loss functions but doesn't explore how different loss functions might change the trade-off landscape
- Why unresolved: Different loss functions emphasize different aspects of prediction quality, which could fundamentally alter how fairness constraints impact utility
- What evidence would resolve it: Systematic experiments comparing CFUR values across multiple loss functions (e.g., cross-entropy, F1-score) on the same datasets

### Open Question 3
- Question: What is the relationship between the Shapley value attribution method used in PSEL decomposition and alternative attribution methods like SHAP or counterfactual explanations?
- Basis in paper: [explicit] The paper establishes that PSEL attribution equals Shapley values for a specific value function, but doesn't compare to other attribution frameworks
- Why unresolved: Different attribution methods might reveal different aspects of the fairness-utility trade-off, potentially leading to different policy recommendations
- What evidence would resolve it: Comparative analysis showing how different attribution methods rank causal pathways by their contribution to excess loss

### Open Question 4
- Question: How sensitive are the CFUR results to the specific causal graph structure, particularly when adding or removing confounders/mediators?
- Basis in paper: [inferred] The analysis assumes the standard fairness model structure, but doesn't explore sensitivity to structural changes in the causal graph
- Why unresolved: The identified trade-offs might be artifacts of the assumed causal structure rather than robust properties of the data-generating process
- What evidence would resolve it: Sensitivity analysis showing how CFUR values change when adding/removing edges in the causal graph, or when using alternative graph structures that still fit the observed data

## Limitations
- The core causal decomposition relies on strong assumptions about the identifiability of path-specific effects and the stability of causal estimates across different predictors
- The neural CFCL approach requires careful hyperparameter tuning, and its performance may vary significantly with different architectures or training regimes
- The choice of loss function and disparity metric (TVD) may not capture all relevant fairness concerns in every domain

## Confidence
- Causal decomposition mechanism (Mechanism 1): Medium - theoretically sound but dependent on strong causal assumptions
- CFUR metric interpretation (Mechanism 2): Medium - useful summary but sensitive to choice of metrics and scaling
- CFCL algorithm effectiveness (Mechanism 3): Low-Medium - novel approach with limited external validation

## Next Checks
1. Sensitivity analysis: Systematically vary causal assumptions (add/remove edges in causal graph) and measure impact on PSEL decomposition and CFUR values
2. Cross-dataset validation: Apply method to datasets with known ground truth causal effects (e.g., synthetic data) and compare estimated vs. true PSEL values
3. Baseline comparison: Benchmark CFCL against standard fairness-aware ML methods on the same datasets using identical metrics and evaluate whether causal constraints provide meaningful advantages