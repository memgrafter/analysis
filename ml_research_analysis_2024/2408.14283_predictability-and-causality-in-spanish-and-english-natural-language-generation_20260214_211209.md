---
ver: rpa2
title: Predictability and Causality in Spanish and English Natural Language Generation
arxiv_id: '2408.14283'
source_url: https://arxiv.org/abs/2408.14283
tags:
- language
- english
- spanish
- causal
- non-causal
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper examines whether causal transformer language models introduce
  generation bias in languages with less rigid word ordering and different grammatical
  structures than English, focusing on Spanish as a case study. It defines and computes
  novel metrics of causal and non-causal context-conditioned entropy for grammatical
  category distributions in English and Spanish, finding that Spanish is more predictable
  than English given a non-causal context.
---

# Predictability and Causality in Spanish and English Natural Language Generation

## Quick Facts
- **arXiv ID**: 2408.14283
- **Source URL**: https://arxiv.org/abs/2408.14283
- **Reference count**: 40
- **Primary result**: Non-causal NLG models perform better for Spanish while causal models perform better for English

## Executive Summary
This paper investigates whether the success of causal transformer models in English NLG generalizes to languages with different grammatical structures, using Spanish as a case study. The authors propose novel metrics for measuring grammatical category predictability under causal and non-causal contexts, finding that Spanish is more predictable than English when bidirectional context is available. They evaluate both model types using automatic and manual metrics, demonstrating that Spanish NLG benefits from non-causal (bidirectional) models while English NLG performs better with causal (unidirectional) models. The findings suggest that the architectural advantages of causal transformers may be language-specific and encourage further research into non-causal approaches for languages with flexible syntax.

## Method Summary
The authors compute average conditional entropy for grammatical categories given causal vs. non-causal contexts in English and Spanish using POS-tagged data from tales datasets and Wikipedia. They fine-tune Spanish GPT-2 and BERT, and English GPT-2 and BERT on the tales datasets for 10 epochs. The models generate 50-token sequences which are evaluated using conditional relative entropy (automatic) and manual evaluation by 5 annotators assessing concordance, syntactic structure, repetitions, word sense, and general rating.

## Key Results
- Spanish exhibits lower average non-causal conditional entropy compared to English, while English shows lower causal conditional entropy
- Spanish BERT non-causal language model achieved the lowest conditional relative entropy, outperforming even English GPT-2
- Manual evaluation confirmed that non-causal models generate more coherent Spanish text while causal models perform better for English

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Spanish is more predictable than English when non-causal context is available.
- **Mechanism**: The paper computes average conditional entropy for grammatical categories given causal vs. non-causal contexts. Lower entropy indicates higher predictability. The hypothesis is that bidirectional context improves predictability for Spanish due to its flexible syntax.
- **Core assumption**: Predictability measured by grammatical category entropy correlates with the suitability of non-causal vs. causal NLG models.
- **Evidence anchors**:
  - [abstract]: "Spanish is more predictable than English given a non-causal context."
  - [section]: "lower average non-causal conditional entropy in Spanish and lower causal conditional entropy in English."
- **Break condition**: If the grammatical category distribution does not adequately capture syntactic predictability differences between the languages.

### Mechanism 2
- **Claim**: Causal NLG performs better for English, while non-causal NLG performs better for Spanish.
- **Mechanism**: The paper evaluates causal (GPT-2) and non-causal (BERT) models for both languages using automatic (conditional relative entropy) and manual evaluation. The performance difference is attributed to language-specific syntactic structures.
- **Core assumption**: The superiority of a generation model type for a language is reflected in both automatic and manual evaluation metrics.
- **Evidence anchors**:
  - [abstract]: "causal NLG performs better for English, while non-causal NLG performs better for Spanish"
  - [section]: "Spanish BERT non-causal language model had the lowest conditional relative entropy, outperforming even English GPT-2"
- **Break condition**: If the evaluation metrics do not accurately capture the quality of language generation for each language.

### Mechanism 3
- **Claim**: The mutual benefit between causal language modeling and English syntax does not apply to Spanish due to its flexible word order and subject omission.
- **Mechanism**: The paper hypothesizes that decoder-only transformers, which work well for English, may introduce generation bias in languages with less rigid word ordering like Spanish. This is tested by comparing predictability and NLG performance.
- **Core assumption**: The syntactic flexibility of Spanish makes non-causal modeling more suitable than causal modeling.
- **Evidence anchors**:
  - [abstract]: "depart from the hypothesis that they may introduce generation bias in target languages with less rigid word ordering, subject omission, or different attachment preferences for relative clauses"
  - [section]: "Spanish NLG may benefit from bidirectional transformer models"
- **Break condition**: If the syntactic flexibility of Spanish does not significantly impact the effectiveness of causal vs. non-causal NLG.

## Foundational Learning

- **Concept**: Conditional entropy and its application in language predictability.
  - **Why needed here**: The paper uses conditional entropy to quantify the predictability of grammatical categories in English and Spanish given causal and non-causal contexts.
  - **Quick check question**: How does conditional entropy differ from regular entropy, and why is it more appropriate for this study?

- **Concept**: Transformer language models and the difference between causal and non-causal models.
  - **Why needed here**: The paper compares the performance of causal (decoder-only) and non-causal (encoder-only) transformer models for NLG in English and Spanish.
  - **Quick check question**: What is the key architectural difference between causal and non-causal transformer models, and how does it affect their application in NLG?

- **Concept**: Natural Language Generation (NLG) and the challenges of multilingual NLG.
  - **Why needed here**: The paper addresses the challenges of NLG in languages other than English, focusing on Spanish as a case study.
  - **Quick check question**: What are the main challenges in developing NLG systems for languages with different grammatical structures and word orderings compared to English?

## Architecture Onboarding

- **Component map**: Data preprocessing (POS tagging) -> Entropy computation module -> NLG model training and evaluation -> Automatic and manual evaluation metrics

- **Critical path**:
  1. Preprocess English and Spanish text data using POS tagging.
  2. Compute average causal and non-causal context-conditioned entropy for both languages.
  3. Train and evaluate causal (GPT-2) and non-causal (BERT) models for both languages.
  4. Compare the performance of the models using automatic (conditional relative entropy) and manual evaluation metrics.

- **Design tradeoffs**:
  - Using grammatical categories instead of words/subwords to reduce data requirements vs. potential loss of information.
  - Choosing a small context length (e.g., 2 words) due to data limitations vs. capturing longer-range dependencies.
  - Using fine-tuned small models vs. state-of-the-art larger models for comparability.

- **Failure signatures**:
  - High conditional entropy values indicating low predictability.
  - Poor performance of both causal and non-causal models in automatic and manual evaluations.
  - Significant discrepancies between automatic and manual evaluation results.

- **First 3 experiments**:
  1. Compute average causal and non-causal context-conditioned entropy for English and Spanish using a small context length (e.g., 2 words) and POS-tagged data.
  2. Train and evaluate a causal (GPT-2) and non-causal (BERT) model for English and Spanish using the same data and evaluation metrics.
  3. Perform manual evaluation of the generated text to validate the automatic evaluation results and assess the quality of the NLG models for each language.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: How do causal and non-causal NLG models compare across languages with varying word order flexibility beyond Spanish and English?
- **Basis in paper**: [explicit] The paper focuses on Spanish and English but suggests the methodology could extend to other languages.
- **Why unresolved**: The paper only evaluates two languages, leaving the generalizability of findings to other languages untested.
- **What evidence would resolve it**: Comparative studies of NLG performance across a broader set of languages with varying grammatical structures.

### Open Question 2
- **Question**: What is the impact of different tokenization strategies (word, subword, character-level) on the predictability and causality findings in multilingual NLG?
- **Basis in paper**: [inferred] The paper uses POS tagging to reduce vocabulary size but notes that word or subword tokenization are not feasible due to data requirements.
- **Why unresolved**: The paper does not explore how different tokenization approaches might influence the results, especially in multilingual contexts.
- **What evidence would resolve it**: Experiments comparing NLG performance and predictability metrics across different tokenization strategies.

### Open Question 3
- **Question**: How do lossy context surprisal models, which incorporate working memory constraints, compare to traditional conditional entropy models in predicting NLG performance across languages?
- **Basis in paper**: [explicit] The paper mentions lossy context surprisal as a psycholinguistic concept that combines expectation and memory-based predictability theories.
- **Why unresolved**: The paper does not apply lossy context surprisal models to the NLG task, leaving the potential benefits unexplored.
- **What evidence would resolve it**: Implementation and evaluation of NLG models using lossy context surprisal metrics alongside traditional conditional entropy measures.

## Limitations

- The use of grammatical categories rather than surface forms may obscure important lexical and syntactic patterns that differ between English and Spanish
- Manual evaluation lacks detailed reporting on inter-annotator agreement metrics, making reliability assessment difficult
- The study focuses on tales as a specific domain and may not generalize to other text types or register variations

## Confidence

- **High confidence**: The finding that Spanish exhibits lower non-causal conditional entropy than English - this is directly computed from the data with clear methodology
- **Medium confidence**: The conclusion that non-causal models perform better for Spanish while causal models perform better for English - supported by both automatic and manual metrics, though the manual evaluation reliability is uncertain
- **Medium confidence**: The hypothesis that Spanish syntactic flexibility drives the observed differences - mechanistically plausible but not definitively proven by the current experiments

## Next Checks

1. Replicate the entropy and NLG experiments using surface forms (words/subwords) rather than grammatical categories to verify if the patterns hold at the lexical level
2. Conduct a formal inter-annotator agreement analysis on the manual evaluation data, including Krippendorff's alpha or similar reliability metrics
3. Test the same causal/non-causal model comparison on a different Spanish-English parallel corpus (e.g., news articles or conversational dialogue) to assess domain generalizability