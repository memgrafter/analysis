---
ver: rpa2
title: 'DreamWaltz-G: Expressive 3D Gaussian Avatars from Skeleton-Guided 2D Diffusion'
arxiv_id: '2409.17145'
source_url: https://arxiv.org/abs/2409.17145
tags:
- avatar
- avatars
- generation
- human
- smpl-x
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper addresses the challenge of generating high-quality,
  expressive 3D avatars from text prompts that can be animated realistically. The
  core method introduces two innovations: Skeleton-guided Score Distillation (SkelSD),
  which conditions 2D diffusion models with skeleton images to improve 3D consistency
  and reduce artifacts like multiple faces; and Hybrid 3D Gaussian Avatar representation
  (H3GA), which combines 3D Gaussians with neural implicit fields and parameterized
  meshes for real-time rendering and expressive animation.'
---

# DreamWaltz-G: Expressive 3D Gaussian Avatars from Skeleton-Guided 2D Diffusion

## Quick Facts
- arXiv ID: 2409.17145
- Source URL: https://arxiv.org/abs/2409.17145
- Authors: Yukun Huang; Jianan Wang; Ailing Zeng; Zheng-Jun Zha; Lei Zhang; Xihui Liu
- Reference count: 40
- Primary result: Achieves 84.93% preference for geometry quality and 86.30% for appearance quality compared to state-of-the-art

## Executive Summary
DreamWaltz-G addresses the challenge of generating high-quality, expressive 3D avatars from text prompts that can be animated realistically. The method introduces skeleton-guided score distillation (SkelSD) to improve view and pose consistency between 3D avatar rendering and 2D diffusion model supervision, reducing artifacts like multiple faces. It also presents a hybrid 3D Gaussian avatar representation (H3GA) that combines 3D Gaussians with neural implicit fields and parameterized meshes for real-time rendering and expressive animation. The framework learns avatars in two stages—first generating a static canonical avatar using Instant-NGP, then learning an animatable version using H3GA. Extensive experiments show DreamWaltz-G outperforms existing methods in both visual quality and animation expressiveness.

## Method Summary
DreamWaltz-G employs a two-stage learning framework to generate expressive 3D avatars from text prompts. Stage I uses Instant-NGP and skeleton-guided score distillation (SkelSD) to create a static canonical avatar with accurate geometry and appearance. Stage II uses the hybrid 3D Gaussian avatar representation (H3GA) and SkelSD with randomly sampled poses to learn pose-dependent shape and appearance variations, enabling animation. The H3GA combines 3D Gaussians for efficiency, neural implicit fields for local continuity, and parameterized meshes for geometric accuracy. Skeleton images from 3D human templates (SMPL-X) are injected into the diffusion model via ControlNet to enhance 3D consistency and prevent artifacts like multiple faces.

## Key Results
- Achieves 84.93% user preference for geometry quality compared to previous state-of-the-art
- Achieves 86.30% user preference for appearance quality compared to previous state-of-the-art
- Achieves 78.08% user preference for text consistency compared to previous state-of-the-art

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Skeleton-guided score distillation (SkelSD) improves view and pose consistency between the 3D avatar rendering and the 2D diffusion model supervision, reducing artifacts like multiple faces, extra limbs, and blurring.
- **Mechanism**: By injecting skeleton images from 3D human templates (SMPL-X) into the diffusion model via ControlNet, SkelSD ensures that the conditioning image's viewpoint is strictly aligned with the avatar's rendering viewpoint. This alignment reduces the disparity that causes artifacts in vanilla SDS.
- **Core assumption**: Skeleton images provide sufficient 3D-aware conditioning to guide the diffusion model without introducing overly restrictive shape priors that could eliminate complex appearances.
- **Evidence anchors**: [abstract] states: "enhancing the consistency of SDS supervision in terms of view and human pose" and "mitigating issues such as multiple faces, extra limbs, and blurring." [section 3.2] explains that skeleton guidance from 3D human templates "enhances the 3D consistency of SDS and prevents the Janus (multi-face) problem."

### Mechanism 2
- **Claim**: The hybrid 3D Gaussian avatar representation (H3GA) enables stable SDS optimization, real-time rendering, and expressive animation with finger movements and facial expressions.
- **Mechanism**: H3GA combines 3D Gaussians for efficiency, neural implicit fields for local continuity, and parameterized meshes for geometric accuracy. This hybrid structure allows for stable optimization during SDS training and efficient deformation during animation.
- **Core assumption**: The combination of these three representations can be trained jointly without introducing significant optimization instability or computational overhead.
- **Evidence anchors**: [abstract] states: "builds on the efficient 3D Gaussians, combining neural implicit fields and parameterized 3D meshes to enable real-time rendering, stable SDS optimization, and expressive animation." [section 3.3] describes how H3GA integrates "the efficiency of 3D Gaussian Splatting, the local continuity of neural implicit fields, and the geometric accuracy of parameterized meshes."

### Mechanism 3
- **Claim**: The two-stage training process (canonical avatar generation followed by animatable avatar learning) ensures high-quality static avatars that can be accurately animated.
- **Mechanism**: Stage I uses Instant-NGP and SkelSD to create a static canonical avatar with accurate geometry and appearance. Stage II uses H3GA and SkelSD with randomly sampled poses to learn pose-dependent shape and appearance variations, enabling animation.
- **Core assumption**: The canonical avatar from Stage I provides a sufficiently good initialization for the animatable avatar in Stage II, and the random pose sampling in Stage II effectively captures the pose-dependent variations.
- **Evidence anchors**: [abstract] states: "learns avatars in two stages—first generating a static canonical avatar using Instant-NGP, then learning an animatable version using H3GA." [section 3.4] describes the two-stage process and its goals.

## Foundational Learning

- **Concept**: Score Distillation Sampling (SDS)
  - **Why needed here**: SDS is the core technique used to distill knowledge from a pre-trained 2D diffusion model into the 3D avatar representation, enabling text-driven generation.
  - **Quick check question**: What is the key equation for SDS gradients, and what are its components (e.g., timestep t, noise prediction, rendered image)?

- **Concept**: 3D Gaussian Splatting (3DGS)
  - **Why needed here**: 3DGS provides an efficient and high-quality 3D representation that can be rendered in real-time and deformed for animation.
  - **Quick check question**: How are the geometry and color of each 3D Gaussian parameterized, and how is the final pixel color computed using alpha blending?

- **Concept**: SMPL-X Parametric Human Model
  - **Why needed here**: SMPL-X provides a 3D human template with articulated body, hands, and face, which is used for skeleton guidance and mesh binding in the avatar generation process.
  - **Quick check question**: What are the input parameters for SMPL-X, and how is the canonical mesh transformed into the observed pose using Linear Blend Skinning (LBS)?

## Architecture Onboarding

- **Component map**: Text prompt → Diffusion model (Stable-Diffusion-v1.5) → SDS gradients → Skeleton images from SMPL-X → ControlNet → SDS conditioning → Instant-NGP (Stage I) / H3GA (Stage II) → 3D avatar representation

- **Critical path**: Text prompt → Skeleton extraction → ControlNet conditioning → SDS optimization → 3D avatar representation update

- **Design tradeoffs**:
  - Using skeleton guidance vs. depth or normal maps: Skeletons provide informative but less restrictive supervision, avoiding the loss of complex appearances.
  - Two-stage training vs. single-stage: Two-stage training ensures a good canonical avatar initialization and effective learning of pose-dependent variations, but adds complexity.
  - H3GA vs. pure 3DGS: H3GA combines the benefits of 3DGS, NeRF, and meshes, but requires joint optimization of multiple components.

- **Failure signatures**:
  - Multiple faces or extra limbs: Indicates insufficient view consistency in SDS.
  - Blurry or over-saturated textures: Indicates high variance in SDS gradients.
  - Inaccurate hand or facial geometry: Indicates insufficient geometric priors or poor mesh binding.

- **First 3 experiments**:
  1. Implement and test skeleton-guided SDS with a simple 3DGS representation to verify improved view consistency and reduced artifacts.
  2. Implement and test the two-stage training process with a basic NeRF representation to verify effective canonical avatar generation and animatable avatar learning.
  3. Implement and test the H3GA representation with mesh binding for hands and face to verify improved geometric accuracy and animation quality.

## Open Questions the Paper Calls Out
- None specified in the provided content

## Limitations
- The paper's claims about superior geometry quality and appearance quality are based on user studies, but the methodology for these preference evaluations is not detailed in the abstract.
- The hybrid representation combining 3D Gaussians, neural implicit fields, and parameterized meshes represents a complex optimization challenge where joint training stability is not empirically demonstrated.
- The skeleton-guided SDS approach assumes skeleton images provide sufficient 3D-aware conditioning without overly restrictive shape priors, but the paper doesn't quantify the trade-off between consistency improvements and appearance fidelity loss.

## Confidence
- **High Confidence**: The two-stage training framework (canonical avatar generation followed by animatable avatar learning) is clearly specified and follows established patterns in 3D generation literature.
- **Medium Confidence**: The claim that skeleton guidance reduces multiple-face artifacts is supported by qualitative descriptions but lacks quantitative metrics or ablation studies comparing different conditioning strategies.
- **Low Confidence**: The assertion that H3GA enables "real-time rendering" while maintaining high quality is questionable without performance benchmarks (FPS measurements) or comparisons to existing real-time avatar systems.

## Next Checks
1. **Ablation Study on Conditioning Strategy**: Implement and compare SDS with skeleton guidance versus depth/normal map guidance versus no guidance, measuring view consistency metrics (e.g., LPIPS across viewpoints) and counting artifact occurrences (multiple faces, extra limbs).

2. **Joint Optimization Stability Analysis**: Monitor the training dynamics of the H3GA representation, tracking the relative contributions of 3D Gaussians, neural implicit fields, and mesh components across training epochs, with particular attention to gradient magnitudes and loss convergence patterns.

3. **Real-time Performance Benchmarking**: Measure actual rendering performance (FPS) of the H3GA avatar across different hardware configurations (GPU memory usage, inference latency) and compare against baseline 3DGS implementations with comparable visual quality metrics.