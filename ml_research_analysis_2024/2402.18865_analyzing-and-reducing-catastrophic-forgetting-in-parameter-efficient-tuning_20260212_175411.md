---
ver: rpa2
title: Analyzing and Reducing Catastrophic Forgetting in Parameter Efficient Tuning
arxiv_id: '2402.18865'
source_url: https://arxiv.org/abs/2402.18865
tags:
- learning
- accuracy
- mmlu
- lukaemon
- continual
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses catastrophic forgetting in parameter-efficient
  tuning of large language models (LLMs) during continual learning. The authors analyze
  mode connectivity in LLMs and propose I-LoRA, a dual-memory framework that interpolates
  between fast and slow learners to balance plasticity and stability.
---

# Analyzing and Reducing Catastrophic Forgetting in Parameter Efficient Tuning

## Quick Facts
- arXiv ID: 2402.18865
- Source URL: https://arxiv.org/abs/2402.18865
- Reference count: 22
- Primary result: I-LoRA achieves up to 11% performance gains over state-of-the-art approaches in LLM continual learning

## Executive Summary
This paper addresses catastrophic forgetting in parameter-efficient tuning of large language models during continual learning. The authors analyze mode connectivity in LLM parameter space and propose I-LoRA, a dual-memory framework that interpolates between fast and slow learners to balance plasticity and stability. Through extensive experiments on eight domain-specific benchmarks, I-LoRA demonstrates superior adaptation and memorization capabilities while maintaining comparable performance on general tasks.

## Method Summary
I-LoRA builds upon LoRA by introducing a dual-memory framework with fast and slow learners. The fast learner (working memory) quickly adapts to new tasks, while the slow learner (long-term memory) maintains historical knowledge through exponential moving average updates. The method uses experience replay buffers to store historical samples and applies embedding deviation loss to preserve task-specific representations. The framework interpolates between the two learners to balance adaptation and preservation, achieving state-of-the-art performance on continual learning benchmarks.

## Key Results
- Up to 11% performance gains over state-of-the-art approaches on domain-specific benchmarks
- Superior adaptation and memorization capabilities compared to baseline methods
- Maintains comparable performance on general tasks while significantly improving specialized domain performance
- Demonstrates strong baseline for future research in LLM continual learning

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Mode connectivity exists in LLM parameter space during continual learning
- Mechanism: Different task-specific minima can be connected by low-loss valleys, enabling interpolation between them
- Core assumption: The loss landscape contains smooth paths between optima for different tasks
- Evidence anchors:
  - [abstract] "we investigate the geometric connections of different minima through the lens of mode connectivity, which means different minima can be connected by a low-loss valley"
  - [section 3.1] "there exists a parametric path connecting the optima of historical tasks to that of the new task"
  - [corpus] Weak evidence - related papers focus on PEFT forgetting but not mode connectivity specifically
- Break condition: If the loss landscape becomes too rugged or contains sharp barriers between task minima

### Mechanism 2
- Claim: Dual-memory framework balances plasticity and stability
- Mechanism: Fast learner adapts quickly to new tasks while slow learner maintains historical knowledge through exponential moving average
- Core assumption: Separating adaptation speed into two modules prevents interference between new and old knowledge
- Evidence anchors:
  - [section 4.1] "The framework comprises a fast learner (parameterized by working memory θw) and a slow learner (parameterized by long-term memory θl)"
  - [section 4.2] "the slow learner will be updated as an exponential moving average of the fast learner weights"
  - [corpus] Moderate evidence - related work on CUR decomposition and parameter-efficient methods suggests similar approaches exist
- Break condition: If the interpolation factor λ is poorly chosen, causing either excessive forgetting or inability to adapt

### Mechanism 3
- Claim: Embedding deviation loss preserves task-specific representations
- Mechanism: MSE loss between fast learner and slow learner embeddings on historical data maintains representation consistency
- Core assumption: Historical task representations are important for maintaining performance and can be preserved through embedding-level supervision
- Evidence anchors:
  - [section 4.2] "the latter objective is implemented as the MSE loss on embeddings"
  - [section 5.3] "we further examine the produced representation space" using CKA similarity
  - [corpus] Weak evidence - corpus neighbors don't discuss embedding preservation strategies
- Break condition: If historical data distribution shifts significantly from current data, making old embeddings less relevant

## Foundational Learning

- Concept: Mode connectivity in neural networks
  - Why needed here: Understanding that different optima can be connected by low-loss paths is crucial for the theoretical foundation of I-LoRA
  - Quick check question: What does it mean for two minima to be "mode connected" in neural network training?

- Concept: Catastrophic forgetting in continual learning
  - Why needed here: The problem I-LoRA addresses is forgetting previously learned tasks when learning new ones
  - Quick check question: What is the trade-off between plasticity and stability in continual learning?

- Concept: Parameter-efficient fine-tuning (PEFT) with LoRA
  - Why needed here: I-LoRA builds upon LoRA by adding dual-memory and interpolation mechanisms
  - Quick check question: How does LoRA achieve parameter efficiency in fine-tuning large language models?

## Architecture Onboarding

- Component map:
  - Fast learner: LoRA module that quickly adapts to new tasks (working memory)
  - Slow learner: LoRA module that preserves historical knowledge through exponential moving average (long-term memory)
  - Experience replay buffer: Stores historical samples for embedding preservation
  - Embedding extractor: Part of the model that maps inputs to representation space

- Critical path:
  1. Sample from current task and historical buffer
  2. Compute classification loss on current task
  3. Compute embedding deviation loss on historical samples
  4. Update fast learner with combined loss
  5. Update slow learner as exponential moving average of fast learner

- Design tradeoffs:
  - Memory buffer size vs. forgetting mitigation effectiveness
  - Interpolation factor λ vs. balance between adaptation and preservation
  - Embedding deviation weight γ vs. computational overhead

- Failure signatures:
  - Performance on historical tasks degrades rapidly → λ too low or buffer too small
  - Poor adaptation to new tasks → λ too high or fast learner learning rate too low
  - Excessive computational overhead → buffer size or embedding computation frequency too high

- First 3 experiments:
  1. Ablation study: Remove slow learner to confirm its contribution to stability
  2. Interpolation sensitivity: Vary λ across a range to find optimal value
  3. Buffer size impact: Test different historical sample counts to optimize memory-accuracy tradeoff

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the mode connectivity phenomenon scale with increasing model size and task complexity in LLMs?
- Basis in paper: [explicit] The paper mentions investigating mode connectivity in continual fine-tuning scenarios of LLMs, but doesn't explore scaling effects
- Why unresolved: The study focuses on Llama-2-7B and domain-specific benchmarks; no analysis of how mode connectivity changes with model scale or task complexity
- What evidence would resolve it: Comparative studies across different model sizes (e.g., 7B, 13B, 70B) and task complexities showing mode connectivity patterns

### Open Question 2
- Question: What is the theoretical foundation connecting mode connectivity to catastrophic forgetting in LLMs?
- Basis in paper: [inferred] The paper leverages mode connectivity empirically but doesn't provide theoretical analysis of why it reduces forgetting
- Why unresolved: While empirical results show benefits, the paper doesn't explore the mathematical relationship between mode connectivity and forgetting
- What evidence would resolve it: Formal proofs or theoretical framework linking mode connectivity properties to forgetting reduction mechanisms

### Open Question 3
- Question: How does the interpolation parameter λ affect the trade-off between plasticity and stability across different types of tasks?
- Basis in paper: [explicit] The paper varies λ in experiments but doesn't systematically analyze its effects across task types
- Why unresolved: The study shows λ affects performance but doesn't provide guidelines for λ selection based on task characteristics
- What evidence would resolve it: Task-specific λ recommendations based on systematic analysis of task properties (domain, complexity, language, etc.)

### Open Question 4
- Question: What is the minimum amount of historical data needed for effective mode connectivity-based continual learning?
- Basis in paper: [inferred] The paper uses experience replay but doesn't analyze the relationship between historical data amount and mode connectivity effectiveness
- Why unresolved: The study doesn't explore how varying amounts of historical data affect the ability to maintain mode connectivity
- What evidence would resolve it: Experiments showing performance degradation as historical data decreases, identifying minimum effective data requirements

## Limitations
- Mode connectivity assumption may not hold for all LLM architectures or highly dissimilar tasks
- Reliance on experience replay buffers introduces memory constraints and potential data distribution shifts
- Optimal hyperparameters (λ, γ, buffer size) may be task-dependent requiring extensive tuning

## Confidence

- **High Confidence**: The core mechanism of dual-memory framework with fast and slow learners, and the empirical improvements on domain-specific benchmarks (up to 11% gains on 8 datasets)
- **Medium Confidence**: The theoretical foundation of mode connectivity in LLM parameter space, as the evidence is primarily indirect through ablation studies and related work
- **Medium Confidence**: The generalization to general benchmarks (MMLU, BBH, PIQA), as the improvements are modest (1.1-1.5%) and may not justify the additional complexity

## Next Checks

1. **Architecture Generalization Test**: Apply I-LoRA to different LLM architectures (beyond Llama-2-7B) such as Mistral or Vicuna to verify the mode connectivity assumption holds across various model families and sizes.

2. **Task Dissimilarity Stress Test**: Design experiments with highly dissimilar tasks (e.g., combining vision-language tasks with pure text tasks) to evaluate whether the low-loss valley assumption breaks down and I-LoRA performance degrades.

3. **Long-Term Stability Evaluation**: Extend training to 20+ tasks and measure performance degradation over time, particularly examining whether the exponential moving average mechanism maintains stability or if catastrophic forgetting re-emerges in extended continual learning scenarios.