---
ver: rpa2
title: 'Gamba: Marry Gaussian Splatting with Mamba for single view 3D reconstruction'
arxiv_id: '2403.18795'
source_url: https://arxiv.org/abs/2403.18795
tags:
- arxiv
- reconstruction
- gamba
- image
- gaussians
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes Gamba, an end-to-end single-view 3D reconstruction
  model based on 3D Gaussian Splatting (3DGS) and Mamba. The core idea is to model
  3DGS reconstruction as sequential prediction with linear scalability of token length,
  enabling efficient processing of a large number of Gaussians.
---

# Gamba: Marry Gaussian Splatting with Mamba for single view 3D reconstruction

## Quick Facts
- arXiv ID: 2403.18795
- Source URL: https://arxiv.org/abs/2403.18795
- Authors: Qiuhong Shen; Zike Wu; Xuanyu Yi; Pan Zhou; Hanwang Zhang; Shuicheng Yan; Xinchao Wang
- Reference count: 40
- Single-view 3D reconstruction with 3D Gaussian Splatting using Mamba, achieving ~1000× faster inference than optimization-based methods

## Executive Summary
Gamba introduces an end-to-end single-view 3D reconstruction model that combines 3D Gaussian Splatting with Mamba's linear scalability. The model uses a Mamba-based GambaFormer network to predict Gaussian parameters sequentially, eliminating the need for multi-stage training and point cloud supervision. By employing radial mask constraints derived from multi-view masks and leveraging Mamba's efficient sequence processing, Gamba achieves competitive reconstruction quality while running in just 0.05 seconds per object on a single GPU, representing a dramatic speed improvement over existing optimization-based approaches.

## Method Summary
Gamba uses a single RGB image and camera pose as input, processing them through a DINO ViT tokenizer to extract image tokens. These tokens, along with camera embeddings, are fed into a GambaFormer network consisting of 14 Mamba blocks with prepending operations. The 512x512 image is expanded to 16,384 tokens through convolution and four scan orders, then decoded into 3D Gaussian parameters (position, scale, rotation, color, opacity) via a Gaussian decoder MLP. The model is trained on the Objaverse dataset using MSE, LPIPS, and radial mask constraint losses, with progressive view training from 2 to 6 views. The output is rendered using 3DGS differentiable rasterization for novel view synthesis.

## Key Results
- Achieves competitive PSNR, SSIM, and LPIPS metrics compared to state-of-the-art methods
- Demonstrates 0.05 second inference time per object, approximately 1000× faster than optimization-based approaches
- Eliminates need for point cloud supervision through innovative radial mask constraint technique
- Successfully trains end-to-end without multi-stage warm-up procedures

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Mamba's linear scalability enables end-to-end reconstruction of 16384 Gaussians without multi-stage training.
- Mechanism: GambaFormer uses Mamba blocks to model the sequential prediction of Gaussian tokens, replacing the quadratic attention of transformers. The scan-based token expansion preserves spatial structure while maintaining linear complexity.
- Core assumption: Sequential Gaussian densification during 3DGS reconstruction can be approximated as a unidirectional token prediction task.
- Evidence anchors:
  - [abstract] "introducing a Mamba-based GambaFormer network to model 3D Gaussian Splatting (3DGS) reconstruction as sequential prediction with linear scalability of token length"
  - [section 3.2] "our GambaFormer architecture obviates this two-stage training paradigm by leveraging the linear complexity of state space models"
  - [corpus] Weak evidence: no directly comparable models in corpus, but Mamba's linear scaling is established in other domains
- Break condition: If the sequential prediction assumption fails for complex geometry, the model may collapse to predicting only visible surface Gaussians.

### Mechanism 2
- Claim: Radial mask constraints eliminate the need for explicit point cloud supervision during training.
- Mechanism: The model uses 2D ray casting from image center to mask contours to create distance fields. Gaussians projected outside these contours receive explicit positional loss, guiding them to plausible 3D locations without 3D supervision.
- Core assumption: Multi-view masks encode sufficient 3D occupancy information for guiding Gaussian placement.
- Evidence anchors:
  - [abstract] "deriving radial mask constraints from multi-view masks to eliminate the need for warmup supervision of 3D point clouds in training"
  - [section 3.3] "we introduce a radial mask constraint, inspired by the notion that images from multiple viewpoints can depict the occupied 3D space of an object"
  - [corpus] Weak evidence: radial constraints are novel for this application, but distance field guidance exists in other 3D contexts
- Break condition: If object masks are noisy or incomplete, the radial constraint may push Gaussians to incorrect positions.

### Mechanism 3
- Claim: Prepending camera and image tokens to Mamba input enables conditional prediction of Gaussians.
- Mechanism: Camera and image tokens are projected and added to the beginning of each layer's input sequence, providing global context for Gaussian prediction while maintaining Mamba's unidirectional processing.
- Core assumption: Global context from camera and image tokens can be effectively propagated through Mamba's state space layers.
- Evidence anchors:
  - [section 3.2] "The operation Prepend involves adding projected camera embeddings and image tokens to the beginning of the sequence before processing through the hidden 3DGS features"
  - [section 3.3] "removing this component leads to a 1.24 dB drop in terms of PSNR, which demonstrates the necessity of this prepending operation"
  - [corpus] No direct evidence; this is a novel adaptation of Mamba for conditional prediction
- Break condition: If the prepending operation disrupts Mamba's state space dynamics, prediction quality may degrade.

## Foundational Learning

- Concept: State Space Models (SSMs) and Mamba architecture
  - Why needed here: Mamba's linear complexity with sequence length enables processing 16384 Gaussians without memory explosion
  - Quick check question: What is the computational complexity difference between Mamba and transformer attention for sequence length N?

- Concept: 3D Gaussian Splatting representation and differentiable rasterization
  - Why needed here: Understanding how Gaussians are parameterized (position, scale, rotation, color, opacity) and rendered is crucial for implementing the decoder and loss functions
  - Quick check question: How does the anisotropic Gaussian covariance matrix Σ relate to scale r and rotation quaternion q?

- Concept: Multi-view consistency and mask-based constraints
  - Why needed here: The radial mask constraint relies on understanding how multiple views constrain 3D geometry and how distance fields can guide Gaussian placement
  - Quick check question: Why can radial distance fields from multiple views provide 3D geometric constraints?

## Architecture Onboarding

- Component map: Single image + camera pose -> DINO ViT tokenizer -> GambaFormer (14 Mamba blocks) -> 3DGS tokens expansion -> Gaussian decoder MLP -> 3DGS differentiable renderer -> loss functions
- Critical path: Image → tokens → GambaFormer → Gaussians → render → loss → gradients
- Design tradeoffs:
  - Mamba vs Transformer: Linear scaling vs quadratic attention, unidirectional vs bidirectional
  - End-to-end vs two-stage: Simpler training vs potentially better initialization
  - Radial mask vs point cloud: No 3D supervision needed vs potentially less precise geometry
- Failure signatures:
  - Model collapse: Gaussians converge to small sphere with zero opacity (loss of radial constraint)
  - Blurry textures: Insufficient information in 3DGS tokens or poor decoder conditioning
  - Geometric distortion: Radial mask constraints failing on complex shapes
- First 3 experiments:
  1. Train with λrdist=0 to verify radial mask constraint prevents model collapse
  2. Replace Mamba with Transformer (L=4096) to confirm linear scaling benefit
  3. Remove prepending operation to validate its contribution to reconstruction quality

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of Gamba scale with an increasing number of Gaussians beyond the current 16,384 limit?
- Basis in paper: [explicit] The paper discusses that 87% of 3D objects in their dataset have between 10,000 and 20,000 Gaussians, suggesting the current limit is sufficient. However, they also acknowledge that the scalability of the Gamba model remains underexplored.
- Why unresolved: The paper only tests with 16,384 Gaussians and does not explore the effects of using more or fewer Gaussians on reconstruction quality and efficiency.
- What evidence would resolve it: Experiments comparing reconstruction quality and inference speed using varying numbers of Gaussians (e.g., 8,192, 32,768, 65,536) would show the optimal balance between quality and efficiency.

### Open Question 2
- Question: Can Gamba be adapted to handle multiple input images instead of a single image, and how would this affect its performance?
- Basis in paper: [inferred] The paper focuses on single-view reconstruction, but mentions that previous methods like AGG and Triplane-Meets-Gaussian use multi-view images for better reconstruction. This suggests potential for multi-view adaptation.
- Why unresolved: The paper does not explore multi-view input scenarios or compare performance with single-view input.
- What evidence would resolve it: Experiments using Gamba with multiple input images from different viewpoints and comparing the reconstruction quality and speed with the single-view version would demonstrate the benefits and limitations of multi-view input.

### Open Question 3
- Question: How does Gamba's performance compare to other methods when trained on different datasets, such as those with more diverse object categories or higher resolution images?
- Basis in paper: [explicit] The paper trains Gamba on the Objaverse dataset and evaluates it on the GSO Dataset. It mentions that LRM, a baseline method, is trained on a dataset 50× larger than theirs, yet Gamba still performs competitively.
- Why unresolved: The paper does not test Gamba on other datasets or explore the effects of dataset diversity and image resolution on its performance.
- What evidence would resolve it: Training and evaluating Gamba on various datasets with different object categories and image resolutions, then comparing the results with other methods, would show its generalizability and robustness.

## Limitations
- Limited evaluation scope on only 60 GSO objects raises questions about generalization to complex real-world scenes
- Radial mask constraint relies on clean multi-view masks that may not be available in practical scenarios
- Fixed Gaussian count of 16384 may be suboptimal for objects with varying complexity

## Confidence

**High Confidence**: The speed claims (0.05s inference time) and linear scalability benefits of Mamba are well-supported by the architecture and comparison with optimization-based methods. The ablation study results for component removal are clear and reproducible.

**Medium Confidence**: The reconstruction quality metrics (PSNR, SSIM, LPIPS) are reliable but may not fully capture perceptual quality. The Chamfer Distance and Volume IoU metrics for geometry reconstruction are less comprehensive than what would be ideal for evaluating 3D reconstruction fidelity.

**Low Confidence**: The generalization claims to real-world scenarios are weak given the limited evaluation scope. The effectiveness of radial mask constraints in noisy mask conditions has not been thoroughly tested.

## Next Checks

1. **Cross-dataset generalization test**: Evaluate Gamba on a broader range of datasets including real-world scenes with varying complexity, object counts, and environmental conditions to validate generalization claims beyond the curated GSO dataset.

2. **Robustness to mask quality**: Systematically degrade input mask quality (adding noise, occlusions, partial views) and measure reconstruction degradation to quantify the model's sensitivity to the radial mask constraint assumption.

3. **Memory and speed scaling analysis**: Measure actual memory consumption and inference time across different Gaussian counts (4096, 8192, 32768) to empirically validate the claimed linear scaling benefits and identify practical limits of the Mamba architecture for this application.