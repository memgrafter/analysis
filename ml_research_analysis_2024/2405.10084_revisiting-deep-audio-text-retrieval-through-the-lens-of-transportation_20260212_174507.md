---
ver: rpa2
title: Revisiting Deep Audio-Text Retrieval Through the Lens of Transportation
arxiv_id: '2405.10084'
source_url: https://arxiv.org/abs/2405.10084
tags:
- audio
- loss
- learning
- m-ltm
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proposes a mini-batch Learning-to-Match (m-LTM) framework
  for audio-text retrieval, addressing the scalability issues of the conventional
  LTM framework. The m-LTM leverages mini-batch subsampling and Mahalanobis-enhanced
  ground metrics to learn a rich and expressive joint embedding space.
---

# Revisiting Deep Audio-Text Retrieval Through the Lens of Transportation

## Quick Facts
- arXiv ID: 2405.10084
- Source URL: https://arxiv.org/abs/2405.10084
- Reference count: 31
- The paper proposes mini-batch Learning-to-Match (m-LTM) framework achieving state-of-the-art performance in audio-text retrieval tasks and improved noise tolerance.

## Executive Summary
This paper addresses the scalability challenges of the Learning-to-Match (LTM) framework for audio-text retrieval by introducing a mini-batch Learning-to-Match (m-LTM) approach. The method leverages mini-batch subsampling and Mahalanobis-enhanced ground metrics to learn a rich joint embedding space while maintaining computational efficiency. A variant using partial optimal transport is also introduced to handle misaligned training data. Experimental results on three datasets (AudioCaps, Clotho, and ESC-50) demonstrate superior performance compared to baseline methods, with the framework achieving state-of-the-art retrieval results and improved noise tolerance.

## Method Summary
The m-LTM framework addresses scalability issues in deep audio-text retrieval by using mini-batch subsampling instead of processing the entire dataset for each parameter update. It employs a Mahalanobis distance metric as the ground metric to reduce modality gaps between audio and text embeddings, and introduces partial optimal transport to handle noisy correspondence data. The framework uses stochastic gradient descent with mini-batches, Sinkhorn algorithm for entropic optimal transport, and projected gradient descent for updating the Mahalanobis matrix. The method is evaluated on three datasets using log mel-spectrograms and BERT embeddings, with training performed for 30 epochs using hybrid stochastic gradient descent.

## Key Results
- Achieves state-of-the-art performance in audio-text retrieval on AudioCaps, Clotho, and ESC-50 datasets
- Outperforms triplet and contrastive loss in zero-shot sound event detection on ESC-50
- Demonstrates greater noise tolerance than baseline methods on the AudioCaps dataset

## Why This Works (Mechanism)

### Mechanism 1
The mini-batch Learning-to-Match (m-LTM) framework scales optimal transport to large datasets by using mini-batches instead of the entire dataset for each parameter update. By subsampling mini-batches, the computational cost of solving the entropic optimal transport problem becomes manageable while still providing stochastic gradient estimates that converge to a local minimum. The core assumption is that mini-batch samples are representative enough of the full dataset distribution for gradient estimation.

### Mechanism 2
Using Mahalanobis distance as the ground metric reduces the modality gap between audio and text embeddings, improving downstream task transferability. Mahalanobis distance accounts for scaling differences across dimensions by incorporating a positive definite matrix M, allowing the embedding spaces to align more naturally. The core assumption is that scaling differences between audio and text embedding dimensions are systematic and can be captured by a learnable matrix M.

### Mechanism 3
Partial optimal transport (POT) with m-LTM handles noisy correspondence training data by discarding mismatched pairs from the transportation plan. POT relaxes the transportation preservation constraint, allowing the model to exclude noisy pairs by not assigning them transportation mass, thus preventing them from corrupting the learned embedding space. The core assumption is that noisy pairs in training data can be identified by their low transportation mass in the optimal plan.

## Foundational Learning

- **Optimal Transport (OT) and Entropic Regularization**
  - Why needed here: The m-LTM framework is built on solving an optimal transport problem to learn the ground metric between audio and text embeddings.
  - Quick check question: What is the computational advantage of using entropic regularization in optimal transport problems?

- **Mahalanobis Distance**
  - Why needed here: It's used as a more flexible ground metric than Euclidean distance to account for scaling differences between modalities.
  - Quick check question: How does Mahalanobis distance differ from Euclidean distance in terms of invariance properties?

- **Partial Optimal Transport**
  - Why needed here: It's the key mechanism for handling noisy correspondence data by allowing incomplete transportation plans.
  - Quick check question: What constraint does partial OT relax compared to standard OT, and why is this useful for noisy data?

## Architecture Onboarding

- **Component map**: Audio encoder (fθ) -> Text encoder (gϕ) -> Mahalanobis matrix (M) -> Sinkhorn algorithm -> Stochastic gradient descent
- **Critical path**: 1. Extract audio and text embeddings 2. Compute pairwise distances using Mahalanobis metric 3. Solve entropic OT problem to get soft matching 4. Compute KL divergence loss against identity matching 5. Backpropagate gradients to update encoders and M
- **Design tradeoffs**: Mini-batch size vs. computational efficiency; Entropy regularization coefficient (ε) vs. matching sharpness; Mahalanobis matrix complexity vs. overfitting
- **Failure signatures**: High modality gap despite Mahalanobis distance; Poor performance on noisy data even with POT; Gradient instability during training
- **First 3 experiments**: 1. Compare m-LTM with Euclidean distance vs. Mahalanobis distance on validation set 2. Test different ε values (0.01, 0.05, 0.1, 0.5) to find optimal entropy regularization strength 3. Evaluate m-LTM with and without POT on datasets with controlled noise injection

## Open Questions the Paper Calls Out
None explicitly stated in the paper.

## Limitations
- The effectiveness of Mahalanobis distance in reducing modality gaps may not generalize to more complex cross-modal relationships
- Partial optimal transport's noise handling capability needs more thorough exploration across different noise distributions and levels
- Computational benefits of mini-batch subsampling lack detailed analysis of how batch size affects convergence speed and final performance

## Confidence

- **High confidence**: Core claim that m-LTM outperforms baseline methods on tested datasets (supported by comprehensive experimental results)
- **Medium confidence**: Mechanism explanations for Mahalanobis distance reducing modality gaps and POT handling noisy data (empirical evidence limited to indirect metrics)
- **Low confidence**: Scalability claims without extensive ablation studies on different dataset sizes and mini-batch configurations

## Next Checks
1. **Modality Gap Analysis**: Conduct systematic study varying Mahalanobis matrix complexity to determine minimum effective model complexity and test simpler alternatives
2. **Noise Robustness Evaluation**: Test m-LTM with POT on datasets with structured noise rather than random noise to evaluate real-world robustness
3. **Computational Efficiency Benchmark**: Measure wall-clock training time and memory usage across different mini-batch sizes and dataset scales to quantify claimed scalability improvements