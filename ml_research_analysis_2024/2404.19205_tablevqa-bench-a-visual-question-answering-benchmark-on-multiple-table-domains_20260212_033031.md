---
ver: rpa2
title: 'TableVQA-Bench: A Visual Question Answering Benchmark on Multiple Table Domains'
arxiv_id: '2404.19205'
source_url: https://arxiv.org/abs/2404.19205
tags:
- table
- arxiv
- performance
- mllms
- images
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces TableVQA-Bench, a benchmark dataset for table
  visual question answering that combines images, text representations, and QA pairs.
  The authors create this dataset by leveraging existing table QA and table structure
  recognition datasets, generating images through a rendering system or by applying
  stylesheets, and using GPT-4 to generate QA pairs from text-formatted tables.
---

# TableVQA-Bench: A Visual Question Answering Benchmark on Multiple Table Domains

## Quick Facts
- **arXiv ID:** 2404.19205
- **Source URL:** https://arxiv.org/abs/2404.19205
- **Reference count:** 32
- **Primary result:** Introduces TableVQA-Bench, a benchmark dataset for table visual question answering

## Executive Summary
This paper introduces TableVQA-Bench, a benchmark dataset for table visual question answering that combines images, text representations, and QA pairs. The authors create this dataset by leveraging existing table QA and table structure recognition datasets, generating images through a rendering system or by applying stylesheets, and using GPT-4 to generate QA pairs from text-formatted tables. TableVQA-Bench consists of 1,500 QA pairs across four domains: VWTQ, VWTQ-Syn, VTabFact, and FinTabNetQA. Experiments on TableVQA-Bench show that GPT-4V achieves the highest accuracy among commercial and open-sourced MLLMs, highlighting the importance of vision queries and the challenge of processing visual inputs compared to text inputs.

## Method Summary
The authors create TableVQA-Bench by leveraging existing table QA and table structure recognition datasets, generating images through a rendering system or by applying stylesheets, and using GPT-4 to generate QA pairs from text-formatted tables. The dataset consists of 1,500 QA pairs across four domains: VWTQ, VWTQ-Syn, VTabFact, and FinTabNetQA. Experiments on TableVQA-Bench show that GPT-4V achieves the highest accuracy among commercial and open-sourced MLLMs.

## Key Results
- TableVQA-Bench achieves 1,500 QA pairs across four table domains
- GPT-4V demonstrates highest accuracy among tested commercial and open-sourced MLLMs
- Benchmark highlights challenges in processing visual table inputs versus text representations

## Why This Works (Mechanism)
The benchmark's effectiveness stems from combining multiple table domains with realistic rendering techniques and leveraging large language models for QA generation, creating a comprehensive evaluation framework for table understanding.

## Foundational Learning
1. **Table structure recognition**: Understanding how tables are visually organized is essential for accurate question answering. Quick check: Can the model identify headers, rows, and columns correctly?
2. **Multi-modal learning**: Combining vision and language processing capabilities is crucial for interpreting table images. Quick check: Does the model maintain accuracy when switching between image and text inputs?
3. **Question generation with LLMs**: Using GPT-4 to generate QA pairs provides diverse and natural language queries. Quick check: Are generated questions varied in complexity and type?

## Architecture Onboarding
**Component Map:** Image Renderer -> Table Extractor -> GPT-4 QA Generator -> Evaluation Framework
**Critical Path:** Image generation → Text extraction → QA pair generation → Model evaluation
**Design Tradeoffs:** Using existing datasets ensures diversity but may limit domain specificity; GPT-4 generation provides scalability but raises quality control concerns
**Failure Signatures:** Poor image quality affecting table structure recognition; inconsistent QA pair quality from GPT-4 generation
**First Experiments:**
1. Test model performance on single-domain versus multi-domain inputs
2. Evaluate impact of image resolution on answer accuracy
3. Compare performance between text-only and image-based table inputs

## Open Questions the Paper Calls Out
None

## Limitations
- Data quality concerns from GPT-4 generated QA pairs without human validation
- Limited diversity from reliance on existing table datasets
- Unclear train-test split methodology for benchmark evaluation

## Confidence
- **High:** Experimental methodology for evaluating MLLMs is sound
- **Medium:** Dataset creation pipeline and domain selection are reasonable
- **Low:** Claims about GPT-4V's highest accuracy need verification on held-out test sets

## Next Checks
1. Conduct human evaluation of a random sample of 100 QA pairs from each domain to assess answer correctness and question quality
2. Perform ablation studies to determine the impact of image quality variations on model performance
3. Test the benchmark on additional MLLM models beyond those mentioned, particularly newer models that have emerged since the study was conducted