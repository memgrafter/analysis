---
ver: rpa2
title: 'SCALM: Towards Semantic Caching for Automated Chat Services with Large Language
  Models'
arxiv_id: '2406.00025'
source_url: https://arxiv.org/abs/2406.00025
tags:
- cache
- semantic
- patterns
- queries
- ratio
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work addresses the challenge of improving cache performance
  for LLM-based chat services by leveraging semantic analysis of real-world human-LLM
  interaction data. The authors propose SCALM, a semantics-oriented cache architecture
  that identifies significant query patterns through hierarchical semantic clustering,
  selectively caches queries based on their semantic ranks, and dynamically adjusts
  storage and eviction strategies.
---

# SCALM: Towards Semantic Caching for Automated Chat Services with Large Language Models

## Quick Facts
- arXiv ID: 2406.00025
- Source URL: https://arxiv.org/abs/2406.00025
- Authors: Jiaxing Li; Chi Xu; Feng Wang; Isaac M von Riedemann; Cong Zhang; Jiangchuan Liu
- Reference count: 39
- Primary result: 63% relative increase in cache hit ratio and 77% relative improvement in token savings compared to state-of-the-art GPTCache solutions

## Executive Summary
SCALM addresses the challenge of improving cache performance for LLM-based chat services by leveraging semantic analysis of real-world human-LLM interaction data. The approach uses hierarchical semantic clustering to identify significant query patterns, then selectively caches queries based on semantic ranks and token-saving metrics. Evaluations demonstrate substantial improvements over state-of-the-art solutions, with stable performance across varying conversation scales and cache sizes.

## Method Summary
SCALM implements a semantic-oriented cache architecture that converts user queries into vectors using text-embedding-3-small, then applies hierarchical semantic clustering (CO-HSC and SE-HSC) to identify significant cache entries and patterns. The system calculates token saving ratios to determine which queries to cache, using semantic pattern ranks (high=3, mid=2, low=1) combined with frequency-based metrics for cache replacement decisions. The approach processes multi-turn conversations by treating each round separately, capturing semantic evolution across conversation contexts.

## Key Results
- 63% relative increase in cache hit ratio compared to GPTCache baseline
- 77% relative improvement in token savings over state-of-the-art solutions
- Stable performance across varying conversation scales (5000 to 25000) and cache sizes (20 to 200)

## Why This Works (Mechanism)

### Mechanism 1
Semantic clustering reduces cache misses by grouping semantically similar queries into patterns that can be served with the same response. SCALM uses hierarchical semantic clustering to identify semantic patterns across conversation rounds, then selectively caches queries based on pattern ranks and token-saving metrics. Core assumption: Queries within the same semantic pattern share similar responses. Evidence: Weak - no direct evidence that similar queries always share responses; depends on embedding quality.

### Mechanism 2
Selective caching based on token-saving metrics improves cache efficiency compared to simple hit ratio optimization. SCALM calculates token saving ratio and uses this along with pattern ranks to decide which queries to cache. Core assumption: Longer queries with fewer repetitions can still yield higher total token savings than shorter, more frequent queries. Evidence: Weak - no empirical data showing token-saving metric consistently outperforms hit ratio in real deployments.

### Mechanism 3
Hierarchical clustering across conversation rounds captures evolving context better than single-round clustering. CO-HSC processes each round separately, building semantic patterns that