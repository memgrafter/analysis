---
ver: rpa2
title: 'QMOS: Enhancing LLMs for Telecommunication with Question Masked loss and Option
  Shuffling'
arxiv_id: '2409.14175'
source_url: https://arxiv.org/abs/2409.14175
tags:
- language
- phi-2
- performance
- accuracy
- question
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of applying Large Language Models
  (LLMs) to specialized domains like telecommunications, where domain-specific terminology
  and technical complexity can hinder performance. The authors introduce QMOS, a novel
  approach combining Question-Masked loss and Option Shuffling to enhance LLMs' performance
  in answering Multiple-Choice Questions in the telecommunications domain.
---

# QMOS: Enhancing LLMs for Telecommunication with Question Masked loss and Option Shuffling

## Quick Facts
- arXiv ID: 2409.14175
- Source URL: https://arxiv.org/abs/2409.14175
- Reference count: 29
- Primary result: QMOS achieves 84.65% accuracy on telecommunications MCQs using Phi-2, compared to 42.07% baseline

## Executive Summary
This paper addresses the challenge of applying Large Language Models (LLMs) to specialized domains like telecommunications, where domain-specific terminology and technical complexity can hinder performance. The authors introduce QMOS, a novel approach combining Question-Masked loss and Option Shuffling to enhance LLMs' performance in answering Multiple-Choice Questions in the telecommunications domain. Focusing on open-source, smaller models (Phi-2 and Falcon-7B) within an improved Retrieval-Augmented Generation (RAG) framework, the study incorporates several enhancements including diversified embedding models, prompt engineering, and Low-Rank Adaptation (LoRA) fine-tuning. The results demonstrate significant improvements over existing baselines, achieving accuracy increases from 24.70% to 49.30% with Falcon-7B and from 42.07% to 84.65% with Phi-2. These findings suggest that small language models, when combined with advanced techniques, can achieve competitive performance compared to larger, proprietary models while maintaining efficiency and reducing computational costs.

## Method Summary
QMOS combines multiple innovations: an improved RAG system using custom chunking, multiple embedding models (stella en 400M v5 and gte-Qwen2-1.5B-instruct) with BM25, and prompt engineering with abbreviation expansion. The approach employs LoRA fine-tuning with a novel Question-Masked loss function that focuses optimization on answer tokens rather than question tokens. For Phi-2 models, an Option Batch-Shuffle technique presents multiple permutations of answer options during inference to eliminate positional bias. The method is evaluated on the TeleQnA dataset containing telecommunications multiple-choice questions, demonstrating significant accuracy improvements over baseline models.

## Key Results
- QMOS achieves 84.65% accuracy on the private test set using Phi-2, compared to 42.07% baseline
- Falcon-7B with QMOS reaches 49.30% accuracy, up from 24.70% baseline
- Question-Masked loss improves validation accuracy during fine-tuning while standard cross-entropy plateaus
- Option Batch-Shuffle reduces model bias toward specific answer positions in multiple-choice questions

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The Question-Masked loss function improves fine-tuning efficiency by focusing optimization on answer tokens rather than question tokens, leading to higher validation accuracy despite lower validation loss.
- Mechanism: During fine-tuning, the cross-entropy loss is masked so that only tokens corresponding to the answer contribute to the loss. This prevents the model from over-optimizing its ability to reconstruct the question at the expense of generating correct answers.
- Core assumption: The question part of the prompt is already well-learned from pre-training, and the model's primary weakness lies in answer generation, not question comprehension.
- Evidence anchors:
  - [section] "Figure 2 shows no improvement in the validation accuracy even as the validation loss decreases. We suspected that this decrease in validation loss results from the model getting better at predicting the question and not the answers."
  - [section] "Figure 3 shows the result obtained when the loss associated with questions is masked out, allowing the training objective to focus only on the answers. Figure 3 shows that the validation accuracy increases as the validation loss decreases."
- Break condition: If the question comprehension is the bottleneck rather than answer generation, masking question tokens would degrade performance. Also, if the answer tokens are too sparse or ambiguous, the masked loss may not provide sufficient learning signals.

### Mechanism 2
- Claim: The Option Batch-Shuffle trick eliminates selection bias in LLMs by presenting multiple option orderings and selecting the most frequently chosen answer, thereby improving accuracy.
- Mechanism: For each multiple-choice question, multiple prompts are generated with different permutations of the answer options. The model generates answers for all permutations, and the answer chosen most frequently across permutations is selected as the final answer. This mitigates the model's tendency to favor certain answer positions.
- Core assumption: LLMs exhibit systematic positional bias in multiple-choice questions, consistently favoring certain answer positions regardless of content correctness.
- Evidence anchors:
  - [abstract] "We implement an innovative optimization technique: inference and train time option batch-shuffling, which enhances the accuracy of the Phi-2 model by eliminating bias in option position of correct answers."
  - [section] "Recent research [2], [5], [27] has unveiled a significant bias in LLMs when answering multiple-choice questions (MCQs). These models exhibit a strong sensitivity to the order of options, often selecting specific answer positions regardless of the content."
- Break condition: If the model's selection bias is minimal or non-existent, the computational overhead of generating multiple permutations would not justify the marginal accuracy gains. Also, if the model's answer consistency across permutations is low, the most frequent answer may not represent the correct answer.

### Mechanism 3
- Claim: Enhanced RAG with diversified embedding models and custom chunking improves retrieval quality for telecom-specific questions, leading to better context for answer generation.
- Mechanism: The RAG system uses multiple embedding models (stella en 400M v5 and gte-Qwen2-1.5B-instruct) and BM25 retrieval to capture different aspects of semantic and lexical similarity. Custom chunking preserves document structure by starting each chunk with its section heading, ensuring that retrieved contexts maintain coherent topical information.
- Core assumption: Telecom domain questions benefit from both semantic understanding (captured by neural embeddings) and exact term matching (captured by BM25), and that document structure preservation improves context relevance.
- Evidence anchors:
  - [section] "We employed a custom document splitting strategy where we first split the documents into individual sections, excluding the table of contents. Each section is further split into chunks containing chunk-size characters... Additionally we ensured that each chunk begins with the heading of the document section to which it belongs by prepending this heading to each chunk."
  - [section] "Additionally, we employed the use of the BM25 [20] algorithm which is a statistical approach for information retrieval that measures similarity based on the frequency of terms from the query that appear in the chunks."
- Break condition: If the additional embedding models and BM25 retrieval significantly increase latency without proportional accuracy gains, or if the custom chunking strategy introduces boundary artifacts that fragment coherent information.

## Foundational Learning

- Concept: Retrieval-Augmented Generation (RAG)
  - Why needed here: Telecom domain knowledge is too specialized and rapidly evolving to be fully captured in model pre-training, requiring external document retrieval for accurate answers.
  - Quick check question: What are the two main components of a RAG system, and how do they interact during inference?

- Concept: Low-Rank Adaptation (LoRA)
  - Why needed here: Fine-tuning large language models on small telecom datasets requires parameter-efficient methods to avoid overfitting while adapting to domain-specific terminology.
  - Quick check question: How does LoRA reduce the number of trainable parameters compared to full fine-tuning, and what is the trade-off?

- Concept: Cross-entropy loss and masking techniques
  - Why needed here: The Question-Masked loss focuses optimization on answer generation rather than question reconstruction, addressing the specific failure mode observed during fine-tuning.
  - Quick check question: How does masking portions of the loss function affect gradient updates during training, and what assumptions does this make about the pre-trained model's capabilities?

## Architecture Onboarding

- Component map: Document preprocessing → Custom chunking with section headers → Embedding generation (multiple models + BM25) → Retrieval (KNN + BM25 ranking) → Prompt construction (abbreviation expansion, option inclusion/exclusion) → Model generation (Phi-2/Falcon-7B with LoRA) → Answer selection (Option batch-shuffle for Phi-2, embedding similarity for Falcon-7B)

- Critical path: Document chunking → Embedding generation → Retrieval → Prompt construction → Model generation → Answer selection

- Design tradeoffs: Multiple embedding models increase retrieval coverage but add computational overhead; option batch-shuffle improves accuracy but increases inference time; question-masked loss focuses training but assumes question comprehension is already adequate

- Failure signatures: Low retrieval recall (contexts don't contain answer), model selection bias (consistent wrong answer positions), fine-tuning plateau (validation loss decreases but accuracy doesn't improve)

- First 3 experiments:
  1. Compare single vs. multiple embedding models for RAG retrieval accuracy on a validation set
  2. Test option batch-shuffle with different values of k (number of permutations) to find the efficiency-accuracy sweet spot
  3. Validate question-masked loss by comparing validation accuracy trends with standard cross-entropy during fine-tuning

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does QMOS perform on other specialized domains beyond telecommunications, such as healthcare or legal?
- Basis in paper: [explicit] The paper suggests future work to investigate QMOS performance on other language models and MCQ datasets.
- Why unresolved: The paper only evaluates QMOS on telecommunications datasets, limiting generalizability claims.
- What evidence would resolve it: Testing QMOS on multiple domain-specific MCQ datasets and comparing results to baseline models.

### Open Question 2
- Question: What is the optimal balance between inference-time and training-time option shuffling for different model sizes?
- Basis in paper: [explicit] The paper notes that using k=20 prompts provides a good balance between efficiency and accuracy, but suggests further investigation is needed.
- Why unresolved: The paper only tests k=20 and does not explore the trade-offs systematically across different model sizes or dataset characteristics.
- What evidence would resolve it: Systematic experiments varying k across different model sizes and measuring accuracy-efficiency trade-offs.

### Open Question 3
- Question: How does the question-masked loss function perform compared to other fine-tuning objectives for MCQ tasks?
- Basis in paper: [explicit] The paper introduces question-masked loss and shows improved validation accuracy, but notes this requires further investigation with additional experiments.
- Why unresolved: The paper only compares question-masked loss to standard next-token prediction, not to other potential fine-tuning objectives for MCQ tasks.
- What evidence would resolve it: Comparative experiments testing question-masked loss against alternative fine-tuning objectives on multiple MCQ datasets.

## Limitations
- The TeleQnA dataset appears to be domain-specific and proprietary, limiting external validation
- Limited implementation details for critical components like the question-masked loss function and option batch-shuffling parameters make exact reproduction challenging
- Evaluation focuses solely on multiple-choice questions, leaving open questions about performance on open-ended questions or other task formats common in telecommunications

## Confidence
- High confidence: The core RAG architecture with custom chunking and multiple embedding models is well-established and reproducible
- Medium confidence: The reported accuracy improvements are significant but may be sensitive to implementation details not fully specified in the paper
- Low confidence: The long-term effectiveness of these techniques for other telecommunications tasks or different question formats remains unproven

## Next Checks
1. Replicate the option batch-shuffling effectiveness test with varying k values (permutations) to determine optimal efficiency-accuracy tradeoffs
2. Validate the question-masked loss approach by comparing training dynamics with standard cross-entropy on a held-out validation set
3. Test the enhanced RAG system's retrieval quality independently by measuring recall@K on a subset of questions with known ground truth contexts