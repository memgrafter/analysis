---
ver: rpa2
title: 'Unveiling the Impact of Multi-Modal Interactions on User Engagement: A Comprehensive
  Evaluation in AI-driven Conversations'
arxiv_id: '2406.15000'
source_url: https://arxiv.org/abs/2406.15000
tags:
- user
- engagement
- audio
- length
- multi-modal
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigates how multi-modal interactions (text, image,
  and audio) influence user engagement in AI-driven chatbot conversations. The researchers
  collected a large-scale dataset of 747,350 dialogues from 146,179 real users interacting
  with 198 chatbot characters across diverse demographics.
---

# Unveiling the Impact of Multi-Modal Interactions on User Engagement: A Comprehensive Evaluation in AI-driven Conversations

## Quick Facts
- arXiv ID: 2406.15000
- Source URL: https://arxiv.org/abs/2406.15000
- Reference count: 15
- This study investigates how multi-modal interactions (text, image, and audio) influence user engagement in AI-driven chatbot conversations.

## Executive Summary
This research investigates the impact of multi-modal interactions on user engagement in AI-driven chatbot conversations. The study collected a large-scale dataset of 747,350 dialogues from 146,179 real users interacting with 198 chatbot characters across diverse demographics. A comprehensive evaluation framework was designed to measure three engagement metrics: retention rate, conversation length, and user utterance length. The analysis examined various interaction factors including text length, repetition, non-verbal descriptions, image aesthetics and styles, audio duration and styles, as well as text-image and text-audio alignment. Results showed that multi-modal interactions significantly enhance user engagement compared to text-only conversations, with text-audio alignment showing the strongest positive effect.

## Method Summary
The study collected a large-scale dataset of 747,350 dialogues from real user-bot interactions and designed an evaluation framework measuring retention rate, conversation length, and user utterance length. The analysis examined various interaction factors including text length, repetition, non-verbal descriptions, image aesthetics and styles, audio duration and styles, and text-image/audio alignment. The researchers used Qwen-vl and Qwen1.5-32B-Chat models with specific prompts to score alignment quality. Statistical correlations between interaction factors and engagement metrics were calculated to identify significant effects.

## Key Results
- Multi-modal interactions significantly enhance user engagement compared to text-only conversations
- Text-audio alignment showed the strongest positive effect on engagement metrics (0.261 retention rate, 51.53 conversation length, 29.55 user utterance length)
- Longer text utterances and slight repetition in chatbot responses were found to enhance user engagement

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Multi-modal interactions enhance user engagement by aligning with human attention and interest through sensorial signals from multiple modalities.
- Mechanism: The paper argues that incorporating multiple modalities (text, image, audio) in chatbot interactions creates richer information comprehension and optimizes cognitive processing, leading to higher user engagement.
- Core assumption: Human attention and interest are better captured through multiple sensory inputs rather than single modality interactions.
- Evidence anchors:
  - [abstract] "Considering that sensorial signals from multiple modalities align more closely with human attention and interest, our chatbots incorporate multi-modal mechanisms, performing in human-like manner, to engage users intuitively."
  - [section] "Our findings reveal that all non-text modalities, particularly images and audio, significantly enhance user engagement. Additionally, the integration of multiple modalities can further amplify these improvements."
  - [corpus] Found 25 related papers, average neighbor FMR=0.546, average citations=0.0. Weak corpus evidence for multi-modal engagement mechanisms.

### Mechanism 2
- Claim: Text-audio alignment shows the strongest positive effect on user engagement metrics (retention, conversation length, user utterance length).
- Mechanism: When the audio content semantically aligns with the preceding text dialogue intent, it creates a more coherent and engaging conversation experience for users.
- Core assumption: Semantic alignment between text and audio content is crucial for maintaining user engagement in multi-modal interactions.
- Evidence anchors:
  - [abstract] "Results showed that multi-modal interactions significantly enhance user engagement compared to text-only conversations, with text-audio alignment showing the strongest positive effect (0.261 retention rate, 51.53 conversation length, 29.55 user utterance length)."
  - [section] "We obtain substantial evidences that multi-modal factors continually improve the user engagement performance."
  - [corpus] Weak corpus evidence for text-audio alignment specific mechanisms.

### Mechanism 3
- Claim: Longer text utterances and slight repetition in chatbot responses enhance user engagement.
- Mechanism: The paper finds that longer chatbot responses provide more information, increasing user engagement, while slight repetition of semantic content indicates user interest and engagement.
- Core assumption: Users prefer more detailed responses and view repetition as a sign of engagement rather than redundancy.
- Evidence anchors:
  - [section] "When comparing user behavior between longer (with a mean value of 91.5) and shorter (with a mean value of 43.4) utterances from the chatbot, we observe that longer utterances can significantly increase the retention rate, conversation length, and length of the user's utterance, thereby enhancing user engagement."
  - [section] "Interestingly, our results reveal that, compared to non-repetitive adjacent responses from the chatbot, slight repetition can improve results of retention, CL and UUL, thereby enhancing user engagement."
  - [corpus] Weak corpus evidence for text length and repetition effects.

## Foundational Learning

- Concept: Multi-modal interaction design
  - Why needed here: Understanding how to effectively combine text, image, and audio in chatbot interactions to maximize user engagement.
  - Quick check question: What are the key considerations when designing multi-modal chatbot interactions to ensure they enhance rather than hinder user engagement?

- Concept: User engagement metrics and evaluation
  - Why needed here: The study uses retention rate, conversation length, and user utterance length to measure engagement. Understanding these metrics is crucial for interpreting the results and designing effective chatbots.
  - Quick check question: How do retention rate, conversation length, and user utterance length provide a comprehensive view of user engagement in chatbot interactions?

- Concept: Text-image and text-audio alignment techniques
  - Why needed here: The study uses specific techniques (Qwen-vl for text-image alignment, Qwen1.5-32B-Chat for text-audio alignment) to measure alignment between modalities. Understanding these techniques is important for replicating or improving the study.
  - Quick check question: How do text-image and text-audio alignment techniques contribute to measuring and improving user engagement in multi-modal chatbot interactions?

## Architecture Onboarding

- Component map: Data collection platform -> Multi-modal generation system -> Evaluation framework -> Analysis tools
- Critical path: 1. Collect real user-bot interaction data across diverse demographics 2. Generate multi-modal responses (text, image, audio) with alignment scoring 3. Analyze engagement metrics across different multi-modal factors 4. Identify significant factors and their impact on user engagement
- Design tradeoffs:
  - Real user data vs. synthetic data: Real data provides more authentic insights but is harder to collect and control
  - Manual vs. automated alignment scoring: Automated scoring is scalable but may be less accurate than human judgment
  - Number of modalities: More modalities can enhance engagement but also increase complexity and potential for user overwhelm
- Failure signatures: Low retention rates despite multi-modal interactions, high drop-off rates when switching between modalities, negative correlation between multi-modal factors and engagement metrics
- First 3 experiments: 1. Compare user engagement in text-only vs. multi-modal conversations across different user demographics 2. Analyze the impact of text length on user engagement, varying from short to long responses 3. Test the effectiveness of different text-image and text-audio alignment scoring methods on user engagement

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do different image aesthetics scores specifically impact user engagement across different demographic groups?
- Basis in paper: [explicit] The paper mentions using LAION-Aesthetics-Predictor V2 to measure image aesthetics scores but notes only minor differences between lower and higher aesthetics conditions.
- Why unresolved: The paper provides aggregate results without breaking down the impact by demographic factors such as age, gender, or country.
- What evidence would resolve it: Detailed analysis of user engagement metrics (retention, CL, UUL) stratified by demographics for different aesthetic score ranges.

### Open Question 2
- Question: What is the causal relationship between semantic repetition in chatbot responses and user engagement?
- Basis in paper: [explicit] The paper finds that slight repetition can improve retention, CL, and UUL, but acknowledges this is based on statistical correlation.
- Why unresolved: The study relies on correlation rather than experimental manipulation to establish causality.
- What evidence would resolve it: Controlled experiments varying repetition levels while holding other factors constant, measuring resulting changes in engagement metrics.

### Open Question 3
- Question: How do different combinations of multi-modal interactions affect user engagement compared to single modalities?
- Basis in paper: [explicit] The paper shows that multi-modal interactions enhance engagement but doesn't systematically compare all possible modality combinations.
- Why unresolved: The analysis focuses on presence/absence of multimodality rather than specific combinations (e.g., text+image vs. text+audio vs. text+image+audio).
- What evidence would resolve it: Comparative analysis of user engagement across all possible modality combinations, controlling for other factors.

## Limitations
- The study relies on proprietary dialogue data from a commercial platform, preventing independent verification of core findings
- Specific implementation details of non-verbal descriptions, repetition detection, and aesthetic scoring were not fully specified
- The exact prompts used for text-image and text-audio alignment evaluation were partially redacted

## Confidence
- High Confidence: The finding that multi-modal interactions enhance user engagement compared to text-only conversations is well-supported by the large-scale dataset and consistent across all three engagement metrics
- Medium Confidence: The relative impact of different multi-modal factors shows strong statistical significance, but the proprietary nature of the data and partial disclosure of implementation details limit complete reproducibility
- Low Confidence: The specific numerical values reported for engagement metrics cannot be independently verified due to data inaccessibility

## Next Checks
1. Implement the Qwen-based alignment scoring with multiple prompt variations and compare the resulting distributions against the ranges reported in the study to verify consistency in multi-modal factor categorization
2. Conduct additional analysis to examine whether the reported multi-modal engagement benefits hold across different user demographics using the available demographic breakdowns
3. Systematically vary the threshold values used to categorize interaction factors to assess the robustness of the reported engagement improvements and identify the most critical parameter ranges