---
ver: rpa2
title: 'Connecting the Dots: Evaluating Abstract Reasoning Capabilities of LLMs Using
  the New York Times Connections Word Game'
arxiv_id: '2406.11012'
source_url: https://arxiv.org/abs/2406.11012
tags: []
core_contribution: This paper evaluates large language models (LLMs) on the New York
  Times Connections word game, a benchmark for abstract reasoning. The authors collected
  438 games and tested four state-of-the-art LLMs, finding that even the best-performing
  model, Claude 3.5 Sonnet, could only fully solve 18% of the games.
---

# Connecting the Dots: Evaluating Abstract Reasoning Capabilities of LLMs Using the New York Times Connections Word Game

## Quick Facts
- arXiv ID: 2406.11012
- Source URL: https://arxiv.org/abs/2406.11012
- Authors: Prisha Samadarshi; Mariam Mustafa; Anushka Kulkarni; Raven Rothkopf; Tuhin Chakrabarty; Smaranda Muresan
- Reference count: 31
- Primary result: Even state-of-the-art LLMs can only fully solve 18% of Connections games, significantly underperforming expert humans

## Executive Summary
This paper evaluates large language models on the New York Times Connections word game, a challenging benchmark for abstract reasoning that requires identifying semantic, encyclopedic, and associative relationships between words. The authors collected 438 Connections games and tested four state-of-the-art LLMs using few-shot and chain-of-thought prompting, finding that Claude 3.5 Sonnet achieved the highest performance at 18% complete solutions, while expert humans significantly outperformed all models. The study reveals that LLMs struggle particularly with encyclopedic knowledge, multiword expressions, and combined knowledge types, while performing relatively well on semantic relations.

## Method Summary
The authors collected 200 distinct Connections games from June 2023 to January 2024, plus 3 games for few-shot prompting, and evaluated four state-of-the-art LLMs (Gemini 1.5 Pro, Claude 3 Opus, GPT-4o, Llama 3 70B) using few-shot and chain-of-thought prompting with default sampling parameters. Performance was measured using clustering scores (unweighted and weighted) and categorical reasoning scores, with results compared against novice and expert human players who solved a subset of the games. The study also created a taxonomy of knowledge types required for the game, revealing that LLMs struggle with encyclopedic, multiword expressions, and combined knowledge while performing relatively well on semantic relations.

## Key Results
- Claude 3.5 Sonnet achieved the highest performance at 18% complete solutions, while other models ranged from 0-8%
- Expert human players significantly outperformed all LLMs, while novice players performed marginally better
- LLMs struggled with encyclopedic knowledge, multiword expressions, and combined knowledge types, but performed relatively well on semantic relations
- Red herrings (distractors that appear to form valid categories) caused systematic errors, particularly in associative knowledge types

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLMs perform better on semantic relations than on encyclopedic knowledge because semantic relations are more linguistically encoded in training data
- Mechanism: During pretraining, LLMs encounter word co-occurrence patterns and definitional contexts that capture semantic relations more frequently than encyclopedic entity facts
- Core assumption: Training data distribution favors linguistic relations over encyclopedic facts
- Evidence anchors: Abstract states LLMs struggle with encyclopedic knowledge; experimental evidence shows GPT-4o solves only 8% of games completely

### Mechanism 2
- Claim: Red herrings exploit LLMs' tendency toward greedy clustering, leading to systematic errors in associative knowledge tasks
- Mechanism: LLMs make local decisions without global consistency checks, so when 3 words form an apparent category with 1 distractor, the model commits to the grouping before considering alternative associations
- Core assumption: LLMs lack explicit reasoning strategies for handling distractors
- Evidence anchors: Abstract mentions struggle with associative knowledge; mistakes from red herrings occur in associative categories

### Mechanism 3
- Claim: Multiword expressions are challenging because they require compositional understanding that standard transformers struggle to capture
- Mechanism: Standard attention mechanisms process tokens independently without explicit modeling of idiomatic compositionality
- Core assumption: Transformer architecture lacks specialized mechanisms for idiomatic composition
- Evidence anchors: Abstract states struggle with multiword expressions; only Claude 3 Opus has success rate above zero on these categories

## Foundational Learning

- Concept: Abstract reasoning through pattern recognition
  - Why needed here: Connections game requires identifying non-obvious patterns across word groups
  - Quick check question: Can you identify the pattern in {BASE, BOND, ELEMENT, SOLUTION} without looking at the answer?

- Concept: Knowledge representation taxonomy
  - Why needed here: Different knowledge types (semantic, encyclopedic, associative) require different reasoning strategies
  - Quick check question: Which knowledge type is primarily needed for {CHEERS, EUPHORIA, FELICITY, GLEE}?

- Concept: Red herring detection
  - Why needed here: Games intentionally include distractors that appear to form valid categories
  - Quick check question: How would you verify if {Whole, Skim, Soy} forms a complete category or is a red herring?

## Architecture Onboarding

- Component map: Prompt template -> LLM core -> Response parsing -> Scoring evaluation
- Critical path: Prompt generation → LLM inference → Response parsing → Scoring
- Design tradeoffs: Few-shot prompting vs. fine-tuning; instruction-following vs. reasoning capability
- Failure signatures: Incorrect clustering (grouping errors), incorrect reasoning (category mislabeling), instruction non-compliance (format errors)
- First 3 experiments:
  1. Test model performance on semantic vs. encyclopedic categories separately to isolate knowledge type effects
  2. Implement red herring detection strategy and measure performance improvement
  3. Add retrieval augmentation for encyclopedic knowledge and compare results

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Why do LLMs struggle specifically with Multiword Expressions and Combined Knowledge categories in the Connections game?
- Basis in paper: The paper explicitly states that LLMs struggle with these categories, which require recognizing compound words with common prefixes/suffixes and simultaneous use of multiple knowledge types respectively
- Why unresolved: The paper provides results showing this struggle but does not deeply analyze the underlying reasons for why LLMs find these specific types of reasoning particularly challenging
- What evidence would resolve it: Further experiments isolating and testing LLM performance on sub-components of these categories, or ablation studies on LLM architectures focusing on their handling of these knowledge types

### Open Question 2
- Question: How would the performance of LLMs and humans change if they were allowed multiple attempts to solve each Connections game?
- Basis in paper: The paper mentions that humans could not receive a clustering score of 3 due to the interface setup, and suggests that allowing LLMs to solve the game one category at a time with feedback could improve performance
- Why unresolved: The current experimental setup only allows one attempt for both LLMs and humans, preventing analysis of how iterative problem-solving affects performance
- What evidence would resolve it: Conducting experiments with multiple attempts allowed, tracking performance improvements across attempts, and comparing the rate of improvement between LLMs and humans

### Open Question 3
- Question: How does the frequency of different types of reasoning in the Connections game affect the overall performance of LLMs and humans?
- Basis in paper: The paper presents a breakdown of reasoning types in the dataset (Semantic > Associative > Encyclopedic > Linguistic > Multiword > Combined) and correlates this with performance, but doesn't analyze the impact of this distribution
- Why unresolved: While the paper notes the correlation between reasoning type frequency and performance, it doesn't explore how the distribution of reasoning types in the dataset affects overall success rates
- What evidence would resolve it: Creating synthetic datasets with different distributions of reasoning types and measuring the impact on LLM and human performance, or analyzing performance trends across games with varying reasoning type compositions

## Limitations

- Sample size of 438 games may not capture full diversity of reasoning patterns
- Few-shot prompting without fine-tuning limits task-specific optimization
- Small human comparison groups (5 novice, 3 expert players) limit statistical power
- Taxonomy of knowledge types relies on subjective categorization
- Evaluation metrics focus on correctness rather than quality of reasoning process

## Confidence

**High Confidence**: The core finding that LLMs struggle with encyclopedic and multiword expression knowledge types is well-supported by experimental results.

**Medium Confidence**: The mechanism explanations for why LLMs struggle with specific knowledge types are plausible but not definitively proven.

**Low Confidence**: The generalizability of the Connections game as a benchmark for abstract reasoning capabilities is uncertain.

## Next Checks

1. Test models on semantic vs. encyclopedic categories separately to determine if the performance gap persists when knowledge types are isolated.

2. Compare model performance across different architectures on multiword expressions to test whether compositional understanding limitation is architecture-specific.

3. Increase the expert human evaluation group to 20+ players to improve statistical power and better characterize the upper bound of human performance.