---
ver: rpa2
title: Evaluating Input Feature Explanations through a Unified Diagnostic Evaluation
  Framework
arxiv_id: '2406.15085'
source_url: https://arxiv.org/abs/2406.15085
tags:
- explanations
- explanation
- spanintex
- input
- tokenex
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper introduces a unified evaluation framework for comparing\
  \ different types of input feature explanations\u2014TokenEx, TokenIntEx, and SpanIntEx\u2014\
  across four diagnostic properties: Faithfulness, Agreement with Human Annotations,\
  \ Simulatability, and Complexity. The framework enables fair comparison by standardizing\
  \ the number of tokens considered across explanation types."
---

# Evaluating Input Feature Explanations through a Unified Diagnostic Evaluation Framework

## Quick Facts
- arXiv ID: 2406.15085
- Source URL: https://arxiv.org/abs/2406.15085
- Reference count: 27
- Primary result: SpanIntEx explanations generally outperform TokenEx and TokenIntEx across most diagnostic properties, particularly in agreement with human annotations and simulatability.

## Executive Summary
This paper introduces a unified evaluation framework for comparing different types of input feature explanations—TokenEx, TokenIntEx, and SpanIntEx—across four diagnostic properties: Faithfulness, Agreement with Human Annotations, Simulatability, and Complexity. The framework enables fair comparison by standardizing the number of tokens considered across explanation types. Experiments on SNLI and FEVER datasets with BERT and BART models reveal that SpanIntEx explanations generally outperform others on most properties, particularly in agreement with human annotations and simulatability, due to their enhanced semantic coherence. However, TokenEx explanations tend to be more comprehensive. The study highlights trade-offs between comprehensiveness and plausibility, suggesting future work should focus on methods that combine the strengths of different explanation types to improve overall utility.

## Method Summary
The unified evaluation framework applies three explanation techniques (Shapley, Attention, Integrated Gradients) to generate TokenEx, TokenIntEx, and SpanIntEx explanations for BERT-base and BART-base models on SNLI and FEVER datasets. A dynamic threshold θx,k standardizes token counts across explanation types to enable fair comparison. Four diagnostic properties are evaluated: Faithfulness (using Comprehensiveness and Sufficiency), Agreement with Human Annotations, Simulatability (using agent models to reproduce predictions), and Complexity (using entropy over attribution scores). The framework processes 4,000 instances from train, dev, and test sets for each dataset.

## Key Results
- SpanIntEx explanations achieve the highest agreement with human annotations across all experimental conditions
- TokenEx explanations show superior comprehensiveness but lower agreement with human annotations compared to SpanIntEx
- SpanIntEx demonstrates highest simulatability, helping agent models accurately reproduce original model predictions in 9/12 cases
- TokenIntEx generally shows lower performance across all properties, particularly in agreement with human annotations

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The unified framework enables fair comparison by standardizing the number of tokens considered across explanation types.
- Mechanism: The framework introduces a dynamic threshold θx,k that ensures each explanation type affects the same number of tokens when evaluating faithfulness. This prevents bias from differing token counts across TokenEx, TokenIntEx, and SpanIntEx.
- Core assumption: Different explanation types can be meaningfully compared when they perturb the same number of tokens.
- Evidence anchors:
  - [abstract] "The framework enables fair comparison by standardizing the number of tokens considered across explanation types."
  - [section 2.2] "To take different thresholds k, we average over k ∈ [0, s = m + n − 1]. We then also average the results across instances within D to compute the final Comprehensiveness and Sufficiency scores"
- Break Condition: If the semantic content of explanations varies significantly even when token counts are standardized, the fairness assumption breaks down.

### Mechanism 2
- Claim: SpanIntEx achieves better agreement with human annotations due to enhanced semantic coherence from including span-level interactions.
- Mechanism: By averaging importance scores over token interactions within spans, SpanIntEx captures more contextually meaningful patterns that align with human reasoning, even though it may be less comprehensive than TokenEx.
- Core assumption: Human annotations reflect semantic coherence that can be better captured at the span level than token level.
- Evidence anchors:
  - [abstract] "SpanIntEx explanations generally outperform others on most properties, particularly in agreement with human annotations and simulatability, due to their enhanced semantic coherence."
  - [section 4.2] "SpanIntEx and TokenIntEx show higher agreement scores with human annotations than TokenEx"
- Break Condition: If human annotations are based primarily on token-level importance rather than semantic relationships, this mechanism would fail.

### Mechanism 3
- Claim: SpanIntEx significantly improves simulatability by providing more contextually rich explanations that help agent models better replicate model decisions.
- Mechanism: The enhanced semantic coherence of SpanIntEx allows agent models to capture the reasoning process more effectively, leading to improved simulation accuracy compared to TokenEx or TokenIntEx.
- Core assumption: Contextual information in span interactions improves an agent model's ability to simulate the original model's behavior.
- Evidence anchors:
  - [abstract] "SpanIntEx demonstrate the highest interaction overlap with human annotations. Further, SpanIntEx are found to be most beneficial for Simulatability."
  - [section 4.3] "Our results show that SpanIntEx achieve the highest simulatability in 9/12 cases, helping the agent model accurately reproduce the original model's prediction"
- Break Condition: If the added complexity of span interactions confuses rather than helps the agent model, simulatability improvements would not materialize.

## Foundational Learning

- Concept: Faithfulness evaluation through perturbation
  - Why needed here: Faithfulness measures whether explanations reflect the model's actual decision-making process, which requires systematic perturbation of input features
  - Quick check question: How do sufficiency and comprehensiveness differ in faithfulness evaluation, and why are both important?

- Concept: Interactive vs. highlight explanations
  - Why needed here: The framework compares three types of explanations (TokenEx, TokenIntEx, SpanIntEx), requiring understanding of how interactions between features differ from individual feature importance
  - Quick check question: What is the key difference between TokenIntEx and SpanIntEx in terms of what they capture about feature relationships?

- Concept: Automated evaluation metrics for explainability
  - Why needed here: The framework uses automated metrics (Agreement with Human Annotations, Simulatability, Complexity) rather than human studies, requiring understanding of how these metrics work
  - Quick check question: Why might automated simulatability using agent models be preferable to human studies in some contexts?

## Architecture Onboarding

- Component map:
  - Unified Evaluation Framework -> Explanation Type Handlers (TokenEx, TokenIntEx, SpanIntEx) -> Attribution Method Interface (Shapley, Attention, Integrated Gradients) -> Dynamic Threshold Calculator -> Diagnostic Property Evaluators (Faithfulness, Agreement, Simulatability, Complexity) -> Results Aggregator

- Critical path:
  1. Input data and model loaded
  2. Explanation methods applied to generate TokenEx, TokenIntEx, and SpanIntEx
  3. Dynamic threshold θx,k calculated for each instance
  4. Four diagnostic properties evaluated using standardized token counts
  5. Results aggregated and compared across explanation types

- Design tradeoffs:
  - Standardization vs. explanation fidelity: Forcing same token count may reduce some explanations' natural expressiveness
  - Computational cost vs. comprehensiveness: Shapley-based methods are expensive but potentially more accurate
  - Static vs. dynamic thresholds: Dynamic θx,k provides fairness but adds complexity

- Failure signatures:
  - Token count standardization fails: TokenEx explanations consistently show higher faithfulness than others even after standardization
  - Agent model overfitting: Simulatability improvements are due to memorization rather than genuine understanding
  - Human annotation mismatch: All explanation types show low agreement with human annotations regardless of type

- First 3 experiments:
  1. Verify dynamic threshold calculation: Run explanation generation with and without θx,k standardization and compare faithfulness scores
  2. Test insertion format sensitivity: Evaluate simulatability with different explanation insertion formats (ISymbol vs. IT ext) to confirm findings
  3. Random baseline comparison: Generate random explanations with matched token counts and ensure they perform worse than all explanation types across metrics

## Open Questions the Paper Calls Out

- What specific architectural or algorithmic modifications to the Louvain-based span interaction extraction could improve the comprehensiveness of SpanIntEx explanations while maintaining their semantic coherence?
- Could integrating multiple explanation types (e.g., combining SpanIntEx and TokenEx) produce explanations that simultaneously optimize comprehensiveness, plausibility, and simulatability?

## Limitations

- The core assumption that token count standardization enables fair comparison across explanation types with different semantic granularities remains uncertain
- Computational cost of Shapley-based methods, particularly for SpanIntEx with interaction terms, may limit practical applicability
- The framework focuses on text classification tasks and may not generalize to other domains or task types

## Confidence

**High Confidence**: The framework successfully enables standardized comparison across explanation types (Section 4 results show consistent methodology application). The observation that TokenEx tends to be more comprehensive while SpanIntEx achieves better agreement with human annotations is well-supported by the experimental results across multiple datasets and models.

**Medium Confidence**: The claim that SpanIntEx's enhanced semantic coherence drives its superior performance in agreement and simulatability. While the results support this, the exact mechanism by which span-level interactions capture human-like reasoning patterns could benefit from additional qualitative analysis. The trade-off between comprehensiveness and plausibility is observed but not fully explored.

**Low Confidence**: The assertion that future work should focus on methods combining the strengths of different explanation types. This recommendation is reasonable but lacks specific proposals or preliminary evidence for what such hybrid approaches might look like or how they would overcome the observed trade-offs.

## Next Checks

1. **Semantic Coherence Validation**: Conduct ablation studies that systematically vary the semantic coherence of explanations while maintaining constant token counts to isolate whether semantic coherence or some other factor drives SpanIntEx's superior agreement with human annotations.

2. **Computational Efficiency Benchmarking**: Measure the actual wall-clock time and memory usage for generating each explanation type at scale, particularly comparing Shapley-based methods across TokenEx, TokenIntEx, and SpanIntEx, to quantify the practical trade-offs beyond the theoretical computational complexity analysis.

3. **Human-in-the-Loop Simulatability**: Design a controlled user study where human participants attempt to predict model outputs using different explanation types, comparing these results with the automated agent model simulatability to validate whether computational simulatability correlates with human reasoning facilitation.