---
ver: rpa2
title: 'Mars-PO: Multi-Agent Reasoning System Preference Optimization'
arxiv_id: '2411.19039'
source_url: https://arxiv.org/abs/2411.19039
tags:
- reasoning
- arxiv
- mars-po
- preference
- mathematical
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the challenge of improving mathematical reasoning
  capabilities in large language models (LLMs), which are often prone to errors, hallucinations,
  and inconsistencies during multi-step reasoning tasks. To tackle this, the authors
  propose Mars-PO, a novel framework that leverages a multi-agent system to enhance
  LLM performance.
---

# Mars-PO: Multi-Agent Reasoning System Preference Optimization

## Quick Facts
- arXiv ID: 2411.19039
- Source URL: https://arxiv.org/abs/2411.19039
- Reference count: 6
- Primary result: Mars-PO improves Llama3.1-8B-Instruct MATH accuracy from 50.38% to 57.82%

## Executive Summary
Mars-PO is a novel preference optimization framework that leverages multiple LLM agents to improve mathematical reasoning performance. The approach constructs high-quality preference pairs by combining hybrid positive samples from all agents with agent-specific negative samples, then applies iterative preference optimization. By aligning agents with shared positive samples while addressing individual weaknesses, Mars-PO achieves substantial performance improvements over existing methods like supervised fine-tuning and vanilla DPO.

## Method Summary
Mars-PO operates through a three-stage process: (1) Response generation using CoT reasoning from three instruction-tuned LLMs, (2) Preference pair construction using a reward model to create hybrid positive samples paired with agent-specific negatives, and (3) Hybrid preference optimization with iterative training over multiple epochs. The framework uses Qwen2.5-Math-7B-Instruct, DeepSeek-Math-7B-RL, and Llama3.1-8B-Instruct as agents, with Qwen2.5-Math-RM-72B for reward modeling. Training employs specific hyperparameters including batch size 16, learning rate 7e-7, and Î± progression from 0.1 to 0.4 across iterations.

## Key Results
- Mars-PO increases Llama3.1-8B-Instruct MATH accuracy from 50.38% to 57.82%
- Consistent outperformance over supervised fine-tuning and vanilla DPO baselines
- Iterative training shows progressive accuracy improvement across three iterations
- Agent-specific negative samples contribute to targeted improvement of individual model weaknesses

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Mars-PO leverages a hybrid positive sample set to improve mathematical reasoning performance.
- Mechanism: The framework generates responses from multiple agents, evaluates them using a reward model, and constructs a high-quality hybrid positive sample set by selecting the best outputs across all agents. This set is then used in preference optimization to train the model.
- Core assumption: The reward model can effectively distinguish high-quality mathematical reasoning outputs from lower-quality ones.
- Evidence anchors:
  - [abstract] "It combines high-quality outputs from multiple agents into a hybrid positive sample set and pairs them with agent-specific negative samples to construct robust preference pairs for training."
  - [section] "The hybrid positive sample set Gw is extracted from the outputs of all LLM agents... A reward model, pre-trained to evaluate solution quality, assigns a reward score to each candidate. The highest-scoring outputs across all agents are merged into the hybrid positive sample set."
- Break condition: If the reward model fails to accurately evaluate reasoning quality, the hybrid positive sample set will not represent true high-quality solutions.

### Mechanism 2
- Claim: Agent-specific negative samples allow targeted improvement of individual model weaknesses.
- Mechanism: While positive samples are shared across all agents, negative samples are tailored to each agent's unique failure modes. This allows the model to learn what it does poorly while reinforcing what it does well.
- Core assumption: Different agents have complementary strengths and weaknesses that can be effectively exploited through this approach.
- Evidence anchors:
  - [section] "Unlike the hybrid positive set, these negative samples are agent-specific, reflecting each agent's unique failure modes. By pairing hybrid positive samples with negative samples tailored to individual agents, the constructed preference pairs expose the limitations of each agent while reinforcing the benefits of the shared positive solutions."
  - [abstract] "By aligning agents with shared positive samples while addressing individual weaknesses, Mars-PO achieves substantial performance improvements"
- Break condition: If agents have very similar failure patterns, the benefit of agent-specific negative samples diminishes.

### Mechanism 3
- Claim: Iterative training progressively refines model reasoning capabilities.
- Mechanism: The framework performs multiple rounds of preference optimization, where each iteration builds upon the outputs and refinements from the previous round, allowing the model to continually improve its understanding and alignment with high-quality reasoning patterns.
- Core assumption: Each iteration provides meaningful improvement that can be built upon in subsequent iterations.
- Evidence anchors:
  - [section] "To further enhance agent performance, we adopt an iterative training method to repeatedly update the parameters of the target LLMs."
  - [section] "Table 1 presents the prediction accuracy of the models across three iterations of training. As shown in the results, the accuracy generally increases with each iteration, demonstrating the effectiveness of iterative training in improving model performance."
- Break condition: If the model reaches a performance plateau, additional iterations may not provide meaningful improvement.

## Foundational Learning

- Concept: Direct Preference Optimization (DPO)
  - Why needed here: Mars-PO builds upon DPO by extending it to a multi-agent setting. Understanding DPO is essential to grasp how Mars-PO modifies the preference optimization process.
  - Quick check question: How does DPO differ from traditional reinforcement learning approaches for LLM alignment?

- Concept: Chain-of-Thought (CoT) reasoning
  - Why needed here: The framework generates CoT reasoning steps as part of the response samples. Understanding CoT is crucial for comprehending how mathematical reasoning is structured in the generated outputs.
  - Quick check question: Why is CoT particularly important for mathematical reasoning tasks in LLMs?

- Concept: Reward modeling for text evaluation
  - Why needed here: A reward model is used to score the quality of generated positive samples. Understanding how reward models work is essential for grasping how the hybrid positive sample set is constructed.
  - Quick check question: What are the key challenges in designing reward models for mathematical reasoning evaluation?

## Architecture Onboarding

- Component map:
  Multi-agent system -> Reward model -> Preference pair constructor -> Mars-PO trainer -> Iterative training loop

- Critical path:
  1. Generate response samples from all agents
  2. Score samples with reward model
  3. Construct hybrid positive sample set
  4. Create agent-specific negative samples
  5. Build preference pairs
  6. Apply Mars-PO training
  7. Repeat iteratively

- Design tradeoffs:
  - Single vs. multi-agent approach: Multi-agent provides diversity but increases computational cost
  - Number of samples per agent: More samples improve quality but increase processing time
  - Number of iterations: More iterations may improve performance but risk overfitting

- Failure signatures:
  - Performance degradation after certain iterations (overfitting)
  - Inconsistent improvement across different agents
  - Reward model bias affecting sample selection

- First 3 experiments:
  1. Compare single-agent DPO vs. Mars-PO on a small dataset to verify the multi-agent benefit
  2. Test different numbers of response samples (N) to find the optimal balance between quality and efficiency
  3. Evaluate the impact of removing the NLL loss component to assess its contribution to performance

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of Mars-PO scale with the number of agents in the multi-agent system?
- Basis in paper: [inferred] The paper mentions that Mars-PO leverages a multi-agent system but does not explore the impact of varying the number of agents on performance.
- Why unresolved: The paper uses a fixed number of three agents (Qwen2.5-Math, DeepSeek-Math, and Llama3.1) without investigating whether adding more agents would further improve results or if there is a diminishing return.
- What evidence would resolve it: Conducting experiments with varying numbers of agents (e.g., 2, 3, 4, 5) and comparing the resulting performance on benchmarks like GSM8K and MATH would clarify the scalability of Mars-PO.

### Open Question 2
- Question: Can Mars-PO be effectively applied to domains outside of mathematical reasoning, such as logical reasoning or scientific problem-solving?
- Basis in paper: [explicit] The paper focuses on mathematical reasoning but suggests that Mars-PO is a general framework for preference optimization in multi-agent systems.
- Why unresolved: The paper does not test Mars-PO on other domains, leaving its generalizability to other reasoning tasks unclear.
- What evidence would resolve it: Applying Mars-PO to other reasoning benchmarks (e.g., logical reasoning datasets or scientific problem-solving tasks) and comparing its performance to domain-specific methods would demonstrate its versatility.

### Open Question 3
- Question: What is the optimal number of iterations for Mars-PO training to balance performance gains and computational cost?
- Basis in paper: [explicit] The paper mentions iterative training but only explores up to three iterations, noting minor fluctuations in accuracy between iterations.
- Why unresolved: The paper does not investigate whether additional iterations could lead to further improvements or if there is a point of diminishing returns.
- What evidence would resolve it: Conducting experiments with more iterations (e.g., 4, 5, or 6) and analyzing the trade-off between accuracy gains and computational resources would clarify the optimal stopping point.

## Limitations
- The specific prompt format for CoT reasoning generation is not disclosed, which could significantly impact reproducibility
- The reward model's exact implementation and scoring methodology remain unspecified
- The iterative training approach could lead to overfitting, though this is not explicitly addressed

## Confidence
- High confidence: The overall multi-agent preference optimization framework is sound and well-explained
- Medium confidence: The performance improvements on MATH benchmark are significant, but could be influenced by specific implementation details
- Medium confidence: The effectiveness of agent-specific negative samples, as this depends on the diversity of agent failure modes

## Next Checks
1. **Cross-dataset validation**: Test Mars-PO on additional mathematical reasoning datasets beyond GSM8K and MATH to verify generalization of performance improvements
2. **Ablation study**: Systematically remove components (agent-specific negatives, iterative training, NLL loss) to quantify their individual contributions to performance gains
3. **Robustness testing**: Evaluate Mars-PO's performance when the reward model is intentionally biased or when agents have highly correlated failure patterns