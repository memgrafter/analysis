---
ver: rpa2
title: A Contrastive Symmetric Forward-Forward Algorithm (SFFA) for Continual Learning
  Tasks
arxiv_id: '2409.07387'
source_url: https://arxiv.org/abs/2409.07387
tags:
- sffa
- learning
- activity
- algorithm
- each
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces the Symmetric Forward-Forward Algorithm (SFFA),
  a novel modification of the Forward-Forward Algorithm (FFA) designed to address
  the inherent asymmetry in FFA's loss function that negatively impacts model generalization.
  SFFA partitions each layer into positive and negative neuron sets, enabling a symmetric
  goodness function defined as the ratio of their activations.
---

# A Contrastive Symmetric Forward-Forward Algorithm (SFFA) for Continual Learning Tasks

## Quick Facts
- arXiv ID: 2409.07387
- Source URL: https://arxiv.org/abs/2409.07387
- Reference count: 40
- Primary result: SFFA achieves competitive accuracy compared to FFA and gradient back-propagation in image classification, with improved stability, and shows promise for continual learning tasks.

## Executive Summary
This paper introduces the Symmetric Forward-Forward Algorithm (SFFA), a novel modification of the Forward-Forward Algorithm (FFA) designed to address the inherent asymmetry in FFA's loss function that negatively impacts model generalization. SFFA partitions each layer into positive and negative neuron sets, enabling a symmetric goodness function defined as the ratio of their activations. This reformulation results in a balanced loss landscape during training. Experiments on multiple image classification benchmarks demonstrate that SFFA achieves competitive accuracy compared to FFA and gradient back-propagation, with improved stability. Furthermore, the study explores SFFA's applicability to Continual Learning (CL) tasks. The sparse representations and layer-wise training induced by SFFA complement conventional CL techniques, leading to better performance than back-propagation in Class and Domain Incremental Learning scenarios. However, SFFA shows limitations in Task Incremental Learning due to architectural differences with multi-head layers.

## Method Summary
The Symmetric Forward-Forward Algorithm (SFFA) is a modification of the Forward-Forward Algorithm (FFA) that addresses the asymmetry in FFA's loss function by partitioning each layer into positive and negative neuron sets. The goodness function is redefined as the ratio of positive and negative neuron activations, resulting in a balanced loss landscape during training. SFFA employs k-WTA dynamics for lateral inhibition, ensuring sparse representations and improving model generalization. The algorithm is tested on image classification tasks using datasets such as MNIST, Fashion MNIST, KMNIST, and EMNIST Letters, and its performance is compared to FFA and gradient back-propagation. Additionally, SFFA is adapted for Continual Learning (CL) tasks, including Class Incremental Learning (Class IL), Domain Incremental Learning (Domain IL), and Task Incremental Learning (Task IL). The paper explores the integration of SFFA with conventional CL techniques like Elastic Weight Consolidation (EWC), Synaptic Intelligence (SI), Memory Aware Synapses (MAS), Replay, and Gradient Episodic Memory (GEM) to mitigate catastrophic forgetting and improve overall performance in CL scenarios.

## Key Results
- SFFA achieves competitive accuracy compared to FFA and gradient back-propagation in image classification tasks, with improved stability.
- In Continual Learning tasks, SFFA outperforms back-propagation in Class and Domain Incremental Learning scenarios when combined with CL techniques.
- SFFA shows limitations in Task Incremental Learning due to architectural differences with multi-head layers and the inability to share weights across tasks.

## Why This Works (Mechanism)
SFFA works by addressing the inherent asymmetry in FFA's loss function through the introduction of a symmetric goodness function. By partitioning each layer into positive and negative neuron sets, SFFA ensures that the loss landscape is balanced during training. This balanced loss function leads to more stable gradients and improved model generalization. Additionally, the use of k-WTA dynamics for lateral inhibition promotes sparse representations, which further enhances model performance and stability. The layer-wise training approach in SFFA also contributes to its effectiveness in Continual Learning tasks, as it allows for more efficient adaptation to new tasks while minimizing catastrophic forgetting.

## Foundational Learning
- Forward-Forward Algorithm (FFA): A neural network training algorithm that uses positive and negative data samples to update weights, unlike back-propagation which relies on gradients. Needed for understanding the baseline algorithm and its limitations. Quick check: Verify the asymmetry in FFA's loss function and its impact on model generalization.
- Continual Learning (CL): A paradigm in machine learning where models are trained on a sequence of tasks, aiming to retain knowledge from previous tasks while adapting to new ones. Needed for understanding the application of SFFA in real-world scenarios. Quick check: Evaluate SFFA's performance in Class, Domain, and Task Incremental Learning scenarios.
- k-WTA dynamics: A mechanism for lateral inhibition that promotes sparse representations by activating only the top k neurons in a layer. Needed for understanding the role of sparsity in SFFA's effectiveness. Quick check: Assess the impact of different k values on SFFA's performance and stability.

## Architecture Onboarding

Component Map: Input -> Convolutional Layers -> Fully Connected Layers -> Output
Critical Path: Input -> Convolutional Layers (with k-WTA dynamics) -> Fully Connected Layers (with SFFA) -> Output
Design Tradeoffs: SFFA offers improved stability and performance compared to FFA, but may require more computational resources due to the need for positive and negative neuron sets. The use of k-WTA dynamics promotes sparsity but may limit the model's capacity to learn complex representations.
Failure Signatures: Unstable gradients when activity approaches zero, poor performance in Task IL due to architectural incompatibility, and limited scalability to complex image classification tasks with non-uniform backgrounds.

First Experiments:
1. Implement SFFA with different k-WTA sparsity levels and evaluate its impact on model performance and stability.
2. Conduct ablation studies to determine the optimal value for the regularization term scaling hyper-parameter α and its effect on SFFA's performance.
3. Compare SFFA's performance with other prominent CL techniques (e.g., EWC, SI, MAS, Replay, GEM) on a wider range of CL scenarios and datasets to assess its competitiveness in the CL domain.

## Open Questions the Paper Calls Out
- Can the SFFA be extended to Task IL scenarios while maintaining weight sharing across tasks?
- How does the SFFA perform in complex image classification tasks with non-uniform backgrounds?
- Can parameter isolation/masking techniques be effectively integrated with SFFA to improve performance in Class IL scenarios?

## Limitations
- SFFA shows limitations in Task Incremental Learning due to architectural differences with multi-head layers and the inability to share weights across tasks.
- The scalability of SFFA to complex image classification tasks with non-uniform backgrounds remains unexplored, as the current implementation is limited to datasets with uniform backgrounds.
- The paper does not provide sufficient details on the optimal values for hyper-parameters, such as the regularization term scaling hyper-parameter α, which may affect the reproducibility of the results.

## Confidence
- Claims about SFFA's performance in image classification: Medium
- Claims about SFFA's applicability to Continual Learning tasks: Medium
- Claims about SFFA's limitations in Task Incremental Learning: High

## Next Checks
1. Implement and test SFFA with different k-WTA sparsity levels to evaluate its impact on model performance and stability.
2. Conduct ablation studies to determine the optimal value for the regularization term scaling hyper-parameter α and its effect on SFFA's performance.
3. Compare SFFA's performance with other prominent CL techniques (e.g., EWC, SI, MAS, Replay, GEM) on a wider range of CL scenarios and datasets to assess its competitiveness in the CL domain.