---
ver: rpa2
title: How do you know that? Teaching Generative Language Models to Reference Answers
  to Biomedical Questions
arxiv_id: '2407.05015'
source_url: https://arxiv.org/abs/2407.05015
tags:
- abstracts
- answers
- search
- relevant
- pubmed
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper presents a biomedical retrieval-augmented generation
  (RAG) system that integrates a hybrid search mechanism with fine-tuned large language
  models (LLMs) to generate referenced answers to biomedical questions. The IR component
  combines lexical and semantic search, achieving a 23% absolute improvement in precision
  compared to PubMed's native search.
---

# How do you know that? Teaching Generative Language Models to Reference Answers to Biomedical Questions

## Quick Facts
- arXiv ID: 2407.05015
- Source URL: https://arxiv.org/abs/2407.05015
- Reference count: 8
- Primary result: Biomedical RAG system with hybrid search and fine-tuned Mistral-7B achieves 23% precision improvement over PubMed and comparable referencing performance to GPT-4 Turbo

## Executive Summary
This paper presents a biomedical retrieval-augmented generation (RAG) system that generates referenced answers to biomedical questions using PubMed abstracts. The system combines a hybrid IR component (lexical BM25 + semantic dense vectors) with fine-tuned Mistral-7B models to produce answers where each claim is supported by citations. The hybrid search achieves 23% absolute precision improvement over PubMed's native search, while the fine-tuned models demonstrate comparable abstract referencing performance to GPT-4 Turbo in manual evaluation. The system addresses the critical need for verifiable biomedical information by ensuring each statement in the generated answer is grounded in specific PubMed abstracts.

## Method Summary
The system uses a two-stage approach: first, a hybrid IR component combines lexical BM25 search with semantic dense vector search using weighted summation (optimal weights: 0.7 lexical, 0.3 semantic) to retrieve 10 relevant PubMed abstracts per query. Second, fine-tuned Mistral-7B models (using LoRA adapters and QLoRA) generate referenced answers by extracting claims from the provided abstracts and citing specific PMIDs. The models were fine-tuned on a custom PQAref dataset of 9,075 questions with referenced answers and 10 relevant abstracts each. The system outputs answers where each statement includes a citation to the supporting abstract.

## Key Results
- Hybrid IR achieves 23% absolute precision improvement over PubMed search (P@10: 0.66 vs 0.43)
- Fine-tuned Mistral-7B models show comparable abstract referencing performance to GPT-4 Turbo in manual evaluation
- The system produces referenced answers with each claim supported by PubMed citations
- Fine-tuning improves abstract selection recall from 0.56 to 0.76 (M1 model)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Hybrid retrieval combining lexical and semantic search yields higher precision than either method alone.
- Mechanism: The system normalizes scores from both search methods to [0,1] and combines them using weighted summation. Lexical search with BM25 captures exact term matches, while semantic search with dense embeddings captures contextual meaning. By balancing these, the system retrieves documents matching both term presence and topical relevance.
- Core assumption: Optimal performance requires non-zero weights on both lexical and semantic components, with lexical weight > semantic weight.
- Evidence anchors:
  - [abstract] "Our retrieval system achieves an absolute improvement of 23% compared to the PubMed search engine."
  - [section] "The initial improvement is noted with the hybrid search employing a 0.1 lexical search weight, followed by a second significant enhancement achieved with a 0.6 lexical search weight."
  - [corpus] Weak - no direct corpus evidence of optimal weights, only inference from evaluation table.
- Break condition: If lexical weight approaches 0, precision degrades significantly; if semantic weight dominates, recall of exact matches suffers.

### Mechanism 2
- Claim: Fine-tuning Mistral-7B on referenced QA dataset improves abstract selection and citation accuracy compared to zero-shot performance.
- Mechanism: Fine-tuning with LoRA adapters on PQAref dataset teaches the model to extract relevant claims from provided abstracts and generate answers with accurate PMID citations. The model learns to associate answer content with specific abstract IDs rather than hallucinating sources.
- Core assumption: Domain-specific fine-tuning on referenced QA tasks transfers to better performance on unseen biomedical questions.
- Evidence anchors:
  - [abstract] "our fine-tuned LLM component achieves comparable results to GPT-4 Turbo in referencing relevant abstracts."
  - [section] "M1 exhibited the highest recall of 0.76... showing the highest benefit from the fine-tuning process."
  - [corpus] Weak - only 10 examples evaluated manually, small sample size.
- Break condition: If training data distribution differs significantly from deployment queries, fine-tuning may not generalize.

### Mechanism 3
- Claim: Reference grounding reduces hallucination by constraining LLM generation to retrieved abstracts.
- Mechanism: The system provides 10 retrieved abstracts as context and instructs the model to answer using only that information, citing specific abstract IDs for each claim. This forces the model to ground responses in verifiable sources rather than generating unsupported content.
- Core assumption: Providing explicit context and citation instructions is sufficient to prevent hallucination in most cases.
- Evidence anchors:
  - [abstract] "The final output is an answer based on PubMed abstracts, where each statement is referenced accordingly, allowing the users to verify the answer."
  - [section] "M2 produced only 3 [hallucinated IDs]... this point needs further attention."
  - [corpus] Weak - hallucination still occurs (3-79 cases), indicating mechanism not fully reliable.
- Break condition: If model encounters concepts not covered in provided abstracts, it may still generate unsupported claims or hallucinate IDs.

## Foundational Learning

- Concept: Information retrieval evaluation metrics (precision, MAP, P@10)
  - Why needed here: The paper uses these metrics to compare IR component performance against PubMed baseline and different retrieval strategies.
  - Quick check question: If a system retrieves 10 documents and 3 are relevant, what is P@10? (Answer: 0.3)

- Concept: RAG architecture components and data flow
  - Why needed here: Understanding how IR component feeds context to generative component is crucial for system design and debugging.
  - Quick check question: In this system, what are the two main components that must work together to produce referenced answers? (Answer: IR component and generative component)

- Concept: Fine-tuning vs zero-shot inference trade-offs
  - Why needed here: The paper compares fine-tuned models against zero-shot versions to demonstrate performance improvements.
  - Quick check question: What is the main advantage of fine-tuning on domain-specific referenced QA data compared to using zero-shot inference? (Answer: Better abstract selection and citation accuracy)

## Architecture Onboarding

- Component map:
  User query → IR component (hybrid search) → 10 abstracts → Generative component (fine-tuned Mistral-7B) → Referenced answer
  IR component: OpenSearch (lexical) + Qdrant (semantic) + hybrid scoring
  Generative component: Mistral-7B with LoRA adapters, QLoRA fine-tuning

- Critical path: Query → IR retrieval → Context generation → LLM inference → Answer with citations
  Each step must complete successfully; failure in IR component directly impacts answer quality

- Design tradeoffs:
  Hybrid search weights: 0.7 lexical / 0.3 semantic provides best precision but may miss semantically relevant but lexically different documents
  Context window: 10 abstracts limits information but keeps inference manageable
  Fine-tuning vs. zero-shot: Fine-tuning improves performance but requires dataset creation and training time

- Failure signatures:
  Low P@10 or MAP@10: IR component not retrieving relevant documents
  Answers without citations or with hallucinated IDs: Generative component failing to ground responses
  Inconsistent performance across query types: Potential distribution shift between training and deployment data

- First 3 experiments:
  1. Test IR component with sample queries and verify retrieval precision vs PubMed baseline
  2. Run generative component in zero-shot mode with fixed abstract set to check citation accuracy
  3. Evaluate fine-tuned vs zero-shot models on same queries to measure performance improvement

## Open Questions the Paper Calls Out

- How does fine-tuning sentence transformers on biomedical domain-specific data impact the quality of semantic embeddings for retrieval in PubMed articles?
- What is the optimal balance between hybrid search parameters (lexical weight vs semantic weight) for different types of biomedical queries?
- What causes the hallucination of reference IDs in fine-tuned models, and how can this be prevented while maintaining the models' ability to reference relevant abstracts?

## Limitations
- Small manual evaluation sample size (10 examples) limits generalizability of generative component results
- Hallucination still occurs in fine-tuned models, with 3-79 hallucinated reference IDs across different models
- System performance on complex queries requiring multi-abstract synthesis remains untested

## Confidence
- **High confidence**: Hybrid IR component's precision improvement over PubMed (23% absolute gain)
- **Medium confidence**: Comparable performance to GPT-4 Turbo in referencing relevant abstracts (small manual sample)
- **Medium confidence**: Effectiveness of fine-tuning in improving abstract selection and citation accuracy (limited evaluation scope)

## Next Checks
1. Expand manual evaluation to at least 100 examples assessing both reference relevance and factual accuracy across different biomedical question types
2. Test system generalization on biomedical questions from diverse sources beyond BioASQ and PQAref datasets
3. Systematically analyze hallucination patterns to identify failure modes and develop mitigation strategies