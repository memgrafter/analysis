---
ver: rpa2
title: 'Bridging State and History Representations: Understanding Self-Predictive
  RL'
arxiv_id: '2401.08898'
source_url: https://arxiv.org/abs/2401.08898
tags:
- learning
- steps
- latent
- environment
- state
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper establishes a unified view of state and history representations
  in RL by revealing that many prior methods optimize a collection of interconnected
  properties, all representing facets of the same fundamental concept of self-predictive
  abstraction. The key contributions are: (1) A theoretical analysis showing that
  self-predictive representations can be learned by optimizing expected reward prediction
  and next latent state distribution prediction; (2) Insights into why stop-gradient
  techniques help avoid representational collapse in learning self-predictive representations;
  (3) A minimalist RL algorithm that learns self-predictive representations end-to-end
  with a single auxiliary loss, without reward model learning, planning, or metric
  learning; (4) Extensive empirical validation across standard MDPs, distracting MDPs,
  and sparse-reward POMDPs, showing that the proposed approach achieves similar or
  better sample efficiency than prior methods while being simpler and more robust
  to distractions.'
---

# Bridging State and History Representations: Understanding Self-Predictive RL

## Quick Facts
- arXiv ID: 2401.08898
- Source URL: https://arxiv.org/abs/2401.08898
- Reference count: 40
- Primary result: Establishes unified theoretical framework showing that self-predictive representations can be learned end-to-end with a single auxiliary loss, achieving comparable sample efficiency to prior methods

## Executive Summary
This paper provides a unified theoretical framework for understanding state and history representations in reinforcement learning by revealing that many prior methods optimize interconnected properties representing facets of self-predictive abstraction. The authors develop a minimalist RL algorithm that learns self-predictive representations end-to-end using a single auxiliary loss, without requiring reward model learning, planning, or metric learning. Through extensive empirical validation across standard MDPs, distracting MDPs, and sparse-reward POMDPs, the approach demonstrates similar or better sample efficiency compared to prior methods while being simpler and more robust to distractions.

## Method Summary
The method learns self-predictive representations by optimizing expected reward prediction and next latent state distribution prediction through a single auxiliary loss. The core algorithm consists of an encoder that maps histories/states to latent representations, a latent transition model that predicts next latent states, a critic that estimates values, and an actor that selects actions. The ZP loss connects the encoder and transition model, while the RL loss connects all components. Key technical innovations include the use of stop-gradient techniques to prevent representational collapse and the theoretical insight that end-to-end learning can implicitly capture reward prediction when learning self-predictive representations.

## Key Results
- The minimalist RL algorithm achieves comparable sample efficiency to prior methods on standard MuJoCo and MiniGrid benchmarks
- Stop-gradient techniques provably avoid representational collapse in linear models and empirically prevent collapse in deep networks
- End-to-end learning of self-predictive representations matches the performance of phased training approaches
- The approach demonstrates superior robustness to observation distractions compared to observation-predictive methods

## Why This Works (Mechanism)

### Mechanism 1
Self-predictive representations capture essential state/history information by learning to predict next latent states. The encoder learns a compressed representation that contains sufficient information to predict both expected rewards (RP) and next latent state distributions (ZP), forming a self-consistent predictive loop. This works because the ZP condition creates a stationary point where the encoder contains all information needed for future prediction.

### Mechanism 2
Stop-gradient techniques prevent representational collapse by maintaining orthogonal initialization properties. By detaching the target encoder in the ZP loss, the gradient dynamics preserve the rank of the encoder matrix during training, preventing the representation from collapsing to a constant. This is theoretically proven under linear assumptions where the continuous-time dynamics of stop-gradient training maintain ϕ⊤ϕ constant.

### Mechanism 3
End-to-end learning of self-predictive representations can achieve comparable sample efficiency to phased training because Thm. 2 shows that ZP + ϕQ∗ imply RP. This means that learning these conditions together end-to-end implicitly learns the reward prediction, eliminating the need for separate phases. The optimal value function Q(ϕ(h), a) = Q∗(h, a) can be learned while simultaneously learning ZP.

## Foundational Learning

- **Concept: Markov Decision Processes (MDPs) and Partially Observable MDPs (POMDPs)**
  - Why needed here: The entire paper builds on the distinction between fully observable MDPs and partially observable POMDPs, where history representations become crucial
  - Quick check question: What is the key difference between an MDP and a POMDP, and why does this difference necessitate history representations?

- **Concept: State and History Abstractions**
  - Why needed here: Understanding different abstraction types (Q∗-irrelevance, self-predictive, observation-predictive) is fundamental to grasping the paper's unified framework
  - Quick check question: What are the three main types of abstractions discussed, and how do they relate hierarchically?

- **Concept: Representation Learning in RL**
  - Why needed here: The paper's core contribution is about learning effective representations, so understanding representation learning objectives and techniques is essential
  - Quick check question: What is the difference between online and stop-gradient targets in representation learning, and why does this distinction matter?

## Architecture Onboarding

- **Component map**: history/state → encoder → latent representation → (1) value prediction for RL loss, (2) next latent prediction for ZP loss → update all parameters jointly
- **Critical path**: The critical path for learning is: history/state → encoder → latent representation → (1) value prediction for RL loss, (2) next latent prediction for ZP loss → update all parameters jointly
- **Design tradeoffs**: Stop-gradient vs online targets (stability vs direct optimization), ℓ2 vs KL divergence for ZP loss (simplicity vs distribution matching), phased vs end-to-end learning (stability vs sample efficiency)
- **Failure signatures**: Dimensional collapse (low-rank representations), poor sample efficiency (learning slower than baselines), representational mismatch (encoder doesn't capture task-relevant information)
- **First 3 experiments**:
  1. Implement the minimalist algorithm on a simple MDP (like Mountain Car) and verify that ZP targets preserve rank while online targets collapse
  2. Compare sample efficiency of end-to-end vs phased learning on a sparse-reward POMDP (like MiniGrid tasks)
  3. Test distraction robustness by adding Gaussian noise to observations and measuring performance degradation of observation-predictive vs self-predictive methods

## Open Questions the Paper Calls Out

### Open Question 1
How does the performance of minimalist RL algorithms compare to more complex representation learning methods in high-dimensional pixel-based environments with real-world video backgrounds as distractors? The paper suggests their minimalist algorithm can provide clearer understanding of how different representation learning strategies affect sample efficiency, but their experiments are limited to vector-based tasks without covering more complicated domains requiring pixel-based observations.

### Open Question 2
Can the benefits of end-to-end learning of self-predictive representations extend to POMDPs when employing a minimalist approach in representation learning? While theoretical analysis suggests end-to-end learning can implicitly learn reward prediction, this is based on linear models without incorporating RL loss, and the paper acknowledges this may not translate to practical scenarios with deep RL agents.

### Open Question 3
What are the reasons behind the significant performance degradation observed when switching from stop-gradient ZP targets to online ZP targets in both MDPs and POMDPs? The paper observes lower rank of online targets relative to both detached and EMA targets, but does not provide detailed analysis of the reasons behind the performance degradation and the relationship between representational rank and expected returns.

## Limitations
- Theoretical analysis assumes linear models for stop-gradient effects, but practical implementation uses nonlinear deep networks where these guarantees may not hold
- Empirical evaluation focuses on standard benchmark tasks, but robustness claims to distractions and sparse rewards need validation on more diverse real-world scenarios
- The minimalist algorithm's hyperparameter sensitivity (particularly the ZP loss coefficient) is not thoroughly explored

## Confidence
- **High confidence** in the theoretical framework unifying state and history abstractions through self-predictive representations (supported by formal theorems and empirical validation across multiple benchmarks)
- **Medium confidence** in the stop-gradient mechanism preventing representational collapse (theoretically proven for linear cases, but unverified for deep networks)
- **Medium confidence** in end-to-end learning achieving comparable sample efficiency to phased methods (empirically demonstrated, but with limited hyperparameter tuning)
- **Low confidence** in the practical guidelines for practitioners (preliminary and based on limited empirical scope)

## Next Checks
1. Test the stop-gradient mechanism on deeper nonlinear networks by systematically comparing representation rank dynamics with and without stop-gradient across varying network depths
2. Evaluate distraction robustness on environments with structured noise (e.g., adversarial observations) beyond Gaussian noise to validate claims about observation-predictive vs self-predictive methods
3. Implement the minimalist algorithm on high-dimensional visual control tasks (e.g., Atari games) to assess scalability and identify potential failure modes in complex observation spaces