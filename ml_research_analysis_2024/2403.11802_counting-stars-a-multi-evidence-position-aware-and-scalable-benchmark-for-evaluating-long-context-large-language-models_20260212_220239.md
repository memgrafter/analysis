---
ver: rpa2
title: 'Counting-Stars: A Multi-evidence, Position-aware, and Scalable Benchmark for
  Evaluating Long-Context Large Language Models'
arxiv_id: '2403.11802'
source_url: https://arxiv.org/abs/2403.11802
tags:
- llms
- context
- multi-evidence
- long-context
- long
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'Counting-Stars introduces a scalable benchmark for evaluating
  long-context LLMs'' multi-evidence retrieval capabilities through two tasks: searching
  (finding numerical evidence in long contexts) and reasoning (identifying correct
  evidence among distractors). The benchmark tests LLMs across context lengths from
  4K to 128K tokens with 32 pieces of evidence inserted at regular intervals.'
---

# Counting-Stars: A Multi-evidence, Position-aware, and Scalable Benchmark for Evaluating Long-Context Large Language Models

## Quick Facts
- **arXiv ID**: 2403.11802
- **Source URL**: https://arxiv.org/abs/2403.11802
- **Reference count**: 8
- **Key outcome**: Gemini 1.5 Pro achieves the best overall performance on multi-evidence retrieval tasks across context lengths from 4K to 128K tokens, while GPT-4 Turbo exhibits the most stable performance.

## Executive Summary
Counting-Stars is a new benchmark designed to evaluate long-context large language models' (LLMs) ability to retrieve and reason with multiple pieces of evidence distributed throughout extended contexts. The benchmark introduces two tasks—searching and reasoning—that test whether models can not only find numerical evidence but also distinguish correct from incorrect information. Tested across context lengths from 4K to 128K tokens with 32 pieces of evidence inserted at regular intervals, the benchmark reveals that current long-context LLMs still struggle with multi-evidence tasks despite strong performance on simpler single-evidence benchmarks like needle-in-a-haystack.

## Method Summary
The Counting-Stars benchmark evaluates LLMs on two tasks: searching (finding numerical evidence in long contexts) and reasoning (identifying correct evidence among distractors). Long context text is generated from sources like Paul Graham Essays and The Story of the Stone, with 32 pieces of numerical evidence inserted at regular intervals. The benchmark tests models across context lengths from 4K to 128K tokens, measuring performance using P@32 (precision at 32) and P@32* (precision at 32 with reasoning). The evaluation is conducted using various access approaches (API and poe.com) for different models, with results parsed and scored to assess multi-evidence retrieval capabilities.

## Key Results
- Gemini 1.5 Pro achieves the best overall performance on both searching and reasoning tasks across all context lengths
- GPT-4 Turbo exhibits the most stable performance across varying context lengths, showing minimal degradation
- Performance generally degrades as context length increases, with no strong evidence of lost-in-the-middle phenomenon
- All tested LLMs struggle significantly more with the reasoning task compared to the searching task

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Multi-evidence retrieval tasks reveal LLM performance gaps that simple single-evidence benchmarks miss
- Mechanism: By requiring LLMs to collect multiple pieces of evidence distributed throughout long contexts, Counting-Stars exposes limitations in attention mechanisms and memory that aren't apparent in single-evidence tasks like needle-in-a-haystack
- Core assumption: Long-context LLMs need to demonstrate not just retrieval but also reasoning across multiple evidence points to be considered truly capable
- Evidence anchors:
  - [abstract] "Despite these developments, the efficacy of models in long-context settings still needs to be examined, primarily due to the lack of a robust evaluation benchmark"
  - [section] "However, many newly released long-context LLMs have adopted the needle-in-a-haystack benchmark to evaluate their long-context processing capabilities and have achieved nearly perfect performance"
  - [corpus] Weak - corpus doesn't directly address this specific mechanism, but related papers mention "long-context reasoning" and "multi-document question answering"
- Break condition: If LLMs develop perfect attention mechanisms that can track all evidence regardless of position, this mechanism's discriminative power would diminish

### Mechanism 2
- Claim: Position-aware evaluation reveals lost-in-the-middle phenomena and other positional biases
- Mechanism: By inserting evidence at regular intervals and testing performance across different context lengths, the benchmark can identify whether LLMs have positional biases in their processing, particularly middle-of-context degradation
- Core assumption: The distribution of evidence throughout the context will reveal systematic weaknesses in how LLMs process different positions
- Evidence anchors:
  - [abstract] "Furthermore, our analysis of these LLMs, which have been extended to handle long-context scenarios, indicates that significant room for improvement remains as the length of the input context and the complexity of the tasks increase"
  - [section] "Position-aware. In long-context scenarios, a typical bad case is that when the answer to a question appears in different positions of the long context, the performance of LLMs varies greatly, such as the lost-in-the-middle phenomenon"
  - [corpus] Weak - corpus contains related benchmarks but doesn't specifically discuss position-aware evaluation methods
- Break condition: If LLMs develop perfect positional invariance or if positional effects are found to be task-specific rather than model-specific

### Mechanism 3
- Claim: Scalable benchmark design allows for testing across varying context lengths and evidence quantities
- Mechanism: The benchmark's parametric design (adjustable context length and evidence count) enables researchers to systematically test LLM performance across different scales, revealing how performance degrades with increasing complexity
- Core assumption: The ability to scale the benchmark parameters is essential for evaluating future LLMs with even longer context windows
- Evidence anchors:
  - [abstract] "Scalable. As mentioned earlier, developing long-context benchmarks often lags behind the speed of long-context LLMs"
  - [section] "Various approaches have been proposed to expand the context window of LLMs to accommodate even up to 128K input tokens or more"
  - [corpus] Weak - corpus mentions related benchmarks but doesn't specifically discuss scalable benchmark design
- Break condition: If benchmark scaling becomes computationally prohibitive or if performance plateaus at certain scales

## Foundational Learning

- Concept: Multi-evidence retrieval and reasoning
  - Why needed here: The benchmark tests both searching (finding evidence) and reasoning (distinguishing correct from incorrect evidence), requiring understanding of how LLMs process multiple pieces of information simultaneously
  - Quick check question: How does the reasoning task differ from the searching task in Counting-Stars?

- Concept: Position-aware evaluation methodology
  - Why needed here: Understanding how to design experiments that reveal positional biases requires knowledge of experimental design and statistical analysis
  - Quick check question: What would you measure to determine if a model exhibits lost-in-the-middle behavior?

- Concept: Scalable benchmark design principles
  - Why needed here: The parametric nature of the benchmark requires understanding how to design experiments that can be adjusted for different scales
  - Quick check question: Why is it important for a benchmark to be scalable when evaluating long-context LLMs?

## Architecture Onboarding

- Component map: Context generation → Evidence insertion → Prompt construction → LLM inference → Result parsing → Evaluation scoring
- Critical path: Context generation → Evidence insertion → Prompt construction → LLM inference → Result parsing → Evaluation scoring
- Design tradeoffs: Simple numerical evidence vs. more complex text evidence (simpler evaluation but less realistic); fixed intervals vs. random distribution (easier analysis but less natural); parametric scalability vs. fixed design (future-proof but more complex)
- Failure signatures: Poor performance on reasoning task indicates attention/memory issues; degraded performance at specific positions indicates positional bias; inconsistent performance across similar context lengths indicates instability
- First 3 experiments:
  1. Test all LLMs on 4K context length to establish baseline performance
  2. Test performance degradation across context lengths (4K→128K) to identify scaling issues
  3. Compare searching vs. reasoning task performance to isolate attention vs. reasoning capabilities

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the lost-in-the-middle phenomenon occur in long-context LLMs when evaluating tasks beyond 16K tokens?
- Basis in paper: [explicit] The paper states that prior research indicates a performance decline in some LLMs when answers are positioned around the middle of long contexts (Liu et al., 2023), but their findings cannot strongly corroborate this phenomenon beyond 16K tokens.
- Why unresolved: The paper's experiments are based on context lengths up to 128K tokens, but the analysis of the lost-in-the-middle phenomenon is limited to a specific task and does not provide conclusive evidence for tasks beyond 16K tokens.
- What evidence would resolve it: Conducting experiments on a wider range of tasks and context lengths, particularly focusing on tasks beyond 16K tokens, would provide more comprehensive insights into the occurrence of the lost-in-the-middle phenomenon in long-context LLMs.

### Open Question 2
- Question: What are the specific reasons behind the length-stability dilemma observed in LLMs, where performance varies significantly across different context lengths?
- Basis in paper: [explicit] The paper discusses the length-stability dilemma, noting that the same task performs well with long input context lengths but poorly at shorter contexts, such as 112K and 108K.
- Why unresolved: The paper acknowledges the phenomenon but does not provide a definitive explanation for why LLMs exhibit varying performance across different context lengths.
- What evidence would resolve it: Investigating the internal mechanisms of LLMs, such as attention mechanisms and context processing strategies, across different context lengths could reveal the underlying reasons for the length-stability dilemma.

### Open Question 3
- Question: How do different access approaches (e.g., API vs. poe.com) affect the evaluation results of long-context LLMs?
- Basis in paper: [explicit] The paper mentions that different services are used to test the LLMs, with some models tested via API and others via poe.com, which may introduce biases in the testing results.
- Why unresolved: The paper does not provide a detailed analysis of how different access approaches impact the evaluation outcomes of LLMs.
- What evidence would resolve it: Conducting controlled experiments using multiple access approaches for the same models and comparing the results would help determine the impact of different access methods on evaluation outcomes.

## Limitations

- The benchmark's focus on numerical evidence may not fully capture the complexity of real-world long-context tasks that often involve nuanced textual information
- Fixed-interval insertion of evidence at regular positions (every 4K tokens) may create artificial patterns that models could learn to exploit
- Evaluation is limited to a specific set of LLMs and may not generalize to all long-context models, particularly those using different architectural approaches

## Confidence

- **High Confidence**: The benchmark's core design and methodology for evaluating multi-evidence retrieval capabilities are sound and well-documented
- **Medium Confidence**: The claim that Gemini 1.5 Pro outperforms other models is supported by the data, but results may vary with different experimental conditions
- **Low Confidence**: The assertion that there is "no strong evidence of lost-in-the-middle phenomenon" is based on the specific experimental setup and may not hold for all types of long-context tasks

## Next Checks

1. **Cross-task generalization**: Test whether Counting-Stars performance correlates with other long-context tasks (e.g., document QA, multi-hop reasoning) to validate its effectiveness as a general benchmark for long-context capabilities.

2. **Text evidence variant**: Implement and evaluate a variant of Counting-Stars using textual rather than numerical evidence to assess whether the benchmark's findings hold for more realistic information retrieval scenarios.

3. **Positional distribution analysis**: Conduct a more granular analysis of model performance across different positions within the context, testing whether the apparent absence of lost-in-the-middle effects holds when examining smaller positional windows and different types of evidence placement strategies.