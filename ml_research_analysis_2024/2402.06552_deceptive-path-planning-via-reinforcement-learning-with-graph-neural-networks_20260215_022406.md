---
ver: rpa2
title: Deceptive Path Planning via Reinforcement Learning with Graph Neural Networks
arxiv_id: '2402.06552'
source_url: https://arxiv.org/abs/2402.06552
tags:
- goal
- deception
- agent
- graph
- policies
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces a reinforcement learning method for deceptive
  path planning using graph neural networks. It addresses the limitations of existing
  approaches that rely on unrealistic assumptions, lack scalability, and cannot generalize
  to unseen problems.
---

# Deceptive Path Planning via Reinforcement Learning with Graph Neural Networks

## Quick Facts
- arXiv ID: 2402.06552
- Source URL: https://arxiv.org/abs/2402.06552
- Authors: Michael Y. Fatemi; Wesley A. Suttle; Brian M. Sadler
- Reference count: 40
- Key outcome: Reinforcement learning method for deceptive path planning using graph neural networks that generalizes to unseen environments without retraining.

## Executive Summary
This paper introduces a reinforcement learning approach to deceptive path planning that overcomes limitations of classical methods. The method uses graph neural networks (GNNs) with local perception to enable scalable deception in arbitrary weighted graphs. By incorporating deception bonuses into the reward function and training on varied small environments, the agent learns to balance goal-reaching with misleading an observer about its true destination.

## Method Summary
The method formulates deceptive path planning as an RL problem where an agent navigates from start to true goal while misleading observers using either exaggeration (moving toward decoy first) or ambiguity (maintaining uncertainty). The agent observes only a k-hop neighborhood subgraph at each step, encoded with node attributes and processed by a GraphSAGE network. Deception is rewarded through two bonuses: r_e for exaggeration (maximizing decoy probability minus true goal probability) and r_a for ambiguity (equalizing distances to decoy and true goals). Training uses PPO on small gridworlds with sampled start/goal/decoy positions and time limits, enabling the policy to adapt deception levels based on available time.

## Key Results
- Trained policies generalize to larger, more complex environments than seen during training
- Policies scale to previously unseen problems with different graph topologies
- Policies exhibit tunable levels of deception by varying maximum time limits without additional fine-tuning
- Method outperforms classical linear programming approaches in scalability and generalization

## Why This Works (Mechanism)

### Mechanism 1
Local perception combined with GNN message passing enables scalable deception in unseen environments. The agent observes only a k-hop neighborhood subgraph (Nₖ(sₜ)) at each step, encoded into node attributes and passed through a GraphSAGE network. Message passing aggregates neighbor information over k layers, allowing the agent to infer deceptive actions without seeing the full graph. Core assumption: k-hop subgraphs contain sufficient structural information to distinguish deceptive vs. direct paths, and GNN weights generalize across unseen graph topologies.

### Mechanism 2
Deceptive reward bonuses translate classical deception objectives into RL-compatible signals. Two bonuses are defined: rₑ for exaggeration (maximizing decoy probability minus true goal probability) and rₐ for ambiguity (rewarding equal distance to decoy and true goals). These are embedded in the episode reward Rₜ, which also enforces goal reaching and penalizes cycles/timeouts. Core assumption: The reward bonuses are aligned with deception metrics from classical DPP and provide stable gradients during RL training.

### Mechanism 3
Training on small, varied graphs with time-limit sampling yields tunable deceptiveness without retraining. During training, start/goal/decoy positions and Tₘₐₓ are sampled from uniform distributions, forcing the agent to learn policies that adapt to urgency: low Tₘₐₓ yields direct paths, high Tₘₐₓ enables deception. At test time, Tₘₐₓ is varied dynamically. Core assumption: The RL agent internalizes a mapping from Tₘₐₓ to deception level, and the GNN architecture preserves this mapping across unseen graphs.

## Foundational Learning

- **Graph Neural Networks and message passing**: Why needed here: GNNs process the local subgraph perception model, enabling generalization to arbitrary weighted graphs without retraining. Quick check question: How does a k-layer GNN propagate information from a node's k-hop neighborhood into its representation?

- **Reinforcement Learning with custom reward shaping**: Why needed here: RL trains the agent to balance deception bonuses with goal reaching, replacing classical linear programming with online learning. Quick check question: What is the effect of embedding the deception bonus r(ζ₁:ₜ) into the episode reward Rₜ on the policy's learned behavior?

- **Local observability and perception models in sequential decision making**: Why needed here: The agent only sees Nₖ(sₜ), mimicking realistic sensor ranges and forcing the policy to infer global deception cues from local structure. Quick check question: Why does limiting perception to a k-hop subgraph not prevent the agent from planning deceptive long-range paths?

## Architecture Onboarding

- **Component map**: Environment (graph with nodes S, edges A, weights c; start s₁, true goal G*, decoy goal G) -> Perception (k-hop neighborhood subgraph Nₖ(sₜ) with node attributes) -> Policy/value networks (k-layer GraphSAGE with 64-dim features) -> Training loop (PPO with Adam, sampled Tₘₐₓ) -> Deception bonuses (rₑ for exaggeration, rₐ for ambiguity)

- **Critical path**: 
  1. Sample episode: graph, start, goals, Tₘₐₓ
  2. At each step t: observe Nₖ(sₜ), encode node attributes, run GNN to produce action distribution
  3. Sample action, step environment, receive reward Rₜ
  4. After episode: update policy/value via PPO; retroactively nullify deceptive rewards if goal missed

- **Design tradeoffs**:
  - k (GNN layers / perception radius): higher k → more context but higher compute and potential overfitting; lower k → faster but may miss long-range deception cues
  - Node attribute dimensionality: richer attributes → better discrimination but larger GNN; minimal attributes → faster but weaker signal
  - Reward shaping: strong deception bonuses → more deceptive but risk of missing goal; weak bonuses → goal-focused but less deceptive

- **Failure signatures**:
  - Policy consistently chooses shortest path → deception bonuses too weak or GNN underfit
  - Policy fails to reach goal within Tₘₐₓ → Tₘₐₓ sampling too low or reward for reaching goal insufficient
  - Policy performance drops sharply on larger graphs → k too small or GNN overfitting to training graph size

- **First 3 experiments**:
  1. Train on 8x8 gridworlds only, test on same size: verify basic deception and goal reaching
  2. Train on 8x8 and 16x16, test on 32x32: assess scalability and generalization
  3. Fix graph, vary Tₘₐₓ at test time: confirm tunability without retraining

## Open Questions the Paper Calls Out

### Open Question 1
How does the performance of the proposed method compare to classical planning-based methods (e.g., [36]) on unseen gridworld environments, especially in terms of deception metrics and computational efficiency? Basis in paper: The paper mentions that existing methods rely on unrealistic assumptions and lack generalizability to unseen problems. It also compares the proposed method to a linear programming baseline in the appendix. Why unresolved: The paper does not provide a direct comparison of the proposed method to classical planning-based methods on unseen gridworld environments. The comparison to the linear programming baseline is limited to a few specific gridworld sizes and does not cover the full range of experiments presented for the proposed method. What evidence would resolve it: A comprehensive comparison of the proposed method to classical planning-based methods on a wide range of unseen gridworld environments, evaluating both deception metrics and computational efficiency.

### Open Question 2
How does the choice of the perception radius (k-hop neighborhood) affect the performance of the proposed method on continuous navigation tasks, and what is the optimal value for different task complexities? Basis in paper: The paper mentions that the perception radius (k) affects the agent's local observability and is a key factor in the proposed method. It also presents experiments on continuous forest navigation tasks with varying visibility radii. Why unresolved: The paper does not provide a systematic analysis of the impact of the perception radius on the performance of the proposed method on continuous navigation tasks. It also does not discuss how to determine the optimal value of k for different task complexities. What evidence would resolve it: A thorough analysis of the relationship between the perception radius and the performance of the proposed method on continuous navigation tasks, including an investigation of the optimal value of k for different task complexities.

### Open Question 3
How does the proposed method perform in dynamic environments where the true goal or decoy goal positions change over time, and how can the method be extended to handle such scenarios effectively? Basis in paper: The paper presents an experiment on dynamic adaptivity where the decoy goal position is switched during a trajectory. It mentions that this could lead to a pursuing agent or different deception strategies. Why unresolved: The paper does not provide a comprehensive analysis of the proposed method's performance in fully dynamic environments where both the true goal and decoy goal positions change over time. It also does not discuss potential extensions of the method to handle such scenarios effectively. What evidence would resolve it: A thorough evaluation of the proposed method's performance in dynamic environments with changing goal positions, including an analysis of its adaptivity and potential extensions to handle such scenarios effectively.

## Limitations
- Limited direct comparison against classical DPP solvers beyond single benchmark instances
- Assumption that k-hop subgraphs contain sufficient structural information for deception not rigorously validated
- Continuous forest environment evaluation relies on specific Voronoi-based obstacle generator without exploring alternative continuous spaces

## Confidence

- **High confidence**: The claim that the method scales to larger graphs than training environments is supported by clear empirical evidence showing successful navigation and deception in 64x64 and 128x128 grids when trained only on 8x8 and 16x16. The core architectural approach (GNN + local perception + deception bonuses) is internally consistent and mechanistically sound.

- **Medium confidence**: The claim that deception is tunable via T_max without retraining is demonstrated in controlled experiments but relies on the assumption that the GNN policy preserves the T_max-to-deception mapping across unseen graphs. While heatmaps show the expected trend, the transferability across diverse graph structures remains an assumption.

- **Low confidence**: The claim of superiority over classical DPP methods is weakly supported, as comparisons are limited to a single illustrative example rather than systematic benchmarking. The assertion that this approach solves the scalability limitations of classical methods is not rigorously tested against state-of-the-art DPP solvers on standard benchmarks.

## Next Checks

1. **Ablation study on k-hop radius**: Systematically vary k (1-4) and measure deception success rates and goal completion on graphs with increasing diameter to identify the minimum k that preserves deception capability across diverse topologies.

2. **Classical solver comparison suite**: Implement the strongest available linear programming or mixed-integer programming DPP solver and compare performance (deception metrics, runtime, success rate) on a standardized benchmark set of gridworlds and random graphs across multiple size scales.

3. **Transfer robustness analysis**: Train on a specific graph family (e.g., gridworlds with 30% obstacle density) and test on structurally different graphs (e.g., scale-free networks, random geometric graphs) while measuring both deception metrics and GNN feature similarity to identify when and why generalization fails.