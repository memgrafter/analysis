---
ver: rpa2
title: 'LEGO: Language Model Building Blocks'
arxiv_id: '2410.18287'
source_url: https://arxiv.org/abs/2410.18287
tags:
- client
- slms
- data
- lego
- global
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: LEGO introduces a novel federated learning framework that enables
  efficient, privacy-preserving training of large language models (LLMs) by decomposing
  them into smaller, task-specific language models (SLMs) that can be recombined after
  local fine-tuning. The method leverages model pruning and heterogeneous model aggregation
  to create flexible client-side architectures that adapt to diverse computational
  constraints while maintaining model robustness.
---

# LEGO: Language Model Building Blocks

## Quick Facts
- **arXiv ID**: 2410.18287
- **Source URL**: https://arxiv.org/abs/2410.18287
- **Authors**: Shrenik Bhansali; Alwin Jin; Tyler Lizzo; Larry Heck
- **Reference count**: 40
- **Primary result**: LEGO achieves comparable performance to centralized fine-tuning baselines while enabling up to 1.6× speedup in inference and 1.4× speedup in fine-tuning, particularly excelling in scenarios with non-i.i.d. data distributions and heterogeneous client resources.

## Executive Summary
LEGO introduces a novel federated learning framework that enables efficient, privacy-preserving training of large language models (LLMs) by decomposing them into smaller, task-specific language models (SLMs) that can be recombined after local fine-tuning. The method leverages model pruning and heterogeneous model aggregation to create flexible client-side architectures that adapt to diverse computational constraints while maintaining model robustness. Experimental results demonstrate that LEGO achieves comparable performance to centralized fine-tuning baselines while enabling significant speedups in both inference and fine-tuning.

## Method Summary
LEGO is a federated learning framework that decomposes large language models into smaller, task-specific language models (SLMs) through model pruning, fine-tunes these SLMs locally on client devices using parameter-efficient LoRA adapters, and recombines them through a novel heterogeneous aggregation scheme (HeteAgg). The framework uses SparseGPT for pruning, creates multiple sparsity levels (0%, 25%, 50%, 75%), and employs a custom aggregation method that preserves sparsity patterns by only averaging overlapping non-zero parameters. This approach enables flexible client model selection, mitigates data heterogeneity effects, and maintains privacy while achieving computational efficiency gains.

## Key Results
- LEGO achieves comparable performance to centralized fine-tuning baselines on HellaSwag, MMLU, SciQ, and ARC benchmarks
- Demonstrates up to 1.6× speedup in inference and 1.4× speedup in fine-tuning compared to traditional approaches
- Shows robustness in non-i.i.d. data distribution scenarios while maintaining LLM performance
- Smaller models (25% sparsity) show stronger knowledge transfer to larger models compared to the reverse direction

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LEGO's aggregation scheme (HeteAgg) allows knowledge transfer between heterogeneous SLMs by preserving sparsity patterns and aggregating only overlapping non-zero parameters.
- Mechanism: During aggregation, each client's LoRA adapter is masked by its sparsity pattern. Only the non-zero parameters are averaged with the corresponding parameters in the global adapter. Zero parameters in the client adapter are preserved, maintaining the client's sparsity structure.
- Core assumption: The non-zero parameters in a pruned model still capture essential information that can be transferred to other models, even if those models have different sparsity patterns or architectures.
- Evidence anchors: [abstract]: "LEGO utilizes Federated Learning and a novel aggregation scheme for the LLM reconstruction, maintaining robustness without high costs and preserving user data privacy." [section]: "By only aggregating across the nonzero weights, we can retain the sparsity in the client model's adapter without halving the global adapter's weights when there is no corresponding nonzero value."

### Mechanism 2
- Claim: Smaller models (SLMs) are better learners than larger models (LLMs) due to their constrained capacity forcing stronger, more general representations.
- Mechanism: With fewer parameters, SLMs must learn more efficient representations of the data. This constraint acts like a regularizer, preventing overfitting and encouraging the model to capture the most essential features. When aggregated with larger models, these strong representations can improve the overall performance.
- Core assumption: The representational efficiency gained from model compression outweighs the loss of capacity, especially when the compressed model is fine-tuned on the same task.
- Evidence anchors: [section]: "Previous work has shown that smaller models are better learners (Turc et al., 2019; Raffel et al., 2020), creating an effect similar to dropout, forcing the limited neurons to create stronger and more general representations." [section]: "The 75% sparsity model's degraded performance after aggregation is likely due to the SLM's limited size."

### Mechanism 3
- Claim: LEGO mitigates the effects of data heterogeneity by allowing each client to learn from their local data distribution and then share this knowledge through aggregation.
- Mechanism: In traditional federated learning, data heterogeneity can lead to client models learning different representations that are difficult to aggregate. LEGO addresses this by allowing each SLM to learn from its local data distribution and then aggregating the learned representations. This process allows the global model to incorporate knowledge from diverse data distributions.
- Core assumption: The knowledge learned by each SLM from its local data distribution is complementary and can be effectively combined through aggregation to create a more robust global model.
- Evidence anchors: [abstract]: "We experimentally demonstrate the versatility of LEGO, showing its ability to enable model heterogeneity and mitigate the effects of data heterogeneity while maintaining LLM robustness." [section]: "Figure 3 demonstrates that after every global update, the performance of the client SLMs increase almost linearly, despite the local data for each client not being the same length."

## Foundational Learning

- Concept: Federated Learning (FL)
  - Why needed here: LEGO is built on top of FL, using its distributed training framework to preserve user data privacy while allowing for collaborative model training.
  - Quick check question: What is the main difference between traditional centralized training and federated learning in terms of data privacy and communication?

- Concept: Model Pruning
  - Why needed here: LEGO uses model pruning to create SLMs from LLMs, reducing their size and computational requirements while preserving essential information.
  - Quick check question: What is the main difference between magnitude pruning and activation-based pruning, and why might activation-based pruning be more effective for LEGO?

- Concept: Parameter-Efficient Fine-Tuning (PEFT)
  - Why needed here: LEGO uses LoRA adapters for fine-tuning, which are a type of PEFT method that adds small, low-rank updates to the frozen model weights, reducing the computational cost of fine-tuning.
  - Quick check question: How does LoRA differ from full fine-tuning in terms of the number of parameters updated and the computational cost?

## Architecture Onboarding

- Component map: LLM -> SparseGPT pruning -> SLM -> LoRA adapter -> HeteAgg aggregation -> Global LLM
- Critical path:
  1. Prune an LLM to create SLMs with different sparsity levels
  2. Assign each SLM to a client device
  3. Fine-tune each SLM on the client's local data using LoRA adapters
  4. Aggregate the LoRA adapters using HeteAgg to create a global update
  5. Apply the global update to all SLMs and the global LLM
  6. Repeat steps 3-5 for multiple rounds of federated learning
  7. Evaluate the final global LLM on benchmark tasks

- Design tradeoffs:
  - Model size vs. performance: Smaller models (higher sparsity) are more efficient but may have lower performance. The optimal sparsity level depends on the specific task and computational constraints.
  - Aggregation method: HeteAgg preserves sparsity patterns but may introduce noise due to the separate aggregation of LoRA modules. Other aggregation methods may be more accurate but less efficient.
  - Pruning method: Different pruning methods (e.g., SparseGPT vs. Wanda) have different computational costs and may produce models with different performance characteristics.

- Failure signatures:
  - Degradation in performance after aggregation: This may indicate that the sparsity patterns are too divergent or that the aggregation method is not effectively combining the learned representations.
  - Underfitting or overfitting: This may indicate that the model size or fine-tuning strategy is not well-suited to the task or data distribution.
  - Communication or computational bottlenecks: This may indicate that the model size or aggregation method is not well-suited to the available resources or network conditions.

- First 3 experiments:
  1. Compare the performance of SLMs with different sparsity levels on a benchmark task to determine the optimal sparsity level for a given task and computational constraint.
  2. Evaluate the effectiveness of HeteAgg in combining SLMs with different sparsity patterns and data distributions on a benchmark task.
  3. Compare the performance of LEGO with traditional federated learning methods (e.g., FedAvg) on a benchmark task with heterogeneous data distributions.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the performance degradation of 75% sparsity models after aggregation stem from fundamental limitations of extreme pruning or from suboptimal aggregation techniques that could be improved?
- Basis in paper: [explicit] The paper notes that "The 75% sparsity model's degraded performance after aggregation is likely due to the SLM's limited size" and that "as all aggregations degrade its performance, with the larger, denser models degrading it more"
- Why unresolved: The paper attributes the degradation to the smaller model's limited capacity but doesn't experimentally verify whether alternative aggregation methods or different pruning techniques could mitigate this issue.
- What evidence would resolve it: Comparative experiments testing different aggregation strategies (weighted averaging, selective aggregation, or hierarchical aggregation) specifically for high-sparsity models, or experiments with different pruning methods at 75% sparsity.

### Open Question 2
- Question: How does LEGO's performance scale when applied to larger language models (e.g., LLaMA-70B or beyond) and does the relative advantage over FedIT increase or decrease with model size?
- Basis in paper: [inferred] The experiments use LLaMA-7B as the base model, and the paper discusses efficiency gains but doesn't explore scaling to larger models where computational constraints become more severe.
- Why unresolved: The paper focuses on a single model size and doesn't investigate whether the relative benefits of LEGO (speedups, robustness) scale with model size or if different challenges emerge at larger scales.
- What evidence would resolve it: Experiments replicating the same LEGO methodology with larger base models, comparing both absolute performance and relative improvements over FedIT baselines across different model sizes.

### Open Question 3
- Question: Can LEGO's heterogeneous aggregation approach be extended to support dynamic model selection during federated learning, where client models adapt their sparsity levels based on real-time resource availability?
- Basis in paper: [explicit] The paper mentions that "LEGO allows for flexible client model selection, enabling knowledge transfer between models of different sizes and tailoring client models to suit device capabilities" but only demonstrates static assignment.
- Why unresolved: While the paper shows that heterogeneous models can be aggregated successfully, it doesn't explore whether models could dynamically change their sparsity levels during training or whether the aggregation method supports such dynamic heterogeneity.
- What evidence would resolve it: Experiments where client models can change their assigned sparsity levels mid-training, measuring whether the HeteAgg method can still produce robust global updates and whether performance is maintained or improved.

### Open Question 4
- Question: What is the theoretical relationship between model sparsity levels and their contribution to global knowledge transfer in LEGO, and can this relationship be formalized to predict optimal client model configurations?
- Basis in paper: [explicit] The paper shows empirical evidence that "smaller models create greater contributions to the 0% sparsity LLM" and "larger models do not transfer knowledge as effectively to smaller models" but doesn't provide a theoretical framework.
- Why unresolved: The paper presents observational results about knowledge transfer asymmetry but doesn't develop a theoretical understanding of why this occurs or how to predict optimal configurations for a given resource budget.
- What evidence would resolve it: Theoretical analysis connecting model capacity, sparsity levels, and learning dynamics to explain the observed knowledge transfer patterns, validated through controlled experiments with systematic sparsity configurations.

## Limitations
- The aggregation scheme (HeteAgg) lacks detailed implementation specifications, particularly around how sparsity patterns are preserved during parameter averaging.
- The claim that smaller models are "better learners" relies on previous work citations but lacks direct experimental validation within LEGO itself.
- The mechanism for mitigating data heterogeneity is plausible but not conclusively demonstrated - the study shows improved client performance but doesn't isolate whether this is due to the SLM decomposition or simply the federated learning framework itself.

## Confidence
- **High Confidence**: The performance claims showing LEGO achieving comparable results to centralized baselines while providing computational efficiency gains (1.6× inference speedup, 1.4× fine-tuning speedup) are well-supported by experimental results.
- **Medium Confidence**: The hypothesis that model pruning and recombination maintains robustness while preserving privacy is supported by results, but the exact mechanisms could benefit from additional ablation studies.
- **Low Confidence**: The specific claim about smaller models being "better learners" due to representational efficiency is based on external citations rather than LEGO-specific experiments.

## Next Checks
1. **Aggregation Scheme Validation**: Implement a controlled experiment comparing HeteAgg against standard FedAvg with identical model architectures to isolate the impact of heterogeneous model handling on performance.
2. **Sparsity Pattern Analysis**: Conduct ablation studies varying the degree of sparsity pattern divergence between client models to determine the breaking point where aggregation fails to transfer knowledge effectively.
3. **Data Heterogeneity Isolation**: Run experiments where the only variable is data distribution heterogeneity (identical models, identical aggregation) to confirm that LEGO specifically addresses the data heterogeneity problem rather than the aggregation framework generally improving performance.