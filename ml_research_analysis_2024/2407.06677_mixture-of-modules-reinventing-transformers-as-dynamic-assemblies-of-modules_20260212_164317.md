---
ver: rpa2
title: 'Mixture-of-Modules: Reinventing Transformers as Dynamic Assemblies of Modules'
arxiv_id: '2407.06677'
source_url: https://arxiv.org/abs/2407.06677
tags:
- modules
- computation
- training
- each
- assembly
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes Mixture-of-Modules (MoM), a novel Transformer
  architecture that breaks the depth-ordered convention by allowing tokens to dynamically
  assemble computation graphs from a set of modules. Each token iteratively selects
  K attention and feed-forward modules from a module pool using specialized routers,
  forming an assembly that expands its computation graph.
---

# Mixture-of-Modules: Reinventing Transformers as Dynamic Assemblies of Modules

## Quick Facts
- arXiv ID: 2407.06677
- Source URL: https://arxiv.org/abs/2407.06677
- Reference count: 28
- This paper proposes Mixture-of-Modules (MoM), a novel Transformer architecture that breaks the depth-ordered convention by allowing tokens to dynamically assemble computation graphs from a set of modules.

## Executive Summary
This paper introduces Mixture-of-Modules (MoM), a Transformer architecture that replaces fixed depth-ordered layers with dynamic module assembly. Each token iteratively selects attention and feed-forward modules from a pool using specialized routers, creating individualized computation graphs. The architecture is trained using a two-phase approach: first pre-training a vanilla Transformer, then initializing MoM with its modules and fine-tuning with dynamic assembly. Experiments on three model sizes demonstrate consistent performance improvements on GLUE and XSUM benchmarks.

## Method Summary
MoM introduces a dynamic assembly mechanism where tokens iteratively select K attention and FFN modules from a module pool using specialized routers. The architecture uses a two-phase training approach: pre-training a vanilla Transformer to acquire specialized modules, then fine-tuning with dynamic assembly. The router architecture uses GRU to maintain state across multiple selection steps. MoM unifies several existing Transformer variants as special cases while offering a learnable approach to reducing parameter redundancy.

## Key Results
- MoM consistently outperforms vanilla Transformers on GLUE and XSUM benchmarks across three model sizes
- MoM-large achieves 38% deeper computation graphs than GPT-2-large with 1.4 absolute GLUE and 1 absolute XSUM gain
- MoM achieves 16% TFLOPs reduction and 43% memory usage decrease while maintaining comparable performance

## Why This Works (Mechanism)

### Mechanism 1
MoM achieves superior performance by allowing tokens to dynamically assemble computation graphs from a pool of modules rather than following fixed depth-ordered layers. Each token iteratively selects K attention and FFN modules from a module pool using specialized routers, expanding its computation graph in the forward pass. This dynamic assembly replaces the static layer-by-layer processing of vanilla Transformers.

### Mechanism 2
The two-phase training approach significantly improves MoM performance by first pre-training modules with specialized functions and then fine-tuning the routers. Phase 1 pre-trains a vanilla Transformer where modules acquire distinct functionalities. Phase 2 initializes MoM with these pre-trained modules and trains the routers to assemble them dynamically. This avoids the degeneration issue where modules become homogeneous.

### Mechanism 3
MoM provides a unified framework that encompasses various dynamic computation allocation techniques as special cases, offering unprecedented flexibility in forward computation. With specific configurations, MoM can represent layer-skip, parameter sharing, and mixture-of-experts methods. This unification allows for dynamic depth and width adjustments beyond traditional paradigms.

## Foundational Learning

- Concept: Dynamic routing and conditional computation
  - Why needed here: MoM relies on routers to make multi-step decisions about which modules to select for each token, requiring understanding of how conditional computation works in neural networks.
  - Quick check question: What's the difference between a router in MoE and a router in MoM, and why does MoM's router need to maintain state across multiple selection steps?

- Concept: Module specialization and parameter initialization
  - Why needed here: The two-phase training approach depends on pre-trained modules having specialized functions, so understanding how parameter initialization affects learning is crucial.
  - Quick check question: Why does initializing MoM from scratch lead to homogeneous modules, and how does pre-training a vanilla Transformer solve this problem?

- Concept: Transformer architecture and attention mechanisms
  - Why needed here: MoM builds on standard Transformer components (MHA, FFN) but modifies their organization, requiring solid understanding of how these components work individually and together.
  - Quick check question: How does the attention mechanism in MoM differ from vanilla Transformers when modules are selected dynamically, and what implications does this have for gradient flow?

## Architecture Onboarding

- Component map: Token -> Router RA (selects K MHA modules) -> Assembly function ϕ -> Router RF (selects K FFN modules) -> Assembly function ϕ -> Output
- Critical path:
  1. Input token processed by router RA to select K MHA modules
  2. Selected MHA modules assembled via ϕ to produce intermediate representation
  3. Intermediate representation processed by router RF to select K FFN modules
  4. Selected FFN modules assembled via ϕ to produce final output
  5. Process repeats for H assembly steps

- Design tradeoffs:
  - Larger module pool (NA, NF) → More specialized functions but higher parameter count
  - More modules per step (K) → Better performance but increased computation per token
  - More assembly steps (H) → Deeper computation graphs but longer training and inference
  - GRU vs MLP routers → GRU maintains state across steps but is more complex to train

- Failure signatures:
  - Performance similar to vanilla Transformer → Routers not learning effective module selection
  - High variance in training loss → Router decisions too unstable or module specialization insufficient
  - Memory errors during training → Module pool too large for available GPU memory
  - Slow convergence → Router architecture too complex or learning rate too low

- First 3 experiments:
  1. Compare MoM-small (K1H1S) vs vanilla GPT-2-small on GLUE validation set to verify basic performance gains
  2. Test MoM-small with different K values (K1 vs K2 vs K3) to find optimal number of modules per step
  3. Implement MoM-medium with K3H1S configuration to verify the efficiency claims on TFLOPs and memory usage

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can router architectures be improved to handle multi-step decision making more effectively in MoM?
- Basis in paper: The paper discusses that current router implementation uses GRU but notes that the decision space grows exponentially with assembly steps, making it challenging for routers to make optimal decisions.
- Why unresolved: The paper acknowledges this limitation but doesn't propose specific solutions beyond mentioning potential future directions like reinforcement learning or neural architecture search.
- What evidence would resolve it: Experiments comparing different router architectures (e.g., transformer-based routers, hierarchical routing) on both convergence speed and final performance metrics.

### Open Question 2
- Question: What is the optimal balance between increasing module count versus increasing assembly depth for achieving maximum performance efficiency in MoM?
- Basis in paper: The paper shows that both larger K (modules per step) and larger H (steps) improve performance, but notes that benefits become marginal beyond certain thresholds.
- Why unresolved: While the paper explores various K and H combinations, it doesn't systematically analyze the trade-offs between expanding module pools versus increasing assembly depth across different model scales.
- What evidence would resolve it: Comprehensive ablation studies varying K and H independently while keeping total parameter count constant, measuring both performance and computational efficiency.

### Open Question 3
- Question: How does module specialization evolve during the two-phase training process, and can this be optimized?
- Basis in paper: The paper mentions that without two-phase training, modules become homogeneous and routers struggle to converge, but doesn't analyze how specialization develops during training.
- Why unresolved: The paper demonstrates the effectiveness of two-phase training but doesn't provide insights into the learning dynamics of module specialization or how to accelerate this process.
- What evidence would resolve it: Analysis of module similarity metrics during training, correlation between specialization and performance improvements, and experiments testing alternative initialization or regularization strategies.

## Limitations

- Limited dataset diversity: All experiments conducted on OpenWebText, GLUE, and XSUM datasets; performance gains may not generalize to other domains or languages
- Router architecture sensitivity: Training instability with MLP routers and lack of full specification for GRU router parameters
- Two-phase training dependency: Significant performance gains require complex two-phase training approach, adding computational overhead

## Confidence

**High confidence**: Core architectural framework is well-defined and experimental results showing consistent GLUE and XSUM improvements across three model sizes are reproducible.

**Medium confidence**: Claim that MoM unifies several existing Transformer variants is theoretically sound but practical validation across all mentioned variants is limited.

**Low confidence**: Assertion that MoM represents fundamentally different approach to conditional computation compared to existing methods lacks direct comparison with specific alternative implementations.

## Next Checks

1. **Cross-dataset generalization test**: Evaluate MoM on diverse datasets beyond GLUE and XSUM, including specialized domains (biomedical, legal, code) and non-English languages, to assess whether the 1.4 GLUE point improvement and 1.0 XSUM gain are consistent across different data distributions.

2. **Router architecture ablation study**: Systematically compare GRU routers against various MLP configurations with different hidden dimensions and initialization schemes to determine if training instability can be resolved without sacrificing performance, and to identify minimum viable router complexity.

3. **Alternative initialization validation**: Test whether two-phase training approach is essential by comparing MoM initialized from scratch (with techniques to prevent module homogenization) against pre-trained initialization, measuring both performance and training efficiency across different model scales.