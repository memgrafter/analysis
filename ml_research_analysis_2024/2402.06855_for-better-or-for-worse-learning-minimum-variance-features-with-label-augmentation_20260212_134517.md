---
ver: rpa2
title: For Better or For Worse? Learning Minimum Variance Features With Label Augmentation
arxiv_id: '2402.06855'
source_url: https://arxiv.org/abs/2402.06855
tags:
- label
- variance
- mixup
- smoothing
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The authors analyze why label smoothing and Mixup improve generalization
  by showing they learn low-variance features. They prove that on binary classification
  with linear models, these methods converge to solutions using only low-variance
  features, while standard training with weight decay also learns high-variance features.
---

# For Better or For Worse? Learning Minimum Variance Features With Label Augmentation

## Quick Facts
- arXiv ID: 2402.06855
- Source URL: https://arxiv.org/abs/2402.06855
- Authors: Muthu Chidambaram; Rong Ge
- Reference count: 40
- The authors analyze why label smoothing and Mixup improve generalization by showing they learn low-variance features.

## Executive Summary
This paper investigates why label smoothing and Mixup, two popular data augmentation techniques, improve generalization in deep learning models. The authors provide theoretical analysis showing that these methods converge to solutions using only minimum variance features in linear models, while standard training with weight decay also learns high-variance features. For nonlinear models, they demonstrate that label smoothing and Mixup losses are lower bounded by functions of model output variance, requiring lower variance predictions. Empirically, they show that these methods lead to significantly lower variance in penultimate layer activations and output probabilities compared to standard training, correlating with better test performance on CIFAR-10/100 datasets. However, they also reveal a tradeoff: these methods can be more susceptible to low-variance spurious correlations in training data.

## Method Summary
The authors compare three training methods: standard empirical risk minimization with weight decay, ERM with label smoothing (α=0.1), and ERM with Mixup (Beta(1,1) mixing distribution). They conduct experiments on CIFAR-10 and CIFAR-100 datasets using ResNet-18/50/101 architectures, training for 200 epochs with AdamW optimizer. For theoretical analysis, they study binary classification with linear models and logistic regression. They also create synthetic datasets with variance differences in features and colored MNIST datasets with spurious background correlations. The key measurements include test error, penultimate layer activation variance, and output probability variance across different training methods and hyperparameter settings.

## Key Results
- Label smoothing and Mixup losses are lower bounded by functions of model output variance, incentivizing low-variance predictions
- For linear binary classification, these methods converge to solutions using only minimum variance features, unlike standard training with weight decay
- Label smoothing and Mixup lead to significantly lower variance in penultimate layer activations and output probabilities compared to standard training, correlating with better test performance on CIFAR-10/100
- These methods are more susceptible to low-variance spurious correlations in training data compared to standard training

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Label smoothing and Mixup losses are minimized when model outputs have low variance across classes.
- **Mechanism:** Both losses incorporate mixture distributions over labels, penalizing confident outputs that don't match the smoothed reference distribution.
- **Core assumption:** Data contains separable features with different variances; label smoothing and Mixup apply weighting that favors low-variance feature learning.
- **Evidence anchors:**
  - [abstract] "we show that label smoothing and Mixup losses are lower bounded by a function of the model output variance"
  - [section] "label smoothing and Mixup hone in on low variance features in the data"
  - [corpus] Weak evidence - corpus contains Mixup variants but no direct variance minimization theory
- **Break condition:** If features have similar variance or if explicit feature normalization is applied before training.

### Mechanism 2
- **Claim:** For linear binary classification, label smoothing and Mixup converge to solutions using only minimum variance features.
- **Mechanism:** Optimization of these losses under variance constraints leads to sparse weight vectors aligned with low-variance dimensions.
- **Core assumption:** Some input dimensions have much lower variance than others and the data distribution satisfies separability conditions in both low and high variance dimensions.
- **Evidence anchors:**
  - [abstract] "linear models on binary classification data trained with label augmentation learn only the minimum variance features"
  - [section] "both label smoothing and Mixup incentivize learning models that correlate more strongly with the low variance features"
  - [corpus] Weak evidence - no direct mention of minimum variance features in linear models
- **Break condition:** When weight decay is strong enough to overcome variance bias, or when data variance is uniform across dimensions.

### Mechanism 3
- **Claim:** Low variance learned features correlate with better generalization on standard benchmarks.
- **Mechanism:** Empirical results show penultimate layer activation variance is significantly lower for label smoothing and Mixup compared to standard training, correlating with improved test performance.
- **Core assumption:** Standard benchmarks contain low-variance latent features that generalize well, while high-variance features contain more noise.
- **Evidence anchors:**
  - [abstract] "we show that the strong performance of label smoothing and Mixup on image classification benchmarks is correlated with learning low variance hidden representations"
  - [section] "label smoothing and Mixup lead to significantly lower variance in penultimate layer activations and output probabilities compared to standard training, correlating with better test performance"
  - [corpus] Moderate evidence - corpus contains Mixup variants but no direct correlation with generalization studies
- **Break condition:** When spurious correlations in training data align with low-variance features, causing overfitting to noise.

## Foundational Learning

- **Concept:** Variance in feature learning
  - Why needed here: Understanding how different training objectives bias toward high or low variance features is central to explaining the paper's theoretical results
  - Quick check question: If two features have variances 1 and 100, which one will weight decay favor in linear classification?

- **Concept:** Label smoothing and Mixup as regularization techniques
  - Why needed here: The paper builds on prior work showing these methods act as regularizers, but extends this to feature learning perspectives
  - Quick check question: How does label smoothing modify the target distribution compared to standard cross-entropy?

- **Concept:** Empirical risk minimization vs. variance-minimizing objectives
  - Why needed here: The paper contrasts ERM with weight decay against label augmentation methods that implicitly minimize variance
  - Quick check question: What is the key difference between the loss landscapes of ERM with weight decay and label smoothing?

## Architecture Onboarding

- **Component map:** Data preprocessing → Model architecture (ResNet variants) → Training loop with different loss functions → Evaluation metrics
- **Critical path:** Data preprocessing → model forward pass → loss computation → backpropagation → variance computation on activations/outputs → test evaluation
- **Design tradeoffs:** Batch size vs. variance estimation stability; Model depth vs. variance propagation; Hyperparameter tuning for label smoothing/Mixup vs. standard training
- **Failure signatures:** High activation variance persisting throughout training; Test performance degrading with label smoothing/Mixup; Oscillatory variance patterns
- **First 3 experiments:**
  1. Compare variance in penultimate layer activations between standard training and label smoothing on CIFAR-10 with ResNet-18
  2. Test logistic regression on synthetic low/high variance feature data to verify theoretical predictions
  3. Evaluate Mixup performance when training data contains spurious low-variance correlations

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the minimum variance feature learning property of label smoothing and Mixup causally improve generalization performance, or is the observed correlation merely coincidental?
- Basis in paper: [explicit] The authors demonstrate a correlation between lower variance representations and better test performance but explicitly state that "our results only establish a correlation between this low variance property and better test performance – it would require a significantly more in-depth empirical study to assess a causal relationship between this kind of feature learning and generalization."
- Why unresolved: While the authors show empirical evidence of lower variance in learned representations with label smoothing and Mixup, they haven't designed experiments that directly test whether forcing models to learn lower variance features improves generalization independent of these specific methods.
- What evidence would resolve it: Experiments comparing generalization performance when using regularization techniques specifically designed to encourage lower variance features versus standard training, while controlling for other factors.

### Open Question 2
- Question: What are the specific characteristics of low variance features that make them generalize well across different image classification benchmarks?
- Basis in paper: [inferred] The authors hypothesize that "for standard benchmarks, there exist low variance latent features that generalize well as opposed to high-variance, noisy features" but don't investigate what these features actually are or why they generalize better.
- Why unresolved: The paper focuses on proving that label smoothing and Mixup learn lower variance features, but doesn't examine what these features represent or why they might be more generalizable than high variance features.
- What evidence would resolve it: Detailed analysis of the learned low variance features across different models and datasets to identify common patterns or properties that correlate with generalization performance.

### Open Question 3
- Question: Can regularization techniques be designed that encourage lower variance features without the potential drawbacks of label smoothing and Mixup (such as susceptibility to spurious correlations)?
- Basis in paper: [explicit] The authors conclude by suggesting "a natural follow-up direction to our results is to investigate whether regularizers for encouraging lower variance features can be implemented directly to improve model performance."
- Why unresolved: While the authors demonstrate that label smoothing and Mixup lead to lower variance features, they also show these methods can be more susceptible to spurious correlations, suggesting the need for more targeted approaches.
- What evidence would resolve it: Development and testing of new regularization techniques that explicitly encourage lower variance features while being robust to spurious correlations, with empirical comparison to label smoothing and Mixup.

## Limitations
- The theoretical analysis assumes idealized data distributions that may not hold in practice
- Experiments focus primarily on image classification with ResNet architectures, limiting generalizability
- Variance measurements are sensitive to batch size and implementation details

## Confidence
- **High Confidence**: The theoretical results for linear binary classification with label smoothing and Mixup learning minimum variance features
- **Medium Confidence**: The empirical correlation between low variance features and generalization performance on CIFAR datasets
- **Low Confidence**: The generalizability of variance-minimization effects to other architectures, datasets, and training scenarios

## Next Checks
1. Test the variance-minimization hypothesis on non-image datasets (e.g., text or tabular data) with different model architectures to verify generalizability
2. Conduct ablation studies varying batch sizes and normalization techniques to understand their impact on variance propagation and learned features
3. Design synthetic experiments where spurious correlations are intentionally introduced with known variance properties to quantify the tradeoff between robustness and susceptibility to low-variance noise