---
ver: rpa2
title: Transfer Learning for Molecular Property Predictions from Small Data Sets
arxiv_id: '2404.13393'
source_url: https://arxiv.org/abs/2404.13393
tags:
- data
- learning
- pre-training
- sets
- molecular
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper investigates the use of transfer learning to improve
  molecular property prediction on small data sets. The authors benchmark standard
  ML models (SOAP descriptors + KRR/NN/GBoost, and PaiNN) on two small chemistry data
  sets: HOPV (350 molecules, HOMO-LUMO gaps) and Freesolv (643 molecules, solvation
  energies).'
---

# Transfer Learning for Molecular Property Predictions from Small Data Sets

## Quick Facts
- arXiv ID: 2404.13393
- Source URL: https://arxiv.org/abs/2404.13393
- Authors: Thorren Kirschbaum; Annika Bande
- Reference count: 40
- One-line primary result: Transfer learning with pre-trained PaiNN improves molecular property prediction on small datasets when pre-training and fine-tuning labels are similar

## Executive Summary
This paper investigates transfer learning to improve molecular property prediction on small datasets. The authors benchmark standard ML models (SOAP descriptors + KRR/NN/GBoost, and PaiNN) on two small chemistry datasets: HOPV (350 molecules, HOMO-LUMO gaps) and Freesolv (643 molecules, solvation energies). PaiNN performs best on both. They then propose a transfer learning strategy where PaiNN is pre-trained on large datasets (OE62 for HOPV, QM9 for Freesolv) using computationally cheap quantum chemistry methods (LDA-DFT or XTB) to generate labels, followed by fine-tuning on the target data. This approach significantly improves predictions on HOPV (MAE reduced from 7.6 to 6.1 meV), but not on Freesolv.

## Method Summary
The method involves pre-training PaiNN on large molecular datasets using computationally cheap quantum chemistry methods to generate labels, then fine-tuning on small target datasets. Both pre-training and target datasets are normalized to zero mean and unit variance to correct for biases in cheap QC methods. The authors test various pre-training dataset sizes (10k to 50k points) and find that for HOPV, fewer pre-training points sometimes yield better results. Discriminative fine-tuning is implemented but shows no improvement over standard fine-tuning for this transfer learning task.

## Key Results
- PaiNN outperforms baseline ML models (SOAP+KRR, SOAP+NN, SOAP+GBoost) on both HOPV and Freesolv datasets
- Transfer learning with pre-trained PaiNN reduces MAE from 7.6 to 6.1 meV on HOPV dataset
- Pre-training with fewer data points (10k vs 50k) can lead to better final results on HOPV
- Transfer learning fails to improve performance on Freesolv, attributed to dissimilar pre-training and fine-tuning labels

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Pre-training with computationally cheap QC methods improves transfer to small target data
- Mechanism: Linear regression corrects bias from cheap QC labels, allowing PaiNN to learn general features before fine-tuning
- Core assumption: Cheap QC methods produce qualitatively correct but quantitatively biased predictions
- Evidence anchors:
  - [abstract] "The pre-training labels are obtained from computationally cheap ab initio or semi-empirical models and both data sets are normalized to mean zero and standard deviation one to align the labels' distributions."
  - [section] "To correct for the biases inherent in the results obtained from these methods, we normalize both data sets to have mean 0 and standard deviation 1 before NN training."
- Break condition: If cheap QC methods produce qualitatively incorrect predictions, normalization cannot fix the bias

### Mechanism 2
- Claim: Pre-training on structurally similar molecules improves performance more than on dissimilar molecules
- Mechanism: Neural networks learn transferable structural features that are more applicable when pre-training and fine-tuning domains overlap
- Core assumption: Molecular structural similarity correlates with property prediction transferability
- Evidence anchors:
  - [section] "The HOPV data set consists almost entirely of S-functionalized aromatic molecules, while the OE62 data set used for pre-training contains a much more diverse set of molecules"
  - [section] "we filtered all molecules from the OE62 pre-training data set containing any of the elements As, Se, Br, Te or I, and furthermore removed all molecules that do not contain any sulfur atoms"
- Break condition: If the learning task is highly complex and methods are dissimilar (Freesolv case), structural similarity alone is insufficient

### Mechanism 3
- Claim: Limited pre-training data can produce better transfer results than extensive pre-training
- Mechanism: Smaller pre-training sets create more biased models that capture high-level concepts rather than overfitting to specific pre-training task details
- Core assumption: Overfitting to pre-training data can reduce transfer effectiveness
- Evidence anchors:
  - [abstract] "Finally, we find that for the HOPV data set, the final training results do not improve monotonically with the size of the pre-training data set, but pre-training with fewer data points can lead to more biased pre-trained models and higher accuracy after fine-tuning."
  - [section] "the final training errors after pre-training on 10k data points do not strongly improve when more pre-training data (up to 50k data points) is used"
- Break condition: If the pre-training task is too dissimilar from the target task, even limited pre-training may not help

## Foundational Learning

- Concept: Transfer learning fundamentals
  - Why needed here: The paper's core contribution is a transfer learning strategy for molecular property prediction
  - Quick check question: What are the two main phases of transfer learning and their purposes?

- Concept: Molecular fingerprinting and descriptor methods
  - Why needed here: The paper benchmarks SOAP descriptors with various ML models alongside PaiNN
  - Quick check question: How do SOAP descriptors capture molecular structure information?

- Concept: Message passing neural networks
  - Why needed here: PaiNN is the primary model used and shown to be most effective
  - Quick check question: What distinguishes message passing neural networks from traditional feed-forward networks for molecular data?

## Architecture Onboarding

- Component map:
  - Data preprocessing -> Cheap QC label generation -> Normalization -> Pre-training -> Fine-tuning -> Evaluation

- Critical path:
  1. Compute cheap QC properties for large dataset
  2. Linear regression to correct bias and normalize
  3. Pre-train PaiNN on normalized large dataset
  4. Fine-tune on small target dataset with validation

- Design tradeoffs:
  - Pre-training data size: Larger may cause overfitting vs. smaller may be more biased
  - QC method choice: Accuracy vs. computational cost (LDA vs. XTB)
  - Structural similarity: Diverse vs. targeted pre-training molecules

- Failure signatures:
  - No improvement or degradation in fine-tuning performance
  - High variance in MAE across runs (indicates instability)
  - MAE plateaus or increases with more pre-training data

- First 3 experiments:
  1. Train PaiNN from scratch on HOPV to establish baseline
  2. Pre-train on full OE62+XTB then fine-tune on HOPV
  3. Pre-train on filtered OE62 (S-containing only) then fine-tune on HOPV

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does pre-training on small data sets consistently yield better results than pre-training on large data sets for molecular property prediction tasks?
- Basis in paper: [explicit] The authors found that for the HOPV data set, pre-training with fewer data points can lead to more biased pre-trained models and higher accuracy after fine-tuning, contrary to expectations.
- Why unresolved: The study only tested a limited range of pre-training data set sizes (10k to 50k points) and only on one data set (HOPV). The findings may not generalize to other molecular property prediction tasks or data sets.
- What evidence would resolve it: Additional experiments pre-training on different data sets with various sizes of pre-training data sets, comparing the final fine-tuning results.

### Open Question 2
- Question: What is the optimal similarity between pre-training and fine-tuning data sets for transfer learning to be effective in molecular property prediction?
- Basis in paper: [inferred] The authors hypothesize that the failure of transfer learning on the Freesolv data set may be due to the dissimilarity between the pre-training and fine-tuning methods used to obtain labels. They also attempted to increase similarity by filtering the pre-training data set but found only a small improvement.
- Why unresolved: The study only explored one method of increasing similarity (filtering by molecular structure) and did not investigate other factors such as the similarity of the molecular properties being predicted or the similarity of the computational methods used to generate labels.
- What evidence would resolve it: Experiments varying the similarity of pre-training and fine-tuning data sets in multiple dimensions (molecular structure, properties, and computational methods) and measuring the impact on transfer learning performance.

### Open Question 3
- Question: Does discriminative fine-tuning improve transfer learning performance for molecular property prediction tasks?
- Basis in paper: [explicit] The authors implemented discriminative fine-tuning for PaiNN but found that it did not yield any improvements over non-discriminative fine-tuning for transfer learning with PaiNN.
- Why unresolved: The study only tested one implementation of discriminative fine-tuning with one neural network architecture (PaiNN) and one transfer learning strategy. Other implementations or architectures may benefit from discriminative fine-tuning.
- What evidence would resolve it: Experiments testing different implementations of discriminative fine-tuning (e.g., different learning rate schedules) with various neural network architectures and transfer learning strategies on multiple molecular property prediction tasks.

## Limitations
- The study only examines two small datasets, limiting generalizability to other molecular property prediction tasks
- The mechanism for why fewer pre-training samples sometimes works better remains incompletely understood
- No systematic exploration of alternative QC methods or their bias correction approaches

## Confidence
- **High confidence**: PaiNN outperforms baseline ML models on both datasets; pre-training with cheap QC methods can improve predictions on structurally similar tasks (HOPV results)
- **Medium confidence**: The normalization procedure adequately corrects for QC method biases; structural similarity between pre-training and fine-tuning sets is necessary for transfer success
- **Low confidence**: The explanation for why limited pre-training data sometimes outperforms extensive pre-training is fully validated; the findings generalize to other molecular property prediction tasks beyond HOMO-LUMO gaps and solvation energies

## Next Checks
1. Test the transfer learning approach on additional small molecular datasets with varying structural characteristics and property types to assess generalizability
2. Conduct ablation studies varying the pre-training data size systematically to better understand the non-monotonic relationship between pre-training data and final performance
3. Compare different bias correction methods beyond simple normalization to determine if more sophisticated approaches could improve Freesolv results