---
ver: rpa2
title: Retrieval-style In-Context Learning for Few-shot Hierarchical Text Classification
arxiv_id: '2406.17534'
source_url: https://arxiv.org/abs/2406.17534
tags:
- label
- retrieval
- text
- labels
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces the first ICL-based framework for few-shot
  hierarchical text classification. The core idea is to retrieve relevant demonstrations
  for each test input using a label-aware retrieval database, then perform iterative
  ICL layer-by-layer to predict hierarchical labels.
---

# Retrieval-style In-Context Learning for Few-shot Hierarchical Text Classification

## Quick Facts
- arXiv ID: 2406.17534
- Source URL: https://arxiv.org/abs/2406.17534
- Reference count: 38
- First ICL-based framework for few-shot hierarchical text classification

## Executive Summary
This paper introduces a novel retrieval-style in-context learning (ICL) framework for few-shot hierarchical text classification (HTC). The approach leverages a label-aware retrieval database to identify semantically relevant demonstrations for each test input, then performs iterative ICL layer-by-layer to predict hierarchical labels. The method employs a novel divergent contrastive learning (DCL) objective during retriever training to improve discrimination between semantically-similar labels. Experiments on three datasets (WOS, DBpedia, Patent) demonstrate state-of-the-art performance, with improvements of 4.51% micro-F1 and 5.28% macro-F1 on WOS in 1-shot settings.

## Method Summary
The framework builds a retrieval database using a PLM indexer trained with masked language modeling, layer-wise classification, and a novel divergent contrastive learning objective. For each test instance, top-K demonstrations are retrieved based on semantic similarity, then an iterative ICL approach predicts labels layer-by-layer, progressively narrowing the candidate label set. The LLM (Vicuna-7B for English, ChatGLM-6B for Chinese) generates final hierarchical labels using constructed prompts with retrieved demonstrations. Label descriptions are enhanced using LLM to improve retrieval quality.

## Key Results
- Achieves state-of-the-art performance on three datasets (WOS, DBpedia, Patent) in few-shot settings
- 1-shot: Improves micro-F1 by 4.51% and macro-F1 by 5.28% over previous SOTA on WOS
- Iterative method improves micro-F1 by 28.70% and macro-F1 by 28.35% compared to zero-shot LLM
- DCL improves micro-F1 by 2.41% and macro-F1 by 1.78% on average across datasets

## Why This Works (Mechanism)

### Mechanism 1
Retrieval-style ICL outperforms direct LLM inference for HTC by supplying semantically relevant demonstrations that reduce label ambiguity. The method retrieves top-K demonstrations from a label-aware database, iteratively inferring labels layer-by-layer rather than presenting the entire label set at once. The retrieval database captures HTC-specific label semantics, enabling the LLM to leverage this narrowed candidate set for improved accuracy.

### Mechanism 2
Divergent Contrastive Learning (DCL) improves discrimination between semantically similar adjacent labels by sampling hard negatives. DCL samples negative examples from semantically close labels and pulls their index vectors apart while pushing positive pairs together, sharpening the label boundaries. This addresses a primary source of classification errors in HTC where semantically similar labels cause confusion.

### Mechanism 3
Layer-wise iterative ICL reduces the candidate label space, enabling the LLM to focus on relevant sub-hierarchies at each step. The method first predicts a high-level label, then uses its child nodes as the candidate set for the next layer, repeating until leaf labels are reached. This progressive narrowing of candidates improves label prediction accuracy compared to presenting the entire label hierarchy at once.

## Foundational Learning

- **Hierarchical label structures (trees/graphs)**: HTC deals with nested label sets where understanding parent-child relationships is key to designing iterative inference and label-aware representations. Quick check: In a 3-level hierarchy with 10 top-level, 70 second-level, and 219 third-level labels, how many candidate labels does the iterative approach present at the final layer versus the flat approach?

- **Dense retrieval and vector similarity**: The retrieval database relies on semantic similarity between encoded inputs and demonstrations; retrieval quality directly affects ICL performance. Quick check: If two texts have cosine similarity 0.9 in embedding space, how should the retriever rank them relative to a text with similarity 0.3?

- **Contrastive learning objectives and hard negative sampling**: DCL's effectiveness depends on selecting informative negative samples to sharpen label boundaries. Quick check: In DCL, why might sampling negatives from semantically similar labels be more effective than random negatives?

## Architecture Onboarding

- **Component map**: Input text → Indexer (BERT/PLM) → Soft prompts [P1..PC] → Index vectors m1..mC → Retrieval DB; Retrieval DB + Test text → Similarity scoring → Top-K demonstrations; Demonstrations + Test text → Iterative ICL prompt → LLM inference → Predicted label path

- **Critical path**: 1. Build retrieval DB with HTC label-aware representations. 2. Retrieve top-K demonstrations for each test instance. 3. Construct iterative ICL prompt with candidate label set pruning. 4. LLM generates final hierarchical labels.

- **Design tradeoffs**: Retrieval vs. classification-based prediction (retrieval leverages semantic similarity; classification needs explicit training per label); Number of retrieved demonstrations (K) (more examples increase context but risk redundancy); DCL vs. random negative sampling (DCL may improve discrimination but increases training complexity).

- **Failure signatures**: Retrieval DB yields noisy or irrelevant demonstrations → ICL degrades; LLM misinterprets candidate label set → incorrect hierarchical predictions; DCL sampling fails to find hard negatives → contrastive loss ineffective.

- **First 3 experiments**: 1. Verify retrieval quality: retrieve top-3 demonstrations for a test set and manually inspect relevance to ground truth label path. 2. Ablation of DCL: train with and without DCL, compare retrieval similarity distributions and final HTC metrics. 3. Iterative ICL vs. flat ICL: run both on a small dataset, measure F1 and inference time to confirm iterative approach improves accuracy.

## Open Questions the Paper Calls Out

The paper does not explicitly call out specific open questions, but based on the experimental scope and methodology, several questions emerge regarding scalability and generalizability. The framework is evaluated only on datasets with up to 4 hierarchical levels, leaving open questions about performance on deeper structures. The paper also focuses exclusively on single-path HTC, not addressing multi-path scenarios where instances can have multiple label paths. Additionally, the impact of label description quality when generated by different LLM sizes or architectures remains unexplored.

## Limitations

- **Evaluation scope limited**: Results are measured on three specific datasets with fixed label structures; generalizability to different hierarchical depths, distributions, or domains remains unclear.

- **Implementation details missing**: Critical specifics such as exact prompt templates, retriever configuration, and LLM inference parameters beyond temperature are not fully specified.

- **DCL effectiveness validation incomplete**: While DCL shows improvement, the ablation study doesn't isolate its contribution or analyze whether "hard negatives" are actually more informative than random negatives.

## Confidence

**High Confidence**: The core retrieval-based ICL framework with iterative layer-wise inference is well-supported by experimental results. Ablation studies clearly demonstrate iterative inference improves over flat ICL, and retrieval mechanism provides meaningful demonstrations to the LLM.

**Medium Confidence**: DCL objective's effectiveness is supported by results but lacks detailed analysis of why it works or sensitivity to hyperparameters like β=0.01. Improvement could be partially attributed to other factors in the training pipeline.

**Low Confidence**: Claims about general superiority of retrieval-style ICL for HTC are based on comparison with limited baselines. Paper doesn't benchmark against more recent HTC methods or explore failure cases where retrieval-based approaches might underperform.

## Next Checks

1. **Retrieval Quality Analysis**: Perform detailed analysis of retrieval precision@k on a validation set for each dataset. Calculate the percentage of cases where ground truth label path appears in top-K retrieved demonstrations to validate whether poor HTC performance stems from retrieval failure or LLM inference errors.

2. **DCL Ablation with Hard Negative Analysis**: Create ablation study comparing DCL against (a) random negative sampling, and (b) baseline with no contrastive learning. For each variant, analyze distribution of cosine similarities between positive and negative pairs to verify DCL actually produces harder negatives as claimed.

3. **Layer-wise Performance Breakdown**: For each dataset, measure per-layer classification accuracy (root, intermediate, leaf) separately. This reveals whether iterative approach helps more at certain hierarchy levels and identifies potential failure modes where candidate set pruning might be too aggressive or too conservative.