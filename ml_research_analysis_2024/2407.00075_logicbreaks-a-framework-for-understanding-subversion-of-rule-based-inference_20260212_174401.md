---
ver: rpa2
title: 'Logicbreaks: A Framework for Understanding Subversion of Rule-based Inference'
arxiv_id: '2407.00075'
source_url: https://arxiv.org/abs/2407.00075
tags:
- rule
- then
- attention
- arxiv
- rules
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This work studies how to subvert large language models from following\
  \ prompt-specified rules, formalizing rule-following as inference in propositional\
  \ Horn logic and proving that maliciously crafted prompts can mislead both theoretical\
  \ and learned models. The authors develop a logic-based framework to characterize\
  \ three properties of rule-following\u2014monotonicity, maximality, and soundness\u2014\
  and devise theoretical attacks (fact amnesia, rule suppression, state coercion)\
  \ that transfer to learned reasoners."
---

# Logicbreaks: A Framework for Understanding Subversion of Rule-based Inference

## Quick Facts
- arXiv ID: 2407.00075
- Source URL: https://arxiv.org/abs/2407.00075
- Reference count: 40
- Primary result: Theoretical framework shows maliciously crafted prompts can subvert rule-following in both theoretical and learned models

## Executive Summary
This work presents a formal framework for understanding how large language models can be subverted from following prompt-specified rules. The authors formalize rule-following as inference in propositional Horn logic and prove that malicious prompts can mislead both theoretical constructions and learned models. They develop three theoretical attack strategies (fact amnesia, rule suppression, state coercion) that transfer to learned reasoners, and demonstrate that popular jailbreak algorithms like GCG find adversarial prompts aligning with their theoretical predictions.

## Method Summary
The authors construct theoretical reasoners using one-layer transformers with embedding dimension d=2n to encode rules as binary vectors. They train GPT-2 models on structured Minecraft crafting datasets for propositional inference tasks, achieving >85% accuracy. Using the GCG algorithm, they generate adversarial suffixes targeting specific rules/facts and evaluate attack success through multiple metrics including attention weight analysis and suffix-target overlap. The framework characterizes rule-following properties (monotonicity, maximality, soundness) and proves theoretical attacks that manipulate the transformer's weighted summation approximation of binary disjunctions.

## Key Results
- Malicious prompts can subvert rule-following in both theoretical and learned models
- GCG jailbreak attacks find adversarial suffixes that align with theoretical predictions
- Attention patterns induced by attacks match theoretical expectations of attention suppression
- Binary classifier probes accurately predict proof states from LLM embeddings

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Malicious prompts mislead models by exploiting weighted summation approximation of binary disjunctions
- Mechanism: Large negative values in adversarial suffixes suppress or remove facts from proof states by manipulating the transformer's softmax attention
- Core assumption: Attention mechanism can be manipulated to "forget" facts or suppress rule applications
- Evidence: Theory shows summation approximation creates vulnerability; corpus includes related rule-based reasoning work
- Break condition: If attention manipulation is neutralized by layer normalization or other mechanisms

### Mechanism 2
- Claim: GCG attacks find suffixes that suppress attention on specific rules
- Mechanism: GCG searches for tokens reducing attention weights on suppressed rules while maintaining legitimate reasoning appearance
- Core assumption: Attention patterns correlate with theoretical attention suppression mechanism
- Evidence: GCG attacks induce attention patterns aligning with predictions; corpus includes jailbreak attack research
- Break condition: If attention mechanism is too robust or safety training overrides manipulation

### Mechanism 3
- Claim: Binary-valued encodings in LLM embeddings predict rule-following behavior
- Mechanism: Linear classifier probes on embeddings accurately predict final proof states
- Core assumption: Embedding space can be well-approximated by binary vectors for rule-following
- Evidence: Probe accuracy shows propositions are linearly separable; framework provides foundation for rule-based settings
- Break condition: If embedding space complexity or non-linear relationships dominate

## Foundational Learning

- Propositional Horn Logic: Provides mathematical foundation for formalizing rule-following as inference; how does it differ from general propositional logic in clause structure and inference?
- Transformer Attention Mechanism: Primary computational component for rule application and subversion; what role does softmax play and how can it be manipulated?
- Autoregressive Generation: Sequential generation of proof states crucial for analyzing subversions; how does it differ from parallel processing for logical inference?

## Architecture Onboarding

- Component map: Input encoding layer (Γ, Φ → X₀) → Transformer layer (R: RN×d → RN×d) → Attention mechanism → Feedforward network → Classifier head → Adversarial suffix injection point
- Critical path: Input encoding → Transformer reasoning → Output generation, with attention mechanism as key vulnerability
- Design tradeoffs: Binary vs. continuous representations (simplicity vs. expressiveness); single vs. multi-layer architectures (analysis vs. performance); attention vs. other mechanisms (capability vs. attack surface)
- Failure signatures: Fact amnesia (missing expected facts), rule suppression (absent rule consequences), state coercion (logically unsound conclusions)
- First 3 experiments: 1) Verify binary probe accuracy on clean embeddings for different proposition counts; 2) Test theory-based attacks on learned reasoners; 3) Measure attention weight changes between attacked and non-attacked generations

## Open Questions the Paper Calls Out

- Can the framework extend to handle quantified rules ("for all" and "exists") beyond propositional Horn logic?
- Do attention suppression patterns indicate fundamental mechanism or artifacts of optimization?
- How does fact amnesia attack effectiveness scale with dependency graph complexity?
- What relationship exists between attack repetition needs and model architecture parameters?

## Limitations

- Theoretical framework may oversimplify real LLM reasoning by modeling through propositional Horn logic
- Binary probe accuracy degrades significantly for larger proposition sets, raising scalability questions
- Minecraft dataset may not represent complexity of real-world rule-following scenarios
- Empirical validation relies on probing techniques that may not capture full complexity of learned reasoning

## Confidence

- High: Characterization of monotonicity, maximality, and soundness as fundamental properties
- Medium: Empirical demonstration that GCG attacks align with theoretical predictions
- Low: Claim that binary-valued encodings provide foundation for understanding rule-following

## Next Checks

1. Evaluate binary probe accuracy across varying proposition counts (n=10, 20, 30) to identify reliability threshold
2. Implement attention mechanism variant preventing large negative value suppression to isolate vulnerability
3. Apply framework to complex rule-following dataset (logical reasoning benchmarks) to assess insight transferability