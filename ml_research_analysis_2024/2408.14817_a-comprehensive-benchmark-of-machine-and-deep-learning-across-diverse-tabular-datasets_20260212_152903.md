---
ver: rpa2
title: A Comprehensive Benchmark of Machine and Deep Learning Across Diverse Tabular
  Datasets
arxiv_id: '2408.14817'
source_url: https://arxiv.org/abs/2408.14817
tags:
- datasets
- performance
- regression
- other
- average
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper benchmarks 20 ML/DL models across 111 diverse tabular
  datasets, covering both regression and classification tasks. The study aims to characterize
  when DL models outperform traditional ML models on tabular data.
---

# A Comprehensive Benchmark of Machine and Deep Learning Across Diverse Tabular Datasets

## Quick Facts
- arXiv ID: 2408.14817
- Source URL: https://arxiv.org/abs/2408.14817
- Reference count: 40
- 20 ML/DL models benchmarked across 111 diverse tabular datasets

## Executive Summary
This comprehensive benchmark evaluates 20 machine learning and deep learning models across 111 diverse tabular datasets, covering both regression and classification tasks. The study challenges the conventional wisdom that traditional ML models always outperform DL models on tabular data. Results show that while tree-based ensembles generally lead, specific dataset characteristics—small row counts, high kurtosis, and high dimensionality—favor DL performance. The researchers developed a meta-learning model that predicts when DL will outperform ML with 86.1% accuracy, providing actionable insights for practitioners selecting models based on dataset properties.

## Method Summary
The study benchmarks 20 models (7 DL-based, 7 Tree-based Ensemble, 6 classical ML) on 111 datasets (57 regression, 54 classification) using 10-fold cross-validation. Datasets vary in size (43-245,057 rows, 4-267 columns) and include categorical features. Performance is measured using RMSE, MAE, R2 for regression and accuracy, AUC, F1 for classification. A meta-learning approach extracts 20 dataset features to predict when DL outperforms ML, using logistic regression and symbolic regression models.

## Key Results
- Tree-based ensembles outperform DL models on most datasets
- High kurtosis, small row counts, and high dimensionality favor DL performance
- Classification tasks show smaller performance gaps between DL and ML compared to regression
- AutoGluon ensemble (combining DL and ML) best-performed on 39 of 111 datasets (35%)
- Meta-learning model predicts DL advantage with 86.1% accuracy (AUC 0.78)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Deep Learning models outperform traditional ML models on tabular datasets with small row counts, high kurtosis, and high dimensionality.
- Mechanism: DL models benefit from the high capacity of neural networks, which can capture complex, non-linear relationships in data, especially when traditional ML models (like tree-based ensembles) may underfit due to insufficient data points or inability to model heavy-tailed distributions.
- Core assumption: The increased model capacity of DL compensates for the risk of overfitting in small datasets when the data exhibits high kurtosis (heavy tails) or many features.
- Evidence anchors:
  - [abstract] "Results show that while ML models (especially tree-based ensembles) generally outperform DL models, certain dataset characteristics—such as small row count, high kurtosis, and high dimensionality—favor DL performance."
  - [section] "Interestingly, this heatmap reveals configurations where the probability is higher than 0.5. For sub-figure (c), one can notice that the number of rows does have much influence on the probability while a large X-kurtosis signals that DL models are probably preferred over ML models."
  - [corpus] Found 25 related papers (using 8). Average neighbor FMR=0.46, average citations=0.0. Top related titles: Tabular Data: Is Deep Learning all you need?, PMLBmini: A Tabular Classification Benchmark Suite for Data-Scarce Applications, TabularBench: Benchmarking Adversarial Robustness for Tabular Deep Learning in Real-world Use-cases. (Weak corpus support for kurtosis claim)
- Break condition: If dataset is very small and low kurtosis, traditional ML models will likely outperform DL models due to overfitting risk.

### Mechanism 2
- Claim: Classification tasks show a smaller performance gap between DL and ML models compared to regression tasks.
- Mechanism: In classification, all errors contribute equally to the performance metric, while in regression, large errors (e.g., in RMSE) are heavily penalized. DL models, with their large parameter space, may exhibit large errors that are heavily penalized in regression but less so in classification.
- Core assumption: The nature of the error distribution in classification vs. regression tasks influences the relative performance of DL models.
- Evidence anchors:
  - [abstract] "We also find that the gap between the two groups is smaller for classification tasks compared to regression tasks."
  - [section] "A possible explanation for this phenomenon is that in classification tasks, all errors contribute equally to the performance metric while in regression tasks, large errors have more weight (RMSE is unbounded)."
  - [corpus] Weak corpus support for this specific mechanism; no direct evidence found.
- Break condition: If regression task uses MAE (Mean Absolute Error) instead of RMSE, the performance gap between DL and ML models may decrease.

### Mechanism 3
- Claim: An ensemble of DL and ML models (e.g., AutoGluon) outperforms individual DL or ML models on tabular data.
- Mechanism: Ensemble methods combine the strengths of different model types, mitigating individual weaknesses and capturing a broader range of patterns in the data.
- Core assumption: Combining diverse model types through ensembling leads to improved predictive performance.
- Evidence anchors:
  - [abstract] "AutoGluon, an ensemble of DL and ML models method best performed for 39 out of the 111 datasets (35%), almost four times more than SVM, the second-best model, which best-performed in only 10 datasets (9%)."
  - [section] "In particular, we found that AutoGluon, an automatic ML model [46] that uses ensembles of both ML and DL models, outperforms the other models by a large margin. This outcome aligns with the findings presented by [26]."
  - [corpus] Weak corpus support for this specific mechanism; no direct evidence found.
- Break condition: If the dataset is very small or has specific characteristics that favor one model type over others, the ensemble may not outperform the best individual model.

## Foundational Learning

- Concept: Understanding of different ML and DL model types (e.g., tree-based ensembles, neural networks, ensemble methods).
  - Why needed here: The paper benchmarks 20 different models, including 7 DL-based models, 7 Tree-based Ensemble models, and 6 classical ML-based models. A solid understanding of these models is crucial for interpreting the results and understanding why certain models perform better on specific datasets.
  - Quick check question: Can you explain the difference between a Random Forest and a Gradient Boosting Machine, and when you might choose one over the other?

- Concept: Familiarity with performance metrics for regression (RMSE, MAE, R2) and classification (accuracy, AUC, F1 score).
  - Why needed here: The paper uses these metrics to evaluate model performance on different datasets. Understanding these metrics is essential for comparing model performance and interpreting the results.
  - Quick check question: What is the difference between RMSE and MAE, and when would you use one over the other?

- Concept: Knowledge of meta-learning and feature engineering for dataset profiling.
  - Why needed here: The paper uses a meta-learning approach to predict when DL models will outperform ML models based on dataset characteristics. Understanding meta-learning and feature engineering is crucial for understanding this prediction model.
  - Quick check question: What is meta-learning, and how can it be used to predict the performance of different ML models on a new dataset?

## Architecture Onboarding

- Component map:
  Data Preprocessing -> Model Training (20 models) -> 10-fold Cross-Validation -> Performance Evaluation -> Meta-Analysis

- Critical path:
  1. Data preprocessing and feature engineering
  2. Training of 20 different ML and DL models
  3. Performance evaluation using 10-fold cross-validation
  4. Construction of the meta-dataset
  5. Training of the prediction models (logistic regression and symbolic regression)

- Design tradeoffs:
  - Model complexity vs. interpretability: DL models are more complex but may offer better performance on certain datasets
  - Ensemble methods vs. individual models: Ensembles can combine strengths but may be more computationally expensive
  - Feature engineering vs. automated feature extraction: Manual feature engineering can be time-consuming but may lead to better performance

- Failure signatures:
  - Overfitting: DL models may overfit on small datasets
  - Underfitting: ML models may underfit on complex datasets
  - Incorrect feature engineering: Poor feature engineering can lead to suboptimal model performance

- First 3 experiments:
  1. Train a simple ML model (e.g., Random Forest) and a DL model (e.g., MLP) on a small tabular dataset with high kurtosis and compare their performance
  2. Train an ensemble model (e.g., AutoGluon) and compare its performance to individual ML and DL models on a diverse set of tabular datasets
  3. Use the meta-learning approach to predict whether DL or ML models will perform better on a new tabular dataset based on its characteristics

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Do specific dataset characteristics (beyond kurtosis and classification vs. regression) consistently predict when DL models outperform ML models on tabular data?
- Basis in paper: [inferred] The meta-analysis identified only kurtosis and task type as statistically significant predictors, but the heatmap analysis suggests other features may have influence.
- Why unresolved: The logistic regression analysis found limited statistically significant predictors, but the heatmap visualizations suggest potential non-linear relationships that linear models may miss.
- What evidence would resolve it: Applying non-linear models like decision trees or neural networks to the meta-learning task to identify additional predictive features, or conducting controlled experiments varying specific dataset properties.

### Open Question 2
- Question: How do feature selection and feature engineering techniques impact the relative performance of ML versus DL models on tabular data?
- Basis in paper: [explicit] "Second, an analysis of feature selection or feature engineering, which is known to have a significant impact on the down-the-line model’s performance, has not been included in this work."
- Why unresolved: The study did not investigate the role of preprocessing techniques that could significantly affect model performance.
- What evidence would resolve it: Benchmarking the same models with and without feature selection/engineering techniques to quantify their impact on ML vs. DL performance.

### Open Question 3
- Question: How do DL and ML models compare on tabular data tasks beyond regression and classification, such as time-series forecasting or multi-label classification?
- Basis in paper: [explicit] "Finally, while this study included diverse regression and classification datasets, there are additional tasks that have not been included, such as time-series or multilabel classifications."
- Why unresolved: The study focused exclusively on standard regression and classification tasks, leaving other common tabular data tasks unexplored.
- What evidence would resolve it: Extending the benchmark to include time-series and multi-label classification datasets and evaluating the same models on these tasks.

## Limitations

- The kurtosis finding has minimal corpus validation with only 25 related papers and weak empirical support
- Meta-learning model's 86.1% accuracy may suffer from overfitting to the 111 datasets without extensive cross-validation
- The study does not investigate feature selection/engineering impact on ML vs. DL performance differences

## Confidence

- **High confidence**: AutoGluon ensemble outperforming individual models (strong empirical evidence and alignment with prior work [26])
- **Medium confidence**: Classification vs. regression performance gap (plausible theoretical explanation but limited direct corpus support)
- **Low confidence**: Kurtosis-based DL advantage (minimal corpus validation and theoretical mechanism requiring further empirical testing)

## Next Checks

1. **Replication on independent datasets**: Test the meta-learning model's predictions on a held-out set of tabular datasets not included in the original 111 to assess generalizability
2. **Ablation study on preprocessing**: Verify that the reported performance differences persist when varying data preprocessing strategies, particularly for categorical features and missing data handling
3. **Statistical significance testing**: Conduct rigorous statistical tests (e.g., paired t-tests with multiple comparison corrections) to confirm that performance differences between DL and ML models are not due to random variation across the 111 datasets