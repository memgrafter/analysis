---
ver: rpa2
title: Enhancing textual textbook question answering with large language models and
  retrieval augmented generation
arxiv_id: '2402.05128'
source_url: https://arxiv.org/abs/2402.05128
tags:
- questions
- question
- context
- llama-2
- accuracy
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a retrieval-augmented fine-tuning approach
  (PLRTQA) for textbook question answering (TQA), focusing on textual questions. The
  method combines the Llama-2 language model with retrieval-augmented generation (RAG)
  to address challenges like long context comprehension and out-of-domain scenarios.
---

# Enhancing textual textbook question answering with large language models and retrieval augmented generation

## Quick Facts
- arXiv ID: 2402.05128
- Source URL: https://arxiv.org/abs/2402.05128
- Reference count: 40
- Primary result: 9.84% accuracy improvement on textual multiple-choice questions over existing methods

## Executive Summary
This paper presents PLRTQA, a retrieval-augmented fine-tuning approach for textbook question answering (TQA) that focuses on textual questions. The method combines the Llama-2 language model with retrieval-augmented generation (RAG) and parameter-efficient fine-tuning (LoRA) to address challenges in long context comprehension and out-of-domain scenarios. The approach demonstrates significant accuracy improvements on the CK12-QA dataset, achieving 4.12% better performance on validation and 9.84% on test sets for textual multiple-choice questions.

## Method Summary
The PLRTQA approach integrates Llama-2 with RAG by first retrieving relevant lesson content from the CK12-QA dataset using TF-IDF-based similarity scoring. The retrieved content, along with the question and answer options, forms an augmented context that is processed by the fine-tuned Llama-2 model. Parameter-efficient fine-tuning is performed using LoRA, training only 7 million parameters (1% of Llama-2's total) while freezing the rest of the model. This approach enables efficient adaptation to the TQA task while maintaining the strong foundational capabilities of Llama-2.

## Key Results
- Achieved 4.12% accuracy improvement on validation set for textual multiple-choice questions
- Achieved 9.84% accuracy improvement on test set for textual multiple-choice questions
- Outperformed existing methods on the CK12-QA dataset

## Why This Works (Mechanism)
The integration of RAG with fine-tuned LLMs addresses the context limitation challenge in textbook question answering. By retrieving relevant lesson content and incorporating it into the model's context, the approach provides the necessary background information that might not be present in the question alone. The parameter-efficient fine-tuning via LoRA allows the model to adapt to the specific characteristics of textbook questions while maintaining computational efficiency.

## Foundational Learning
1. **Retrieval-augmented generation (RAG)**: Combines information retrieval with text generation to enhance model responses with external knowledge
   - Why needed: Addresses context limitations in LLMs by incorporating relevant external information
   - Quick check: Verify retrieved documents are semantically relevant to questions

2. **Parameter-efficient fine-tuning (LoRA)**: Trains a small set of parameters while freezing the rest of the model
   - Why needed: Reduces computational cost while maintaining performance improvements
   - Quick check: Confirm that only target parameters are being updated during training

3. **TF-IDF similarity scoring**: Measures relevance between queries and documents based on term frequency
   - Why needed: Provides efficient retrieval of relevant lesson content from textbook corpus
   - Quick check: Ensure retrieved documents have high TF-IDF scores with input questions

## Architecture Onboarding

**Component Map**: Question -> TF-IDF Retriever -> Llama-2 (LoRA fine-tuned) -> Answer

**Critical Path**: The question flows through TF-IDF retrieval to find relevant lesson content, which is then combined with the question and passed to the fine-tuned Llama-2 model for answer generation.

**Design Tradeoffs**: The approach balances between retrieval quality (TF-IDF vs. more sophisticated methods) and fine-tuning scope (full vs. parameter-efficient). Using TF-IDF provides computational efficiency but may miss semantic nuances that embedding-based methods would capture.

**Failure Signatures**: Poor retrieval quality will manifest as irrelevant context being passed to the model, leading to incorrect answers. Overfitting during fine-tuning could cause performance degradation on out-of-domain questions.

**First 3 Experiments to Run**:
1. Evaluate retrieval quality by measuring precision@k of TF-IDF retrieved documents
2. Test model performance with varying numbers of retrieved documents (k=1,3,5)
3. Compare LoRA fine-tuning with full fine-tuning on a subset of parameters

## Open Questions the Paper Calls Out
None

## Limitations
- Focuses exclusively on textual questions, leaving multimodal questions (with images/diagrams) unexplored
- Experimental validation relies solely on the CK12-QA dataset, limiting generalizability
- Does not address potential biases in training data or complex reasoning across multiple lessons

## Confidence

| Claim | Confidence Level |
|-------|------------------|
| Technical implementation of RAG and LoRA fine-tuning | High |
| Accuracy improvements reported | Medium |
| Generalizability of results to other educational contexts | Medium |

## Next Checks

1. Evaluate the PLRTQA approach on additional textbook QA datasets beyond CK12-QA to assess generalizability across different educational domains

2. Test the method's performance on multimodal questions that combine text with images/diagrams to address the full scope of textbook QA

3. Conduct ablation studies comparing different fine-tuning approaches (full fine-tuning vs. LoRA) and varying the percentage of trainable parameters to optimize the balance between performance and efficiency