---
ver: rpa2
title: 'LLM-driven Imitation of Subrational Behavior : Illusion or Reality?'
arxiv_id: '2402.08755'
source_url: https://arxiv.org/abs/2402.08755
tags:
- reject
- offer
- human
- accept
- proposal
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper explores using Large Language Models (LLMs) to generate
  synthetic human demonstrations for learning subrational agent policies through Imitation
  Learning (IL). The authors propose that LLMs can act as implicit computational models
  of humans, allowing them to synthesize behaviors like myopia and risk aversion without
  requiring human data.
---

# LLM-driven Imitation of Subrational Behavior : Illusion or Reality?

## Quick Facts
- arXiv ID: 2402.08755
- Source URL: https://arxiv.org/abs/2402.08755
- Reference count: 37
- Primary result: LLMs can generate synthetic human demonstrations for learning subrational agent policies through IL, replicating human behavioral patterns in economic games without requiring human data.

## Executive Summary
This paper proposes using Large Language Models (LLMs) to generate synthetic human demonstrations for learning subrational agent policies through Imitation Learning (IL). The authors argue that LLMs can act as implicit computational models of humans, encoding diverse behavioral patterns from their training data. They test their framework on four economic games—ultimatum game, marshmallow experiment, double-or-nothing gamble, and academic procrastination task—showing that LLM-generated demonstrations can successfully replicate well-established findings from human studies. The approach offers a potentially cost-effective alternative to human subject studies for calibrating behavioral models across diverse human subgroups.

## Method Summary
The framework integrates LLMs with IL to learn policies that exhibit subrational human behaviors. The method involves prompting GPT-4 to generate synthetic demonstrations for specific economic games by impersonating human players with defined characteristics (e.g., fairness concerns, age groups). These demonstrations are then processed into state-action pairs and used to train a 3-layer neural network policy through IL. The approach assumes LLMs can capture human-like decision patterns including myopia, risk aversion, and social preferences, allowing them to generate realistic behavioral data without explicit reward engineering.

## Key Results
- IL policies trained on "human" LLM demonstrations correctly reject unfair offers below 30% in the ultimatum game, matching human behavior
- Age-conditioned LLM demonstrations successfully predict immediate vs delayed gratification choices in the marshmallow experiment (2-year-olds choose immediate, 5-year-olds choose delayed)
- LLM-generated demonstrations enable IL to learn risk-averse policies that match prospect theory predictions in the double-or-nothing gamble
- The framework reproduces established human behavioral patterns across all four economic games without requiring human subject data

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** LLMs can act as implicit computational models of human behavior by encoding human-like biases and decision patterns from their training data.
- **Mechanism:** LLMs trained on diverse human-generated text naturally capture patterns of human reasoning, including subrational behaviors like myopia, risk aversion, and social preferences. When prompted appropriately, they can generate synthetic demonstrations that reflect these human-like decision patterns without explicit reward engineering.
- **Core assumption:** LLMs have effectively memorized and generalized human behavioral patterns from their training corpus, allowing them to simulate specific human subgroups when properly conditioned.
- **Evidence anchors:** [abstract] "LLMs naturally encode a wide range of human behavior seen in the training data," [section 4.1] prompts like "Impersonate Jerry who cares a lot about fairness" match human acceptance/rejection rates.
- **Break condition:** If LLM training data lacks sufficient coverage of the specific human behavioral patterns being modeled, or if the prompting strategy fails to properly condition the model to the target human subgroup.

### Mechanism 2
- **Claim:** IL with LLM demonstrations can capture time-inconsistent human preferences that standard RL with exponential discounting cannot model.
- **Mechanism:** By using LLM-generated demonstrations that exhibit time-inconsistent behavior (like procrastination or hyperbolic discounting), IL can learn policies that mirror these patterns without requiring explicit modeling of complex discounting functions.
- **Core assumption:** Time-inconsistent human behavior can be effectively demonstrated and captured through LLM prompting, and IL can learn from these demonstrations even when they violate Bellman equation assumptions.
- **Evidence anchors:** [abstract] "such behavior can be generated through IL using synthetic human demonstrations from LLMs," [section 4.4] shows procrastination behavior matching quasi-hyperbolic discounting.
- **Break condition:** If the LLM demonstrations do not sufficiently cover the state space or if the IL algorithm cannot handle the non-stationary nature of time-inconsistent preferences.

### Mechanism 3
- **Claim:** LLM demonstrations provide a cost-effective alternative to human subject studies for calibrating subrational behavior models across diverse human subgroups.
- **Mechanism:** Instead of recruiting and testing different human populations, LLMs can be prompted to simulate various demographic and psychological profiles (e.g., different ages, GPAs, personality types). This allows rapid generation of behavioral data across many subgroups at minimal cost.
- **Core assumption:** LLMs can be reliably conditioned to represent different human subgroups with distinct behavioral characteristics, and these representations are sufficiently accurate for scientific purposes.
- **Evidence anchors:** [abstract] "We ground our framework by comparing our experimental findings in four economic games with real human studies in the literature, where we are able to reproduce similar findings."
- **Break condition:** If LLM conditioning to specific subgroups is unreliable or biased, leading to inaccurate behavioral predictions that don't match real human populations.

## Foundational Learning

- **Concept: Markov Decision Processes (MDPs)**
  - Why needed here: The framework models decision-making scenarios as MDPs where agents transition between states based on actions, with rewards guiding behavior.
  - Quick check question: In an MDP, what does the transition function P: S × A → ∆(S) represent, and why is it important for modeling sequential decision-making?

- **Concept: Imitation Learning (IL)**
  - Why needed here: The framework uses IL to learn policies from synthetic demonstrations rather than through trial-and-error RL.
  - Quick check question: What is the key difference between behavior cloning and inverse reinforcement learning, and when might each be more appropriate for learning from demonstrations?

- **Concept: Subrational Human Behavior**
  - Why needed here: The paper focuses on modeling behaviors that deviate from perfect rationality, such as myopia, risk aversion, and social preferences.
  - Quick check question: How does prospect theory's value function differ from expected utility theory, and what behavioral phenomenon does this difference capture?

## Architecture Onboarding

- **Component map:** Prompt design → LLM response generation → Data preprocessing → IL training → Policy evaluation
- **Critical path:** Prompt design → LLM response generation → Data preprocessing → IL training → Policy evaluation
- **Design tradeoffs:**
  - Few demonstrations vs. comprehensive coverage: Using few demonstrations is faster but may miss important state-action pairs
  - Prompt engineering complexity vs. generality: Highly specific prompts may work better but are less reusable
  - Simple IL vs. advanced methods: The paper uses basic IL for simplicity, but GAIL or other methods might perform better
- **Failure signatures:**
  - LLM responses are inconsistent or don't match expected human behavior
  - IL policy doesn't converge or produces random actions
  - Learned policy doesn't reproduce known results from human studies
  - State space coverage is insufficient, leading to poor generalization
- **First 3 experiments:**
  1. Implement and test the ultimatum game with "human" and "fair" responder prompts to verify basic LLM-IL pipeline
  2. Add age conditioning to the marshmallow experiment to test subgroup modeling capability
  3. Implement the double-or-nothing gamble to test risk preference modeling and comparison with prospect theory

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do LLM-generated demonstrations change when the model is fine-tuned on specific datasets of human behavioral data rather than general web data?
- Basis in paper: [inferred] The paper discusses using GPT-4's general knowledge but doesn't explore fine-tuning on specialized behavioral datasets.
- Why unresolved: The current framework relies on prompting a general-purpose LLM, which may not capture domain-specific human behaviors as accurately as a fine-tuned model would.
- What evidence would resolve it: Comparative experiments showing behavioral policy performance when using LLMs fine-tuned on specific behavioral datasets versus general-purpose LLMs.

### Open Question 2
- Question: What is the optimal balance between demonstration quality and quantity when using LLMs to generate synthetic human data for imitation learning?
- Basis in paper: [explicit] The paper generates 10 demonstrations per state but doesn't systematically explore the trade-off between demonstration quantity and policy quality.
- Why unresolved: While the paper shows demonstrations can be effective, it doesn't determine the minimum number of demonstrations needed or the point of diminishing returns.
- What evidence would resolve it: Systematic experiments varying the number of demonstrations per state and measuring the resulting policy performance.

### Open Question 3
- Question: How do LLM-generated demonstrations generalize to novel scenarios that differ significantly from training data in terms of state space or reward structure?
- Basis in paper: [explicit] The paper notes uncertainty about trusting LLM demonstrations in "unseen scenarios" but doesn't test this systematically.
- Why unresolved: The experiments focus on replicating established human studies, but don't explore how well the approach works when scenarios differ substantially from those in the training data.
- What evidence would resolve it: Experiments testing LLM demonstrations and resulting policies in modified versions of the experimental games with significantly altered parameters or rules.

## Limitations

- The framework lacks systematic validation of LLM-generated behavioral predictions against diverse real human populations, relying instead on reproducing established findings
- Prompt engineering remains largely heuristic with no clear methodology for ensuring consistent and reproducible behavioral conditioning across different LLM versions
- The paper doesn't explore the trade-off between demonstration quality and quantity, nor does it test generalization to novel scenarios beyond the established economic games

## Confidence

- **High Confidence**: The core demonstration that IL with LLM-generated data can learn policies matching established human behavioral patterns in economic games. The methodology for integrating LLMs with IL is clearly articulated and reproducible.
- **Medium Confidence**: The claim that LLMs can effectively model diverse human subgroups. While successful in the tested cases, the generalization to broader human populations and novel behavioral domains requires further validation.
- **Low Confidence**: The assertion that this approach provides a cost-effective alternative to human subject studies. Without systematic comparison of prediction accuracy and resource requirements across multiple domains, this remains speculative.

## Next Checks

1. **Cross-LLM Validation**: Test the framework across multiple LLM versions (GPT-3.5, Claude, LLaMA) to assess reproducibility and sensitivity to model variations. Document prompt sensitivity and required prompt engineering effort for each model.

2. **Population-Level Comparison**: Generate synthetic behavioral data for a specific demographic (e.g., age groups in the marshmallow experiment) and compare against a new human subject study. Measure prediction accuracy and identify systematic biases in LLM-generated behavior.

3. **State Space Coverage Analysis**: For each economic game, systematically evaluate whether synthetic demonstrations provide sufficient coverage of the state space. Identify critical state-action pairs that may be missing and assess their impact on policy performance through ablation studies.