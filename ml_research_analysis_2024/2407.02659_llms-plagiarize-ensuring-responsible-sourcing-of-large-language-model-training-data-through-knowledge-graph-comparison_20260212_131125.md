---
ver: rpa2
title: 'LLMs Plagiarize: Ensuring Responsible Sourcing of Large Language Model Training
  Data Through Knowledge Graph Comparison'
arxiv_id: '2407.02659'
source_url: https://arxiv.org/abs/2407.02659
tags:
- source
- graph
- document
- similarity
- continuation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a novel system for assessing whether a large
  language model (LLM) has been trained or fine-tuned on a specific copyrighted document,
  addressing legal concerns about LLM training data sourcing. The method uses RDF
  triples to convert both a source document and an LLM continuation of that document
  into knowledge graphs, then compares them using cosine similarity of vectorized
  one-edge walks and normalized graph edit distance to measure structural isomorphism.
---

# LLMs Plagiarize: Ensuring Responsible Sourcing of Large Language Model Training Data Through Knowledge Graph Comparison

## Quick Facts
- arXiv ID: 2407.02659
- Source URL: https://arxiv.org/abs/2407.02659
- Authors: Devam Mondal; Carlo Lipizzi
- Reference count: 6
- Primary result: Novel system detects if LLMs were trained on specific copyrighted documents using knowledge graph comparison

## Executive Summary
This paper proposes a novel method to determine whether a large language model (LLM) has been trained or fine-tuned on a specific copyrighted document. The approach uses RDF triples to convert both source documents and LLM continuations into knowledge graphs, then compares them using cosine similarity of vectorized one-edge walks and normalized graph edit distance to measure content and structural similarity. Unlike traditional plagiarism detection that focuses on keyword matching, this method evaluates broader relationships between ideas and their organization. The system is designed to work with closed "black-box" LLMs without requiring access to training data or model internals.

## Method Summary
The proposed system converts source documents and LLM continuations into knowledge graphs using RDF triples in [subject, predicate, object] format. It then compares these graphs through two metrics: cosine similarity of vectorized one-edge walks to assess semantic content similarity, and normalized graph edit distance to measure structural isomorphism. The method is designed to detect whether an LLM was trained on specific copyrighted material by analyzing both the content relationships and organizational patterns in the generated text.

## Key Results
- Knowledge graph comparison can detect training data usage without requiring access to LLM internals or training data
- Cosine similarity of vectorized RDF triples provides a bounded metric for semantic content comparison
- Normalized graph edit distance measures structural isomorphism between source and continuation knowledge graphs

## Why This Works (Mechanism)

### Mechanism 1
- Claim: RDF triples capture complex relationships between ideas in a structured format that can be compared across documents.
- Mechanism: The system converts both source document and LLM continuation into knowledge graphs using RDF triples in [subject, predicate, object] format, then compares these graphs through vectorized one-edge walks and cosine similarity.
- Core assumption: The relationships and organization of ideas in the source document will be reflected in the LLM continuation if the model was trained on that document.
- Evidence anchors:
  - [abstract]: "we utilize an approach that uses Resource Description Framework (RDF) triples to create knowledge graphs from both a source document and a LLM continuation of that document"
  - [section 3.1]: "we first extract RDF triples from the corpus because of their ability to capture complex relationships that encompass the main idea(s) of a sentence"
  - [corpus]: Weak - The corpus neighbors focus on copyright compliance and LLM sourcing but don't directly validate RDF triple effectiveness for plagiarism detection
- Break condition: If the LLM generates content that follows different organizational patterns than the training data, or if the continuation is based on common knowledge not present in the source document.

### Mechanism 2
- Claim: Cosine similarity of vectorized one-edge walks measures semantic content similarity between source and continuation.
- Mechanism: Each RDF triple/one-edge walk is concatenated and vectorized, then compared using cosine similarity to find matches above a threshold, with cumulative similarity indicating plagiarism likelihood.
- Core assumption: Semantic similarity between vectorized RDF triples indicates that the LLM has been trained on the source document.
- Evidence anchors:
  - [abstract]: "These graphs are then analyzed with respect to content using cosine similarity"
  - [section 3.3]: "We choose cosine similarity compared to other similarity metrics (Manhattan, Euclidean, etc.) because it is bounded and is consistently used in other literature when dealing with word embeddings"
  - [corpus]: Weak - No direct evidence in corpus about cosine similarity effectiveness for this specific application
- Break condition: If the source document contains very generic content that many other documents also contain, leading to false positives, or if the LLM uses paraphrasing that changes vector representations significantly.

### Mechanism 3
- Claim: Normalized graph edit distance measures structural isomorphism between knowledge graphs to detect organizational similarity.
- Mechanism: Calculates the minimum number of graph edit operations needed to transform one graph into another, normalized by the sum of edit distances to null graphs.
- Core assumption: Structural similarity between knowledge graphs indicates that the LLM was trained on the source document's organizational patterns.
- Evidence anchors:
  - [abstract]: "with respect to structure using a normalized version of graph edit distance that shows the degree of isomorphism"
  - [section 3.6]: "We therefore measure the degree of isomorphism by considering graph edit distance"
  - [corpus]: Weak - Corpus doesn't provide evidence about graph edit distance effectiveness for this specific application
- Break condition: If two documents have similar structures but completely different content (like both discussing "stocks" vs "cars" with identical structures), or if the continuation has a different structure but similar content.

## Foundational Learning

- Concept: Knowledge graphs and RDF triples
  - Why needed here: The entire plagiarism detection mechanism relies on converting text into structured graph representations that capture relationships between concepts
  - Quick check question: What are the three components of an RDF triple and how do they represent relationships between ideas?

- Concept: Cosine similarity and vector embeddings
  - Why needed here: Used to measure semantic similarity between the vectorized RDF triples from source and continuation documents
  - Quick check question: Why is cosine similarity preferred over Euclidean distance when comparing word embeddings?

- Concept: Graph edit distance and isomorphism
  - Why needed here: Used to measure structural similarity between knowledge graphs, detecting whether the LLM reproduced the organizational patterns from the source document
  - Quick check question: What is the difference between exact graph isomorphism and approximate isomorphism as measured by graph edit distance?

## Architecture Onboarding

- Component map: Source document → RDF triple extraction → Knowledge graph generation → Vectorization → Cosine similarity comparison + Graph edit distance calculation → Plagiarism assessment
- Critical path: RDF triple extraction → Knowledge graph generation → Vectorization → Similarity calculations → Threshold comparison
- Design tradeoffs:
  - RDF triples provide semantic richness but may miss nuanced relationships
  - Cosine similarity is bounded and interpretable but sensitive to vectorization quality
  - Graph edit distance captures structure but is computationally expensive (NP-hard)
- Failure signatures:
  - High cosine similarity but low graph edit distance (content matches but different structure)
  - Low cosine similarity but high graph edit distance (similar structure but different content)
  - Thresholds too high/low causing false negatives/positives
- First 3 experiments:
  1. Test with known training data: Fine-tune an LLM on a fabricated document, generate continuation, verify high similarity scores
  2. Test with unrelated documents: Use completely unrelated source and continuation, verify low similarity scores
  3. Test with paraphrased content: Use source document and continuation with similar meaning but different wording, verify system detects similarity

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How effective is the proposed system at distinguishing between LLMs trained on specific copyrighted documents versus those that have only been exposed to similar content during pre-training?
- Basis in paper: [inferred] The paper discusses using knowledge graphs and cosine similarity to detect training data usage, but does not provide experimental validation comparing fine-tuned models against pre-trained ones on similar topics.
- Why unresolved: The paper only outlines the methodology and proposes future work to test the system, without presenting actual experimental results or validation data.
- What evidence would resolve it: Empirical results showing false positive rates when testing the system on LLMs trained on similar but different documents, and comparison data showing the system's ability to distinguish between fine-tuned and pre-trained models.

### Open Question 2
- Question: What is the optimal threshold combination (mincos θ and mt) for different types of documents and use cases?
- Basis in paper: [explicit] The paper states "The thresholds mentioned in our work are up to the user and use case of the document and LLM" and proposes future work to find definite values.
- Why unresolved: The authors acknowledge that threshold values are arbitrary and use-case dependent, but do not provide guidance on how to determine appropriate thresholds for different scenarios.
- What evidence would resolve it: A comprehensive study testing the system across various document types, lengths, and domains with documented threshold performance metrics for each use case.

### Open Question 3
- Question: How can content and structure metrics be effectively weighted in a combined metric to avoid misleading results from structurally similar but semantically different knowledge graphs?
- Basis in paper: [explicit] Section 3.7 discusses the limitation that normGED only considers structure and may be misleading when content differs, and Section 5 mentions the need for a combined metric.
- Why unresolved: While the paper identifies this as a limitation and suggests the need for a weighted compound metric, it does not propose a specific method for weighting or combining the metrics.
- What evidence would resolve it: A proposed formula or algorithm for weighting content (GCcos θ) and structure (normGED) metrics, along with experimental validation showing improved accuracy over using either metric alone.

## Limitations

- Threshold sensitivity creates high false positive/negative potential without empirical calibration
- Graph edit distance calculation is NP-hard, limiting computational scalability
- System may struggle with paraphrased content or semantically equivalent statements in different syntactic structures

## Confidence

- High confidence in RDF triple conversion and knowledge graph creation methodology
- Medium confidence in overall system effectiveness due to limited empirical validation
- Low confidence in threshold values and their generalizability across domains

## Next Checks

1. **Threshold calibration experiment**: Systematically test the system across a diverse corpus of known training data pairs and unrelated document pairs to empirically determine optimal threshold values for cosine similarity and normalized graph edit distance that maximize precision and recall.

2. **Paraphrasing robustness test**: Evaluate system performance on source documents and their paraphrased versions to assess whether the knowledge graph comparison can detect semantic equivalence despite syntactic differences, identifying potential failure modes.

3. **Cross-LLM generalization study**: Test the system across multiple LLM architectures (GPT, Claude, LLaMA, etc.) and model sizes to validate that the approach works consistently regardless of the specific black-box model being analyzed.