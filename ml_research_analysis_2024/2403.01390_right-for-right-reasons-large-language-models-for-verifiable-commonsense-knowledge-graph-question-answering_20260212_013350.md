---
ver: rpa2
title: 'Right for Right Reasons: Large Language Models for Verifiable Commonsense
  Knowledge Graph Question Answering'
arxiv_id: '2403.01390'
source_url: https://arxiv.org/abs/2403.01390
tags:
- answer
- shot
- reasoning
- question
- facts
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces Right for Right Reasons (R3), a method to
  improve verifiable commonsense reasoning for KGQA using LLMs. R3 addresses hallucination
  in existing LLM-based KGQA by surfacing commonsense axioms and enforcing grounding
  on KG facts at every reasoning step.
---

# Right for Right Reasons: Large Language Models for Verifiable Commonsense Knowledge Graph Question Answering

## Quick Facts
- arXiv ID: 2403.01390
- Source URL: https://arxiv.org/abs/2403.01390
- Reference count: 40
- Primary result: R3 improves verifiable commonsense KGQA by enforcing KG grounding at every reasoning step, achieving near-perfect factual scores and robustness to entity popularity

## Executive Summary
This paper introduces Right for Right Reasons (R3), a method to enhance verifiable commonsense reasoning for knowledge graph question answering (KGQA) using large language models. R3 addresses hallucination in existing LLM-based KGQA by surfacing commonsense axioms and enforcing grounding on KG facts at every reasoning step. It treats KGQA as a tree-structured search where each step is verifiable. Experiments on three tasks (question answering, claim verification, preference matching) show R3 outperforms baselines in accuracy, reduces hallucination, and is robust to entity popularity.

## Method Summary
R3 improves KGQA by enforcing verifiable reasoning through commonsense axioms and KG fact grounding at each step. It models KGQA as a tree-structured search problem where each reasoning step must be grounded in either a KG fact or a commonsense axiom. The method surfaces relevant axioms during reasoning and validates each step against the knowledge graph, ensuring that the reasoning chain remains factual and traceable. This approach addresses hallucination by making each intermediate reasoning step verifiable, rather than only checking the final answer.

## Key Results
- R3 achieves near-perfect factual scores across all three evaluated tasks
- Demonstrates robustness to entity popularity, maintaining high accuracy for both popular and long-tail entities
- Significantly reduces hallucination compared to standard LLM baselines while maintaining high reasoning accuracy

## Why This Works (Mechanism)
R3 works by transforming KGQA into a verifiable tree-search problem where each reasoning step is constrained by either KG facts or commonsense axioms. By surfacing relevant commonsense axioms during reasoning and enforcing grounding at every step, R3 prevents the accumulation of hallucinated content that typically occurs in free-form LLM reasoning. The method ensures that each intermediate conclusion can be traced back to verifiable knowledge sources, creating a chain of reasoning that is both explainable and fact-checked.

## Foundational Learning
1. **Commonsense Knowledge Representation**: Understanding how commonsense axioms are structured and retrieved
   - Why needed: R3 relies on surfacing relevant commonsense knowledge during reasoning
   - Quick check: Verify axiom format consistency and retrieval accuracy

2. **Knowledge Graph Query Processing**: Understanding KG structure and query execution patterns
   - Why needed: R3 must efficiently traverse and validate against KG facts
   - Quick check: Confirm KG query response times and accuracy

3. **Tree-Structured Search Algorithms**: Understanding search space exploration and pruning strategies
   - Why needed: R3 models reasoning as a tree search with verifiable nodes
   - Quick check: Validate search completeness and efficiency

## Architecture Onboarding
**Component Map**: LLM reasoning engine -> Commonsense axiom retriever -> KG fact validator -> Search tree manager -> Answer generator

**Critical Path**: User query -> Tree initialization -> Axiom retrieval -> Step-by-step reasoning with KG validation -> Final answer generation

**Design Tradeoffs**: R3 trades some reasoning flexibility for verifiability, requiring more computational overhead per step but gaining in factual accuracy and explainability.

**Failure Signatures**: Common failure modes include: axiom retrieval failures leading to reasoning dead-ends, KG fact validation mismatches causing search pruning of valid paths, and context window limitations restricting complex reasoning chains.

**First Experiments**: 1) Test axiom retrieval accuracy on simple commonsense questions, 2) Validate KG grounding accuracy on isolated reasoning steps, 3) Evaluate search tree performance on benchmark KGQA tasks

## Open Questions the Paper Calls Out
None specified in the provided materials.

## Limitations
- The evaluation assumes correctness of LLM-generated commonsense axioms, potentially creating circular validation
- Performance may be tied to specific LLM configurations (GPT-4o-mini with 4K context)
- Lacks assessment of commonsense reasoning quality beyond factual accuracy

## Confidence
- High Confidence: Core claim of hallucination reduction is well-supported by experimental results
- Medium Confidence: Entity popularity robustness claims are supported but need broader domain analysis
- Low Confidence: Generalizability to KGQA tasks beyond evaluated domains remains uncertain

## Next Checks
1. **Error Propagation Analysis**: Study how incorrect commonsense axioms affect final answer accuracy and develop independent axiom quality metrics

2. **Cross-Model Validation**: Test R3 across different LLM architectures and context windows to establish generalizability

3. **Domain Transfer Evaluation**: Assess R3 performance on KGQA datasets from different domains (biomedical, financial) to evaluate consistency and identify domain-specific challenges