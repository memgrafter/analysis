---
ver: rpa2
title: 'Analysis of Linear Mode Connectivity via Permutation-Based Weight Matching:
  With Insights into Other Permutation Search Methods'
arxiv_id: '2402.04051'
source_url: https://arxiv.org/abs/2402.04051
tags:
- singular
- vectors
- values
- permutation
- value
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work provides a theoretical and empirical analysis of linear
  mode connectivity (LMC) achieved via weight matching (WM). It challenges the prevailing
  belief that WM achieves LMC by reducing L2 distance between models, demonstrating
  instead that the key mechanism is the alignment of singular vectors corresponding
  to large singular values across layers.
---

# Analysis of Linear Mode Connectivity via Permutation-Based Weight Matching: With Insights into Other Permutation Search Methods

## Quick Facts
- arXiv ID: 2402.04051
- Source URL: https://arxiv.org/abs/2402.04051
- Reference count: 40
- This work challenges the prevailing belief that weight matching achieves LMC by reducing L2 distance, showing instead that singular vector alignment is the key mechanism.

## Executive Summary
This paper provides a theoretical and empirical analysis of linear mode connectivity (LMC) achieved through weight matching (WM). The authors demonstrate that WM achieves LMC not by reducing L2 distance between models, but by aligning singular vectors corresponding to large singular values across layers. Through both theoretical arguments and experimental validation, they show that WM preferentially aligns these dominant singular vectors, thereby preserving model functionality after merging. The study also compares WM with straight-through estimator (STE), showing that WM is more effective for merging three or more models due to its consistent alignment of singular vectors independent of the loss landscape.

## Method Summary
The authors train multiple instances of neural networks (VGG11, ResNet20, MLP) on standard datasets (MNIST, FMNIST, CIFAR10) with different random seeds. They then apply permutation search using either weight matching (WM) or straight-through estimator (STE) methods to find optimal permutations between model pairs. WM uses Sinkhorn's algorithm to minimize L2 distance between weight matrices, while STE directly minimizes the barrier using dataset information. The effectiveness of each method is evaluated by measuring L2 distances, singular vector alignment (using R metric), and barrier values. Singular value decomposition analysis is used to understand how weight permutations affect model functionality.

## Key Results
- WM achieves LMC by aligning directions of singular vectors with large singular values, not by reducing L2 distance between models
- WM is more effective than STE for merging three or more models because it consistently aligns singular vectors independent of the loss landscape
- The alignment of dominant singular vectors preserves model functionality during merging, as these vectors determine the model's input-output behavior

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Weight matching achieves LMC by aligning directions of singular vectors corresponding to large singular values, not by reducing L2 distance.
- Mechanism: WM minimizes L2 distance between weight matrices across layers, but this minimization does not make the weights themselves close. Instead, the permutation search found by WM preferentially aligns the directions of singular vectors with large singular values between models, preserving model functionality during merging.
- Core assumption: The difference in functionality between independently trained models stems from differences in the directions of singular vectors, not singular values.
- Evidence anchors:
  - [abstract] "permutations found by WM mainly align the directions of singular vectors associated with large singular values across models"
  - [section] "Equation (3) shows that the permutation matrices Pℓ and Pℓ−1 are multiplied by the left and right singular vectors of the model θb, respectively. Since permutation matrices are orthogonal, the permutation matrices can change their directions, but not the norms, and therefore do not affect the singular values."
  - [corpus] Weak - related papers discuss permutations and LMC but do not provide direct evidence for singular vector alignment as the mechanism.
- Break condition: If singular values between models differ significantly, or if the model architecture prevents meaningful singular vector alignment.

### Mechanism 2
- Claim: WM is more effective than STE for merging three or more models because it consistently aligns singular vectors independent of the loss landscape.
- Mechanism: STE finds permutations that minimize the barrier directly using dataset information, leading to data-dependent alignment that may not generalize across multiple models. WM aligns singular vectors based on weight structure alone, creating consistent alignment across all models being merged.
- Core assumption: Permutations found by STE depend on the local loss landscape rather than the linear algebraic properties of weight matrices.
- Evidence anchors:
  - [abstract] "WM outperforms STE, especially when merging three or more models"
  - [section] "permutations found by STE do not align the directions of singular vectors, highlighting a fundamental difference from WM in achieving LMC"
  - [corpus] Weak - related papers discuss STE and WM but do not provide direct evidence for this specific comparison.
- Break condition: If the loss landscape topology prevents consistent singular vector alignment across multiple models.

### Mechanism 3
- Claim: Singular vectors with large singular values determine model functionality, making their alignment critical for LMC.
- Mechanism: The input-output relationship of linear maps induced from weight matrices of each layer is approximately determined by their dominant singular vectors with large singular values. By aligning these dominant vectors, WM ensures merged models retain similar functionality to original models.
- Core assumption: Model functionality is primarily determined by the behavior of dominant singular vectors rather than the entire singular vector space.
- Evidence anchors:
  - [abstract] "This alignment brings the singular vectors with large singular values, which determine the model's functionality, closer between pre-merged and post-merged models"
  - [section] "Since the input-output relationship of a linear map induced from the weight matrices of each layer is determined by the dominant singular vector with large singular values"
  - [corpus] Weak - related papers discuss LMC but do not provide direct evidence for the functional importance of dominant singular vectors.
- Break condition: If model functionality depends on fine-grained details captured by smaller singular values.

## Foundational Learning

- Concept: Singular Value Decomposition (SVD)
  - Why needed here: The analysis fundamentally relies on decomposing weight matrices into singular values and vectors to understand how WM achieves LMC.
  - Quick check question: What property of permutation matrices ensures they can change singular vector directions but not singular values?

- Concept: Linear Mode Connectivity (LMC)
  - Why needed here: Understanding LMC is essential to grasp why achieving it through weight matching is valuable for model merging and federated learning.
  - Quick check question: How does LMC differ from simply having two models with similar performance?

- Concept: Permutation Invariance in Neural Networks
  - Why needed here: The entire analysis builds on the fact that neural networks have permutation symmetries that can be exploited to connect different solutions.
  - Quick check question: Why can we permute the weights of one model without changing its input-output behavior?

## Architecture Onboarding

- Component map: Train models -> Apply WM/STE permutation search -> Merge models -> Evaluate LMC and functionality preservation
- Critical path: Train two models → apply WM/STE permutation search → merge models → evaluate LMC and functionality preservation
- Design tradeoffs: WM provides dataset-independent alignment but may not always find optimal permutations for specific tasks, while STE uses dataset information but may create inconsistent alignments across multiple models
- Failure signatures: High barrier values after merging indicate failed LMC; poor singular vector alignment indicates WM/STE failed to find appropriate permutations
- First 3 experiments:
  1. Train two MLPs on MNIST with different seeds, apply WM, measure L2 distance change and barrier value
  2. Compare WM vs STE on merging three ResNet20 models trained on CIFAR10
  3. Vary model width multipliers and measure how R values (singular vector alignment) change with WM

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the fundamental mechanism behind LMC satisfaction for methods like STE that do not rely on singular vector alignment?
- Basis in paper: [explicit] The paper demonstrates that STE does not align singular vectors but still achieves LMC, suggesting a different mechanism at play.
- Why unresolved: The paper primarily focuses on analyzing WM's effectiveness through singular vector alignment, leaving the mechanism behind STE's LMC satisfaction unexplored.
- What evidence would resolve it: Further experimental analysis comparing the behavior of STE with other methods that do align singular vectors, such as examining the impact on the loss landscape or activation patterns.

### Open Question 2
- Question: How does the model width impact the effectiveness of WM in achieving LMC?
- Basis in paper: [explicit] The paper suggests that increasing model width makes it easier to satisfy LMC by WM due to a decrease in the proportion of dominant singular vectors.
- Why unresolved: The paper only provides a theoretical explanation and limited experimental evidence, leaving the quantitative relationship between model width and LMC effectiveness unexplored.
- What evidence would resolve it: Conducting a comprehensive experimental study varying model width across different architectures and datasets to establish a clear relationship between width and LMC success rate.

### Open Question 3
- Question: Can the insights gained from analyzing LMC in image classification tasks be generalized to other domains like natural language processing or reinforcement learning?
- Basis in paper: [inferred] The paper focuses on image classification tasks using CNNs, leaving the applicability of the findings to other domains uncertain.
- Why unresolved: The paper does not explore the applicability of the analysis to different types of models or tasks, limiting the generalizability of the insights.
- What evidence would resolve it: Extending the analysis to other domains and model architectures, such as Transformers for NLP or recurrent neural networks for reinforcement learning, to determine if similar principles apply.

## Limitations

- The analysis primarily focuses on MLPs and CNNs, leaving unclear whether the singular vector alignment mechanism generalizes to other architectures like transformers
- The comparison between WM and STE is based on specific datasets and model architectures, which may not represent performance on more complex or diverse datasets
- The theoretical framework assumes that singular vectors with large singular values determine model functionality, which may not hold for all model architectures or tasks

## Confidence

- **High Confidence**: The empirical demonstration that WM achieves LMC through permutation search and the observation that WM aligns singular vectors with large singular values across layers
- **Medium Confidence**: The theoretical explanation that singular vector alignment, rather than L2 distance reduction, is the key mechanism for WM's effectiveness
- **Low Confidence**: The claim that WM consistently outperforms STE for merging three or more models

## Next Checks

1. **Architecture Generalization Test**: Apply WM and STE to transformer-based architectures (BERT, ViT) and evaluate whether the singular vector alignment mechanism still holds. Measure R values and barrier heights across different attention mechanisms and positional encodings.

2. **Loss Landscape Dependence Analysis**: Systematically vary training hyperparameters (learning rate, batch size, optimizer type) and measure how the quality of permutations found by WM versus STE changes. This would test the claim that STE permutations depend on loss landscape while WM permutations do not.

3. **Multi-Model Merging Robustness**: Extend the three-model merging experiments to five or more models with varying degrees of similarity in their training trajectories. Measure how WM and STE performance scales with the number of models and the diversity of their weight spaces.