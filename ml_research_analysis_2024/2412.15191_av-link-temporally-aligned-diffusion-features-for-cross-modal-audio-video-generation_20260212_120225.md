---
ver: rpa2
title: 'AV-Link: Temporally-Aligned Diffusion Features for Cross-Modal Audio-Video
  Generation'
arxiv_id: '2412.15191'
source_url: https://arxiv.org/abs/2412.15191
tags:
- video
- audio
- generation
- arxiv
- alignment
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces AV-Link, a unified framework for cross-modal
  audio-video generation that achieves both video-to-audio (V2A) and audio-to-video
  (A2V) generation in a single model. The key innovation is a Fusion Block that enables
  bidirectional information exchange between frozen video and audio diffusion models
  through temporally-aligned self-attention operations.
---

# AV-Link: Temporally-Aligned Diffusion Features for Cross-Modal Audio-Video Generation

## Quick Facts
- arXiv ID: 2412.15191
- Source URL: https://arxiv.org/abs/2412.15191
- Authors: Moayed Haji-Ali; Willi Menapace; Aliaksandr Siarohin; Ivan Skorokhodov; Alper Canberk; Kwot Sin Lee; Vicente Ordonez; Sergey Tulyakov
- Reference count: 40
- Primary result: 76.4% improvement in onset accuracy vs baselines, preferred by users 63.6% over MovieGen

## Executive Summary
AV-Link introduces a unified framework for cross-modal audio-video generation that achieves both video-to-audio (V2A) and audio-to-video (A2V) generation in a single model. The key innovation is a Fusion Block that enables bidirectional information exchange between frozen video and audio diffusion models through temporally-aligned self-attention operations. By leveraging the activations from these pretrained models directly, AV-Link bypasses the need for specialized feature extractors and achieves significantly improved audio-video synchronization. On the VGGSounds benchmark, AV-Link achieves a 76.4% improvement in onset accuracy compared to the best baseline and is preferred by users 63.6% of the time over the larger MovieGen model for temporal alignment.

## Method Summary
AV-Link is a unified framework for cross-modal audio-video generation that uses frozen pretrained diffusion models for audio and video generation, connected through Fusion Blocks that enable bidirectional information exchange. The Fusion Blocks use temporally-aligned self-attention with 1D-RoPE embeddings to establish correspondence between audio and video tokens, while symmetric feature reinjection allows the conditioning modality to adapt based on the generated output. The framework achieves temporal alignment by leveraging activations from frozen diffusion models, which contain semantically and temporally aligned information. Training involves 50k iterations with fixed conditioning timesteps (0.96 for V2A, 0.8 for A2V), and the model demonstrates strong performance on both V2A and A2V tasks while maintaining a compact design with only 186M additional parameters.

## Key Results
- 76.4% improvement in onset accuracy compared to best baseline on VGGSounds
- 63.6% user preference over MovieGen for temporal alignment
- Strong performance in both V2A and A2V tasks while maintaining compact design
- Demonstrates bidirectional information flow between modalities through Fusion Blocks

## Why This Works (Mechanism)

### Mechanism 1
Using activations from frozen diffusion models provides richer temporal alignment than external feature extractors. Pretrained diffusion models learn to generate modalities with temporal components, so their internal activations contain both semantic and temporal information aligned to the generated sequence. The core assumption is that diffusion model activations encode temporal structure that correlates with the content being generated.

### Mechanism 2
Symmetric feature reinjection enables bidirectional information flow that dynamically refines conditioning signals. Features processed by Fusion Blocks are reinjected back into both conditioning and generated modality backbones, allowing the conditioning modality to adapt based on the generated output. The core assumption is that the conditioning modality can benefit from feedback about the generated modality's current state.

### Mechanism 3
Temporally-aligned RoPE embeddings establish correspondence between audio and video tokens. RoPE embeddings rotate corresponding audio and video tokens by the same angle proportional to their temporal position, creating temporal correspondence. The core assumption is that temporal alignment between modalities can be established through shared positional rotation patterns.

## Foundational Learning

- Concept: Flow Matching framework
  - Why needed here: The paper builds on Flow Matching as the generative foundation, understanding how it differs from diffusion is crucial
  - Quick check question: What is the key difference between Flow Matching and standard diffusion in terms of the training objective?

- Concept: Cross-attention and self-attention in transformers
  - Why needed here: Fusion Blocks use both self-attention (within modalities) and cross-attention (between modalities), understanding their roles is essential
  - Quick check question: How does the attention mechanism differ when processing concatenated audio-video features versus separate modality features?

- Concept: Temporal alignment in multimodal generation
  - Why needed here: The paper's core contribution is temporal alignment, understanding what makes alignment "good" is fundamental
  - Quick check question: What metrics would you use to quantify temporal alignment between generated audio and video?

## Architecture Onboarding

- Component map: Frozen video DiT (576M) -> 8 Fusion Blocks (186M) -> Frozen audio DiT (576M)

- Critical path:
  1. Extract activations from frozen generator at specific flow timestep
  2. Project to common dimension and concatenate modalities
  3. Apply self-attention with temporally-aligned RoPE
  4. Project back and reinject features into both backbones
  5. Generate output using conditioned backbone

- Design tradeoffs:
  - Frozen generators vs. finetuning: Freezing preserves generation quality but limits adaptation
  - Fixed vs. variable flow timesteps: Fixed timesteps (0.96 for V2A, 0.8 for A2V) work better but reduce flexibility
  - Shared vs. separate Fusion Block parameters: Separate parameters give marginal improvement at cost of more parameters

- Failure signatures:
  - Audio-video misalignment: Check if RoPE alignment is working correctly
  - Degraded generation quality: Verify frozen generators aren't being corrupted by feature reinjection
  - Slow convergence: Ensure flow timestep distribution is shifted toward noisier steps

- First 3 experiments:
  1. Test different flow timesteps for conditioning features (uniform sampling vs. fixed at 0.96/0.8)
  2. Compare diffusion features vs. external feature extractors (CLIP, CA VP) for conditioning
  3. Validate symmetric feature reinjection by disabling it and measuring alignment degradation

## Open Questions the Paper Calls Out

### Open Question 1
How does AV-Link's performance scale with larger video models and higher resolutions? The paper uses a relatively small video model (576M parameters) and acknowledges that leveraging a large high-resolution latent video model may further improve performance. Training and evaluating AV-Link with larger video models (e.g., 1-10B parameters) and higher resolutions would resolve this question.

### Open Question 2
What is the impact of reducing the number of sampling steps through distillation techniques on AV-Link's temporal alignment performance? The paper acknowledges that distillation methods can significantly improve inference time but regards this direction as orthogonal to their work. Implementing distillation techniques and evaluating temporal alignment metrics would resolve this question.

### Open Question 3
How does AV-Link perform on datasets outside the VGGSounds and AudioSet distributions, particularly for rare or complex audio-visual events? The paper evaluates primarily on VGGSounds and AudioSet, which are well-curated datasets. Testing AV-Link on diverse datasets with rare audio-visual events would reveal limitations in handling complex scenarios.

## Limitations

- Performance depends on quality of frozen diffusion model activations, which may not capture all temporal relationships
- Evaluation methodology limited to VGGSounds and AudioSet benchmarks, which may not reflect real-world diversity
- User study sample size of 30 participants is relatively small for establishing robust perceptual preferences

## Confidence

- High confidence: The effectiveness of the Fusion Block architecture for enabling bidirectional information flow between modalities
- Medium confidence: The superiority of diffusion model activations over external feature extractors for conditioning
- Low confidence: The claim that temporal alignment is primarily achieved through 1D-RoPE embeddings

## Next Checks

1. **Layer-wise feature analysis**: Conduct systematic evaluation of how feature quality varies across different layers of the frozen diffusion models when used as conditioning signals.

2. **Cross-dataset generalization test**: Evaluate AV-Link performance on out-of-distribution audio-video datasets not seen during training to assess real-world robustness.

3. **Ablation on temporal alignment mechanisms**: Perform detailed ablation studies isolating the contribution of 1D-RoPE from other architectural components.