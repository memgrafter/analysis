---
ver: rpa2
title: Simplicity bias and optimization threshold in two-layer ReLU networks
arxiv_id: '2410.02348'
source_url: https://arxiv.org/abs/2410.02348
tags:
- training
- bias
- loss
- equation
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates why neural networks often generalize well
  despite overfitting the training data. While most research assumes networks converge
  to global minima, this work shows they often stop short of full interpolation.
---

# Simplicity bias and optimization threshold in two-layer ReLU networks

## Quick Facts
- arXiv ID: 2410.02348
- Source URL: https://arxiv.org/abs/2410.02348
- Authors: Etienne Boursier; Nicolas Flammarion
- Reference count: 40
- Key outcome: Neural networks often stop short of full interpolation, converging to simpler solutions related to the true loss minimizer rather than global minima, improving test loss by effectively ignoring label noise

## Executive Summary
This paper investigates why neural networks often generalize well despite overfitting training data. The authors demonstrate that two-layer ReLU networks exhibit a "simplicity bias," converging to simpler solutions rather than interpolating training data. This occurs through an early alignment phase where neurons align with specific directions (extremal vectors) that drive training dynamics. For large training datasets, these extremal vectors concentrate around key directions associated with the ground truth model, creating an optimization threshold beyond which networks no longer interpolate but approximate the optimal least squares estimator.

## Method Summary
The paper analyzes two-layer ReLU networks trained via gradient flow (continuous-time gradient descent) on linear regression tasks. Using synthetic data from a linear model y_k = x_k^T β⋆ + η_k with sub-Gaussian inputs and centered noise, the authors study how network behavior changes with training set size. The analysis focuses on the early alignment phase where neurons align with extremal vectors in the loss landscape. Experiments vary the number of training samples n to observe the transition from interpolation to simpler solutions, measuring train and test loss at convergence.

## Key Results
- Neural networks trained on large datasets often fail to reach global minima, instead converging to spurious local minima as n increases
- For large n, extremal vectors concentrate around the true signal directions, preventing full interpolation
- When n exceeds an optimization threshold, networks approximate the optimal least squares estimator rather than fitting all training data
- This simplicity bias improves test loss by effectively ignoring label noise, with test loss close to the noise level rather than exhibiting tempered overfitting

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Neural networks trained on large datasets often fail to reach global minima due to early directional alignment of neurons.
- **Mechanism**: During early alignment, neurons align toward extremal vectors that correspond to critical directions in the loss landscape. For large datasets, these vectors concentrate around ground truth directions.
- **Core assumption**: Training data is sufficiently large and structured so extremal vectors concentrate around true signal directions.
- **Evidence anchors**:
  - [abstract]: "Instead for such tasks, it has been empirically observed that the trained models goes from global minima to spurious local minima of the training loss as the number of training samples becomes larger than some level we call optimization threshold."
  - [section]: "Proposition 1 states that for large n, the extremal vectors concentrate towards the vectors satisfying Equation (6)."
- **Break condition**: If data is orthogonal or has very few samples relative to dimensionality, many extremal vectors may exist, preventing concentration effect.

### Mechanism 2
- **Claim**: When training samples exceed an optimization threshold, networks converge to simpler solutions that approximate the optimal least squares estimator.
- **Mechanism**: After early alignment, neurons remain aligned with extremal vectors associated with ground truth. For large datasets, this results in a final model behaving like the difference of two ReLU neurons with nearly opposite directions.
- **Core assumption**: Data follows a symmetric distribution and noise has bounded fourth moment.
- **Evidence anchors**:
  - [abstract]: "This leads to an optimization threshold: when the number of training samples exceeds this threshold, networks no longer interpolate but instead approximate the optimal least squares estimator."
  - [section]: "Theorem 2 below, which states that at convergence and for a large enough number of training samples, the sum of the positive (resp. negative) neurons will correspond to the OLS estimator on the data subset S+ (resp. S−)."
- **Break condition**: If data distribution violates symmetry assumptions or has very complex ground truth structure, network may still interpolate or converge to more complex solution.

### Mechanism 3
- **Claim**: The simplicity bias improves test loss by effectively ignoring label noise when the network does not interpolate.
- **Mechanism**: By converging to simpler solution rather than fitting all training data points, the network avoids fitting noise in labels, resulting in test loss close to noise level.
- **Core assumption**: Noise in labels is independent of input features and has finite variance.
- **Evidence anchors**:
  - [abstract]: "This paper explores theoretically this phenomenon in the context of two-layer ReLU networks. We demonstrate that, despite overparametrization, networks often converge toward simpler solutions rather than interpolating the training data, which can lead to a drastic improvement on the test loss."
  - [section]: "As a consequence, the model fits the true signal of the data, while effectively ignoring label noise."
- **Break condition**: If noise is structured or correlated with input features, ignoring some data points may not lead to optimal generalization.

## Foundational Learning

- **Concept**: Early alignment in neural network training
  - Why needed here: The paper's main mechanism relies on understanding how neurons align during early training phases before they significantly change magnitude.
  - Quick check question: What happens to the norm of neurons during the early alignment phase compared to their direction?

- **Concept**: Extremal vectors in ReLU network optimization
  - Why needed here: The concentration of extremal vectors determines which directions the network neurons align to, which is crucial for understanding the simplicity bias.
  - Quick check question: How do extremal vectors relate to the gradient information in the loss landscape?

- **Concept**: Optimization threshold and its relation to sample complexity
  - Why needed here: The transition from interpolation to simpler solutions depends on the relationship between dataset size and dimensionality.
  - Quick check question: How does the optimization threshold scale with the dimensionality of the input space?

## Architecture Onboarding

- **Component map**:
  - Input layer → Hidden layer (ReLU activations) → Output layer
  - Key components: Neuron weights (w_j, a_j), training data (x_k, y_k), gradient flow dynamics
  - Critical phase: Early alignment where neurons align with extremal vectors

- **Critical path**:
  1. Initialize small weights following Equation (2)
  2. Train using gradient flow until convergence
  3. Monitor neuron alignment during early phase
  4. Observe whether interpolation occurs based on dataset size

- **Design tradeoffs**:
  - Initialization scale (λ): Smaller values enhance early alignment but may slow convergence
  - Network width (m): Must be large enough for overparameterization but doesn't affect optimization threshold
  - Dataset size (n): Must exceed threshold for simplicity bias to manifest

- **Failure signatures**:
  - If neurons don't align with extremal vectors: Check initialization scale and training duration
  - If interpolation occurs despite large n: Verify data distribution satisfies symmetry assumptions
  - If test loss remains poor: Check noise characteristics and data complexity

- **First 3 experiments**:
  1. Linear data model with varying n: Generate synthetic data y_k = x^T β⋆ + η_k with increasing sample sizes to observe transition from interpolation to simplicity bias
  2. Orthogonal data test: Create orthogonal input data to verify that many extremal vectors exist when n ≤ d
  3. Non-symmetric data test: Modify data distribution to violate symmetry assumptions and observe how optimization threshold changes

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: Can the concentration bounds on extremal vectors be extended to more complex teacher architectures beyond linear and 5-ReLU models?
- **Basis in paper**: [explicit] The paper states "we believe that the different conclusions of our work remain valid in more complex setups" and mentions additional experiments with a 5-ReLU teacher yielding similar behaviors.
- **Why unresolved**: The authors explicitly state that proving such results for more complex teachers is left for future work. Current analysis relies on specific properties of linear models that may not extend to more complex architectures.
- **What evidence would resolve it**: Mathematical proofs showing that for non-linear teachers, the number of extremal vectors remains small and concentrates around the teacher's parameters, along with experimental validation on various teacher architectures.

### Open Question 2
- **Question**: What is the fundamental reason for the transition from interpolation to generalization as the number of training samples increases, and how general is this phenomenon?
- **Basis in paper**: [explicit] The paper identifies this as a central question and investigates it theoretically in the two-layer ReLU network setting, observing similar patterns in diffusion models and in-context learning.
- **Why unresolved**: While the paper demonstrates this behavior in specific settings, it acknowledges that "a theoretical analysis remains intractable" for more complex and realistic settings where this has been empirically observed.
- **What evidence would resolve it**: A unified theoretical framework explaining the optimization threshold phenomenon across different architectures and tasks, validated through extensive experiments across diverse learning scenarios.

### Open Question 3
- **Question**: How does the dimensionality of the input space affect the optimization threshold and the simplicity bias phenomenon?
- **Basis in paper**: [inferred] The paper shows experiments in 5 dimensions and mentions that running similar experiments with dimension d=10 yields a similar threshold around n=25000, confirming the cubic dependency. Theorem 2 requires n ≳ d^3 log d.
- **Why unresolved**: The paper only provides experimental evidence for two specific dimensions and theoretical bounds that may be loose. The exact relationship between dimensionality and the optimization threshold remains unclear.
- **What evidence would resolve it**: Systematic experiments varying dimensionality across multiple orders of magnitude, combined with refined theoretical analysis to determine the precise scaling relationship between input dimension and the optimization threshold.

## Limitations

- Theoretical framework assumes sub-Gaussian data distributions and symmetric noise conditions that may not hold in practical scenarios
- Analysis focuses on gradient flow (continuous-time) rather than practical discrete-time SGD training dynamics
- Results are proven specifically for two-layer ReLU networks and may not generalize to deeper architectures or other activation functions

## Confidence

- Early alignment mechanism and extremal vector concentration: High confidence (supported by Proposition 1 and theoretical proofs)
- Optimization threshold existence and its relation to sample size: Medium confidence (theoretically proven for linear models, supported by experiments but with limited empirical breadth)
- Simplicity bias improving test loss by ignoring label noise: Medium confidence (mechanistically sound but relies on specific noise assumptions)

## Next Checks

1. Test the optimization threshold with non-linear ground truth models and asymmetric noise distributions to verify the robustness of the simplicity bias phenomenon
2. Compare gradient flow analysis with practical SGD training dynamics, particularly examining how discrete updates affect early alignment and extremal vector concentration
3. Extend the analysis to multi-layer networks to determine if the simplicity bias and optimization threshold generalize beyond two-layer architectures