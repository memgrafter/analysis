---
ver: rpa2
title: RLHF from Heterogeneous Feedback via Personalization and Preference Aggregation
arxiv_id: '2405.00254'
source_url: https://arxiv.org/abs/2405.00254
tags:
- reward
- aggregation
- human
- function
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper tackles the challenge of learning reward models from
  heterogeneous human feedback in reinforcement learning from human feedback (RLHF).
  The authors propose two frameworks: personalization-based and aggregation-based.'
---

# RLHF from Heterogeneous Feedback via Personalization and Preference Aggregation

## Quick Facts
- arXiv ID: 2405.00254
- Source URL: https://arxiv.org/abs/2405.00254
- Reference count: 40
- Primary result: Two frameworks (personalization and aggregation) for handling heterogeneous human feedback in RLHF, with theoretical guarantees and empirical validation on text summarization

## Executive Summary
This paper addresses the challenge of learning reward models from heterogeneous human feedback in reinforcement learning from human feedback (RLHF). The authors propose two complementary frameworks: personalization-based approaches that learn multiple reward models through representation learning and clustering, and aggregation-based approaches that combine diverse preferences using social choice theory and probabilistic opinion pooling. The methods are evaluated on text summarization tasks, showing improved performance compared to naive RLHF methods. The work provides principled approaches to handle heterogeneous human feedback, with theoretical guarantees on sample complexity and incentive compatibility.

## Method Summary
The paper proposes two main frameworks for handling heterogeneous human feedback in RLHF. The personalization-based framework uses representation learning to learn a shared feature space across users while estimating individual reward model parameters, and clustering to group users with similar preferences. The aggregation-based framework uses social choice theory to aggregate individual reward models into a representative output, or directly aggregates probabilistic opinions from users. For strategic users who may misreport preferences, the authors develop a mechanism design approach that ensures truthful reporting through properly designed costs. The methods are evaluated on text summarization tasks using the Reddit TL;DR dataset.

## Key Results
- Personalization with clustering efficiently learns personalized reward models with guaranteed sample complexity
- Reward aggregation using social choice theory yields representative outputs that satisfy important axioms
- Mechanism design approach ensures truthful preference reporting even when users are strategic
- Empirical evaluation shows improved performance compared to naive RLHF methods on text summarization tasks

## Why This Works (Mechanism)

### Mechanism 1: Personalization via Representation Learning
- Claim: Learning multiple reward models through a shared representation reduces bias from heterogeneous preferences while maintaining manageable variance
- Mechanism: The algorithm estimates a common representation function ψ_ω and individual parameter vectors θ_i. By pooling data across users to learn the representation, the model captures common structure while personalization reduces bias from preference heterogeneity
- Core assumption: Individual reward functions share a common representation structure, and user parameters are diverse
- Evidence anchors: "representation learning and clustering approaches to learn multiple reward models, balancing bias and variance"; "model each reward function as the inner product of a common representation and a parameter vector"

### Mechanism 2: Preference Aggregation via Probabilistic Opinion Pooling
- Claim: Direct aggregation of probabilistic opinions avoids the need to learn reward functions while handling heterogeneous preferences
- Mechanism: Users provide probability vectors over answers instead of binary choices. The aggregation function Agg-p_α combines these opinions into a consensus distribution
- Core assumption: Probabilistic opinions can express preferences more accurately than binary choices, and the aggregation function preserves unanimity
- Evidence anchors: "directly aggregates the human feedback in the form of probabilistic opinions"; "instead of choosing a single answer from a pool of candidate answers, we allow the human labeler to choose a probability distribution"

### Mechanism 3: Strategic Incentive Design via Mechanism Design
- Claim: Properly designed costs can incentivize truthful reporting of preferences even when users are strategic
- Mechanism: The aggregation rule and cost function form a VCG-like mechanism that maximizes social welfare while ensuring truthful reporting is a dominant strategy
- Core assumption: Users' utilities can be modeled as quasi-linear functions of distance to aggregated opinion plus cost
- Evidence anchors: "develops an approach to handle strategic human labelers who may bias and manipulate the aggregated preferences with untruthful feedback"; "based on the ideas in mechanism design, our approach ensures truthful preference reporting"

## Foundational Learning

- Concept: Representation Learning in Multi-Task Settings
  - Why needed here: Enables learning a shared structure across heterogeneous user preferences while allowing personalization
  - Quick check question: Why does pooling data across users help learn the common representation better than learning individual representations?

- Concept: Social Choice Theory and Aggregation Axioms
  - Why needed here: Provides principled foundation for aggregating diverse preferences into a single model
  - Quick check question: What are the six pivotal axioms that aggregation rules should satisfy according to the paper?

- Concept: Mechanism Design and Incentive Compatibility
  - Why needed here: Ensures users report preferences truthfully even when they might benefit from strategic misreporting
  - Quick check question: How does the VCG mechanism ensure that truthful reporting is a dominant strategy?

## Architecture Onboarding

- Component map: Data Collection -> Representation Learning -> Personalization Module -> Aggregation Module -> Mechanism Design -> Fine-tune LLM

- Critical path:
  1. Collect preference data from users
  2. Learn shared representation using pooled data
  3. Estimate individual reward models or probabilistic opinions
  4. Aggregate preferences using chosen method
  5. Apply mechanism design costs if handling strategic users
  6. Fine-tune LLM using aggregated model

- Design tradeoffs:
  - Personalization vs. variance: More personalized models reduce bias but increase variance
  - Aggregation method: Reward aggregation requires learning reward functions; preference aggregation avoids this but may be less interpretable
  - Mechanism complexity: Adding costs for incentive compatibility increases implementation complexity

- Failure signatures:
  - Poor representation learning: High variance in individual reward estimates
  - Ineffective aggregation: Aggregated model performs worse than individual models
  - Strategic behavior: Users consistently misreport preferences despite costs

- First 3 experiments:
  1. Test representation learning on synthetic heterogeneous preference data
  2. Compare reward aggregation vs. preference aggregation on real preference data
  3. Validate mechanism design by testing strategic user behavior in controlled environment

## Open Questions the Paper Calls Out
None

## Limitations
- Scalability to very large user populations remains unverified, with performance on industrial-scale applications unknown
- Clustering-based personalization assumes users naturally form distinct groups with similar preferences, which may not hold in all domains
- Mechanism design approach relies on accurate cost function computation, which may be challenging when user utilities are complex

## Confidence
- High Confidence in the core theoretical framework: representation learning, social choice theory, and mechanism design are well-established with solid theoretical foundations
- Medium Confidence in empirical results: improvements demonstrated on text summarization, but evaluation limited to single domain
- Medium Confidence in mechanism design guarantees: incentive compatibility claims are theoretically sound but real-world human behavior may deviate from models

## Next Checks
1. **Scalability Test**: Evaluate the representation learning approach on a dataset with 100+ users with diverse preferences to verify that the method maintains performance as user heterogeneity increases

2. **Cross-Domain Generalization**: Apply the personalization and aggregation methods to a non-text domain (e.g., robotics control or recommendation systems) to test whether the approach generalizes beyond the summarization task

3. **Strategic Behavior Validation**: Conduct a controlled experiment with human participants who are informed about the mechanism design incentives to empirically verify whether the cost function successfully prevents strategic misreporting