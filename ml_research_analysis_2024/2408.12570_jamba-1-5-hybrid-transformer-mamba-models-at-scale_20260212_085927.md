---
ver: rpa2
title: 'Jamba-1.5: Hybrid Transformer-Mamba Models at Scale'
arxiv_id: '2408.12570'
source_url: https://arxiv.org/abs/2408.12570
tags:
- jamba-1
- large
- jamba
- mini
- latency
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Jamba-1.5 introduces hybrid Transformer-Mamba mixture-of-experts
  models that achieve 256K effective context length while delivering 10x reduction
  in KV cache memory and superior throughput/latency compared to similarly sized models.
  The architecture combines attention and Mamba layers with a 16-expert MoE module,
  enabling efficient long-context processing.
---

# Jamba-1.5: Hybrid Transformer-Mamba Models at Scale

## Quick Facts
- arXiv ID: 2408.12570
- Source URL: https://arxiv.org/abs/2408.12570
- Reference count: 40
- Models achieve 256K effective context length with 10x reduction in KV cache memory

## Executive Summary
Jamba-1.5 introduces hybrid Transformer-Mamba mixture-of-experts models that achieve 256K effective context length while delivering 10x reduction in KV cache memory and superior throughput/latency compared to similarly sized models. The architecture combines attention and Mamba layers with a 16-expert MoE module, enabling efficient long-context processing. A novel ExpertsInt8 quantization technique allows Jamba-1.5-Large (94B active parameters) to run on 8 A100/H100 GPUs for 256K-token contexts without quality loss. On RULER benchmark, Jamba-1.5 models are the only open-weight models with confirmed 256K effective length, achieving 95.7% average accuracy. The models match or exceed performance of LLaMA-3.1 and Mistral-Large-2 on academic and chatbot benchmarks while providing 2-3x better latency and throughput.

## Method Summary
Jamba-1.5 uses a hybrid Transformer-Mamba architecture with 1:7 attention-to-Mamba ratio, 16-expert MoE selecting top-2 experts, and 8192 hidden dimensionality. The model is trained in three stages: pre-training on a proprietary multilingual corpus, mid-training with long-document emphasis, and post-training with supervised fine-tuning on synthetic data including table-based QA, document QA, tool use, and steerability. ExpertsInt8 quantization is applied to MoE and MLP weights to enable 8 A100/H100 GPU deployment for 256K-token contexts. The architecture achieves linear-time sequence processing through Mamba layers while retaining attention's expressive power, with a 10x reduction in KV cache memory compared to pure Transformer models.

## Key Results
- Jamba-1.5-Large achieves 95.7% average accuracy on RULER benchmark with 256K effective context length
- Models provide 2-3x better latency and throughput compared to LLaMA-3.1 and Mistral-Large-2
- ExpertsInt8 quantization enables 94B active parameter model to run on 8 A100/H100 GPUs for 256K contexts
- Jamba-1.5 models match or exceed performance of LLaMA-3.1-8B and Mistral-Large-2 on academic and chatbot benchmarks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The hybrid Transformer-Mamba architecture provides a 10x reduction in KV cache memory while maintaining or improving quality.
- Mechanism: Mamba layers process sequences in linear time without storing key-value pairs, unlike self-attention which requires O(n²) memory. By interleaving Mamba and attention layers, the model retains the expressive power of attention while reducing memory footprint.
- Core assumption: The Mamba state transitions can effectively capture long-range dependencies when interleaved with attention layers, avoiding the need for full attention at every layer.
- Evidence anchors: [abstract] "providing high throughput and low memory usage across context lengths", [section] "10x reduction in KV cache memory as well as superior throughput and latency", [corpus] Weak evidence - only 1 related paper mentions memory efficiency in hybrid architectures
- Break condition: If Mamba layers cannot effectively propagate information across long sequences, the model quality would degrade despite memory savings.

### Mechanism 2
- Claim: ExpertsInt8 quantization enables Jamba-1.5-Large to fit on 8 A100/H100 GPUs for 256K-token contexts without quality loss.
- Mechanism: Quantizing MoE and MLP weights to INT8 reduces memory footprint by 50%, while dequantization happens inside the fused_moe kernel for negligible overhead. This allows large models to run on limited GPU memory.
- Core assumption: INT8 quantization of MoE weights preserves model quality while the dequantization step doesn't add significant computational overhead.
- Evidence anchors: [section] "allows fitting Jamba-1.5-Large on a machine with 8 80GB GPUs when processing 256K-token contexts without loss of quality", [section] "Our ExpertsInt8 method has several advantages. First, it is fast; quantization only takes a few seconds at model loading", [corpus] Weak evidence - no related papers discuss this specific quantization technique
- Break condition: If INT8 quantization introduces significant numerical errors or the dequantization overhead becomes substantial, quality would degrade or latency would increase.

### Mechanism 3
- Claim: The 1:7 attention-to-Mamba ratio found optimal in Jamba provides the best quality-throughput tradeoff.
- Mechanism: More Mamba layers increase throughput and reduce memory usage, while maintaining sufficient attention layers for quality. The 1:7 ratio balances these competing objectives.
- Core assumption: The optimal ratio is not highly sensitive to model scale and applies to 94B active parameter models as well as smaller ones.
- Evidence anchors: [section] "a : m = 1 : 7 ratio of attention-to-Mamba layers. This ratio was found optimal in our work on Jamba [24] and similar ratios was also confirmed as successful in follow-up work [6, 37]", [section] "While Mamba-2 outperforms Mamba-1 without attention, the hybrid Mamba-1-Attention performs better", [corpus] Moderate evidence - 2 related papers confirm similar attention-to-Mamba ratios in hybrid architectures
- Break condition: If the optimal ratio changes significantly with scale or if Mamba-2 with attention provides better tradeoffs at large scale, this mechanism would break.

## Foundational Learning

- Concept: Mixture-of-Experts (MoE) routing
  - Why needed here: MoE allows scaling to 398B total parameters while keeping 94B active, providing capacity without proportional computational cost
  - Quick check question: What determines which experts are activated for a given token in MoE?

- Concept: State-Space Models (SSM) and Mamba
  - Why needed here: Mamba provides linear-time sequence processing without KV cache, enabling long-context processing with reduced memory
  - Quick check question: How does Mamba's selective state update differ from traditional RNNs?

- Concept: Quantization techniques (INT8 vs BF16/FP8)
  - Why needed here: ExpertsInt8 quantization reduces memory footprint while maintaining quality, enabling 256K context on limited GPUs
  - Quick check question: What are the tradeoffs between quantization speed, quality preservation, and hardware compatibility?

## Architecture Onboarding

- Component map: Token generation requires attention → Mamba → MoE → attention sequence through 9 blocks × 8 layers/block = 72 total layers
- Critical path: Token generation follows attention → Mamba → MoE pattern through 9 blocks
- Design tradeoffs:
  - More Mamba layers → better throughput but potential quality degradation
  - More attention layers → better quality but worse memory usage
  - Expert count → capacity vs routing overhead
  - Quantization → memory savings vs potential quality loss
- Failure signatures:
  - Quality degradation → likely attention/Mamba ratio or MoE routing issues
  - Memory errors → quantization problems or incorrect expert parallelism
  - Slow inference → attention bottleneck or inefficient MoE routing
- First 3 experiments:
  1. Validate Mamba-1 vs Mamba-2 performance in hybrid architecture (as shown in Figure 1)
  2. Test INT8 quantization on a small MoE layer to verify quality preservation
  3. Measure KV cache memory usage with different attention-to-Mamba ratios to find optimal balance

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the hybrid Transformer-Mamba architecture maintain its efficiency advantages when scaled beyond 398B total parameters?
- Basis in paper: [explicit] The paper demonstrates Jamba-1.5-Large with 398B total parameters but does not explore scaling beyond this point.
- Why unresolved: The paper focuses on a specific scale but does not investigate whether the architectural benefits persist at larger scales.
- What evidence would resolve it: Comparative experiments with models exceeding 400B parameters using the same hybrid architecture, measuring throughput, latency, and memory usage.

### Open Question 2
- Question: How does the ExpertsInt8 quantization technique perform on other MoE architectures beyond Jamba?
- Basis in paper: [explicit] The paper introduces ExpertsInt8 for Jamba models but does not test it on other MoE architectures.
- Why unresolved: The technique's generalizability to different MoE structures and model types is untested.
- What evidence would resolve it: Application of ExpertsInt8 to other MoE models (e.g., Mixtral, LLaMA-3.1) with comparative latency and quality metrics.

### Open Question 3
- Question: What is the impact of the Activation Loss term on model performance across different training stages?
- Basis in paper: [explicit] The paper introduces Activation Loss to prevent numerical issues but only applies it during training, not evaluating its impact on final model quality.
- Why unresolved: The paper mentions using α=10^-5 without analyzing how different α values or timing of application affect performance.
- What evidence would resolve it: Systematic ablation studies varying α values and timing of Activation Loss application, comparing downstream task performance.

## Limitations

- The core evaluation relies on proprietary datasets and synthetic data generation pipelines that are not publicly available, making independent verification impossible.
- The claim of being the "only open-weight models with confirmed 256K effective length" on RULER is difficult to verify given that other models with 1M context windows could potentially achieve similar results.
- The ExpertsInt8 quantization technique is presented without comparison to other quantization methods like FP8 or traditional INT8 approaches, leaving open questions about whether claimed advantages are specific to this implementation.

## Confidence

**High confidence**: The hybrid Transformer-Mamba architecture design and implementation details are well-specified and verifiable. The basic claims about interleaving attention and Mamba layers with MoE routing follow established patterns in the literature and can be reproduced given the architectural specifications.

**Medium confidence**: The quantitative claims about memory reduction (10x KV cache), throughput improvements (2-3x better latency), and quality preservation through INT8 quantization are based on internal measurements. While the methodology appears sound, the lack of public baselines and datasets makes independent verification challenging.

**Low confidence**: The claim of being the "only open-weight models with confirmed 256K effective length" on RULER is difficult to verify given the closed nature of the evaluation and the potential for other models to achieve similar results on this benchmark. The comparison to proprietary models (LLaMA-3.1, Mistral-Large-2) without access to their implementations or evaluation code also limits confidence in these specific performance claims.

## Next Checks

1. **Reproduce the memory reduction claims**: Implement the hybrid architecture with varying attention-to-Mamba ratios (1:3, 1:7, 1:10) and measure actual KV cache memory usage during inference on standard benchmarks, comparing against pure Transformer baselines with equivalent parameter counts.

2. **Validate quantization quality preservation**: Test the ExpertsInt8 quantization technique on smaller MoE models (1-10B parameters) across different tasks, comparing INT8, BF16, and FP8 quantization approaches to determine if the claimed advantages are specific to this implementation or generalize to other quantization methods.

3. **Independent RULER benchmark evaluation**: Apply the Jamba-1.5 architecture to a publicly available model (e.g., LLaMA-3.1-8B) and evaluate on RULER benchmark tasks to verify if the 256K effective context claims hold when using different pre-trained weights, or attempt to recreate the synthetic training data generation pipeline for needle-in-haystack tasks to validate the training methodology.