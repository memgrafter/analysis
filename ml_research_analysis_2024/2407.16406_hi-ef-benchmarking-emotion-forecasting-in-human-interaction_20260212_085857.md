---
ver: rpa2
title: 'Hi-EF: Benchmarking Emotion Forecasting in Human-interaction'
arxiv_id: '2407.16406'
source_url: https://arxiv.org/abs/2407.16406
tags:
- emotion
- task
- mcis
- information
- emotional
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper introduces Emotion Forecasting (EF), a novel task in\
  \ affective computing that predicts future emotions during two-party interactions.\
  \ Unlike traditional emotion recognition, EF leverages contextual information and\
  \ one party\u2019s current emotional state to forecast the other party\u2019s future\
  \ emotions."
---

# Hi-EF: Benchmarking Emotion Forecasting in Human-interaction

## Quick Facts
- arXiv ID: 2407.16406
- Source URL: https://arxiv.org/abs/2407.16406
- Authors: Haoran Wang; Xinji Mai; Zeng Tao; Junxiong Lin; Xuan Tong; Ivy Pan; Shaoqi Yan; Yan Wang; Shuyong Gao
- Reference count: 14
- Primary result: Emotion Forecasting (EF) task introduced with baseline model achieving 35.19% WAR and 23.72% UAR on Hi-EF dataset

## Executive Summary
This paper introduces Emotion Forecasting (EF), a novel task in affective computing that predicts future emotions during two-party interactions. Unlike traditional emotion recognition, EF leverages contextual information and one party's current emotional state to forecast the other party's future emotions. To support this task, the authors construct the Human-interaction-based Emotion Forecasting (Hi-EF) dataset, containing 3,069 Multilayered-Contextual Interaction Samples (MCIS) with rich multi-modal and affective annotations. A baseline model using vision, audio, and text encoders with fusion strategies is proposed and evaluated on the dataset, demonstrating the feasibility of EF and the effectiveness of the Hi-EF dataset.

## Method Summary
The Hi-EF dataset contains 3,069 MCIS with 5,242 video clips, each MCIS consisting of four video clips: two contextual clips (I & II), one current emotion state clip (III), and one target future emotion clip (IV). The baseline model employs vision encoders (ResNet18, I3D, ViT), audio encoder (AudioCLIP), and text encoder (CLIP text encoder) with intra-video fusion and inter-video fusion blocks. The model uses LSTM+Transformer for inter-video fusion and is trained using PyTorch on RTX 3090 GPUs with Adam optimizer, batch size 8, learning rates 1e-2 to 1e-5, weight decay 1e-6, and 50 epochs.

## Key Results
- Baseline model achieves 35.19% Weighted Average Recall (WAR) and 23.72% Unweighted Average Recall (UAR)
- Video modality contributes most to emotion analysis; audio improves text interpretation while text alone degrades performance
- LSTM+Transformer inter-video fusion strategy performs best for temporal modeling

## Why This Works (Mechanism)

### Mechanism 1
- Claim: EF leverages bidirectional emotional influence in dyadic interactions to forecast future emotions more accurately than ER.
- Mechanism: The model assumes that one party's current emotional state provides predictive cues for the other party's near-future emotional trajectory, due to established emotional contagion and interaction dynamics.
- Core assumption: Emotional influence in two-party interactions is sufficiently predictable and temporally aligned to support short-term forecasting.
- Evidence anchors: [abstract] "This task is set within the context of a two-party interaction, positing that an individual's emotions are significantly influenced by their interaction partner's emotional expressions and informational cues." [section] "First, emotional exchange is bidirectional: one person's emotions can affect the other's, allowing for more accurate predictions of emotional changes (Hatfield, Cacioppo, and Rapson 1993)."
- Break condition: If emotional influence is too delayed, noisy, or context-dependent, forecasting accuracy degrades.

### Mechanism 2
- Claim: Multi-modal input fusion improves EF accuracy by capturing complementary emotional cues across modalities.
- Mechanism: The model fuses video, audio, and text modalities to construct a richer emotional representation than any single modality can provide.
- Core assumption: Different modalities capture orthogonal aspects of emotional state and interaction context, and their combination yields superior predictive power.
- Evidence anchors: [abstract] "Hi-EF boasts 3,069 Multilayered-Contextual Interaction Samples (MCIS), featuring in 5,242 video clips with abundant affect-related labels." [section] "The results show that the video modality contributes most to emotion analysis. Adding text alone degrades performance, as the text is treated as noise, whereas adding audio enables the model to better interpret the textual information."
- Break condition: If modalities are redundant or conflicting, fusion may introduce noise rather than improve accuracy.

### Mechanism 3
- Claim: Temporal structure in MCIS (three preceding clips + target clip) enables modeling of emotional evolution within interaction sequences.
- Mechanism: The model uses short-term sequential context to learn temporal patterns of emotional change, with LSTM+Transformer inter-video fusion capturing both temporal dependencies and key transition points.
- Core assumption: Emotional states follow predictable temporal patterns within short interaction windows, and these patterns can be learned from labeled sequences.
- Evidence anchors: [section] "The dynamic context of interactions provides rich background information, such as conversation content, tone, and emotional shifts, enhancing the precision of affective forecasting (Barsade 2002)." [section] "LSTM+Transformer performs best, where LSTM captures temporal dependencies among the three clips and the transformer highlights key signals that drive emotional changes."
- Break condition: If emotional transitions are too abrupt or non-linear, the temporal model may fail to capture relevant patterns.

## Foundational Learning

- Concept: Multi-modal data fusion and alignment
  - Why needed here: EF requires integrating video, audio, and text streams that are temporally aligned but represent different aspects of the interaction.
  - Quick check question: How would you align a 3-second video clip with its corresponding audio and transcribed text for model input?

- Concept: Temporal sequence modeling for affective forecasting
  - Why needed here: EF predicts future emotional states based on current and past states, requiring models that can capture temporal dependencies in emotional trajectories.
  - Quick check question: What's the difference between using a simple average of past states versus an LSTM for predicting future emotional states?

- Concept: Emotion recognition and classification across multiple categories
  - Why needed here: The baseline model must first recognize the current emotional state (ER) before forecasting future states, requiring robust emotion classification capabilities.
  - Quick check question: How would you evaluate whether your emotion recognition component is performing well enough to support accurate forecasting?

## Architecture Onboarding

- Component map:
  Input clips → Vision/Audio/Text encoding → Intra-video fusion → Inter-video fusion → Classification

- Critical path:
  Input clips → Vision/Audio/Text encoding → Intra-video fusion → Inter-video fusion → Classification

- Design tradeoffs:
  - Simple averaging vs. transformer-based fusion for modality combination
  - Frame-level vs. clip-level temporal modeling
  - Separate encoders vs. shared encoder architectures

- Failure signatures:
  - Low accuracy on validation set with high variance across emotion classes
  - Mode collapse where model predicts only dominant emotions
  - Temporal misalignment errors in sequential predictions

- First 3 experiments:
  1. Train with video-only input using ViT backbone to establish baseline performance
  2. Add audio modality to evaluate impact of multi-modal fusion
  3. Implement LSTM+Transformer inter-video fusion to assess temporal modeling benefits

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of the Emotion Forecasting (EF) task scale with larger datasets or longer-term contextual information beyond the current MCIS format?
- Basis in paper: [explicit] The paper mentions plans to expand the Hi-EF dataset in the future and acknowledges that the current model uses simple and commonly adopted fusion strategies without much innovation.
- Why unresolved: The paper uses a relatively small dataset (3,069 MCIS) and does not explore the impact of increasing dataset size or extending contextual information beyond two preceding clips. The baseline model architecture and fusion strategies are also basic, leaving room for improvement with more advanced methods.
- What evidence would resolve it: Conducting experiments with larger datasets and longer contextual sequences, along with testing more sophisticated model architectures and fusion strategies, would provide insights into the scalability and robustness of the EF task.

### Open Question 2
- Question: What is the impact of incorporating external factors (e.g., personality traits, cultural background) on the accuracy of Emotion Forecasting (EF)?
- Basis in paper: [inferred] The paper discusses the limitations of traditional Affective Forecasting, which often relies on external factors leading to inaccuracies. While the EF task narrows the scope to two-party interactions, it does not explicitly address how external factors like personality or culture might influence emotional predictions.
- Why unresolved: The current Hi-EF dataset focuses on interaction dynamics but does not include annotations or data related to personality traits, cultural background, or other external factors that could influence emotional forecasting.
- What evidence would resolve it: Collecting data that includes personality and cultural annotations, along with experiments testing the integration of these factors into the EF model, would clarify their impact on forecasting accuracy.

### Open Question 3
- Question: How effective is the Emotion Forecasting (EF) task in real-world applications compared to controlled experimental settings?
- Basis in paper: [explicit] The paper highlights the potential applications of EF in areas like individual emotion modeling and anthropomorphic emotion generation but does not provide empirical evidence of its effectiveness in real-world scenarios.
- Why unresolved: The Hi-EF dataset is derived from TV shows, which are somewhat controlled environments. The paper does not address how well the EF task generalizes to more dynamic and unpredictable real-world interactions.
- What evidence would resolve it: Testing the EF model on real-world datasets or deploying it in practical applications (e.g., therapy, human-robot interaction) and measuring its performance and adaptability would demonstrate its real-world effectiveness.

## Limitations

- Limited dataset size (3,069 MCIS) may restrict model generalization and scalability
- Lack of comparison with established emotion recognition benchmarks to validate EF improvements
- No empirical evidence of EF effectiveness in real-world applications beyond controlled TV show data

## Confidence

- Core claim feasibility: High (demonstrated WAR 35.19% and UAR 23.72%)
- Generalizability of results: Medium (limited dataset size, no benchmark comparison)
- Multi-modal fusion effectiveness: High (ablation studies show video dominance and audio's specific role)

## Next Checks

1. **Benchmark Comparison**: Evaluate the EF model on established emotion recognition datasets (like IEMOCAP or MELD) to assess whether EF actually improves over standard ER approaches when applied to the same data.

2. **Temporal Robustness Testing**: Systematically vary the temporal gap between context clips and target clips (e.g., 1, 3, 5, 10 time steps) to determine the model's effective forecasting horizon and identify breakdown points.

3. **Cross-Dataset Validation**: Test the trained model on a held-out subset of Hi-EF data from different sources or demographics to assess generalization beyond the training distribution and identify potential bias in the dataset construction.