---
ver: rpa2
title: 'Beyond Single-Audio: Advancing Multi-Audio Processing in Audio Large Language
  Models'
arxiv_id: '2409.18680'
source_url: https://arxiv.org/abs/2409.18680
tags:
- speech
- audio
- sound
- allms
- tasks
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the limitation of existing audio large language
  models (ALLMs), which are primarily evaluated on single-audio tasks but struggle
  with real-world multi-audio scenarios. To bridge this gap, the authors propose the
  first multi-audio evaluation (MAE) benchmark, comprising 20 datasets from 11 tasks
  across speech and sound domains.
---

# Beyond Single-Audio: Advancing Multi-Audio Processing in Audio Large Language Models

## Quick Facts
- arXiv ID: 2409.18680
- Source URL: https://arxiv.org/abs/2409.18680
- Authors: Yiming Chen; Xianghu Yue; Xiaoxue Gao; Chen Zhang; Luis Fernando D'Haro; Robby T. Tan; Haizhou Li
- Reference count: 14
- One-line primary result: MALLM achieves 73.8% accuracy on speech tasks and 74.3% on sound tasks while maintaining single-audio performance

## Executive Summary
This paper addresses the critical limitation of existing audio large language models (ALLMs), which are primarily evaluated on single-audio tasks despite the prevalence of multi-audio scenarios in real-world applications. The authors propose the first Multi-Audio Evaluation (MAE) benchmark comprising 20 datasets across 11 tasks in speech and sound domains. They introduce MALLM, a novel model trained via discriminative learning on synthetic data that significantly outperforms existing ALLMs on multi-audio tasks while maintaining competitive single-audio performance. The approach demonstrates high data efficiency without requiring human annotations, advancing the field toward effective multi-audio processing capabilities.

## Method Summary
The authors propose MALLM, a multi-audio LLM trained via discriminative learning on synthetic data to capture audio context among multiple similar audios. The model uses a scalable audio pairs synthesis strategy that automatically generates synthetic training data without human labeling by mixing single-audio recordings with distinct sound events and using text-based LLMs to generate ground-truth descriptions of differences. MALLM is fine-tuned using LoRA adapters on Qwen-Audio with joint training that includes both synthetic multi-audio pairs and single-audio samples to prevent catastrophic forgetting. The model is evaluated on the newly proposed MAE benchmark, which consists of 20 datasets from 11 multi-audio tasks across speech and sound domains.

## Key Results
- MALLM achieves 73.8% average accuracy on speech tasks and 74.3% on sound tasks in the MAE benchmark
- MALLM outperforms all 15 evaluated ALLMs on multi-audio processing while maintaining competitive single-audio performance
- The discriminative learning approach on synthetic data demonstrates high data efficiency without requiring human annotations

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Synthetic data generation enables scalable multi-audio discriminative learning without human annotation
- Mechanism: The model synthesizes audio pairs by mixing single-audio recordings with distinct sound events and uses text-based LLM to generate ground-truth descriptions of differences for speech pairs
- Core assumption: The text-based LLM can generate semantically correct descriptions of audio differences that capture subtle distinctions needed for discriminative learning
- Evidence anchors: [abstract], [section 4], [corpus]

### Mechanism 2
- Claim: Discriminative training on audio pairs improves the model's ability to capture intra-audio and inter-audio relationships
- Mechanism: By fine-tuning the ALLM to describe subtle distinctions between two similar audio samples, the model learns to reason about both individual audio content and relationships between multiple audio inputs
- Core assumption: Learning to discriminate between similar audio pairs transfers to improved performance on diverse multi-audio tasks that require understanding relationships
- Evidence anchors: [abstract], [section 4], [corpus]

### Mechanism 3
- Claim: Joint training with single-audio samples prevents catastrophic forgetting while learning multi-audio processing
- Mechanism: During fine-tuning, the model samples single audios from various tasks alongside the multi-audio discriminative pairs, maintaining its single-audio capabilities while acquiring multi-audio reasoning
- Core assumption: The model can simultaneously maintain and improve both single-audio and multi-audio processing abilities through balanced joint training
- Evidence anchors: [section 4], [section 5.4], [corpus]

## Foundational Learning

- Concept: Discriminative learning in neural networks
  - Why needed here: The paper leverages discriminative training (rather than generative) to teach the model to distinguish between similar audio pairs, which is more efficient for capturing subtle differences
  - Quick check question: What is the key difference between discriminative and generative training approaches in machine learning?

- Concept: Synthetic data generation and its limitations
  - Why needed here: The entire training approach depends on automatically generated synthetic audio pairs and text descriptions, making understanding synthetic data quality crucial
  - Quick check question: What are the potential risks of using synthetic data for training models that will be deployed on real-world data?

- Concept: Catastrophic forgetting in continual learning
  - Why needed here: The model must learn new multi-audio capabilities without losing existing single-audio performance, making forgetting prevention essential
  - Quick check question: How does joint training with both new and old tasks help prevent catastrophic forgetting in neural networks?

## Architecture Onboarding

- Component map: Audio encoder (Whisper-large-v2) -> LLM backbone (Qwen-7B) -> LoRA adapters -> Text-based LLM (GPT-4)
- Critical path: Synthetic data generation → Discriminative fine-tuning → Evaluation on MAE benchmark
- Design tradeoffs: Using synthetic data enables scalability but may introduce domain shift; discriminative training is more efficient than generative but may miss broader context; joint training maintains single-audio performance but requires careful balancing
- Failure signatures: Model consistently fails on multi-audio tasks while maintaining single-audio performance (indicates discriminative learning not generalizing); model performance degrades on single-audio tasks (indicates catastrophic forgetting); evaluation results show poor correlation between synthetic training data and real tasks (indicates synthetic data quality issues)
- First 3 experiments: 1) Generate synthetic audio pairs and verify the quality of text descriptions through human evaluation; 2) Fine-tune a small ALLM on synthetic discriminative pairs and test on a simple multi-audio task; 3) Jointly train with both synthetic pairs and single-audio samples, measuring performance on both to verify forgetting prevention

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the proposed MALLM model perform on multilingual speech tasks, and what are the challenges in extending the MAE benchmark to non-English languages?
- Basis in paper: [inferred] The paper mentions that the MAE benchmark currently covers English data and suggests extending it to multilingual scenarios as an important future direction to ensure its comprehensiveness and applicability across diverse linguistic contexts
- Why unresolved: The paper does not provide any experimental results or analysis on multilingual speech tasks
- What evidence would resolve it: Experiments evaluating MALLM on multilingual speech tasks using diverse datasets from multiple languages

### Open Question 2
- Question: How does the performance of MALLM on multi-audio tasks compare to single-audio tasks, and what are the trade-offs between the two?
- Basis in paper: [explicit] The paper discusses the performance of MALLM on both multi-audio and single-audio tasks
- Why unresolved: The paper does not provide a detailed comparison of MALLM's performance on multi-audio and single-audio tasks
- What evidence would resolve it: A comprehensive evaluation of MALLM on tasks involving varying numbers of audio inputs

### Open Question 3
- Question: How does the synthetic data generation approach used in MALLM impact its ability to generalize to real-world multi-audio scenarios, and what are the potential biases introduced by this approach?
- Basis in paper: [explicit] The paper proposes a synthetic data generation approach to train MALLM on multi-audio tasks
- Why unresolved: The paper does not discuss the potential limitations or biases introduced by the synthetic data generation approach
- What evidence would resolve it: Experiments evaluating MALLM on real-world multi-audio datasets and comparing its performance to models trained on human-labeled data

## Limitations
- The evaluation excludes audio+vision models and text-only LLMs, limiting the scope of comparison
- The MAE benchmark only covers 11 task types and may not fully represent all real-world multi-audio scenarios
- The synthetic data generation process introduces potential domain shift that could affect generalization
- The evaluation methodology relies heavily on automated LLM evaluation with limited human annotation validation

## Confidence
- High confidence: The general approach of using discriminative training on synthetic data is technically sound and well-established in machine learning literature
- Medium confidence: The specific implementation details and hyperparameters are clearly specified, but the quality of synthetic data generation and its impact on real-world performance remains uncertain
- Low confidence: The generalization of MALLM's performance from synthetic training data to diverse real-world multi-audio tasks has not been fully validated

## Next Checks
1. Conduct human evaluation studies on a broader range of multi-audio scenarios not included in the MAE benchmark to validate generalization
2. Compare MALLM's performance against a baseline trained on real multi-audio data (when available) to quantify the impact of synthetic data domain shift
3. Perform ablation studies to determine the optimal balance between single-audio and multi-audio training samples in the joint training approach