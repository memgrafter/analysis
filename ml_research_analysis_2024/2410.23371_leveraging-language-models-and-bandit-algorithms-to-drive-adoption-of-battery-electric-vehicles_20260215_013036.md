---
ver: rpa2
title: Leveraging Language Models and Bandit Algorithms to Drive Adoption of Battery-Electric
  Vehicles
arxiv_id: '2410.23371'
source_url: https://arxiv.org/abs/2410.23371
tags:
- preference
- intervention
- survey
- llms
- bevs
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a novel approach to leverage Large Language
  Models (LLMs) and bandit algorithms to drive adoption of Battery-Electric Vehicles
  (BEVs) through personalized conversational interventions. The core method involves
  training a contextual bandit to select BEV-relevant values based on participant
  demographics, which are then used to prompt an LLM to generate targeted conversational
  interventions.
---

# Leveraging Language Models and Bandit Algorithms to Drive Adoption of Battery-Electric Vehicles

## Quick Facts
- arXiv ID: 2410.23371
- Source URL: https://arxiv.org/abs/2410.23371
- Reference count: 5
- Primary result: PeaR-RL improves preference shifts for BEVs compared to using GPT-4 alone

## Executive Summary
This paper presents a novel approach to leverage Large Language Models (LLMs) and bandit algorithms to drive adoption of Battery-Electric Vehicles (BEVs) through personalized conversational interventions. The core method involves training a contextual bandit to select BEV-relevant values based on participant demographics, which are then used to prompt an LLM to generate targeted conversational interventions. To overcome the challenges of data collection, the authors use LLMs as virtual survey participants to train their bandit algorithm in an offline manner. The primary results show that the proposed intervention system, PeaR-RL, improves preference shifts for BEVs compared to using GPT-4 alone. The authors also replicate a human survey on BEV attitudes using LLMs and find moderate correlation in intervention effectiveness, though LLMs tend to exhibit higher baseline preferences for BEVs than humans.

## Method Summary
The paper combines contextual bandit algorithms with LLMs to create personalized interventions for BEV adoption. The system samples demographic attributes from US Census data, uses LLMs to simulate virtual participants, and employs a contextual bandit (UCB algorithm) to select from eight BEV-relevant values based on demographics. The selected value is used to prompt an LLM to generate conversational interventions. The bandit is trained offline using virtual participants before being evaluated on real participants. The authors compare three conditions: pure LLM interventions, random value selection, and UCB-guided value selection, measuring preference shifts on a 0-100 scale.

## Key Results
- PeaR-RL improves preference shifts for BEVs compared to using GPT-4 alone
- Moderate correlation between LLM and human intervention effectiveness, though LLMs exhibit higher baseline BEV preferences
- Bandit algorithm successfully learns to target values based on demographics, showing improvement over random selection
- No significant difference between random and UCB policies in the second half of learning, suggesting room for bandit algorithm improvement

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Contextual bandit with demographic context learns to match high-level BEV-relevant values to user subgroups
- Mechanism: Bandit observes demographic context (age, gender) and selects one of eight predefined BEV values (e.g., "carbon emission reduction", "economic benefits"). Selected value is used to prompt LLM for personalized intervention. Bandit updates via UCB rule balancing exploration/exploitation.
- Core assumption: The mapping from demographics to effective persuasive values is stable enough for bandit learning in offline virtual participant data
- Break condition: Demographics insufficiently predictive, bandit cannot differentiate subgroups, or values too coarse for nuanced persuasion

### Mechanism 2
- Claim: LLM serves as virtual participant to generate synthetic survey data for bandit training
- Mechanism: LLM is prompted with demographic attributes to simulate human responses. Bandit trains offline on LLM-generated preferences before deployment. LLM-generated interventions are evaluated for effectiveness on both virtual and real participants.
- Core assumption: LLM responses to demographic prompts correlate sufficiently with real human preferences to enable useful bandit training
- Break condition: LLM bias dominates (e.g., always high BEV preference), synthetic data poorly correlates with real human responses, or bandit overfits to LLM patterns

### Mechanism 3
- Claim: Combining bandit-selected values with LLM generates more effective interventions than LLM alone
- Mechanism: Bandit selects value based on demographics → LLM crafts intervention using that value → intervention shifts participant preference. This is compared to LLM generating interventions without value guidance.
- Core assumption: Value-targeted interventions are more persuasive than generic ones, even when generated by the same LLM
- Break condition: LLM already generates optimal interventions regardless of guidance, or value selection provides no additional information to LLM

## Foundational Learning

- Contextual multi-armed bandit learning
  - Why needed here: To learn which BEV-relevant values work best for different demographic subgroups without requiring real human trials for every combination
  - Quick check question: What exploration strategy does the bandit use to balance trying new value-demographic combinations vs exploiting known effective ones?

- Large language model prompting for role-play
  - Why needed here: To generate synthetic survey participants with realistic demographic distributions for bandit training
  - Quick check question: How does the system prompt the LLM to adopt the perspective of a specific demographic group?

- Preference measurement via self-reported scalar rating
  - Why needed here: To quantify intervention effectiveness as a change in BEV preference on a 0-100 scale
- Quick check question: How is the preference shift calculated from pre- and post-intervention ratings?

## Architecture Onboarding

- Component map: Demographic sampler → Virtual participant LLM → Preference measurement → Bandit value selector → Intervention LLM → Post-preference measurement → Reward calculation → Bandit update
- Critical path: Virtual participant creation → Bandit selection → LLM intervention generation → Preference measurement → Reward calculation
- Design tradeoffs: Virtual participants enable scalable bandit training but introduce LLM bias; bandit adds interpretability but may limit LLM creativity; offline training avoids costly human trials but may not generalize
- Failure signatures: Bandit converges to suboptimal policy (random policy performs equally well), LLM consistently generates ineffective interventions, or virtual participants poorly represent real humans (low correlation in preference shifts)
- First 3 experiments:
  1. Run bandit with random policy to establish baseline performance
  2. Run bandit with UCB policy and virtual participants, measure preference shifts
  3. Compare intervention effectiveness distributions between virtual and real participants using same fixed interventions

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the LLM-generated virtual participants truly replicate the diverse attitudes and behaviors of real human participants in large-scale surveys?
- Basis in paper: [explicit] The authors note that LLMs are "biased to prefer environmentally friendly BEVs more strongly than the typical human" and that "there is a limit to simulating a real human only with temperature parameters."
- Why unresolved: The comparison between LLM-generated virtual participants and real humans showed moderate correlation in intervention effectiveness, but LLMs exhibited higher baseline preferences for BEVs. This suggests that LLMs may not fully capture the nuanced attitudes of diverse human populations.
- What evidence would resolve it: Conducting large-scale surveys with both real humans and LLM-generated virtual participants across various demographics and topics, then comparing the results in terms of attitude distributions, preference shifts, and demographic influences.

### Open Question 2
- Question: How can the contextual bandit algorithm be improved to select more effective intervention values for different demographic groups?
- Basis in paper: [explicit] The authors note that "there was no significant difference in the second half of learning" between the random policy and UCB policy, suggesting room for improvement in the bandit algorithm's performance.
- Why unresolved: The bandit algorithm may not have sufficient information to select optimal values, as only age and gender were observed among demographics. The authors suggest that extending the context to include more demographic information or using multi-turn reinforcement could be potential solutions.
- What evidence would resolve it: Testing the bandit algorithm with extended context information (e.g., income, education, car ownership) and comparing its performance to the current implementation. Additionally, exploring multi-turn reinforcement learning approaches and evaluating their effectiveness in targeting intervention values.

### Open Question 3
- Question: How can the conversational intervention system be adapted to address other impactful behavioral change applications beyond BEV adoption?
- Basis in paper: [explicit] The authors state that "the contextual bandit has the benefit that it is an interpretable component that may be added to any existing LLM and can easily be adapted by domain experts to other applications."
- Why unresolved: While the system shows promise in shifting preferences for BEVs, its effectiveness in other domains remains untested. The authors suggest potential applications in climate action and resource conservation but do not explore these in detail.
- What evidence would resolve it: Implementing the conversational intervention system in other domains, such as promoting energy conservation, reducing plastic waste, or encouraging healthy eating habits. Comparing the system's performance across different applications and evaluating its adaptability to various behavioral change goals.

## Limitations

- Virtual participants may not fully capture the diverse attitudes and behaviors of real human participants, particularly regarding BEV preferences
- The contextual bandit algorithm shows no significant improvement over random selection in later stages of learning, suggesting limited effectiveness
- Only age and gender were used as demographic context, potentially limiting the bandit's ability to identify effective value-targeting strategies

## Confidence

- High confidence: The technical implementation of the contextual bandit algorithm and LLM integration is clearly specified and reproducible
- Medium confidence: The effectiveness of the bandit-LLM combination compared to LLM alone, as this relies on controlled comparisons with clear metrics
- Low confidence: The generalizability of virtual participants to real human behavior, as this requires external validation beyond the reported correlations

## Next Checks

1. **Correlation validation**: Test whether the LLM-human correlation in intervention effectiveness holds across different BEV value sets and demographic distributions
2. **Generalization test**: Deploy the trained bandit with real human participants to measure actual preference shifts versus those predicted by virtual participants
3. **Value granularity study**: Compare performance when using more fine-grained value dimensions versus the current eight high-level BEV values to determine if the bandit can leverage more nuanced targeting