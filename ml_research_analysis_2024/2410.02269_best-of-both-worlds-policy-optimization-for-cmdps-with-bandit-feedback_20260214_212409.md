---
ver: rpa2
title: Best-of-Both-Worlds Policy Optimization for CMDPs with Bandit Feedback
arxiv_id: '2410.02269'
source_url: https://arxiv.org/abs/2410.02269
tags:
- algorithm
- holds
- probability
- regret
- least
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces the first best-of-both-worlds algorithm for
  online learning in episodic constrained Markov decision processes (CMDPs) with bandit
  feedback. The proposed algorithm, PDB-PS, uses a policy optimization approach combined
  with a primal-dual framework to handle both stochastic and adversarial constraints.
---

# Best-of-Both-Worlds Policy Optimization for CMDPs with Bandit Feedback

## Quick Facts
- arXiv ID: 2410.02269
- Source URL: https://arxiv.org/abs/2410.02269
- Authors: Francesco Emanuele Stradi; Anna Lunghi; Matteo Castiglioni; Alberto Marchesi; Nicola Gatti
- Reference count: 40
- Primary result: First best-of-both-worlds algorithm for online learning in episodic CMDPs with bandit feedback

## Executive Summary
This paper introduces PDB-PS, the first best-of-both-worlds algorithm for online learning in episodic constrained Markov decision processes (CMDPs) with bandit feedback. The algorithm achieves O(√T) regret and constraint violation when constraints are stochastic, and O(√T) constraint violation with a fraction of optimal reward in the adversarial setting. Notably, PDB-PS automatically bounds Lagrangian multipliers during learning without requiring knowledge of the Slater's parameter, and introduces a weaker baseline that circumvents impossibility results in the adversarial setting.

## Method Summary
The method uses a primal-dual policy optimization approach combining fixed-share updates with policy optimization. The primal algorithm (FS-PODB.UPDATE) maintains no-interval regret properties through fixed-share updates, while the dual algorithm (OGD) handles constraint violations. The algorithm estimates transition functions and uses dilated bonuses for optimism. Lagrangian multipliers are automatically bounded through the no-interval regret property of both primal and dual components, eliminating the need to know the Slater's parameter ρ.

## Key Results
- Achieves O(√T) regret and constraint violation when constraints are stochastic
- Achieves O(√T) constraint violation and a fraction of optimal reward in adversarial setting
- First algorithm to automatically bound Lagrangian multipliers without knowing Slater's parameter
- Introduces weaker baseline that circumvents impossibility results in adversarial setting

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** The algorithm achieves no-interval regret by combining fixed-share updates with policy optimization.
- **Mechanism:** The fixed-share update guarantees a lower bound on the policy value at each episode, allowing regret bounds over any interval [t₁, t₂]. This is critical because standard adversarial MDP regret bounds don't apply over subintervals.
- **Core assumption:** The Slater's parameter ρ is not needed; the no-interval regret property holds regardless of ρ.
- **Evidence anchors:** [abstract], [section 3.2]
- **Break condition:** If the policy optimization step fails to maintain the lower bound guarantee, the no-interval regret property breaks.

### Mechanism 2
- **Claim:** Lagrangian multipliers are automatically bounded during learning without knowing ρ.
- **Mechanism:** The no-interval regret property of both primal and dual algorithms ensures that the cumulative Lagrangian loss is bounded. By comparing upper and lower bounds on this loss over intervals, we derive that λₜ must be bounded.
- **Core assumption:** Condition 2 (ρ ≥ T¹/⁸H/√112m) holds, ensuring the feasible region is "large enough."
- **Evidence anchors:** [section 3.4], [section F]
- **Break condition:** If Condition 2 fails, the automatic bounding fails and λₜ may grow unbounded.

### Mechanism 3
- **Claim:** The algorithm achieves best-of-both-worlds performance by adapting to stochastic vs adversarial constraints.
- **Mechanism:** When constraints are stochastic, the algorithm uses the automatically bounded λₜ to achieve O(√T) regret and violation. When constraints are adversarial, it uses a weaker baseline that must satisfy constraints at every episode, circumventing the impossibility result.
- **Core assumption:** The weaker baseline (OPTW) is a valid comparator that can be computed efficiently.
- **Evidence anchors:** [section 4.2.1], [section 2.3]
- **Break condition:** If the weaker baseline cannot be computed or is not a valid comparator, the adversarial setting guarantee breaks.

## Foundational Learning

- **Concept: Constrained Markov Decision Processes (CMDPs)**
  - Why needed here: The algorithm operates in CMDPs, which extend MDPs with constraints on cumulative costs.
  - Quick check question: What is the difference between the reward objective and the constraint objectives in a CMDP?

- **Concept: Slater's Condition**
  - Why needed here: The algorithm's performance depends on how "strictly feasible" the CMDP is, measured by ρ.
  - Quick check question: How does ρ affect the regret bounds when it is very small?

- **Concept: Lagrangian Duality in Online Learning**
  - Why needed here: The algorithm uses a primal-dual approach to handle constraints, requiring understanding of Lagrangian methods.
  - Quick check question: Why is it important to bound the Lagrangian multipliers during learning?

## Architecture Onboarding

- **Component map:**
  - PDB-PS (Primal-Dual Bandit Policy Search): Main algorithm
  - FS-PODB.UPDATE: Primal regret minimizer with fixed-share update
  - OGD (Online Gradient Descent): Dual regret minimizer
  - TRANSITION.UPDATE: Transition function estimator
  - Constraint violation monitor: Tracks cumulative violations

- **Critical path:**
  1. Initialize policy π₁ uniformly
  2. For each episode t:
     - Play πₜ, observe trajectory and bandit feedback
     - Build Lagrangian loss ℓₜ(x,a) = Γₜ + Σᵢλₜ,ᵢgₜ,ᵢ(x,a) − rₜ(x,a)
     - Update policy using FS-PODB.UPDATE
     - Update Lagrangian multipliers using OGD
     - Update Γₜ and Ξₜ

- **Design tradeoffs:**
  - Policy optimization vs occupancy measure optimization: Policy optimization is more efficient but requires careful regret analysis.
  - Fixed-share vs standard OMD: Fixed-share enables no-interval regret but adds complexity.
  - Bounded dual space [0,T¹/⁴]ᵐ: Ensures convergence but may be overly conservative.

- **Failure signatures:**
  - If λₜ grows unbounded: Likely Condition 2 is violated or no-interval regret property fails.
  - If regret is linear: Either the primal or dual algorithm has broken regret guarantees.
  - If violations are high: The policy is not respecting constraints; check λₜ updates.

- **First 3 experiments:**
  1. Verify no-interval regret: Run FS-PODB.UPDATE on a simple adversarial MDP and check regret over subintervals.
  2. Test automatic λₜ bounding: Run the full algorithm on a CMDP with known ρ and verify λₜ stays bounded.
  3. Compare baselines: Run with both the standard and weaker baselines in the adversarial setting and compare regret.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What are the precise theoretical conditions under which the algorithm's performance degrades from $\widetilde{O}(\sqrt{T})$ to $\widetilde{O}(T^{3/4})$ when Slater's parameter $\rho$ becomes too small?
- Basis in paper: Explicit - The paper states that when Condition 2 (which depends on $\rho$) does not hold, the algorithm achieves $\widetilde{O}(T^{3/4})$ regret and violation bounds instead of $\widetilde{O}(\sqrt{T})$.
- Why unresolved: The paper provides the threshold for Condition 2 but does not fully characterize the performance degradation path as $\rho$ approaches zero or becomes arbitrarily small.
- What evidence would resolve it: A complete characterization of the regret and violation bounds as a function of $\rho$, showing the exact transition point and rate of degradation.

### Open Question 2
- Question: How does the choice of the weaker baseline in the adversarial setting affect the practical performance and computational complexity compared to the standard baseline?
- Basis in paper: Explicit - The paper introduces a weaker baseline that requires constraints to be satisfied at every round and shows it achieves better regret bounds than the standard baseline.
- Why unresolved: While theoretical guarantees are provided, the paper does not discuss practical implementation details or compare computational complexity between the two baseline approaches.
- What evidence would resolve it: Empirical comparisons showing the practical differences in computation time, memory usage, and learning dynamics between the two baseline approaches.

### Open Question 3
- Question: What is the impact of the fixed share update parameter $\sigma$ on the algorithm's ability to achieve the no-interval regret property?
- Basis in paper: Explicit - The paper mentions that the fixed share update is crucial for achieving the no-interval regret property, but does not provide a detailed analysis of how the parameter $\sigma$ affects this property.
- Why unresolved: The theoretical analysis does not explore the sensitivity of the algorithm's performance to the choice of $\sigma$ or provide guidelines for its optimal selection.
- What evidence would resolve it: A detailed sensitivity analysis showing how different values of $\sigma$ affect the regret bounds and the algorithm's ability to maintain the no-interval regret property.

## Limitations

- The algorithm's performance degrades from O(√T) to O(T^(3/4)) when the Slater's parameter ρ becomes too small, which may be restrictive in practice.
- The weaker baseline approach requires efficient computation of OPT_W, but the paper does not provide detailed implementation guidance or discuss computational complexity.
- The no-interval regret property for adversarial MDPs with bandit feedback is claimed to be novel but lacks empirical validation.

## Confidence

- High confidence: The regret and violation bounds when ρ is large (≥ T^(1/8)H/√112m) are well-supported by theoretical analysis.
- Medium confidence: The claim of achieving no-interval regret for adversarial MDPs with bandit feedback requires further validation.
- Low confidence: The practical implications of the weaker baseline approach and its computational feasibility are unclear.

## Next Checks

1. Implement FS-PODB.UPDATE on a simple adversarial MDP with bandit feedback and empirically verify the no-interval regret property by measuring regret over multiple subintervals.
2. Test the automatic bounding of λₜ empirically by running the full algorithm on CMDPs with varying ρ values and tracking λₜ growth over time.
3. Construct concrete examples where the weaker baseline OPT_W can be computed efficiently, and validate whether the algorithm achieves the claimed Õ(√T) bounds in the adversarial setting.