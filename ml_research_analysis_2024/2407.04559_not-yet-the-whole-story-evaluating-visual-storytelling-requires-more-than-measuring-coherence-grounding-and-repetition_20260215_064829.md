---
ver: rpa2
title: 'Not (yet) the whole story: Evaluating Visual Storytelling Requires More than
  Measuring Coherence, Grounding, and Repetition'
arxiv_id: '2407.04559'
source_url: https://arxiv.org/abs/2407.04559
tags:
- visual
- stories
- story
- tapm
- image
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces a novel evaluation method for visual storytelling
  that measures the distance between model-generated and human-written stories along
  three key dimensions: visual grounding, coherence, and repetitiveness. The authors
  use this method to compare several visual storytelling models, including foundation
  models and smaller task-specific models.'
---

# Not (yet) the whole story: Evaluating Visual Storytelling Requires More than Measuring Coherence, Grounding, and Repetition

## Quick Facts
- arXiv ID: 2407.04559
- Source URL: https://arxiv.org/abs/2407.04559
- Reference count: 32
- Primary result: Foundation models like LLaVA perform visual storytelling competitively with smaller task-specific models when evaluated using human-likeness metrics across coherence, visual grounding, and repetitiveness

## Executive Summary
This paper addresses the challenge of evaluating visual storytelling models by introducing a novel distance-based metric (dHM) that measures how closely model-generated stories align with human-written stories across three key dimensions: visual grounding, coherence, and repetitiveness. The authors test this metric on several visual storytelling models, including foundation models (LLaVA, BLIP-2) and task-specific models (GLAC Net, AREL, TAPM), finding that while LLaVA performs best, a much smaller TAPM model with upgraded components achieves competitive results. A human evaluation study validates the metric's effectiveness but reveals that humans still prefer human-written stories, suggesting additional factors beyond the measured dimensions contribute to story quality.

## Method Summary
The authors introduce a reference-free evaluation framework that measures the distance between model-generated and human-written stories using three metrics: GROOViST for visual grounding, modified RoViST-C for coherence, and RoViST-NR for non-repetitiveness. They apply this framework to evaluate both foundation models (LLaVA, BLIP-2) using zero-shot prompting and task-specific models (GLAC Net, AREL, TAPM) on two datasets (VIST and VWP). To improve efficiency, they upgrade TAPM's language and vision components with LLAMA 2 and ViT respectively, creating new variants that achieve competitive performance with fewer parameters.

## Key Results
- LLaVA, a large foundation model, performs best on visual storytelling but only marginally outperforms TAPM, a much smaller task-specific model
- Upgrading TAPM's language and vision components creates a model that achieves competitive performance with significantly fewer parameters than LLaVA
- Human evaluation validates the dHM metric but reveals that humans still prefer human-written stories despite quantitative closeness in the measured dimensions
- The study demonstrates that visual storytelling can be effectively performed by foundation models without task-specific training

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Human-like evaluation metrics improve model alignment on visual storytelling
- Mechanism: By measuring the distance between model-generated and human-written stories along coherence, visual grounding, and repetitiveness, the model is guided to produce stories that align more closely with human preferences in these dimensions
- Core assumption: These three dimensions (coherence, visual grounding, repetitiveness) are the primary factors that distinguish human-quality stories from machine-generated ones
- Evidence anchors:
  - [abstract] "we introduce a novel method that measures story quality in terms of human likeness regarding three key aspects: visual grounding, coherence, and repetitiveness"
  - [section 3.1] "Wang et al. (2022) proposed a metric—RoViST—specifically for the visual storytelling task, which assesses three aspects of generated stories: visual grounding, coherence, and repetition"
  - [corpus] Weak evidence - corpus search found no direct citations to this specific mechanism

### Mechanism 2
- Claim: Foundation models can be adapted for visual storytelling without task-specific training
- Mechanism: Large foundation models like LLaVA, pre-trained on multimodal data, possess the necessary capabilities to generate coherent stories from image sequences through zero-shot prompting
- Core assumption: The general-purpose visual and language capabilities learned during pre-training transfer effectively to the specific task of visual storytelling
- Evidence anchors:
  - [abstract] "we test in a zero-shot manner... LLaVA, a powerful foundation model, performs best on the task"
  - [section 4.2] "BLIP-2 Unlike the previously described models, Bootstrapping Language-Image Pre-training (Li et al., 2023) is a multimodal foundation model designed for general-purpose vision-language tasks"
  - [corpus] Moderate evidence - corpus search found related papers on zero-shot multimodal generation

### Mechanism 3
- Claim: Modular architecture enables efficient model improvement
- Mechanism: By upgrading specific components (language and vision backbones) of the TAPM model while keeping the overall architecture intact, performance can be improved without retraining from scratch
- Core assumption: The visual storytelling task can be decomposed into separable language and vision components that can be independently optimized
- Evidence anchors:
  - [abstract] "Upgrading the visual and language components of TAPM results in a model that yields competitive performance with a relatively low number of parameters"
  - [section 5.1] "we leverage the modular architecture of TAPM and test whether we can obtain better results by replacing its original language and vision components"
  - [corpus] Weak evidence - corpus search found no direct citations to this specific modular improvement approach

## Foundational Learning

- Concept: Reference-free evaluation metrics
  - Why needed here: Traditional evaluation metrics require ground truth references, but visual storytelling allows multiple valid stories for the same image sequence
  - Quick check question: What is the key difference between reference-based and reference-free evaluation metrics in visual storytelling?

- Concept: Multimodal model architecture
  - Why needed here: Understanding how vision and language components interact is crucial for both evaluating existing models and designing improvements
  - Quick check question: How do foundation models like LLaVA connect vision encoders to language models differently than task-specific models?

- Concept: Distance-based evaluation
  - Why needed here: The proposed method measures distance from human stories rather than absolute scores, providing a more nuanced evaluation framework
  - Quick check question: Why might measuring distance from human stories be more informative than measuring absolute quality scores?

## Architecture Onboarding

- Component map: Image sequence -> Visual feature extraction (ResNet/Faster R-CNN/ViT) -> Context enrichment -> Story generation (GPT-2/LLaMA2) -> Evaluation (GROOViST, RoViST-C, RoViST-NR) -> Output story

- Critical path: 1. Image feature extraction -> 2. Context enrichment -> 3. Story generation -> 4. Evaluation across three dimensions -> 5. Distance calculation from human stories

- Design tradeoffs:
  - Zero-shot vs. fine-tuning: Foundation models offer strong baseline performance but lack task-specific optimization
  - Parameter efficiency vs. performance: Smaller models with upgraded components can match larger foundation models
  - Modularity vs. integration: Separate components allow targeted improvements but may miss cross-modal interactions

- Failure signatures:
  - High visual grounding scores but poor coherence: Vision component works but language generation is incoherent
  - Low repetition scores but poor visual grounding: Language model generates varied content but doesn't connect to images
  - Consistent underperformance across all metrics: Fundamental architectural mismatch with task requirements

- First 3 experiments:
  1. Implement zero-shot story generation with LLaVA using different prompt variations to establish baseline performance
  2. Upgrade TAPM's language component to LLAMA2 while keeping vision component unchanged to isolate language impact
  3. Replace TAPM's vision component with ViT while keeping language component unchanged to isolate vision impact

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What specific additional factors beyond visual grounding, coherence, and repetition contribute to human preferences for human-written stories over model-generated ones?
- Basis in paper: [explicit] The paper states that "humans still prefer human-written stories by a significant margin despite the quantitative closeness we observe," and annotators reported considering "subjective world knowledge," "overarching narrative," and "phrases or sentences expressing emotions."
- Why unresolved: The study only identified these factors anecdotally through annotator comments without systematically investigating their relative importance or how to measure them automatically.
- What evidence would resolve it: A controlled study isolating each proposed factor (world knowledge, narrative structure, emotional content) and measuring their individual impact on human story preference ratings.

### Open Question 2
- Question: Why does the combined (+LLAMA 2, +ViT) model underperform compared to either individual upgrade, despite both components improving performance separately?
- Basis in paper: [explicit] The authors note that "despite the significant difference in the number of parameters, we notice that (+LLAMA 2) achieves performance on par with LLaVA" but that the combined model "consistently under-performed on our distance measure dHM compared to the (+LLAMA 2) and (+ViT) versions."
- Why unresolved: The paper does not investigate the interaction between the upgraded language and vision components or whether architectural conflicts arise when combining them.
- What evidence would resolve it: Ablation studies testing different integration strategies for the combined model, or analysis of internal representations to identify conflicts between the two upgraded components.

### Open Question 3
- Question: How would visual storytelling models perform on datasets with more diverse cultural backgrounds and languages beyond English and Western-centric content?
- Basis in paper: [explicit] The authors acknowledge this as a limitation: "we experimented with only two visual storytelling datasets, both in English and both Western-centric" and state they "strongly support the creation of such resources."
- Why unresolved: The paper's evaluation is limited to the VIST and VWP datasets, which share cultural and linguistic characteristics, making it impossible to assess cross-cultural generalization.
- What evidence would resolve it: Testing the same models on visual storytelling datasets from different cultures and languages, or creating synthetic culturally-diverse test sets to evaluate robustness across cultural contexts.

## Limitations
- The proposed dHM metric, while effective, may not capture all factors that contribute to human story quality preferences beyond coherence, visual grounding, and repetitiveness
- The study's findings are based primarily on English-language datasets (VIST and VWP) with Western-centric content, limiting cross-cultural generalizability
- Zero-shot prompting details for foundation models are limited, making it difficult to assess the robustness of the approach or identify optimal prompting strategies

## Confidence
- **High confidence**: Foundation models can perform visual storytelling without task-specific training (robust experimental support)
- **Medium confidence**: Modular architecture improvements can achieve competitive performance with fewer parameters (promising but component interactions need further study)
- **Low confidence**: The three measured dimensions are sufficient to capture story quality (contradicted by human evaluation results)

## Next Checks
1. Conduct controlled experiments that independently upgrade language and vision components of TAPM to quantify their individual contributions to performance improvements
2. Design a human study that specifically investigates which additional factors influence story quality preferences beyond those measured by dHM
3. Test the dHM metric and model performance on additional visual storytelling datasets with different image sources and narrative styles to assess generalization