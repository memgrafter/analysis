---
ver: rpa2
title: 'When Attention Sink Emerges in Language Models: An Empirical View'
arxiv_id: '2410.10781'
source_url: https://arxiv.org/abs/2410.10781
tags:
- attention
- sink
- latexit
- sha1
- base64
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates the phenomenon of attention sinks in language
  models, where significant attention is allocated to the first token regardless of
  its semantic importance. Through comprehensive empirical studies on various open-source
  models and controlled pre-training experiments, the authors demonstrate that attention
  sinks universally exist in auto-regressive language models across different inputs
  and model sizes.
---

# When Attention Sink Emerges in Language Models: An Empirical View

## Quick Facts
- **arXiv ID**: 2410.10781
- **Source URL**: https://arxiv.org/abs/2410.10781
- **Reference count**: 40
- **Primary Result**: Attention sinks universally exist in auto-regressive language models and can be eliminated by replacing softmax with sigmoid attention while maintaining performance.

## Executive Summary
This paper investigates the phenomenon of attention sinks in language models, where significant attention is allocated to the first token regardless of its semantic importance. Through comprehensive empirical studies on various open-source models and controlled pre-training experiments, the authors demonstrate that attention sinks universally exist in auto-regressive language models across different inputs and model sizes. The research reveals that attention sinks act more like key biases storing extra attention scores rather than contributing to value computation, and this phenomenon partially stems from tokens' inner dependence on attention scores due to softmax normalization. Most importantly, the authors show that by replacing softmax attention with operations like sigmoid attention without normalization, attention sinks can be eliminated in models up to 1 billion parameters while maintaining comparable model performance.

## Method Summary
The authors conducted comprehensive empirical studies across various open-source language models, examining attention patterns across different inputs and model sizes. They performed controlled pre-training experiments using 1B parameter models to isolate the emergence of attention sinks. The key methodological innovation involved replacing the standard softmax attention mechanism with sigmoid attention without normalization, allowing them to test whether this architectural change could eliminate attention sinks while preserving model performance. The experiments systematically varied model architectures, training procedures, and attention mechanisms to characterize the conditions under which attention sinks emerge and persist.

## Key Results
- Attention sinks universally exist in auto-regressive language models across different inputs and model sizes
- Attention sinks act more like key biases storing extra attention scores rather than contributing to value computation
- Sigmoid attention without normalization can eliminate attention sinks in models up to 1B parameters while maintaining comparable performance

## Why This Works (Mechanism)
The emergence of attention sinks is fundamentally tied to the softmax normalization in standard attention mechanisms. When softmax is applied to attention scores, tokens become interdependent - each token's final attention distribution depends on all other tokens' scores. This creates a self-reinforcing mechanism where certain positions (particularly the first token) can accumulate disproportionate attention weights. The sigmoid attention replacement works by removing this normalization step, allowing attention scores to be computed independently for each token without the interdependency that drives attention sink formation. This preserves the core attention mechanism while eliminating the pathological behavior that arises from softmax normalization.

## Foundational Learning

**Attention Mechanisms**: The mathematical foundation for how transformers compute relationships between tokens using query-key dot products and softmax normalization. Understanding this is crucial because attention sinks emerge specifically from the softmax normalization step.

*Why needed*: The paper's core contribution depends on understanding how standard attention works and how its normalization creates pathological behaviors.

*Quick check*: Can you explain why softmax creates interdependence between tokens while sigmoid does not?

**Softmax vs Sigmoid Normalization**: Softmax converts raw scores into a probability distribution where all outputs sum to 1, creating interdependence. Sigmoid independently scales each value to (0,1) without normalization constraints.

*Why needed*: The paper's solution replaces softmax with sigmoid, making understanding the mathematical differences critical to grasping why this eliminates attention sinks.

*Quick check*: What mathematical property of softmax creates the attention sink phenomenon that sigmoid avoids?

**Auto-regressive Language Models**: Models that generate text sequentially, predicting one token at a time based on previous context. The paper focuses exclusively on this architecture.

*Why needed*: The attention sink phenomenon appears specific to auto-regressive models, making this architectural constraint important for understanding the scope of the findings.

*Quick check*: Why might attention sinks be less problematic in bidirectional models like BERT?

## Architecture Onboarding

**Component Map**: Input tokens -> Query/Key/Value projections -> Attention scores (dot products) -> Softmax normalization -> Weighted value aggregation -> Output

**Critical Path**: The critical path for attention sink formation is: Query/Key projections -> Attention scores computation -> Softmax normalization -> Attention distribution calculation. The sink emerges specifically at the softmax normalization stage where token scores become interdependent.

**Design Tradeoffs**: Standard softmax attention provides probabilistic interpretation and ensures attention weights sum to 1, which is mathematically elegant but creates pathological behaviors. Sigmoid attention removes normalization constraints, eliminating attention sinks but potentially losing the probabilistic interpretation and requiring careful score scaling.

**Failure Signatures**: Attention sinks manifest as consistently high attention weights on the first token across diverse inputs and contexts, regardless of semantic relevance. This appears as a horizontal line of high values in attention weight heatmaps for the first position.

**First 3 Experiments**:
1. Visualize attention weight heatmaps for various auto-regressive models to confirm attention sink existence
2. Compare attention distributions before and after softmax normalization to isolate where sinks emerge
3. Replace softmax with sigmoid in a small model and measure attention patterns and performance impact

## Open Questions the Paper Calls Out
None

## Limitations
- The analysis focuses primarily on auto-regressive language models and may not generalize to other architectures like bidirectional encoders or multimodal models
- The controlled pre-training experiments with 1B parameter models may not scale to larger models where training dynamics become more complex
- The claim that attention sinks "act more like key biases" rather than contributing to value computation needs further theoretical justification beyond empirical observations

## Confidence

**High Confidence**: The empirical observation that attention sinks universally exist in auto-regressive language models across different inputs and model sizes is well-supported by comprehensive experiments.

**Medium Confidence**: The characterization of attention sinks as "key biases storing extra attention scores" is plausible based on empirical evidence but lacks complete theoretical grounding.

**Medium Confidence**: The claim that sigmoid attention can eliminate attention sinks while maintaining performance in 1B parameter models is supported but needs validation on larger scales.

## Next Checks

1. **Scale Validation**: Test the sigmoid attention replacement approach on models larger than 1B parameters (e.g., 7B or 70B scale) to verify whether the elimination of attention sinks remains effective and performance is maintained.

2. **Cross-Architecture Testing**: Evaluate whether attention sinks exist and can be eliminated in non-auto-regressive architectures like BERT, T5, or multimodal models to determine if the phenomenon is architecture-specific.

3. **Theoretical Analysis**: Develop a rigorous mathematical framework explaining why attention sinks emerge from softmax normalization and how sigmoid attention fundamentally changes this behavior, moving beyond empirical observations to theoretical guarantees.