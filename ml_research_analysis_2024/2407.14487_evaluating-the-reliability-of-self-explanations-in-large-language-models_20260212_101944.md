---
ver: rpa2
title: Evaluating the Reliability of Self-Explanations in Large Language Models
arxiv_id: '2407.14487'
source_url: https://arxiv.org/abs/2407.14487
tags:
- self-explanations
- explanations
- methods
- task
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This study evaluates the reliability of self-explanations generated\
  \ by large language models (LLMs) through prompting. Two explanation types\u2014\
  extractive and counterfactual\u2014were generated by three LLMs (2B to 8B parameters)\
  \ on two classification tasks (objective and subjective)."
---

# Evaluating the Reliability of Self-Explanations in Large Language Models

## Quick Facts
- **arXiv ID**: 2407.14487
- **Source URL**: https://arxiv.org/abs/2407.14487
- **Reference count**: 39
- **Primary result**: Prompting LLMs for counterfactual self-explanations can produce faithful and easily validated explanations, though extractive explanations show correlation with human judgment without necessarily reflecting actual decision processes

## Executive Summary
This study evaluates the reliability of self-explanations generated by large language models through prompting techniques. The researchers compared extractive and counterfactual explanation types across three LLMs (2B to 8B parameters) on objective and subjective classification tasks. They found that while extractive explanations often aligned with human judgment, they did not always accurately represent the model's decision process. Counterfactual explanations, when properly generated, demonstrated high faithfulness and similarity to original text, offering a promising alternative to traditional explainability methods.

## Method Summary
The researchers employed a prompt-tuning approach to generate two types of self-explanations: extractive (selecting relevant input features) and counterfactual (identifying minimal input changes that would alter predictions). Three LLMs with 2B to 8B parameters were evaluated on two classification tasks covering both objective and subjective domains. The study used human judgment as a baseline for comparison and implemented validation methods by feeding counterfactual explanations back through the LLM. The experimental design focused on measuring explanation reliability, faithfulness, and the relationship between human-aligned explanations and actual model decision processes.

## Key Results
- Extractive self-explanations correlated with human judgment but did not always reflect the model's actual decision process
- Valid counterfactual explanations demonstrated high faithfulness and similarity to original text
- Prompt tuning was identified as crucial for successful counterfactual explanation generation
- Counterfactual explanations could be easily validated by feeding them back through the LLM

## Why This Works (Mechanism)
The mechanism underlying successful self-explanations appears to rely on the LLM's ability to reason about its own input processing and decision boundaries. For extractive explanations, the model identifies salient features that align with human judgment through pattern recognition and attention mechanisms. For counterfactual explanations, the model must understand the minimal perturbations required to cross decision boundaries, demonstrating deeper reasoning about its own classification logic. The validation process works because the LLM can re-evaluate modified inputs consistently, providing a built-in check for explanation reliability.

## Foundational Learning

**Explanation Types**
- *Why needed*: Different explanation types serve different purposes - extractive for feature identification, counterfactual for understanding decision boundaries
- *Quick check*: Verify that both explanation types can be generated and are semantically distinct

**Prompt Tuning**
- *Why needed*: Effective prompt engineering is essential for generating meaningful self-explanations
- *Quick check*: Test multiple prompt variations to assess sensitivity of explanation quality

**Faithfulness vs. Plausibility**
- *Why needed*: Distinguish between explanations that sound reasonable to humans versus those that actually reflect model reasoning
- *Quick check*: Compare explanation-human correlation against explanation-model alignment metrics

## Architecture Onboarding

**Component Map**
Prompt -> LLM (2B-8B parameters) -> Extractive/Counterfactual Explanation -> Human Validation / Model Re-evaluation

**Critical Path**
Prompt design → Explanation generation → Validation through model re-evaluation → Reliability assessment

**Design Tradeoffs**
- Model size vs. explanation quality (smaller models may struggle with complex reasoning)
- Explanation type selection (extractive vs. counterfactual) based on task requirements
- Human validation effort vs. automated validation through model re-evaluation

**Failure Signatures**
- Extractive explanations that correlate with human judgment but fail model re-evaluation
- Counterfactual explanations that are invalid or produce inconsistent results when re-evaluated
- Prompt sensitivity where small changes drastically affect explanation quality

**3 First Experiments**
1. Test prompt sensitivity by varying explanation prompts across all three models
2. Compare explanation reliability between objective and subjective classification tasks
3. Evaluate explanation faithfulness by measuring correlation between original and counterfactual inputs

## Open Questions the Paper Calls Out
The paper identifies major uncertainties regarding generalizability across different model architectures, sizes, and tasks. The findings from relatively small models (2B to 8B parameters) may not extend to larger frontier models. The evaluation framework's focus on specific classification tasks may not capture the full spectrum of scenarios requiring self-explanations. The study acknowledges limitations in quantifying the gap between correlation with human judgment and genuine decision process alignment for extractive explanations.

## Limitations
- Results may not generalize to larger frontier models (70B+ parameters)
- Evaluation focused on classification tasks, limiting applicability to other domains
- Correlation between extractive explanations and human judgment does not establish causal alignment with model decision processes
- Counterfactual explanation failure conditions and reliability boundaries are not fully characterized
- Prompt tuning requirements may limit practical real-world applicability

## Confidence

**High confidence**: The observation that extractive explanations correlate with human judgment

**Medium confidence**: The finding that valid counterfactual explanations demonstrate high faithfulness

**Medium confidence**: The importance of prompt tuning for counterfactual explanations

**Low confidence**: Generalizability to larger models and diverse task types

## Next Checks

1. Replicate the study using frontier models (70B+ parameters) to assess scalability of explanation reliability patterns
2. Test the explanation reliability across a broader range of task types including reasoning, generation, and multi-modal tasks
3. Develop and validate metrics to quantify the gap between correlation with human judgment and actual decision process alignment for extractive explanations