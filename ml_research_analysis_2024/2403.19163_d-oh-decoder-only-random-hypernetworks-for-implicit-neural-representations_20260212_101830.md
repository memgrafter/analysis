---
ver: rpa2
title: 'D''OH: Decoder-Only Random Hypernetworks for Implicit Neural Representations'
arxiv_id: '2403.19163'
source_url: https://arxiv.org/abs/2403.19163
tags:
- neural
- quantization
- target
- coin
- https
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces D'OH, a Decoder-Only Hypernetwork approach
  for implicit neural representations that optimizes a low-dimensional latent code
  at runtime to generate target network weights via fixed random projections. Unlike
  existing encoder-decoder hypernetworks that require offline training on large datasets,
  D'OH requires no offline training and works directly on the target signal instance.
---

# D'OH: Decoder-Only Random Hypernetworks for Implicit Neural Representations

## Quick Facts
- arXiv ID: 2403.19163
- Source URL: https://arxiv.org/abs/2403.19163
- Reference count: 40
- Key result: Decoder-only hypernetwork achieves 2.38dB BD-PSNR gain and 78.66% BD-Rate reduction in image compression without offline training

## Executive Summary
D'OH introduces a decoder-only hypernetwork approach for implicit neural representations that eliminates the need for offline training on large datasets. The method generates network weights directly from a low-dimensional latent code using fixed random projections, enabling runtime optimization on the target signal instance. By decoupling weight generation from training data, D'OH provides direct control over memory footprint through latent code dimension while maintaining competitive rate-distortion performance.

## Method Summary
The D'OH framework uses a decoder-only architecture where a small latent code vector is transformed through random projections to generate the weights of a target neural network. Unlike traditional encoder-decoder hypernetworks that require extensive offline training on large datasets, D'OH operates directly on the target signal instance without any pretraining. The random projection mechanism provides inherent robustness to quantization and allows for memory-efficient deployment by controlling the latent code dimension.

## Key Results
- Achieves 2.38dB BD-PSNR gain over traditional MLPs in image compression
- Reduces BD-Rate by 78.66% compared to COIN on Kodak and DIV2K datasets
- Demonstrates similar performance under both Post-Training Quantization and Quantization-Aware Training
- Enables direct memory footprint control through latent code dimension adjustment

## Why This Works (Mechanism)
The decoder-only architecture with random projections works by decoupling weight generation from training data dependencies. The random projections create a high-dimensional space where the latent code can be efficiently mapped to network weights without requiring learned transformations. This approach provides two key advantages: it eliminates the need for large-scale pretraining datasets, and the randomness introduces inherent regularization that improves generalization and quantization robustness.

## Foundational Learning

**Implicit Neural Representations**: Functions that map coordinates to signal values without discrete sampling. Why needed: Enables continuous signal representation and compression. Quick check: Verify signal continuity across coordinate space.

**Hypernetworks**: Networks that generate weights for other networks. Why needed: Allows dynamic weight generation without storing full parameter sets. Quick check: Confirm weight generation stability across different latent codes.

**Random Projections**: Linear transformations using random matrices. Why needed: Provides efficient dimensionality reduction and inherent regularization. Quick check: Verify Johnson-Lindenstrauss property preservation.

## Architecture Onboarding

Component map: Latent Code -> Random Projections -> Weight Generator -> Target Network -> Signal Output

Critical path: The transformation from latent code through random projections to weight generation is the most computationally intensive component, as it determines the quality of the generated weights and thus the final signal representation.

Design tradeoffs: 
- Higher latent code dimensions improve representation quality but increase memory usage
- Random projection dimension affects weight generation quality and computational cost
- Target network architecture depth impacts compression efficiency and runtime performance

Failure signatures: 
- Poor rate-distortion performance indicates insufficient latent code capacity
- Quantization artifacts suggest inadequate random projection design
- Training instability may result from improper weight initialization

First experiments:
1. Test different latent code dimensions (16, 32, 64) on Kodak dataset to establish memory-performance tradeoff
2. Compare random projection dimensions (512, 1024, 2048) for weight generation quality
3. Evaluate quantization robustness across 4-bit, 8-bit, and 16-bit representations

## Open Questions the Paper Calls Out
None

## Limitations
- Evaluation primarily on small-scale image datasets (Kodak 24 images, DIV2K 800 images) limits generalizability
- Claims of quantization robustness need validation across more quantization schemes and bit depths
- Design choices for latent code dimension and projection parameters may impact practical deployment

## Confidence
High confidence: Claims about eliminating offline training requirements and enabling runtime optimization
Medium confidence: Rate-distortion performance claims on image compression and occupancy fields, as these are demonstrated but on limited datasets
Medium confidence: Claims about quantization robustness, as the experiments are shown but may not cover all relevant quantization scenarios
Low confidence: Claims about avoiding neural architecture search complexity, as the practical implications of design choices are not fully explored

## Next Checks
1. Test D'OH on larger-scale datasets (ImageNet, COCO) and diverse signal types (audio, video, scientific data) to verify scalability and generalizability
2. Conduct ablation studies on quantization robustness across different bit depths and quantization schemes with statistical significance testing
3. Compare D'OH against more baseline methods including recent INR approaches and traditional compression codecs under standardized implementation conditions