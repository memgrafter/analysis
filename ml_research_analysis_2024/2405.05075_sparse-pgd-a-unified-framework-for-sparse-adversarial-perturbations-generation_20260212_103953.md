---
ver: rpa2
title: 'Sparse-PGD: A Unified Framework for Sparse Adversarial Perturbations Generation'
arxiv_id: '2405.05075'
source_url: https://arxiv.org/abs/2405.05075
tags:
- adversarial
- perturbations
- sparse
- spgd
- attacks
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Sparse-PGD (sPGD), a unified framework for
  generating both unstructured and structured sparse adversarial perturbations. The
  key innovation is decomposing perturbations into magnitude tensors and binary masks,
  then optimizing the mask using continuous approximations and gradient-based updates.
---

# Sparse-PGD: A Unified Framework for Sparse Adversarial Perturbations Generation

## Quick Facts
- arXiv ID: 2405.05075
- Source URL: https://arxiv.org/abs/2405.05075
- Authors: Xuyang Zhong; Chen Liu
- Reference count: 40
- Primary result: sPGD achieves state-of-the-art sparse adversarial attack performance on CIFAR-10, ImageNet-100, and GTSRB

## Executive Summary
This paper introduces Sparse-PGD (sPGD), a unified framework for generating both unstructured and structured sparse adversarial perturbations. The key innovation is decomposing perturbations into magnitude tensors and binary masks, then optimizing the mask using continuous approximations and gradient-based updates. For unstructured sparsity, sPGD uses an unprojected gradient and random reinitialization to escape local optima. For structured sparsity, it approximates the group l0 norm using group masks and transposed convolution to map groups to pixels. Experiments show sPGD outperforms existing white-box sparse attacks and, when combined with black-box Sparse-RS in Sparse-AutoAttack (sAA), achieves state-of-the-art performance across various adversarial training regimes.

## Method Summary
Sparse-PGD (sPGD) generates sparse adversarial perturbations by decomposing the perturbation into a magnitude tensor p and a binary mask m, where δ = p ⊙ m. The magnitude p is optimized using standard projected gradient descent on an l∞ constraint, while the mask m is optimized via a continuous proxy fm that is projected to binary using sigmoid activation and thresholding. For unstructured sparsity, sPGD employs unprojected gradients and random reinitialization to escape local optima. For structured sparsity, it uses group masks v and transposed convolution to map groups to pixels while approximating the group l0 norm. The method demonstrates effectiveness across classification, object detection, and segmentation tasks.

## Key Results
- sPGD outperforms existing white-box sparse attacks on CIFAR-10, ImageNet-100, and GTSRB
- Combined with Sparse-RS in sAA, achieves state-of-the-art performance across l∞, l2, l1, and l0 adversarially trained models
- sPGD's efficiency enables effective adversarial training, producing models robust to various sparse attacks
- The method extends to object detection and segmentation tasks with good transferability across architectures
- Visualization shows perturbations concentrate on semantically important regions in adversarially trained models

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Decomposing perturbations into magnitude tensors and binary masks enables gradient-based optimization even for discrete sparsity constraints.
- **Mechanism:** By separating the perturbation δ into p (continuous magnitude) and m (binary sparsity mask), sPGD can optimize p using standard projected gradient descent, while m is optimized via a continuous approximation fm that is later projected to binary using sigmoid and thresholding.
- **Core assumption:** The loss landscape with respect to fm is smooth enough that gradient ascent will guide it toward configurations that, when thresholded, yield high-performing sparse masks.
- **Evidence anchors:**
  - [abstract] "The key innovation is decomposing perturbations into magnitude tensors and binary masks, then optimizing the mask using continuous approximations and gradient-based updates."
  - [section] "we decompose the perturbation δ as the product of a magnitude tensor p and a binary sparse mask m: δ = p ⊙ m, where p and m determine the magnitudes and the locations of perturbed features, respectively."
- **Break condition:** If the projection from fm to m causes the mask to become too sparse or too dense relative to the budget, the attack may fail to find strong perturbations.

### Mechanism 2
- **Claim:** Unprojected gradients and random reinitialization help escape local optima in the non-convex l0 adversarial budget.
- **Mechanism:** When updating the magnitude tensor p, using the unprojected gradient egp (which multiplies the gradient by the continuous mask σ(fm) instead of the binary mask m) encourages exploration by updating all elements of p. Random reinitialization of fm is triggered when the mask m stops changing for several iterations, allowing the search to escape stagnation.
- **Core assumption:** The local optima in the discrete mask space are sensitive to the continuous proxy fm; reinitializing fm allows discovery of new promising regions.
- **Evidence anchors:**
  - [abstract] "For unstructured sparsity, sPGD uses an unprojected gradient and random reinitialization to escape local optima."
  - [section] "To further boost the performance, we propose the unprojected gradient of p and random reinitialization mechanism."
- **Break condition:** If the tolerance for reinitialization is set too low, the search may become too stochastic and lose exploitation ability; if too high, it may get stuck in poor local optima.

### Mechanism 3
- **Claim:** Approximating group l0 norm via a binary group mask and transposed convolution enables structured sparse attacks.
- **Mechanism:** Structured sparsity is modeled by grouping pixels (e.g., patches) and selecting which groups to perturb via a binary group mask v. The continuous proxy ev is optimized, and the group mask v is mapped to pixel mask m using transposed convolution with a kernel k defining the group pattern, followed by clipping to ensure binary outputs.
- **Core assumption:** The approximation Ω′₀(x,v) ≤ ∥v∥₀ is tight enough in practice that optimizing v bounded by l0 norm yields near-optimal structured sparse perturbations.
- **Evidence anchors:**
  - [abstract] "For structured sparsity, it approximates the group l0 norm using group masks and transposed convolution to map groups to pixels."
  - [section] "we propose a white-box attack named Sparse-PGD (sPGD) to effectively and efficiently generate unstructured and structured sparse perturbations."
- **Break condition:** If group overlaps are significant, the approximation gap Ω′₀(x,v) - Ω₀(x) may become large, reducing attack effectiveness.

## Foundational Learning

- **Concept:** Non-convex optimization and projection onto discrete sets.
  - Why needed here: The l0 norm defines a non-convex set; direct gradient descent cannot be applied. Understanding how to project continuous updates onto discrete feasible sets is essential for designing sPGD.
  - Quick check question: What is the difference between projecting onto a convex set (like l∞ ball) vs a non-convex set (like l0 ball) in terms of gradient-based optimization?

- **Concept:** Continuous relaxation of discrete variables.
  - Why needed here: Binary masks m cannot be directly optimized with gradients. The continuous proxy fm with sigmoid activation and thresholding allows gradient-based search in a relaxed space.
  - Quick check question: Why is the sigmoid function used before thresholding fm to obtain m, and what would happen if we omitted it?

- **Concept:** Group sparsity and submodular functions.
  - Why needed here: Structured sparse perturbations are defined via group l0 norm. Understanding submodular functions and their convex envelopes (like Bach's formulation) is key to approximating and optimizing structured sparsity.
  - Quick check question: How does the group l0 norm differ from the standard l0 norm, and why is it more challenging to optimize?

## Architecture Onboarding

- **Component map:** Magnitude tensor p -> Continuous mask fm -> Binary mask m -> Loss function -> Gradients -> Update p and fm -> Project to m -> Repeat
- **Critical path:** Clean image → forward pass → compute loss → backpropagate to obtain gradients → update p and fm (or ev) → project to m (or v) → repeat until success or max iterations.
- **Design tradeoffs:**
  - Using unprojected vs sparse gradient: Balances exploration and exploitation; ensemble improves robustness.
  - Tolerance for reinitialization: Higher tolerance favors exploitation (better for adversarial training), lower tolerance favors exploration (better for strong attacks).
  - Step sizes α and β: Smaller values yield more stable but slower convergence; larger values risk instability or overshooting.
- **Failure signatures:**
  - Mask m stops changing for several iterations → triggers reinitialization.
  - Gradient magnitude ||∇fmL||₂ < γ → skip fm update to avoid saturation.
  - Attack fails to find adversarial example within iteration budget → consider increasing ϵ or iterations.
- **First 3 experiments:**
  1. Implement sPGD for unstructured sparsity on CIFAR-10 with ϵ=20, compare against PGD0 and SparseFool on a ResNet-18 model.
  2. Add unprojected gradient and random reinitialization to sPGD, measure improvement in attack success rate.
  3. Extend sPGD to structured sparsity (e.g., 3×3 patches), evaluate on ImageNet-100, compare with LOAP and extended Sparse-RS.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of Sparse-PGD compare when using different gradient approximation methods beyond the sigmoid-based projection used in the paper?
- Basis in paper: [explicit] The paper mentions that the gradient of the continuous alternative fm is calculated by discarding the projection operator and using the approximation ∂ΠSm(σ(fm))/∂fm ≃ σ'(fm).
- Why unresolved: The paper does not explore alternative gradient approximation methods for the binary mask optimization, which could potentially improve performance.
- What evidence would resolve it: Comparative experiments showing robust accuracy and convergence rates using different gradient approximation techniques (e.g., straight-through estimators, soft thresholding, or learned projections) on various datasets and sparsity levels.

### Open Question 2
- Question: What is the theoretical relationship between the group l0 norm and its convex envelope, and how does this affect the approximation quality in practice?
- Basis in paper: [explicit] The paper introduces Theorem IV.1 showing that Ω0(x) ≤ Ω'0(x,v) ≤ ||v||0, but does not provide a deeper theoretical analysis of when and why this approximation works well.
- Why unresolved: The paper provides empirical evidence that the approximation works well but does not establish theoretical guarantees or bounds on approximation quality.
- What evidence would resolve it: A theoretical analysis proving bounds on the approximation error between Ω0 and Ω'0 under different group configurations, or empirical studies showing how the approximation quality varies with group overlap, group size, and sparsity levels.

### Open Question 3
- Question: How does Sparse-PGD's performance scale with image resolution and what are the computational bottlenecks at higher resolutions?
- Basis in paper: [inferred] The paper mentions that Sparse-PGD shows higher efficiency on high-resolution images compared to black-box methods, but does not provide detailed scaling analysis or identify specific bottlenecks.
- Why unresolved: The paper focuses on CIFAR-10, ImageNet-100, and GTSRB datasets but does not systematically study the scaling behavior or identify where computational overhead increases with resolution.
- What evidence would resolve it: Scaling experiments on progressively higher resolution datasets (e.g., full ImageNet, medical imaging datasets) with detailed profiling of runtime components, memory usage, and attack success rates at different resolutions.

### Open Question 4
- Question: How does the choice of kernel patterns in structured sparse perturbations affect transferability across different architectures and tasks?
- Basis in paper: [explicit] The paper mentions that customized kernels can create perturbations in various patterns (hearts, stars, letters) and shows some transferability results, but does not systematically study the effect of different kernel patterns.
- Why unresolved: The paper uses a few specific kernel patterns but does not explore how different pattern choices affect transferability, task-specific effectiveness, or generalization across architectures.
- What evidence would resolve it: Systematic experiments comparing transferability rates and attack success across multiple kernel patterns, different architectures (CNNs, transformers), and tasks (classification, detection, segmentation), potentially revealing which patterns are most transferable or task-specific.

## Limitations

- The reliance on continuous approximations for binary mask optimization may not always yield optimal solutions in the discrete space.
- The approximation gap for structured sparsity (Ω′₀(x,v) ≤ ∥v∥₀) could become significant when group overlaps are substantial, potentially reducing attack effectiveness.
- The computational overhead of the random reinitialization mechanism, while beneficial for escaping local optima, may impact practical deployment efficiency.

## Confidence

- **High Confidence:** The core mechanism of decomposing perturbations into magnitude tensors and binary masks for gradient-based optimization (Mechanism 1) is well-supported by theoretical foundations and experimental results.
- **Medium Confidence:** The effectiveness of unprojected gradients and random reinitialization for escaping local optima (Mechanism 2) shows strong empirical results but may be sensitive to hyperparameter tuning.
- **Medium Confidence:** The structured sparsity approximation using group masks and transposed convolution (Mechanism 3) is innovative but the approximation gap could affect performance in edge cases.

## Next Checks

1. **Approximation Gap Analysis:** Quantify the difference between Ω′₀(x,v) and Ω₀(x) across various group overlap scenarios to understand when the structured sparsity approximation breaks down.
2. **Reinitialization Sensitivity:** Systematically test how different reinitialization tolerances affect both attack success rate and adversarial training stability across multiple datasets and models.
3. **Computational Overhead Benchmark:** Measure the actual runtime overhead introduced by the unprojected gradient and random reinitialization mechanisms compared to standard PGD, particularly for large-scale models like ImageNet-100.