---
ver: rpa2
title: 'Towards Full Delegation: Designing Ideal Agentic Behaviors for Travel Planning'
arxiv_id: '2411.13904'
source_url: https://arxiv.org/abs/2411.13904
tags:
- agent
- travel
- traveler
- think
- preferred
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes APEC Agent Constitution, a framework for evaluating
  LLM agents beyond task accuracy, incorporating proactivity, efficiency, and credibility.
  It introduces APEC-Travel, a travel planning agent optimized to extract personalized
  preferences through multi-round dialogs.
---

# Towards Full Delegation: Designing Ideal Agentic Behaviors for Travel Planning

## Quick Facts
- arXiv ID: 2411.13904
- Source URL: https://arxiv.org/abs/2411.13904
- Reference count: 40
- APEC-Travel surpasses baselines by 20.7% on rule-based metrics and 9.1% on LLM-as-a-Judge scores across the APEC dimensions.

## Executive Summary
This paper introduces APEC Agent Constitution, a framework for evaluating LLM agents in travel planning beyond task accuracy, incorporating proactivity, efficiency, and credibility. The authors propose APEC-Travel, an agent optimized to extract personalized preferences through multi-round dialogs. APEC-Travel is trained using synthetic dialog data generated by Llama3.1-405B-Instruct, fine-tuned with supervised fine-tuning (SFT), and iteratively improved via Direct Preference Optimization (DPO). The agent outperforms baselines by 20.7% on rule-based metrics and 9.1% on LLM-as-a-Judge scores, achieving better accuracy and more concise dialogs while adhering to APEC principles.

## Method Summary
The authors generate synthetic dialog data between travel agents, travelers, and stenographers using Llama3.1-405B-Instruct in BF16 precision. This data is used to train three models (agent, traveler, stenographer) via supervised fine-tuning. The agent is then iteratively improved using Direct Preference Optimization (DPO), where reward scores combine accuracy metrics and LLM-as-a-Judge evaluations of proactivity, efficiency, and credibility. The training process involves generating new dialogs with the latest agent model, scoring them, and updating the agent model to maximize the combined reward.

## Key Results
- APEC-Travel achieves 20.7% improvement on rule-based metrics compared to baselines.
- APEC-Travel scores 9.1% higher on LLM-as-a-Judge evaluations across APEC dimensions.
- Synthetic data generated in BF16 precision yields 3.78 points higher scores than FP8 quantization.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: APEC-Travel's iterative DPO training aligns the agent with human preferences across multiple behavior axes (Accuracy, Proactivity, Efficiency, Credibility).
- Mechanism: Synthetic dialog data is generated by a strong LLM, then the agent is fine-tuned using preference pairs ranked by a reward combining accuracy and agentic scores. Each iteration improves adherence to the APEC principles.
- Core assumption: Reward annotations from LLM-as-a-Judge are reliable proxies for human judgment on agentic behavior.
- Evidence anchors:
  - [abstract]: "Iteratively fine-tuned to follow APEC Agent Constitution, APEC-Travel surpasses baselines by 20.7% on rule-based metrics and 9.1% on LLM-as-a-Judge scores across the constitution axes."
  - [section 3.1]: "We use the LLM-as-a-Judge approach (Bai et al., 2022) to assign scores on plan & priority, proactive and clarification of a given dialog, and then calculate the sum of these scores as the overall agentic score Ra for each dialog."
  - [corpus]: Weak - no direct comparison to human annotations in neighbor papers.
- Break condition: If LLM-as-a-Judge scores diverge significantly from human preferences, the alignment degrades.

### Mechanism 2
- Claim: APEC-Travel achieves better accuracy with fewer dialog rounds by prioritizing critical traveler entries.
- Mechanism: During dialog, the agent focuses on uncovering the most important (critical) persona entries early, using a weighted distance metric for accuracy and tracking efficiency by rounds used.
- Core assumption: Critical entries vary per traveler and can be identified through targeted questioning.
- Evidence anchors:
  - [abstract]: "APEC-Travel achieves strong performance along our Agent Constitution APEC, and can infer hidden personalized travel requests with high accuracy."
  - [section 3.1]: "While there are a lot of dimensions for travelers' preferences, each traveler has critical entries that are way more important than others and thus we use a weighed distance."
  - [corpus]: Missing - no neighbor papers directly discuss critical entry prioritization.
- Break condition: If the set of critical entries is mis-specified or the agent fails to adapt questioning strategy, efficiency drops.

### Mechanism 3
- Claim: Synthetic data generation using Llama3.1-405B-Instruct in BF16 precision yields higher-quality dialogs than FP8 quantization.
- Mechanism: Stronger model generates more realistic traveler personas and dialogs, leading to better seed data for SFT and DPO training.
- Core assumption: Model capacity and precision directly impact the realism and diversity of synthetic dialogs.
- Evidence anchors:
  - [section 4.3]: "the seed data generated by bf16 scores 3.78 points higher (out of a full score of 15) than FP8."
  - [section 4.3]: "agent models trained on bf16-generated data scored 2.25 (SFT) and 2.07 (DPO) points higher."
  - [corpus]: Weak - neighbor papers do not compare model precision effects on dialog quality.
- Break condition: If synthetic data does not reflect real-world traveler diversity, the agent fails to generalize.

## Foundational Learning

- Concept: Large Language Models and their capabilities in multi-turn task-oriented dialog
  - Why needed here: APEC-Travel relies on LLM-generated dialogs for training; understanding model limitations is crucial.
  - Quick check question: What are the typical failure modes of LLMs in multi-turn dialog (e.g., hallucination, context loss)?

- Concept: Preference optimization and reward modeling (e.g., DPO, RLAIF)
  - Why needed here: The iterative DPO training step depends on constructing meaningful reward signals from accuracy and agentic scores.
  - Quick check question: How does DPO differ from standard supervised fine-tuning in aligning with human preferences?

- Concept: Travel planning as a complex decision-making task
  - Why needed here: The APEC framework is instantiated on travel planning, which requires handling diverse constraints and preferences.
  - Quick check question: What are common constraints in travel planning (e.g., budget, accessibility, timing) and how do they affect dialog structure?

## Architecture Onboarding

- Component map: Synthetic dialog generation (agent, traveler, stenographer roles) -> reward annotation -> SFT -> iterative DPO
- Models: Llama3.1-405B-Instruct (data gen), Llama3.1-8B-Instruct (base agent), LLM-as-a-Judge (reward scoring)
- Metrics: Accuracy (weighted), Efficiency (rounds), Agentic scores (plan & priority, proactive, clarification)
- Critical path: Seed data generation -> SFT training -> reward construction -> DPO iterations -> evaluation
- Design tradeoffs:
  - Synthetic vs. real dialog data: Synthetic scales but may lack realism
  - Reward balance (α): Trade-off between accuracy and agentic behavior adherence
  - Fixed vs. recursive DPO: Fixed improves agentic scores; recursive improves accuracy but reduces efficiency
- Failure signatures:
  - Low accuracy: Agent fails to uncover critical entries or stenographer produces invalid JSON
  - Poor efficiency: Agent asks too many non-critical questions or repeats topics
  - Low agentic scores: Agent lacks proactive questioning or clarification attempts
- First 3 experiments:
  1. Compare APEC-Travel to baseline LLMs on a held-out test set measuring all four APEC axes
  2. Vary α in reward construction to see effect on accuracy vs. agentic scores
  3. Test impact of synthetic data precision (BF16 vs FP8) on final agent performance

## Open Questions the Paper Calls Out
- How does APEC-Travel perform when extended to other travel planning tasks like multi-city itineraries with complex constraints?
- How does APEC-Travel's performance scale with the number of traveler personas and the diversity of travel preferences?
- How does APEC-Travel's performance compare to human travel agents in terms of accuracy and efficiency?

## Limitations
- The agent's performance was evaluated primarily on synthetic dialogs, raising questions about real-world generalizability.
- The iterative DPO training assumes that reward scores from LLM-as-a-Judge accurately reflect human preferences, which may not hold.
- The framework's applicability beyond travel planning remains untested.

## Confidence
- High confidence: Synthetic data generation using stronger models in higher precision (BF16) produces measurably better training outcomes, as this was directly measured with clear numerical differences.
- Medium confidence: Overall performance gains, as evaluation relies heavily on synthetic data and LLM-as-a-Judge scoring rather than direct human preference validation.
- Low confidence: That the APEC principles (Accuracy, Proactivity, Efficiency, Credibility) comprehensively capture what makes an ideal travel planning agent, since the evaluation framework itself was developed by the authors and lacks external validation.

## Next Checks
1. **Human Preference Validation**: Conduct a user study where real travelers interact with APEC-Travel and baseline agents, then rate dialogs on APEC dimensions to validate LLM-as-a-Judge scoring alignment.
2. **Cross-Domain Generalization**: Apply the APEC framework to a different task domain (e.g., technical support or personal finance planning) to test whether the four principles generalize beyond travel.
3. **Long-Term Consistency Test**: Run APEC-Travel over extended dialog sessions (10+ rounds) to evaluate whether the efficiency gains observed in shorter dialogs persist, and whether the agent maintains proactive behavior without becoming repetitive.