---
ver: rpa2
title: Learning to Reason via Self-Iterative Process Feedback for Small Language Models
arxiv_id: '2412.08393'
source_url: https://arxiv.org/abs/2412.08393
tags:
- reasoning
- process
- feedback
- answer
- performance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes Self-Iterative Process Feedback (SIPF), a method
  to improve reasoning in small language models (SLMs) by using self-generated process
  feedback instead of costly external annotations. The approach combines Monte Carlo
  Tree Search-based simulation to label step correctness, a process reward model to
  score reasoning paths, and odds ratio preference optimization (ORPO) to align the
  model on these self-generated preferences.
---

# Learning to Reason via Self-Iterative Process Feedback for Small Language Models

## Quick Facts
- arXiv ID: 2412.08393
- Source URL: https://arxiv.org/abs/2412.08393
- Reference count: 29
- Key outcome: Improves Gemma-2B accuracy by 12.43 on GSM8K and 3.95 Pass@1 on MBPP over supervised fine-tuning using self-generated process feedback

## Executive Summary
This paper proposes Self-Iterative Process Feedback (SIPF), a method to improve reasoning in small language models (SLMs) by using self-generated process feedback instead of costly external annotations. The approach combines Monte Carlo Tree Search-based simulation to label step correctness, a process reward model to score reasoning paths, and odds ratio preference optimization (ORPO) to align the model on these self-generated preferences. By iteratively sampling, verifying, and refining, the method enables SLMs to gradually enhance reasoning ability. Experiments on GSM8K, MBPP, MMLU_Math, and HumanEval show that SIPF improves Gemma-2B accuracy by 12.43 on GSM8K and 3.95 Pass@1 on MBPP over supervised fine-tuning, and demonstrates better out-of-domain generalization than existing self-taught and self-refine methods.

## Method Summary
The SIPF method improves SLM reasoning through an iterative self-training loop. First, a fine-tuned SLM samples diverse reasoning paths at high temperature. Second, Monte Carlo Tree Search-based inference simulation labels the correctness of intermediate steps by sampling multiple continuations and checking if they lead to correct final answers. Third, a process reward model is trained on this simulated data to score reasoning paths. Finally, ORPO aligns the SLM using both supervised fine-tuning on chosen paths and preference optimization on the ratio between chosen and rejected paths. This cycle repeats, with each iteration refining the model's reasoning capabilities based on self-generated feedback signals.

## Key Results
- Gemma-2B accuracy improved by 12.43 on GSM8K over supervised fine-tuning
- Gemma-2B Pass@1 improved by 3.95 on MBPP over supervised fine-tuning
- SIPF shows better out-of-domain generalization than self-taught and self-refine methods on HumanEval
- Process feedback methods (SIPF, SIPO) continue to improve with iteration while outcome feedback methods plateau

## Why This Works (Mechanism)

### Mechanism 1
- Claim: High-temperature sampling generates diverse candidate solutions that expose reasoning flaws
- Mechanism: High-temperature sampling increases entropy in the model's output distribution, producing varied reasoning paths for the same question
- Core assumption: The model's internal reasoning capabilities contain both correct and incorrect patterns that can be surfaced through stochastic sampling
- Evidence anchors: [abstract] "First, using fine-tuned SLMs for sampling reasoning paths (CoT); Second, applying sampling-based inference simulation to label the correctness of steps"

### Mechanism 2
- Claim: Monte Carlo Tree Search-based inference simulation provides process-level supervision signals without human annotation
- Mechanism: The simulator samples multiple continuations from each intermediate reasoning step and checks if these continuations lead to correct final answers
- Core assumption: If an intermediate step frequently leads to correct answers in multiple simulations, that step is likely correct
- Evidence anchors: [abstract] "sampling-based inference simulation to label the correctness of steps in some examples"

### Mechanism 3
- Claim: Odds ratio preference optimization (ORPO) effectively aligns the model on process feedback by balancing positive and negative examples
- Mechanism: ORPO incorporates both supervised fine-tuning on chosen paths and preference optimization on the ratio between chosen and rejected paths
- Core assumption: The relative odds between positive and negative examples provide a stable optimization signal that improves reasoning
- Evidence anchors: [abstract] "By combining odds ratio preference optimization (ORPO), we fine-tune and align SLMs using positive and negative signals generated by themselves"

## Foundational Learning

- Concept: Monte Carlo Tree Search
  - Why needed here: Provides the theoretical foundation for sampling-based inference simulation that labels step correctness without human annotation
  - Quick check question: How does MCTS differ from simple random sampling in evaluating intermediate reasoning steps?

- Concept: Preference Optimization
  - Why needed here: Forms the basis for ORPO, which balances learning from both positive examples and the relative preference between good and bad responses
  - Quick check question: What distinguishes ORPO from direct preference optimization (DPO) in terms of the loss function?

- Concept: Reinforcement Learning from Process Feedback
  - Why needed here: Enables the model to improve based on the correctness of intermediate reasoning steps rather than just final outcomes
  - Quick check question: Why might process feedback lead to better generalization than outcome-based feedback?

## Architecture Onboarding

- Component map: SLM -> Simulator -> Process Reward Model -> ORPO Trainer -> Dataset Manager
- Critical path: 1. Sample diverse reasoning paths from current SLM 2. Use simulator to evaluate step correctness via inference simulation 3. Train PRM on simulated data 4. Score all sampled paths using PRM 5. Construct preference dataset with chosen/rejected paths 6. Apply ORPO to align SLM 7. Repeat for next iteration
- Design tradeoffs: Higher temperature sampling vs. computational cost; number of simulation samples per step vs. label accuracy; ORPO weight vs. supervised fine-tuning weight
- Failure signatures: Performance plateaus or degrades after iterations; PRM accuracy remains low; ORPO loss diverges
- First 3 experiments: 1. Run single iteration with baseline SFT to establish performance ceiling 2. Run SIPF with temperature=1 and K=8 simulations to verify basic mechanism 3. Test different ORPO weights (0.01, 0.1, 1.0) to find optimal balance between SFT and preference optimization objectives

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does SIPF scale to larger open-source models (7B+ parameters) compared to current experiments with models ≤2B?
- Basis in paper: [explicit] The paper explicitly states that "The proposed method has only been experimented on pre-trained models with sizes ≤ 2B, and its feasibility has not been validated on larger open-source models (≥ 7B)."
- Why unresolved: The paper's experiments were limited to models with 2B parameters or less, leaving uncertainty about whether the self-iterative process feedback approach would be equally effective for larger models.
- What evidence would resolve it: Running SIPF experiments on 7B+ parameter models like Llama-2-7B, Mistral-7B, or Qwen-7B would provide evidence of whether the method scales effectively to larger models.

### Open Question 2
- Question: How does the quality and reliability of the process reward model (PRM) affect overall SIPF performance?
- Basis in paper: [explicit] The paper discusses that "the accuracy of the reward model is crucial for enhancing SIPF's performance" and presents a comparison of different reward models in Table 3
- Why unresolved: While the paper demonstrates that PRMs are more accurate than ORMs, it doesn't explore how variations in PRM quality affect SIPF performance or whether alternative reward model architectures could further improve results.
- What evidence would resolve it: Systematic experiments varying the quality of reward models and measuring their impact on SIPF performance would clarify this relationship.

### Open Question 3
- Question: What is the optimal stopping point for self-iteration in SIPF, and how does over-iteration affect performance?
- Basis in paper: [explicit] The paper shows that "RFT and SIOF, based on outcome feedback, show degraded performance after reaching their capacity limits" while process feedback methods "continue to improve," but doesn't establish clear stopping criteria
- Why unresolved: The paper demonstrates continuous improvement with iteration for process feedback methods but doesn't identify when additional iterations might lead to diminishing returns or performance degradation
- What evidence would resolve it: Experiments tracking performance across many more iteration rounds than tested would reveal optimal stopping points and potential over-iteration effects.

## Limitations
- Reliance on self-generated feedback raises concerns about potential confirmation bias
- Monte Carlo Tree Search-based simulation may struggle with complex reasoning tasks requiring long-term dependencies
- Method's performance appears highly dependent on the quality of the simulator model
- Only validated on models ≤2B parameters, with scalability to larger models uncertain

## Confidence

**Confidence Labels:**
- High confidence in the core mechanism of using self-generated process feedback to improve reasoning
- Medium confidence in the generalization claims across different domains and model sizes
- Medium confidence in the scalability of the iterative process given computational constraints
- Low confidence in the method's robustness to noisy or ambiguous reasoning paths

## Next Checks

1. **Simulator Quality Validation**: Systematically vary the strength of the simulator model to quantify the impact on final reasoning performance and identify the minimum viable simulator quality threshold.

2. **Iteration Saturation Analysis**: Track performance changes across multiple SIPF iterations (beyond what was reported) to determine if there's a point of diminishing returns or potential degradation due to overfitting to the iterative dataset.

3. **Cross-Domain Robustness Test**: Evaluate SIPF-trained models on reasoning tasks from completely different domains than those used in training to better assess true generalization capabilities versus memorization of reasoning patterns.