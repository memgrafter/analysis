---
ver: rpa2
title: 'RETQA: A Large-Scale Open-Domain Tabular Question Answering Dataset for Real
  Estate Sector'
arxiv_id: '2412.10104'
source_url: https://arxiv.org/abs/2412.10104
tags:
- query
- table
- labels
- real
- estate
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: RETQA introduces the first large-scale open-domain Chinese Tabular
  Question Answering dataset for real estate, containing 4,932 tables and 20,762 QA
  pairs across 16 intents. The dataset features long-table structures (avg.
---

# RETQA: A Large-Scale Open-Domain Tabular Question Answering Dataset for Real Estate Sector

## Quick Facts
- arXiv ID: 2412.10104
- Source URL: https://arxiv.org/abs/2412.10104
- Reference count: 14
- Primary result: SLUTQA framework improves LLM performance by 1.6-4.42% in table exact match accuracy and over 20% in executable code ratio for SQL answers

## Executive Summary
RETQA introduces the first large-scale open-domain Chinese Tabular Question Answering dataset for real estate, containing 4,932 tables and 20,762 QA pairs across 16 intents. The dataset features long-table structures (avg. 252.9 rows) and open-domain retrieval challenges, making it more difficult than existing TQA benchmarks. To address these challenges, the authors propose SLUTQA, a framework that integrates large language models with spoken language understanding (SLU) tasks through in-context learning. SLUTQA improves LLM performance by leveraging SLU labels for better retrieval accuracy and answer generation.

## Method Summary
The paper introduces RETQA, a large-scale open-domain Chinese Tabular Question Answering dataset for real estate, containing 4,932 tables and 20,762 QA pairs. To address the challenges of long-table structures and open-domain retrieval, the authors propose SLUTQA, a framework that integrates large language models with spoken language understanding (SLU) tasks through in-context learning. SLUTQA consists of three modules: SLU for intent and slot prediction, SR for table retrieval using SLU-generated summaries, and SFA for table simplification and answer generation. The framework uses in-context learning with few-shot examples to adapt LLMs to the TQA task without requiring fine-tuning.

## Key Results
- SLUTQA improves table exact match accuracy by 1.6-4.42% across Qwen2 and GLM4 models
- The framework achieves over 20% improvement in executable code ratio for SQL answers
- RETQA dataset contains tables with an average of 252.9 rows, making it significantly more challenging than existing TQA benchmarks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: SLU labels improve LLM retrieval accuracy by replacing original queries with query summaries
- Mechanism: SLU module predicts intent and slot labels → SR module uses these labels to generate table caption-like summaries → BM25 retrieves based on summary instead of raw query
- Core assumption: Table captions contain structured, domain-relevant keywords that align with real estate entities better than natural language queries
- Evidence anchors:
  - [abstract]: "SLU labels enable more accurate parsing of user intent and relevant information, thereby enabling more precise retrieval"
  - [section]: "SR-generated summary, rather than the original query, is used to retrieve the table caption via BM25"
  - [corpus]: Weak - no direct retrieval comparison studies in corpus

### Mechanism 2
- Claim: SLU-based filtering simplifies long tables by removing irrelevant rows/columns before LLM processing
- Mechanism: SFA module removes rows/columns not matching predicted slots → simplified table fed to LLM → more focused answer generation
- Core assumption: Long tables contain mostly irrelevant data for any given query, and removing this noise improves LLM performance
- Evidence anchors:
  - [abstract]: "SFA module generates accurate SQL statements or refined Markdown answers based on the predicted SLU labels"
  - [section]: "SFA module is designed to filter out rows and columns that are irrelevant to the query"
  - [corpus]: Weak - no ablation showing performance without filtering

### Mechanism 3
- Claim: In-context learning with SLU-labeled examples enables few-shot adaptation of LLMs to TQA tasks
- Mechanism: Few-shot examples (22 for SLU, 5-10 for SR/SFA) provide demonstration of SLU labeling and table processing → LLM learns task patterns without fine-tuning
- Core assumption: LLMs can learn task-specific patterns from a small number of well-constructed examples
- Evidence anchors:
  - [abstract]: "Extensive experiments demonstrate that SLUTQA significantly improves the performance of large language models on RETQA by in-context learning"
  - [section]: "We randomly select 22 examples from the training set to serve as prompt examples for the LLMs"
  - [corpus]: Weak - no comparison with different few-shot sizes

## Foundational Learning

- Concept: In-context learning (ICL) and few-shot prompting
  - Why needed here: RETQA requires adapting LLMs to specialized real estate domain without full fine-tuning
  - Quick check question: What's the difference between few-shot learning and in-context learning?

- Concept: Spoken Language Understanding (SLU) tasks
  - Why needed here: RETQA includes intent and slot annotations that help parse complex real estate queries
  - Quick check question: How do intent and slot labels differ in task-oriented dialogue systems?

- Concept: Table retrieval using BM25 and semantic matching
  - Why needed here: Open-domain nature requires retrieving relevant tables from entire dataset based on query
  - Quick check question: Why might BM25 be insufficient for real estate table retrieval?

## Architecture Onboarding

- Component map: Query → SLU → SR → Table Retrieval → SFA → Answer
- Critical path: Natural language query → SLU module (intent/slot prediction) → SR module (query summary generation) → BM25 retrieval → SFA module (table simplification/answer generation) → Final answer
- Design tradeoffs:
  - Fine-tuning vs ICL: Fine-tuning gives better SLU performance but requires labeled data; ICL is more flexible but less accurate
  - Table simplification vs completeness: Removing irrelevant data speeds processing but risks losing important information
  - SQL vs Markdown output: SQL is more structured but requires accurate parsing; Markdown is more flexible but less precise
- Failure signatures:
  - Low retrieval precision: Check if SLU labels capture correct entities; verify table captions are descriptive
  - Poor table simplification: Verify slot predictions include all necessary entities; check if simplification removes critical rows
  - Incorrect SQL generation: Check if intent classification is correct; verify slot extraction accuracy
- First 3 experiments:
  1. Test SLU module accuracy on validation set with both BERT and ICL approaches
  2. Compare retrieval performance with and without SLU-generated summaries
  3. Measure table simplification effectiveness by comparing row/column counts before/after SFA processing

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the SLUTQA framework perform on datasets outside the real estate domain?
- Basis in paper: Inferred - The paper demonstrates SLUTQA's effectiveness on RETQA but doesn't explore its generalizability to other domains.
- Why unresolved: The framework was specifically designed and tested for real estate data, so its performance on other domains with different table structures and query patterns is unknown.
- What evidence would resolve it: Testing SLUTQA on other open-domain TQA datasets like Spider or WikiTableQuestions and comparing performance metrics.

### Open Question 2
- Question: What is the impact of varying the number of in-context learning examples on SLUTQA's performance?
- Basis in paper: Inferred - The paper uses 5 examples for SR and SFA modules but doesn't explore how performance changes with different numbers of examples.
- Why unresolved: The optimal number of examples for in-context learning may vary depending on task complexity and could affect performance.
- What evidence would resolve it: Systematic experiments varying the number of in-context examples (e.g., 1, 3, 5, 10) and measuring impact on retrieval accuracy and answer quality.

### Open Question 3
- Question: How does SLUTQA handle queries involving more than two tables?
- Basis in paper: Explicit - The paper mentions "multi-table queries" but only provides statistics for queries involving up to two tables.
- Why unresolved: The dataset contains 23 templates for multi-table queries, but the maximum number of tables involved isn't specified, and the framework's effectiveness for higher numbers of tables is unknown.
- What evidence would resolve it: Analyzing performance on queries involving 3+ tables and identifying limitations or bottlenecks in the framework.

## Limitations
- Limited exploration of SLUTQA's generalizability to other domains beyond real estate
- No ablation studies showing performance impact of table simplification module
- Uncertainty about optimal number of in-context learning examples for different task components

## Confidence
- Medium confidence in SLU-based retrieval improvement claims - supported by metrics but lacking direct comparison with alternative retrieval methods
- Medium confidence in table simplification benefits - reported improvements but no ablation studies or failure analysis
- High confidence in overall framework architecture - clear methodology and consistent experimental design

## Next Checks
1. **Direct SLU comparison**: Run experiments comparing ICL-based SLU with fine-tuned BERT SLU on the RETQA validation set to quantify accuracy trade-offs
2. **Retrieval ablation study**: Evaluate retrieval performance using original queries vs SLU summaries on a subset of RETQA tables to isolate the contribution of SLU-based retrieval
3. **Simplification impact analysis**: Create a detailed analysis of table simplification by measuring row/column reduction ratios and identifying cases where simplification removed critical information needed for correct answers