---
ver: rpa2
title: What Should Baby Models Read? Exploring Sample-Efficient Data Composition on
  Model Performance
arxiv_id: '2411.06672'
source_url: https://arxiv.org/abs/2411.06672
tags:
- dataset
- language
- tinystories
- datasets
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigated the impact of dataset composition on small
  language model performance in sample-efficient training settings. Models ranging
  from 18M to 705M parameters were trained on datasets limited to 10 million words,
  including CHILDES (child-directed speech), Gutenberg (classic books), TinyStories
  (synthetic data), and a mixed dataset.
---

# What Should Baby Models Read? Exploring Sample-Efficient Data Composition on Model Performance

## Quick Facts
- arXiv ID: 2411.06672
- Source URL: https://arxiv.org/abs/2411.06672
- Authors: Hong Meng Yam; Nathan J Paek
- Reference count: 6
- Models ranging from 18M to 705M parameters were trained on datasets limited to 10 million words, including CHILDES, Gutenberg, TinyStories, and a mixed dataset

## Executive Summary
This study investigated how dataset composition affects small language model performance when training with sample efficiency constraints (10 million words). The key finding reveals that optimal dataset choice depends critically on model size: smaller models (GPT2-18M, GPT2-44M) perform best with diverse datasets like Mix, while larger models (GPT2-97M, GPT2-705M, Llama-360M) benefit more from complex datasets like Gutenberg. CHILDES and TinyStories consistently underperformed across all model sizes. Readability analysis showed CHILDES had the highest simplicity score (FRE=115.70) while Gutenberg was most complex (FRE=87.49). These results suggest that neither child-directed speech nor simplified stories are optimal for language models of any size, highlighting the importance of matching dataset composition to model capacity for effective sample-efficient training.

## Method Summary
The study trained GPT2 models of varying sizes (18M, 44M, 97M, 705M parameters) and a Llama-360M model on four different datasets limited to 10 million words: CHILDES (child-directed speech), Gutenberg (classic books), TinyStories (synthetic data), and Mix (combined dataset). Models were trained for 4 epochs using consistent hyperparameters through the BabyLlama repository. Performance was evaluated using the BabyLM evaluation suite including BLiMP (linguistic minimal pairs), EWoK (world knowledge), and GLUE benchmarks. Readability was assessed using FRE, ARI, and Gunning Fog scores to quantify dataset complexity.

## Key Results
- Smaller models (GPT2-18M, GPT2-44M) achieved best performance with diverse datasets like Mix, while larger models (GPT2-97M, GPT2-705M, Llama-360M) excelled on complex datasets like Gutenberg
- CHILDES and TinyStories datasets consistently underperformed across all model sizes in BLiMP, EWoK, and GLUE evaluations
- Readability analysis showed CHILDES had the highest FRE score (115.70) indicating simplest grammatical structures, while Gutenberg was most complex (FRE=87.49)
- CHILDES datasets converged faster than Gutenberg or Mix for smaller models, demonstrating that simpler text enables quicker training convergence

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Smaller models perform better on diverse datasets because they lack capacity to fully exploit complex datasets.
- Mechanism: Limited parameter count causes high bias when exposed to high-complexity data, leading to underfitting. Diverse datasets provide broader but simpler exposure that fits the model's representational capacity.
- Core assumption: Model capacity directly limits ability to extract patterns from complex linguistic structures.
- Evidence anchors:
  - [abstract] "smaller models (e.g., GPT2-18M and GPT2-44M) benefit from training on diverse datasets like Mix"
  - [section 4.1] "Due to their limited capacity, small models cannot fully learn from the complexity of the dataset. They oversimplify the language patterns, leading to high bias and poor generalization."
  - [corpus] Weak - corpus provides neighbor papers but no direct evidence for this mechanism

### Mechanism 2
- Claim: Larger models perform better on complex datasets because they have sufficient capacity to extract nuanced patterns.
- Mechanism: Increased parameter count enables representation of intricate syntactic structures and semantic relationships found in complex text like Gutenberg.
- Core assumption: Model capacity scales linearly with ability to learn from dataset complexity.
- Evidence anchors:
  - [abstract] "larger models (e.g., GPT2-97M, GPT2-705M, and LLaMA-360M) perform better when trained on more complex and rich datasets like Gutenberg"
  - [section 4.1] "larger models... possess greater capacity to learn and represent complex patterns due to their increased number of parameters"
  - [corpus] Weak - corpus provides neighbor papers but no direct evidence for this mechanism

### Mechanism 3
- Claim: Dataset convergence speed depends on text simplicity measured by FRE score.
- Mechanism: Simpler text with lower perplexity creates smoother loss landscape, enabling faster convergence during training.
- Core assumption: Readability metrics correlate with training difficulty and convergence speed.
- Evidence anchors:
  - [section 4.2] "CHILDES converged faster than either then Gutenberg or the Mix datasets for both GPT2-44M and GPT2-18M models" and "The higher FRE score (115.70) of this child-directed speech dataset indicates simpler grammatical structures"
  - [section 2] FRE scores show CHILDES (115.70) is simplest, Gutenberg (87.49) is most complex
  - [corpus] Weak - corpus provides neighbor papers but no direct evidence for this mechanism

## Foundational Learning

- Concept: Readability metrics (FRE, ARI, Gunning Fog)
  - Why needed here: These metrics quantify dataset complexity, which determines optimal model-dataset pairing
  - Quick check question: Which dataset has the highest FRE score and what does that indicate about its complexity?

- Concept: Model capacity vs dataset complexity trade-off
  - Why needed here: Determines which models perform best on which datasets in sample-efficient settings
  - Quick check question: Why do smaller models underperform on Gutenberg despite its linguistic richness?

- Concept: Sample efficiency in language model training
  - Why needed here: The study operates under strict word limits (10M words), making data choice critical
  - Quick check question: How does the 10M word limit compare to typical language model pretraining datasets?

## Architecture Onboarding

- Component map: GPT2 models (18M, 44M, 97M, 705M parameters) + LLaMA-360M model; datasets (CHILDES, Gutenberg, Mix, TinyStories); evaluation suite (BLiMP, EWoK, GLUE)
- Critical path: Dataset preprocessing → Tokenizer training → Model training (4 epochs) → Evaluation on benchmarks
- Design tradeoffs: Fixed hyperparameters across experiments vs. tuned per model-dataset pair; limited training epochs vs. potential overfitting; small word limits vs. representation quality
- Failure signatures: Underperformance on BLiMP/EWoK indicates insufficient syntactic/semantic complexity in training data; faster convergence on CHILDES indicates oversimplified training data
- First 3 experiments:
  1. Train GPT2-18M on Mix dataset to establish baseline for small model performance
  2. Train GPT2-705M on Gutenberg dataset to establish baseline for large model performance
  3. Train same model size on CHILDES to observe underfitting patterns and convergence behavior

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal dataset composition for small language models of different sizes, and how does this vary with model capacity?
- Basis in paper: [explicit] The paper explores how different dataset compositions affect model performance across various model sizes, finding that smaller models benefit from diverse datasets like Mix, while larger models perform better with more complex datasets like Gutenberg.
- Why unresolved: The study provides initial insights but does not determine a precise optimal dataset composition for each model size, nor does it explore a wide range of potential datasets beyond CHILDES, Gutenberg, TinyStories, and Mix.
- What evidence would resolve it: Systematic experimentation with a broader range of datasets, including news articles, scientific texts, and conversational data, combined with detailed analysis of how model performance varies with different dataset compositions and sizes, would help identify optimal compositions.

### Open Question 2
- Question: How do different evaluation benchmarks affect the perceived performance of models trained on child-directed speech or simplified stories?
- Basis in paper: [explicit] The paper notes that neither CHILDES nor TinyStories datasets performed well on the BLiMP or EWoK evaluation suites, suggesting a potential mismatch between the training data and the benchmarks used.
- Why unresolved: The study highlights a discrepancy but does not investigate whether alternative benchmarks or evaluation methods might better capture the strengths of models trained on these datasets.
- What evidence would resolve it: Conducting evaluations using a variety of benchmarks that assess different linguistic and cognitive skills, such as coherent text generation or task-specific evaluations, would provide a more comprehensive understanding of model performance.

### Open Question 3
- Question: How does curriculum learning with child-directed speech data impact the sample efficiency and performance of small language models?
- Basis in paper: [explicit] The paper references previous work on curriculum learning with child data but notes the lack of success in effectively capturing the language acquisition process, suggesting a need for more sophisticated approaches.
- Why unresolved: The study does not explore curriculum learning as a method to enhance the performance of models trained on child-directed speech data, leaving open the question of whether this approach could improve sample efficiency.
- What evidence would resolve it: Implementing and evaluating curriculum learning strategies that incorporate child-directed speech data, comparing their performance to models trained without such strategies, would clarify the potential benefits and limitations of this approach.

## Limitations

- The findings are based on a sample-efficient training paradigm with a strict 10 million word limit, which may not generalize to typical large-scale pretraining scenarios
- The evaluation suite focuses on specific benchmarks (BLiMP, EWoK, GLUE) that may not capture all aspects of language model capabilities, particularly long-form text generation or task-specific performance
- Dataset composition effects observed could be specific to this constrained setting rather than representing fundamental principles of model-dataset alignment

## Confidence

**High Confidence**: The observation that dataset choice impacts performance differently across model sizes is well-supported by the experimental results. The consistent pattern across multiple model sizes and datasets provides strong evidence for this claim.

**Medium Confidence**: The mechanism explanations linking model capacity to dataset complexity handling are plausible but rely on indirect evidence. While the convergence speed differences and benchmark performance patterns support these mechanisms, they are not definitively proven.

**Low Confidence**: The generalizability of these findings beyond the sample-efficient setting remains uncertain. The specific dataset combinations and size constraints may create conditions that don't reflect broader pretraining practices.

## Next Checks

1. **Dataset Complexity Gradient**: Conduct experiments varying dataset complexity systematically (e.g., intermediate complexity datasets) to test whether the performance drop between Mix and Gutenberg for small models is gradual or abrupt.

2. **Extended Training Duration**: Run longer training schedules (beyond 4 epochs) to determine if convergence speed differences persist and whether models can overcome initial complexity barriers with more training time.

3. **Cross-Domain Evaluation**: Test models on additional evaluation tasks beyond BLiMP, EWoK, and GLUE to verify that the dataset composition effects generalize across different linguistic and reasoning capabilities.