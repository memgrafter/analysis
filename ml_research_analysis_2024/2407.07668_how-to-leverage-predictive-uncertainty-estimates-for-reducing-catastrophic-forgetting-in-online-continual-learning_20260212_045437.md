---
ver: rpa2
title: How to Leverage Predictive Uncertainty Estimates for Reducing Catastrophic
  Forgetting in Online Continual Learning
arxiv_id: '2407.07668'
source_url: https://arxiv.org/abs/2407.07668
tags:
- uncertainty
- learning
- memory
- samples
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work investigates how predictive uncertainty estimates can
  help reduce catastrophic forgetting (CF) in online continual learning. It compares
  different uncertainty scores (Least Confidence, Margin Sampling, Ratio of Confidence,
  Entropy, and a new Bregman Information-based measure) for populating a fixed-size
  memory buffer with samples from past tasks.
---

# How to Leverage Predictive Uncertainty Estimates for Reducing Catastrophic Forgetting in Online Continual Learning

## Quick Facts
- arXiv ID: 2407.07668
- Source URL: https://arxiv.org/abs/2407.07668
- Authors: Giuseppe Serra; Ben Werner; Florian Buettner
- Reference count: 29
- Key outcome: Selecting samples with low epistemic uncertainty for replay consistently outperforms selecting high-uncertainty samples in reducing catastrophic forgetting while maintaining competitive accuracy

## Executive Summary
This work investigates how predictive uncertainty estimates can help reduce catastrophic forgetting in online continual learning. The authors propose selecting the most representative (easiest-to-remember) samples—those with low epistemic uncertainty—for replay from a fixed-size memory buffer, rather than selecting marginal samples. They introduce a new Bregman Information-based uncertainty measure that estimates epistemic uncertainty through a bias-variance decomposition of the loss, showing it reduces forgetting more effectively than common uncertainty scores like entropy or margin sampling.

## Method Summary
The method uses a memory-based replay approach where samples are selected for the replay buffer based on predictive uncertainty estimates. A slim ResNet18 model is trained in an online continual learning setting with incremental class learning tasks. The memory manager uses class-balanced reservoir sampling combined with uncertainty-based selection. Six uncertainty metrics are compared: Least Confidence, Margin Sampling, Ratio of Confidence, Entropy, Random Margin, and the proposed Bregman Information (BI) measure. Test-time augmentation is used to estimate uncertainty, and the bottom-k strategy selects the k most representative (lowest uncertainty) samples for memory storage.

## Key Results
- Selecting low-epistemic-uncertainty samples for replay consistently outperforms selecting high-uncertainty samples in reducing catastrophic forgetting
- The proposed BI-based uncertainty estimate further reduces forgetting compared to confidence-based (aleatoric) uncertainty scores
- Class-balanced memory management based on predictive uncertainty improves continual learning under imbalanced data conditions

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Selecting samples with low epistemic uncertainty (high representativeness) for replay reduces catastrophic forgetting more effectively than selecting high-uncertainty (marginal) samples.
- Mechanism: Low-epistemic-uncertainty samples are close to high data density regions, making them easier to recall and preserving class decision boundaries. High-epistemic-uncertainty samples are closer to decision boundaries or outliers, which may distort learned boundaries.
- Core assumption: Epistemic uncertainty can be reliably estimated from model predictions (e.g., via TTA + BI), and low epistemic uncertainty corresponds to representativeness of class distribution.
- Evidence anchors:
  - [abstract] "selecting the most representative (easiest-to-remember) samples—those with low epistemic uncertainty—consistently outperforms selecting marginal samples in mitigating CF"
  - [section] "we hypothesize it would be beneficial to focus on samples with a low epistemic uncertainty"
  - [corpus] Weak evidence: No direct neighbor studies confirm epistemic uncertainty sampling, but related works on uncertainty-aware memory management exist.
- Break condition: If epistemic uncertainty estimates are miscalibrated or if data distribution changes abruptly, the representativeness assumption may fail.

### Mechanism 2
- Claim: Using Bregman Information (BI) as an epistemic uncertainty estimate reduces catastrophic forgetting more than confidence-based (aleatoric) scores.
- Mechanism: BI estimates the variance of the loss via a bias-variance decomposition, focusing on epistemic uncertainty rather than aleatoric noise. This captures model uncertainty about patterns, not data inherent noise.
- Core assumption: BI provides a statistically grounded estimate of epistemic uncertainty that is more sensitive to changes in the predictive space than entropy or margin-based scores.
- Evidence anchors:
  - [abstract] "we propose an alternative method for estimating predictive uncertainty via the generalised variance induced by the negative log-likelihood"
  - [section] "we interpret BI as a measure of epistemic uncertainty that is statistically well grounded in a bias-variance decomposition"
  - [corpus] No direct neighbor evidence, but related uncertainty estimation works exist.
- Break condition: If the bias-variance decomposition assumption does not hold for the loss function or model, BI may not accurately reflect epistemic uncertainty.

### Mechanism 3
- Claim: Class-balanced memory management based on predictive uncertainty improves continual learning under imbalanced data.
- Mechanism: Ensures each class has a fair representation in the replay buffer by selecting the most representative samples per class, preventing bias toward frequent classes.
- Core assumption: Class-balanced sampling combined with epistemic uncertainty selection prevents domination by majority classes while preserving representativeness.
- Evidence anchors:
  - [section] "we introduce a class-balanced memory management based on predictive uncertainty estimates"
  - [section] "class-imbalance may further deteriorate the predictive performance of the framework"
  - [corpus] Weak evidence: Neighbors discuss memory management but not specifically class-balanced uncertainty sampling.
- Break condition: If class imbalance is extreme or if uncertainty estimates are noisy, the balance may be lost.

## Foundational Learning

- Concept: Continual Learning (CL)
  - Why needed here: The paper studies how to reduce catastrophic forgetting in a setting where tasks arrive sequentially and data distribution shifts.
  - Quick check question: What is the difference between online-CL and offline-CL in terms of data access?

- Concept: Predictive Uncertainty
  - Why needed here: Uncertainty scores guide which samples to store in memory for replay, aiming to select the most representative samples.
  - Quick check question: What is the difference between aleatoric and epistemic uncertainty?

- Concept: Bregman Information (BI)
  - Why needed here: BI is proposed as a measure of epistemic uncertainty derived from a bias-variance decomposition of the loss.
  - Quick check question: How does BI differ from entropy or margin sampling in terms of information used?

## Architecture Onboarding

- Component map: Model -> Uncertainty Estimator -> Memory Buffer -> Memory Manager -> Training Loop
- Critical path:
  1. Get mini-batch from stream
  2. Sample replay set from memory
  3. Train model on combined batch + replay
  4. Update memory with new batch (select samples via uncertainty + class balance)
- Design tradeoffs:
  - Memory size vs. forgetting: Larger memory reduces forgetting but increases compute
  - Uncertainty metric choice: BI is more principled but costlier than entropy
  - Class balance vs. diversity: Class balance ensures fairness, diversity sampling ensures coverage
- Failure signatures:
  - High forgetting despite memory usage → uncertainty selection not effective or memory too small
  - Slow training → TTA overhead too high
  - Class imbalance in memory → class-balanced sampling broken
- First 3 experiments:
  1. Run ER baseline (random memory) on CIFAR-10 and measure forgetting
  2. Replace random selection with BI-bottom-k and compare forgetting
  3. Test on long-tailed CIFAR-100 and measure class-specific forgetting

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of BI-based uncertainty estimation compare to other uncertainty estimation methods when using a larger or smaller memory buffer size?
- Basis in paper: [explicit] The paper tests memory sizes of 500 and 1000 for CIFAR-10 and CIFAR-100, but does not explore other buffer sizes.
- Why unresolved: The paper only tests a limited range of memory buffer sizes, so it is unclear how the performance of BI-based uncertainty estimation would scale with different buffer sizes.
- What evidence would resolve it: Running experiments with a wider range of memory buffer sizes (e.g., 100, 200, 5000) and comparing the performance of BI-based uncertainty estimation to other methods.

### Open Question 2
- Question: How does the performance of BI-based uncertainty estimation compare to other uncertainty estimation methods on datasets with different characteristics (e.g., number of classes, image size, class imbalance)?
- Basis in paper: [inferred] The paper tests BI-based uncertainty estimation on CIFAR-10, CIFAR-100, and BloodCell datasets, but does not explore other datasets with different characteristics.
- Why unresolved: The paper only tests BI-based uncertainty estimation on a limited number of datasets, so it is unclear how the method would perform on datasets with different characteristics.
- What evidence would resolve it: Running experiments on a wider range of datasets with different characteristics and comparing the performance of BI-based uncertainty estimation to other methods.

### Open Question 3
- Question: How does the performance of BI-based uncertainty estimation compare to other uncertainty estimation methods when using different neural network architectures?
- Basis in paper: [explicit] The paper uses a slim version of Resnet18 for all experiments, but does not explore other architectures.
- Why unresolved: The paper only tests BI-based uncertainty estimation with one neural network architecture, so it is unclear how the method would perform with other architectures.
- What evidence would resolve it: Running experiments with different neural network architectures (e.g., VGG, DenseNet) and comparing the performance of BI-based uncertainty estimation to other methods.

### Open Question 4
- Question: How does the performance of BI-based uncertainty estimation compare to other uncertainty estimation methods when using different online continual learning algorithms?
- Basis in paper: [inferred] The paper uses Experience Replay (ER) as the baseline algorithm, but does not explore other online continual learning algorithms.
- Why unresolved: The paper only tests BI-based uncertainty estimation with one online continual learning algorithm, so it is unclear how the method would perform with other algorithms.
- What evidence would resolve it: Running experiments with different online continual learning algorithms (e.g., Gradient Episodic Memory, iCaRL) and comparing the performance of BI-based uncertainty estimation to other methods.

## Limitations
- The test-time augmentation procedure is underspecified, potentially affecting reproducibility
- No direct empirical comparison with state-of-the-art CL methods beyond ER
- The claim that BI is superior due to its statistical grounding lacks ablation study support

## Confidence
- High confidence: The experimental setup and evaluation protocol are clearly described and reproducible
- Medium confidence: The superiority of low-epistemic-uncertainty sampling over high-uncertainty sampling is supported by experiments but lacks theoretical justification
- Low confidence: The claim that BI is superior to other uncertainty metrics due to its statistical grounding, as no ablation study compares BI with other epistemic uncertainty estimates

## Next Checks
1. Replicate the CIFAR-10 experiments with a complete implementation of the TTA procedure and verify the reported accuracy/forgetting improvements
2. Conduct an ablation study comparing BI against other epistemic uncertainty estimates (e.g., mutual information) to isolate the effect of the uncertainty metric choice
3. Test the method on a CL benchmark with multiple competing methods (e.g., Split-CIFAR with comparisons to MIR, PR, or ASER) to establish relative performance