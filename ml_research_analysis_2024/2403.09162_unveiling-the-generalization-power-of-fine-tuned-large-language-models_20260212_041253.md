---
ver: rpa2
title: Unveiling the Generalization Power of Fine-Tuned Large Language Models
arxiv_id: '2403.09162'
source_url: https://arxiv.org/abs/2403.09162
tags:
- tasks
- fine-tuned
- fine-tuning
- llms
- generation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper examines how fine-tuning affects the generalization
  ability of large language models (LLMs). It evaluates Llama-2 models fine-tuned
  on various datasets across five language tasks: summarization, question generation,
  sentiment classification, paraphrase detection, and natural language inference.'
---

# Unveiling the Generalization Power of Fine-Tuned Large Language Models

## Quick Facts
- **arXiv ID**: 2403.09162
- **Source URL**: https://arxiv.org/abs/2403.09162
- **Reference count**: 21
- **Primary result**: Fine-tuning on classification tasks leads to better out-of-domain generalization than generation tasks, while integrating in-context learning during fine-tuning on generation tasks (FTICL) can enhance generalization.

## Executive Summary
This paper investigates how fine-tuning affects the generalization ability of large language models across five language tasks. The study finds that fine-tuning on classification tasks leads to better out-of-domain generalization than on generation tasks, and that fine-tuned models often underperform compared to zero-shot settings when using in-context learning. Interestingly, integrating in-context learning during fine-tuning on generation tasks (FTICL) can improve out-of-domain and cross-task generalization, though this approach does not yield similar benefits for classification tasks. The results suggest that fine-tuning strategies should be tailored based on task type to preserve or enhance generalization.

## Method Summary
The study uses Llama-2-7b as the base model and fine-tunes it on subsets of 2,000, 4,000, and 6,000 samples for each task. Classification tasks are converted to text generation by predicting labels in text form. Fine-tuning uses AdamW optimizer with a learning rate of 0.002 for 2 epochs, with generation length set to 60 for generation tasks and 5 for classification tasks. The study evaluates models on in-domain, out-of-domain, and cross-task datasets, and explores FTICL for generation tasks by prepending in-context examples during training.

## Key Results
- Fine-tuning on classification tasks leads to better out-of-domain generalization than fine-tuning on generation tasks
- Fine-tuned models often underperform using in-context learning compared to zero-shot settings
- FTICL can enhance out-of-domain and cross-task generalization for generation tasks but not for classification tasks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Fine-tuning on classification tasks leads to better out-of-domain generalization than fine-tuning on generation tasks.
- Mechanism: Classification tasks have a predefined and limited output space, allowing fine-tuned models to apply their adapted knowledge more easily to new domains. Generation tasks, on the other hand, have an expansive and open-ended output space, making it harder for fine-tuned models to adapt to out-of-domain data distributions.
- Core assumption: The nature of the task output space significantly impacts the model's ability to generalize to new domains.
- Evidence anchors:
  - [abstract]: "Our main findings reveal that models fine-tuned on generation and classification tasks exhibit dissimilar behaviors in generalizing to different domains and tasks."
  - [section]: "The diverging effects of fine-tuning on generation and classification tasks for out-of-domain testing may originate from the difference in task output space constraints."
  - [corpus]: Weak corpus evidence. The corpus does not directly support this specific mechanism.
- Break condition: If the task output space constraint is not the primary factor influencing out-of-domain generalization, or if other factors such as dataset characteristics play a more significant role.

### Mechanism 2
- Claim: Integrating in-context learning (ICL) during fine-tuning on generation tasks (FTICL) can enhance the model's generalization ability.
- Mechanism: FTICL tends to deviate less from the original LLM than vanilla fine-tuning, preserving more general knowledge inherent in the LLM. The provided in-context examples encourage the LLM to leverage its existing knowledge to solve new tasks, rather than relying solely on task-specific patterns learned during fine-tuning.
- Core assumption: Preserving general knowledge from the original LLM is crucial for maintaining generalization ability.
- Evidence anchors:
  - [abstract]: "Intriguingly, we observe that integrating the in-context learning strategy during fine-tuning on generation tasks can enhance the model's generalization ability."
  - [section]: "We provide one hypothesis that could drive the success of FTICL in enhancing LLMs' generalization: FTICL tends to deviate less from the original LLM than vanilla fine-tuning."
  - [corpus]: Weak corpus evidence. The corpus does not directly support this specific mechanism.
- Break condition: If the preservation of general knowledge is not the primary factor contributing to improved generalization, or if other aspects of FTICL (e.g., regularization effect) play a more significant role.

### Mechanism 3
- Claim: Fine-tuning on classification tasks can negatively impact the model's ability to perform generation tasks.
- Mechanism: Fine-tuning on classification tasks constrains the model's output space to predefined category labels, inhibiting its ability to generate other tokens. Additionally, the prompt format used for classification tasks (e.g., "### Input: {input} ### Sentiment:") may cause confusion when the model encounters inputs from other tasks during cross-task evaluation.
- Core assumption: The output space specialization induced by fine-tuning on classification tasks is the primary factor affecting cross-task performance.
- Evidence anchors:
  - [abstract]: "Intriguingly, we observe that integrating the in-context learning strategy during fine-tuning on generation tasks can enhance the model's generalization ability."
  - [section]: "Models fine-tuned on classification tasks fail to generalize to generation tasks... The second may be induced by the prompt format, as listed as Prompt-1 in Table 5."
  - [corpus]: Weak corpus evidence. The corpus does not directly support this specific mechanism.
- Break condition: If the output space specialization is not the primary factor affecting cross-task performance, or if other factors such as task-specific patterns learned during fine-tuning play a more significant role.

## Foundational Learning

- Concept: In-context learning (ICL)
  - Why needed here: ICL is a key component of the FTICL approach proposed in the paper. Understanding how ICL works and its benefits is crucial for comprehending the proposed method and its potential advantages over vanilla fine-tuning.
  - Quick check question: How does ICL differ from traditional fine-tuning, and what are the main benefits of using ICL?

- Concept: Task output space
  - Why needed here: The paper highlights the importance of task output space in determining the generalization ability of fine-tuned models. Understanding the concept of task output space and its implications is essential for grasping the key findings of the study.
  - Quick check question: What is meant by task output space, and how does it differ between classification and generation tasks?

- Concept: Catastrophic forgetting
  - Why needed here: The paper mentions that FTICL can help mitigate catastrophic forgetting for generation tasks. Understanding what catastrophic forgetting is and how it relates to fine-tuning is important for appreciating the potential benefits of FTICL.
  - Quick check question: What is catastrophic forgetting in the context of fine-tuning, and why is it a concern when adapting pre-trained models to new tasks?

## Architecture Onboarding

- Component map:
  Llama-2-7b -> Fine-tuning module -> (Optional FTICL module) -> Evaluation module

- Critical path:
  1. Pre-train the Llama-2 model on a large corpus of text data
  2. Fine-tune the pre-trained Llama-2 model on task-specific datasets using the fine-tuning module
  3. Optionally, apply the FTICL module to incorporate in-context learning during fine-tuning for generation tasks
  4. Evaluate the performance of the fine-tuned models using the evaluation module across various settings (in-domain, out-of-domain, cross-task)

- Design tradeoffs:
  - Fine-tuning vs. ICL: Fine-tuning offers task-specific performance improvements but may lead to reduced generalization ability. ICL maintains broader applicability but may underperform on specific tasks. FTICL aims to strike a balance by incorporating ICL during fine-tuning.
  - Task-specific vs. multi-task fine-tuning: Task-specific fine-tuning optimizes performance for a particular task but may limit cross-task generalization. Multi-task fine-tuning can enhance cross-task abilities but may sacrifice task-specific performance.

- Failure signatures:
  - Overfitting: If the fine-tuned model performs well on the training set but poorly on in-domain or out-of-domain test sets, it may indicate overfitting to the training data.
  - Catastrophic forgetting: If the fine-tuned model performs well on the target task but poorly on other tasks or on the original pre-training tasks, it may indicate catastrophic forgetting of previously learned knowledge.
  - Prompt sensitivity: If the model's performance varies significantly across different prompt formats or in-context examples, it may indicate sensitivity to the prompt structure.

- First 3 experiments:
  1. Fine-tune Llama-2 on a sentiment classification dataset (e.g., Amazon reviews) and evaluate its performance on in-domain and out-of-domain sentiment classification datasets, as well as cross-task evaluations (e.g., sentiment classification to question generation).
  2. Fine-tune Llama-2 on a summary generation dataset (e.g., XSum) using FTICL and evaluate its performance on in-domain and out-of-domain summary generation datasets, as well as cross-task evaluations (e.g., summary generation to sentiment classification).
  3. Compare the performance of fine-tuned models (with and without FTICL) on out-of-domain datasets for both classification and generation tasks to assess the impact of FTICL on generalization ability.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What specific characteristics of fine-tuning data influence the degree of catastrophic forgetting in LLMs?
- Basis in paper: [inferred] The paper discusses how fine-tuning on classification tasks leads to better out-of-domain generalization than generation tasks, and how integrating in-context learning during fine-tuning on generation tasks can improve generalization.
- Why unresolved: The paper mentions these observations but does not deeply investigate the underlying reasons for these differences in generalization behavior.
- What evidence would resolve it: Experiments comparing fine-tuning on datasets with varying characteristics (e.g., size, domain overlap, task similarity) and measuring the resulting model performance on out-of-domain tasks would provide insights into what drives catastrophic forgetting.

### Open Question 2
- Question: How does the number and diversity of in-context examples during fine-tuning affect the model's ability to generalize to new tasks and domains?
- Basis in paper: [explicit] The paper explores the impact of in-context learning during fine-tuning on generation tasks, finding it can enhance generalization, but the optimal number and diversity of examples are not thoroughly investigated.
- Why unresolved: While the paper shows that FTICL can improve generalization, it does not systematically vary the number and diversity of in-context examples to determine the most effective approach.
- What evidence would resolve it: Conducting experiments with different numbers and types of in-context examples during fine-tuning, and evaluating the resulting models on various out-of-domain and cross-task scenarios, would clarify the optimal conditions for FTICL.

### Open Question 3
- Question: What are the long-term effects of fine-tuning on the model's ability to perform zero-shot learning across diverse tasks?
- Basis in paper: [inferred] The paper notes that fine-tuned models often underperform using in-context learning compared to zero-shot settings, suggesting a potential trade-off between task-specific performance and general zero-shot capabilities.
- Why unresolved: The paper does not explore how fine-tuning impacts the model's zero-shot performance over time or across a wide range of tasks beyond those it was fine-tuned on.
- What evidence would resolve it: Long-term studies tracking the zero-shot performance of fine-tuned models across various tasks, including those not related to the fine-tuning data, would reveal the broader implications of fine-tuning on zero-shot learning abilities.

## Limitations

- Experimental scope is limited to a single LLM architecture (Llama-2-7b) and relatively small fine-tuning dataset sizes (2k-6k samples)
- The analysis of why classification tasks generalize better than generation tasks relies on speculative mechanisms rather than systematic ablation studies
- The FTICL approach shows promise but the underlying reasons for its success versus failure on classification tasks remain inadequately explained

## Confidence

**High Confidence**: The finding that fine-tuned models generally underperform in-context learning compared to zero-shot settings is well-supported by multiple evaluations across different task types and dataset sizes. The observation that classification tasks show better out-of-domain generalization than generation tasks is consistently observed across experiments.

**Medium Confidence**: The claim that FTICL enhances generation task generalization has empirical support but the mechanism (preserving general knowledge) is not conclusively proven. The observation about classification fine-tuning harming cross-task generation performance is observed but may be partially attributed to prompt format issues rather than fundamental output space constraints.

**Low Confidence**: The hypothesis that output space constraints are the primary driver of generalization differences between task types lacks direct experimental validation. The paper's explanation for why FTICL doesn't help classification tasks is speculative without systematic investigation of alternative explanations.

## Next Checks

1. **Ablation on Output Space**: Systematically vary the output space constraints in generation tasks (e.g., by adding structured outputs or templates) to determine if output space constraints alone explain the generalization differences between classification and generation tasks.

2. **Prompt Format Sensitivity**: Conduct controlled experiments varying prompt formats across all task combinations to isolate how much cross-task performance degradation is due to prompt confusion versus fundamental output space specialization.

3. **Scale Sensitivity Analysis**: Repeat the core experiments with larger fine-tuning dataset sizes (10k-50k samples) and different LLM scales (Llama-2-13b, Llama-2-70b) to determine if the observed patterns hold across training scales and model capacities.