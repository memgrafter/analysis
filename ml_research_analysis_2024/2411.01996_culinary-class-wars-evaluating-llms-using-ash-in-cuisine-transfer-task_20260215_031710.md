---
ver: rpa2
title: 'Culinary Class Wars: Evaluating LLMs using ASH in Cuisine Transfer Task'
arxiv_id: '2411.01996'
source_url: https://arxiv.org/abs/2411.01996
tags:
- recipe
- cuisine
- gemma2
- evaluation
- llms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study evaluates large language models' (LLMs) ability to transfer
  culinary elements between cuisines using the ASH benchmark, which assesses authenticity,
  sensitivity, and harmony in recipe generation. Six open-source LLMs generated 4,800
  recipes through cuisine transfer tasks, which were then evaluated by nine different
  LLMs and human annotators.
---

# Culinary Class Wars: Evaluating LLMs using ASH in Cuisine Transfer Task

## Quick Facts
- arXiv ID: 2411.01996
- Source URL: https://arxiv.org/abs/2411.01996
- Reference count: 40
- Primary result: LLM evaluators show systematic biases and focus on superficial cultural markers rather than deeper integration in cuisine transfer tasks

## Executive Summary
This study evaluates large language models' (LLMs) ability to transfer culinary elements between cuisines using the ASH benchmark, which assesses authenticity, sensitivity, and harmony in recipe generation. Six open-source LLMs generated 4,800 recipes through cuisine transfer tasks, which were then evaluated by nine different LLMs and human annotators. Results showed significant variations in evaluation consistency across models, with some LLMs displaying leniency while others were more critical. The study found that certain cuisines like Kosher and Islamic consistently received higher ratings, while others like Aztec and Ethiopian received lower ratings. Llama2:13b demonstrated the most consistent agreement with human evaluators across all criteria.

## Method Summary
The study employed six open-source LLMs to generate 4,800 recipes by applying elements of 40 cuisines to 20 base dishes, following standardized cuisine transfer instructions. These recipes were evaluated by nine different LLMs (including the generators plus three proprietary models) and human annotators using the ASH benchmark criteria: authenticity, sensitivity, and harmony. The generated recipes were assessed on a scale from 1 to 10 for each criterion, with inter-rater agreement measured using kappa scores. Human evaluation was performed on a stratified sample of 200 recipes to provide ground truth comparisons for the LLM evaluators.

## Key Results
- Significant variations in evaluation consistency across different LLM evaluators, with some showing leniency while others were more critical
- Certain cuisines (Kosher, Islamic) consistently received higher ratings while others (Aztec, Ethiopian) received lower ratings
- Llama2:13b showed the most consistent agreement with human evaluators across all three ASH criteria
- LLMs tended to focus on superficial cultural markers rather than meaningful integration of cultural elements

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLMs can evaluate recipe creativity by using a multi-criteria framework (authenticity, sensitivity, harmony)
- Mechanism: The ASH benchmark evaluates recipe quality through three orthogonal criteria that capture both preservation of original dish characteristics and successful integration of cultural elements
- Core assumption: Recipe evaluation can be meaningfully decomposed into these three distinct criteria that capture different aspects of culinary quality
- Evidence anchors:
  - [abstract]: "We introduce the ASH (authenticity, sensitivity, harmony) benchmark to evaluate LLMs' recipe generation abilities in the cuisine transfer task"
  - [section]: "We propose a novel criteriaASH to evaluate the generated recipes which are authenticity, sensitivity and harmony"
  - [corpus]: Weak - no direct corpus evidence for this specific decomposition approach
- Break condition: If evaluators cannot reliably distinguish between the three criteria, the framework collapses into a single-dimensional assessment

### Mechanism 2
- Claim: LLM evaluators show systematic biases that correlate with their architectural differences
- Mechanism: Different LLM families (gemma, llama, mistral, gemini) demonstrate consistent patterns in their evaluation behavior - some being more lenient while others more critical
- Core assumption: Evaluation patterns are consistent enough across similar model families to identify systematic biases
- Evidence anchors:
  - [section]: "In contrast, other evaluators displayed more leniency" and "both gemini variants assigned relatively low ratings while llama2:13b was inclined towards all generators"
  - [section]: "While the gemini and llama variants seem to have discrepancy to each other, LLMs from the same family tend to resemble each other"
  - [corpus]: Weak - corpus contains related culinary research but no direct evidence of LLM evaluation bias patterns
- Break condition: If evaluation patterns become inconsistent across different tasks or contexts, the systematic bias assumption fails

### Mechanism 3
- Claim: LLM evaluators focus on superficial elements rather than deeper cultural understanding in culinary tasks
- Mechanism: Evaluators give high ratings to recipes that include specific cultural markers (like "certified" for Kosher or "berbere" for Ethiopian) without assessing whether these elements are meaningfully integrated into the recipe
- Core assumption: The presence of cultural markers is being used as a proxy for cultural authenticity rather than assessing deeper integration
- Evidence anchors:
  - [section]: "After examining the generated recipes and evaluation rationales, we discovered that the generators' focus tended to be polarized towards certified"
  - [section]: "While this ingredient played a pivotal role in attracting positive sensitivity ratings, we speculate that its combination with the base dishes did not seem to help the generator earn good authenticity and harmony ratings"
  - [corpus]: Weak - corpus contains related culinary research but no direct evidence of superficial vs. deep cultural understanding assessment
- Break condition: If evaluators demonstrate ability to assess meaningful integration beyond token presence, this mechanism breaks down

## Foundational Learning

- Concept: Cuisine transfer as a creativity task
  - Why needed here: Understanding that cuisine transfer involves both preserving original dish essence and integrating new cultural elements is crucial for designing appropriate evaluation criteria
  - Quick check question: What are the two main competing objectives in cuisine transfer tasks?

- Concept: Multi-dimensional evaluation frameworks
  - Why needed here: The ASH benchmark's three-criteria approach requires understanding how to decompose complex creative tasks into measurable dimensions
  - Quick check question: How does the harmony criterion relate to the authenticity and sensitivity criteria?

- Concept: LLM evaluation consistency and bias
  - Why needed here: Interpreting the evaluation results requires understanding that different LLM models may have systematic biases based on their architecture and training
  - Quick check question: Why might LLM evaluators from the same family show similar evaluation patterns?

## Architecture Onboarding

- Component map: 
  - Recipe generators (6 open-source LLMs) → 800 cuisine transfer prompts each → 4,800 recipes
  - Recipe evaluators (9 LLMs including proprietary models) → rate all 4,800 recipes on 3 criteria
  - Human annotators → rate stratified sample of 200 recipes for comparison
  - Analysis pipeline → compute inter-rater agreement, identify biases, analyze patterns

- Critical path: Recipe generation → LLM evaluation → human evaluation → inter-agreement analysis → insight generation
- Design tradeoffs: Using multiple LLM evaluators provides scale but introduces model-specific biases; human evaluation provides ground truth but is limited in scope
- Failure signatures: Inconsistent ratings across evaluators, low inter-rater agreement, systematic over/under-rating of certain cuisines
- First 3 experiments:
  1. Test individual LLM evaluators on a small set of recipes with known ground truth to establish baseline reliability
  2. Compare LLM evaluator agreement on recipes with obvious quality differences to validate discrimination ability
  3. Analyze evaluation patterns for specific cuisine pairs to identify systematic biases in cultural understanding

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How would the results change if we used a more diverse set of base dishes beyond the 20 used in this study?
- Basis in paper: explicit
- Why unresolved: The study uses a fixed set of 20 base dishes to generate 800 cuisine transfer instructions. The choice of base dishes may have influenced the results, especially for cuisines that are less compatible with certain dishes. For example, it may be easier to transfer Japanese elements to sushi than to barbecue meat.
- What evidence would resolve it: Replicating the study with a different set of base dishes, particularly including dishes that are more neutral or adaptable to various cuisines, would help determine if the results are robust to the choice of base dishes.

### Open Question 2
- Question: Would using a regression-based approach instead of traditional Kappa scores provide a more accurate measure of evaluator agreement?
- Basis in paper: explicit
- Why unresolved: The paper acknowledges that traditional Kappa scores are not ideal for continuous score variations. It suggests that regression-based approaches might yield more accurate measures of evaluator agreement, but does not implement them.
- What evidence would resolve it: Implementing and comparing the results of both Kappa scores and regression-based approaches on the same dataset would demonstrate which method provides a more accurate assessment of evaluator agreement.

### Open Question 3
- Question: How would the results differ if we used human culinary experts instead of non-expert annotators for evaluation?
- Basis in paper: explicit
- Why unresolved: The paper mentions that human annotators are not culinary experts, and speculates that evaluators with limited culinary knowledge may possess narrower recipe evaluation standards. This could bias the results.
- What evidence would resolve it: Repeating the evaluation process with a panel of professional chefs or culinary experts would help determine if the current results are influenced by the non-expert nature of the annotators.

## Limitations

- Lack of detailed specification for evaluation criteria implementation and human annotator backgrounds
- Limited corpus evidence supporting the effectiveness of the ASH benchmark's three-criteria decomposition
- Evaluation revealed LLMs focus on superficial cultural markers rather than meaningful integration

## Confidence

- **High confidence**: The observation that different LLM families show systematic evaluation patterns correlating with their architecture
- **Medium confidence**: The effectiveness of the ASH benchmark's three-criteria framework for evaluating cuisine transfer tasks
- **Low confidence**: The claim that LLMs focus on superficial elements rather than deeper cultural understanding

## Next Checks

1. **Criterion decomposition validation**: Test whether human evaluators can reliably distinguish between authenticity, sensitivity, and harmony criteria on a validation set of recipes to confirm the ASH framework's effectiveness.
2. **Model bias characterization**: Systematically analyze evaluation patterns across all cuisine pairs to quantify systematic biases and determine whether they stem from training data or architectural differences.
3. **Cultural understanding assessment**: Design targeted experiments where recipes include culturally appropriate ingredients used either superficially or meaningfully integrated, then test whether LLM evaluators can distinguish between these cases.