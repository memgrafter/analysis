---
ver: rpa2
title: A Resource Efficient Fusion Network for Object Detection in Bird's-Eye View
  using Camera and Raw Radar Data
arxiv_id: '2411.13311'
source_url: https://arxiv.org/abs/2411.13311
tags:
- radar
- camera
- detection
- fusion
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of fusing raw radar data with
  camera images for object detection in autonomous driving, focusing on computational
  efficiency. The authors propose a fusion architecture that independently processes
  camera images into a Bird's-Eye View (BEV) Polar representation and radar features
  into a Range-Azimuth (RA) representation, then fuses them for detection.
---

# A Resource Efficient Fusion Network for Object Detection in Bird's-Eye View using Camera and Raw Radar Data

## Quick Facts
- arXiv ID: 2411.13311
- Source URL: https://arxiv.org/abs/2411.13311
- Reference count: 40
- Achieves 95.75% AP, 91.35% AR, and 93.49% F1-Score while maintaining 58.91 FPS

## Executive Summary
This paper addresses the challenge of fusing raw radar data with camera images for object detection in autonomous driving, focusing on computational efficiency. The authors propose a fusion architecture that independently processes camera images into a Bird's-Eye View (BEV) Polar representation and radar features into a Range-Azimuth (RA) representation, then fuses them for detection. The method achieves competitive accuracy while significantly outperforming state-of-the-art models in computational efficiency with 58.91 FPS, 6.58 million parameters, and 79.8 MB model size.

## Method Summary
The proposed fusion network independently processes camera images to BEV Polar representation and raw radar data to RA features before fusing them. The camera pipeline transforms front-view images through Cartesian and then Polar coordinates using spline interpolation, while the radar pipeline employs a MIMO pre-encoder with channel swapping to recover angle information from the RD spectrum. Features from both modalities are concatenated and processed through Conv-BatchNorm layers before detection. The network uses a ResNet-50 backbone for both camera and radar encoders, with focal loss for classification and smooth L1 loss for regression.

## Key Results
- Achieves 95.75% AP, 91.35% AR, and 93.49% F1-Score on the RADIal dataset
- Maintains computational efficiency with 58.91 FPS, 6.58M parameters, and 79.8MB model size
- Shows error metrics of 0.11m range error and 0.09° angle error

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Independent preprocessing of camera images to BEV Polar space eliminates the need for complex feature transformations within the network, improving computational efficiency.
- Mechanism: By transforming camera images into a BEV Polar representation before feeding them into the network, the architecture avoids the computational overhead of performing this transformation during training. This preprocessing step aligns the camera features with the radar features, which are already in a polar-like representation, enabling direct fusion without additional transformation layers.
- Core assumption: The preprocessing step is accurate and maintains the semantic information of the original camera images.
- Evidence anchors:
  - [abstract]: "We independently process camera images within the proposed comprehensive image processing pipeline. Specifically, first, we transform the camera images to Bird's-Eye View (BEV) Polar domain and extract the corresponding features with our camera encoder-decoder architecture."
  - [section]: "This whole independent process of pre-aligning camera images in BEV Polar space offers multiple advantages. Mainly, it eliminates the necessity for feature transformation within the network, potentially enhancing computational efficiency during the training process."
- Break condition: If the preprocessing introduces significant distortions or loses crucial semantic information, the model's performance would degrade despite the computational efficiency gains.

### Mechanism 2
- Claim: Channel swapping strategy in the radar feature extractor allows the network to learn a dense feature embedding of RA maps, effectively recovering angle information from the RD spectrum.
- Mechanism: The raw radar data is in the form of a Range-Doppler (RD) tensor. By swapping the Doppler and azimuth axes, the network can upsample the feature maps to create a denser representation. This dense embedding helps the network to effectively recognize the relevance of the angle information for the object detection task.
- Core assumption: The angle information can be reliably recovered from the RD spectrum through the channel swapping and upsampling process.
- Evidence anchors:
  - [section]: "To this end, we adapt a Multiple Input Multiple Output (MIMO) pre-encoder [46] that reorganizes this RD tensor into a meaningful representation for the resnet-50 [47] like encoder blocks... Since the objective is to acquire the angle information, channel swapping strategy is employed by swapping the Doppler and azimuth axes before upscaling the feature maps."
- Break condition: If the angle information cannot be reliably recovered from the RD spectrum, the model's detection accuracy would suffer, particularly in distinguishing objects at different angles.

### Mechanism 3
- Claim: Fusion of RA latent features from camera and radar networks through channel concatenation provides complementary cues, enhancing detection robustness.
- Mechanism: The camera and radar networks independently extract features from their respective inputs (camera images in BEV Polar and radar data in RA representation). These features are then concatenated and processed through Conv-BatchNorm layers before being passed to the detection head. This fusion strategy allows the model to leverage the strengths of both modalities, improving overall detection performance.
- Core assumption: The features extracted from camera and radar are complementary and can be effectively combined through channel concatenation.
- Evidence anchors:
  - [abstract]: "The resultant feature maps are fused with Range-Azimuth (RA) features, recovered from the RD spectrum input from the radar decoder to perform object detection."
  - [section]: "The RA latent features from the camera only and radar only network are fused by channel concatenation, which is then processed using four Conv-BatchNorm layers..."
- Break condition: If the features from camera and radar are not sufficiently complementary or if the fusion strategy is not optimal, the model's performance might not improve or could even degrade.

## Foundational Learning

- Concept: Bird's-Eye View (BEV) representation
  - Why needed here: BEV representation is crucial for autonomous driving as it provides a top-down view of the environment, making it easier to understand the spatial relationships between objects. This paper leverages BEV to fuse camera and radar data effectively.
  - Quick check question: What are the advantages of using BEV representation in autonomous driving perception systems?

- Concept: Range-Azimuth (RA) representation
  - Why needed here: RA representation is used to process radar data, which provides information about the range and angle of objects relative to the radar sensor. This paper recovers RA features from the RD spectrum to enable fusion with camera data.
  - Quick check question: How does the RA representation differ from the RD spectrum, and why is it beneficial for object detection?

- Concept: Channel swapping and upsampling
  - Why needed here: This technique is used to recover angle information from the RD spectrum by rearranging the tensor dimensions and increasing the feature map resolution. It allows the network to learn a dense feature embedding of RA maps.
  - Quick check question: Explain how channel swapping and upsampling can help recover angle information from radar data.

## Architecture Onboarding

- Component map:
  - Camera images -> Image processing pipeline -> Camera feature extractor -> Fusion layer -> Detection head
  - Raw radar data -> Radar feature extractor (with channel swapping and upsampling) -> Fusion layer -> Detection head

- Critical path:
  - Raw radar data → Radar feature extractor (with channel swapping and upsampling) → Fusion layer
  - Camera images → Image processing pipeline → Camera feature extractor → Fusion layer
  - Fusion layer → Detection head (classification and regression)

- Design tradeoffs:
  - Computational efficiency vs. accuracy: The independent preprocessing of camera images improves computational efficiency but relies on the accuracy of the preprocessing step.
  - Feature extraction complexity: The channel swapping and upsampling in the radar feature extractor add complexity but are necessary to recover angle information from the RD spectrum.
  - Fusion strategy: Channel concatenation is a simple yet effective fusion strategy, but more complex fusion methods might yield better results at the cost of increased computational overhead.

- Failure signatures:
  - Poor detection accuracy: Could indicate issues with the preprocessing step, feature extraction, or fusion strategy.
  - High computational cost: Might suggest that the channel swapping and upsampling are too resource-intensive or that the fusion layer is not optimized.
  - Inconsistent performance across frames: Could be due to variations in the preprocessing step or the fusion strategy not handling certain scenarios well.

- First 3 experiments:
  1. Ablation study on the image processing pipeline: Compare the performance of the model with and without the independent preprocessing of camera images to BEV Polar representation.
  2. Evaluation of different fusion strategies: Experiment with alternative fusion methods (e.g., attention-based fusion) to assess their impact on detection accuracy and computational efficiency.
  3. Analysis of the channel swapping and upsampling in the radar feature extractor: Investigate the effect of different upsampling factors and channel swapping configurations on the model's ability to recover angle information from the RD spectrum.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How would the proposed fusion architecture perform when extended to detect multiple object classes beyond vehicles, such as pedestrians and cyclists, given suitable datasets?
- Basis in paper: [inferred] The paper acknowledges that the RADIal dataset is limited to vehicle detection but suggests that the method could be extended to other road users given a suitable dataset.
- Why unresolved: The current evaluation is restricted to vehicle detection, and there is no empirical evidence on how the model would handle multiple object classes with varying sizes and shapes.
- What evidence would resolve it: Testing the model on a dataset containing multiple object classes (e.g., nuScenes or KITTI) and comparing its performance metrics (AP, AR, F1-Score) across different categories.

### Open Question 2
- Question: What is the impact of varying the camera mounting height and pitch angle on the accuracy of the BEV Polar transformation and subsequent object detection performance?
- Basis in paper: [explicit] The paper mentions that the height of the camera mounted above the ground and the pitch of the camera toward the ground are important parameters when transforming the front-facing camera data to a BEV object, and references Fig. 5 which shows a discrepancy in the BEV image.
- Why unresolved: The paper does not provide an analysis of how changes in these parameters affect the transformation accuracy or detection performance.
- What evidence would resolve it: Conducting experiments with varying camera heights and pitch angles, and measuring the resulting changes in detection accuracy and transformation errors.

### Open Question 3
- Question: How does the computational efficiency of the proposed method scale when using heavier backbone models for the camera feature extractor, and what is the trade-off between accuracy and computational complexity?
- Basis in paper: [explicit] The paper discusses the computational efficiency of the proposed method and mentions in Section VII that the camera backbone module could be replaced by heavier models depending on computational budgets, with an ablation study comparing different backbones.
- Why unresolved: While the paper provides an ablation study with different backbones, it does not explore the full range of potential backbone models or provide a detailed analysis of the trade-offs between accuracy and computational complexity.
- What evidence would resolve it: Evaluating the model with a wider range of backbone architectures, including very deep models, and analyzing the resulting accuracy and computational metrics to determine the optimal balance.

## Limitations
- Performance evaluated on single dataset (RADIal), limiting generalization claims
- Relies on consistent sensor calibration and preprocessing accuracy
- Does not extensively address handling of occlusions or adverse weather conditions

## Confidence
- **High Confidence**: Computational efficiency claims (58.91 FPS, 6.58M parameters, 79.8MB model size)
- **Medium Confidence**: Accuracy metrics (95.75% AP, 91.35% AR, 93.49% F1-Score) based on single dataset
- **Low Confidence**: Claims about "superior performance" relative to state-of-the-art models without detailed benchmarking

## Next Checks
1. Cross-dataset validation: Evaluate the model's performance on multiple autonomous driving datasets (e.g., nuScenes, Waymo Open Dataset) to assess generalization across different sensor configurations and environmental conditions.

2. Ablation study on preprocessing dependency: Systematically test the model's performance when varying the quality and accuracy of the camera-to-BEV preprocessing step to understand the sensitivity to this component.

3. Real-world deployment testing: Implement the model in a controlled autonomous driving environment with varying weather conditions, lighting scenarios, and sensor calibrations to validate the claimed computational efficiency under practical constraints.