---
ver: rpa2
title: 'Towards Understanding Jailbreak Attacks in LLMs: A Representation Space Analysis'
arxiv_id: '2406.10794'
source_url: https://arxiv.org/abs/2406.10794
tags:
- jailbreak
- prompts
- attacks
- autodan
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper analyzes jailbreak attacks in large language models
  (LLMs) through representation space analysis. The authors visualize hidden representations
  of harmful and harmless prompts, finding that successful attacks move harmful prompts
  toward harmless prompt representations in the model's embedding space.
---

# Towards Understanding Jailbreak Attacks in LLMs: A Representation Space Analysis

## Quick Facts
- arXiv ID: 2406.10794
- Source URL: https://arxiv.org/abs/2406.10794
- Authors: Yuping Lin; Pengfei He; Han Xu; Yue Xing; Makoto Yamada; Hui Liu; Jiliang Tang
- Reference count: 18
- Primary result: Representation-based objective improves GCG attack success by up to 62.31% on Llama-2-13b-chat

## Executive Summary
This paper analyzes jailbreak attacks in large language models through representation space analysis, finding that successful attacks move harmful prompts toward harmless prompt representations in the model's embedding space. The authors propose incorporating this representation-based objective into existing white-box attacks, achieving up to 62.31% attack success rate on Llama-2-13b-chat (36.16% higher than baseline). The method shows improved ASR for GCG-based attacks but limited transferability and reduced effectiveness against perplexity-based defenses.

## Method Summary
The authors collect anchor datasets of 100 harmless and 100 harmful prompts, then visualize their representations in PCA-reduced space to identify an "acceptance direction" from harmful to harmless prompts. They introduce a new optimization objective that maximizes projected distance along this acceptance direction and integrate it with existing white-box attacks (GCG and AutoDAN) and black-box attack (PAIR). An early stopping strategy using a double-check approach (string matching + LLM classifier) prevents excessive semantic changes that would cause off-topic responses.

## Key Results
- Representation-based objective improves GCG attack success by up to 62.31% on Llama-2-13b-chat
- Successful jailbreak attacks consistently move harmful prompts toward harmless representation directions in PCA space
- The method shows limited transferability across different model architectures
- Perplexity-based defenses reduce the effectiveness of representation-based attacks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Successful jailbreak attacks move harmful prompts toward harmless prompt representations in the LLM's embedding space
- Mechanism: The attack optimizes prompts to shift their representation in PCA-reduced space toward the acceptance direction (from harmful to harmless anchor prompt centers)
- Core assumption: Harmfulness is a major source of variation in the representation space that LLMs can effectively capture
- Evidence anchors:
  - [abstract] "We hypothesize that successful attacks share some similar properties: They are effective in moving the representation of the harmful prompt towards the direction to the harmless prompts"
  - [section] "Observation 2. Succeeded jailbreak attacks move the harmful prompts toward the direction of harmless anchor prompts farther than the failed ones"
  - [corpus] Weak - no corpus papers directly validate this specific spatial relationship mechanism
- Break condition: If the model's representation space does not separate harmful and harmless prompts effectively (as with gemma-7b), this mechanism fails

### Mechanism 2
- Claim: The proposed optimization objective increases attack success by maximizing projected distance along the acceptance direction
- Mechanism: Replaces existing attack objectives with a representation-based objective that moves prompts toward the acceptance center in anchored PCA space
- Core assumption: Moving along the acceptance direction increases the likelihood of affirmative responses
- Evidence anchors:
  - [abstract] "We leverage hidden representations into the objective of existing jailbreak attacks to move the attacks along the acceptance direction"
  - [section] "max L(x) = [g(h(x)) - g(h(x0))]âŠ¤ea" - formalizes the objective
  - [corpus] Weak - no corpus papers directly validate this specific optimization objective
- Break condition: If optimization changes semantic meaning too much (off-topic responses), or if the acceptance direction doesn't transfer between models

### Mechanism 3
- Claim: Early stopping prevents excessive semantic changes that would cause off-topic responses
- Mechanism: Terminates optimization when classifier detects jailbreak success, preventing over-optimization that shifts representation too far
- Core assumption: There's an optimal point where representation has shifted enough for jailbreak but not so much that semantic meaning is lost
- Evidence anchors:
  - [section] "To mitigate this risk, we employ an early stopping strategy to limit the total number of iterations"
  - [section] "The above double-check strategy balances the efficiency and accuracy of detection"
  - [corpus] Weak - no corpus papers directly validate this specific early stopping mechanism
- Break condition: If classifier is too slow or inaccurate, or if early stopping threshold is poorly calibrated

## Foundational Learning

- Concept: Principal Component Analysis (PCA) for dimensionality reduction
  - Why needed here: Used to project high-dimensional representations into 2D space for visualization and to define the acceptance direction
  - Quick check question: What does the first principal component capture in the PCA of harmless vs harmful prompts?

- Concept: Representation space and embeddings in neural networks
  - Why needed here: Understanding how LLMs encode semantic information in hidden states is crucial for interpreting the visualization results
  - Quick check question: What part of the LLM's output is used as the representation for each prompt?

- Concept: White-box vs black-box attacks
  - Why needed here: The proposed method requires white-box access to internal representations, distinguishing it from black-box approaches
  - Quick check question: Why can't the proposed optimization objective be used in black-box jailbreak attacks?

## Architecture Onboarding

- Component map: 
  Anchor dataset collection -> Victim LLM -> PCA transformation -> Optimization module -> Early stopping classifier -> Evaluation classifier

- Critical path: 
  1. Collect anchor datasets
  2. Run PCA on anchor prompts to find acceptance direction
  3. Initialize attack on target prompts
  4. Optimize using representation-based objective
  5. Apply early stopping when classifier confirms success
  6. Evaluate with classifier

- Design tradeoffs:
  - White-box requirement vs effectiveness: The method requires internal access but shows higher ASR
  - Computational cost vs early stopping: Double-check strategy balances speed and accuracy
  - Transferability vs model-specific optimization: Method is optimized for specific models but doesn't transfer well

- Failure signatures:
  - Poor separation between harmful/harmless in PCA space (gemma-7b case)
  - Optimization causing off-topic responses
  - Early stopping classifier false positives/negatives
  - No improvement over baseline attacks

- First 3 experiments:
  1. Visualize representations of anchor prompts on Llama-2-7b to verify separation
  2. Apply proposed objective to GCG on Llama-2-7b and compare ASR
  3. Test defense effectiveness (perplexity filter and paraphrasing) on improved attack

## Open Questions the Paper Calls Out

The paper identifies several open questions about the relationship between representation space movement and semantic preservation during jailbreak attacks, and how different LLM architectures affect the separability of harmful and harmless prompts in representation space.

## Limitations

- The proposed method requires white-box access to internal representations, limiting practical applicability
- Limited transferability of the representation-based approach across different model architectures
- Reduced effectiveness against perplexity-based defenses

## Confidence

- **High Confidence**: The visualization results showing separation between harmful and harmless prompts in PCA space are well-supported by the presented evidence.
- **Medium Confidence**: The effectiveness of the proposed optimization objective for improving ASR, particularly for GCG-based attacks.
- **Low Confidence**: The generalizability of the representation-based approach across different model architectures and the robustness of the early stopping mechanism.

## Next Checks

1. Cross-architecture validation: Test the representation-based optimization objective on a broader range of model architectures to determine whether the acceptance direction concept generalizes beyond the Llama family of models.

2. Defense robustness evaluation: Systematically evaluate the effectiveness of the representation-based attacks against a comprehensive suite of defenses beyond perplexity-based methods.

3. Semantic preservation analysis: Conduct a detailed analysis of how the optimization objective affects the semantic content of jailbreak prompts, measuring the trade-off between attack success and semantic drift.