---
ver: rpa2
title: 'Krutrim LLM: A Novel Tokenization Strategy for Multilingual Indic Languages
  with Petabyte-Scale Data Processing'
arxiv_id: '2407.12481'
source_url: https://arxiv.org/abs/2407.12481
tags:
- data
- language
- tokenizer
- indic
- languages
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a comprehensive approach to developing a high-quality
  multilingual Indic large language model (LLM) with a novel tokenization strategy.
  The authors address the challenge of limited high-quality Indic language data and
  complex tokenization by curating a large dataset from open-source and proprietary
  sources, including Common Crawl, Indic books, news articles, and Wikipedia.
---

# Krutrim LLM: A Novel Tokenization Strategy for Multilingual Indic Languages with Petabyte-Scale Data Processing

## Quick Facts
- arXiv ID: 2407.12481
- Source URL: https://arxiv.org/abs/2407.12481
- Reference count: 9
- A novel tokenization strategy for multilingual Indic languages achieving superior token-to-word ratio

## Executive Summary
This paper presents a comprehensive approach to developing a high-quality multilingual Indic large language model (LLM) with a novel tokenization strategy. The authors address the challenge of limited high-quality Indic language data and complex tokenization by curating a large dataset from open-source and proprietary sources, including Common Crawl, Indic books, news articles, and Wikipedia. They design custom preprocessing pipelines for each Indic language, perform deduplication on Common Crawl data, and introduce a novel multilingual tokenizer training strategy. The custom-trained Indic tokenizer outperforms the state-of-the-art OpenAI Tiktoken tokenizer, achieving a superior token-to-word ratio for Indic languages.

## Method Summary
The methodology involves acquiring and preprocessing petabyte-scale data from sources like Common Crawl, Indic books, news articles, and Wikipedia, with deduplication and language-specific cleaning. A custom tokenizer is developed using Byte Pair Encoding (BPE) with optimal vocabulary size, corpus size, and character coverage. The Indic LLM is then trained and evaluated using the curated dataset and custom tokenizer, focusing on performance metrics like token-to-word ratio.

## Key Results
- Custom-trained Indic tokenizer achieves superior token-to-word ratio compared to OpenAI Tiktoken
- Deduplication on Common Crawl data reduces redundancy in 70% of crawled web pages
- Optimal vocabulary size selection (70k-100k) balances token-to-word ratio and inference efficiency

## Why This Works (Mechanism)

### Mechanism 1
Deduplication significantly improves tokenizer performance by reducing redundancy in training data. Removing duplicate documents ensures the tokenizer learns a more diverse vocabulary distribution, improving token-to-word ratio. Core assumption: Duplicate content inflates token frequency counts for certain words, biasing the tokenizer toward over-segmentation. Evidence anchors: [abstract] states "deduplication on Common Crawl data to address the redundancy present in 70% of the crawled web pages"; [section] describes using Minhash LSH with 5-gram shingles and a 0.7 similarity threshold to identify duplicates; [corpus] shows token count reduction after deduplication (e.g., Hi from 108M to 51.2M tokens). Break condition: If duplicate content is below 10% or the corpus is already diverse, deduplication yields minimal gains.

### Mechanism 2
Custom preprocessing per language improves data quality for tokenizer training. Language-specific pipelines remove noise and normalize text, ensuring the tokenizer learns meaningful subword units. Core assumption: Indic languages have distinct noise patterns (scripts, boilerplate) requiring tailored filtering. Evidence anchors: [abstract] mentions "custom preprocessing pipeline to effectively eliminate redundant and low-quality text content"; [section] describes heuristic filters like token count, mean sentence length, and perplexity thresholds applied per language; [corpus] lists language-specific filter ranges (e.g., Hi [50,10000] tokens, [3,10] word length). Break condition: If data is already clean or language differences are negligible, custom preprocessing adds overhead without benefit.

### Mechanism 3
Optimal vocabulary size selection improves tokenizer efficiency and performance. Balancing vocabulary size against token-to-word ratio avoids over-segmentation while maintaining coverage. Core assumption: Larger vocabularies reduce token count but increase memory/compute cost during inference. Evidence anchors: [abstract] shows "custom-trained Indic tokenizer outperforms... achieving a superior token-to-word ratio"; [section] experiments with 70k, 85k, 100k vocabularies, noting improvements in token-to-word ratio; [corpus] table 3 compares ratios across languages and vocabulary sizes. Break condition: If inference speed is not a constraint or token count is already optimal, further vocabulary expansion is unnecessary.

## Foundational Learning

- Concept: Minhash Locality Sensitive Hashing
  - Why needed here: Efficiently identifies near-duplicate documents in large-scale web crawl data
  - Quick check question: What is the role of the similarity threshold in Minhash LSH deduplication?

- Concept: Byte Pair Encoding (BPE)
  - Why needed here: Subword tokenization balances vocabulary size and out-of-vocabulary handling for multilingual data
  - Quick check question: How does BPE differ from WordPiece in handling rare words?

- Concept: Heuristic text quality filtering
  - Why needed here: Removes low-quality documents that would degrade tokenizer learning
  - Quick check question: Which heuristic feature best identifies gibberish text in Indic languages?

## Architecture Onboarding

- Component map: Data pipeline → Preprocessing → Language detection → Heuristic filtering → Deduplication → Tokenizer training → Evaluation
- Critical path: Common Crawl processing → Deduplication → Tokenizer training → Performance validation
- Design tradeoffs: Vocabulary size vs. inference speed, deduplication thoroughness vs. data retention, custom preprocessing vs. generalizability
- Failure signatures: Poor token-to-word ratio, high proportion of UNK tokens, slow inference, tokenizer vocabulary containing non-Indic characters
- First 3 experiments:
  1. Run Minhash LSH deduplication on a small Common Crawl batch and measure duplicate removal rate
  2. Train two tokenizers with 70k vs 100k vocabulary on the same corpus and compare token-to-word ratios
  3. Apply heuristic filters with different percentile thresholds and evaluate impact on token count and quality

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the token-to-word ratio of the custom-trained Indic tokenizer compare to other state-of-the-art multilingual tokenizers beyond OpenAI Tiktoken?
- Basis in paper: [explicit] The paper compares the custom-trained Indic tokenizer to the OpenAI Tiktoken tokenizer and demonstrates superior performance, but does not compare it to other multilingual tokenizers.
- Why unresolved: The paper does not provide a comprehensive comparison with other multilingual tokenizers, leaving the relative performance of the custom-trained Indic tokenizer unclear.
- What evidence would resolve it: A comparative analysis of the custom-trained Indic tokenizer's token-to-word ratio against other state-of-the-art multilingual tokenizers would provide clarity on its relative performance.

### Open Question 2
- Question: What is the impact of increasing the corpus size beyond the 225 million sampled corpus size used in the experiments?
- Basis in paper: [inferred] The paper mentions that increasing the corpus size beyond a certain point yields diminishing returns in terms of tokenizer improvement, but does not explore the impact of further increases.
- Why unresolved: The paper does not investigate the potential benefits or drawbacks of significantly increasing the corpus size beyond the tested threshold.
- What evidence would resolve it: Experiments with progressively larger corpus sizes, assessing the impact on tokenizer performance, would provide insights into the optimal corpus size for training.

### Open Question 3
- Question: How does the custom-trained Indic tokenizer perform on low-resource Indic languages not included in the initial training data?
- Basis in paper: [inferred] The paper focuses on 12 Indic languages, including English, but does not address the tokenizer's performance on other low-resource Indic languages.
- Why unresolved: The paper does not evaluate the tokenizer's generalization capabilities to low-resource Indic languages, which could be crucial for its broader applicability.
- What evidence would resolve it: Testing the tokenizer's performance on low-resource Indic languages, assessing its ability to handle unseen linguistic features, would provide insights into its robustness and generalizability.

## Limitations
- Lack of complete specification of critical parameters, particularly heuristic filtering thresholds and Minhash LSH configuration details
- Insufficient ablation studies showing individual contribution of each preprocessing step to final tokenizer performance
- Limited evaluation of downstream task performance beyond token-to-word ratio improvements

## Confidence

**High Confidence**: The general approach of using custom preprocessing per language and deduplication to improve tokenizer quality is well-supported by the evidence provided.

**Medium Confidence**: The claim that the custom-trained Indic tokenizer outperforms state-of-the-art tokenizers (specifically OpenAI Tiktoken) is supported by reported token-to-word ratio improvements, but lacks comprehensive comparisons.

**Low Confidence**: The optimal vocabulary size selection claims and specific impact of each preprocessing heuristic on final tokenizer performance have low confidence due to insufficient ablation studies and parameter sensitivity analysis.

## Next Checks

1. **Parameter Sensitivity Analysis**: Reproduce the Minhash LSH deduplication with varying similarity thresholds (0.5, 0.7, 0.9) and measure the impact on token-to-word ratio and downstream model performance.

2. **Ablation Study of Preprocessing Steps**: Train tokenizers with different combinations of preprocessing steps (with and without deduplication, with and without heuristic filtering, with and without language-specific preprocessing) to quantify individual contribution of each component.

3. **Cross-Architecture Comparison**: Compare the custom tokenizer performance against multiple established tokenizer architectures (WordPiece, SentencePiece, BPE variants) on the same Indic language corpus to validate claimed superiority.