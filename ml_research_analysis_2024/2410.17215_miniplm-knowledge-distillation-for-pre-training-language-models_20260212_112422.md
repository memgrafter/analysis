---
ver: rpa2
title: 'MiniPLM: Knowledge Distillation for Pre-Training Language Models'
arxiv_id: '2410.17215'
source_url: https://arxiv.org/abs/2410.17215
tags:
- arxiv
- mini
- student
- teacher
- pre-training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces MiniPLM, a knowledge distillation (KD) framework
  for pre-training language models (LMs). Existing KD methods for pre-training either
  incur high computational overhead due to online teacher inference, require tokenization
  matching between teacher and student LMs, or risk losing data difficulty and diversity.
---

# MiniPLM: Knowledge Distillation for Pre-Training Language Models

## Quick Facts
- **arXiv ID**: 2410.17215
- **Source URL**: https://arxiv.org/abs/2410.17215
- **Reference count**: 40
- **Primary result**: 2.4× reduction in pre-training data requirements while improving student LM performance on 9 downstream tasks

## Executive Summary
MiniPLM introduces an efficient knowledge distillation framework for pre-training language models that addresses key limitations of existing methods. By performing offline teacher inference and using Difference Sampling to refine the pre-training data distribution, MiniPLM achieves better data difficulty and diversity while reducing computational overhead. The method compares teacher and reference model probabilities to selectively down-sample easy patterns, up-sample hard diverse instances, and filter noisy data. Experiments demonstrate improved student performance across multiple model sizes and downstream tasks, with the added benefit of supporting knowledge distillation across different model families.

## Method Summary
MiniPLM employs a reference LM trained on a subset of the pre-training corpus to approximate student behavior. Offline teacher inference computes probability distributions for the entire corpus, which are then compared with reference model probabilities using Difference Sampling. This process down-samples easy and common patterns, up-samples hard and diverse knowledge, and filters noisy data. The refined corpus is then used to pre-train student models from scratch. The method achieves computational efficiency by performing teacher inference only once and storing results offline, enabling multiple student models to be pre-trained without additional teacher computation overhead.

## Key Results
- 2.4× reduction in pre-training data requirements while maintaining or improving performance
- Improved student LM performance on 9 downstream tasks compared to baselines
- Enhanced language modeling capabilities with better data utilization
- Support for knowledge distillation across different model families (Qwen family in experiments)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Difference Sampling improves pre-training data difficulty and diversity by leveraging the gap between teacher and reference model probabilities.
- Mechanism: Down-samples easy and common patterns where both teacher and reference models assign high probabilities, up-samples hard and diverse knowledge where teacher outperforms reference, and filters out noisy data where reference outperforms teacher.
- Core assumption: The relative data difficulty remains consistent across different model sizes, allowing a small reference model to approximate student model behavior.
- Evidence anchors:
  - [abstract] "MiniPLM leverages the differences between large and small LMs to enhance the training data difficulty and diversity"
  - [section] "Difference Sampling samples training instances that the teacher LM prefers but that a small reference LM assigns low probabilities to"
  - [corpus] Weak - the corpus doesn't provide direct evidence about data difficulty consistency across model sizes
- Break condition: If the relative difficulty ordering changes significantly between reference and student models, the sampling strategy would become ineffective.

### Mechanism 2
- Claim: Offline teacher inference enables efficient knowledge distillation across multiple student models without additional training-time overhead.
- Mechanism: Teacher model probabilities are computed once and stored, allowing multiple student models to be pre-trained from the same refined corpus without re-computing teacher probabilities.
- Core assumption: Teacher model inference can be performed offline and stored efficiently (200MB for 50B tokens with 1024 sequence length).
- Evidence anchors:
  - [abstract] "MiniPLM performs offline teacher inference, allowing KD for multiple student LMs without adding training costs"
  - [section] "MINI PLM relies only on p(x) and pref(x), the teacher and reference LMs' probability over the entire sequence, which can be computed and stored offline"
  - [corpus] Weak - no specific evidence about storage efficiency provided
- Break condition: If teacher model inference becomes too expensive to store offline, or if per-token distributions become necessary for effective distillation.

### Mechanism 3
- Claim: Difference Sampling achieves better data diversity than sequence-level KD methods while maintaining mode-seeking behavior.
- Mechanism: By comparing teacher and reference model probabilities, Difference Sampling removes noisy patterns while up-sampling diverse hard instances, compensating for diversity loss through targeted sampling.
- Core assumption: The combination of down-sampling easy patterns and up-sampling hard diverse instances results in overall higher data diversity.
- Evidence anchors:
  - [section] "Difference Sampling increases the diversity of the refined pre-training distribution, which helps LM pre-training"
  - [section] "Difference Sampling down-samples the easy parts of the corpus containing repeated contents while up-sampling the hard parts consisting of diverse texts"
  - [corpus] Weak - the corpus doesn't provide direct evidence about diversity improvement mechanisms
- Break condition: If the diversity gain from up-sampling hard instances is insufficient to compensate for the loss from down-sampling easy patterns.

## Foundational Learning

- Concept: Kullback-Leibler divergence and its variants (forward vs reverse KL)
  - Why needed here: Understanding the optimization objective for knowledge distillation and why reverse KL is preferred for avoiding overestimation in low-probability regions
  - Quick check question: Why does minimizing reverse KL divergence encourage the student to focus on high-reward regions preferred by the teacher?

- Concept: Scaling laws for language models
  - Why needed here: Understanding how model performance scales with size and compute, which is crucial for evaluating MiniPLM's efficiency gains
  - Quick check question: According to scaling laws, how does the required training compute scale with model size when trying to match the performance of larger models?

- Concept: Reinforcement learning concepts (reward maximization, Best-of-N sampling)
  - Why needed here: Understanding how knowledge distillation can be formulated as a reward maximization problem and how sampling strategies affect learning
  - Quick check question: In the context of MiniPLM, what does the reward function measure and how does it relate to teacher model preferences?

## Architecture Onboarding

- Component map:
  Teacher model (1.8B Qwen-1.5) -> Reference model (104M) -> Pre-training corpus (50B tokens) -> Difference Sampling module -> Student model training pipeline

- Critical path:
  1. Train reference model on small subset (5B tokens)
  2. Compute teacher and reference probabilities for all instances in corpus
  3. Apply Top-K selection based on probability differences
  4. Pre-train student model on refined corpus
  5. Evaluate on downstream tasks

- Design tradeoffs:
  - Reference model size vs accuracy: Larger reference models provide better approximations but require more computation
  - Sampling ratio α vs data quality: Lower ratios yield better quality data but require larger original corpora
  - Teacher model size vs effectiveness: Very large teachers may not provide additional benefits due to scale differences

- Failure signatures:
  - If student performance is worse than pre-training without KD: Reference model is too small or sampling ratio is incorrect
  - If computational savings are minimal: Difference Sampling is not effectively filtering easy patterns
  - If diversity is reduced: Up-sampling of hard instances is insufficient to compensate for down-sampling

- First 3 experiments:
  1. Train reference model and verify it approximates student behavior by comparing probability distributions on held-out data
  2. Run Difference Sampling on small subset (1B tokens) and verify the three effects (down-sampling easy, up-sampling hard, filtering noise) are present
  3. Pre-train small student (200M) on refined data and compare performance against baseline on simple downstream task (e.g., LAMBADA)

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the size of the teacher model affect the effectiveness of MiniPLM in knowledge distillation?
- Basis in paper: [explicit] The paper mentions that larger teacher models are not necessarily more helpful for KD, as observed in recent works. However, the reason for this is attributed to different factors for Vanilla KD and MiniPLM.
- Why unresolved: The paper provides a general explanation but does not delve into the specific reasons why a 500M teacher model is most effective for distilling into a 200M student model.
- What evidence would resolve it: Detailed experiments comparing the performance of MiniPLM with different teacher model sizes, along with an analysis of the log probability values of the teacher and reference models.

### Open Question 2
- Question: Can MiniPLM be applied to pre-train language models larger than the teacher model, enabling weak-to-strong generalization?
- Basis in paper: [inferred] The paper mentions that a promising future direction is to apply the difference-sampled corpus to pre-train LMs larger than the teacher LM, enabling weak-to-strong generalization.
- Why unresolved: The paper does not explore this direction and only speculates on its potential benefits.
- What evidence would resolve it: Experiments demonstrating the performance of MiniPLM when pre-training LMs larger than the teacher model, along with an analysis of the generalization capabilities.

### Open Question 3
- Question: How does the diversity of the difference-sampled data affect the performance of the student model?
- Basis in paper: [explicit] The paper discusses the impact of Difference Sampling on improving the diversity of the pre-training corpus, which helps LM pre-training.
- Why unresolved: The paper provides a theoretical explanation but does not provide concrete evidence of the impact of data diversity on the student model's performance.
- What evidence would resolve it: Experiments comparing the performance of the student model when trained on data with different levels of diversity, along with an analysis of the semantic diversity of the difference-sampled corpus.

## Limitations

- Empirical evidence is based on limited downstream tasks and model sizes, constraining generalizability
- Heavy reliance on the assumption that reference model approximates student behavior without direct validation
- Computational savings claims are based on theoretical estimates rather than comprehensive benchmarking
- Limited evaluation of cross-domain generalization beyond The Pile corpus

## Confidence

**High confidence**: The core mechanism of Difference Sampling (down-sampling easy patterns, up-sampling hard diverse instances, filtering noise) is well-supported by the theoretical framework and basic empirical results. The claim that offline teacher inference enables efficient KD across multiple student models is straightforward and well-demonstrated.

**Medium confidence**: The claims about 2.4× reduction in pre-training data requirements and improvements across 9 downstream tasks are supported by experiments, but the evaluation scope is limited. The assertion that MiniPLM supports KD across model families is based on experiments with Qwen models only.

**Low confidence**: The claim about achieving better data diversity than sequence-level KD methods is primarily theoretical rather than empirically validated through diversity metrics. The effectiveness of Difference Sampling for different corpus domains (beyond The Pile) is not demonstrated.

## Next Checks

1. **Reference model approximation validation**: Conduct systematic experiments comparing probability distributions between reference, student, and teacher models on held-out data to verify that the reference model adequately approximates student behavior across different data patterns.

2. **Diversity measurement validation**: Implement and report quantitative diversity metrics (e.g., Self-BLEU, n-gram diversity scores) comparing the original corpus, Difference-Sampled corpus, and traditional KD approaches to empirically validate diversity improvement claims.

3. **Cross-domain generalization study**: Apply MiniPLM to a different corpus domain (e.g., biomedical or code datasets) and evaluate whether the three effects of Difference Sampling (down-sampling, up-sampling, noise filtering) remain consistent and beneficial across domains.