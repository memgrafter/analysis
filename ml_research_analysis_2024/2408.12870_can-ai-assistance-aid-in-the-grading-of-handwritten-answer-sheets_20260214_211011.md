---
ver: rpa2
title: Can AI Assistance Aid in the Grading of Handwritten Answer Sheets?
arxiv_id: '2408.12870'
source_url: https://arxiv.org/abs/2408.12870
tags:
- answer
- grading
- time
- question
- handwritten
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces an AI-assisted grading pipeline to aid in
  grading handwritten answer sheets. The pipeline automatically detects question regions
  in the question paper PDF and highlights important keywords in the handwritten answer
  regions of scanned answer sheets.
---

# Can AI Assistance Aid in the Grading of Handwritten Answer Sheets?

## Quick Facts
- arXiv ID: 2408.12870
- Source URL: https://arxiv.org/abs/2408.12870
- Reference count: 7
- AI-based keyword highlighting reduced grading time by 31% per response and 33% per answer sheet

## Executive Summary
This paper introduces an AI-assisted grading pipeline for handwritten answer sheets that automatically detects question regions in question papers and highlights important keywords in student answers. The system was evaluated on 5 real-life examinations across 4 courses with 468 submissions from 17 graders. Results showed significant time savings of 31% per response and 33% per answer sheet when using the AI assistance, demonstrating that keyword highlighting can effectively reduce grading effort.

## Method Summary
The pipeline processes blank question paper PDFs to detect question regions and extract text, then uploads scanned answer sheets to extract student identifiers and crop handwritten answer regions. It uses text detection methods (docTR library) to highlight keywords in student answers and presents results through a manual grading interface that logs grading time. Instructors provide relevant keywords for each question, and the system automatically maps answer sheets to students while enabling manual corrections when needed.

## Key Results
- Graders took 31% less time per response with AI assistance
- Graders took 33% less time per answer sheet with AI assistance
- Variability in grading time was significantly reduced when using keyword highlighting

## Why This Works (Mechanism)

### Mechanism 1
- Claim: AI-based keyword highlighting reduces cognitive load by directing attention to relevant regions
- Mechanism: Text detection identifies question regions and highlights important keywords in answer regions, enabling quick location of key parts
- Core assumption: Graders primarily look for specific keywords when evaluating answers
- Evidence: Abstract mentions SOTA text detection methods for keyword highlighting; graders look for specific keywords
- Break condition: If answers are verbose without clear keyword context, graders may still need full reading

### Mechanism 2
- Claim: Automated extraction of student identifiers reduces administrative overhead
- Mechanism: OCR extracts text from predefined bounding boxes for name and roll number
- Core assumption: OCR model is sufficiently accurate for most cases
- Evidence: Pipeline uses text detection methods; mappings shown to instructor with manual correction option
- Break condition: Frequent OCR misidentification would make manual correction time-consuming

### Mechanism 3
- Claim: AI assistance leads to more consistent grading by reducing variability
- Mechanism: Highlighting ensures graders focus on same key elements, leading to uniform evaluation
- Core assumption: Graders consistently focus on highlighted keywords rather than other parts
- Evidence: Figure 3 shows significantly reduced grading time variability
- Break condition: Over-reliance on keywords might cause overlooking important contextual information

## Foundational Learning

- **Text Detection and OCR**
  - Why needed: System relies on accurate text detection to identify question regions and extract student identifiers
  - Quick check: What challenges exist in developing OCR models for handwritten text that could affect pipeline performance?

- **Natural Language Processing for Keyword Extraction**
  - Why needed: NLP techniques identify and highlight important keywords crucial for efficient grading
  - Quick check: How might keyword extraction method choice impact AI assistance effectiveness?

- **User Interface Design for AI-Assisted Grading**
  - Why needed: Effectiveness depends on how well AI results are presented to graders through the interface
  - Quick check: What design considerations create an intuitive interface for presenting AI-generated keyword highlights?

## Architecture Onboarding

- **Component map**: PDF Processing -> Text Detection -> OCR Extraction -> Keyword Highlighting -> User Interface -> Logging System
- **Critical path**: 1) Upload question paper PDF and define regions, 2) Upload scanned answer sheets and define bounding boxes, 3) AI pipeline processes sheets extracting information and highlighting keywords, 4) Grader reviews highlighted answers and inputs grades, 5) System logs grading metrics
- **Design tradeoffs**: Accuracy vs. Speed (complex models vs. processing time), Flexibility vs. Standardization (customization vs. consistency)
- **Failure signatures**: High manual correction rates for identifier extraction, inconsistent grading times across graders, grader complaints about keyword relevance
- **First 3 experiments**: 1) Test accuracy of student identifier extraction on sample sheets, 2) Evaluate relevance of automatically extracted keywords with subject matter experts, 3) Conduct small-scale grading trial comparing time and consistency with/without AI assistance

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does AI-assisted keyword highlighting improve grading accuracy in addition to reducing grading time?
- Basis: Paper only evaluates time reduction, not grading accuracy or quality
- Why unresolved: Evaluation focused solely on time metrics without analyzing grade accuracy or consistency
- Evidence needed: Controlled study comparing grades assigned with/without AI assistance with statistical analysis

### Open Question 2
- Question: How does system performance vary across different handwriting styles and quality levels?
- Basis: Paper mentions no handwritten text detector is 100% accurate but lacks detailed performance analysis across handwriting qualities
- Why unresolved: Evaluation doesn't break down performance by handwriting quality or style
- Evidence needed: Systematic testing with handwritten samples varying in quality, style, and legibility

### Open Question 3
- Question: What is the optimal number and selection of keywords for effective grading assistance?
- Basis: Instructors provide keywords but criteria for selection are not specified
- Why unresolved: Study doesn't investigate how keyword selection strategies impact grading efficiency
- Evidence needed: Comparative studies testing different keyword selection strategies and their impact

### Open Question 4
- Question: How does AI-assisted grading system perform for different question types beyond those tested?
- Basis: Paper mentions testing on various question types but lacks comprehensive analysis across all types
- Why unresolved: Evaluation doesn't cover all possible question types in real-world scenarios
- Evidence needed: Systematic testing across wide range of common examination question types

## Limitations

- Results may not generalize to different educational contexts, subjects, or examination formats
- Keyword selection methodology lacks clear criteria and scalability discussion
- Technical implementation details insufficient for independent reproduction

## Confidence

- **High Confidence**: AI-based keyword highlighting can reduce grading time (supported by concrete experimental results)
- **Medium Confidence**: Reduced variability in grading time (supported by data but interpretation may be oversimplified)
- **Low Confidence**: Reduced cognitive load for graders (not directly measured or validated)

## Next Checks

1. Test the AI-assisted grading pipeline on examinations from different subjects to assess generalizability beyond STEM courses
2. Conduct controlled study where subject matter experts evaluate relevance of automatically highlighted keywords versus manually selected keywords
3. Track grader performance and satisfaction over extended periods to identify potential issues like keyword blindness or over-reliance on highlighting system