---
ver: rpa2
title: 'Komodo: A Linguistic Expedition into Indonesia''s Regional Languages'
arxiv_id: '2403.09362'
source_url: https://arxiv.org/abs/2403.09362
tags:
- language
- languages
- output
- indonesian
- yang
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Komodo-7B, a family of 7-billion-parameter
  language models tailored for Indonesian and 11 regional languages in Indonesia.
  The authors address the lack of high-performing language models for regional languages
  by training on a diverse dataset including textbooks, colloquial data, and regional
  language corpora.
---

# Komodo: A Linguistic Expedition into Indonesia's Regional Languages

## Quick Facts
- arXiv ID: 2403.09362
- Source URL: https://arxiv.org/abs/2403.09362
- Reference count: 40
- Key outcome: Komodo-7B-Instruct achieves state-of-the-art performance across Indonesian and 11 regional languages, enabling direct English-to-regional-language translations

## Executive Summary
This paper introduces Komodo-7B, a family of 7-billion-parameter language models specifically designed for Indonesian and 11 regional languages in Indonesia. The authors address the critical gap in high-performing language models for regional languages by developing a comprehensive training approach that includes tokenizer expansion, alternate parallel training, and LORA-based incremental pretraining. Komodo-7B-Instruct demonstrates superior performance across multiple tasks and languages, outperforming established models like GPT-3.5, Llama-2, and Aya-101, while maintaining competitive English capabilities.

## Method Summary
Komodo-7B is built by first expanding the Llama-2 tokenizer vocabulary by approximately 3,000 tokens specific to Indonesian and regional languages, then performing incremental pretraining on 8.79 billion tokens using LORA for 3 epochs to adapt to regional languages while preventing catastrophic forgetting. The model is then fine-tuned using Supervised Fine-Tuning (SFT) on diverse tasks for 5 epochs with LORA. The training incorporates alternate parallel sentences in English and Indonesian (and all language combinations) to improve cross-lingual alignment, enabling direct translations without intermediate languages.

## Key Results
- Komodo-7B-Instruct achieves state-of-the-art performance across multiple tasks and 12 languages (English + 11 regional)
- Tokenizer fertility score improved by up to 28.90% for Indonesian and 24.90% for regional languages
- Enables direct translations from English to 11 regional languages without intermediate steps
- Outperforms GPT-3.5, Llama-2, and Aya-101 on benchmark tasks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Expanded tokenizer reduces fertility scores for Indonesian and regional languages by up to 28.90% and 24.90% respectively
- Mechanism: Adding 3,000 frequently used Indonesian and regional language words reduces average tokens needed per text
- Core assumption: Additional tokens correspond to high-frequency words that meaningfully reduce token fragmentation
- Evidence anchors: Abstract mentions fertility score improvements; corpus section notes vocabulary expansion but lacks direct evidence

### Mechanism 2
- Claim: Alternate parallel training improves cross-lingual alignment for direct translations
- Mechanism: Bilingual next-token prediction with alternate sentences forces cross-lingual attention during training
- Core assumption: Cross-lingual attention during training leads to better cross-lingual understanding during inference
- Evidence anchors: Abstract mentions direct translation capability; section describes mechanism but lacks empirical validation

### Mechanism 3
- Claim: LORA-based incremental pretraining prevents catastrophic forgetting while improving regional language understanding
- Mechanism: Building on Llama-2-7B-Base with 3 epochs of LORA pretraining preserves English capabilities while adapting to regional languages
- Core assumption: LORA-based incremental pretraining preserves English capabilities while adapting to regional languages
- Evidence anchors: Abstract claims state-of-the-art performance; section mentions LORA approach but lacks specific English performance evidence

## Foundational Learning

- Concept: Fertility score in tokenization
  - Why needed here: Understanding fertility score is critical to evaluating the tokenizer's efficiency improvements and their impact on model performance and latency
  - Quick check question: What does a lower fertility score indicate about a tokenizer's performance?

- Concept: Catastrophic forgetting in continual learning
  - Why needed here: The paper uses incremental pretraining with LORA to avoid catastrophic forgetting, so understanding this concept is essential for evaluating the training approach
  - Quick check question: How does LORA help prevent catastrophic forgetting compared to full fine-tuning?

- Concept: Cross-lingual alignment and code-mixing
  - Why needed here: The model's ability to handle direct translations between English and regional languages without intermediate steps depends on effective cross-lingual alignment learned through alternate parallel training
  - Quick check question: Why might alternate parallel training be more effective than traditional bilingual training for code-mixing scenarios?

## Architecture Onboarding

- Component map: Base Llama-2-7B model → Expanded tokenizer (35,008 tokens) → Incremental pretraining with LORA (8.79B tokens, 3 epochs) → Supervised fine-tuning (SFT) with task-specific data → Komodo-7B-Instruct
- Critical path: Tokenizer expansion → Pretraining with LORA → SFT fine-tuning → Evaluation
- Design tradeoffs: Expanded vocabulary increases embedding matrix size and training time vs. improved tokenization efficiency; LORA reduces computational cost vs. potentially lower adaptation capacity compared to full fine-tuning
- Failure signatures: High fertility scores indicate tokenizer inefficiency; catastrophic forgetting shows as degraded English performance; poor cross-lingual performance suggests inadequate alternate parallel training
- First 3 experiments:
  1. Measure fertility scores on Indonesian and regional language test sets before and after tokenizer expansion
  2. Evaluate English performance on standard benchmarks (MMLU, HellaSwag) after incremental pretraining to check for catastrophic forgetting
  3. Test direct translation capability from English to each regional language vs. baseline models like Google Translate

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the impact of increasing the Komodo model size beyond 7B parameters on performance for regional languages?
- Basis in paper: [inferred]
- Why unresolved: The paper mentions future work on developing a 13B parameter variant but does not provide experimental results or comparisons to demonstrate the expected performance gains for regional languages
- What evidence would resolve it: Training and evaluating a 13B parameter Komodo model on the same benchmarks used for the 7B model, with a focus on regional language performance metrics

### Open Question 2
- Question: How does the Komodo model's performance on regional languages compare to a model specifically trained on each individual regional language?
- Basis in paper: [inferred]
- Why unresolved: The paper demonstrates Komodo's strong performance across 11 regional languages but does not compare it to specialized models trained exclusively on each language, which could provide insights into the trade-offs between multilingual and monolingual approaches
- What evidence would resolve it: Training and evaluating separate models for each regional language on the same benchmarks used for Komodo, then comparing the results to Komodo's performance

### Open Question 3
- Question: What is the long-term effectiveness of Komodo's approach to vocabulary expansion for regional languages in terms of model latency and inference speed?
- Basis in paper: [explicit]
- Why unresolved: While the paper shows Komodo's tokenizer fertility score improvements and mentions better latency, it does not provide detailed analysis of how the expanded vocabulary impacts inference speed and latency in real-world deployment scenarios
- What evidence would resolve it: Benchmarking Komodo's inference latency and throughput on representative regional language tasks, comparing it to baseline models and analyzing the relationship between vocabulary size and inference performance

## Limitations
- Data provenance limitations: The paper does not provide detailed statistics about the regional language corpora used for pretraining
- Evaluation methodology gaps: The evaluation primarily focuses on aggregate metrics without detailed ablation studies
- Generalization concerns: Performance disparities between high-resource and low-resource regional languages are not differentiated

## Confidence

**High confidence**: Tokenizer expansion mechanism and documented fertility score improvements (28.90% for Indonesian, 24.90% for regional languages) are well-supported by the described methodology

**Medium confidence**: Cross-lingual alignment claims through alternate parallel training are plausible but lack empirical validation in the paper

**Low confidence**: Catastrophic forgetting prevention through LORA-based incremental pretraining is supported by LORA literature generally, but the paper provides no specific evidence that English performance was maintained

## Next Checks

1. **Ablation study on tokenizer expansion**: Train a baseline Komodo-7B model without tokenizer expansion (using standard Llama-2 tokenizer) and compare fertility scores and task performance on Indonesian and regional language benchmarks to isolate the contribution of the expanded vocabulary

2. **Cross-lingual capability testing**: Systematically evaluate the model's ability to perform direct English-to-regional-language translations across all 11 languages using standardized translation benchmarks, comparing against both traditional bilingual baselines and the claimed direct translation capability

3. **Language-specific performance analysis**: Break down evaluation metrics by individual regional language to identify performance disparities between high-resource and low-resource languages, and assess whether the training approach provides equitable benefits across all 11 target languages