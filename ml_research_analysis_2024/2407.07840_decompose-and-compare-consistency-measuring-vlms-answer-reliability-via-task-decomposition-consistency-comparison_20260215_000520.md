---
ver: rpa2
title: 'Decompose and Compare Consistency: Measuring VLMs'' Answer Reliability via
  Task-Decomposition Consistency Comparison'
arxiv_id: '2407.07840'
source_url: https://arxiv.org/abs/2407.07840
tags:
- answer
- consistency
- question
- reliability
- cons
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces Decompose and Compare Consistency (DeCC),
  a method for assessing the reliability of vision-language model (VLM) responses
  by comparing the consistency between direct answers and answers derived through
  task decomposition. DeCC first decomposes the question into sub-questions, generates
  sub-QA pairs using the candidate VLM, and then employs both the VLM and a separate
  LLM to reason over these pairs to produce reasoned answers.
---

# Decompose and Compare Consistency: Measuring VLMs' Answer Reliability via Task-Decomposition Consistency Comparison

## Quick Facts
- **arXiv ID**: 2407.07840
- **Source URL**: https://arxiv.org/abs/2407.07840
- **Reference count**: 35
- **Key outcome**: DeCC achieves better correlation with task accuracy compared to uncertainty-based metrics and self-consistency, reducing relative mean Brier Score by 8.7% to 14.3% and increasing relative mean Effective Reliability by 16.5% to 25.6%.

## Executive Summary
This paper introduces Decompose and Compare Consistency (DeCC), a method for assessing the reliability of vision-language model (VLM) responses by comparing the consistency between direct answers and answers derived through task decomposition. The approach decomposes questions into sub-questions, generates sub-QA pairs using the candidate VLM, and employs both the VLM and a separate LLM to reason over these pairs to produce reasoned answers. The consistency between the direct answer and the reasoned answers is compared to determine reliability. Experiments on six vision-language tasks with three different VLMs demonstrate that DeCC outperforms existing reliability assessment methods.

## Method Summary
DeCC measures VLM answer reliability by decomposing questions into sub-questions, generating sub-QA pairs, and comparing the consistency between direct answers and reasoned answers obtained through multi-agent reasoning. The method uses a decomposer (typically another VLM) to break down complex questions into simpler sub-questions, which the candidate VLM answers. Both the candidate VLM and an independent LLM then reason over these sub-QA pairs to generate reasoned answers. The consistency between the direct answer and these reasoned answers is compared to assess reliability, with different settings (single-agent vs. multi-agent) performing better for VLMs of different capabilities.

## Key Results
- DeCC reduces relative mean Brier Score by 8.7% to 14.3% compared to baseline methods
- DeCC increases relative mean Effective Reliability by 16.5% to 25.6% across three VLMs
- The effectiveness of different consistency comparison settings correlates with the candidate VLM's capabilities

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Decomposing a question into sub-questions exposes reasoning gaps in VLMs by forcing them to break down complex understanding into simpler components.
- Mechanism: The decomposer generates sub-questions that target key visual and contextual aspects of the original question. When the candidate VLM answers these sub-questions, discrepancies between its direct answer and the combined reasoning over sub-answers reveal unreliable reasoning patterns.
- Core assumption: If a VLM understands the question well and conducts reliable reasoning, a conflict is less likely to occur between its direct answer and the decomposed answer.
- Evidence anchors:
  - [abstract] "By comparing the consistency between the direct answer generated using the VLM's internal reasoning process, and the indirect answers obtained by decomposing the question into sub-questions and reasoning over the sub-answers produced by the VLM, DeCC measures the reliability of VLM's direct answer."
  - [section 3.1] "The candidate VLM then answers these sub-questions, generating a sequence of sub-QA pairs. We use both the candidate VLM and a separate LLM, acting as two independent agents, to reason over the sub-QA pairs and obtain their respective reasoned answers."
  - [corpus] No direct evidence found; this is an assumption based on the paper's methodology.
- Break condition: If the decomposer fails to generate meaningful sub-questions that isolate key reasoning components, the method loses its ability to expose reasoning gaps.

### Mechanism 2
- Claim: Using an independent LLM to reason over sub-QA pairs provides an objective assessment that mitigates the candidate VLM's inherent biases.
- Mechanism: The LLM operates under incomplete information (lack of image) but uses textual decomposition information to reason over sub-QA pairs. This provides a more objective assessment compared to the VLM's reasoning, which might suffer from biases toward certain responses.
- Core assumption: "The LLM's response is more likely to change since it is operating under incomplete information... So a change in VLM's response indicates it potentially overcame its biases with additional sub-QA pairs."
- Evidence anchors:
  - [section 3.2] "We trust the LLM's consistency check, as it provides a more objective assessment, relying solely on textual decomposition information, whereas the VLM might suffer from its inherent biases towards certain responses."
  - [corpus] No direct evidence found; this is an assumption based on the paper's methodology.
- Break condition: If the LLM's reasoning is too disconnected from the visual context, it may provide assessments that don't accurately reflect the reliability of VLM answers.

### Mechanism 3
- Claim: The effectiveness of different consistency comparison settings correlates with the candidate VLM's capabilities, allowing the method to adapt to different model strengths.
- Mechanism: For weaker VLMs, LLM Agent Consistency performs best because VLMs struggle to reason over sub-QA pairs. For stronger VLMs, Multi-Agent Consistency works best as the agents complement each other. For the strongest VLMs, VLM Agent Consistency (self-consistency) is optimal as the VLM can effectively leverage sub-QA pair information.
- Core assumption: "The effectiveness of different consistency comparison settings correlates with the candidate VLM's capabilities."
- Evidence anchors:
  - [section 4.3] "We observe an interesting trend: (1) For weaker VLMs, i.e., LLaV A, LLM Agent Consistency achieves the best performance... (2) For stronger VLMs, i.e. Idefics2, Multi-Agent Consistency performs the best... (3) For the strongest VLMs, i.e. InternVL, VLM Agent Consistency (self-consistency) achieves the best performance..."
  - [corpus] No direct evidence found; this is an observation from the paper's experimental results.
- Break condition: If the correlation between VLM capabilities and consistency setting effectiveness doesn't hold across different tasks or VLM architectures, the adaptive mechanism breaks down.

## Foundational Learning

- Concept: Task decomposition and sub-question generation
  - Why needed here: The method relies on breaking down complex questions into simpler components that can be answered independently and then reasoned over collectively.
  - Quick check question: Can you explain why decomposing a question about "Which person is everyone staring at?" into sub-questions about visibility and direction helps assess reliability?

- Concept: Consistency comparison and reliability measurement
  - Why needed here: The core of the method is comparing answers from different reasoning processes to determine reliability, requiring understanding of consistency metrics and their interpretation.
  - Quick check question: How does comparing a VLM's direct answer with its reasoned answer over sub-QA pairs help identify unreliable responses?

- Concept: Multi-agent collaboration and bias mitigation
  - Why needed here: The method uses both the candidate VLM and an independent LLM to provide complementary assessments and reduce individual model biases.
  - Quick check question: Why might using both a VLM and LLM to reason over the same sub-QA pairs provide more reliable reliability assessments than using either alone?

## Architecture Onboarding

- Component map: Decomposer -> Candidate VLM -> LLM -> Consistency checker -> Reliability scorer
- Critical path: Decompose → Answer sub-questions → Reason over sub-QA pairs (VLM + LLM) → Compare consistency → Output reliability score
- Design tradeoffs:
  - Using LLM vs. VLM for decomposition: LLM may be more consistent but less task-specific; VLM may better understand visual context but could be less reliable
  - Single-agent vs. multi-agent consistency: Single-agent is simpler but more prone to bias; multi-agent is more robust but computationally heavier
  - Number of sub-questions: More sub-questions provide more information but increase computational cost and potential for cascading errors
- Failure signatures:
  - Low reliability scores across all questions: Decomposer may be generating poor sub-questions or consistency comparison may be too strict
  - High reliability scores but low task accuracy: Consistency comparison may be too lenient or decomposer may be generating sub-questions that don't effectively test understanding
  - Inconsistent reliability scores across similar questions: Decomposition process may not be consistent or sub-question generation may be sensitive to minor question variations
- First 3 experiments:
  1. Implement basic decomposition and consistency comparison with a single VLM, measure Brier Score on a small dataset
  2. Add LLM reasoning component and compare performance between single-agent and multi-agent settings
  3. Test with different numbers of sub-questions and different decomposition strategies to optimize reliability measurement

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of DeCC vary with different types of decomposition strategies, and can the decomposition process be optimized to improve reliability measurement?
- Basis in paper: [inferred]
- Why unresolved: The paper mentions that the effectiveness of the framework is influenced by the decomposition process but does not fully explore the optimization and impact of different decomposition strategies. This suggests that the performance of DeCC could be significantly affected by how questions are decomposed.
- What evidence would resolve it: Conducting experiments with various decomposition strategies and analyzing their impact on the reliability measurement would provide insights into how to optimize the decomposition process.

### Open Question 2
- Question: How does the choice of LLM for reasoning affect the robustness and generalization of DeCC's reliability measurements?
- Basis in paper: [explicit]
- Why unresolved: The paper notes that they used only one LLM for reasoning in the multi-agent part and suggests that conducting experiments with various LLMs would help assess the generalization and robustness of the framework. This indicates that the choice of LLM could influence the reliability of the measurements.
- What evidence would resolve it: Performing experiments with multiple LLMs and comparing the reliability measurement results would help determine the impact of different LLMs on the robustness and generalization of DeCC.

### Open Question 3
- Question: How do different consistency comparison settings (single-agent vs. multi-agent) affect the reliability measurement across various VLM capabilities, and can these settings be tailored to specific VLM strengths?
- Basis in paper: [explicit]
- Why unresolved: The paper observes that the effectiveness of different consistency comparison settings correlates with the candidate VLM's capabilities but does not provide a detailed analysis of how these settings can be optimized for specific VLM strengths. This suggests that there may be room for improvement in tailoring the settings to enhance reliability measurement.
- What evidence would resolve it: Conducting a detailed analysis of how different consistency comparison settings perform across a wider range of VLM capabilities and identifying patterns that could inform tailored settings would help optimize the reliability measurement process.

## Limitations
- The method is evaluated on six specific vision-language tasks, which may not represent the full diversity of VLM applications.
- The paper lacks detailed specifications for critical components like the exact few-shot prompts used for decomposition and the specific threshold values for reliability scoring.
- The method requires multiple forward passes through both VLMs and LLMs, potentially making it computationally expensive for real-time applications.

## Confidence

- **High Confidence**: The experimental results showing DeCC's improved correlation with task accuracy compared to uncertainty-based metrics and self-consistency are well-supported by the data across all six tasks and three VLMs.
- **Medium Confidence**: The claim that decomposer consistency improves with model scale is supported by the data, but the mechanism behind this relationship could be more thoroughly explained.
- **Medium Confidence**: The observation that different consistency settings work better for different VLM capabilities is empirically demonstrated but may not generalize to all VLM architectures.

## Next Checks

1. **Cross-task generalization test**: Apply DeCC to a new set of vision-language tasks (e.g., visual reasoning tasks with temporal components or tasks requiring multi-step planning) to assess whether the decomposition and consistency mechanisms generalize beyond the six tasks tested.

2. **Ablation study on decomposition quality**: Systematically vary the quality of sub-questions generated by the decomposer (using different few-shot prompts or manual quality control) and measure the impact on reliability assessment accuracy to quantify the importance of decomposition quality.

3. **Real-time applicability assessment**: Measure the computational overhead of DeCC across different VLM scales and compare it to the performance gains to determine if the method is practical for real-time applications or best suited for offline reliability assessment.