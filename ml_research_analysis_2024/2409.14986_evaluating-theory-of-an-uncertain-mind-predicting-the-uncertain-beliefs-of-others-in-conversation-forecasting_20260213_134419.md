---
ver: rpa2
title: 'Evaluating Theory of (an uncertain) Mind: Predicting the Uncertain Beliefs
  of Others in Conversation Forecasting'
arxiv_id: '2409.14986'
source_url: https://arxiv.org/abs/2409.14986
tags:
- uncertainty
- language
- data
- variance
- about
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper studies how language models predict the uncertainty
  of other speakers' beliefs in conversation. The authors frame this as a regression
  problem, where models predict probabilities rather than binary outcomes, motivated
  by the idea that people often hold uncertain beliefs about others' opinions and
  feelings.
---

# Evaluating Theory of (an uncertain) Mind: Predicting the Uncertain Beliefs of Others in Conversation Forecasting

## Quick Facts
- arXiv ID: 2409.14986
- Source URL: https://arxiv.org/abs/2409.14986
- Authors: Anthony Sicilia; Malihe Alikhani
- Reference count: 21
- Language models can explain up to 7% variance in others' uncertainty beliefs in conversation

## Executive Summary
This paper studies how language models predict the uncertainty of other speakers' beliefs in conversation by framing it as a regression problem where models predict probabilities rather than binary outcomes. The authors introduce three tasks (1st-order, 2nd-order, and false uncertainty quantification) and apply scaling methods, bagging, and demographic context to improve predictions. Experiments on three dialogue corpora show models can explain up to 7% variance in others' uncertainty, with improvements from post-hoc scaling and bagging strategies. The work highlights both the promise and difficulty of uncertainty-aware Theory of Mind modeling.

## Method Summary
The authors frame uncertainty prediction as a regression problem, using human Likert-scale annotations mapped to probabilities through a "more than chance" calibration strategy. They apply direct forecasting with Chain-of-Thought prompting, post-hoc scaling (linear and Platt), fine-tuning regression heads, and Bag of Thoughts (BoT) variance reduction. The approach is evaluated across three dialogue corpora using regression metrics including R², MAE, and correlation coefficients.

## Key Results
- Models achieve up to 7% explained variance (R²) in predicting interlocutors' uncertainty beliefs
- Post-hoc scaling and fine-tuning improve direct forecasting performance
- Bagging and BoT strategies reduce variance and improve small model predictions
- Performance varies significantly across dialogue domains (CaSiNo: 7.0%, CANDOR: 2.5%, MultiWOZ: 0.0% R²)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Language models can explain variance in others' uncertainty when given calibrated probability estimates
- Mechanism: Human Likert-scale annotations are mapped to probabilities by comparing belief intensity to the distribution of all annotations, enabling direct comparison with ground-truth probabilities
- Core assumption: Human expressions of belief intensity can be meaningfully mapped to probabilities using the "more than chance" formulation
- Evidence anchors: [abstract] "belief intensity needs calibration to a world outcome to make sense as a probability"; [section] "ground-truth probability is defined by a p-value for the magnitude of the belief"

### Mechanism 2
- Claim: Variance reduction strategies like bagging and BoT improve language model performance on regression tasks
- Mechanism: Bagging reduces variance by averaging predictions from models trained on random samples; BoT reduces variance by averaging multiple Chain-of-Thought inferences from the same model
- Core assumption: Variance in language model predictions is a significant source of error that can be reduced through aggregation
- Evidence anchors: [section] "Indeed, like bagging, one can show BoT lowers the variance of the direct forecasts"; [section] "Our hypothesis is that variance reduction in feature space (by BoT) actually increases the signal-to-noise-ratio"

### Mechanism 3
- Claim: Including demographic information in prompts can improve language model predictions by providing context
- Mechanism: Demographic data adds situational context, which is an important aspect of Theory of Mind; it may also help mitigate social biases in generative inference
- Core assumption: Demographic information is relevant to predicting uncertainty in others' beliefs and can improve model performance
- Evidence anchors: [section] "We hypothesize these characteristics reduce prediction bias because they add situational context"; [section] "language models are known to inherent and propagate certain social biases"

## Foundational Learning

- Concept: Regression tasks in language models
  - Why needed here: The paper frames uncertainty prediction as a regression problem, requiring models to predict probabilities rather than binary outcomes
  - Quick check question: What is the difference between regression and classification tasks in machine learning?

- Concept: Calibration of probability estimates
  - Why needed here: Human annotations of belief intensity need to be mapped to probabilities for comparison with ground-truth outcomes
  - Quick check question: How can you calibrate a model's confidence scores to match true probabilities?

- Concept: Theory of Mind in language models
  - Why needed here: The paper evaluates language models' ability to predict the uncertainty of others' beliefs, which is a form of Theory of Mind
  - Quick check question: What is Theory of Mind and why is it important for language models?

## Architecture Onboarding

- Component map:
  Prompt generation -> Inference engine -> Calibration module -> Evaluation module -> Aggregation module

- Critical path:
  1. Generate prompt with conversation context and demographic information
  2. Execute prompt and collect prediction
  3. Apply calibration (if necessary) to map prediction to probability
  4. Compute regression metrics to evaluate performance
  5. Apply aggregation strategies (bagging/BoT) to improve predictions

- Design tradeoffs:
  - Direct forecasting vs. fine-tuning: Direct forecasting is simpler but may require more post-hoc scaling; fine-tuning can improve performance but requires more data and training
  - Bagging vs. BoT: Bagging reduces variance by training multiple models, while BoT reduces variance by aggregating multiple inferences from the same model; BoT is simpler but may be less effective

- Failure signatures:
  - Poor R² scores: Indicates the model is not explaining variance in uncertainty predictions
  - High MAE: Indicates large errors in probability predictions
  - Negative R²: Indicates the model is performing worse than a constant mean prediction

- First 3 experiments:
  1. Direct forecasting with post-hoc scaling on CaSiNo corpus to establish baseline performance
  2. Fine-tuning a regression head on MultiWOZ corpus to compare with direct forecasting
  3. Applying bagging and BoT strategies to improve performance on CANDOR corpus

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does incorporating demographic information consistently improve language model performance in predicting interlocutors' uncertainty across different model sizes and tasks?
- Basis in paper: [explicit] The paper shows mixed results when including demographic context in prompts, with larger models showing more consistent improvement while smaller models sometimes experiencing performance degradation
- Why unresolved: The paper's ablation studies show that demographic context's effectiveness varies significantly by model size and specific task, with some configurations showing improvement while others show decline, suggesting complex interactions that require further investigation
- What evidence would resolve it: Systematic experiments varying model sizes, task types, and demographic context formats across multiple datasets would clarify under what conditions demographic information is beneficial or detrimental

### Open Question 2
- Question: What are the underlying reasons for language models' difficulty in predicting false uncertainty (FUnQ) compared to first-order (1TUQ) and second-order (2TUQ) uncertainty tasks?
- Basis in paper: [explicit] The paper shows that no model achieves positive explained variance on FUnQ even after scaling and fine-tuning, while some models perform reasonably well on 1TUQ and 2TUQ tasks
- Why unresolved: While the paper hypothesizes that compounding errors across multiple inference steps might be responsible, it doesn't systematically investigate alternative explanations such as the complexity of perspective shifting or the quality of ground-truth probability estimates
- What evidence would resolve it: Experiments isolating perspective-shifting errors, testing alternative architectures designed specifically for multi-step reasoning, and comparing against human performance on the same tasks would help identify the root causes

### Open Question 3
- Question: How does the Bag of Thoughts (BoT) variance reduction strategy interact with different scaling methods and fine-tuning approaches in language model regression tasks?
- Basis in paper: [explicit] The paper shows that BoT improves small model performance and reduces variance in feature space, but doesn't systematically explore how it interacts with different scaling methods or fine-tuning approaches
- Why unresolved: The paper demonstrates BoT's effectiveness in isolation but doesn't examine whether combining it with other techniques like fine-tuning or different scaling methods produces synergistic or antagonistic effects
- What evidence would resolve it: Comparative experiments systematically combining BoT with various scaling methods, fine-tuning approaches, and model architectures would reveal optimal combinations and underlying interaction mechanisms

## Limitations

- Calibration validity: The "more than chance" formulation for mapping Likert-scale annotations to probabilities relies on assumptions about human probability judgment that warrant further validation
- Generalization across domains: Performance varies significantly across dialogue types, with task-oriented dialogue showing particularly poor results
- Demographic context utility: The actual impact of including demographic information is not fully validated and shows mixed results

## Confidence

- High Confidence: The core finding that language models can explain variance in others' uncertainty beliefs (up to 7% R²) across three distinct dialogue corpora; variance reduction effects of bagging and BoT
- Medium Confidence: The claim that post-hoc scaling improves direct forecasting performance; the calibration strategy using "more than chance" formulation
- Low Confidence: The assertion that demographic information meaningfully improves predictions; the claim about social bias mitigation through demographic context inclusion

## Next Checks

1. Conduct experiments using the same model and methodology across a broader range of dialogue domains to quantify how well uncertainty prediction generalizes beyond the three tested corpora

2. Systematically remove demographic information from prompts in controlled experiments to measure its actual impact on prediction accuracy, distinguishing between genuine performance improvement versus correlation with other factors

3. Compare model uncertainty predictions against human uncertainty judgments in a controlled setting where humans predict others' beliefs in the same dialogues, establishing whether model performance correlates with human-level theory-of-mind reasoning