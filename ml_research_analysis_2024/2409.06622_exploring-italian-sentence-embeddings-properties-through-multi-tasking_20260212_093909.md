---
ver: rpa2
title: Exploring Italian sentence embeddings properties through multi-tasking
arxiv_id: '2409.06622'
source_url: https://arxiv.org/abs/2409.06622
tags:
- type
- sentence
- task
- tasks
- information
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates whether Italian language models encode
  abstract linguistic information through multi-task learning using Blackbird Language
  Matrices (BLM) tasks. The authors develop a two-level architecture that first compresses
  sentence embeddings to capture syntactic and semantic information, then uses these
  compressed representations to solve BLM tasks for subject-verb agreement and verb
  alternations.
---

# Exploring Italian sentence embeddings properties through multi-tasking

## Quick Facts
- arXiv ID: 2409.06622
- Source URL: https://arxiv.org/abs/2409.06622
- Authors: Vivi Nastase; Giuseppe Samo; Chunyang Jiang; Paola Merlo
- Reference count: 40
- Key outcome: Multi-task learning underperforms single-task training by 10-15 F1 points in Italian sentence embedding tasks for agreement and verb alternations

## Executive Summary
This paper investigates whether Italian language models encode abstract linguistic information through multi-task learning using Blackbird Language Matrices (BLM) tasks. The authors develop a two-level architecture that first compresses sentence embeddings to capture syntactic and semantic information, then uses these compressed representations to solve BLM tasks for subject-verb agreement and verb alternations. They create new BLM datasets for Italian covering three levels of lexical complexity. Their results show that multi-task training underperforms single-task training, with F1 scores dropping by 10-15 percentage points across tasks. Error analysis reveals that multi-task learning fails to properly encode chunk structure and grammatical number information crucial for agreement tasks, and struggles to represent semantic roles needed for verb alternation tasks.

## Method Summary
The authors use a two-level VAE architecture with Electra-based sentence embeddings to solve Italian BLM tasks. The first level compresses individual sentence embeddings into small latent vectors capturing chunk structure and phrase properties. The second level learns to map sequences of these compressed vectors to correct BLM answers. They create three BLM datasets for Italian (agreement, causative, object-drop alternations) with three lexical complexity levels. The model is trained using max-margin loss with contrastive examples, comparing single-task vs multi-task training configurations. Training uses 3000 instances (1000 per task for multi-task, 2160 for Caus/Od and 2052/3000 for agreement) with a 90:20:10 train:dev:test split.

## Key Results
- Multi-task training underperforms single-task training by 10-15 F1 points across agreement and verb alternation tasks
- Error analysis shows multi-task learning fails to properly encode chunk structure and grammatical number information crucial for agreement tasks
- Multi-task models struggle to represent semantic roles needed for verb alternation tasks, particularly for causative vs non-causative distinctions

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The two-level VAE architecture isolates sentence-level structural encoding from sequence-level BLM pattern detection
- Mechanism: The first level compresses Electra's CLS embeddings into small latent vectors capturing chunk structure and phrase properties. The second level learns to map sequences of these compressed vectors to correct BLM answers
- Core assumption: Abstract linguistic notions like constituents and thematic roles are represented separately in the compressed sentence representation
- Evidence anchors: [abstract] "We use a two-level architecture to model separately a compression of the sentence embeddings into a representation that contains relevant information for a task, and a BLM task." [section 4] "In this system, one level processes individual sentences and learns to compress them into small vectors that retain information pertinent to a task and the other level uses the compressed sentence representation to find patterns across an input sequence to solve a BLM task."
- Break condition: If the latent representation cannot capture chunk structure and grammatical properties simultaneously, the sequence-level pattern detector will fail to find the correct answer patterns

### Mechanism 2
- Claim: Multi-task learning fails because syntactic chunk structure and semantic role properties compete for encoding space in the compressed representation
- Mechanism: When trained jointly, the sentence-level encoder tries to capture both grammatical number (for agreement) and semantic roles (for verb alternations) in the same latent space, leading to interference and degraded performance
- Core assumption: The same compressed representation must encode both syntactic and semantic properties simultaneously
- Evidence anchors: [abstract] "performance and error analysis show that the clues for the different tasks are encoded in different manners in the sentence embeddings" [section 5] "Both tasks require information about the syntactic structure (or sequence of phrases), while each requires different phrase properties – grammatical number for the agreement task, and semantic properties for the verb alternation."
- Break condition: If the tasks had more overlapping requirements (e.g., both only needed grammatical number), multi-task learning might succeed

### Mechanism 3
- Claim: Single-task training outperforms multi-task because each task can optimize its own sentence-level encoder for its specific linguistic requirements
- Mechanism: With separate encoders, each can learn to extract exactly the features needed for its task without interference from competing requirements
- Core assumption: Task-specific optimization leads to better performance than shared representations when tasks have different feature requirements
- Evidence anchors: [abstract] "single-task training outperforms multi-tasking in the agreement and verb alternation subtasks" [section 5] "The drop suggests that the multi-task model is not able to learn shared properties for these tasks, and forcing it to do so leads to a model that is not optimal for either of them."
- Break condition: If the tasks were more similar (e.g., both agreement tasks with different languages), shared representations might be beneficial

## Foundational Learning

- Concept: Variational Autoencoder (VAE) architecture
  - Why needed here: The two-level system uses a VAE to learn compressed sentence representations that capture relevant linguistic structure
  - Quick check question: What is the purpose of the KL divergence term in the VAE loss function?

- Concept: Chunk structure and grammatical properties
  - Why needed here: The agreement task requires information about phrase boundaries and grammatical number, while verb alternations require semantic role information
  - Quick check question: How do you identify the grammatical number of a noun phrase in Italian?

- Concept: Thematic roles and verb alternations
  - Why needed here: The verb alternation tasks test whether the system can distinguish between causative and non-causative verb uses based on semantic roles
  - Quick check question: What is the difference between an Agent and a Patient in thematic role theory?

## Architecture Onboarding

- Component map: Electra pretrained model -> VAE encoder-decoder -> Task-level pattern detector -> Answer prediction
- Critical path: Input sentence → Electra CLS embedding → VAE compression → compressed representation → sequence pattern detection → answer prediction
- Design tradeoffs: Single-task vs multi-task training, size of compressed representation, number of contrastive samples
- Failure signatures: Poor performance on both tasks in multi-task setting, specific error patterns (WN1, WN2 for agreement; I-int for causatives)
- First 3 experiments:
  1. Train single-task agreement model and evaluate on all test types
  2. Train single-task causative alternation model and evaluate
  3. Train multi-task model with all three tasks and compare performance drop

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do the representations of syntactic structure (chunk sequences) differ between single-task and multi-task learning settings, and why do these differences lead to performance degradation in multi-task settings?
- Basis in paper: [explicit] The authors show that multi-task training underperforms single-task training by 10-15 percentage points and that error analysis reveals multi-task learning fails to properly encode chunk structure information crucial for agreement tasks
- Why unresolved: While the paper identifies that chunk structure is not properly encoded in multi-task settings, it does not provide a detailed analysis of how the representations of syntactic structure differ between single-task and multi-task settings or why this leads to performance degradation
- What evidence would resolve it: Detailed analysis of the learned representations (e.g., through probing tasks or visualization techniques) showing how chunk structure is encoded differently in single-task vs. multi-task settings, and experiments ablating specific components of the multi-task model to isolate the source of performance degradation

### Open Question 2
- Question: Can the performance gap between single-task and multi-task learning be reduced by architectural modifications, such as task-specific layers or attention mechanisms, that better preserve task-specific information while allowing for some sharing?
- Basis in paper: [inferred] The paper shows that forcing the model to share information across tasks leads to worse performance, suggesting that task-specific information is being lost. However, it does not explore whether architectural modifications could mitigate this issue
- Why unresolved: The paper uses a fixed two-level architecture and does not experiment with modifications that could potentially improve multi-task learning performance
- What evidence would resolve it: Experiments with modified architectures (e.g., adding task-specific layers, attention mechanisms, or gating functions) that show whether the performance gap can be reduced while still maintaining some benefits of multi-task learning

### Open Question 3
- Question: How does the degree of lexical complexity in the training data affect the ability of the model to generalize to unseen lexical items in both single-task and multi-task settings?
- Basis in paper: [explicit] The authors create datasets with three levels of lexical complexity (Type I, II, III) and test on all three types, showing that performance decreases as lexical complexity increases
- Why unresolved: While the paper shows that performance decreases with increasing lexical complexity, it does not analyze how this effect differs between single-task and multi-task settings or identify the specific mechanisms that lead to this degradation
- What evidence would resolve it: Detailed analysis of model performance across lexical complexity levels for both single-task and multi-task settings, including error analysis to identify which types of lexical items are most problematic and whether multi-task learning exacerbates or mitigates these issues

## Limitations
- Limited task diversity: Only tests two linguistic phenomena (agreement and verb alternations) across three lexical complexity levels
- Architecture specifics unclear: Exact implementation details of VAE compression and chunk structure extraction are not fully specified
- Low confidence in multi-task interference hypothesis: Error analysis doesn't definitively prove syntactic and semantic features compete for encoding space

## Confidence
- High confidence: Single-task training consistently outperforms multi-task training by 10-15 F1 points across agreement and verb alternation tasks
- Medium confidence: The two-level VAE architecture effectively compresses sentence embeddings to capture task-relevant information at the sentence level before sequence-level pattern detection
- Low confidence: Abstract linguistic notions like constituents and thematic roles are not consistently encoded in sentence embeddings in a way that allows sharing across different linguistic tasks

## Next Checks
1. Replicate the error analysis to verify that multi-task models show increased sequence errors (WN1, WN2) compared to single-task models, and that these errors correlate with specific failure modes in chunk structure detection
2. Visualize the compressed latent representations from single-task vs multi-task models to determine if syntactic and semantic features are indeed competing for encoding space
3. Test whether multi-task models trained on agreement and verb alternations show improved performance on other syntactic or semantic tasks not seen during training, which would challenge the interference hypothesis