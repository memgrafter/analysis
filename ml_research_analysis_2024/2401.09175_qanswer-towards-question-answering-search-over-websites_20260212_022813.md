---
ver: rpa2
title: 'QAnswer: Towards Question Answering Search over Websites'
arxiv_id: '2401.09175'
source_url: https://arxiv.org/abs/2401.09175
tags:
- question
- search
- answer
- answering
- only
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper presents QAnswer, a demo system combining question answering
  over knowledge graphs and free text to provide website search functionality. The
  system indexes Wikipedia (unstructured) and Wikidata (structured) and uses a retriever/reader
  architecture for text QA and a SPARQL query generation approach for KG QA.
---

# QAnswer: Towards Question Answering Search over Websites

## Quick Facts
- arXiv ID: 2401.09175
- Source URL: https://arxiv.org/abs/2401.09175
- Authors: Kunpeng Guo; Clement Defretiere; Dennis Diefenbach; Christophe Gravier; Antoine Gourru
- Reference count: 19
- Primary result: Demo system combining KG QA and text QA for website search

## Executive Summary
QAnswer presents a demonstration system that integrates question answering over knowledge graphs (Wikidata) and free text (Wikipedia) to enhance website search functionality. The system employs a retriever/reader architecture for text QA and SPARQL query generation for KG QA, showcasing complementary strengths: KG QA provides contextual information and aggregation capabilities, while text QA offers explanations and handles ambiguity better. Demonstrated on Wikimedia Foundation websites, the system aims to showcase how modern QA technologies can improve content access and user experience on websites.

## Method Summary
The QAnswer system implements a dual-approach architecture combining knowledge graph question answering with text-based question answering. For KG QA, it generates SPARQL queries from natural language questions and executes them against Wikidata. For text QA, it uses a retriever/reader pipeline to process Wikipedia content. The system indexes both Wikipedia (unstructured text) and Wikidata (structured knowledge) integrally and exclusively for the Wikimedia Foundation websites. The demo is accessible at http://wikimedia.qanswer.ai and demonstrates how these complementary approaches can enhance website search capabilities.

## Key Results
- Combines KG QA (Wikidata) and text QA (Wikipedia) approaches for enhanced website search
- Demonstrates complementary strengths: KG QA provides contextual information and aggregation, while text QA offers explanations and handles ambiguity
- Showcases system on Wikimedia Foundation websites with full content indexing
- Demo available at http://wikimedia.qanswer.ai for public access

## Why This Works (Mechanism)
The system leverages complementary strengths of knowledge graph and text-based question answering. KG QA excels at providing structured, contextual information and aggregation capabilities through SPARQL queries on Wikidata. Text QA complements this by offering explanations for answers and better handling of ambiguous questions through the retriever/reader architecture applied to Wikipedia content. By integrating both approaches, the system can address a wider range of user queries more effectively than either approach alone.

## Foundational Learning
1. **Retriever/Reader Architecture**: Used for text QA; the retriever selects relevant documents and the reader extracts answers from them. Needed for processing unstructured text efficiently; quick check: verify document selection accuracy and answer extraction quality.
2. **SPARQL Query Generation**: Converts natural language questions to structured queries for KG QA; needed to leverage structured knowledge in Wikidata; quick check: test query generation accuracy across different question types.
3. **Dual-Indexing Strategy**: Indexes both Wikipedia and Wikidata integrally; needed to enable seamless integration of structured and unstructured data sources; quick check: verify synchronization between indices.
4. **Knowledge Graph Reasoning**: Enables contextual information retrieval and aggregation from Wikidata; needed for handling complex queries requiring multiple facts; quick check: test multi-hop reasoning capabilities.
5. **Ambiguity Handling**: Text QA component better manages ambiguous questions; needed for real-world query scenarios; quick check: evaluate performance on ambiguous versus clear questions.

## Architecture Onboarding

**Component Map**: User Query -> KG QA Pipeline (SPARQL Generator -> Wikidata) AND Text QA Pipeline (Retriever -> Reader -> Wikipedia) -> Combined Results -> User Interface

**Critical Path**: User query processing through both KG and text pipelines simultaneously, followed by result combination and presentation

**Design Tradeoffs**: 
- Advantages: Leverages complementary strengths of structured and unstructured data, handles diverse query types
- Disadvantages: Increased complexity, potential for redundant processing, requires maintaining two separate indices

**Failure Signatures**:
- KG QA failures: SPARQL generation errors, missing relevant entities in Wikidata, multi-hop reasoning failures
- Text QA failures: Poor document retrieval, incorrect answer extraction, hallucination of unsupported facts

**First Experiments**:
1. Test single-hop factual questions that should be answerable from both sources to compare answer quality
2. Evaluate ambiguous questions to verify text QA's superior handling of unclear queries
3. Test aggregation queries (e.g., "list all...") to verify KG QA's aggregation capabilities

## Open Questions the Paper Calls Out
None

## Limitations
- No quantitative evaluation results or performance metrics provided
- No discussion of error rates, precision/recall metrics, or user studies
- Does not address multi-hop reasoning across structured and unstructured data sources
- Scalability considerations for larger websites beyond Wikipedia not discussed

## Confidence
- High confidence in the technical description of the architecture and components
- Medium confidence in the claimed advantages of the dual-approach system
- Low confidence in the practical effectiveness without empirical validation

## Next Checks
1. Conduct user studies comparing QAnswer's performance against traditional keyword search on representative website queries, measuring both task completion rates and user satisfaction
2. Perform systematic error analysis by categorizing failure modes for both KG and text QA components across diverse question types
3. Measure response time and resource utilization for queries requiring both KG and text retrieval to assess practical scalability for production deployment