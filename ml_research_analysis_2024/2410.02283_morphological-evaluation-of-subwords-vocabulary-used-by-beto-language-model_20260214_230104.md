---
ver: rpa2
title: Morphological evaluation of subwords vocabulary used by BETO language model
arxiv_id: '2410.02283'
source_url: https://arxiv.org/abs/2410.02283
tags:
- beto
- wordpiece
- tokenizer
- vocabulario
- para
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This study evaluates the morphological quality of the vocabulary
  used by the BETO language model, a Spanish BERT-based model. The evaluation method
  measures how well the vocabulary aligns with real Spanish morphemes across three
  criteria: morphological relevance (proportion of actual morphemes present), morphological
  coherence (consistency in using morphemes to segment words), and morphological accuracy
  (ability to correctly segment words).'
---

# Morphological evaluation of subwords vocabulary used by BETO language model

## Quick Facts
- **arXiv ID**: 2410.02283
- **Source URL**: https://arxiv.org/abs/2410.02283
- **Authors**: Óscar García-Sierra; Ana Fernández-Pampillón Cesteros; Miguel Ortega-Martín
- **Reference count**: 36
- **Primary result**: BETO's vocabulary shows very low morphological quality with precision as low as 0.18% for prefixes and 3.84% for stems

## Executive Summary
This study evaluates the morphological quality of the vocabulary used by BETO, a Spanish BERT-based language model. Using a previously established evaluation method based on morphological relevance, coherence, and accuracy, the researchers found BETO's vocabulary performs poorly in aligning with real Spanish morphemes. Despite claims that BETO uses BPE tokenization, analysis suggests it actually employs Wordpiece tokenization, resolving inconsistencies in the model's documentation. The study also reveals that training with a 500x larger corpus does not improve morphological quality.

## Method Summary
The study applies a previously established evaluation method to BETO's vocabulary, measuring morphological quality across three criteria: morphological relevance (proportion of actual morphemes present), morphological coherence (consistency in using morphemes to segment words), and morphological accuracy (ability to correctly segment words). The evaluation compares BETO's 31,000-token vocabulary against reference vocabularies generated by BPE and Wordpiece algorithms using smaller training corpora. Error analysis identifies four types of tokenization errors: under-segmentation, over-segmentation, missing morphemes, and morpheme selection errors.

## Key Results
- BETO's vocabulary precision values are extremely low: 0.18% for prefixes and 3.84% for stems
- Coverage is higher but still limited: 90% for prefixes, 73% for suffixes, 24% for stems
- Morphological coherence is poor, with BETO using the same prefixes to segment words only 12% of the time
- Morphological accuracy is 14.54%, indicating only 14.54% of words are correctly segmented
- BETO exhibits predominantly Type 1 (under-segmentation) and Type 3 (missing morphemes) errors
- Training with a 500x larger corpus does not improve morphological quality

## Why This Works (Mechanism)

### Mechanism 1
- Claim: BETO's tokenizer is a Wordpiece algorithm, not BPE as originally claimed.
- Mechanism: The tokenizer configuration explicitly states Wordpiece, and the absence of a "merges.txt" file rules out BPE. Experimental evaluation comparing BETO's vocabulary quality metrics to known Wordpiece and BPE tokenizers confirms this conclusion.
- Core assumption: The tokenizer configuration file accurately reflects the algorithm used and the evaluation metrics are sensitive enough to distinguish between Wordpiece and BPE.
- Evidence anchors:
  - [abstract] "Analysis suggests BETO uses a Wordpiece algorithm despite claims it uses BPE, resolving inconsistencies in the model's documentation."
  - [section] "The tokenizer used by BETO's language model has been trained on a corpus of 300 million sentences... According to its authors, the tokenizer is based on the BPE algorithm... Yet, this information conflicts with what is specified on the Hugging Face download page and in the tokenizer's configuration file, where it is indicated that the tokenizer used is of the Wordpiece type."

### Mechanism 2
- Claim: Training with a 500x larger corpus does not improve morphological quality of vocabularies.
- Mechanism: Despite BETO's tokenizer being trained on a corpus 500 times larger than the reference tokenizers, its morphological quality metrics (precision, coverage, F1-score) are similar to or worse than those of tokenizers trained on smaller corpora.
- Core assumption: Morphological quality is independent of corpus size beyond a certain threshold, and the evaluation method accurately measures this quality.
- Evidence anchors:
  - [abstract] "The study finds that training with a 500x larger corpus does not improve morphological quality."
  - [section] "The evaluation of BETO's vocabulary quality is of interest because... (ii) it will allow us to determine whether its vocabulary could be a potential limitation on the model's performance and, if so, explore whether improving the vocabulary leads to a significant improvement in the model..."

### Mechanism 3
- Claim: Statistical tokenizers (BPE, Wordpiece, Unigram) exhibit low morphological quality for Spanish.
- Mechanism: These tokenizers rely on statistical criteria (frequency of character strings) rather than linguistic rules, resulting in subwords that do not align with real morphemes, as evidenced by low precision, coverage, and F1-scores in morphological relevance, coherence, and accuracy evaluations.
- Core assumption: The evaluation method accurately measures the degree of alignment between subwords and real morphemes, and Spanish's morphological complexity makes it a suitable test case.
- Evidence anchors:
  - [abstract] "Results show BETO's vocabulary has very low morphological quality, with precision values as low as 0.18% for prefixes and 3.84% for stems."
  - [section] "Compared to symbolic tokenizers based on linguistic rules, statistical tokenizers have the advantage of generating vocabularies with tens of thousands of tokens from large text corpora without human supervision... However, as several studies have shown, they have the drawback that subword tokenization does not largely correspond to the morphemes of the language..."

## Foundational Learning

- **Concept**: Morphological quality evaluation
  - Why needed here: To assess the effectiveness of BETO's tokenizer in segmenting Spanish text into meaningful morphemes, which is crucial for the model's performance.
  - Quick check question: What are the three criteria used to evaluate morphological quality, and how do they differ?

- **Concept**: Subword tokenization algorithms (BPE, Wordpiece, Unigram)
  - Why needed here: To understand the differences between the algorithms used by BETO and reference tokenizers, and how these differences impact morphological quality.
  - Quick check question: What is the key difference between BPE and Wordpiece algorithms in terms of how they generate subwords?

- **Concept**: Error analysis in tokenization
  - Why needed here: To identify the types of errors made by BETO's tokenizer and compare them to errors made by other tokenizers, providing insights into its limitations.
  - Quick check question: What are the four types of errors identified in the study, and how do they relate to morphological segmentation?

## Architecture Onboarding

- **Component map**: BETO language model -> tokenizer -> subword vocabulary -> morphological quality evaluation
- **Critical path**: Tokenizer configuration -> corpus selection and training -> vocabulary generation -> morphological quality evaluation -> error analysis
- **Design tradeoffs**: Statistical tokenizers offer efficiency and multilingual support but sacrifice morphological accuracy; symbolic tokenizers provide better morphological alignment but require linguistic expertise and are less scalable.
- **Failure signatures**: Low precision and coverage in morphological relevance, poor coherence in morpheme segmentation, and low accuracy in word segmentation indicate morphological quality issues.
- **First 3 experiments**:
  1. Compare BETO's tokenizer configuration file to other known tokenizers to confirm the algorithm used.
  2. Evaluate BETO's tokenizer on a small set of Spanish words with known morphological structure to identify specific segmentation errors.
  3. Train a tokenizer on a smaller Spanish corpus and compare its morphological quality to BETO's to assess the impact of corpus size.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does morphological quality of subword vocabularies significantly impact the downstream performance of large language models on morphologically rich languages?
- Basis in paper: [explicit] The authors explicitly state this is their current and future research focus, noting that measuring vocabulary quality opens the door to studying this relationship.
- Why unresolved: While the paper demonstrates that BETO's vocabulary has low morphological quality, it doesn't empirically test whether improving this quality would enhance BETO's performance on specific tasks.
- What evidence would resolve it: Empirical studies comparing BETO's performance on tasks like sentiment analysis or named entity recognition before and after improving its vocabulary's morphological quality, ideally with multiple morphologically rich languages.

### Open Question 2
- Question: Is there an optimal vocabulary size threshold beyond which increasing training corpus size no longer improves morphological quality for any tokenization algorithm?
- Basis in paper: [inferred] The paper found that increasing BETO's training corpus by 500x did not improve morphological quality, suggesting a possible threshold effect, though this was only tested for one model.
- Why unresolved: The study only examined one model (BETO) and compared it to smaller reference vocabularies, not systematically testing different vocabulary sizes or multiple corpus sizes.
- What evidence would resolve it: Systematic experiments varying both vocabulary size and training corpus size across multiple tokenization algorithms (BPE, Wordpiece, Unigram) and languages to identify potential saturation points.

### Open Question 3
- Question: Can the discrepancy between authors' claims and model configuration (BETO claiming BPE while configuration indicates Wordpiece) be explained by intermediate processing steps or model evolution that are not documented?
- Basis in paper: [explicit] The authors note this inconsistency and rule out simple explanations like switching tokenizers between training and inference phases, but acknowledge they cannot definitively explain it.
- Why unresolved: The paper eliminated some explanations but couldn't identify the actual cause of the discrepancy between the claimed and implemented tokenization algorithm.
- What evidence would resolve it: Documentation or evidence of BETO's development history, including intermediate model versions, changes in tokenization approach, or clarification from the original authors about any algorithm changes during development.

## Limitations

- The study evaluates only one tokenizer (BETO) against two reference vocabularies trained on much smaller corpora, limiting generalizability to other Spanish BERT models or different corpus sizes.
- The evaluation relies entirely on previously established datasets and methods without exploring alternative evaluation approaches that might yield different insights into morphological quality.
- Results may not generalize to other languages or tokenization algorithms, as the study focuses specifically on Spanish and the three most common statistical tokenization approaches.

## Confidence

**High Confidence** in the claim that BETO uses Wordpiece rather than BPE tokenization: This conclusion is supported by direct evidence from the tokenizer configuration file and the absence of expected BPE artifacts.

**Medium Confidence** in the finding that training with a 500x larger corpus does not improve morphological quality: While the results show BETO's quality is comparable to smaller-vocabulary models, this single data point doesn't establish a definitive relationship between corpus size and morphological quality across different languages or tokenization approaches.

**Medium Confidence** in the claim that statistical tokenizers exhibit low morphological quality for Spanish: The results demonstrate poor morphological alignment, but this conclusion is based on BETO alone. Different training data or hyperparameters might yield different results.

## Next Checks

1. Replicate evaluation on additional Spanish BERT models: Apply the same morphological quality assessment to other Spanish language models (like cased versions or models from different organizations) to determine if BETO's patterns are typical or unique.

2. Vary corpus size systematically: Train multiple tokenizers on Spanish corpora of varying sizes (e.g., 600K, 6M, 60M, 300M sentences) using the same algorithm to establish the relationship between corpus size and morphological quality more definitively.

3. Test alternative evaluation methodologies: Apply complementary evaluation approaches, such as human judgment studies or downstream task performance comparisons, to validate whether morphological quality as measured by the current method correlates with actual model performance on Spanish NLP tasks.