---
ver: rpa2
title: Privacy-Preserving Federated Learning with Consistency via Knowledge Distillation
  Using Conditional Generator
arxiv_id: '2409.06955'
source_url: https://arxiv.org/abs/2409.06955
tags:
- local
- fedmd-cg
- generator
- performance
- fedcg
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes FedMD-CG, a privacy-preserving federated learning
  method that uses conditional generators instead of feature extractors to achieve
  high performance while maintaining strong privacy protection. The method decouples
  each client's local model into a feature extractor and classifier, and employs knowledge
  distillation to train local models and generators at both latent feature and logit
  levels, ensuring consistency between them.
---

# Privacy-Preserving Federated Learning with Consistency via Knowledge Distillation Using Conditional Generator

## Quick Facts
- arXiv ID: 2409.06955
- Source URL: https://arxiv.org/abs/2409.06955
- Reference count: 40
- Primary result: Achieves competitive performance on EMNIST, Fashion-MNIST, and CIFAR-10 while providing strong privacy protection (PSNR < 10 under DLG attacks)

## Executive Summary
This paper introduces FedMD-CG, a novel privacy-preserving federated learning method that uses conditional generators instead of feature extractors to achieve high performance while maintaining strong privacy protection. The method decouples each client's local model into a feature extractor and classifier, and employs knowledge distillation to train local models and generators at both latent feature and logit levels, ensuring consistency between them. Extensive experiments demonstrate that FedMD-CG achieves highly competitive performance compared to state-of-the-art baselines while providing strong privacy protection.

## Method Summary
FedMD-CG decouples each client's local model into a feature extractor and classifier, and uses a conditional generator to mimic the feature extractor's behavior without exposing raw data. The method employs a two-stage client-side distillation process (local model update and local generator update) and a crossed server-side distillation aggregation. Knowledge distillation is applied at both latent feature and logit levels to ensure consistency between local generators and classifiers. The server performs crossed data-free KD aggregation to extract maximal knowledge from local generators and classifiers without accessing raw data.

## Key Results
- FedMD-CG achieves test accuracy of 93.56%, 91.07%, and 89.89% on EMNIST, Fashion-MNIST, and CIFAR-10 respectively
- PSNR values under DLG attacks are below 10, indicating strong privacy protection
- FedMD-CG outperforms state-of-the-art baselines like DP-FedAVG, pFedPGAN, and FedMD in both performance and privacy preservation

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Decoupling the local model into feature extractor and classifier allows selective sharing, reducing privacy leakage while preserving performance.
- Mechanism: The client sends only the classifier (top layers) and a conditional generator that mimics the feature extractor's latent output space, rather than raw data or gradients.
- Core assumption: The classifier contains highly abstract representations that are not easily reversible to raw data, and the generator can approximate the feature extractor's behavior without exposing the actual features.
- Evidence anchors:
  - [abstract] "decouples each client's local model into a feature extractor and a classifier, and utilizes a conditional generator instead of the feature extractor to perform server-side model aggregation"
  - [section 2.1] "We maintain a local generator in client i to extract the knowledge of the trained local model without accessing its private data"
  - [corpus] Weak evidence; the neighbor papers discuss privacy-preserving methods but not specifically this decomposition approach.
- Break condition: If the classifier's abstraction level is insufficient or if the generator's approximation introduces significant bias, privacy leakage may occur or performance may degrade.

### Mechanism 2
- Claim: Knowledge distillation (KD) at both latent feature and logit levels ensures consistency between the local generator and classifier, improving generalization.
- Mechanism: KD transfers knowledge from the global generator to the local model and from the local model to the local generator, aligning their outputs at both the latent feature space and the final prediction space.
- Core assumption: KD can effectively bridge the gap between the generator's outputs and the classifier's expectations, ensuring that the generated features are meaningful for the classifier.
- Evidence anchors:
  - [abstract] "utilizes knowledge distillation to train local models and generators at both the latent feature level and the logit level"
  - [section 2.1] "To transfer the knowledge of the global generator to the local model efficiently, we construct the following two losses based on KD"
  - [corpus] Weak evidence; while KD is a known technique, the specific application in this federated setting with generators is not directly supported by the corpus.
- Break condition: If the KD process fails to align the generator and classifier, or if the distillation introduces noise, the consistency and performance may suffer.

### Mechanism 3
- Claim: Cross-data-free KD aggregation on the server extracts maximal knowledge from local generators and classifiers without accessing raw data.
- Mechanism: The server performs a crossed aggregation where the global classifier learns from the local classifiers' outputs on the global generator's features, and vice versa, ensuring comprehensive knowledge transfer.
- Core assumption: The crossed aggregation can effectively combine the diverse knowledge from local models without requiring access to the original data, and the diversity constraints ensure the generators produce varied and meaningful features.
- Evidence anchors:
  - [abstract] "performs aggregation of local generators and classifiers in a crossed data-free KD fashion"
  - [section 2.2] "To alleviate this issue, we train the preliminary global generator and classifier via crossed data-free KD to distill as much knowledge as possible from the local generators and classifiers"
  - [corpus] Weak evidence; the corpus does not provide direct support for this specific crossed aggregation method.
- Break condition: If the crossed aggregation fails to properly align the global and local models, or if the diversity constraints are insufficient, the aggregated model may not generalize well.

## Foundational Learning

- Concept: Federated Learning (FL)
  - Why needed here: The paper proposes a method within the FL framework, where multiple clients collaboratively train a model without sharing raw data.
  - Quick check question: What is the primary goal of federated learning, and how does it differ from traditional centralized learning?

- Concept: Knowledge Distillation (KD)
  - Why needed here: KD is used to transfer knowledge between models at different levels (latent features and logits), ensuring consistency and improving performance.
  - Quick check question: How does knowledge distillation work, and what are the key components of a KD loss function?

- Concept: Conditional Generative Adversarial Networks (cGANs)
  - Why needed here: The paper uses a conditional generator, similar to those in cGANs, to mimic the feature extractor's behavior without exposing raw data.
  - Quick check question: What is the role of the generator in a cGAN, and how does conditioning on labels affect its output?

## Architecture Onboarding

- Component map:
  - Client side: Local model (feature extractor + classifier), local generator, KD losses, diversity constraints
  - Server side: Global generator, global classifier, crossed data-free KD aggregation
- Critical path: Client trains local model and generator using KD, sends them to server, server performs crossed aggregation, updates global model, broadcasts back to clients
- Design tradeoffs:
  - Privacy vs. performance: Sharing only the classifier and generator reduces privacy leakage but may impact performance if the abstraction is too high
  - Communication cost: Sending only the classifier and generator reduces communication compared to full models but requires additional training on the client
  - Computational load: Training the generator and applying KD increases client-side computation
- Failure signatures:
  - Low PSNR values: Indicates potential privacy leakage
  - High G+D loss: Suggests inconsistency between local generator and classifier
  - Poor convergence: May indicate issues with KD or diversity constraints
- First 3 experiments:
  1. Validate that the classifier abstraction is sufficient to prevent privacy leakage (e.g., using DLG attack)
  2. Test the effectiveness of KD at both latent feature and logit levels by comparing performance with and without each KD loss
  3. Evaluate the impact of different diversity constraints on the generator's output quality and model performance

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the proposed FedMD-CG method compare to FedAvg in terms of communication efficiency, considering FedMD-CG requires additional label statistics to be transmitted?
- Basis in paper: [inferred] The paper mentions that FedMD-CG requires each client to upload label statistics along with the generator and classifier, which could potentially increase communication costs.
- Why unresolved: The paper does not provide a direct comparison of communication efficiency between FedMD-CG and FedAvg.
- What evidence would resolve it: Empirical results comparing the total communication cost (including label statistics) of FedMD-CG and FedAvg over the same number of communication rounds.

### Open Question 2
- Question: What is the impact of different diversity constraints (L0div, L1div, L2div) on the privacy-preserving capabilities of FedMD-CG?
- Basis in paper: [explicit] The paper discusses the impact of different diversity constraints on model performance but does not explicitly address their impact on privacy.
- Why unresolved: The paper does not investigate the relationship between diversity constraints and privacy leakage.
- What evidence would resolve it: Experiments comparing the privacy leakage (e.g., PSNR values under DLG attacks) of FedMD-CG using different diversity constraints.

### Open Question 3
- Question: How does FedMD-CG perform when the number of clients varies significantly, especially in cases with a large number of clients with limited data each?
- Basis in paper: [inferred] The paper experiments with different data heterogeneity levels but does not explore the impact of varying the number of clients.
- Why unresolved: The paper does not investigate the scalability of FedMD-CG with respect to the number of clients.
- What evidence would resolve it: Experiments evaluating the performance and convergence speed of FedMD-CG with varying numbers of clients, especially in scenarios with many clients having limited data.

## Limitations

- The specific mathematical formulation of diversity constraints (L0_div, L1_div, L2_div) is not clearly specified
- The exact procedure for server-side crossed distillation aggregation needs clarification
- The DLG attack implementation details are not provided

## Confidence

- **High confidence**: The overall architecture and two-stage client-side distillation process
- **Medium confidence**: The crossed server-side aggregation mechanism
- **Low confidence**: The specific diversity constraint formulations and their impact on privacy

## Next Checks

1. Implement and test the DLG attack to verify the claimed PSNR values (should be below 10 for strong privacy)
2. Conduct ablation studies to isolate the contribution of each KD loss component (latent feature vs logit level)
3. Verify the effectiveness of diversity constraints by measuring the variance in generated features across different clients