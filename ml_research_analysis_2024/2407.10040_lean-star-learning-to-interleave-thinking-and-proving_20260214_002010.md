---
ver: rpa2
title: 'Lean-STaR: Learning to Interleave Thinking and Proving'
arxiv_id: '2407.10040'
source_url: https://arxiv.org/abs/2407.10040
tags:
- proof
- theorem
- language
- tactic
- lean-star
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Lean-STaR introduces informal thought generation into formal theorem
  proving by training language models to produce natural language reasoning prior
  to each proof step. The method first generates synthetic thought-augmented examples
  from existing proofs using GPT-4, then fine-tunes a model on this data.
---

# Lean-STaR: Learning to Interleave Thinking and Proving
## Quick Facts
- arXiv ID: 2407.10040
- Source URL: https://arxiv.org/abs/2407.10040
- Authors: Haohan Lin; Zhiqing Sun; Sean Welleck; Yiming Yang
- Reference count: 28
- Primary result: 36.1% pass@64 on miniF2F-test

## Executive Summary
Lean-STaR introduces informal thought generation into formal theorem proving by training language models to produce natural language reasoning prior to each proof step. The method first generates synthetic thought-augmented examples from existing proofs using GPT-4, then fine-tunes a model on this data. Expert iteration is applied to further improve the model by self-generating and verifying proofs. The approach achieves 36.1% pass@64 on miniF2F-test, significantly outperforming the base model and setting a new state-of-the-art for Lean-based theorem proving.

## Method Summary
Lean-STaR uses a two-phase approach to integrate informal reasoning into formal theorem proving. First, GPT-4 retrospectively generates natural language thoughts for each proof step in the Mathlib dataset, creating synthetic thought-augmented training data. A base model is then fine-tuned on this data to create Lean-CoT. Second, expert iteration is applied where the model samples proofs, verifies successful ones with the Lean solver, and is fine-tuned on the union of initial and newly generated correct proofs to create Lean-STaR.

## Key Results
- Achieves 36.1% pass@64 on miniF2F-test
- Outperforms base model by 5.8 percentage points (30.3% â†’ 36.1%)
- Sets new state-of-the-art for Lean-based theorem proving

## Why This Works (Mechanism)
### Mechanism 1
- Claim: Generating synthetic thought-augmented examples from existing proofs improves theorem-proving performance by capturing informal reasoning steps.
- Mechanism: GPT-4 retrospectively generates natural language thoughts for each proof step using ground-truth tactics, creating a dataset that links formal states, informal thoughts, and tactics.
- Core assumption: The informal reasoning process humans use when proving theorems contains information that is not present in the formal proof steps themselves.
- Evidence anchors: [abstract] "Our key observation is that a wealth of informal information that is not present in formal proofs can be useful for learning to prove theorems."

### Mechanism 2
- Claim: Expert iteration improves the thought-augmented theorem prover by fine-tuning on proofs it generates and verifies.
- Mechanism: The model samples proofs, keeps successful ones, and is fine-tuned on the union of initial thought-augmented data and newly generated correct proofs.
- Core assumption: Self-generated proofs that are verified correct by the formal system contain useful learning signals that can improve the model.
- Evidence anchors: [abstract] "Building on the self-taught reasoner framework, we then apply expert iteration to further fine-tune the model on the correct proofs it samples and verifies using the Lean solver."

### Mechanism 3
- Claim: The sampling-based search strategy works better than best-first search for thought-augmented theorem proving.
- Mechanism: Instead of expanding the "best" state based on log probabilities, the method samples K complete proof trajectories in parallel and ignores illegal tactics during sampling.
- Core assumption: Average log probabilities are not reliable quality indicators when the thought sequence is generated, making best-first search less effective.
- Evidence anchors: [section] "Our preliminary experiments indicate that best-first search with beam search does not work well for the thoughts in the natural language format."

## Foundational Learning
- Concept: Markov Decision Process (MDP) formulation of theorem proving
  - Why needed here: The paper frames theorem proving as an MDP where states are proof states, actions are tactics, and rewards indicate successful proofs.
  - Quick check question: In the theorem proving MDP, what constitutes a terminal state with reward 1?

- Concept: Chain-of-Thought (CoT) reasoning
  - Why needed here: The core innovation involves generating informal thoughts prior to each formal proof step.
  - Quick check question: How does the thought-augmented model's action space differ from the direct tactic prediction model?

- Concept: Expert iteration as offline reinforcement learning
  - Why needed here: The paper explicitly frames expert iteration as an offline RL method.
  - Quick check question: What is the cumulative reward function R in the theorem proving MDP, and when does it equal 1?

## Architecture Onboarding
- Component map: Oracle model (GPT-4) -> Thought-augmented tactic predictor -> Lean solver -> Expert iteration loop -> Sampling-based search
- Critical path: 1) Generate synthetic thought-augmented training data using GPT-4 2) Fine-tune base model on this data to create Lean-CoT 3) Sample proofs using Lean-CoT, verify with Lean solver 4) Collect successful proofs into STaR dataset 5) Fine-tune Lean-CoT on combined datasets to create Lean-STaR 6) Evaluate using sampling-based search
- Design tradeoffs: Using GPT-4 for data generation provides high-quality thoughts but introduces cost and potential data leakage concerns
- Failure signatures: Performance plateaus after expert iteration iterations, generated thoughts become repetitive or generic, sampling process generates mostly invalid proofs
- First 3 experiments: 1) Fine-tune base model on synthetic thought-augmented data and measure pass@32 on miniF2F-test 2) Run one iteration of expert iteration and measure improvement in pass@32 3) Compare sampling-based search vs best-first search on the thought-augmented model

## Open Questions the Paper Calls Out
### Open Question 1
- Question: Does the effectiveness of Lean-STaR's thought generation depend on the specific oracle model used for synthetic data generation?
- Basis in paper: [inferred] The paper uses GPT-4 as the oracle model for generating synthetic thoughts, but doesn't explore how different models might affect performance
- Why unresolved: The paper only tests with GPT-4, leaving open whether smaller models or different architectures could achieve similar results with less computational cost
- What evidence would resolve it: Systematic comparison of different oracle models generating thoughts and measuring downstream theorem proving performance

### Open Question 2
- Question: How does the quality of generated thoughts correlate with proof success, and can poor thoughts still lead to correct proofs?
- Basis in paper: [explicit] "Note that there is a calculation error in the thought (in red), but this does not affect the correctness of the proof because the calculation task is actually completed by the interactive theorem prover"
- Why unresolved: The paper observes this phenomenon but doesn't systematically analyze when thought errors matter versus when the formal system can compensate
- What evidence would resolve it: Detailed analysis of thought-accuracy versus proof-success correlation, including controlled experiments where thought generation is intentionally perturbed

### Open Question 3
- Question: What is the optimal balance between thought generation quality and tactic prediction accuracy during sampling?
- Basis in paper: [inferred] The paper uses a fixed temperature of 0.7 for sampling, but doesn't explore how different temperature settings affect the interplay between thought quality and tactic selection
- Why unresolved: The sampling method treats thoughts as hidden variables, but doesn't investigate whether certain thought generation strategies are more beneficial for different theorem proving scenarios
- What evidence would resolve it: Systematic ablation studies varying temperature, beam search, and other sampling parameters specifically for thought generation versus tactic generation

## Limitations
- Heavy reliance on GPT-4 for synthetic data generation makes it unclear whether gains come from the thought-augmented approach or GPT-4's superior generation capabilities
- Limited analysis of whether generated thoughts actually capture human-like reasoning patterns versus adding noise
- Best-first search comparison lacks direct experimental data

## Confidence
- Performance improvements: High
- Thought generation capturing human reasoning: Medium
- Expert iteration effectiveness: Medium
- Sampling vs best-first search: Low

## Next Checks
1. Ablation study removing the thought generation phase to isolate the contribution of synthetic data quality versus the thought-augmented approach itself
2. Human evaluation of generated thoughts to assess whether they capture meaningful reasoning patterns versus generic commentary
3. Direct comparison of sampling and best-first search methods using the same thought-augmented model architecture