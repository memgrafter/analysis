---
ver: rpa2
title: 'OMH: Structured Sparsity via Optimally Matched Hierarchy for Unsupervised
  Semantic Segmentation'
arxiv_id: '2403.06546'
source_url: https://arxiv.org/abs/2403.06546
tags:
- hierarchy
- sparsity
- semantic
- unsupervised
- segmentation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper tackles unsupervised semantic segmentation by introducing
  Optimally Matched Hierarchy (OMH), a method that learns structured sparsity in feature
  space. OMH employs a multi-level hierarchy of parallel clusterings, where relationships
  between clusters are learned via Optimal Transport.
---

# OMH: Structured Sparsity via Optimally Matched Hierarchy for Unsupervised Semantic Segmentation

## Quick Facts
- arXiv ID: 2403.06546
- Source URL: https://arxiv.org/abs/2403.06546
- Authors: Baran Ozaydin; Tong Zhang; Deblina Bhattacharjee; Sabine Süsstrunk; Mathieu Salzmann
- Reference count: 15
- Primary result: OMH consistently outperforms state-of-the-art methods on COCOStuff, Cityscapes, and Potsdam, achieving higher mIoU and accuracy when integrated into frameworks like STEGO, HP, and SmooSeg.

## Executive Summary
This paper introduces Optimally Matched Hierarchy (OMH), a method that learns structured sparsity in feature space for unsupervised semantic segmentation (USS). OMH employs a multi-level hierarchy of parallel clusterings, where relationships between clusters are learned via Optimal Transport. This imposes a soft but sparse structure, enabling features to encode information at varying granularities and improving semantic alignment. Evaluated on COCOStuff, Cityscapes, and Potsdam, OMH consistently outperforms state-of-the-art methods, achieving higher mIoU and accuracy when integrated into frameworks like STEGO, HP, and SmooSeg.

## Method Summary
OMH introduces structured sparsity to unsupervised semantic segmentation by learning a multi-level hierarchy of parallel clusterings. The method uses N parallel clustering heads with increasing cluster counts, where relationships between clusters are learned via Optimal Transport. This creates a sparse hierarchy matrix that links clusters across levels. Matching losses align lower-level cluster activations with higher-level clusters, ensuring the features encode information at multiple granularities. OMH can be incorporated into existing USS frameworks like STEGO, HP, and SmooSeg without additional test-time complexity.

## Key Results
- OMH consistently outperforms state-of-the-art methods on COCOStuff, Cityscapes, and Potsdam datasets.
- A 3-level hierarchy with an expansion factor of 1.5-2 achieves optimal performance.
- Sparse Optimal Transport regularization effectively models part-whole and class-class relationships.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The structured sparsity imposed by OMH enables features to encode information at multiple granularities.
- Mechanism: Multiple parallel clustering heads with increasing cluster counts create a hierarchy. Optimal Transport computes a sparse relationship matrix between cluster centers across levels. Matching losses transport lower-level activations to the best-matching higher-level clusters.
- Core assumption: The semantic sparsity of SS can be encoded through a learned soft but sparse hierarchy between clusters.
- Evidence anchors:
  - [abstract] "The core of our method lies in imposing structured sparsity on the feature space, which allows the features to encode information with different levels of granularity."
  - [section] "To achieve this structured sparsity, we present a novel Optimally Matched Hierarchy (OMH), which is learned during the training process."
- Break condition: If the Optimal Transport problem fails to produce a sparse hierarchy matrix, the structure collapses and benefits are lost.

### Mechanism 2
- Claim: Optimal Transport creates a sparse but meaningful relationship matrix between cluster centers across hierarchy levels.
- Mechanism: The cosine distance between cluster centers forms a cost matrix. Sinkhorn's algorithm solves the Optimal Transport problem with entropy regularization, yielding a sparse transportation plan that links clusters across levels.
- Core assumption: A sparse transportation plan between cluster centers can effectively model part-whole relationships and class-class relationships.
- Evidence anchors:
  - [section] "To obtain such a hierarchy between clusters H (i) and H (i+1), we solve an optimal transport problem (Sinkhorn & Knopp, 1967; Cuturi, 2013)."
- Break condition: If entropy regularization is too high, the transportation plan becomes dense and loses its sparse structure.

### Mechanism 3
- Claim: The matching loss based on L∞ Wasserstein distance ensures lower-level cluster activations align with higher-level clusters.
- Mechanism: Joint cluster activation maps are computed using the transportation plan. Max-pooling over lower-level cluster indices yields transported higher-level activations. The L1 difference between transported and actual higher-level activations forms the matching loss.
- Core assumption: Max-pooling the joint distribution computed via Optimal Transport parallels the L∞ Wasserstein Distance, effectively aligning activations across hierarchy levels.
- Evidence anchors:
  - [section] "Matching lower- and higher-level clusters via max-pooling has two motivations: 1) Matching each lower-level cluster with the single closest higher-level cluster at each point is intuitively desirable for a hierarchy; 2) max-pooling the joint distribution computed via an Optimal Transport matrix theoretically parallels the L∞ Wasserstein Distance."
- Break condition: If the matching loss weight is too low, the hierarchy alignment becomes ineffective; if too high, it may dominate other loss terms.

## Foundational Learning

- Concept: Optimal Transport and Sinkhorn's algorithm
  - Why needed here: To compute a sparse transportation plan between cluster centers across hierarchy levels.
  - Quick check question: How does entropy regularization in Sinkhorn's algorithm control the sparsity of the transportation plan?

- Concept: Hierarchical clustering and its relationship to semantic segmentation
  - Why needed here: To understand how multiple levels of clustering with increasing granularity can capture part-whole relationships and class-class relationships.
  - Quick check question: Why is it beneficial to have an expansion factor greater than 1 in the number of clusters across hierarchy levels?

- Concept: L∞ Wasserstein distance and its connection to max-pooling
  - Why needed here: To understand the theoretical basis for using max-pooling to align cluster activations across hierarchy levels.
  - Quick check question: How does max-pooling the joint distribution computed via Optimal Transport theoretically parallel the L∞ Wasserstein Distance?

## Architecture Onboarding

- Component map: Feature extractor -> Projection head -> Clustering heads -> Optimal Transport -> Matching loss -> Feature update
- Critical path: Features → Projection → Clustering heads → Optimal Transport → Matching loss → Feature update
- Design tradeoffs:
  - Number of hierarchy levels (N): More levels capture finer granularity but increase computational cost
  - Expansion factor (ρ): Larger factors create more distinct levels but may lead to unstable clustering
  - Entropy regularization (λ): Smaller values create sparser hierarchies but may lead to numerical instability
- Failure signatures:
  - NaNs during training: Likely due to numerical instability in Optimal Transport (check λ and 32-bit precision)
  - No improvement in segmentation accuracy: Likely due to ineffective hierarchy structure (check expansion factor and matching loss weight)
  - Overfitting: Likely due to excessive hierarchy levels or expansion factor (check N and ρ)
- First 3 experiments:
  1. Train OMH with N=2 and ρ=1 to verify basic functionality without hierarchical structure
  2. Train OMH with N=3 and ρ=2 to verify benefits of structured hierarchy
  3. Train OMH with varying λ values to find optimal sparsity level in hierarchy matrix

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of OMH scale with deeper hierarchies (N > 3), and what is the optimal depth for balancing accuracy and computational cost?
- Basis in paper: [explicit] The paper notes that a 3-level hierarchy is effective but does not explore deeper levels, leaving open questions about scalability.
- Why unresolved: The paper only experiments with up to 3 levels and does not provide analysis for deeper hierarchies or their computational trade-offs.
- What evidence would resolve it: Systematic experiments testing OMH with N = 4, 5, or more levels, comparing mIoU, accuracy, and computational cost (e.g., training time, memory usage).

### Open Question 2
- Question: Can OMH be adapted to incorporate temporal consistency for unsupervised video segmentation, and what would be the impact on performance?
- Basis in paper: [inferred] The paper focuses on image-based USS and does not address video segmentation, where temporal consistency could be valuable.
- Why unresolved: The paper does not explore extensions to video data or the integration of temporal constraints into the hierarchical clustering framework.
- What evidence would resolve it: Experiments applying OMH to video datasets (e.g., Cityscapes-Video) and measuring improvements in temporal consistency and segmentation accuracy.

### Open Question 3
- Question: How sensitive is OMH to the choice of backbone architecture (e.g., CNNs vs. Transformers), and can it generalize to non-ViT backbones?
- Basis in paper: [explicit] The paper uses DINO (a ViT-based backbone) but does not test OMH with CNN-based backbones like ResNet or Swin.
- Why unresolved: The paper’s experiments are limited to ViT-based backbones, leaving uncertainty about OMH’s compatibility with other architectures.
- What evidence would resolve it: Comparative experiments using OMH with different backbone architectures (e.g., ResNet, Swin Transformer) and analyzing performance differences.

## Limitations

- Limited exploration of deeper hierarchies (N > 3) and their scalability.
- No experiments with non-ViT backbones (e.g., CNNs or Swin Transformers).
- Lack of analysis on the robustness of the learned hierarchy to different random initializations or dataset variations.

## Confidence

- **High confidence**: OMH's ability to improve USS performance when integrated into existing frameworks (e.g., STEGO, HP, SmooSeg) on COCOStuff, Cityscapes, and Potsdam datasets.
- **Medium confidence**: The effectiveness of the 3-level hierarchy with an expansion factor of 1.5-2, and sparse Optimal Transport regularization in achieving structured sparsity.
- **Low confidence**: The claim that the matching loss based on L∞ Wasserstein distance effectively aligns activations across hierarchy levels, as the theoretical basis is not fully explored.

## Next Checks

1. **Hierarchy Robustness Test**: Train OMH with multiple random seeds and evaluate the stability of the learned hierarchy matrix. Check if the sparsity pattern and cluster relationships remain consistent across runs.

2. **Expansion Factor Sensitivity**: Systematically vary the expansion factor (ρ) between 1.2 and 3 and measure the impact on segmentation accuracy and computational efficiency. Identify the optimal range for different dataset characteristics.

3. **Optimal Transport Sparsity Analysis**: Visualize the learned hierarchy matrices for different entropy regularization values (λ) and quantify the sparsity. Verify that sparse matrices lead to better segmentation performance compared to dense matrices.