---
ver: rpa2
title: Adversarial Robustness Overestimation and Instability in TRADES
arxiv_id: '2410.07675'
source_url: https://arxiv.org/abs/2410.07675
tags:
- adversarial
- training
- gradient
- robustness
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper identifies and analyzes robustness overestimation in
  TRADES, where PGD validation accuracy significantly exceeds AutoAttack testing accuracy
  due to gradient masking. The study finds that smaller batch sizes, lower beta values,
  larger learning rates, and higher dataset complexity increase instability.
---

# Adversarial Robustness Overestimation and Instability in TRADES

## Quick Facts
- arXiv ID: 2410.07675
- Source URL: https://arxiv.org/abs/2410.07675
- Reference count: 31
- This paper identifies robustness overestimation in TRADES where PGD validation accuracy significantly exceeds AutoAttack testing accuracy due to gradient masking

## Executive Summary
This paper identifies and analyzes robustness overestimation in TRADES, where PGD validation accuracy significantly exceeds AutoAttack testing accuracy due to gradient masking. The study finds that smaller batch sizes, lower beta values, larger learning rates, and higher dataset complexity increase instability. By examining FOSC and gradient information, the authors show that instability arises from overfitting to TPGD perturbations and batch-level gradient spikes. They discover that some training instances self-heal, returning to stable states. The proposed solution introduces Gaussian noise when FOSC exceeds a threshold, effectively mitigating overestimation with minimal performance impact.

## Method Summary
The paper identifies robustness overestimation in TRADES training by comparing PGD-10 validation accuracy against AutoAttack testing accuracy. The authors analyze instability across different hyperparameter settings (beta values of 1, 3, 6; batch sizes of 128, 256, 512; learning rates of 0.01, 0.05, 0.1) on CIFAR-100 using ResNet-18. They monitor FOSC values to detect gradient masking and examine batch-level gradient patterns. The proposed solution adds Gaussian noise (N(0, 0.1)) to the first ten batches of an epoch when FOSC exceeds a threshold, stabilizing training and reducing overestimation.

## Key Results
- Robustness overestimation occurs when PGD-10 accuracy significantly exceeds AutoAttack accuracy, linked to gradient masking
- Instability increases with smaller batch sizes, lower beta values, larger learning rates, and higher dataset complexity
- Batch-level gradient spikes and cosine similarity drops indicate local ruggedness in the adversarial loss landscape
- Some training instances self-heal, naturally recovering from instability without external intervention
- Gaussian noise injection at FOSC threshold effectively mitigates overestimation with minimal performance impact

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Robustness overestimation occurs due to gradient masking, where the model appears robust to white-box attacks (PGD) but fails against black-box attacks (Square Attack).
- Mechanism: The model learns to obscure gradients during adversarial training, making it difficult for gradient-based attacks to find effective perturbations while remaining vulnerable to black-box methods.
- Core assumption: The loss landscape becomes locally rugged, creating regions where gradient-based attacks struggle to converge effectively.
- Evidence anchors:
  - [abstract] "disproportionately high PGD validation accuracy compared to the AutoAttack testing accuracy...linked to gradient masking"
  - [section 4.2] "We support our perspective by utilizing FOSC...metrics represent values that measure the degree of adversarial convergence"
  - [corpus] Weak evidence - no directly comparable mechanisms found in neighbors
- Break condition: When FOSC values remain consistently low, indicating effective convergence of adversarial examples and absence of gradient masking.

### Mechanism 2
- Claim: TRADES' inner maximization causes overfitting to TPGD perturbations, creating learnable patterns that mask gradients.
- Mechanism: The push-and-pull dynamic between KL divergence maximization and CE loss minimization causes the model to overfit to specific perturbation characteristics rather than learning genuine robustness.
- Core assumption: The logit pairing technique creates local minima where gradient-based attacks cannot effectively navigate.
- Evidence anchors:
  - [section 5.1] "minimizing the distance between the clean and adversarial logits...suggests the model may overfit to the unique characteristics of TPGD perturbations"
  - [section 5.2] "The push-and-pull dynamic in logit-logit relations...suggests the model may overfit to the unique characteristics of TPGD perturbations"
  - [corpus] No direct evidence - this appears to be a novel mechanism identified in this paper
- Break condition: When clean training accuracy drops significantly while FOSC remains low, indicating the model has escaped problematic local minima.

### Mechanism 3
- Claim: Batch-level gradient spikes and cosine similarity drops indicate local ruggedness in the adversarial component of the loss landscape.
- Mechanism: Specific batches trigger sharp changes in gradient norms and directions, creating locally rugged optimization landscapes that prevent effective adversarial example generation.
- Core assumption: The adversarial component of TRADES loss creates non-smooth optimization surfaces that manifest as batch-level gradient irregularities.
- Evidence anchors:
  - [section 5.2] "multiple spikes in W grad norm are observed across several batches...KL norm also shows spikes in these batches"
  - [section 5.2] "cosine similarity of weight gradients before and after each training step sharply declines at the same points where the KL and total gradient norms spike"
  - [corpus] Weak evidence - batch-level gradient analysis is not present in neighbor papers
- Break condition: When gradient cosine similarity remains consistently high across batches, indicating smooth optimization landscape.

## Foundational Learning

- Concept: Projected Gradient Descent (PGD) attacks and their convergence properties
  - Why needed here: Understanding how PGD attacks work and fail is central to identifying gradient masking
  - Quick check question: What does a high FOSC value indicate about the convergence of a PGD attack?

- Concept: First-Order Stationary Condition (FOSC) and its relationship to loss landscape smoothness
  - Why needed here: FOSC serves as the primary diagnostic tool for detecting gradient masking
  - Quick check question: How does FOSC relate to the ability of an attacker to find effective adversarial examples?

- Concept: Kullback-Leibler divergence and its role in TRADES loss function
  - Why needed here: Understanding the KL divergence term is essential for grasping how TRADES creates its unique optimization challenges
  - Quick check question: What is the effect of maximizing KL divergence between clean and adversarial logits?

## Architecture Onboarding

- Component map: Data preprocessing -> ResNet-18 model -> TPGD adversary generation -> TRADES loss (CE + KL) -> SGD optimization -> FOSC monitoring -> Evaluation (PGD-10 + AutoAttack)

- Critical path:
  1. Training loop with TPGD adversary generation
  2. Loss computation and backpropagation
  3. FOSC evaluation on validation set
  4. Instability detection based on FOSC threshold
  5. Gaussian noise injection when instability detected

- Design tradeoffs:
  - Using TRADES vs. standard PGD-AT: Better baseline performance but introduces gradient masking risk
  - Batch size selection: Smaller batches increase instability but may provide better gradient estimates
  - Beta parameter tuning: Lower values increase instability but may improve clean accuracy

- Failure signatures:
  - PGD-10 accuracy significantly higher than AutoAttack accuracy
  - Sharp spikes in FOSC values during training
  - Batch-level gradient norm spikes coinciding with KL norm increases
  - Declining cosine similarity between pre/post-step gradients

- First 3 experiments:
  1. Run TRADES training with default parameters and monitor FOSC vs. PGD-10 vs. AutoAttack accuracy
  2. Test instability across different beta values (1, 3, 6) while keeping other parameters constant
  3. Implement Gaussian noise injection at FOSC threshold and compare stability improvements

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the logit pairing technique in TRADES fundamentally compromise robustness, or can it be modified to maintain reliability while preserving its benefits?
- Basis in paper: [explicit] The paper draws parallels between TRADES and abandoned logit pairing methods (ALP, LSQ) due to gradient masking issues, suggesting TRADES may share similar reliability problems
- Why unresolved: The paper identifies instability but doesn't determine whether the logit pairing mechanism itself is inherently flawed or if specific parameter settings cause the issues
- What evidence would resolve it: Experiments comparing TRADES with alternative loss formulations that avoid logit pairing, or systematic analysis of how different trade-off parameters affect gradient masking behavior

### Open Question 2
- Question: Can the self-healing phenomenon be systematically predicted and leveraged to create more stable training algorithms without external intervention?
- Basis in paper: [explicit] The authors observe that some training instances naturally recover from instability through internal mechanisms, suggesting inherent regularization properties in the training dynamics
- Why unresolved: While the phenomenon is observed, the conditions triggering self-healing are not characterized, and the mechanism remains unclear
- What evidence would resolve it: Analysis of batch-level gradient patterns, loss landscape evolution, and parameter updates that consistently precede self-healing events across multiple training runs

### Open Question 3
- Question: Is there a fundamental trade-off between the benefits of minimizing KL divergence in TRADES and the risk of gradient masking, or can this be resolved through alternative optimization strategies?
- Basis in paper: [inferred] The paper shows that TRADES' unique loss formulation (minimizing KL divergence between clean and adversarial logits) may cause overfitting to TPGD perturbations, suggesting inherent tension in the objective
- Why unresolved: The analysis identifies the correlation but doesn't establish whether the KL divergence term is fundamentally problematic or if optimization techniques could mitigate the issue
- What evidence would resolve it: Comparative studies of TRADES variants with modified inner maximization objectives, or experiments testing whether alternative regularization techniques can preserve benefits while eliminating instability

## Limitations
- The analysis is primarily empirical with limited theoretical grounding for why gradient masking occurs specifically in TRADES
- The proposed Gaussian noise solution appears effective but lacks rigorous justification for why noise injection resolves the issue
- The study focuses on ResNet-18 architectures and three specific datasets, limiting generalizability to other model architectures and data domains

## Confidence
- **High Confidence**: The identification of robustness overestimation in TRADES (PGD-10 vs AutoAttack discrepancy) - well-supported by empirical measurements across multiple configurations
- **Medium Confidence**: The claim that smaller batch sizes, lower beta values, larger learning rates, and higher dataset complexity increase instability - demonstrated through experiments but lacks mechanistic explanation
- **Low Confidence**: The Gaussian noise solution's effectiveness across different training scenarios - while shown to work in the presented experiments, the solution appears somewhat ad-hoc without clear theoretical motivation

## Next Checks
1. **Cross-architecture validation**: Test the gradient masking phenomenon and noise solution on architectures beyond ResNet-18 (e.g., WideResNet, EfficientNet) to verify generalizability of the findings.

2. **Theoretical analysis of noise injection**: Develop a theoretical framework explaining why Gaussian noise added to the first ten batches of an epoch specifically addresses gradient masking, including analysis of how noise affects the optimization landscape.

3. **Alternative stability metrics**: Validate the findings using alternative stability metrics beyond FOSC (such as empirical robustness certificates or different gradient-based convergence measures) to confirm that FOSC is indeed the most appropriate diagnostic tool.