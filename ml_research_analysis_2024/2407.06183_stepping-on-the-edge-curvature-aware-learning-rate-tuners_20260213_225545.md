---
ver: rpa2
title: 'Stepping on the Edge: Curvature Aware Learning Rate Tuners'
arxiv_id: '2407.06183'
source_url: https://arxiv.org/abs/2407.06183
tags:
- learning
- rate
- cdat
- epoch
- loss
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work studies how classical learning rate tuners, such as linesearch
  and quadratically greedy methods, perform in deep learning settings. In the full-batch
  regime, these tuners qualitatively underperform constant learning rate baselines,
  despite being better at instantaneous loss reduction.
---

# Stepping on the Edge: Curvature Aware Learning Rate Tuners

## Quick Facts
- arXiv ID: 2407.06183
- Source URL: https://arxiv.org/abs/2407.06183
- Reference count: 40
- Classical learning rate tuners underperform constant learning rates in full-batch deep learning despite better instantaneous loss reduction

## Executive Summary
This paper investigates why classical learning rate tuners like linesearch and quadratically greedy methods, despite achieving better one-step loss reduction, ultimately underperform constant learning rate baselines in full-batch deep learning settings. The analysis reveals that these tuners tend to undershoot the edge of stability, leading to ever-decreasing learning rates and ever-increasing sharpness. To address this, the authors propose Curvature Dynamics Aware Tuning (CDAT), a new method that prioritizes long-term curvature stabilization over immediate loss reduction, achieving better performance than fine-tuned constant learning rates in full-batch experiments.

## Method Summary
The paper studies classical learning rate tuners (linesearch with Armijo-Goldstein criterion, quadratically greedy methods) and proposes a new Curvature Dynamics Aware Tuning (CDAT) method. CDAT uses a scaling factor σ to position learning rates at or near the edge of stability based on curvature feedback. The method requires computing the largest eigenvalue of the Hessian using power iteration, which is approximated using forward-mode automatic differentiation. The authors compare these approaches across multiple architectures (ResNet34/50, MLP Mixer, NanoLM, ViT) and datasets (CIFAR10, Tiny Shakespeare, ImageNet Imagenette) in both full-batch and mini-batch regimes.

## Key Results
- Classical tuners undershoot the edge of stability in full-batch regime, causing learning rates to decrease by 3 orders of magnitude while sharpness increases
- CDAT outperforms fine-tuned constant learning rates in full-batch experiments and exhibits progressive learning rate increases similar to automatic warm-up schedules
- In mini-batch regime, CDAT's performance is subdued due to stochasticity effects

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Classical learning rate tuners undershoot the edge of stability, causing progressive sharpness increase and learning rate decrease
- Mechanism: Linesearch and quadratically greedy methods optimize for immediate loss reduction, which drives learning rates below the stability threshold. This removes the nonlinear feedback that normally stabilizes sharpness at the edge of stability, triggering a "snowball effect" where sharpness increases and learning rates decrease
- Core assumption: The edge of stability provides essential nonlinear feedback that prevents runaway sharpening
- Evidence anchors:
  - [abstract] "classical learning rate tuners may yield greater one-step loss reduction, yet they ultimately underperform in the long term when compared to constant learning rates in the full batch regime"
  - [section 2.2] "Classical learning rate tuners can undershoot the edge of stability. Learning rate decreases by 3 orders of magnitude for tuners (1st panel) while sharpness increases (2nd panel)"
  - [corpus] Weak - no direct corpus evidence, but consistent with "Edge of Stochastic Stability" paper's findings

### Mechanism 2
- Claim: Curvature Dynamics Aware Tuning (CDAT) maintains proximity to the edge of stability by dynamically adjusting learning rate based on curvature feedback
- Mechanism: CDAT uses a scaling factor σ to position the learning rate at or near the edge of stability. When sharpness increases, CDAT increases the learning rate to maintain edge proximity, preventing runaway sharpening while still making progress
- Core assumption: Maintaining proximity to the edge of stability is more important for long-term success than greedy loss minimization
- Evidence anchors:
  - [abstract] "CDAT outperforms fine-tuned constant learning rates and exhibits a progressive increase in learning rate, akin to automatic warm-up schedules"
  - [section 3.1] "Selecting the learning rate to be on edge (σ = 2) is on par with or better than a fine-tuned constant learning rate"
  - [corpus] Weak - no direct corpus evidence, but aligns with "A Unified Noise-Curvature View of Loss of Trainability" paper's findings

### Mechanism 3
- Claim: Stochasticity in mini-batch regimes introduces confounding effects that explain previous success of learning rate tuners
- Mechanism: Mini-batch noise attenuates the sharpness dynamics observed in full-batch training. Linesearch methods applied to mini-batches tend to select larger learning rates than they would in full-batch regime, potentially allowing them to avoid undershooting the full objective's edge of stability
- Core assumption: Mini-batch noise provides implicit regularization that affects curvature dynamics
- Evidence anchors:
  - [abstract] "In the mini-batch regime, CDAT's performance is subdued due to stochasticity"
  - [section 2.2] "The results presented in Fig. 1 in the full batch regime do not contradict previous results from Vaswani et al. [2019], who studied linesearches for deep learning in the stochastic regime"
  - [corpus] Weak - limited corpus evidence, but "Edge of Stochastic Stability" paper suggests similar findings

## Foundational Learning

- Concept: Edge of Stability phenomenon
  - Why needed here: Understanding why constant learning rates work well despite not being optimal for each step requires knowing about the edge of stability and its stabilizing effects
  - Quick check question: What happens to sharpness when learning rate is fixed at a value that would theoretically cause instability?

- Concept: Hessian eigenvalues and sharpness
  - Why needed here: The paper analyzes how learning rate tuners affect the largest eigenvalue of the Hessian (sharpness), which determines stability
  - Quick check question: How is sharpness formally defined in terms of the Hessian matrix?

- Concept: Gradient descent dynamics with fixed vs adaptive learning rates
  - Why needed here: The paper compares constant learning rates with adaptive tuners, requiring understanding of how different learning rate strategies affect training dynamics
  - Quick check question: What is the theoretical advantage of adaptive learning rates in convex optimization?

## Architecture Onboarding

- Component map: Base optimizer -> Curvature estimation -> CDAT rule -> Parameter update
- Critical path: Forward pass → Loss computation → Backward pass → Curvature estimation → CDAT learning rate selection → Parameter update
- Design tradeoffs:
  - Memory vs accuracy: Computing second derivatives requires more memory but provides better curvature estimates
  - Batch size vs stability: Larger batches give more stable curvature estimates but may reduce the benefits of mini-batch noise
  - EMA smoothing vs responsiveness: More smoothing reduces noise but may delay learning rate adjustments
- Failure signatures:
  - Learning rate decreasing to near zero while sharpness increases (classical tuners undershooting edge)
  - Sharpness increasing without bound (lack of stabilization feedback)
  - Poor performance in highly stochastic regimes (curvature estimation unreliable)
  - Divergence when σ > 2 (operating above edge of stability)
- First 3 experiments:
  1. Run GD with constant learning rate vs CDAT (σ=2) on a simple MLP on MNIST - verify CDAT maintains learning rate while improving convergence
  2. Run linesearch vs constant learning rate on ResNet on CIFAR10 - observe learning rate decrease and sharpness increase with linesearch
  3. Run CDAT with varying σ values on a Vision Transformer - find optimal σ that balances stability and progress

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What are the precise mechanisms by which the higher-order terms in the dynamics provide additional stability to the edge of stability (EOS) phenomenon?
- Basis in paper: [inferred] The paper mentions that higher-order terms in the dynamics provide additional stability to EOS dynamics, but does not provide a detailed analysis of these terms.
- Why unresolved: The paper acknowledges the importance of higher-order terms but does not explore them in depth, leaving a gap in understanding the full dynamics of EOS.
- What evidence would resolve it: A detailed mathematical analysis of the higher-order terms in the dynamics, possibly through numerical simulations or analytical methods, could provide insights into their stabilizing effects.

### Open Question 2
- Question: How does the CDAT rule perform in terms of computational efficiency compared to traditional learning rate tuners?
- Basis in paper: [explicit] The paper mentions that the CDAT rule requires approximately three times the computational cost of computing the objective due to the use of forward mode automatic differentiation.
- Why unresolved: While the paper acknowledges the increased computational cost, it does not provide a comprehensive analysis of the trade-offs between the performance gains and computational overhead of CDAT.
- What evidence would resolve it: Benchmarking the CDAT rule against traditional learning rate tuners in terms of wall time and memory usage across various architectures and datasets would provide a clearer picture of its computational efficiency.

### Open Question 3
- Question: What are the optimal strategies for adapting the scaling factor and EMA parameter in the CDAT rule for different batch sizes and architectures?
- Basis in paper: [explicit] The paper mentions that the optimal scaling factor and EMA parameter vary with batch size and architecture, but does not provide a detailed strategy for their adaptation.
- Why unresolved: The paper identifies the need for adaptation but does not propose a systematic approach for determining the optimal values of these parameters in different settings.
- What evidence would resolve it: Developing a set of guidelines or a heuristic for selecting the scaling factor and EMA parameter based on the characteristics of the dataset and architecture would help in optimizing the performance of CDAT.

## Limitations
- Experiments primarily conducted in full-batch regime with limited mini-batch validation
- No validation set performance metrics reported, making it difficult to assess generalization
- Limited architectural diversity (primarily ResNets and Transformers)
- CDAT introduces additional hyperparameters (EMA smoothing, σ) that may affect robustness

## Confidence
- High confidence: The empirical observation that classical tuners undershoot the edge of stability in full-batch training, leading to ever-decreasing learning rates and increasing sharpness
- Medium confidence: CDAT's superiority over fine-tuned constant learning rates, as results are limited to specific architectures and datasets
- Low confidence: The claim that mini-batch noise explains previous success of learning rate tuners, as this requires additional controlled experiments varying batch sizes systematically

## Next Checks
1. Test CDAT across diverse architectures (CNNs, RNNs, Graph Neural Networks) and datasets to verify generalizability beyond ResNets and Transformers
2. Conduct systematic experiments varying batch sizes to quantify the relationship between mini-batch noise and tuner performance
3. Measure validation performance and generalization gaps for all methods to assess whether edge proximity benefits translate to better generalization