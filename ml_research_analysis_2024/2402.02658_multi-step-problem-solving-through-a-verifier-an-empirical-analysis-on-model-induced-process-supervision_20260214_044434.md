---
ver: rpa2
title: 'Multi-step Problem Solving Through a Verifier: An Empirical Analysis on Model-induced
  Process Supervision'
arxiv_id: '2402.02658'
source_url: https://arxiv.org/abs/2402.02658
tags:
- verifier
- data
- arxiv
- solutions
- mips
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Model-induced Process Supervision (MiPS),
  an automated method for creating training data for verifiers in multi-step problem
  solving. MiPS generates intermediate step annotations by sampling completions from
  a reasoning model and calculating the percentage of correct completions.
---

# Multi-step Problem Solving Through a Verifier: An Empirical Analysis on Model-induced Process Supervision

## Quick Facts
- arXiv ID: 2402.02658
- Source URL: https://arxiv.org/abs/2402.02658
- Authors: Zihan Wang; Yunxuan Li; Yuexin Wu; Liangchen Luo; Le Hou; Hongkun Yu; Jingbo Shang
- Reference count: 6
- One-line primary result: MiPS improves PaLM 2 performance on math and coding tasks with accuracy gains of +0.67% on GSM8K, +4.16% on MATH, and +0.92% on MBPP

## Executive Summary
This paper introduces Model-induced Process Supervision (MiPS), an automated method for creating training data for verifiers in multi-step problem solving. MiPS generates intermediate step annotations by sampling completions from a reasoning model and calculating the percentage of correct completions. The authors show that MiPS improves the performance of PaLM 2 on math and coding tasks compared to output supervision trained verifiers. The paper also demonstrates that verifiers trained on MiPS data generalize well across different reasoning models.

## Method Summary
The method involves generating solutions using a reasoner with temperature-based decoding, then creating MiPS data by sampling completions of intermediate steps and calculating correctness scores as the percentage of correct completions. Verifiers are trained on this MiPS data using either direct correctness scores (Soft Objective) or binarized scores (Hard Objective). The trained verifiers use aggregation functions to combine step-wise prediction scores into final solution scores, with the paper finding that aggregation functions focusing on high predicted scores work better for MiPS data than those focusing on low scores.

## Key Results
- MiPS-trained verifiers achieve +0.67% accuracy improvement on GSM8K compared to output supervision trained verifiers
- Performance gains of +4.16% on MATH and +0.92% on MBPP demonstrate consistent improvement across different problem types
- Verifiers trained on MiPS data generated by one reasoner can effectively validate solutions from different and more competent reasoners

## Why This Works (Mechanism)

### Mechanism 1
- Claim: MiPS generates training data by sampling completions from a reasoning model and using the proportion of correct completions as accuracy scores for intermediate steps
- Mechanism: For each intermediate solution, the reasoner generates nmc completions using temperature-based decoding, and the correctness score is calculated as the percentage of these completions that solve the problem correctly
- Core assumption: The reasoner's completion behavior provides reliable signal about intermediate step quality, despite the reasoner itself being imperfect
- Evidence anchors:
  - [abstract] "MiPS annotates an intermediate step by sampling completions of this solution through the reasoning model, and obtaining an accuracy defined as the proportion of correct completions"
  - [section] "By completing an intermediate solution with a reasoner several times through a sample decoding mechanism, and the percentage of the completed solutions being correct is referred to as the correctness of the solution"
  - [corpus] Weak - related papers discuss process supervision but don't provide specific evidence for this sampling mechanism
- Break condition: If the reasoner consistently generates poor completions for intermediate steps, the accuracy estimates become unreliable and the training signal degrades

### Mechanism 2
- Claim: Aggregation functions focusing on high predicted scores work better for MiPS data than those focusing on low scores
- Mechanism: The verification process uses aggregation functions to combine step-wise prediction scores into a final score, and MiPS data benefits from aggregation functions that emphasize high scores rather than low scores
- Core assumption: The noise in MiPS data causes earlier steps to have lower quality predictions, making aggregation functions that focus on high scores more effective
- Evidence anchors:
  - [abstract] "we suggest and empirically show that verification focusing on high predicted scores of the verifier shall be preferred over that of low predicted scores, contrary to prior observations on human curated data"
  - [section] "sum_logprob does not have a high correlation with a correct solution, as it naturally penalizes long solutions"
  - [corpus] Moderate - related papers discuss aggregation but don't specifically compare high-score vs low-score focusing functions
- Break condition: If the reasoner becomes highly accurate, the noise differences between early and late steps may diminish, potentially making low-score-focused aggregation functions competitive again

### Mechanism 3
- Claim: MiPS-trained verifiers generalize well across different reasoning models
- Mechanism: Verifiers trained on MiPS data generated by one reasoner can effectively validate solutions from different and more competent reasoners
- Core assumption: The MiPS data captures generalizable patterns about intermediate step quality rather than overfitting to the specific reasoning model that generated it
- Evidence anchors:
  - [abstract] "our study demonstrates that the verifier exhibits strong generalization ability across different reasoning models"
  - [section] "we show that verifiers trained onMiPS data generated by a reasoner can transfer to validate solutions by a different (and more competent) reasoner"
  - [corpus] Weak - related papers don't provide evidence for cross-model generalization of verifiers
- Break condition: If the reasoning models have fundamentally different solution patterns or error modes, the verifier may fail to generalize effectively

## Foundational Learning

- Concept: Monte Carlo sampling
  - Why needed here: MiPS uses Monte Carlo sampling to estimate the accuracy of intermediate steps by generating multiple completions and calculating the proportion of correct ones
  - Quick check question: How does Monte Carlo sampling help estimate intermediate step accuracy in MiPS?

- Concept: Temperature-based decoding
  - Why needed here: Temperature-based decoding is used both for generating initial solutions and for completing intermediate steps to introduce diversity in the sampling process
  - Quick check question: What role does temperature-based decoding play in generating diverse samples for MiPS accuracy estimation?

- Concept: Aggregation functions
  - Why needed here: Aggregation functions combine step-wise prediction scores into a final score for solution selection, and the choice of aggregation function significantly impacts performance with MiPS data
  - Quick check question: Why do aggregation functions that focus on high scores perform better with MiPS data compared to those focusing on low scores?

## Architecture Onboarding

- Component map:
  - Reasoner model -> MiPS data generator -> Verifier model -> Aggregation function -> Solution selector

- Critical path:
  1. Generate initial solutions using reasoner with temperature t_g
  2. For each intermediate step, sample n_mc completions with temperature t_mc
  3. Calculate correctness scores as percentage of correct completions
  4. Train verifier on MiPS data to predict step-wise scores
  5. Use trained verifier with appropriate aggregation function to select best solutions

- Design tradeoffs:
  - Higher n_mc provides more accurate MiPS scores but increases computational cost
  - Different aggregation functions trade off sensitivity to early vs late step quality
  - Training objectives (soft vs hard) affect how verifier handles noisy MiPS data
  - Choice of reasoner model affects both data quality and transfer ability

- Failure signatures:
  - Poor performance despite high MiPS data accuracy suggests aggregation function mismatch
  - Degradation with more generated solutions indicates overfitting to reasoner's patterns
  - Cross-model transfer failure suggests overly specific training rather than generalizable patterns
  - Inconsistent results across runs may indicate insufficient sampling or unstable training

- First 3 experiments:
  1. Compare performance of OSV vs PSV with max aggregation on GSM8K using PaLM 2-S
  2. Test different aggregation functions (max, sum_logit, min, sum_logprob) on MiPS data accuracy vs test performance
  3. Evaluate cross-model transfer by training verifier on PaLM 2-S solutions and testing on PaLM 2-L solutions for GSM8K

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal temperature setting for the reasoner-verifier framework across different datasets and problem types?
- Basis in paper: [explicit] The paper uses temperature values of 0.7 for both constructing MiPS and generating solutions, but notes that this may not be optimal for all datasets and tasks
- Why unresolved: The paper does not systematically explore different temperature settings or provide evidence for why 0.7 is optimal. The temperature could significantly impact the diversity and quality of generated solutions
- What evidence would resolve it: A comprehensive ablation study varying temperature values across different datasets (GSM8K, MATH, MBPP) and problem types, measuring verification accuracy and solution quality

### Open Question 2
- Question: How does the performance of MiPS-trained verifiers scale with the competence level of the reasoner model used to generate the MiPS data?
- Basis in paper: [explicit] The paper notes that using a more competent reasoner for completion could reduce noise in MiPS data, but only experiments with using the same model for both generation and completion
- Why unresolved: The paper does not explore how different levels of reasoner competence affect the quality of MiPS data and the subsequent verifier performance. This could reveal important trade-offs between data quality and computational cost
- What evidence would resolve it: Experiments comparing MiPS-trained verifiers using data generated by reasoners of varying competence levels (e.g., PaLM 2-S vs. PaLM 2-L), measuring both verification accuracy and generalization to other reasoners

### Open Question 3
- Question: What is the impact of different fine-tuning strategies on the performance of the reasoner model when trained with MiPS-verified data?
- Basis in paper: [explicit] The paper mentions that future work could explore using MiPS-verified data to tune the reasoner via reinforcement learning, but does not conduct such experiments
- Why unresolved: The paper only explores using MiPS for verifier training, not for reasoner improvement. Different fine-tuning strategies could leverage the MiPS data to significantly improve reasoner performance
- What evidence would resolve it: Experiments comparing various fine-tuning approaches (e.g., standard fine-tuning, reinforcement learning with MiPS rewards, supervised fine-tuning on MiPS-verified solutions) on the reasoner's performance across the three datasets

## Limitations
- The effectiveness of MiPS depends on the assumption that reasoner's completion behavior provides reliable signal about intermediate step quality, which is not systematically validated
- Cross-model transfer claims are based on limited experiments (only PaLM 2-S to PaLM 2-L transfer) without testing fundamentally different model architectures
- The paper does not explore how different temperature settings affect performance across various datasets and problem types

## Confidence

**High confidence**: The empirical results showing MiPS-trained verifiers outperform output supervision trained verifiers on GSM8K (+0.67%), MATH (+4.16%), and MBPP (+0.92%) are supported by direct experimental comparisons. The methodology for generating MiPS data and training verifiers is clearly specified.

**Medium confidence**: The claim about aggregation functions benefiting from focusing on high predicted scores is supported by experimental evidence, but the analysis could be strengthened with more rigorous statistical testing and ablation studies to isolate the effect of aggregation function choice from other factors.

**Low confidence**: The generalization claims for cross-model transfer are based on limited experiments (only PaLM 2-S to PaLM 2-L transfer). The paper does not test transfer to fundamentally different model architectures or reasoning approaches, which would be necessary to establish robust generalization properties.

## Next Checks

1. **Ablation study on sampling parameters**: Conduct experiments varying n_mc (number of completions sampled) and temperature values to quantify their impact on MiPS data quality and downstream verifier performance. This would help establish optimal parameter ranges and test the sensitivity of the approach to these design choices.

2. **Cross-architecture transfer validation**: Test whether MiPS-trained verifiers can transfer to reasoning models with different architectures (e.g., GPT, Claude, or specialized math models) rather than just different sizes of the same family. This would provide stronger evidence for the claimed generalization ability.

3. **Error analysis on aggregation functions**: Perform detailed analysis of when high-score-focused aggregation functions outperform low-score-focused ones, including error cases where this pattern breaks down. This would help understand the limitations and conditions under which the aggregation function recommendation holds.