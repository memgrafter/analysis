---
ver: rpa2
title: On the Nonconvexity of Push-Forward Constraints and Its Consequences in Machine
  Learning
arxiv_id: '2403.07471'
source_url: https://arxiv.org/abs/2403.07471
tags:
- convex
- such
- convexity
- measures
- maps
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates the convexity properties of push-forward
  constraints in machine learning, focusing on transport maps (functions that redistribute
  one probability measure to another) and equalizing maps (functions that produce
  equal output distributions across distinct probability measures). The authors prove
  that these sets are generally nonconvex for most probability measures, with specific
  conditions identified for when they become convex.
---

# On the Nonconvexity of Push-Forward Constraints and Its Consequences in Machine Learning

## Quick Facts
- arXiv ID: 2403.07471
- Source URL: https://arxiv.org/abs/2403.07471
- Reference count: 11
- Key outcome: Push-forward constraints between arbitrary probability measures are generally nonconvex, preventing convex loss design for generative modeling and fairness problems.

## Executive Summary
This paper investigates the convexity properties of push-forward constraints in machine learning, focusing on transport maps (functions that redistribute one probability measure to another) and equalizing maps (functions that produce equal output distributions across distinct probability measures). The authors prove that these sets are generally nonconvex for most probability measures, with specific conditions identified for when they become convex. This structural result has significant implications for learning problems like generative modeling and group-fairness, showing that these problems are typically not convex due to the nonconvexity of push-forward constraints. The paper demonstrates that no convex loss can quantify the deviation from a nonconvex constraint, making it impossible to design convex optimization problems for these tasks in general settings. Two approaches to recover convexity are proposed: weakening or strengthening the constraint, or radically changing the models from deterministic to random couplings.

## Method Summary
The paper employs mathematical proofs using convex analysis, probability theory, and measure theory to analyze the convexity properties of push-forward constraints. The authors examine transport maps (T(P,Q)) between two probability measures and equalizing maps (E(P,Q)) that produce equal output distributions from distinct input measures. The analysis covers both continuous and discrete measures, including empirical measures with finite support. The main theoretical contributions include proving general nonconvexity conditions, identifying special cases where convexity holds, and demonstrating the implications for machine learning applications. The methodology relies on establishing necessary and sufficient conditions for convexity through rigorous mathematical arguments involving measure theory and convex analysis.

## Key Results
- Push-forward constraints (transport and equalizing maps) are generally nonconvex for most probability measures
- No convex loss can vanish exactly on a nonconvex constraint set, ruling out convex optimization for these problems
- Empirical measures only yield convex constraints under restrictive arithmetic conditions (sample sizes must be multiples of each other)
- Convexity can be recovered through random coupling relaxations (Kantorovich formulation) or by weakening/strengthening constraints

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Push-forward constraints between arbitrary probability measures are generally nonconvex, which directly prevents convex loss design.
- Mechanism: The paper proves that for most measures, the set of functions pushing one measure to another (T(P,Q)) or equalizing two measures (E(P,Q)) fails to be convex, and by convex analysis, a convex loss can only vanish on a convex set. Therefore, no convex loss can enforce such constraints.
- Core assumption: The push-forward operation is deterministic and defined via measure preservation; convexity must hold for arbitrary measures, not just special cases.
- Evidence anchors:
  - [abstract]: "we provide a range of sufficient and necessary conditions for the (non)convexity of two sets of functions... These push-forward constraints are not convex"
  - [section 4.1]: "Theorem 4.2... If L : G → [0, +∞] is a convex function, then L−1({0}) is a convex set. Therefore, if C ⊆ G is nonconvex, then there exist no convex C-loss."
  - [corpus]: Weak, no direct match; corpus entries focus on other nonconvex ML optimization problems but do not discuss push-forward constraints.
- Break condition: If the measures have special structure (e.g., empirical measures where one divides the other evenly), the sets can become convex, enabling convex losses.

### Mechanism 2
- Claim: Even when using empirical measures, convexity is rare and only occurs under restrictive arithmetic conditions.
- Mechanism: For uniform finitely supported measures, convexity of T(P,Q) or E(P,Q) only holds if the sample sizes are multiples of each other; otherwise the constraint sets are nonconvex, blocking convex optimization.
- Core assumption: Empirical measures are almost surely finitely supported and uniform in weights; arithmetic relationships between sample sizes govern convexity.
- Evidence anchors:
  - [section 3.1]: "Proposition 3.6... if n=m, then T(P,Q) contains exactly n! functions... if n>m and m does not divide n, then T(P,Q)=∅"
  - [section 3.2]: "Proposition 3.10... if m and n are coprime, then E(P,Q) is the set of constant functions... if m and n are not coprime, then E(P,Q) is not convex."
  - [corpus]: No direct evidence; corpus entries do not discuss empirical measure arithmetic.
- Break condition: If the number of samples in each group is not a multiple of the other, convexity fails; if they are multiples, the constraint set may collapse to trivial constants.

### Mechanism 3
- Claim: Replacing deterministic push-forward by random couplings restores convexity.
- Mechanism: Optimal transport literature shows that Kantorovich's relaxation (random couplings) yields a convex feasible set Π(P,Q), enabling convex optimization even when the deterministic Monge map is nonconvex.
- Core assumption: Random couplings can approximate the same marginal constraints as deterministic maps; the objective remains convex under this reformulation.
- Evidence anchors:
  - [section 5.2]: "Kantorovich formulation of optimal transport... addresses the following relaxation: min π∈Π(P,Q) ∫∥x−y∥²dπ(x,y). Not only this problem always admits a solution... but it remains convex since Π(P,Q) is a convex set."
  - [corpus]: No direct match; corpus focuses on nonconvex optimization but not coupling-based relaxations.
- Break condition: The relaxation changes the feasible set; solutions may not be deterministic functions, which may be unsuitable for applications requiring explicit mappings.

## Foundational Learning

- Concept: Push-forward measure
  - Why needed here: Central to defining transport and equalizing map constraints; all convexity analysis depends on understanding how a deterministic function redistributes probability mass.
  - Quick check question: If P is a Dirac measure at x and Q is uniform on {y₁,y₂}, does any deterministic f satisfy f♯P = Q?

- Concept: Convexity of sets and functions
  - Why needed here: The paper's main argument hinges on proving sets of admissible functions are nonconvex and invoking convex analysis to rule out convex losses.
  - Quick check question: If a set C contains two functions f and g, must the average (f+g)/2 also be in C for C to be convex?

- Concept: Empirical measures and their structure
  - Why needed here: The analysis for practical ML involves empirical distributions; their finite, uniform structure dictates when convexity can occur.
  - Quick check question: If P and Q are uniform on n and m points respectively, under what arithmetic condition on n and m can E(P,Q) be convex?

## Architecture Onboarding

- Component map:
  - Measure theory module: defines push-forward, supports, absolutely continuous vs discrete measures
  - Convexity analysis module: proves nonconvexity of T(P,Q) and E(P,Q) under general conditions
  - ML application module: connects nonconvex constraints to GANs and fairness problems
  - Convexity recovery module: explores weakening constraints or using random couplings

- Critical path:
  1. Verify convexity of constraint set for given measures
  2. If nonconvex, confirm no convex loss exists (Theorem 4.2)
  3. Either switch to a convex surrogate or adopt coupling relaxation

- Design tradeoffs:
  - Deterministic maps preserve interpretability but yield nonconvex problems
  - Random couplings restore convexity but lose explicit function form
  - Weakening fairness constraints (e.g., covariance) gives convexity but weaker guarantees

- Failure signatures:
  - Attempting to minimize a convex loss over T(P,Q) or E(P,Q) without checking convexity leads to vacuous or misleading solutions
  - Empirical measure arithmetic mismatch (coprime sizes) implies unavoidable nonconvexity

- First 3 experiments:
  1. Test convexity of T(P,Q) for two simple discrete measures (e.g., Dirac vs two-point uniform)
  2. Verify that a convex loss cannot vanish exactly on a nonconvex set by constructing a counterexample
  3. Implement Kantorovich relaxation for a small optimal transport problem and compare solutions to Monge's formulation

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Under what precise conditions does the convexity of equalizing maps between discrete measures with nondisjoint supports depend on the structure of their common support?
- Basis in paper: [explicit] Theorem A.1 and Proposition A.2 provide conditions for convexity when supports are disjoint, but the extension to nondisjoint supports via the "minimum measure" operation leaves open questions about how the structure of overlapping supports affects convexity
- Why unresolved: The paper only provides a characterization for the case where supports are disjoint, and the extension to nondisjoint supports relies on decomposing measures into disjoint parts, but doesn't fully characterize when convexity holds for arbitrary overlapping support structures
- What evidence would resolve it: A complete characterization theorem for convexity of E(P,Q) when P and Q have arbitrary overlapping supports, specifying the exact conditions on the probability weights and locations of the common support elements

### Open Question 2
- Question: Can the nonconvexity of equalizing maps between absolutely continuous measures be proven without relying on the specific construction using uniform measures and disjoint supports?
- Basis in paper: [inferred] The proof of Proposition 3.9 uses a specific construction involving uniform measures and disjoint supports, but the paper notes this as a "more general case" suggesting there might be a more direct approach
- Why unresolved: The current proof strategy requires multiple cases and constructions, suggesting the possibility of a more elegant, unified proof that directly demonstrates nonconvexity without these intermediate steps
- What evidence would resolve it: A simpler proof that directly establishes nonconvexity of E(P,Q) for any two distinct absolutely continuous measures without using uniform measure transformations or requiring separate cases for disjoint/non-disjoint supports

### Open Question 3
- Question: Does the convexity/nonconvexity distinction between transport maps and equalizing maps have implications for designing learning algorithms beyond what's discussed in the paper?
- Basis in paper: [explicit] Section 4.2 discusses how nonconvexity affects generative modeling and fairness, but suggests there may be broader implications for other learning problems involving push-forward constraints
- Why unresolved: The paper focuses on two specific applications but mentions that push-forward constraints appear in many other contexts, leaving open the question of whether similar convexity issues arise in these other domains
- What evidence would resolve it: A systematic study of additional machine learning problems that involve push-forward constraints (e.g., domain adaptation, multi-view learning, or certain reinforcement learning settings) showing whether they face similar convexity limitations or whether the transport/equalizing distinction has different implications in those contexts

## Limitations

- The exact conditions for convexity of transport maps between discrete measures remain unknown (Corollary 3.3), which limits the applicability of the results to specific practical scenarios.
- The analysis of equalizing maps for discrete measures relies on complex arithmetic conditions that may not capture all practical cases, particularly when dealing with empirical distributions that don't follow simple uniform structures.
- The paper focuses on deterministic push-forward constraints, but real-world ML applications often involve stochastic or approximate mappings that could exhibit different convexity properties.

## Confidence

High Confidence:
- General nonconvexity of push-forward constraints for most probability measures
- Impossibility of convex losses for nonconvex constraint sets (Theorem 4.2)
- Convexity recovery through Kantorovich relaxation

Medium Confidence:
- Specific arithmetic conditions for convexity of empirical measures
- Practical implications for GAN and fairness applications

Low Confidence:
- Complete characterization of convexity for all discrete measure cases
- Generalizability of results to stochastic or approximate mappings

## Next Checks

1. Test convexity of T(P,Q) for two simple discrete measures (e.g., Dirac vs two-point uniform) to verify the general nonconvexity claim.

2. Implement Kantorovich relaxation for a small optimal transport problem and compare solutions to Monge's formulation to validate the convexity recovery approach.

3. Examine a practical GAN or fairness problem where push-forward constraints are used, and empirically verify the nonconvexity implications suggested by the theoretical results.