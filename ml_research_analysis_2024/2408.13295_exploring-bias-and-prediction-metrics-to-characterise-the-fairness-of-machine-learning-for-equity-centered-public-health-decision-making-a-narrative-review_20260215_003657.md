---
ver: rpa2
title: 'Exploring Bias and Prediction Metrics to Characterise the Fairness of Machine
  Learning for Equity-Centered Public Health Decision-Making: A Narrative Review'
arxiv_id: '2408.13295'
source_url: https://arxiv.org/abs/2408.13295
tags:
- health
- bias
- public
- fairness
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This review identified 72 studies describing types of algorithmic
  bias and quantitative fairness metrics relevant to public health ML applications.
  Key bias types include data quality issues (insufficient representation, historical
  bias), algorithmic processing errors (model specification, feature selection, weighting
  bias), cultural/social context gaps (misconceptions, socio-economic bias), human-AI
  interaction problems (automation, confirmation, complacency bias), feedback loops
  reinforcing existing biases, and temporal dynamics (model drift, non-stationary
  effects).
---

# Exploring Bias and Prediction Metrics to Characterise the Fairness of Machine Learning for Equity-Centered Public Health Decision-Making: A Narrative Review

## Quick Facts
- arXiv ID: 2408.13295
- Source URL: https://arxiv.org/abs/2408.13295
- Reference count: 40
- 72 studies describing types of algorithmic bias and quantitative fairness metrics relevant to public health ML applications

## Executive Summary
This narrative review systematically identifies and categorizes algorithmic bias sources and quantitative fairness metrics relevant to public health machine learning applications. The authors present a comprehensive framework covering seven main bias categories (data quality, algorithmic processing, cultural/social context, human-AI interaction, feedback loops, temporal dynamics, and equity/access) along with ten specific fairness metrics. The framework aims to provide public health practitioners with tools to evaluate and mitigate bias in ML systems, promoting equitable outcomes across diverse population subgroups. While the review offers extensive theoretical coverage, practical implementation guidance for resource-constrained settings remains limited.

## Method Summary
The authors conducted a comprehensive literature search across PubMed, MEDLINE, IEEE, ACM Digital Library, Science Direct, and Springer Nature for peer-reviewed articles published between 2008-2023. Using JBI quality appraisal standards, they systematically selected and categorized studies focusing on ML bias types and fairness metrics in public health contexts. The narrative review methodology involved systematic article selection, data extraction on bias categories and fairness metrics, quality assessment, and synthesis into a comprehensive framework for public health ML applications.

## Key Results
- Framework identifies 7 main categories of algorithmic bias with specific subcategories for public health ML applications
- Catalog of 10+ quantitative fairness metrics including disparate impact ratio, equalized odds, and calibration
- Comprehensive coverage of bias sources across the ML lifecycle from data collection through post-implementation
- Framework provides systematic approach for evaluating and mitigating bias in public health ML applications

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Systematic identification and categorization of algorithmic bias sources enables targeted mitigation strategies in public health ML.
- Mechanism: By organizing biases into distinct categories (data quality, algorithmic processing, cultural/social context, human-AI interaction, feedback loops, temporal dynamics, and equity/access), the framework provides a structured approach to identify where biases originate and how they propagate through ML systems.
- Core assumption: Different types of biases have distinct origins and require different mitigation approaches.
- Evidence anchors:
  - [abstract] "Key bias types include data quality issues (insufficient representation, historical bias), algorithmic processing errors (model specification, feature selection, weighting bias), cultural/social context gaps (misconceptions, socio-economic bias), human-AI interaction problems (automation, confirmation, complacency bias), feedback loops reinforcing existing biases, and temporal dynamics (model drift, non-stationary effects)."
  - [section] "Our study also highlights biases occur at all stages of the ML lifecycle, including data processing and collection, model development, evaluation, implementation, and post-implementation."
- Break condition: If biases are interdependent or cannot be cleanly categorized, the framework may oversimplify complex interactions between different bias sources.

### Mechanism 2
- Claim: Quantitative fairness metrics provide objective measures to evaluate and compare ML model performance across population subgroups.
- Mechanism: The review catalogs specific fairness metrics (disparate impact ratio, false positive/negative rates, group size ratio, equalized odds, balanced accuracy, predictive parity, calibration, background negative subgroup positive AUC) that allow researchers to quantify bias and assess whether ML models treat different groups equitably.
- Core assumption: Mathematical metrics can accurately capture fairness concepts and translate them into measurable quantities.
- Evidence anchors:
  - [abstract] "The review also cataloged fairness metrics including disparate impact ratio, false positive/negative rates, group size ratio, equalized odds, balanced accuracy, predictive parity, calibration, and background negative subgroup positive AUC."
  - [section] "Fairness metrics are quantitative measures designed to capture and evaluate the fairness or equity of a system or decision-making process."
- Break condition: If the chosen metrics do not align with the specific public health application or fail to capture important aspects of fairness relevant to that context.

### Mechanism 3
- Claim: The framework's comprehensive coverage of both bias types and fairness metrics enables systematic evaluation of ML fairness in public health applications.
- Mechanism: By providing both the sources of bias and the tools to measure them, the framework creates a complete evaluation pipeline where researchers can identify potential biases, measure their impact using appropriate metrics, and iteratively improve model fairness.
- Core assumption: Having both bias identification and measurement tools in one framework creates synergistic value beyond either component alone.
- Evidence anchors:
  - [abstract] "These findings provide a comprehensive framework for evaluating and mitigating bias in public health ML applications to promote equitable outcomes across diverse population subgroups."
  - [section] "Our findings will facilitate the use of these metrics to develop and evaluate of ML, thus promoting the equitable application of ML in public health."
- Break condition: If the framework becomes too complex to implement practically or if users cannot effectively match bias types to appropriate metrics.

## Foundational Learning

- Concept: Types of algorithmic bias in ML systems
  - Why needed here: Understanding different bias categories is essential for identifying where problems may arise in public health ML applications
  - Quick check question: What are the three main categories of bias discussed in the paper and how do they differ?

- Concept: Fairness metrics and their mathematical definitions
  - Why needed here: Quantitative metrics are needed to measure and compare model performance across different population groups
  - Quick check question: What is the difference between disparate impact ratio and equalized odds as fairness metrics?

- Concept: Public health equity and social determinants of health
  - Why needed here: Public health applications must consider how health disparities affect different population groups and how ML can either exacerbate or reduce these disparities
  - Quick check question: How do social determinants of health relate to the concept of "sensitive attributes" in ML fairness?

## Architecture Onboarding

- Component map: Literature search and screening -> Bias categorization system -> Fairness metric catalog -> Framework synthesis and validation -> Application guidance for public health contexts

- Critical path:
  1. Search and screen relevant literature
  2. Categorize identified biases into framework
  3. Catalog appropriate fairness metrics
  4. Validate framework coverage and completeness
  5. Provide application guidance

- Design tradeoffs:
  - Comprehensive coverage vs. practical usability - the framework includes many bias types and metrics, which may be overwhelming for practitioners
  - Academic rigor vs. practical applicability - the framework is based on extensive literature review but may need adaptation for real-world public health settings
  - Static framework vs. evolving field - ML fairness is rapidly evolving, so the framework may need regular updates

- Failure signatures:
  - Inability to map specific public health problems to appropriate bias categories
  - Confusion about which fairness metrics to use for different applications
  - Oversimplification of complex bias interactions
  - Lack of guidance for practical implementation in resource-constrained settings

- First 3 experiments:
  1. Test the framework on a specific public health ML application (e.g., disease prediction model) by mapping identified biases and selecting appropriate metrics
  2. Validate that the framework captures biases not previously considered by testing it on diverse public health scenarios
  3. Assess the practical usability by having public health practitioners apply the framework to their ML projects and provide feedback on clarity and completeness

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What specific quantitative fairness metrics should be recommended for different types of public health applications (surveillance, outbreak control, health promotion, resource allocation)?
- Basis in paper: [explicit] The paper states "While we provide a comprehensive list of fairness metrics, the use of particular metrics should depend on the specific public health applications of ML algorithms" and suggests "recommendations and frameworks to guide the appropriate fairness metrics suited for specific public health activities should be developed."
- Why unresolved: The paper catalogs fairness metrics but doesn't provide application-specific guidance on which metrics are most appropriate for different public health use cases.
- What evidence would resolve it: A systematic evaluation framework comparing the effectiveness and appropriateness of different fairness metrics across various public health ML applications, with empirical validation in real-world settings.

### Open Question 2
- Question: How can we effectively address and mitigate algorithmic bias in public health ML models when critical equity-relevant variables (race, income, gender) are unavailable at the individual level?
- Basis in paper: [explicit] The paper notes "the application of the fairness metrics in real-world practice may be limited by the unavailability of the key variables characterizing equity-deserving variables... in the population data" and mentions proxy measures may reduce algorithmic fairness.
- Why unresolved: The paper identifies this as a practical barrier but doesn't provide solutions for developing fair ML models without access to individual-level sensitive attributes.
- What evidence would resolve it: Development and validation of methods that can effectively use proxy variables or other indirect approaches to assess and improve fairness in ML models when individual-level sensitive attributes are unavailable.

### Open Question 3
- Question: What are the most effective strategies to break feedback loops that reinforce existing biases in public health ML systems?
- Basis in paper: [explicit] The paper discusses feedback loops where "outcomes may be shaped by pre-existing biases, which then get integrated into the system during subsequent training phases, amplifying and possibly worsening these biases."
- Why unresolved: While the paper identifies feedback loops as a significant source of bias, it doesn't provide specific strategies for detecting and breaking these loops in public health ML systems.
- What evidence would resolve it: Empirical studies demonstrating effective interventions to detect and break feedback loops in public health ML systems, with measurable improvements in model fairness over time.

## Limitations
- Limited practical implementation guidance for resource-constrained public health settings
- Potential publication bias favoring more documented bias types over emerging or context-specific issues
- Framework may require regular updates due to rapidly evolving ML fairness concepts

## Confidence
- High confidence: Systematic literature search across multiple databases with comprehensive categorization framework
- Medium confidence: Practical applicability due to limited implementation guidance for real-world public health settings

## Next Checks
1. Test framework completeness by applying it to diverse public health ML applications (e.g., disease prediction, resource allocation, health screening) to identify any missing bias categories or metrics

2. Conduct expert review with public health practitioners to evaluate the framework's practical usability, clarity of bias-to-metric mapping, and implementation guidance adequacy

3. Perform longitudinal validation by tracking how well the framework captures emerging bias types and fairness metrics in subsequent ML literature (2024-2025 publications)