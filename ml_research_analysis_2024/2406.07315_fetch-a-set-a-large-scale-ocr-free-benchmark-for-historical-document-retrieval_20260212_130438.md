---
ver: rpa2
title: 'Fetch-A-Set: A Large-Scale OCR-Free Benchmark for Historical Document Retrieval'
arxiv_id: '2406.07315'
source_url: https://arxiv.org/abs/2406.07315
tags:
- document
- historical
- retrieval
- documents
- systems
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Fetch-A-Set (FAS), a large-scale benchmark
  for evaluating historical document retrieval systems without relying on OCR. FAS
  contains 400K document fragments from 3 centuries of Spanish legislative documents,
  paired with natural language queries.
---

# Fetch-A-Set: A Large-Scale OCR-Free Benchmark for Historical Document Retrieval

## Quick Facts
- **arXiv ID**: 2406.07315
- **Source URL**: https://arxiv.org/abs/2406.07315
- **Reference count**: 40
- **Primary result**: Vision-based retrieval outperforms OCR-based methods on low-legibility historical documents

## Executive Summary
This paper introduces Fetch-A-Set (FAS), a large-scale benchmark for evaluating historical document retrieval systems without relying on OCR. FAS contains 400K document fragments from 3 centuries of Spanish legislative documents, paired with natural language queries. The benchmark addresses the challenges of text-to-image retrieval (topic spotting) and image-to-text extraction in historical contexts with varying legibility levels. Two baselines are presented: a vision-based approach using CLIP-pretrained ViT and an OCR-based approach using sentence-BERT. The vision-based method outperforms OCR-based retrieval on low-legibility documents, while text features excel when legibility is high. The dataset also reveals that visual representations encode temporal information useful for date estimation, suggesting hybrid approaches could enhance robustness. FAS provides a valuable resource for advancing research in historical document understanding beyond traditional word spotting.

## Method Summary
The FAS benchmark consists of 400K document fragments from Spanish legislative records spanning three centuries, each paired with natural language queries. The vision-based baseline uses a CLIP-pretrained ViT-B/32 encoder fine-tuned with triplet or contrastive loss to map document fragments to the query embedding space. The OCR-based baseline encodes queries and OCR transcriptions using sentence-BERT and computes cosine similarities for ranking. Both methods are evaluated using top-K accuracy (Acc@1, Acc@5, Acc@10) and average rank on retrieval tasks with 1024 distractor documents per query. The dataset construction involved manual selection of document regions and queries by domain experts, with legibility scores annotated for each fragment.

## Key Results
- Vision-based retrieval with CLIP-ViT outperforms OCR-based methods on low-legibility documents
- Visual features encode temporal information that aids in date estimation across historical periods
- Text features excel at retrieval when OCR is reliable (high legibility), while vision features excel when OCR fails
- Hybrid approaches combining vision and text features could enhance robustness by using each modality when it performs best

## Why This Works (Mechanism)

### Mechanism 1
Vision-based retrieval with CLIP-pretrained ViT outperforms OCR-based retrieval on low-legibility documents by directly encoding document fragments at the pixel level, bypassing OCR errors that degrade with document degradation. CLIP's visual embedding space is aligned with natural language queries, enabling semantic matching even when text is unreadable.

### Mechanism 2
Temporal bias in the dataset contributes to retrieval performance by encoding historical period information in visual features. The ViT encoder captures layout patterns and visual characteristics that correlate with specific historical periods, allowing it to match documents to queries about historical topics even without explicit date information.

### Mechanism 3
Hybrid approaches combining vision and text features could enhance robustness by using each modality when it performs best. Text features excel when OCR is reliable (high legibility), while vision features excel when OCR fails (low legibility). A system that can dynamically select or combine both would outperform either approach alone.

## Foundational Learning

- **Multimodal retrieval using contrastive learning**: Needed to match text queries to image fragments through aligned embedding spaces. Quick check: What loss function would you use to train a model that needs to match image fragments with their corresponding text descriptions while pushing apart unrelated pairs?

- **Document layout analysis and segmentation**: Required for identifying relevant regions within full-page documents without relying on OCR. Quick check: How would you segment a newspaper page into individual articles or sections without relying on OCR?

- **Temporal pattern recognition in visual data**: Important for understanding how visual features encode historical period information. Quick check: What visual features in historical documents might change systematically over centuries that a deep learning model could learn to recognize?

## Architecture Onboarding

- **Component map**: Document repository (400K fragments) -> Query encoder (text transformer) -> Visual encoder (CLIP-pretrained ViT) -> Matching system (distance computation) -> Evaluation framework (distractor sets)

- **Critical path**: 1) Load fragment image and corresponding query, 2) Encode both using their respective encoders, 3) Compute similarity score, 4) Rank against distractor set, 5) Evaluate using Acc@1, Acc@5, Acc@10, AvgRank

- **Design tradeoffs**: Vision vs text features (vision more robust to OCR errors but may miss fine-grained semantic details), model size vs performance (larger models capture more temporal patterns but increase computational cost), training data vs generalization (more diverse periods improve robustness but require more data)

- **Failure signatures**: Low performance on modern documents with clear text suggests text encoder issues; poor performance on all legibility levels suggests vision encoder or alignment problems; good performance on high-legibility but poor on low-legibility suggests missing vision component

- **First 3 experiments**: 1) Compare CLIP-ViT vs random baseline on full test set to establish baseline performance, 2) Plot accuracy vs legibility score to verify vision outperforms text on low-legibility documents, 3) Train simple linear classifier on visual embeddings to predict document dates and measure MAE to confirm temporal encoding

## Open Questions the Paper Calls Out

- Does temporal bias in visual representations persist across different document types beyond legislative records, and can it be reliably leveraged for retrieval tasks in other historical domains?

- How do hybrid systems that dynamically switch between text and vision features based on legibility compare to purely text- or vision-based systems in terms of overall retrieval accuracy and robustness?

- What is the impact of dataset temporal imbalance on model performance, and can synthetic data generation or reweighting strategies mitigate potential biases introduced by overrepresentation of certain historical periods?

## Limitations

- The benchmark's effectiveness depends on the assumption that visual features encode sufficient semantic content for retrieval, which may not hold for highly specialized queries requiring precise textual matching.

- The study focuses on Spanish legislative documents, limiting generalizability to other document types, languages, or cultural contexts.

- The temporal encoding observed in visual features could be an artifact of the specific document collection rather than a generalizable property of historical documents.

## Confidence

- **High confidence** in the retrieval performance results and the comparative advantage of vision-based methods for low-legibility documents
- **Medium confidence** in the temporal encoding claims, as the paper provides evidence but doesn't establish causation
- **Low confidence** in the generalizability of findings beyond Spanish legislative documents and the 17th-20th century timeframe

## Next Checks

1. **Cross-dataset validation**: Test the vision-based retrieval method on a different historical document collection (e.g., English legal documents or newspapers) to assess generalizability beyond Spanish legislative records.

2. **Controlled legibility study**: Systematically vary OCR quality through synthetic degradation and measure the exact transition point where vision-based retrieval outperforms text-based retrieval.

3. **Temporal feature ablation**: Train vision models on temporally shuffled document fragments and compare retrieval performance to the original setup to determine whether temporal encoding provides genuine advantages beyond random correlations.