---
ver: rpa2
title: Optimization Solution Functions as Deterministic Policies for Offline Reinforcement
  Learning
arxiv_id: '2408.15368'
source_url: https://arxiv.org/abs/2408.15368
tags:
- function
- policy
- actor
- optimization
- where
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes an implicit actor-critic (iAC) framework for
  offline reinforcement learning that uses optimization solution functions as policies
  and monotone functions over optimal optimization values as critics. The key insight
  is that encoding "optimality" in the actor policy via optimization solutions provides
  robustness to parameter estimation errors through the exponentially decaying sensitivity
  (EDS) property.
---

# Optimization Solution Functions as Deterministic Policies for Offline Reinforcement Learning

## Quick Facts
- arXiv ID: 2408.15368
- Source URL: https://arxiv.org/abs/2408.15368
- Authors: Vanshaj Khattar; Ming Jin
- Reference count: 40
- One-line primary result: iAC significantly outperforms state-of-the-art offline RL baselines on building energy management and supply chain problems by leveraging optimization solution functions as policies and combining with relative pessimism for robustness.

## Executive Summary
This paper proposes an implicit actor-critic (iAC) framework for offline reinforcement learning that uses optimization solution functions as policies and monotone functions over optimal optimization values as critics. The key insight is that encoding "optimality" in the actor policy via optimization solutions provides robustness to parameter estimation errors through the exponentially decaying sensitivity (EDS) property. This property ensures that the effect of parameter mismatch on policy performance decays exponentially with time, unlike general function approximation schemes. The authors combine iAC with relative pessimism to expand the region of robustness and provide theoretical performance guarantees. Experiments on building energy management (CityLearn challenge) and supply chain management show that iAC significantly outperforms state-of-the-art offline RL baselines, achieving lower costs and higher profits.

## Method Summary
The implicit actor-critic framework learns parameters of an optimization model that implicitly defines both the actor policy (solution function) and critic value function (monotone function over optimal optimization value). The actor uses a planning horizon T (24 for CityLearn, 20 for supply chain) with constraints designed by domain experts. The critic employs a monotonic reward warping function. The method is trained offline using trajectory datasets collected with behavior policies (rule-based control for CityLearn, default optimization model for supply chain). Relative pessimism is combined with iAC to ensure robust policy improvement over behavior policies by formulating offline RL as a Stackelberg game.

## Key Results
- iAC achieves lower costs (total, peak demand, ramping) than state-of-the-art offline RL baselines on the CityLearn building energy management challenge
- iAC maximizes total profit more effectively than baselines on the supply chain management problem
- The exponentially decaying sensitivity property provides robustness to parameter estimation errors, ensuring policy performance remains close to optimal even with suboptimal parameters
- Combining iAC with relative pessimism expands the region of robustness to value function estimation errors

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** The exponentially decaying sensitivity (EDS) property ensures that the policy mismatch due to parameter estimation errors decays exponentially with the time horizon, providing robustness to learned policy parameters.
- **Mechanism:** The EDS property bounds the sensitivity of the solution of an MPC problem at time step t' against a parameter perturbation at time step t by Hλ^|t-t'|, where λ ∈ (0,1). This means that even if the learned actor parameters θ are suboptimal, the resulting policy π_θ(s) remains close to the optimal policy π_θ*(s) for later time steps, because the sensitivity decays exponentially with the time difference.
- **Core assumption:** The actor optimization satisfies LICQ and SSOSC at the optimal parameter θ*, ensuring the EDS property holds in a neighborhood around θ*.
- **Evidence anchors:**
  - [abstract]: "By encoding 'optimality' in the actor policy, we show that the learned policies are robust to the suboptimality of the learned actor parameters via the exponentially decaying sensitivity (EDS) property."
  - [section II.B]: "EDS property states that the sensitivity of the solution to an MPC ... at some time step t' against a parameter perturbation at time step t decays exponentially with |t - t'|"
  - [corpus]: No direct evidence. The corpus papers focus on other aspects of offline RL like metric learning, diffusion policies, and actor-critic methods, but do not discuss the EDS property specifically.
- **Break condition:** The EDS property only holds in a local region around the optimal parameters. If the learned parameters are too far from optimal, the EDS property may not apply, and the policy robustness may break down.

### Mechanism 2
- **Claim:** Combining iAC with relative pessimism (RP) expands the region of robustness to value function estimation errors, ensuring robust policy improvement over behavior policies.
- **Mechanism:** RP formulates offline RL as a Stackelberg game where the actor maximizes the relative pessimistic policy evaluation of the critic with respect to the behavior policy distribution. This pessimistic evaluation accounts for potential overestimation of value functions due to limited data coverage. By combining RP with iAC, the region where the EDS property holds is expanded, allowing for robust performance even when the learned parameters are not exactly optimal.
- **Core assumption:** The behavior policy distribution µ well-covers the distribution of interest ν* with respect to the function class F and any π_θ(k).
- **Evidence anchors:**
  - [section III.C]: "As the EDS conditions only hold around a neighborhood of θ*, we could only have its benefits inside a local region. Therefore, we expand the region of robustness to overestimation of the value functions by combining iAC with RP"
  - [section IV.A]: "RP formulates offline RL as a Stackelberg game with the actor as the leader and the critic as the follower"
  - [corpus]: No direct evidence. The corpus papers do not discuss the combination of EDS with relative pessimism.
- **Break condition:** If the behavior policy distribution µ does not adequately cover the distribution of interest ν*, the pessimistic evaluation may be too conservative, leading to suboptimal policies.

### Mechanism 3
- **Claim:** The implicit actor-critic framework learns the parameters of the optimization model, implicitly determining both the model and policy, which is more effective than learning a separate model and policy.
- **Mechanism:** In iAC, the actor policy comes from the solution of an optimization problem, and the critic evaluates this policy. The parameters of the optimization model are learned to shape the solution function optimally. This implicit approach allows the actor and critic to be learned jointly, with the optimization model providing a structured way to represent the policy.
- **Core assumption:** The optimization problem used for the actor is well-designed to capture the task's objectives and constraints.
- **Evidence anchors:**
  - [section I]: "To leverage the synergistic strength of both optimization-based and pessimistic offline RL methods, we propose an implicit actor-critic (iAC) framework that employs optimization solution functions as a policy for the actor and a monotone function over the optimal value of optimization as a critic."
  - [section III.A]: "We propose an actor-critic framework [5] for offline RL where we consider that the actor's policy comes from the solution of (1), which we refer to as actor optimization."
  - [corpus]: No direct evidence. The corpus papers do not discuss implicit actor-critic frameworks that use optimization solution functions as policies.
- **Break condition:** If the optimization problem is not well-designed or the optimization solver fails to find a good solution, the implicit actor-critic approach may not work effectively.

## Foundational Learning

- **Concept:** Markov Decision Processes (MDPs)
  - **Why needed here:** The paper is about offline reinforcement learning, which is a method for learning optimal policies in MDPs from historical data.
  - **Quick check question:** What are the components of an MDP, and how do they relate to the reinforcement learning problem?

- **Concept:** Bellman Equations and Dynamic Programming
  - **Why needed here:** The critic in iAC is based on the optimal value function, which satisfies the Bellman equation. Understanding Bellman equations is crucial for grasping how the critic evaluates the actor's policy.
  - **Quick check question:** What is the Bellman equation for the optimal value function, and how is it used in dynamic programming to find optimal policies?

- **Concept:** Convex Optimization and Constraint Qualification
  - **Why needed here:** The actor policy is derived from the solution of a convex optimization problem. Understanding convex optimization and constraint qualification conditions like LICQ is essential for understanding the EDS property and the assumptions made in the paper.
  - **Quick check question:** What is a convex optimization problem, and what are some common constraint qualification conditions like LICQ and SSOSC?

## Architecture Onboarding

- **Component map:** Actor optimization -> Actor policy (solution function) -> Critic evaluation -> Parameter updates

- **Critical path:**
  1. Collect offline data using a behavior policy
  2. Initialize actor and critic parameters
  3. For each iteration:
     a. Update critic parameters to minimize the pessimistic Bellman error
     b. Update actor parameters to maximize the pessimistic policy evaluation
  4. Use the learned actor policy for decision-making

- **Design tradeoffs:**
  - Planning horizon T: Longer horizons may provide better performance but increase computational complexity
  - Number of constraints in the optimization model: More constraints can improve approximation power but may lead to more critical regions
  - Monotone function for the critic: Different monotone functions may lead to different approximation properties

- **Failure signatures:**
  - EDS property not holding: Learned parameters are too far from optimal, leading to policy mismatch
  - Insufficient data coverage: Behavior policy does not adequately cover the state-action space, leading to value overestimation
  - Poor optimization model design: Optimization problem does not capture the task's objectives and constraints effectively

- **First 3 experiments:**
  1. Test the iAC framework on a simple benchmark problem with known optimal policy to verify the EDS property and policy robustness.
  2. Compare the performance of iAC with and without relative pessimism on a problem with limited data coverage to assess the benefit of RP.
  3. Vary the planning horizon T and the number of constraints in the optimization model to understand their impact on the iAC's performance and computational complexity.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of iAC compare to online RL methods when a small amount of environment interaction is allowed?
- Basis in paper: [inferred] The paper focuses on offline RL and does not compare iAC to online RL methods, even when limited interaction is possible.
- Why unresolved: The paper's scope is limited to offline RL, and the authors do not explore the potential benefits of combining iAC with limited online interaction.
- What evidence would resolve it: Experiments comparing iAC to online RL methods (e.g., PPO, SAC) in settings where a small amount of environment interaction is allowed would provide insights into the potential benefits of combining offline and online learning.

### Open Question 2
- Question: What is the impact of the choice of the surrogate reward function and constraints in the actor optimization model on the final performance of iAC?
- Basis in paper: [explicit] The paper mentions that the surrogate reward function and constraints are designed by a domain expert, but does not explore the sensitivity of iAC's performance to these design choices.
- Why unresolved: The paper assumes that the surrogate reward and constraints are given and does not investigate how different choices might affect the learned policy and its performance.
- What evidence would resolve it: Experiments varying the surrogate reward function and constraints in the actor optimization model, and analyzing the resulting performance of iAC, would provide insights into the importance of these design choices.

### Open Question 3
- Question: How does the performance of iAC scale with the size of the dataset and the complexity of the MDP?
- Basis in paper: [inferred] The paper provides theoretical guarantees and experimental results, but does not systematically explore how iAC's performance scales with dataset size and MDP complexity.
- Why unresolved: The paper's theoretical analysis and experiments focus on specific settings, and the authors do not investigate the general scalability of iAC.
- What evidence would resolve it: Experiments varying the size of the dataset and the complexity of the MDP (e.g., number of states, actions, and planning horizon), and analyzing the resulting performance of iAC, would provide insights into its scalability.

## Limitations
- The exact impact of the optimization model design (e.g., number of constraints, planning horizon) on the iAC's performance and computational complexity is not thoroughly explored.
- The empirical validation of the exponentially decaying sensitivity (EDS) property and its combination with relative pessimism is limited.
- The paper does not investigate how iAC's performance scales with dataset size and MDP complexity.

## Confidence
- **High Confidence:** The general framework of using optimization solution functions as policies and monotone functions over optimal optimization values as critics is well-defined and implemented. The experimental results on the CityLearn and supply chain problems demonstrate the effectiveness of the iAC framework compared to state-of-the-art offline RL baselines.
- **Medium Confidence:** The theoretical claims about the EDS property and the combination of iAC with relative pessimism are sound based on the provided proofs and explanations. However, the empirical validation of these claims is limited.
- **Low Confidence:** The exact impact of the optimization model design (e.g., number of constraints, planning horizon) on the iAC's performance and computational complexity is not thoroughly explored.

## Next Checks
1. Conduct ablation studies to isolate the effect of the EDS property on policy robustness by comparing iAC with and without the optimization-based actor.
2. Perform empirical validation of the exponential decay in policy sensitivity with respect to parameter estimation errors, as claimed by the EDS property.
3. Investigate the impact of the optimization model design (e.g., number of constraints, planning horizon) on the iAC's performance and computational complexity through systematic experiments.