---
ver: rpa2
title: Causal Micro-Narratives
arxiv_id: '2410.05252'
source_url: https://arxiv.org/abs/2410.05252
tags:
- inflation
- narratives
- narrative
- data
- causal
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces causal micro-narratives, defined as sentence-level
  explanations of causes and/or effects of a target subject. The authors develop a
  classification task to extract these narratives from text, focusing on inflation
  as a case study.
---

# Causal Micro-Narratives

## Quick Facts
- arXiv ID: 2410.05252
- Source URL: https://arxiv.org/abs/2410.05252
- Reference count: 40
- Primary result: Introduces causal micro-narratives as sentence-level cause/effect explanations and achieves F1 scores of 0.87 for detection and 0.71 for classification

## Executive Summary
This paper introduces causal micro-narratives as sentence-level explanations that identify causes and/or effects of a target subject. The authors develop a classification task to extract these narratives from text, using inflation as a case study. They create a dataset of human-annotated inflation-related narratives from historical and contemporary US news articles, covering 8 causes and 11 effects. Several large language models are evaluated on this multi-label classification task, with a fine-tuned Llama 3.1 8B model achieving the best performance. The error analysis reveals that model errors often mirror human annotator disagreements, highlighting the inherent complexity of the task.

## Method Summary
The authors operationalize causal micro-narratives as sentence-level explanations of causes and/or effects of a target subject. They develop a classification task where models must first detect whether a sentence contains a causal micro-narrative about the target subject, then classify which specific causes and effects are present. The methodology involves creating a dataset of human-annotated inflation-related narratives from news articles, establishing clear annotation guidelines for identifying cause-effect relationships, and evaluating multiple large language models on this task. A fine-tuned Llama 3.1 8B model was found to perform best on the classification task.

## Key Results
- Fine-tuned Llama 3.1 8B model achieves F1 scores of 0.87 for narrative detection and 0.71 for narrative classification
- Multi-label classification task requires models to identify both presence of causal narratives and specific causes/effects
- Model errors often mirror human annotator disagreements, indicating inherent task complexity
- Dataset covers 8 causes and 11 effects of inflation from historical and contemporary US news sources

## Why This Works (Mechanism)
The approach works by breaking down complex causal explanations into discrete, sentence-level units that can be systematically classified. By focusing on micro-narratives rather than full articles, the method captures granular causal relationships that might be obscured in broader analysis. The multi-label classification framework allows for the recognition that real-world causal explanations often involve multiple simultaneous causes and effects, making it more realistic than binary classification approaches.

## Foundational Learning
- Causal inference basics: Understanding cause-effect relationships is fundamental to identifying micro-narratives - quick check: Can identify explicit cause-effect statements in text
- Multi-label classification: Required because narratives can have multiple causes and effects simultaneously - quick check: Can assign multiple labels to a single instance
- Annotation guidelines development: Critical for ensuring consistent human labels used for training - quick check: Can distinguish between causes, effects, and unrelated information
- Evaluation metrics for multi-label tasks: F1 scores must account for multiple possible labels - quick check: Understands precision/recall tradeoffs in multi-label settings
- Error analysis methodology: Comparing model errors to human disagreements reveals task difficulty - quick check: Can identify patterns in classification mistakes

## Architecture Onboarding

Component Map:
Data Collection -> Annotation Guidelines -> Human Annotation -> Model Training -> Evaluation -> Error Analysis

Critical Path:
The critical path flows from data collection through human annotation to model training and evaluation. Each step depends on the quality of the previous one - poor annotations lead to poor model performance, and inadequate evaluation prevents understanding of model capabilities.

Design Tradeoffs:
The authors chose a multi-label classification approach over sequence labeling or generation tasks. This tradeoff favors precision and interpretability over capturing more complex narrative structures. They also chose to focus on a single domain (inflation) rather than multiple topics, trading generalizability for depth of analysis in a specific area.

Failure Signatures:
Model failures manifest as both false positives (classifying non-narrative sentences as containing causal micro-narratives) and false negatives (missing actual causal narratives). The error analysis reveals that many failures occur in sentences with ambiguous or implicit causal relationships, suggesting the task's inherent difficulty rather than purely technical limitations.

Three First Experiments:
1. Evaluate model performance on sentences with explicit vs. implicit causal relationships to understand where the model struggles
2. Test model generalization by evaluating on a different economic topic (e.g., unemployment) using the same framework
3. Conduct ablation study removing different numbers of cause/effect categories to understand which are most challenging for the model

## Open Questions the Paper Calls Out
None

## Limitations
- Evaluation is constrained to a single domain (US inflation-related news), raising questions about generalizability to other topics
- F1 score of 0.71 for narrative classification indicates substantial room for improvement, particularly given the multi-label nature of the task
- Reliance on human annotations introduces inherent subjectivity, as evidenced by model errors mirroring human annotator disagreements

## Confidence
- High confidence: The methodology for defining and operationalizing causal micro-narratives is sound and clearly articulated
- Medium confidence: The classification performance metrics are reliable within the specific inflation news domain tested
- Medium confidence: The observation that model errors mirror human disagreements is valid but requires broader validation

## Next Checks
1. Test the classification framework on a different economic topic (e.g., unemployment or GDP growth) to assess domain transferability
2. Conduct inter-annotator agreement studies with expanded annotator pools to better characterize the inherent ambiguity in narrative classification
3. Evaluate model performance on out-of-distribution text (e.g., academic papers or social media) to test robustness across text genres