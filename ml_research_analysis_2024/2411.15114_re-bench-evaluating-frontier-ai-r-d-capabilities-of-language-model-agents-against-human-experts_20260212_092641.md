---
ver: rpa2
title: 'RE-Bench: Evaluating frontier AI R&D capabilities of language model agents
  against human experts'
arxiv_id: '2411.15114'
source_url: https://arxiv.org/abs/2411.15114
tags:
- agent
- score
- agents
- solution
- human
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: RE-Bench is a new benchmark for evaluating AI agents on realistic
  ML research engineering tasks, featuring 7 hand-crafted environments with human
  expert baselines (71 runs, 61 experts). It enables direct comparison between human
  and AI agent performance under identical conditions.
---

# RE-Bench: Evaluating frontier AI R&D capabilities of language model agents against human experts

## Quick Facts
- arXiv ID: 2411.15114
- Source URL: https://arxiv.org/abs/2411.15114
- Reference count: 40
- Agents outperform humans in 2-hour time budgets but humans surpass agents in 8-hour budgets

## Executive Summary
RE-Bench is a new benchmark suite for evaluating AI agents on ML research engineering tasks, featuring 7 hand-crafted environments with human expert baselines. The benchmark enables direct comparison between human and AI agent performance under identical conditions. Tested models (o1-preview, Claude 3.5 Sonnet) in two scaffolds show that agents outperform humans in 2-hour time budgets but humans surpass agents in 8-hour budgets. Agents are over 10× faster and cheaper than humans per unit time, with some finding novel solutions exceeding all human attempts.

## Method Summary
RE-Bench consists of 7 ML research engineering environments where agents must improve starting solutions using GPU resources and scoring functions. Human experts complete tasks in 8-hour sessions under identical conditions to AI agents. AI agents are tested using two scaffolds (Modular and AIDE) with varying time budgets and best-of-k sampling. Performance is measured through normalized scores (0-2) based on improvement over starting solutions relative to reference solutions. The benchmark tracks score progression, token usage, and comparison between best-of-k results from humans and agents.

## Key Results
- Agents submit solutions over 10× faster than human experts (36.8 vs 3.4 runs per hour)
- Humans achieve higher scores than agents given 8-hour time budgets
- Some agents found novel solutions faster than any human expert attempts
- Agents are significantly cheaper than humans per unit time of research

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Human experts improve more with time than AI agents
- Mechanism: Human experts develop and refine their approach over time, while AI agents get stuck in local optima and fail to recover from errors
- Core assumption: AI agents cannot effectively learn from new information or build on previous work
- Evidence anchors:
  - [abstract]: "humans currently display better returns to increasing time budgets, narrowly exceeding the top AI agent scores given an 8-hour budget"
  - [section]: "AI agents often make stubborn and incorrect assumptions, and do not notice or correctly interpret contradictory information"
  - [corpus]: Weak - no direct corpus evidence about learning over time
- Break condition: If agents develop better context management and error recovery capabilities

### Mechanism 2
- Claim: AI agents can try many more solutions than humans due to faster evaluation
- Mechanism: AI agents run scoring functions 36.8 times per hour vs 3.4 times for humans, enabling them to find highly optimized local solutions
- Core assumption: Fast evaluation feedback allows agents to explore solution space more thoroughly
- Evidence anchors:
  - [abstract]: "agents submit new solutions over ten times faster than human experts"
  - [section]: "On average, AIDE and modular agents run score 36.8 and 25.3 times per hour respectively, while human experts only do so 3.4 times"
  - [corpus]: Weak - no corpus evidence about solution exploration speed
- Break condition: If evaluation cost increases or agent generation speed decreases

### Mechanism 3
- Claim: AI agents have broader domain knowledge than human experts
- Mechanism: AI agents can access and apply knowledge across many ML subfields, while human experts may lack specific expertise
- Core assumption: AI agents have been trained on diverse ML literature and can retrieve relevant information
- Evidence anchors:
  - [abstract]: "an agent wrote a faster custom Triton kernel than any of our human experts"
  - [section]: "This was a surprising result, though it may be partially attributed to an expertise gap between the AI agent and many of the human experts"
  - [corpus]: Weak - no corpus evidence about knowledge breadth
- Break condition: If human experts have equivalent access to ML knowledge resources

## Foundational Learning

- Concept: Best-of-k evaluation methodology
  - Why needed here: To fairly compare human and AI performance across different time budgets
  - Quick check question: Why do we use best-of-k instead of average scores when comparing humans and AI agents?

- Concept: Normalization of scores across different environments
  - Why needed here: To make scores from different tasks with different metrics comparable
  - Quick check question: How is the normalized score calculated and what does it represent?

- Concept: Time budget vs token budget trade-offs
  - Why needed here: To understand how wall-clock time limitations affect agent performance
  - Quick check question: Why did the authors choose wall-clock time limits instead of token limits for evaluation?

## Architecture Onboarding

- Component map: RE-Bench environments (7 tasks) -> Human expert evaluation system -> AI agent scaffolding (Modular and AIDE) -> Scoring and normalization system -> Analysis and comparison framework

- Critical path: 1. Set up environment with hardware resources 2. Run agent with time budget 3. Score and log results 4. Normalize scores across environments 5. Compare best-of-k results between humans and agents

- Design tradeoffs:
  - Short time horizon (8 hours) vs. real-world research duration (months)
  - Well-defined tasks vs. ambiguous real research problems
  - Automated scoring vs. human judgment
  - Limited scope vs. comprehensive coverage

- Failure signatures:
  - Agent gets stuck in local optima
  - Agent fails to recover from errors
  - Agent misunderstands task instructions
  - Agent overfits to specific evaluation metrics

- First 3 experiments:
  1. Run both agent scaffolds on a single environment with 30-minute time budget
  2. Compare agent performance to human expert baseline on same task
  3. Test agent's ability to recover from simulated errors or memory issues

## Open Questions the Paper Calls Out
None

## Limitations
- Benchmark focuses on well-defined engineering tasks rather than open-ended research problems
- 8-hour time horizon may not reflect real-world research durations (months)
- Limited scope may not capture all aspects of AI R&D automation
- Agent performance could be overfitted to specific evaluation conditions

## Confidence
- **High**: AI agents are faster and cheaper than humans per unit time
- **Medium**: Human experts improve more with time than AI agents
- **Medium**: AI agents can explore solution space more thoroughly due to faster evaluation
- **Low**: AI agents have broader domain knowledge than human experts

## Next Checks
1. Test agent performance on open-ended research problems without clear success metrics
2. Evaluate long-term research coordination across multiple parallel projects
3. Assess generalization to tasks outside the ML research engineering domain