---
ver: rpa2
title: Feedback Reciprocal Graph Collaborative Filtering
arxiv_id: '2408.02404'
source_url: https://arxiv.org/abs/2408.02404
tags:
- graph
- feedback
- items
- frgcf
- recommendation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of graph collaborative filtering
  (GCF) models recommending unfascinating items due to not distinguishing between
  fascinated and unfascinated user interactions. To solve this, the authors propose
  Feedback Reciprocal Graph Collaborative Filtering (FRGCF), which partitions the
  interaction graph into Interacted & Fascinated (I&F) and Interacted & Unfascinated
  (I&U) subgraphs based on user feedback.
---

# Feedback Reciprocal Graph Collaborative Filtering

## Quick Facts
- arXiv ID: 2408.02404
- Source URL: https://arxiv.org/abs/2408.02404
- Authors: Weijun Chen; Yuanchen Bei; Qijie Shen; Hao Chen; Xiao Huang; Feiran Huang
- Reference count: 40
- Key outcome: FRGCF outperforms state-of-the-art methods on four benchmark datasets and a billion-scale industrial dataset, achieving up to 7.82% improvement in NDCG

## Executive Summary
This paper addresses the problem of graph collaborative filtering (GCF) models recommending unfascinating items due to not distinguishing between fascinated and unfascinated user interactions. To solve this, the authors propose Feedback Reciprocal Graph Collaborative Filtering (FRGCF), which partitions the interaction graph into Interacted & Fascinated (I&F) and Interacted & Unfascinated (I&U) subgraphs based on user feedback. FRGCF then applies separate collaborative filtering on these subgraphs, connected by feedback-reciprocal contrastive learning and macro-level feedback modeling. This approach allows the I&F recommender to learn from the I&U graph without being misdirected by it.

## Method Summary
FRGCF first partitions the entire interaction graph into the Interacted & Fascinated (I&F) graph and the Interacted & Unfascinated (I&U) graph based on user feedback. It then applies separate collaborative filtering on each subgraph using LightGCN-style message passing. The two models are connected through feedback-reciprocal contrastive learning that aligns corresponding user/item embeddings across both perspectives without directly aggregating I&U information into the I&F model. Additionally, macro-level feedback modeling compensates for data incompleteness by aggregating information at cluster level through K-means clustering and propagating this back to individual nodes. The model is trained using pairwise BPR loss with distance regularization to prevent feature decoupling.

## Key Results
- FRGCF outperforms state-of-the-art methods on four benchmark datasets (MovieLens, Douban, Beauty, KuaiRec) and a billion-scale industrial dataset from Taobao
- Achieves up to 7.82% improvement in NDCG@20 compared to LightGCN
- Online A/B tests on Taobao's recommender system show increased user engagement and GMV while maintaining efficiency
- Ablation studies demonstrate the importance of both feedback-reciprocal contrastive learning and macro-level feedback modeling components

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Partitioning the interaction graph into I&F and I&U subgraphs allows the model to separately learn representations for fascinated and unfascinated interactions without mixing their signals.
- Mechanism: The model creates two separate adjacency matrices from user feedback partitions, then runs independent graph convolution layers on each. This isolates the aggregation of information from fascinated neighbors versus unfascinated neighbors.
- Core assumption: User feedback can be cleanly separated into fascinated and unfascinated categories, and these categories have distinct enough characteristics to benefit from separate modeling.
- Evidence anchors: [abstract] "first partitions the entire interaction graph into the Interacted & Fascinated (I&F) graph and the Interacted & Unfascinated (I&U) graph based on the user feedback" and [section] "Gğ¼ğ¹ = {U, I, Eğ¼ğ¹}, Gğ¼ğ‘ˆ = {U, I, Eğ¼ğ‘ˆ}, (1)" and "ğ‘¨ğ¼ğ¹ = [0 ğ‘¹ğ¼ğ¹; ğ‘¹ğ‘‡ğ¼ğ¹ 0], ğ‘¨ğ¼ğ‘ˆ = [0 ğ‘¹ğ¼ğ‘ˆ; ğ‘¹ğ‘‡ğ¼ğ‘ˆ 0]. (2)"
- Break condition: If user feedback is noisy or ambiguous, the partitioning may create artificial boundaries that harm representation quality.

### Mechanism 2
- Claim: Feedback-reciprocal contrastive learning enables information exchange between I&F and I&U representations while preventing the I&F model from being misdirected by negative signals.
- Mechanism: The model defines an "impulsiveness" matrix that captures shared latent relationships between the two graphs, then uses contrastive loss to align corresponding user/item embeddings across perspectives without directly aggregating I&U information into the I&F model.
- Core assumption: There exists shared latent structure across fascinated and unfascinated interactions that can be exploited for mutual benefit without transferring harmful signals.
- Evidence anchors: [abstract] "introduces separate collaborative filtering on the I&F graph and the I&U graph with feedback-reciprocal contrastive learning" and [section] "ğ‘°ğ’ğ’‘ = {ğ‘–ğ‘šğ‘ |ğ‘–ğ‘šğ‘ = ğ¼&ğ¹ âˆ© ğ¼&ğ‘ˆ}. (7)" and "Lğ‘“ğ‘Ÿğ‘ğ‘™ = Lğ‘¢ğ‘ ğ‘’ğ‘Ÿğ‘“ğ‘Ÿğ‘ğ‘™ + Lğ‘–ğ‘¡ğ‘’ğ‘šğ‘“ğ‘Ÿğ‘ğ‘™, (16)"
- Break condition: If the shared latent structure is weak or non-existent, contrastive learning may introduce noise rather than useful signals.

### Mechanism 3
- Claim: Macro-level feedback modeling compensates for data incompleteness caused by graph partitioning by aggregating information at a cluster level rather than individual node level.
- Mechanism: The model applies K-means clustering to embeddings, computes macro-centroid representations, then aggregates these back to nodes using directional cues from GNN layer differences, with contrastive loss ensuring quality.
- Core assumption: Information aggregated at the cluster/centroid level can capture macro-level interaction patterns that are lost when partitioning the graph, and these patterns can be effectively propagated back to individual nodes.
- Evidence anchors: [abstract] "macro-level feedback modeling" and "enables the I&F graph recommender to learn multi-grained interaction characteristics from the I&U graph" and [section] "ğ’ğ‘¥,ğ‘¢ = ğ‘“ğ‘šğ‘ğ‘ğ‘Ÿğ‘œ(ğ‘¬(0)ğ‘¥,ğ‘¢), ğ’ğ‘¥,ğ‘– = ğ‘“ğ‘šğ‘ğ‘ğ‘Ÿğ‘œ(ğ‘¬(0)ğ‘¥,ğ‘–), (17)" and "Lğ‘¥ğ‘šğ‘ğ‘ğ‘Ÿğ‘œ = InfoNCE(ğ‘¬(ğ¾)ğ‘¥,ğ‘¢, ğ‘¬ğ‘šğ‘ğ‘ğ‘Ÿğ‘œğ‘¥,ğ‘¢) + ğœ‡Â·InfoNCE(ğ‘¬(ğ¾)ğ‘¥,ğ‘–, ğ‘¬ğ‘šğ‘ğ‘ğ‘Ÿğ‘œğ‘¥,ğ‘–), (21)"
- Break condition: If clustering creates poor representations, the macro-level information may be misleading.

## Foundational Learning

- Concept: Graph Neural Networks and message passing
  - Why needed here: FRGCF extends GNNs by running them separately on partitioned graphs and connecting them through contrastive learning
  - Quick check question: What is the difference between the message passing in standard GNNs and the separate GNN layers used for I&F and I&U graphs in FRGCF?

- Concept: Contrastive learning and mutual information maximization
  - Why needed here: The feedback-reciprocal contrastive learning component relies on maximizing agreement between corresponding embeddings across I&F and I&U graphs
  - Quick check question: How does the contrastive loss in FRGCF (Eqs. 14-16) differ from standard contrastive learning in computer vision?

- Concept: User feedback modeling in recommendation systems
  - Why needed here: The core innovation relies on distinguishing between fascinated and unfascinated interactions based on user feedback signals
  - Quick check question: What are the advantages and limitations of using explicit feedback (ratings) versus implicit feedback (dwell time, clicks) for partitioning interactions?

## Architecture Onboarding

- Component map: Input â†’ Graph partition (I&F/I&U) â†’ Separate GNN layers (I&F GNN, I&U GNN) â†’ Feedback-reciprocal contrastive learning â†’ Macro-level feedback modeling â†’ Distance regularization â†’ BPR loss â†’ Inference (I&F GNN only)
- Critical path: Graph partition â†’ Separate GNN training â†’ Contrastive connection â†’ Macro modeling â†’ Final recommendation
- Design tradeoffs: Separate modeling of I&F/I&U vs. unified modeling; complex contrastive connections vs. simpler architectures; macro-level aggregation vs. direct node-level modeling
- Failure signatures: Performance drops when I&U interactions are too dominant; contrastive learning fails to converge; macro clustering produces poor representations; distance regularization creates over-decoupling
- First 3 experiments:
  1. Train FRGCF on a small dataset (e.g., MovieLens 100k) and verify that the I&F and I&U graphs have different interaction distributions
  2. Compare recommendation performance with and without the feedback-reciprocal contrastive learning component
  3. Test the effect of different numbers of clusters in the macro-level feedback modeling on recommendation quality

## Open Questions the Paper Calls Out

The paper doesn't explicitly call out open questions, but several important research directions emerge from the work:

### Open Question 1
- Question: How does the feedback-reciprocal contrastive learning component scale to extremely large interaction graphs with billions of nodes and edges, and what are the computational bottlenecks?
- Basis in paper: [inferred] The paper mentions testing on a billion-scale industrial dataset and achieving good results, but doesn't discuss scalability challenges or computational efficiency of the contrastive learning component in detail.
- Why unresolved: The paper focuses on effectiveness rather than scalability analysis. Large-scale implementation details and runtime performance are not provided.
- What evidence would resolve it: Detailed complexity analysis, runtime benchmarks, and memory usage statistics for the contrastive learning component on graphs of varying sizes.

### Open Question 2
- Question: What is the optimal way to define and parameterize the "fascinated" vs "unfascinated" feedback threshold, and how sensitive is FRGCF's performance to this choice?
- Basis in paper: [explicit] The paper mentions different threshold values for different datasets but doesn't explore sensitivity analysis or provide guidance on threshold selection.
- Why unresolved: The threshold choice appears arbitrary and dataset-specific. The paper doesn't investigate how performance varies with different threshold values or provide a principled method for threshold selection.
- What evidence would resolve it: Systematic experiments varying threshold values, analysis of performance sensitivity to threshold choice, and guidelines for threshold selection across different domains.

### Open Question 3
- Question: How does FRGCF handle cold-start users and items, particularly when there's insufficient feedback data to reliably partition the interaction graph?
- Basis in paper: [inferred] The paper doesn't address cold-start scenarios or discuss how the graph partitioning mechanism works when feedback data is limited or unavailable for new users/items.
- Why unresolved: Cold-start is a fundamental challenge in recommendation systems, but the paper focuses on scenarios with sufficient historical interaction data for feedback partitioning.
- What evidence would resolve it: Experimental results showing FRGCF performance on cold-start scenarios, analysis of how graph partitioning degrades with limited feedback data, and proposed solutions for handling cold-start cases.

## Limitations
- The partitioning assumption may not hold for datasets with ambiguous or sparse feedback signals
- Performance depends on the existence of meaningful shared latent structure between I&F and I&U graphs
- Macro-level feedback modeling introduces additional complexity and potential failure points through clustering steps
- Cold-start scenarios and implicit feedback-only datasets are not adequately addressed

## Confidence
**High Confidence**: Experimental results showing FRGCF outperforming baseline methods on multiple benchmark datasets and achieving significant improvements in NDCG. The ablation studies demonstrating the importance of both feedback-reciprocal contrastive learning and macro-level feedback modeling.

**Medium Confidence**: The claim that FRGCF successfully addresses the unfascinating item recommendation problem in production environments, based on online A/B tests on Taobao's recommender system. While promising, industrial results can be influenced by numerous factors beyond the model architecture itself.

**Low Confidence**: The generalizability of FRGCF to domains with different interaction patterns or feedback mechanisms not covered in the evaluation (e.g., sequential recommendation, multi-behavior scenarios).

## Next Checks
1. **Ablation on Feedback Partitioning**: Systematically vary the thresholds for partitioning interactions into I&F and I&U categories across different datasets to determine the robustness of FRGCF to different partitioning strategies and identify failure modes when clean partitioning is not possible.

2. **Cross-Domain Transferability**: Evaluate FRGCF on datasets with different characteristics (e.g., implicit feedback only, sequential interactions, multi-behavior data) to assess whether the core mechanisms generalize beyond the studied scenarios.

3. **Scalability and Efficiency Analysis**: Conduct detailed runtime and memory usage analysis of FRGCF compared to baseline methods, particularly focusing on the computational overhead introduced by feedback-reciprocal contrastive learning and macro-level feedback modeling in large-scale production environments.