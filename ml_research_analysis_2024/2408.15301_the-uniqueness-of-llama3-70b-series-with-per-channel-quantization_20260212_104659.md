---
ver: rpa2
title: The Uniqueness of LLaMA3-70B Series with Per-Channel Quantization
arxiv_id: '2408.15301'
source_url: https://arxiv.org/abs/2408.15301
tags:
- quantization
- llama3-70b
- per-channel
- figure
- series
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper identifies a unique quantization vulnerability in LLaMA3-70B
  models that is not present in other models. While most LLMs can be quantized to
  8-bit weights and activations (W8A8) with minimal accuracy loss, LLaMA3-70B experiences
  significant accuracy degradation due to large weight outliers in initial layers.
---

# The Uniqueness of LLaMA3-70B Series with Per-Channel Quantization

## Quick Facts
- arXiv ID: 2408.15301
- Source URL: https://arxiv.org/abs/2408.15301
- Authors: Minghai Qin
- Reference count: 10
- Primary result: LLaMA3-70B uniquely suffers from quantization degradation due to weight outliers in initial layers

## Executive Summary
This paper identifies a quantization vulnerability unique to LLaMA3-70B models where standard 8-bit per-channel quantization causes significant accuracy degradation. The issue stems from large weight outliers in initial Transformer layers that expand quantization ranges, reducing precision for smaller weights. The authors propose two solutions: a mixed per-channel/per-group quantization strategy targeting less than 3% of layers, and a bi-smoothing technique that balances weight and activation magnitudes. Both methods restore LLaMA3-70B's accuracy to levels comparable to FP16, addressing a challenge absent in other models like LLaMA2-70B and LLaMA3-8B.

## Method Summary
The study investigates quantization degradation in LLaMA3-70B models through empirical analysis of weight distributions and quantization error patterns. The authors develop two solutions: (1) a mixed per-channel/per-group quantization strategy that applies finer granularity to specific layers with outliers, and (2) a bi-smoothing approach that scales weights and activations to balance their magnitudes. Both methods are evaluated on eight reasoning tasks using standard benchmarks. The work contrasts LLaMA3-70B's behavior with other models to establish the uniqueness of the quantization vulnerability.

## Key Results
- LLaMA3-70B experiences significant accuracy degradation with W8A8 per-channel quantization, unlike LLaMA2-70B and LLaMA3-8B
- Weight outliers in initial layers (Q, K, V, Up, and Gate matrices) cause quantization errors exceeding 1%
- Mixed per-channel/per-group quantization restores accuracy by applying finer granularity to less than 3% of layers
- Bi-smoothing strategy effectively normalizes outlier magnitudes, reducing them from over 90 to below 2.0

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLaMA3-70B models have unique weight outliers in initial layers that cause W8A8 quantization degradation.
- Mechanism: Large weight values in Q, K, V, Up, and Gate matrices expand quantization ranges, leading to coarse intervals and reduced precision for smaller weights.
- Core assumption: Weight outliers are confined to initial Transformer blocks and significantly affect per-channel quantization.
- Evidence anchors:
  - [abstract] "the weight distribution of the LLaMA3-70B is the primary factor behind the vulnerability."
  - [section 3.6.1] "Weights with large magnitudes are concentrated on specific input dimensions, with the largest values exceeding 90."
  - [corpus] Weak. No direct neighbor evidence for LLaMA3-70B-specific outliers.
- Break condition: If outliers appear in later layers or other model families, this mechanism fails to explain uniqueness.

### Mechanism 2
- Claim: Mixed per-channel/per-group quantization restores accuracy by applying finer granularity only where needed.
- Mechanism: Applying per-group quantization to less than 3% of layers (those with outliers) reduces quantization intervals without hardware overhead across the full model.
- Core assumption: Per-group quantization in specific layers compensates for outlier-induced quantization error while preserving efficiency.
- Evidence anchors:
  - [abstract] "mixed strategy where less than 3% of the layers employ finer per-group W8A8 quantization granularity."
  - [section 4.1] "experimental results demonstrate that both strategies effectively preserve the accuracy of the entire LLaMA3-70B model series under W8A8 quantization."
  - [corpus] Weak. No neighbor evidence for per-channel/per-group mixed approaches.
- Break condition: If outlier layers exceed 3% or require larger group sizes, accuracy may degrade.

### Mechanism 3
- Claim: Bi-smoothing balances weight and activation magnitudes, reducing dynamic range and quantization error.
- Mechanism: Multiplying weights by a smooth-factor S and dividing activations by S preserves output while shrinking outlier magnitudes to near-normal ranges.
- Core assumption: Smooth-factor computed from median or max activation and weight ranges effectively normalizes dynamic range.
- Evidence anchors:
  - [abstract] "bi-smoothing strategy that balances quantization errors between weights and activations."
  - [section 5.3.1] "problematic weight outliers, with magnitudes exceeding 60, are suppressed to below 2.0."
  - [corpus] Weak. No neighbor evidence for bi-directional smoothing.
- Break condition: If smooth-factor computation is unstable or data-dependent, accuracy may not recover.

## Foundational Learning

- Concept: Symmetric integer quantization and scale-factor calculation.
  - Why needed here: Understanding how weight outliers inflate scale-factors and quantization intervals.
  - Quick check question: How is the scale-factor s computed for symmetric quantization?
- Concept: Per-channel vs per-group quantization granularity.
  - Why needed here: Knowing why per-channel is hardware-preferred but fails with outliers, while per-group fixes it.
  - Quick check question: What is the group size in per-channel quantization of a weight matrix W with dimension M×N?
- Concept: Transformer block weight matrix roles (Q, K, V, Up, Gate, O, Down).
  - Why needed here: Identifying which matrices host outliers and require special treatment.
  - Quick check question: Which five matrices in the first block show large weight outliers in LLaMA3-70B?

## Architecture Onboarding

- Component map: Model → Layers → Transformer blocks → Seven weight matrices (Q,K,V,O,Up,Gate,Down) → Quantization groups
- Critical path: Quantization → Identify outlier layers → Apply mixed grouping or bi-smoothing → Validate accuracy vs FP16
- Design tradeoffs: Mixed quantization reduces hardware overhead but adds per-layer granularity control; bi-smoothing adds calibration step but keeps per-channel quantization
- Failure signatures: Accuracy drops >1% vs FP16, quantization error spikes in initial layers, outlier magnitudes remain >90 after baseline quantization
- First 3 experiments:
  1. Run per-channel W8A8 quantization on LLaMA3-70B, plot quantization error per layer, confirm outlier spikes in first 3 blocks
  2. Apply mixed per-channel/per-group quantization to identified outlier layers, compare accuracy to FP16 baseline
  3. Apply bi-smoothing to the same layers, compare accuracy and quantization error to mixed grouping results

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What specific architectural or training modifications to LLaMA3-70B could eliminate the weight outliers while maintaining model performance?
- Basis in paper: [explicit] The paper identifies that weight outliers in initial layers cause quantization sensitivity, and notes LLaMA2-70B and LLaMA3-8B lack this issue
- Why unresolved: The paper only identifies the problem but doesn't explore architectural changes or training modifications that could prevent outlier formation
- What evidence would resolve it: Comparative analysis of modified LLaMA3-70B architectures with altered initialization, regularization, or training procedures showing reduced outlier formation while maintaining accuracy

### Open Question 2
- Question: How does the bi-smoothing technique generalize to other LLM architectures with different weight distributions?
- Basis in paper: [inferred] The paper shows bi-smoothing works for LLaMA3-70B but only tests it on models with similar outlier patterns, not on architectures like Qwen or Mistral
- Why unresolved: The method was developed specifically for LLaMA3-70B's unique weight distribution and hasn't been validated on diverse architectures
- What evidence would resolve it: Systematic testing of bi-smoothing across diverse LLM families showing consistent performance improvements or identifying architectural limitations

### Open Question 3
- Question: What is the relationship between weight outlier formation and reasoning capabilities in LLMs?
- Basis in paper: [explicit] The paper notes that unraveling why LLaMA3-70B uniquely develops outliers "could potentially offer valuable insights into the learning processes of Large Language Models (LLMs), especially in the context of reasoning tasks"
- Why unresolved: The paper identifies the correlation but doesn't investigate whether outliers are causally linked to reasoning performance or are merely a byproduct
- What evidence would resolve it: Controlled experiments comparing reasoning performance between models with/without outliers, or ablation studies showing how outlier magnitude affects reasoning capabilities

### Open Question 4
- Question: Can the mixed per-channel/per-group quantization strategy be automated to dynamically identify which layers need finer granularity?
- Basis in paper: [explicit] The paper manually identifies specific layers (15 out of 560) needing per-group quantization based on observed outliers
- Why unresolved: The current approach requires manual layer identification through weight distribution analysis, which doesn't scale to model families or deployment scenarios
- What evidence would resolve it: Development and validation of an automated metric that identifies quantization-vulnerable layers during model deployment, showing comparable accuracy to the manual approach

## Limitations
- The uniqueness claim requires broader testing across diverse model families to confirm LLaMA3-70B-specific vulnerability
- Mixed quantization approach adds per-layer granularity control complexity, potentially complicating deployment
- Bi-smoothing calibration depends on stable smooth-factor computation across diverse input distributions

## Confidence
- **High confidence**: The empirical observation that LLaMA3-70B experiences significant accuracy degradation with standard W8A8 quantization compared to other models. The proposed solutions (mixed quantization and bi-smoothing) demonstrably restore accuracy in controlled experiments.
- **Medium confidence**: The assertion that this vulnerability is unique to LLaMA3-70B series, as this requires broader testing across diverse model families to confirm. The effectiveness of bi-smoothing may also depend on the specific distribution characteristics of the target model.
- **Low confidence**: The exact quantification of "less than 3% of layers" requiring mixed quantization, as this percentage may vary significantly with model size, architecture variations, or different initialization schemes.

## Next Checks
1. **Cross-model validation**: Test the quantization vulnerability and proposed solutions on other contemporary LLM architectures (Mistral, Gemma, Qwen) to determine if the phenomenon is LLaMA3-70B-specific or represents a broader challenge in LLM quantization.

2. **Layer-wise sensitivity analysis**: Conduct ablation studies removing or replacing initial layers to quantify the exact contribution of outlier-containing layers to overall quantization error, and test whether the solutions remain effective when outliers are distributed across more layers.

3. **Dynamic range calibration verification**: Implement and validate the bi-smoothing calibration process across diverse input distributions to ensure the smooth-factor computation remains stable and effective outside the paper's test conditions.