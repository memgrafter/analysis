---
ver: rpa2
title: 'Towards Dynamic Resource Allocation and Client Scheduling in Hierarchical
  Federated Learning: A Two-Phase Deep Reinforcement Learning Approach'
arxiv_id: '2406.14910'
source_url: https://arxiv.org/abs/2406.14910
tags:
- edge
- client
- uni00000013
- clients
- aggregation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proposes a two-phase deep deterministic policy gradient
  (DDPG) framework called TP-DDPG for dynamic resource allocation and client scheduling
  in hierarchical federated learning (HFL) systems with energy harvesting-powered
  clients. The key idea is to divide optimization decisions into two groups, using
  DDPG to learn one group (client selection, CPU configurations, and transmission
  powers) in the first phase, while interpreting the other group (client association
  and bandwidth allocation) as part of the environment and optimizing it using a straggler-aware
  client association and bandwidth allocation (SCABA) algorithm in the second phase.
---

# Towards Dynamic Resource Allocation and Client Scheduling in Hierarchical Federated Learning: A Two-Phase Deep Reinforcement Learning Approach

## Quick Facts
- arXiv ID: 2406.14910
- Source URL: https://arxiv.org/abs/2406.14910
- Reference count: 40
- Key outcome: TP-DDPG reduces HFL training time by 39.4% compared to benchmarks when achieving 0.9 accuracy, reaching 0.93 accuracy on CIFAR-10

## Executive Summary
This paper addresses the challenge of optimizing resource allocation and client scheduling in hierarchical federated learning (HFL) systems with energy harvesting-powered clients. The authors propose a two-phase deep deterministic policy gradient (DDPG) framework called TP-DDPG that divides optimization decisions into two groups: DDPG learns client selection, CPU frequencies, and transmission powers in phase one, while a straggler-aware client association and bandwidth allocation (SCABA) algorithm handles the remaining decisions in phase two. This approach significantly reduces the action space complexity and accelerates convergence. Experimental results demonstrate that TP-DDPG outperforms five baseline algorithms, achieving 39.4% shorter training time and higher test accuracy on both MNIST and CIFAR-10 datasets.

## Method Summary
The TP-DDPG framework formulates the joint optimization of client scheduling and resource allocation as a Markov Decision Process, where the DDPG agent learns to select clients, configure CPU frequencies, and set transmission powers while the SCABA algorithm optimizes client association and bandwidth allocation. The system state includes battery levels, channel gains, and client selection history, with rewards based on an exponential function of the objective balancing client count against learning delay. The two-phase approach divides the complex optimization problem into manageable subproblems, reducing computational complexity and enabling faster convergence compared to solving all variables jointly with DDPG.

## Key Results
- TP-DDPG achieves 39.4% shorter training time than benchmarks when reaching 0.9 accuracy requirement
- The algorithm reaches 0.93 test accuracy on CIFAR-10 dataset, outperforming all baselines
- Experimental comparison shows TP-DDPG outperforms five baselines (GA, EBA, RS, NS, DDPG-Only, HO) across multiple metrics
- Convergence is achieved within 2500 episodes for MNIST and CIFAR-10 datasets

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Dividing optimization decisions into two groups (DDPG-learned vs SCABA-optimized) substantially reduces the DDPG action space and accelerates convergence.
- Mechanism: The DDPG agent only decides client selection, CPU frequencies, and transmission powers. Client association and bandwidth allocation are handled by the SCABA algorithm, which is interpreted as part of the environment and provides rewards for DDPG training.
- Core assumption: The SCABA can solve its subproblem optimally or near-optimally, enabling the DDPG to focus on the remaining decisions without needing to jointly optimize all variables.
- Evidence anchors:
  - [abstract] "The key idea is that we divide optimization decisions into two groups, and employ DDPG to learn one group in the first phase, while interpreting the other group as part of the environment to provide rewards for training the DDPG in the second phase."
  - [section] "Apart from its new consideration of energy harvesting-powered clients and comprehensiveness of the problem tackled, a key contribution of the TP-DDPG framework is that we interpret the client association and bandwidth allocation as part of the environment."

### Mechanism 2
- Claim: The two-phase structure allows the system to adapt to time-varying wireless channels and energy arrivals while maintaining efficient learning.
- Mechanism: In each edge aggregation round, the DDPG agent perceives the current state (battery levels, channel gains, etc.) and takes action. The SCABA then optimizes client association and bandwidth allocation based on this action, producing a reward that reflects the trade-off between learning delay and accuracy.
- Core assumption: The system state captures all relevant information for making good decisions, and the reward function properly balances the competing objectives.
- Evidence anchors:
  - [abstract] "The proposed TP-DDPG algorithm optimizes client scheduling and resource allocation (including bandwidth allocation, and the CPU frequencies and transmit powers of the clients) online during each round of edge aggregation, adapting to the time-varying system environment."
  - [section] "In the t-th edge aggregation round, the system state st∈ S is defined to be st ={Et− 1 n,c , Et n, ht nk, τ t n,∀n∈ N ,∀k ∈ K}."

### Mechanism 3
- Claim: The straggler-aware client association and bandwidth allocation (SCABA) algorithm specifically addresses the straggler effect that degrades HFL performance.
- Mechanism: SCABA identifies the slowest edge server (straggler) in each iteration and reduces its latency by iteratively adjusting client association and bandwidth allocation until the delay cannot be further shortened.
- Core assumption: By minimizing the latency of the slowest edge server, the overall edge aggregation delay is reduced, improving the total learning delay.
- Evidence anchors:
  - [abstract] "A new straggler-aware client association and bandwidth allocation (SCABA) algorithm efficiently optimizes the other decisions and evaluates the reward for the DDPG."
  - [section] "To mitigate the straggler effect in an edge aggregation round for the considered synchronous FL updates, we propose to use DDPG in the first phase of TP-DDPG to adaptively select clients preferably in good channel conditions in a round and avoid selecting straggling clients."

## Foundational Learning

- Concept: Markov Decision Process (MDP)
  - Why needed here: The problem of optimizing resource allocation and client scheduling in HFL can be formulated as an MDP, where the agent takes actions based on the current state to maximize cumulative rewards.
  - Quick check question: Can you explain how the state, action, and reward components of an MDP apply to this HFL system?

- Concept: Deep Deterministic Policy Gradient (DDPG)
  - Why needed here: DDPG is used to learn the policy for client selection, CPU frequencies, and transmission powers in the continuous action space of the HFL system.
  - Quick check question: What are the key components of DDPG (actor network, critic network, target networks) and their roles in the learning process?

- Concept: Straggler Effect in Federated Learning
  - Why needed here: The straggler effect occurs when the overall progress is limited by the slowest client, increasing the training latency. The SCABA algorithm specifically addresses this issue.
  - Quick check question: How does the straggler effect manifest in synchronous FL, and why is it particularly problematic in HFL systems?

## Architecture Onboarding

- Component map:
  - DDPG Agent -> Makes decisions on client selection, CPU frequencies, and transmission powers
  - SCABA Algorithm -> Optimizes client association and bandwidth allocation based on DDPG decisions
  - HFL System -> Consists of cloud server, edge servers, and energy harvesting-powered clients
  - Experience Replay Buffer -> Stores past experiences for DDPG training
  - State -> Includes battery levels, channel gains, and client selection history
  - Reward -> Exponential function of the objective (number of clients - delay) with penalty for constraint violations

- Critical path:
  1. DDPG agent perceives state and takes action (client selection, CPU freq, transmit power)
  2. SCABA optimizes client association and bandwidth allocation based on DDPG action
  3. System executes the decisions and produces reward (learning delay and accuracy)
  4. Experience is stored in replay buffer and used to update DDPG networks

- Design tradeoffs:
  - Centralized vs. decentralized DDPG: Centralized training with distributed execution balances convergence and scalability
  - Action space division: Dividing decisions between DDPG and SCABA reduces complexity but requires SCABA to be effective
  - Reward function design: Exponential reward with penalty balances exploration and constraint satisfaction

- Failure signatures:
  - Slow DDPG convergence: May indicate too large action space or poor reward signal
  - Poor learning accuracy: Could result from ineffective client selection or straggler mitigation
  - Energy depletion: May indicate poor energy management or constraint handling

- First 3 experiments:
  1. Test DDPG convergence speed with different state and action space sizes
  2. Evaluate SCABA's effectiveness in mitigating stragglers and reducing edge aggregation delay
  3. Measure the overall system performance (learning delay and accuracy) compared to baseline algorithms under varying energy arrival rates and channel conditions

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of the TP-DDPG algorithm scale with increasing numbers of clients and edge servers?
- Basis in paper: [explicit] The paper states that "direct use of DDPG to solve problem (18) may not converge, since, as the numbers of clients and edge servers grow, the MDP is increasingly complex with the number of variables in At."
- Why unresolved: While the paper demonstrates improved convergence and performance compared to benchmarks in a system with 3 edge servers and 10 clients, it does not explore how the algorithm scales to larger systems with more clients and edge servers.
- What evidence would resolve it: Experimental results showing the performance of TP-DDPG in systems with varying numbers of clients and edge servers, including very large-scale deployments.

### Open Question 2
- Question: How does the TP-DDPG algorithm handle dynamic changes in the number of clients or edge servers during the FL process?
- Basis in paper: [inferred] The paper focuses on optimizing resource allocation and client scheduling for a static set of clients and edge servers, but does not discuss how the algorithm adapts to changes in the network topology during runtime.
- Why unresolved: Real-world FL systems may experience dynamic changes in network conditions, client availability, or the addition/removal of edge servers, which could impact the algorithm's performance.
- What evidence would resolve it: Experimental results demonstrating the algorithm's ability to adapt to dynamic changes in the network topology, such as adding or removing clients or edge servers during the FL process.

### Open Question 3
- Question: How does the TP-DDPG algorithm perform in non-IID data distributions among clients?
- Basis in paper: [inferred] The paper mentions that the importance-oriented weighting scheme in the edge model aggregation can effectively deal with sample heterogeneity among clients, but does not provide experimental results on non-IID data distributions.
- Why unresolved: Real-world FL systems often involve non-IID data distributions, where clients have different data distributions, which can impact the convergence and performance of FL algorithms.
- What evidence would resolve it: Experimental results showing the performance of TP-DDPG in scenarios with non-IID data distributions among clients, comparing it to baselines that do not consider data heterogeneity.

## Limitations

- Neural network architecture details for DDPG agent are not specified, creating uncertainty about exact replication
- SCABA algorithm implementation details and importance-oriented weighting scheme lack sufficient specification
- Performance scaling with increasing numbers of clients and edge servers is not explored

## Confidence

**High Confidence**: The overall framework design (two-phase DDPG with SCABA) and the problem formulation are well-established and theoretically sound. The mathematical models for energy harvesting, channel dynamics, and HFL system operation are clearly specified.

**Medium Confidence**: The experimental results showing 39.4% reduction in training time and 0.93 accuracy on CIFAR-10 are presented with clear methodology, but the lack of specific network architecture details creates uncertainty about exact replication.

**Low Confidence**: The SCABA algorithm's implementation details and the importance-oriented weighting scheme in equation (4) are insufficiently specified, making faithful reproduction challenging.

## Next Checks

1. **Baseline Implementation Verification**: Replicate the five baseline algorithms (GA, EBA, RS, NS, DDPG-Only) exactly as described to establish the experimental foundation before testing TP-DDPG.

2. **State Space Sensitivity Analysis**: Conduct experiments varying the state space components to verify which elements are truly essential for DDPG convergence, testing the claim that the current state definition optimally captures system dynamics.

3. **SCABA Algorithm Performance Isolation**: Test the SCABA algorithm independently of the DDPG to quantify its contribution to straggler mitigation and verify it achieves the claimed performance improvements when paired with different client selection policies.