---
ver: rpa2
title: Limits to Predicting Online Speech Using Large Language Models
arxiv_id: '2407.12850'
source_url: https://arxiv.org/abs/2407.12850
tags:
- context
- tweets
- user
- social
- users
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigates the predictability of online speech on
  X (Twitter) using large language models. The authors define predictability as a
  measure of model uncertainty, specifically the negative log-likelihood.
---

# Limits to Predicting Online Speech Using Large Language Models

## Quick Facts
- arXiv ID: 2407.12850
- Source URL: https://arxiv.org/abs/2407.12850
- Authors: Mina Remeli; Moritz Hardt; Robert C. Williamson
- Reference count: 40
- Primary result: Predicting individual users' tweets remains surprisingly difficult, with user context providing 1.8σ more predictive information than peer context

## Executive Summary
This study investigates the predictability of online speech on X (Twitter) using large language models. The authors collected 6.25 million posts from over 5,000 users and their peers, plus 10 million additional tweets for model training. Using four large language models ranging from 1.5 to 70 billion parameters, they found that individual users' posts are surprisingly hard to predict, with most users being less predictable than financial news accounts. The key finding is that a user's own post history provides significantly more predictive information than posts from their social circle, suggesting individual linguistic patterns are more informative than social network effects.

## Method Summary
The researchers collected 6.25M tweets from 5,000+ X users and their peers, plus 10M additional tweets for model training. They evaluated four LLMs (1.5B-70B parameters) using prompting and finetuning approaches across three context conditions: user context (own history), peer context (social circle), and random context. Predictability was measured using negative log-likelihood converted to bits per character (BPC), with effect sizes comparing different context types. The study tested whether peer context provides unique predictive information beyond what's already in user context.

## Key Results
- User context provides 1.8σ more predictive information than peer context for tweet prediction
- Most individual users' posts are less predictable than financial news accounts
- Up to 20% of predictability improvement comes from learning syntax patterns like @-mentions and hashtags
- Results were robust across different demographic groups and model sizes, though predictability varied by location

## Why This Works (Mechanism)

### Mechanism 1
- Claim: User context provides significantly more predictive information than peer context because individual users have unique linguistic patterns that are not fully captured by their peers' language.
- Mechanism: The language model learns individual-specific features (style, vocabulary, syntax preferences) from user context that are distinct from general social network patterns found in peer context.
- Core assumption: Individual users develop consistent linguistic signatures over time that differ from their social circle's collective patterns.
- Evidence anchors:
  - [abstract] "models using the users' own history significantly outperform models using posts from their social circle"
  - [section] "a user's own posts have significantly more predictive information than posts from their close social ties"
  - [corpus] Weak - the corpus shows related work on peer-based profiling but doesn't directly address why user context would be more informative
- Break condition: If users' language patterns are primarily shaped by social influence rather than individual preferences, peer context could become equally or more informative.

### Mechanism 2
- Claim: Base models learn syntax patterns (like @-mentions and hashtags) from context rather than deeper semantic content.
- Mechanism: When given additional context, the model primarily learns to predict structural elements of social media posts (length, formatting, common phrases) rather than understanding the user's unique voice or content preferences.
- Core assumption: The most immediately learnable patterns from context are surface-level syntax features rather than deeper semantic understanding.
- Evidence anchors:
  - [abstract] "Up to 20% of what is learned in-context is the use of @-mentions and hashtags"
  - [section] "models learned to assign higher probability to frequently used @-mentions and hashtags from context"
  - [corpus] Moderate - the corpus includes related work on social media language modeling but doesn't directly address this specific learning mechanism
- Break condition: If the model could learn deeper semantic patterns more efficiently than syntax patterns, the proportion of syntax learning would decrease.

### Mechanism 3
- Claim: Predictability improvements are limited because individual online speech contains substantial entropy even within a single user's history.
- Mechanism: Each user's language has inherent variability that makes perfect prediction impossible, even with extensive context from that same user.
- Core assumption: Human language use, even by a single individual, contains sufficient randomness and variability to prevent perfect predictability.
- Evidence anchors:
  - [abstract] "predicting posts of individual users remains surprisingly hard" and "most users' posts are less predictable than posts from financial news accounts"
  - [section] "even with additional context prediction performs relatively poorly" and "Only our largest model with additional user context...is able to approximate the estimated entropy rate"
  - [corpus] Moderate - the corpus includes related work on information theory and language entropy but doesn't directly address individual user entropy
- Break condition: If users' language patterns were highly deterministic or if the model could capture long-range dependencies perfectly, predictability would improve significantly.

## Foundational Learning

- Concept: Negative log-likelihood (NLL) as a measure of model uncertainty
  - Why needed here: The paper uses NLL to quantify how uncertain the model is about predicting a user's next tweet, with lower NLL indicating higher predictability
  - Quick check question: If a model assigns probabilities of 0.1, 0.2, and 0.7 to three possible next tokens, what is the NLL for the correct token (assuming it's the third one)?

- Concept: Bits per character (BPC) for cross-model comparison
  - Why needed here: Different language models use different tokenizers, so BPC normalizes the predictability measure across models by accounting for average characters per token
  - Quick check question: If Model A has NLL of 2.3 and average 4 characters per token, while Model B has NLL of 1.8 and average 3 characters per token, which model achieves lower BPC?

- Concept: Effect size (standardized mean difference) for comparing contexts
  - Why needed here: The paper quantifies how much more predictive user context is compared to peer context using effect sizes to show the magnitude of differences
  - Quick check question: If the average difference in NLL between user and peer context is 0.5 with a standard deviation of 0.2 across users, what is the effect size?

## Architecture Onboarding

- Component map: Data collection pipeline -> Model loading -> Prompt generation -> NLL calculation -> BPC conversion -> Effect size analysis
- Critical path: Collect user/peer/random tweets -> Load pre-trained model -> Generate prompts with context -> Calculate NLL for each tweet -> Convert to BPC -> Aggregate results
- Design tradeoffs: Using prompting vs. finetuning - prompting is faster and avoids training costs but is sensitive to prompt format; finetuning learns context better but requires significant computation
- Failure signatures: High variance in NLL across users suggests model struggles with certain linguistic patterns; minimal BPC improvement from context suggests model isn't learning effectively
- First 3 experiments:
  1. Run baseline experiment with no context to establish user predictability floor
  2. Add user context and verify BPC improvement as expected
  3. Add peer context and compare improvement to user context to validate the main finding

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the predictability of online speech vary across different languages and dialects beyond English?
- Basis in paper: [inferred] The paper notes its analysis is constrained to English-speaking populations and suggests external validity is a limitation.
- Why unresolved: The study focused exclusively on English tweets and used English language models, making it unclear how results generalize to other linguistic contexts.
- What evidence would resolve it: Conducting similar predictability experiments on non-English Twitter datasets or other social media platforms in different languages, using appropriate multilingual language models.

### Open Question 2
- Question: What specific linguistic or behavioral features beyond @-mentions and hashtags contribute to predictability improvements from user context?
- Basis in paper: [explicit] The study found up to 20% of predictability improvement comes from learning @-mentions and hashtags, but the remaining 80% remains unexplained.
- Why unresolved: The authors analyzed only the most improved tokens but did not perform a comprehensive feature-level analysis of what other aspects of writing style or content drive predictability.
- What evidence would resolve it: Detailed feature importance analysis (lexical patterns, syntactic structures, topical consistency, posting timing patterns) comparing user context versus peer context effects.

### Open Question 3
- Question: How does the predictability of individual users change over time, and what factors influence temporal variations in predictability?
- Basis in paper: [inferred] The study collected tweets spanning from 2011 to 2023, but did not analyze temporal dynamics of predictability for individual users.
- Why unresolved: The cross-sectional analysis provides a snapshot but doesn't capture how predictability might evolve with changing user behavior, life circumstances, or platform usage patterns.
- What evidence would resolve it: Longitudinal study tracking the same users' predictability over extended periods, correlating changes with external events, user activity patterns, or demographic shifts.

## Limitations

- Data scope uncertainty: Findings may not generalize beyond English-language Twitter posts from 2022-2023, representing a convenience sample rather than representative online speech
- Model architecture limitations: The tested models may have inherent architectural constraints that prevent capturing long-range dependencies or subtle stylistic patterns
- Context definition ambiguity: The peer context definition based on Twitter's follow graph may not capture all meaningful social relationships affecting language use

## Confidence

- High Confidence: User context provides significantly more predictive information than peer context (effect size 1.8σ)
- Medium Confidence: Up to 20% of predictability improvement comes from learning syntax patterns like @-mentions and hashtags
- Medium Confidence: Nigerian users are the least predictable demographic group

## Next Checks

1. **Cross-platform validation:** Replicate the study using posts from other social media platforms (Reddit, Facebook, Instagram) to test whether the user context advantage holds across different communication styles and platform constraints.

2. **Temporal stability analysis:** Examine whether predictability changes over time within individual users by comparing early vs. late tweets in their posting history, and test if models trained on recent context perform better than those using historical context.

3. **Alternative social relationship definitions:** Test predictability using different definitions of peer context beyond follow relationships, such as users who frequently interact (replies/mentions), users with similar demographic profiles, or users in the same geographic region.