---
ver: rpa2
title: Universal Black-Box Reward Poisoning Attack against Offline Reinforcement Learning
arxiv_id: '2402.09695'
source_url: https://arxiv.org/abs/2402.09695
tags:
- attack
- learning
- reward
- policy
- offline
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a universal black-box reward poisoning attack
  strategy called the 'policy contrast attack' against offline reinforcement learning
  (RL). The attacker aims to make any efficient offline RL algorithm learn a low-performing
  policy by corrupting the reward signals in the training dataset, while having limited
  budget and no knowledge of the learning algorithm.
---

# Universal Black-Box Reward Poisoning Attack against Offline Reinforcement Learning

## Quick Facts
- arXiv ID: 2402.09695
- Source URL: https://arxiv.org/abs/2402.09695
- Authors: Yinglun Xu; Rohan Gumaste; Gagandeep Singh
- Reference count: 12
- This paper proposes a universal black-box reward poisoning attack strategy against offline RL that makes any efficient algorithm learn a low-performing policy.

## Executive Summary
This paper introduces a novel reward poisoning attack against offline reinforcement learning called the "policy contrast attack." The attack corrupts reward signals in training datasets to make any efficient offline RL algorithm learn poor-performing policies, while the attacker has limited budget and no knowledge of the learning algorithm. The key innovation is identifying high- and low-performing policies in the dataset and making them appear low- and high-performing to the agent, respectively. The attack is evaluated on standard D4RL datasets and state-of-the-art offline RL algorithms, demonstrating high efficiency with limited attack budget and robustness to hyperparameter choices.

## Method Summary
The Policy Contrast Attack (PCA) constructs an adversarial reward function by identifying good and bad policies from the dataset and inverting their perceived performance. The attack uses two main algorithms: Algorithm 1 constructs the adversarial reward by increasing rewards for actions close to low-performing policies and decreasing rewards for actions close to high-performing policies; Algorithm 2 learns a good policy set to identify which policies should be contrasted. The attacker operates under budget constraints (maximum perturbation per reward and total perturbation) and has no knowledge of the specific offline RL algorithm being used.

## Key Results
- The policy contrast attack successfully degrades performance of state-of-the-art offline RL algorithms (IQL, CQL, TD3_BC) on D4RL benchmarks
- The attack achieves high efficiency with limited budget constraints, requiring fewer corrupted rewards compared to baseline attacks
- The attack's performance is robust to hyperparameter choices and remains effective across different dataset types and algorithm configurations

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The attack exploits the "pessimistic learning" property of efficient offline RL algorithms, which favor policies supported by the dataset.
- Mechanism: By making low-performing policies appear high-performing and vice versa, the attacker ensures that any efficient offline RL algorithm will converge to a poor-performing policy.
- Core assumption: The learning agent uses a δ-optimal pessimistic learning algorithm that learns the best-supported policy within the dataset.
- Evidence anchors:
  - [abstract]: "The key idea is to identify high- and low-performing policies covered by the dataset, and make them appear low- and high-performing to the agent, respectively."
  - [section]: "By the optimization problem in Eq 1, we can naturally define the efficiency of an attack as follows."
  - [corpus]: Weak evidence. No direct mention of "pessimistic learning" or similar concepts in neighboring papers.
- Break condition: If the learning algorithm does not adhere to pessimistic learning assumptions, or if it can detect and reject poisoned data.

### Mechanism 2
- Claim: The attack is effective because it modifies rewards in a way that significantly impacts the perceived performance of supported policies without requiring large perturbations.
- Mechanism: The attacker increases rewards for actions close to those of identified low-performing policies and decreases rewards for actions close to those of high-performing policies.
- Core assumption: The attacker can accurately identify good and bad policies from the dataset.
- Evidence anchors:
  - [abstract]: "We propose an attack strategy called the ‘policy contrast attack.’ The idea is to find low- and high-performing policies covered by the dataset and make them appear to be high- and low-performing to the agent, respectively."
  - [section]: "Based on the discussion above, we formulate ‘policy contrast attack.’ The attack consists of two parts as follow..."
  - [corpus]: Weak evidence. No direct mention of reward modification strategies in neighboring papers.
- Break condition: If the attacker cannot accurately identify good and bad policies, or if the learning algorithm is robust to small reward perturbations.

### Mechanism 3
- Claim: The attack remains effective even with limited budgets by focusing perturbations on specific state-action pairs that influence the perceived performance of supported policies.
- Mechanism: The attacker strategically allocates the limited budget to maximize the impact on the perceived performance of supported policies.
- Core assumption: The attacker can effectively allocate the limited budget to maximize the impact on the perceived performance of supported policies.
- Evidence anchors:
  - [abstract]: "The attack is evaluated on various standard datasets and state-of-the-art offline RL algorithms, showing high efficiency with limited attack budget."
  - [section]: "In terms of finding the good policies, Lemma 4.7 shows that policies of high performance will behave similarly to some policies from the good policy set constructed through Alg 2."
  - [corpus]: Weak evidence. No direct mention of budget allocation strategies in neighboring papers.
- Break condition: If the attacker cannot effectively allocate the limited budget, or if the learning algorithm is robust to small perturbations.

## Foundational Learning

- Concept: Reinforcement Learning (RL)
  - Why needed here: Understanding the basics of RL is crucial for grasping how the attack manipulates the learning process.
  - Quick check question: What is the difference between online and offline RL?

- Concept: Reward Function
  - Why needed here: The attack specifically targets the reward function to manipulate the learning process.
  - Quick check question: How does the reward function influence the behavior of an RL agent?

- Concept: Policy
  - Why needed here: The attack aims to make the agent learn a poor-performing policy by manipulating the reward function.
  - Quick check question: What is a policy in the context of RL, and how is it learned?

## Architecture Onboarding

- Component map:
  - Attacker (identifies good/bad policies, modifies rewards) -> Dataset (contains state-action-reward tuples) -> Learning Agent (uses offline RL algorithm)

- Critical path:
  1. Attacker identifies good and bad policies from the dataset
  2. Attacker modifies rewards within budget constraints to make good policies appear bad and bad policies appear good
  3. Learning agent trains on the poisoned dataset and learns a poor-performing policy

- Design tradeoffs:
  - Budget vs. Effectiveness: Higher budget allows for more effective attacks but may be more detectable
  - Accuracy of Policy Identification vs. Attack Efficiency: More accurate policy identification leads to more efficient attacks but may require more computational resources

- Failure signatures:
  - Learning agent detects and rejects poisoned data
  - Learning agent learns a good policy despite the attack
  - Attacker fails to accurately identify good and bad policies

- First 3 experiments:
  1. Test the attack on a simple RL environment with a known good and bad policy
  2. Vary the attack budget and measure the impact on the learned policy's performance
  3. Test the attack against different offline RL algorithms to evaluate its universality

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the choice of action distance metric (da) affect the efficiency of the policy contrast attack across different offline RL environments?
- Basis in paper: [explicit] The paper uses L2 distance for continuous actions and mentions that the attack performance is not sensitive to hyperparameter choices, but doesn't systematically test different distance metrics.
- Why unresolved: Different environments may have different action space characteristics that make certain distance metrics more effective for identifying contrasting policies.
- What evidence would resolve it: Systematic experiments comparing attack success rates using different distance metrics (L1, L2, cosine similarity) across multiple environments and dataset types.

### Open Question 2
- Question: What is the theoretical upper bound on attack efficiency when the offline RL algorithm doesn't perfectly satisfy the δ-optimal pessimistic learning assumption?
- Basis in paper: [inferred] The paper assumes algorithms satisfy Assumption 4.1 but notes "the learning algorithms may not strictly satisfy the assumption" while still showing attack effectiveness.
- Why unresolved: The analysis assumes near-optimal pessimistic learning, but real algorithms have approximation errors and exploration components that could reduce attack effectiveness.
- What evidence would resolve it: Theoretical analysis quantifying the relationship between algorithm approximation error and attack efficiency, plus empirical validation across algorithms with varying levels of pessimism.

### Open Question 3
- Question: How does the policy contrast attack scale to high-dimensional state and action spaces compared to simpler random attacks?
- Basis in paper: [explicit] The paper evaluates on standard D4RL benchmarks but doesn't explicitly compare computational complexity or attack budget requirements as state/action dimensionality increases.
- Why unresolved: The attack involves finding and contrasting multiple policies, which may become computationally prohibitive or require more budget in high dimensions where state-action coverage is sparse.
- What evidence would resolve it: Experiments measuring attack preparation time, success rate, and required budget across environments with varying state/action dimensionalities, plus computational complexity analysis.

### Open Question 4
- Question: Can the attacker achieve better efficiency by adaptively modifying the corruption strategy during the attack rather than using a fixed adversarial reward function?
- Basis in paper: [inferred] The attack uses a static adversarial reward function construction, but the online RL poisoning literature shows adaptive strategies can be more efficient.
- Why unresolved: The paper focuses on black-box attacks with no access to training process, but adaptive strategies could potentially identify more influential state-actions to corrupt.
- What evidence would resolve it: Comparison of fixed vs. adaptive corruption strategies in a gray-box setting where the attacker can observe training progress, measuring attack efficiency and required budget.

## Limitations
- The attack's effectiveness depends on the attacker's ability to accurately identify high- and low-performing policies in the dataset, which may be challenging in complex, real-world scenarios.
- While shown to be effective against several state-of-the-art offline RL algorithms, the attack's performance against other, potentially more robust algorithms remains unknown.
- The attack assumes access to the complete dataset for policy identification, which may not be feasible in all practical settings where data privacy or access constraints exist.

## Confidence
- Claim: The Policy Contrast Attack is a universal black-box strategy
  - Label: Medium
  - The paper provides theoretical justification and empirical evaluation, but effectiveness may vary depending on dataset characteristics and algorithm specifics
- Claim: The attack is efficient with limited budget
  - Label: High
  - Results clearly show effectiveness with constrained budget, though exact budget requirements could be further explored

## Next Checks
1. Test the attack against a broader range of offline RL algorithms, including those with different design philosophies or robustness properties.
2. Evaluate the attack's performance on more diverse and complex datasets, such as those with higher dimensional state and action spaces or non-stationary environments.
3. Investigate the impact of different distance measures and hyperparameter settings on the attack's effectiveness and efficiency.