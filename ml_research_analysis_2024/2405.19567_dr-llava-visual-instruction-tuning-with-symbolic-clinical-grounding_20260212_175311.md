---
ver: rpa2
title: 'Dr-LLaVA: Visual Instruction Tuning with Symbolic Clinical Grounding'
arxiv_id: '2405.19567'
source_url: https://arxiv.org/abs/2405.19567
tags:
- image
- arxiv
- medical
- clinical
- dr-llava
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses hallucination in vision-language models (VLMs)
  for medical imaging, where models generate outputs inconsistent with clinical reasoning
  and diagnostic pathways during multi-turn conversations. To solve this, the authors
  propose Dr-LLaVA, which uses symbolic representations of clinical reasoning to automatically
  generate training data and reward functions without human involvement.
---

# Dr-LLaVA: Visual Instruction Tuning with Symbolic Clinical Grounding

## Quick Facts
- **arXiv ID**: 2405.19567
- **Source URL**: https://arxiv.org/abs/2405.19567
- **Reference count**: 40
- **Primary result**: Dr-LLaVA achieves 89.6% question accuracy and 92.0% conversation accuracy on bone marrow pathology diagnosis, outperforming state-of-the-art VLMs in reducing hallucination during multi-turn medical conversations.

## Executive Summary
Dr-LLaVA addresses hallucination in vision-language models for medical imaging by grounding outputs in symbolic clinical reasoning. The approach uses decision tree-based symbolic rules to decompose diagnostic questions into sequential logical steps, ensuring clinically coherent reasoning paths. A two-stage fine-tuning process (SFT + RL) optimizes for both individual question accuracy and conversation-level consistency using an automated reward function that evaluates correctness and clinical validity without human feedback.

## Method Summary
The method employs a two-stage fine-tuning approach on a pre-trained VLM. First, supervised fine-tuning trains on synthetic multi-turn clinician-VLM conversations generated using GPT-4 and guided by symbolic clinical reasoning rules for bone marrow pathology. Second, reinforcement learning optimizes the model using a symbolic reward function that evaluates both individual response correctness (via keyword matching) and conversation-level consistency with valid clinical reasoning paths. The reward model combines correctness rewards, consistency rewards, and regularization terms, trained using PPO with KL divergence penalties to prevent reward hacking.

## Key Results
- Dr-LLaVA achieves 89.6% question accuracy and 92.0% conversation accuracy on bone marrow pathology diagnosis, surpassing state-of-the-art VLMs.
- The model demonstrates superior robustness to varied question sequences and better detection of misleading clinician prompts.
- Dr-LLaVA shows lower hallucination rates (Hcc) compared to baseline models, indicating better alignment with clinical reasoning pathways.

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Symbolic clinical reasoning rules decompose complex diagnostic questions into sequential logical steps that constrain valid reasoning paths.
- **Mechanism**: The symbolic representation defines a decision tree where each node represents a diagnostic step and edges represent valid transitions. This structure ensures VLM responses follow clinically coherent pathways by enforcing consistency with previous steps.
- **Core assumption**: Medical diagnostic reasoning can be adequately represented as a hierarchical set of discrete logical rules.
- **Evidence anchors**: Abstract states "many clinical reasoning processes can be formalized as a hierarchical set of symbolic rules"; section defines "a set of symbolic rules, S, which outlines all valid reasoning paths in the decision tree".
- **Break condition**: If medical diagnosis requires continuous reasoning that cannot be discretized, or if symbolic rules miss critical clinical exceptions.

### Mechanism 2
- **Claim**: The reward function automatically evaluates both individual response correctness and conversation-level consistency without human feedback.
- **Mechanism**: The reward model computes correctness reward (RC) checking required keywords, consistency reward (RS) verifying valid reasoning paths, and regularization terms penalizing ambiguous responses.
- **Core assumption**: Clinical responses can be evaluated through keyword matching and rule verification without human expert judgment.
- **Evidence anchors**: Abstract mentions "reward function that automatically evaluates VLM responses"; section defines "reward model that assesses both correctness of model responses and their alignment with valid clinical reasoning".
- **Break condition**: If keyword matching fails to capture nuanced clinical reasoning, or if symbolic rules don't cover all valid diagnostic pathways.

### Mechanism 3
- **Claim**: The two-stage finetuning (SFT + RL) balances individual question accuracy with conversation-level clinical coherence.
- **Mechanism**: Supervised finetuning establishes baseline performance on synthetic conversations, then reinforcement learning optimizes for the combined reward function encouraging correct individual answers and consistent reasoning across conversations.
- **Core assumption**: Starting from SFT provides better initialization for RL than random initialization when optimizing for conversation-level rewards.
- **Evidence anchors**: Abstract describes "two-stage approach to optimize the VLM for clinical tasks"; section mentions "initialized the value model based on the LLavA-13B-based reward model".
- **Break condition**: If SFT model learns harmful biases that RL cannot overcome, or if KL penalty is too strong and prevents necessary policy updates.

## Foundational Learning

- **Concept**: Symbolic representation of clinical reasoning as decision trees
  - **Why needed here**: Provides formal structure enabling automated reward computation and ensuring clinically valid reasoning paths
  - **Quick check question**: Can you draw the decision tree for bone marrow pathology analysis showing valid transitions between image quality assessment, cell abnormality detection, and final diagnosis?

- **Concept**: Reinforcement learning with automatic reward functions
  - **Why needed here**: Enables scaling to medical domains where human feedback is expensive while maintaining alignment with clinical reasoning
  - **Quick check question**: How does the consistency reward RS differ from standard RLHF reward models that rely on pairwise human preferences?

- **Concept**: Multi-turn conversation evaluation metrics (AQ, AC, AD)
  - **Why needed here**: Different levels of evaluation are needed to assess both individual question accuracy and overall conversation coherence in medical diagnostic contexts
  - **Quick check question**: What's the key difference between Conversation-level Accuracy (AC) and Diagnostic Accuracy (AD), and why are both important for medical VLMs?

## Architecture Onboarding

- **Component map**: VLM architecture consists of Vicuna-V1.5-7b LLM paired with pre-trained CLIP ViT-L/14 visual encoder. Image features are mapped to word embedding space via linear layer. Two-stage training pipeline: SFT on synthetic conversations followed by RL with symbolic reward function.
- **Critical path**: Image → Visual encoder → Feature mapping → LLM → Answer generation → Reward computation (RC + RS + regularization) → Policy update (PPO with KL penalty)
- **Design tradeoffs**: Keyword-based rewards are efficient but may miss nuanced reasoning; symbolic rules provide structure but may not capture all clinical exceptions; two-stage training balances accuracy with coherence but adds complexity
- **Failure signatures**: Low AC but high AQ suggests model answers questions correctly but inconsistently; high Hcc (hallucination rate) indicates reward function not properly constraining valid reasoning paths; KL divergence spikes suggest reward hacking
- **First 3 experiments**:
  1. Train with only correctness reward (Rc) to establish baseline individual question accuracy
  2. Train with only consistency reward (RS) to measure impact on conversation coherence
  3. Vary λ hyperparameter to find optimal balance between correctness and consistency

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: How does the performance of Dr-LLaVA compare to human experts in bone marrow pathology diagnosis?
- **Basis in paper**: [explicit] The paper demonstrates Dr-LLaVA's strong performance against state-of-the-art VLMs but does not compare it to human experts.
- **Why unresolved**: The paper focuses on benchmarking against other VLMs but lacks a direct comparison with human pathologists.
- **What evidence would resolve it**: A head-to-head comparison of Dr-LLaVA's diagnostic accuracy and reasoning against experienced human pathologists on the same dataset.

### Open Question 2
- **Question**: Can the symbolic clinical grounding approach be generalized to other medical specialties beyond hematopathology?
- **Basis in paper**: [inferred] The paper demonstrates success in bone marrow pathology but does not explore other medical domains.
- **Why unresolved**: The methodology is validated only on a specific pathology domain, leaving open whether it can be adapted to other specialties.
- **What evidence would resolve it**: Application and validation of the symbolic grounding approach to other medical imaging domains (e.g., radiology, dermatology) with comparable performance improvements.

### Open Question 3
- **Question**: What is the long-term impact of using Dr-LLaVA on clinical workflows and patient outcomes?
- **Basis in paper**: [explicit] The paper focuses on technical performance but does not address real-world clinical integration or patient outcomes.
- **Why unresolved**: The study is limited to model performance metrics without considering practical implementation challenges or downstream effects.
- **What evidence would resolve it**: A longitudinal study measuring how Dr-LLaVA integration affects diagnostic accuracy, efficiency, and patient outcomes in actual clinical settings.

### Open Question 4
- **Question**: How robust is Dr-LLaVA to adversarial inputs or rare disease presentations?
- **Basis in paper**: [explicit] The paper tests robustness to varied question sequences but does not explore adversarial scenarios or rare conditions.
- **Why unresolved**: The evaluation focuses on standard diagnostic scenarios without testing model resilience to unusual or intentionally misleading inputs.
- **What evidence would resolve it**: Systematic testing of Dr-LLaVA's performance on rare diseases, ambiguous cases, and deliberately constructed adversarial examples.

## Limitations

- The effectiveness of symbolic clinical reasoning rules depends heavily on how comprehensively they capture real-world medical diagnostic complexity, which hasn't been validated.
- The synthetic conversation generation process using GPT-4 may not fully capture the diversity and complexity of real clinician-VLM interactions, potentially leading to overfitting to synthetic patterns.
- The keyword-based reward function, while efficient, may not capture nuanced clinical reasoning that requires contextual understanding beyond simple keyword matching.

## Confidence

- **High confidence**: The two-stage fine-tuning approach (SFT + RL) and the overall framework design are well-established in the literature and the implementation appears sound.
- **Medium confidence**: The performance improvements on synthetic datasets are convincing, but the translation to real clinical settings remains unproven.
- **Low confidence**: The claim that symbolic rules can fully capture clinical reasoning complexity and that keyword-based rewards can adequately evaluate medical diagnostic reasoning without human oversight.

## Next Checks

1. **External Validation on Real Clinical Data**: Test Dr-LLaVA on a held-out dataset of real clinician-VLM interactions (not synthetically generated) to verify that the performance gains translate beyond synthetic conversations.

2. **Human Expert Evaluation**: Conduct a blind study where hematopathologists evaluate Dr-LLaVA's responses against other VLMs on the same clinical cases, assessing not just accuracy but also clinical reasoning quality and diagnostic coherence.

3. **Symbolic Rule Coverage Analysis**: Perform a systematic analysis of the symbolic clinical reasoning rules to identify gaps or missing diagnostic pathways, and test whether Dr-LLaVA can handle cases that fall outside the predefined symbolic rules.