---
ver: rpa2
title: 'ConvSDG: Session Data Generation for Conversational Search'
arxiv_id: '2403.11335'
source_url: https://arxiv.org/abs/2403.11335
tags:
- conversational
- data
- search
- query
- session
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of limited training data for conversational
  dense retrieval by proposing a framework to automatically generate conversational
  search sessions using large language models (LLMs). The ConvSDG framework generates
  session data at both dialogue-level and query-level using unsupervised and semi-supervised
  learning approaches.
---

# ConvSDG: Session Data Generation for Conversational Search

## Quick Facts
- arXiv ID: 2403.11335
- Source URL: https://arxiv.org/abs/2403.11335
- Reference count: 40
- Authors: Fengran Mo; Bole Yi; Kelong Mao; Chen Qu; Kaiyu Huang; Jian-Yun Nie
- Primary result: LLM-generated conversational sessions effectively train conversational dense retrievers, achieving state-of-the-art performance on CAsT datasets

## Executive Summary
This paper addresses the problem of limited training data for conversational dense retrieval by proposing a framework to automatically generate conversational search sessions using large language models (LLMs). The ConvSDG framework generates session data at both dialogue-level and query-level using unsupervised and semi-supervised learning approaches. For dialogue-level generation, the LLM generates entire sessions from a topic description with pseudo-relevance feedback providing supervision. For query-level generation, the LLM rewrites queries to create diverse expressions of the same search intent, using existing relevance judgments as supervision. The generated data is used to fine-tune a conversational dense retriever based on ANCE. Experiments on four datasets show ConvSDG outperforms several strong baselines and even surpasses fully supervised models in many cases.

## Method Summary
ConvSDG generates conversational search sessions using LLMs in two ways: dialogue-level generation creates entire conversations from topic descriptions using pseudo-relevance feedback for supervision, while query-level generation rewrites existing queries to create diverse expressions of the same intent using existing relevance judgments. The generated sessions are then used to fine-tune a conversational dense retriever (ANCE) through contrastive learning. The framework operates in an unsupervised manner for dialogue-level generation and semi-supervised for query-level generation, requiring minimal human intervention while producing high-quality training data.

## Key Results
- ConvSDG achieves 59.5 MRR on CAsT-19 compared to 42.0 for ConvDR and 62.1 for ConvGQR
- Dialogue-level generation outperforms query-level generation in unsupervised settings
- The approach surpasses fully supervised models in many cases despite using less human-labeled data
- ConvSDG demonstrates consistent improvements across multiple conversational search datasets (CAsT-19, CAsT-20, CAsT-21, TopiOCQA)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLM-generated conversational sessions can serve as effective training data for conversational dense retrieval models.
- Mechanism: LLMs generate coherent, context-aware multi-turn conversations from a topic description, capturing the natural variability and context-dependency of real user queries. These generated sessions are then used to fine-tune dense retrievers.
- Core assumption: LLMs can generate high-quality conversational sessions that preserve semantic coherence and reflect realistic user behavior.
- Evidence anchors:
  - [abstract] "we propose ConvSDG, a simple yet effective framework to explore the feasibility of boosting conversational search by using LLM for session data generation."
  - [section 3.3] "we opt for dialogue-level session data generation, which involves creating the entire conversation session in one go by providing a specific topic description."
- Break condition: If LLM-generated sessions lack coherence or fail to preserve the context-dependency of real conversations, the model will not learn effective representations.

### Mechanism 2
- Claim: Pseudo-relevance feedback can substitute human relevance judgments for generating supervision signals in unsupervised settings.
- Mechanism: An off-the-shelf retriever retrieves top passages for each query turn in the generated session; a subset of these is treated as pseudo-relevant for training. This avoids the cost of human annotation.
- Core assumption: Top retrieved passages from a reasonable retriever are likely relevant to the query, making them suitable pseudo-labels.
- Evidence anchors:
  - [section 3.3] "we adopt the idea of pseudo-relevance feedback [39] to create pseudo supervision signals for each query turn."
  - [section 3.3] "we perform off-the-shelf retrieval on each query turn, selecting three passages from top-5 at random as pseudo-relevant documents."
- Break condition: If the retriever is poor or the query is too ambiguous, pseudo-labels will be noisy and degrade model performance.

### Mechanism 3
- Claim: Query-level augmentation with existing relevance judgments improves diversity without losing label quality.
- Mechanism: Each original query is rewritten multiple times to produce alternative expressions of the same intent; the original relevance labels are copied to the new queries. This enriches the training set while preserving supervision.
- Core assumption: Alternative natural language expressions of the same intent can be reliably generated and mapped to the same relevance labels.
- Evidence anchors:
  - [abstract] "For query-level generation, the LLM rewrites queries to create diverse expressions of the same search intent, using existing relevance judgments as supervision."
  - [section 3.4] "we prompt the LLM to rewrite each query, providing an alternative natural language expression with the same meaning."
- Break condition: If rewritten queries diverge semantically from the original intent, label quality degrades and model performance drops.

## Foundational Learning

- Concept: Dense retrieval and contrastive learning.
  - Why needed here: The model is trained using a contrastive loss that pulls relevant query-passage pairs together and pushes irrelevant pairs apart in the embedding space.
  - Quick check question: What loss function is used to train the conversational dense retriever in ConvSDG?
    - Answer: The contrastive learning loss (Equation 3 in the paper).

- Concept: Pseudo-relevance feedback and its role in unsupervised IR.
  - Why needed here: In the absence of human relevance judgments, pseudo-relevance feedback is used to generate supervision signals from retrieved passages.
  - Quick check question: How are pseudo-relevance feedback signals generated in ConvSDG?
    - Answer: By randomly selecting 3 passages from the top-5 retrieved for each query turn.

- Concept: Query rewriting and intent preservation.
  - Why needed here: Query-level augmentation relies on generating multiple expressions of the same search intent to increase data diversity.
  - Quick check question: Why is query rewriting important for conversational search data augmentation?
    - Answer: It increases diversity in query expressions while preserving the same underlying search intent, improving model robustness.

## Architecture Onboarding

- Component map: LLM (ChatGPT API) -> generates dialogue-level or query-level sessions -> Off-the-shelf retriever (ANCE) -> retrieves passages for pseudo-relevance feedback -> Dense encoders (ANCE backbone) -> encode queries and passages -> Fine-tuning pipeline -> trains conversational dense retriever with generated data

- Critical path: 1. Generate sessions via LLM 2. Produce supervision signals (PRF or copy existing labels) 3. Fine-tune conversational dense retriever on generated query-passage pairs

- Design tradeoffs:
  - Dialogue-level vs. query-level generation: Dialogue-level ensures coherence but may lack diversity; query-level increases diversity but depends on existing annotations.
  - Pseudo-relevance vs. manual labels: PRF is cheap but noisier; manual labels are expensive but cleaner.
  - Single-turn reformulation vs. full-context reformulation: Simpler but less context-aware; more context may introduce noise.

- Failure signatures:
  - Low coherence in generated sessions -> model learns poor representations
  - Noisy pseudo-labels -> degraded contrastive learning signal
  - Overfitting to augmented data -> poor generalization to unseen queries

- First 3 experiments:
  1. Generate dialogue-level sessions with LLM and evaluate coherence manually.
  2. Compare PRF-based supervision vs. no supervision in a small test run.
  3. Test query rewriting quality by comparing rewritten queries to original intent.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal method for generating pseudo-relevance feedback supervision signals in unsupervised dialogue-level session generation?
- Basis in paper: [explicit] The paper states "there is not a single fixed method for this purpose, and we leave further exploration of this area for future research" regarding pseudo-relevance feedback generation.
- Why unresolved: The paper only explores selecting 3 passages from top-5 retrieved results but acknowledges this could be improved.
- What evidence would resolve it: Comparative experiments testing different pseudo-relevance feedback methods (e.g., varying numbers of passages, using different retrieval models, incorporating query expansion) and their impact on downstream conversational retrieval performance.

### Open Question 2
- Question: How can we effectively filter or validate generated augmented queries to ensure they maintain the same search intent as original queries?
- Basis in paper: [inferred] The paper notes that "generated data points might alter the data distribution" in semi-supervised learning and mentions the need for "appropriate filtering mechanisms" but doesn't provide a solution.
- Why unresolved: While the paper generates augmented queries by rewriting, it doesn't address quality control or validation of whether these truly preserve search intent.
- What evidence would resolve it: Development and evaluation of filtering mechanisms (e.g., using LLM-based intent verification, human validation studies, or automatic quality metrics) and their impact on retrieval performance.

### Open Question 3
- Question: What is the relationship between the quality of automatically generated conversational sessions and the amount of topic information provided?
- Basis in paper: [explicit] The paper states "the topic information will help the generation process of augmented data to produce more relevant and topic-related data" but doesn't systematically explore this relationship.
- Why unresolved: The paper only mentions using topic information without investigating how varying amounts or quality of topic descriptions affect generated session quality.
- What evidence would resolve it: Controlled experiments varying the richness and specificity of topic descriptions and measuring the resulting session quality through both automatic metrics and human evaluation.

## Limitations

- The reliance on pseudo-relevance feedback for unsupervised learning introduces potential noise in supervision signals that could affect model performance.
- The approach focuses on specific datasets (CAsT-19, CAsT-20, CAsT-21, TopiOCQA) and may not generalize to all conversational search scenarios.
- The paper does not thoroughly evaluate the quality of generated sessions against human-annotated data, leaving uncertainty about semantic fidelity.

## Confidence

- High confidence in the experimental methodology and baseline comparisons
- Medium confidence in the scalability of the approach across diverse domains
- Medium confidence in the quality of pseudo-labels generated through PRF
- Low confidence in the semantic fidelity of LLM-generated sessions without human validation

## Next Checks

1. Conduct a human evaluation comparing the coherence and relevance of LLM-generated sessions versus human-annotated sessions from CAsT datasets
2. Measure semantic drift between original and rewritten queries using embedding similarity metrics to validate query-level augmentation quality
3. Test the approach on a domain-shifted conversational search task (e.g., medical or legal queries) to assess generalization beyond the evaluated datasets