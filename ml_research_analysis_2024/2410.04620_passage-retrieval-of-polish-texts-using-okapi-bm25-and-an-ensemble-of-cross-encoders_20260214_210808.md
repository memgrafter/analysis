---
ver: rpa2
title: Passage Retrieval of Polish Texts Using OKAPI BM25 and an Ensemble of Cross
  Encoders
arxiv_id: '2410.04620'
source_url: https://arxiv.org/abs/2410.04620
tags:
- no-ft
- dataset
- passages
- retrieval
- polish
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper presents the winning solution to the Poleval 2023 Task
  3: Passage Retrieval challenge, which involves retrieving relevant passages from
  Polish texts across three domains: trivia, legal, and customer support. The proposed
  method uses a two-stage approach: first, OKAPI BM25 is used to retrieve a subset
  of passages, and then an ensemble of multilingual Cross Encoder models is used to
  rerank the passages.'
---

# Passage Retrieval of Polish Texts Using OKAPI BM25 and an Ensemble of Cross Encoders

## Quick Facts
- arXiv ID: 2410.04620
- Source URL: https://arxiv.org/abs/2410.04620
- Reference count: 17
- Primary result: 69.36 NDCG@10 score on final test dataset

## Executive Summary
This paper presents the winning solution to the Poleval 2023 Task 3: Passage Retrieval challenge, which involves retrieving relevant passages from Polish texts across three domains: trivia, legal, and customer support. The proposed method uses a two-stage approach: first, OKAPI BM25 is used to retrieve a subset of passages, and then an ensemble of multilingual Cross Encoder models is used to rerank the passages. The ensemble includes models fine-tuned on the trivia domain and models used in a zero-shot manner for the legal and customer support domains. The solution achieved a score of 69.36 NDCG@10 on the final test dataset.

## Method Summary
The solution employs a two-stage retrieval pipeline combining sparse and neural approaches. First, OKAPI BM25 retrieves an initial set of passages from the document collection. Then, an ensemble of multilingual Cross Encoder models re-ranks these passages to improve relevance. The ensemble combines models fine-tuned on the trivia domain with zero-shot cross-encoders for legal and customer support domains, leveraging both domain-specific knowledge and multilingual capabilities to handle the Polish language texts effectively.

## Key Results
- Achieved 69.36 NDCG@10 score on final test dataset
- OKAPI BM25 + Cross Encoder ensemble outperformed translation-based approaches
- Zero-shot cross-encoders effective for legal and customer support domains

## Why This Works (Mechanism)
The two-stage approach combines the broad recall capability of sparse retrieval (OKAPI BM25) with the semantic understanding of neural re-ranking (Cross Encoders). OKAPI BM25 efficiently narrows down the candidate set from potentially thousands of passages to a manageable subset, while the Cross Encoder ensemble captures nuanced semantic relationships that traditional term-matching cannot. The ensemble strategy allows domain-specific fine-tuning where data is available (trivia) while maintaining zero-shot performance where fine-tuning data is limited (legal, customer support), providing a flexible framework that adapts to varying domain characteristics.

## Foundational Learning

**OKAPI BM25** - A probabilistic ranking function that scores documents based on term frequency and inverse document frequency. Needed for efficient initial retrieval with strong baseline performance. Quick check: Verify IDF values are properly calculated for Polish language corpus.

**Cross Encoder** - Neural architecture that processes query and passage together to produce a relevance score. Needed for capturing complex semantic relationships beyond keyword matching. Quick check: Confirm model can handle Polish text effectively.

**Ensemble Learning** - Technique combining multiple models to improve overall performance. Needed to leverage strengths of different domain-specific and zero-shot models. Quick check: Ensure proper weighting scheme for combining model predictions.

**NDCG@10** - Normalized Discounted Cumulative Gain at position 10, measuring ranking quality. Needed for evaluating retrieval effectiveness where top results matter most. Quick check: Verify relevance judgments are correctly applied.

**Zero-shot Learning** - Model inference without domain-specific fine-tuning. Needed for domains with limited training data while maintaining performance. Quick check: Test model generalization across unseen domains.

## Architecture Onboarding

**Component Map**: Query -> OKAPI BM25 -> Initial Candidates -> Cross Encoder Ensemble -> Reranked Results

**Critical Path**: The critical path flows from initial retrieval through reranking to final output. OKAPI BM25 must complete first to generate candidates for the Cross Encoder ensemble. The ensemble's reranking directly determines final rankings and thus the NDCG@10 score.

**Design Tradeoffs**: The approach trades computational efficiency for accuracy by using expensive Cross Encoders only on a subset of passages rather than the entire collection. This two-stage design enables practical deployment while maintaining strong performance. The ensemble approach balances domain-specific fine-tuning with zero-shot generalization.

**Failure Signatures**: Performance degradation occurs when OKAPI BM25 fails to retrieve relevant passages in the initial stage, when Cross Encoders cannot capture domain-specific semantics, or when ensemble weighting is suboptimal. Translation-based approaches were explicitly tested and found ineffective, indicating language-specific models are crucial.

**First Experiments**:
1. Test OKAPI BM25 retrieval quality independently on each domain
2. Evaluate individual Cross Encoder model performance on reranking task
3. Analyze ensemble performance with different model combinations and weights

## Open Questions the Paper Calls Out
None

## Limitations
- Effectiveness demonstrated only on Polish language texts across three specific domains
- Cross Encoder ensemble computationally expensive, limiting scalability to larger collections
- Limited ablation studies and experimental details for translation-based and bi-encoder approaches
- Lack of comparison with more recent dense retrieval methods

## Confidence
**Major claim clusters confidence:**
- Effectiveness of OKAPI BM25 + Cross Encoder ensemble for Polish passage retrieval: **High** (validated on official test set)
- Translation-based approaches are not beneficial for this task: **Medium** (limited experimental details provided)
- Cross Encoders outperform Bi Encoders for this specific task: **Medium** (only briefly mentioned without detailed comparison)

## Next Checks
1. Evaluate the ensemble approach on additional languages and domains to assess generalizability beyond Polish texts
2. Conduct comprehensive ablation studies comparing different ensemble combinations, including varying numbers of cross-encoder models and different reranking depths
3. Test the approach against modern dense retrieval methods (e.g., SPLADE, COIL) that were not included in the original comparison