---
ver: rpa2
title: 'Skywork-Reward: Bag of Tricks for Reward Modeling in LLMs'
arxiv_id: '2410.18451'
source_url: https://arxiv.org/abs/2410.18451
tags:
- reward
- preference
- arxiv
- data
- dataset
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces the Skywork-Reward dataset and models for
  reward modeling in large language models. The key idea is curating a high-quality,
  compact preference dataset (80K pairs) using targeted data selection and filtering
  strategies, focusing on publicly available sources and prioritizing pairs from stronger
  models and specific task categories.
---

# Skywork-Reward: Bag of Tricks for Reward Modeling in LLMs

## Quick Facts
- arXiv ID: 2410.18451
- Source URL: https://arxiv.org/abs/2410.18451
- Reference count: 9
- Top RewardBench ranking: Skywork-Reward-Gemma-2-27B achieved first place

## Executive Summary
This paper introduces the Skywork-Reward dataset and models for reward modeling in large language models. The key contribution is curating a high-quality, compact preference dataset (80K pairs) using targeted data selection and filtering strategies, focusing on publicly available sources and prioritizing pairs from stronger models and specific task categories. The Skywork-Reward models, trained using the Bradley-Terry loss on this dataset, achieved top rankings on the RewardBench leaderboard, with Skywork-Reward-Gemma-2-27B securing the first position and demonstrating superior performance across multiple categories, particularly in the challenging Chat Hard category. The work demonstrates that careful curation of smaller, high-quality datasets can outperform larger, less focused ones in reward modeling tasks.

## Method Summary
The Skywork-Reward dataset was created by curating and filtering publicly available preference datasets including HelpSteer2, OffsetBias, WildGuardMix, and Magpie series. The curation process involved selecting data synthesized by stronger models, focusing on specific task categories, and filtering for high-quality examples using ArmoRM scores. Reward models were trained using the Bradley-Terry loss function on this curated dataset, with base models including Meta-Llama-3.1-8B-Instruct and Gemma-2-27B-it. The models were trained for 2 epochs using AdamW optimizer with cosine learning rate schedules, and evaluated on the RewardBench benchmark.

## Key Results
- Skywork-Reward-Gemma-2-27B achieved first place on the RewardBench leaderboard
- The curated 80K dataset outperformed larger datasets (378K pairs) in reward modeling tasks
- Bradley-Terry loss consistently outperformed alternative loss functions including Focal Loss, Hinge Loss, and Cross-Entropy
- Superior performance was demonstrated across Chat, Chat Hard, Safety, and Reasoning categories

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Smaller, high-quality datasets can outperform larger, noisier ones in reward modeling
- Mechanism: Targeted data selection and filtering strategies prioritize preference pairs that contribute most effectively to improving model performance, focusing on publicly available sources and specific task categories
- Core assumption: Quality of preference pairs is more important than quantity for effective reward modeling
- Evidence anchors:
  - [abstract]: "curating a high-quality, compact preference dataset (80K pairs) using targeted data selection and filtering strategies"
  - [section 3.2]: Detailed description of data selection and filtering techniques for Magpie and WildGuardMix datasets
  - [corpus]: Weak evidence - corpus neighbors do not directly support this specific mechanism
- Break condition: If the filtering process removes too many valuable pairs or introduces selection bias

### Mechanism 2
- Claim: The Bradley-Terry loss consistently outperforms alternative loss functions in reward modeling tasks
- Mechanism: The vanilla Bradley-Terry loss maximizes the reward difference between pairwise comparisons, effectively capturing preference differences without overfitting
- Core assumption: Maximizing margin between chosen and rejected responses is the most effective approach for reward modeling
- Evidence anchors:
  - [abstract]: "the vanilla Bradley-Terry loss (Bradley and Terry, 1952; Ouyang et al., 2022) consistently outperforms alternative approaches"
  - [section 3.3.1]: Experimental results showing Bradley-Terry loss outperforms other loss functions including Focal Loss, Hinge Loss, and Cross-Entropy
  - [corpus]: Weak evidence - corpus neighbors do not directly support this specific mechanism
- Break condition: If alternative loss functions are better suited for specific types of preference data or tasks

### Mechanism 3
- Claim: Prioritizing data synthesized by stronger models improves reward model performance
- Mechanism: Data generated by larger or stronger models (e.g., Llama 3.1 405B vs Llama 3 8B) is associated with higher-quality outputs and more reliable preference signals
- Core assumption: Larger models produce more reliable and higher-quality preference data
- Evidence anchors:
  - [section 3.2.1]: Description of prioritizing data synthesized by stronger models and adjusting scores to account for rating bias
  - [corpus]: Weak evidence - corpus neighbors do not directly support this specific mechanism
- Break condition: If the assumption about larger models producing better data is incorrect or if rating bias cannot be adequately addressed

## Foundational Learning

- Concept: Bradley-Terry model and pairwise ranking loss
  - Why needed here: The paper relies on this model as the primary loss function for training reward models
  - Quick check question: What is the mathematical formulation of the Bradley-Terry loss and how does it differ from other ranking losses?

- Concept: Data curation and filtering techniques
  - Why needed here: The paper's main contribution involves developing effective strategies for curating high-quality preference datasets
  - Quick check question: What are the key steps in the data selection and filtering process described in section 3.2?

- Concept: Reward modeling in reinforcement learning from human feedback (RLHF)
  - Why needed here: Understanding the broader context of reward modeling and its role in aligning LLMs with human preferences
  - Quick check question: How do reward models fit into the overall RLHF pipeline and what are their primary functions?

## Architecture Onboarding

- Component map: HelpSteer2 -> OffsetBias -> WildGuardMix -> Magpie series -> Data selection and filtering -> Skywork-Reward dataset -> Meta-Llama-3.1-8B-Instruct/Gemma-2-27B-it with reward head -> Bradley-Terry loss training -> RewardBench evaluation

- Critical path:
  1. Curate and filter preference data
  2. Prepare training data mixture
  3. Initialize base models with reward heads
  4. Train models using Bradley-Terry loss
  5. Evaluate on RewardBench
  6. Analyze results and iterate

- Design tradeoffs:
  - Dataset size vs. quality: Smaller, carefully curated datasets vs. larger, noisier ones
  - Model size: Gemma-2-27B vs. Llama-3.1-8B performance trade-offs
  - Data sources: Balancing between synthetic and human-annotated data
  - Loss functions: Bradley-Terry vs. alternative approaches

- Failure signatures:
  - Overfitting to specific task categories
  - Bias introduced by filtering criteria
  - Poor generalization to unseen preference types
  - Inconsistent performance across different evaluation categories

- First 3 experiments:
  1. Ablation study on dataset size: Compare performance of models trained on full 378K vs. curated 80K datasets
  2. Loss function comparison: Train identical models using different loss functions (Bradley-Terry, Focal Loss, Hinge Loss) and compare performance
  3. Data source impact: Train models using different combinations of the four main data sources to identify most impactful components

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal margin parameter for loss functions like hinge loss and margin MSE in reward modeling?
- Basis in paper: [explicit] The paper states that "we experimented with several alternative loss functions, each designed to increase or maximize the margin between the chosen and rejected responses" and tested hinge loss and margin MSE but found no performance improvements.
- Why unresolved: The paper only tested a few margin values without systematically exploring the optimal margin parameter for these loss functions.
- What evidence would resolve it: A comprehensive ablation study testing different margin values for hinge loss and margin MSE on the RewardBench benchmark.

### Open Question 2
- Question: How does prompt contamination affect reward model performance beyond RewardBench evaluation?
- Basis in paper: [explicit] The paper discusses contamination issues and shows that removing contaminated prompts improved RewardBench scores, but raises questions about whether the original contamination reflects genuine preferences.
- Why unresolved: The paper only examines contamination effects on RewardBench scores and doesn't investigate how it impacts real-world RLHF applications or other evaluation metrics.
- What evidence would resolve it: Experiments comparing reward models trained on contaminated vs. decontaminated data in actual RLHF pipelines and other evaluation benchmarks.

### Open Question 3
- Question: What is the optimal data selection strategy for creating high-quality preference datasets?
- Basis in paper: [inferred] The paper demonstrates that their data selection strategy (prioritizing data from stronger models, focusing on specific task categories) improved performance, but acknowledges that "we do not claim it to be the 'optimal' data selection method."
- Why unresolved: The paper only explores one data selection approach and doesn't systematically compare different strategies or determine optimal selection criteria.
- What evidence would resolve it: Comparative studies testing different data selection strategies (e.g., different model prioritization schemes, task category distributions) on reward model performance.

## Limitations
- Claims about dataset curation effectiveness are based on performance against a single benchmark (RewardBench)
- Filtering and selection strategies lack complete transparency in their implementation
- The study doesn't address potential bias introduced by the data selection process

## Confidence
- **High Confidence:** The experimental methodology and benchmark results are clearly presented and reproducible
- **Medium Confidence:** Claims about dataset size reduction maintaining performance are supported by data but specific filtering criteria's impact remains partially unexplored
- **Medium Confidence:** The assertion that stronger models produce higher-quality preference data is plausible but not definitively proven

## Next Checks
1. Run the contamination detection script on the RewardBench test set with the Skywork-Reward dataset to quantify potential overlap and assess whether reported performance gains might be partially inflated by data leakage.

2. Evaluate the trained models on at least two additional reward modeling benchmarks beyond RewardBench to verify that the performance improvements generalize across different test distributions and task types.

3. Conduct systematic ablation studies varying the filtering thresholds and selection criteria to determine which specific curation decisions contribute most to the performance gains, and test whether the 80K dataset size is optimal or whether smaller/larger curated datasets might perform better.