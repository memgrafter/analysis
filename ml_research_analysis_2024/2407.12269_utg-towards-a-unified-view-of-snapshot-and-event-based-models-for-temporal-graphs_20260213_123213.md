---
ver: rpa2
title: 'UTG: Towards a Unified View of Snapshot and Event Based Models for Temporal
  Graphs'
arxiv_id: '2407.12269'
source_url: https://arxiv.org/abs/2407.12269
tags:
- temporal
- graph
- time
- snapshot
- graphs
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces UTG, a unified framework that bridges the
  gap between snapshot-based and event-based temporal graph models, enabling both
  types to be applied to both continuous-time and discrete-time dynamic graphs. The
  authors propose a novel UTG training procedure that enhances snapshot-based models
  for streaming evaluation, where newly observed information can be incorporated into
  predictions.
---

# UTG: Towards a Unified View of Snapshot and Event Based Models for Temporal Graphs

## Quick Facts
- arXiv ID: 2407.12269
- Source URL: https://arxiv.org/abs/2407.12269
- Authors: Shenyang Huang; Farimah Poursafaei; Reihaneh Rabbany; Guillaume Rabusseau; Emanuele Rossi
- Reference count: 40
- Primary result: UTG framework unifies snapshot-based and event-based temporal graph models, showing snapshot-based models with UTG training achieve competitive performance while being significantly faster in inference.

## Executive Summary
This paper introduces UTG (Unified Temporal Graph), a framework that bridges the gap between snapshot-based and event-based temporal graph models, enabling both to handle both continuous-time and discrete-time dynamic graphs. The authors propose UTG training with truncated backpropagation through time (TBTT) to improve snapshot-based model performance in streaming settings, and input/output mappers to convert between CTDG and DTDG representations. Experiments on five DTDG and three CTDG datasets demonstrate that while event-based models like NAT and DyGFormer achieve the best performance by leveraging joint neighborhood structural features, snapshot-based models with UTG training achieve competitive results with an order of magnitude faster inference time.

## Method Summary
UTG provides a unified framework with input and output mappers to convert temporal graphs between continuous-time (CTDG) and discrete-time (DTDG) representations. The framework enables both snapshot-based and event-based models to be applied to both graph types. UTG training implements truncated backpropagation through time (TBTT) with window size 1, backpropagating loss at each snapshot rather than accumulating over the entire sequence. This addresses the vanishing gradient problem in snapshot-based models. The output mapper uses zero-order hold interpolation to convert discrete predictions to continuous timestamps, allowing snapshot-based models to handle continuous-time tasks.

## Key Results
- Snapshot-based models with UTG training achieve competitive performance with event-based models while being ~10x faster in inference
- Top-performing models (NAT, DyGFormer) leverage joint neighborhood structural features for superior performance
- Event-based models require careful batch handling on DTDGs to prevent data leakage from simultaneous edges
- UTG successfully unifies both model types and data formats for temporal link prediction tasks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: UTG training with truncated backpropagation through time (TBTT) alleviates vanishing gradients in snapshot-based models.
- Mechanism: Standard snapshot-based training accumulates loss over the entire sequence before backpropagation, equivalent to classical backpropagation through time (BPTT). UTG training backpropagates at each snapshot (window size 1), matching TBTT.
- Core assumption: The vanishing gradient problem is the bottleneck in snapshot-based models; TBTT improves gradient flow.
- Evidence anchors:
  - [section] "By considering the temporal graph as a sequence (of snapshots), the link prediction problem can be seen as a time series prediction task... From this perspective, backpropagating the loss at each snapshot in UTG training can be interpreted as using the Truncated Backpropagation Through Time (TBTT) algorithm to train the model, with a window size of one."
  - [abstract] "We also propose a novel UTG training procedure to boost the performance of snapshot-based models in the streaming setting."
- Break condition: If the sequence length is short enough that vanishing gradients are not an issue, TBTT would offer no advantage.

### Mechanism 2
- Claim: Zero-order hold mapping allows snapshot-based models to handle continuous-time tasks.
- Mechanism: Snapshot-based models implicitly predict for the next snapshot. By broadcasting predictions across the snapshot duration using zero-order hold, these predictions can be aligned to continuous timestamps.
- Core assumption: The event to predict next is within the next snapshot, and the continuous task can tolerate this discretization.
- Evidence anchors:
  - [section] "By applying zero-order hold for snapshot-based models, the predictions can now be broadcasted for a period of time (specifically for the duration of a given snapshot [τi, τj]). Therefore, it is now possible to utilize snapshot-based models on continuous-time dynamic graphs."
  - [abstract] "The UTG output mapper transforms the prediction of the model to the required time granularity of the task."
- Break condition: If events occur between snapshots (time gaps), zero-order hold will misalign predictions.

### Mechanism 3
- Claim: Avoiding batch splitting on DTDGs prevents data leakage in streaming evaluation.
- Mechanism: Event-based models expect fixed-size batches. Without precautions, DTDG snapshots (all edges share the same timestamp) could be split across batches, causing edges to be predicted using other simultaneous edges.
- Core assumption: Event-based models update memory after each batch, so simultaneous edges in the same snapshot must stay together.
- Evidence anchors:
  - [section] "Event-based models often receive batches of events (or edges) with a fixed dimension as inputs... In discrete-time dynamic graphs, all edges in a snapshot have the same timestamp and are assumed to arrive simultaneously... To avoid data leakage on DTDGs, we ensure that each snapshot is contained in a single batch for event-based models."
  - [abstract] "UTG has two key components: input mapper and output mapper."
- Break condition: If memory constraints force splitting, delayed memory update or gradient accumulation must be used.

## Foundational Learning

- Concept: Continuous vs discrete-time dynamic graphs
  - Why needed here: UTG must convert between CTDG (event stream) and DTDG (snapshot sequence); understanding their differences is foundational to input mapping.
  - Quick check question: What is the main structural difference between a CTDG and a DTDG representation?
- Concept: Truncated backpropagation through time
  - Why needed here: UTG training leverages TBTT to mitigate vanishing gradients; knowing how TBTT differs from full BPTT is essential for understanding performance gains.
  - Quick check question: How does TBTT with window size 1 differ from classical BPTT in gradient flow?
- Concept: Zero-order hold interpolation
  - Why needed here: UTG output mapper uses zero-order hold to convert discrete predictions to continuous-time outputs; understanding this ensures correct temporal alignment.
  - Quick check question: In zero-order hold, what value is assigned to the continuous output between two discrete sample points?

## Architecture Onboarding

- Component map: Input mapper -> Model (snapshot-based/event-based) -> Output mapper -> Task format
- Critical path:
  1. Input mapper transforms data → model input
  2. Model processes → predictions
  3. Output mapper transforms predictions → task format
  4. UTG training updates model state at each step
- Design tradeoffs:
  - Snapshot-based models: Faster inference, but limited temporal granularity unless discretized finely
  - Event-based models: High accuracy, but slower inference and higher memory usage
  - TBTT vs BPTT: TBTT reduces memory and gradient issues but may lose long-term dependencies
- Failure signatures:
  - Data leakage: Observed when DTDG snapshots are split into multiple batches for event-based models
  - Time gaps: Occur when discretization is too coarse, leaving empty snapshots
  - Vanishing gradients: If TBTT is not applied, snapshot-based models may fail to learn long sequences
- First 3 experiments:
  1. Apply UTG input mapper to convert a CTDG to DTDG and run a snapshot-based model (e.g., GCN)
  2. Test UTG training on a snapshot-based model with streaming evaluation to verify performance gain
  3. Apply UTG output mapper with zero-order hold to a snapshot-based model on a continuous-time dataset

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions in the provided text.

## Limitations
- The comparison between snapshot-based and event-based models is limited by the small number of datasets (8 total) and specific models chosen
- The effectiveness of UTG training depends on vanishing gradients being the bottleneck, but lacks direct empirical validation
- Zero-order hold may introduce temporal misalignment for datasets with irregular time gaps between events

## Confidence
- High confidence: The basic UTG framework design (input/output mappers) is sound and well-motivated
- Medium confidence: The mechanism by which UTG training improves performance (via TBTT) is theoretically plausible but lacks direct empirical validation
- Low confidence: The claim that UTG eliminates the need to choose between model types may overstate practical implications

## Next Checks
1. Perform gradient norm analysis comparing standard BPTT vs TBTT training on snapshot-based models to empirically verify the vanishing gradient mechanism
2. Test UTG with different discretization granularities on continuous-time datasets to measure the impact of zero-order hold temporal misalignment
3. Evaluate additional model architectures beyond the five top-performing ones to assess whether the performance gap persists across different architectures