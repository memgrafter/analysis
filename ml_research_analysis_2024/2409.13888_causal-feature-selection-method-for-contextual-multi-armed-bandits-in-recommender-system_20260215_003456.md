---
ver: rpa2
title: Causal Feature Selection Method for Contextual Multi-Armed Bandits in Recommender
  System
arxiv_id: '2409.13888'
source_url: https://arxiv.org/abs/2409.13888
tags:
- feature
- features
- selection
- contextual
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces two model-free feature selection methods
  for contextual multi-armed bandits (CMABs) in recommender systems, addressing the
  limitation of traditional feature selection approaches that focus on outcome correlation
  rather than heterogeneous treatment effects (HTE) across arms. The proposed Heterogeneous
  Incremental Effect (HIE) method measures a feature's ability to induce changes in
  the optimal arm, while Heterogeneous Distribution Divergence (HDD) quantifies its
  impact on reward distribution divergence across arms.
---

# Causal Feature Selection Method for Contextual Multi-Armed Bandits in Recommender System

## Quick Facts
- arXiv ID: 2409.13888
- Source URL: https://arxiv.org/abs/2409.13888
- Authors: Zhenyu Zhao; Yexi Jiang
- Reference count: 15
- This paper introduces two model-free feature selection methods (HIE and HDD) that identify heterogeneous treatment effect features in contextual multi-armed bandits, achieving higher rewards than non-HTE features while requiring only 2.7 seconds per trial versus 82.5-665.5 seconds for model-based approaches.

## Executive Summary
This paper addresses a critical limitation in recommender systems: traditional feature selection methods focus on outcome correlation rather than identifying features that cause heterogeneous treatment effects (HTE) across different arms in contextual multi-armed bandits (CMABs). The authors propose two model-free methods - Heterogeneous Incremental Effect (HIE) and Heterogeneous Distribution Divergence (HDD) - that measure a feature's ability to induce changes in the optimal arm and affect reward distribution divergence across arms, respectively. These methods are computationally efficient, robust to model misspecification, and adaptable to various feature types. Evaluated on both synthetic data with known ground truth and real-world data from an online recommender system, the methods consistently identify influential HTE features, leading to higher CMAB rewards compared to non-HTE features.

## Method Summary
The paper introduces two model-free feature selection methods for CMABs: HIE measures a feature's ability to induce changes in the optimal arm by partitioning feature space into bins and calculating reward differences, while HDD quantifies impact on reward distribution divergence across arms using KL divergence. Both methods are computationally efficient (2.7 seconds per trial) and robust to model misspecification. The combined score balances both aspects through a weighted sum of normalized HIE and HDD scores. The methods were evaluated on synthetic data (50,000 samples, 3 arms, 10 features) and real-world data (600,000 samples, 4 variants) using LinUCB, Nonlinear LinUCB, and Cohort-based Thompson Sampling algorithms.

## Key Results
- HIE and HDD consistently identified influential HTE features in synthetic data with known ground truth, leading to higher CMAB rewards
- Model-free approach demonstrated significant computational efficiency (2.7 seconds vs 82.5-665.5 seconds for model-based methods)
- In real-world data evaluation, features selected by HIE and HDD led to higher offline matching method rewards compared to non-HTE features
- The combined score effectively balanced optimal arm changes and distributional divergence for feature ranking

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The HIE score identifies features that cause changes in the optimal arm across different contexts.
- Mechanism: By partitioning the feature space into bins and measuring the reward difference between the locally optimal arm and the globally optimal arm, HIE quantifies how much a feature contributes to shifting the optimal decision boundary.
- Core assumption: The optimal arm changes meaningfully across different bins of the feature, and the reward difference reflects the feature's causal impact.
- Evidence anchors:
  - [abstract]: "HIE quantifies a feature's value based on its ability to induce changes in the optimal arm"
  - [section]: "The winning arm in binğ‘ is denoted asğ‘¤ğ‘ = ğ‘ğ‘Ÿğ‘”ğ‘šğ‘ğ‘¥ğ‘– âˆˆ1,...,ğ‘˜ ğ‘ƒğ‘ğ‘– (ğ‘Œ = 1)"
  - [corpus]: Weak correlation - no direct mention of HIE or optimal arm shifting in neighboring papers
- Break condition: If the feature distribution is highly imbalanced across bins, or if the optimal arm remains constant across all bins, the HIE score loses discriminative power.

### Mechanism 2
- Claim: The HDD score captures how a feature affects the distributional divergence of rewards across arms.
- Mechanism: By computing the KL divergence of reward distributions both within each bin and across the entire dataset, HDD measures how much a feature causes the arms to behave differently in different contexts.
- Core assumption: Reward distributions across arms are sufficiently different to produce meaningful KL divergence, and this divergence reflects causal heterogeneity.
- Evidence anchors:
  - [abstract]: "HDD measures its impact on reward distribution divergence across arms"
  - [section]: "The HDD Score is calculated as the sample weighted sum of contextual KL divergence offset the non-contextual KL divergence"
  - [corpus]: No direct mention of KL divergence or distributional divergence in neighboring papers
- Break condition: If reward distributions are nearly identical across arms or bins, KL divergence approaches zero and HDD loses sensitivity.

### Mechanism 3
- Claim: The combined score balances the need for both optimal arm changes and distributional divergence.
- Mechanism: By normalizing and weighting HIE and HDD scores, the combined score captures both the directional change in optimal arms and the overall heterogeneity in reward distributions.
- Core assumption: Both optimal arm heterogeneity and distributional divergence are necessary for good CMAB performance, and they can be meaningfully combined through weighted summation.
- Evidence anchors:
  - [abstract]: "The combined score is a weighted sum of two normalized scores"
  - [section]: "ğ¹ ğ¼(ğ‘¥) = ğ›¼1 Â· Ë†ğ¹ ğ¼ğ» ğ¼ ğ¸(ğ‘¥) + ğ›¼2 Â· Ë†ğ¹ ğ¼ğ» ğ·ğ· (ğ‘¥)"
  - [corpus]: No mention of combined scores or weighted approaches in neighboring papers
- Break condition: If HIE and HDD are poorly correlated with actual CMAB performance, the combined score will not improve selection.

## Foundational Learning

- Concept: Heterogeneous Treatment Effects (HTE)
  - Why needed here: CMAB performance depends on features that cause different arms to be optimal in different contexts, not just features correlated with average outcomes
  - Quick check question: What distinguishes a feature that causes HTE from one that merely correlates with the outcome?

- Concept: KL Divergence
  - Why needed here: HDD uses KL divergence to quantify how reward distributions change across arms when conditioned on different feature values
  - Quick check question: How does KL divergence between two probability distributions capture their dissimilarity?

- Concept: Multi-armed Bandits
  - Why needed here: The entire feature selection problem is framed within the CMAB context, where the goal is to maximize cumulative reward by selecting the right arm based on contextual features
  - Quick check question: In a CMAB problem, what determines the "optimal" arm at any given decision point?

## Architecture Onboarding

- Component map: Feature binning â†’ HIE calculation â†’ HDD calculation â†’ Score normalization â†’ Combined score â†’ Feature ranking â†’ CMAB integration
- Critical path: The most time-sensitive path is the binning and HIE/HDD calculation, as these must be recomputed for each feature evaluation
- Design tradeoffs: Model-free approach trades off some accuracy for computational speed and robustness to model misspecification
- Failure signatures: Poor feature ranking despite known ground truth, computational time exceeding thresholds, or HIE/HDD scores showing no differentiation across features
- First 3 experiments:
  1. Run HIE on a synthetic dataset with known HTE features and verify it ranks them highest
  2. Run HDD on the same dataset and compare its ranking to HIE
  3. Combine HIE and HDD with different weightings and measure impact on CMAB reward

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do the HIE and HDD scores perform in online experiments compared to offline evaluations, particularly in terms of reward improvement and computational efficiency?
- Basis in paper: [explicit] The paper mentions an upcoming online experiment with four variants, each implementing a CMAB with a different feature, to evaluate whether features with high feature importance scores lead to better online CMAB reward.
- Why unresolved: The online experiment has not been conducted yet, and the paper only provides offline evaluation results.
- What evidence would resolve it: Results from the online experiment comparing the performance of features selected using HIE and HDD scores against those selected by other methods, including reward improvement and computational efficiency metrics.

### Open Question 2
- Question: How does the combined feature importance score (Equation 1) perform when varying the hyperparameters Î±1 and Î±2, and what is the optimal combination for different types of recommender systems?
- Basis in paper: [explicit] The paper introduces a combined feature importance score that balances heterogeneous incremental effect and distribution divergence, but does not provide an optimal combination of hyperparameters Î±1 and Î±2.
- Why unresolved: The paper does not provide empirical results or guidelines for selecting the optimal combination of Î±1 and Î±2 for different scenarios.
- What evidence would resolve it: Empirical results showing the performance of the combined score with different values of Î±1 and Î±2 across various recommender system types, leading to recommendations for optimal hyperparameter selection.

### Open Question 3
- Question: How do the HIE and HDD methods scale with an increasing number of features and arms in real-world large-scale recommender systems?
- Basis in paper: [inferred] The paper demonstrates the methods' effectiveness on synthetic data and real data from a small-scale experiment, but does not explore scalability to large-scale systems with many features and arms.
- Why unresolved: The paper's experiments are limited in scope and do not address the challenges of scaling the methods to large-scale systems.
- What evidence would resolve it: Performance metrics (e.g., computation time, reward improvement) of the HIE and HDD methods as the number of features and arms increases, demonstrating their scalability and effectiveness in large-scale recommender systems.

## Limitations

- The paper lacks detailed implementation specifications for critical components like binning approach and normalization methods, making faithful reproduction challenging
- Evaluation relies heavily on synthetic data with known ground truth, which may not fully capture real-world complexity
- While computational efficiency is demonstrated, there's no rigorous comparison of feature selection accuracy against established model-based methods
- The claim that traditional feature selection fails to capture HTE features needs stronger empirical validation through direct comparisons

## Confidence

- **High Confidence**: The computational efficiency claims (2.7 seconds vs 82.5-665.5 seconds) and the basic framework of using HIE and HDD scores for feature selection
- **Medium Confidence**: The effectiveness of HIE and HDD in identifying truly influential HTE features, as this relies on proper implementation details not fully specified
- **Low Confidence**: The generalizability of results from synthetic to real-world data without more extensive validation across different CMAB algorithms

## Next Checks

1. **Ground Truth Validation**: Implement the synthetic data generation with known HTE features and verify that HIE and HDD scores correctly rank the 5 known influential features above the 5 non-influential ones, using the exact parameters and evaluation metrics described.

2. **Implementation Verification**: Test the binning and normalization procedures on a simple synthetic dataset with clearly separable features to ensure the HIE and HDD calculations behave as expected before scaling to the full CMAB evaluation.

3. **Model-Free vs Model-Based Comparison**: Run a direct accuracy comparison between the proposed model-free approach and at least one model-based method (such as T-learner or X-learner) on the same synthetic dataset to quantify the tradeoff between computational efficiency and selection accuracy.