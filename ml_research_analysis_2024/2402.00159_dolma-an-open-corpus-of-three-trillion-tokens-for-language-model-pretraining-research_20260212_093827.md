---
ver: rpa2
title: 'Dolma: an Open Corpus of Three Trillion Tokens for Language Model Pretraining
  Research'
arxiv_id: '2402.00159'
source_url: https://arxiv.org/abs/2402.00159
tags:
- data
- tokens
- dolma
- total
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents Dolma, a three-trillion-token English corpus
  for language model pretraining research. It is built from diverse sources including
  web content, scientific papers, code, public-domain books, social media, and encyclopedic
  materials.
---

# Dolma: an Open Corpus of Three Trillion Tokens for Language Model Pretraining Research

## Quick Facts
- arXiv ID: 2402.00159
- Source URL: https://arxiv.org/abs/2402.00159
- Reference count: 40
- Result: Curated three-trillion-token English corpus for LM pretraining with improved downstream performance

## Executive Summary
This paper introduces Dolma, a three-trillion-token English corpus designed for language model pretraining research. The authors developed a comprehensive pipeline that processes diverse data sources including web content, scientific papers, code, books, and social media through multiple filtering stages: language identification, quality heuristics, content safety, PII removal, and deduplication. The corpus is built using a high-performance, open-source toolkit that enables reproducible large-scale data curation. Experimental results demonstrate that models trained on Dolma outperform those trained on other open corpora across multiple downstream tasks.

## Method Summary
The authors curated Dolma by combining data from six sources: Common Crawl web pages, Semantic Scholar papers, GitHub code, Project Gutenberg books, Reddit posts, and Wikipedia/Wikibooks. They developed the Dolma Toolkit to process this data through a pipeline that applies language filtering (FastText with 0.5 threshold), quality filtering (Gopher rules + C4 NoPunc), content filtering (Jigsaw-trained FastText for toxicity/PII), and hierarchical deduplication (URL, document, paragraph levels). The processed subsets were mixed and subsampled to create a ~3T token corpus, with test set decontamination using Bloom filters. The methodology was validated through ablation experiments training 1B-parameter models on different corpus versions.

## Key Results
- Successfully curated three-trillion-token English corpus from diverse sources
- Multi-stage deduplication removed 53.2% (URL), 14.9% (document), and 18.7% (paragraph) of content
- Quality filtering with C4 NoPunc outperformed both Gopher and C4 All on perplexity and downstream tasks
- Code inclusion in pretraining improved performance on reasoning tasks like bAbI and WebNLG

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Multi-stage deduplication effectively removes redundant content while preserving unique text
- Mechanism: Hierarchical deduplication operates at URL (53.2% removal), document (14.9%), and paragraph (18.7%) levels to target different redundancy types
- Core assumption: Different duplication types require different matching granularities for efficiency
- Evidence anchors:
  - [section]: "We perform three stages of deduplication: (i) Exact URL dedup filters 53.2% of documents..."
  - [corpus]: Weak - corpus provides overall removal counts but not duplicate rates per stage
- Break condition: Similar but non-identical pages (e.g., rewritten news articles) may evade paragraph-level deduplication

### Mechanism 2
- Claim: Heuristic quality filtering removes non-prose content without expensive model scoring
- Mechanism: Combines Gopher-style rules and C4 NoPunc to tag undesirable content for removal
- Core assumption: Heuristics can capture low-quality signals that harm training
- Evidence anchors:
  - [section]: "we find that C4 NoPunc on its own outperforms both C4 All as well as Gopher All..."
  - [corpus]: Weak - corpus provides character counts tagged but not direct quality improvement evidence
- Break condition: Thresholds too strict remove useful content; too lenient allow low-quality content

### Mechanism 3
- Claim: Content filtering removes toxic/PII without overly reducing dataset size
- Mechanism: FastText classifiers detect toxic content at sentence level; PII is masked/removed via regex
- Core assumption: Classifier thresholds balance safety and scale; regex PII detection is accurate enough
- Evidence anchors:
  - [section]: "Low Threshold (τ = 0.0004) removes more content (29.1–34.9%) but yields better downstream performance..."
  - [corpus]: Weak - corpus lacks removed content examples or classifier performance metrics
- Break condition: High false positive rates remove useful content; high thresholds allow harmful content

## Foundational Learning

- Concept: Tokenization and its impact on model training
  - Why needed here: GPT-NeoX tokenization affects text representation and model performance, especially for code/multilingual content
  - Quick check question: How does tokenization fertility differ between code and natural language in Dolma, and why does this matter?

- Concept: Language identification and filtering
  - Why needed here: English-only corpus requires accurate language ID to exclude non-English without losing valid English
  - Quick check question: What threshold is used for FastText language ID in Dolma, and how was its effectiveness validated?

- Concept: Deduplication strategies and their trade-offs
  - Why needed here: Removing redundant content is essential for efficient training; different levels target different duplication types
  - Quick check question: What are the three stages of deduplication in Dolma, and why is paragraph-level deduplication performed last?

## Architecture Onboarding

- Component map: Data ingestion -> language filtering -> quality filtering -> content filtering (toxic/PII) -> deduplication -> mixing/upsampling -> final corpus
- Critical path: Pipeline must process each document through all filtering stages before deduplication to maximize efficiency
- Design tradeoffs: Speed vs. accuracy in filtering (regex vs. model-based PII), strictness vs. inclusiveness in content filtering thresholds, granularity vs. processing cost in deduplication
- Failure signatures: Low-quality/toxic content slipping through, over-aggressive deduplication removing useful content, language ID misclassifying valid English
- First 3 experiments:
  1. Run full pipeline on small Common Crawl sample and verify removal counts match expected percentages
  2. Train 1B-parameter model on processed sample and evaluate perplexity on held-out English text
  3. Test language ID classifier on diverse English dialects to ensure no valid content is excluded

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What are the long-term effects of including code in language model pretraining on reasoning and code generation tasks?
- Basis in paper: [explicit] Paper shows code inclusion improves reasoning tasks like bAbI and WebNLG, but more research needed on full impact
- Why unresolved: Only initial evidence provided; suggests need for more research on long-term effects
- What evidence would resolve it: Experiments with larger models, diverse datasets, wider range of reasoning/code generation tasks over longer training periods

### Open Question 2
- Question: How do different data curation strategies impact the fairness and bias of language models?
- Basis in paper: [inferred] Paper discusses potential biases and need for further research to address these issues
- Why unresolved: Does not provide comprehensive analysis of fairness/bias implications of different curation strategies
- What evidence would resolve it: Experiments measuring fairness/bias of models trained on datasets curated using different strategies, analyzing impact on performance/behavior

### Open Question 3
- Question: What are the optimal mixing strategies for creating diverse and effective pretraining corpus?
- Basis in paper: [explicit] Discusses importance of source diversity and presents experimental results on mixing strategies
- Why unresolved: Only limited set of mixing strategies and effects provided; does not explore all possible strategies or domain-specific requirements
- What evidence would resolve it: Extensive experiments with wide range of mixing strategies, evaluating effects across different tasks/domains, developing guidelines for optimal corpora

## Limitations

- Exact implementation details of critical components like Gopher quality filtering rules are not provided
- Corpus lacks direct evidence of downstream model performance improvements from proposed filtering heuristics alone
- PII detection approach relies on regex patterns that may not generalize well across diverse data sources
- Language identification threshold of 0.5 chosen without extensive validation across English dialects

## Confidence

- High Confidence: Dataset construction methodology and pipeline architecture are clearly described and reproducible
- Medium Confidence: Effectiveness of multi-stage deduplication and quality filtering heuristics in improving performance is demonstrated but lacks detailed validation metrics
- Medium Confidence: Balance achieved between safety and dataset scale through content filtering thresholds is supported by removal rate data but lacks classifier performance specifics

## Next Checks

1. Replication of Quality Filter Impact: Train identical 1B-parameter models on corpus versions with only quality filtering stage enabled (Gopher rules vs. C4 NoPunc), then measure and compare perplexity and downstream task performance on held-out data.

2. Language ID Threshold Validation: Test FastText language identification classifier at 0.5 threshold on diverse set of English documents from different domains to quantify false positive/negative rates, particularly for non-standard English varieties.

3. PII Detection Accuracy Assessment: Apply PII detection pipeline to manually annotated sample dataset with known PII instances, measuring precision and recall to evaluate whether regex-based approach is sufficiently accurate for large-scale processing.