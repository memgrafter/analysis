---
ver: rpa2
title: A Survey on Sequential Recommendation
arxiv_id: '2412.12770'
source_url: https://arxiv.org/abs/2412.12770
tags:
- item
- sequential
- recommendation
- items
- user
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper provides a comprehensive survey of sequential recommendation
  (SR), categorizing SR models into four types based on how they construct item properties:
  pure ID-based, side information-based, multi-modal, and LLM-powered. It reviews
  traditional models, deep learning models (RNNs, CNNs, Transformers, GNNs, etc.),
  and recent advancements like generative recommendation, ultra-long sequence modeling,
  and data augmentation.'
---

# A Survey on Sequential Recommendation

## Quick Facts
- arXiv ID: 2412.12770
- Source URL: https://arxiv.org/abs/2412.12770
- Reference count: 40
- One-line primary result: Comprehensive survey categorizing sequential recommendation models by item property construction, reviewing traditional and deep learning approaches, and identifying key challenges and future directions.

## Executive Summary
This paper provides a comprehensive survey of sequential recommendation (SR) by organizing models into four categories based on how they construct item properties: pure ID-based, side information-based, multi-modal, and LLM-powered. The survey reviews traditional methods, deep learning models (RNNs, CNNs, Transformers, GNNs), and recent advancements like generative recommendation, ultra-long sequence modeling, and data augmentation. Empirical studies demonstrate that models leveraging side information and multi-modal features outperform pure ID-based methods, while LLM-powered models achieve further improvements. The paper also highlights open challenges and future research directions, including open-domain SR, data-centric approaches, and explainable recommendations.

## Method Summary
The survey systematically categorizes SR models by their item property construction approach, analyzing the strengths and limitations of each category. It reviews over 40 representative works, covering traditional methods like Markov chains and FPMC, deep learning architectures (RNNs, CNNs, Transformers, GNNs), and recent innovations in generative models and LLM integration. The methodology involves examining how different item representations capture user preferences and address challenges like cold-start, sparsity, and transferability. The survey also conducts empirical studies comparing model performance across different categories using standard metrics (HR@10, NDCG@10) on public datasets.

## Key Results
- SR models leveraging side information and multi-modal features outperform pure ID-based methods
- LLM-powered models achieve further improvements by capturing richer semantic information
- Generative approaches with semantic IDs can handle cold-start better than traditional methods
- Multi-modal SR enables cross-platform transferability through fine-tuning pre-trained modality encoders

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Categorizing SR models by item property construction clarifies how different approaches handle cold-start, transferability, and semantic richness
- Mechanism: Different item representations capture different facets of user preference - IDs for collaborative signals, side information for richer semantics, multi-modal for cross-platform transfer, LLMs for semantic reasoning
- Core assumption: The choice of item representation fundamentally shapes what aspects of user behavior the model can capture
- Evidence anchors: [abstract] "We study the SR problem from a new perspective (i.e., the construction of an item's properties)"; [section 3.3] "We classify SR models into four categories based on the properties of items"
- Break condition: If item properties are not the dominant factor in recommendation performance, or if hybrid approaches dominate in practice

### Mechanism 2
- Claim: Multi-modal SR models can achieve cross-platform transferability by replacing item IDs with universal modality features
- Mechanism: Pre-trained modality encoders (BERT, ResNet) provide universal representations that generalize across domains, enabling fine-tuning rather than retraining from scratch
- Core assumption: Modality features have a shared semantic space that transcends domain-specific encoding schemes
- Evidence anchors: [section 6.1] "By replacing item IDs with text descriptions and images, SR models can acquire rich semantic knowledge and enhance their transferability"
- Break condition: If modality features fail to capture collaborative signals or domain-specific nuances essential for recommendation

### Mechanism 3
- Claim: Generative SR with semantic IDs can handle cold-start better than traditional ID-based methods by sharing knowledge across similar items
- Mechanism: Semantic IDs are token sequences derived from item descriptions, allowing hierarchical semantic representation and reducing embedding table size
- Core assumption: Similar items share tokens in their semantic IDs, enabling knowledge transfer between items
- Evidence anchors: [section 6.2] "Using semantic IDs prevents the embedding table size from increasing linearly with the number of items"
- Break condition: If semantic ID generation fails to produce valid or meaningful IDs during prediction, or if collaborative signals are too important to ignore

## Foundational Learning

- Concept: Item representation in sequential recommendation
  - Why needed here: The paper's core contribution is organizing SR research by how items are represented. Understanding this is essential to grasp the taxonomy.
  - Quick check question: What are the four categories of item property construction mentioned in the paper, and how does each address cold-start problems differently?

- Concept: Cross-attention mechanism and feature fusion strategies
  - Why needed here: Multiple sections discuss how to combine item IDs with side information using different fusion approaches (early, late, hybrid).
  - Quick check question: What are the three main fusion strategies described for combining item embeddings with feature embeddings, and when might each be preferable?

- Concept: Contrastive learning in sequential recommendation
  - Why needed here: Several recent approaches use contrastive learning for data augmentation, denoising, and improving representation quality.
  - Quick check question: How does contrastive learning help improve sequential recommendation performance, and what are the different contexts in which it's applied according to the survey?

## Architecture Onboarding

- Component map: Pure ID-based (RNNs, CNNs, Transformers) -> Side information fusion (early/late/hybrid) -> Multi-modal (pre-trained encoders + backbone) -> LLM-powered (prompting, embedding generation, data augmentation) -> Generative (semantic ID generation + autoregressive decoding)
- Critical path: Understanding the taxonomy is the critical path. Start by grasping how item properties are constructed, then explore how each category handles the core challenges (cold-start, sparsity, transferability, ultra-long sequences).
- Design tradeoffs: Pure ID-based is simple but struggles with cold-start; side information helps but may introduce noise; multi-modal enables transfer but needs more compute; LLM-powered offers semantic richness but is expensive; generative can handle new items but may generate invalid IDs.
- Failure signatures: If a model category doesn't address its core challenges (e.g., multi-modal SR fails to capture collaborative signals, or LLM-powered models generate non-existent items), it indicates fundamental limitations of that approach.
- First 3 experiments:
  1. Implement a simple pure ID-based Transformer (SASRec) and evaluate on a standard dataset to establish baseline performance.
  2. Add side information fusion (early, late, and hybrid approaches) to compare how different fusion strategies affect recommendation quality.
  3. Replace item IDs with pre-trained BERT embeddings for a multi-modal approach and measure cross-platform transferability by fine-tuning on a different domain.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can sequential recommendation models effectively leverage multi-modal features (text, images, videos) while avoiding the loss of fine-grained semantic information that occurs when using semantic IDs?
- Basis in paper: [explicit] The paper discusses the trade-off between using multi-modal features and semantic IDs, noting that semantic IDs can capture different-grained semantic information but might damage fine-grained semantic information.
- Why unresolved: The paper highlights this challenge but doesn't provide a clear solution for balancing the benefits of both approaches.
- What evidence would resolve it: Empirical studies comparing the performance of models using different combinations of multi-modal features and semantic IDs on various datasets, particularly in cold-start and long-tail scenarios.

### Open Question 2
- Question: What are the most effective data augmentation techniques for ultra-long sequential recommendation, and how can they be applied without introducing bias or noise?
- Basis in paper: [explicit] The paper discusses the challenges of modeling ultra-long interaction sequences and mentions data augmentation techniques, but doesn't delve into their specific effectiveness for this problem.
- Why unresolved: The paper identifies the need for effective data augmentation but doesn't provide a comprehensive evaluation of different techniques.
- What evidence would resolve it: Comparative studies evaluating the impact of various data augmentation methods (e.g., masking, cropping, reordering) on the performance of ultra-long sequential recommendation models across different datasets.

### Open Question 3
- Question: How can large language models (LLMs) be effectively integrated into sequential recommendation models to capture both collaborative signals and semantic information, and what are the computational trade-offs?
- Basis in paper: [explicit] The paper discusses the use of LLMs for recommendation, learning semantic embeddings, and data generation, but doesn't provide a clear framework for integrating them with collaborative filtering techniques.
- Why unresolved: The paper highlights the potential of LLMs but doesn't offer a unified approach for combining their strengths with traditional recommendation methods.
- What evidence would resolve it: Empirical studies comparing the performance of models that integrate LLMs with collaborative filtering techniques, analyzing the impact on recommendation accuracy, computational efficiency, and explainability.

## Limitations
- The taxonomy may oversimplify the landscape where hybrid approaches combining multiple property types are increasingly common
- Most performance claims are based on individual studies rather than systematic comparisons across all four categories
- Cross-platform transferability claims for multi-modal approaches lack sufficient empirical validation
- Computational efficiency tradeoffs for LLM-powered and generative approaches are not adequately addressed

## Confidence
- High confidence in the taxonomy's logical coherence and descriptive accuracy
- Medium confidence in the relative performance claims between categories
- Low confidence in the generalizability of cross-platform transfer results and the practical viability of generative approaches at scale

## Next Checks
1. Conduct controlled experiments comparing pure ID-based, side information-based, and multi-modal approaches on the same dataset with identical hyperparameters to isolate the impact of property construction.
2. Test cross-platform transferability by training a multi-modal SR model on one domain (e.g., movies) and evaluating fine-tuning performance on a different domain (e.g., books).
3. Implement a semantic ID generation system and measure both its ability to handle cold-start items and its rate of generating invalid or nonsensical IDs during inference.