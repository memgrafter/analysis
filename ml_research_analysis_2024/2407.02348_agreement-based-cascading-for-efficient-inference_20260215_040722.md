---
ver: rpa2
title: Agreement-Based Cascading for Efficient Inference
arxiv_id: '2407.02348'
source_url: https://arxiv.org/abs/2407.02348
tags:
- inference
- cascade
- cascading
- ensembles
- accuracy
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the problem of efficient inference for machine
  learning models by proposing a simple adaptive inference technique called Agreement-Based
  Cascading (ABC). ABC builds a cascade of models of increasing size/complexity, using
  agreement between ensembles of models at each level as a basis for data-dependent
  routing.
---

# Agreement-Based Cascading for Efficient Inference

## Quick Facts
- arXiv ID: 2407.02348
- Source URL: https://arxiv.org/abs/2407.02348
- Reference count: 40
- Primary result: Agreement-Based Cascading (ABC) achieves 2-25x reduction in average price per token/request compared to state-of-the-art LLM cascades

## Executive Summary
This paper addresses the challenge of efficient inference for machine learning models by proposing Agreement-Based Cascading (ABC), a simple adaptive inference technique. ABC constructs cascades of models with increasing complexity, using agreement between ensembles of models at each level as a basis for data-dependent routing. The method leverages parallel inference execution to offset the cost of ensemble execution. Experiments across diverse language and vision tasks demonstrate that ABC reliably serves as a drop-in replacement for existing models, surpassing single models in both efficiency and accuracy.

## Method Summary
The method builds cascades of ensembles (CoE) where each tier consists of 2-5 models executed in parallel. Agreement between ensemble predictions determines whether to return the result or cascade to the next tier. A 2/3 majority agreement threshold is used for cascading decisions. The approach uses pre-trained models from HuggingFace Zoo and evaluates performance on CIFAR-10, ImageNet-1K, SWAG (MCQ), SST-2, and Twitter Financial News datasets using accuracy, FLOPs, inference latency, GPU cost, and communication cost as metrics.

## Key Results
- ABC achieves up to 14x reduction in communication costs for edge-to-cloud inference scenarios
- Cloud-based model serving costs reduced by 3x compared to baseline approaches
- 2-25x reduction in average price per token/request compared to state-of-the-art LLM cascades
- ABC consistently outperforms the best single model across diverse language and vision tasks

## Why This Works (Mechanism)

### Mechanism 1
Ensembles in the first cascade tier reduce variance and improve accuracy relative to single models, while still being cheaper than large models. By aggregating predictions from multiple smaller models, the ensemble's majority vote is more robust to individual model errors. This allows more examples to be correctly classified at the cheaper tier, reducing the need to invoke larger, more expensive models.

### Mechanism 2
The voting threshold for consistency can be tuned to balance accuracy and efficiency; lower thresholds trigger cascades less often, saving compute. By requiring only a majority (e.g., 2/3) rather than unanimous agreement, the cascade is less likely to defer to higher tiers, keeping inference in the cheaper ensemble tier more often.

### Mechanism 3
Parallel execution of ensemble models offsets the cost of running multiple models, making the ensemble tier nearly as fast as a single large model. Modern hardware and frameworks allow simultaneous inference across multiple models, so the time cost is dominated by the slowest model in the ensemble rather than the sum of all times.

## Foundational Learning

- Concept: Bayes optimal classifier and risk decomposition
  - Why needed here: Understanding that the deferral rule aims to approximate the Bayes optimal rule clarifies why ensemble agreement is a good heuristic
  - Quick check question: If you have access to the true conditional distribution P(y|x), what is the optimal deferral rule for a two-tier cascade?

- Concept: Ensemble variance reduction and diversity
  - Why needed here: The mechanism relies on ensembles reducing variance and improving accuracy; without this, the first tier wouldn't be better than single models
  - Quick check question: Why does averaging predictions from diverse models reduce variance more than averaging predictions from very similar models?

- Concept: Pareto efficiency and model selection
  - Why needed here: The cascade is built from a Pareto-optimal set of models; understanding this ensures that each tier offers a unique accuracy-cost trade-off
  - Quick check question: What does it mean for a model to be Pareto-optimal in the context of accuracy vs. computational cost?

## Architecture Onboarding

- Component map: Model zoo -> Ensemble tier (2-5 models) -> Deferral rule (2/3 agreement) -> Cascade controller -> Performance monitor
- Critical path: Input arrives -> Ensemble tier runs all models in parallel -> Majority vote and consistency check -> If consistent, return prediction; else, move to next tier -> Repeat until prediction or last tier reached
- Design tradeoffs: Ensemble size vs. cost (larger ensembles improve accuracy but increase compute), Consistency threshold vs. deferral rate (lower thresholds reduce cascading but may increase errors), Parallelism vs. resource usage (full parallelism is fastest but uses more GPU/memory)
- Failure signatures: High deferral rate (ensemble accuracy too low or threshold too strict), Low accuracy despite low cost (ensemble models are too similar), Slow inference (parallelism not enabled or slowest model in ensemble is too slow)
- First 3 experiments: 1) Baseline: run each model individually, measure accuracy and cost, identify Pareto set; 2) Single-tier ensemble: run ensembles of 2-5 models in parallel, tune consistency threshold, measure accuracy and cost; 3) Full cascade: build cascade with 2-3 tiers, measure overall accuracy, cost, and deferral rates, compare to best single model

## Open Questions the Paper Calls Out
- How can we design an ensemble strategy that eliminates incorrect consensus predictions while maintaining computational efficiency?
- What is the optimal cascade depth and ensemble size configuration for maximizing accuracy-efficiency trade-offs across different task domains?
- How can CoE be extended to handle open-ended generation tasks like those encountered with large language models?
- What is the impact of different model compression techniques on CoE performance, and which compressions work best for different cascade tiers?

## Limitations
- Efficiency gains may not hold across different model architectures beyond tested BERT, RoBERTa, XLNet, and ELECTRA variants
- Reliance on parallel execution infrastructure - benefits may not materialize where true parallel inference is unavailable
- Assumes ensembles can be constructed with sufficient diversity to achieve variance reduction without systematic analysis of ensemble diversity
- Claims about serving as a "drop-in replacement" oversell generality requiring careful tuning per task and deployment

## Confidence

**High confidence**: The fundamental mechanism of using ensemble agreement for data-dependent routing is theoretically sound and empirical demonstrations show consistent improvements in accuracy-efficiency trade-offs.

**Medium confidence**: Specific efficiency gains (14x communication cost reduction, 3x cloud rental cost savings) are likely task and deployment dependent.

**Low confidence**: Claims about ABC serving as a "drop-in replacement" oversells the generality - the method requires careful tuning of ensemble composition, consistency thresholds, and cascade structure for each specific task.

## Next Checks
1. **Ensemble Diversity Analysis**: Systematically measure and report the prediction diversity within ensembles across different model pairs, including pairwise agreement rates, variance in confidence scores, and correlation in prediction errors.

2. **Resource Contention Study**: Evaluate ABC performance under realistic resource constraints where parallel execution cannot be perfectly achieved, testing scenarios with limited GPU availability, memory contention, and sequential execution fallback.

3. **Architecture Transferability**: Test ABC with completely different model families (vision transformers, convolutional networks, different NLP architectures) on the same tasks to determine whether efficiency gains are architecture-specific or generalize across model types.