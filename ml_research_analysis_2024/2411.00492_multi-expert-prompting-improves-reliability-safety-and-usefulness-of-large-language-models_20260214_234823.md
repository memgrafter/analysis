---
ver: rpa2
title: Multi-expert Prompting Improves Reliability, Safety, and Usefulness of Large
  Language Models
arxiv_id: '2411.00492'
source_url: https://arxiv.org/abs/2411.00492
tags:
- answer
- prompting
- expert
- multi-expert
- experts
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Multi-expert Prompting is a novel extension of ExpertPrompting
  that significantly improves the reliability, safety, and usefulness of large language
  model (LLM) generation. It addresses the limitations of single-expert approaches
  by simulating multiple experts with diverse perspectives to respond to an input
  instruction, aggregating their responses, and selecting the best among individual
  and combined answers.
---

# Multi-expert Prompting Improves Reliability, Safety, and Usefulness of Large Language Models

## Quick Facts
- arXiv ID: 2411.00492
- Source URL: https://arxiv.org/abs/2411.00492
- Reference count: 40
- Multi-expert Prompting achieves state-of-the-art truthfulness, outperforming best baseline by 8.69% with ChatGPT

## Executive Summary
Multi-expert Prompting is a novel extension of ExpertPrompting that significantly improves the reliability, safety, and usefulness of large language model generation. It addresses the limitations of single-expert approaches by simulating multiple experts with diverse perspectives, aggregating their responses through a structured Nominal Group Technique framework, and selecting the best among individual and combined answers. Evaluations on multiple benchmarks show substantial improvements in truthfulness, factuality, informativeness, usefulness, and reductions in toxicity and hurtfulness.

## Method Summary
The method involves two main steps: generating n expert identities with one-sentence role descriptions tailored to the instruction, and using a seven-subtask aggregation framework based on the Nominal Group Technique to combine responses. The aggregation process identifies agreed viewpoints, conflicted viewpoints, resolves conflicts, collects isolated viewpoints, generates an aggregated response, and selects the best among individual and aggregated responses. The approach requires no manual prompt construction, is efficient and explainable, and performs best with three experts.

## Key Results
- Achieves state-of-the-art truthfulness, outperforming best baseline by 8.69% with ChatGPT
- Improves factuality, informativeness, and usefulness across multiple benchmarks
- Reduces toxicity and hurtfulness while maintaining response quality
- Requires no manual prompt construction and is efficient and explainable

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Aggregating responses through Nominal Group Technique (NGT) reduces individual expert bias and improves truthfulness.
- **Mechanism:** Systematically identifies agreed, conflicted, and isolated viewpoints, ensuring majority-supported facts are prioritized while conflicting opinions are resolved using model knowledge.
- **Core assumption:** The LLM can reliably distinguish between majority-supported facts and conflicts, and resolve conflicts accurately using its knowledge.
- **Evidence anchors:** [abstract] "This process is performed in a single chain of thoughts through our seven carefully designed subtasks derived from the Nominal Group Technique (NGT; Ven and Delbecq, 1974)"
- **Break condition:** If the LLM cannot accurately identify majority viewpoints or resolve conflicts, the aggregation fails and may propagate incorrect information.

### Mechanism 2
- **Claim:** Diverse expert roles generate complementary perspectives that improve response quality across multiple dimensions.
- **Mechanism:** By generating multiple experts with different domains, the system captures varied viewpoints that address different aspects of a question, ensuring comprehensive coverage.
- **Core assumption:** The LLM can generate distinct, relevant expert identities without significant overlap or hallucination.
- **Evidence anchors:** [abstract] "generating n expert identities with their concise, one-sentence role descriptions tailored to the instruction"
- **Break condition:** If generated experts are too similar or irrelevant, the diversity benefit disappears.

### Mechanism 3
- **Claim:** Selecting the best response from both individual and aggregated answers prevents degradation from poor aggregation.
- **Mechanism:** The final selection step compares the aggregated response against all individual expert responses, choosing the most factual and useful answer.
- **Core assumption:** At least one individual expert response will be better than the aggregated response when aggregation fails.
- **Evidence anchors:** [abstract] "Multi-expert Prompting further selects the best among individual expert answers and the aggregated one"
- **Break condition:** If all responses are poor quality, the selection step cannot improve outcomes.

## Foundational Learning

- **Concept: Nominal Group Technique (NGT)**
  - Why needed here: Provides structured framework for aggregating multiple viewpoints systematically rather than ad-hoc combination.
  - Quick check question: What are the four main steps of NGT and how do they map to Multi-expert Prompting's subtasks?

- **Concept: Chain-of-thought prompting**
  - Why needed here: Enables the LLM to perform complex multi-step reasoning required for the seven subtasks in a single turn.
  - Quick check question: How does chain-of-thought prompting differ from simple instruction following in this context?

- **Concept: Expert identity generation**
  - Why needed here: Creates diverse perspectives necessary for comprehensive coverage of complex questions.
  - Quick check question: Why does Multi-expert Prompting use one-sentence descriptions instead of paragraph-long descriptions like ExpertPrompting?

## Architecture Onboarding

- **Component map:** Expert generation module → Individual response generation → Seven-subtask aggregation pipeline → Best-response selection module → Output formatting wrapper
- **Critical path:** Expert generation → Individual responses → Seven subtasks (S1→S7) → Best selection → Final output
- **Design tradeoffs:**
  - Single-turn aggregation vs. iterative refinement (faster but potentially less accurate)
  - Equal weighting vs. expert credibility weighting (simpler but may not reflect real-world expertise)
  - One-sentence vs. detailed expert descriptions (more efficient but potentially less specific)
- **Failure signatures:**
  - Poor expert diversity → Similar viewpoints, missing perspectives
  - Aggregation errors → Conflicting viewpoints not resolved correctly
  - Selection bias → Consistently choosing wrong answer type
  - Hallucination → Experts or viewpoints that don't make sense
- **First 3 experiments:**
  1. Compare 1-expert vs 3-expert vs 5-expert configurations on TruthfulQA to find optimal expert count
  2. Test skipping individual subtasks (S1, S3, S4, S7) to validate their necessity
  3. Compare against ExpertPrompting with same number of experts to isolate aggregation benefit

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal number of experts to include in Multi-expert Prompting for different types of tasks?
- Basis in paper: [explicit] "Multi-expert Prompting performs best with three experts" from the abstract
- Why unresolved: The paper suggests three experts are optimal but doesn't provide systematic analysis across different task types or compare performance with varying numbers of experts for different domains.
- What evidence would resolve it: Comprehensive ablation studies testing Multi-expert Prompting with 2, 3, 4, and 5 experts across multiple task categories with statistical analysis of performance differences.

### Open Question 2
- Question: How does Multi-expert Prompting perform on tasks requiring specialized domain expertise beyond general knowledge?
- Basis in paper: [inferred] The paper focuses on general-purpose LLMs and doesn't address performance on highly specialized domains where LLM training data may be limited
- Why unresolved: The evaluations use general benchmarks but don't test Multi-expert Prompting on specialized domains like medical diagnosis, legal reasoning, or technical engineering problems.
- What evidence would resolve it: Systematic evaluation on specialized benchmark datasets or real-world domain-specific tasks, comparing performance against domain experts.

### Open Question 3
- Question: What are the computational trade-offs between Multi-expert Prompting and other response aggregation methods in terms of token usage and API costs?
- Basis in paper: [explicit] The paper mentions "cost analysis" and provides token usage data, but doesn't systematically compare computational efficiency against baselines
- Why unresolved: While the paper provides token usage for Multi-expert Prompting, it doesn't compare these costs against alternatives like AutoGen, Universal Self-consistency, or iterative refinement methods.
- What evidence would resolve it: Detailed comparative analysis of computational costs (tokens, API calls, latency) for Multi-expert Prompting versus competing methods across multiple tasks, normalized by output quality metrics.

## Limitations

- Performance gains may be model-specific rather than universal across different LLM architectures
- Computational efficiency comparisons with iterative approaches are not provided
- Lack of ablation studies showing which components of the seven-subtask pipeline are most critical

## Confidence

- **High confidence**: The improvement in truthfulness and reduction in toxicity metrics are well-supported by the benchmark results presented.
- **Medium confidence**: The mechanism by which Nominal Group Technique aggregation improves response quality is theoretically sound but lacks direct empirical validation of individual components.
- **Low confidence**: The universality of the "three experts is optimal" finding across different model architectures and task domains.

## Next Checks

1. **Component ablation study**: Systematically remove each of the seven subtasks to quantify their individual contributions to the overall performance improvement, determining whether simpler aggregation methods could achieve similar results.

2. **Cross-model validation**: Test Multi-expert Prompting on a diverse set of LLM architectures (including open-source models like LLaMA and Mistral) to verify that the performance gains are not specific to OpenAI models.

3. **Scalability analysis**: Evaluate the method with varying numbers of experts (1, 3, 5, 7, 10) across multiple task domains to validate the claimed optimal configuration and identify breaking points where aggregation quality degrades.