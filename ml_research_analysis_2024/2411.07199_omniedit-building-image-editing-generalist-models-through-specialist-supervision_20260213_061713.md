---
ver: rpa2
title: 'OmniEdit: Building Image Editing Generalist Models Through Specialist Supervision'
arxiv_id: '2411.07199'
source_url: https://arxiv.org/abs/2411.07199
tags:
- image
- editing
- object
- conference
- omni
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: OMNI-EDIT addresses the limitations of existing instruction-based
  image editing models, which suffer from biased synthetic data, poor data quality
  control, and limited support for varying resolutions and aspect ratios. The paper
  introduces a specialist-to-generalist learning framework that trains a generalist
  model using supervision from seven specialist models, each handling a specific editing
  task.
---

# OmniEdit: Building Image Editing Generalist Models Through Specialist Supervision

## Quick Facts
- arXiv ID: 2411.07199
- Source URL: https://arxiv.org/abs/2411.07199
- Authors: Cong Wei; Zheyang Xiong; Weiming Ren; Xinrun Du; Ge Zhang; Wenhu Chen
- Reference count: 31
- Primary result: Introduces OMNI-EDIT, a generalist image editing model trained via specialist supervision that outperforms existing models on automatic and human evaluations

## Executive Summary
OMNI-EDIT addresses the limitations of existing instruction-based image editing models by introducing a specialist-to-generalist learning framework. The model is trained using supervision from seven specialist models, each handling a specific editing task, combined with importance sampling based on large multimodal model scores for data quality control. The proposed EditNet architecture enhances editing success by allowing interaction between control and original branches through intermediate representations, enabling the model to handle diverse editing tasks across varying aspect ratios and high resolutions.

## Method Summary
OMNI-EDIT employs a specialist-to-generalist learning framework where a generalist model is trained using supervision from seven task-specific specialist models. The training data consists of 1.2M image-editing pairs curated from LAION-5B and OpenImageV6 with diverse aspect ratios and minimum 1 megapixel resolution. Importance sampling using GPT-4o scores (distilled to InternVL2) filters low-quality examples, retaining only those with scores ≥9. The EditNet architecture, built on Stable Diffusion 3 Medium, introduces control branch DIT blocks that interact with original DIT tokens and editing prompts, allowing adaptive adjustment of representations based on editing instructions.

## Key Results
- OMNI-EDIT outperforms existing models on both automatic (VIEScore) and human evaluations across seven editing tasks
- Achieves higher perceptual quality and semantic consistency scores, with overall score O = √SC × PQ
- Successfully handles diverse aspect ratios (1:1, 2:3, 3:2, 3:4, 4:3, 9:16, 16:9) and high-resolution images (minimum 1 megapixel)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Specialist-to-generalist supervision enables broader editing capabilities by leveraging task-specific expertise from multiple specialists
- Mechanism: The generalist model OMNI-EDIT is trained using supervision signals from seven specialist models, each handling a distinct editing task (object swap, object addition, object removal, attribute modification, background swap, environment change, and style transfer)
- Core assumption: The specialist models can generate high-quality demonstrations that accurately represent their respective editing tasks
- Evidence anchors:
  - [abstract] "OMNI-EDIT is trained by utilizing the supervision from seven different specialist models to ensure task coverage."
  - [section 3.2] Details the seven specialist models and their specific roles in generating training data
- Break condition: If specialist models fail to generate accurate demonstrations or if the generalist cannot effectively learn from multiple supervision signals

### Mechanism 2
- Claim: Importance sampling using large multimodal models (LMMs) improves data quality by filtering out low-quality synthetic examples
- Mechanism: Instead of using simple CLIP-score filtering, OMNI-EDIT employs GPT-4o to assign quality scores to synthesized samples, retaining only those with scores ≥ 9
- Core assumption: LMMs can reliably distinguish high-quality from low-quality image editing pairs
- Evidence anchors:
  - [abstract] "we utilize importance sampling based on the scores provided by large multimodal models (like GPT-4o) instead of CLIP-score to improve the data quality."
  - [section 3.3] Explains the distillation process from GPT-4o to InternVL2 for efficient scoring
- Break condition: If the distilled scoring model fails to maintain the accuracy of GPT-4o's evaluations or if the filtering threshold removes too many valid examples

### Mechanism 3
- Claim: EditNet architecture enhances editing success by allowing interaction between control and original branches through intermediate representations
- Mechanism: EditNet introduces control branch DIT blocks that interact with original DIT tokens and editing prompts, allowing adaptive adjustment of representations based on editing instructions
- Core assumption: Allowing interaction between control and original branches improves the model's ability to understand and execute editing tasks
- Evidence anchors:
  - [abstract] "we propose a new editing architecture called EditNet to greatly boost the editing success rate"
  - [section 4] Detailed comparison between EditNet and ControlNet, highlighting the advantages of intermediate representation interaction
- Break condition: If the additional complexity of EditNet does not translate to improved editing performance or if it introduces training instability

## Foundational Learning

- Concept: Diffusion models and their denoising process
  - Why needed here: OMNI-EDIT builds upon diffusion models as its base architecture, requiring understanding of how they reverse the diffusion process to generate images
  - Quick check question: How does a diffusion model reconstruct the original data from noisy latent representations?

- Concept: Supervised learning with paired data
  - Why needed here: The instruction-based image editing task is formulated as supervised learning using pairs of source images, edited images, and corresponding instructions
  - Quick check question: What is the objective function for training a diffusion model on instruction-based image editing pairs?

- Concept: Importance sampling and weighted loss functions
  - Why needed here: OMNI-EDIT uses importance sampling to weight training examples based on their quality scores, requiring understanding of how to incorporate these weights into the loss function
  - Quick check question: How does importance sampling modify the standard maximum likelihood training objective?

## Architecture Onboarding

- Component map: Specialist models -> Importance scoring (GPT-4o distilled to InternVL2) -> Filtered dataset -> EditNet (Stable Diffusion 3 Medium + Control branch DIT blocks) -> Generalist training
- Critical path: Specialist model generation → Importance scoring → EditNet training → Evaluation
- Design tradeoffs:
  - EditNet vs ControlNet: EditNet updates text representations and enables intermediate interaction but adds complexity
  - Specialist models vs single pipeline: Multiple specialists ensure coverage but increase implementation complexity
  - LMM scoring vs CLIP-score: Higher quality filtering but increased computational cost
- Failure signatures:
  - Poor editing performance may indicate specialist model failures or ineffective distillation of scoring function
  - Training instability may result from EditNet's complex interactions between branches
  - Low data quality despite LMM scoring may indicate threshold misconfiguration
- First 3 experiments:
  1. Verify specialist models can generate accurate demonstrations for their respective tasks
  2. Test distilled scoring model (InternVL2) against GPT-4o on a validation set
  3. Compare EditNet against ControlNet baseline on a subset of editing tasks

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal balance between specialist model diversity and generalist model performance in the specialist-to-generalist learning framework?
- Basis in paper: [explicit] The paper uses seven specialist models for different editing tasks and claims that this ensures task coverage
- Why unresolved: The paper does not explore whether using more or fewer specialist models would improve or degrade OMNI-EDIT's performance, or if certain tasks benefit more from specialization than others
- What evidence would resolve it: Comparative studies showing OMNI-EDIT's performance with different numbers of specialist models, or ablation studies demonstrating which tasks benefit most from specialization

### Open Question 2
- Question: How does the importance sampling threshold (score >= 9) affect the quality and diversity of the training data?
- Basis in paper: [explicit] The paper uses a threshold of 9 for filtering data based on LMM scores but does not explore alternative thresholds
- Why unresolved: The choice of threshold appears arbitrary and could significantly impact both the quality of the final model and the diversity of training examples
- What evidence would resolve it: Experiments comparing OMNI-EDIT trained with different importance sampling thresholds to determine the optimal balance between quality and diversity

### Open Question 3
- Question: What is the long-term impact of EditNet's architecture on the preservation of original image details compared to other approaches?
- Basis in paper: [inferred] The paper claims EditNet preserves original image details better than ControlNet and channel-wise concatenation methods, but only shows short-term comparisons
- Why unresolved: The paper does not investigate whether EditNet's advantage in preserving details persists across many editing iterations or with different types of source images
- What evidence would resolve it: Longitudinal studies tracking detail preservation across multiple editing operations, or systematic tests across diverse image categories

## Limitations
- The evaluation relies heavily on human judgment for perceptual quality and semantic consistency, introducing subjective bias
- Specialist models used for supervision are not publicly available, making it difficult to verify the quality of specialist-generated demonstrations
- Importance sampling using large multimodal models is computationally expensive, and the effectiveness of distillation is not thoroughly validated

## Confidence
- **High Confidence**: The EditNet architecture provides improvements over ControlNet for image editing tasks, supported by quantitative comparisons and ablation studies
- **Medium Confidence**: The specialist-to-generalist supervision framework effectively covers all seven editing tasks, though this depends on the quality of the specialist models which are not independently verified
- **Medium Confidence**: Importance sampling using LMM scores improves data quality compared to CLIP-score filtering, though the computational cost-benefit tradeoff is not fully explored

## Next Checks
1. **Specialist Model Verification**: Obtain or train specialist models independently and verify that they generate accurate, high-quality demonstrations for each of the seven editing tasks before using them for generalist training
2. **Scoring Model Validation**: Compare the distilled InternVL2 scoring model against GPT-4o on a held-out validation set to ensure the quality threshold of 9 maintains consistency with the original LMM-based filtering
3. **Cross-Dataset Generalization**: Test OMNI-EDIT on an independent dataset with diverse editing tasks and aspect ratios not seen during training to evaluate true generalization beyond the curated test set