---
ver: rpa2
title: 'Dialogue with Robots: Proposals for Broadening Participation and Research
  in the SLIVAR Community'
arxiv_id: '2404.01158'
source_url: https://arxiv.org/abs/2404.01158
tags:
- language
- robots
- dialogue
- research
- robot
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper addresses the growing need for robots to interact with
  humans using natural language, particularly spoken dialogue. The authors propose
  three key areas for future research: (1) Educational Resources - creating a central
  repository of course materials and resources to train students in robotics, NLP,
  SDS, and HRI.'
---

# Dialogue with Robots: Proposals for Broadening Participation and Research in the SLIVAR Community

## Quick Facts
- arXiv ID: 2404.01158
- Source URL: https://arxiv.org/abs/2404.01158
- Reference count: 11
- Primary result: Three proposals for advancing spoken dialogue with robots: educational resources, benchmarks, and efficient LLM integration

## Executive Summary
This paper presents three key proposals for advancing the field of spoken dialogue with robots (SLIVAR). The authors propose creating a centralized educational resource repository to lower barriers to entry, developing standardized benchmarks for evaluating human-robot dialogue, and exploring efficient, ethical integration of large language models with robots. The paper emphasizes the need for smaller, more efficient models that can run on robot hardware while addressing bias, safety, and inclusivity concerns.

## Method Summary
The paper outlines three distinct proposals rather than a single empirical method. For educational resources, it suggests creating a repository of course materials across robotics, NLP, SDS, and HRI. For benchmarks, it proposes a three-phase approach: requirements gathering, infrastructure development, and challenge establishment. For LLM integration, it calls for research on smaller, efficient models with better data efficiency and bias mitigation. The proposals are based on expert consensus and community needs rather than experimental validation.

## Key Results
- Proposed centralized educational repository would aggregate syllabi, lecture materials, assignments, and research tools across multiple disciplines
- Benchmarks should focus on multimodal, co-located, high-stakes, and user-centered dialogue scenarios
- LLM research should prioritize smaller models that can run on robot hardware while addressing ethical concerns

## Why This Works (Mechanism)

### Mechanism 1
- Claim: A centralized educational resource repository lowers the barrier to entry for researchers entering the SLIVAR field.
- Mechanism: By aggregating syllabi, lecture materials, assignments, and assessment methods across robotics, NLP, SDS, and HRI, the repository enables students and researchers to identify prerequisite knowledge and fill gaps efficiently, regardless of their institutional offerings.
- Core assumption: Educational resources from different subfields are complementary and can be combined effectively into a coherent curriculum.
- Evidence anchors:
  - [abstract] "We propose the creation of a central resource that allows members of the community to share syl-labi, course content, lecture slides, example code, assignments, assessment methods, platforms, and research tools used in coursework."
  - [section] "We propose the creation of a repository of resources for education of students at this crucial, yet complex intersection between HRI, SDS, and robotics."
  - [corpus] Weak - no direct neighbor papers discuss educational resource aggregation.
- Break condition: If the shared resources are not kept current or if the repository becomes fragmented, users may lose trust and stop contributing.

### Mechanism 2
- Claim: Developing standardized benchmarks and challenges accelerates research progress in spoken dialogue with robots.
- Mechanism: Benchmarks provide common tasks, datasets, and evaluation metrics, allowing researchers to compare models objectively and identify performance gaps across multimodal, co-located, high-stakes, and user-centered dialogue scenarios.
- Core assumption: A well-designed benchmark captures the essential complexities of human-robot dialogue and motivates the community to innovate.
- Evidence anchors:
  - [abstract] "The second on benchmarks, and the third on the modeling of language when it comes to spoken interaction with robots."
  - [section] "Benchmarks and challenges help research move forward by giving a community a common way to compare their work."
  - [corpus] Weak - neighbor papers discuss participation metrics and DAO semantics, not technical benchmarks.
- Break condition: If the benchmark is too narrow or too broad, or if it fails to represent real-world interaction complexities, it may not drive meaningful research advances.

### Mechanism 3
- Claim: Smaller, efficient, multimodal language models enable real-time, ethical, and inclusive human-robot dialogue.
- Mechanism: By focusing on model size reduction, data efficiency, bias mitigation, and multimodal integration, researchers can deploy LLMs on robots that interact naturally with users in diverse contexts while addressing safety and inclusivity concerns.
- Core assumption: Smaller models can achieve comparable task performance to large models if trained with targeted data and techniques.
- Evidence anchors:
  - [abstract] "The third on the modeling of language when it comes to spoken interaction with robots... addressing issues of model size, data requirements, and ethical considerations."
  - [section] "We encourage work on language model research that (1) focuses on smaller, yet effective LLMs (particularly MLLMs) that can work on a small piece of hardware on a robot..."
  - [corpus] Weak - neighbor papers discuss AI governance and democracy, not technical LLM scaling.
- Break condition: If bias mitigation or inclusivity measures are insufficient, the deployed systems may perpetuate harmful stereotypes or exclude certain user groups.

## Foundational Learning

- Concept: Multimodal machine learning
  - Why needed here: Robots interact with humans using multiple modalities (speech, vision, proprioception). Understanding how to fuse and process these modalities is essential for natural dialogue.
  - Quick check question: What are the three main types of data fusion strategies in multimodal learning, and when would each be appropriate?

- Concept: Dialogue state tracking
  - Why needed here: Maintaining an accurate representation of the conversation context and user/robot intentions is critical for coherent and task-oriented interaction.
  - Quick check question: How does dialogue state tracking differ in a co-located robot setting compared to a text-only chatbot?

- Concept: Human-robot interaction evaluation
  - Why needed here: Assessing the effectiveness and user experience of spoken dialogue systems with robots requires appropriate metrics and experimental designs.
  - Quick check question: What are the key differences between subjective and objective measures of HRI quality?

## Architecture Onboarding

- Component map: Speech recognition -> Visual perception -> Sensor fusion -> Token embedding -> Multimodal fusion -> Dialogue state tracking -> Policy selection -> Response planning -> Action planning -> Robot control -> Sensor feedback

- Critical path: User utterance → speech recognition → language understanding → dialogue state tracking → dialogue management → language generation → robot action → sensor feedback → updated state

- Design tradeoffs:
  - Model size vs. inference latency vs. hardware constraints
  - Modality richness vs. data collection complexity
  - Task specificity vs. generalization across domains
  - Open-domain vs. task-oriented dialogue focus

- Failure signatures:
  - Misunderstanding user intent due to ambiguous or context-dependent language
  - Failure to maintain common ground leading to repeated clarifications
  - Sensor noise or misalignment causing incorrect object references
  - Ethical violations due to biased model outputs

- First 3 experiments:
  1. Deploy a simple speech-to-text + rule-based dialogue manager on a simulated robot platform; measure task completion rate and dialogue length.
  2. Integrate a pre-trained multimodal language model (e.g., VilBERT) with a simulated robot environment; evaluate object reference accuracy and response relevance.
  3. Run a small user study comparing a baseline dialogue system to one augmented with clarification and common ground mechanisms; collect subjective satisfaction scores and task success rates.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can we create smaller, more efficient language models that maintain effectiveness while requiring less computational resources and training data for robotics applications?
- Basis in paper: [explicit] The paper explicitly calls for research on smaller language models that require fewer compute resources to train and run in real-time, but also require less data to be effective.
- Why unresolved: Current large language models (LLMs) like ChatGPT are too large and computationally expensive for practical deployment on robots. There's a need to balance model size with effectiveness while reducing resource requirements.
- What evidence would resolve it: Development of new model architectures or training methods that demonstrate comparable performance to large models while using significantly fewer parameters and training examples, validated on robotics-specific tasks.

### Open Question 2
- Question: What is the most effective way to encode the physical world and ongoing interactions for robots to effectively interact with humans using speech, considering the limitations of text-only LLMs?
- Basis in paper: [explicit] The paper discusses the challenge of how to encode the world and unfolding interaction for robots to effectively interact, noting that LLMs require text as input but struggle with referring to objects in physical environments.
- Why unresolved: There's uncertainty about whether to represent the physical world symbolically as text or through vector embeddings, and how to handle different robot modalities beyond vision and language.
- What evidence would resolve it: Comparative studies demonstrating which encoding approaches work best for different types of robot interactions and modalities, with measurable improvements in task completion rates.

### Open Question 3
- Question: How can we effectively integrate reinforcement learning with human feedback (RLHF) into human-robot dialogue systems to improve learning during interaction?
- Basis in paper: [explicit] The paper mentions that RLHF has direct implications for human-robot dialogue as robots are often tasked with learning as they verbally interact with humans.
- Why unresolved: While RLHF has shown promise in other NLP applications, its application to human-robot dialogue systems is unexplored, particularly regarding how to collect and incorporate human feedback in real-time interaction scenarios.
- What evidence would resolve it: Development and evaluation of RLHF-based dialogue systems that show measurable improvements in learning efficiency and interaction quality compared to traditional supervised learning approaches.

## Limitations
- Proposals are based on expert opinion rather than empirical validation
- Technical implementation challenges for benchmarks and LLM integration are acknowledged but not fully addressed
- Feasibility of community adoption and sustained participation in proposed initiatives remains unproven

## Confidence
- **High** confidence that educational resource aggregation would benefit the community (well-established need)
- **Medium** confidence that standardized benchmarks would accelerate research (mechanisms understood but implementation challenges significant)
- **Medium** confidence that smaller, ethical LLMs can effectively support robot dialogue (technical challenges acknowledged)

## Next Checks
1. Conduct a small-scale pilot with 3-5 research teams to test whether the proposed benchmark requirements (multimodal, co-located, high-stakes, user-centered) are practical to implement and evaluate

2. Create a minimal working version of the proposed repository with 10-15 curated resources and track usage patterns and community contributions over 3 months

3. Test whether a 7B parameter multimodal model can achieve comparable task performance to a 70B parameter model on a robot dialogue task while meeting real-time constraints (latency < 500ms)