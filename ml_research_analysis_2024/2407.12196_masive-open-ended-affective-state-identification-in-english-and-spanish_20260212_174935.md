---
ver: rpa2
title: 'MASIVE: Open-Ended Affective State Identification in English and Spanish'
arxiv_id: '2407.12196'
source_url: https://arxiv.org/abs/2407.12196
tags:
- emotion
- affective
- data
- spanish
- english
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a new task, affective state identification
  (ASI), which aims to predict natural, nuanced affective state labels from text rather
  than fixed emotion categories. The authors collect MASIVE, a large-scale dataset
  in English and Spanish containing over 1,000 unique affective states each, derived
  from Reddit posts via a bootstrapping method.
---

# MASIVE: Open-Ended Affective State Identification in English and Spanish

## Quick Facts
- arXiv ID: 2407.12196
- Source URL: https://arxiv.org/abs/2407.12196
- Reference count: 36
- Primary result: Smaller fine-tuned mT5 models outperform much larger LLMs on open-ended affective state identification in English and Spanish

## Executive Summary
This paper introduces MASIVE, a large-scale dataset for affective state identification (ASI) containing over 1,000 unique affective states in English and Spanish derived from Reddit posts via bootstrapping. The authors frame ASI as a masked span prediction task and demonstrate that fine-tuned mT5 models significantly outperform zero-shot LLMs like Llama-3 and Mixtral. They also show that native speaker-written data is essential for good performance, with machine translation degrading results, particularly in Spanish. MASIVE enables models to capture nuanced, open-ended emotional expressions beyond fixed emotion categories.

## Method Summary
The authors collected MASIVE using a bootstrapping procedure starting from Ekman's basic emotions. They queried Reddit posts containing "I feel <affect> and..." patterns, extracted conjunctive adjectives as new affective states, and repeated this process for four rounds to expand the vocabulary. The dataset contains 28,710 English and 32,734 Spanish posts with 1,177 and 1,159 unique affective states respectively. Models were fine-tuned on this data using masked span prediction with mT5-large, and evaluated using top-k accuracy, log perplexity, and similarity metrics. Cross-lingual experiments involved machine translation with Opus-MT models.

## Key Results
- mT5-large fine-tuned on MASIVE outperforms zero-shot Llama-3-70B and Mixtral-8x7B by 10-20% absolute in top-1 accuracy
- Performance drops significantly when using machine-translated data for fine-tuning or evaluation, with larger drops in Spanish
- Fine-tuning on MASIVE improves performance on fixed-label emotion benchmarks (GoEmotions, ArCo) by 1-2% absolute
- Monolingual models (T5) show better generalization to unseen affective states than multilingual mT5

## Why This Works (Mechanism)

### Mechanism 1
Bootstrapping from a small seed set (Ekman emotions) to a large affective vocabulary works because each iteration expands coverage without drift. Seed adjectives are used to query for posts containing "I feel <affect> and...", extracting conjunctive adjectives that are assumed to also be affective states. Each round re-uses newly found terms as new query seeds, expanding the vocabulary exponentially. Core assumption: Any adjective conjunct of an affective state in the "I feel X and Y" construction is itself an affective state.

### Mechanism 2
Fine-tuning smaller multilingual models outperforms larger LLMs on open-ended affective state identification because the task requires precise, generative span prediction rather than broad knowledge. mT5-large is fine-tuned with masked span prediction on MASIVE data, learning to predict specific affective states in context. LLMs are evaluated zero-shot, so they lack task-specific adaptation. Core assumption: Generative span prediction on a curated dataset yields better performance than zero-shot inference from a general-purpose LLM.

### Mechanism 3
Training on native speaker-written data is essential for good affective state identification performance because machine-translated data lacks the nuances of natural emotional expression. Models fine-tuned on original Reddit posts in English and Spanish perform better than those fine-tuned on machine-translated data or evaluated on translated data. Core assumption: Machine translation introduces artifacts that degrade performance on tasks requiring nuanced understanding of emotional language.

## Foundational Learning

- **Masked language modeling (MLM) and span prediction**: Why needed here: ASI is framed as a masked span prediction task, requiring models to predict affective state terms that replace <MASK> tokens. Quick check: How does masked span prediction differ from standard classification in emotion detection tasks?

- **Bootstrapping and iterative data collection**: Why needed here: MASIVE is collected using a bootstrapping procedure that expands the set of affective states iteratively. Quick check: What is the core assumption behind using conjunctive adjectives as new affective state labels in the bootstrapping process?

- **Cross-lingual transfer and the role of native data**: Why needed here: The paper investigates the impact of using machine-translated vs. native data for cross-lingual generalization. Quick check: Why might machine translation introduce artifacts that degrade performance on nuanced tasks like affective state identification?

## Architecture Onboarding

- **Component map**: Data collection -> Model fine-tuning -> Evaluation -> Cross-lingual experiments
- **Critical path**: 1. Collect MASIVE data via bootstrapping. 2. Fine-tune mT5 on MASIVE. 3. Evaluate on MASIVE and fixed-label emotion datasets. 4. Conduct machine translation experiments.
- **Design tradeoffs**: Using Reddit data provides large, diverse emotional expressions but may include noise and sensitive content. Fine-tuning smaller models is computationally efficient but may limit performance on broader tasks. Relying on machine translation for cross-lingual experiments is practical but introduces artifacts.
- **Failure signatures**: Poor performance on unseen or region-specific affective states indicates overfitting to the training data. Significant drops in performance when using machine-translated data suggest sensitivity to linguistic nuances. Low similarity scores despite high top-k accuracy may indicate predictions are semantically different from gold labels.
- **First 3 experiments**: 1. Fine-tune mT5 on MASIVE and evaluate on the test set to establish baseline performance. 2. Fine-tune mT5 on machine-translated data and compare performance to the native data baseline. 3. Evaluate the impact of pre-training on MASIVE by fine-tuning on fixed-label emotion datasets with and without prior MASIVE fine-tuning.

## Open Questions the Paper Calls Out

- **Open Question 1**: How does model performance on affective state identification change when training on multiple social media platforms beyond Reddit? The paper only evaluates models trained on Reddit data, but acknowledges this limits demographic representation.

- **Open Question 2**: Can monolingual models consistently outperform multilingual models across all affective states, or only for certain categories? The paper shows T5 outperforms mT5 on unseen affective states but doesn't analyze all categories.

- **Open Question 3**: What is the impact of grammatical gender agreement errors on downstream applications of affective state identification? The paper treats gender mismatches as completely incorrect but doesn't explore how this affects practical applications.

- **Open Question 4**: How does performance on regional Spanish affective states vary across different Spanish-speaking countries? The authors collected a regional Spanish challenge set but only evaluated overall performance, not by country.

- **Open Question 5**: What is the relationship between figurative usage of affective states and model performance on these instances? The paper identifies figurative usage but doesn't analyze how this affects model performance.

## Limitations
- Bootstrapping validity is uncertain with 12-28% of terms not validated as affective states
- Cross-lingual generalization claims lack causal mechanism for translation degradation
- Benchmark transfer improvements are modest (1-2% absolute) and not tested on broader affective tasks

## Confidence
- **High Confidence**: Claims about mT5 outperforming LLMs on MASIVE (substantial effect sizes, sound methodology)
- **Medium Confidence**: Claims about native data being essential (experimental evidence but unclear mechanism)
- **Low Confidence**: Claims about bootstrapping capturing nuanced affective states without drift (unvalidated core assumption)

## Next Checks
1. Conduct systematic linguistic analysis of bootstrapped affective states to verify grammatical and semantic validity
2. Design controlled translation experiments isolating domain/style differences from translation artifacts
3. Evaluate MASIVE fine-tuning on broader affective computing tasks beyond the two emotion benchmarks tested