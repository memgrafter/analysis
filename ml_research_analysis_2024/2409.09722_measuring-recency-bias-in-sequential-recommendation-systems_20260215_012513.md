---
ver: rpa2
title: Measuring Recency Bias In Sequential Recommendation Systems
arxiv_id: '2409.09722'
source_url: https://arxiv.org/abs/2409.09722
tags:
- recommendation
- bias
- recency
- item
- hrli
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces a novel metric, HRLI, to quantify recency
  bias in sequential recommendation systems. Recency bias refers to the tendency of
  models to overly focus on recent items, reducing serendipity and long-term interest
  capture.
---

# Measuring Recency Bias In Sequential Recommendation Systems

## Quick Facts
- arXiv ID: 2409.09722
- Source URL: https://arxiv.org/abs/2409.09722
- Authors: Jeonglyul Oh; Sungzoon Cho
- Reference count: 18
- Key outcome: Introduces HRLI metric to quantify recency bias in sequential recommendation systems

## Executive Summary
This paper addresses the problem of recency bias in sequential recommendation systems, where models disproportionately focus on recent items within user sessions. The authors introduce a novel metric called HRLI (Hit Rate of Last Item) that measures the frequency of recommending the last item in a session, which correlates with high recency bias. Experiments across six models (GRU4Rec, SASRec, STAMP, LightSANs, CORE, FEARec) on Amazon and MovieLens datasets demonstrate that models with high HRLI values exhibit significant performance improvements (up to 43.02% in NDCG@5) when the last item is excluded from ranking. These findings highlight the importance of measuring and mitigating recency bias to enhance recommendation performance and user experience.

## Method Summary
The authors propose HRLI as a metric to quantify recency bias by measuring how often the last item in a session appears in the top-K recommendations. The metric is calculated as HRLI@K = (# of times last item is in Top-K) / (# of evaluation sessions), where the ground truth item is replaced with the last item during evaluation. The study evaluates six sequential recommendation models on Amazon (Beauty, Sports, Clothing) and MovieLens-1M datasets, with sessions truncated to maximum length 50 and users/items with fewer than 5 interactions removed. Performance is measured using standard metrics (Hit@K, NDCG@K) and modified metrics (Hit*@K, NDCG*@K) when the last item is excluded from ranking by assigning it a negative infinity relevance score.

## Key Results
- HRLI consistently exceeds Hit Rate across all models, indicating strong recency bias
- SASRec, LightSANs, and FEARec show particularly high HRLI values (up to 0.99) in sparse datasets
- Models with high HRLI exhibit performance improvements up to 43.02% in NDCG@5 when last item is excluded
- GRU4Rec and STAMP show comparatively lower HRLI values and minimal performance gains when excluding last item

## Why This Works (Mechanism)

### Mechanism 1
- Claim: HRLI metric quantifies recency bias by measuring the frequency of recommending the last item in a session.
- Mechanism: HRLI@K = (# of times last item is in Top-K) / (# of evaluation sessions). This replaces the ground truth item with the last item when calculating Hit Rate, thus measuring recency bias instead of recommendation performance.
- Core assumption: High recency bias manifests as a high frequency of recommending the last item in a session.
- Evidence anchors:
  - [abstract] "Our findings also demonstrate that high recency bias measured in our proposed metric adversely impacts recommendation performance too"
  - [section] "HRLI is proportional to the degree of recency bias of a sequential recommendation model"
  - [corpus] Weak evidence - no direct mention of HRLI metric or recency bias quantification in corpus
- Break condition: If HRLI does not correlate with actual recency bias in model behavior, or if the metric fails to capture the full scope of recency bias effects.

### Mechanism 2
- Claim: High HRLI values lead to reduced recommendation performance because the last item displaces the ground truth item from Top-K rankings.
- Mechanism: When the last item consistently ranks highly due to recency bias, it pushes the actual ground truth item out of the Top-K cutoff, reducing metrics like Hit Rate and NDCG.
- Core assumption: Standard evaluation metrics like Hit Rate and NDCG are negatively impacted when the ground truth item is displaced from Top-K by another item.
- Evidence anchors:
  - [section] "another problem is derived that the high rank of the last item can lower evaluated recommendation performance results by displacing the GT item from the Top-K ranking"
  - [section] "removing the last item from the Top-K ranking by assigning it a relevance score of negative infinity" shows performance gains
  - [corpus] Weak evidence - no direct mention of GT item displacement in corpus
- Break condition: If removing the last item from ranking does not improve performance, or if the relationship between HRLI and performance is not consistent across different datasets and models.

### Mechanism 3
- Claim: Different models exhibit varying levels of recency bias, with some models being more susceptible than others.
- Mechanism: Models like SASRec, LightSANs, and FEARec show higher HRLI values (up to 0.99) compared to GRU4Rec and STAMP (lower HRLI values), indicating their susceptibility to recency bias.
- Core assumption: The architecture and attention mechanisms of different models influence their susceptibility to recency bias.
- Evidence anchors:
  - [section] "SASRec, LightSANs, and FEARec exhibit particularly high HRLI values, reaching up to 0.99 in sparse datasets. GRU4Rec and STAMP show comparatively lower HRLI values"
  - [corpus] No direct evidence in corpus about model-specific recency bias susceptibility
- Break condition: If the relationship between model architecture and recency bias susceptibility is not consistent across different datasets or if other factors influence recency bias more than model architecture.

## Foundational Learning

- Concept: Hit Rate and NDCG metrics
  - Why needed here: Understanding these standard recommendation metrics is crucial for interpreting HRLI and its impact on performance
  - Quick check question: What is the difference between Hit Rate and NDCG, and how do they evaluate recommendation performance?

- Concept: Sequential recommendation systems
  - Why needed here: The paper focuses on recency bias in sequential recommendation, so understanding the basics of how these systems work is essential
  - Quick check question: How do sequential recommendation systems differ from traditional recommendation systems in terms of input and prediction goals?

- Concept: Attention mechanisms in neural networks
  - Why needed here: Many of the models mentioned (SASRec, LightSANs) use attention mechanisms, which are likely related to their susceptibility to recency bias
  - Quick check question: How do attention mechanisms in sequential recommendation models influence the emphasis on recent items?

## Architecture Onboarding

- Component map:
  User session data (sequence of items) -> Sequential recommendation model (e.g., SASRec, GRU4Rec) -> Standard evaluation metrics (Hit Rate, NDCG) -> HRLI calculation (modified Hit Rate with last item as ground truth) -> Performance comparison (original metrics vs. metrics with last item excluded)

- Critical path:
  1. Load user session data
  2. Train sequential recommendation model
  3. Calculate standard Hit Rate and NDCG
  4. Calculate HRLI metric
  5. Compare performance with and without last item in ranking
  6. Analyze correlation between HRLI and performance impact

- Design tradeoffs:
  - Simplicity vs. comprehensiveness of HRLI metric
  - Computational cost of full item set evaluation vs. sampled metrics
  - Balancing recency bias mitigation with maintaining model's ability to capture short-term trends

- Failure signatures:
  - HRLI does not correlate with observed recency bias in model behavior
  - Performance improvements when excluding last item are not proportional to HRLI values
  - Inconsistent results across different datasets or model architectures

- First 3 experiments:
  1. Implement HRLI calculation and verify it consistently exceeds Hit Rate across different models
  2. Test performance impact by excluding last item from ranking in models with varying HRLI values
  3. Compare recency bias susceptibility across different model architectures using HRLI metric

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does recency bias impact long-term user engagement metrics like return frequency or session duration, or is its effect limited to immediate recommendation performance?
- Basis in paper: [inferred] The paper mentions that high recency bias can lead to user disengagement but doesn't measure actual long-term engagement outcomes.
- Why unresolved: The paper focuses on immediate recommendation performance metrics rather than longitudinal user behavior analysis.
- What evidence would resolve it: Longitudinal studies tracking user return rates, session lengths, and interaction diversity before and after implementing recency bias mitigation strategies.

### Open Question 2
- Question: How does recency bias manifest differently across recommendation domains (e.g., entertainment vs. e-commerce vs. news), and are there domain-specific thresholds for acceptable HRLI values?
- Basis in paper: [inferred] The paper uses datasets from different domains but doesn't analyze domain-specific patterns in recency bias or establish domain-specific benchmarks.
- Why unresolved: The experiments treat all domains uniformly without analyzing whether certain types of recommendations are more or less susceptible to recency bias.
- What evidence would resolve it: Comparative analysis of HRLI distributions and performance impacts across multiple recommendation domains with domain-specific optimal HRLI ranges.

### Open Question 3
- Question: What is the relationship between recency bias and other known recommendation biases (e.g., popularity bias, diversity bias), and can they be addressed simultaneously without creating new unintended biases?
- Basis in paper: [explicit] The paper discusses recency bias in isolation without examining interactions with other recommendation biases.
- Why unresolved: The paper focuses solely on recency bias without investigating trade-offs or synergies with other recommendation system biases.
- What evidence would resolve it: Empirical studies measuring the correlation between recency bias and other biases, and experiments testing multi-bias mitigation strategies.

## Limitations
- The correlation between HRLI and actual recency bias behavior is assumed rather than directly validated through behavioral analysis of model predictions
- The mechanism by which recency bias displaces ground truth items from Top-K rankings requires further empirical validation across diverse datasets and model architectures
- The generalizability of findings to real-world recommendation scenarios with longer sessions and different user behavior patterns is uncertain

## Confidence
- High confidence: HRLI consistently exceeds Hit Rate across models and datasets; performance improvements when excluding last item are observed
- Medium confidence: The correlation between HRLI and recency bias; the relationship between recency bias and ground truth item displacement
- Low confidence: The generalizability of findings to real-world recommendation scenarios with longer sessions and different user behavior patterns

## Next Checks
1. Analyze the actual prediction patterns of high-HRLI models to verify that they consistently recommend the last item and understand the conditions under which this occurs.

2. Test the HRLI metric and its relationship with performance across additional datasets with varying characteristics (session length distributions, item popularity distributions) to assess generalizability.

3. Implement and evaluate recency bias mitigation strategies (e.g., attention mechanism modifications) to determine if reducing recency bias improves recommendation performance as predicted by the HRLI metric.