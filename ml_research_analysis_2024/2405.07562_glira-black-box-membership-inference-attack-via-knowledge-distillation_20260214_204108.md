---
ver: rpa2
title: 'GLiRA: Black-Box Membership Inference Attack via Knowledge Distillation'
arxiv_id: '2405.07562'
source_url: https://arxiv.org/abs/2405.07562
tags:
- target
- shadow
- membership
- glira
- attack
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of membership inference attacks
  (MIAs) on black-box neural networks, specifically when the attacker does not know
  the model architecture. The core method, GLiRA, leverages knowledge distillation
  to train shadow models that mimic the target model's behavior using Mean Squared
  Error (MSE) loss instead of Kullback-Leibler divergence.
---

# GLiRA: Black-Box Membership Inference Attack via Knowledge Distillation

## Quick Facts
- arXiv ID: 2405.07562
- Source URL: https://arxiv.org/abs/2405.07562
- Authors: Andrey V. Galichin; Mikhail Pautov; Alexey Zhavoronkin; Oleg Y. Rogov; Ivan Oseledets
- Reference count: 40
- Primary result: GLiRA outperforms state-of-the-art MIA methods in black-box settings using MSE-based knowledge distillation

## Executive Summary
This paper presents GLiRA, a novel black-box membership inference attack that leverages knowledge distillation with Mean Squared Error (MSE) loss to train shadow models that accurately mimic the target model's behavior. Unlike previous approaches that require knowledge of the target model's architecture, GLiRA operates effectively without this information, making it more practical for real-world scenarios. The method demonstrates superior performance across multiple image classification datasets, achieving higher true positive rates at low false positive rates and improved AUC scores compared to existing state-of-the-art approaches.

## Method Summary
GLiRA addresses the challenge of performing membership inference attacks when the attacker has no knowledge of the target model's architecture. The core innovation lies in using knowledge distillation with MSE loss instead of the traditional Kullback-Leibler divergence. This approach trains shadow models to closely match the target model's output distribution without requiring architectural information. The method operates by training multiple shadow models on data similar to what the target model was trained on, then using these shadow models to learn the decision boundary between members and non-members of the target training set. The MSE loss in knowledge distillation proves crucial for better aligning the shadow models with the target model's behavior, particularly in black-box scenarios where architectural mismatch could otherwise degrade attack performance.

## Key Results
- GLiRA achieves higher true positive rates at low false positive rates compared to state-of-the-art black-box MIA methods
- The approach demonstrates improved AUC scores across multiple image classification datasets and models
- Experiments show GLiRA's effectiveness even when shadow models have different architectures than the target model

## Why This Works (Mechanism)
GLiRA works by exploiting the subtle differences in how target models respond to data points they were trained on versus data they haven't seen. The knowledge distillation process with MSE loss allows shadow models to capture the target model's unique output patterns and decision boundaries without requiring architectural knowledge. By training multiple shadow models with different random seeds and architectures, GLiRA creates a robust approximation of the target model's behavior, which the membership inference classifier can then use to distinguish between members and non-members of the training set. The MSE loss proves particularly effective at preserving the nuanced output characteristics that differentiate training set members from non-members.

## Foundational Learning
**Knowledge Distillation**: A technique where a smaller model (student) learns to mimic the behavior of a larger model (teacher) by matching its output distributions. *Why needed*: Enables training shadow models without architectural knowledge of the target model. *Quick check*: Verify the student model's outputs approximate the teacher's after training.

**Membership Inference Attack (MIA)**: A privacy attack where an adversary attempts to determine whether a specific data point was part of a target model's training set. *Why needed*: Fundamental privacy concern in machine learning where models can leak information about their training data. *Quick check*: Confirm attack performance metrics (AUC, TPR, FPR) are computed correctly.

**Black-box Setting**: Attack scenario where the adversary only has access to the model's input-output behavior without any internal information about the model's architecture or parameters. *Why needed*: More realistic threat model for practical privacy attacks. *Quick check*: Ensure no architectural information is used in the attack process.

**Shadow Model Training**: The process of training surrogate models that mimic the target model's behavior for the purpose of launching inference attacks. *Why needed*: Essential for black-box attacks where direct access to the target model's internals is unavailable. *Quick check*: Verify shadow models achieve similar performance to the target model on held-out data.

## Architecture Onboarding
**Component Map**: Target Model -> Shadow Models (multiple) -> Membership Inference Classifier
**Critical Path**: Data sampling → Shadow model training (with MSE loss) → Attack model training → Inference
**Design Tradeoffs**: Using MSE loss instead of KL divergence improves black-box performance but may reduce effectiveness in white-box scenarios; multiple shadow models increase robustness but also computational cost
**Failure Signatures**: Poor attack performance when shadow models fail to adequately mimic target model behavior; degraded results when training data distributions between shadow and target models differ significantly
**First Experiments**: 1) Train shadow models with both MSE and KL divergence to isolate the effect of the loss function; 2) Test attack performance with varying numbers of shadow models; 3) Evaluate attack effectiveness across different target model architectures

## Open Questions the Paper Calls Out
None

## Limitations
- The assumption that shadow models can be trained effectively without knowing the target model's architecture may not hold for complex or highly specialized models
- The reliance on MSE loss for knowledge distillation lacks theoretical justification for why it outperforms KL divergence
- The evaluation focuses on image classification tasks, limiting generalizability to other domains

## Confidence
- **High Confidence**: Experimental results demonstrating GLiRA's superiority over baseline methods in black-box settings
- **Medium Confidence**: Claim that MSE loss improves shadow model alignment with target models
- **Low Confidence**: Assertion that GLiRA is broadly applicable to any black-box neural network

## Next Checks
1. Conduct ablation studies to isolate the impact of MSE loss versus KL divergence in knowledge distillation
2. Test GLiRA on non-image datasets (e.g., text or tabular data) to assess generalizability
3. Investigate whether existing or novel defenses can mitigate GLiRA's effectiveness