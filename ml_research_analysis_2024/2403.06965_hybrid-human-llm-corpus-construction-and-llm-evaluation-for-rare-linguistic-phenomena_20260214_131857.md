---
ver: rpa2
title: Hybrid Human-LLM Corpus Construction and LLM Evaluation for Rare Linguistic
  Phenomena
arxiv_id: '2403.06965'
source_url: https://arxiv.org/abs/2403.06965
tags:
- sentence
- sentences
- construction
- prompt
- label
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a hybrid human-LLM annotation pipeline to
  enable large-scale data collection for rare linguistic phenomena, exemplified by
  the caused-motion construction (CMC). The pipeline uses dependency parsing and GPT-3.5
  to prefilter and classify sentences, reducing human annotation cost while preserving
  diversity.
---

# Hybrid Human-LLM Corpus Construction and LLM Evaluation for Rare Linguistic Phenomena

## Quick Facts
- arXiv ID: 2403.06965
- Source URL: https://arxiv.org/abs/2403.06965
- Reference count: 40
- Primary result: A hybrid human-LLM pipeline successfully constructed a CMC corpus (765 manual + 127,955 semi-auto instances) and revealed all evaluated LLMs struggle with motion understanding in CMC sentences, with best model at 69.75% accuracy

## Executive Summary
This paper addresses the challenge of collecting large-scale data for rare linguistic phenomena by introducing a hybrid human-LLM annotation pipeline. The pipeline uses dependency parsing to prefilter sentences matching the caused-motion construction (CMC) pattern, followed by GPT-3.5 classification to reduce false positives before human annotation. Applied to CMC, the approach produced a manually verified dataset of 765 sentences and expanded to 127,955 high-confidence instances. Evaluation across multiple LLM families (GPT-3.5, GPT-4, Gemini Pro, Llama2, Mistral, Mixtral) revealed consistent struggles with understanding the motion component of CMC, with the best model (Mixtral 8x7b) achieving only 69.75% accuracy. The work demonstrates both the potential and limitations of LLMs for capturing subtle syntactic-semantic patterns.

## Method Summary
The hybrid pipeline combines dependency parsing (spaCy) to extract sentences matching the CMC syntactic pattern (verb → direct object → preposition → prepositional object), GPT-3.5 classification with engineered prompts to filter false positives, and human verification to create a gold standard corpus. The 4-tuple of (verb, direct object, preposition, prepositional object) was found to almost perfectly determine CMC class, enabling semi-automatic expansion to 127,955 instances. LLM evaluation used prompt-based classification where models determined if direct objects were physically moving, with variations including verb replacement ("throw") to test understanding.

## Key Results
- The hybrid pipeline successfully reduced annotation costs while producing 765 manually verified CMC instances
- All evaluated LLMs (including GPT-4) showed poor performance on CMC motion understanding, with error rates around 30%
- Mixtral 8x7b achieved the highest accuracy at 69.75%, but still made errors on 30.25% of cases
- The 4-tuple extrapolation mechanism successfully expanded the dataset to 127,955 high-confidence instances without additional manual annotation

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Dependency parsing can filter out non-CMC sentences before costly LLM annotation
- Mechanism: The CMC follows a consistent syntactic pattern: verb with direct object, followed by preposition and prepositional object. By extracting this subtree from dependency parses, non-matching sentences are removed, reducing annotation burden
- Core assumption: Most CMC sentences will have this exact dependency structure, and most non-CMC sentences will not
- Evidence anchors: [section 3.1.2] describes extracting matching sentences from dependency parses, and the paper releases a corpus of 765 verified instances

### Mechanism 2
- Claim: Prompt-based classification with GPT-3.5 can further reduce false positives before human annotation
- Mechanism: After dependency filtering, GPT-3.5 classifies remaining sentences to keep true positives and remove false positives, with prompts engineered to maximize precision while minimizing API costs
- Core assumption: GPT-3.5 can accurately distinguish CMC from non-CMC sentences based on provided examples and instructions
- Evidence anchors: [section 3.2.2] details prompt engineering to minimize cost per true positive, and [section 4.2] shows LLM struggles suggest classification challenges

### Mechanism 3
- Claim: The 4-tuple of (verb, direct object, preposition, prepositional object) almost perfectly determines the CMC class
- Mechanism: After manual annotation, the 4-tuple was observed to reliably determine CMC class, enabling automatic expansion from manually annotated sentences to all sentences with matching 4-tuples
- Core assumption: The 4-tuple is a reliable indicator of CMC class, and sentences with same 4-tuple will have same classification
- Evidence anchors: [section 3.3] explains the 4-tuple extrapolation that produced 127,955 additional instances, and the paper releases this expanded dataset

## Foundational Learning

- Concept: Dependency parsing
  - Why needed here: To extract syntactic structure and identify CMC pattern
  - Quick check question: Can you explain the difference between a subject, a direct object, and a prepositional object in a sentence?

- Concept: Prompt engineering
  - Why needed here: To create effective prompts for GPT-3.5 that maximize precision and minimize cost
  - Quick check question: What are the key elements of a well-designed prompt for an LLM?

- Concept: Construction Grammar
  - Why needed here: To understand theoretical background of CMC and its significance
  - Quick check question: How does Construction Grammar differ from traditional lexical semantics in explaining CMC meaning?

## Architecture Onboarding

- Component map: Reddit corpus -> Dependency parser (spaCy) -> GPT-3.5 API -> Human annotators -> Output datasets
- Critical path: Dependency parsing → GPT-3.5 classification → Human annotation → Dataset release
- Design tradeoffs: Balancing GPT-3.5 API costs against human annotation burden while maximizing precision
- Failure signatures: Low GPT-3.5 classification precision leading to high human costs, dependency parser misidentifying syntactic roles, 4-tuple extrapolation introducing errors
- First 3 experiments:
  1. Test dependency parsing accuracy on small sentence sample to verify CMC identification
  2. Experiment with different GPT-3.5 prompts to find most cost-effective classification
  3. Manually annotate small subset to verify 4-tuple reliably determines CMC class

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the cost-effectiveness of the hybrid pipeline be further improved by optimizing the number of shots per class in the prompt?
- Basis in paper: [inferred] The paper discusses prompt engineering and testing different shot counts (5, 10, 20) but lacks detailed analysis of optimal number
- Why unresolved: General framework provided but specific impact of shot count on cost-effectiveness not explored in detail
- What evidence would resolve it: Detailed analysis comparing cost per true positive across different shot counts, holding other factors constant

### Open Question 2
- Question: How does pipeline performance vary across different rare linguistic phenomena beyond CMC?
- Basis in paper: [explicit] Pipeline designed for rare phenomena and demonstrated on CMC, with discussion of applying guidelines to other phenomena
- Why unresolved: Only one specific phenomenon tested, limiting generalizability
- What evidence would resolve it: Applying pipeline to diverse rare linguistic phenomena and comparing results

### Open Question 3
- Question: What is the impact of using different dependency parsing tools or treebanks on pipeline performance?
- Basis in paper: [explicit] Uses spaCy but mentions Universal Dependencies as alternative, suggesting tool choice might affect results
- Why unresolved: Specific tool used without exploring alternatives
- What evidence would resolve it: Comparison of pipeline performance using different dependency parsing tools on same dataset

## Limitations

- The hybrid pipeline's effectiveness may be specific to CMC's regular syntactic pattern and may not generalize to more variable constructions
- The 4-tuple extrapolation mechanism lacks broader testing on other linguistic phenomena
- Evaluation focuses on single semantic dimension (physical motion) and may not capture full CMC understanding

## Confidence

- Medium: Pipeline successfully reduced annotation costs while maintaining reasonable data quality (765 verified instances from 127,955 candidates)
- Medium: LLMs show consistent difficulty with CMC motion component across multiple model families
- Low: Claim that approach can be readily applied to other rare phenomena without modification

## Next Checks

1. Test pipeline on a different construction with more syntactic variability (e.g., resultative constructions) to assess generalizability
2. Conduct ablation studies comparing pipeline performance with different dependency parsing configurations and GPT-3.5 classification thresholds
3. Evaluate human annotator agreement rates on same CMC instances to establish performance baseline for LLM evaluation