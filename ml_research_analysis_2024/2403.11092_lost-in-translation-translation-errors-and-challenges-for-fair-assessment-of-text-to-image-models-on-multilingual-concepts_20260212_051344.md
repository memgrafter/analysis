---
ver: rpa2
title: Lost in Translation? Translation Errors and Challenges for Fair Assessment
  of Text-to-Image Models on Multilingual Concepts
arxiv_id: '2403.11092'
source_url: https://arxiv.org/abs/2403.11092
tags:
- translation
- language
- concept
- error
- japanese
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper identifies translation errors in the CoCo-CroLa benchmark,
  which assesses multilingual text-to-image model performance. Through manual analysis,
  the authors find 4.7% to 12.9% of concept translations in Spanish, Chinese, and
  Japanese are erroneous.
---

# Lost in Translation? Translation Errors and Challenges for Fair Assessment of Text-to-Image Models on Multilingual Concepts

## Quick Facts
- arXiv ID: 2403.11092
- Source URL: https://arxiv.org/abs/2403.11092
- Reference count: 30
- Manual analysis finds 4.7%â€“12.9% erroneous translations in CoCo-CroLa's Spanish, Chinese, and Japanese concept translations.

## Executive Summary
This study uncovers significant translation errors in the CoCo-CroLa benchmark, which is used to evaluate multilingual text-to-image (T2I) model performance. The authors manually analyze translations for Spanish, Chinese, and Japanese, finding that 4.7% to 12.9% contain errors that could skew model rankings. They propose corrections and show that translation changes' impact can be predicted using text similarity metrics. After applying impactful corrections, they release CoCo-CroLa v1.1, highlighting the need for accurate translations and multiple translations per concept in future multilingual T2I benchmarks.

## Method Summary
The authors manually reviewed translations in the CoCo-CroLa benchmark for Spanish, Chinese, and Japanese, identifying translation errors and proposing corrections. They used text similarity metrics to predict the impact of translation changes on model scores and applied corrections to create an updated version of the benchmark. The study emphasizes the importance of accurate translations and the use of multiple translations per concept to ensure fair and reliable evaluation of multilingual T2I models.

## Key Results
- 4.7% to 12.9% of concept translations in Spanish, Chinese, and Japanese are erroneous.
- Translation changes' impact on model scores can be predicted using text similarity metrics.
- CoCo-CroLa v1.1 is released after applying impactful corrections to the original benchmark.

## Why This Works (Mechanism)
Translation errors in multilingual benchmarks can significantly affect model performance and fairness. By identifying and correcting these errors, the benchmark becomes a more accurate tool for evaluating T2I models across languages. The use of text similarity metrics to predict the impact of translation changes allows for targeted corrections, improving the reliability of the benchmark.

## Foundational Learning
- **Translation accuracy**: Ensures that the benchmark concepts are correctly represented in each language, preventing unfair model evaluations.
- **Text similarity metrics**: Enables prediction of translation changes' impact on model scores, guiding efficient corrections.
- **Multiple translations per concept**: Provides redundancy and reduces the risk of errors affecting model rankings, promoting fairness.

## Architecture Onboarding
- **Component map**: CoCo-CroLa benchmark -> Manual translation review -> Error identification -> Correction proposal -> Text similarity prediction -> CoCo-CroLa v1.1 release.
- **Critical path**: Manual translation review identifies errors, which are corrected based on predicted impact, resulting in a more accurate benchmark.
- **Design tradeoffs**: Balancing thoroughness of manual review with scalability, and using text similarity metrics to prioritize corrections.
- **Failure signatures**: Inaccurate translations lead to skewed model rankings; unaddressed errors perpetuate unfair evaluations.
- **First experiments**:
  1. Re-evaluate model rankings using corrected translations.
  2. Test the impact of using multiple translations per concept on model evaluation consistency.
  3. Validate the accuracy of text similarity metrics in predicting translation changes' impact.

## Open Questions the Paper Calls Out
None

## Limitations
- Manual error detection relied on a small set of annotators, potentially missing linguistic or cultural subtleties.
- Re-translation step is not independently verified, and impact of corrections is inferred from text similarity rather than direct human re-evaluation.
- No empirical validation provided for the use of multiple translations per concept in improving model ranking stability or fairness.

## Confidence
- Translation error rates: Medium (based on manual review by limited annotators)
- Correction impact prediction: Medium (inferred from text similarity metrics)
- Multiple translations mitigation: Medium (proposed but not empirically validated)

## Next Checks
1. Have independent annotators verify proposed corrected translations and reassess error rates.
2. Run model evaluations with corrected translations to measure actual changes in ranking and fairness.
3. Test the effect of using multiple translations per concept on model evaluation consistency across languages.