---
ver: rpa2
title: Aligning Large Language Models with Diverse Political Viewpoints
arxiv_id: '2406.14155'
source_url: https://arxiv.org/abs/2406.14155
tags:
- political
- party
- more
- chatgpt
- llms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates political biases in large language models
  (LLMs) like ChatGPT, which often exhibit progressive leanings. The authors propose
  aligning LLMs with diverse political viewpoints from 100,000 comments by Swiss parliamentary
  candidates.
---

# Aligning Large Language Models with Diverse Political Viewpoints

## Quick Facts
- arXiv ID: 2406.14155
- Source URL: https://arxiv.org/abs/2406.14155
- Reference count: 12
- Primary result: ORPO fine-tuning on Swiss political data produces more accurate and diverse political viewpoints than zero-shot or SFT approaches

## Executive Summary
This paper addresses political bias in large language models (LLMs) by proposing a method to align models with diverse political viewpoints. The authors use a dataset of 100,000 comments from Swiss parliamentary candidates and fine-tune Llama 3 models using monolithic preference optimization (ORPO). The aligned models generate more accurate and diverse political viewpoints compared to commercial models like ChatGPT, with significantly higher MAUVE scores and preferred human evaluations. The approach shows promise for creating balanced political overviews that could aid in finding compromises or understanding political issues.

## Method Summary
The authors fine-tune Llama 3 8B models using LoRA adapters with monolithic preference optimization (ORPO) on Swiss political data from the smartvote voting advice application. The training uses party-labeled comments as preferred choices and comments from different parties on the same issue as rejected choices. The models are evaluated using MAUVE scores for semantic fidelity, Jaccard similarity for diversity, and human preference comparisons. The aligned models can generate contextually accurate political stances based on metadata attributes including party, language, and issue.

## Key Results
- ORPO-aligned models achieve MAUVE scores of 0.64 versus 0.24 for ChatGPT in zero-shot setting
- Human evaluators prefer ORPO-aligned generations 60% of the time
- Jaccard similarity between party responses drops to 0.24 for ORPO models versus higher overlap for zero-shot approaches
- ORPO reduces overlapping generations by roughly half compared to ChatGPT zero-shot

## Why This Works (Mechanism)

### Mechanism 1
Fine-tuning Llama 3 with ORPO on Swiss political data produces more party-accurate and diverse political responses than zero-shot or SFT. ORPO uses a preference ranking loss that explicitly pushes apart model outputs conditioned on different party metadata, whereas SFT only minimizes cross-entropy without enforcing separation. This works because Smartvote party metadata is reliable enough that ORPO can meaningfully separate viewpoints.

### Mechanism 2
Jaccard similarity and MAUVE metrics together capture both diversity and semantic alignment of generated political views. Low Jaccard similarity indicates distinct word usage across parties, while high MAUVE indicates the generated text distribution matches human reference distribution in embedding space. These metrics capture complementary aspects of generation quality (lexical diversity + semantic fidelity).

### Mechanism 3
Human evaluators prefer ORPO-aligned outputs because they are more accurate and nuanced in reflecting party positions. Fine-tuning on party-labeled Swiss political comments enables the model to generate contextually accurate stances, which human evaluators recognize. This assumes evaluators have sufficient political knowledge to distinguish accuracy and nuance in generated stances.

## Foundational Learning

- **Concept: Preference Optimization (ORPO)**
  - Why needed here: Provides a way to fine-tune models so that responses conditioned on different metadata (party) are more distinct and accurate
  - Quick check question: What loss function does ORPO optimize to separate preferred and rejected choices?

- **Concept: MAUVE Metric**
  - Why needed here: Quantifies semantic overlap between generated text and human references without requiring exact string matches
  - Quick check question: How does MAUVE use LLM representations to measure similarity?

- **Concept: Conditional Generation**
  - Why needed here: Enables the model to generate politically aligned responses based on metadata (party, language, issue)
  - Quick check question: What metadata attributes are used to condition the model in this study?

## Architecture Onboarding

- **Component map**: Data ingestion -> Smartvote comments + metadata -> Preprocessing -> Tokenization, LoRA adapter preparation -> Training -> ORPO fine-tuning (supervised + preference loss) -> Evaluation -> Jaccard similarity, MAUVE, human preference -> Deployment -> Prompt template with party/language/issue variables

- **Critical path**:
  1. Load and preprocess Smartvote data with metadata
  2. Prepare ORPO preference pairs (preferred vs rejected comments)
  3. Fine-tune Llama 3 with LoRA using ORPO loss
  4. Evaluate diversity (Jaccard) and semantic fidelity (MAUVE)
  5. Conduct human evaluation if needed

- **Design tradeoffs**:
  - ORPO vs SFT: ORPO enforces diversity across parties but may be harder to stabilize; SFT is simpler but less discriminative
  - LoRA vs full fine-tuning: LoRA is parameter-efficient but may limit adaptation capacity
  - Zero-shot vs few-shot: Zero-shot avoids data collection overhead but yields low quality; few-shot balances data needs and performance

- **Failure signatures**:
  - Low Jaccard diversity but high MAUVE: model is semantically close to references but not diverse enough across parties
  - Low MAUVE overall: generated text is semantically far from references, indicating poor alignment
  - Human preference split evenly: model fails to capture party-specific nuances accurately

- **First 3 experiments**:
  1. Run zero-shot Llama 3 on Swiss party prompts and compute Jaccard similarity and MAUVE
  2. Fine-tune Llama 3 with SFT on Smartvote data and evaluate same metrics
  3. Fine-tune Llama 3 with ORPO and compare all metrics to zero-shot and SFT results

## Open Questions the Paper Calls Out

### Open Question 1
How do political biases in large language models (LLMs) like ChatGPT affect user behavior and political decision-making? The paper acknowledges that LLMs can influence user views and behavior, but it does not provide empirical evidence on the extent of this influence in political contexts. Conducting controlled experiments to measure changes in user political views and voting intentions after interacting with biased versus unbiased LLMs would help resolve this question.

### Open Question 2
Can the proposed ORPO alignment method effectively mitigate political biases in LLMs across different languages and cultural contexts? The paper demonstrates the effectiveness of ORPO in aligning LLMs with Swiss political viewpoints, but it does not explore the method's applicability to other languages or cultural settings. Applying the ORPO alignment method to LLMs using political data from other countries and languages, and evaluating the resulting models' ability to generate unbiased political content, would help resolve this question.

### Open Question 3
What are the potential risks and ethical implications of using LLMs to generate balanced political overviews? The paper highlights the need for further research on the promises and pitfalls of AI in political contexts but does not delve into specific risks or ethical considerations. Conducting a comprehensive analysis of the potential risks, such as the reinforcement of existing biases or the manipulation of public opinion, and developing guidelines for the responsible use of LLMs in political contexts, would help resolve this question.

## Limitations
- The study uses a single country's political data (Switzerland), limiting generalizability to other political systems
- Human evaluation involved only 100 comparisons, which may not be statistically robust for all claim clusters
- The paper doesn't address potential feedback loops where aligned models could amplify certain viewpoints or create echo chambers

## Confidence
- **Medium confidence** in the overall claim that ORPO fine-tuning produces more accurate and diverse political viewpoints, based on MAUVE scores and human evaluation results
- **Low confidence** in generalizability to other political systems, as the study focuses exclusively on Swiss parliamentary candidates and issues
- **Medium confidence** in the mechanism that ORPO's preference ranking loss explicitly separates viewpoints by party metadata, though the evaluation could be more comprehensive

## Next Checks
1. **Cross-jurisdiction validation**: Test the ORPO alignment approach on political data from other countries (e.g., US, UK, Germany) to assess generalizability and identify if the method works across different political systems with varying party structures

2. **Long-term stability assessment**: Evaluate whether the ORPO-aligned models maintain their diversity and accuracy over extended use periods, including potential drift in generated political content when exposed to real-world conversations

3. **Comprehensive bias audit**: Conduct a systematic audit for new forms of bias that might emerge from the alignment process, including examining whether the method inadvertently favors certain political ideologies or marginalizes minority viewpoints within the Swiss context