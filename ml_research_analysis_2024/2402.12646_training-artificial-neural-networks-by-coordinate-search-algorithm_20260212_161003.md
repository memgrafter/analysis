---
ver: rpa2
title: Training Artificial Neural Networks by Coordinate Search Algorithm
arxiv_id: '2402.12646'
source_url: https://arxiv.org/abs/2402.12646
tags:
- training
- optimization
- neural
- data
- network
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a gradient-free Coordinate Search (CS) algorithm
  for training artificial neural networks (ANNs). The method addresses limitations
  of gradient-based approaches, such as the requirement for differentiable activation
  functions and inability to handle multiple independent loss functions.
---

# Training Artificial Neural Networks by Coordinate Search Algorithm

## Quick Facts
- arXiv ID: 2402.12646
- Source URL: https://arxiv.org/abs/2402.12646
- Reference count: 28
- Primary result: A gradient-free Coordinate Search (CS) algorithm is introduced for training ANNs, showing comparable performance to SGD on full MNIST and outperforming SGD with limited data.

## Executive Summary
This paper presents a gradient-free Coordinate Search (CS) algorithm for training artificial neural networks, addressing the limitations of gradient-based approaches such as the need for differentiable activation functions and inability to handle multiple independent loss functions. The proposed method uses a two-extreme-point search strategy and bundles weights to reduce computational complexity. Experimental results on the MNIST-Digit dataset demonstrate that the CS algorithm is comparable to Stochastic Gradient Descent (SGD) and outperforms it in scenarios with insufficient training data.

## Method Summary
The proposed method employs a two-extreme-point Coordinate Search (CS) algorithm with bundled weights to optimize the weights of fully connected neural networks. The algorithm uses a derivative-free approach, making it suitable for non-differentiable activation functions and multi-objective problems. Weights are grouped into bundles, and in each iteration, the algorithm evaluates two extreme points for each bundle using a subset of the training data. The bundle with the better fitness becomes the new center, and the opposite bound is contracted by a shrinkage factor. This process is repeated until convergence or a maximum number of iterations is reached.

## Key Results
- The CS algorithm achieves a test accuracy of 96.12% on the full MNIST dataset, comparable to SGD.
- With only 1,000 training samples, CS achieves 77.4% accuracy compared to SGD's 71.1%.
- The method demonstrates fast convergence, reaching 92.11% accuracy within 10 iterations.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Coordinate Search with two-extreme-point sampling finds better directions faster than center-point sampling for high-dimensional weight optimization.
- Mechanism: For each weight coordinate, the algorithm evaluates the lower and upper bounds of its current box-constraint. The bound with the better fitness becomes the new center, and the opposite bound is contracted by 5%. This binary search on each dimension drives the search space to shrink exponentially while still exploring extreme values early.
- Core assumption: Weight coordinates are largely independent in their contribution to loss, so one-dimensional coordinate-wise exploration suffices to approach the optimum.
- Evidence anchors:
  - [abstract] "We propose an efficient version of the gradient-free Coordinate Search (CS) algorithm... The proposed algorithm can be used with non-differentiable activation functions and tailored to multi-objective/multi-loss problems."
  - [section II-A] "In two-extreme-point CS, two sample points (i.e., extreme points) for each variable are evaluated in each iteration."
- Break condition: If weights are highly coupled, this independent coordinate update will miss interactions and stall progress.

### Mechanism 2
- Claim: Bundling weights reduces function call cost and improves scalability for huge-scale neural networks.
- Mechanism: Instead of optimizing one weight at a time, weights are grouped into bundles. During a bundle's turn, all weights in the bundle are updated simultaneously to either their lower or upper bound depending on the bundle's aggregate fitness. This reduces the number of coordinate updates from D to D/BS per iteration.
- Core assumption: Updating a group of weights together preserves enough search diversity while drastically cutting the number of fitness evaluations needed.
- Evidence anchors:
  - [abstract] "Finding the optimal values for weights of ANNs is a large-scale optimization problem. Therefore instead of finding the optimal value for each variable... we accelerate optimization and convergence by bundling the variables (i.e., weights)."
  - [section II-B] "we bundle all variables into n bundles, where n = Nw/BS... In each iteration, all variables in a bundle are optimized simultaneously."
- Break condition: If bundle size is too large, early exploration is weakened and premature convergence occurs.

### Mechanism 3
- Claim: Using a small subset of training data per fitness evaluation yields fast convergence with minimal loss in accuracy.
- Mechanism: The training set is split into folds. In each fitness call, only one fold is fed to the network to compute loss. This mimics mini-batch SGD but is compatible with a black-box optimizer. Over many iterations, the network sees all data.
- Core assumption: Each fold provides a statistically representative gradient direction, so per-fold evaluation approximates full-batch training.
- Evidence anchors:
  - [abstract] "Furthermore, the training (i.e., optimization of weights) in any DNN can be possible with a small size of the training dataset."
  - [section II-D] "Instead of using the whole data for training, data can be separated into different folds... the network will see all the training data in multiple phases, and accuracy will not suffer very much."
- Break condition: If folds are too small or not shuffled, training variance increases and convergence degrades.

## Foundational Learning

- Concept: Coordinate Descent vs Coordinate Search
  - Why needed here: CS is derivative-free, enabling training with non-differentiable activations, while CD relies on exact gradients.
  - Quick check question: What is the key difference between CD and CS in terms of derivative usage?
- Concept: Bundle Size Trade-off
  - Why needed here: Larger bundles speed up iterations but reduce exploration; smaller bundles increase diversity but raise cost.
  - Quick check question: How does changing bundle size affect the number of function calls per iteration?
- Concept: Box-Constraint Shrinkage
  - Why needed here: Exponentially shrinking bounds focus the search but must balance exploration early and exploitation later.
  - Quick check question: What is the role of the Box-Constraint Shrinkage Factor (BSF) in the algorithm?

## Architecture Onboarding

- Component map:
  - Data loader -> Folds (whole/batch/sliding) -> Weight vector -> Bundles -> Box-constraints -> Fitness evaluator -> Forward pass on selected fold -> Loss computation -> Optimizer loop -> CS coordinate update + bundle shuffling
- Critical path:
  1. Initialize weights (random normal μ=0, σ=0.1 recommended).
  2. Split training data into folds.
  3. For each iteration: shuffle bundles, for each bundle evaluate two extreme points using one fold, update bounds and center, shrink box-constraint.
  4. Repeat until max iterations or early-stop threshold.
- Design tradeoffs:
  - Bundle size vs convergence speed: larger -> fewer function calls, smaller -> better exploration.
  - Fold size vs variance: larger folds -> stable loss, smaller folds -> faster iterations.
  - BSF value vs search space contraction: higher -> faster focus, lower -> more exploration.
- Failure signatures:
  - Accuracy plateaus early -> bundle size too large or BSF too aggressive.
  - High training loss variance -> fold size too small or insufficient shuffling.
  - Slow convergence -> too many folds or overly conservative BSF.
- First 3 experiments:
  1. MNIST 784-300-100-10, BS=50, 10 folds, random normal init, 50 iterations -> baseline accuracy.
  2. Same as (1) but BS=100 -> measure speedup vs accuracy drop.
  3. Same as (1) but feed whole data each call -> compare accuracy vs (1).

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of the Coordinate Search (CS) algorithm compare to other gradient-free optimization methods, such as Genetic Algorithms (GA) or Particle Swarm Optimization (PSO), for training large-scale neural networks?
- Basis in paper: [explicit] The paper mentions that metaheuristic algorithms like GA and PSO have been used for training ANNs but are usually very time-consuming or used for small networks. It suggests that the proposed CS algorithm is more efficient for large-scale problems.
- Why unresolved: The paper only compares the CS algorithm to Stochastic Gradient Descent (SGD) and does not provide a direct comparison with other gradient-free methods like GA or PSO.
- What evidence would resolve it: Experimental results comparing the performance of the CS algorithm with GA, PSO, and other gradient-free methods on various large-scale neural network architectures and datasets.

### Open Question 2
- Question: Can the proposed CS algorithm be effectively applied to train neural networks with non-differentiable activation functions, such as ReLU or Leaky ReLU, and how does its performance compare to gradient-based methods in such cases?
- Basis in paper: [explicit] The paper states that the proposed CS algorithm can work with non-differentiable activation functions and is structure-independent, making it suitable for graph-based networks. However, it does not provide experimental results to support this claim.
- Why unresolved: The paper does not present any experiments or results demonstrating the performance of the CS algorithm with non-differentiable activation functions.
- What evidence would resolve it: Experimental results showing the performance of the CS algorithm on neural networks with non-differentiable activation functions, compared to gradient-based methods, on various datasets and network architectures.

### Open Question 3
- Question: How does the performance of the CS algorithm scale with the size and complexity of the neural network, and what are the limitations in terms of the number of layers, nodes, and weights that can be effectively optimized using this method?
- Basis in paper: [inferred] The paper demonstrates the effectiveness of the CS algorithm on a fully connected neural network with 266,610 weights. However, it does not provide information on how the algorithm performs on larger or more complex networks, such as deep convolutional neural networks or recurrent neural networks.
- Why unresolved: The paper does not explore the scalability of the CS algorithm to larger and more complex neural network architectures.
- What evidence would resolve it: Experimental results showing the performance of the CS algorithm on various neural network architectures with different numbers of layers, nodes, and weights, and analysis of the limitations and scalability of the method.

## Limitations
- The scalability of the two-extreme-point CS algorithm to larger neural networks remains untested.
- The sensitivity of performance to hyperparameter choices like bundle size and box-constraint shrinkage factor is not fully explored.
- The general applicability of the method to datasets beyond MNIST is not demonstrated.

## Confidence
- High: The algorithm can train ANNs without gradients and works with non-differentiable activations.
- Medium: Performance is "comparable" to SGD on full MNIST and "outperforms" on small datasets, given the experimental evidence provided.
- Low: Claims about fast convergence (92.11% within 10 iterations) and general applicability to any DNN without further hyperparameter tuning.

## Next Checks
1. Test the CS algorithm on a larger, more complex dataset (e.g., CIFAR-10) to evaluate scalability and robustness beyond MNIST.
2. Conduct an ablation study varying bundle size, box-constraint shrinkage factor, and fold size to map the sensitivity of performance to these hyperparameters.
3. Compare the convergence speed and final accuracy of CS against a broader set of optimizers (e.g., Adam, RMSprop) on both full and reduced datasets.