---
ver: rpa2
title: 'MedMax: Mixed-Modal Instruction Tuning for Training Biomedical Assistants'
arxiv_id: '2412.12661'
source_url: https://arxiv.org/abs/2412.12661
tags:
- image
- biomedical
- data
- generation
- visual
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: MedMax introduces a large-scale multimodal biomedical instruction-tuning
  dataset of 1.47 million instances to train mixed-modal foundation models. It covers
  diverse tasks including image captioning, generation, VQA, visual chat, and report
  understanding across radiology and histopathology domains.
---

# MedMax: Mixed-Modal Instruction Tuning for Training Biomedical Assistants

## Quick Facts
- arXiv ID: 2412.12661
- Source URL: https://arxiv.org/abs/2412.12661
- Authors: Hritik Bansal, Daniel Israel, Siyan Zhao, Shufan Li, Tung Nguyen, Aditya Grover
- Reference count: 40
- Primary result: 26% improvement over Chameleon and 18.3% improvement over GPT-4o on biomedical VQA tasks

## Executive Summary
MedMax introduces a large-scale multimodal biomedical instruction-tuning dataset of 1.47 million instances to train mixed-modal foundation models. The dataset covers diverse tasks including image captioning, generation, VQA, visual chat, and report understanding across radiology and histopathology domains. Fine-tuning a mixed-modal model on MedMax yields significant performance gains, establishing a new state-of-the-art for biomedical VQA tasks while enabling interleaved image-text generation capabilities.

## Method Summary
The MedMax approach combines diverse biomedical data sources including medical papers and YouTube videos to create instruction-tuning data spanning VQA, image captioning/generation, visual chat, and report understanding tasks. The dataset is filtered using BioMedCLIPScore and GPT-4o-mini to ensure quality, then used to fine-tune a mixed-modal autoregressive transformer (Anole-7B) with LoRA adapters. The model processes interleaved text and image tokens using VQGAN discrete representations and is evaluated on 12 downstream VQA tasks using a combination of exact match and LLM-based scoring metrics.

## Key Results
- Achieves 26% performance gain over Chameleon and 18.3% improvement over GPT-4o across 12 VQA tasks
- Establishes new state-of-the-art for biomedical multimodal reasoning
- Enables interleaved image-text generation capabilities through instruction tuning
- Provides unified evaluation suite for guiding future multimodal biomedical model development

## Why This Works (Mechanism)

### Mechanism 1
- Claim: MedMax achieves strong downstream performance through diverse task coverage across multiple biomedical domains and modalities.
- Mechanism: By combining VQA, image captioning/generation, visual chat, and report understanding tasks from radiology and histopathology domains, MedMax exposes the model to varied biomedical contexts, improving generalization.
- Core assumption: Diverse tasks and domains in instruction tuning lead to better multimodal reasoning and generation capabilities.
- Evidence anchors:
  - [abstract] "MedMax encompasses a diverse range of tasks, including interleaved image-text generation, biomedical image captioning and generation, visual chat, and report understanding."
  - [section 3.2] "These tasks span knowledge across diverse biomedical domains, including radiology and histopathology, grounded in medical papers and YouTube videos."
  - [corpus] Weak - corpus focuses on related mixed-modal works but not specifically on task diversity impact.
- Break condition: If the model fails to generalize across unseen tasks or domains, indicating insufficient diversity in training data.

### Mechanism 2
- Claim: High-quality curated data filtering improves model performance by reducing noise.
- Mechanism: Using BioMedCLIPScore to filter images and GPT-4o-mini to assess caption quality ensures that only relevant, high-quality biomedical images and descriptions are used for training.
- Core assumption: Cleaner, more relevant data leads to better model learning and downstream performance.
- Evidence anchors:
  - [section 3.1] "To filter this data, we utilize BioMedCLIPScore [65] score... Following this, we are left with 88K instances, leading to the removal of 25% captions."
  - [section 3.1] "Since we aim to generate multimodal conversations conditioned on the captions, it is critical to ensure that they describe the visual contents of the image well."
  - [corpus] Weak - corpus does not discuss data filtering techniques specifically.
- Break condition: If performance degrades on downstream tasks, suggesting the filtering process removed useful data.

### Mechanism 3
- Claim: Instruction tuning with interleaved image-text generation data enables novel multimodal generation capabilities.
- Mechanism: MedMax-Instruct introduces a new dataset for generating interleaved image-text content, allowing the model to produce multimodal responses grounded in biomedical text queries.
- Core assumption: Models pretrained on internet-scale data can learn to generate interleaved multimodal content when fine-tuned on appropriately structured instruction data.
- Evidence anchors:
  - [abstract] "MedMax introduces a large-scale multimodal biomedical instruction-tuning dataset of 1.47 million instances to train mixed-modal foundation models."
  - [section 3.1] "MedMax-Instruct, a multimodal generation instruction-tuning dataset for biomedical mixed-modal assistants, created in three stages as described."
  - [corpus] Weak - corpus mentions related mixed-modal works but not specifically interleaved generation.
- Break condition: If the model cannot generate coherent interleaved image-text responses, indicating the instruction tuning data was insufficient.

## Foundational Learning

- Concept: Autoregressive sequence modeling for multimodal data
  - Why needed here: Chameleon uses autoregressive modeling to predict next tokens in interleaved text-image sequences, enabling generation of multimodal content.
  - Quick check question: How does autoregressive modeling differ from diffusion models in handling multimodal data?

- Concept: Instruction tuning for specialized domain adaptation
  - Why needed here: While foundation models have broad knowledge, instruction tuning with domain-specific data teaches them to perform specialized biomedical tasks effectively.
  - Quick check question: Why is instruction tuning preferred over continued pretraining for adapting to new domains?

- Concept: Visual question answering (VQA) evaluation metrics
  - Why needed here: The evaluation suite uses exact match for closed-ended questions and LLM scoring for open-ended questions to assess model performance across different question types.
  - Quick check question: What are the advantages and limitations of using LLM-based evaluation for open-ended VQA?

## Architecture Onboarding

- Component map: VQGAN encoder -> Discrete token representation -> Transformer layers -> Discrete token output -> VQGAN decoder

- Critical path:
  1. Tokenize input (text + image)
  2. Pass through transformer layers
  3. Generate output tokens autoregressively
  4. Convert discrete tokens back to text/image as needed

- Design tradeoffs:
  - Discrete tokens vs continuous representations: Discrete tokens enable autoregressive generation but may lose fine-grained visual information
  - Early fusion vs late fusion: Early fusion (Chameleon) simplifies architecture but may limit modality-specific processing
  - LoRA fine-tuning vs full fine-tuning: LoRA is parameter-efficient but may limit capacity for learning new tasks

- Failure signatures:
  - Poor image generation: Likely issues with VQGAN encoder or token representation
  - Inaccurate text responses: May indicate insufficient instruction tuning data or model capacity
  - Mode collapse in generation: Could suggest overfitting or insufficient diversity in training data

- First 3 experiments:
  1. Test tokenization pipeline: Verify that images and text are correctly converted to discrete tokens and can be reconstructed
  2. Validate LoRA fine-tuning: Check that LoRA adapters are updating correctly and not causing instability
  3. Evaluate basic generation: Test simple text and image generation before moving to complex interleaved tasks

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the quality of the MedMax dataset scale with increasing size, and what are the specific diminishing returns or saturation points in downstream performance?
- Basis in paper: [inferred] from Section 6.1, which shows performance increasing monotonically with data scale but does not explore saturation or quality thresholds.
- Why unresolved: The paper only tests up to 100% of the data and does not analyze whether further scaling would yield meaningful gains or whether quality degrades at larger scales.
- What evidence would resolve it: Systematic experiments scaling beyond the current dataset size, measuring both performance and data quality metrics (e.g., caption relevance, task diversity).

### Open Question 2
- Question: How do different types of biomedical domains (e.g., radiology vs. histopathology) contribute to the overall performance of the MedMax model, and are there domain-specific weaknesses?
- Basis in paper: [explicit] from Section 6.2, which ablates VQA and visual chat data but does not isolate domain-specific contributions or analyze per-domain performance gaps.
- Why unresolved: The ablation studies focus on task types but not on whether certain biomedical domains (e.g., radiology vs. histopathology) are better represented or lead to higher performance.
- What evidence would resolve it: Detailed breakdown of model performance by domain, with targeted experiments to identify and address domain-specific weaknesses.

### Open Question 3
- Question: What is the impact of fine-tuning the visual encoder (VQGAN) on biomedical images before instruction tuning, and why does the fine-tuned encoder lead to worse performance?
- Basis in paper: [explicit] from Section 6.3, which finds that fine-tuning VQGAN on biomedical images results in a 3% performance drop compared to the original encoder.
- Why unresolved: The paper does not explore the underlying reasons for this performance drop or whether alternative fine-tuning strategies could mitigate the issue.
- What evidence would resolve it: Analysis of token distribution shifts, experiments with alternative fine-tuning objectives (e.g., contrastive learning), and evaluation of hybrid approaches that combine base and fine-tuned encoders.

## Limitations

- Performance improvements rely on proprietary evaluation setup that may not generalize to real-world clinical scenarios
- Data filtering process may introduce domain-specific biases by removing 25% of captions
- Claims about "novel" multimodal generation capabilities are overstated given reliance on existing Chameleon architecture

## Confidence

**High Confidence**: The methodology for creating the MedMax dataset (data collection, filtering, and instruction generation) is clearly described and reproducible. The evaluation framework using 12 VQA tasks with standardized metrics is well-defined.

**Medium Confidence**: The performance improvements over baselines are credible given the dataset scale and diversity, but the absolute performance numbers may not translate directly to clinical utility. The 1.47 million instances represent substantial training data, but the quality distribution across tasks isn't fully characterized.

**Low Confidence**: The claim that MedMax "establishes a new state-of-the-art" is limited by the evaluation scope. The comparison only covers VQA tasks and doesn't address other multimodal capabilities or clinical validation requirements.

## Next Checks

1. **Clinical Relevance Validation**: Test the fine-tuned model on actual clinical decision support scenarios with domain experts to verify that benchmark performance translates to practical utility. This should include both accuracy assessment and workflow integration studies.

2. **Data Bias Analysis**: Conduct a systematic analysis of the filtered dataset to identify potential biases introduced by the BioMedCLIPScore filtering process. Specifically examine whether certain biomedical subdomains (e.g., rare conditions, specific imaging modalities) are underrepresented.

3. **Generalization Testing**: Evaluate the model's performance on out-of-distribution biomedical tasks not included in the training set, such as novel imaging modalities or interdisciplinary medical queries, to assess true generalization capabilities beyond the 12 benchmark tasks.