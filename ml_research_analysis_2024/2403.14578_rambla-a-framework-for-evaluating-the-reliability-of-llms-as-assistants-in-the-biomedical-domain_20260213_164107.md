---
ver: rpa2
title: 'RAmBLA: A Framework for Evaluating the Reliability of LLMs as Assistants in
  the Biomedical Domain'
arxiv_id: '2403.14578'
source_url: https://arxiv.org/abs/2403.14578
tags:
- question
- context
- answer
- task
- prompt
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces RAmBLA, a framework for evaluating the reliability
  of LLMs as assistants in the biomedical domain. The framework assesses three key
  criteria: prompt robustness, high recall, and lack of hallucinations.'
---

# RAmBLA: A Framework for Evaluating the Reliability of LLMs as Assistants in the Biomedical Domain

## Quick Facts
- arXiv ID: 2403.14578
- Source URL: https://arxiv.org/abs/2403.14578
- Authors: William James Bolton; Rafael Poyiadzi; Edward R. Morrell; Gabriela van Bergen Gonzalez Bueno; Lea Goetz
- Reference count: 36
- One-line primary result: Larger LLMs exhibit lower hallucination tendencies, better recall, and robustness to prompt phrasing compared to smaller models in biomedical tasks.

## Executive Summary
This paper introduces RAmBLA, a framework for evaluating the reliability of LLMs as assistants in the biomedical domain. The framework assesses three key criteria: prompt robustness, high recall, and lack of hallucinations. It uses shortform tasks and freeform tasks mimicking real-world user interactions, evaluating performance using semantic similarity with a ground truth response through an evaluator LLM. Experiments on four state-of-the-art foundation LLMs (GPT-4, GPT-3.5, Llama, and Mistral) show that larger models exhibit lower hallucination tendencies, better recall, and robustness to prompt phrasing compared to smaller models. However, all models struggle with more complex tasks like summarization and conclusion generation, highlighting the need for further research on LLM reliability in real-world use cases.

## Method Summary
The RAmBLA framework evaluates LLM reliability using PubMedQA-Labelled and Bioasq datasets for shortform and freeform tasks respectively. It employs GPT-4 as an evaluator LLM to measure semantic similarity between generated and ground truth responses. The framework tests prompt robustness through paraphrasing, assesses recall by measuring answer completeness, and detects hallucinations by checking for unsupported claims. The method involves running tasks across four foundation LLMs and analyzing performance across the three reliability criteria.

## Key Results
- Larger models (GPT-4) exhibit lower hallucination tendencies and better recall than smaller models
- All models struggle with complex tasks like summarization compared to simple question-answering
- Model robustness to prompt phrasing varies significantly, with larger models showing better consistency

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Larger LLMs exhibit lower hallucination tendencies and better recall in biomedical tasks.
- Mechanism: Model scale correlates with improved capability to follow instructions and maintain factual consistency, reducing hallucinatory outputs while improving context utilization.
- Core assumption: Model size directly impacts the model's ability to maintain factual consistency and follow complex instructions.
- Evidence anchors:
  - [abstract]: "Experiments on four state-of-the-art foundation LLMs... show that larger models exhibit lower hallucination tendencies, better recall, and robustness to prompt phrasing compared to smaller models."
  - [section 5]: "Overall, larger models had a lower tendency to hallucinate and were able to refuse answers where they lacked knowledge."
- Break condition: If model scaling hits diminishing returns or smaller models achieve similar performance through fine-tuning.

### Mechanism 2
- Claim: Semantic similarity evaluation using an LLM evaluator provides reliable assessment of freeform responses.
- Mechanism: GPT-4 evaluator can accurately judge semantic similarity between generated and ground truth responses, enabling automated evaluation of complex tasks.
- Core assumption: LLM evaluators can match or exceed human judgment quality for semantic similarity tasks.
- Evidence anchors:
  - [section 3.3]: "In all freeform tasks, we measure semantic similarity using GPT-4 as an evaluator LLM, due to its superior performance in our evaluation of several semantic similarity methods."
- Break condition: If evaluator model shows bias or alignment with ground truth decreases over time.

### Mechanism 3
- Claim: Task design mimicking real-world interactions provides more realistic evaluation of LLM reliability.
- Mechanism: Shortform and freeform tasks designed to reflect actual user interactions reveal limitations not captured by traditional benchmarks.
- Core assumption: Real-world task scenarios better reveal reliability issues than artificial benchmarks.
- Evidence anchors:
  - [abstract]: "We design shortform tasks and tasks requiring LLM freeform responses mimicking real-world user interactions."
- Break condition: If task design becomes too domain-specific to generalize or fails to capture key reliability aspects.

## Foundational Learning

- Concept: Semantic similarity evaluation
  - Why needed here: Core to evaluating freeform responses where ground truth exists but answers may vary syntactically
  - Quick check question: Can you explain how semantic similarity differs from exact string matching and why this matters for evaluating LLM outputs?

- Concept: Prompt engineering and robustness
  - Why needed here: Critical for ensuring models respond consistently across variations in how questions are asked
  - Quick check question: What are the key differences between robustness to spelling errors versus robustness to semantic paraphrases?

- Concept: Biomedical domain knowledge
  - Why needed here: Understanding domain-specific context is essential for interpreting task design and results
  - Quick check question: How does the specialized nature of biomedical literature impact the design of evaluation tasks?

## Architecture Onboarding

- Component map: PubMedQA-Labelled dataset -> Shortform task generator -> Model responses -> Semantic evaluator -> Results aggregator
- Critical path: Task generation → Model response → Semantic evaluation → Results analysis
- Design tradeoffs:
  - Using LLM evaluator trades computational cost for evaluation quality
  - Real-world task design trades standardization for realism
  - Dataset size vs. diversity in biomedical domain
- Failure signatures:
  - Inconsistent evaluator responses across similar inputs
  - Model overfitting to specific prompt formats
  - Evaluation metrics not correlating with human judgment
- First 3 experiments:
  1. Run baseline QA task with all four models to establish performance floor
  2. Test prompt paraphrasing with single model to validate robustness
  3. Compare evaluator consistency by running same responses through multiple times

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the reliability of LLMs in the biomedical domain change when evaluated on more complex tasks like summarization or conclusion generation compared to simpler question-answering tasks?
- Basis in paper: [explicit] The paper states that "all models performed better at simple question answering compared to more realistic responses such as summarisation or conclusion generation."
- Why unresolved: The paper only provides a sample of LLM performance and does not explore the full range of potential prompts relevant to the tasks.
- What evidence would resolve it: Additional experiments evaluating LLM performance on a wider variety of tasks and prompt variations, along with an analysis of the factors contributing to performance differences.

### Open Question 2
- Question: To what extent does fine-tuning LLMs on medical data improve their reliability in the biomedical domain?
- Basis in paper: [explicit] The paper mentions that "prior research has indicated that fine-tuning on medical data can improve performance on relevant downstream tasks" but notes that "it remains to be seen whether this performance improvement translates into an improvement in the aspects of reliability highlighted in RAmBLA."
- Why unresolved: The paper does not evaluate the performance of fine-tuned models on the tasks in the RAmBLA framework.
- What evidence would resolve it: Experiments comparing the performance of pre-trained and fine-tuned LLMs on the RAmBLA tasks, with a focus on the aspects of reliability (prompt robustness, high recall, and lack of hallucinations).

### Open Question 3
- Question: How can LLM reliability in high-risk scenarios, such as applications impacting patients, be improved to the level required for responsible use?
- Basis in paper: [explicit] The paper states that "LLMs are not ready for delegation in high-risk scenarios, such as applications impacting patients, because their outputs are difficult to verify even for biomedical domain experts."
- Why unresolved: The paper highlights the need for evaluation frameworks that assess LLM reliability in real-world use cases but does not provide specific solutions for improving reliability in high-risk scenarios.
- What evidence would resolve it: Development and evaluation of methods to improve LLM reliability in high-risk scenarios, such as incorporating human oversight, developing more robust evaluation frameworks, and exploring the use of fine-tuned models.

## Limitations
- Semantic similarity evaluation using GPT-4 lacks direct validation against human judgment benchmarks
- Reliance on two specific datasets limits coverage of broader biomedical domain
- Variable effectiveness of prompt engineering across model sizes with significant performance gaps in complex tasks

## Confidence

**High Confidence**: The finding that larger foundation models exhibit lower hallucination tendencies and better recall in biomedical tasks is supported by multiple experimental results and aligns with broader observations about model scaling effects.

**Medium Confidence**: The effectiveness of semantic similarity evaluation using GPT-4 as an evaluator is supported by internal validation but lacks external validation against human judgment or alternative evaluation methods.

**Low Confidence**: The claim that task design mimicking real-world interactions provides superior evaluation of LLM reliability compared to traditional benchmarks lacks comparative analysis with established evaluation frameworks.

## Next Checks

1. **Evaluator Reliability Test**: Conduct a human-in-the-loop validation study comparing GPT-4 evaluator judgments against domain expert assessments for a subset of responses, particularly for complex freeform tasks like summarization and conclusion generation.

2. **Temporal Knowledge Validation**: Implement a temporal split of the biomedical datasets to test whether model performance degrades for more recent publications, and whether this degradation differs across model sizes.

3. **Cross-domain Generalization**: Test the RAmBLA framework on a non-biomedical expert domain (such as legal or financial) to evaluate whether the identified reliability criteria and evaluation methodology generalize beyond the biomedical context.