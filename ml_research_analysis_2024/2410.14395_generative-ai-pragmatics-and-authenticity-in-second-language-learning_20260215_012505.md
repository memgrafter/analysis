---
ver: rpa2
title: Generative AI, Pragmatics, and Authenticity in Second Language Learning
arxiv_id: '2410.14395'
source_url: https://arxiv.org/abs/2410.14395
tags: []
core_contribution: Generative AI, while proficient at producing grammatically correct
  and coherent text, lacks the lived experience and sociocultural grounding necessary
  for authentic human language use. Its statistical model, trained predominantly on
  English and Western cultural data, results in outputs that are often linguistically
  accurate but pragmatically inappropriate and culturally biased.
---

# Generative AI, Pragmatics, and Authenticity in Second Language Learning

## Quick Facts
- arXiv ID: 2410.14395
- Source URL: https://arxiv.org/abs/2410.14395
- Authors: Robert Godwin-Jones
- Reference count: 0
- Primary result: Generative AI lacks lived experience and sociocultural grounding necessary for authentic language use, resulting in pragmatic inappropriateness and cultural bias.

## Executive Summary
Generative AI, while proficient at producing grammatically correct and coherent text, lacks the lived experience and sociocultural grounding necessary for authentic human language use. Its statistical model, trained predominantly on English and Western cultural data, results in outputs that are often linguistically accurate but pragmatically inappropriate and culturally biased. Studies show AI systems like ChatGPT frequently violate Gricean maxims of communication and struggle with nuanced pragmatic contexts, such as speech acts and implied meanings. These limitations stem from AI's inability to negotiate shared cultural and linguistic common ground, a critical component of authentic human interaction. Consequently, while AI can support language learning through functional tasks and genre awareness, it cannot fully replicate the authenticity of human conversation or serve as a substitute for real-world language practice. Critical AI literacy and supplementary human interaction remain essential for effective second language acquisition.

## Method Summary
The paper analyzes generative AI's linguistic and cultural authenticity in second language learning by examining its pragmatic limitations through established communication frameworks. The methodology involves evaluating AI outputs against Gricean maxims of cooperative communication, assessing cultural bias in training data, and comparing AI-generated texts with human performance in pragmatic contexts. The study draws on discourse completion tasks, role plays, and pragmatic frameworks to identify patterns of inauthenticity and cultural bias in AI language use across multiple languages and cultural contexts.

## Key Results
- AI systems produce grammatically correct but pragmatically inappropriate text due to lack of lived human experience
- AI outputs reflect Western cultural biases due to training data predominantly in English and from Western sources
- AI cannot effectively teach pragmatic competence as it requires interactive negotiation rather than pattern matching

## Why This Works (Mechanism)

### Mechanism 1
- Claim: AI systems produce grammatically correct text but lack pragmatic appropriateness because they have no lived human experience to negotiate sociocultural common ground.
- Mechanism: The statistical probability model in LLMs predicts word sequences based on frequency patterns, not social meaning. Without human sensorimotor interaction, AI cannot infer context-dependent appropriateness.
- Core assumption: Pragmatic competence requires real-world grounding beyond linguistic patterns.
- Evidence anchors:
  - [abstract] "AI systems like ChatGPT frequently violate Gricean maxims of communication and struggle with nuanced pragmatic contexts"
  - [section] "The statistical model of language in AI lacks the sociocultural grounding humans have through sensorimotor interactions"
  - [corpus] Weak - corpus contains related pragmatics work but not direct evidence for this specific claim
- Break condition: If multimodal training or real-world sensor data is integrated into AI systems, the grounding gap could narrow.

### Mechanism 2
- Claim: AI outputs reflect Western cultural biases because training data is predominantly English and sourced from Western digital contexts.
- Mechanism: The LLM training corpus overrepresents Western cultural narratives, values, and linguistic norms, which become encoded as default patterns in generated text.
- Core assumption: Cultural bias in AI output is proportional to cultural skew in training data.
- Evidence anchors:
  - [abstract] "built-in linguistic and cultural biases based on their training data which is mostly in English and predominantly from Western sources"
  - [section] "AI output is often problematic in its linguistic and cultural authenticity... the dataset used to build an LLM is not representative of the world at large"
  - [corpus] Moderate - several papers discuss cultural bias in LLMs but not specific to Western dominance
- Break condition: If training datasets become globally representative across languages and cultures, cultural bias would decrease.

### Mechanism 3
- Claim: AI cannot effectively teach pragmatic competence because pragmatic appropriateness emerges from negotiated interaction, not pre-programmed patterns.
- Mechanism: Pragmatic meaning requires real-time adaptation to interlocutor state, which AI cannot achieve without shared lived experience. Formulaic knowledge is insufficient.
- Core assumption: Pragmatic competence is developed through interactive negotiation, not pattern matching.
- Evidence anchors:
  - [abstract] "AI is incapable of handling the nuances of pragmatic language use, as that requires an ability to negotiate shared cultural and linguistic common ground"
  - [section] "Pragmatic appropriateness is not pre-set but is developed in interaction... Contextual factors such as social distance, relative power, ranking of imposition determine pragmatic language"
  - [corpus] Strong - multiple corpus papers discuss pragmatic reasoning in LLMs
- Break condition: If AI systems develop real-time adaptive reasoning capabilities that simulate negotiation, this limitation could be reduced.

## Foundational Learning

- Concept: Gricean maxims of cooperative communication
  - Why needed here: The paper uses violations of Gricean maxims as a primary metric for evaluating AI pragmatic competence
  - Quick check question: What are the four Gricean maxims and how might an AI violate each in conversation?

- Concept: Statistical language modeling vs. sociocultural language acquisition
  - Why needed here: The paper contrasts AI's mathematical approach to language with human socialization-based learning
  - Quick check question: How does the statistical probability model in LLMs differ fundamentally from how humans acquire language pragmatics?

- Concept: Cultural grounding in language use
  - Why needed here: The paper emphasizes that authentic language requires cultural context that AI lacks
  - Quick check question: Why is shared cultural background essential for pragmatic appropriateness in conversation?

## Architecture Onboarding

- Component map: Language generation system (statistical model) → Output layer (text) → Evaluation layer (pragmatic appropriateness testing) → Feedback loop (training data updates)
- Critical path: Training data collection → Model training → Language generation → Pragmatic evaluation → Performance analysis
- Design tradeoffs: Statistical accuracy vs. cultural authenticity; computational efficiency vs. multimodal grounding; general capability vs. cultural specificity
- Failure signatures: Output that is grammatically correct but contextually inappropriate; systematic cultural bias; inability to resolve implied meanings; overly formal or verbose responses
- First 3 experiments:
  1. Test AI output against Gricean maxim violations across different cultural contexts
  2. Compare AI-generated text with human-generated text for pragmatic appropriateness in speech acts
  3. Evaluate AI performance on implied meaning resolution tasks with and without fine-tuning

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can AI systems be effectively trained to better handle pragmatic language use in second language acquisition contexts?
- Basis in paper: [explicit] The paper discusses the limitations of AI in pragmatic language use and suggests that additional training could improve AI's pragmatic abilities.
- Why unresolved: The paper acknowledges the potential for improved training but does not specify the methods or effectiveness of such training.
- What evidence would resolve it: Studies demonstrating the impact of enhanced training techniques on AI's pragmatic language performance in diverse linguistic and cultural contexts.

### Open Question 2
- Question: What are the specific impacts of AI's cultural biases on second language learners' pragmatic competence and intercultural communication skills?
- Basis in paper: [explicit] The paper highlights AI's cultural biases and their implications for language learning and intercultural communication.
- Why unresolved: The paper identifies the presence of cultural biases but does not explore their direct effects on learners' pragmatic competence.
- What evidence would resolve it: Empirical research comparing learners' pragmatic and intercultural communication skills when using AI tools versus human interactions.

### Open Question 3
- Question: Can AI-generated content be adapted to better reflect the linguistic and cultural nuances required for authentic language learning experiences?
- Basis in paper: [inferred] The paper suggests that AI lacks authenticity in language use, implying a need for adaptation to better serve language learners.
- Why unresolved: The paper does not provide specific strategies or examples of how AI content could be adapted for authenticity.
- What evidence would resolve it: Case studies or experiments showing successful adaptations of AI-generated content to enhance authenticity in language learning settings.

## Limitations

- Limited direct empirical evidence linking AI training data composition to specific pragmatic failures
- Indirect measurement of cultural bias based on training data composition rather than direct output analysis
- Insufficient exploration of multimodal training's potential to bridge the grounding gap

## Confidence

- **High Confidence**: AI systems produce grammatically correct but pragmatically inappropriate text due to lack of lived experience and sociocultural grounding.
- **Medium Confidence**: AI outputs reflect Western cultural biases due to training data skew, but direct measurement of cultural bias in outputs is limited.
- **Low Confidence**: AI cannot effectively teach pragmatic competence through pattern matching alone, as this requires interactive negotiation not yet demonstrated in current systems.

## Next Checks

1. **Cross-Cultural Pragmatic Testing**: Conduct systematic evaluations of AI outputs across diverse cultural contexts using standardized pragmatic appropriateness metrics to quantify cultural bias and contextual appropriateness.
2. **Multimodal Training Impact**: Investigate whether integrating multimodal data (e.g., images, sensor inputs) into AI training improves pragmatic competence and reduces cultural bias.
3. **Human-AI Interaction Analysis**: Compare AI-human interactions with human-human interactions in terms of pragmatic negotiation and cultural grounding to identify specific failure modes in AI communication.