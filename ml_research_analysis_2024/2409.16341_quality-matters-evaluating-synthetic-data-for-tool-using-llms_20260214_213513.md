---
ver: rpa2
title: 'Quality Matters: Evaluating Synthetic Data for Tool-Using LLMs'
arxiv_id: '2409.16341'
source_url: https://arxiv.org/abs/2409.16341
tags:
- data
- instruction
- quality
- training
- toolbench
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper addresses the challenge of improving data quality for
  training large language models (LLMs) to use external tools. The authors propose
  two approaches to assess data reliability: intrinsic quality evaluation using human-defined
  correctness criteria and in-context evaluation (ICE) to measure the educational
  value of data instances.'
---

# Quality Matters: Evaluating Synthetic Data for Tool-Using LLMs

## Quick Facts
- arXiv ID: 2409.16341
- Source URL: https://arxiv.org/abs/2409.16341
- Reference count: 24
- Primary result: High-quality training data improves tool-using LLM performance more than larger unvalidated datasets

## Executive Summary
This paper addresses the critical challenge of improving data quality for training large language models to use external tools. The authors propose two complementary approaches to assess data reliability: intrinsic quality evaluation using human-defined correctness criteria and in-context evaluation (ICE) to measure educational value. Through systematic evaluation on ToolBench and ToolAlpaca benchmarks, they demonstrate that models trained on filtered high-quality subsets significantly outperform those trained on full unvalidated datasets, even when using only 14% of the original data. The work establishes a practical framework for data quality assessment in tool-using LLM training.

## Method Summary
The authors develop automated metrics to assess data quality based on six intrinsic criteria: specificity, coherence, solvability, parameter alignment, sufficiency, and minimality. They implement these metrics using ChatGPT and validate them through manual annotation of 50 instances per dataset. The in-context evaluation (ICE) method evaluates educational value by measuring one-shot performance on test instructions. Models are fine-tuned using LoRA adapters on filtered high-quality subsets versus original datasets, with performance measured using pass rate metrics on tool usage tasks.

## Key Results
- Models trained on high-quality data subsets outperformed those trained on full unvalidated datasets
- Filtered high-quality subsets (14% of ToolBench data) achieved pass rates up to 0.54 vs 0.45 on full dataset
- Over 33% of ToolBench instances had parameter alignment errors, demonstrating significant quality issues
- ICE scores correlated with human-defined correctness and predicted educational value

## Why This Works (Mechanism)

### Mechanism 1
- Claim: High-quality training data improves model performance more than larger quantities of unvalidated data
- Mechanism: Filtering out erroneous instances prevents models from learning incorrect patterns during fine-tuning
- Core assumption: Quality metrics reliably identify problematic training instances
- Evidence anchors:
  - [abstract] Models trained on high-quality data outperform those on unvalidated data
  - [section 4.3.2] Over 33% of instances have parameter alignment errors
  - [corpus] Weak evidence - no directly related papers found
- Break condition: If quality metrics produce too many false positives or negatives

### Mechanism 2
- Claim: ICE scores predict educational value of training instances
- Mechanism: High ICE scores indicate instances that teach useful patterns for tool selection and usage
- Core assumption: In-context performance correlates with fine-tuning effectiveness
- Evidence anchors:
  - [section 5.2] ICE scores correlate with human-defined correctness
  - [section 5] Connection between in-context learning and fine-tuning
  - [corpus] Weak evidence - no directly related papers found
- Break condition: If ICE scores don't correlate with actual fine-tuning performance

### Mechanism 3
- Claim: Intrinsic quality criteria capture essential properties for effective tool-using training data
- Mechanism: Criteria ensure clear, logically consistent instructions with valid API sequences
- Core assumption: These human-defined properties are necessary for effective training
- Evidence anchors:
  - [section 4.1] Detailed definition of six intrinsic quality criteria
  - [section 4.3.2] Higher error percentages in ToolBench vs ToolAlpaca
  - [corpus] Weak evidence - no directly related papers found
- Break condition: If criteria are unnecessary or missing important aspects

## Foundational Learning

- Concept: Quality metrics for data filtering
  - Why needed here: Automated metrics identify low-quality training instances at scale
  - Quick check question: How would you design a metric to detect when an instruction contains all necessary details for API parameter extraction?

- Concept: In-context learning evaluation
  - Why needed here: ICE predicts educational value through one-shot example performance
  - Quick check question: What would happen to ICE scores if test API set overlapped significantly with training APIs?

- Concept: Fine-tuning with LoRA adapters
  - Why needed here: LoRA enables efficient fine-tuning of large language models
  - Quick check question: How does LoRA's low-rank adaptation differ from full fine-tuning in parameter efficiency?

## Architecture Onboarding

- Component map: Data quality assessment pipeline (intrinsic metrics + ICE) -> Fine-tuning system with LoRA adapters -> Evaluation framework using pass rate metric -> API function documentation processing

- Critical path: Load training dataset -> Apply quality metrics to filter instances -> Fine-tune model on filtered data -> Evaluate model on test set -> Calculate pass rate and compare to baselines

- Design tradeoffs:
  - ChatGPT assessment vs. rule-based approaches (accuracy vs. computational cost)
  - Intrinsic metrics vs. ICE (explainability vs. computational efficiency)
  - Small high-quality dataset vs. larger unvalidated dataset (training time vs. performance)

- Failure signatures:
  - Poor pass rates despite high-quality training data (evaluation setup issues)
  - No performance difference between filtered and unfiltered data (quality metrics ineffective)
  - Very few instances passing quality filters (filters too strict)

- First 3 experiments:
  1. Run quality metrics on small sample and manually verify accuracy
  2. Fine-tune on tiny high-quality subset (100 instances) and evaluate
  3. Compare ICE scores vs. intrinsic metrics on same dataset

## Open Questions the Paper Calls Out

- How do different levels of API documentation quality affect tool-using LLM performance?
- Can diversity in training data improve generalization beyond instance-level quality?
- How does instruction complexity affect data quality and model performance?
- Can ICE evaluate other aspects of data quality beyond educational value?
- How do different data filtration methods compare in computational efficiency?

## Limitations

- Automated quality assessment metrics' reliability remains uncertain
- Manual annotation process limited to only 50 instances per dataset
- Specific ChatGPT prompts for quality assessment not fully specified
- Correlation between ICE scores and fine-tuning performance needs stronger validation

## Confidence

- **High Confidence**: Core observation that data quality significantly impacts performance
- **Medium Confidence**: Intrinsic quality criteria framework needs broader validation
- **Low Confidence**: ICE methodology's predictive power requires more rigorous testing

## Next Checks

1. Conduct ablation studies to quantify individual quality criterion contributions
2. Test ICE's predictive power across multiple model sizes and architectures
3. Implement blind validation study with independent human annotators