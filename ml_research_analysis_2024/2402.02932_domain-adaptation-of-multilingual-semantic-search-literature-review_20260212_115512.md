---
ver: rpa2
title: Domain Adaptation of Multilingual Semantic Search -- Literature Review
arxiv_id: '2402.02932'
source_url: https://arxiv.org/abs/2402.02932
tags:
- retrieval
- online
- dense
- arxiv
- query
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This literature review explores the combination of domain adaptation
  techniques with multilingual semantic search for dense text information retrieval
  (tIR) systems in low-resource settings. The authors develop a new typology to cluster
  domain adaptation approaches based on which components of the tIR system they modify,
  including data adaptation (query generation, contrastive learning, knowledge distillation),
  model adaptation (scaling model size or capability), training adaptation (multi-task,
  domain-invariant, parameter-efficient learning), and ranking adaptation (integrating
  sparse retrieval).
---

# Domain Adaptation of Multilingual Semantic Search -- Literature Review

## Quick Facts
- arXiv ID: 2402.02932
- Source URL: https://arxiv.org/abs/2402.02932
- Authors: Anna Bringmann; Anastasia Zhukova
- Reference count: 40
- Key outcome: New typology for clustering domain adaptation approaches based on which IR system components they modify, with analysis of compatibility and data requirements for multilingual semantic search

## Executive Summary
This literature review systematically categorizes domain adaptation approaches for dense text information retrieval (tIR) systems, organizing methods by which components of the IR system they modify: data adaptation (query generation, contrastive learning, knowledge distillation), model adaptation (scaling, capability enhancement), training adaptation (multi-task, domain-invariant, parameter-efficient learning), and ranking adaptation (integrating sparse retrieval). The review surveys recent advances in multilingual semantic search, including multilingual dense retrievers and cross-lingual retrieval techniques. The authors identify research directions for combining domain adaptation with multilingual semantic search, such as using cross-lingual models for query generation and extending shared domain-invariant embedding spaces to include multilingual embeddings.

## Method Summary
The authors developed a new typology to cluster domain adaptation approaches based on which components of dense textual information retrieval systems they modify. The review analyzes how to combine these approaches efficiently by focusing on their compatibility and data requirements. The methodology involves surveying existing literature on domain adaptation techniques and multilingual semantic search, then synthesizing this information into a coherent framework that maps different adaptation strategies to specific IR system components. The review also examines how these approaches can be combined, particularly in low-resource settings where labeled data is scarce.

## Key Results
- Developed a new typology that categorizes domain adaptation approaches based on which IR system components they modify
- Identified four main categories: data adaptation, model adaptation, training adaptation, and ranking adaptation
- Proposed research directions including using cross-lingual models for query generation and extending domain-invariant embedding spaces to multilingual contexts

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The new typology improves domain adaptation efficiency by clustering methods according to which IR system components they modify
- Mechanism: By separating adaptation into data, model, training, and ranking levels, researchers can combine complementary approaches without redundancy
- Core assumption: Different IR components are largely independent, so modifying one does not break another
- Evidence anchors: [abstract] "developed a new typology to cluster domain adaptation approaches based on the part of dense textual information retrieval systems, which they adapt, focusing on how to combine them efficiently." [section 5] "we categorised them based on the components of the IR system they modify." [corpus] "Weak corpus support for this exact mechanism; only general mentions of 'efficiency' and 'combining'."
- Break condition: If component interactions are tighter than assumed (e.g., model changes break ranking integration), clustering becomes misleading

### Mechanism 2
- Claim: Query generation plus knowledge distillation synergistically improve adaptation quality
- Mechanism: Query generation creates pseudo-query-document pairs from unlabelled text, while knowledge distillation refines them with high-quality relevance judgments from a cross-encoder
- Core assumption: Cross-encoders provide sufficiently accurate relevance labels to correct noisy synthetic queries
- Evidence anchors: [section 5.1.1] "The second approach to training supervised QGs utilises a pipeline...utilising a more capable model (teacher)." [section 5.1.3] "Knowledge distillation automatically establishes the missing link, forming (pseudo) query-document-pairs and generating relevance judgments." [corpus] "Weak corpus support for synergy claim; no explicit study of combination effectiveness."
- Break condition: If cross-encoder teacher is too slow or inaccurate, distillation noise outweighs synthetic data benefits

### Mechanism 3
- Claim: Domain-invariant learning improves cross-lingual transfer by aligning embedding distributions across domains
- Mechanism: Minimising the distance between source and target domain embedding distributions forces the model to learn representations invariant to domain shift
- Core assumption: Domain shift is the primary barrier to cross-lingual retrieval; once distributions align, zero-shot performance follows
- Evidence anchors: [section 5.3] "Domain-invariant learning is another approach...aims to equip models with representations that remain invariant across different domains." [section 6] "This transfer between languages is achieved through creating a shared multilingual embedding space by mapping between the different embedding spaces of the two languages." [corpus] "Weak corpus support; only mentions of 'distribution' without empirical alignment results."
- Break condition: If linguistic differences dominate over domain differences, alignment alone cannot recover retrieval quality

## Foundational Learning

- Concept: Dense retrieval vs sparse retrieval distinction
  - Why needed here: The review repeatedly contrasts dense (semantic) and sparse (lexical) methods; misunderstanding leads to incorrect adaptation strategy choice
  - Quick check question: What is the primary difference in how dense and sparse retrievers compute relevance?

- Concept: Cross-encoder vs bi-encoder architectures
  - Why needed here: Different adaptation techniques target different architectures; choosing wrong target breaks training pipeline
  - Quick check question: Which architecture type is used for first-stage retrieval and which for re-ranking?

- Concept: Negative sampling in contrastive learning
  - Why needed here: Many adaptation methods rely on contrastive objectives; poor negative selection kills model convergence
  - Quick check question: Why is in-batch negative sampling usually insufficient for dense retrieval training?

## Architecture Onboarding

- Component map: Query encoder -> Embedding space -> Similarity function -> Ranking layer -> Output
- Critical path: Unlabelled text -> Query generation / contrastive learning -> Dense retriever training -> Domain adaptation fine-tuning -> Evaluation
- Design tradeoffs:
  - Higher-quality synthetic queries -> Slower training; lower-quality -> Faster but weaker adaptation
  - Knowledge distillation -> Better label quality but requires cross-encoder teacher; no distillation -> Faster but noisier
  - Domain-invariant loss -> Better cross-lingual transfer but higher training complexity; no domain loss -> Simpler but domain-biased
- Failure signatures:
  - Poor recall: Negative sampling too narrow, model overfits to synthetic queries
  - Low precision: Teacher model noisy, knowledge distillation propagates errors
  - Cross-lingual collapse: Domain-invariant loss too strong, embeddings lose language-specific nuance
- First 3 experiments:
  1. Train bi-encoder with in-batch negatives on target domain text; measure recall@10 vs BM25 baseline
  2. Add cross-encoder teacher for knowledge distillation; compare precision@1 vs step 1
  3. Add domain-invariant loss term; test zero-shot retrieval on unseen language pair

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How effective are cross-lingual models for query generation on specialized target domain texts compared to existing machine translation systems?
- Basis in paper: [explicit] The authors propose exploring using cross-lingual models for query generation on specialized target domain texts as an interesting research direction
- Why unresolved: This approach has not been empirically tested or compared to existing machine translation systems in the context of domain adaptation for multilingual semantic search
- What evidence would resolve it: Experimental results comparing the performance of cross-lingual models for query generation against machine translation systems on specialized domain texts, measured by metrics such as retrieval accuracy and query quality

### Open Question 2
- Question: Can issues arising from complex loss formulation in domain-invariant learning be mitigated by applying a mapping function to create a joint space for source- and target-domain embeddings?
- Basis in paper: [inferred] The authors suggest that issues with domain-invariant learning's complex loss formulation could potentially be addressed by extending the shared source- and target domain vector space to include multilingual embeddings
- Why unresolved: While the idea is proposed, no specific mapping function or empirical evidence is provided to demonstrate its effectiveness in mitigating the issues with domain-invariant learning
- What evidence would resolve it: Experimental results showing improved performance of domain-invariant learning when using a mapping function to create a joint space for source- and target-domain embeddings, compared to traditional domain-invariant learning approaches

### Open Question 3
- Question: How effective is the combination of knowledge distillation with query generation or contrastive learning in generating fine-grained (pseudo) relevance judgments for the generated (pseudo) query-document pairs?
- Basis in paper: [explicit] The authors propose combining knowledge distillation with query generation or contrastive learning to generate fine-grained (pseudo) relevance judgments, mitigating the requirement of independent queries and documents for knowledge distillation
- Why unresolved: While the idea is proposed, no empirical evidence is provided to demonstrate the effectiveness of this combination in generating high-quality (pseudo) relevance judgments
- What evidence would resolve it: Experimental results comparing the performance of knowledge distillation when combined with query generation or contrastive learning against traditional knowledge distillation approaches, measured by metrics such as retrieval accuracy and relevance judgment quality

## Limitations

- Weak empirical validation of claimed synergies between different adaptation techniques, particularly query generation and knowledge distillation
- Limited evidence supporting the independence assumption between IR system components in the proposed typology
- Theoretical claims about cross-lingual alignment benefits not empirically demonstrated in low-resource settings

## Confidence

- **High**: The distinction between dense vs sparse retrieval methods and their respective adaptation challenges
- **Medium**: The categorization of adaptation approaches by IR system component
- **Low**: Claims about synergistic benefits from combining specific adaptation techniques

## Next Checks

1. Conduct ablation studies testing whether query generation + knowledge distillation actually outperforms either method alone in low-resource multilingual settings
2. Measure the impact of component interactions by systematically varying model architecture while holding other adaptation methods constant
3. Evaluate domain-invariant learning across multiple language pairs to determine if distribution alignment generalizes beyond the studied domains