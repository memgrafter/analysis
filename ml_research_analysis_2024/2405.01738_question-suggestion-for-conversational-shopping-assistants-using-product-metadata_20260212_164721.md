---
ver: rpa2
title: Question Suggestion for Conversational Shopping Assistants Using Product Metadata
arxiv_id: '2405.01738'
source_url: https://arxiv.org/abs/2405.01738
tags:
- question
- product
- questions
- shopping
- customers
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proposes an LLM-based framework for generating contextual
  product questions to improve conversational shopping assistant interactions. It
  leverages in-context learning and supervised fine-tuning on product metadata and
  reviews to generate diverse, relevant, and answerable questions.
---

# Question Suggestion for Conversational Shopping Assistants Using Product Metadata

## Quick Facts
- **arXiv ID**: 2405.01738
- **Source URL**: https://arxiv.org/abs/2405.01738
- **Reference count**: 23
- **Primary result**: LLM-based framework generates contextual product questions with 70-98% agreement on relevance and fluency, evaluated via GPT-4 and human annotation

## Executive Summary
This paper proposes an LLM-based framework for generating contextual product questions to improve conversational shopping assistant interactions. The approach leverages in-context learning and supervised fine-tuning on product metadata and reviews to generate diverse, relevant, and answerable questions. The method is evaluated offline using GPT-4 and human annotation, achieving good performance on relevance and fluency metrics while identifying scope for improvement in usefulness and style dimensions.

## Method Summary
The paper employs an LLM-based approach using in-context learning (ICL) and supervised fine-tuning (SFT) to generate product questions grounded in catalog-derived metadata and buyer reviews. For ICL, a prompt is constructed to generate diverse product question suggestions. For SFT, a high-quality training dataset of 1.8K question-context pairs is created from the Amazon Reviews Dataset, and the 11B Flan-T5-xxl model is fine-tuned with this data for 8 epochs using an initial learning rate of 1e-5.

## Key Results
- Generated questions achieve 70-98% agreement on relevance and fluency metrics across GPT-4 and human evaluation
- ICL and SFT approaches both produce questions covering diverse product aspects and contexts
- Model demonstrates ability to generate answerable questions grounded in provided product metadata
- Scope for improvement identified in usefulness and style dimensions

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: In-context learning enables zero-shot generation of product questions that follow quality criteria by leveraging LLM's world knowledge
- **Core assumption**: LLM's pre-trained knowledge includes sufficient understanding of product domains and conversational question styles
- **Evidence**: [abstract] "We propose a framework that employs Large Language Models (LLMs) to automatically generate contextual, useful, answerable, fluent and diverse questions about products, via in-context learning and supervised fine-tuning."

### Mechanism 2
- **Claim**: Supervised fine-tuning with a high-quality dataset improves question generation consistency across all quality dimensions
- **Core assumption**: Training dataset sufficiently covers diversity of product types, question types, and quality dimensions needed for robust generalization
- **Evidence**: [section 4] "For SFT, we train the 11B Flan-T5-xxl [9] model with 1.8K pairs of product contexts and their corresponding question suggestions... for 8 epochs with an initial learning rate of 1ùëí ‚àí5."

### Mechanism 3
- **Claim**: Grounding questions in product metadata and reviews ensures answerability while avoiding hallucination risks
- **Core assumption**: Product metadata and reviews contain sufficient information to answer generated questions about product features and aspects
- **Evidence**: [abstract] "We propose an LLM-based approach leveraging in-context learning (ICL) [14] and supervised fine-tuning (SFT), to automatically generate question suggestions grounded in catalog-derived product metadata and buyer reviews."

## Foundational Learning

- **In-context learning and few-shot prompting**: Enables zero-shot question generation without expensive fine-tuning while maintaining quality through prompt-based constraints. *Quick check*: Can you construct a prompt that generates relevant product questions from a given product description without any training?

- **Supervised fine-tuning methodology**: Improves generation consistency and quality across all dimensions by adapting model parameters to specific task distribution. *Quick check*: What size and diversity characteristics should a training dataset have to effectively fine-tune an LLM for question generation?

- **Quality criteria formulation and evaluation**: Ensures generated questions meet customer needs for relevance, usefulness, and style in shopping contexts. *Quick check*: How would you design an evaluation rubric to assess question relevance and usefulness for e-commerce applications?

## Architecture Onboarding

- **Component map**: Product metadata and reviews ‚Üí LLM generation (ICL/SFT) ‚Üí Quality assessment ‚Üí Question recommendation ‚Üí Customer interface
- **Critical path**: Product context ‚Üí LLM generation (ICL/SFT) ‚Üí Quality assessment ‚Üí Question recommendation
- **Design tradeoffs**:
  - Model size vs. inference latency: Larger models perform better but may have slower generation
  - Dataset size vs. annotation effort: 1.8K pairs provide good coverage but require manual inspection
  - Question length vs. mobile display constraints: Longer questions summarize better but may be harder to display
  - Zero-shot vs. fine-tuned performance: ICL requires no training but SFT provides more consistent quality
- **Failure signatures**:
  - Low diversity scores: Model generating repetitive question patterns or focusing on limited product aspects
  - Poor answerability: Questions referencing information not present in source context
  - Style violations: Questions using first/second person pronouns or assistant-style framing
  - Low usefulness: Questions about obvious features or lacking decision-making value
- **First 3 experiments**:
  1. Zero-shot ICL generation with quality prompt on diverse product contexts to establish baseline performance
  2. Fine-tuning on curated dataset with ablation study on dataset size and quality dimensions
  3. A/B testing of question suggestions vs. traditional search to measure customer engagement impact

## Open Questions the Paper Calls Out
1. How do different LLM architectures (e.g., Claude-2 vs. Flan-T5-xxl) compare in generating product questions that balance relevance, usefulness, and style?
2. What is the impact of incorporating multi-turn conversational history into the input context for generating product questions?
3. How can customer interaction signals (e.g., clicks, likes) be effectively used to improve the quality of generated product questions in real-time?

## Limitations
- Small human evaluation sample size (3 annotators) limits reliability of subjective quality assessments
- No real-world deployment validation; all evaluations are conducted offline
- Prompt engineering details for ICL not fully specified, requiring reverse engineering for reproduction

## Confidence

**High Confidence**: Core methodology of using LLMs for question generation is well-established and technical implementation details are clearly specified.

**Medium Confidence**: Reported evaluation results are plausible but small human evaluation sample size and lack of real-world validation create uncertainty about practical performance.

**Low Confidence**: Claims about customer engagement improvement cannot be substantiated without real-world A/B testing data.

## Next Checks

1. **Inter-annotator Agreement Analysis**: Re-run human evaluation with at least 10 annotators and calculate Cohen's kappa or Fleiss' kappa to establish reliability of subjective quality assessments across all six dimensions.

2. **Real-World A/B Testing**: Deploy question suggestion system in live shopping environment and measure actual customer engagement metrics comparing suggested questions versus traditional search interfaces over minimum 4-week period.

3. **Prompt Engineering Replication**: Attempt to reconstruct ICL prompt based on paper's description and test effectiveness across diverse product categories, comparing performance against alternative prompt formulations.