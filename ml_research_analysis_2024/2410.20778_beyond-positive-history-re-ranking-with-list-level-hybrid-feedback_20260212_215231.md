---
ver: rpa2
title: 'Beyond Positive History: Re-ranking with List-level Hybrid Feedback'
arxiv_id: '2410.20778'
source_url: https://arxiv.org/abs/2410.20778
tags:
- user
- candidate
- feedback
- re-ranking
- items
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the limitation in re-ranking models that only
  use positive feedback (clicks) from user history, ignoring the context of negative
  feedback (non-clicks) within lists. The authors propose RELIFE, a re-ranking framework
  that incorporates list-level hybrid feedback to capture user preferences and comparison
  behavior patterns.
---

# Beyond Positive History: Re-ranking with List-level Hybrid Feedback

## Quick Facts
- arXiv ID: 2410.20778
- Source URL: https://arxiv.org/abs/2410.20778
- Reference count: 40
- Key outcome: RELIFE achieves up to 4.69% improvement in Click@10 on PRM Public and 1.06% in Click@5 on MovieLens-20M by incorporating list-level hybrid feedback

## Executive Summary
This paper addresses a critical limitation in re-ranking models that only consider positive user feedback (clicks) from historical interactions. The authors propose RELIFE, a novel re-ranking framework that leverages list-level hybrid feedback by incorporating both clicked and non-clicked items within user browsing sessions. By capturing user interests, disinterests, and comparison behavior patterns within lists, RELIFE provides richer preference signals than traditional item-level positive feedback approaches. The model demonstrates significant performance improvements over state-of-the-art re-ranking baselines on two public datasets.

## Method Summary
RELIFE is a re-ranking framework that incorporates list-level hybrid feedback to capture user preferences and comparison behavior patterns. The model uses three main modules: a Disentangled Interest Miner (DIM) to separate interests from disinterests using co-attention, a Sequential Preference Mixer (SPM) to learn entangled preferences with temporal information via GRU, and a Comparison-aware Pattern Extractor (CPE) to capture user behavior patterns within lists using distance-aware attention and contrastive learning. The framework employs contrastive learning to align behavior patterns between historical and candidate lists, with training objectives combining cross-entropy loss for utility optimization and InfoNCE loss for pattern alignment.

## Key Results
- RELIFE achieves up to 4.69% improvement in Click@10 on PRM Public dataset compared to state-of-the-art re-ranking baselines
- On MovieLens-20M, RELIFE shows 1.06% improvement in Click@5 metric
- Ablation studies demonstrate that each component (DIM, SPM, CPE) contributes significantly to overall performance
- The model shows consistent improvements across multiple evaluation metrics including MAP@K and NDCG@K

## Why This Works (Mechanism)

### Mechanism 1
- Claim: List-level hybrid feedback captures both positive and negative feedback within the same list context, providing richer user preference signals than item-level positive feedback alone.
- Mechanism: The model extracts interests from clicked items and disinterests from non-clicked items within each historical list, then uses co-attention to align these with candidate items, effectively modeling the user's preference context.
- Core assumption: User preferences can be disentangled into interests and disinterests, and these are influenced by the context of both positive and negative feedback within the same list.
- Evidence anchors:
  - [abstract] "This list-level hybrid feedback can reveal users' holistic preferences and reflect users' comparison behavior patterns manifesting within a list."
  - [section 1] "list-level hybrid feedback can not only reveal users' interests and disinterests but also provide a clear context for positive feedback, e.g., surrounding negative feedback."
  - [corpus] Weak - corpus neighbors focus on different aspects of re-ranking without specifically addressing list-level hybrid feedback mechanisms.
- Break condition: If negative feedback items are too sparse or noisy, the disentanglement becomes unreliable and the model degrades to treating all non-clicks as uniform negative signals.

### Mechanism 2
- Claim: User comparison behavior patterns within lists can be captured and aligned between historical and candidate lists using contrastive learning.
- Mechanism: The CPE module constructs distance-aware attention matrices based on the position of clicked vs non-clicked items, then uses contrastive learning (InfoNCE loss) to align behavior patterns across historical and candidate lists.
- Core assumption: Users exhibit consistent comparison behavior patterns across different lists, and these patterns can be learned and transferred from historical to candidate lists.
- Evidence anchors:
  - [abstract] "Secondly, list-level hybrid feedback can reflect the user's comparison behavior patterns beneficial for re-ranking."
  - [section 4.5] "Many studies have pointed out that users often compare an item with surrounding items before clicking on it [11, 57]."
  - [corpus] Weak - corpus neighbors don't specifically address comparison behavior pattern alignment between lists.
- Break condition: If user comparison patterns are highly context-dependent and don't transfer between different list contexts, the alignment becomes ineffective.

### Mechanism 3
- Claim: Sequential preference mixing with temporal information captures how user preferences evolve over time while considering the context of feedback.
- Mechanism: SPM uses GRU to model the chronological sequence of positive and negative feedback, then applies candidate-aware attention to extract temporal preferences relevant to each candidate item.
- Core assumption: User preferences change over time and the order of feedback provides meaningful information about preference evolution.
- Evidence anchors:
  - [section 4.4] "user preferences evolve over time, so we incorporate temporal information into the interaction modeling between positive and negative feedback in SPM."
  - [section 4.4] "we leverage a GRU [6] to model the user's evolving interests"
  - [corpus] Weak - corpus neighbors focus on different aspects of preference modeling without specific emphasis on temporal evolution.
- Break condition: If temporal patterns in feedback are random or if user preferences change too rapidly, the GRU-based temporal modeling becomes ineffective.

## Foundational Learning

- Concept: Co-attention mechanisms
  - Why needed here: To align historical feedback (both positive and negative) with candidate items for preference extraction
  - Quick check question: How does co-attention differ from standard attention in handling bi-directional information flow between two sequences?

- Concept: Contrastive learning with InfoNCE loss
  - Why needed here: To align behavior patterns between historical and candidate lists for better re-ranking performance
  - Quick check question: What role does the temperature parameter τ play in the InfoNCE loss formulation?

- Concept: Distance-aware attention mechanisms
  - Why needed here: To capture comparison behavior by adjusting attention weights based on the distance between positive and negative feedback items
  - Quick check question: How does the learnable sigmoid function transform distance into influence factors in the CPE module?

## Architecture Onboarding

- Component map: Embedding Layer → ICC (Self-Attention) → DIM (Co-Attention ×2) → SPM (GRU + Candidate Attention) → CPE (Distance-aware Attention + Contrastive Learning) → Prediction Layer (MLP)
- Critical path: DIM → SPM → CPE → Prediction Layer (the main preference extraction and pattern alignment path)
- Design tradeoffs: Using co-attention for disentanglement vs. simpler attention mechanisms; incorporating contrastive learning vs. relying solely on pattern extraction; using GRU for temporal modeling vs. simpler positional encoding
- Failure signatures: Poor performance on Click@K metrics suggests weak preference extraction; low MAP/NDCG suggests ineffective pattern alignment; high variance across runs suggests instability in co-attention or contrastive learning
- First 3 experiments:
  1. Remove contrastive learning (RELIFE-CL variant) to measure the impact of pattern alignment on performance
  2. Remove temporal modeling in SPM to test the importance of preference evolution over time
  3. Replace co-attention in DIM with standard attention to evaluate the benefit of disentanglement approach

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of RELIFE compare when using different numbers of historical lists (N) beyond the tested range of 3-6?
- Basis in paper: [explicit] The paper shows results for varying N from 1 to 6, but does not explore beyond this range.
- Why unresolved: The paper only tests up to N=6, leaving uncertainty about performance at higher values.
- What evidence would resolve it: Conducting experiments with N > 6 and plotting performance metrics (MAP, NDCG, Click) against N would show if performance plateaus, declines, or continues improving.

### Open Question 2
- Question: How sensitive is RELIFE's performance to the weight (β) of the InfoNCE loss beyond the tested range of 0 to 1?
- Basis in paper: [explicit] The paper tests β values from 0 to 1 but does not explore values outside this range.
- Why unresolved: The impact of β values greater than 1 or less than 0 on model performance is unknown.
- What evidence would resolve it: Testing β values outside the range [0,1] and analyzing the effect on ranking and utility metrics would determine the sensitivity and optimal range.

### Open Question 3
- Question: How does RELIFE's performance scale with increasing list lengths (M) beyond the tested values of 10 and 30?
- Basis in paper: [inferred] The paper tests list lengths of 10 and 30, but does not explore longer lists which might be more realistic in some domains.
- Why unresolved: The computational complexity and performance of RELIFE at larger list lengths are not evaluated.
- What evidence would resolve it: Experiments with list lengths greater than 30, measuring both performance metrics and computational resources (time, memory), would show scalability limits and optimal list lengths.

## Limitations

- The model's effectiveness depends heavily on the availability of sufficient negative feedback items within historical lists, which may not be present in all user sessions
- The assumption that comparison behavior patterns transfer consistently between historical and candidate lists may not hold in dynamic recommendation environments with rapidly changing user preferences
- The paper lacks analysis of how robust the disentanglement mechanism is when feedback is noisy or when users have very different browsing patterns

## Confidence

- **High Confidence**: The experimental methodology is sound, using established datasets and standard re-ranking metrics. The ablation studies clearly demonstrate the contribution of each component to overall performance.
- **Medium Confidence**: The disentanglement mechanism in DIM is well-motivated but the paper lacks analysis of how robust this disentanglement is when feedback is noisy or when users have very different browsing patterns.
- **Low Confidence**: The paper's claims about comparison behavior patterns being transferable through contrastive learning are not well-supported by empirical evidence. The CPE module's effectiveness in capturing these patterns remains largely theoretical.

## Next Checks

1. **Robustness Test**: Evaluate RELIFE's performance when varying the density of negative feedback in historical lists (e.g., randomly removing non-clicked items) to assess the model's sensitivity to feedback sparsity.

2. **Pattern Transfer Analysis**: Conduct a controlled experiment where comparison behavior patterns are deliberately altered between historical and candidate lists to measure how well the contrastive learning component adapts to pattern shifts.

3. **Cross-Domain Evaluation**: Test RELIFE on datasets from different domains (e.g., news recommendation, music streaming) to validate whether the list-level hybrid feedback approach generalizes beyond e-commerce and movie recommendation scenarios.