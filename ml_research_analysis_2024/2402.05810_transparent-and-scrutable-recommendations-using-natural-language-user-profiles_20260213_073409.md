---
ver: rpa2
title: Transparent and Scrutable Recommendations Using Natural Language User Profiles
arxiv_id: '2402.05810'
source_url: https://arxiv.org/abs/2402.05810
tags:
- user
- profile
- profiles
- preferences
- recommendation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a novel approach to making recommender systems
  transparent and scrutable by replacing uninterpretable user embeddings with natural
  language user profiles. The method uses large language models to generate personalized
  profiles summarizing user preferences from past reviews, then fine-tunes a language
  model for rating prediction using only these profiles.
---

# Transparent and Scrutable Recommendations Using Natural Language User Profiles

## Quick Facts
- arXiv ID: 2402.05810
- Source URL: https://arxiv.org/abs/2402.05810
- Reference count: 14
- Primary result: Achieves RMSE ~0.94 on Amazon-MT with transparent natural language profiles that users can edit

## Executive Summary
This paper introduces User Profile-based Recommendations (UPR), a novel approach to making recommender systems transparent and scrutable by replacing uninterpretable user embeddings with natural language user profiles. The method leverages large language models to generate personalized profiles from past reviews, then fine-tunes a language model for rating prediction using only these profiles. Experiments on Amazon Movies & TV and TripAdvisor datasets demonstrate that UPR achieves competitive performance with traditional recommender systems while enabling users to easily modify their preferences through profile editing.

## Method Summary
The approach generates natural language user profiles by extracting and ranking features from user reviews using sentiment analysis, then synthesizing profiles with top features using LLM prompting. These profiles are used to fine-tune a pre-trained language model (GPT-2) for rating prediction. The system validates scrutability by measuring how profile edits affect recommendations. The method processes user reviews to extract features, ranks them by utility scores combining rating averages, coverage, and significance, then generates concise profiles using LLMs. The fine-tuned model learns to map these natural language profiles to item ratings, maintaining performance comparable to traditional collaborative filtering methods.

## Key Results
- Achieves RMSE of 0.94 on Amazon-MT dataset, competitive with traditional methods
- User profile editing significantly changes recommendation outcomes, validating scrutability
- Natural language profiles maintain comprehension while achieving comparable accuracy to uninterpretable embeddings

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Natural language user profiles can substitute for uninterpretable user embeddings while maintaining competitive recommendation performance
- Mechanism: By distilling user preferences into natural language descriptions, the model learns to map these profiles to item ratings using the LLM's pretrained knowledge of item features
- Core assumption: The LLM's internal representations of items and their features can effectively connect with natural language user preferences
- Evidence anchors: [abstract] "these NL profiles can then be leveraged to fine-tune a LLM using only NL profiles to make transparent and scrutable recommendations"; [section] "we observe that this novel approach maintains a performance level on par with established recommender systems"

### Mechanism 2
- Claim: Users can easily scrutinize and modify recommendations by editing natural language profiles
- Mechanism: Since preferences are expressed in natural language rather than embeddings, users can directly modify their profiles and receive updated recommendations without retraining
- Core assumption: The LLM-based recommender can interpret changes in natural language preferences and adjust recommendations accordingly
- Evidence anchors: [abstract] "we validate the scrutability of our user profile-based recommender by investigating the impact on recommendation changes after editing NL user profiles"; [section] "by adding new preferences to profiles significantly changes recommendation outcomes, validating the scrutability of the approach"

### Mechanism 3
- Claim: Instruction-tuned LLMs can generate high-quality natural language user profiles from past reviews
- Mechanism: By prompting LLMs with a user's review history, the model can synthesize a coherent, personalized profile that captures preferences across multiple features
- Core assumption: LLMs can effectively summarize and synthesize user preferences from review text without ground truth profiles
- Evidence anchors: [section] "these machine-generated NL profiles qualitatively summarize a user's preferences concisely while maintaining a scrutable format"; [section] "annotators were highly satisfied with the quality of our generated NL profiles" (user study results)

## Foundational Learning

- Natural language processing fundamentals
  - Why needed here: Understanding how LLMs process and generate text is crucial for prompt engineering and profile generation
  - Quick check question: What is the difference between fine-tuning and prompt engineering for LLMs?

- Recommendation system basics
  - Why needed here: Understanding collaborative filtering, embeddings, and evaluation metrics provides context for why this approach differs
  - Quick check question: How do traditional collaborative filtering methods represent users differently from this approach?

- Large language model architecture
  - Why needed here: Understanding transformer architecture and how LLMs process prompts is essential for debugging and improving the system
  - Quick check question: What is the role of attention mechanisms in connecting user profiles to item recommendations?

## Architecture Onboarding

- Component map:
  Review preprocessing pipeline → Feature extraction → Profile generation (LLM) → Profile storage → Recommendation model (finetuned LLM) → Rating prediction
  User interface → Profile editing → Recommendation request → Inference pipeline

- Critical path:
  User profile generation (offline/batch) → Model training (fine-tuning) → User profile editing → Real-time inference → Recommendation delivery

- Design tradeoffs:
  Profile length vs. comprehensiveness: Longer profiles capture more preferences but increase cognitive load and may dilute focus
  LLM size vs. performance: Larger models may perform better but require more resources for training and inference
  Feature selection vs. coverage: More features provide better representation but increase complexity and potential noise

- Failure signatures:
  Poor recommendations: May indicate issues with profile generation quality or insufficient feature coverage
  Profile editing doesn't change recommendations: Suggests model isn't learning profile-item relationships effectively
  Slow inference: LLM-based approach is inherently slower than traditional methods

- First 3 experiments:
  1. Baseline comparison: Run all baseline models and UPR on a small subset of data to verify competitive performance
  2. Profile editing test: Take a user profile, edit it with new preferences, and verify recommendation changes
  3. Feature ablation study: Generate profiles with 1, 3, and 5 features to observe impact on recommendation quality

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of UPR scale with different LLM sizes and architectures beyond GPT-2?
- Basis in paper: [inferred] The paper mentions that newer models like GPT-3 have shown better performance than GPT-2 and are trained on more data, but states that SOTA models contain significantly more parameters requiring more powerful hardware to train.
- Why unresolved: The study only uses GPT-2 as the pre-trained language model for fine-tuning. While they acknowledge that performance could improve with larger-scale models, they don't empirically test this hypothesis due to hardware constraints.
- What evidence would resolve it: Experiments comparing UPR performance using different LLM architectures (GPT-2, GPT-3, Llama, Mistral, etc.) on the same recommendation tasks with consistent training conditions.

### Open Question 2
- Question: What is the optimal balance between profile length, number of features, and recommendation performance from a cognitive load perspective?
- Basis in paper: [explicit] The authors note that there's a concern that verbose NL profiles increase cognitive load for users and mention exploring the tradeoffs between recommendation performance and human preferences in future work.
- Why unresolved: The study sets a maximum token limit of 200 tokens based on qualitative assessment but doesn't systematically investigate how different profile lengths affect both recommendation accuracy and user experience. They also don't measure actual cognitive load on users.
- What evidence would resolve it: User studies measuring recommendation performance, profile comprehension, and perceived cognitive effort across different profile lengths and feature counts, ideally with real users modifying profiles.

### Open Question 3
- Question: How effectively can UPR handle negative preferences and dislikes in user profiles?
- Basis in paper: [explicit] The authors report that "the average rating in both datasets is 4.0/5.0, meaning that the vast majority of NL profiles contain only positive preferences" and that their attempts to edit positive preferences to negative ones resulted in only minor changes in recommendations.
- Why unresolved: The model is trained primarily on positive preferences, making it difficult to edit negative preferences into profiles. The study acknowledges this limitation but doesn't propose solutions or conduct experiments to determine if this is a fundamental limitation or can be addressed through different training approaches.
- What evidence would resolve it: Experiments using datasets with more balanced positive/negative ratings, or synthetic negative preference data augmentation, to train models that can effectively incorporate and act on negative preferences in user profiles.

## Limitations

- Limited generalizability across domains and languages - only tested on Amazon Movies & TV and TripAdvisor in English
- Assumes static user preferences without addressing how profiles should evolve over time
- Computational scalability concerns not addressed - large language models require substantial resources

## Confidence

- **Low** on generalizability across different domains and languages due to limited experimental scope
- **Medium** in long-term stability of natural language profiles as user preferences evolve over time
- **Medium** in computational scalability of the approach for large-scale deployment

## Next Checks

1. Cross-domain validation: Apply UPR to music streaming and e-commerce domains to test generalizability across different item characteristics and user behaviors

2. Longitudinal study: Implement user simulation with periodic profile updates to measure recommendation quality drift and adaptation to changing preferences over time

3. Computational efficiency analysis: Benchmark runtime and resource requirements for profile generation, model training, and inference across different LLM sizes and compare with traditional collaborative filtering under realistic load conditions