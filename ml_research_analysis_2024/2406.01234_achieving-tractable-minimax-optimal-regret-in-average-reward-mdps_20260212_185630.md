---
ver: rpa2
title: Achieving Tractable Minimax Optimal Regret in Average Reward MDPs
arxiv_id: '2406.01234'
source_url: https://arxiv.org/abs/2406.01234
tags:
- lemma
- bias
- regret
- have
- algorithm
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper introduces the first tractable algorithm with minimax\
  \ optimal regret for learning average-reward Markov Decision Processes (MDPs). The\
  \ proposed Projected Mitigated Extended Value Iteration (PMEVI) algorithm achieves\
  \ a regret bound of O(\u221Asp(h) S A T) where sp(h) is the span of the optimal\
  \ bias function, S and A are the state and action space sizes, and T is the number\
  \ of learning steps."
---

# Achieving Tractable Minimimax Optimal Regret in Average Reward MDPs

## Quick Facts
- arXiv ID: 2406.01234
- Source URL: https://arxiv.org/abs/2406.01234
- Reference count: 40
- Introduces first tractable algorithm with minimax optimal regret for average-reward MDPs

## Executive Summary
This paper presents the Projected Mitigated Extended Value Iteration (PMEVI) algorithm, which achieves the first tractable minimax optimal regret bound of O(√sp(h*) S A T) for average-reward Markov Decision Processes without requiring prior knowledge of the bias span. The algorithm combines bias-constrained optimization with a novel extended value iteration that projects onto estimated bias constraints and mitigates estimation errors. PMEVI can be integrated into existing optimistic algorithms to improve their regret bounds, and experiments demonstrate its effectiveness in both prior-less and prior-informed settings.

## Method Summary
PMEVI is a tractable algorithm that achieves minimax optimal regret for average-reward MDPs by combining bias estimation with extended value iteration. The method projects onto estimated bias constraints and mitigates estimation errors through variance-aware techniques. Unlike previous approaches, PMEVI does not require prior knowledge of the bias span. The algorithm runs in polynomial time and can be applied to various existing algorithms to improve their regret bounds. It constructs bias confidence regions from commute times between states and uses tractable approximations for variance-aware mitigation.

## Key Results
- Achieves first tractable minimax optimal regret bound of O(√sp(h*) S A T) for average-reward MDPs
- PMEVI-DT matches EVI counterparts when no bias prior is given, but significantly outperforms them with adequate bias prior information
- Algorithm runs in polynomial time and integrates with existing optimistic algorithms
- Successfully eliminates the need for prior knowledge of bias span while maintaining optimal regret

## Why This Works (Mechanism)

### Mechanism 1
- Claim: PMEVI achieves minimax optimal regret by combining bias-constrained optimization with mitigated extended value iteration
- Mechanism: PMEVI replaces standard EVI with projected mitigated extended Bellman operator that constrains search to estimated bias regions and reduces estimation error through variance-aware mitigation
- Core assumption: Bias confidence region H_t contains true optimal bias function h* with high probability, and projection operator preserves monotonicity and non-expansivity
- Break condition: If bias estimation error grows faster than √T, mitigation becomes insufficient and regret degrades to sub-optimal bounds

### Mechanism 2
- Claim: Bias confidence region construction enables tight regret bounds without prior knowledge of bias span
- Mechanism: PMEVI-DT estimates bias differences using commute times between state pairs and constructs pairwise constraints that shrink as more commutes are observed, while enforcing polynomial bound on bias span
- Core assumption: Number of observed commutes between states grows sufficiently fast relative to regret accumulation
- Break condition: If algorithm fails to observe enough state commutes (e.g., in highly disconnected MDPs), bias estimates remain loose and regret bounds degrade

### Mechanism 3
- Claim: Variance-aware mitigation strategy prevents aggressive overestimation while maintaining tractability
- Mechanism: Instead of using empirical Bernstein bounds for each bias vector, PMEVI-DT computes tractable upper bound on maximum variance across all bias vectors in confidence region
- Core assumption: Convex maximization problem for maximum variance can be upper-bounded by linear combination of empirical variance and bias estimation error
- Break condition: If bias region H_t becomes too large relative to sample size, variance bound becomes uninformative and mitigation fails to provide benefits

## Foundational Learning

- Concept: Bellman operators and their properties (monotonicity, non-expansivity in span norm)
  - Why needed here: PMEVI relies on analyzing projected mitigated Bellman operator L_t, which requires understanding how Bellman operators behave under projection and mitigation
  - Quick check question: Why does the span semi-norm, rather than the L∞ norm, play a crucial role in the convergence analysis of value iteration in average-reward MDPs?

- Concept: Bias function and its relationship to optimal gain
  - Why needed here: Algorithm's regret bounds depend critically on span of optimal bias function h*, and PMEVI explicitly estimates and constrains this bias
  - Quick check question: What is the fundamental difference between the bias function in average-reward MDPs versus the value function in discounted MDPs?

- Concept: Martingale concentration inequalities (Azuma, Freedman, Bernstein)
  - Why needed here: Algorithm's confidence regions rely on time-uniform concentration bounds for both rewards and transitions
  - Quick check question: How does Freedman's inequality improve upon Azuma-Hoeffding when dealing with bounded variance?

## Architecture Onboarding

- Component map:
  PMEVI-DT (main algorithm) -> BiasEstimation -> BiasProjection -> VarianceApprox -> PMEVI -> Confidence region generators

- Critical path:
  1. Episode starts → Update confidence regions
  2. BiasEstimation → Construct H_t
  3. BiasProjection → Define Γ_t
  4. VarianceApprox → Compute β_t
  5. PMEVI → Compute h_t, g_t
  6. Greedy policy selection → Execute actions
  7. Double visit counts → Episode ends

- Design tradeoffs:
  - Tighter bias regions → Better regret but more expensive projection
  - More aggressive mitigation → Potentially better bounds but risk of ill-behaved operators
  - Frequent bias updates → Better tracking but higher computational cost

- Failure signatures:
  - Linear regret growth → Bias estimation failing to converge
  - Numerical instability in PMEVI → Overly aggressive mitigation or ill-conditioned projection
  - Slow policy improvement → Confidence regions too conservative

- First 3 experiments:
  1. Run PMEVI-DT on simple 3-state river swim with known bias ordering to verify performance improvement with bias prior
  2. Compare PMEVI-UCRL2 vs standard UCRL2 on 5-state chain MDP to measure practical regret reduction
  3. Test sensitivity to bias estimation frequency by varying how often H_t is updated in fixed environment

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the exact computational complexity of PMEVI-DT in practice, beyond theoretical O(DS³AT) bound?
- Basis in paper: Explicit - "Moreover, if PMEVI-DT runs with the same confidence regions that UCRL2 Auer et al. [2009], then it enjoys a time complexity O(DS 3AT)." and "Every call to the projection operator solves a linear program... in practice, reducing the number of calls to the projection operator is key to run PMEVI-DT in reasonable time."
- Why unresolved: Paper only provides theoretical complexity bounds but acknowledges practical performance depends heavily on number of projection operator calls, which is not explicitly quantified
- What evidence would resolve it: Empirical runtime measurements comparing PMEVI-DT with different projection frequencies on various MDP sizes and structures

### Open Question 2
- Question: How sensitive is PMEVI-DT's performance to choice of bias prior information H*?
- Basis in paper: Explicit - "The geometry of the prior bias region H∗ is discussed later... It can be taken trivial with H∗ = RS to obtain a completely prior-less algorithm" and "We observe on the first experiment that PMEVI behaves almost identically to its EVI counterparts when no prior on the bias region is given."
- Why unresolved: While paper mentions experiments with different priors, it doesn't systematically analyze how different prior qualities or structures affect regret bounds or convergence rates
- What evidence would resolve it: Comprehensive study varying prior quality and structure across multiple MDP types, measuring both regret and convergence speed

### Open Question 3
- Question: Can mitigation parameter β in PMEVI be optimized dynamically rather than using static approximation in Algorithm 5?
- Basis in paper: Inferred - Paper presents Algorithm 5 as "variance approximation" but notes computing exact β would require solving NP-hard problems, suggesting potential for improvement
- Why unresolved: Current approach uses tractable upper bound for β, but paper doesn't explore whether adaptive or problem-specific approaches could yield better performance
- What evidence would resolve it: Experiments comparing different β computation strategies (adaptive, problem-specific, heuristic) against current static approximation across diverse MDP environments

## Limitations
- Computational complexity of solving projection LPs may be prohibitive for large state-action spaces in practice
- Algorithm's performance depends heavily on quality of bias estimation, vulnerable to poor exploration or highly disconnected state spaces
- Tractability of variance approximation for complex MDPs with large confidence regions remains uncertain

## Confidence

- **High confidence**: Theoretical framework and regret bounds (O(√sp(h*) S A T)) are well-established with clear connections to prior work on EVI and OFU algorithms
- **Medium confidence**: Practical performance gains depend heavily on quality of bias prior information and frequency of bias updates; experiments show promising results but are limited in scope and scale
- **Low confidence**: Tractability of variance approximation in Algorithm 5 for complex MDPs with large confidence regions remains uncertain, as does algorithm's behavior in continuous or high-dimensional state spaces

## Next Checks

1. Implement PMEVI-DT on suite of benchmark MDPs (GridWorld, RiverSwim, Chain MDPs) with varying levels of bias prior information to quantify practical benefit of bias constraints

2. Conduct thorough computational complexity analysis comparing PMEVI vs EVI in terms of LP solve times and memory usage for MDPs with S = 10, 50, 100 states

3. Test algorithm's sensitivity to choice of confidence region Mt (C1-C4) and impact on both regret bounds and computational efficiency