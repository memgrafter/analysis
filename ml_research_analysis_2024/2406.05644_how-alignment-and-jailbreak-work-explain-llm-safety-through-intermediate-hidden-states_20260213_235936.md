---
ver: rpa2
title: 'How Alignment and Jailbreak Work: Explain LLM Safety through Intermediate
  Hidden States'
arxiv_id: '2406.05644'
source_url: https://arxiv.org/abs/2406.05644
tags:
- layer
- jailbreak
- inputs
- chat
- states
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Large language models (LLMs) rely on safety alignment to prevent
  harmful responses, but jailbreak techniques can circumvent these protections. Due
  to the models' complexity, understanding how alignment and jailbreak work internally
  is challenging.
---

# How Alignment and Jailbreak Work: Explain LLM Safety through Intermediate Hidden States

## Quick Facts
- arXiv ID: 2406.05644
- Source URL: https://arxiv.org/abs/2406.05644
- Reference count: 40
- Key outcome: LLMs learn ethical concepts during pre-training and alignment bridges early ethical beliefs with middle-layer emotions; jailbreak disrupts this association rather than deceiving early classification.

## Executive Summary
This paper introduces a Weak-to-Strong Explanation framework to understand how alignment and jailbreak work in large language models by analyzing intermediate hidden states during inference. The authors find that LLMs learn ethical concepts during pre-training, not alignment, and can distinguish malicious from normal inputs in early layers. Alignment creates associations between these early ethical beliefs and emotional tokens in middle layers, which are refined into specific rejection or response tokens in later layers. Jailbreak attacks work by disrupting this association rather than deceiving the early ethical classifier. The paper proposes Logit Grafting, a method that approximates jailbreak effects by grafting positive emotions from normal inputs onto jailbreak inputs in middle layers, successfully reproducing jailbreak outcomes.

## Method Summary
The paper employs weak classifiers (SVM and MLP) to analyze the last token position of intermediate hidden states from each layer of multiple LLMs (7B to 70B). These classifiers distinguish malicious from normal inputs with >95% accuracy in early layers. The authors use Logit Lens to transform hidden states into tokens and identify emotional tokens emerging in middle layers for aligned models. They propose Logit Grafting, which modifies middle-layer hidden states by grafting positive emotions from normal inputs onto jailbreak inputs, to approximate jailbreak effects. Top-K Intermediate Consistency measures the association between early ethical beliefs and middle-layer emotions, showing correlation with attack success rates.

## Key Results
- Weak classifiers achieve >95% accuracy in distinguishing malicious from normal inputs using early layer hidden states
- Logit grafting successfully reproduces jailbreak effects by modifying middle-layer hidden states
- Top-K Intermediate Consistency in middle layers correlates with attack success rates for both malicious and jailbreak inputs

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLMs learn ethical concepts during pre-training rather than alignment, and can distinguish malicious from normal inputs in early layers.
- Mechanism: The model implicitly learns to attribute distinct features to safe and unsafe inputs during pre-training. This ethical knowledge is then refined and associated with emotional guesses in middle layers during alignment.
- Core assumption: Ethical classification is primarily a pre-training phenomenon, and alignment only bridges these early ethical beliefs with emotional tokens.
- Evidence anchors:
  - [abstract]: "We first confirm that LLMs learn ethical concepts during pre-training rather than alignment and can identify malicious and normal inputs in the early layers."
  - [section]: "We argue that LLMs which are strong have learned to judge and fit ethical concepts in pre-training data and can distinguish unethical or harmful inputs."
  - [corpus]: Weak classifier accuracy in early layers across multiple model families suggests a pre-training learned ethical feature space.
- Break condition: If alignment were the primary source of ethical knowledge, unaligned models would not be able to distinguish malicious from normal inputs as effectively.

### Mechanism 2
- Claim: Alignment associates early ethical beliefs with emotion guesses in middle layers, which are refined into specific rejection or response tokens in later layers.
- Mechanism: Alignment acts as a conceptual bridge, linking the ethical features extracted in early layers to emotional tokens in middle layers. These emotional tokens are then refined into the initial tokens of either a response or rejection in later layers.
- Core assumption: The alignment process focuses on creating associations between pre-trained ethical features and emotional tokens, rather than directly teaching ethical concepts.
- Evidence anchors:
  - [abstract]: "Alignment actually associates the early concepts with emotion guesses in the middle layers and then refines them to the specific reject tokens for safe generations."
  - [section]: "We argue that the alignment bridges feature extraction in the early layers with emotional tokens in the middle layers."
  - [corpus]: Visualization of intermediate hidden states shows the emergence of emotional tokens in middle layers for aligned models, but not for base models.
- Break condition: If alignment were teaching ethical concepts directly, we would expect to see significant changes in early layer hidden states between base and aligned models.

### Mechanism 3
- Claim: Jailbreak disrupts the transformation of early unethical classification into negative emotions, rather than deceiving early ethical classification.
- Mechanism: Jailbreak does not trick the model into misclassifying inputs as ethical in the early layers. Instead, it interferes with the association between early ethical beliefs and middle-layer emotional tokens, leading to ambiguous or positive emotions for malicious inputs.
- Core assumption: Jailbreak attacks target the alignment mechanism, specifically the association between early and middle layers, rather than the pre-trained ethical classifier.
- Evidence anchors:
  - [abstract]: "Jailbreak disturbs the transformation of early unethical classification into negative emotions."
  - [section]: "Weak classifiers can classify jailbreak, malicious, and normal inputs for the early layer hidden states with high accuracy, indicating that jailbreak inputs are unlikely to deceive the ethical concepts learned during pre-training."
  - [corpus]: Logit grafting experiments show that modifying middle-layer hidden states can reproduce jailbreak effects, confirming the importance of the association stage.
- Break condition: If jailbreak were deceiving early ethical classification, weak classifiers would not be able to accurately classify jailbreak inputs in early layers.

## Foundational Learning

- Concept: Intermediate hidden states and their role in LLM inference
  - Why needed here: The paper relies heavily on analyzing and interpreting intermediate hidden states to understand how alignment and jailbreak work. A solid understanding of what these states represent and how they are generated is crucial for following the arguments.
  - Quick check question: What information is typically preserved in the last position of the hidden states from each layer, and why is this useful for comparing inputs of different lengths?

- Concept: Weak classifiers and their application in interpretability
  - Why needed here: The paper introduces a Weak-to-Strong Explanation framework that uses weak classifiers to analyze intermediate hidden states. Understanding how these classifiers work and what their performance indicates is essential for interpreting the results.
  - Quick check question: If a weak classifier can accurately distinguish between malicious and normal inputs using intermediate hidden states, what does this imply about the model's internal representation of these inputs?

- Concept: Logit grafting and its use in approximating jailbreak effects
  - Why needed here: The paper proposes Logit grafting as a method to approximate the disruption caused by jailbreak. Understanding how this technique works and what it reveals about the jailbreak mechanism is crucial for evaluating the paper's conclusions.
  - Quick check question: How does logit grafting modify the forward pass of a model, and what does its success in reproducing jailbreak effects tell us about the nature of jailbreak attacks?

## Architecture Onboarding

- Component map:
  Input embedding layer -> Transformer layers (early, middle, later) -> Final linear layer -> Softmax function

- Critical path:
  1. Input embedding
  2. Transformer layers (early, middle, later)
  3. Final linear layer and softmax
  4. Token sampling

- Design tradeoffs:
  - Using weak classifiers vs. stronger models for interpretability: Weak classifiers are less prone to overfitting and can reveal more about the inherent structure of the hidden states.
  - Analyzing only the last position of hidden states vs. the entire sequence: Focusing on the last position allows for comparison across different input lengths but may miss some context.

- Failure signatures:
  - Weak classifier accuracy dropping to near-random in early layers: Suggests the model is not learning ethical concepts during pre-training.
  - Lack of emotional tokens in middle layers for aligned models: Indicates alignment is not properly associating early ethical beliefs with emotions.
  - Logit grafting failing to reproduce jailbreak effects: Suggests jailbreak is not primarily disrupting the association between early and middle layers.

- First 3 experiments:
  1. Train weak classifiers (SVM and MLP) on intermediate hidden states of aligned and base models to classify malicious vs. normal inputs.
  2. Visualize the decoding of middle-layer hidden states to identify the emergence of emotional tokens.
  3. Apply logit grafting to malicious inputs and evaluate the attack success rate to confirm the role of the association stage in jailbreak.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the ethical knowledge learned during pre-training differ across various model families, and what impact does this have on their alignment effectiveness?
- Basis in paper: [explicit] The paper confirms that LLMs learn ethical concepts during pre-training rather than alignment, and that unaligned models can also distinguish unethical or harmful inputs with similar performance to aligned models.
- Why unresolved: The paper does not investigate whether different model families (Llama, Mistral, Vicuna, etc.) learn different ethical concepts during pre-training, or how these differences affect their alignment effectiveness.
- What evidence would resolve it: Comparative analysis of ethical concept learning across different model families during pre-training, examining how these differences manifest in alignment effectiveness and safety performance.

### Open Question 2
- Question: What specific mechanisms in the middle layers allow for the association between early ethical beliefs and emotional tokens, and how can these mechanisms be strengthened to improve alignment?
- Basis in paper: [explicit] The paper finds that alignment associates early ethical beliefs with emotional guesses in middle layers, which are refined into specific reject tokens in later layers. Jailbreak disrupts this association rather than deceiving early ethical classification.
- Why unresolved: While the paper identifies the association between early ethical beliefs and emotional tokens as the target for alignment improvement, it does not explore the specific mechanisms in middle layers that enable this association or how to strengthen them.
- What evidence would resolve it: Detailed analysis of the middle layer mechanisms that facilitate the association between early ethical beliefs and emotional tokens, including experiments on strengthening these mechanisms to improve alignment.

### Open Question 3
- Question: How do different jailbreak techniques affect the association stage, and can we develop more effective defenses by targeting these specific disruptions?
- Basis in paper: [explicit] The paper demonstrates that jailbreak disrupts the association between early ethical beliefs and middle-layer emotional tokens, and proposes Logit Grafting as an approximation of this disruption.
- Why unresolved: The paper does not investigate how different jailbreak techniques (GCG, AutoDAN, Deepinception) specifically affect the association stage, or whether defenses can be developed by targeting these specific disruptions.
- What evidence would resolve it: Comparative analysis of how different jailbreak techniques affect the association stage, including experiments on developing defenses that target these specific disruptions to improve model safety.

## Limitations
- Limited model diversity: Analysis restricted to models fine-tuned from Llama architectures, generalizability to other architectures remains untested.
- Synthetic normal data: Normal input dataset generated by GPT-4 and Claude3-Opus rather than natural conversational data, potentially affecting weak classifier training.
- Jailbreak dataset imbalance: Paper uses multiple jailbreak datasets without addressing potential distribution shifts or quality variations between sources.

## Confidence

**High Confidence** (Supporting evidence is strong and consistent):
- Weak classifiers can distinguish malicious from normal inputs in early layers with >95% accuracy
- Logit grafting can reproduce jailbreak effects by modifying middle-layer hidden states
- Alignment shows measurable association between early ethical beliefs and middle-layer emotions (Top-K Intermediate Consistency)

**Medium Confidence** (Supporting evidence is present but has limitations):
- Ethical concepts are learned during pre-training rather than alignment
- Jailbreak disrupts the transformation of early unethical classification into negative emotions
- The three-stage process (early detection → middle association → late refinement) accurately describes LLM safety mechanisms

**Low Confidence** (Supporting evidence is indirect or based on limited samples):
- Logit grafting's quantitative contribution to attack success rate improvement
- The universality of the weak-to-strong explanation framework across all LLM architectures
- The complete absence of ethical feature learning during alignment in any layer

## Next Checks
1. **Cross-Architecture Validation**: Test the weak-to-strong explanation framework on non-Llama architectures (GPT, Claude, Gemini) to verify the universality of the three-stage process across different model families.

2. **Natural Data Comparison**: Replace the synthetic normal dataset with naturally occurring conversational data to assess whether the weak classifier performance and ethical concept detection remain consistent with synthetically generated inputs.

3. **Token Position Analysis**: Extend the analysis beyond the last token position to examine whether the three-stage process holds consistently across all token positions, or if the last position is uniquely informative for ethical concept detection.