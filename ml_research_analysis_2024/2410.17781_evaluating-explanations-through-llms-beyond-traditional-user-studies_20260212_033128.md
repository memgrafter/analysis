---
ver: rpa2
title: 'Evaluating Explanations Through LLMs: Beyond Traditional User Studies'
arxiv_id: '2410.17781'
source_url: https://arxiv.org/abs/2410.17781
tags:
- llms
- user
- arxiv
- human
- alignment
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates whether Large Language Models (LLMs) can
  replicate human participants in user studies evaluating Explainable AI (XAI) tools.
  The authors replicate a user study comparing counterfactual and causal explanations
  using seven LLMs across different experimental settings.
---

# Evaluating Explanations Through LLMs: Beyond Traditional User Studies

## Quick Facts
- arXiv ID: 2410.17781
- Source URL: https://arxiv.org/abs/2410.17781
- Reference count: 40
- LLMs can replicate human responses in XAI evaluation with partial alignment, showing strongest results for objective prediction tasks and weaker alignment for subjective confidence assessments.

## Executive Summary
This paper investigates whether Large Language Models (LLMs) can effectively replace human participants in user studies evaluating Explainable AI (XAI) tools. The authors conducted a replication study comparing counterfactual and causal explanations using seven different LLMs across multiple experimental conditions including memory usage and output aggregation strategies. Results demonstrate that LLMs can successfully replicate most conclusions from the original human study, though with varying degrees of alignment depending on the model architecture and task type.

The study reveals that while LLMs show strong performance in replicating objective task outcomes like prediction accuracy, they struggle more with subjective metrics such as confidence assessments. Qwen 2 72B and GPT-4o Mini demonstrated the highest general alignment with human responses, and the use of memory generally improved LLM performance. These findings suggest LLMs offer a promising, scalable alternative for qualitative XAI evaluation, though careful consideration of task types and experimental design is necessary for reliable results.

## Method Summary
The authors replicated a user study comparing counterfactual and causal explanations through a series of experiments with seven different LLMs. They tested various experimental factors including memory usage (with and without memory) and output aggregation strategies (aggregated vs non-aggregated responses). The study employed statistical tests to measure alignment between LLM and human responses across different task types, with particular focus on prediction accuracy and confidence assessment metrics. Multiple LLM architectures were evaluated to understand how different model designs affect alignment with human participants.

## Key Results
- LLMs successfully replicated most conclusions from the original human study, with varying alignment levels across different model architectures
- Best performance was observed for prediction tasks, while confidence assessments showed notably weaker alignment with human responses
- Qwen 2 72B and GPT-4o Mini demonstrated the highest general alignment with human participants across experimental conditions

## Why This Works (Mechanism)
LLMs can simulate human reasoning patterns when evaluating XAI explanations by leveraging their training on diverse human-generated content and their ability to process complex logical relationships. The success of this approach depends on the models' capacity to understand causal relationships and counterfactual scenarios, which are fundamental to evaluating explanation quality in XAI systems.

## Foundational Learning
- **LLM architecture variations** - Understanding differences between model architectures (like Qwen 2 72B vs GPT-4o Mini) is needed to select appropriate models for XAI evaluation, quick check: compare parameter counts and training approaches.
- **Memory utilization in LLMs** - Critical for maintaining context across multiple explanation evaluations, quick check: test with and without memory to observe performance differences.
- **Output aggregation strategies** - Different methods for combining multiple LLM responses affect reliability, quick check: compare aggregated vs non-aggregated response quality.
- **Statistical alignment metrics** - Methods for measuring correspondence between LLM and human responses, quick check: apply correlation analysis to different task types.
- **XAI explanation types** - Understanding counterfactual vs causal explanations is essential for proper evaluation, quick check: verify LLMs can distinguish between these explanation types.
- **Human perception of explanation quality** - Subjective aspects that LLMs may struggle to replicate, quick check: compare LLM confidence assessments with human ratings.

## Architecture Onboarding
Component map: Human study data -> LLM experimental setup -> Statistical analysis -> Alignment assessment
Critical path: Explanation generation → LLM evaluation → Statistical comparison → Result validation
Design tradeoffs: Computational efficiency vs. human-like reasoning quality
Failure signatures: Low correlation on subjective tasks, high variability in non-aggregated outputs
3 first experiments:
1. Test individual LLM performance on prediction tasks vs. confidence assessments
2. Compare memory vs. non-memory conditions for a single model
3. Evaluate aggregated vs. non-aggregated response strategies

## Open Questions the Paper Calls Out
None

## Limitations
- Alignment between LLM and human responses remains partial rather than complete
- Results are sensitive to methodological choices like memory usage and output aggregation
- LLMs show notably weaker performance on subjective metrics like confidence assessments

## Confidence
- Claims about LLMs replicating objective task outcomes: **Medium**
- Claims regarding subjective human experiences (confidence ratings): **Low**
- Generalizability across different XAI tools and domains: **Medium**

## Next Checks
1. Cross-domain validation - test LLM evaluation approach with XAI tools in different domains beyond the original study's focus
2. Longitudinal consistency testing - evaluate whether LLM responses remain stable over time and across different prompt formulations
3. Human-in-the-loop calibration - develop frameworks for calibrating LLM responses against human feedback iteratively to create hybrid evaluation systems