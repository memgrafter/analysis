---
ver: rpa2
title: Advancements and limitations of LLMs in replicating human color-word associations
arxiv_id: '2411.02116'
source_url: https://arxiv.org/abs/2411.02116
tags:
- llms
- human
- color
- colors
- associations
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study assessed the ability of Large Language Models (LLMs)
  to replicate human color-word associations. Using data from over 10,000 Japanese
  participants and testing seven LLM models (from GPT-3 to GPT-4o), researchers found
  that while newer models showed improved performance, the highest median accuracy
  remained around 50% (chance level 10%).
---

# Advancements and limitations of LLMs in replicating human color-word associations

## Quick Facts
- arXiv ID: 2411.02116
- Source URL: https://arxiv.org/abs/2411.02116
- Reference count: 0
- Primary result: Newer LLM models show improved color-word association accuracy (up to 50%) but still significantly underperform human performance

## Executive Summary
This study evaluates seven LLM models' ability to replicate human color-word associations using data from over 10,000 Japanese participants. The research reveals a clear generational performance progression, with GPT-4o achieving the highest accuracy around 50% (chance level 10%). Performance varies significantly across word categories, with abstract categories like Emotions showing much lower accuracy than concrete categories like Rhythm and Landscape. The study also demonstrates that LLMs develop color discrimination abilities similar to humans, with correlation coefficients around 0.7, despite fundamental limitations in semantic association capabilities.

## Method Summary
The study used 17 colors and 8 word categories with 10 words each, collected from 12,369 Japanese participants. Seven LLM conditions were tested (gpt4o_visual, gpt4o_text, gpt4_vision, gpt4_1106, gpt4_0613, gpt35_turbo_0613, davinci-002) using OpenAI API with temperature=0.0 for deterministic output. Each model received 100 randomized trials per color-category combination, and performance was evaluated using accuracy metrics, entropy calculations, and Pearson correlation coefficients for color discrimination patterns using Jensen-Shannon divergence.

## Key Results
- GPT-4o with visual inputs achieved highest accuracy at ~50% (chance level 10%)
- Performance varied across categories: Rhythm and Landscape outperformed Emotions and Impression
- Color discrimination correlation coefficients between humans and LLMs ranged from 0.32 to 0.73
- GPT-3 showed significantly lower correlation (0.3) compared to newer models

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLMs show a clear generational performance progression in replicating human color-word associations.
- Mechanism: Newer LLM models (GPT-4o) achieve higher accuracy by leveraging more sophisticated architectures and larger training datasets, allowing them to better approximate human semantic associations.
- Core assumption: LLM performance scales predictably with model version and training data quality.
- Evidence anchors:
  - [abstract]: "Our findings reveal a clear progression in LLM performance across generations, with GPT-4o achieving the highest accuracy"
  - [section]: "A general trend of decreasing entropy from earlier to more recent LLM versions was observed"
  - [corpus]: Weak evidence - corpus neighbors focus on color sentiment and visualization, not directly on LLM performance progression
- Break condition: If newer models plateau or performance regresses despite architectural improvements

### Mechanism 2
- Claim: Performance varies systematically across word categories, with abstract categories being more challenging.
- Mechanism: LLMs struggle with abstract semantic associations (Emotions, Impression) because these require nuanced cultural and contextual understanding that is less prevalent in training data compared to concrete categories (Taste, Rhythm).
- Core assumption: Training data distribution affects category-specific performance.
- Evidence anchors:
  - [abstract]: "while LLMs tended to excel in categories such as Rhythm and Landscape, they struggled with categories such as Emotions"
  - [section]: "In the Emotion category, human responses showed associations with Red strongly linked equally to 'angry' and 'excited'"
  - [corpus]: Weak evidence - corpus neighbors discuss color sentiment but not category-specific LLM performance
- Break condition: If performance differences across categories disappear with model scaling

### Mechanism 3
- Claim: LLMs demonstrate color discrimination abilities similar to humans despite limitations in semantic associations.
- Mechanism: LLMs develop internal representations of color similarity based on co-occurrence patterns in training data, leading to correlation coefficients around 0.7 with human discrimination patterns.
- Core assumption: Color discrimination can be inferred from distributional differences in color-word associations.
- Evidence anchors:
  - [abstract]: "color discrimination ability estimated from our color-word association data showed that LLMs demonstrated high correlation with human color discrimination patterns"
  - [section]: "The scatter plots illustrate the relationship between human and LLM divergence values, with correlation coefficients ranging from 0.32 to 0.73"
  - [corpus]: Weak evidence - corpus neighbors focus on color perception and visualization, not LLM color discrimination metrics
- Break condition: If correlation with human patterns drops significantly in newer models

## Foundational Learning

- Concept: Color-word association fundamentals
  - Why needed here: Understanding how humans form color-word associations is crucial for evaluating LLM replication attempts
  - Quick check question: What are the key factors that influence human color-word associations across different cultures?

- Concept: Semantic memory structures
  - Why needed here: The paper suggests fundamental differences in semantic memory between humans and LLMs, requiring understanding of how semantic associations are formed and stored
  - Quick check question: How do human semantic memory structures differ from the distributed representations in LLMs?

- Concept: Jensen-Shannon divergence
  - Why needed here: This metric is used to quantify color discrimination patterns between humans and LLMs
  - Quick check question: What does a high Jensen-Shannon divergence value indicate about the relationship between two probability distributions?

## Architecture Onboarding

- Component map: Human data collection -> LLM API integration (multiple models) -> Response processing -> Performance analysis -> Results interpretation
- Critical path: Human data collection → LLM query generation → Response processing → Performance analysis → Results interpretation
- Design tradeoffs: Text vs visual inputs for color specification, temperature parameter settings for LLM responses, choice of evaluation metrics
- Failure signatures: Low accuracy across all models, inconsistent performance across categories, failure to capture basic color discrimination
- First 3 experiments:
  1. Replicate the entropy analysis comparing human and LLM response distributions across all categories
  2. Test GPT-4o with varying temperature settings to examine the effect on distribution flatness
  3. Implement cross-cultural validation by testing the same LLM models with non-Japanese color-word association data

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How would different temperature parameter settings affect LLMs' ability to replicate human color-word associations and capture the diversity of human responses?
- Basis in paper: [explicit] The paper mentions that temperature was set to 0.0 for all LLM trials to obtain reliable estimates of choice distributions, and notes that previous studies showed GPT-3 generated basic color terms at a frequency very similar to that of human non-synaesthetes, especially when allowed more variability.
- Why unresolved: The study used a deterministic approach (temperature = 0) which may not capture the full spectrum of human associative diversity. The authors acknowledge this limitation and suggest future studies could examine how different parameters and settings could help explain variability in human responses.
- What evidence would resolve it: Experimental results comparing LLM performance across a range of temperature settings (e.g., 0.0, 0.5, 1.0) on the same color-word association task, measuring both accuracy in predicting human best-voted words and the distribution of responses to assess diversity.

### Open Question 2
- Question: Do LLMs demonstrate consistent patterns in color-word associations across different languages and cultural contexts, or are their associations primarily reflective of the training data's predominant language and culture?
- Basis in paper: [explicit] The study used Japanese participants and Japanese language questions for both humans and LLMs, and the authors note that "GPT-3 generally has poorer capability in handling Japanese compared to English, given that these models are often trained predominantly on English-language data."
- Why unresolved: The study was conducted only with Japanese participants and Japanese language data, making it unclear whether the observed patterns and limitations are universal or specific to the Japanese language/cultural context. The authors acknowledge potential language-specific limitations.
- What evidence would resolve it: Comparative studies using the same color-word association methodology with participants and LLMs across multiple languages and cultures (e.g., English, Japanese, Chinese, Spanish), measuring both performance and the nature of associations to identify universal vs. culture-specific patterns.

### Open Question 3
- Question: What specific aspects of LLM training data or architecture lead to the observed category-specific performance differences in color-word associations (e.g., better performance on Rhythm and Landscape vs. poorer performance on Emotions and Impression)?
- Basis in paper: [inferred] The paper observes significant performance variations across word categories, with LLMs excelling in categories like Rhythm and Landscape but struggling with categories such as Emotions and Impression. This suggests fundamental differences in how LLMs process different types of semantic associations.
- Why unresolved: While the paper documents these performance differences, it doesn't investigate the underlying causes. The authors suggest this might reflect "LLMs' difficulty in associating abstract or subjective words with colors" but don't explore the mechanisms behind this.
- What evidence would resolve it: Analysis of LLM training data composition across different semantic domains, architectural experiments testing various model modifications (e.g., attention mechanisms, layer configurations) specifically on emotion vs. concrete concept associations, and correlation studies between training data prevalence of certain concepts and model performance on those categories.

## Limitations
- Human dataset from Japanese participants only, limiting cultural generalizability
- Ceiling effect at 50% accuracy suggests fundamental gaps in LLM associative capabilities
- Unknown training data composition prevents full explanation of category-specific performance differences

## Confidence
- Generational progression: High - clear trend from GPT-3 to GPT-4o
- Category-specific performance: Medium - consistent patterns but influenced by unknown training data biases
- Color discrimination findings: Medium - correlation coefficients around 0.7 but GPT-3 shows much lower correlation

## Next Checks
1. Conduct cross-cultural validation by testing the same LLM models with color-word association data from non-Japanese populations to assess cultural dependency
2. Implement ablation studies varying temperature settings to examine how response variability affects both accuracy and entropy metrics
3. Analyze training data distributions across categories to quantify potential biases explaining performance differences between concrete (Rhythm, Landscape) and abstract (Emotions, Impression) categories