---
ver: rpa2
title: 'Multiverse of Greatness: Generating Story Branches with LLMs'
arxiv_id: '2411.14672'
source_url: https://arxiv.org/abs/2411.14672
tags:
- story
- game
- llms
- generated
- each
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces a framework called Dynamic Context Prompting/Programming
  (DCP/P) for generating graph-based story content using large language models (LLMs).
  The method involves providing LLMs with dynamic context history during story generation,
  inspired by dynamic programming and graph traversal algorithms, to create coherent
  visual novel games.
---

# Multiverse of Greatness: Generating Story Branches with LLMs

## Quick Facts
- arXiv ID: 2411.14672
- Source URL: https://arxiv.org/abs/2411.14672
- Reference count: 40
- Primary result: DCP/P approach generates more coherent visual novel stories than baseline, scoring 6.81±0.36 vs 6.53 across five linguistic metrics

## Executive Summary
This paper introduces Dynamic Context Prompting/Programming (DCP/P), a framework for generating coherent, multi-branch visual novel games using large language models. The method treats story generation as a graph traversal problem, using dynamic programming principles to maintain narrative continuity by providing LLMs with localized context history during generation. Evaluation shows that games created with DCP/P consistently outperform baseline approaches that lack context history, producing longer and more coherent narratives with smoother transitions between story segments.

## Method Summary
The DCP/P framework generates visual novel games through a graph-based approach where story chunks are created iteratively with dynamic context history. The method uses breadth-first search traversal to explore story branches, providing each generated chunk with conversation history from its parent chunk. A context overflow policy manages token limits by selectively retaining recent user messages and initial story chunks when approaching maximum token counts. The system generates story data (title, characters, scenes, synopsis), then uses this data to create a graph structure where each node represents a story chunk with branching decisions. The approach is evaluated against a baseline that generates stories without context history, using LLM judges to assess five linguistic aspects.

## Key Results
- DCP/P-generated games scored 6.81±0.36 across five linguistic metrics versus 6.53 for baseline approach
- Stories generated with DCP/P are longer and exhibit better narrative coherence with smoother transitions
- Qualitative analysis shows DCP/P produces better narrative structure and character consistency
- Word choice analysis reveals persistent biases in LLM outputs regardless of model family used

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Dynamic Context Prompting/Programming (DCP/P) improves narrative coherence by conditioning LLM generation on localized story context rather than just the initial story data.
- Mechanism: DCP/P treats each story chunk as a subproblem, using breadth-first search and dynamic programming ideas. The algorithm generates story chunks iteratively, passing the conversation history from the parent chunk to the child chunk, ensuring the model has sufficient context to maintain narrative continuity.
- Core assumption: Providing the LLM with recent conversation history improves its ability to generate coherent and contextually appropriate content compared to only using initial story data.
- Evidence anchors:
  - [abstract] "We evaluate DCP/P against our baseline, which does not provide context history to an LLM and only relies on the initial story data. Through objective evaluation, we show that simply providing the LLM with a summary leads to a subpar story compared to additionally providing the LLM with the proper context of the story."
  - [section] "The context overflow policy in our framework plays a crucial role in managing conversation histories efficiently while adhering to token limits."

### Mechanism 2
- Claim: The context overflow policy prevents loss of narrative coherence by selectively retaining relevant conversation history when approaching token limits.
- Mechanism: When the accumulated token count exceeds 80% of the maximum allowable tokens, the policy truncates the conversation history by prioritizing recent user messages and maintaining a few initial story chunks. It then limits the total token count to 60% of the maximum tokens after truncation.
- Core assumption: Retaining recent user messages and initial story chunks provides sufficient context for the LLM to continue generating coherent content without exceeding token limits.
- Evidence anchors:
  - [section] "During truncation, the policy prioritizes retaining recent user messages, ensuring essential context is preserved. It starts from the latest user message and iteratively checks the format and order of preceding assistant messages to maintain conversation coherence."
  - [corpus] Weak evidence - no direct corpus data supporting this specific mechanism.

### Mechanism 3
- Claim: Using dynamic branching types (normal, end-of-chapter, end-of-game) guides the LLM in generating appropriate story chunks based on the current narrative state.
- Mechanism: The algorithm selects prompts dynamically based on the branching type attached to the chunk-to-be-generated information in the frontier. This ensures that the LLM generates content that is appropriate for the current point in the story, whether it's a regular story progression, a chapter ending, or a game ending.
- Core assumption: Providing the LLM with information about the current branching type helps it generate more contextually appropriate content.
- Evidence anchors:
  - [section] "The branching types consist of normal, end-of-chapter, and end-of-game branching. Each story chunk encapsulates crucial narrative elements: the current story progression, detailed narrative events, and decision points that shape the narrative direction."
  - [corpus] Weak evidence - no direct corpus data supporting this specific mechanism.

## Foundational Learning

- Concept: Dynamic Programming
  - Why needed here: Dynamic programming is used to break down the problem of generating a coherent story into smaller subproblems (story chunks), solving each subproblem and combining the solutions to form the overall narrative.
  - Quick check question: How does dynamic programming help in managing the complexity of generating a coherent story?

- Concept: Graph Traversal Algorithms (Breadth-First Search)
  - Why needed here: Graph traversal algorithms are used to navigate the story graph, ensuring that all story branches are explored and that the narrative progresses logically.
  - Quick check question: Why is breadth-first search particularly useful for generating story branches in a visual novel game?

- Concept: Large Language Model Context Windows
  - Why needed here: Understanding the limitations of LLM context windows is crucial for designing the context overflow policy and ensuring that the model has sufficient context to generate coherent content.
  - Quick check question: What are the potential issues if the LLM's context window is too small for the narrative being generated?

## Architecture Onboarding

- Component map: Story Data Generation -> DCP/P Algorithm -> Context Overflow Policy -> Graph Database -> Web Interface -> Image Generation
- Critical path:
  1. Generate story data using LLM
  2. Initialize story chunk generation with DCP/P
  3. Manage context overflow during generation
  4. Store generated content in graph database
  5. Display game using web interface
- Design tradeoffs:
  - Using a dynamic context history improves coherence but increases computational complexity
  - Limiting the number of choice opportunities reduces generation costs but may limit narrative depth
  - Cross-initializing story data mitigates biases but may reduce variability
- Failure signatures:
  - Incoherent narrative progression
  - Missing or malformed choices
  - Black screens or unknown characters due to incorrect scene/character selection
  - Token limit exceeded errors
- First 3 experiments:
  1. Generate a simple story with minimal branching to test the basic DCP/P algorithm
  2. Introduce context overflow policy and test with longer stories to ensure coherence is maintained
  3. Test with different branching types to ensure the LLM generates appropriate content for each narrative state

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the Dynamic Context Prompting/Programming (DCP/P) approach scale to more complex story structures, such as non-linear narratives or stories with multiple protagonists?
- Basis in paper: [explicit] The paper mentions that the DCP/P approach could potentially be used to generate "any type of graph-based content" beyond visual novel games.
- Why unresolved: The paper focuses on evaluating the DCP/P approach for generating visual novel games with a single protagonist. It does not explore how the approach would handle more complex narrative structures or multiple protagonists.
- What evidence would resolve it: Further research is needed to test the DCP/P approach on different types of narrative structures and with multiple protagonists to assess its scalability and effectiveness.

### Open Question 2
- Question: How do different LLM architectures and training datasets impact the quality and creativity of the generated stories in the DCP/P approach?
- Basis in paper: [inferred] The paper discusses the potential for using different LLMs and image generative models, but only uses Claude 3 Opus in the current study. It also mentions that LLMs exhibit biases towards certain words and themes, which could be influenced by their training data.
- Why unresolved: The paper only uses one LLM and does not explore how different architectures or training datasets might impact the generated stories. It also does not investigate potential biases in the LLMs beyond word frequency and sentiment analysis.
- What evidence would resolve it: Further research is needed to compare the performance of the DCP/P approach using different LLM architectures and training datasets. It would also be valuable to conduct more in-depth analyses of potential biases in the generated stories.

### Open Question 3
- Question: How can the DCP/P approach be extended to incorporate user feedback and personalization in the story generation process?
- Basis in paper: [explicit] The paper mentions the potential for using retrieval-augmented generation to personalize story branches, but does not explore this further.
- Why unresolved: The paper focuses on generating stories based on pre-defined themes and parameters, but does not investigate how user feedback or preferences could be incorporated into the process.
- What evidence would resolve it: Further research is needed to develop methods for incorporating user feedback and preferences into the DCP/P approach. This could involve techniques such as active learning or interactive story generation.

## Limitations

- The evaluation relies on LLM judges rather than human evaluation, which may not accurately capture subjective narrative quality
- The study uses only one LLM (Claude 3 Opus), limiting generalizability to other model families
- Word choice analysis reveals persistent biases in LLM outputs that exist regardless of the prompting approach

## Confidence

**Confidence: Low** - The paper demonstrates that DCP/P outperforms the baseline in controlled evaluations, but several key limitations exist. The evaluation relies on LLM judges rather than human evaluation, which may not accurately capture the subjective quality of narrative experiences. The study uses only one LLM (Claude 3 Opus) for story generation, limiting generalizability to other model families. The word choice analysis reveals inherent biases in LLM outputs that persist regardless of the prompting approach, suggesting fundamental limitations in the models themselves rather than the framework design.

**Confidence: Medium** - The DCP/P framework shows technical improvements in coherence metrics, but the complexity of the system introduces practical challenges. The context overflow policy and retry mechanisms are not fully detailed, making implementation replication difficult. The study's use of a single seed for all stories and limited sampling of branching narratives (only exploring one path to the end) constrains the validity of the performance claims across diverse story scenarios.

**Confidence: Medium** - While the graph-based approach enables visual novel generation, the method has scalability constraints. The token management system may not generalize well to longer, more complex narratives. The reliance on DALL-E 3 for image generation introduces external dependencies that could fail independently of the DCP/P framework's effectiveness.

## Next Checks

1. **Human Evaluation Replication**: Conduct user studies with actual players to validate the LLM judge scores across the five linguistic dimensions (coherence, inspiration, narrative fluency, readability, and word complexity). Compare human ratings with LLM evaluations to establish correlation and identify potential gaps.

2. **Cross-Model Generalization Test**: Implement the DCP/P framework using different LLM families (e.g., GPT-4, Llama, Mistral) to verify whether the performance improvements hold across model architectures and training approaches. Document any model-specific failures or adaptations needed.

3. **Long-Form Narrative Stress Test**: Generate stories with increased complexity (more chapters, characters, and branching paths) to evaluate whether the context overflow policy and dynamic context mechanisms scale effectively. Measure coherence degradation points and identify at what narrative complexity the framework breaks down.