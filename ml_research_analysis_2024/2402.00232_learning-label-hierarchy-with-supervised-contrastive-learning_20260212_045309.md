---
ver: rpa2
title: Learning Label Hierarchy with Supervised Contrastive Learning
arxiv_id: '2402.00232'
source_url: https://arxiv.org/abs/2402.00232
tags:
- label
- learning
- contrastive
- classes
- hierarchical
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a family of Label-Aware Supervised Contrastive
  Learning (LASCL) methods that incorporate hierarchical label information into supervised
  contrastive learning. The core idea is to scale the temperature in the contrastive
  loss based on pairwise class similarities and introduce instance-center-wise contrastive
  learning using learnable label parameters as class centers.
---

# Learning Label Hierarchy with Supervised Contrastive Learning

## Quick Facts
- arXiv ID: 2402.00232
- Source URL: https://arxiv.org/abs/2402.00232
- Authors: Ruixue Lian; William A. Sethares; Junjie Hu
- Reference count: 29
- Primary result: LASCL variants outperform baseline supervised approaches in text classification accuracy, especially in few-shot settings

## Executive Summary
This paper introduces a family of Label-Aware Supervised Contrastive Learning (LASCL) methods that incorporate hierarchical label information into supervised contrastive learning. The core innovation involves scaling the temperature in the contrastive loss based on pairwise class similarities and introducing instance-center-wise contrastive learning using learnable label parameters as class centers. Experiments on three text classification datasets (20News, WOS, DBPedia) demonstrate that LASCL variants outperform baseline supervised approaches, particularly in few-shot settings. The learned label parameters can also be directly used as a nearest neighbor classifier without further finetuning.

## Method Summary
The method constructs textual descriptions for each class label using predefined templates that capture hierarchical relationships, then encodes these descriptions to obtain initial label representations. A pairwise class similarity matrix W is computed using cosine similarity between label representations. During training, four LASCL variants modify the supervised contrastive loss by scaling temperature based on W (LI, LIUC, LIC) and/or adding instance-center-wise contrastive terms using label representations as centers (LIC, LISC). The model uses BERT-base-uncased as the backbone encoder and periodically re-encodes label representations for stability. Evaluation uses linear probe classification with frozen encoder and direct testing without finetuning.

## Key Results
- LASCL variants achieve higher classification accuracy than baseline supervised contrastive learning across all three datasets
- Performance gains are particularly pronounced in few-shot settings (1-shot, 100-shot)
- The learned label representations can serve as effective class centers for nearest neighbor classification
- Combining both temperature scaling and center-based contrastive learning (LISC) yields the best overall performance

## Why This Works (Mechanism)

### Mechanism 1
Scaling temperature in contrastive loss by class similarity improves intra-class compactness and inter-class separation. By weighting negative pairs based on the similarity matrix W, samples from similar classes receive smaller penalties while dissimilar classes are pushed further apart, creating tighter clusters for similar classes while maintaining separation between different high-level categories.

### Mechanism 2
Using label representations as class centers improves intra-cluster compactness through instance-center-wise contrastive learning. By constructing positive pairs between instances and their corresponding label representations (treated as centers), the loss pulls instances closer to their class centers while pushing them away from other centers, creating more compact clusters.

### Mechanism 3
Combining both scaling and center-based contrastive learning yields superior performance by addressing both inter-class separation and intra-class compactness simultaneously. The LISC variant integrates both mechanisms, creating well-structured, discriminative feature spaces through dual optimization objectives.

## Foundational Learning

- Concept: Contrastive learning fundamentals (positive/negative pairs, temperature scaling, InfoNCE loss)
  - Why needed here: The paper builds on SCL by modifying how positive/negative pairs are weighted and introduces new contrastive objectives with class centers.
  - Quick check question: What happens to the contrastive loss when temperature τ approaches 0 or infinity?

- Concept: Hierarchical label structures and taxonomy encoding
  - Why needed here: The method explicitly leverages label hierarchy by constructing textual descriptions that capture multi-level relationships between classes.
  - Quick check question: How does the depth of the label hierarchy affect the quality of the learned class similarity matrix?

- Concept: Representation learning and embedding space geometry
  - Why needed here: The method's effectiveness depends on creating meaningful geometric structures in the embedding space (compact clusters, separated categories).
  - Quick check question: What geometric properties of the embedding space indicate successful hierarchical structure preservation?

## Architecture Onboarding

- Component map: BERT encoder → Sentence embeddings → Similarity computation → Contrastive loss computation → Linear probe classifier
- Critical path: 1) Encode label descriptions to obtain initial label representations; 2) Compute pairwise cosine similarities to form W; 3) During training, scale temperature in contrastive loss using W; 4) Update both sentence and label representations via backpropagation; 5) Re-encode label representations periodically for stability
- Design tradeoffs: Re-encoding frequency (more frequent updates provide better label representations but increase computation); template design (different templates capture different levels of hierarchical information); temperature scaling range (aggressive scaling may create overly separated clusters)
- Failure signatures: If class similarity matrix W shows little variation, the scaling mechanism provides no benefit; if learned label representations fail to cluster by high-level categories, the method cannot exploit hierarchical structure; if linear probe performance significantly lags behind direct testing, the learned label embeddings may not generalize well
- First 3 experiments: 1) Ablation study: Compare SCL vs LI (only scaling) to isolate the effect of class similarity scaling; 2) Label template sensitivity: Test different label description templates to find optimal hierarchical information capture; 3) Hierarchy depth analysis: Evaluate performance across different label hierarchy depths to understand granularity requirements

## Open Questions the Paper Calls Out

### Open Question 1
How does the proposed method perform on multi-label classification tasks where instances can belong to multiple categories simultaneously? The paper acknowledges this as a limitation and future direction, having only tested single-label classification scenarios.

### Open Question 2
How sensitive is the method to different label hierarchy granularities and depths? While the authors show performance varies with hierarchy depth on 20News, they don't systematically explore how different types of hierarchical relationships affect performance.

### Open Question 3
What is the impact of re-encoding label embeddings frequency on the final performance? The authors mention re-encoding every 500 steps but don't explore how different frequencies affect results, treating it as a fixed hyperparameter.

## Limitations
- The evaluation primarily focuses on classification accuracy without extensive analysis of embedding space geometry or generalization to unseen classes
- The effectiveness of temperature scaling relies heavily on the quality of the class similarity matrix W, which is constructed from textual label descriptions and may not generalize well to all datasets
- The computational overhead of maintaining and re-encoding learnable label representations is not thoroughly discussed, particularly for large-scale label spaces

## Confidence
- **High Confidence**: The core mechanism of using label representations as class centers is well-supported by both theoretical grounding and experimental results
- **Medium Confidence**: The temperature scaling mechanism shows promise but relies heavily on the assumption that textual label descriptions accurately capture semantic relationships
- **Low Confidence**: The claim that combining both mechanisms yields superior performance is less substantiated, as the ablation studies do not fully isolate the individual contributions of each mechanism

## Next Checks
1. Conduct t-SNE or UMAP visualizations of the learned embedding spaces for LASCL variants versus baseline SCL to empirically verify that LASCL creates more compact intra-class clusters and better-separated inter-class boundaries.
2. Perform nearest neighbor classification using only the learned label representations to evaluate whether the label embeddings themselves capture meaningful semantic relationships between classes.
3. Test the learned label representations on a held-out dataset with a similar label hierarchy to assess whether the representations generalize beyond the training distribution or overfit to specific dataset characteristics.