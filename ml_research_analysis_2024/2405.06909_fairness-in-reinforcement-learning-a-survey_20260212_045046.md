---
ver: rpa2
title: 'Fairness in Reinforcement Learning: A Survey'
arxiv_id: '2405.06909'
source_url: https://arxiv.org/abs/2405.06909
tags:
- fairness
- fair
- learning
- agent
- reinforcement
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper provides a comprehensive survey of fairness in reinforcement\
  \ learning (RL), a topic that has received less attention compared to fairness in\
  \ other machine learning domains. The authors review the fundamental concepts of\
  \ RL and discuss various definitions of fairness in RL, including welfare-based,\
  \ proportional fairness, coefficient of variation, Q-value based, \u03B1-fair utility,\
  \ issue-based, calibrated fairness, and regularized maximin fairness."
---

# Fairness in Reinforcement Learning: A Survey

## Quick Facts
- arXiv ID: 2405.06909
- Source URL: https://arxiv.org/abs/2405.06909
- Reference count: 15
- One-line primary result: Comprehensive survey of fairness in RL, covering definitions, applications, methodologies, and future research directions.

## Executive Summary
This paper provides a comprehensive survey of fairness in reinforcement learning (RL), a topic that has received less attention compared to fairness in other machine learning domains. The authors review fundamental RL concepts and discuss various definitions of fairness in RL, including welfare-based, proportional fairness, coefficient of variation, Q-value based, α-fair utility, issue-based, calibrated fairness, and regularized maximin fairness. They present application domains of fair RL, such as recommendation systems, work distribution, scheduling and resource distribution, infrastructure control, and robotics. The paper also discusses methodologies used to incorporate fairness in single-agent and multi-agent RL systems, highlighting trade-offs between fairness and performance. Finally, the authors identify future research directions, emphasizing the need for fairness in reinforcement learning from human feedback (RLHF), cross-domain fair RL approaches, additional application domains, fairness over time, and adversarial fairness.

## Method Summary
This survey paper provides a comprehensive overview of fairness in reinforcement learning by reviewing existing literature across definitions, methodologies, and applications. The authors conducted a literature review of 15+ papers to identify key fairness definitions (welfare-based, proportional fairness, CV, Q-value based, α-fair utility, issue-based, calibrated fairness, regularized maximin fairness), application domains (recommendation systems, work distribution, scheduling/resource distribution, infrastructure control, IoT, robotics), and methodologies for implementing fairness in single-agent vs multi-agent RL systems. The survey aims to provide the most up-to-date snapshot of the frontiers of fairness in RL, highlighting current gaps and future research directions.

## Key Results
- Fairness in RL requires explicitly choosing a welfare-based, proportional, or Q-value-based measure, as each captures fundamentally different fairness philosophies
- Multi-objective RL approaches can encode fairness by modifying the reward function with fairness penalties or using multi-agent architectures optimizing both individual and collective welfare
- In multi-agent RL, fairness can be achieved by regularizing policies to be equivariant across sensitive groups, ensuring equitable reward distributions

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Defining fairness in RL requires explicitly choosing a welfare-based, proportional, or Q-value-based measure, as each captures fundamentally different fairness philosophies.
- Mechanism: Different fairness notions map to distinct objective functions in RL, so the choice of fairness metric directly determines the optimization target and the resulting policy behavior.
- Core assumption: The RL system can represent the chosen fairness notion mathematically within its reward or objective function without breaking the MDP structure.
- Evidence anchors:
  - [abstract] "we discuss the various definitions of fairness in RL that have been put forth thus far."
  - [section] Lists multiple definitions: welfare-based, proportional fairness, Q-value based, α-fair utility, etc.
  - [corpus] Weak evidence; corpus papers focus on other fairness topics not directly tied to RL-specific definitions.
- Break condition: If the fairness measure cannot be expressed in a differentiable or tractable form, the RL algorithm cannot optimize it, causing the fairness definition to be infeasible in practice.

### Mechanism 2
- Claim: Multi-objective RL approaches can encode fairness by modifying the reward function with fairness penalties or by using multi-agent architectures where agents optimize both individual and collective welfare.
- Mechanism: By integrating fairness metrics into the reward signal (e.g., via the coefficient of variation or α-fair utility), the agent's policy gradients naturally incorporate fairness as part of the optimization landscape.
- Core assumption: The reward modification preserves the Markov property or the policy gradient theorem holds under the modified rewards.
- Evidence anchors:
  - [abstract] "We continue to highlight the methodologies researchers used to implement fairness in single- and multi-agent RL systems."
  - [section] "Jiang and Lu (2019), on the other hand, introduce the Fair-Efficient Network (FEN), an RL-based model that makes use of a 'fair-efficient reward'..."
  - [corpus] Weak evidence; corpus neighbors focus on static ML fairness, not RL-specific reward engineering.
- Break condition: If the fairness term dominates the reward, the agent may fail to learn any useful task behavior, or if it's too weak, fairness is ignored.

### Mechanism 3
- Claim: In multi-agent RL, fairness can be achieved by regularizing policies to be equivariant across sensitive groups, ensuring equitable reward distributions.
- Mechanism: By enforcing that the joint policy transforms equivariantly with respect to group identity, the agents learn strategies that treat groups similarly regardless of underlying biases.
- Core assumption: The environment and task structure permit such equivariant transformations without compromising overall performance.
- Evidence anchors:
  - [abstract] "Finally, we critically examine gaps in the literature, such as understanding fairness in the context of RLHF, that still need to be addressed in future work to truly operationalize fair RL in real-world systems."
  - [section] "Grupen, Selman, and Lee (2022) introduce a different multi-agent angle to fair RL: team-based fairness, a group-based fairness measure that seeks equitable reward distributions across sensitive groups within a team."
  - [corpus] Weak evidence; corpus does not contain multi-agent equivariant fairness examples.
- Break condition: If the environment's dynamics are asymmetric or the task inherently benefits one group, enforcing equivariance may lead to suboptimal or infeasible policies.

## Foundational Learning

- Concept: MDP formulation and the role of the reward function
  - Why needed here: Fairness in RL is always expressed through modifications to the reward or objective function; without understanding MDPs, one cannot see how fairness integrates.
  - Quick check question: In an MDP, what are the three key components that determine the agent's policy?
- Concept: Policy gradient theorem and its extensions to non-linear objectives
  - Why needed here: Many fairness definitions lead to non-linear reward functions; understanding when the policy gradient theorem still applies is crucial for correct algorithm design.
  - Quick check question: What condition must a reward function satisfy for the standard policy gradient theorem to hold?
- Concept: Multi-objective optimization and Pareto efficiency
  - Why needed here: Fairness in RL often involves balancing fairness with performance; knowing how to find Pareto-optimal solutions prevents arbitrary trade-off choices.
  - Quick check question: In a multi-objective setting, what does it mean for a policy to be Pareto-optimal?

## Architecture Onboarding

- Component map:
  - MDP environment -> Agent -> Policy network -> Reward function (includes fairness term) -> Optimizer
  - Optional: Multi-agent coordinator -> Team welfare function -> Individual agent policies
- Critical path:
  1. Define fairness notion -> Encode in reward/objective
  2. Verify MDP structure preserved (Markov property)
  3. Choose RL algorithm (e.g., DQN, PPO, actor-critic)
  4. Train and monitor fairness metrics alongside performance
  5. Evaluate Pareto trade-offs
- Design tradeoffs:
  - Fairness term strength vs. task performance: Too strong fairness hurts performance; too weak ignores fairness.
  - Computational cost of fairness evaluation vs. training speed: Fairness metrics may require additional rollouts or group statistics.
  - Generalization vs. domain-specific fairness: General fairness definitions may be less effective than tailored ones.
- Failure signatures:
  - Agent collapses to a degenerate policy that ignores the main task.
  - Training diverges when fairness term is non-differentiable or unstable.
  - Performance drops sharply with no measurable fairness improvement.
- First 3 experiments:
  1. Single-agent MDP with a simple fairness reward penalty (e.g., coefficient of variation); measure both task reward and fairness metric.
  2. Multi-agent setup with team welfare function; compare individual vs. team reward optimization.
  3. Vary the strength of the fairness term and plot the fairness-performance Pareto frontier.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What are the most effective fairness definitions for RLHF across different domains?
- Basis in paper: [explicit] The paper identifies fairness in RLHF as a key future research direction, noting that current approaches like MaxMin-RLHF focus on synthetic data and need evaluation on real-world human preference data with organic diversity.
- Why unresolved: Different fairness definitions may perform better in different contexts, and there is no systematic evaluation of how various definitions (e.g., welfare-based, proportional fairness, α-fair utility) apply to RLHF.
- What evidence would resolve it: Comparative studies across multiple domains evaluating the performance of different fairness definitions in RLHF settings, including both synthetic and real-world human preference data.

### Open Question 2
- Question: How can fairness be maintained continuously throughout the runtime of RL systems rather than only at the final time step?
- Basis in paper: [explicit] The paper highlights "Fairness over Time" as a research gap, noting that current literature treats fairness as a final objective rather than ensuring it at any point during interaction.
- Why unresolved: Most existing approaches focus on long-term fairness outcomes, but real-world applications may require continuous fairness monitoring and adjustment.
- What evidence would resolve it: Development and empirical validation of RL algorithms that incorporate runtime fairness monitoring and adjustment mechanisms, tested across various continuous RL applications.

### Open Question 3
- Question: What are the trade-offs between different fairness definitions in RL, and how can multiple fairness requirements be achieved simultaneously?
- Basis in paper: [explicit] The paper notes that fairness definitions in RL are highly inconsistent and often mutually exclusive, and existing frameworks are fairness-definition-agnostic but lack validation.
- Why unresolved: Different fairness definitions may conflict with each other, and there is no clear methodology for selecting or combining multiple fairness definitions in a single RL system.
- What evidence would resolve it: Empirical studies comparing the performance and outcomes of different fairness definitions in the same RL settings, along with frameworks that can integrate multiple fairness definitions while maintaining system performance.

## Limitations
- The field remains nascent with relatively few published works compared to fairness in supervised learning, limiting empirical validation
- Many claims about fairness-RL integration are theoretical rather than experimentally validated
- The survey does not report original experiments to test the proposed mechanisms

## Confidence
- Mechanism 1 (fairness definition mapping): Medium
- Mechanism 2 (reward function modification): Medium
- Mechanism 3 (equivariant regularization in multi-agent settings): Low

## Next Checks
1. Implement a simple MDP with coefficient of variation-based fairness reward and measure the fairness-performance Pareto frontier empirically.
2. Replicate the Fair-Efficient Network approach on a benchmark RL task to assess the impact of fairness penalties on both fairness metrics and task performance.
3. Design a multi-agent environment where group-based fairness is explicitly defined and test whether equivariant regularization can achieve equitable reward distributions without catastrophic performance loss.