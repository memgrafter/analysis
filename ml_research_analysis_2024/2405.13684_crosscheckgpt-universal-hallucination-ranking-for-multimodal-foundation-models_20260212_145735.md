---
ver: rpa2
title: 'CrossCheckGPT: Universal Hallucination Ranking for Multimodal Foundation Models'
arxiv_id: '2405.13684'
source_url: https://arxiv.org/abs/2405.13684
tags:
- inputs
- evidence
- visual
- audio
- hallucination
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: CrossCheckGPT is a reference-free universal hallucination ranking
  method for multimodal foundation models. It assesses hallucination by comparing
  outputs of a target model against those from a set of independent evidence models,
  exploiting the principle that hallucinated content is unlikely to be consistently
  generated across models.
---

# CrossCheckGPT: Universal Hallucination Ranking for Multimodal Foundation Models

## Quick Facts
- arXiv ID: 2405.13684
- Source URL: https://arxiv.org/abs/2405.13684
- Reference count: 40
- CrossCheckGPT achieves 98% and 89% system-level correlation with human judgments on MHaluBench and AVHalluBench, respectively.

## Executive Summary
CrossCheckGPT is a reference-free method for ranking hallucination susceptibility across multimodal foundation models. It exploits the principle that hallucinated content is unlikely to be consistently generated across independent models by comparing target model outputs against evidence from multiple independent models. The approach uses either explicit generation of evidence passages or implicit prompting to determine factual consistency, with confidence-based weighting to account for varying evidence quality. Evaluated across text, image, and audio-visual domains, CrossCheckGPT demonstrates superior correlation with human judgments compared to baselines like SelfCheckGPT and UniHD.

## Method Summary
CrossCheckGPT detects hallucinations by comparing target model outputs against evidence from multiple independent models, exploiting the principle that hallucinated content is unlikely to be consistently generated across different models. It offers two variants: CrossCheck-explicit, which generates multiple evidence passages per evidence model to capture diverse outputs, and CrossCheck-implicit, which prompts evidence models to analyze factual errors. A confidence-based weighting mechanism down-weights evidence from unreliable models using SelfCheckGPT-derived uncertainty scores. The method is evaluated on WikiBio, MHaluBench, and AVHalluBench benchmarks, achieving high correlation with human judgments across text, image, and audio-visual domains.

## Key Results
- Achieves 98% system-level Spearman correlation with human judgments on MHaluBench
- Achieves 89% system-level correlation with human judgments on AVHalluBench
- Outperforms baselines like SelfCheckGPT and UniHD across all evaluated benchmarks

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: CrossCheckGPT detects hallucinations by comparing target model outputs against evidence from multiple independent models.
- **Mechanism**: It exploits the principle that hallucinated content is unlikely to be consistently generated across different models. By aggregating agreement or disagreement signals across multiple evidence models, it quantifies factual consistency.
- **Core assumption**: Hallucinations are idiosyncratic to individual models and unlikely to recur across independent systems.
- **Evidence anchors**:
  - [abstract] "the same hallucinated content is unlikely to be generated by different independent systems, hence cross-system consistency can provide meaningful and accurate hallucination assessment scores"
  - [section] "CrossCheckGPT checks the cross-consistency by comparing against evidence generated from a set of independent models"
- **Break condition**: If evidence models share training data or architectural biases, they may generate similar hallucinations, reducing detection power.

### Mechanism 2
- **Claim**: CrossCheck-explicit generates multiple evidence passages per evidence model to capture diverse outputs, improving robustness.
- **Mechanism**: By stochastically sampling multiple passages from each evidence model and comparing them to the target output, it covers a broader output space and disentangles generation from verification.
- **Core assumption**: Higher diversity in evidence passages reduces the risk of missing hallucinated content due to sampling bias.
- **Evidence anchors**:
  - [section] "CrossCheck-explicit stochastically generates a set of evidence passages from each evidence model and computes the average distance between each evidence passage and the target response"
  - [section] "CrossCheck-explicit can better cover the output space by disentangling the evidence generation and verification tasks"
- **Break condition**: If sampling introduces too much noise or if evidence models have high self-consistency (e.g., overfitted outputs), this method may become less discriminative.

### Mechanism 3
- **Claim**: Confidence-based weighting down-weights evidence from unreliable models, improving score calibration.
- **Mechanism**: Each evidence model's contribution is weighted by the inverse of its SelfCheckGPT score, reflecting its propensity to hallucinate.
- **Core assumption**: Models that hallucinate more often are less reliable as judges of factual consistency.
- **Evidence anchors**:
  - [section] "Each evidence model Mj stochastically generates Nj passages to check the response against, and since systems may have different levels of reliability, a factor Î·j can be assigned to the passages generated from model Mj"
  - [section] "a weighting mechanism is proposed where the scores are weighted by model uncertainty reflected by SelfCheckGPT scores"
- **Break condition**: If all evidence models are similarly unreliable, weighting may not improve results; or if SelfCheckGPT itself is poorly calibrated, the weights may be misleading.

## Foundational Learning

- **Concept**: Cross-consistency as a proxy for factual correctness.
  - **Why needed here**: This is the core assumption that hallucinations are model-specific; understanding this is essential to grasp why comparing outputs across models helps detect hallucinations.
  - **Quick check question**: Why might two different models agree on a factual claim but disagree on a hallucinated one?

- **Concept**: Multimodal large language models (MLLMs).
  - **Why needed here**: The paper applies CrossCheckGPT to models with text, image, and audio-visual inputs; understanding how these models generate and process multimodal inputs is key to applying the method.
  - **Quick check question**: What distinguishes a multimodal LLM from a unimodal one in terms of input handling?

- **Concept**: Self-consistency and hallucination detection.
  - **Why needed here**: CrossCheckGPT builds on SelfCheckGPT; understanding how self-consistency works (and its limitations) clarifies why cross-consistency is an improvement.
  - **Quick check question**: What is a key weakness of SelfCheckGPT that CrossCheckGPT addresses?

## Architecture Onboarding

- **Component map**:
  - Target model -> Evidence models -> LLM judge -> Confidence weighting -> Final hallucination score

- **Critical path**:
  1. Target model generates output for a query.
  2. Each evidence model either generates multiple passages or is prompted to analyze factual errors.
  3. LLM judge compares target sentences to evidence, outputting binary support/contradiction scores.
  4. Scores are aggregated (optionally weighted by evidence model confidence).
  5. Final hallucination score is computed per model, enabling ranking.

- **Design tradeoffs**:
  - CrossCheck-explicit vs. CrossCheck-implicit: The former is more robust but computationally heavier; the latter is faster but may miss diverse hallucinations.
  - Number of evidence models vs. diversity: More models improve coverage but increase computational cost and potential for correlated errors.
  - Confidence weighting vs. equal weighting: Weighting can improve calibration but depends on the reliability of SelfCheckGPT scores.

- **Failure signatures**:
  - Poor correlation with human judgments: May indicate evidence models are too similar or not sufficiently independent.
  - High variance in scores across runs: May indicate insufficient sampling or instability in evidence generation.
  - Weight concentration: If one model dominates weighting, it may signal a lack of model diversity or a failure in confidence estimation.

- **First 3 experiments**:
  1. **CrossCheck-explicit on WikiBio**: Compare hallucination ranking of 10 open-source LLMs against leaderboard metrics; measure system-level Spearman correlation.
  2. **CrossCheck-implicit on MHaluBench**: Evaluate hallucination ranking for image-to-text models; measure correlation against human annotations and CHAIR/POPE scores.
  3. **CrossCheck-explicit weighted on AVHalluBench**: Benchmark audio-visual LLMs; measure system-level correlation against manual reference descriptions; analyze impact of audio-visual inputs on hallucination rates.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does CrossCheckGPT's weighting mechanism based on SelfCheckGPT scores effectively account for model correlations arising from similar training data or checkpoints?
- Basis in paper: [inferred] The paper mentions that model correlations due to similar training data or checkpoints could lead to similar mistakes but does not investigate whether the weighting mechanism accounts for this.
- Why unresolved: The paper proposes a weighting mechanism based on SelfCheckGPT scores but does not evaluate its effectiveness in mitigating bias from correlated models.
- What evidence would resolve it: An experiment comparing CrossCheckGPT performance with and without correlation-aware weighting would demonstrate the effectiveness of addressing model correlations.

### Open Question 2
- Question: How does CrossCheckGPT's performance vary with the diversity and quality of evidence models?
- Basis in paper: [inferred] The paper uses a fixed set of evidence models but does not investigate how varying their diversity or quality affects CrossCheckGPT's performance.
- Why unresolved: The impact of evidence model selection on CrossCheckGPT's robustness and reliability is not explored.
- What evidence would resolve it: Experiments systematically varying the number, diversity, and quality of evidence models would reveal the impact on CrossCheckGPT's performance.

### Open Question 3
- Question: Can CrossCheckGPT be extended to handle non-text modalities like images or audio directly without relying on text-based evidence?
- Basis in paper: [explicit] The paper focuses on multimodal models generating text outputs and explores CrossCheck-explicit and CrossCheck-implicit for text-based evidence.
- Why unresolved: The paper does not investigate CrossCheckGPT's applicability to non-text modalities like images or audio.
- What evidence would resolve it: Developing and evaluating CrossCheckGPT variants for non-text modalities would demonstrate its broader applicability.

### Open Question 4
- Question: How does CrossCheckGPT perform in real-world applications with noisy or ambiguous inputs?
- Basis in paper: [inferred] The paper evaluates CrossCheckGPT on benchmark datasets but does not assess its performance in real-world scenarios with noisy or ambiguous inputs.
- Why unresolved: The robustness of CrossCheckGPT to real-world data variations is not explored.
- What evidence would resolve it: Testing CrossCheckGPT on real-world datasets with varying levels of noise and ambiguity would demonstrate its practical applicability.

## Limitations

- **Model Independence Assumption**: The core assumption that hallucinations are idiosyncratic to individual models is critical but untested. If evidence models share significant training data or architectural biases, their outputs may exhibit correlated hallucinations, undermining detection accuracy.

- **Generalizability Across Domains**: While evaluated on text, image, and audio-visual inputs, the method's effectiveness on other multimodal tasks (e.g., cross-modal retrieval, reasoning over long-form content) remains unclear.

- **Confidence Weighting Reliability**: The inverse SelfCheckGPT weighting assumes that models with higher self-reported uncertainty are less reliable as evidence providers. However, if SelfCheckGPT itself is poorly calibrated or if all evidence models are similarly unreliable, this weighting may not improve results and could even degrade performance.

## Confidence

**High Confidence**:
- CrossCheckGPT achieves high system-level correlation (98% on MHaluBench, 89% on AVHalluBench) with human judgments, outperforming baselines like SelfCheckGPT and UniHD.
- The CrossCheck-explicit variant is more robust than CrossCheck-implicit due to stochastic evidence generation, covering a broader output space.

**Medium Confidence**:
- Confidence-based weighting improves score calibration by down-weighting unreliable evidence models. This depends on the reliability of SelfCheckGPT uncertainty estimates, which is not fully validated.
- CrossCheckGPT is effective for multimodal foundation models (text, image, audio-visual). While results are strong, the method's performance on less common modalities or complex cross-modal tasks is untested.

**Low Confidence**:
- CrossCheckGPT is universally applicable across all multimodal tasks without further adaptation. The paper does not test edge cases or adversarial inputs that could expose limitations.

## Next Checks

1. **Independence Analysis**: Conduct an empirical study to quantify the overlap in training data and architectural biases among evidence models. Test whether correlated hallucinations reduce CrossCheckGPT's detection accuracy.

2. **Cross-Domain Robustness**: Evaluate CrossCheckGPT on additional multimodal benchmarks, including tasks like cross-modal retrieval, reasoning over long-form content, and adversarial inputs. Measure performance degradation or improvement.

3. **Weighting Calibration**: Validate the reliability of SelfCheckGPT uncertainty estimates by comparing them against ground-truth hallucination rates. Test alternative weighting schemes (e.g., ensemble methods) to determine if they improve calibration.