---
ver: rpa2
title: Offline Risk-sensitive RL with Partial Observability to Enhance Performance
  in Human-Robot Teaming
arxiv_id: '2402.05703'
source_url: https://arxiv.org/abs/2402.05703
tags:
- policy
- pomdp
- data
- human
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work proposes a method for offline risk-sensitive reinforcement
  learning with partial observability to improve human-robot teaming performance.
  The core idea is to learn a Partially Observable Markov Decision Process (POMDP)
  model from a limited dataset of human-robot interaction experiences, incorporating
  a Bayesian representation of model uncertainty.
---

# Offline Risk-sensitive RL with Partial Observability to Enhance Performance in Human-Robot Teaming

## Quick Facts
- arXiv ID: 2402.05703
- Source URL: https://arxiv.org/abs/2402.05703
- Reference count: 40
- One-line primary result: A method for offline risk-sensitive RL with POMDPs improves human-robot teaming performance and generalization across diverse participants.

## Executive Summary
This work addresses the challenge of improving human-robot teaming performance using offline reinforcement learning with partial observability. The authors propose learning a Partially Observable Markov Decision Process (POMDP) model from limited human-robot interaction data, incorporating physiological and behavioral features to infer hidden performance states. The method extends risk-sensitive policy selection to POMDPs via Monte Carlo evaluation over model uncertainty, selecting policies that balance performance and robustness. Experiments with 26 human participants in a simulated robot teleoperation environment demonstrate statistically significant performance improvements over the random policy used to collect the initial data.

## Method Summary
The method learns a POMDP from a dataset of human-robot interactions using Extra Tree Classifiers to map high-dimensional physiological and behavioral features to low-dimensional states. The POMDP incorporates a Bayesian representation of model uncertainty through Dirichlet posteriors over observation functions. Risk-sensitive policy selection via Value-at-Risk (VaR) at risk level 0.5 selects the most robust policy from multiple candidates solved with different discount factors. The approach handles partial observability by maintaining a belief state over hidden performance states inferred from multimodal signals like heart rate, HRV, gaze, and keystrokes.

## Key Results
- Statistically significant higher mission scores compared to random policy baseline
- Spearman correlation of 0.325 between risk sensitivity parameter and global mission score (p < 0.001)
- Friedman test shows significant difference among policies (Ï‡Â²(3) = 13.07, p < 0.01)
- Demonstrates generalization across 26 diverse human participants

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Offline risk-sensitive RL with POMDP model learning can improve human-robot teaming performance even with limited and noisy human data.
- **Mechanism:** The method learns a Bayesian POMDP from a small dataset of human-robot interactions, using physiological and behavioral features to infer a hidden performance state. Risk-sensitive policy selection via Monte Carlo evaluation over model uncertainty (via Dirichlet posteriors) selects policies that balance performance and robustness to model errors.
- **Core assumption:** The human performance state is partially observable but can be inferred from multimodal signals (HR, HRV, gaze, keystrokes) via classifiers trained on labeled data.
- **Evidence anchors:**
  - [abstract] "Experiments were conducted with a group of twenty-six human participants within a simulated robot teleoperation environment, yielding empirical evidence of the methodâ€™s efficacy."
  - [section 4] "Using this data set, the Human-Robot system is modeled as a POMDP, achieving an interpretable state space representation and a Bayesian representation of model uncertainty."
  - [corpus] Weak - no direct comparison with offline RL for human-robot teaming.

### Mechanism 2
- **Claim:** Extending the EvC risk-sensitive policy selection to POMDPs enables robust behavior despite model uncertainty from partial observability.
- **Mechanism:** For each candidate policy, multiple POMDP models are sampled from the Dirichlet posterior over observation functions. The policy's performance is evaluated across these models, and the Value-at-Risk at 0.5 quantile selects the most robust one.
- **Core assumption:** The Dirichlet posterior over observation functions adequately represents uncertainty in the learned model, and sampling enough models approximates the performance distribution.
- **Evidence anchors:**
  - [section 4] "By resorting to the Bayesian formalism and Monte Carlo sampling, we extend the EvC method to handle partial observable domains and to select the safest policy according to a risk-sensitive measure and POMDP model uncertainty."
  - [section 5.2] "The Spearman's rank correlation reveals a significant positive correlation between ð›½ and the global mission score (ðœŒ(ð›½)= 0.325, ð‘(ð›½)= 153, ð‘(ð›½)-value < 0.001)."
  - [corpus] Weak - no other POMDP-based offline RL with risk-sensitive selection.

### Mechanism 3
- **Claim:** Using low-dimensional interpretable states (performance, robot mode, alarm status) in the POMDP improves generalization across diverse participants.
- **Mechanism:** The hidden performance state is inferred via classifiers trained on labeled data, and the visible states are robot autonomy mode and alarm status. This structure allows the policy to adapt task allocation based on inferred human state.
- **Core assumption:** The low-dimensional state space is sufficient to capture the key aspects of human-robot team performance and to generalize across participants.
- **Evidence anchors:**
  - [section 4] "The POMDP's hidden states marked team performance, while the visible ones indicated robot autonomy mode and alarm status."
  - [section 5.2] "Friedman's test was performed to analyze the overall rating, revealing a significant difference among the policies (ðœ’Â²(3)= 13.07, ð‘ < 0.01)."
  - [corpus] Weak - no other studies comparing low-dimensional POMDP states for HRI generalization.

## Foundational Learning

- **Concept: Partially Observable Markov Decision Processes (POMDPs)**
  - Why needed here: The human state is not directly observable; physiological and behavioral features provide noisy observations. POMDPs naturally model this partial observability and allow belief updates.
  - Quick check question: How does a POMDP differ from an MDP in terms of what the agent observes?

- **Concept: Bayesian model uncertainty and Dirichlet posteriors**
  - Why needed here: The data is limited and may not fully represent the true environment dynamics. Bayesian uncertainty quantification over observation and transition functions allows risk-sensitive planning.
  - Quick check question: Why use a Dirichlet distribution for modeling uncertainty in discrete state-action transitions?

- **Concept: Value-at-Risk (VaR) for risk-sensitive policy selection**
  - Why needed here: Standard expected return maximization can lead to high-variance policies; VaR at risk level 0.5 selects policies that perform well in the worst half of sampled models, improving robustness.
  - Quick check question: How does VaR differ from expected value in guiding policy selection under uncertainty?

## Architecture Onboarding

- **Component map:** Data preprocessing â†’ Classifiers â†’ Observation functions â†’ HMM dynamics â†’ POMDP â†’ Policy solving â†’ Risk-sensitive selection
- **Critical path:** 1. Split data into performant/non-performant windows 2. Train classifiers for each autonomy/alarm configuration 3. Build observation functions from confusion matrices 4. Learn HMM dynamics via EM â†’ POMDP 5. Solve POMDP for multiple discount factors 6. Sample models, evaluate policies, select via VaR
- **Design tradeoffs:** Low-dimensional state space vs. expressiveness (interpretable but may omit nuances); Sampling N_M models vs. computational cost (more samples improve VaR estimate but increase runtime); Fixed discount factor selection vs. continuous tuning (discrete set simplifies search but may miss optimal Î³*)
- **Failure signatures:** Poor classifier balanced accuracy (<0.6): unreliable observation function â†’ bad belief updates; High variance in sampled models: VaR selection unstable â†’ erratic policy; Low correlation between belief and score: POMDP belief not meaningful â†’ no adaptation benefit
- **First 3 experiments:** 1. Validate classifiers: compute balanced accuracy and confusion matrices on held-out data; ensure >0.6 balanced accuracy. 2. Test belief tracking: run the POMDP policy in simulation, plot belief vs. true performance (if available) to check correlation. 3. Ablation on model uncertainty: compare VaR selection vs. mean reward selection on a small set of sampled models.

## Open Questions the Paper Calls Out

- **Question:** How does the performance of the robust POMDP policy compare to other state-of-the-art offline RL methods specifically designed for partially observable environments, such as transformer-based approaches?
  - Basis in paper: [inferred] The paper mentions transformer-based methods but does not directly compare the proposed approach to them.
  - Why unresolved: The paper focuses on extending the EvC method to POMDPs and does not benchmark against transformer-based methods.
  - What evidence would resolve it: Experimental results comparing the robust POMDP policy to transformer-based methods on the same dataset and task.

- **Question:** What is the impact of different physiological feature normalization techniques on the performance of the classifiers and the overall policy?
  - Basis in paper: [explicit] The paper notes discrepancies in HRV normalization and its potential impact on classification.
  - Why unresolved: The paper uses a simple subtraction of resting baselines but does not explore alternative normalization methods.
  - What evidence would resolve it: Experiments comparing the performance of the pipeline using different normalization techniques for physiological features.

- **Question:** How does the proposed method handle inter-subject variability in physiological and behavioral features, and can it generalize to participants with significantly different baseline measurements?
  - Basis in paper: [explicit] The paper acknowledges inter-subject variability as a challenge and notes discrepancies in resting HRV baselines across subjects.
  - Why unresolved: The paper does not explicitly address strategies for handling inter-subject variability beyond the normalization approach mentioned.
  - What evidence would resolve it: Experiments evaluating the policy's performance on a diverse set of participants with varying baseline physiological and behavioral measurements.

## Limitations
- Limited validation to a single simulated environment (Firefighter Robot Game) with no real-world deployment
- Classifier performance heavily dependent on feature quality and inter-subject variability in physiological measurements
- Computational cost of Monte Carlo evaluation over model uncertainty scales with number of sampled models

## Confidence
- **High confidence**: The mechanism of using VaR for risk-sensitive selection under model uncertainty is well-established and the mathematical framework is sound.
- **Medium confidence**: The POMDP learning approach and its application to human-robot teaming is novel but lacks extensive validation beyond the controlled simulation environment.
- **Low confidence**: Generalization across different tasks, populations, and real-world environments remains largely untested.

## Next Checks
1. **Cross-population validation**: Test the classifiers on held-out participants not seen during training to measure inter-subject variability and robustness of the physiological-to-performance mapping.
2. **Real-world deployment**: Implement the method in a physical human-robot teaming task (e.g., warehouse picking or search-and-rescue simulation) to assess performance outside the controlled game environment.
3. **Ablation on state space dimensionality**: Systematically vary the number of hidden states and evaluate impact on policy performance and generalization to identify the minimal sufficient representation.