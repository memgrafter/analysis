---
ver: rpa2
title: 'Daisy-TTS: Simulating Wider Spectrum of Emotions via Prosody Embedding Decomposition'
arxiv_id: '2402.14523'
source_url: https://arxiv.org/abs/2402.14523
tags:
- emotions
- emotion
- prosody
- speech
- primary
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Daisy-TTS is an emotional text-to-speech system that learns emotionally-separable
  prosody embeddings to simulate a wider spectrum of emotions grounded on the structural
  model of emotions. The method learns prosody embeddings as emotion proxies by encoding
  non-lexical speech features through a prosody encoder equipped with an emotion discriminator.
---

# Daisy-TTS: Simulating Wider Spectrum of Emotions via Prosody Embedding Decomposition

## Quick Facts
- arXiv ID: 2402.14523
- Source URL: https://arxiv.org/abs/2402.14523
- Authors: Rendi Chevi; Alham Fikri Aji
- Reference count: 9
- Daisy-TTS achieves higher emotional speech naturalness (MOS: 3.689) and emotion perceivability (accuracy: 0.533) compared to baseline rank-based method across primary emotions.

## Executive Summary
Daisy-TTS is an emotional text-to-speech system that learns emotionally-separable prosody embeddings to simulate a wider spectrum of emotions grounded on the structural model of emotions. The method encodes non-lexical speech features (mel-spectrogram, pitch, energy) through a prosody encoder equipped with an emotion discriminator, learning prosody embeddings that act as emotion proxies. These embeddings can be decomposed and manipulated to simulate primary emotions, secondary emotions as mixtures of primaries, intensity-level through scaling, and polarity through negation. Perceptual evaluations show Daisy-TTS outperforms a baseline rank-based method in both speech naturalness and emotion perceivability across primary emotions.

## Method Summary
Daisy-TTS uses Grad-TTS as its backbone, conditioning speech generation on prosody embeddings learned through a prosody encoder with emotion discriminator. The system extracts non-lexical features (mel-spectrogram, pitch, energy) from speech, encodes them into emotion-separable embeddings, and decomposes these embeddings using PCA. Emotion characteristics are simulated by linear manipulation of the decomposed components - scaling for intensity, negation for polarity, and mixing for secondary emotions. The manipulated embeddings are then used to condition the Grad-TTS decoder, which generates mel-spectrograms converted to audio by a HifiGAN vocoder.

## Key Results
- Achieved higher emotional speech naturalness (MOS: 3.689) compared to baseline rank-based method
- Demonstrated improved emotion perceivability (accuracy: 0.533) across primary emotions
- Showed enhanced performance for secondary emotions and intensity-varying emotions

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Prosody embeddings act as emotion proxies that can be separated by primary emotion classes.
- Mechanism: The prosody encoder maps non-lexical speech features into a lower-dimensional space, while an emotion discriminator maximizes cross-entropy between these embeddings and emotion labels, enforcing separability.
- Core assumption: Prosody contains the primary cues for emotion and can be disentangled from lexical content when temporal dimensions are collapsed.
- Evidence anchors: [abstract] "The method learns prosody embeddings as emotion proxies by encoding non-lexical speech features through a prosody encoder equipped with an emotion discriminator."

### Mechanism 2
- Claim: Emotion characteristics (intensity, polarity, mixture) can be simulated by linear manipulation of prosody embeddings.
- Mechanism: PCA decomposes embeddings into principal components; sampling and scaling these components simulates intensity and polarity, while mixing components simulates secondary emotions.
- Core assumption: Emotion information in embeddings is linearly representable and follows a multivariate Gaussian distribution.
- Evidence anchors: [section 4.2] "We decompose u as a linear combination of its 'prototypes' weighted by parameter vector w..."

### Mechanism 3
- Claim: The learned embedding space is structured such that polar opposites are represented by negated vectors, and secondary emotions lie between primary emotion clusters.
- Mechanism: Cosine similarity between primary emotion embeddings reveals structural relationships; negation of joy embedding resembles sadness, and secondary emotions fall between their primary components in the embedding space.
- Core assumption: The structural model of emotions corresponds to geometric relationships in the embedding space.
- Evidence anchors: [section 4.6] "We found that by negating wε, we can simulate an emotion that is perceivably the opposite of that emotion."

## Foundational Learning

- Concept: Prosody and its role in emotion expression
  - Why needed here: The entire approach hinges on prosody as the primary emotion carrier; understanding its features (pitch, energy, rhythm) is critical to interpreting how embeddings encode emotion.
  - Quick check question: What are the three main non-lexical speech features used to encode prosody in Daisy-TTS, and why are they chosen?

- Concept: Diffusion probabilistic models and Grad-TTS architecture
  - Why needed here: Daisy-TTS uses Grad-TTS as its backbone; understanding how it generates speech from latent representations conditioned on prosody is essential for grasping the model's data flow.
  - Quick check question: How does Grad-TTS use the encoder's output µ{x,u} to guide the iterative decoding of mel-spectrograms from Gaussian noise?

- Concept: PCA and multivariate Gaussian distributions in embedding space
  - Why needed here: Emotion simulation relies on decomposing embeddings into principal components and sampling from Gaussian distributions; without this, the manipulation methods for intensity, polarity, and mixture would be unclear.
  - Quick check question: How does the covariance matrix Σ relate to the eigenvalues in the PCA decomposition used for emotion simulation?

## Architecture Onboarding

- Component map: Text → Phonemes → Grad-TTS Encoder (with FiLM conditioning on prosody embeddings) → Grad-TTS Decoder (diffusion) → Mel-spectrogram → HifiGAN Vocoder → Speech output

- Critical path: Text → Phonemes → Grad-TTS Encoder (with FiLM conditioning on prosody embeddings) → Grad-TTS Decoder (diffusion) → Mel-spectrogram → HifiGAN Vocoder → Speech output

- Design tradeoffs:
  - Using non-lexical features allows focus on prosody but may lose some lexical emotion cues
  - Joint training of prosody encoder and TTS backbone ensures alignment but increases complexity
  - Linear decomposition assumes Gaussian structure, which may not capture all emotion nuances

- Failure signatures:
  - Embeddings not separable by emotion → likely emotion discriminator not effective
  - Poor speech naturalness → issues in Grad-TTS conditioning or vocoder quality
  - Simulated emotions not perceivable → embedding decomposition/sampling not capturing emotion information

- First 3 experiments:
  1. Train Daisy-TTS without emotion discriminator; visualize embedding space with t-SNE to confirm lack of emotion separability.
  2. Generate speech with varying intensity (α scaling) for a single emotion; evaluate naturalness and perceivability.
  3. Mix two primary emotion embeddings to simulate a secondary emotion; compare to ground truth secondary emotion samples if available.

## Open Questions the Paper Calls Out
- How well does Daisy-TTS perform on emotional speech synthesis in languages other than English?
- How does Daisy-TTS handle speech samples that do not conform to the primary emotions defined in the structural model of emotions?
- How does the performance of Daisy-TTS change when using different backbone TTS models?

## Limitations
- The method relies on three critical assumptions that lack direct empirical validation: prosody alone contains sufficient emotion information, embeddings follow Gaussian distribution, and geometric relationships correspond to structural model
- Emotion discriminator's architecture and training details are underspecified, making it difficult to assess the source of separability
- The paper provides qualitative evidence for separability and structural relationships but lacks ablation studies quantifying the importance of prosody versus lexical cues

## Confidence
- **High confidence**: Claims about Daisy-TTS achieving higher MOS (3.689) and emotion perceivability (accuracy 0.533) compared to baseline, as these are directly measured and reported with statistical comparisons.
- **Medium confidence**: Claims about the effectiveness of prosody embedding decomposition for simulating intensity, polarity, and secondary emotions, as these are supported by perceptual evaluations but rely on the unverified Gaussian assumption.
- **Low confidence**: Claims about the structural relationships in embedding space (e.g., negation producing polar opposites, secondary emotions as linear mixtures) due to lack of rigorous geometric analysis and validation against the structural model.

## Next Checks
1. **Ablation study on emotion discriminability**: Train a version of Daisy-TTS without the emotion discriminator and visualize the prosody embedding space using t-SNE. Quantify the effect of the discriminator on emotion separability by measuring classification accuracy on held-out prosody embeddings and comparing intra- and inter-emotion embedding distances.

2. **Validation of Gaussian assumption**: Analyze the distribution of prosody embeddings in the PCA space by computing kurtosis, skewness, and conducting normality tests (e.g., Shapiro-Wilk) for each principal component. Generate synthetic embeddings from the fitted Gaussian and evaluate their emotional naturalness and perceivability to confirm that linear manipulation produces meaningful emotion variations.

3. **Geometric relationship validation**: For each primary emotion, compute the cosine similarity between its embedding and the negation of other primary emotion embeddings. Measure the perceivability of synthesized emotions from these negated vectors and compare to ground truth polar opposites. Additionally, simulate secondary emotions as convex combinations of primary emotion embeddings and evaluate their similarity to actual secondary emotion samples using both perceptual tests and embedding space distances.