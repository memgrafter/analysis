---
ver: rpa2
title: Grounded learning for compositional vector semantics
arxiv_id: '2401.06808'
source_url: https://arxiv.org/abs/2401.06808
tags:
- compositional
- distributional
- fluffy
- vector
- have
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a way to implement compositional distributional
  semantics within a biologically plausible spiking neural network architecture, specifically
  the Nengo framework. The core idea is to map the tensor-based composition methods
  of compositional distributional semantics onto the binding and unbinding operations
  of vector symbolic architectures.
---

# Grounded learning for compositional vector semantics

## Quick Facts
- arXiv ID: 2401.06808
- Source URL: https://arxiv.org/abs/2401.06808
- Authors: Martha Lewis
- Reference count: 20
- This paper proposes implementing compositional distributional semantics within a spiking neural network architecture using Nengo, mapping tensor operations to vector symbolic architecture binding and unbinding.

## Executive Summary
This paper presents a novel approach to implementing compositional distributional semantics within spiking neural networks, specifically using the Nengo framework. The core idea is to map tensor-based composition methods to the binding and unbinding operations of vector symbolic architectures. The author proposes a strategy for learning word representations from labelled images by updating semantic pointers based on input image representations. The work bridges theoretical linguistics and neuroscience by creating a biologically plausible implementation of compositional semantics.

## Method Summary
The paper proposes mapping tensor-based compositional distributional semantics to the semantic pointer architecture used in Nengo. This involves treating tensor product as circular convolution (binding) and inner product as circular correlation (unbinding). The approach leverages the fact that in compositional distributional semantics, words are represented as vectors or tensors depending on their grammatical type, with composition achieved through tensor contraction. The author also proposes a learning strategy where word representations are updated based on labelled image inputs, using semantic pointers to represent both words and visual concepts.

## Key Results
- Conceptual framework established for mapping tensor-based compositional semantics to spiking neural networks
- Proposed learning strategy for updating word representations from labelled images
- Demonstration of the "pet fish" problem using the proposed semantic pointer approach
- Connection established between formal semantics and biologically plausible neural implementations

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Tensor-based compositional distributional semantics can be implemented in spiking neural networks by mapping tensor contraction to circular convolution and circular correlation.
- Mechanism: Words are represented as vectors or tensors in different spaces depending on their grammatical type. Composition is achieved through tensor contraction, which can be mapped to circular convolution (binding) and circular correlation (unbinding) in Nengo's semantic pointer architecture.
- Core assumption: Semantic pointers in Nengo are approximately orthogonal.
- Evidence anchors: [abstract] "This work proposes a way for compositional distributional semantics to be implemented within a spiking neural network architecture"; [section 3.1] "We therefore have an immediate way of mapping to the semantic pointer architecture needed for implementation in Nengo, by viewing inner product as an unbinding operator and tensor product as a binding operator."
- Break condition: If semantic pointers are not approximately orthogonal, the mapping from tensor contraction to circular correlation may not hold.

### Mechanism 2
- Claim: Additive binding in vector symbolic architectures can better model human concept combination than conjunctive binding.
- Mechanism: Additive binding (vector addition) increases similarity of concepts that share a common predicate, making fluffy dogs and fluffy cats more similar than dogs and cats.
- Core assumption: Human similarity judgments align with cosine similarity measure used in vector spaces.
- Evidence anchors: [section 2.2] "Martin and Doumas [2020] set up an experiment to investigate whether conjunctive binding (via tensor product or circular convolution) or additive binding (via vector addition) is a better predictor of human similarity judgements."
- Break condition: If human similarity judgments do not align with cosine similarity, or if certain predicates require more nuanced combination than additive binding provides.

### Mechanism 3
- Claim: Tensor-based compositional distributional models can reflect polysemy better than additive models.
- Mechanism: Tensor-based models represent ambiguous words as different matrices or tensors, allowing for more flexible composition than additive models.
- Core assumption: Polysemy is an important aspect of language that needs to be captured in semantic models.
- Evidence anchors: [section 2.2] "Boleda [2020] argues that tensor-based compositional distributional models of meaning can reflect polysemy"; [section 2.2] "A hand-crafted example of this is given in Grefenstette et al. [2010] where they show how to design a representation for catch so that the similarity of the phrases catch ball and catch disease is not boosted by the word catch."
- Break condition: If polysemy is not a significant factor in the language being modeled, or if the tensor-based approach does not significantly outperform additive models in handling polysemy.

## Foundational Learning

- Concept: Vector symbolic architectures (VSAs)
  - Why needed here: VSAs provide a framework for representing and manipulating symbols as vectors, which is crucial for implementing compositional distributional semantics in spiking neural networks.
  - Quick check question: Can you explain how VSAs use binding and unbinding operations to combine and separate symbols?

- Concept: Tensor-based compositional distributional semantics
  - Why needed here: This approach combines the strengths of vector-based models of meaning with the compositional power of formal semantics, providing a basis for the proposed implementation in spiking neural networks.
  - Quick check question: How do tensor-based models represent different grammatical types (e.g., nouns, adjectives, verbs) and their composition?

- Concept: Spiking neural networks and the Nengo framework
  - Why needed here: These provide the biologically plausible substrate for implementing the proposed semantic model, allowing for integration with other cognitive modules like vision and decision-making.
  - Quick check question: What are the key features of spiking neural networks that make them suitable for implementing cognitive models?

## Architecture Onboarding

- Component map: Input module -> Semantic pointer module -> Composition module -> Learning module -> Output module

- Critical path:
  1. Input image is processed and represented as a semantic pointer
  2. Semantic pointers for words are retrieved or initialized
  3. Composition operation (binding/unbinding) is performed
  4. Result is compared to existing representations for similarity
  5. Learning update is applied if necessary

- Design tradeoffs:
  - Tradeoff between biological plausibility and computational efficiency
  - Balance between flexibility of tensor-based models and simplicity of additive models
  - Choice between supervised and self-supervised learning strategies

- Failure signatures:
  - Poor performance on compositional tasks
  - Inability to capture polysemy or other linguistic phenomena
  - Incompatibility with other cognitive modules in the Nengo framework

- First 3 experiments:
  1. Implement the "pet fish" problem in Nengo and verify that the model correctly captures the composition of "pet" and "fish" to produce a representation similar to "goldfish".
  2. Test the learning strategy on a small dataset of labelled images, verifying that word representations are updated correctly based on the input images.
  3. Compare the performance of the tensor-based model with an additive model on a compositional task, evaluating their ability to capture polysemy and other linguistic phenomena.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How effective are tensor-based compositional distributional semantics for modeling human behavior compared to other compositional approaches?
- Basis in paper: [explicit] The paper mentions that future work includes examining the effectiveness of compositional distributional semantics for modeling human behavior.
- Why unresolved: The paper does not provide empirical results comparing tensor-based compositional distributional semantics with other compositional approaches in modeling human behavior.
- What evidence would resolve it: Experimental studies comparing the performance of tensor-based compositional distributional semantics with other compositional approaches in modeling human behavior.

### Open Question 2
- Question: Can compositional distributional semantics be effectively implemented within a dialogue setting for self-supervised learning between multiple agents?
- Basis in paper: [explicit] The paper mentions that the proposed representations are amenable to self-supervised learning between two or more agents and that future work includes implementing these ideas within a dialogue setting.
- Why unresolved: The paper does not provide a concrete implementation or experimental results demonstrating the effectiveness of compositional distributional semantics in a dialogue setting for self-supervised learning.
- What evidence would resolve it: Successful implementation and experimental evaluation of compositional distributional semantics in a dialogue setting for self-supervised learning between multiple agents.

### Open Question 3
- Question: What is the optimal binding and unbinding mechanism for compositional distributional semantics in a spiking neural network architecture?
- Basis in paper: [inferred] The paper proposes mapping tensor product and inner product from compositional distributional semantics to circular convolution and correlation in a spiking neural network architecture, but does not provide empirical evidence on the effectiveness of this mapping.
- Why unresolved: The paper does not provide empirical results comparing different binding and unbinding mechanisms for compositional distributional semantics in a spiking neural network architecture.
- What evidence would resolve it: Experimental studies comparing the performance of different binding and unbinding mechanisms for compositional distributional semantics in a spiking neural network architecture.

## Limitations

- The mapping from tensor operations to circular convolution and correlation relies on the assumption of approximately orthogonal semantic pointers, which has not been empirically validated.
- Claims about additive binding better modeling human similarity judgments are based primarily on citations to external work rather than direct experimental evidence.
- The proposed learning strategy for updating word representations from labelled images remains theoretical without implementation or evaluation.
- Limited empirical support is provided for claims about tensor-based models handling polysemy better than additive models.

## Confidence

- High confidence: The conceptual framework connecting compositional distributional semantics to vector symbolic architectures is theoretically sound
- Medium confidence: The proposed mapping to Nengo's circular convolution and correlation operations is plausible but unverified
- Low confidence: Claims about human similarity judgments and polysemy handling are supported primarily by citations rather than direct evidence

## Next Checks

1. Implement the circular convolution-based binding operation in Nengo and empirically test whether semantic pointers maintain approximate orthogonality during composition operations

2. Conduct human similarity judgment experiments comparing additive versus conjunctive binding models to verify the claims about which better matches human cognition

3. Build and evaluate a small-scale implementation of the proposed learning strategy on a labelled image dataset to verify that word representations update appropriately based on visual input