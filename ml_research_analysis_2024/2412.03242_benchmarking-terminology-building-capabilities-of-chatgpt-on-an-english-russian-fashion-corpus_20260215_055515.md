---
ver: rpa2
title: Benchmarking terminology building capabilities of ChatGPT on an English-Russian
  Fashion Corpus
arxiv_id: '2412.03242'
source_url: https://arxiv.org/abs/2412.03242
tags:
- terms
- chatgpt
- corpus
- fashion
- terminology
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper benchmarks terminology extraction using TBXTools, SketchEngine,
  and ChatGPT on a comparable English-Russian fashion corpus. TBXTools and SketchEngine
  achieve high recall but suffer from low precision due to excessive noise and irrelevant
  terms.
---

# Benchmarking terminology building capabilities of ChatGPT on an English-Russian Fashion Corpus

## Quick Facts
- arXiv ID: 2412.03242
- Source URL: https://arxiv.org/abs/2412.03242
- Reference count: 2
- Primary result: ChatGPT outperforms statistical tools in precision for terminology extraction but produces shorter term lists

## Executive Summary
This paper benchmarks terminology extraction capabilities of TBXTools, SketchEngine, and ChatGPT on a comparable English-Russian fashion corpus. While TBXTools and SketchEngine achieve high recall, they suffer from low precision due to excessive noise and irrelevant terms. ChatGPT demonstrates superior precision and maintains accuracy as more terms are considered, but produces shorter term lists. The study reveals fundamental trade-offs between precision and recall across tools, with ChatGPT showing promise for high-quality, context-aware terminology extraction despite missing some relevant terms.

## Method Summary
The study compiled a comparable English-Russian fashion corpus (1.8M words total) from magazines and websites (2021-2024), then cleaned it using ChatGPT to remove noise. A gold standard of 354 fashion terms with definitions was created by harvesting bilingual glossaries from fashion websites. Terms were extracted using TBXTools, SketchEngine, and ChatGPT, then evaluated against the gold standard using precision, recall, and F-measure. Definition quality was assessed using Levenshtein distance to measure similarity between ChatGPT-generated definitions and reference definitions.

## Key Results
- ChatGPT achieves significantly higher precision and F-measure than TBXTools and SketchEngine
- TBXTools and SketchEngine extract 15,000+ English and 7,800+ Russian terms but include excessive noise
- ChatGPT definitions show generally accurate core concepts but occasional deviations and unnecessary elaboration
- No single tool excels universally across all evaluation metrics

## Why This Works (Mechanism)

### Mechanism 1
- Claim: ChatGPT's superior precision stems from its ability to generate contextually relevant terms based on the input prompt rather than relying solely on frequency-based extraction.
- Mechanism: Unlike statistical methods that rank terms purely by frequency, ChatGPT interprets domain context and generates semantically relevant terms, filtering out non-relevant high-frequency words.
- Core assumption: The input prompt effectively constrains ChatGPT's output to domain-relevant terms.
- Evidence anchors:
  - [abstract] "ChatGPT demonstrates superior performance, maintaining or improving precision as more terms are considered"
  - [section 3] "ChatGPT is not specifically designed for terminology extraction, but it can extract terms if prompted correctly"
  - [section 4.1] "ChatGPT obtains a significantly higher precision and f-measure scores, but the recall is the lowest"

### Mechanism 2
- Claim: The trade-off between precision and recall across tools reflects fundamental differences in their extraction methodologies.
- Mechanism: Statistical tools prioritize recall by extracting high-frequency terms regardless of semantic relevance, while ChatGPT prioritizes precision by generating contextually appropriate terms even if fewer are produced.
- Core assumption: High-frequency words in specialized domains often include general language that isn't domain-specific terminology.
- Evidence anchors:
  - [abstract] "TBXTools and SketchEngine, while capable of high recall, suffer from reduced precision as the number of terms increases"
  - [section 4.1] "TBXTools extracted approximately 15,000 terms in English and 7,800 terms in Russian" vs "ChatGPT produces significantly shorter term lists"
  - [section 4.3] "TBXTools favours extraction of words related to the fashion industry, but which are too general to be included in our gold standard"

### Mechanism 3
- Claim: ChatGPT's ability to generate definitions is effective because it can synthesize information from its training data rather than extracting pre-existing definitions.
- Mechanism: When prompted to define terms, ChatGPT draws on its general knowledge to produce definitions that may be more comprehensive than simple extraction from glossaries, though this can lead to elaboration beyond the reference.
- Core assumption: ChatGPT's training data includes sufficient fashion-related content to generate accurate definitions.
- Evidence anchors:
  - [abstract] "ChatGPT maintains a reasonable level of accuracy and fidelity across languages, but sometimes the definitions in both languages miss crucial specifics and include unnecessary deviations"
  - [section 4.4] "Significant variations in Levenshtein distances were observed, with values ranging from as low as 1 to as high as 221"
  - [section 4.4] "ChatGPT often elaborates on the reference... This adds context and details which are not present in the reference leading to high Levenshtein distance"

## Foundational Learning

- Concept: Precision vs. Recall trade-off in information retrieval
  - Why needed here: Understanding why different tools perform differently requires grasping this fundamental metric relationship
  - Quick check question: If a tool extracts 100 terms and 80 are relevant, what is its precision? If the gold standard contains 200 relevant terms and the tool found 80 of them, what is its recall?

- Concept: Levenshtein distance as a similarity metric
  - Why needed here: The paper uses this to evaluate definition quality by measuring word-level differences
  - Quick check question: If the reference definition is "A long coat" and ChatGPT generates "A long warm coat worn over other clothing in cold weather", what would be the approximate Levenshtein distance?

- Concept: Comparable corpora vs. parallel corpora
  - Why needed here: The paper uses a comparable corpus (similar content in two languages) rather than a parallel corpus (direct translations)
  - Quick check question: What is the key difference between a comparable corpus and a parallel corpus in terms of structure and content?

## Architecture Onboarding

- Component map:
  - Data collection -> Corpus cleaning -> Gold standard creation -> Tool integration -> Evaluation

- Critical path:
  1. Corpus compilation and cleaning
  2. Gold standard development
  3. Term extraction using all three tools
  4. Evaluation of extraction quality
  5. Definition extraction and evaluation

- Design tradeoffs:
  - Manual cleaning vs. automated cleaning: Manual cleaning with ChatGPT is slower but produces higher quality results than automated tools
  - Recall vs. precision: Statistical tools capture more terms but include noise; ChatGPT is more precise but may miss terms
  - Resource allocation: ChatGPT requires iterative prompting which is time-consuming but produces better contextual results

- Failure signatures:
  - Low precision scores indicate excessive noise in extracted terms
  - High Levenshtein distances (>50) suggest ChatGPT definitions deviate significantly from references
  - Unexpected language switching in output (Russian prompts producing English text) indicates prompt formulation issues

- First 3 experiments:
  1. Compare term extraction on a small sample corpus using all three tools to validate the methodology
  2. Test different prompt formulations with ChatGPT to optimize term extraction quality
  3. Evaluate definition extraction on a subset of terms to calibrate Levenshtein distance thresholds for acceptable quality

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of ChatGPT for terminology extraction compare to specialized terminology extraction tools when evaluated using automated metrics that account for synonym variations and additional contextual information?
- Basis in paper: [inferred] The paper notes that automatic evaluations using Levenshtein distance may yield lower results when ChatGPT uses synonyms or includes specific details not present in the reference, even when definitions are grammatically and factually correct.
- Why unresolved: The paper primarily uses Levenshtein distance at the word level, which doesn't account for semantic equivalence through synonyms or the potential value of additional contextual information in definitions.
- What evidence would resolve it: Comparative evaluation using metrics that capture semantic similarity (e.g., BERTScore, BLEURT) alongside traditional word-level metrics, and assessment of whether additional context in ChatGPT definitions improves practical utility for terminologists.

### Open Question 2
- Question: What specific prompting strategies maximize ChatGPT's precision and recall balance for terminology extraction in specialized domains?
- Basis in paper: [explicit] The paper states that ChatGPT requires iterative prompting to produce longer lists of terms, and that it can deviate from the specific corpus to extract more general domain-specific terminology.
- Why unresolved: While the paper experimented with prompts, it didn't systematically investigate how different prompt formulations, parameters, or prompting strategies affect the trade-off between precision and recall.
- What evidence would resolve it: Systematic experimentation with different prompt templates, temperature settings, and prompting strategies (e.g., few-shot prompting, chain-of-thought prompting) to determine optimal configurations for specialized domain terminology extraction.

### Open Question 3
- Question: How does the noise level in automatically cleaned corpora affect the quality of terminology extraction across different tools and languages?
- Basis in paper: [explicit] The paper details extensive challenges with corpus cleaning, using ChatGPT to clean noisy text from PDF and HTML conversions, and notes that SketchEngine in particular produced errors where English terms appeared in Russian text and single words were incorrectly split.
- Why unresolved: The paper cleaned the corpus before terminology extraction but didn't investigate how residual noise or different cleaning approaches might impact extraction quality, particularly for morphologically rich languages like Russian.
- What evidence would resolve it: Controlled experiments comparing terminology extraction quality on corpora with varying levels of noise and different cleaning approaches, analyzing how noise specifically impacts tool performance across languages with different morphological complexities.

## Limitations

- The iterative prompting approach for ChatGPT is labor-intensive and may not be easily scalable to larger corpora or different domains
- The gold standard created from web harvesting may contain inconsistencies or missing terms without validation procedures
- Corpus cleaning relies heavily on ChatGPT's judgment for what constitutes "noise," introducing potential bias
- The study focuses exclusively on the fashion domain, limiting generalizability to other specialized domains

## Confidence

**High confidence** in the comparative analysis between tools: The precision-recall trade-offs observed between TBXTools/SketchEngine and ChatGPT are consistent with their fundamental operational mechanisms (frequency-based vs. context-aware extraction). The quantitative metrics (precision, recall, F-measure) are standard evaluation measures with clear interpretation.

**Medium confidence** in ChatGPT's performance superiority: While ChatGPT shows better precision, this advantage depends heavily on prompt quality and may not transfer to all domains or less carefully formulated prompts. The labor-intensive nature of iterative prompting also raises questions about practical applicability.

**Medium confidence** in definition quality assessment: Levenshtein distance provides a quantitative measure, but the interpretation that deviations represent "unnecessary elaboration" versus "valuable context" is subjective. The study acknowledges this ambiguity but doesn't fully resolve how to evaluate such differences.

## Next Checks

1. **Cross-domain validation**: Test the same three-tool methodology on a different specialized corpus (e.g., medical or legal domain) to assess whether ChatGPT maintains its precision advantage across domains with different terminology density and specificity.

2. **Prompt optimization study**: Systematically vary prompt formulations for ChatGPT to identify the minimal prompt requirements that maintain high precision, addressing the scalability concerns of the iterative approach.

3. **Human evaluation of definitions**: Conduct expert review of ChatGPT-generated definitions to validate whether high Levenshtein distances correspond to genuinely inferior definitions or simply more elaborate but equally valid explanations.