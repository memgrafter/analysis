---
ver: rpa2
title: 'MM-Ego: Towards Building Egocentric Multimodal LLMs for Video QA'
arxiv_id: '2410.07177'
source_url: https://arxiv.org/abs/2410.07177
tags:
- egocentric
- video
- visual
- understanding
- videos
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work introduces MM-Ego, a multimodal large language model
  designed for egocentric video understanding. It addresses the challenge of comprehending
  long egocentric videos (30 seconds to 1 hour) by introducing a novel "Memory Pointer
  Prompting" mechanism that combines a global overview step with targeted detail retrieval.
---

# MM-Ego: Towards Building Egocentric Multimodal LLMs for Video QA

## Quick Facts
- **arXiv ID**: 2410.07177
- **Source URL**: https://arxiv.org/abs/2410.07177
- **Reference count**: 18
- **Primary result**: Achieves state-of-the-art performance on egocentric understanding benchmarks with a novel Memory Pointer Prompting mechanism

## Executive Summary
This work introduces MM-Ego, a multimodal large language model designed for egocentric video understanding. The research addresses the challenge of comprehending long egocentric videos (30 seconds to 1 hour) by introducing a novel "Memory Pointer Prompting" mechanism that combines a global overview step with targeted detail retrieval. The proposed approach achieves state-of-the-art performance on egocentric understanding benchmarks while maintaining competitive performance on general video understanding tasks.

## Method Summary
MM-Ego employs a two-step Memory Pointer Prompting mechanism: first, it extracts compressed frame-level visual embeddings and uses a learnable memory pointer embedding to identify key visual information through a global glimpse; second, it processes only the selected high-resolution visual embeddings corresponding to the key frames for final answer generation. The model is trained on a large-scale egocentric QA dataset (7M samples) generated from human-annotated video narrations using GPT-4o, along with other video understanding datasets. The training involves joint image-video supervised fine-tuning with 1 epoch, base learning rate of 1×10^-5, and batch size of 128.

## Key Results
- Achieves state-of-the-art performance on egocentric benchmarks (EgoMemoria MDA: 32.9%, EgoSchema: 49.2%)
- Outperforms previous methods on Video-MME benchmark while maintaining general video understanding capabilities
- Memory Pointer Prompting mechanism enables efficient processing of long videos by identifying key frames relevant to specific questions

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Memory Pointer Prompting enables efficient processing of long egocentric videos by first gaining a global understanding and then selectively focusing on relevant frames.
- **Mechanism**: The two-step approach compresses all frame embeddings for an initial global glimpse, uses a memory pointer embedding to identify key frames based on the question context, then processes only those selected frames in high resolution for the final answer generation.
- **Core assumption**: Humans cannot remember every detail of long videos and instead use progressive understanding - first getting an overview then focusing on specific details.
- **Evidence anchors**:
  - [abstract] "This design includes a global glimpse step to gain an overarching understanding of the entire video and identify key visual information, followed by a fallback step that utilizes the key visual information to generate responses."
  - [section 2.2.2] "Memory Pointer Prompting consists of two steps during inference: global glimpse and fallback."
  - [corpus] Weak - no direct corpus evidence supporting this specific mechanism

### Mechanism 2
- **Claim**: Generating large-scale egocentric QA data from human narrations enables training of models for egocentric video understanding.
- **Mechanism**: Uses existing human-annotated video narrations as input to a language model (GPT-4o) to generate egocentric QA pairs, avoiding the chicken-or-egg problem of needing a strong egocentric VLM to create training data.
- **Core assumption**: Human narrations contain sufficient information to generate high-quality egocentric QA pairs without requiring visual input.
- **Evidence anchors**:
  - [section 2.1] "we develop an innovative 'narration to egocentric QA' data engine that automatically generates episodic memory-related QA samples based on human-annotated video clip narrations"
  - [section E] "The motivation for using a language model to convert egocentric video captions into egocentric QA conversations is to address the 'chicken-or-egg dilemma'."
  - [corpus] Weak - no direct corpus evidence supporting this specific data generation approach

### Mechanism 3
- **Claim**: Debiasing evaluation methods provide more accurate assessment of egocentric understanding capabilities.
- **Mechanism**: Identifies questions that can be answered without visual input (language-biased questions), removes them from evaluation, and calculates mean debiased accuracy (MDA) to measure true understanding.
- **Core assumption**: Language biases in questions artificially inflate model performance metrics and need to be controlled for accurate evaluation.
- **Evidence anchors**:
  - [abstract] "We introduce a new de-biasing evaluation method to help mitigate the unavoidable language bias present in the models being evaluated."
  - [section 3.3] "we conduct additional experiments aimed at eliminating these language biases... We calculate the mean accuracy of the debiased variants, referred to as the 'Mean Debiased Accuracy (MDA)'."
  - [corpus] Weak - no direct corpus evidence supporting this specific debiasing methodology

## Foundational Learning

- **Concept**: Understanding of multimodal large language models (MLLMs) and their architecture
  - Why needed here: The MM-Ego model builds upon MLLM foundations and requires understanding of how visual and textual embeddings are processed
  - Quick check question: What are the key components of a typical MLLM architecture and how do they process visual vs. textual information?

- **Concept**: Knowledge of egocentric vision and first-person video understanding
  - Why needed here: The research focuses specifically on egocentric videos which have different characteristics from third-person videos
  - Quick check question: How do egocentric videos differ from static-camera videos in terms of content, viewpoint, and data distribution?

- **Concept**: Understanding of memory mechanisms and attention in neural networks
  - Why needed here: The Memory Pointer Prompting mechanism relies on attention mechanisms to identify relevant frames
  - Quick check question: How do pointer networks and attention mechanisms work in the context of selecting relevant information from long sequences?

## Architecture Onboarding

- **Component map**: Visual Encoder (SigLIP-so400m) → MLP Layer → Average Pooling → Memory Pointer Embedding → LLM (Qwen2) → Tokenizer

- **Critical path**: Input video → Frame sampling → Visual feature extraction → Embedding compression → Global glimpse with memory pointer → Key frame selection → Fallback with high-resolution embeddings → Answer generation

- **Design tradeoffs**:
  - Processing all frames vs. computational efficiency (resolved by Memory Pointer Prompting)
  - Using human narrations vs. direct video understanding for data generation
  - Debiased evaluation vs. traditional evaluation metrics

- **Failure signatures**:
  - Poor performance on longer videos (suggests Memory Pointer Prompting not effectively identifying key frames)
  - High accuracy on debiased evaluation (suggests language bias not being properly controlled)
  - Training instability (suggests data quality issues or hyperparameter