---
ver: rpa2
title: 'LCE: A Framework for Explainability of DNNs for Ultrasound Image Based on
  Concept Discovery'
arxiv_id: '2408.09899'
source_url: https://arxiv.org/abs/2408.09899
tags:
- image
- methods
- medical
- concept
- images
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces the Lesion Concept Explainer (LCE) framework
  to explain Deep Neural Network (DNN) decisions for ultrasound images by integrating
  attribution methods with concept-based methods. The key innovation is using a Segment
  Anything Model (SAM), fine-tuned on medical images, to discover lesion concepts
  guided by attribution methods like GradCAM and LIME.
---

# LCE: A Framework for Explainability of DNNs for Ultrasound Image Based on Concept Discovery

## Quick Facts
- arXiv ID: 2408.09899
- Source URL: https://arxiv.org/abs/2408.09899
- Authors: Weiji Kong; Xun Gong; Juan Wang
- Reference count: 40
- Primary result: LCE achieves higher faithfulness scores (Effect Score 0.5572 on BUSI, 0.4947 on FG-US-B) compared to other explainability methods for breast ultrasound images

## Executive Summary
This paper introduces the Lesion Concept Explainer (LCE) framework to explain Deep Neural Network decisions for breast ultrasound images by integrating attribution methods with concept-based methods. The key innovation uses a Segment Anything Model (SAM), fine-tuned on medical images, to discover lesion concepts guided by attribution methods like GradCAM and LIME. The discovered concepts are then evaluated using Shapley values to identify the most crucial explanations for model decisions. Experiments on public (BUSI) and private (FG-US-B) breast ultrasound datasets show that LCE achieves higher faithfulness scores compared to other popular explainability methods, with professional sonographers finding LCE explanations more understandable.

## Method Summary
LCE combines attribution methods (GradCAM and LIME) with concept-based discovery using SAMMed2D to generate explanations for DNN decisions on ultrasound images. The framework first generates attribution maps to guide the SAM in discovering concept masks that represent potential lesions. These concepts are then refined to remove overlaps and evaluated using Shapley values to determine their importance to the model's decision. The top concepts are selected as the final explanation. The framework is model-agnostic and designed to provide faithful and understandable explanations for fine-grained diagnostic tasks in breast ultrasound.

## Key Results
- LCE achieves higher faithfulness scores (Effect Score 0.5572 on BUSI, 0.4947 on FG-US-B) compared to GradCAM and LIME alone
- Professional sonographers found LCE explanations more understandable than baseline methods
- The framework is model-agnostic and provides reliable explanations for fine-grained diagnostic tasks in breast ultrasound
- Effect Score metric provides more accurate evaluation by accounting for explanation size compared to traditional Insertion/Deletion metrics

## Why This Works (Mechanism)

### Mechanism 1
Attribution methods guide SAM to discover lesion concepts that are then evaluated with Shapley values, ensuring discovered concepts are both meaningful and crucial for model decisions. Core assumption: guided concept discovery using attribution methods can effectively identify medically meaningful lesion concepts. Break condition: if attribution methods fail to provide meaningful guidance, discovered concepts may not correlate with model decisions.

### Mechanism 2
Effect Score metric provides more accurate faithfulness evaluation by weighting Insertion/Deletion scores with explanation size ratio. Core assumption: explanation size is crucial for determining explanation quality. Break condition: if explanation size does not correlate with quality, this metric may incorrectly penalize useful but large explanations.

### Mechanism 3
LCE generates understandable explanations by discovering concepts consistent with real-world lesions and using Shapley values to identify crucial explanations. Core assumption: sonographers can accurately assess AI-generated explanation understandability. Break condition: if sonographers' preferences do not align with diagnostic utility, understandability evaluation may not reflect true effectiveness.

## Foundational Learning

- **Segment Anything Model (SAM)**: Provides zero-shot segmentation capability that can be fine-tuned on medical images to discover concepts without additional annotations. Quick check: What is the primary advantage of using SAM for concept discovery in medical images compared to traditional segmentation methods?

- **Shapley values**: Provide a fair way to attribute contribution of each discovered concept to model's decision, ensuring most crucial explanations are identified. Quick check: How do Shapley values help in determining which discovered concepts are most important for the model's decision-making process?

- **Attribution methods (GradCAM, LIME)**: Guide SAM in discovering concepts by highlighting important regions that correlate with model's decision. Quick check: What is the difference between GradCAM and LIME in terms of the type of guidance they provide for concept discovery?

## Architecture Onboarding

- **Component map**: Input ultrasound image -> Pre-trained DNN -> Concept discoverer (SAMMed2D) -> Concept guides (GradCAM, LIME) -> Post-processing -> Shapley value calculation -> Output explanation masks

- **Critical path**: 1) Generate attribution maps using GradCAM and LIME, 2) Use maps to guide SAMMed2D in discovering concept masks, 3) Post-process concepts to remove overlaps, 4) Calculate Shapley values for each concept, 5) Select top concepts based on Shapley values as final explanation

- **Design tradeoffs**: Using SAMMed2D provides better medical image segmentation but requires more computational resources; Effect Score metric provides more accurate evaluation but adds complexity; model-agnostic design allows flexibility but may miss model-specific optimizations

- **Failure signatures**: If discovered concepts don't align with known lesion patterns, attribution methods may not provide effective guidance; if Effect Score is significantly lower than Insertion/Deletion scores, explanation size may be disproportionately large; if sonographers don't find explanations understandable, concept discovery may not capture medically relevant features

- **First 3 experiments**: 1) Compare faithfulness scores of LCE with GradCAM and LIME alone on BUSI subset, 2) Qualitative assessment of discovered concepts by medical professional, 3) Evaluate computational efficiency of LCE compared to baseline methods

## Open Questions the Paper Calls Out

- **Open Question 1**: How does LCE perform on ultrasound image modalities beyond breast ultrasound, such as abdominal or cardiac ultrasound? The framework was only tested on breast ultrasound images, and its generalizability to other ultrasound modalities is not demonstrated.

- **Open Question 2**: Can LCE be adapted to provide explanations for 3D ultrasound images or volumetric medical imaging data? The current framework is designed for 2D image segmentation and explanation, and its extension to 3D data would require significant modifications.

- **Open Question 3**: How does the choice of attribution method (GradCAM vs. LIME) affect the quality of explanations in LCE, and can a dynamic selection strategy be developed? The framework uses both methods but does not investigate whether one is superior or if context-aware selection could improve results.

- **Open Question 4**: What is the computational overhead of LCE compared to traditional attribution methods, and is it feasible for real-time clinical applications? The framework introduces additional steps that may increase inference time, but this is not quantified.

## Limitations
- Framework performance heavily depends on quality of SAMMed2D fine-tuning, with insufficient details provided on this critical step
- Human evaluation by sonographers involved small sample size (five participants) and lacked detailed assessment criteria
- Effect Score metric correlation with actual diagnostic utility remains unclear despite addressing size-related biases

## Confidence
- **High Confidence**: Model-agnostic design and higher faithfulness scores compared to baseline methods
- **Medium Confidence**: Understandability assessment by professional sonographers, limited by sample size
- **Low Confidence**: Assumption that Effect Score metric provides more accurate faithfulness evaluation

## Next Checks
1. Conduct larger-scale human evaluation with at least 20 sonographers using standardized rubrics to assess explanation understandability and diagnostic utility
2. Perform ablation studies to quantify individual contributions of GradCAM and LIME guidance to concept discovery and overall framework performance
3. Test LCE's generalizability by applying it to other medical imaging modalities (e.g., MRI, CT) and comparing performance with modality-specific explainability methods