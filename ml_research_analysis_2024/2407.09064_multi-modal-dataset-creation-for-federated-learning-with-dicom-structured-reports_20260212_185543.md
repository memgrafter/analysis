---
ver: rpa2
title: Multi-Modal Dataset Creation for Federated Learning with DICOM Structured Reports
arxiv_id: '2407.09064'
source_url: https://arxiv.org/abs/2407.09064
tags:
- data
- dicom
- https
- learning
- reports
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work presents a federated learning framework for predicting
  pacemaker dependency after transcatheter aortic valve implantation (TAVI) using
  multi-modal data. The authors developed a DICOM-based structured reporting system
  that integrates heterogeneous clinical data (CT, ECG, segmentations, metadata) from
  eight German university hospitals into a unified platform.
---

# Multi-Modal Dataset Creation for Federated Learning with DICOM Structured Reports

## Quick Facts
- arXiv ID: 2407.09064
- Source URL: https://arxiv.org/abs/2407.09064
- Reference count: 19
- Multi-modal dataset creation using DICOM structured reports for federated learning across eight German university hospitals

## Executive Summary
This work presents a federated learning framework for predicting pacemaker dependency after transcatheter aortic valve implantation (TAVI) using multi-modal clinical data. The authors developed a DICOM-based structured reporting system that integrates heterogeneous data types (CT, ECG, segmentations, metadata) from eight German university hospitals into a unified platform. By implementing nested object-based filtering in Opensearch, they enable concurrent querying across different data modalities while maintaining privacy through federated training. The approach successfully addresses cross-institutional data heterogeneity and demonstrates the feasibility of federated training on multi-modal medical data.

## Method Summary
The method involves converting heterogeneous clinical data from multiple institutions into DICOM structured reports, then indexing these reports in Opensearch with nested objects to preserve parent-child relationships. A nested object-based filtering system enables concurrent querying across different data types and levels of the document tree. The platform provides a graphical dashboard with custom Vega visualizations for cohort selection. Datasets are then exported for federated training on pacemaker dependency prediction, with the system maintaining privacy by keeping data at each institution while aggregating model updates.

## Key Results
- Successfully harmonized multi-modal datasets across eight German university hospitals
- Implemented nested object-based filtering system in Opensearch for concurrent querying
- Enabled federated training that addressed data heterogeneity while maintaining privacy
- Demonstrated improved model performance potential through increased sample size and balanced label distribution

## Why This Works (Mechanism)

### Mechanism 1
- Claim: DICOM structured reports enable multi-modal data linkage across heterogeneous clinical systems
- Mechanism: The DICOM standard defines templates that store diverse data types as linked objects, allowing a single patient record to contain CT scans, ECG waveforms, segmentations, and clinical metadata in a unified format
- Core assumption: All participating institutions can export their clinical data into DICOM SR format
- Evidence anchors: DICOM SR templates enable standardized linkage of arbitrary information beyond imaging domain; structured reports bridge gap between imaging and information systems

### Mechanism 2
- Claim: Nested object filtering in Opensearch enables concurrent querying of linked multi-modal data
- Mechanism: By storing related data as nested objects, the system can evaluate multiple conditions on child elements while maintaining parent-child relationships, allowing complex cohort selection without manual data reconciliation
- Core assumption: The nested object structure accurately preserves parent-child relationships across different data modalities
- Evidence anchors: Implementation of nested object-based filtering system in Opensearch; parent documents evaluated at child annotation level

### Mechanism 3
- Claim: Federated learning on harmonized multi-modal datasets improves model performance through increased sample size and balanced label distribution
- Mechanism: By aggregating data from eight university hospitals while preserving privacy, the system creates a larger, more diverse training set that addresses class imbalance in pacemaker dependency prediction
- Core assumption: Federated training across institutions with harmonized data yields performance gains equivalent to centralized training
- Evidence anchors: Federated data collection obtained more cases and more favorable data balancing; addressed cross-institutional data heterogeneity while maintaining privacy

## Foundational Learning

- Concept: DICOM structured reporting and template hierarchy
  - Why needed here: Understanding how DICOM SR templates encode complex clinical data relationships is essential for data integration and filtering
  - Quick check question: How does a DICOM SR template ensure that a segmentation can reference the specific image series it annotates?

- Concept: Federated learning architecture and privacy preservation
  - Why needed here: The system's success depends on understanding how local model training and aggregation work without centralizing sensitive patient data
  - Quick check question: What is the primary mechanism that federated learning uses to preserve data privacy during multi-institutional model training?

- Concept: Opensearch nested objects and complex querying
  - Why needed here: The filtering system relies on nested objects to query multi-modal data relationships, requiring knowledge of how nested queries differ from standard filtering
  - Quick check question: Why can't standard Opensearch visualizations handle nested object queries without custom Vega implementations?

## Architecture Onboarding

- Component map: DICOM data conversion → Opensearch indexing with nested objects → Graphical dashboard with Vega visualizations → Federated export → Model training pipeline
- Critical path: Data conversion and upload → Cohort selection via filtering → Dataset export for federated training
- Design tradeoffs: High flexibility in filtering attributes vs. complexity in defining nested paths; stand-alone deployment vs. integration with existing platforms like Kaapana
- Failure signatures: Inconsistent data conversion leading to missing references; nested object queries returning incorrect results due to referential integrity issues; federated training failing due to incompatible model architectures
- First 3 experiments:
  1. Convert a small sample of heterogeneous data (CT, ECG, annotations) to DICOM SR and verify upload to Opensearch
  2. Test nested object filtering on the sample data to ensure parent-child relationships are maintained
  3. Export a filtered dataset and verify it can be read by a federated learning pipeline

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the nested object-based filtering system in Opensearch be further optimized for scalability when dealing with extremely large datasets across multiple institutions?
- Basis in paper: [inferred] The paper discusses the use of Opensearch's nested objects for concurrent querying across different data types and levels of the document tree, but does not address scalability challenges with extremely large datasets
- Why unresolved: The current implementation has not been tested or evaluated on extremely large datasets, and the paper does not discuss potential scalability issues or optimizations
- What evidence would resolve it: Performance benchmarks and scalability tests on datasets significantly larger than those used in the study, along with proposed optimizations or improvements to the nested object-based filtering system

### Open Question 2
- Question: What are the potential privacy risks associated with federated learning on multi-modal medical data, and how can they be mitigated?
- Basis in paper: [explicit] The paper mentions that federated learning circumvents privacy concerns by sending the model to each data-owning institution, but does not discuss potential privacy risks or mitigation strategies
- Why unresolved: The paper focuses on the technical implementation of federated learning and dataset harmonization, but does not address the privacy implications of sharing model updates across institutions
- What evidence would resolve it: A comprehensive analysis of potential privacy risks in federated learning on multi-modal medical data, along with proposed mitigation strategies and their effectiveness

### Open Question 3
- Question: How can the platform be extended to support additional data types beyond DICOM, such as genomics or proteomics data?
- Basis in paper: [inferred] The paper focuses on DICOM-based structured reports and their integration with multi-modal medical data, but does not discuss support for other data types like genomics or proteomics
- Why unresolved: The current implementation is tailored to DICOM data and structured reports, and extending support to other data types would require significant modifications to the platform
- What evidence would resolve it: A detailed design and implementation plan for extending the platform to support additional data types, along with a proof-of-concept implementation and evaluation on real-world data

## Limitations
- Implementation details of the nested object-based filtering system in Opensearch remain underspecified
- Actual model performance metrics and validation results are not provided
- The system has not been tested on extremely large datasets to evaluate scalability

## Confidence

- High confidence: DICOM structured reporting mechanism for multi-modal data linkage is well-established in medical imaging standards
- Medium confidence: Federated learning approach for improving model performance through increased sample size is theoretically sound, though empirical validation is limited
- Low confidence: Specific implementation details of Opensearch nested object filtering and its scalability across institutions

## Next Checks

1. Conduct a pilot test with heterogeneous clinical data from multiple institutions to verify DICOM SR conversion consistency and referential integrity
2. Implement a small-scale Opensearch nested object filtering system and validate cohort selection accuracy against known ground truth
3. Run federated learning experiments on the harmonized multi-modal dataset to measure performance improvements against baseline centralized training