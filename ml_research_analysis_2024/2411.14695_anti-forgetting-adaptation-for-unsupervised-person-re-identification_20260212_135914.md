---
ver: rpa2
title: Anti-Forgetting Adaptation for Unsupervised Person Re-identification
arxiv_id: '2411.14695'
source_url: https://arxiv.org/abs/2411.14695
tags:
- domain
- adaptation
- learning
- person
- similarity
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of unsupervised lifelong person
  re-identification (ReID), where a model needs to adapt to new domains without forgetting
  previously acquired knowledge and maintain generalization ability to unseen domains.
  The key challenge is to preserve feature representation consistency across different
  adaptation steps while learning new domain-specific features.
---

# Anti-Forgetting Adaptation for Unsupervised Person Re-identification

## Quick Facts
- arXiv ID: 2411.14695
- Source URL: https://arxiv.org/abs/2411.14695
- Authors: Hao Chen; Francois Bremond; Nicu Sebe; Shiliang Zhang
- Reference count: 40
- Key outcome: DJAA significantly outperforms state-of-the-art methods in anti-forgetting capability, showing better preservation of previously learned knowledge while adapting to new domains.

## Executive Summary
This paper addresses the problem of unsupervised lifelong person re-identification (ReID), where a model needs to adapt to new domains without forgetting previously acquired knowledge and maintain generalization ability to unseen domains. The key challenge is to preserve feature representation consistency across different adaptation steps while learning new domain-specific features. The proposed method, DJAA (Dual-level Joint Adaptation and Anti-forgetting), combines an adaptation module and a rehearsal module to achieve superior anti-forgetting performance while maintaining backward compatibility and generalization to unseen domains.

## Method Summary
The DJAA framework combines an adaptation module with prototype-level and instance-level contrastive losses to learn new domain features while reducing intra-cluster variance, and a rehearsal module with a hybrid memory buffer that stores representative image samples and cluster prototypes from previous domains. The rehearsal module uses similarity consistency losses at both image-to-prototype and image-to-image levels to retain old knowledge during adaptation. The method is evaluated on multiple benchmarks including one-cycle full set, two-cycle subset, and generalization to unseen domains, showing significant improvements over state-of-the-art methods.

## Key Results
- DJAA significantly outperforms state-of-the-art methods in anti-forgetting capability across sequential domain adaptation
- Achieves superior generalization performance on unseen datasets while maintaining backward compatibility
- Maintains backward compatibility, allowing previously extracted gallery features to remain comparable with newly extracted ones without requiring re-extraction

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Dual-level contrastive learning preserves both general cluster information and fine-grained instance details, enabling robust anti-forgetting during multi-step domain adaptation.
- Mechanism: The adaptation module uses cluster prototypes to maintain general identity information while the instance-level loss reduces intra-cluster variance by pulling hard positive instances closer. This dual supervision ensures the model retains both coarse and fine identity features across domains.
- Core assumption: Maintaining both cluster-level and instance-level similarity relationships is necessary for preventing catastrophic forgetting in unsupervised lifelong person ReID.
- Evidence anchors:
  - [abstract] "We explore the possibility of using prototype and instance-level consistency to mitigate the forgetting during the adaptation."
  - [section] "The proposed DJAA consists of an adaptation module and a rehearsal module... In the adaptation module, we first use a clustering algorithm to assign pseudo labels... Based on the pseudo labels, we use an image-to-prototype contrastive loss... We also use an image-to-image contrastive loss to further reduce the distance between hard positives."
  - [corpus] Weak evidence - no direct corpus support for dual-level contrastive learning effectiveness.

### Mechanism 2
- Claim: The rehearsal module with dual-level similarity consistency prevents knowledge forgetting by maintaining similarity relationships across domain adaptation steps.
- Mechanism: A hybrid memory buffer stores representative images and cluster prototypes from previous domains. During adaptation, the rehearsal module enforces consistency between similarity distributions computed by the frozen old model and the trainable new model, preserving old knowledge while learning new domain features.
- Core assumption: Similarity relationships between images and prototypes, and between image pairs, should remain consistent across domain adaptation steps to prevent forgetting.
- Evidence anchors:
  - [abstract] "We store a small number of representative image samples and corresponding cluster prototypes in a memory buffer... With the buffered images and prototypes, we regularize the image-to-image similarity and image-to-prototype similarity to rehearse old knowledge."
  - [section] "We propose an image-to-prototype similarity consistency loss to ensure the upcoming new knowledge would not change the image-to-prototype similarity... We formulate an image-to-image similarity constraint loss with the KL Divergence between the two distributions."
  - [corpus] Weak evidence - no direct corpus support for rehearsal-based similarity consistency approach.

### Mechanism 3
- Claim: The unified framework satisfies both incremental learning and backward-compatible learning criteria simultaneously, enabling efficient lifelong person ReID without requiring re-extraction of gallery features.
- Mechanism: The framework ensures that distance relationships between anchor-positive-negative triples are preserved (incremental learning criterion) and distance relationships between new queries and old galleries are preserved (backward-compatible criterion) through the dual-level adaptation and rehearsal modules.
- Core assumption: The same framework that prevents forgetting can also maintain backward compatibility between old and new representations.
- Evidence anchors:
  - [abstract] "We comprehensively investigate the forgetting, generalization, and backward-compatible problems... Our study shows that, those three challenges could be jointly addressed in a Dual-level Joint Adaptation and Anti-forgetting (DJAA) framework."
  - [section] "We show that the incremental learning criterion Eq. (1) and the backward-compatible criterion Eq. (2) can be both satisfied with a unified framework."
  - [corpus] Weak evidence - no direct corpus support for the unification claim.

## Foundational Learning

- Concept: Contrastive learning and similarity-based representation learning
  - Why needed here: The method relies on maximizing similarity between positive pairs and minimizing similarity between negative pairs to learn discriminative person features without labels.
  - Quick check question: What is the mathematical formulation of a contrastive loss function and how does temperature scaling affect its behavior?

- Concept: Unsupervised domain adaptation and pseudo-label generation
  - Why needed here: The method adapts to new domains without human annotation by generating pseudo labels through clustering and using these for contrastive learning.
  - Quick check question: How does DBSCAN clustering work and what are the implications of different distance thresholds on pseudo-label quality?

- Concept: Incremental learning and catastrophic forgetting
  - Why needed here: The method must learn from multiple domains sequentially without forgetting previously acquired knowledge, which is the core challenge being addressed.
  - Quick check question: What is catastrophic forgetting and how do rehearsal-based methods prevent it compared to regularization-based approaches?

## Architecture Onboarding

- Component map:
  - Online encoder (θ) -> Clustering -> Prototype extraction -> Contrastive adaptation losses -> Memory buffer update -> Similarity consistency losses -> Momentum encoder update

- Critical path: Online encoder → Clustering → Prototype extraction → Contrastive adaptation losses → Memory buffer update → Similarity consistency losses → Momentum encoder update

- Design tradeoffs:
  - Memory buffer size vs. anti-forgetting performance: Larger buffers provide better forgetting prevention but increase memory usage
  - Temperature hyperparameters (τpa, τia, τps, τis) vs. similarity scaling: Different temperatures affect how aggressively the model pulls positive pairs together
  - Weight balancing (λia, λps, λis) vs. loss importance: Different weights determine the relative importance of adaptation vs. rehearsal

- Failure signatures:
  - Performance degradation on previously seen domains indicates catastrophic forgetting
  - Poor generalization to unseen domains suggests insufficient domain-agnostic feature learning
  - Large performance gap between self-test and cross-test indicates poor backward compatibility

- First 3 experiments:
  1. Implement the adaptation module only (Lpa + Lia) and test anti-forgetting performance on sequential domain adaptation to establish baseline
  2. Add image-to-prototype similarity consistency loss (Lps) to the adaptation module and measure improvement in forgetting prevention
  3. Add image-to-image similarity consistency loss (Lis) and evaluate complete DJAA performance against baseline

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal size and composition of the hybrid memory buffer for balancing anti-forgetting performance and computational efficiency across different domain adaptation scenarios?
- Basis in paper: [explicit] The paper mentions that the default memory size is 512 and shows performance varies with buffer size (Table XIX), but doesn't determine an optimal size for different scenarios.
- Why unresolved: The optimal buffer size likely depends on domain diversity, dataset scale, and adaptation frequency, which weren't systematically explored.
- What evidence would resolve it: Systematic experiments varying buffer size across different domain pairs and measuring both performance and computational overhead.

### Open Question 2
- Question: How can the proposed framework be extended to handle more complex domain-incremental scenarios with non-stationary data distributions or concept drift?
- Basis in paper: [inferred] The paper focuses on sequential adaptation to discrete datasets, but real-world scenarios often involve continuous data streams with changing distributions.
- Why unresolved: The current framework assumes discrete adaptation steps with stable domain representations, which may not generalize to streaming scenarios.
- What evidence would resolve it: Testing the framework on continuous data streams and evaluating its ability to detect and adapt to distribution shifts.

### Open Question 3
- Question: What are the theoretical guarantees for the similarity consistency regularization in preventing catastrophic forgetting across multiple adaptation steps?
- Basis in paper: [inferred] The paper proposes similarity consistency losses but doesn't provide theoretical analysis of their effectiveness in preserving knowledge over long adaptation sequences.
- Why unresolved: While empirical results show effectiveness, the theoretical understanding of when and how these regularizers prevent forgetting remains unexplored.
- What evidence would resolve it: Mathematical analysis showing conditions under which similarity consistency regularization prevents forgetting, possibly using information theory or PAC-Bayes bounds.

## Limitations

- Key implementation details remain underspecified, including the "camera proxy" implementation and exact clustering-guided memory buffer update strategy
- Evaluation primarily focuses on one-cycle setting with limited ablation studies on two-cycle and generalization scenarios
- Claims about unification of incremental learning and backward-compatible learning criteria are stated but not rigorously proven

## Confidence

- **High Confidence**: Overall framework design combining adaptation and rehearsal modules is well-articulated and logically sound
- **Medium Confidence**: Empirical results showing DJAA's superiority are convincing but lack detailed ablation studies on individual components
- **Low Confidence**: Claims about unification of incremental learning and backward-compatible learning criteria into a single framework are stated but not extensively validated

## Next Checks

1. **Component Ablation Study**: Systematically remove each component (prototype-level loss, instance-level loss, image-to-prototype consistency, image-to-image consistency) to quantify their individual contributions to anti-forgetting performance.

2. **Memory Buffer Sensitivity Analysis**: Evaluate DJAA performance with varying memory buffer sizes (256, 1024, 2048) to establish the tradeoff between memory usage and forgetting prevention.

3. **Cross-Domain Generalization Testing**: Conduct extensive testing on the 10 unseen domains to verify whether DJAA truly learns domain-agnostic features or simply overfits to the 4 seen training domains.