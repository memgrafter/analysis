---
ver: rpa2
title: Detecting Out-of-Distribution Samples via Conditional Distribution Entropy
  with Optimal Transport
arxiv_id: '2401.11726'
source_url: https://arxiv.org/abs/2401.11726
tags:
- transport
- training
- distribution
- detection
- test
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper tackles the problem of out-of-distribution (OOD) detection,
  where a trained model encounters inputs from distributions different from its training
  data. The authors propose a novel approach that leverages the empirical distributions
  of both training samples and test inputs, incorporating geometric information from
  the feature space.
---

# Detecting Out-of-Distribution Samples via Conditional Distribution Entropy with Optimal Transport

## Quick Facts
- arXiv ID: 2401.11726
- Source URL: https://arxiv.org/abs/2401.11726
- Authors: Chuanwen Feng; Wenlong Chen; Ao Ke; Yilong Ren; Xike Xie; S. Kevin Zhou
- Reference count: 20
- Primary result: Novel OOD detection method using conditional distribution entropy with optimal transport outperforms state-of-the-art on benchmark datasets

## Executive Summary
This paper tackles the problem of out-of-distribution (OOD) detection, where a trained model encounters inputs from distributions different from its training data. The authors propose a novel approach that leverages the empirical distributions of both training samples and test inputs, incorporating geometric information from the feature space. Their method formulates OOD detection as a discrete optimal transport problem, where they construct empirical probability measures for the training and test data. They then introduce a new score function called conditional distribution entropy, which quantifies the uncertainty of a test input being OOD. The key idea is that an OOD input will have a higher conditional distribution entropy compared to an in-distribution input.

## Method Summary
The proposed method for OOD detection involves constructing empirical probability measures for the training and test data using the feature representations of a pretrained model. The authors then formulate the OOD detection problem as a discrete optimal transport problem, where they compute the optimal transport plan between the training and test distributions. Based on this transport plan, they introduce a new score function called conditional distribution entropy, which measures the uncertainty of a test input being OOD. The intuition is that an OOD input will have a higher conditional distribution entropy compared to an in-distribution input, as it is less likely to have a well-defined transport plan with the training distribution. The method is evaluated on benchmark datasets and compared against state-of-the-art OOD detection techniques, demonstrating significant improvements in terms of metrics like AUROC and AUPR.

## Key Results
- The proposed method outperforms state-of-the-art OOD detection methods on benchmark datasets
- Significant improvements in AUROC and AUPR metrics compared to existing techniques
- Theoretical analysis and ablation studies support the effectiveness of the approach

## Why This Works (Mechanism)
The method works by leveraging the geometric structure of the feature space and the empirical distributions of the training and test data. By formulating OOD detection as an optimal transport problem, the authors can capture the intrinsic similarity between samples in the feature space. The conditional distribution entropy score then quantifies the uncertainty of a test input being OOD based on its transport plan with the training distribution. An OOD input will have a higher conditional distribution entropy because it is less likely to have a well-defined transport plan, indicating that it lies in a region of the feature space that is less similar to the training distribution.

## Foundational Learning
1. **Optimal Transport (OT)**: A mathematical framework for comparing probability distributions based on their geometric properties. OT is needed to capture the intrinsic similarity between samples in the feature space. Quick check: Verify that the OT formulation is correctly applied to the empirical distributions of the training and test data.

2. **Empirical Probability Measures**: Discrete probability measures constructed from the feature representations of the training and test data. These measures are needed to represent the distributions in the OT framework. Quick check: Ensure that the empirical measures are properly normalized and that the feature representations are appropriate for the task.

3. **Conditional Distribution Entropy**: A score function that quantifies the uncertainty of a test input being OOD based on its transport plan with the training distribution. This entropy measure is needed to differentiate between in-distribution and OOD samples. Quick check: Verify that the conditional distribution entropy is correctly computed from the optimal transport plan and that it effectively captures the uncertainty of OOD samples.

## Architecture Onboarding
**Component Map**: Pretrained model -> Feature extractor -> Empirical probability measures -> Optimal transport solver -> Conditional distribution entropy score

**Critical Path**: The critical path involves computing the feature representations of the training and test data, constructing the empirical probability measures, solving the optimal transport problem, and computing the conditional distribution entropy score for each test input.

**Design Tradeoffs**: The main tradeoff is between the expressiveness of the feature representations and the computational complexity of the optimal transport problem. Using more complex feature representations may improve the quality of the OOD detection but at the cost of increased computational overhead.

**Failure Signatures**: Potential failure modes include:
- Poor feature representations that do not capture the relevant information for OOD detection
- Suboptimal solutions to the optimal transport problem due to computational constraints
- Overfitting to the training distribution, leading to false positives for OOD samples that are similar to the training data

**3 First Experiments**:
1. Verify the correctness of the optimal transport solver by comparing its output to a known solution on a simple example.
2. Evaluate the sensitivity of the conditional distribution entropy score to the choice of feature representations and optimal transport solver.
3. Assess the impact of the number of training samples on the performance of the OOD detection method.

## Open Questions the Paper Calls Out
None

## Limitations
- Lack of detailed comparison with other recent OOD detection methods that also leverage optimal transport
- Unclear generalizability to real-world scenarios with more complex and diverse data distributions
- Primarily evaluated on image datasets, with limited assessment on other data modalities

## Confidence
- Major claims: High
- Comparison with related work: Medium
- Generalizability to real-world scenarios: Low

## Next Checks
1. Conduct a thorough comparison with other optimal transport-based OOD detection methods, such as "Detecting OOD Samples via Optimal Transport Scoring Function," to provide a more comprehensive evaluation of the proposed approach.

2. Assess the generalizability of the method to real-world scenarios by evaluating its performance on diverse data modalities, such as text or audio, and on datasets with more complex and varied distributions.

3. Investigate the computational efficiency of the proposed method, particularly in terms of its scalability to large-scale datasets and its runtime performance compared to other OOD detection techniques.