---
ver: rpa2
title: 'RadDQN: a Deep Q Learning-based Architecture for Finding Time-efficient Minimum
  Radiation Exposure Pathway'
arxiv_id: '2402.00468'
source_url: https://arxiv.org/abs/2402.00468
tags:
- radiation
- training
- algorithm
- path
- agent
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: RadDQN introduces a deep Q-learning framework for time-efficient
  minimum radiation exposure pathway planning in radioactive zones. The approach uses
  a radiation-aware reward function incorporating inverse-square law for radiation
  intensity, agent proximity to radiation sources and destination, and a positive
  reinforcement for reaching the exit.
---

# RadDQN: a Deep Q Learning-based Architecture for Finding Time-efficient Minimum Radiation Exposure Pathway

## Quick Facts
- arXiv ID: 2402.00468
- Source URL: https://arxiv.org/abs/2402.00468
- Authors: Biswajit Sadhu; Trijit Sadhu; S. Anand
- Reference count: 40
- Primary result: Deep Q-learning framework for time-efficient minimum radiation exposure pathway planning outperforms vanilla DQN in convergence rate and training stability

## Executive Summary
RadDQN introduces a deep Q-learning framework for time-efficient minimum radiation exposure pathway planning in radioactive zones. The approach uses a radiation-aware reward function incorporating inverse-square law for radiation intensity, agent proximity to radiation sources and destination, and positive reinforcement for reaching the exit. Three exploration strategies (vanilla, restricted, and partially-restricted) control exploration-exploitation trade-offs by converting random actions to model-directed ones based on future state radiation levels and winning ratios. The model outperforms vanilla DQN in convergence rate and training stability across multiple radiation source scenarios with varying strengths and positions.

## Method Summary
The RadDQN architecture implements a deep Q-learning agent trained in a 2D grid-world environment with radioactive sources. The agent uses a neural network with two hidden layers (200, 100 units) and LeakyReLU activation to approximate Q-values for 8 possible actions. Training employs experience replay with a buffer size of 2000 and dynamic target network updates based on performance metrics. The reward function combines inverse-square radiation intensity with exit proximity terms, while three exploration strategies control the exploration-exploitation balance. The model is evaluated against Dijkstra's algorithm as ground truth using Frechet distance metrics.

## Key Results
- RadDQN achieves superior cumulative rewards with lower variance compared to vanilla DQN
- Dynamic target network update frequency improves training stability and convergence rate
- Exploration strategy conversion reduces variance in training outcomes
- Model successfully learns complex radiation field distributions for optimal path planning

## Why This Works (Mechanism)

### Mechanism 1
The radiation-aware reward function incorporating inverse-square law enables the agent to learn complex radiation field distributions and optimize for minimum cumulative exposure. By calculating radiation intensity as ΓSi/R²s,i for each source and combining it with a positive reinforcement term n/Re for proximity to exit, the reward function creates a multi-objective optimization landscape where the agent must balance exposure minimization with time efficiency.

### Mechanism 2
The exploration strategy conversion (random to model-directed actions based on future state radiation levels and winning ratios) improves training stability and convergence rate. By converting random actions to model-directed ones when future states have higher radiation exposure than current states, the agent can focus exploration on promising regions while avoiding obviously poor choices, reducing the variance in training.

### Mechanism 3
Dynamic adjustment of target network update frequency based on performance metrics stabilizes training and prevents catastrophic forgetting. By increasing update frequency when winning ratios improve and decreasing it when performance degrades, the target network maintains stability while allowing the model to adapt quickly to learned improvements.

## Foundational Learning

- Concept: Markov Decision Process (MDP) framework
  - Why needed here: The agent must make sequential decisions where current state and action determine future states, requiring proper state representation and reward accumulation
  - Quick check question: Can the agent's next state be fully determined by its current state and action, given the radiation environment?

- Concept: Function approximation with neural networks
  - Why needed here: The state space (radiation levels, agent positions, source locations) is continuous and high-dimensional, making tabular Q-learning infeasible
  - Quick check question: Does the neural network architecture have sufficient capacity to approximate the Q-value function for all state-action pairs?

- Concept: Exploration vs. exploitation trade-off
  - Why needed here: The agent must balance trying new paths (exploration) with following known good paths (exploitation) to find optimal radiation-minimizing routes
  - Quick check question: Does the exploration strategy allow sufficient randomness to discover better paths while still converging on good solutions?

## Architecture Onboarding

- Component map:
  Environment simulator -> Neural network (200→100 hidden units) -> Experience replay buffer -> Target network -> Exploration strategy controller

- Critical path:
  1. Agent takes action in environment
  2. Environment returns new state and reward
  3. Experience stored in replay buffer
  4. Mini-batch sampled and used to update Q-network
  5. Target network updated based on performance metrics
  6. Training continues until convergence

- Design tradeoffs:
  - Fixed vs. dynamic target network update frequency: Dynamic allows faster adaptation but may introduce instability
  - Exploration strategy strictness: More restrictive strategies reduce variance but may miss optimal paths
  - Reward function parameters (n, Γ): Affect balance between radiation minimization and time efficiency

- Failure signatures:
  - High variance in cumulative reward across episodes indicates unstable training
  - Failure to improve winning percentage over time suggests poor exploration-exploitation balance
  - Large Frechet distance from ground truth paths indicates suboptimal learning

- First 3 experiments:
  1. Test convergence with different exploration strategies (expv, expr, exppr) on simple two-source scenario
  2. Validate radiation-aware reward function by comparing learned paths against Dijkstra's algorithm
  3. Evaluate dynamic target network update frequency by comparing against fixed-frequency baseline

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the RadDQN model's performance change when radiation sources move dynamically during training episodes?
- Basis in paper: The paper states "One limitation of our developed model is that the agent is trained in static environment i.e. the location of radiation sources is kept fixed during the training phase."
- Why unresolved: The paper explicitly acknowledges this limitation but does not provide any experimental data or analysis of dynamic source scenarios.
- What evidence would resolve it: Experimental results comparing RadDQN performance with static vs. dynamic radiation source placement during training, including metrics on convergence rate, path optimality, and radiation exposure minimization.

### Open Question 2
- Question: What is the theoretical foundation explaining why the target network stabilization technique works in the RadDQN context?
- Basis in paper: The paper states "The theoretical explanation on how and why target network stabilize the training is still an open topic of investigation."
- Why unresolved: While the paper implements a target network with dynamic update frequency, it does not provide mathematical or theoretical justification for its effectiveness.
- What evidence would resolve it: A rigorous mathematical analysis or empirical study demonstrating the relationship between target network update frequency, convergence stability, and performance in radiation-aware reinforcement learning.

### Open Question 3
- Question: How does the choice of the exploration strategy (exppr vs. expr) affect the agent's ability to discover alternative optimal paths in environments with multiple equivalent solutions?
- Basis in paper: The paper mentions "Case V: Here, the arrangement of sources is such that two possible solution exists regarding the optimal path" and compares exploration strategies.
- Why unresolved: While the paper notes that expr may cause "short-sightedness" in certain scenarios, it does not provide a comprehensive analysis of how different exploration strategies impact the discovery of alternative optimal paths.
- What evidence would resolve it: Systematic experiments comparing path diversity and convergence behavior across multiple runs with different exploration strategies in environments with multiple optimal solutions.

## Limitations
- Agent is trained in static environment with fixed radiation source locations
- No theoretical explanation for target network stabilization effectiveness
- Limited evaluation scenarios and lack of real-world validation
- Unspecified implementation details for exploration strategy conversion logic

## Confidence

**Confidence Labels for Major Claim Clusters**

1. **Radiation-aware reward function effectiveness**: High confidence
2. **Exploration strategy improvements**: Medium confidence  
3. **Dynamic target network update**: Low confidence
4. **Performance superiority over vanilla DQN**: Medium confidence

## Next Checks

1. **Implement and test exploration strategy conversion**: Create a controlled experiment isolating the exploration strategy component to verify that converting random actions based on future state radiation levels actually improves convergence rate and reduces variance compared to vanilla epsilon-greedy exploration.

2. **Validate dynamic target network update mechanism**: Implement the performance-based target network update frequency algorithm with specified thresholds and test whether it indeed provides better stability and faster convergence than fixed-frequency updates across multiple random seeds.

3. **Statistical significance testing**: Conduct Wilcoxon signed-rank tests comparing RadDQN performance against vanilla DQN across 30+ random seeds to verify that observed improvements in cumulative reward and convergence rate are statistically significant rather than due to chance.