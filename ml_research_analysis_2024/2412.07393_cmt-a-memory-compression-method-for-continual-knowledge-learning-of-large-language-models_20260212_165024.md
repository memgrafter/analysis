---
ver: rpa2
title: 'CMT: A Memory Compression Method for Continual Knowledge Learning of Large
  Language Models'
arxiv_id: '2412.07393'
source_url: https://arxiv.org/abs/2412.07393
tags:
- memory
- learning
- knowledge
- training
- documents
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces CMT, a memory compression method for continual
  knowledge learning of large language models (LLMs). The core idea is to compress
  new document knowledge into a memory bank, enabling efficient integration of new
  knowledge without retraining the model parameters.
---

# CMT: A Memory Compression Method for Continual Knowledge Learning of Large Language Models

## Quick Facts
- arXiv ID: 2412.07393
- Source URL: https://arxiv.org/abs/2412.07393
- Authors: Dongfang Li; Zetian Sun; Xinshuo Hu; Baotian Hu; Min Zhang
- Reference count: 15
- Primary result: Improves model adaptability and robustness with up to +4.07 EM and +4.19 F1 in StreamingQA using Llama-2-7b

## Executive Summary
This paper introduces CMT, a memory compression method for continual knowledge learning of large language models (LLMs). The core idea is to compress new document knowledge into a memory bank, enabling efficient integration of new knowledge without retraining the model parameters. The method includes three key techniques: memory-aware objective, self-matching, and top-k aggregation. Extensive experiments on three datasets demonstrate that CMT improves model adaptability and robustness, achieving up to +4.07 EM and +4.19 F1 in StreamingQA with Llama-2-7b, while reducing trainable parameters and inference time.

## Method Summary
CMT compresses new document knowledge into a memory bank using a document compressor model, enabling efficient integration of new knowledge without retraining the model parameters. The method employs three key techniques: memory-aware objective, self-matching, and top-k aggregation. The compressor transforms raw documents into compressed memory vectors using cross-attention and soft tokens. These vectors are stored in a memory bank and retrieved during inference via cross-attention with the query. The aggregation network produces a single context vector that is mapped into the LLM via an alignment module. The LLM remains frozen during both training and inference, reducing the risk of catastrophic forgetting.

## Key Results
- Achieves up to +4.07 EM and +4.19 F1 improvement in StreamingQA with Llama-2-7b
- Reduces trainable parameters compared to full fine-tuning approaches
- Decreases inference time while maintaining or improving accuracy
- Demonstrates effectiveness across three continual learning datasets (StreamingQA, SQuAD, ArchivalQA)

## Why This Works (Mechanism)

### Mechanism 1
Freezing the LLM parameters while updating only a memory compression module prevents catastrophic forgetting. The LLM's weights remain static during both training and inference. A separate compressor module encodes new documents into a memory bank, which is then used via cross-attention to augment the LLM's outputs without altering its parameters. Core assumption: The base LLM retains sufficient general knowledge and reasoning capability even when frozen, and the memory bank can supply relevant contextual information on demand.

### Mechanism 2
Cross-attention-based aggregation of compressed memory vectors allows the model to selectively integrate relevant knowledge per query. For each query, the model computes attention weights between the query embedding and all compressed document vectors, then aggregates them into a single context vector that is mapped into the LLM via cached key-value pairs. Core assumption: Attention weights effectively identify and weight relevant documents, and the aggregation preserves sufficient semantic information for downstream QA.

### Mechanism 3
Memory-aware conditional objective and self-matching improve the model's ability to use external knowledge while maintaining diversity. The memory-aware objective interpolates the LLM's original logits with those conditioned on the memory, weighted by a hyperparameter. Self-matching maximizes cosine similarity between queries and their corresponding document embeddings, with a uniformity penalty to prevent collapse. Core assumption: Adjusting the output distribution via interpolation enhances integration of external knowledge without overfitting, and self-matching encourages meaningful document-query associations.

## Foundational Learning

- Concept: Cross-attention mechanism
  - Why needed here: Enables selective aggregation of compressed memory vectors based on query relevance, forming the core of the memory integration pipeline.
  - Quick check question: In a cross-attention layer, what roles do the query, key, and value vectors play when integrating memory into the LLM?

- Concept: Catastrophic forgetting in continual learning
  - Why needed here: Justifies the design choice of freezing LLM parameters and using a memory bank to avoid overwriting learned knowledge during adaptation.
  - Quick check question: What is the primary risk when fine-tuning large language models on new tasks without special techniques?

- Concept: Memory compression via soft tokens
  - Why needed here: Allows efficient storage of document knowledge in a condensed form that can be rapidly retrieved and aggregated during inference.
  - Quick check question: How does using a small number of soft tokens to represent a long document help with memory efficiency and computational cost?

## Architecture Onboarding

- Component map: Compressor (Θ) -> Memory Bank -> Aggregation Network (ψ) -> Alignment Module (π) -> LLM (Φ)
- Critical path:
  1. Compress new documents into memory vectors (offline).
  2. Store vectors in memory bank.
  3. For each query, compute cross-attention with memory bank.
  4. Aggregate into single context vector.
  5. Map context into LLM via alignment module.
  6. Generate answer using frozen LLM.
- Design tradeoffs:
  - Freezing LLM vs. fine-tuning: Avoids forgetting but may limit adaptation if base model lacks domain knowledge.
  - Number of soft tokens: More tokens improve representation fidelity but increase memory usage and aggregation cost.
  - Top-k filtering: Reduces computation but may discard useful documents if k is too small.
- Failure signatures:
  - Low QA accuracy despite large memory bank: Likely due to poor document compression or ineffective aggregation.
  - High variance in performance across queries: May indicate sensitivity to irrelevant documents in memory bank.
  - Slow inference: Could be caused by large memory bank without top-k filtering or inefficient aggregation.
- First 3 experiments:
  1. Ablation study removing the memory-aware objective to measure its impact on knowledge integration.
  2. Test knowledge retention by evaluating on a fixed set of queries as more documents are added to the memory bank.
  3. Evaluate robustness by measuring performance degradation as the proportion of irrelevant documents in the memory bank increases.

## Open Questions the Paper Calls Out

### Open Question 1
How does CMT perform on more diverse and challenging datasets beyond the three evaluated (StreamingQA, SQuAD, ArchivalQA)? The paper only evaluates CMT on three datasets, limiting the generalizability of the results. Testing CMT on a wider range of datasets with different characteristics would resolve this question.

### Open Question 2
How does CMT compare to other memory-augmented methods like Retrieval-Augmented Generation (RAG) in terms of efficiency and accuracy? The paper does not provide a direct comparison with RAG, which is a popular and well-established method for integrating external knowledge. Conducting a head-to-head comparison on the same datasets would resolve this question.

### Open Question 3
How does the performance of CMT scale with the size of the memory bank and the complexity of the documents? The paper mentions memory usage scaling but does not provide a detailed analysis of how performance is affected by memory bank size and document complexity. Experiments with varying memory bank sizes and document complexities would resolve this question.

### Open Question 4
How can CMT be extended to handle multimodal data and integrate knowledge from different sources? The paper focuses on text-based knowledge but does not explore integrating knowledge from other modalities. Developing and evaluating extensions that can handle multimodal data would resolve this question.

## Limitations

- Limited generalizability beyond news and Wikipedia domains due to evaluation on only three QA datasets
- Scalability concerns for very large memory banks (hundreds or thousands of documents) not thoroughly explored
- Direct comparison with established methods like Retrieval-Augmented Generation (RAG) is absent

## Confidence

**High Confidence**:
- The method successfully improves QA performance on the tested datasets compared to baselines
- The compression approach reduces trainable parameters and inference time relative to full fine-tuning
- The three proposed techniques (memory-aware objective, self-matching, top-k aggregation) can be implemented as described

**Medium Confidence**:
- The method prevents catastrophic forgetting by freezing LLM parameters
- Cross-attention aggregation effectively selects and integrates relevant knowledge
- The memory-aware objective meaningfully improves knowledge integration

**Low Confidence**:
- The method generalizes to domains beyond news and Wikipedia QA
- The approach scales effectively to very large memory banks (hundreds or thousands of documents)
- The specific architectural choices (number of soft tokens, attention mechanisms) are optimal

## Next Checks

1. **Forgetting prevention validation**: Design an experiment that measures performance on a held-out initial task as new documents are continually added to the memory bank. Compare against a fine-tuning baseline to directly quantify catastrophic forgetting prevention.

2. **Memory bank size scalability**: Evaluate performance as the number of stored documents increases from 50 to 500+ in the StreamingQA setup. Measure both accuracy and inference latency to identify practical limits.

3. **Irrelevant document robustness**: Create controlled experiments where the memory bank contains varying proportions (0% to 80%) of documents irrelevant to the query. Measure how performance degrades and analyze attention weight distributions to understand the mechanism's robustness to noise.