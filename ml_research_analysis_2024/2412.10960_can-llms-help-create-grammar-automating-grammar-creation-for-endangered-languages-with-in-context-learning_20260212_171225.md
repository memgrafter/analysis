---
ver: rpa2
title: 'Can LLMs Help Create Grammar?: Automating Grammar Creation for Endangered
  Languages with In-Context Learning'
arxiv_id: '2412.10960'
source_url: https://arxiv.org/abs/2412.10960
tags:
- moklen
- language
- grammar
- languages
- llms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "The paper demonstrates that large language models can generate\
  \ coherent grammatical rules and lexical entries for endangered languages like Moklen\
  \ using minimal data\u2014specifically bilingual dictionaries and parallel sentences\u2014\
  without requiring model training from scratch. Through in-context learning, the\
  \ model successfully captured key grammatical structures and produced accurate lexical\
  \ entries for 86% of tested words."
---

# Can LLMs Help Create Grammar?: Automating Grammar Creation for Endangered Languages with In-Context Learning

## Quick Facts
- arXiv ID: 2412.10960
- Source URL: https://arxiv.org/abs/2412.10960
- Reference count: 4
- Primary result: LLMs can generate coherent grammatical rules and lexical entries for endangered languages like Moklen using minimal data—bilingual dictionaries and parallel sentences—without requiring model training from scratch.

## Executive Summary
This paper demonstrates that large language models can generate grammatical rules and lexical entries for endangered languages using minimal data through in-context learning. The authors focus on Moklen, an isolating language with limited documentation, and show that LLMs can capture key grammatical structures and produce accurate lexical entries for 86% of tested words. By leveraging bilingual dictionaries and parallel sentences, the model successfully generates formal XLE grammar representations that improve translation quality, with BERTScore F1 reaching up to 0.7110 for Moklen-to-English tasks. The study highlights the potential of LLMs as cost-effective tools for language documentation, though challenges remain with English grammatical biases and evaluation metric limitations for isolating languages.

## Method Summary
The methodology involves organizing existing linguistic data and prompting LLMs to efficiently generate formal XLE grammar for endangered languages. The approach uses bilingual dictionaries and parallel sentences as minimal data sources, implementing dictionary-based tokenisation with a longest-match strategy to prevent over-tokenisation in the isolating language. Through in-context learning, the LLM compares shared structures between Moklen and English to induce grammatical rules, generating both natural language descriptions and formal XLE grammar representations. The model is prompted with varying contexts (bitext, tokenised, concatenated, example, self-explanation) and retrieval-augmented with XLE documentation and dictionary, using OpenAI's gpt-4o-mini-2024-07-18 API with temperature 0.1.

## Key Results
- Generated lexical entries were accurate for 86% of tested words in Moklen
- Translation performance improved significantly when grammar was provided, with BERTScore F1 reaching up to 0.7110 for Moklen-to-English tasks
- The LLM successfully captured key grammatical structures and produced coherent grammatical rules from minimal data sources
- Dictionary-based tokenisation effectively handled the isolating nature of Moklen without morphological complexity

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The model can induce grammatical rules from minimal data by leveraging its pretraining on English linguistic structures and applying analogical reasoning to the target language.
- Mechanism: The LLM uses in-context learning to compare parallel sentences between Moklen and English, identifying shared syntactic patterns and mapping them to formal XLE grammar representations.
- Core assumption: The model's pretraining on English provides sufficient linguistic competence to recognize and transfer grammatical patterns to structurally similar languages.
- Evidence anchors:
  - [abstract] "Our results demonstrate that LLMs can successfully capture key grammatical structures and lexical information"
  - [section] "By comparing the shared structures of Moklen and English, the LLM can induce grammatical rules from the context and information given"
  - [corpus] Weak - no direct corpus evidence for this specific mechanism, though related papers suggest similar approaches work
- Break condition: If the target language has grammatical structures that differ significantly from English, the model may fail to capture them accurately due to English grammatical biases.

### Mechanism 2
- Claim: Dictionary-based tokenisation enables accurate segmentation of isolating languages without morphological complexity.
- Mechanism: The longest-match strategy uses the dictionary to find the largest matching word unit, preventing over-tokenisation of compound words in languages like Moklen.
- Core assumption: Isolating languages with minimal inflectional morphology can be effectively tokenised using dictionary lookups rather than complex morphological analysis.
- Evidence anchors:
  - [section] "This approach is feasible since all words in the Moklen sentences from the bitext exist in the dictionary"
  - [section] "To address this, a longest-match strategy is employed where the tokeniser first looks for the closest and largest item in the dictionary"
  - [corpus] Weak - no direct corpus evidence, but the approach is validated by successful tokenisation in the experiments
- Break condition: If the dictionary is incomplete or contains errors, tokenisation accuracy will degrade, leading to incorrect sense mapping and grammatical analysis.

### Mechanism 3
- Claim: In-context learning with carefully structured prompts can generate formal grammatical representations without model training.
- Mechanism: The model is provided with XLE documentation, dictionary entries, and parallel sentences, then prompted to generate both natural language descriptions and formal XLE grammar rules.
- Core assumption: LLMs can understand and generate formal grammatical representations when provided with appropriate documentation and examples in the prompt context.
- Evidence anchors:
  - [abstract] "Our methodology involves organising the existing linguistic data and prompting to efficiently enable to generate formal XLE grammar"
  - [section] "The second approach is to generate the grammar in a formal language, specifically XLE, by creating rules and lexical entries using a specific template and syntax"
  - [corpus] Moderate - related papers show LLMs can generate grammatical rules from minimal data, though specific XLE generation is novel
- Break condition: If the model cannot understand the formal syntax required for XLE grammar, or if the prompt structure is insufficient, generation will fail or produce incorrect rules.

## Foundational Learning

- Concept: Bilingual dictionary lookup and tokenisation
  - Why needed here: Essential for segmenting the target language text and mapping words to their English meanings, which enables the model to understand the language structure
  - Quick check question: Can you explain how a longest-match strategy prevents over-tokenisation in compound words?

- Concept: In-context learning principles
  - Why needed here: The entire methodology relies on the model learning from examples and documentation provided within the prompt, rather than from training
  - Quick check question: What are the key differences between zero-shot, few-shot, and chain-of-thought prompting, and when would each be most appropriate?

- Concept: Formal grammar representation (XLE format)
  - Why needed here: The generated output must be in a machine-readable format that can be used for actual parsing and language processing tasks
  - Quick check question: Can you describe the basic structure of an XLE grammar rule and explain how it differs from natural language grammar descriptions?

## Architecture Onboarding

- Component map: Data preparation (Dictionary, parallel sentences, tokenisation) -> Prompt engineering (Context selection, documentation inclusion, example provision) -> Model interaction (API calls with temperature control and context management) -> Output processing (Grammar rule extraction, lexical entry generation, evaluation) -> Evaluation pipeline (Translation metrics, grammar accuracy assessment, lexical coherence checking)

- Critical path: Tokenisation -> Sense mapping -> Prompt construction -> Grammar generation -> Evaluation -> Refinement

- Design tradeoffs:
  - Model size vs. computational cost: Smaller models are sufficient but may miss subtle linguistic features
  - Context length vs. prompt complexity: More context improves performance but increases prompt engineering difficulty
  - Translation metrics vs. actual grammar quality: Standard metrics may not capture isolating language characteristics
  - Dictionary completeness vs. tokenisation accuracy: More comprehensive dictionaries improve accuracy but increase processing time

- Failure signatures:
  - Low translation scores despite grammatically correct rules (metric misalignment)
  - Generated rules that are overly English-biased (cross-linguistic interference)
  - Tokenisation failures leading to incorrect sense mapping (dictionary issues)
  - Hallucinations producing non-existent grammatical structures (low-resource data problems)

- First 3 experiments:
  1. Test tokenisation accuracy on a small sample of Moklen sentences with known dictionary entries
  2. Evaluate sense mapping accuracy by comparing model-selected meanings against human annotations
  3. Generate grammar rules for a single sentence type and verify against the gold standard grammar

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does model performance scale with language complexity beyond isolating languages like Moklen?
- Basis in paper: [explicit] The authors note that Moklen's isolating nature without grammatical inflections may have made it easier for the LLM to decipher the language, and they plan to extend this methodology to experiment with various languages from different typologies and morphologies to assess its broader applicability.
- Why unresolved: The study only tested on Moklen, an isolating language with minimal morphological complexity, leaving uncertainty about performance on agglutinative, fusional, or polysynthetic languages.
- What evidence would resolve it: Conducting the same experiment on languages with varying morphological typologies (e.g., Turkish, Russian, Inuktitut) and comparing performance metrics would demonstrate how model accuracy correlates with linguistic complexity.

### Open Question 2
- Question: Can the methodology be adapted to generate grammar for unwritten languages without existing dictionaries or parallel texts?
- Basis in paper: [inferred] The current methodology relies on bilingual dictionaries and parallel sentences as minimal data sources, but the authors suggest that the model's success with minimal resources opens possibilities for other approaches. This implies the need to explore methods that work with even less structured data.
- Why unresolved: The study assumes some written documentation exists (dictionary and bitext), but many endangered languages are purely oral without any written form or digital representation.
- What evidence would resolve it: Testing the approach with audio recordings transcribed phonetically, or using LLMs to generate preliminary lexical items from audio data, would demonstrate whether the methodology can function with purely oral data sources.

### Open Question 3
- Question: How can evaluation metrics be improved to better assess grammatical accuracy for isolating languages?
- Basis in paper: [explicit] The authors identify that existing metrics like BLEU, ROUGE-L, METEOR, and chrF rely heavily on word forms and syntactic similarity, which poorly align with Moklen's structural characteristics, and suggest that even BERTScore falls short in fully capturing the subtleties of Moklen's grammar.
- Why unresolved: Current evaluation metrics are biased toward languages with explicit grammatical markers and inflectional morphology, making them inadequate for isolating languages where grammatical relationships are expressed through word order and content words.
- What evidence would resolve it: Developing and validating new evaluation metrics that focus on semantic coherence and functional structure rather than surface forms, then comparing these metrics' performance against human linguistic judgments for isolating languages, would establish more appropriate evaluation standards.

## Limitations
- The evaluation metrics used (BERTScore, BLEU, etc.) may not accurately capture grammatical quality for isolating languages like Moklen that lack morphological complexity
- English grammatical biases in the LLM can lead to generated rules that are not linguistically accurate for the target language
- The study relies on a relatively small parallel corpus and dictionary, which may not capture the full complexity of Moklen grammar

## Confidence
- **High confidence**: The model can successfully generate coherent grammatical rules and lexical entries from minimal data using in-context learning (supported by 86% lexical accuracy and improved translation scores)
- **Medium confidence**: The translation performance improvements (BERTScore F1 up to 0.7110) directly result from the generated grammar rather than other factors (metric limitations make this uncertain)
- **Medium confidence**: The dictionary-based tokenisation approach is effective for isolating languages (validated by successful tokenisation in experiments, but no comprehensive error analysis provided)

## Next Checks
1. **Expert linguistic validation**: Have a Moklen language expert review the generated XLE grammar rules and lexical entries to verify their linguistic accuracy and identify any English grammatical biases or hallucinations
2. **Cross-linguistic robustness test**: Apply the same methodology to a structurally different language (e.g., a polysynthetic language) to evaluate whether the English grammatical bias becomes more problematic and whether the approach generalizes beyond isolating languages
3. **Metric alignment study**: Compare the generated grammar quality against human-annotated gold standard grammars using linguistic acceptability judgments, and analyze the correlation between traditional MT metrics and actual grammatical correctness for isolating languages