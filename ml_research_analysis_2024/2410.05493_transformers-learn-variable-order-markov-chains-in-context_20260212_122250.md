---
ver: rpa2
title: Transformers learn variable-order Markov chains in-context
arxiv_id: '2410.05493'
source_url: https://arxiv.org/abs/2410.05493
tags:
- layer
- transformer
- layers
- probability
- compression
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates in-context learning (ICL) of variable-order
  Markov chains (VOMCs) by transformers. The authors view language modeling as data
  compression and compare transformers against CTW and PPM algorithms.
---

# Transformers learn variable-order Markov chains in-context

## Quick Facts
- arXiv ID: 2410.05493
- Source URL: https://arxiv.org/abs/2410.05493
- Authors: Ruida Zhou; Chao Tian; Suhas Diggavi
- Reference count: 40
- Key outcome: Transformers can learn variable-order Markov chains in-context through suffix copying and matching mechanisms, outperforming PPM and showing depth insensitivity

## Executive Summary
This paper investigates in-context learning (ICL) of variable-order Markov chains (VOMCs) by transformers, framing language modeling as data compression. The authors compare transformers against CTW and PPM algorithms, finding that transformers significantly outperform PPM and show minimal sensitivity to depth - even 2-layer transformers perform well. They analyze attention maps to identify two key mechanisms: suffix copying and suffix matching. The paper provides two transformer constructions: one with D+2 layers that mimics CTW for context trees of maximum order D, and a 2-layer transformer that uses the feed-forward network for probability blending. Synthetic transformer layers are implemented and shown to match or exceed original transformer performance while using fewer parameters.

## Method Summary
The paper uses transformers to learn VOMCs in-context by training on sequences generated from randomly sampled context trees (CTs) with maximum depth D. The transformer architecture consists of an embedding layer, L transformer layers (each with multi-head attention and feed-forward network), and an output layer. The authors implement synthetic transformer layers that mimic CTW functionality, including finite-memory context extension and statistics collection. Training uses AdamW optimizer with early stopping, batch size 512, and maximum 100 epochs. Performance is measured using compression rates (cross-entropy loss) on test CTs not seen during training. The paper analyzes attention maps to identify suffix copying and matching patterns that explain the transformer's success.

## Key Results
- Transformers can learn to compress VOMC in-context, significantly outperforming PPM algorithms
- Transformer performance is not sensitive to depth - even 2-layer transformers perform well on VOMC tasks
- Transformers trained on non-CTW priors can significantly outperform CTW algorithms when test data doesn't follow CTW priors
- Two key mechanisms identified: suffix copying (copying relevant suffixes into current position) and suffix matching (matching current suffixes to previously seen ones)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Transformers can learn to compress VOMCs in-context by leveraging suffix copying and suffix matching mechanisms
- Mechanism: The transformer uses attention mechanisms to identify and copy relevant suffixes from context (suffix copying) and match current suffixes to previously seen ones (suffix matching), enabling efficient probability estimation for next-token prediction
- Core assumption: The attention mechanism can effectively identify and utilize variable-length suffixes that are relevant for the current prediction context
- Evidence anchors:
  - [abstract] "To explain these observations, we analyze the attention map of the transformers and extract two mechanisms, on which we provide two transformer constructions: suffix copying and suffix matching"
  - [section 4.1] "One pattern is solely relative-position dependent... This pattern usually appears in the first or second layers of the transformers. Combining with the suffix structure in compression algorithms such as CTW, such an attention pattern suggests the suffix is being copied into the current query position for subsequent processing"
  - [corpus] Weak - no direct mention of suffix copying/matching in neighbor papers
- Break condition: If the attention mechanism fails to identify relevant suffixes or the context window is too short to contain necessary historical information

### Mechanism 2
- Claim: Transformers can outperform CTW algorithms when trained on non-CTW priors due to their ability to learn more flexible blending strategies
- Mechanism: Unlike CTW's rigid Bayesian blending based on Dirichlet priors, transformers learn adaptive blending coefficients through the feed-forward network that better match the actual distribution of the test data
- Core assumption: The training data distribution differs sufficiently from CTW priors to make the learned blending strategy more effective
- Evidence anchors:
  - [abstract] "Transformers trained and tested on non-CTW priors can significantly outperform the CTW algorithm"
  - [section 3.2] "When the CTs do not follow those priors, can learning-based transformers perform better than CTWs? We empirically observe that in such settings, transformers indeed have advantages"
  - [corpus] Missing - no neighbor papers discuss CTW priors or blending strategies
- Break condition: If the test data follows a CTW prior closely, or if the transformer architecture is too constrained to learn flexible blending

### Mechanism 3
- Claim: The insensitivity to depth (layer count) is due to the feed-forward network's ability to perform probability blending independently of layer depth
- Mechanism: The feed-forward network can approximate the blending coefficients needed for Bayesian optimal prediction, reducing the dependence on deep recursive structures like CTW
- Core assumption: The feed-forward network has sufficient capacity to approximate the blending function that would otherwise require multiple layers in a recursive construction
- Evidence anchors:
  - [abstract] "The performance of transformers is not very sensitive to the number of layers, and even a two-layer transformer can learn in-context quite well"
  - [section 4.3] "We conduct experiments on the hybrid versions of transformers... As shown in the right panel of Fig. 11, transformers with 2 total layers and 4 total layers form two clusters, which provides strong evidence that the constructed layers are indeed replacing the first two layers of the original transformers in a functional manner"
  - [corpus] Missing - no neighbor papers discuss layer depth insensitivity or FF network blending capabilities
- Break condition: If the blending function becomes too complex for the FF network to approximate, or if the context requires deeper recursive processing

## Foundational Learning

- Concept: Variable-order Markov chains (VOMCs) and context trees
  - Why needed here: The paper studies transformers' ability to learn VOMCs, which are more appropriate models for natural language than fixed-order Markov chains
  - Quick check question: What distinguishes a variable-order Markov chain from a fixed-order one, and why are context trees used to represent VOMCs?

- Concept: Bayesian universal coding and compression algorithms
  - Why needed here: The paper frames in-context learning as a compression problem, using CTW and PPM as baselines for comparison
  - Quick check question: How does the CTW algorithm achieve Bayesian optimality, and what role do the Dirichlet priors play in its performance?

- Concept: Attention mechanisms and transformer architecture
  - Why needed here: The paper analyzes attention maps to understand how transformers perform in-context learning of VOMCs
  - Quick check question: How do the multi-head attention and feed-forward layers work together to process sequential data in transformers?

## Architecture Onboarding

- Component map: Embedding layer -> L transformer layers (multi-head attention + feed-forward network) -> Output layer
- Critical path: Context embedding → Attention (suffix copying/matching) → Statistics collection → Probability blending → Next token prediction
- Design tradeoffs: Depth vs. width trade-off in transformers, with 2-layer transformers performing well due to effective FF network blending; CTW-like recursive structures vs. direct FF approximation
- Failure signatures: Poor compression rates indicating failure to capture VOMC structure; attention patterns not matching expected suffix copying/matching; over-reliance on fixed-order models (like PPM)
- First 3 experiments:
  1. Test transformer performance on VOMC with varying maximum orders (D=1,2,3) and compare against CTW baseline
  2. Compare 2-layer vs. 4-layer transformer performance on same VOMC to verify depth insensitivity
  3. Train transformers on non-CTW priors and test on same distribution to verify performance advantage over CTW

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the proposed transformer construction for CTW be simplified further while maintaining the same performance?
- Basis in paper: [explicit] The paper mentions that "the proposed transformer construction may not be the only way to mimic CTW" and that the first two layers "do capture important universal features"
- Why unresolved: The paper only provides one specific construction for approximating CTW with transformers. There could be more efficient or simpler architectures that achieve the same goal.
- What evidence would resolve it: Developing and testing alternative transformer architectures that can match CTW performance with fewer parameters or simpler designs.

### Open Question 2
- Question: How does the counting mechanism specifically contribute to the success of transformers in ICL-VOMC tasks?
- Basis in paper: [explicit] The paper states "The CTW algorithm relies heavily on counting the occurrences of suffixes of varying lengths to determine the blending coefficients" and that "a significant component of our transformer construction is for such counting"
- Why unresolved: While the paper identifies counting as important, it doesn't fully explain the precise role of counting statistics in transformer performance or how transformers extract and utilize these counts.
- What evidence would resolve it: Detailed analysis of how transformers compute and use counting statistics compared to CTW, potentially through ablation studies or attention mechanism analysis.

### Open Question 3
- Question: Can transformers trained on non-CTW priors consistently outperform CTW algorithms across different domains?
- Basis in paper: [explicit] The paper shows that "transformers trained and tested on non-CTW priors can significantly outperform the CTW algorithm" in their experiments
- Why unresolved: The experiments were limited to specific synthetic datasets. It's unclear if this advantage generalizes to real-world language data or other domains.
- What evidence would resolve it: Extensive testing on diverse real-world datasets and comparison with state-of-the-art compression algorithms beyond CTW.

## Limitations

- Theoretical gaps exist in the connection between observed attention patterns and actual implementation of CTW-like algorithms
- Architecture-specific claims may not generalize across different transformer variants (decoder-only, encoder-only, encoder-decoder)
- Experimental focus on synthetic VOMC data raises questions about generalization to natural language with more complex dependencies

## Confidence

**High Confidence**: The empirical observation that transformers outperform PPM on VOMCs and that this performance doesn't strongly depend on depth

**Medium Confidence**: The identification of suffix copying and matching mechanisms through attention analysis, though the mechanistic explanation remains incomplete

**Low Confidence**: The theoretical claims about why 2-layer transformers can match deeper ones, particularly the assertion that the feed-forward network can fully substitute for recursive CTW-like structures

## Next Checks

1. **Cross-Domain Validation**: Test the same transformer architectures on natural language datasets (like WikiText or PG-19) to verify whether the observed mechanisms (suffix copying/matching) and depth insensitivity generalize beyond synthetic VOMC data

2. **Architectural Ablation Study**: Systematically vary the feed-forward network size, attention head count, and layer normalization parameters to determine the minimum viable architecture that maintains the observed performance characteristics, particularly the depth insensitivity

3. **Theoretical Mechanistic Analysis**: Develop a formal connection between attention patterns and the actual computation being performed, potentially through circuit analysis or synthetic transformer constructions with verifiable behavior, to move from observational to mechanistic understanding of how transformers implement VOMC learning