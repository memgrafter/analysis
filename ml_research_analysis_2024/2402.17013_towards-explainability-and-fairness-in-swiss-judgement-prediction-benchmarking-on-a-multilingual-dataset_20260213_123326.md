---
ver: rpa2
title: 'Towards Explainability and Fairness in Swiss Judgement Prediction: Benchmarking
  on a Multilingual Dataset'
arxiv_id: '2402.17013'
source_url: https://arxiv.org/abs/2402.17013
tags:
- legal
- court
- prediction
- explainability
- lower
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a comprehensive evaluation of explainability
  and fairness in Legal Judgement Prediction (LJP) systems using the multilingual
  Swiss Judgement Prediction (SJP) dataset. The authors curate a novel dataset of
  108 cases with rationales annotated by legal experts, employing an occlusion-based
  approach to assess model explainability across four test sets with varying levels
  of occlusion.
---

# Towards Explainability and Fairness in Swiss Judgement Prediction: Benchmarking on a Multilingual Dataset

## Quick Facts
- arXiv ID: 2402.17013
- Source URL: https://arxiv.org/abs/2402.17013
- Authors: Santosh T. Y. S. S; Nina Baumgartner; Matthias Stürmer; Matthias Grabmair; Joel Niklaus
- Reference count: 0
- Primary result: Legal Judgement Prediction models show improved performance doesn't guarantee better explainability; lower court bias quantified via novel LCI framework

## Executive Summary
This paper evaluates explainability and fairness in Legal Judgement Prediction (LJP) systems using a multilingual Swiss dataset. The authors introduce a novel Lower Court Insertion (LCI) framework to quantify bias from lower court information and employ occlusion-based methods to assess model explainability. Their findings reveal that better prediction performance doesn't necessarily improve explainability, with some models showing significant reliance on legally irrelevant text. The study also identifies biases related to lower court information across different model architectures, with up to 5% changes in prediction confidence due to lower court modifications. Cross-lingual analysis shows varying bias patterns across German, French, and Italian, highlighting the need for continuous bias evaluation in LJP systems.

## Method Summary
The study evaluates LJP models on a multilingual Swiss dataset using hierarchical BERT architectures (monolingual, multilingual, and joint training variants). The authors curate a novel dataset of 108 cases with rationales annotated by legal experts, employing occlusion-based approaches to assess explainability across four test sets with varying occlusion levels. A novel Lower Court Insertion (LCI) framework quantifies lower court bias by measuring prediction confidence changes when replacing lower court mentions. Models are trained with learning rate 1e-5, batch size 64, and AdamW optimizer, evaluated using macro-F1 for prediction performance, F1-scores for explainability, and Mean Explainability Scores (MES) for fairness analysis.

## Key Results
- Improved prediction performance does not correspond to enhanced explainability across all models
- F1-scores for 'Opposes judgement' and 'Neutral' classes are lower compared to 'Supports judgement'
- LCI analysis shows up to 5% changes in prediction confidence due to lower court modifications, with some instances resulting in label flips
- Cross-lingual models exhibit varying biases across languages, with Italian showing significant divergence in MES scores

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Occlusion-based explainability scores reveal misalignment between model predictions and legal expert rationales.
- **Mechanism**: By systematically removing sentences labeled as supporting, opposing, or neutral from case facts and measuring prediction confidence changes, the model's reliance on legally relevant versus irrelevant text can be quantified.
- **Core assumption**: Legal experts' annotations accurately reflect legal relevance for predicting judgments.
- **Evidence anchors**:
  - [abstract]: "Our findings reveal that improved prediction performance does not necessarily correspond to enhanced explainability performance"
  - [section]: "We employ a perturbation-based explainability approach, namely Occlusion, wherein we remove factors from fact statements and measure change in prediction confidence"
- **Break condition**: If expert annotations are inconsistent or subjective, occlusion-based scores may not accurately reflect model alignment with legal relevance.

### Mechanism 2
- **Claim**: The Lower Court Insertion (LCI) framework quantifies lower court information influence on predictions, exposing potential biases.
- **Mechanism**: By replacing lower court mentions in case text with other lower court names and measuring resulting prediction confidence changes, the model's reliance on specific lower court information can be assessed.
- **Core assumption**: Lower court information significantly influences model predictions, and replacements reveal potential biases.
- **Evidence anchors**:
  - [abstract]: "we introduce a novel evaluation framework, Lower Court Insertion (LCI), which allows us to quantify the influence of lower court information on model predictions, exposing current models' biases"
  - [section]: "In their decisions, the FSCS often focuses on specific portions of previous decisions, scrutinizing potential flaws in the lower court's reasoning"
- **Break condition**: If lower court information has negligible influence on predictions or models are robust to court mention changes, LCI may not reveal significant biases.

### Mechanism 3
- **Claim**: Joint training of multilingual BERT models improves prediction performance for underrepresented languages like Italian but may introduce higher biases.
- **Mechanism**: Training a single multilingual BERT model on data from all three languages simultaneously allows leveraging shared linguistic features, potentially improving performance on underrepresented languages.
- **Core assumption**: Multilingual BERT models can effectively capture shared linguistic features across languages, and joint training improves performance without introducing significant biases.
- **Evidence anchors**:
  - [section]: "Joint training models, which aim to generalize across languages, demonstrate improved prediction performance, particularly for Italian, which is underrepresented in the training set"
- **Break condition**: If multilingual BERT models fail to capture shared linguistic features effectively, or joint training introduces significant biases, improved performance may not translate to better overall model performance.

## Foundational Learning

- **Concept**: Legal Judgment Prediction (LJP) task
  - **Why needed here**: Understanding LJP task components (predicting winning party, violated provisions, motion results) is crucial for interpreting study results and implications.
  - **Quick check question**: What are the main components of the LJP task, and how do they relate to court judgment prediction?

- **Concept**: Explainability and fairness in machine learning
  - **Why needed here**: The study focuses on assessing explainability and fairness of LJP models, so understanding these concepts and their importance in high-stakes domains like law is essential.
  - **Quick check question**: What are the key differences between explainability and fairness in machine learning, and why are they particularly important in legal applications?

- **Concept**: Multilingual NLP and cross-lingual transfer learning
  - **Why needed here**: The study uses a multilingual LJP dataset and evaluates monolingual and multilingual BERT models, so understanding challenges and techniques in multilingual NLP is crucial.
  - **Quick check question**: What are the main challenges in multilingual NLP, and how can cross-lingual transfer learning help address these challenges in LJP tasks?

## Architecture Onboarding

- **Component map**: Data preprocessing (tokenization, segmentation) -> Model training (BERT variants) -> Occlusion experiments -> LCI framework -> Evaluation (F1, MES, label flips)
- **Critical path**: Data preprocessing → Model training and evaluation → Explainability assessment → Fairness assessment → Analysis and interpretation
- **Design tradeoffs**:
  - Monolingual vs. multilingual vs. joint training models: Balancing language-specific performance with cross-lingual generalization
  - Occlusion test sets: Choosing number of occluded sentences to balance granularity and computational efficiency
  - LCI framework: Defining scope and granularity of lower court mentions to assess
- **Failure signatures**:
  - Low explainability scores: Model relies on legally irrelevant text or lacks alignment with expert rationales
  - High MES scores: Model exhibits biases related to lower court information
  - Inconsistent performance across languages: Model fails to generalize effectively across languages
- **First 3 experiments**:
  1. Train and evaluate monolingual BERT models for each language, assessing prediction performance and explainability using occlusion
  2. Train and evaluate a multilingual BERT model, comparing its performance and explainability to monolingual models
  3. Implement LCI framework to assess lower court bias in best-performing model from experiments 1 and 2

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does improved prediction performance translate to enhanced explainability in LJP models?
- Basis in paper: [explicit] The paper explicitly states "our findings reveal that improved prediction performance does not necessarily correspond to enhanced explainability performance"
- Why unresolved: The paper shows this is true for their specific dataset and models, but doesn't explore why this relationship doesn't hold or whether it might be different with other architectures or training approaches.
- What evidence would resolve it: Systematic experiments varying model architectures, training data sizes, and explainability methods to identify conditions under which prediction and explainability improvements correlate or diverge.

### Open Question 2
- Question: How does the underrepresentation of certain labels (like "Opposes Judgment") in the dataset affect model explainability performance?
- Basis in paper: [explicit] The paper notes "the 'Opposes Judgment' label has fewer instances, due to the lower number of annotated tokens associated with this label"
- Why unresolved: The paper observes lower F1-scores for underrepresented classes but doesn't systematically study whether this is due to true model limitations or dataset imbalance effects.
- What evidence would resolve it: Controlled experiments with balanced datasets across all labels and ablation studies to separate dataset effects from model capabilities.

### Open Question 3
- Question: Does the inclusion of cross-lingual training data introduce bias specific to language or court systems?
- Basis in paper: [inferred] The paper shows "Italian data shows a significant divergence in MES scores compared to German and French" despite joint training, suggesting language-specific biases.
- Why unresolved: While the paper identifies divergent bias patterns across languages, it doesn't investigate whether these stem from training data quality differences, linguistic features, or court system variations.
- What evidence would resolve it: Comparative analysis of bias patterns across jurisdictions with parallel case data in multiple languages, controlling for legal system differences.

## Limitations

- Small annotated rationale dataset (108 cases) limits statistical power for explainability analysis
- Specific focus on Swiss Federal Supreme Court decisions may limit generalizability to other legal systems
- Occlusion-based approach assumes expert annotations perfectly capture legal relevance, introducing potential subjectivity bias

## Confidence

- Performance-fairness trade-off finding (claim 1): **High confidence** - consistent results across multiple model architectures and metrics
- Cross-lingual bias claims (claim 4): **Medium confidence** - limited Italian training data (4K cases) may amplify observed disparities
- Lower court bias quantification: **Medium confidence** - effectiveness depends on assumption that lower court mentions significantly influence predictions

## Next Checks

1. **Replication with larger rationale corpus**: Expand annotated dataset to 500+ cases to validate explainability findings with higher statistical power and assess inter-annotator agreement rates.

2. **Cross-jurisdictional validation**: Test model architectures on non-Swiss legal datasets to determine if lower court bias and explainability-performance trade-offs generalize beyond the Swiss legal system.

3. **Temporal bias analysis**: Conduct year-by-year analysis of model performance and fairness metrics to identify potential shifts in model behavior over the 2000-2020 period that could indicate learning of temporal legal trends rather than genuine legal reasoning.