---
ver: rpa2
title: 'ExpressivityBench: Can LLMs Communicate Implicitly?'
arxiv_id: '2411.08010'
source_url: https://arxiv.org/abs/2411.08010
tags:
- confusion
- emotions
- llms
- matrix
- signals
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces ExpressivityBench, a framework to quantify
  how well large language models (LLMs) can implicitly convey information such as
  emotion, identity, or tone without explicit mention. The approach uses information-theoretic
  models and LLM-based graders validated against human judgments.
---

# ExpressivityBench: Can LLMs Communicate Implicitly?

## Quick Facts
- arXiv ID: 2411.08010
- Source URL: https://arxiv.org/abs/2411.08010
- Reference count: 40
- Key outcome: ExpressivityBench framework quantifies LLM implicit communication ability, showing models excel at affective content but struggle with sociolinguistic signals and perform worse than humans.

## Executive Summary
This paper introduces ExpressivityBench, a framework to measure how well large language models can implicitly convey information such as emotion, identity, or tone without explicit mention. Using information-theoretic models and LLM-based graders validated against human judgments, the study evaluates nine tasks across poetry, code generation, and conversational domains. Results show that while models achieve expressivity rates of 0.59-0.70 for poetry and 0.31-0.54 for code, they perform significantly worse than humans in maintaining implicit communication. The study provides reproducible benchmarks and highlights critical limitations for human-like implicit communication in current LLMs.

## Method Summary
The ExpressivityBench framework generates text conditioned on a target signal without explicit mention, then uses an LLM grader to identify the signal. The framework was validated against human judgments and tested across nine tasks including emotion, identity, and tone signals in poetry, code, and conversational contexts. Expressivity rates were measured as the proportion of correct signal identification, with experiments tracking changes over conversational turns and comparing performance across different LLM models.

## Key Results
- Poetry generation expressivity rates ranged from 0.59 to 0.70 for emotion and style signals
- Code generation expressivity was lower at 0.31 to 0.54 for skill level and paradigm signals
- Emotional expressivity declined while professional expressivity increased over conversational turns
- LLM graders showed comparable accuracy to humans but with systematic confusion between similar signals

## Why This Works (Mechanism)

### Mechanism 1
- Claim: ExpressivityArena quantifies implicit communication by measuring how often an LLM grader correctly infers a target signal from model-generated text.
- Mechanism: The framework generates text conditioned on a signal without explicit mention, then uses an LLM grader to guess the signal. Accuracy rates indicate expressivity.
- Core assumption: Graders (LLM or human) can accurately detect implicit signals without bias or performance drift over time.
- Evidence anchors: "We employ LLM-based graders validated against human judgments" and "We first established the validity of the grader with a human study."

### Mechanism 2
- Claim: Implicit expressivity is domain-dependent, with creative domains (poetry) yielding higher expressivity rates than logical ones (code).
- Mechanism: Poetry generation tasks show higher expressivity rates (0.59-0.70) than code generation (0.31-0.54) due to the inherently expressive nature of language versus structured syntax.
- Core assumption: Creative domains naturally support more varied and detectable implicit signals than logical domains.
- Evidence anchors: "Models tended to be less expressive while generating code than while generating poetry, suggesting that models perform worse in low-expressivity domains."

### Mechanism 3
- Claim: LLM expressivity in conversations changes over time, improving for profession signals but declining for emotion signals.
- Mechanism: Over successive conversational turns, models become more explicit in hinting at professions but more neutral in emotional expression, likely due to RLHF training toward context-appropriate responses.
- Core assumption: Emotional states are transient, while professions are semi-permanent, leading to different temporal dynamics in expressivity.
- Evidence anchors: "When LLMs are assigned professions as signals and engaged in conversations... the accuracy of the models increases over time."

## Foundational Learning

- Concept: Implicit vs. explicit communication
  - Why needed here: The framework measures implicit communication; understanding the distinction is essential to designing valid prompts and interpreting results.
  - Quick check question: Can you identify which of these two sentences conveys the same information implicitly: "The movie was too long" vs. "I kept checking my watch during the movie"?

- Concept: Information-theoretic communication models
  - Why needed here: The evaluation framework is based on quantifying how well information (the signal) is conveyed without explicit mention.
  - Quick check question: How would you measure the "expressivity rate" for a text that conveys an emotion without naming it?

- Concept: LLM-based evaluation and bias
  - Why needed here: Automated graders replace humans at scale, but may introduce bias; understanding this tradeoff is key to interpreting results.
  - Quick check question: What are potential biases in using an LLM grader for expressivity, and how might they differ from human graders?

## Architecture Onboarding

- Component map: User prompt → Domain + signal → LLM generation → Blind grader → Accuracy rate → Optional: Jury grader aggregation, human grader fallback, cosine distance metrics
- Critical path: 1. Define signal category and set 2. Generate model responses per signal 3. Validate grader accuracy with human baseline 4. Run experiments, collect accuracy rates
- Design tradeoffs: Speed vs. accuracy: LLM graders scale better but may miss subtle cues; Signal granularity: More signals → lower accuracy but richer analysis; Domain selection: Creative domains yield higher expressivity; logical domains test model limits
- Failure signatures: Consistently low accuracy across all signals (grader or generation failure); High confusion between semantically similar signals (semantic overlap); Accuracy drift over conversational turns (temporal dynamics)
- First 3 experiments: 1. Validate grader: Compare LLM vs. human accuracy on professions and emotions 2. Poetry expressivity: Measure emotion and style signal detection across models 3. Code expressivity: Test skill level and paradigm signal detection in generated Python code

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does bias in training data affect the expressivity of LLMs, particularly regarding underrepresented groups like gender or cultural background?
- Basis in paper: The paper notes that models frequently confused female poets, suggesting overgeneralization based on gender and underrepresentation in training data.
- Why unresolved: The study does not explore the broader impact of biases on expressivity beyond this specific example, nor does it propose methods to mitigate these biases.
- What evidence would resolve it: Additional experiments testing expressivity across diverse groups and comparing models trained on balanced vs. biased datasets.

### Open Question 2
- Question: Can expressivity be maintained in longer conversations, and does the type of signal (emotional vs. professional) influence this over extended interactions?
- Basis in paper: The paper observes that emotional expressivity declines over conversational turns, while professional expressivity increases, but does not explore this trend over longer interactions.
- Why unresolved: The experiments were limited to a small number of turns, leaving open questions about the scalability of these findings.
- What evidence would resolve it: Extended conversation simulations with more turns and diverse signal types to assess long-term expressivity trends.

### Open Question 3
- Question: How do different automated grading methods compare in evaluating expressivity, and do they introduce qualitatively different biases?
- Basis in paper: The study uses LLM-based graders validated against human judgments but does not explore the potential biases or limitations of automated graders in depth.
- Why unresolved: The comparison between human and automated graders is limited, and the study does not investigate how different grading schemas might affect results.
- What evidence would resolve it: Systematic comparisons of multiple grading methods, including human, automated, and hybrid approaches, to identify and quantify biases.

## Limitations

- Human vs. LLM grader validation reliability: The study lacks confidence intervals and inter-annotator agreement metrics to assess grader equivalence
- Temporal expressivity dynamics: The mechanism linking RLHF training to expressivity changes is speculative without controlled causal analysis
- Domain expressiveness assumptions: The claim that creative domains are inherently more expressive may oversimplify the evaluation of stylistic signals in logical domains

## Confidence

- High confidence: Domain-dependent expressivity differences (poetry > code) supported by multiple experimental comparisons
- Medium confidence: Grader validation process establishes reasonable baseline accuracy but lacks detailed reliability metrics
- Low confidence: Temporal dynamics of expressivity changes over conversations, particularly the causal mechanism linking RLHF training to declining emotional expressivity

## Next Checks

1. **Grader reliability assessment**: Replicate the human validation study with detailed inter-annotator agreement metrics (Cohen's kappa) and confidence intervals for both human and LLM grader accuracy across all signal categories.

2. **Temporal dynamics control experiment**: Conduct conversational experiments with controlled context lengths and signal repetition patterns to isolate whether expressivity changes are due to temporal factors versus conversation-specific dynamics.

3. **Code generation expressivity sensitivity analysis**: Redesign code generation prompts to explicitly encourage stylistic variation and paradigm adherence, then re-evaluate expressivity rates to determine whether low code expressivity reflects inherent domain limitations or evaluation method sensitivity.