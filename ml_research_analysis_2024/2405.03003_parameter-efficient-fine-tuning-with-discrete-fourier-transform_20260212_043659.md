---
ver: rpa2
title: Parameter-Efficient Fine-Tuning with Discrete Fourier Transform
arxiv_id: '2405.03003'
source_url: https://arxiv.org/abs/2405.03003
tags:
- lora
- fourierft
- arxiv
- fine-tuning
- fourier
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces FourierFT, a parameter-efficient fine-tuning
  method that leverages the expressive power of Fourier transform to compress trainable
  parameters for large foundation models. Instead of using low-rank matrices like
  LoRA, FourierFT treats the weight change as a matrix in the spatial domain and learns
  only a small fraction of its spectral coefficients, which are then used with inverse
  discrete Fourier transform to recover the weight change.
---

# Parameter-Efficient Fine-Tuning with Discrete Fourier Transform

## Quick Facts
- arXiv ID: 2405.03003
- Source URL: https://arxiv.org/abs/2405.03003
- Reference count: 37
- Achieves comparable performance to LoRA with 6.0%-9.4% of LoRA's trainable parameters

## Executive Summary
FourierFT is a parameter-efficient fine-tuning method that leverages the discrete Fourier transform to compress trainable parameters in large foundation models. Instead of using low-rank matrices like LoRA, FourierFT treats weight changes as spatial-domain matrices and learns sparse spectral coefficients that are reconstructed via inverse discrete Fourier transform. The method achieves comparable or better performance than LoRA across multiple tasks including natural language understanding, generation, instruction tuning, and image classification while using significantly fewer parameters.

## Method Summary
FourierFT learns sparse spectral coefficients of weight changes rather than learning full weight matrices or low-rank decompositions. The method constructs a spectral entry matrix E shared across all layers, learns coefficient vectors c per layer, and reconstructs weight changes using inverse discrete Fourier transform. During training, only the coefficient vectors are updated while E remains frozen. The approach uses fewer parameters than LoRA while maintaining comparable performance, with the orthogonal Fourier basis enabling efficient representation of weight changes.

## Key Results
- Achieves comparable performance to LoRA across GLUE benchmark tasks
- Uses only 0.064M trainable parameters vs LoRA's 33.5M on LLaMA2-7B instruction tuning
- Outperforms LoRA by 2.7% absolute on MRPC while using 9.4% of LoRA's parameters
- Demonstrates consistent performance improvements with increasing number of spectral coefficients

## Why This Works (Mechanism)

### Mechanism 1
FourierFT treats weight change matrices as spatial-domain objects and learns sparse spectral coefficients via discrete Fourier transform. Instead of learning full matrices A and B as in LoRA, FourierFT samples n spectral entries shared across layers, learns n coefficients per layer, and reconstructs the weight change through inverse discrete Fourier transform. This works because weight changes have an informative representation in the frequency domain that can be captured by sparse spectral coefficients.

### Mechanism 2
The orthogonal and expressive nature of the Fourier basis allows recovery of informative weight changes with fewer parameters than low-rank matrices. The Fourier basis provides an efficient parameterization for weight changes, leveraging its orthogonality and expressiveness to capture information more effectively than low-rank decomposition structures.

### Mechanism 3
Sharing spectral entry indices across layers reduces parameter storage requirements while maintaining performance. A single randomly initialized spectral entry matrix E is shared across all layers, with only the coefficient vectors c varying per layer. This works because the same spectral positions tend to be informative across different layers of the model.

## Foundational Learning

- **Concept**: Discrete Fourier Transform (DFT) and its inverse (IDFT)
  - **Why needed here**: The method relies on converting weight changes between spatial and frequency domains using DFT/IDFT
  - **Quick check question**: What is the computational complexity of a 2D DFT on a d×d matrix, and how does it compare to matrix multiplication?

- **Concept**: Low-rank matrix approximation and its limitations
  - **Why needed here**: FourierFT is positioned as an alternative to LoRA's low-rank approach, so understanding LoRA's mechanism is crucial
  - **Quick check question**: For a d×d weight matrix, how many parameters does LoRA use with rank r, and how does this compare to FourierFT with n coefficients?

- **Concept**: Parameter-efficient fine-tuning (PEFT) in large language models
  - **Why needed here**: The method is evaluated in the context of PEFT, requiring understanding of the broader landscape
  - **Quick check question**: What are the main categories of PEFT methods, and how does FourierFT fit within them?

## Architecture Onboarding

- **Component map**: Pre-trained base model -> FourierFT adapter modules -> Spectral entry matrix E (shared) -> Coefficient vectors c (per layer) -> IDFT operation -> Weight change reconstruction

- **Critical path**: Forward pass computes ∆W = IDFT(E, c) × α, then h = W0x + ∆W x

- **Design tradeoffs**:
  - Parameter efficiency vs. expressivity: Fewer parameters than LoRA but potentially less flexible than full fine-tuning
  - Shared spectral entries vs. layer-specific: Reduces parameters but may limit layer-specific adaptation
  - Random vs. frequency-biased sampling: Random is more universal but frequency-biased might capture specific patterns

- **Failure signatures**:
  - Training loss plateaus early with poor validation performance
  - Weight changes are too small to affect model outputs
  - Model diverges during training due to unstable weight updates

- **First 3 experiments**:
  1. Replicate GLUE benchmark results with RoBERTa Base to verify parameter efficiency claims
  2. Compare training curves of FourierFT vs LoRA on MRPC with matched parameter counts
  3. Test different frequency biases (no bias, low, middle, high) on RTE to validate frequency sampling strategy

## Open Questions the Paper Calls Out

### Open Question 1
How does the choice of spectral entry initialization (e.g., random vs. frequency-biased sampling) affect the performance and convergence of FourierFT across different model scales and tasks? While the paper shows that no frequency bias often works well, it doesn't systematically explore the impact of different initialization strategies across diverse tasks and model sizes.

### Open Question 2
What is the relationship between the number of trainable spectral coefficients (n) and model performance, and how does this relationship differ from the relationship between rank (r) and performance in LoRA? The paper demonstrates that increasing n consistently improves performance while increasing r in LoRA doesn't always improve performance, but lacks theoretical understanding of this difference.

### Open Question 3
How does the computational efficiency of FourierFT compare to LoRA during both training and inference, particularly for extremely large models where parameter efficiency becomes critical? The paper emphasizes parameter efficiency but doesn't provide detailed computational complexity analysis or runtime comparisons between the methods.

### Open Question 4
Can the Fourier basis used in FourierFT be generalized or extended to other parameter-efficient fine-tuning methods beyond weight-based adaptation? The paper demonstrates success with weight-based fine-tuning but doesn't explore whether the spectral coefficient approach could be applied to adapter-based methods, prompt tuning, or other PEFT paradigms.

## Limitations

- The paper lacks direct comparisons with other Fourier-based PEFT methods like Quantum-PEFT or SSH
- Evaluation primarily focuses on English language tasks and image classification, with limited testing on multilingual or multimodal scenarios
- Performance claims rely heavily on comparisons with LoRA, but the effect sizes vary significantly across tasks

## Confidence

**High confidence**: Claims about parameter efficiency gains (6.0%-9.4% of LoRA parameters) are well-supported by ablation studies and multiple task evaluations. The mechanism of using inverse discrete Fourier transform to reconstruct weight changes is clearly explained and theoretically sound.

**Medium confidence**: Performance parity or improvement over LoRA is demonstrated across multiple tasks, but the effect sizes vary significantly. The 2.7% absolute improvement on MRPC is notable, while the 0.1% improvement on RACE suggests diminishing returns on larger tasks.

**Low confidence**: Claims about the superiority of the Fourier basis over other representations are weakly supported. The paper asserts that "the orthogonal and expressive Fourier basis enables recovery of informative weight changes" but provides limited empirical evidence comparing FourierFT against other basis functions or parameterization schemes.

## Next Checks

1. **Cross-Architecture Validation**: Test FourierFT on architectures beyond RoBERTa and LLaMA2 (e.g., DeBERTa, GPT-Neo) to verify the method's generalizability across different model families and attention mechanisms.

2. **Frequency Bias Sensitivity**: Conduct a comprehensive ablation study varying the frequency bias across all tasks, including non-English language tasks and multimodal datasets, to determine if certain frequency ranges are consistently more effective for specific domains.

3. **Fourier vs. Alternative Bases**: Implement a variant using DCT or wavelet transforms instead of DFT to test whether the Fourier basis is uniquely advantageous or if other orthogonal bases could achieve similar performance.