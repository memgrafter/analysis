---
ver: rpa2
title: Consistent Document-Level Relation Extraction via Counterfactuals
arxiv_id: '2407.06699'
source_url: https://arxiv.org/abs/2407.06699
tags:
- entity
- counterfactual
- data
- factual
- relation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a counterfactual data generation method for
  document-level relation extraction (RE) to address factual bias in RE models. The
  method, called COVERED, replaces entities in documents with suitable alternatives
  to create counterfactual documents that retain minimal factual alignment.
---

# Consistent Document-Level Relation Extraction via Counterfactuals

## Quick Facts
- arXiv ID: 2407.06699
- Source URL: https://arxiv.org/abs/2407.06699
- Reference count: 23
- Models trained on factual data show inconsistent behavior, accurately extracting triples from factual data but failing to extract the same triples after counterfactual modification.

## Executive Summary
This paper addresses factual bias in document-level relation extraction (DocRE) models, which rely on spurious signals like specific entities and external knowledge rather than input context. The authors introduce COVERED, a counterfactual data generation method that replaces entities in documents with semantically similar alternatives to create counterfactual documents with minimal factual alignment. When applied to Re-DocRED, this approach produces RE-DOCRED-CF. Training models on this counterfactual data improves consistency with minimal impact on RE performance, achieving 88.3% consistency (up from 68.6%) while maintaining an F1 score of 76.3% compared to 78.0% for factual-only training.

## Method Summary
The COVERED method generates counterfactual documents by replacing entities in the original dataset with suitable alternatives based on relation maps and context snippets. The approach uses entity embeddings and co-occurrence patterns to find semantically similar replacements that maintain contextual plausibility while changing factual content. Models are then trained using the KD-DocRE framework on a mix of factual and counterfactual data, with adaptive focal loss and knowledge distillation. Consistency is measured by comparing model predictions across factual and counterfactual document pairs.

## Key Results
- Models trained on factual data show inconsistent behavior, accurately extracting triples from factual data but failing on counterfactual modifications of the same documents
- Training on counterfactual data improves consistency from 68.6% to 88.3% while maintaining performance (F1 drops only from 78.0% to 76.3%)
- The inconsistency suggests models rely on entity-specific patterns and external knowledge rather than contextual understanding

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Models trained on factual data rely on entity-specific and factual knowledge rather than the input context for relation extraction.
- Mechanism: The model learns to associate specific entity pairs with their relationships from the training data, creating a spurious correlation between entities and relations. When entities are replaced with semantically similar but factually different ones, the model fails to extract the correct relation because it no longer recognizes the entity pair.
- Core assumption: The model has not learned to extract relations based on contextual understanding but rather on entity co-occurrence patterns and external knowledge.
- Evidence anchors:
  - [abstract] "models trained on factual data rely on spurious signals such as specific entities and external knowledge – rather than on the input context – to extract triples."
  - [section] "This inconsistency suggests that models trained on factual data rely on spurious signals such as specific entities and external knowledge – rather than on the input context – to extract triples."
  - [corpus] Weak - no direct evidence from corpus neighbors.
- Break condition: If the model learns to extract relations based on context rather than entity patterns, the mechanism would fail. This could happen with sufficient context-focused training or architectural changes that emphasize contextual understanding.

### Mechanism 2
- Claim: Training on counterfactual data improves consistency by forcing the model to rely on context rather than entity patterns.
- Mechanism: By exposing the model to documents where entities have been replaced with semantically similar alternatives, the model learns that entity identity alone is insufficient for relation extraction. The model must therefore develop a deeper understanding of the context to make accurate predictions across both factual and counterfactual documents.
- Core assumption: The counterfactual documents maintain sufficient contextual similarity to the original documents while changing the factual content, allowing the model to learn context-based reasoning.
- Evidence anchors:
  - [abstract] "We show that by generating document-level counterfactual data with COVERED and training models on them, consistency is maintained with minimal impact on RE performance."
  - [section] "We show that by generating document-level counterfactual data with COVERED and training models on them, consistency is maintained with minimal impact on RE performance."
  - [corpus] Weak - no direct evidence from corpus neighbors.
- Break condition: If the counterfactual documents are too dissimilar from the original documents, the model may not transfer learning effectively. Additionally, if the replacement entities are not sufficiently similar, the model may not learn the intended lesson.

### Mechanism 3
- Claim: Mixing factual and counterfactual training data achieves both high performance and high consistency.
- Mechanism: The factual data maintains the model's ability to perform well on real-world data, while the counterfactual data improves robustness against entity and factual biases. This balanced approach ensures the model can extract relations accurately from both types of data.
- Core assumption: The model can effectively learn from both factual and counterfactual data simultaneously without one type overwhelming the other.
- Evidence anchors:
  - [abstract] "We show that by generating document-level counterfactual data with COVERED and training models on them, consistency is maintained with minimal impact on RE performance."
  - [section] "the resulting model shows both a high performance with minimum drop in F1 (only -1.7, 76.3 vs 78.0) while also being consistent (88.3% vs 68.6% for the 'factual-training-only' model)."
  - [corpus] Weak - no direct evidence from corpus neighbors.
- Break condition: If the ratio of factual to counterfactual data is not optimal, the model may either retain too much bias (too much factual data) or lose performance (too much counterfactual data).

## Foundational Learning

- Concept: Counterfactual data generation
  - Why needed here: To create training examples that test whether the model relies on entities or context for relation extraction
  - Quick check question: What are the key criteria for selecting replacement entities in the counterfactual generation process?

- Concept: Consistency measurement
  - Why needed here: To quantify how well the model performs across both factual and counterfactual data, indicating whether it relies on spurious patterns
  - Quick check question: How is pairwise consistency calculated in the context of relation extraction?

- Concept: Entity replacement with semantic similarity
  - Why needed here: To ensure counterfactual documents remain plausible while changing the factual content, allowing for effective bias testing
  - Quick check question: What features are used to determine if a replacement entity is suitable for a given entity in the original document?

## Architecture Onboarding

- Component map:
  - Entity replacement pipeline (COVERED) -> Document-level relation extraction model (RoBERTa-large with KD-DocRE) -> Consistency evaluation system -> Counterfactual dataset generation system

- Critical path:
  1. Entity replacement and counterfactual document generation
  2. Training the relation extraction model on mixed factual/counterfactual data
  3. Evaluating model performance and consistency on test sets
  4. Analyzing failure cases to understand remaining biases

- Design tradeoffs:
  - Entity similarity threshold: Higher thresholds produce more realistic counterfactuals but may reduce the diversity of counterfactual examples
  - Training data ratio: More counterfactual data improves consistency but may reduce performance on factual data
  - Replacement scope: Replacing more entities per document creates stronger counterfactuals but may make them less realistic

- Failure signatures:
  - High performance on factual data but low consistency indicates entity/factual bias
  - Low performance on both factual and counterfactual data suggests issues with the core extraction model
  - Very high consistency but low performance may indicate the counterfactual data is too dissimilar from real data

- First 3 experiments:
  1. Train on factual data only and measure consistency on counterfactual test set
  2. Train on counterfactual data only and measure performance on factual test set
  3. Train on mixed factual/counterfactual data and compare both performance and consistency to the previous experiments

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the COVERED method handle cases where suitable entity replacements cannot be found that meet the similarity thresholds?
- Basis in paper: [explicit] The paper mentions that entities are only replaced if they have similar relation maps and similar context snippets, but doesn't address what happens when no suitable replacements exist.
- Why unresolved: The paper doesn't discuss fallback strategies or how the method handles entities with unique relations or contexts that have no good replacements.
- What evidence would resolve it: Experiments showing model performance with and without entities that couldn't be replaced, or documentation of how such cases are handled in the implementation.

### Open Question 2
- Question: What is the impact of COVERED on document-level RE models for languages other than English?
- Basis in paper: [inferred] The paper only demonstrates results on English-language data and mentions the need for a seed DocRE dataset to extend the approach to other languages.
- Why unresolved: The paper doesn't test or discuss cross-lingual applicability of the counterfactual generation method or its effectiveness on non-English documents.
- What evidence would resolve it: Results from applying COVERED to document-level RE datasets in other languages, showing consistency improvements and performance metrics.

### Open Question 3
- Question: How does the number of counterfactual documents generated per original document affect model performance and consistency?
- Basis in paper: [explicit] The paper mentions that multiple counterfactual datasets are generated from Re-DocRED train, but doesn't analyze the relationship between quantity of counterfactuals and model robustness.
- Why unresolved: The paper doesn't explore whether there's an optimal number of counterfactual examples needed per document to achieve maximum consistency improvements.
- What evidence would resolve it: Experiments varying the number of counterfactual documents generated per original document and measuring the resulting model consistency and performance.

## Limitations
- Entity Replacement Quality: The effectiveness of the COVERED method heavily depends on the quality of entity replacements, which may not eliminate all factual bias
- Dataset Specificity: Experiments are conducted exclusively on Re-DocRED, limiting generalizability to other document-level RE datasets
- Performance-Consistency Trade-off: The method still shows room for improvement in achieving both high performance and perfect consistency

## Confidence
- High Confidence: The observation that models trained on factual data show inconsistent behavior when tested on counterfactual data is well-supported by experimental results
- Medium Confidence: The claim that training on counterfactual data improves consistency while maintaining performance is supported but could benefit from more detailed mechanism validation
- Medium Confidence: The assertion that inconsistency stems from reliance on spurious signals rather than contextual understanding is logically sound but lacks direct behavioral evidence

## Next Checks
1. **Ablation Study on Entity Replacement Criteria**: Conduct experiments varying the criteria for entity replacement (e.g., strict vs. relaxed semantic similarity thresholds) to quantify how replacement quality affects both the generation of effective counterfactuals and the resulting model consistency.

2. **Cross-Dataset Generalization Test**: Apply the COVERED method and training procedure to a different document-level RE dataset (such as DocRED or another domain-specific dataset) to evaluate whether the consistency improvements generalize beyond Re-DocRED.

3. **Layer-wise Analysis of Model Behavior**: Perform a detailed analysis of how different layers of the RoBERTa model respond to factual vs. counterfactual inputs during training using attention visualization, probing classifiers, or feature attribution methods to identify where and how the model learns to distinguish between factual and counterfactual patterns.