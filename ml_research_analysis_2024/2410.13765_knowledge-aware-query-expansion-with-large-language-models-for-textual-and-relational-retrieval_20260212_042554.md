---
ver: rpa2
title: Knowledge-Aware Query Expansion with Large Language Models for Textual and
  Relational Retrieval
arxiv_id: '2410.13765'
source_url: https://arxiv.org/abs/2410.13765
tags:
- query
- retrieval
- document
- textual
- expansion
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a knowledge-aware query expansion framework
  that augments large language models with structured document relations from knowledge
  graphs to improve retrieval accuracy for complex semi-structured queries. The method
  extracts entities from queries, retrieves associated textual documents, and filters
  relevant document relations using document-based relation filtering rather than
  entity-based scoring.
---

# Knowledge-Aware Query Expansion with Large Language Models for Textual and Relational Retrieval

## Quick Facts
- arXiv ID: 2410.13765
- Source URL: https://arxiv.org/abs/2410.13765
- Reference count: 16
- Key outcome: Outperforms state-of-the-art query expansion baselines, achieving up to 61.29 MRR on MAG and 39.22 MRR on PRIME

## Executive Summary
This paper proposes KAR (Knowledge-Aware query Expansion with large language models for textual and relational retrieval), a novel framework that augments LLMs with structured document relations from knowledge graphs to improve retrieval accuracy for complex semi-structured queries. The method extracts entities from queries, retrieves associated textual documents, and filters relevant document relations using document-based relation filtering rather than entity-based scoring. Extensive experiments on three datasets (Amazon, MAG, PRIME) demonstrate that KAR significantly outperforms state-of-the-art query expansion baselines, achieving up to 61.29 MRR on MAG and 39.22 MRR on PRIME, while demonstrating scalability and flexibility across different retrievers and LLMs.

## Method Summary
KAR is a knowledge-aware query expansion framework that leverages document texts as rich KG node representations and uses document-based relation filtering. The method extracts entities from queries using an LLM, retrieves associated textual documents, and extracts h-hop neighbors from the knowledge graph. Unlike entity-based filtering, KAR computes semantic similarity between query embeddings and document embeddings for each neighbor, selecting top-k neighbors for expansion. It constructs document-based knowledge triples and uses them to generate query expansions that are semantically similar and structurally related to user intents. The zero-shot nature of KAR allows flexibility across different retrievers and LLMs without requiring additional training.

## Key Results
- Achieves up to 61.29 MRR on MAG dataset, outperforming baselines by 10.51 MRR
- Achieves 39.22 MRR on PRIME dataset, outperforming baselines by 11.65 MRR
- Shows scalability across different retrievers and LLMs with consistent performance improvements

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Document-based relation filtering improves query expansion quality by incorporating rich textual context from KG nodes
- Mechanism: Instead of filtering relations based solely on entity names, the method embeds document texts associated with KG nodes and computes semantic similarity with the query, selecting top-k neighbors for expansion
- Core assumption: Document texts provide richer semantic representations of KG nodes than entity names alone
- Evidence anchors:
  - [abstract]: "we leverage document texts as rich KG node representations and use document-based relation filtering"
  - [section]: "we embed the textual document dj for each neighbor node with a text embedding model as xj = Embed (dj)"
  - [corpus]: "Average neighbor FMR=0.472" - weak correlation between corpus similarity and actual relevance
- Break condition: If document texts are too sparse or noisy, the semantic similarity scores become unreliable and may introduce irrelevant relations

### Mechanism 2
- Claim: Incorporating both textual documents and KG relations enables handling semi-structured queries with textual and relational requirements
- Mechanism: The method combines textual document information with structured relation information from KG to generate query expansions that are semantically similar and structurally related to user intents
- Core assumption: Complex search queries often contain both textual requirements (e.g., "highly rated") and relational requirements (e.g., "compatible with Nikon F-Mount lenses")
- Evidence anchors:
  - [abstract]: "handle such semi-structured queries with both textual and relational requirements"
  - [section]: "documents are often inter-connected with certain types of relations...Both textual and relational details are often queried by users"
  - [corpus]: No direct corpus evidence for this specific mechanism
- Break condition: If the KG relations are too sparse or if the textual documents don't contain sufficient relational information, the method cannot generate meaningful expansions

### Mechanism 3
- Claim: Zero-shot nature of the method allows flexibility across different retrievers and LLMs without requiring additional training
- Mechanism: The method uses document texts as KG node representations and requires no model finetuning when new documents are added to the knowledge base
- Core assumption: Off-the-shelf text embedding models and LLMs can effectively process the document-based knowledge triples without domain-specific training
- Evidence anchors:
  - [abstract]: "Since our method utilizes document texts for KG node representations and thus requires no additional model finetuning, it is scalable and flexible"
  - [section]: "Since our document-based relation filtering uses an off-the-shelf text embedding model, it does not requires any re-training"
  - [corpus]: No direct corpus evidence for this specific mechanism
- Break condition: If the domain requires highly specialized knowledge that off-the-shelf models cannot capture, the zero-shot approach may underperform trained methods

## Foundational Learning

- Concept: Knowledge Graph (KG) structure and traversal
  - Why needed here: The method relies on extracting h-hop neighbors from KG and filtering them based on document-based similarity scores
  - Quick check question: What is the difference between entity-based and document-based relation filtering in KG query answering?

- Concept: Dense passage retrieval and embedding-based similarity
  - Why needed here: The method uses text embedding models to compute similarity between query embeddings and document embeddings for relation filtering
  - Quick check question: How does dot product similarity differ from cosine similarity in the context of embedding-based retrieval?

- Concept: Large Language Model prompting and generation
  - Why needed here: The method uses LLMs to parse entities from queries, generate query expansions, and process document-based knowledge triples
  - Quick check question: What are the key considerations when designing prompts for LLMs to generate query expansions from structured knowledge?

## Architecture Onboarding

- Component map:
  Entity Parsing Module -> Document Retrieval Module -> KG Relation Propagation Module -> Document-based Relation Filtering Module -> Document Triples Construction Module -> Knowledge-Aware Expansion Module -> Final Retrieval Module

- Critical path: Entity Parsing → Document Retrieval → KG Relation Propagation → Document-based Relation Filtering → Document Triples Construction → Knowledge-Aware Expansion → Final Retrieval

- Design tradeoffs:
  - Choosing h-hop neighbors balances completeness vs. computational efficiency
  - Selecting top-k neighbors trades off recall for precision in relation filtering
  - Using zero-shot approach sacrifices potential performance gains from domain-specific training
  - Sampling multiple expansions increases diversity but also latency

- Failure signatures:
  - Poor entity parsing leading to incorrect document retrieval
  - Noisy or sparse KG relations resulting in irrelevant expansions
  - Document-based filtering selecting too few or too many neighbors
  - LLM generating expansions that don't align with user intent

- First 3 experiments:
  1. Test entity parsing accuracy on a sample of queries to ensure correct document retrieval
  2. Validate document-based relation filtering by checking if top-k neighbors are semantically relevant to the query
  3. Evaluate expansion quality by comparing generated expansions against ground truth expansions for sample queries

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the document-based relation filtering (DRF) in KAR compare to entity-based filtering when entities have limited textual information available?
- Basis in paper: [inferred] The paper mentions that entity-based methods overlook rich textual details, suggesting DRF would be more effective, but doesn't directly compare performance when textual information is sparse.
- Why unresolved: The paper focuses on scenarios where entities have rich textual descriptions but doesn't explore edge cases where entity nodes have minimal associated text.
- What evidence would resolve it: Empirical comparison of KAR's DRF versus entity-based filtering on datasets with entities that have varying amounts of textual information.

### Open Question 2
- Question: What is the impact of different text embedding models on KAR's performance across various knowledge graph structures?
- Basis in paper: [explicit] The paper states that KAR uses an off-the-shelf text embedding model and is flexible, but doesn't experiment with different embedding models.
- Why unresolved: While the paper demonstrates effectiveness with OpenAI text-embedding-ada-002, it doesn't explore how alternative embedding models might affect retrieval accuracy.
- What evidence would resolve it: Systematic evaluation of KAR using different text embedding models (e.g., sentence transformers, graph-specific embeddings) across the three datasets.

### Open Question 3
- Question: How does KAR scale to knowledge graphs with different densities and relation types beyond the three evaluated datasets?
- Basis in paper: [explicit] The paper shows KAR works on Amazon (dense text), MAG (dense relations), and PRIME (very dense relations) but doesn't explore other KG structures.
- Why unresolved: The experiments are limited to specific domains and relation patterns, leaving questions about performance on KGs with different characteristics.
- What evidence would resolve it: Evaluation of KAR on KGs with varying relation types (temporal, spatial, hierarchical) and densities outside the product, academic, and biomedical domains.

## Limitations

- The method's performance relies heavily on the quality and completeness of KG relations and associated document texts
- The zero-shot nature of the method may underperform in domains requiring highly specialized knowledge that off-the-shelf models cannot capture
- Limited empirical validation of the assumption that document texts provide richer semantic representations than entity names alone

## Confidence

- Low Confidence Claims:
  - The effectiveness of document-based relation filtering assumes that document texts provide richer semantic representations than entity names alone, with only weak correlation evidence (Average neighbor FMR=0.472)

- Medium Confidence Claims:
  - The zero-shot nature of the method is presented as a key advantage, but lacks ablation studies comparing zero-shot performance against finetuned approaches

- High Confidence Claims:
  - The overall retrieval performance improvements over baseline methods are well-supported by extensive experiments across three different datasets with multiple evaluation metrics

## Next Checks

1. **Entity Parsing Validation**: Test the LLM-based entity parsing module on a sample of 50 queries to measure accuracy and identify failure patterns. This directly validates whether the critical first step correctly identifies entities for subsequent document retrieval.

2. **Relation Filtering Ablation Study**: Compare document-based relation filtering against entity-based filtering using the same KG and documents to quantify the actual contribution of the document-based approach to expansion quality.

3. **Zero-shot vs. Finetuned Comparison**: Implement a domain-specific finetuning approach for one dataset (e.g., MAG) and compare performance against the zero-shot method to validate the claimed flexibility advantage.