---
ver: rpa2
title: 'LocMoE: A Low-Overhead MoE for Large Language Model Training'
arxiv_id: '2401.13920'
source_url: https://arxiv.org/abs/2401.13920
tags:
- expert
- experts
- communication
- tokens
- figure
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces LocMoE, a routing strategy and communication
  optimization scheme for Mixture-of-Experts (MoE) models. The key contributions are:
  (1) A locality-based expert regularization that combines load balance with intra-node
  communication, reducing inter-node communication overhead; (2) A theoretical proof
  for the minimum expert capacity threshold, calculated using the maximal angular
  deviation between gating weights and assigned tokens; (3) An orthogonal gating weight
  mechanism using Grouped Average Pooling (GrAP) layer for efficient feature extraction
  and explicit expert decisions.'
---

# LocMoE: A Low-Overhead MoE for Large Language Model Training

## Quick Facts
- arXiv ID: 2401.13920
- Source URL: https://arxiv.org/abs/2401.13920
- Reference count: 11
- Key outcome: Reduces training time per epoch by 12.68% to 22.24% compared to classical routers without impacting model accuracy

## Executive Summary
LocMoE introduces a novel routing strategy and communication optimization scheme for Mixture-of-Experts (MoE) models designed to reduce training overhead while maintaining accuracy. The method combines locality-based expert regularization with orthogonal gating weights and a theoretical proof for minimum expert capacity thresholds. Evaluated on the PanGu-Σ model using Ascend clusters, LocMoE demonstrates significant performance improvements by transforming inter-node communication into intra-node communication and reducing redundant computation through capacity optimization.

## Method Summary
LocMoE employs three key mechanisms: (1) orthogonal gating weights using Grouped Average Pooling (GrAP) layers that simplify routing decisions while reducing computational overhead, (2) locality-based expert regularization that encourages routing to local experts to minimize inter-node communication, and (3) a theoretical framework for determining minimum expert capacity thresholds based on angular deviation between gating weights and assigned tokens. The method is implemented in the PanGu-Σ model using the MindSpore framework and trained on Ascend clusters with 64, 128, and 256 NPUs. Training optimization includes group-wise All-to-All operations and communication overlap to further reduce overhead.

## Key Results
- Training time per epoch reduced by 12.68% to 22.24% compared to hash router and switch router baselines
- Model accuracy maintained while achieving significant computational efficiency gains
- Performance validated across 64, 128, and 256 Ascend 910A NPUs with consistent improvements
- Effective on domain-specific mobile network operator service materials

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The orthogonal gating weight mechanism using Grouped Average Pooling (GrAP) layer reduces computational overhead while facilitating explicit routing decisions.
- Mechanism: By replacing the dense layer with a GrAP layer, the gating weights become orthogonal, which means tokens with different semantics are naturally routed to different experts. This orthogonality simplifies the routing decision-making process and reduces computation.
- Core assumption: Orthogonal gating weights lead to more explicit and efficient routing decisions.
- Evidence anchors:
  - [abstract]: "Orthogonal gating weight with Grouped Average Pooling (GrAP) layer. The GrAP layer is adopted in gating values computation... the orthogonality of gating weight facilitates the explicit decisions of the router."
  - [section]: "The gating value is obtained via the GrAP layer in- stead of the dense layer... The feature extraction with the GrAP layer is delineated in Figure 4. It can be regarded as the dense layer with the fixed weight ωi... From a perspective of semantic, irrelevant tokens are inclined to be routed to experts of different domains, which is conducive to convergence and accuracy."
- Break condition: If the input data distribution does not benefit from orthogonal feature extraction, the GrAP layer might not provide significant advantages over dense layers.

### Mechanism 2
- Claim: Locality-based expert regularization transforms partial inter-node communication into intra-node communication, reducing latency and improving bandwidth utilization.
- Mechanism: By adding a locality loss term to the training objective, the model is encouraged to route tokens to local experts whenever possible. This reduces the need for inter-node communication during the All-to-All operation, as more data can be exchanged within a node.
- Core assumption: Intra-node communication has higher bandwidth and lower latency than inter-node communication.
- Evidence anchors:
  - [abstract]: "A locality-based expert regularization that combines load balance with intra-node communication, reducing inter-node communication overhead."
  - [section]: "The second part is locality loss, in line with the expectation that tokens are more likely to be assigned to local experts under the premise of load balance... The local experts are encouraged to compete with skilled experts, and the time consumption of communication is reduced while avoiding the under-training of some experts."
- Break condition: If the number of experts per node is insufficient or the data distribution is such that locality constraints cannot be met, the benefits of this mechanism may be limited.

### Mechanism 3
- Claim: The minimum expert capacity threshold ensures that each expert receives enough tokens to be effectively trained, preventing under-training while reducing redundant computation.
- Mechanism: By calculating the critical value of expert capacity based on the maximum angular deviation between gating weights and assigned tokens, the model can operate with the minimum necessary capacity. This prevents over-provisioning of expert capacity while ensuring adequate training.
- Core assumption: There exists a lower bound on the number of tokens each expert needs to process for effective training.
- Evidence anchors:
  - [abstract]: "Notably, we elucidate that there is a minimum threshold for expert capacity, calculated through the maximal angular deviation between the gating weights of the experts and the assigned tokens."
  - [section]: "Theorem 1(Lower Bound of Expert Capacity)... The lower bound of the expert capacity can be described as: ecmin = 1/pi ≥ 1/(n[1 − Iδ2(1/2, d−1/2)])"
- Break condition: If the input data distribution or model architecture changes significantly, the calculated minimum capacity threshold may no longer be valid.

## Foundational Learning

- Concept: Mixture-of-Experts (MoE) architecture
  - Why needed here: LocMoE is built upon the MoE framework, so understanding how MoE works is essential.
  - Quick check question: What are the main components of a typical MoE layer, and how do they interact during inference?

- Concept: All-to-All communication in distributed systems
  - Why needed here: LocMoE optimizes the All-to-All communication pattern to reduce latency and improve efficiency.
  - Quick check question: How does the All-to-All communication pattern work in a distributed MoE system, and what are its main performance bottlenecks?

- Concept: Regularization techniques in deep learning
  - Why needed here: LocMoE uses a novel locality-based regularization term to improve routing efficiency.
  - Quick check question: What is the purpose of regularization in deep learning, and how does the locality loss term in LocMoE differ from standard regularization techniques?

## Architecture Onboarding

- Component map: Input layer → Gating network (with GrAP) → Routing module → Expert networks → Output layer
- Critical path: 1. Token embedding through the gating network, 2. Routing decision based on gating values and regularization terms, 3. Token exchange between nodes (optimized All-to-All), 4. Expert computation, 5. Output aggregation
- Design tradeoffs: Locality vs. load balance: Enforcing locality may lead to slight load imbalance, but the benefits in communication efficiency outweigh this cost. Expert capacity vs. redundancy: Operating at the minimum capacity threshold reduces computation but requires careful tuning to avoid under-training.
- Failure signatures: Increased inter-node communication despite locality regularization, degraded model accuracy due to insufficient expert capacity, load imbalance despite auxiliary loss
- First 3 experiments: 1. Compare training time and accuracy of LocMoE vs. SwitchMoE on a small-scale dataset to verify communication reduction claims, 2. Measure the impact of varying the locality loss weight (μ) on load balance and communication efficiency, 3. Test the effect of reducing expert capacity below the theoretical minimum threshold to confirm the critical value proof

## Open Questions the Paper Calls Out

- Question: How does the performance of LocMoE scale when the number of experts exceeds the number of nodes in the cluster?
  - Basis in paper: [explicit] The paper states that "LocMoE is more appropriate for cases whose number of experts is larger than that of nodes" and that "The locality would lose efficacy when the local expert does not exist."
  - Why unresolved: The paper does not provide experimental results or analysis for scenarios where the number of experts is significantly larger than the number of nodes.
  - What evidence would resolve it: Experiments comparing LocMoE performance with different ratios of experts to nodes, and analysis of the impact on locality loss and overall training time.

- Question: What is the impact of dataset composition on the convergence speed and accuracy of LocMoE compared to other MoE variants?
  - Basis in paper: [inferred] The paper mentions that "the unlearned routers make it hard to distinguish experts and converge rapidly" and suggests that the dataset's inherent similarities may affect convergence.
  - Why unresolved: The paper does not provide a detailed analysis of how different dataset compositions affect the performance of LocMoE and other MoE variants.
  - What evidence would resolve it: Experiments training LocMoE and other MoE variants on datasets with varying levels of similarity between tokens, and analysis of the impact on convergence speed and accuracy.

- Question: How does the choice of expert capacity affect the trade-off between training efficiency and model accuracy in LocMoE?
  - Basis in paper: [explicit] The paper introduces a theoretical proof for the minimum expert capacity threshold and states that "the model accuracy would not be affected after downward adjusting the expert capacity within the critical limit."
  - Why unresolved: The paper does not provide a detailed analysis of the relationship between expert capacity, training efficiency, and model accuracy for LocMoE.
  - What evidence would resolve it: Experiments varying the expert capacity in LocMoE and measuring the impact on training time and model accuracy, and analysis of the trade-offs involved.

## Limitations
- Evaluation limited to a single model (PanGu-Σ) and hardware platform (Ascend clusters)
- No ablation studies on individual components to quantify their separate contributions
- Theoretical proof for minimum expert capacity may not generalize to all data distributions or model architectures
- Locality loss mechanism could potentially create load imbalance in difficult scenarios

## Confidence
- **High Confidence**: The orthogonal gating mechanism with GrAP layer and its theoretical foundation are well-supported by the mathematical proofs and implementation details provided.
- **Medium Confidence**: The locality-based regularization shows promising results in the specific PanGu-Σ evaluation, but generalizability to other models and datasets needs further validation.
- **Low Confidence**: The combined effect of all three mechanisms working together is demonstrated but not independently validated.

## Next Checks
1. **Ablation Study Validation**: Implement LocMoE without the locality regularization component and compare training time and accuracy against the full LocMoE implementation to quantify the exact contribution of the locality mechanism versus the orthogonal gating and capacity optimization.

2. **Cross-Model Generalization**: Apply LocMoE to a different MoE architecture (such as LLaMA-MoE or GPT-MoE) on GPU clusters rather than Ascend NPUs to test whether performance improvements generalize beyond the specific hardware and model configuration used in evaluation.

3. **Edge Case Performance Analysis**: Design synthetic input distributions where locality constraints cannot be satisfied (e.g., uniform token distribution across all experts) and measure how LocMoE performance degrades compared to baseline routers to reveal practical limitations of the locality regularization approach.