---
ver: rpa2
title: 'KET-QA: A Dataset for Knowledge Enhanced Table Question Answering'
arxiv_id: '2405.08099'
source_url: https://arxiv.org/abs/2405.08099
tags:
- table
- knowledge
- question
- information
- ket-qa
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces KET-QA, a novel dataset for table question
  answering that incorporates external knowledge from knowledge bases. The dataset
  includes fine-grained gold evidence annotations to address the challenge of missing
  information in tables.
---

# KET-QA: A Dataset for Knowledge Enhanced Table Question Answering

## Quick Facts
- arXiv ID: 2405.08099
- Source URL: https://arxiv.org/abs/2405.08099
- Reference count: 32
- Primary result: Introduces KET-QA dataset with fine-grained gold evidence annotations for table question answering requiring external knowledge

## Executive Summary
This paper introduces KET-QA, a novel dataset for table question answering that incorporates external knowledge from knowledge bases. The dataset includes fine-grained gold evidence annotations to address the challenge of missing information in tables. A retriever-reasoner pipeline model is proposed to effectively integrate knowledge base information into the table question answering process. Experimental results show that incorporating external knowledge significantly improves performance across different models and settings, with relative improvements of 1.9 to 6.5 times and absolute improvements of 11.66% to 44.64% in exact match scores. However, even the best model achieves a 60.23% exact match score, which still lags behind human-level performance, highlighting the challenging nature of KET-QA for the question-answering community.

## Method Summary
The KET-QA dataset construction involves extracting tables from Wikipedia, identifying Wikidata entities, and annotating question-answer pairs that require external knowledge. A retriever-reasoner pipeline model is proposed, featuring a multistage KB retriever with bi-encoder for initial filtering and cross-encoder for re-ranking, followed by a PLM-based reasoner that integrates questions, tables, and retrieved triples. The dataset includes fine-grained gold evidence annotations, enabling supervised training of the retriever. Experiments evaluate the approach across fine-tuning, few-shot, and zero-shot settings using various PLM-based reasoners.

## Key Results
- Incorporating external knowledge improves exact match scores by 1.9 to 6.5 times relative and 11.66% to 44.64% absolute across three settings
- The multistage retriever (bi-encoder + cross-encoder) achieves strong performance with kNN negative sampling
- Even the best model (ChatGPT) achieves only 60.23% exact match, highlighting the dataset's challenging nature
- Using triple-related sub-table improves retriever performance by 3.08% R@20 compared to full table

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Structured knowledge bases improve TableQA by providing precise semantic relationships that tables lack.
- Mechanism: The retriever identifies relevant KB triples, and the reasoner integrates them with table data to answer questions requiring external knowledge.
- Core assumption: Tables inherently contain incomplete or missing information that structured KBs can supplement effectively.
- Evidence anchors:
  - [abstract]: "most existing datasets either fail to address the issue of external knowledge in TableQA or only utilize unstructured text as supplementary information for tables."
  - [section]: "Incorporating a knowledge base into the TableQA process poses two challenges: (i) The amount of information contained within the grounded sub-graph remains substantial and redundant for a specific question; (ii) Integrating three different types of data, namely questions (unstructured text), tables (semi-structured), and a knowledge base (structured), for reasoning purposes."
- Break condition: If the KB sub-graph contains insufficient or irrelevant triples, or if the retriever fails to identify pertinent information.

### Mechanism 2
- Claim: Multistage retrieval (bi-encoder + cross-encoder) optimizes both speed and precision for KB integration.
- Mechanism: The bi-encoder quickly narrows the search space to top candidates, while the cross-encoder re-ranks them with higher precision.
- Core assumption: Simple bi-encoders are fast but imprecise; cross-encoders are precise but slow; combining them balances both needs.
- Evidence anchors:
  - [abstract]: "Experimental results demonstrate that our model consistently achieves remarkable relative performance improvements ranging from 1.9 to 6.5 times and absolute improvements of 11.66% to 44.64% on EM scores across three distinct settings."
  - [section]: "The retriever predicts the relevance scores s(t, q, T), and then the top k triples with the highest score will be fed into the reasoner."
- Break condition: If the initial bi-encoder retrieval misses all relevant triples, the cross-encoder cannot recover them.

### Mechanism 3
- Claim: Fine-grained gold evidence annotation enables supervised training of the retriever for KB-QA tasks.
- Mechanism: Annotators label which KB triples are needed for each question, creating training data that teaches the retriever to identify relevant knowledge.
- Core assumption: Without gold evidence, retrievers must rely on unsupervised or weak supervision, leading to poorer performance.
- Evidence anchors:
  - [abstract]: "Each table in the dataset corresponds to a sub-graph of the entire KB, and every question requires the integration of information from both the table and the sub-graph to be answered."
  - [section]: "In the second stage, annotators annotate the fine-grained gold evidence required to answer a specific question from the sub-graph corresponding to the current table."
- Break condition: If annotation quality is low or inconsistent, the retriever training signal becomes unreliable.

## Foundational Learning

- Concept: Table question answering (TableQA) fundamentals
  - Why needed here: Understanding how models answer questions using only table data establishes the baseline for measuring knowledge base enhancements.
  - Quick check question: What are the typical architectures used for answering questions from tables without external knowledge?

- Concept: Knowledge base (KB) retrieval and reasoning
  - Why needed here: The paper introduces a retriever-reasoner pipeline that leverages KB triples; understanding this process is essential for implementing the approach.
  - Quick check question: How does a bi-encoder differ from a cross-encoder in retrieval tasks?

- Concept: Multistage information retrieval
  - Why needed here: The paper uses a two-stage retrieval process; understanding why this improves performance is key to grasping the design.
  - Quick check question: What are the trade-offs between speed and precision in retrieval systems?

## Architecture Onboarding

- Component map:
  - Question + Table + KB sub-graph -> Retriever (Bi-encoder -> Cross-encoder) -> Retrieved triples -> Reasoner (PLM) -> Answer

- Critical path:
  1. Input: Question, table, and corresponding KB sub-graph
  2. Retriever: Bi-encoder filters to top candidates, Cross-encoder re-ranks
  3. Reasoner: PLM processes question, table, and retrieved triples to produce answer

- Design tradeoffs:
  - Speed vs. precision in retrieval (bi-encoder vs. cross-encoder)
  - Completeness vs. noise in KB sub-graph (one-hop vs. multi-hop extraction)
  - Model complexity vs. training data requirements (supervised vs. unsupervised retrieval)

- Failure signatures:
  - Retriever fails to find relevant triples → reasoner cannot answer questions requiring external knowledge
  - Incorrect triple integration → wrong answers even when triples are retrieved
  - Table serialization issues → retrieval model cannot match table content with questions

- First 3 experiments:
  1. Implement the retriever-only pipeline (bi-encoder → cross-encoder) and measure R@20 on KET-QA dev set
  2. Add the reasoner component and test on the subset of questions answerable without KB
  3. Test the full pipeline on questions requiring external knowledge and compare EM scores with table-only baseline

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of the retriever change when using multi-hop sub-graphs instead of one-hop sub-graphs?
- Basis in paper: [explicit] The paper mentions that using more hops would provide a wider range of external knowledge but presents greater challenges for the reasoner.
- Why unresolved: The paper only extracted one-hop sub-graphs due to complexity, leaving the impact of multi-hop retrieval unexplored.
- What evidence would resolve it: Comparative experiments showing retriever and reasoner performance on one-hop vs multi-hop sub-graphs in KET-QA.

### Open Question 2
- Question: What is the impact of different negative sampling strategies on retriever performance?
- Basis in paper: [explicit] The paper describes using kNN Negative Sampling (kNS) for bi-encoder training but notes cross-encoder training is time-consuming with kNS.
- Why unresolved: The paper only experiments with kNS for bi-encoder and random sampling for cross-encoder, without exploring other negative sampling strategies.
- What evidence would resolve it: Experiments comparing retriever performance using different negative sampling strategies (e.g., hard negative mining, dynamic negative sampling) on KET-QA.

### Open Question 3
- Question: How does the choice of table representation method affect the reasoner's ability to perform numerical calculations?
- Basis in paper: [explicit] The paper shows that using triple-related sub-table improves retriever performance but does not analyze its impact on the reasoner's calculation ability.
- Why unresolved: The paper focuses on retrieval performance but doesn't provide evidence on how different table representations affect the reasoner's numerical reasoning capabilities.
- What evidence would resolve it: Comparative experiments showing reasoner performance on calculated answers using different table representation methods (full table, no table, triple-related sub-table) in KET-QA.

## Limitations

- The dataset creation process relies on human annotation for gold evidence, which may introduce bias or inconsistency
- The retrieval mechanism's effectiveness depends heavily on the quality of entity linking between tables and the knowledge base
- The study focuses on Wikidata as the knowledge source, potentially limiting generalizability to other knowledge bases

## Confidence

- **High confidence:** The dataset creation methodology and the general retriever-reasoner pipeline architecture are well-specified and reproducible
- **Medium confidence:** The specific performance improvements across different models, as implementation details may vary
- **Medium confidence:** The claim that KET-QA is uniquely challenging due to its requirement for external knowledge integration

## Next Checks

1. **Cross-dataset validation:** Test the MKBR retriever on other table-KB integration tasks to assess generalization beyond KET-QA
2. **Error analysis:** Conduct a detailed analysis of the 39.77% of questions the best model fails to answer correctly to identify failure patterns
3. **Ablation study:** Remove the gold evidence annotations from training to quantify their specific contribution to retriever performance