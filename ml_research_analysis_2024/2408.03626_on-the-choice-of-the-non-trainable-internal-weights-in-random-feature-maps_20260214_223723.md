---
ver: rpa2
title: On the choice of the non-trainable internal weights in random feature maps
arxiv_id: '2408.03626'
source_url: https://arxiv.org/abs/2408.03626
tags:
- good
- feature
- features
- random
- internal
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of selecting internal weights
  in random feature maps to improve their forecasting capabilities for chaotic dynamical
  systems. The authors introduce a computationally efficient hit-and-run algorithm
  to sample "good" internal weights that maximize the model's expressivity.
---

# On the choice of the non-trainable internal weights in random feature maps

## Quick Facts
- arXiv ID: 2408.03626
- Source URL: https://arxiv.org/abs/2408.03626
- Authors: Pinak Mandal; Georg A. Gottwald; Nicholas Cranch
- Reference count: 40
- Primary result: Demonstrates a hit-and-run algorithm for selecting "good" internal weights in random feature maps, achieving superior forecasting of chaotic systems while being nearly 100,000 times faster than gradient descent-trained networks

## Executive Summary
This paper addresses the challenge of selecting internal weights in random feature maps for chaotic dynamical system forecasting. The authors introduce a computationally efficient hit-and-run algorithm to sample internal weights that maximize model expressivity by keeping activations in the nonlinear, non-saturated region of the tanh function. They demonstrate that the number of good features, rather than overall feature dimension, is the primary factor controlling forecasting skill, with an effective feature dimension of approximately 256 features. The method outperforms single-layer feedforward neural networks trained with gradient descent, achieving superior forecasting capabilities while being nearly 100,000 times faster computationally.

## Method Summary
The method employs random feature maps with tanh activation, where internal weights are sampled using a hit-and-run algorithm to ensure features remain in the nonlinear, non-saturated region of the activation function. This creates a set of "good" internal parameters that maximize expressivity. The outer weights are learned via ridge regression, which suppresses bad features (linear and saturated) while enhancing good features. The approach is demonstrated on the Lorenz-63 system, with forecasting skill measured in Lyapunov time units as the time when prediction significantly deviates from the true validation trajectory.

## Key Results
- The number of good features (Ng ≈ 256) is the primary factor controlling forecasting skill, not total feature dimension
- Mean forecast time saturates once sufficient good features are present, with no additional benefit from increasing total feature dimension
- Ridge regression effectively suppresses bad features (linear and saturated) once sufficient good features are available
- Random feature maps with good internal weights outperform single-layer feedforward networks by 2-5 Lyapunov times while being nearly 100,000 times faster computationally

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Good features are defined by mapping training data into the nonlinear, non-saturated region of the tanh activation.
- Mechanism: By constraining internal weights so that L0 < |Win u + b| < L1 for all training data, the activation function remains in its most expressive range, avoiding linear and saturated outputs.
- Core assumption: The tanh activation's expressivity is maximized in the interval (L0, L1).
- Evidence anchors:
  - [abstract] states that "good internal weights which lead to good forecasting skill" are found by sampling from a set that explores the activation function's expressivity.
  - [section 3.1] explains that linear features render the model essentially linear, while saturated features cause loss of expressivity.

### Mechanism 2
- Claim: The number of good features, not total feature dimension, is the primary factor controlling forecasting skill.
- Mechanism: Increasing the fraction of good features increases the effective dimensionality of the feature space relevant to chaotic system forecasting, as evidenced by saturation of forecast skill beyond ~256 good features.
- Core assumption: Good features contribute more to forecasting skill than bad features, and their contribution is independent of total feature count.
- Evidence anchors:
  - [section 5.1] shows that "the number of good rows is the main factor controlling the forecasting skill" and that performance saturates once Ng reaches ~256.
  - [figure 10] demonstrates that mean forecast time depends on Ng, not Dr, beyond a threshold.

### Mechanism 3
- Claim: Ridge regression suppresses bad features (linear and saturated) while enhancing good features.
- Mechanism: The outer weight matrix learned via ridge regression assigns small weights to features produced by bad internal parameters, effectively removing their influence from the final model.
- Core assumption: The optimization process can distinguish between good and bad features based on their contribution to the loss function.
- Evidence anchors:
  - [section 5.2] shows that columns of W corresponding to linear and saturated features are suppressed (supremum norm < δ = 1) once sufficient good features are present.
  - [figure 14 and 15] illustrate how linear and saturated features are progressively suppressed as the number of good features increases.

## Foundational Learning

- Concept: Hit-and-run algorithm for sampling from convex sets.
  - Why needed here: To efficiently sample from the convex subsets S+ and S- that define the set of good internal parameters.
  - Quick check question: Can you explain why hit-and-run is preferred over naive uniform sampling in high-dimensional spaces for this application?

- Concept: Ridge regression and its role in feature selection.
  - Why needed here: To understand how the outer weights are learned and how they interact with the quality of internal parameters.
  - Quick check question: How does the regularization parameter β affect the balance between fitting the data and suppressing bad features?

- Concept: Chaotic dynamical systems and their sensitivity to initial conditions.
  - Why needed here: To grasp why forecasting skill is a critical metric and how small changes in the model can lead to large differences in predictions.
  - Quick check question: What is the Lyapunov time, and why is it used as a measure of forecast skill in this context?

## Architecture Onboarding

- Component map: Input D-dimensional state vector u -> Internal weights (Win, bin) sampled using hit-and-run -> tanh activation -> Linear combination via learned outer weights W -> Forecast output
- Critical path:
  1. Sample internal weights (Win, bin) using hit-and-run to ensure good features
  2. Compute feature matrix Φ(U) using the sampled internal weights
  3. Solve for outer weights W using ridge regression
  4. Evaluate forecast skill on validation data
- Design tradeoffs:
  - Feature dimension Dr vs. number of good features Ng: Increasing Dr beyond the effective dimension (Ng ≈ 256) provides diminishing returns.
  - Regularization parameter β: Controls the trade-off between fitting the data and preventing overfitting; optimal value depends on the specific dataset and feature dimension.
- Failure signatures:
  - Low forecast skill despite high feature dimension: Indicates insufficient number of good features.
  - High variance in forecast skill across different training runs: Suggests poor sampling of good internal parameters.
  - Training loss much lower than validation loss: May indicate overfitting, requiring adjustment of β or feature dimension.
- First 3 experiments:
  1. Vary the fraction of good features pg from 0 to 1 and observe the effect on mean forecast time E[τf] and coefficient of variation.
  2. Compare the performance of random feature maps with good internal weights to a single-layer feedforward network trained with gradient descent.
  3. Investigate the effect of different values of L0 and L1 on the effective range of features and forecast skill.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the number of good features remain the controlling factor for forecasting skill when using deeper random feature networks with multiple layers?
- Basis in paper: [explicit] The authors note that it would be interesting to test if the ideas of selecting weights according to the domain of the tanh-activation function can be employed to generate deep random feature networks where at each layer weights and biases are drawn using their hit-and-run algorithm.
- Why unresolved: The paper only explores single-layer random feature maps and demonstrates that the number of good features is the controlling factor for forecasting skill in this architecture. The behavior in deeper architectures is unknown.
- What evidence would resolve it: Numerical experiments comparing the forecasting skill of deep random feature networks (with multiple layers using the hit-and-run algorithm at each layer) to their single-layer counterparts, while varying the number of good features at each layer.

### Open Question 2
- Question: How does the forecasting skill change when using activation functions other than tanh in random feature maps?
- Basis in paper: [explicit] The authors mention that the distinction into linear, saturated and good features readily translates to other sigmoidal activation functions.
- Why unresolved: The paper focuses on tanh activation functions and establishes the concept of good features for this specific activation. The behavior with other activation functions is not explored.
- What evidence would resolve it: Numerical experiments comparing the forecasting skill of random feature maps using different sigmoidal activation functions (e.g., sigmoid, hard tanh, etc.) while varying the number of good features for each activation function.

### Open Question 3
- Question: Can the loss function be augmented with a penalty term that explicitly promotes the generation of good features in single-layer feedforward networks, thereby improving their forecasting skill?
- Basis in paper: [inferred] The authors observe that single-layer feedforward networks trained with gradient descent do not generate good features and have inferior forecasting skill compared to random feature maps with good internal parameters. They suggest that it would be interesting to see if the network generates good features if the loss function is augmented by a penalty term promoting good features.
- Why unresolved: The paper demonstrates that the optimization process in single-layer feedforward networks does not find solutions on the measure-zero set of good parameters. It is unclear if adding a penalty term to the loss function can overcome this limitation.
- What evidence would resolve it: Numerical experiments comparing the forecasting skill of single-layer feedforward networks with and without a penalty term that promotes the generation of good features, while keeping other parameters constant.

## Limitations

- The study demonstrates strong results only on the Lorenz-63 system, with generalizability to other chaotic dynamical systems remaining uncertain.
- The choice of L0 and L1 bounds (0.5 and 2.0) appears somewhat arbitrary and may not translate well to different activation functions or dynamical systems with different attractor geometries.
- The computational efficiency claim (100,000x faster) depends heavily on implementation details and hardware specifics that aren't fully specified.

## Confidence

- High confidence: The mechanism of hit-and-run sampling for good internal weights and its effectiveness in improving forecasting skill (Mechanism 1 and 2)
- Medium confidence: The claim that ridge regression effectively suppresses bad features (Mechanism 3), as this relies on specific hyperparameter choices
- Medium confidence: The quantitative performance comparisons with gradient descent-trained networks, as implementation details significantly affect computational efficiency measurements

## Next Checks

1. Test the hit-and-run sampling method on a different chaotic system (e.g., Lorenz-96 or Rössler) to verify generalizability beyond Lorenz-63
2. Perform an ablation study varying L0 and L1 bounds systematically to identify optimal ranges for different dynamical systems
3. Benchmark against modern neural network architectures (LSTM, GRU) on the same forecasting task to contextualize the performance claims relative to current state-of-the-art methods