---
ver: rpa2
title: 'CliMedBench: A Large-Scale Chinese Benchmark for Evaluating Medical Large
  Language Models in Clinical Scenarios'
arxiv_id: '2410.03502'
source_url: https://arxiv.org/abs/2410.03502
tags:
- medical
- llms
- clinical
- data
- climedbench
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces CliMedBench, a large-scale Chinese benchmark
  for evaluating medical large language models (LLMs) in clinical scenarios. CliMedBench
  comprises 33,735 questions across 14 core clinical scenarios, derived from real-world
  medical reports of top-tier tertiary hospitals and authentic examination exercises.
---

# CliMedBench: A Large-Scale Chinese Benchmark for Evaluating Medical Large Language Models in Clinical Scenarios

## Quick Facts
- arXiv ID: 2410.03502
- Source URL: https://arxiv.org/abs/2410.03502
- Authors: Zetian Ouyang; Yishuai Qiu; Linlin Wang; Gerard de Melo; Ya Zhang; Yanfeng Wang; Liang He
- Reference count: 5
- Key outcome: CliMedBench comprises 33,735 questions across 14 core clinical scenarios, derived from real-world medical reports of top-tier tertiary hospitals and authentic examination exercises.

## Executive Summary
CliMedBench is a large-scale Chinese benchmark designed to evaluate medical large language models (LLMs) in clinical scenarios. It addresses the gap between exam-based benchmarks and real-world clinical practice by using real-world EHR data from top-tier hospitals. The benchmark assesses LLMs across 7 pivot dimensions including clinical question answering, knowledge application, reasoning, information retrieval, summarization abilities, hallucination, and toxicity. Experiments reveal that Chinese medical LLMs underperform on this benchmark, especially in medical reasoning and factual consistency, while general-domain LLMs show substantial potential in medical clinics.

## Method Summary
The benchmark employs a taxonomy-based approach to categorize real-world clinical medical practice, integrating expertise from Chinese medical practitioners. It contains 33,735 questions across 14 core clinical scenarios derived from real-world medical reports of top-tier tertiary hospitals and authentic examination exercises. The evaluation assesses LLMs across 7 pivot dimensions using both automatic and human evaluation metrics. The authors also propose an agent-based Computerized Adaptive Testing (CAT) approach to enable rapid assessment with minimal problem sets by synthesizing participant data via multi-agent LLMs and applying IRT-based adaptive testing.

## Key Results
- Chinese medical LLMs underperform on CliMedBench, especially in medical reasoning and factual consistency
- General-domain LLMs show substantial potential in medical clinics compared to specialized medical LLMs
- Limited input capacity of many medical LLMs hinders their practical use in clinical scenarios

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The benchmark's use of real-world EHR data improves evaluation authenticity and reduces data contamination.
- Mechanism: Real-world EHRs provide heterogeneous, authentic clinical cases that reflect actual medical practice, avoiding overfit to exam-style questions.
- Core assumption: Real-world data sources are less likely to overlap with LLM training corpora compared to public exam datasets.
- Evidence anchors: [abstract] "CliMedBench... derived from real-world medical reports of top-tier tertiary hospitals..."; [section] "CliMedBench is derived from real-world Electronic Health Records (EHRs)..."

### Mechanism 2
- Claim: Multi-dimensional evaluation (7 pivot dimensions) enables fine-grained assessment of LLM capabilities.
- Mechanism: Covering clinical QA, reasoning, knowledge application, information retrieval, summarization, hallucination, and toxicity ensures comprehensive performance profiling.
- Core assumption: Each dimension captures a distinct aspect of clinical competence relevant to real-world deployment.
- Evidence anchors: [abstract] "...assess the medical ability of LLMs across 7 pivot dimensions..."; [section] "The benchmark assesses LLMs across 7 pivot dimensions..."

### Mechanism 3
- Claim: Agent-based CAT reduces evaluation cost while preserving ranking fidelity.
- Mechanism: Synthesizing participant data via multi-agent LLMs and applying IRT-based adaptive testing enables accurate ability estimation with fewer questions.
- Core assumption: Synthesized data sufficiently mimics human participant behavior for IRT modeling.
- Evidence anchors: [section] "We propose an agent-based CAT approach to enable rapid assessment with minimal problem sets."; [section] "Our agent-based CAT consists of two main steps: Multi-Agent Based Participant Synthesis (MPS) and Computerized Adaptive Testing (CAT)."

## Foundational Learning

- Concept: Item Response Theory (IRT) for adaptive testing
  - Why needed here: Enables efficient ability estimation by modeling the probability of correct responses as a function of item difficulty and examinee proficiency.
  - Quick check question: What three parameters define the IRT-3PL model used in this work?

- Concept: Spearman rank correlation for benchmark validity
  - Why needed here: Confirms that CliMedBench rankings align with established benchmarks, indicating external validity.
  - Quick check question: What correlation value between CliMedBench and MedBench indicates strong validity?

- Concept: Hallucination detection in medical QA
  - Why needed here: Ensures that LLM responses are factually consistent and not fabricated, critical for clinical safety.
  - Quick check question: Which dataset type is specifically designed to trigger model hallucinations in this benchmark?

## Architecture Onboarding

- Component map: CliMedBench → Taxonomy → Data Collection → Question Generation → Evaluation → Agent-based CAT Pipeline
- Critical path: Taxonomy design → EHR data curation → Multi-dimensional question generation → Human expert validation → LLM evaluation → CAT synthesis
- Design tradeoffs: Real-world EHRs offer authenticity but introduce noise; multi-dimensional coverage increases complexity; CAT reduces cost but relies on data synthesis quality.
- Failure signatures: Poor LLM differentiation in hard questions → insufficient question difficulty range; high variance in human evaluation → unclear scoring rubric; low CAT accuracy → inadequate participant synthesis.
- First 3 experiments:
  1. Validate taxonomy alignment by having clinicians map 20 sample questions to the 14 scenarios.
  2. Test data contamination by checking overlap between EHR-derived questions and known LLM training corpora.
  3. Benchmark a small LLM set using both full CliMedBench and agent-based CAT to compare ranking consistency.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal Chain-of-Thought (CoT) prompt structure for medical LLMs across different clinical scenarios?
- Basis in paper: [explicit] The paper shows CoT prompts improve reasoning performance on seven datasets, but effectiveness varies by model and scenario, with Qwen showing 6.5% average accuracy increase while ChatGLM3 shows minimal improvement.
- Why unresolved: The paper only tests basic CoT prompts without exploring variations in prompt length, structure, or specificity for different clinical tasks.
- What evidence would resolve it: Comparative experiments testing multiple CoT prompt variants across all 14 clinical scenarios with statistical analysis of which structures work best for which task types.

### Open Question 2
- Question: How does model performance on CliMedBench correlate with actual clinical outcomes in real-world medical practice?
- Basis in paper: [inferred] The paper notes that medical exams are "inefficient clinical performance indicators" and that CliMedBench uses real-world EHRs to better reflect clinical practice, but doesn't validate against actual clinical performance.
- Why unresolved: The benchmark is validated through expert review and correlation with other benchmarks, but lacks validation against real clinical decision-making outcomes.
- What evidence would resolve it: A longitudinal study tracking LLM recommendations derived from CliMedBench performance against actual patient outcomes and clinical decisions made by human physicians.

### Open Question 3
- Question: What is the minimum input window size required for medical LLMs to maintain performance on complex clinical cases?
- Basis in paper: [explicit] The paper shows performance drops from 47.3 to 43.1 as input length increases, with medical LLMs declining more sharply (29.5 to 22.6) than general LLMs (60.5 to 58.0).
- Why unresolved: The paper only tests 10 input length segments but doesn't determine the precise threshold where performance becomes clinically inadequate.
- What evidence would resolve it: Systematic testing of input windows from 1k to 50k tokens with performance curves showing the exact point where accuracy falls below clinically acceptable thresholds (e.g., 85%).

## Limitations

- Data contamination risk remains unaddressed despite claims that real-world EHR data reduces overlap with LLM training corpora
- CAT synthesis validity is unclear as the approach relies on unverified assumptions about synthetic data quality and IRT model applicability
- Limited external validity evidence with no specific correlation values provided for alignment with real-world clinical decision-making

## Confidence

**High confidence**: Real-world EHR data provides authentic clinical scenarios distinct from exam-style questions. This is well-supported by the methodology description and aligns with established medical education practices.

**Medium confidence**: Multi-dimensional evaluation captures distinct aspects of clinical competence. While the 7 dimensions are theoretically justified, the paper lacks empirical validation showing these dimensions are orthogonal and collectively comprehensive.

**Low confidence**: Agent-based CAT achieves rapid assessment without sacrificing ranking fidelity. The mechanism depends on unverified assumptions about synthetic data quality and IRT model applicability to LLM evaluation.

## Next Checks

1. **Contamination analysis**: Conduct explicit overlap analysis between CliMedBench questions and the pretraining corpora of evaluated LLMs to quantify contamination risk.

2. **CAT accuracy validation**: Compare rankings from full CliMedBench evaluation versus agent-based CAT for a diverse set of medical LLMs, measuring Spearman correlation and identifying systematic biases.

3. **Clinical utility assessment**: Have practicing clinicians complete a subset of CliMedBench questions and compare their performance distribution to LLM outputs to validate real-world relevance.