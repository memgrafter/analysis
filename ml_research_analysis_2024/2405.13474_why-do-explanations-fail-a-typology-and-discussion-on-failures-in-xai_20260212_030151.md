---
ver: rpa2
title: Why do explanations fail? A typology and discussion on failures in XAI
arxiv_id: '2405.13474'
source_url: https://arxiv.org/abs/2405.13474
tags:
- explanations
- explanation
- users
- failures
- user
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper addresses the challenge of understanding failures in
  eXplainable AI (XAI) systems by proposing a comprehensive typology of explanation
  failures. The authors identify two main categories of failures: system-specific
  and user-specific.'
---

# Why do explanations fail? A typology and discussion on failures in XAI

## Quick Facts
- arXiv ID: 2405.13474
- Source URL: https://arxiv.org/abs/2405.13474
- Authors: Clara Bove; Thibault Laugel; Marie-Jeanne Lesot; Charles Tijus; Marcin Detyniecki
- Reference count: 27
- Primary result: A comprehensive typology of XAI failures distinguishing system-specific (misleading, competing, unstable, incompatible) from user-specific (mismatch, counterintuitive, biased inferences) failures

## Executive Summary
This paper addresses the critical challenge of understanding failures in eXplainable AI (XAI) systems by proposing a comprehensive typology that distinguishes between system-specific and user-specific failures. The authors systematically analyze 108 research papers to identify how and why explanations can fail, creating a framework that helps practitioners diagnose and address these issues. By mapping failures to specific development stages and identifying both technical and human factors, the framework provides a holistic perspective on XAI failures that goes beyond traditional component-wise solutions.

## Method Summary
The authors conducted a systematic literature analysis using Nickerson et al.'s taxonomy development methodology to investigate XAI failures. They collected 108 research papers from AI, HCI, and explainability venues using keywords related to explainability, interpretability, transparency, failures, problems, risks, pitfalls, and inconsistencies. The methodology involved defining a meta-characteristic (system-specific vs user-specific failures), iteratively categorizing failures, and grouping them into dimensions until no new categories emerged and each category had at least 5 supporting papers. The resulting typology distinguishes between failures originating from the ML system (misleading, competing, unstable, incompatible explanations) and those arising from user interpretation (mismatch, counterintuitive, biased inferences).

## Key Results
- Identified two main categories of XAI failures: system-specific (technical) and user-specific (interpretive)
- System-specific failures include misleading explanations, competing explanations, unstable explanations, and incompatible explanations
- User-specific failures encompass mismatches, counterintuitive explanations, and biased inferences
- Framework enables practitioners to pinpoint failure origins and apply targeted mitigation strategies
- Research directions identified for enhancing XAI quality through transparency, personalization, and user interaction

## Why This Works (Mechanism)

### Mechanism 1
- Claim: A typology that distinguishes system-specific from user-specific failures enables practitioners to pinpoint the origin of XAI issues rather than conflating technical and interpretive failures.
- Mechanism: By structuring failures into a two-level taxonomy (system vs. user) and further decomposing them into observable symptoms (misleading, competing, unstable, incompatible vs. mismatch, counterintuitive, biased inferences), the framework creates clear decision points for diagnosis and remediation.
- Core assumption: XAI failures arise from distinct, identifiable sources that can be separated into technical and human factors.
- Evidence anchors:
  - [abstract] "distinguishing between system-specific and user-specific failures"
  - [section] "we propose a typological framework that helps revealing the nuanced complexities of explanation failures"
  - [corpus] Weak: No corpus neighbor directly addresses this mechanism, but "Why the Agent Made that Decision" explores interpretability failures that could map to this taxonomy.
- Break condition: When failure symptoms overlap so heavily that categorization becomes ambiguous, or when systemic failures cannot be isolated from user perception.

### Mechanism 2
- Claim: Explicitly mapping failures to their development phase (model, explainer, explanation output) enables targeted mitigation strategies.
- Mechanism: By associating each failure type with the stage where it originates, practitioners can trace problems to specific components and apply phase-appropriate fixes (e.g., model retraining vs. explainer redesign vs. UI adjustments).
- Core assumption: The explanation process has discrete stages whose failures are traceable and remediable in isolation.
- Evidence anchors:
  - [section] "we discuss explanation failures that can be ascribed to the Machine Learning system, depending on its development step"
  - [section] "by distinguishing between system-specific and user-specific failures"
  - [corpus] Weak: No direct corpus support, but "Transparent AI: Developing an Explainable Interface" implies stage-specific design choices.
- Break condition: When the failure propagates across stages, making it impossible to isolate the root cause to a single development phase.

### Mechanism 3
- Claim: Including both technical and human factors in a single framework prevents the omission of critical user-centric failure modes in XAI research.
- Mechanism: By integrating system-centric and user-centric perspectives, the typology ensures that failures like mismatches and cognitive biases are not overlooked, leading to more comprehensive XAI evaluation and design.
- Core assumption: User-centric failures are as critical as technical failures and must be systematically included in XAI frameworks.
- Evidence anchors:
  - [abstract] "By distinguishing between system-specific and user-specific failures"
  - [section] "We propose a typological framework that helps revealing the nuanced complexities of explanation failures"
  - [corpus] Weak: No corpus neighbor explicitly discusses this integration, but "Why is 'Problems' Predictive of Positive Sentiment?" touches on user confusion from unintuitive explanations.
- Break condition: When user-centric failures are treated as secondary or optional, leading to incomplete XAI solutions.

## Foundational Learning

- Concept: Difference between post-hoc and ante-hoc explanation methods
  - Why needed here: The typology covers failures for both types of methods; understanding the distinction helps identify which failures apply in a given context.
  - Quick check question: Does the explainer generate explanations after the model is trained (post-hoc) or is it inherently interpretable (ante-hoc)?

- Concept: Feature attribution vs. counterfactual explanations
  - Why needed here: Competing and incompatible failures often arise from using different explanation types; knowing their characteristics helps diagnose contradictions.
  - Quick check question: Does the explanation highlight which features mattered (attribution) or suggest minimal changes to get a different outcome (counterfactual)?

- Concept: Cognitive biases affecting AI interpretation
  - Why needed here: User-specific failures hinge on biases like negativity bias or confirmation bias; awareness is needed to mitigate biased inferences.
  - Quick check question: Which bias might cause a user to overweight negative outcomes when reviewing model explanations?

## Architecture Onboarding

- Component map:
  - ML model → Prediction → Explainer → Explanation → User Interface → User
  - Failures can originate at any arrow or node (model inaccuracy, explainer unfaithfulness, UI mismatch, user bias)

- Critical path:
  1. Accurate prediction generation
  2. Faithful explanation generation
  3. Clear, user-aligned explanation display
  4. Correct user interpretation

- Design tradeoffs:
  - Stability vs. locality in explanations
  - Complexity vs. understandability of explanation formats
  - Diversity vs. consistency in multiple explanations

- Failure signatures:
  - Model: Misleading predictions, unstable behavior
  - Explainer: Competing info, incompatible methods, unfaithful outputs
  - UI: Mismatch with user expectations, poor framing
  - User: Counterintuitive content, biased interpretation

- First 3 experiments:
  1. Compare stability of LIME vs. SHAP on a fixed dataset; document variance and trace to model or explainer.
  2. Conduct A/B test: counterfactual vs. feature importance explanations with domain experts; measure mismatch perception.
  3. Test user trust under manipulated explanations (e.g., selectively show competing explanations); measure under/overtrust bias.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can we effectively integrate user knowledge into XAI systems to prevent both mismatch and counterintuitive explanation failures?
- Basis in paper: [explicit] The paper discusses mismatched explanations when format doesn't meet user expectations and counterintuitive explanations when content contradicts user prior knowledge, suggesting personalized explanations as a potential solution.
- Why unresolved: The paper identifies these as user-specific failures but doesn't provide concrete methodologies for integrating user knowledge systematically during XAI system design.
- What evidence would resolve it: Empirical studies demonstrating successful integration of user knowledge (e.g., through concept-bottleneck models or co-design approaches) that reduce mismatch and counterintuitive failures while maintaining explanation faithfulness.

### Open Question 2
- Question: What are the most effective strategies to measure and mitigate cognitive biases that lead to biased inferences in XAI interpretation?
- Basis in paper: [explicit] The paper identifies biased inferences as user-specific failures caused by cognitive biases like negativity bias, availability bias, and primacy effect, noting that measuring these effects is challenging.
- Why unresolved: While the paper mentions the need for identifying and measuring biased inferences, it doesn't provide validated frameworks or tools for assessing how different biases affect XAI interpretation.
- What evidence would resolve it: Development and validation of standardized assessment tools that can measure specific cognitive biases in XAI contexts, along with proven mitigation strategies through XUI design.

### Open Question 3
- Question: How can XAI systems be designed holistically to address both system-specific and user-specific failures simultaneously rather than as separate components?
- Basis in paper: [explicit] The paper argues that failures arise from complex interactions between ML system components and users, suggesting that traditional component-wise solutions are insufficient.
- Why unresolved: The paper identifies the need for holistic design but doesn't provide concrete architectural frameworks or design methodologies that simultaneously address technical and user-centered failures.
- What evidence would resolve it: Case studies or prototype systems demonstrating successful holistic XAI design that reduces multiple types of failures through integrated system-user considerations.

## Limitations
- Framework relies on literature review rather than empirical validation in real-world deployments
- Distinction between system-specific and user-specific failures may blur when technical failures cascade into user misinterpretation
- Does not address severity or frequency of different failure types, nor provide quantitative guidance for prioritizing mitigation efforts
- Focuses primarily on individual explanation instances rather than longitudinal or interactive explanation scenarios

## Confidence
- High Confidence: The categorization of system-specific failures (misleading, competing, unstable, incompatible explanations) is well-supported by the literature and clearly delineated.
- Medium Confidence: The user-specific failure categories (mismatch, counterintuitive, biased inferences) are theoretically sound but may require empirical validation across diverse user populations.
- Medium Confidence: The proposed distinction between system and user failures is methodologically sound but may face practical challenges in real-world boundary cases.

## Next Checks
1. Conduct a user study comparing how practitioners actually diagnose XAI failures against the proposed typology's categorizations to identify any mismatches or ambiguities.
2. Test the framework's predictive validity by applying it to a set of documented XAI failures from deployed systems and evaluating whether the typology accurately identifies the root causes.
3. Evaluate the framework's completeness by attempting to classify failures from recent XAI papers published after the literature review period to identify any missing categories.