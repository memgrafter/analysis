---
ver: rpa2
title: 'MASSIVE Multilingual Abstract Meaning Representation: A Dataset and Baselines
  for Hallucination Detection'
arxiv_id: '2405.19285'
source_url: https://arxiv.org/abs/2405.19285
tags:
- sparql
- relations
- parsing
- hallucination
- detection
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: We introduce MASSIVE-AMR, the largest and most diverse multilingual
  AMR dataset, consisting of 84,000 text-to-graph annotations spanning 50+ typologically
  diverse languages. We evaluate large language models on multilingual AMR and SPARQL
  parsing, finding GPT-4 outperforms GPT-3.5 by 10-13% F1 on AMR parsing, while SPARQL
  parsing exceeds 90% executability across languages.
---

# MASSIVE Multilingual Abstract Meaning Representation: A Dataset and Baselines for Hallucination Detection

## Quick Facts
- arXiv ID: 2405.19285
- Source URL: https://arxiv.org/abs/2405.19285
- Reference count: 23
- Primary result: MASSIVE-AMR is the largest multilingual AMR dataset (84K annotations across 50+ languages), with GPT-4 outperforming GPT-3.5 by 10-13% F1 on AMR parsing, but hallucination detection remains challenging (0-17% accuracy)

## Executive Summary
This paper introduces MASSIVE-AMR, a large-scale multilingual AMR dataset spanning 50+ languages with 84,000 text-to-graph annotations. The authors evaluate large language models on multilingual AMR and SPARQL parsing tasks, finding GPT-4 significantly outperforms GPT-3.5 and that SPARQL parsing achieves higher performance than AMR parsing. Most critically, they demonstrate that while models frequently hallucinate SPARQL relations (16-71% across conditions), detection accuracy remains extremely low (0-17%) even with joint AMR-SPARQL models, highlighting fundamental challenges in using LLMs for structured parsing verification.

## Method Summary
The authors create MASSIVE-AMR by annotating 84K utterances across 50+ languages with AMR graphs, using localized entities for each language. They evaluate GPT-3.5 and GPT-4 on both AMR and SPARQL parsing using in-context learning with relation lists. For hallucination detection, they train joint AMR-SPARQL models (both in-context and fine-tuned) and compare confidence scores between representations. They also fine-tune LLaMA-13B and GPT-2-XLDISTILL on joint data (6K AMR + 3K SPARQL examples) and evaluate using AMR F1 (Smatch), SPARQL executability, and hallucination detection accuracy.

## Key Results
- GPT-4 outperforms GPT-3.5 by 10-13% F1 on AMR parsing across languages
- SPARQL parsing achieves >90% executability across languages, outperforming AMR parsing
- Models hallucinate SPARQL relations in 16-71% of cases but detection accuracy is only 0-17%
- Joint AMR-SPARQL models fail to effectively detect hallucinations despite the theoretical advantage of comparing confidence scores

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Joint AMR-SPARQL models can detect SPARQL relation hallucinations by comparing confidence between AMR and SPARQL parses.
- **Mechanism:** When a model produces both AMR and SPARQL parses, discrepancies in ranking (AMR higher than SPARQL) signal potential hallucinations. This works because AMR's open ontology makes hallucinations less likely than in SPARQL's fixed ontology.
- **Core assumption:** The model's confidence scores are comparable across AMR and SPARQL representations, and the model can meaningfully rank them.
- **Evidence anchors:**
  - [abstract] "we confirm models frequently hallucinate SPARQL relations (16-71% across conditions) but fail at detection (0-17% accuracy), even with joint AMR-SPARQL models"
  - [section 4] "we train a single semantic parser to parse both SPARQL and AMRs, simply mixing the training data (i.e. for multi-task learning), and produce multiple parse candidates in a target N-best"
- **Break condition:** If the model's confidence scores are not comparable across representations, or if the model cannot meaningfully rank AMRs and SPARQLs.

### Mechanism 2
- **Claim:** LLMs perform better SPARQL parsing than AMR parsing due to their pretraining on structured query data.
- **Mechanism:** LLMs have seen more SPARQL-like structures during pretraining, making them better at generating syntactically correct SPARQL queries than AMR graphs.
- **Core assumption:** LLMs' pretraining data includes substantial SPARQL-like structured query examples.
- **Evidence anchors:**
  - [section 5.7] "models display good performance for MASSIVE+, where AMR performance was observed to decrease, evidence that LLMs have more knowledge of SPARQL than AMR structures"
  - [section 5.7] "SPARQL parsing performance is high across languages, at least in small studies using the QALD9-AMR dataset"
- **Break condition:** If the LLM's pretraining data doesn't include significant SPARQL-like structures, or if the specific LLM architecture doesn't benefit from such pretraining.

### Mechanism 3
- **Claim:** Localized entities in MASSIVE-AMR increase parsing difficulty compared to datasets with shared entities.
- **Mechanism:** When each language has different entities (e.g., German questions about German entities vs. shared entities across languages), the model must handle more variability and potentially less Wikipedia coverage for each entity.
- **Core assumption:** Localized entities mean less cross-language entity overlap and potentially lower Wikipedia coverage for some entities.
- **Evidence anchors:**
  - [section 3] "localized entities for each language (see exs. Table 2)"
  - [section 5.7] "GPT-4 outperforms GPT-3.5 by a margin of 10-13% F1, with performance on QALD9 14-17% F1 higher than MASSIVE-AMR"
- **Break condition:** If the localized entities don't significantly reduce cross-language overlap or if Wikipedia coverage is actually better for localized entities.

## Foundational Learning

- **Concept:** Abstract Meaning Representation (AMR) graph structure
  - **Why needed here:** Understanding AMR is essential for interpreting the dataset and evaluating parsing results
  - **Quick check question:** What are the basic components of an AMR graph (nodes, edges, concepts)?

- **Concept:** SPARQL query structure
  - **Why needed here:** SPARQL is the target structured representation being parsed and evaluated
  - **Quick check question:** What are the main components of a SPARQL query (SELECT, WHERE, prefixes, etc.)?

- **Concept:** Semantic relation hallucination
  - **Why needed here:** The core problem being addressed - detecting when models generate relations not in the target ontology
  - **Quick check question:** What distinguishes an "easy" hallucination (non-existent relation) from a "hard" hallucination (ambiguous relation)?

## Architecture Onboarding

- **Component map:** MASSIVE-AMR dataset creation → LLM-based parsing (AMR, SPARQL, joint) → Hallucination detection evaluation → Experimental comparison
- **Critical path:** Data creation → Model training/inference → Hallucination detection → Evaluation → Analysis
- **Design tradeoffs:** In-context learning vs. fine-tuning (faster vs. potentially more accurate); joint models vs. separate models (potential synergy vs. complexity)
- **Failure signatures:** Poor hallucination detection (0-17% accuracy), low AMR parsing F1 compared to SOTA, SPARQL queries returning no results from DBpedia
- **First 3 experiments:**
  1. Test AMR parsing with GPT-4 in-context learning on MASSIVE-AMR vs QALD9 to establish baseline difficulty
  2. Test SPARQL parsing with GPT-3.5/4 on the same datasets to compare relative performance
  3. Test hallucination detection with a simple in-context learning approach using controlled relation perturbations

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How does the inclusion of localized entities in MASSIVE-AMR affect the performance of AMR parsing models compared to using shared entities across languages?
- **Basis in paper:** Explicit - The paper discusses the use of localized entities in MASSIVE-AMR and compares it to QALD9-AMR which uses shared entities.
- **Why unresolved:** The paper does not provide a direct comparison of parsing performance between localized and shared entities.
- **What evidence would resolve it:** Conducting experiments comparing AMR parsing performance on MASSIVE-AMR with localized entities and a version with shared entities.

### Open Question 2
- **Question:** What is the impact of the long-tail nature of questions in MASSIVE-AMR on the effectiveness of LLM-based AMR parsing models?
- **Basis in paper:** Explicit - The paper mentions that MASSIVE-AMR includes long-tail questions and discusses their characteristics.
- **Why unresolved:** The paper does not analyze the specific impact of long-tail questions on parsing performance.
- **What evidence would resolve it:** Analyzing parsing performance on subsets of MASSIVE-AMR categorized by question complexity and comparing it to more standard questions.

### Open Question 3
- **Question:** How does the ratio of AMR to SPARQL examples in fine-tuning data affect the performance of joint AMR-SPARQL models in hallucination detection?
- **Basis in paper:** Explicit - The paper discusses the use of fine-tuned joint models and mentions the ratio of AMR to SPARQL examples.
- **Why unresolved:** The paper does not explore the effect of varying this ratio on model performance.
- **What evidence would resolve it:** Training and evaluating joint models with different ratios of AMR to SPARQL examples and comparing their hallucination detection performance.

## Limitations
- Hallucination detection remains extremely challenging with joint AMR-SPARQL models achieving only 0-17% accuracy
- The paper relies on in-context learning which may be less effective than fine-tuning for complex tasks
- Localized entities may reduce cross-language entity overlap and potentially lower Wikipedia coverage

## Confidence
- Mechanism 1 (Joint detection): Low - experimental results show near-zero detection accuracy
- Mechanism 2 (SPARQL vs AMR performance): Medium - supported by results but causal mechanism not verified
- Mechanism 3 (Localized entities difficulty): Medium-Low - plausible but evidence is correlational

## Next Checks
1. Test whether fine-tuning the joint model on balanced AMR-SPARQL data improves hallucination detection accuracy above random chance
2. Verify that the LLM's pretraining data actually contains SPARQL-like structures through dataset analysis
3. Measure Wikipedia coverage and cross-language entity overlap in MASSIVE-AMR to directly test the localized entities hypothesis