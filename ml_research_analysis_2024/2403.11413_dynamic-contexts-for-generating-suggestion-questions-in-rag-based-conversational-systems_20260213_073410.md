---
ver: rpa2
title: Dynamic Contexts for Generating Suggestion Questions in RAG Based Conversational
  Systems
arxiv_id: '2403.11413'
source_url: https://arxiv.org/abs/2403.11413
tags:
- questions
- dynamic
- contexts
- suggestion
- question
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Dynamic Contexts, a novel approach for generating
  suggestion questions in Retrieval-Augmented Generation (RAG)-based conversational
  systems. The method addresses the challenge of users not fully understanding system
  capabilities, leading to ambiguous queries.
---

# Dynamic Contexts for Generating Suggestion Questions in RAG Based Conversational Systems

## Quick Facts
- arXiv ID: 2403.11413
- Source URL: https://arxiv.org/abs/2403.11413
- Authors: Anuja Tayal; Aman Tyagi
- Reference count: 19
- Outperforms baseline prompting strategies (Zero-Shot, Few-Shot, Dynamic Few-Shot) in generating relevant suggestion questions for RAG-based conversational systems

## Executive Summary
This paper introduces Dynamic Contexts, a novel approach for generating suggestion questions in Retrieval-Augmented Generation (RAG)-based conversational systems. The method addresses the challenge of users not fully understanding system capabilities, leading to ambiguous queries. Dynamic Contexts leverages both dynamic few-shot examples and dynamically retrieved contexts to generate relevant and answerable suggestion questions. Experiments demonstrate that this approach outperforms other prompting strategies, including Zero-Shot, Few-Shot, and Dynamic Few-Shot methods, across multiple large language models.

## Method Summary
The Dynamic Contexts approach combines dynamic few-shot examples and dynamically retrieved contexts to generate suggestion questions for RAG-based conversational systems. It uses OpenAI embeddings and cosine similarity to retrieve 4 relevant passages from a dataset of 228 blog posts (with 35 manually annotated QAS triplets) based on the user's initial query. The approach dynamically selects 3 triplets most similar to the user's query as examples, then constructs a prompt combining the query, retrieved contexts, and examples for LLM generation. The method was tested with ChatGPT, GPT-4, and Claude-2 across 35 annotated posts, with results evaluated through manual assessment and Preference Benchmarking.

## Key Results
- Dynamic Contexts outperforms Zero-Shot, Few-Shot, and Dynamic Few-Shot approaches in generating relevant and answerable suggestion questions
- Manual evaluation shows improved correctness, relevance, and soundness of generated questions
- Preference Benchmarking with GPT-4 and Claude-2 as judges confirms the superiority of Dynamic Contexts
- The approach demonstrates effectiveness across multiple LLMs (ChatGPT, GPT-4, Claude-2)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Dynamic Contexts generate more relevant and answerable suggestion questions by leveraging dynamically retrieved contexts that are specific to the user's query.
- Mechanism: The approach retrieves 4 relevant passages from the dataset based on the user's initial query using cosine similarity of OpenAI embeddings. These passages form the "dynamic contexts" that guide the generation of suggestion questions.
- Core assumption: Relevant passages can be effectively retrieved from the dataset to provide sufficient context for generating answerable questions.
- Evidence anchors:
  - [abstract]: "To generate suggestion questions, our approach involves utilizing dynamic context, which includes both dynamic few-shot examples and dynamically retrieved contexts."
  - [section 4]: "To ensure that the questions generated by the Suggestion Question Generator remain pertinent and answerable, dynamic contexts consist of dynamically retrieved contexts."
  - [corpus]: Weak evidence - The corpus doesn't provide specific examples of the retrieved contexts or how they were used to generate questions.
- Break condition: If the retrieved passages are not sufficiently relevant to the user's query or if the dataset doesn't contain enough relevant information, the generated questions may not be answerable or relevant.

### Mechanism 2
- Claim: Dynamic few-shot examples improve the quality of generated questions by providing contextually relevant examples for the LLM to learn from.
- Mechanism: Instead of using a static set of examples, the approach dynamically selects 3 triplets (question, answer, suggestion question) from the dataset that are most similar to the user's query. These examples guide the LLM in generating similar but novel questions.
- Core assumption: Dynamically selected examples that are similar to the user's query will be more effective in guiding the generation of relevant suggestion questions than static examples.
- Evidence anchors:
  - [abstract]: "Dynamic Contexts leverages both dynamic few-shot examples and dynamically retrieved contexts to generate relevant and answerable suggestion questions."
  - [section 4]: "Dynamic few-shot diverges from traditional few-shot prompting by dynamically choosing each example based on the user's query, rather than relying on a static set of examples."
  - [corpus]: Weak evidence - The corpus doesn't provide specific examples of the dynamic few-shot examples or how they were used.
- Break condition: If the dynamically selected examples are not sufficiently diverse or if they don't cover the range of question types needed, the generated questions may be too similar to the examples or may not be relevant to the user's query.

### Mechanism 3
- Claim: The combination of dynamic few-shot examples and dynamically retrieved contexts provides a rich context for the LLM to generate relevant and answerable suggestion questions.
- Mechanism: The final prompt includes both the dynamically retrieved contexts and the dynamic few-shot examples. This combination provides the LLM with both specific information about the user's query and examples of how to generate relevant questions.
- Core assumption: Providing both specific context and examples will be more effective than providing either alone in guiding the generation of relevant questions.
- Evidence anchors:
  - [abstract]: "Dynamic Contexts leverages both dynamic few-shot examples and dynamically retrieved contexts to generate relevant and answerable suggestion questions."
  - [section 4]: "Finally, we generate suggestion questions by constituting a dynamic context prompt consisting of the dynamically retrieved contexts, and dynamic few-shot examples."
  - [corpus]: Weak evidence - The corpus doesn't provide the complete prompt structure or examples of how the combination was used.
- Break condition: If the combination of contexts and examples is too complex or if it confuses the LLM, it may lead to poor quality questions or failure to generate questions at all.

## Foundational Learning

- Concept: Retrieval-Augmented Generation (RAG)
  - Why needed here: RAG is the underlying technique used to retrieve relevant passages from the dataset based on the user's query. Understanding RAG is crucial for understanding how the dynamic contexts are generated.
  - Quick check question: How does RAG combine retrieval and generation to answer questions? (Expected answer: RAG retrieves relevant passages from a dataset and uses them as context for a language model to generate an answer.)

- Concept: In-context learning
  - Why needed here: The approach uses in-context learning by providing the LLM with examples (dynamic few-shot examples) to guide the generation of suggestion questions. Understanding in-context learning is important for understanding how the LLM is prompted.
  - Quick check question: What is in-context learning and how does it differ from traditional fine-tuning? (Expected answer: In-context learning involves providing the model with examples in the prompt to guide its behavior, without updating the model's parameters. It differs from fine-tuning, which involves updating the model's parameters on a specific dataset.)

- Concept: Few-shot learning
  - Why needed here: The approach uses few-shot learning by providing the LLM with a small number of examples (dynamic few-shot examples) to learn from. Understanding few-shot learning is important for understanding how the LLM can generalize from a small number of examples.
  - Quick check question: What is few-shot learning and how does it enable models to learn from a small number of examples? (Expected answer: Few-shot learning is a paradigm where a model learns to perform a task from a small number of examples. It enables models to generalize from limited data by leveraging prior knowledge and learning to learn from examples.)

## Architecture Onboarding

- Component map:
  1. User query: The initial question posed by the user.
  2. Dynamic context retrieval: Retrieves 4 relevant passages from the dataset based on the user's query.
  3. Dynamic few-shot example selection: Selects 3 triplets (question, answer, suggestion question) from the dataset that are most similar to the user's query.
  4. Prompt construction: Constructs the final prompt by combining the user query, dynamic contexts, and dynamic few-shot examples.
  5. LLM generation: The LLM generates 3 suggestion questions based on the prompt.
  6. Output: The 3 generated suggestion questions are presented to the user.

- Critical path: User query -> Dynamic context retrieval -> Dynamic few-shot example selection -> Prompt construction -> LLM generation -> Output

- Design tradeoffs:
  - Number of retrieved contexts: More contexts may provide more information but could also make the prompt too long or confusing for the LLM.
  - Number of dynamic few-shot examples: More examples may provide more guidance but could also lead to the LLM simply copying the examples rather than generating novel questions.
  - Choice of LLM: Different LLMs may have different capabilities and biases that could affect the quality of the generated questions.

- Failure signatures:
  - Irrelevant or unanswerable questions: This could indicate that the retrieved contexts or dynamic few-shot examples are not sufficiently relevant to the user's query.
  - Questions that are too similar to the examples: This could indicate that the dynamic few-shot examples are too influential and are causing the LLM to simply copy them.
  - Failure to generate questions: This could indicate that the prompt is too complex or that the LLM is not capable of generating questions in the given context.

- First 3 experiments:
  1. Vary the number of retrieved contexts (e.g., 2, 4, 6) and measure the impact on the quality of generated questions.
  2. Vary the number of dynamic few-shot examples (e.g., 1, 3, 5) and measure the impact on the diversity and relevance of generated questions.
  3. Compare the performance of different LLMs (e.g., ChatGPT, GPT-4, Claude-2) on the task of generating suggestion questions.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the Dynamic Contexts approach perform when personalized based on user history?
- Basis in paper: [explicit] The paper mentions future plans to personalize suggestion questions based on user history.
- Why unresolved: The paper does not provide any results or insights on the performance of the Dynamic Contexts approach when personalized based on user history.
- What evidence would resolve it: Experiments comparing the performance of the Dynamic Contexts approach with and without personalization based on user history.

### Open Question 2
- Question: How does the Dynamic Contexts approach handle numerical reasoning tasks, particularly those involving age calculations?
- Basis in paper: [explicit] The paper mentions that the dataset used presents significant challenges in showcasing the numerical reasoning capabilities of LLMs, particularly when handling queries related to a baby's specific age.
- Why unresolved: The paper does not provide a detailed analysis of how the Dynamic Contexts approach handles numerical reasoning tasks, nor does it discuss any potential improvements or limitations in this area.
- What evidence would resolve it: A comprehensive evaluation of the Dynamic Contexts approach on a variety of numerical reasoning tasks, including age calculations, with a focus on accuracy and robustness.

### Open Question 3
- Question: How does the order of query and contexts in the prompt affect the performance of the Dynamic Contexts approach?
- Basis in paper: [explicit] The paper mentions an ablation study conducted to determine the optimal order of the final prompt, altering the arrangement of example contexts and queries.
- Why unresolved: The paper does not provide a detailed analysis of how the order of query and contexts affects the performance of the Dynamic Contexts approach, nor does it discuss any potential improvements or limitations in this area.
- What evidence would resolve it: Experiments comparing the performance of the Dynamic Contexts approach with different orders of query and contexts, with a focus on accuracy and relevance of the generated suggestion questions.

## Limitations
- Limited generalizability due to single-domain evaluation (baby sleep coaching)
- Manual evaluation introduces potential rater bias and limits reproducibility
- Computational efficiency concerns not addressed, particularly for real-time applications

## Confidence
- **High Confidence**: The core mechanism of combining dynamic few-shot examples with retrieved contexts is technically sound and well-grounded in existing RAG literature.
- **Medium Confidence**: The empirical superiority over baseline methods is demonstrated, but the limited dataset and single-domain evaluation reduce confidence in broad applicability.
- **Low Confidence**: The long-term effectiveness and user experience improvements in real-world conversational systems remain unproven.

## Next Checks
1. **Cross-domain validation**: Test the Dynamic Contexts approach on diverse conversational domains (technical support, healthcare, education) to assess generalizability beyond the baby sleep coaching dataset.

2. **A/B user testing**: Conduct controlled user studies comparing the Dynamic Contexts approach against baseline methods in live conversational systems, measuring actual user engagement and query resolution rates.

3. **Efficiency benchmarking**: Measure and optimize the computational overhead of dynamic context retrieval and few-shot example selection, particularly for high-traffic conversational systems with real-time requirements.