---
ver: rpa2
title: 'History repeats Itself: A Baseline for Temporal Knowledge Graph Forecasting'
arxiv_id: '2404.16726'
source_url: https://arxiv.org/abs/2404.16726
tags:
- baseline
- recurrency
- temporal
- knowledge
- graph
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a simple yet effective baseline for temporal
  knowledge graph (TKG) forecasting that predicts recurring facts. The method scores
  quadruples based on their past occurrences, with variants including strict recurrence,
  relaxed recurrence, and a combination.
---

# History repeats Itself: A Baseline for Temporal Knowledge Graph Forecasting

## Quick Facts
- arXiv ID: 2404.16726
- Source URL: https://arxiv.org/abs/2404.16726
- Reference count: 15
- Primary result: Simple recurrence-based baseline achieves top performance on temporal knowledge graph forecasting, challenging perceived progress in the field

## Executive Summary
This paper introduces a simple yet effective baseline for temporal knowledge graph (TKG) forecasting that predicts recurring facts. The method scores quadruples based on their past occurrences, with variants including strict recurrence, relaxed recurrence, and a combination. When compared to 11 state-of-the-art TKG methods across five datasets, the baseline ranks first or third in three datasets, achieving notably high MRR (e.g., 90.9% on YAGO). This challenges the perceived progress in TKG forecasting and highlights the importance of simple baselines in evaluating model performance.

## Method Summary
The baseline operates on quadruples (s, r, o, t) and uses recurrence-based scoring functions to predict future facts. Three variants are proposed: strict recurrency scores based on exact past occurrences with time weighting, relaxed recurrency scores based on co-occurrence frequencies, and a combined approach using relation-specific hyperparameters. The method requires minimal hyperparameter tuning and no iterative training, making it computationally efficient while achieving competitive results.

## Key Results
- The recurrence baseline ranks first or third across three of five datasets
- Achieves 90.9% MRR on YAGO, significantly outperforming state-of-the-art models
- Strict recurrency variant works well on datasets with high recurrency degrees (YAGO: 92.7%, WIKI: 87.0%)
- Combined recurrency baseline with relation-specific hyperparameters shows consistent strong performance across all datasets

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Strict recurrency scores future quadruples by checking if the exact triple occurred in the past
- Mechanism: The scoring function assigns 1 if the triple was seen before and 0 otherwise, optionally weighted by temporal proximity
- Core assumption: Facts in temporal knowledge graphs often repeat over time
- Evidence anchors:
  - [abstract] "based on predicting recurring facts"
  - [section] "we design an intuitive baseline for TKG Forecasting based on the principle that something that happened in the past will happen again in the future"
- Break condition: If the dataset contains mostly novel facts or facts with no repetitive pattern, strict recurrency will fail

### Mechanism 2
- Claim: Relaxed recurrency assigns scores based on how often parts of a triple (subject or object) appeared with the same relation in the past
- Mechanism: For a query (s, r, ?, t+), it computes the normalized frequency of object o appearing with relation r for any subject; similarly for head queries
- Core assumption: Even if the exact triple never occurred, related triples can inform predictions
- Evidence anchors:
  - [section] "we are interested in how often parts of the triple have been observed in the data"
  - [section] "This scoring function suffers from the problem that it does not take the temporal distance into account"
- Break condition: If relations have no clear pattern of object/subject co-occurrence, relaxed recurrency will be uninformative

### Mechanism 3
- Claim: Combining strict and relaxed recurrency leverages both exact repetition and related patterns
- Mechanism: A weighted linear combination of strict recurrency scores (ψ∆) and relaxed recurrency scores (ξ) with relation-specific hyperparameters α and λ
- Core assumption: Different relations benefit from different balances of exact vs. related recurrence
- Evidence anchors:
  - [section] "we conclude the section with a linear combination of the Strict Recurrency Baseline ψ∆λ and the Relaxed Recurrency Baseline ξ"
  - [section] "Results for ψ∆ξ on all datasets reflect the reported values of the recurrency degree"
- Break condition: If neither strict nor relaxed recurrency patterns exist for a relation, the combination will not help

## Foundational Learning

- Concept: Temporal Knowledge Graph (TKG)
  - Why needed here: The baseline operates on quadruples (s, r, o, t) where t is time; understanding this format is essential to apply the recurrence scoring
  - Quick check question: What are the four components of a quadruple in a TKG?

- Concept: Scoring function and ranking equivalence
  - Why needed here: The baseline uses scoring functions to rank entities for future predictions; ranking equivalence allows comparison of different time-weighting functions
  - Quick check question: When are two scoring functions considered ranking-equivalent?

- Concept: Hyperparameter tuning for relation-specific decay
  - Why needed here: The baseline tunes λ and α per relation to balance recency vs. frequency; understanding this process is key to reproducing results
  - Quick check question: How many hyperparameter combinations are evaluated per relation in the experiments?

## Architecture Onboarding

- Component map: The baseline consists of three scoring variants: strict recurrency (ϕ∆), relaxed recurrency (ξ), and combined recurrency (ψ∆ξ). Each variant is a function from quadruples to scores
- Critical path: For a given query, compute scores using the chosen variant, rank entities by score, and output the top-ranked entities
- Design tradeoffs: Strict recurrency is simple but fails on unseen triples; relaxed recurrency handles unseen triples but may be less precise; combination balances both but adds hyperparameters
- Failure signatures: Low MRR when dataset has few recurring facts; poor performance on symmetric relations if not handled; incompatible types in predictions indicate model failure
- First 3 experiments:
  1. Apply strict recurrency to YAGO dataset and compare MRR to published results
  2. Tune λ and α per relation on ICEWS14 and evaluate combined recurrency performance
  3. Analyze per-relation MRR to identify relations where the baseline underperforms compared to state-of-the-art models

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do recurrency-based baselines perform on multi-step TKG forecasting compared to single-step?
- Basis in paper: [explicit] The authors mention "We report results for the multi-step setting in the supplementary material" but do not provide detailed analysis in the main paper
- Why unresolved: The main paper focuses on single-step forecasting, leaving multi-step performance analysis incomplete
- What evidence would resolve it: Comprehensive multi-step forecasting results showing MRR and H@10 metrics for recurrency baselines compared to state-of-the-art methods

### Open Question 2
- Question: What is the theoretical upper bound for recurrency-based methods on TKGs with different recurrency degrees?
- Basis in paper: [inferred] The authors show high performance on datasets with high recurrency degrees (YAGO: 92.7%, WIKI: 87.0%) but don't establish theoretical limits
- Why unresolved: The paper demonstrates empirical success but doesn't provide theoretical analysis of when recurrency-based methods should be optimal
- What evidence would resolve it: Mathematical analysis proving conditions under which recurrency-based methods achieve optimal performance, including relation-specific bounds

### Open Question 3
- Question: How can recurrency-based baselines be extended to handle multi-relational patterns beyond simple recurrence?
- Basis in paper: [explicit] The authors note "none of the methods proposed so far can accomplish the results achieved by a combination of two very naive baselines for two out of five datasets" suggesting limitations in capturing complex patterns
- Why unresolved: The current baseline only captures simple recurrence patterns and doesn't address more complex temporal or relational dependencies
- What evidence would resolve it: Empirical results showing improved performance when recurrency baselines incorporate multi-hop reasoning or temporal pattern detection capabilities

## Limitations
- Performance on datasets with low recurrency degrees (novel facts) remains untested
- Relation-specific hyperparameter tuning may not scale well to datasets with many relations
- The method may not capture complex temporal or multi-relational patterns beyond simple recurrence

## Confidence
- Central claim about baseline effectiveness: High confidence (strong empirical results across multiple datasets)
- Generalizability to low-recurrency datasets: Medium confidence (not tested)
- Scalability to many relations: Medium confidence (relation-specific tuning may become burdensome)

## Next Checks
1. Test the baseline on synthetic or real datasets specifically designed to have low recurrency (novel facts dominate) to identify the breaking point of recurrence-based approaches
2. Measure the computational cost and hyperparameter sensitivity when scaling to datasets with 100+ relations to assess practical limitations
3. Implement the time-weighting function ∆(t+, k) with different monotonic transformations to verify the claim that ranking-equivalent functions yield identical results