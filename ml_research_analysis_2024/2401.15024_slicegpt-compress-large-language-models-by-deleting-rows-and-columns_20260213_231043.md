---
ver: rpa2
title: 'SliceGPT: Compress Large Language Models by Deleting Rows and Columns'
arxiv_id: '2401.15024'
source_url: https://arxiv.org/abs/2401.15024
tags:
- matrix
- llama
- dense
- slicegpt
- network
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'SliceGPT introduces a new post-training sparsification method
  for large language models that reduces embedding dimensions by deleting rows and
  columns from weight matrices. The key insight is computational invariance: applying
  orthogonal transformations to transformer weights leaves model predictions unchanged,
  allowing principal component analysis on intermediate signals to guide structured
  pruning.'
---

# SliceGPT: Compress Large Language Models by Deleting Rows and Columns

## Quick Facts
- arXiv ID: 2401.15024
- Source URL: https://arxiv.org/abs/2401.15024
- Reference count: 40
- Key outcome: Removes up to 25% of parameters while maintaining 99%, 99%, and 90% of dense model performance on LLAMA2-70B, OPT 66B, and Phi-2 respectively

## Executive Summary
SliceGPT introduces a novel post-training sparsification method for large language models that achieves compression by deleting entire rows and columns from weight matrices. The key insight is computational invariance - applying orthogonal transformations to transformer weights leaves model predictions unchanged, enabling structured pruning based on principal component analysis of intermediate signals. The method achieves significant parameter reduction (up to 25%) while maintaining near-original model performance, and provides faster inference on consumer GPUs compared to unstructured pruning methods.

## Method Summary
SliceGPT compresses LLMs through a single-step post-training process that leverages orthogonal transformations and PCA. The method converts LayerNorm to RMSNorm, computes orthogonal transformations Qℓ using PCA on transformed signals, applies these transformations to all weight matrices and residual connections, then deletes rows/columns corresponding to minor principal components. The process requires only a single GPU and 1-3 hours for compression, with optional recovery fine-tuning taking 1-5 hours total. Unlike unstructured pruning methods, SliceGPT maintains dense matrix multiplication patterns that hardware can optimize, enabling faster inference on both consumer and datacenter GPUs.

## Key Results
- Removes up to 25% of parameters while maintaining 99% (LLAMA2-70B), 99% (OPT 66B), and 90% (Phi-2) of dense model zero-shot performance
- On consumer GPUs (24GB), inference compute reduced to 64% of dense models; on A100 GPUs (40GB), to 66%
- Requires only a single GPU and 1-3 hours to compress models
- Outperforms unstructured pruning methods in inference speed despite similar parameter reduction

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Orthogonal transformations leave transformer predictions invariant, enabling parameter deletion without accuracy loss
- **Mechanism**: Applying orthogonal matrix Q before RMSNorm and Q⊤ after RMSNorm commutes with normalization, so predictions remain unchanged
- **Core assumption**: RMSNorm's normalization step is invariant to orthogonal transformations of its input
- **Evidence anchors**: Theorem 1 proves transformed weight matrices produce identical outputs; no related papers mention this invariance property
- **Break condition**: If normalization layers aren't properly converted or residual connections aren't adjusted with Q⊤ℓ-1Qℓ

### Mechanism 2
- **Claim**: PCA on transformed signals identifies which rows/columns can be deleted with minimal impact
- **Mechanism**: After applying orthogonal transformations Qℓ, PCA identifies principal components; deleting minor components reduces embedding dimension while preserving information
- **Core assumption**: Principal components of transformed signals capture most important information flow between layers
- **Evidence anchors**: "we use PCA...such that the signal between blocks is projected onto its principal components"
- **Break condition**: If PCA computed in single precision instead of double, numerical errors degrade accuracy

### Mechanism 3
- **Claim**: Structured deletion provides computational speedup that unstructured pruning cannot achieve
- **Mechanism**: Deleting entire rows/columns maintains dense matrix patterns that hardware optimizes, while unstructured pruning creates irregular sparsity
- **Core assumption**: Hardware acceleration favors dense matrix operations over sparse ones
- **Evidence anchors**: "our sliced models run on fewer GPUs and run faster without any additional code optimization"
- **Break condition**: If hardware doesn't support efficient dense matrix operations for reduced dimensions

## Foundational Learning

- **Concept**: Computational invariance under orthogonal transformations
  - Why needed here: This is the foundational mathematical insight that enables the entire approach
  - Quick check question: Why does multiplying a signal by orthogonal matrix Q before RMSNorm and Q⊤ after RMSNorm leave the output unchanged?

- **Concept**: Principal Component Analysis (PCA) and eigenvalue decomposition
  - Why needed here: PCA identifies which components of transformed signals carry most information, guiding row/column deletion
  - Quick check question: How does the eigenvalue spectrum of transformed signals relate to importance of different principal components?

- **Concept**: Transformer architecture components and their interactions
  - Why needed here: Understanding attention blocks, FFN blocks, and normalization layer interactions is crucial for correctly applying transformations
  - Quick check question: What modification must be made to residual connections when applying different orthogonal transformations at each layer?

## Architecture Onboarding

- **Component map**: Weight matrices (Win, Wout) for each transformer layer -> LayerNorm/RMSNorm operations -> Residual connections -> Embedding matrix and head matrix -> Orthogonal transformation matrices Qℓ for each layer

- **Critical path**:
  1. Convert LayerNorm to RMSNorm by absorbing scales into adjacent matrices
  2. Compute orthogonal transformations Qℓ using PCA on transformed signals
  3. Apply transformations to all weight matrices and residual connections
  4. Delete rows/columns based on PCA results
  5. Fine-tune if needed to recover accuracy

- **Design tradeoffs**:
  - Single transformation vs layer-specific transformations: Different Qℓ per layer allows better compression but requires Q⊤ℓ-1Qℓ operations in residuals
  - Calibration dataset size vs accuracy: Larger calibration sets improve PCA quality but increase computation time
  - Precision for PCA: Double precision prevents numerical errors but increases computation cost

- **Failure signatures**:
  - Accuracy degradation without recovery: Likely caused by improper PCA computation or insufficient calibration data
  - No speedup despite parameter reduction: May indicate hardware doesn't efficiently handle new matrix dimensions
  - Memory errors during PCA: Calibration set or sequence length too large for available GPU memory

- **First 3 experiments**:
  1. Apply SliceGPT to small transformer (OPT-125M) and verify WikiText-2 perplexity matches dense model
  2. Vary calibration set size (32, 128, 1024) and measure impact on perplexity to find optimal tradeoff
  3. Compare speedup on consumer GPU (24GB) vs datacenter GPU (40GB) to understand hardware dependency

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the computational invariance principle extend to other neural network architectures beyond transformers?
- Basis in paper: Authors express hope it will inspire future work on other architectures
- Why unresolved: Only demonstrated on transformer networks
- What evidence would resolve it: Experimental validation on convolutional or graph neural networks

### Open Question 2
- Question: What is the optimal strategy for layer-wise slicing that maximizes compression while minimizing performance degradation?
- Basis in paper: Different layers have different eigenvalue distributions suggesting varying slicing could be applied
- Why unresolved: Only uniform slicing percentages explored in ablation study
- What evidence would resolve it: Systematic experiments testing various layer-wise slicing strategies

### Open Question 3
- Question: How does SliceGPT compare to other compression methods like quantization when combined?
- Basis in paper: Authors mention complementary methods including quantization could be used
- Why unresolved: Only evaluates SliceGPT in isolation
- What evidence would resolve it: Experimental results comparing combined methods

## Limitations
- Computational invariance property relies on specific architectural details that may not generalize to all transformer variants
- PCA-based component selection effectiveness assumed rather than empirically validated across different model architectures
- Speedup claims highly dependent on specific GPU architectures and memory configurations

## Confidence
- Claim Cluster 1 (25% parameter reduction with minimal accuracy loss): High confidence - extensive empirical results across multiple models
- Claim Cluster 2 (Single GPU, 1-3 hour compression time): High confidence - explicitly stated timing estimates consistent with typical PCA computation
- Claim Cluster 3 (Faster inference without code optimization): Medium confidence - depends heavily on hardware specifics not fully detailed

## Next Checks
1. Reproduce the method using both FP32 and FP64 precision for PCA calculations to verify single precision causes numerical errors
2. Apply SliceGPT to transformer variants with different normalization layers to test computational invariance across architectures
3. Test inference speedup on a wider range of GPU configurations and batch sizes to validate claimed compute reduction