---
ver: rpa2
title: 'The Synergy between Data and Multi-Modal Large Language Models: A Survey from
  Co-Development Perspective'
arxiv_id: '2407.08583'
source_url: https://arxiv.org/abs/2407.08583
tags:
- data
- mllms
- arxiv
- wang
- zhang
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper surveys the synergy between multi-modal data and large
  language models (MLLMs) from a data-model co-development perspective. It systematically
  reviews how data-centric approaches enhance MLLM capabilities and how MLLMs, in
  turn, facilitate multi-modal data curation.
---

# The Synergy between Data and Multi-Modal Large Language Models: A Survey from Co-Development Perspective

## Quick Facts
- **arXiv ID**: 2407.08583
- **Source URL**: https://arxiv.org/abs/2407.08583
- **Reference count**: 40
- **Key outcome**: This survey systematically reviews the synergy between multi-modal data and large language models (MLLMs) from a data-model co-development perspective, categorizing data contributions into scaling and usability improvements, and model contributions into data synthesis and insights provision.

## Executive Summary
This survey explores the co-development of multi-modal data and large language models (MLLMs), identifying a virtuous cycle where improved data enhances MLLM capabilities and enhanced MLLMs improve data curation. The paper formalizes this relationship through a bi-level optimization framework and categorizes existing works into data contributions for MLLMs and model contributions for data. It identifies the current reliance on well-trained models or human assistance in data-model co-development and proposes a roadmap for future research, including infrastructure development and self-boosted MLLM development paradigms.

## Method Summary
The survey systematically reviews existing literature on MLLMs from a data-model co-development perspective, organizing contributions into two main categories: data contributions for MLLMs (scaling and usability improvements) and model contributions for data (synthesis and insights provision). The authors propose a bi-level optimization framework where data curation and model training are optimized alternatively, creating a feedback loop. The methodology involves categorizing existing works, analyzing how data-centric approaches enhance MLLM capabilities, and how MLLMs assist in data curation, while identifying limitations and proposing future research directions.

## Key Results
- Data-model co-development for MLLMs operates through a bi-level optimization framework where data curation and model training are alternately optimized
- Current co-development paradigms primarily rely on externally-boosted approaches using well-trained models or human assistance
- The survey identifies four key roles for MLLMs in data curation: navigator, extractor, analyzer, and visualizer
- Future directions include developing infrastructure for data-model co-development and exploring self-boosted MLLM development paradigms

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Data and models co-develop through a virtuous cycle where improved data enhances MLLM capabilities, and enhanced MLLM capabilities in turn improve data curation.
- Mechanism: The paper formalizes this as a bi-level optimization problem where data curation (Eq. 2) and model training (Eq. 1) are optimized alternatively. This creates a feedback loop where better models can curate better data, which trains better models.
- Core assumption: The similarity function Q(·,·) can effectively measure the quality of both data and model outputs, enabling meaningful optimization.
- Evidence anchors:
  - [abstract]: "The co-development of multi-modal data and MLLMs requires a clear view of 1) at which development stages of MLLMs specific data-centric approaches can be employed to enhance certain MLLM capabilities, and 2) how MLLMs, utilizing those capabilities, can contribute to multi-modal data in specific roles."
  - [section]: "Data-model co-development can be classified into two paradigms based on the optimization tools for D: 1) (Self-Boosted Paradigm). The model w to be trained is also used to improve dataset D... 2) (Externally-Boosted Paradigm). As an alternative, D can be curated with a well-trained model w* such as GPT-4V..."
- Break condition: If the similarity function Q(·,·) cannot effectively measure data and model quality, or if the optimization becomes computationally intractable.

### Mechanism 2
- Claim: Scaling up MLLMs requires both increasing data quantity and improving data quality through targeted curation strategies.
- Mechanism: The paper categorizes data contributions into scaling (increasing cardinality) and scaling effectiveness (improving quality and organization). This dual approach addresses both the need for massive datasets and the need for high-quality, well-organized data.
- Core assumption: Simply increasing data quantity leads to diminishing returns, and targeted quality improvements are necessary for effective scaling.
- Evidence anchors:
  - [abstract]: "As LLMs and MLLMs rely on vast amounts of model parameters and data to achieve emergent capabilities, the importance of data is receiving increasingly widespread attention and recognition."
  - [section]: "Simply increasing the amount of data is not economical [25]. It is indicated that only exponential data growth can lead to linear performance improvements of models [52]."
- Break condition: If the cost of data quality improvements outweighs the benefits, or if quality improvements do not translate to better model performance.

### Mechanism 3
- Claim: MLLMs can serve as data scientists, providing insights and automating data curation tasks that traditionally required human effort.
- Mechanism: The paper identifies four roles for MLLMs in data curation: navigator, extractor, analyzer, and visualizer. These roles leverage MLLM capabilities like knowledge retrieval, information extraction, and data analysis to automate data science workflows.
- Core assumption: MLLMs have sufficient capabilities in understanding, reasoning, and generation to effectively perform data science tasks.
- Evidence anchors:
  - [abstract]: "we systematically review existing works related to MLLMs from the data-model co-development perspective. A regularly maintained project associated with this survey is accessible at https://github.com/modelscope/data-juicer/blob/main/docs/awesome_llm_data.md."
  - [section]: "With the emerging capabilities of MLLMs, they can help data scientists to navigate data consumers to desired data and knowledge (§6.1), extract information and relationships from data (§6.2), provide data insights and statistics (§6.3), and reduce the manual involvements in data visualization (§6.4)."
- Break condition: If MLLMs lack the necessary capabilities for specific data science tasks, or if human oversight remains essential for quality control.

## Foundational Learning

- Concept: Bi-level optimization
  - Why needed here: Understanding the formal definition of data-model co-development as a bi-level optimization problem is crucial for grasping the paper's theoretical framework.
  - Quick check question: What are the two levels of optimization in the data-model co-development paradigm?

- Concept: Multimodal data curation
  - Why needed here: The paper extensively discusses various strategies for curating multimodal datasets, which is essential for understanding how to effectively train MLLMs.
  - Quick check question: What are the key differences between data scaling and scaling effectiveness in the context of MLLM development?

- Concept: Reinforcement learning from AI feedback (RLAIF)
  - Why needed here: The paper mentions RLAIF as a potential approach for aligning MLLMs with human preferences, which is important for understanding the future directions of MLLM development.
  - Quick check question: How does RLAIF differ from traditional reinforcement learning from human feedback (RLHF)?

## Architecture Onboarding

- Component map: The MLLM architecture consists of an LLM (e.g., LLaMA), one or more foundation models (encoders) for non-text data (e.g., ViT, CLIP), one or more projectors to align features, and optionally modality-specific generators (e.g., Stable Diffusion).
- Critical path: The critical path for data-model co-development involves: 1) data acquisition and augmentation, 2) data scaling and effectiveness improvements, 3) MLLM training and capability enhancement, 4) MLLM-assisted data curation, and 5) iterative refinement.
- Design tradeoffs: Balancing data quantity and quality, choosing between self-boosted and externally-boosted paradigms, and determining the appropriate level of human involvement in data curation.
- Failure signatures: Poor model performance despite large datasets, inability to effectively curate data, or lack of meaningful feedback loops between data and model improvements.
- First 3 experiments:
  1. Implement a simple data deduplication strategy using CLIP embeddings and measure its impact on MLLM performance.
  2. Compare the effectiveness of self-boosted vs. externally-boosted data curation for a specific MLLM task.
  3. Develop a prototype MLLM-assisted data visualization tool and evaluate its usability and effectiveness.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What specific metrics or evaluation criteria can effectively measure the success of self-boosted data-model co-development for MLLMs compared to externally-boosted approaches?
- Basis in paper: [explicit] The paper identifies that current data-model co-development for MLLMs mainly relies on externally-boosted approaches using well-trained models like GPT-4V, and proposes self-boosted MLLM development as a promising future direction in section 8.3.
- Why unresolved: While the paper proposes self-boosted approaches, it doesn't provide specific metrics or evaluation frameworks to compare their effectiveness against externally-boosted methods. The success criteria for self-boosted approaches remain undefined.
- What evidence would resolve it: Empirical studies comparing model performance, data quality metrics, and resource efficiency between self-boosted and externally-boosted approaches using standardized evaluation frameworks.

### Open Question 2
- Question: How can MLLMs effectively discover and incorporate long-tail data without misclassifying valuable samples as worthless?
- Basis in paper: [explicit] Section 8.2.1 identifies automatic long-tail data discovery as a research issue, noting that MLLMs may perform sub-optimally with long-tail domains and potentially misclassify some long-tail data as worthless.
- Why unresolved: The paper acknowledges this challenge but doesn't provide concrete solutions for how MLLMs can better distinguish valuable long-tail data from truly worthless samples while maintaining efficiency.
- What evidence would resolve it: Development and validation of algorithms that can accurately identify and incorporate long-tail data samples, with measurable improvements in MLLM performance on previously underrepresented domains.

### Open Question 3
- Question: What are the optimal strategies for modality-compatibility detection that go beyond text-centric anchoring?
- Basis in paper: [explicit] Section 8.2.2 points out that current model-driven techniques for assessing cross-modal compatibility mainly use text as an anchor and suggests exploring additional modalities as anchors to enhance alignment efficacy.
- Why unresolved: While the paper identifies the limitation of text-centric approaches, it doesn't specify which alternative modalities would be most effective as anchors or how to implement such multi-modal anchoring systems.
- What evidence would resolve it: Comparative studies demonstrating improved cross-modal alignment performance using non-text anchors (e.g., audio, visual features) versus text-centric approaches, with quantitative metrics showing enhanced MLLM understanding and generation capabilities.

## Limitations
- The survey is primarily conceptual with limited quantitative comparisons between different data-model co-development approaches
- Current approaches still heavily rely on well-trained models (GPT-4V) or human assistance, with limited exploration of fully automated paradigms
- Limited concrete guidance on addressing ethical challenges in data curation and MLLM development, particularly regarding bias and privacy in multi-modal contexts

## Confidence
- **High confidence**: The categorization of data contributions (scaling vs. usability) and model contributions (synthesis vs. insights) is well-supported by the literature and provides a clear framework for understanding data-model co-development.
- **Medium confidence**: The bi-level optimization formulation provides a theoretically sound framework, but practical implementation challenges and computational costs are not fully addressed.
- **Low confidence**: Specific quantitative claims about the relative effectiveness of different data curation strategies lack empirical backing from controlled experiments.

## Next Checks
1. **Empirical comparison study**: Conduct controlled experiments comparing self-boosted vs. externally-boosted data curation approaches on a standardized MLLM benchmark, measuring both performance improvements and computational costs.
2. **Ethical impact assessment**: Develop and test a framework for evaluating bias and privacy risks in MLLM datasets, using real-world multi-modal datasets to validate its effectiveness.
3. **Automation feasibility analysis**: Implement a prototype of the proposed self-boosted MLLM development paradigm on a specific task (e.g., image-text retrieval) and measure the degree of human intervention required versus performance gains.