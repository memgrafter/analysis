---
ver: rpa2
title: Mitigating Noisy Supervision Using Synthetic Samples with Soft Labels
arxiv_id: '2406.16966'
source_url: https://arxiv.org/abs/2406.16966
tags:
- samples
- labels
- noisy
- label
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes MixNN, a method for training deep neural networks
  robustly on datasets with noisy labels. The core idea is to generate synthetic training
  samples by mixing each original sample with its K nearest neighbors based on learned
  representations, and then training on these synthetic samples with dynamically estimated
  weights.
---

# Mitigating Noisy Supervision Using Synthetic Samples with Soft Labels

## Quick Facts
- arXiv ID: 2406.16966
- Source URL: https://arxiv.org/abs/2406.16966
- Reference count: 40
- MixNN consistently outperforms state-of-the-art methods across all datasets and noise conditions, achieving 86.08% accuracy on CIFAR-10 with 80% symmetric label noise.

## Executive Summary
This paper introduces MixNN, a method for training deep neural networks robustly on datasets with noisy labels. The approach generates synthetic training samples by mixing each original sample with its K nearest neighbors based on learned representations, and trains on these synthetic samples with dynamically estimated weights. To further improve performance, especially under extreme label noise, the paper introduces a strategy to gradually correct noisy labels using an exponential moving average of model predictions and given labels. The proposed method is evaluated on two benchmark datasets (CIFAR-10 and CIFAR-100) with simulated label noise and two large-scale real-world datasets (Clothing1M and Webvision).

## Method Summary
MixNN addresses noisy label training by generating synthetic samples through convex combination with nearest neighbors in representation space, using dynamic weights estimated from per-sample loss distributions modeled by a two-component Gaussian Mixture Model. The method also incorporates gradual label correction using exponential moving average of predictions and noisy labels. The approach leverages the observation that deep neural networks exhibit an early learning phase where they fit clean samples before memorizing noisy ones, enabling identification of reliable training data through loss-based separation.

## Key Results
- MixNN achieves 86.08% accuracy on CIFAR-10 with 80% symmetric label noise, outperforming the best baseline by more than 11%.
- The method shows excellent generalization capability on real-world noisy datasets, consistently outperforming state-of-the-art methods.
- MixNN demonstrates superior performance across different noise types and levels on both benchmark and large-scale real-world datasets.

## Why This Works (Mechanism)

### Mechanism 1
Deep neural networks exhibit an "early learning" phase where they fit clean samples before memorizing noisy ones. During early training epochs, the model's loss distribution naturally separates clean samples (low loss) from noisy samples (high loss), enabling identification of reliable training data. The network architecture and optimization dynamics inherently prioritize learning clean patterns before complex noisy patterns.

### Mechanism 2
Mixing each training sample with its K nearest neighbors based on learned representations reduces the impact of noisy labels by creating synthetic samples where correct information is preserved while wrong supervision is suppressed. Nearest neighbors based on learned representations are likely to share the same true class even when labels are noisy, as learned representations of images from the same category still congregate together regardless of noisy labels.

### Mechanism 3
Gradually correcting noisy labels using exponential moving average of model predictions improves generalization by making the soft targets computed from EMA of predictions and noisy labels increasingly accurate approximations of ground truth labels. Model predictions in early learning stages are more accurate than the noisy labels they were trained on, enabling progressive label refinement.

## Foundational Learning

- **Concept: Gaussian Mixture Model (GMM) for modeling loss distributions**
  - Why needed here: To estimate the probability that a sample is clean based on its loss value, enabling dynamic weight calculation
  - Quick check question: How does a two-component GMM help distinguish between clean and noisy samples?

- **Concept: Nearest neighbor search and HNSW graphs**
  - Why needed here: To efficiently find K nearest neighbors in high-dimensional representation space for the mixing strategy
  - Quick check question: Why is approximate nearest neighbor search preferred over exact search in this context?

- **Concept: Exponential moving average (EMA) for label correction**
  - Why needed here: To gradually update noisy labels toward more accurate predictions as training progresses
  - Quick check question: What role does the momentum parameter α play in the EMA label correction strategy?

## Architecture Onboarding

- **Component map**: Representation learning (penultimate layer features) -> HNSW-based approximate KNN search -> GMM-based weight estimation from per-sample losses -> Sample mixing function (Eq. 4-5) -> EMA-based label correction (Eq. 12) -> Training loop with synthetic samples

- **Critical path**: 1. Warmup training with CE loss 2. For each epoch: estimate clean probabilities with GMM 3. Compute KNNs for all samples 4. Calculate dynamic mixing weights 5. Generate synthetic samples and labels 6. Train with the new loss function

- **Design tradeoffs**: K vs. performance: Larger K provides more diverse mixing but can over-complicate inputs; GMM complexity: Two components are sufficient for loss distribution separation; Label correction timing: Starting too early or too late affects performance

- **Failure signatures**: Performance degradation when K is too large; No improvement over baseline when label noise is very low; Instability when α is set too close to 1 or 0

- **First 3 experiments**: 1. Baseline: Train with CE loss on noisy CIFAR-10 2. MixNN with K=1, no label correction, compare to baseline 3. MixNN with full components, test sensitivity to K and α hyperparameters

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: How does the choice of K (number of nearest neighbors) affect the performance of MixNN, and what is the optimal value of K?
- **Basis in paper**: The paper discusses the effect of K on the performance of MixNN and finds that a larger K worsens the performance. However, it does not provide an optimal value for K.
- **Why unresolved**: The paper does not provide a detailed analysis of the effect of K on the performance of MixNN. It only mentions that a larger K worsens the performance, but does not provide an optimal value for K.
- **What evidence would resolve it**: A detailed analysis of the effect of K on the performance of MixNN, including the optimal value of K, would resolve this question.

### Open Question 2
- **Question**: How does the hyperparameter α (momentum) affect the performance of MixNN, and what is the optimal value of α?
- **Basis in paper**: The paper mentions that the sensitivity to hyperparameter α is quite mild, but it does not provide an optimal value for α.
- **Why unresolved**: The paper does not provide a detailed analysis of the effect of α on the performance of MixNN. It only mentions that the sensitivity to α is quite mild, but does not provide an optimal value for α.
- **What evidence would resolve it**: A detailed analysis of the effect of α on the performance of MixNN, including the optimal value of α, would resolve this question.

### Open Question 3
- **Question**: How does the proposed method MixNN perform on other real-world datasets with noisy labels, such as ImageNet or medical imaging datasets?
- **Basis in paper**: The paper evaluates the performance of MixNN on two real-world datasets (Clothing1M and Webvision), but it does not mention other real-world datasets with noisy labels.
- **Why unresolved**: The paper does not provide any information on the performance of MixNN on other real-world datasets with noisy labels.
- **What evidence would resolve it**: Evaluating the performance of MixNN on other real-world datasets with noisy labels, such as ImageNet or medical imaging datasets, would resolve this question.

## Limitations

- The paper's claims about early learning dynamics are supported by established literature but lack direct empirical validation within this work.
- The effectiveness of the mixing strategy depends critically on the assumption that learned representations cluster by true class despite noisy labels, which is stated but not empirically verified.
- The GMM-based weight estimation assumes a clear bimodal loss distribution, which may not hold for all architectures or noise patterns.

## Confidence

- **High confidence**: The experimental results showing MixNN outperforming baselines across multiple datasets and noise conditions are well-documented with specific accuracy numbers.
- **Medium confidence**: The mechanism explanations (early learning, representation clustering, EMA correction) are logically sound but lack direct supporting evidence from the paper's experiments.
- **Low confidence**: The paper claims superior generalization on real-world noisy datasets without providing detailed analysis of what types of noise each dataset contains or how MixNN handles them differently.

## Next Checks

1. **Ablation study on representation quality**: Visualize learned representations (t-SNE/PCA) for samples with noisy vs. clean labels to verify the claim that true classes congregate despite label noise.

2. **GMM loss distribution analysis**: Plot per-sample loss distributions across training epochs to empirically verify the bimodal separation that enables GMM-based clean probability estimation.

3. **Hyperparameter sensitivity analysis**: Systematically vary K (neighbor count) and α (EMA momentum) across a wider range to identify optimal values and failure points for different noise rates.