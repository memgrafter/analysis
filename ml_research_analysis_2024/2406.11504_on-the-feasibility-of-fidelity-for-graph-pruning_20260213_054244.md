---
ver: rpa2
title: On the Feasibility of Fidelity$^-$ for Graph Pruning
arxiv_id: '2406.11504'
source_url: https://arxiv.org/abs/2406.11504
tags:
- graph
- edge
- fidelity
- pruning
- methods
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper explores whether the fidelity measure can be used for
  graph pruning by investigating if it can induce a global soft mask for edge removal.
  The authors propose Fidelity^-inspired Pruning (FiP), a framework that constructs
  global edge masks from local explanations.
---

# On the Feasibility of Fidelity$^-$ for Graph Pruning

## Quick Facts
- arXiv ID: 2406.11504
- Source URL: https://arxiv.org/abs/2406.11504
- Authors: Yong-Min Shin; Won-Yong Shin
- Reference count: 8
- Key outcome: General XAI methods outperform GNN-specific methods in graph pruning performance

## Executive Summary
This paper investigates whether the fidelity measure can be used for graph pruning by proposing Fidelity-inspired Pruning (FiP), a framework that constructs global edge masks from local explanations. The authors evaluate FiP using 7 edge attribution methods on 4 benchmark datasets and find that surprisingly, general XAI methods like attention, saliency, and integrated gradient outperform GNN-specific methods in graph pruning performance. The study shows that the best methods can prune up to 50% of edges with minimal performance degradation, highlighting the potential of using fidelity-based approaches for graph pruning to enhance GNN efficiency.

## Method Summary
The Fidelity-inspired Pruning (FiP) framework aggregates local edge attributions from various explanation methods (attention, saliency, integrated gradient, guided backpropagation, GNNExplainer, PGExplainer, and FastDnX) to create global soft edge masks. The framework uses simple aggregation methods (summation or averaging) across all nodes to determine which edges can be pruned. The pruned graph is then evaluated on node classification tasks to measure the impact of edge removal. The approach is tested on 4 benchmark datasets (BA-Shapes, Cora, Citeseer, Pubmed) using a 2-layer GAT model.

## Key Results
- General XAI methods (attention, saliency, integrated gradient) outperform GNN-specific methods in graph pruning performance
- The best methods can prune up to 50% of edges with less than 4% performance degradation on BA-Shapes
- Effective pruning methods tend to remove edges not included in ground-truth explanations
- GNNExplainer consistently shows the worst performance across most datasets

## Why This Works (Mechanism)

### Mechanism 1
Fidelity- measures the output difference when removing unimportant edges, and aggregating these local explanations globally creates a mask that can guide graph pruning. By computing fidelity- for each node's explanation and summing/averaging these edge attribution scores across all nodes, the framework creates a global soft edge mask. Edges frequently deemed unimportant in local explanations are assigned low scores and can be pruned. This works because good explanations consistently identify truly unimportant edges across different nodes.

### Mechanism 2
General XAI methods outperform GNN-specific methods for graph pruning because they capture more general patterns of edge importance. These methods assign edge attributions based on gradient-based or attention mechanisms that capture global importance patterns, while GNN-specific methods may overfit to specific graph structures or local patterns that don't generalize well to pruning decisions.

### Mechanism 3
The relationship between fidelity- scores and pruning performance is not direct - high fidelity- doesn't guarantee good pruning. While fidelity- measures how well an explanation preserves output when removing unimportant edges, pruning requires identifying which edges can be removed without harming performance. These are related but distinct objectives.

## Foundational Learning

- **Graph Neural Networks and their attention mechanisms**: Understanding how GNNs process graph-structured data and generate attention weights is crucial for interpreting edge attribution methods like attention and guided backpropagation. Quick check: How does a 2-layer GAT model aggregate information from neighbors to make predictions?

- **Edge attribution and explanation methods**: The framework relies on various edge attribution methods to generate local explanations, and understanding their mechanisms is essential for implementing FiP. Quick check: What's the difference between saliency-based edge attribution and integrated gradient-based attribution?

- **Graph pruning and its impact on model efficiency**: The ultimate goal of FiP is to improve GNN efficiency through graph pruning, so understanding how pruning affects computational complexity is important. Quick check: How does the number of edges in a graph affect the time complexity of GNN computations?

## Architecture Onboarding

- **Component map**: Local edge attributions -> Aggregation module (sum/average) -> Global mask generator -> Pruning selector -> Pruned graph output
- **Critical path**: Local explanations → aggregation → global mask generation → edge selection → pruned graph
- **Design tradeoffs**: Simple aggregation (sum/average) vs. more sophisticated methods; computational efficiency vs. pruning quality
- **Failure signatures**: Poor pruning performance despite high-quality local explanations; inconsistent results across different aggregation methods
- **First 3 experiments**:
  1. Implement FiP with attention-based edge attributions on BA-Shapes dataset, measure accuracy degradation at 50% pruning
  2. Compare FiP performance using different aggregation methods (sum vs. average) on Cora dataset
  3. Visualize pruned graphs using FiP with different edge attribution methods to understand pruning patterns

## Open Questions the Paper Calls Out

### Open Question 1
What is the relationship between the sparsity level of explanations and their human comprehensibility? The paper mentions that sparse explanations are generally considered more human-comprehensible and that FiP may result in sparser explanations, making manual inspection feasible. This relationship is only mentioned in passing without empirical investigation. Human studies comparing explanations at different sparsity levels would help determine optimal interpretability.

### Open Question 2
Why do general XAI methods (like IG, SA, and Attention) outperform GNN-specific methods in graph pruning, despite GNN-specific methods having better fidelity scores? The paper finds this discrepancy but does not provide a theoretical explanation. Analysis of edge characteristics identified by each method and investigation of whether general XAI methods capture different aspects of edge importance could resolve this question.

### Open Question 3
What are the limitations of the current aggregation methods (sum and average) in FiP, and how could more sophisticated aggregation methods improve performance? The paper uses simple summation and averaging and notes that more sophisticated methods could be explored. Development and evaluation of alternative aggregation methods (weighted aggregation, attention-based aggregation) and comparison with current methods would address this limitation.

## Limitations
- Framework relies on a 2-layer GAT model without exploring deeper architectures or different GNN types
- Aggregation methods are relatively simple and may not capture complex relationships between local and global edge importance
- Study focuses on node classification tasks, leaving uncertainty about performance on other graph learning tasks

## Confidence

**High Confidence**: The core finding that general XAI methods (attention, saliency, integrated gradient) outperform GNN-specific methods for graph pruning is well-supported by experimental results across multiple datasets and pruning levels.

**Medium Confidence**: The mechanism explaining why general methods work better (capturing global patterns vs. overfitting to local structures) is plausible but not definitively proven.

**Low Confidence**: The claim that fidelity- scores don't directly correlate with pruning performance, while supported by one example, needs more systematic investigation across all methods and datasets.

## Next Checks

1. **Cross-architecture validation**: Test FiP with different GNN architectures (GCN, GraphSAGE, deeper GAT models) to verify if general XAI methods consistently outperform GNN-specific methods across architectures.

2. **Aggregation method comparison**: Implement and compare more sophisticated aggregation methods (weighted averaging, attention-based aggregation, learned aggregation) against simple sum/average to determine if performance can be further improved.

3. **Task generalization**: Apply FiP to other graph learning tasks (link prediction, graph classification) to validate whether the observed patterns hold beyond node classification.