---
ver: rpa2
title: 'OpenMathInstruct-2: Accelerating AI for Math with Massive Open-Source Instruction
  Data'
arxiv_id: '2410.01560'
source_url: https://arxiv.org/abs/2410.01560
tags:
- data
- math
- solutions
- question
- problem
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces OpenMathInstruct-2, a large-scale open-source
  instruction-tuning dataset for mathematical reasoning. The dataset consists of 14
  million question-solution pairs with 600K unique questions, making it nearly eight
  times larger than the previous largest open-source math reasoning dataset.
---

# OpenMathInstruct-2: Accelerating AI for Math with Massive Open-Source Instruction Data

## Quick Facts
- **arXiv ID**: 2410.01560
- **Source URL**: https://arxiv.org/abs/2410.01560
- **Reference count**: 40
- **Primary result**: Llama-3.1-8B-Base finetuned on OpenMathInstruct-2 achieves 67.8% on MATH, outperforming Llama3.1-8B-Instruct by 15.9%

## Executive Summary
OpenMathInstruct-2 is a large-scale open-source instruction-tuning dataset for mathematical reasoning, containing 14 million question-solution pairs with 600K unique questions. The dataset is nearly eight times larger than the previous largest open-source math reasoning dataset. Through careful ablation studies using Llama3.1 models, the authors identify key insights: excessively verbose solutions harm performance, strong teacher models generate superior synthetic data, finetuning is robust to low-quality solutions (up to 20%), and question diversity is crucial for scaling gains. By applying these insights, they construct OpenMathInstruct-2 using Llama-3.1-405B-Instruct for data synthesis. Finetuning Llama-3.1-8B-Base with this dataset achieves 67.8% accuracy on the MATH benchmark, outperforming the base instruct model by 15.9%. The dataset, code, and fine-tuned models are released under a commercially permissive license to accelerate open-source mathematical reasoning capabilities.

## Method Summary
The authors conducted systematic ablation studies on solution format, teacher model choice, data quality tolerance, and question diversity to inform dataset construction. They generated synthetic data using Llama-3.1-405B-Instruct with a proposed OpenMath CoT format that is 40% shorter than Llama's format. The dataset was augmented through question and solution generation, then decontaminated using embedding similarity search and paraphrase detection to remove potential test-set contamination. Finally, Llama-3.1-8B-Base and 70B models were finetuned on the constructed dataset for 2 epochs using AdamW optimization.

## Key Results
- OpenMathInstruct-2 contains 14M question-solution pairs with 600K unique questions
- Finetuning Llama-3.1-8B-Base achieves 67.8% accuracy on MATH, a 15.9% absolute improvement over Llama3.1-8B-Instruct
- OpenMath CoT format outperforms Llama's CoT format by 3.9% while being 40% shorter
- Strong teacher models (Llama3.1-405B-Instruct) generate data that surpasses weaker models by 7.8%
- SFT is robust to up to 20% low-quality solutions

## Why This Works (Mechanism)

### Mechanism 1
- Claim: OpenMathInstruct-2 outperforms Llama3.1-8B-Instruct by 15.9% on MATH through targeted data synthesis.
- Mechanism: Finetuning Llama3.1-8B-Base with OpenMathInstruct-2 leverages synthetic math instruction data generated by Llama3.1-405B-Instruct, which provides higher-quality reasoning paths than weaker models or shorter solutions.
- Core assumption: A stronger teacher model (Llama3.1-405B-Instruct) generates more effective synthetic data than weaker models or on-policy data, even when controlling for dataset size.
- Evidence anchors:
  - [abstract] "Finetuning Llama-3.1-8B-Base with OpenMathInstruct-2 achieves 67.8% accuracy on the MATH benchmark, outperforming Llama3.1-8B-Instruct by an absolute 15.9%."
  - [section 2.2.2] "Data generated by a strong teacher model surpasses that of on-policy data produced by a weaker student model by 7.8%."

### Mechanism 2
- Claim: Chain-of-Thought format quality directly impacts finetuning performance.
- Mechanism: The proposed OpenMath CoT format is 40% shorter and more effective than Llama's CoT format, improving the base model's ability to follow few-shot examples.
- Core assumption: Excessively verbose solutions are detrimental to finetuning performance, and a concise CoT format enhances learning efficiency.
- Evidence anchors:
  - [abstract] "Solution format matters, with excessively verbose solutions proving detrimental to SFT performance."
  - [section 2.2.1] "Our proposed CoT format outperforms Llama's CoT format by 3.9% while being 40% shorter in solution length."

### Mechanism 3
- Claim: SFT is robust to low-quality solutions, allowing for imprecise data filtering.
- Mechanism: The finetuning process is tolerant to up to 20% low-quality solutions, whether introduced by incorrect pairing or wrong-answer solutions, without significant performance degradation.
- Core assumption: The model can learn effectively even with some noisy data, reducing the need for perfect data filtering.
- Evidence anchors:
  - [abstract] "SFT is robust to low-quality solutions, allowing for imprecise data filtering."
  - [section 2.2.3] "We find SFT performance to be robust to the presence of up to 20% low-quality data."

## Foundational Learning

- Concept: Chain-of-Thought reasoning
  - Why needed here: The dataset relies on synthetic CoT solutions to teach mathematical reasoning, and the format directly impacts finetuning performance.
  - Quick check question: What is the impact of solution verbosity on finetuning performance, and how does the OpenMath CoT format address this?

- Concept: Data scaling and diversity
  - Why needed here: The performance gains are attributed to both increased data size (14M pairs) and question diversity, with unique questions increasing from 1K to 6.5K improving accuracy by 10.5%.
  - Quick check question: How does question diversity affect finetuning performance, and what is the impact of increasing unique questions?

- Concept: Teacher-student model dynamics
  - Why needed here: The choice of teacher model (Llama3.1-405B-Instruct vs. Llama3.1-8B-Base) significantly affects the quality of synthetic data and subsequent finetuning outcomes.
  - Quick check question: How does the choice of teacher model impact the quality of synthetic data and finetuning performance?

## Architecture Onboarding

- Component map:
  Data generation pipeline -> LLM decontamination -> Model finetuning -> Benchmark evaluation

- Critical path:
  Generate synthetic data using Llama3.1-405B-Instruct with OpenMath CoT format -> Perform LLM decontamination to remove potential test set contamination -> Finetune base models (Llama3.1-8B-Base, Llama3.1-70B) on the curated dataset -> Evaluate zero-shot performance on standard math benchmarks

- Design tradeoffs:
  - Solution verbosity vs. finetuning performance: Shorter CoT formats are more effective but may lose some reasoning depth
  - Data quality vs. quantity: Allowing up to 20% low-quality solutions simplifies data generation but may limit peak performance
  - Teacher model strength vs. computational cost: Stronger models (Llama3.1-405B-Instruct) generate better data but at higher cost

- Failure signatures:
  - Performance degradation if the teacher model generates excessive noise
  - Reduced effectiveness if the CoT format is not properly adhered to by the base model
  - Contamination of test sets if LLM decontamination is not thorough

- First 3 experiments:
  1. Ablation study on solution format: Compare Llama CoT vs. OpenMath CoT on MATH validation accuracy
  2. Teacher model comparison: Generate data with Llama3.1-8B-Base vs. Llama3.1-405B-Instruct and compare finetuning outcomes
  3. Data quality robustness: Introduce varying proportions of low-quality solutions and measure impact on MATH validation accuracy

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal solution format for math reasoning instruction tuning?
- Basis in paper: [explicit] The paper compares Llama CoT format vs OpenMath CoT format and finds OpenMath CoT performs better despite being 40% shorter
- Why unresolved: The paper only compares two specific formats; there may be other solution formats that could perform even better
- What evidence would resolve it: Systematic comparison of multiple solution format variations (different levels of verbosity, step-by-step vs condensed, with/without special tokens, etc.) across various math domains

### Open Question 2
- Question: How does the quality of synthetic data scale with model size beyond Llama-3.1-405B?
- Basis in paper: [explicit] The paper compares Llama3.1-8B-Base vs Llama3.1-405B-Instruct and finds the stronger model generates better data, but doesn't test larger frontier models
- Why unresolved: The paper only tests up to Llama-3.1-405B-Instruct; frontier models may generate even higher quality data
- What evidence would resolve it: Direct comparison of synthetic data quality and downstream performance using models larger than 405B (e.g., GPT-4o, Claude-3-5-Sonnet, Gemini-1.5-Pro)

### Open Question 3
- Question: What is the precise threshold of low-quality data tolerance during SFT before performance degrades?
- Basis in paper: [explicit] The paper shows SFT is robust to 20% low-quality data but doesn't explore finer-grained thresholds or different types of noise
- Why unresolved: The paper only tests up to 80% incorrect solutions and doesn't distinguish between different types of errors (arithmetic vs logical vs final answer)
- What evidence would resolve it: Systematic ablation studies testing various percentages (5%, 10%, 15%, etc.) and different types of low-quality solutions to find exact breaking points

## Limitations
- The exact prompt templates for synthetic data generation are not fully specified, making faithful reproduction challenging
- The completeness of the LLM decontamination pipeline is not independently verified, leaving potential for test-set contamination
- Results are limited to the Llama3.1 family of models, restricting generalizability to other model architectures

## Confidence

**High Confidence Claims:**
- OpenMathInstruct-2 is a large-scale dataset with 14M question-solution pairs
- Finetuning Llama3.1-8B-Base on OpenMathInstruct-2 achieves 67.8% accuracy on MATH
- The dataset is significantly larger than previous open-source math reasoning datasets
- Solution format impacts finetuning performance

**Medium Confidence Claims:**
- OpenMath CoT format is 40% shorter and 3.9% more effective than Llama's CoT format
- Teacher model choice (Llama3.1-405B-Instruct vs. 8B-Base) creates 7.8% performance difference
- SFT is robust to up to 20% low-quality solutions
- Question diversity scaling (1K to 6.5K unique questions) yields 10.5% accuracy improvement

**Low Confidence Claims:**
- The exact mechanisms by which the proposed solution format improves performance
- The completeness of the LLM decontamination pipeline
- The generalizability of results to other model families beyond Llama3.1

## Next Checks

1. **Independent Dataset Generation Verification**: Replicate the data generation pipeline using publicly available prompts to verify that the reported dataset characteristics (14M pairs, 600K unique questions) can be consistently reproduced, and assess whether the OpenMath CoT format can be properly implemented without the special tokens.

2. **Teacher Model Ablation Study**: Conduct a controlled experiment varying the teacher model strength across multiple scales (e.g., 8B, 70B, 405B) to verify the claimed 7.8% performance gap between strong and weak teacher models, while controlling for dataset size and solution format.

3. **LLM Decontamination Effectiveness Audit**: Implement an independent contamination detection method (e.g., semantic similarity search on a held-out validation set) to verify that the reported test-set contamination is below the claimed 0.1% threshold, and measure the impact of any residual contamination on benchmark performance.