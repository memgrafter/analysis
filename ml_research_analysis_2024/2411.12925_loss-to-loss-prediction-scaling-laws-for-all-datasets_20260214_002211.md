---
ver: rpa2
title: 'Loss-to-Loss Prediction: Scaling Laws for All Datasets'
arxiv_id: '2411.12925'
source_url: https://arxiv.org/abs/2411.12925
tags:
- loss
- test
- data
- trained
- train
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'The paper introduces loss-to-loss prediction, a method for translating
  scaling laws across different data distributions. It identifies three key relationships:
  (1) train-to-train loss follows a shifted power law when models are paired by compute,
  (2) train-to-test loss also follows a shifted power law, and (3) test-to-test loss
  shows similar behavior.'
---

# Loss-to-Loss Prediction: Scaling Laws for All Datasets

## Quick Facts
- **arXiv ID**: 2411.12925
- **Source URL**: https://arxiv.org/abs/2411.12925
- **Reference count**: 40
- **Key outcome**: Introduces loss-to-loss prediction method for translating scaling laws across different data distributions using shifted power law relationships

## Executive Summary
This paper introduces loss-to-loss prediction, a method for translating scaling laws across different data distributions by identifying simple shifted power law relationships between losses. The key insight is that when models are paired by compute, the ratio of their losses on different datasets can be described by a simple multiplicative constant and exponent, independent of model size and dataset size. The method accurately extrapolates up to 20x beyond the largest training compute used and can improve scaling law predictions when limited data is available on a new distribution.

## Method Summary
The approach involves training models of varying sizes on multiple pre-training datasets, then fitting scaling laws using a power law functional form. Loss-to-loss prediction is applied by fitting two parameters (K and κ) that capture the multiplicative relationship between losses across datasets. The method is validated through extensive experiments on 6 pre-training datasets and 11 downstream tasks, demonstrating accurate extrapolation and translation of scaling behaviors.

## Key Results
- Train-to-train loss follows a shifted power law when models are paired by compute
- Train-to-test and test-to-test losses also follow shifted power law relationships
- Accurate extrapolation up to 20x beyond largest training compute
- Compute-optimal model size is invariant under loss-to-loss translation

## Why This Works (Mechanism)

### Mechanism 1
Loss-to-loss prediction works because the relationship between losses across different data distributions follows a shifted power law. When models are paired by compute, the ratio of their losses on different datasets can be described by a simple multiplicative constant and exponent (K and κ), independent of model size and dataset size.

### Mechanism 2
The compute-optimal model size is invariant under loss-to-loss translation because under the functional form used, the optimal model size for a given FLOP budget depends only on the ratio of parameters to data, not on the specific data distribution.

### Mechanism 3
Downstream task performance can be predicted from pre-training loss via a shifted power law relationship. The relationship between train loss and test loss on downstream tasks follows a shifted power law, where the exponent κ can be less than 1 (increasing returns) or greater than 1 (diminishing returns) depending on the task.

## Foundational Learning

- **Power laws and scaling relationships**: Needed to understand how quantities scale with each other according to power laws. Quick check: Given y = kx^a, what happens to y when x is doubled and a = 2?
- **Cross-entropy loss and its interpretation**: Needed as the primary metric for model performance. Quick check: What is the relationship between cross-entropy loss and classification error for binary classification?
- **Compute-optimal model sizing**: Needed to understand how the optimal model size for a given compute budget is invariant under loss-to-loss translation. Quick check: What is the relationship between model size, dataset size, and compute budget in neural scaling laws?

## Architecture Onboarding

- **Component map**: Pre-training datasets (FineWeb, FineWeb-Edu, ProofPile 2, SlimPajama, SmolLM Corpus, StarCoder) -> Model training (OLMo models 20M-1.7B) -> Evaluation (cross-entropy loss) -> Analysis (scaling law fitting and loss-to-loss prediction)
- **Critical path**: 1) Train models on pre-training datasets across model sizes and FLOP budgets, 2) Evaluate on pre-training and downstream tasks, 3) Fit scaling laws using Equation 4, 4) Compute loss-to-loss relationships, 5) Validate extrapolations
- **Design tradeoffs**: Using cross-entropy loss vs. accuracy (loss provides smoother gradients), fixed vs. adaptive learning rates (linear warmup and cosine decay used), number of training runs (more runs improve fits but increase cost)
- **Failure signatures**: Poor extrapolation beyond 20x largest FLOP budget, inconsistent κ values across dataset pairs, large discrepancies in predicted vs. actual irreducible entropy terms
- **First 3 experiments**: 1) Reproduce train-to-train prediction by fitting shifted power law to paired model losses, 2) Validate train-to-test prediction by fitting relationship between pre-training and downstream task losses, 3) Test scaling law translation by comparing translated predictions to actual scaling laws

## Open Questions the Paper Calls Out

- **Open Question 1**: What is the precise theoretical justification for the shifted power law relationship between train-to-train losses across different datasets, especially across diverse datasets like code vs. non-code?
- **Open Question 2**: How do pre-training hyperparameters affect the robustness of loss-to-loss predictions, and are there regimes where these relationships break down?
- **Open Question 3**: Why do some downstream tasks exhibit concave relationships (κ < 1) while others exhibit convex relationships (κ > 1) with pre-training loss?
- **Open Question 4**: Can loss-to-loss prediction be effectively used for data selection and filtering, and what are optimal strategies for leveraging these relationships?

## Limitations

- Method may break down for datasets with fundamentally different characteristics (structured vs. unstructured data)
- Extrapolation becomes unreliable beyond 20x largest training compute due to irreducible loss estimation errors
- Results may not generalize to other hyperparameter settings or architectures

## Confidence

- **High Confidence**: Train-to-train loss prediction with extensive experimental validation
- **Medium Confidence**: Train-to-test and test-to-test predictions with noted noisiness at high loss values
- **Low Confidence**: Compute-optimal model size invariance primarily derived theoretically rather than experimentally

## Next Checks

1. **Cross-domain robustness test**: Apply loss-to-loss prediction to datasets from fundamentally different domains (protein sequences, audio data, mathematical reasoning) to verify generalizability beyond text data.

2. **Out-of-distribution extrapolation stress test**: Systematically measure prediction accuracy as a function of extrapolation distance beyond the largest training compute, particularly focusing on identifying when irreducible loss estimation errors dominate.

3. **Alternative functional form comparison**: Compare the shifted power law approach against other potential functional forms (polynomial, exponential) for loss-to-loss translation to establish whether the power law relationship is truly optimal or merely sufficient.