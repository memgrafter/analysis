---
ver: rpa2
title: Provable Policy Gradient Methods for Average-Reward Markov Potential Games
arxiv_id: '2403.05738'
source_url: https://arxiv.org/abs/2403.05738
tags:
- policy
- gradient
- amax
- lemma
- markov
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper studies Markov potential games under the average reward
  criterion, where most prior work focuses on discounted rewards. It establishes that
  the average reward is a smooth function of policies and provides sensitivity bounds
  for differential value functions under ergodicity assumptions.
---

# Provable Policy Gradient Methods for Average-Reward Markov Potential Games

## Quick Facts
- arXiv ID: 2403.05738
- Source URL: https://arxiv.org/abs/2403.05738
- Reference count: 40
- Provides first sample complexity bounds for policy gradient methods in average-reward Markov potential games

## Executive Summary
This paper establishes provable convergence guarantees for policy gradient methods in average-reward Markov potential games, where most prior work focuses on discounted rewards. The authors prove that the average reward function is smooth with respect to policies and provide sensitivity bounds for differential value functions under ergodicity assumptions. They analyze three algorithms—policy gradient ascent, proximal-Q, and natural policy gradient—proving global convergence to an ε-Nash equilibrium with time complexity O(1/ε²) given gradient oracles. When gradients must be estimated from samples, they propose a single-trajectory estimator achieving δ approximation error with sample complexity Õ(1/(minₛ,ₐ π(a|s)δ)), leading to overall sample complexity Õ(1/ε⁵) for the sample-based policy gradient ascent algorithm.

## Method Summary
The paper analyzes average-reward Markov potential games by establishing that the average reward is a smooth function of policies under ergodicity assumptions. Three oracle-based algorithms (policy gradient ascent, proximal-Q, and natural policy gradient) are analyzed with O(1/ε²) time complexity. For practical implementation, a single-trajectory gradient estimator is proposed with sample complexity Õ(1/(minₛ,ₐ π(a|s)δ)) to achieve δ approximation error. The sample-based policy gradient ascent algorithm combines this estimator with projected updates to achieve sample complexity Õ(1/ε⁵), providing the first such bound for policy gradient methods in this setting.

## Key Results
- Establishes smoothness of average reward function with respect to policies (L-smooth with extra S dependence compared to discounted case)
- Proves three oracle-based algorithms converge to ε-Nash equilibrium with O(1/ε²) time complexity
- Proposes single-trajectory gradient estimator with Õ(1/(minₛ,ₐ π(a|s)δ)) sample complexity
- Achieves overall sample complexity Õ(1/ε⁵) for sample-based policy gradient ascent
- Provides sensitivity bounds for differential value functions under ergodicity assumptions

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The average reward function is L-smooth with respect to policies under ergodicity assumptions.
- Mechanism: By leveraging perturbation theory of stochastic matrices and ergodicity, the second-order derivatives of the average reward with respect to policy parameters can be bounded, leading to the smoothness property.
- Core assumption: The underlying MDP is irreducible and aperiodic (Assumption 1), ensuring unique stationary distributions and allowing for perturbation analysis.
- Evidence anchors:
  - [abstract]: "We first establish that the average reward is a smooth function of policies and provide sensitivity bounds for the differential value functions, under certain conditions on ergodicity..."
  - [section]: "Lemma 4 (Smoothness of ρ and Φ)... (a) For any i, and π−i ∈ Π−i, the average value ρπ i is κ2 0S3/2Ai + κ0 √ S Ai-smooth with respect to policy πi."
  - [corpus]: Weak - the corpus contains papers on average reward policy gradient but none provide the smoothness proof or perturbation theory argument used here.
- Break condition: If the ergodicity assumption fails (e.g., reducible MDP or periodic states), the stationary distribution may not be unique or the perturbation analysis may not hold, invalidating the smoothness bound.

### Mechanism 2
- Claim: Three algorithms (policy gradient ascent, proximal-Q, and natural policy gradient) converge to an ε-Nash equilibrium with time complexity O(1/ε²) given a gradient oracle.
- Mechanism: The smoothness of the average reward function enables the use of standard optimization theory for projected gradient ascent. For NPG, the Fisher information matrix preconditioning accelerates convergence. The monotonicity of policy improvement is established via sensitivity bounds on differential value functions.
- Core assumption: Access to exact gradient/differential Q function oracles and bounded mixing coefficients (κ0 finite under ergodicity).
- Evidence anchors:
  - [abstract]: "We prove that three algorithms, policy gradient, proximal-Q, and natural policy gradient (NPG), converge to an ε-Nash equilibrium with time complexity O(1/ε²), given a gradient/differential Q function oracle."
  - [section]: "Theorem 1... the Nash-regret* of Algorithm 1 is bounded by Nash-regret*(T) = O(D²LΦCΦS/T)." "Theorem 3... the time complexity for an ε-Nash equilibrium is T = O(N CΦDS3/2Amaxκ2 0/(1−Γ)ε²)."
  - [corpus]: Weak - while the corpus contains papers on average reward RL, none establish the specific convergence rates or oracle-based convergence proofs for these three algorithms in Markov potential games.
- Break condition: If the gradient oracle is inaccurate or the mixing coefficient κ0 grows unboundedly (e.g., slow mixing chains), the convergence rate bounds may not hold.

### Mechanism 3
- Claim: A single-trajectory policy gradient estimator achieves δ approximation error with sample complexity Õ(1/(minₛ,ₐ π(a|s)δ)).
- Mechanism: The estimator uses truncated trajectory segments to approximate the differential Q function while controlling bias and variance through mixing time bounds. The single-trajectory design avoids reset requirements.
- Core assumption: The policy class is restricted to Πα to ensure sufficient exploration (π(a|s) ≥ α for all s,a) and that the MDP mixing time is finite.
- Evidence anchors:
  - [abstract]: "When policy gradients have to be estimated, we propose an algorithm with Õ(1/(minₛ,ₐ π(a|s)δ)) sample complexity to achieve δ approximation error w.r.t. the ℓ2 norm."
  - [section]: "Lemma 5... the estimated gradient has ℓ2 error bounded as... E∥ˆgi − ∂ρπ i/∂πi∥2 2 ≤ (1/α + 1)²AmaxN2 2/K + ..."
  - [corpus]: Weak - the corpus contains papers on average reward RL sample complexity but none provide the specific single-trajectory estimator design or the Õ(1/(minₛ,ₐ π(a|s)δ)) bound.
- Break condition: If the policy has very small probabilities for some state-action pairs (α too small), the variance of the estimator may become too large to control, breaking the approximation guarantee.

## Foundational Learning

- Concept: Ergodic Markov Decision Processes
  - Why needed here: The ergodicity assumption (irreducible and aperiodic MDP) is fundamental for establishing unique stationary distributions, which are used throughout the analysis for performance bounds and sensitivity analysis.
  - Quick check question: What happens to the stationary distribution if the MDP is reducible or periodic? How does this affect the convergence analysis?

- Concept: Potential Games and Nash Equilibria
  - Why needed here: The paper studies Markov potential games where a potential function exists whose maximizer is a Nash equilibrium. Understanding the relationship between potential functions and Nash equilibria is crucial for the convergence analysis.
  - Quick check question: Can you construct a simple 2-player matrix game that is a potential game? What is its potential function and Nash equilibrium?

- Concept: Differential Value Functions in Average Reward MDPs
  - Why needed here: Unlike discounted MDPs, average reward MDPs use differential value functions to capture deviations from steady-state performance. These are central to the policy gradient theorem and sensitivity analysis.
  - Quick check question: How do differential value functions differ from standard value functions in discounted MDPs? Why are they necessary for average reward settings?

## Architecture Onboarding

- Component map: Policy Gradient Ascent (Algorithm 1) -> Proximal-Q Algorithm -> Natural Policy Gradient (Algorithm 2) -> Sample-based Policy Gradient Ascent (Algorithm 3)
- Critical path: For oracle-based algorithms: initialize policies → compute ascent direction (gradient, Q-function, or natural gradient) → project to feasible policy set → repeat until convergence. For sample-based version: trajectory collection and gradient estimation are added as preliminary steps.
- Design tradeoffs: The paper trades off between computational complexity and sample efficiency. Oracle-based methods have better time complexity (O(1/ε²)) but require exact gradient access. The sample-based method is more practical but has higher sample complexity (Õ(1/ε⁵)). The single-trajectory estimator reduces computational burden compared to multi-trajectory alternatives.
- Failure signatures: Convergence may fail if ergodicity assumptions are violated (reducible MDP), if the gradient estimator has high variance (small α in Πα), or if the learning rate is improperly tuned (too large for smoothness bounds, too small for practical convergence). The Nash gap not decreasing or oscillating indicates potential issues.
- First 3 experiments:
  1. Implement and test the independent projected policy gradient ascent (Algorithm 1) on a small randomly generated Markov potential game (e.g., S=10, A1=A2=3) with a known gradient oracle to verify the O(1/ε²) convergence rate.
  2. Implement the gradient estimator (Algorithm 3) and validate its ℓ2 error bound on the same game by comparing estimated gradients to true gradients across multiple random seeds.
  3. Run the complete sample-based policy gradient ascent (Algorithm 2) on the game and measure both ℓ1 accuracy and Nash gap over iterations, comparing against the theoretical sample complexity bound.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What are the theoretical limits of the smoothness parameter L in the average reward setting compared to the discounted reward setting, and how does this impact convergence rates?
- Basis in paper: [explicit] The paper establishes that the smoothness factor for average reward settings has an extra dependence on state size S compared to discounted settings, but does not explore the theoretical limits of this parameter.
- Why unresolved: The paper provides bounds but does not investigate the tightness of these bounds or the fundamental limits of smoothness in the average reward case.
- What evidence would resolve it: Rigorous analysis comparing the derived smoothness bounds to lower bounds or empirical measurements on various MDP structures would clarify the tightness of the results.

### Open Question 2
- Question: How does the sample complexity of the proposed single-trajectory estimator compare to other gradient estimation methods in terms of variance and bias?
- Basis in paper: [explicit] The paper introduces a single-trajectory estimator with a specific sample complexity but does not compare it to other estimation methods.
- Why unresolved: While the paper establishes the sample complexity of the proposed estimator, it does not benchmark it against alternative approaches or analyze its variance properties.
- What evidence would resolve it: Comparative studies of the estimator's variance, bias, and sample complexity against other gradient estimation techniques in both theoretical and empirical settings would provide insight into its efficiency.

### Open Question 3
- Question: What is the impact of the exploration factor c on the convergence of natural policy gradient methods, and how can it be optimized in practice?
- Basis in paper: [explicit] The paper notes that the convergence of NPG depends on the exploration factor c, which can be small with a small reward gap, but does not provide methods to optimize it.
- Why unresolved: The paper identifies the importance of c but does not explore strategies to ensure adequate exploration or analyze its impact on practical convergence.
- What evidence would resolve it: Empirical studies on how different reward structures and exploration strategies affect c, along with theoretical analysis of its role in convergence, would clarify how to optimize it.

### Open Question 4
- Question: How do the theoretical results extend to parameterized policy classes beyond the tabular setting, particularly with function approximation?
- Basis in paper: [inferred] The paper mentions the potential application of sensitivity bounds to other parameterized policy classes but does not explore this extension.
- Why unresolved: The analysis is primarily for tabular settings, and the paper does not investigate how the results translate to more complex policy parameterizations.
- What evidence would resolve it: Extending the smoothness and sensitivity analyses to specific function approximation architectures and validating the results through theoretical bounds or empirical tests would address this question.

## Limitations

- The analysis critically relies on ergodicity assumptions (irreducible, aperiodic MDP) to establish smoothness and sensitivity bounds
- The single-trajectory gradient estimator's variance control depends heavily on the exploration parameter α, which can be very small in practice
- Time complexity bounds assume exact gradient oracles that may not be available in practical implementations

## Confidence

- High confidence in smoothness and sensitivity analysis given ergodicity (supported by perturbation theory)
- Medium confidence in convergence rates for oracle-based algorithms (proofs rely on standard optimization theory but game-theoretic analysis adds complexity)
- Low confidence in practical sample complexity without thorough empirical validation (estimator performance highly sensitive to α and trajectory length parameters)

## Next Checks

1. Test algorithm robustness by running experiments on MDPs with varying mixing times to verify convergence rate bounds degrade gracefully as κ0 increases
2. Perform ablation studies on the gradient estimator by systematically varying α and measuring variance to identify when the Õ(1/(minₛ,ₐ π(a|s)δ)) bound breaks down
3. Compare the single-trajectory estimator against multi-trajectory baselines on benchmark average-reward MDPs to quantify practical efficiency gains and identify regimes where reset-based methods may still be preferable