---
ver: rpa2
title: Residual-based Language Models are Free Boosters for Biomedical Imaging
arxiv_id: '2403.17343'
source_url: https://arxiv.org/abs/2403.17343
tags:
- arxiv
- language
- visual
- tasks
- biomedical
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates applying frozen transformer blocks from
  large language models (LLMs) as encoders for biomedical imaging tasks, a domain
  traditionally devoid of language data. The method involves integrating a frozen
  LLM transformer block into a visual encoder architecture, with trainable linear
  layers for feature dimension alignment and residual connections for information
  flow.
---

# Residual-based Language Models are Free Boosters for Biomedical Imaging

## Quick Facts
- arXiv ID: 2403.17343
- Source URL: https://arxiv.org/abs/2403.17343
- Reference count: 40
- This paper investigates applying frozen transformer blocks from large language models (LLMs) as encoders for biomedical imaging tasks, achieving consistent performance improvements without language inputs.

## Executive Summary
This paper presents a novel approach to enhancing biomedical image classification by integrating frozen transformer blocks from pre-trained large language models (LLMs) into vision encoders. The method diverges from traditional multi-modal frameworks by operating without language inputs or prompts, instead using the LLM block as a pure visual feature encoder. Through systematic experiments across diverse 2D and 3D biomedical imaging datasets, the authors demonstrate consistent performance improvements, with some tasks achieving state-of-the-art results. The study concludes that residual-based LLMs can serve as effective, plug-and-play boosters for biomedical imaging tasks.

## Method Summary
The method involves integrating a frozen transformer block from a pre-trained LLM into a vision encoder architecture. Visual tokens from a base vision transformer are passed through linear adaptation layers before and after the LLM block, with residual connections preserving information flow. The LLM block's parameters are frozen during training, while the rest of the encoder and classifier are updated. The approach removes language-specific components like auto-regressive masks and positional embeddings to focus purely on visual token interactions.

## Key Results
- Consistent accuracy and AUC improvements across multiple MedMNIST datasets (BreastMNIST, RetinaMNIST, PneumoniaMNIST, DermaMNIST, OCTMNIST, OrganAMNIST)
- Some tasks achieved state-of-the-art results compared to baseline vision transformers
- Unexpected finding: fine-tuning the LLM block decreased performance compared to keeping it frozen
- The method worked effectively for both 2D and 3D biomedical imaging tasks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Residual-based LLM blocks can act as effective feature encoders for biomedical images without language prompts.
- Mechanism: A frozen transformer block from a pre-trained LLM is inserted into a vision encoder pipeline with linear adaptation layers for dimension alignment and residual connections for gradient flow.
- Core assumption: Pre-trained LLM weights encode sufficiently general representations to extract meaningful features from visual tokens.
- Evidence anchors: [abstract] describes using frozen LLM blocks as innovative encoders; [section] explains dimension alignment via FE and FD layers.
- Break condition: If visual tokens' distribution differs too much from LLM's text training data, the residual connection may dominate and render the LLM block ineffective.

### Mechanism 2
- Claim: Removing language-specific components enables the LLM block to focus on visual token interactions.
- Mechanism: Auto-regressive attention masks are omitted for simultaneous visual token processing, and positional embeddings are removed to avoid mismatch with visual backbones.
- Core assumption: LLM self-attention layers can capture useful structure in visual tokens without language-specific positional cues.
- Evidence anchors: [section] explains forgoing auto-regressive masks and removing positional embeddings.
- Break condition: If visual data relies heavily on spatial ordering, removing positional embeddings could degrade performance.

### Mechanism 3
- Claim: Freezing the LLM block and only training adaptation layers simplifies optimization and avoids overfitting.
- Mechanism: All LLM transformer block parameters are frozen during training while adaptation layers, vision encoder, and classifier are updated.
- Core assumption: Frozen pre-trained weights already encode rich representations, and fine-tuning on small biomedical datasets risks overfitting.
- Evidence anchors: [section] describes freezing all FL parameters and [section] notes unexpected decline in performance with fine-tuning.
- Break condition: For very large-scale biomedical datasets with abundant labeled data, fine-tuning might yield better performance than freezing.

## Foundational Learning

- Concept: Vision Transformer (ViT) architecture and tokenization of images into patches.
  - Why needed here: The method uses ViT as the base encoder to convert images into visual tokens processed by the LLM block.
  - Quick check question: In ViT, how are images converted into tokens before being fed into transformer layers?

- Concept: Residual connections and their role in gradient flow.
  - Why needed here: Residual connections are critical for enabling the frozen LLM block to receive gradients and for preserving original visual information alongside transformed features.
  - Quick check question: What is the purpose of adding a residual connection around a transformer block in a neural network?

- Concept: Large Language Model (LLM) transformer architecture and pre-training objectives.
  - Why needed here: Understanding that the frozen LLM block was pre-trained on text helps explain why it might still capture useful patterns in visual tokens when integrated into a vision pipeline.
  - Quick check question: What are the typical pre-training objectives for LLM transformer blocks, and how do they differ from vision transformer objectives?

## Architecture Onboarding

- Component map: Input image → ViT encoder (FV) → Patch embeddings → CLS token → Linear adaptation layer (FE) → Frozen LLM transformer block (FL) → Linear adaptation layer (FD) → MLP classifier (FC) → Output label
- Critical path: Image → FV → FE → FL → FD → FC → Prediction with residual paths allowing bypass of FL if needed
- Design tradeoffs:
  - Freezing FL reduces trainable parameters but risks underfitting if visual tokens differ significantly from text
  - Removing positional embeddings simplifies integration but may lose spatial cues
  - Using adaptation layers adds learnable parameters while keeping FL frozen
- Failure signatures:
  - No performance gain over baseline ViT → FL not contributing useful features
  - Performance drop → FL disrupting learned visual representations
  - Training instability → Residual connection not preserving gradient flow
- First 3 experiments:
  1. Baseline ViT vs. ViT + frozen FL (no adaptation layers) to test if raw integration works
  2. ViT + FE and FD only (no FL) to measure impact of added capacity vs. pre-trained weights
  3. ViT + frozen FL + residual with positional embeddings removed vs. with them to test positional embedding impact

## Open Questions the Paper Calls Out
None

## Limitations
- Lack of ablation studies isolating contributions of frozen LLM block, residual connections, and adaptation layers
- No experiments testing different LLM scales or types to determine generalizability beyond the specific LLaMA variant used
- Performance on non-MNIST biomedical datasets with higher resolution and complexity remains untested
- Claim about avoiding overfitting by freezing LLM parameters lacks rigorous validation across datasets with varying sizes

## Confidence
- **High confidence**: Core experimental results showing consistent accuracy and AUC improvements across multiple MedMNIST datasets with clear and reproducible architectural description
- **Medium confidence**: Mechanism explanation for why frozen LLM blocks help, particularly the assumption that pre-trained weights generalize to visual tokens
- **Low confidence**: Claim that removing positional embeddings and auto-regressive masks is beneficial, stated as assumption without direct ablation evidence

## Next Checks
1. **Ablation study**: Run experiments removing the frozen LLM block while keeping residual connections and adaptation layers to quantify specific contribution of pre-trained weights versus architectural changes
2. **Scale sensitivity test**: Repeat key experiments using smaller/larger LLM blocks or different LLM architectures to determine whether observed benefits depend on specific model scales or types
3. **Positional embedding impact**: Run controlled experiments comparing performance with and without positional embeddings in the LLM block to validate the assumption that removing them improves performance for visual tasks