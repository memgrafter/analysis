---
ver: rpa2
title: 'Confidence Matters: Revisiting Intrinsic Self-Correction Capabilities of Large
  Language Models'
arxiv_id: '2402.12563'
source_url: https://arxiv.org/abs/2402.12563
tags:
- answer
- prompt
- your
- arxiv
- confidence
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper revisits the intrinsic self-correction capabilities
  of large language models (LLMs), addressing the debate on their feasibility. The
  authors identify a critical latent factor - the "confidence" of LLMs - in the self-correction
  process.
---

# Confidence Matters: Revisiting Intrinsic Self-Correction Capabilities of Large Language Models

## Quick Facts
- arXiv ID: 2402.12563
- Source URL: https://arxiv.org/abs/2402.12563
- Reference count: 40
- This paper introduces the If-or-Else (IoE) prompting framework to improve LLM self-correction by leveraging confidence assessment.

## Executive Summary
This paper addresses the debate around LLM self-correction capabilities by identifying "confidence" as a critical latent factor. The authors propose an If-or-Else (IoE) prompting framework that guides LLMs to assess their confidence before deciding whether to revise their answers. Through extensive experiments across six benchmark datasets and four LLM models, they demonstrate that the IoE-based prompt consistently improves the accuracy of self-corrected responses over both initial answers and critical prompting baselines.

## Method Summary
The IoE prompting framework operates by first asking the model whether it is "very confident" in its answer. If confident, the answer is retained; if not, the model is prompted to review and potentially revise. This approach replaces traditional two-stage self-correction (find problem → revise) with a single-stage conditional process. The method is evaluated across deterministic math problems, open-ended reasoning tasks, multi-modal datasets, and symbolic problems using GPT-3.5, GPT-4, and Mistral-Medium models with temperature=0.

## Key Results
- The IoE-based prompt consistently improves self-correction accuracy over initial answers and critical prompting baselines
- Notable decrease in correct-to-incorrect transitions when using IoE prompting compared to critical prompting
- Effective across five task types and four model sizes, demonstrating scalability

## Why This Works (Mechanism)

### Mechanism 1
- Claim: IoE prompting improves self-correction by incorporating LLM confidence assessment before revision
- Mechanism: Model first assesses if "very confident" - if yes, retains answer; if no, prompts revision to avoid over-critiquing correct but low-confidence answers
- Core assumption: LLMs possess internal confidence signals correlated with correctness that can be elicited via conditional prompts
- Evidence anchors: LLMs observed to understand their own confidence levels; no direct confidence-evidence studies cited
- Break condition: If confidence signals decouple from correctness or become noisy, IoE method will fail

### Mechanism 2
- Claim: Confidence-aware prompting reduces correct-to-incorrect transitions during self-correction
- Mechanism: High-confidence correct answers are retained while only low-confidence ones are revised, preventing corruption of correct answers
- Core assumption: Accuracy drops when models are forced to revise even correct answers, especially in deterministic tasks
- Evidence anchors: Critical prompt decreased accuracy from 75.6% to 72.4%; notable decrease in correct-to-incorrect transitions with IoE
- Break condition: If confidence estimation becomes unreliable in ambiguous tasks, filtering effect degrades

### Mechanism 3
- Claim: Confidence-based self-correction scales across task types and model sizes
- Mechanism: IoE method works on deterministic math, open-ended reasoning, multi-modal, and symbolic tasks with consistent improvements
- Core assumption: Confidence assessment signal generalizes across diverse reasoning domains
- Evidence anchors: Extensive experimental analyses show IoE's capability to assess confidence and enhance self-correction across 5 task types and 4 models
- Break condition: If confidence assessment fails in new domains or model types, method will not generalize

## Foundational Learning

- Concept: Conditional prompting
  - Why needed here: IoE method hinges on asking model a conditional "If confident, maintain; else revise" question before self-correction
  - Quick check question: Can you write a prompt that asks model to decide between two actions based on its own assessment?

- Concept: Confidence calibration in LLMs
  - Why needed here: Method assumes model can estimate its own certainty correlating with actual correctness
  - Quick check question: How would you test whether an LLM's "confidence" matches consistency across multiple runs?

- Concept: Prompting efficiency vs. two-stage refinement
  - Why needed here: IoE replaces two-stage (find problem → revise) with single-stage (assess confidence → conditional revise), trading slight accuracy for speed
  - Quick check question: What is the trade-off in accuracy if you remove explicit "find your problem" step from critical prompt?

## Architecture Onboarding

- Component map: Standard Prompt (P1) → Confidence Prompt (P2: "If very confident, maintain; else update") → Optional Decision Refinement (P3: "Check and choose best answer") → Final Answer
- Critical path: P1 → P2 → (optional P3) → output
- Design tradeoffs:
  - Single-stage vs. two-stage prompting: 1 inference vs. 2 inferences, slight accuracy difference
  - Explicit confidence cue ("very confident") vs. implicit: affects calibration robustness
  - Politeness ("please") vs. neutral tone: small accuracy benefit but added verbosity
- Failure signatures:
  - Consistent over-correction (correct → incorrect): confidence estimation failing
  - No improvement over baseline: IoE not gating revisions effectively
  - Low accuracy on deterministic tasks: model over-trusting confidence in uncertain contexts
- First 3 experiments:
  1. Run IoE prompt on deterministic GSM8K subset and compare correct→incorrect transitions to baseline
  2. Compare single-stage vs. two-stage IoE prompting on same set to measure speed/accuracy trade-off
  3. Test IoE with and without "very" adverb to check robustness of confidence calibration

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does LLM confidence assessment capability vary across different model architectures and training datasets?
- Basis in paper: Study uses limited set of commercial LLMs without comparing their confidence assessment capabilities or investigating training data influence
- Why unresolved: Uses only GPT-3.5 and GPT-4 without comparing confidence assessment across architectures
- What evidence would resolve it: Comparative studies across multiple LLM architectures with different training datasets, measuring confidence assessment accuracy and consistency

### Open Question 2
- Question: Can IoE prompting principle be adapted for continuous or real-time self-correction scenarios?
- Basis in paper: Presents IoE for discrete question-answering tasks, doesn't address applicability to ongoing interactive scenarios
- Why unresolved: Experimental setup focuses on static benchmarks and individual question processing
- What evidence would resolve it: Implementation and evaluation in interactive dialogue systems, measuring effectiveness over extended interactions

### Open Question 3
- Question: How does confidence assessment in self-correction affect computational efficiency and response time in practical applications?
- Basis in paper: Highlights IoE efficiency compared to multi-trial consistency checking but doesn't analyze computational overhead
- Why unresolved: Demonstrates improved accuracy but doesn't quantify trade-offs between accuracy gains and computational resources or response latency
- What evidence would resolve it: Benchmarking studies comparing computational costs of standard, critical, and IoE-based prompting across task complexities and scales

## Limitations

- The paper's claims rest on the assumption that LLM confidence signals reliably correlate with actual correctness, but this lacks external validation
- Performance improvements demonstrated on six benchmarks may not generalize to all reasoning tasks or domains
- The method's performance with temperature settings beyond the fixed temperature=0 used in experiments remains unexplored

## Confidence

- **High confidence**: Observation that critical prompting alone can degrade accuracy from 75.6% to 72.4% is empirically demonstrated and well-supported
- **Medium confidence**: Claim that IoE prompting consistently improves self-correction across diverse tasks, as results show positive trends with varying effect sizes
- **Medium confidence**: Assertion that LLMs can "understand" their own confidence, as this relies on observed behavior rather than direct measurement of internal mechanisms

## Next Checks

1. Test the IoE method across temperature values from 0.0 to 1.0 to assess robustness of confidence calibration under stochastic generation
2. Conduct cross-domain validation using at least three additional benchmark types (code generation, creative writing, fact verification) to test generalization
3. Implement an ablation study comparing IoE prompting against a simple "revise if wrong" baseline without explicit confidence assessment to isolate the value of the confidence gating mechanism