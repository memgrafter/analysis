---
ver: rpa2
title: Let people fail! Exploring the influence of explainable virtual and robotic
  agents in learning-by-doing tasks
arxiv_id: '2411.10176'
source_url: https://arxiv.org/abs/2411.10176
tags:
- participants
- explanations
- robot
- group
- agents
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The study compared the effects of classical versus partner-aware
  explanations on participants learning a nuclear power plant management task with
  either a computer or a humanoid robot, plus a self-taught baseline. Participants
  using adaptive explanations with the computer made faster decisions and completed
  more actions than those with classical explanations, but learning outcomes were
  similar.
---

# Let people fail! Exploring the influence of explainable virtual and robotic agents in learning-by-doing tasks

## Quick Facts
- arXiv ID: 2411.10176
- Source URL: https://arxiv.org/abs/2411.10176
- Reference count: 38
- Primary result: Autonomous exploration in learning-by-doing tasks produces superior knowledge acquisition compared to guided exploration with explainable AI agents.

## Executive Summary
This study investigates how different explanation strategies affect human learning when collaborating with AI agents in a nuclear power plant management task. Participants either learned autonomously or received classical or adaptive explanations from either a computer or humanoid robot. While adaptive explanations influenced interaction behavior and increased robot persuasiveness, they did not improve learning outcomes. Surprisingly, the self-taught group explored more, made more errors, but ultimately outperformed all assisted groups on post-task tests, especially on scenario-based questions. This suggests that autonomous trial-and-error learning may foster deeper understanding than guided exploration with explainable AI.

## Method Summary
The study employed a between-subject design with 22 participants per computer/robot group (divided into C-XAI and A-XAI conditions) and 11 self-taught baseline participants. The task involved managing a nuclear power plant simulation with 8 features and 12 possible actions over a 30-minute training phase. A decision tree AI model trained via Conservative Q-Improvement selected actions, with explanations generated either classically (most relevant features) or adaptively (contrastive, partner-aware). Post-experiment assessment included knowledge tests and behavioral measures like decision time, action counts, and adherence to AI suggestions.

## Key Results
- Participants using adaptive explanations with the computer made faster decisions and completed more actions than those with classical explanations, but learning outcomes were similar.
- With the robot, adaptive explanations increased persuasion but did not change task performance or knowledge acquisition.
- The self-taught group explored more, made more errors during training, but outperformed all assisted groups on post-task tests, especially on scenario-based questions.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Adaptive explanations automate decision-making in human-computer interaction by reducing decision time and cognitive load.
- Mechanism: Contrastive explanations focus users on the key difference between the AI's suggested action and their own initial choice, eliminating the need to process multiple potential reasoning paths.
- Core assumption: Users are cognitive misers who prefer the most efficient route to decision-making, even if it reduces exploration.
- Evidence anchors:
  - [abstract] "With the computer, participants enhanced their task completion times."
  - [section 4.3] "participants who interacted with the computer moved faster and were more resolute with the adaptive explanations"
  - [section 4.3] "participants in the A-XAI group made faster decisions than those who received classical ones"
- Break condition: When tasks require deep exploration or creative problem-solving, users benefit from slower, more deliberative processing rather than automated decision paths.

### Mechanism 2
- Claim: Adaptive explanations increase robot persuasiveness by leveraging social presence and perceived competence.
- Mechanism: The robot's physical embodiment and social cues (eye contact, facial expressions) create an authority relationship where contrastive explanations are interpreted as guidance from a knowledgeable partner rather than just information.
- Core assumption: Physical presence and social interaction create different cognitive processing than purely digital interfaces, leading to different trust and compliance patterns.
- Evidence anchors:
  - [abstract] "those interacting with the humanoid robot were more inclined to follow its suggestions"
  - [section 4.5] "the adaptive robot persuaded participants to opt for its actions (rather than their first selection) more than the computer did"
  - [section 4.4] "participants relied more on the robot's suggestions when justified with adaptive explanations"
- Break condition: When users have high domain expertise or strong pre-existing preferences, the social influence effect diminishes.

### Mechanism 3
- Claim: Autonomous exploration produces better learning outcomes than guided exploration with explainable AI.
- Mechanism: Self-directed trial-and-error learning forces deeper cognitive engagement with the problem space, creating more robust mental models that transfer to novel scenarios.
- Core assumption: Making mistakes and discovering rules independently creates stronger, more generalizable knowledge than following an expert's guidance.
- Evidence anchors:
  - [abstract] "participants autonomously performing the learning-by-doing task demonstrated superior knowledge acquisition than those assisted by explainable AI"
  - [section 4.6.1] "the Self-taught group performed more actions during training on average than those who interacted with both the artificial agents"
  - [section 4.6.1] "Self-taught participants outperformed those who interacted with the artificial agents at the post-experiment test, especially regarding the scenarios questions"
- Break condition: When time constraints are severe or the task complexity exceeds human working memory capacity, guided learning may produce better outcomes.

## Foundational Learning

- Concept: Explanation selection strategies (classical vs. adaptive)
  - Why needed here: The study compares how different methods of choosing what information to explain affects learning and behavior
  - Quick check question: What is the key difference between classical explanations (using most relevant features) and adaptive explanations (using contrastive, partner-aware features)?

- Concept: Human cognitive biases in AI collaboration
  - Why needed here: Understanding why users over-rely on AI suggestions despite limited learning benefits
  - Quick check question: What is automation bias and how might it explain why assisted participants performed worse on transfer tests?

- Concept: Learning transfer and generalization
  - Why needed here: The study tests whether task-specific learning transfers to novel scenarios
  - Quick check question: Why would autonomous exploration lead to better performance on scenario-based questions compared to guided exploration?

## Architecture Onboarding

- Component map: User action → Environment state update → AI model inference → Explanation generation → User receives explanation → User makes next decision
- Critical path: The loop repeats for each step in the 30-minute training phase, with the AI model selecting actions based on current state and the explanation module providing either classical or adaptive justifications.
- Design tradeoffs: Fixed 30-minute training time creates time pressure that may influence behavior; using a simple decision tree rather than a complex neural network ensures explanations are directly traceable to model decisions; between-subject design eliminates learning effects but requires larger sample size.
- Failure signatures: If explanations are too verbose, users spend more time reading than acting; if AI suggestions are too accurate, users stop exploring; if the robot's social cues are too subtle, the persuasion effect may not manifest; if the task is too simple, all groups perform similarly regardless of condition.
- First 3 experiments:
  1. Replicate the study with a different task domain (e.g., medical diagnosis) to test generalizability of the autonomous learning advantage
  2. Vary the time constraint (fixed steps vs. fixed time) to understand how time pressure affects the exploration vs. exploitation tradeoff
  3. Introduce cognitive forcing strategies to test whether interrupting automated decision patterns improves learning outcomes in assisted conditions

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What are the specific cognitive biases (e.g., automation bias, illusion of explanatory depth) that most significantly influence participants' reliance on AI suggestions in learning-by-doing tasks?
- Basis in paper: [explicit] The authors discuss the potential role of automation biases and the illusion of explanatory depth in participants' over-reliance on AI suggestions, but do not provide specific empirical evidence identifying which biases are most prevalent.
- Why unresolved: While the authors speculate about the influence of cognitive biases, they do not conduct a systematic investigation or provide data to quantify the impact of specific biases on participants' decision-making.
- What evidence would resolve it: A follow-up study that explicitly measures participants' susceptibility to various cognitive biases (e.g., using validated questionnaires) and correlates these measures with their reliance on AI suggestions and learning outcomes.

### Open Question 2
- Question: How do different types of explanation strategies (e.g., contrastive vs. classical) affect participants' long-term retention and transfer of knowledge in learning-by-doing tasks?
- Basis in paper: [explicit] The study found that while contrastive explanations influenced participants' behavior during the task, they did not significantly improve knowledge acquisition compared to classical explanations. However, the long-term effects on knowledge retention and transfer were not assessed.
- Why unresolved: The study only evaluated participants' knowledge immediately after the learning phase, leaving the question of how different explanation strategies impact long-term learning outcomes unanswered.
- What evidence would resolve it: A longitudinal study that tracks participants' performance on similar tasks over an extended period (e.g., weeks or months) to assess the durability and transferability of the knowledge gained with different explanation strategies.

### Open Question 3
- Question: What are the optimal interaction modalities and presentation formats for explanations in human-AI collaboration that maximize learning outcomes and minimize over-reliance on AI suggestions?
- Basis in paper: [inferred] The authors highlight the importance of user-centered XAI and suggest that over-simplified explanations or certain presentation formats may contribute to participants' over-reliance on AI suggestions. However, they do not explore alternative interaction modalities or presentation formats.
- Why unresolved: The study focuses on comparing two specific explanation strategies (contrastive vs. classical) but does not investigate a broader range of interaction modalities (e.g., visual, auditory, haptic) or presentation formats (e.g., interactive, adaptive, personalized) that could potentially enhance learning outcomes.
- What evidence would resolve it: A comparative study that tests various interaction modalities and presentation formats for explanations, measuring their impact on participants' learning outcomes, engagement, and reliance on AI suggestions.

## Limitations

- The between-subject design introduces potential confounds from individual differences in baseline knowledge and learning aptitude.
- The relatively small sample sizes (22 participants per assisted condition, 11 in baseline) limit statistical power for detecting smaller effect sizes.
- The 30-minute time constraint may have artificially constrained exploration behavior, potentially exaggerating the autonomous learning advantage.
- The specific nature of the nuclear power plant task may limit generalizability to other domains where safety constraints or complexity differ substantially.

## Confidence

- High confidence: The finding that autonomous exploration produces better transfer learning outcomes (measured through scenario-based questions) is well-supported by multiple behavioral metrics showing increased action diversity and error rates during training.
- Medium confidence: The differential effects of adaptive explanations between computer and robot interfaces are reasonably supported, though the mechanism for why embodiment changes persuasive outcomes requires further investigation.
- Medium confidence: The conclusion that adaptive explanations increase persuasion without improving learning outcomes is supported, but the lack of knowledge gain may reflect limitations in the assessment design rather than the explanation approach itself.

## Next Checks

1. Conduct a within-subject replication varying the explanation type across multiple tasks to control for individual differences in learning style and baseline knowledge.
2. Test whether interrupting automated decision patterns with cognitive forcing strategies (e.g., requiring justification for AI-suggested actions) improves learning outcomes in assisted conditions.
3. Evaluate whether the autonomous learning advantage persists when time constraints are relaxed or when tasks require different cognitive skills (e.g., pattern recognition vs. causal reasoning).