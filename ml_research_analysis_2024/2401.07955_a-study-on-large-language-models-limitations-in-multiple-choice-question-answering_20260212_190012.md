---
ver: rpa2
title: A Study on Large Language Models' Limitations in Multiple-Choice Question Answering
arxiv_id: '2401.07955'
source_url: https://arxiv.org/abs/2401.07955
tags:
- answer
- response
- choice
- question
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper investigates whether small open-source Large Language
  Models (LLMs) can properly understand and answer Multiple Choice Questions (MCQs).
  The authors test 26 small open-source models on a dataset of 885 statements across
  six categories of truth.
---

# A Study on Large Language Models' Limitations in Multiple-Choice Question Answering

## Quick Facts
- arXiv ID: 2401.07955
- Source URL: https://arxiv.org/abs/2401.07955
- Authors: Aisha Khatun; Daniel G. Brown
- Reference count: 24
- 65% of small LLMs fail to understand MCQ task

## Executive Summary
This paper investigates whether small open-source Large Language Models can properly understand and answer Multiple Choice Questions (MCQs). Testing 26 small models on 885 statements across six truth categories, the authors evaluate two response extraction methods and examine choice order dependence. The results reveal significant limitations: most models fail to understand the task, are heavily choice-order dependent, or both. Only 2 models consistently answer MCQs correctly without order dependence. These findings raise serious concerns about using small LLMs in evaluation benchmarks and highlight fundamental limitations in their instruction-following capabilities.

## Method Summary
The study evaluates 26 small open-source LLMs on the TruthEval dataset containing 885 statements across six truth categories. Two response extraction methods are tested: parsed text responses using heuristics and probability-based answers using token likelihoods. Each statement is presented with 5 different prompt variations asking if it's true, with randomized choice order to detect choice-order dependence. Base models are tested with 8-shot examples while fine-tuned models use zero-shot prompting. The evaluation measures task understanding, choice-order independence, and response quality across all models.

## Key Results
- 65% of models do not understand the MCQ task (showing zero aggregate probability)
- 70-75% of responding models are choice order dependent
- 50-65% of models respond with mostly 'A' regardless of choice content
- Only 2 models consistently answer MCQs correctly without choice order dependence

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLMs fail to follow task-specific instructions due to over-reliance on choice order rather than content.
- Mechanism: When multiple-choice options are presented, many models treat the first option as the default choice. Randomizing the order of choices reveals this dependence because the response distribution becomes uniform across options.
- Core assumption: The model's training data contains enough examples of "A" being correct that it defaults to the first option when unsure.
- Evidence anchors:
  - [abstract] "70-75% of responding models are choice order dependent"
  - [section 7.1] "11 of the 13 models indeed produce randomized outputs: it is not that they always answer 'Yes', but that they always answer 'A'"
  - [corpus] Weak - related papers focus on bias mitigation, not choice order effects.

### Mechanism 2
- Claim: Probability-based evaluation methods are unreliable for MCQ tasks because low-probability tokens can still be selected as answers.
- Mechanism: When using log-probabilities to select answers, models may choose tokens with near-zero probabilities, indicating poor understanding of the task rather than a confident selection.
- Core assumption: A well-calibrated model should assign meaningful probabilities to all valid answer choices.
- Evidence anchors:
  - [section 6.3.3] "65% (17) models' combined probability is zero"
  - [section 6.3.3] "the probabilities of the letters among the vocabulary are extremely small"
  - [corpus] Weak - no direct discussion of probability calibration in related works.

### Mechanism 3
- Claim: Instruction-following ability varies significantly across model architectures, with fine-tuned models generally outperforming base models.
- Mechanism: Models that have undergone instruction tuning or reinforcement learning are better at parsing prompts and extracting the correct answer format, reducing "Bad Output" cases.
- Core assumption: Fine-tuning on instruction datasets teaches models to recognize and respond to specific prompt structures.
- Evidence anchors:
  - [section 7.1] "Six (four Base and two Fine Tuned) of the models have mostly 'Bad Output'"
  - [section 8] "Some Mistral-based models show better performance on both fronts"
  - [corpus] Moderate - "More Bias, Less Bias" discusses instruction tuning for MCQ tasks.

## Foundational Learning

- Concept: Token probability calibration
  - Why needed here: Understanding why models choose answers with near-zero probabilities requires knowledge of how token likelihoods are computed and interpreted.
  - Quick check question: If a model assigns probability 0.0001 to each answer choice, what does that tell you about its confidence in the task?

- Concept: Choice order effects in language models
  - Why needed here: The study reveals that many models are order-dependent, so understanding how positional bias affects predictions is crucial.
  - Quick check question: What would you expect to happen to a model's answer distribution if you always put the correct answer in position C?

- Concept: Instruction tuning vs. base model behavior
  - Why needed here: The paper contrasts performance between base and fine-tuned models, so understanding the impact of instruction tuning is important.
  - Quick check question: Why might a base model generate multiple examples instead of answering the question when given 8-shot examples?

## Architecture Onboarding

- Component map: Prompt generation -> Model inference -> Response extraction (text parsing or probability-based) -> Answer validation -> Randomization testing
- Critical path: Prompt generation -> Model inference -> Response extraction. The extraction method determines whether you can detect "Bad Output" or choice order dependence.
- Design tradeoffs: Text parsing is more interpretable but can miss embedded answers; probability-based methods always produce an answer but may select meaningless tokens. Randomization testing adds compute cost but reveals order dependence.
- Failure signatures: High proportion of "A" responses -> choice order dependence; Zero aggregate probability -> task misunderstanding; Consistent "Bad Output" -> poor instruction following.
- First 3 experiments:
  1. Run a small sample of statements with randomized choice order and compare response distributions to detect order dependence.
  2. Calculate aggregate probabilities for A, B, C, D across all prompts to assess task understanding.
  3. Test both text parsing and probability-based extraction on the same model to compare reliability and identify extraction method limitations.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What specific architectural or training modifications could make small LLMs more reliable at understanding and answering multiple-choice questions?
- Basis in paper: [explicit] The paper identifies that most small LLMs fail to understand MCQ tasks, are choice-order dependent, or both, with only 2 models showing consistent correct performance.
- Why unresolved: The paper focuses on diagnosing the problem but does not explore potential solutions or architectural changes that could address these limitations.
- What evidence would resolve it: Empirical studies testing various architectural modifications (e.g., specialized fine-tuning, attention mechanisms, or token prediction strategies) on MCQ performance across multiple small LLM models.

### Open Question 2
- Question: How does the size of an LLM correlate with its ability to understand and answer multiple-choice questions correctly?
- Basis in paper: [inferred] The study focuses exclusively on small open-source models (up to 7B parameters) and does not compare their performance to larger models or closed-source alternatives.
- Why unresolved: The paper deliberately limits its scope to small models but acknowledges that larger models may show different patterns, leaving the size-performance relationship unexplored.
- What evidence would resolve it: Comparative experiments testing MCQ performance across a spectrum of model sizes (from small to large) and architectures, measuring both task understanding and choice-order independence.

### Open Question 3
- Question: Are there alternative formats or presentation methods for MCQs that would reduce choice-order dependence in LLM responses?
- Basis in paper: [explicit] The paper identifies choice-order dependence as a major limitation, with 70-75% of responding models showing this behavior, but does not explore whether alternative MCQ presentations could mitigate this issue.
- Why unresolved: The study uses a standard MCQ format but does not investigate whether modifications to question presentation (e.g., random symbols, numerical ordering, or structural changes) could improve model performance.
- What evidence would resolve it: Systematic testing of various MCQ presentation formats on the same models, measuring choice-order independence and task understanding across different presentation methods.

## Limitations

- The study relies on a single dataset (TruthEval) with 885 statements, which may not generalize to broader MCQ domains
- Post-processing heuristics for text response extraction are described but not fully specified, creating reproducibility gaps
- The choice order randomization methodology tests only 5 permutations, which may not capture the full spectrum of positional bias effects

## Confidence

**High Confidence**: The observation that 65% of models fail to understand the MCQ task is well-supported by the zero-aggregate probability results across multiple models. The distinction between base and fine-tuned models' performance is also robust, with clear patterns showing fine-tuned models produce fewer "Bad Output" responses.

**Medium Confidence**: The claim that 70-75% of models are choice order dependent is methodologically sound but relies on a specific randomization approach. While the results are internally consistent, different randomization patterns might yield varying proportions.

**Low Confidence**: The assertion that only 2 models consistently answer MCQs correctly is based on narrow criteria and doesn't account for potential performance variations across different MCQ domains or question complexities.

## Next Checks

1. **Cross-dataset validation**: Test the same 26 models on at least two additional MCQ datasets covering different domains (e.g., scientific facts, general knowledge, logical reasoning) to assess whether the observed limitations generalize beyond the TruthEval dataset.

2. **Alternative randomization protocol**: Implement a comprehensive choice order randomization scheme that tests all 24 possible permutations for 4-choice questions on a subset of models, then compare the response distributions to determine if the 70-75% order dependence rate holds under exhaustive testing.

3. **Probability calibration analysis**: For models showing near-zero probabilities, conduct a systematic analysis of their token probability distributions across all vocabulary items (not just A, B, C, D) to determine whether this represents true task misunderstanding or a broader calibration issue affecting all token predictions.