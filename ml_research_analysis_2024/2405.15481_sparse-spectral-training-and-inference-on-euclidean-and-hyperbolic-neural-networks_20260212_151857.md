---
ver: rpa2
title: Sparse Spectral Training and Inference on Euclidean and Hyperbolic Neural Networks
arxiv_id: '2405.15481'
source_url: https://arxiv.org/abs/2405.15481
tags:
- training
- relora
- low-rank
- lora
- full-rank
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Sparse Spectral Training (SST), a parameter-efficient
  training method that optimizes memory usage during pre-training by leveraging spectral
  properties of weight matrices. SST updates all singular values and selectively updates
  singular vectors through multinomial sampling weighted by singular value magnitudes,
  while periodically reinitializing low-rank parameters using SVD to reduce distortion
  relative to full-rank training.
---

# Sparse Spectral Training and Inference on Euclidean and Hyperbolic Neural Networks

## Quick Facts
- **arXiv ID:** 2405.15481
- **Source URL:** https://arxiv.org/abs/2405.15481
- **Reference count:** 40
- **Key outcome:** SST achieves 97.4% reduction in perplexity gap compared to other low-rank methods while using only 18.7% of trainable parameters on LLaMA-1.3B.

## Executive Summary
This paper introduces Sparse Spectral Training (SST), a parameter-efficient training method that optimizes memory usage during pre-training by leveraging spectral properties of weight matrices. SST updates all singular values and selectively updates singular vectors through multinomial sampling weighted by singular value magnitudes, while periodically reinitializing low-rank parameters using SVD to reduce distortion relative to full-rank training. The method is evaluated across multiple tasks and architectures, including OPT and LLaMA language models, Transformers for machine translation, and hyperbolic graph neural networks, consistently outperforming existing memory-efficient training methods.

## Method Summary
SST is a parameter-efficient training method that maintains full-rank expressive power while only training a fraction of parameters. It decomposes weight matrices into SVD components (U, Σ, VT), fully updating all singular values at each step while selectively updating singular vectors through multinomial sampling weighted by singular value magnitudes. To maintain orthogonality and prevent spectral drift, SST periodically performs re-SVD initialization. The method uses AdamW optimizer with learning rates of 1e-3 for full-rank parameters and 3e-3 for low-rank parameters (U, VT, Σ), with iteration intervals of 200 steps and 20-step warmup per iteration.

## Key Results
- On LLaMA-1.3B, SST achieves 97.4% reduction in perplexity gap compared to other low-rank methods while using only 18.7% of trainable parameters
- For machine translation, SST reduces BLEU score gaps by an average of 66.7%
- For hyperbolic graph neural networks, SST reduces performance gaps by 73.7% in node classification and 82.5% in link prediction
- SST consistently outperforms existing memory-efficient training methods and matches or exceeds full-rank training performance across diverse scenarios

## Why This Works (Mechanism)

### Mechanism 1
SST maintains full-rank expressive power while only training a fraction of parameters by updating all singular values and selectively updating singular vectors through multinomial sampling. This preserves the full spectral properties of weight matrices without the rank limitations of LoRA.

### Mechanism 2
Periodic SVD reinitialization prevents drift from true spectral properties by maintaining orthogonality among the vectors of U and VT during training, which tends to diminish over time.

### Mechanism 3
SST balances exploration and exploitation in spectral domain by combining strategies of updating all magnitudes (Σ) at each step while cyclically revisiting previously established dominant directions.

## Foundational Learning

- **Concept: Singular Value Decomposition (SVD)**
  - Why needed here: SST fundamentally relies on decomposing weight matrices into U, Σ, VT to manipulate spectral properties
  - Quick check question: What are the dimensions of U, Σ, and VT in the decomposition of an m×n matrix where m ≤ n?

- **Concept: Low-rank approximation and the Eckart-Young-Mirsky theorem**
  - Why needed here: Understanding why LoRA's low-rank constraint limits performance is crucial for appreciating SST's advantages
  - Quick check question: According to the Eckart-Young-Mirsky theorem, what is the minimum reconstruction error when approximating a matrix with rank r?

- **Concept: Multinomial distribution and sampling**
  - Why needed here: SST uses multinomial sampling to select which singular vectors to update, which is central to its parameter efficiency
  - Quick check question: How does multinomial sampling differ from uniform sampling when selecting vectors based on weights?

## Architecture Onboarding

- **Component map:** Linear layers are replaced with SVD-based parameterization (U, Σ, VT), with Σ updated fully each step and U, VT updated via multinomial sampling; periodic re-SVD maintains orthogonality
- **Critical path:** Forward pass uses UΣVTx; backward pass computes gradients for all components; multinomial sampling selects update targets; periodic re-SVD resets orthogonal bases
- **Design tradeoffs:** More complex than LoRA (requires SVD operations) but achieves better performance; trades computational complexity for parameter efficiency
- **Failure signatures:** Saddle point issues if sampling becomes too uniform; performance degradation if re-SVD frequency is wrong; instability if singular values become negative
- **First 3 experiments:**
  1. Replace a single linear layer in a small MLP with SST and compare training dynamics to full-rank and LoRA
  2. Vary the sampling distribution (uniform vs. multinomial) to understand impact on convergence
  3. Test different re-SVD frequencies to find the optimal balance between orthogonality and gradient flow

## Open Questions the Paper Calls Out

### Open Question 1
How does SST's performance scale with very large models beyond 1.3B parameters, particularly in terms of perplexity reduction and parameter efficiency? The paper demonstrates SST's effectiveness on OPT models ranging from 125M to 1.3B parameters but does not test SST on models larger than 1.3B parameters.

### Open Question 2
What is the optimal iteration interval (T3) for SST across different model architectures and tasks, and how does it affect convergence speed and final performance? The paper uses fixed iteration intervals (e.g., 200 steps) but does not provide a systematic study of how to determine optimal T3 values for different architectures or tasks.

### Open Question 3
How does SST's sampling mechanism (multinomial vs uniform vs sequential) affect its ability to escape local minima during pre-training? While the paper evaluates different sampling mechanisms and finds multinomial performs slightly better, it does not investigate how these sampling strategies impact optimization dynamics like escaping local minima or saddle points during training.

## Limitations

- The exact implementation details for key components like multinomial sampling and enhanced gradient calculation are not fully specified
- Limited ablation studies on hyperparameters like re-SVD frequency and sampling distribution parameters
- The paper does not test SST on models larger than 1.3B parameters, leaving uncertainty about scalability

## Confidence

- **High Confidence**: The core mathematical framework of SST (SVD-based parameterization with selective vector updates) is well-established and theoretically sound
- **Medium Confidence**: The empirical results showing SST's advantages are compelling, but the limited ablation studies and lack of hyperparameter sensitivity analysis reduce confidence in understanding when and why SST works best
- **Low Confidence**: The exact implementation details for key components like multinomial sampling and enhanced gradient calculation are not fully specified

## Next Checks

1. Implement and compare three variants: uniform sampling vs. multinomial sampling for singular vector selection, and test the impact of different re-SVD frequencies (50, 100, 200 steps) on orthogonality preservation and final performance
2. Conduct an ablation study on the sampling distribution parameters, testing different weighting schemes beyond the proposed p(i) = 1/2(1/m + Σi/ΣP) to understand their impact on convergence and final accuracy
3. Measure and compare the actual GPU memory usage during training for SST versus LoRA and full-rank methods across different model sizes to verify the claimed memory efficiency benefits