---
ver: rpa2
title: Unsupervised Model Diagnosis
arxiv_id: '2410.06243'
source_url: https://arxiv.org/abs/2410.06243
tags:
- counterfactual
- attributes
- edit
- classifier
- attribute
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Unsupervised Model Diagnosis (UMO), a method
  that uses generative models to discover model vulnerabilities without human input
  or test sets. UMO optimizes counterfactual directions in latent spaces to generate
  semantic perturbations that reveal model failure modes, then matches these changes
  to attributes using language models.
---

# Unsupervised Model Diagnosis

## Quick Facts
- arXiv ID: 2410.06243
- Source URL: https://arxiv.org/abs/2410.06243
- Authors: Yinong Oliver Wang; Eileen Li; Jinqi Luo; Zhaoning Wang; Fernando De la Torre
- Reference count: 40
- One-line primary result: UMO discovers model vulnerabilities without human input or test sets using generative models to create semantic counterfactuals matched to attributes via language models

## Executive Summary
This paper introduces Unsupervised Model Diagnosis (UMO), a method that discovers model vulnerabilities without human input or test sets. UMO uses generative models to optimize counterfactual directions in latent spaces, creating semantic perturbations that reveal model failure modes. The method then matches these changes to attributes using language models, enabling unsupervised diagnosis across classification, segmentation, and keypoint detection tasks. Experiments demonstrate UMO's ability to identify spurious correlations and visualize failure modes in an unsupervised manner, validated through cross-method consistency and counterfactual training robustness improvements.

## Method Summary
UMO generates counterfactual examples by optimizing latent edit vectors in generative model spaces to create images that flip model predictions while preserving semantic similarity. The method uses CLIP embeddings to measure semantic differences between original and counterfactual images, then matches these differences to attribute candidates generated by language models. An ensemble of generative models (StyleGAN and diffusion) provides robustness, and the analysis identifies attributes most associated with model failures. The pipeline operates without human-annotated test sets, instead relying on foundation models' parametric knowledge to discover meaningful semantic variations that affect model predictions.

## Key Results
- UMO successfully identifies spurious correlations in CelebA, AFHQ, ImageNet, and FITYMI datasets without human input
- Cross-method consistency validates the discovered attributes when using different generative backbones (StyleGAN vs. diffusion models)
- Counterfactual training using UMO's discovered attributes improves model robustness by 15-35% across classification, segmentation, and keypoint detection tasks
- The method reveals interpretable failure modes such as skin color biases in facial recognition and background dependencies in object classification

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Optimizing counterfactual directions in generative latent spaces can reveal model vulnerabilities without labeled test sets.
- Mechanism: UMO uses differentiable generative models (GANs or diffusion) to create semantic edits in latent space that fool the target model while preserving CLIP-based semantic similarity, thus exposing failure modes.
- Core assumption: The latent space of generative models is sufficiently disentangled and expressive to capture meaningful semantic variations that affect model predictions.
- Evidence anchors:
  - [abstract] "UMO optimizes for the most counterfactual directions in a generative latent space"
  - [section 3.1] "StyleSpace [51] found a more disentangled latent space by manipulating the modulation weights (i.e., style codes) in StyleGAN affine transformation"
  - [corpus] Weak evidence - no direct corpus papers on this specific mechanism, though related work exists on GAN-based counterfactual explanations
- Break condition: If the generative model's latent space lacks sufficient disentanglement, the semantic edits may not correlate with meaningful model failures.

### Mechanism 2
- Claim: Language models can provide comprehensive attribute candidates for unsupervised model diagnosis.
- Mechanism: GPT-4 is used to generate exhaustive lists of attributes and their values across domains, which are then matched against CLIP embeddings of counterfactual image differences.
- Core assumption: Large language models possess sufficient parametric knowledge to enumerate relevant attributes without human curation.
- Evidence anchors:
  - [abstract] "matches these changes to attributes from wide-ranging text sources, such as dictionaries or language models"
  - [section 3.2] "we use language models as our bank of attribute candidates for UMO"
  - [corpus] Moderate evidence - related work exists on LLM-augmented image generation, but not specifically for model diagnosis attribute generation
- Break condition: If the language model lacks domain-specific knowledge or the attribute enumeration is incomplete, important failure modes may be missed.

### Mechanism 3
- Claim: Ensemble of generative models improves reliability of discovered counterfactual directions.
- Mechanism: UMO generates counterfactual examples from multiple independent generative backbones (StyleGAN and diffusion models) and analyzes the combined results to mitigate backbone-specific biases.
- Core assumption: Different generative models capture complementary aspects of semantic space, and their combination provides more robust coverage of failure modes.
- Evidence anchors:
  - [section 3.1] "we further enhance the reliability of our diagnosis pipeline through an ensemble of latent generative models"
  - [section 4.2] "Fig. 5 illustrates the consistency of counterfactual analysis conducted by UMO, irrespective of whether we utilize the Diffusion or StyleGAN generative model"
  - [corpus] Weak evidence - no direct corpus papers on ensemble approaches for generative model-based model diagnosis
- Break condition: If generative models share similar failure modes or biases, the ensemble may not provide meaningful diversity in discovered counterfactuals.

## Foundational Learning

- Concept: Counterfactual explanations in machine learning
  - Why needed here: UMO fundamentally relies on generating counterfactual examples to diagnose model failures
  - Quick check question: What distinguishes a counterfactual explanation from an adversarial attack in terms of interpretability goals?

- Concept: Generative model latent spaces and their semantic properties
  - Why needed here: The effectiveness of UMO depends on the disentanglement and expressiveness of the generative model's latent space
  - Quick check question: How does StyleSpace differ from the standard W space in StyleGAN in terms of semantic disentanglement?

- Concept: CLIP embeddings and zero-shot classification
  - Why needed here: CLIP provides the bridge between visual counterfactuals and textual attribute analysis in UMO
  - Quick check question: What property of CLIP enables it to serve as a common embedding space for both images and text in UMO's analysis pipeline?

## Architecture Onboarding

- Component map: Target model -> Counterfactual optimization -> CLIP embedding generation -> Attribute matching -> Diagnosis report
- Critical path: Target model → Counterfactual optimization → CLIP embedding generation → Attribute matching → Diagnosis report
- Design tradeoffs:
  - Single vs. ensemble generative models: Tradeoff between simplicity and robustness
  - Manual vs. LLM-generated attribute lists: Tradeoff between control and comprehensiveness
  - Per-image vs. global counterfactual optimization: Tradeoff between specificity and model-centric insights
- Failure signatures:
  - Low similarity scores across all attributes: May indicate the counterfactuals are not semantically meaningful
  - High redundancy in top-ranked attributes: May indicate insufficient uniqueness penalty in analysis
  - No improvement in counterfactual training: May indicate ineffective counterfactual generation
- First 3 experiments:
  1. Diagnose a simple binary classifier on CelebA with known biases to verify the pipeline discovers the planted attributes
  2. Compare counterfactual explanations from StyleGAN vs. diffusion models on the same classifier to verify ensemble benefits
  3. Apply UMO to a segmentation model and verify the discovered attributes relate to road conditions as expected

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does UMO perform when diagnosing models on highly specialized or niche datasets where foundation models may have limited training coverage?
- Basis in paper: [inferred] The paper validates UMO on CelebA, AFHQ, ImageNet, and FITYMI datasets but does not explore performance on specialized domains where CLIP and GPT-4 may lack robust training data.
- Why unresolved: The authors only demonstrate UMO on general-purpose datasets and assume foundation models provide sufficient generalization, without testing edge cases or domain-specific limitations.
- What evidence would resolve it: Experiments applying UMO to diagnose models on specialized datasets (e.g., medical imaging, satellite imagery, or industrial defect detection) where foundation models have limited or no training coverage, measuring diagnosis accuracy and attribute matching performance.

### Open Question 2
- Question: What is the impact of UMO's counterfactual optimization convergence speed and stability across different generative model architectures and latent spaces?
- Basis in paper: [explicit] The paper mentions optimizing k distinct edit vectors for StyleGAN and DDPM but does not provide convergence analysis or compare optimization efficiency across different generative backbones.
- Why unresolved: While the paper shows visual results, it lacks quantitative metrics on optimization stability, number of iterations required for convergence, or comparison of convergence behavior between StyleGAN's StyleSpace and diffusion models' bottleneck latent space.
- What evidence would resolve it: Empirical studies measuring optimization convergence rates, stability across random initializations, and computational costs for different generative model architectures when applying UMO.

### Open Question 3
- Question: How does UMO's unsupervised attribute discovery compare to human expert analysis in identifying model biases and failure modes?
- Basis in paper: [inferred] The paper claims to eliminate human bias through unsupervised diagnosis but does not benchmark UMO's discovered attributes against domain expert analysis or compare discovery quality.
- Why unresolved: The authors validate through cross-method consistency and synthetic bias planting but do not evaluate whether UMO discovers the same biases that human experts would identify or whether it misses critical attributes that domain knowledge would catch.
- What evidence would resolve it: Comparative studies where UMO's discovered attributes are evaluated by domain experts against their own analysis of the same models, measuring overlap, false positives/negatives, and practical utility for model improvement.

## Limitations
- Reliance on foundation models' parametric knowledge may miss domain-specific attributes in specialized applications
- Effectiveness depends on the disentanglement quality of generative model latent spaces, which varies across architectures
- Ensemble approach assumes diversity between generative models, but StyleGAN and diffusion may share similar failure modes

## Confidence
- **High confidence**: The counterfactual optimization mechanism and its basic implementation are well-specified and theoretically sound
- **Medium confidence**: The attribute matching pipeline works as described, but effectiveness depends heavily on language model quality and CLIP's attribute discrimination ability
- **Medium confidence**: Cross-method consistency validation provides some reassurance, but the correlation between discovered attributes and actual model failures needs more rigorous testing

## Next Checks
1. **Attribute coverage validation**: Systematically test UMO's ability to discover known planted biases in synthetic datasets with controlled attribute distributions
2. **Cross-backbone diversity analysis**: Quantitatively measure the overlap between counterfactuals generated by StyleGAN vs. diffusion models to verify the ensemble provides meaningful diversity
3. **Failure mode sensitivity**: Evaluate how UMO's discovered attributes change when the target model is trained with and without known spurious correlations to verify the method detects relevant vulnerabilities