---
ver: rpa2
title: Hermes 3 Technical Report
arxiv_id: '2408.11857'
source_url: https://arxiv.org/abs/2408.11857
tags:
- hermes
- https
- language
- data
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Hermes 3 is a series of instruction-tuned and tool-use capable
  language models with sizes 8B, 70B, and 405B parameters. The models were trained
  using supervised fine-tuning and direct preference optimization on a diverse dataset
  of 390 million tokens covering domains like coding, mathematics, roleplaying, and
  reasoning tasks.
---

# Hermes 3 Technical Report

## Quick Facts
- arXiv ID: 2408.11857
- Source URL: https://arxiv.org/abs/2408.11857
- Reference count: 40
- Models: 8B, 70B, and 405B parameter instruction-tuned language models with tool-use capabilities

## Executive Summary
Hermes 3 is a series of instruction-tuned language models trained on 390 million tokens across diverse domains including coding, mathematics, and reasoning tasks. The models employ a two-phase training approach with supervised fine-tuning followed by direct preference optimization, achieving state-of-the-art performance among open-weight models on benchmarks like AGIEval, BBH, MATH, and MMLU-PRO. A distinctive design choice is the neutral alignment that responds to instructions without moral refusals, leaving safety considerations to the system level rather than the model itself.

## Method Summary
The training methodology combines supervised fine-tuning (SFT) on a diverse dataset of 390 million tokens with direct preference optimization (DPO) using LoRA adapters for the 405B model. The process employs sample packing with Flash Attention 2 for efficiency and uses AdamW optimizer with cosine decay. Training runs for 4 epochs with specific learning rates per model size (7×10^-6 for 8B/70B, 3.5×10^-6 for 405B), utilizing batch sizes of 48 for smaller models and 128 for the 405B variant.

## Key Results
- Hermes 3 405B achieves state-of-the-art performance among open-weight models on multiple benchmarks
- Strong performance across diverse tasks including coding, mathematics, and reasoning
- Advanced capabilities in agentic tasks through features like scratchpads and structured outputs
- Effective instruction-following without moral refusals, maintaining neutral alignment

## Why This Works (Mechanism)

### Mechanism 1: Sample Packing with Variable Sequence Length
- Claim: Sample packing with Flash Attention 2's variable sequence length improves training efficiency without cross-contamination.
- Mechanism: Multiple samples of different lengths are packed into a single sequence, with attention masks ensuring no cross-attention between samples.
- Core assumption: The attention mechanism can correctly handle variable-length sequences when proper masking is applied.
- Evidence anchors:
  - [section]: "Multiple samples are packed together [13] into a single sequence, utilizing the attention mask-free variable sequence length ability of Flash Attention 2 [4] to avoid cross-attention contamination of samples"
  - [corpus]: Weak - no direct corpus evidence found for this specific mechanism
- Break condition: If masking is incorrectly implemented, cross-contamination could occur, degrading model performance.

### Mechanism 2: LoRA Adapters for Large Models
- Claim: Using LoRA adapters instead of full model fine-tuning reduces memory requirements for large models like 405B.
- Mechanism: Low-rank adaptations are trained instead of full model weights, significantly reducing the number of parameters that need to be updated.
- Core assumption: LoRA can effectively capture the necessary fine-tuning adjustments without full parameter updates.
- Evidence anchors:
  - [section]: "When applying DPO rather than tuning a full model we train a LoRA [9] adapter which side-steps the need to hold both a reference and trained model in GPU memory"
  - [corpus]: Weak - no direct corpus evidence found for this specific mechanism
- Break condition: If the rank r is too low, the adapter may not capture sufficient fine-tuning information, limiting performance gains.

### Mechanism 3: Neutral Alignment Without Moral Refusals
- Claim: Training the model to be neutrally aligned (without moral refusals) allows for broader instruction-following capability.
- Mechanism: The model is trained to follow system prompts and instructions exactly without incorporating moral judgment, leaving guardrails to the system level.
- Core assumption: Separating instruction-following from moral judgment improves the model's ability to complete a wider range of tasks.
- Evidence anchors:
  - [abstract]: "A key design choice is the model's neutral alignment, responding to instructions without moral refusals, leaving guardrails to the system level rather than the model itself"
  - [section]: "Crucially, our training data strongly encourages the model to follow the system and instruction prompts exactly and neutrally"
- Break condition: If the system-level guardrails are insufficient, this approach could lead to problematic outputs in certain contexts.

## Foundational Learning

- Concept: Supervised Fine-Tuning (SFT)
  - Why needed here: SFT is the foundation for transforming a base model into an instruction-following model
  - Quick check question: What is the primary difference between SFT and base model training in terms of loss calculation?

- Concept: Direct Preference Optimization (DPO)
  - Why needed here: DPO refines the model's preferences based on human feedback without requiring a separate reward model
  - Quick check question: How does DPO differ from traditional reinforcement learning approaches in terms of reward modeling?

- Concept: Sample Packing
  - Why needed here: Efficient utilization of GPU memory and computational resources during training
  - Quick check question: What is the main advantage of sample packing over traditional batching approaches?

## Architecture Onboarding

- Component map: Base model (Llama 3.1 variants) -> SFT training -> (optional) DPO fine-tuning -> LoRA adapters -> Flash Attention 2 -> AdamW optimizer
- Critical path: Data preparation → SFT training → (optional) DPO fine-tuning → Evaluation
- Design tradeoffs:
  - Memory efficiency vs. training speed (sample packing)
  - Model size vs. inference performance (LoRA vs. full fine-tuning)
  - Instruction-following capability vs. safety considerations (neutral alignment)
- Failure signatures:
  - Poor performance on benchmarks indicates issues with training data quality or hyperparameters
  - Memory errors during training suggest inadequate resource allocation or inefficient packing
  - Inconsistent instruction-following points to problems with SFT phase or system prompt handling
- First 3 experiments:
  1. Test sample packing with a small subset of data to verify cross-contamination is prevented
  2. Compare LoRA fine-tuning vs. full fine-tuning on a smaller model to validate efficiency gains
  3. Evaluate model performance with and without DPO to assess its impact on instruction-following quality

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the neutral alignment of Hermes 3 affect its performance in real-world applications compared to commercially aligned models?
- Basis in paper: [explicit] The paper states that Hermes 3 is "neutrally-aligned" and distinguishes it from "popular closed weight commercial models, which may refuse instructions on moral grounds."
- Why unresolved: The paper mentions the neutral alignment as a design choice but does not provide empirical data comparing Hermes 3's performance to aligned models in real-world scenarios or user studies.
- What evidence would resolve it: Comparative studies of Hermes 3 and aligned models in real-world applications, user satisfaction surveys, and analysis of ethical implications in various use cases.

### Open Question 2
- Question: What is the long-term impact of training large language models like Hermes 3 on diverse and potentially controversial topics without moral guardrails?
- Basis in paper: [inferred] The paper discusses the model's ability to respond to "any degree of agency to the outside world" and states "For Hermes, there is no such thing as latent thoughtcrime."
- Why unresolved: The paper does not address potential risks or ethical considerations of training models without moral guardrails, nor does it discuss the long-term societal implications.
- What evidence would resolve it: Longitudinal studies on the use of such models, analysis of potential misuse cases, and ethical impact assessments.

### Open Question 3
- Question: How does the performance of Hermes 3 405B compare to other state-of-the-art models when evaluated on more specialized or niche tasks not covered in the standard benchmarks?
- Basis in paper: [explicit] The paper states that Hermes 3 405B "achieves state of the art performance among open weight models on several public benchmarks."
- Why unresolved: The paper focuses on standard benchmarks but does not explore the model's performance on more specialized tasks or in domain-specific applications.
- What evidence would resolve it: Evaluations on domain-specific datasets, specialized tasks, or industry-specific benchmarks not mentioned in the paper.

## Limitations

- Dataset composition and generation process not fully disclosed
- Lack of detailed ablation studies comparing SFT vs SFT+DPO contributions
- Performance claims lack statistical analysis with confidence intervals

## Confidence

**High Confidence Claims:**
- The training methodology using SFT followed by DPO is technically sound
- Sample packing with Flash Attention 2 is an effective technique for improving training efficiency
- The use of LoRA adapters for the 405B model successfully reduces memory requirements

**Medium Confidence Claims:**
- The models achieve state-of-the-art performance among open-weight models
- The neutral alignment approach effectively enables broader instruction-following capability
- The training process successfully produces models with the claimed capabilities across different sizes

**Low Confidence Claims:**
- The specific dataset composition and generation process
- The relative contribution of each training component to final performance
- The generalizability of results across different application domains

## Next Checks

1. Conduct controlled experiments comparing Hermes 3 models trained with only SFT versus those with SFT + DPO to quantify the specific contribution of each training phase to final performance metrics.

2. Run multiple training instances with different random seeds and report mean and standard deviation for all benchmark scores to provide proper statistical confidence intervals for performance claims.

3. Perform detailed analysis of the training dataset composition, including category distribution, quality metrics, and potential biases, to better understand the relationship between training data and model capabilities.