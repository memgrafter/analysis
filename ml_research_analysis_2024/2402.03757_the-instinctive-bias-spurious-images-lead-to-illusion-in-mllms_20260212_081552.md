---
ver: rpa2
title: 'The Instinctive Bias: Spurious Images lead to Illusion in MLLMs'
arxiv_id: '2402.03757'
source_url: https://arxiv.org/abs/2402.03757
tags:
- mllms
- images
- spurious
- image
- answer
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper identifies a widespread instinctive bias in multi-modal
  large language models (MLLMs), where spurious visual inputs cause MLLMs to suffer
  from visual illusion. To quantify this effect, the authors propose CorrelationQA,
  a benchmark containing 7,308 text-image pairs across 13 categories, where each pair
  includes multiple answer-related images that may mislead MLLMs.
---

# The Instinctive Bias: Spurious Images lead to Illusion in MLLMs

## Quick Facts
- arXiv ID: 2402.03757
- Source URL: https://arxiv.org/abs/2402.03757
- Reference count: 7
- Primary result: Spurious visual inputs cause MLLMs to suffer from visual illusion, with models universally showing instinctive bias across 9 mainstream models

## Executive Summary
This paper identifies a widespread instinctive bias in multi-modal large language models (MLLMs), where spurious visual inputs cause MLLMs to suffer from visual illusion. To quantify this effect, the authors propose CorrelationQA, a benchmark containing 7,308 text-image pairs across 13 categories, where each pair includes multiple answer-related images that may mislead MLLMs. Through comprehensive analysis of 9 mainstream MLLMs, including GPT-4V, the study demonstrates that these models universally suffer from this instinctive bias to varying degrees, particularly when presented with spurious images. The findings suggest that spurious information induces visual illusion in MLLMs, leading to responses that are predominantly based on visual information without proper reasoning.

## Method Summary
The authors constructed CorrelationQA, a benchmark containing 7,308 text-image pairs across 13 categories, designed to test whether spurious visual information causes MLLMs to exhibit visual illusion. Each text-image pair includes multiple answer-related images that may mislead models. The benchmark was evaluated across 9 mainstream MLLMs including GPT-4V to measure the extent of instinctive bias. The methodology focused on quantifying how often models' responses were influenced by spurious visual cues rather than proper reasoning, with particular attention to performance differences when spurious images were present versus absent.

## Key Results
- MLLMs universally suffer from instinctive bias when presented with spurious images, with varying degrees of susceptibility across different models
- GPT-4V and other mainstream MLLMs show significantly higher error rates when answer-related spurious images are included in the input
- The instinctive bias demonstrates that MLLMs tend to rely predominantly on visual information without proper reasoning when spurious images are present

## Why This Works (Mechanism)
Assumption: The mechanism likely involves over-reliance on visual features during cross-modal attention, where spurious correlations in training data cause models to incorrectly weight visual information even when it's misleading. Unknown: Whether this represents a fundamental limitation in how MLLMs process visual information or a training artifact that could be corrected through architectural modifications.

## Foundational Learning
1. **Multi-modal Integration**: Why needed - Understanding how MLLMs combine visual and textual information; Quick check - Test models with isolated visual vs textual inputs
2. **Visual Spurious Correlation**: Why needed - Identifying when visual features incorrectly correlate with correct answers; Quick check - Measure performance drop when removing spurious visual cues
3. **Attention Mechanisms**: Why needed - Understanding where models focus during multi-modal processing; Quick check - Visualize attention maps during inference
4. **Cross-modal Alignment**: Why needed - Assessing how well visual and textual representations align in the model; Quick check - Compare embedding similarities across modalities

## Architecture Onboarding
**Component Map**: Text Encoder -> Vision Encoder -> Cross-modal Fusion -> Language Decoder -> Output Generator
**Critical Path**: Input Processing -> Feature Extraction -> Cross-modal Attention -> Reasoning Layer -> Response Generation
**Design Tradeoffs**: The study highlights the tradeoff between strong visual grounding (which can lead to illusion) versus pure language reasoning (which may miss legitimate visual cues)
**Failure Signatures**: High reliance on visual features when spurious correlations exist, failure to override visual information with textual reasoning
**3 First Experiments**:
1. Test models with only visual input versus only textual input to establish baseline modality-specific performance
2. Compare attention distributions between genuine and spurious image conditions
3. Evaluate model calibration scores to determine if confidence is inappropriately tied to visual presence

## Open Questions the Paper Calls Out
Unknown: The paper does not explicitly identify open questions, but potential areas include whether this bias can be mitigated through architectural changes, how it affects real-world deployment scenarios, and whether different training paradigms could reduce susceptibility to spurious visual information.

## Limitations
- Benchmark construction relies on human-curated pairs, potentially introducing selection bias
- Definition of visual illusion remains ambiguous and may conflate expected multimodal behavior with actual illusion
- Evaluation framework oversimplifies the complex relationship between visual inputs and model outputs through binary correctness classification
- Limited exploration of whether the bias stems from vision encoder architecture versus language model reasoning capabilities

## Confidence
**High Confidence**: The finding that MLLMs across the board are affected by visual information in their responses is well-supported by the experimental results.
**Medium Confidence**: The claim that spurious images specifically induce "visual illusion" is less definitively established, requiring additional validation beyond correlation-based benchmarks.
**Low Confidence**: The assertion that this bias represents a fundamental limitation in MLLMs' reasoning capabilities rather than an expected feature of multimodal systems needs further investigation.

## Next Checks
1. **Ablation Studies on Visual Components**: Systematically test whether models with different visual processing architectures (CNN-based vs transformer-based vision encoders) show differential susceptibility to spurious visual cues, helping isolate whether the bias stems from vision or language components.

2. **Temporal Analysis of Decision-Making**: Implement attention visualization and feature importance tracking to determine whether models initially process visual information differently when presented with spurious versus genuine images, revealing whether the bias occurs at early or late stages of inference.

3. **Cross-Domain Generalization Tests**: Evaluate model performance on out-of-distribution visual inputs (e.g., artistic renderings, abstract representations) to determine whether the instinctive bias generalizes beyond the specific categories in CorrelationQA, establishing the breadth of this phenomenon across real-world scenarios.