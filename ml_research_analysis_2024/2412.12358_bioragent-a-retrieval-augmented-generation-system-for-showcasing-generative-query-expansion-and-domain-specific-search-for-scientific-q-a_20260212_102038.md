---
ver: rpa2
title: 'BioRAGent: A Retrieval-Augmented Generation System for Showcasing Generative
  Query Expansion and Domain-Specific Search for Scientific Q&A'
arxiv_id: '2412.12358'
source_url: https://arxiv.org/abs/2412.12358
tags:
- system
- query
- search
- https
- bioragent
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: BioRAGent is a web-based retrieval-augmented generation system
  for biomedical question answering that uses large language models for query expansion,
  snippet extraction, and answer generation while maintaining transparency through
  citation links and editable queries. The system employs few-shot learning with LLMs
  to generate expanded queries incorporating relevant synonyms, retrieves top 50 PubMed
  articles using Elasticsearch with BM25 scoring, and generates both short paragraph
  answers and citation-backed responses.
---

# BioRAGent: A Retrieval-Augmented Generation System for Showcasing Generative Query Expansion and Domain-Specific Search for Scientific Q&A

## Quick Facts
- arXiv ID: 2412.12358
- Source URL: https://arxiv.org/abs/2412.12358
- Authors: Samy Ateia; Udo Kruschwitz
- Reference count: 9
- Primary result: Web-based biomedical QA system using LLMs for query expansion and transparent citation-backed answers, achieving competitive results in BioASQ 2024

## Executive Summary
BioRAGent is a web-based retrieval-augmented generation system designed for biomedical question answering that combines large language models with transparent search processes. The system uses few-shot learning with LLMs to expand biomedical queries with relevant synonyms, retrieves PubMed articles using Elasticsearch with BM25 scoring, and generates both short paragraph answers and citation-backed responses. It emphasizes transparency by displaying expanded queries for user editing and providing inline citations with PubMed IDs for every generated sentence. Evaluated through participation in the BioASQ 2024 challenge, BioRAGent achieved competitive results, winning multiple first and second places in question answering tasks, though it was less competitive in document retrieval and snippet extraction compared to systems using dense and hybrid retrieval techniques.

## Method Summary
BioRAGent employs a multi-stage pipeline where biomedical questions are first expanded using few-shot learning with LLMs to incorporate relevant synonyms and domain-specific terminology. The expanded queries are then executed against an Elasticsearch index of PubMed abstracts and titles using BM25 scoring to retrieve the top 50 most relevant articles. Retrieved documents are processed in parallel to extract relevant snippets using LLM prompts, which are then reranked by relevance to the original question. Finally, the system generates two types of answers: a short paragraph response and a citation-backed response where each sentence is linked to its source document via PubMed ID. The entire process is wrapped in a Gradio web interface that displays generated queries for user editing and maintains transparency through visible source attributions.

## Key Results
- Achieved competitive performance in BioASQ 2024 challenge, winning multiple first and second places in question answering tasks
- Demonstrated effective use of few-shot learning for biomedical query expansion with domain-specific terminology
- Showed transparency benefits through user-editable expanded queries and citation-backed responses, though less competitive in document retrieval compared to dense retrieval systems

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Few-shot learning with LLMs effectively generates query expansions that incorporate relevant synonyms and related terms for biomedical queries.
- Mechanism: The system uses 3-shot learning examples to prompt LLMs to transform simple biomedical questions into expanded queries containing domain-specific terminology and synonyms that better match scientific literature.
- Core assumption: LLMs can learn to generate appropriate query expansions from few examples when provided with the right prompt format and examples.
- Evidence anchors:
  - [abstract] "The system uses large language models (LLMs) for query expansion, snippet extraction, and answer generation"
  - [section 2.1] "Given a biomedical question, the system uses few-shot (3-shot) learning with LLMs to generate an expanded query that incorporates relevant synonyms and related terms"
  - [corpus] Weak - no direct evidence in corpus about few-shot learning effectiveness for query expansion

### Mechanism 2
- Claim: Using Elasticsearch with BM25 scoring and a PubMed article index provides effective document retrieval for biomedical questions.
- Mechanism: The expanded queries are executed against an Elasticsearch index of PubMed abstracts and titles using the default English analyzer and BM25 scoring to retrieve the top 50 most relevant articles.
- Core assumption: Standard BM25 scoring with appropriate analyzers performs adequately for biomedical literature retrieval without requiring specialized biomedical embeddings.
- Evidence anchors:
  - [section 2.2] "The expanded query is executed against an index of abstracts and titles of PubMed articles using the default English analyzer of Elasticsearch. The top 50 articles ranked by the default BM25 based scoring of Elasticsearch are retrieved"
  - [section 3] "in the document retrieval and snippets extraction tasks other systems that also used dense and hybrid retrieval techniques took the leading spots" - implies BM25 alone was less competitive
  - [corpus] No direct evidence about Elasticsearch/BM25 performance in biomedical domains

### Mechanism 3
- Claim: Transparent query expansion and citation processes make semantic knowledge visible and controllable by users, reducing LLM hallucination in professional settings.
- Mechanism: The system displays generated expanded queries for editing and provides inline citations with PubMed IDs for every generated sentence, allowing users to verify and control the information sources.
- Core assumption: Making the query expansion process transparent and providing source attribution significantly reduces hallucination risks and increases user trust in professional applications.
- Evidence anchors:
  - [abstract] "while maintaining transparency through citation links to the source documents and displaying generated queries for further editing"
  - [section 4] "the resulting retrieval rankings based on embedding vectors are not transparent or easily controllable by a search expert. Our approach, on the other hand, makes the encoded semantic knowledge of the LLM used for searching, visible and controllable by displaying the expanded query"
  - [corpus] No direct evidence about transparency reducing hallucination in this specific system

## Foundational Learning

- Concept: Few-shot learning with LLMs
  - Why needed here: The system relies on prompting LLMs with 3 examples to generate query expansions and extract snippets, requiring understanding of how to structure few-shot prompts for domain-specific tasks
  - Quick check question: How would you structure a 3-shot prompt to teach an LLM to expand "What causes diabetes?" into a query with relevant synonyms and related terms?

- Concept: BM25 scoring and information retrieval fundamentals
  - Why needed here: The system uses Elasticsearch with BM25 scoring for document retrieval, requiring understanding of how term frequency and inverse document frequency weighting works
  - Quick check question: What factors does BM25 consider when ranking documents, and how might these affect retrieval of biomedical literature?

- Concept: PubMed and biomedical literature structure
  - Why needed here: The system indexes PubMed articles and generates PubMed ID citations, requiring familiarity with biomedical literature databases and their metadata
  - Quick check question: What information is typically available in PubMed abstracts, and how might this structure affect query expansion strategies?

## Architecture Onboarding

- Component map: Gradio web interface → Query expansion module → Elasticsearch retrieval → Snippet extraction module → Answer generation module → Response display
- Critical path:
  1. User submits question through web interface
  2. Query expansion module generates expanded query using few-shot prompts
  3. Expanded query executed in Elasticsearch to retrieve top 50 PubMed articles
  4. Snippet extraction module processes each document in parallel using LLM prompts
  5. Reranking module orders snippets by relevance to original question
  6. Answer generation module creates responses with and without citations
  7. Results displayed in web interface with editing capabilities

- Design tradeoffs:
  - Speed vs. accuracy: Using Gemini 1.5 Flash for speed but potentially sacrificing some accuracy compared to larger models
  - Transparency vs. performance: Using interpretable BM25 instead of more effective but opaque dense retrieval
  - Few-shot vs. fine-tuning: Choosing in-context learning for flexibility but potentially lower performance than fine-tuned models

- Failure signatures:
  - Poor query expansions → Low-quality retrieval → Irrelevant snippets → Incorrect answers
  - Elasticsearch connection issues → No documents retrieved → Empty responses
  - LLM API failures → System unable to generate queries, snippets, or answers
  - Gradio interface errors → Users cannot interact with the system

- First 3 experiments:
  1. Test query expansion with simple biomedical questions to verify few-shot prompts generate appropriate expansions with synonyms
  2. Verify Elasticsearch indexing of PubMed abstracts works correctly and BM25 scoring retrieves relevant documents
  3. Test end-to-end workflow with a basic question to ensure all components connect properly and generate reasonable responses

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does BioRAGent's transparent query expansion approach compare to dense vector search methods in terms of both performance and user satisfaction for biomedical professionals?
- Basis in paper: [explicit] The paper states "While dense vector search is an intriguing technique due to the demonstrated state-of-the-art (SOTA) performance in multiple benchmarks, the resulting retrieval rankings based on embedding vectors are not transparent or easily controllable by a search expert. Our approach, on the other hand, makes the encoded semantic knowledge of the LLM used for searching, visible and controllable by displaying the expanded query."
- Why unresolved: The paper acknowledges the performance advantage of dense retrieval but doesn't provide direct comparative evaluation between transparent query expansion and dense vector search methods in terms of user satisfaction or actual performance metrics.
- What evidence would resolve it: A controlled user study comparing BioRAGent's query expansion approach with dense retrieval methods, measuring both system performance metrics and user satisfaction scores from biomedical professionals.

### Open Question 2
- Question: What is the impact of allowing users to edit expanded queries on the quality and relevance of retrieved documents and generated answers?
- Basis in paper: [explicit] The paper mentions that "Users can inspect and modify the expanded query after execution, making the search process transparent and controllable" but doesn't evaluate this feature's impact.
- Why unresolved: While the feature is described as a key differentiator, there's no empirical evidence showing how user modifications affect retrieval quality or answer accuracy.
- What evidence would resolve it: An analysis comparing answer quality and retrieval performance with and without user query modifications, including metrics like precision, recall, and user satisfaction scores.

### Open Question 3
- Question: How does the performance of few-shot learning with LLMs for query expansion in BioRAGent scale across different biomedical subdomains and query complexities?
- Basis in paper: [explicit] The paper describes using "few-shot (3-shot) learning with LLMs to generate an expanded query that incorporates relevant synonyms and related terms" but doesn't analyze performance variations across different types of biomedical questions.
- Why unresolved: The paper demonstrates competitive results overall but doesn't break down performance by query type, complexity, or biomedical subdomain.
- What evidence would resolve it: A detailed analysis of BioRAGent's performance across different biomedical subdomains (e.g., genetics, pharmacology, clinical medicine) and query complexity levels, with corresponding accuracy metrics.

## Limitations

- The system relies on standard BM25 scoring rather than more effective dense or hybrid retrieval methods, which may limit retrieval quality for complex biomedical queries requiring semantic understanding
- The few-shot learning approach for query expansion is not validated against alternative methods like domain-specific fine-tuning or query reformulation techniques
- The transparency and hallucination reduction claims are not empirically tested, relying instead on theoretical arguments about user control and citation verification

## Confidence

- **High Confidence**: The system architecture and implementation details are clearly described and reproducible. The competitive results in BioASQ 2024 are verifiable through the challenge leaderboard.
- **Medium Confidence**: The claim that few-shot learning with LLMs effectively generates query expansions is supported by the mechanism description but lacks direct performance validation. The system's less competitive performance in document retrieval suggests potential limitations.
- **Low Confidence**: The claim that transparency features significantly reduce hallucination is largely theoretical, with no direct experimental evidence provided in the paper. The assumption that users will effectively utilize these features is untested.

## Next Checks

1. Conduct a controlled experiment comparing few-shot query expansion against baseline approaches (no expansion, keyword expansion) using standard IR metrics like NDCG to quantify the actual performance impact.
2. Implement A/B testing of the system with and without transparency features to measure whether citation visibility and query editing capabilities actually reduce user-reported hallucination incidents or increase trust scores.
3. Test the system's robustness by evaluating its performance across different biomedical subdomains (genetics, pharmacology, clinical medicine) to identify whether the few-shot approach generalizes effectively or requires domain-specific adaptation.