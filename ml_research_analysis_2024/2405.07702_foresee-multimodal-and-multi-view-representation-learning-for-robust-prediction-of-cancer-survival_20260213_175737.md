---
ver: rpa2
title: 'FORESEE: Multimodal and Multi-view Representation Learning for Robust Prediction
  of Cancer Survival'
arxiv_id: '2405.07702'
source_url: https://arxiv.org/abs/2405.07702
tags:
- data
- features
- multimodal
- feature
- survival
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: FORESEE is a multimodal deep learning framework for cancer survival
  prediction that addresses intra-modality missing data and feature heterogeneity
  across pathology images, molecular data, and EHRs. It uses a cross-fusion transformer
  to extract multi-scale features from whole slide images, a hybrid attention encoder
  to denoise and capture contextual information from molecular data, and an asymmetrically
  masked triplet masked autoencoder to reconstruct missing intra-modality features.
---

# FORESEE: Multimodal and Multi-view Representation Learning for Robust Prediction of Cancer Survival

## Quick Facts
- arXiv ID: 2405.07702
- Source URL: https://arxiv.org/abs/2405.07702
- Reference count: 16
- Primary result: State-of-the-art C-index scores of 0.686±0.008 to 0.730±0.002 on four TCGA cancer datasets with significant KM analysis results (P<0.05)

## Executive Summary
FORESEE is a multimodal deep learning framework designed to address the challenges of cancer survival prediction, specifically targeting intra-modality missing data and feature heterogeneity across pathology images, molecular data, and electronic health records. The framework employs a cross-fusion transformer to extract multi-scale features from whole slide images, a hybrid attention encoder to denoise and capture contextual information from molecular data, and an asymmetrically masked triplet masked autoencoder to reconstruct missing intra-modality features. Evaluated on four TCGA cancer datasets (BLCA, BRCA, LUAD, UCEC), FORESEE demonstrates state-of-the-art performance with C-index scores ranging from 0.686±0.008 to 0.730±0.002 and significant Kaplan-Meier analysis results (P<0.05), showcasing robust performance in both complete and missing data settings.

## Method Summary
FORESEE addresses cancer survival prediction through a three-component architecture: a cross-fusion transformer (CFT) for multi-scale pathology image feature extraction, a hybrid attention encoder (HAE) for molecular data processing, and an asymmetrically masked triplet masked autoencoder (TriMAE) for handling missing intra-modality data. The CFT segments whole slide images into patches at three magnification levels and applies graph neural networks within each scale before fusing features across scales. HAE combines discrete wavelet transform denoising, contextual attention, and channel attention to extract molecular features. TriMAE employs asymmetric masking at >80% rate across parallel branches to reconstruct missing features. The model is trained end-to-end with Cox loss using Adam optimizer (learning rate 5×10⁻³, weight decay 1×10⁻⁵, batch size 50) for 50 epochs with 5-fold cross-validation on four TCGA cancer datasets.

## Key Results
- Achieves state-of-the-art C-index scores of 0.686±0.008 to 0.730±0.002 on four TCGA cancer datasets
- Demonstrates significant Kaplan-Meier analysis results with P<0.05
- Shows robust performance in both complete and missing data settings
- Ablation studies confirm the effectiveness of multi-view feature extraction, TriMAE, and HAE components

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The cross-fusion transformer (CFT) enhances representation by capturing multi-scale semantic relationships across different magnification levels of pathology images.
- Mechanism: CFT first segments whole slide images (WSI) into patches at three magnification levels (small: 256×256, medium: 512×512, large: 1024×1024). Graph neural networks (GNNs) are applied within each scale to learn intra-scale contextual relationships. Two cross-fusion strategies then blend features across scales—first fusing small with medium and medium with large, then fusing all three scales—allowing the model to integrate cellular, tissue, and tumor heterogeneity features.
- Core assumption: Features from different magnification scales contain complementary prognostic information that can be effectively fused through cross-attention mechanisms.
- Evidence anchors:
  - [abstract] "cross-fusion transformer effectively utilizes features at the cellular level, tissue level, and tumor heterogeneity level to correlate prognosis through a cross-scale feature cross-fusion method"
  - [section 3.3] "cross-fusion transformer (CFT) learns contextual relationships across patches through a cross-scale feature cross-fusion method"
  - [corpus] Weak—corpus neighbors focus on missing modality handling but do not directly support multi-scale fusion claims.
- Break condition: If the cross-fusion layers collapse distinct scale information into a single dominant scale, or if the attention mechanism fails to learn meaningful cross-scale dependencies.

### Mechanism 2
- Claim: The hybrid attention encoder (HAE) improves molecular feature extraction by combining denoising, contextual attention, and channel attention modules.
- Mechanism: HAE first applies discrete wavelet transform (DWT) with Daubechies wavelets to denoise molecular data. A contextual attention module (CTA) captures long-range dependencies and local details using LSTM and self-attention. A channel attention module (CNA) introduces global feature weighting to emphasize important channels. The outputs are combined and further refined with multi-head self-attention.
- Core assumption: Molecular data contains both structured global patterns and local details that require distinct attention mechanisms for effective extraction.
- Evidence anchors:
  - [abstract] "hybrid attention encoder (HAE) uses the denoising contextual attention module to obtain the contextual relationship features and local detail features of the molecular data"
  - [section 3.4] "The novel hybrid attention encoder (HAE) uses a new contextual attention (CTA) module to capture the contextual features of molecular data and extract local feature details"
  - [corpus] Weak—corpus neighbors discuss imputation and robustness but do not validate the specific attention architecture.
- Break condition: If denoising removes too much signal, or if the attention modules overfit to training data without generalizing.

### Mechanism 3
- Claim: The asymmetrically masked triplet masked autoencoder (TriMAE) reconstructs missing intra-modality information by leveraging unmasked features and masked tokens across parallel branches.
- Mechanism: TriMAE masks one modality at a time at >80% rate, forcing the encoder to learn meaningful representations from the visible parts. The lightweight decoder reconstructs missing features using cross-attention and self-attention across the three modality branches, improving robustness to missing data.
- Core assumption: High masking rates and asymmetric masking force the model to learn generalizable latent representations rather than memorizing unmasked regions.
- Evidence anchors:
  - [abstract] "asymmetrically masked triplet masked autoencoder to reconstruct lost information within modalities"
  - [section 3.5] "To ensure that the model is not adversely affected by missing features during inference, we designed a TriMAE to reconstruct the missing feature information in multimodal data"
  - [corpus] Weak—corpus neighbors discuss imputation strategies but not the specific triplet MAE architecture.
- Break condition: If masking is too aggressive, reconstruction fails; if too mild, the model does not learn robustness to missing data.

## Foundational Learning

- Concept: Graph Neural Networks for spatial topological feature extraction
  - Why needed here: WSI patches need to be modeled as spatially connected graphs to preserve local relationships before cross-fusion.
  - Quick check question: What does the adjacency matrix represent in the graph construction of WSI patches?

- Concept: Cross-attention mechanisms for multimodal fusion
  - Why needed here: Different modalities (images, molecular data) have heterogeneous feature spaces; cross-attention enables adaptive alignment.
  - Quick check question: How does the cross-attention layer in TriMAE differ from standard self-attention?

- Concept: Masked autoencoder training for robustness
  - Why needed here: Medical datasets often have missing intra-modality data; MAE-style reconstruction teaches the model to infer missing parts.
  - Quick check question: Why is an 80%+ masking rate used instead of a lower rate in TriMAE?

## Architecture Onboarding

- Component map: CFT (graph + cross-fusion transformers) → HAE (denoising + attention encoders) → TriMAE (parallel MAE branches) → modality fusion → survival prediction head
- Critical path: Patch extraction → graph construction → CFT cross-fusion → HAE molecular encoding → TriMAE reconstruction → final fusion → Cox loss
- Design tradeoffs: High masking rate improves robustness but risks reconstruction failure; cross-fusion adds complexity but improves scale integration; wavelet denoising may remove subtle signals.
- Failure signatures: Low C-index indicates poor feature fusion; high variance across folds suggests instability; reconstruction loss spikes indicate masking issues.
- First 3 experiments:
  1. Train CFT alone on pathology images at single scale; measure C-index vs multi-scale fusion.
  2. Test HAE denoising impact by comparing with and without DWT preprocessing on noisy molecular data.
  3. Evaluate TriMAE robustness by systematically removing modalities during inference and measuring degradation.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of FORESEE vary when applied to cancer types with significantly different molecular profiles, such as hematological malignancies versus solid tumors?
- Basis in paper: [inferred] The paper evaluates FORESEE on four solid tumor types (BLCA, BRCA, LUAD, UCEC) but does not address performance on hematological malignancies or other cancer types with distinct molecular characteristics.
- Why unresolved: The study focuses exclusively on TCGA solid tumor datasets, limiting generalizability to cancer types with different molecular and pathological features.
- What evidence would resolve it: Comparative performance analysis of FORESEE on both solid and hematological cancer datasets with diverse molecular profiles.

### Open Question 2
- Question: What is the impact of using different patch sizes and magnifications in the cross-fusion transformer when applied to tissues with varying cellular densities and architectural complexity?
- Basis in paper: [explicit] The paper mentions using patches of 256×256, 512×512, and 1024×1024 for different magnifications but does not explore how performance changes with alternative patch sizes or different tissue types.
- Why unresolved: The optimal patch size and magnification strategy for different tissue architectures is not systematically investigated, leaving uncertainty about generalizability.
- What evidence would resolve it: Systematic ablation studies varying patch sizes and magnifications across tissue types with different cellular densities.

### Open Question 3
- Question: How does the asymmetric masking strategy in TriMAE perform compared to uniform masking when the missing data patterns follow specific clinical protocols (e.g., missing certain molecular tests)?
- Basis in paper: [explicit] The paper introduces an asymmetric masking strategy in TriMAE but does not compare it to uniform masking strategies that might better reflect clinical missing data patterns.
- Why unresolved: The paper simulates random missing data but does not investigate how the masking strategy performs under clinically realistic missing data patterns.
- What evidence would resolve it: Comparative studies of asymmetric versus uniform masking under different clinically-inspired missing data scenarios.

## Limitations

- The specific implementation details of the cross-fusion transformer and asymmetrically masked triplet masked autoencoder remain underspecified, limiting faithful reproduction.
- The effectiveness of the 80%+ masking rate in TriMAE lacks empirical validation across different masking thresholds.
- The robustness claims are based on only four TCGA cancer datasets, limiting generalizability to other cancer types or clinical settings.

## Confidence

- **High confidence**: The survival prediction performance metrics (C-index scores between 0.686 and 0.730) are well-documented and statistically significant (P<0.05 in KM analysis). The multi-view feature extraction approach using separate encoders for each modality is clearly specified.
- **Medium confidence**: The effectiveness of the hybrid attention encoder for molecular data is supported by the overall model performance but lacks ablation studies isolating its specific contribution. The cross-fusion transformer's ability to capture meaningful multi-scale relationships is theoretically sound but not empirically validated in isolation.
- **Low confidence**: The asymmetrically masked triplet masked autoencoder's reconstruction capabilities are claimed but not directly measured. The specific implementation details that would enable faithful reproduction remain unclear.

## Next Checks

1. **Cross-fusion transformer ablation**: Train CFT alone on pathology images at single scale versus multi-scale fusion to measure the exact contribution of cross-scale feature integration to overall performance.
2. **TriMAE masking threshold sensitivity**: Systematically vary the masking rate (60%, 70%, 80%, 90%) in the asymmetrically masked triplet masked autoencoder to determine the optimal balance between robustness and reconstruction accuracy.
3. **HAE denoising impact isolation**: Compare molecular data feature extraction with and without the discrete wavelet transform denoising step to quantify its specific contribution to survival prediction performance.