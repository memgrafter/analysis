---
ver: rpa2
title: Improving Probabilistic Diffusion Models With Optimal Diagonal Covariance Matching
arxiv_id: '2406.10808'
source_url: https://arxiv.org/abs/2406.10808
tags:
- covariance
- diffusion
- arxiv
- ocm-ddpm
- sampling
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a method to improve diffusion models by learning
  optimal diagonal covariance for the denoising distribution. The core idea is to
  directly regress the diagonal of the optimal state-dependent diagonal covariance
  using an unbiased objective called Optimal Covariance Matching (OCM), instead of
  relying on traditional data-driven approaches.
---

# Improving Probabilistic Diffusion Models With Optimal Diagonal Covariance Matching

## Quick Facts
- arXiv ID: 2406.10808
- Source URL: https://arxiv.org/abs/2406.10808
- Authors: Zijing Ou; Mingtian Zhang; Andi Zhang; Tim Z. Xiao; Yingzhen Li; David Barber
- Reference count: 40
- One-line primary result: Learning optimal diagonal covariance through unbiased OCM objective significantly improves diffusion model performance across multiple architectures and datasets

## Executive Summary
This paper addresses the challenge of improving diffusion models by learning optimal diagonal covariance for the denoising distribution. Traditional approaches either use fixed diagonal covariance or learn it through data-driven methods that can introduce significant estimation errors. The authors propose Optimal Covariance Matching (OCM), which directly regresses the optimal diagonal analytic covariance using an unbiased objective, reducing covariance approximation errors. The method is shown to be applicable to both Markovian (DDPM) and non-Markovian (DDIM) diffusion models, as well as latent diffusion models.

## Method Summary
The method involves training a diagonal Hessian prediction network using the Optimal Covariance Matching (OCM) objective, which directly regresses the optimal diagonal analytic covariance from the learned score function. After pretraining a base diffusion model to learn the score function, a separate diagonal Hessian prediction network is trained using the OCM objective with Rademacher random variables to form an unbiased estimate of the diagonal Hessian. During sampling, both the score and learned Hessian are used to compute the mean and covariance for generating samples. The approach requires minimal additional computational cost with negligible memory overhead and only slight increases in inference time.

## Key Results
- On CIFAR10 with cosine schedule, OCM-DDPM achieves FID of 14.32 with 10 timesteps vs 34.76 for DDPM with fixed covariance
- Improves NLL from 4.99 to 4.34 bits/dim on CIFAR10
- Demonstrates best FID and Recall trade-off in latent diffusion models
- Reduces covariance estimation errors compared to traditional data-driven approaches

## Why This Works (Mechanism)

### Mechanism 1
- Claim: OCM directly regresses optimal diagonal covariance using an unbiased estimator, reducing covariance approximation error
- Core assumption: Learned score function is sufficiently accurate for analytical covariance formula to work
- Evidence anchors: Abstract, section 3.1, weak corpus evidence
- Break condition: Poor score function leads to inaccurate analytical covariance results

### Mechanism 2
- Claim: Learning diagonal covariance from score function reduces error amplification vs indirect noise-based approaches
- Core assumption: Stable relationship between score function and covariance enables more accurate direct regression
- Evidence anchors: Section 4, section 3.1, weak corpus evidence
- Break condition: Systematic biases in score function correlate with Hessian diagonal in complex ways

### Mechanism 3
- Claim: Method applies consistently across DDPM, DDIM, and latent diffusion models
- Core assumption: Mathematical relationship between score and optimal covariance holds across diffusion model variants
- Evidence anchors: Abstract, section 3.2, weak corpus evidence
- Break condition: Different diffusion variants have fundamentally different optimal covariance structures

## Foundational Learning

- **Score matching and denoising score matching (DSM)**: Why needed - entire method relies on learning score function ∇xt log pθ(xt) to compute optimal covariance. Quick check - What is the relationship between learned noise prediction network in DDPM and score function used in this method?

- **Tweedie's formula and extensions**: Why needed - generalized analytical covariance identity derived using Tweedie's formula. Quick check - How does Tweedie's formula relate posterior mean to prior mean and score of observed distribution?

- **Diagonal Hessian approximation and Hutchinson's estimator**: Why needed - method uses Rademacher random variables for unbiased diagonal Hessian estimate. Quick check - Why do Rademacher random variables provide unbiased estimate of diagonal Hessian?

## Architecture Onboarding

- **Component map**: Base diffusion model (pretrained score network) -> Diagonal Hessian prediction network -> OCM training objective -> Integration layer (modifies sampling)

- **Critical path**: 1) Pretrain base diffusion model for score function, 2) Train diagonal Hessian prediction network using OCM objective, 3) During sampling use both score and learned Hessian, 4) Generate samples with modified algorithm

- **Design tradeoffs**: Memory - additional diagonal Hessian network adds negligible parameters; Speed - one extra network pass during sampling; Accuracy - better covariance estimation vs computational overhead; Flexibility - works with existing pretrained models

- **Failure signatures**: Poor sample quality despite good score function (Hessian prediction network not learning correctly); Unstable Hessian network training (learning rate too high or initialization poor); No improvement over baseline (score function too inaccurate or covariance clipping too aggressive)

- **First 3 experiments**: 1) Implement OCM on 2D toy problem with known true score and compare diagonal covariance estimation error, 2) Apply method to pretrained DDPM on CIFAR10 and compare FID and NLL, 3) Test method on DDIM variant to verify non-Markovian compatibility

## Open Questions the Paper Calls Out

- **Open Question 1**: How would incorporating full or block-diagonal covariance structures impact performance in high-dimensional tasks? The paper mentions potential of low-rank or block-diagonal alternatives but lacks experimental validation.

- **Open Question 2**: Can OCM objective be effectively applied to other generative models like GANs or VAEs? While mentioned as potentially applicable, the paper doesn't explore this beyond diffusion models.

- **Open Question 3**: How does OCM perform on large-scale video generation tasks using latent diffusion models? The paper suggests straightforward application but provides no experimental results or detailed analysis.

## Limitations
- Assumes learned score function is sufficiently accurate without analyzing how score quality affects covariance estimation
- Diagonal approximation may miss important off-diagonal correlations
- Requires retraining covariance network for each new dataset unlike parameter-free baselines

## Confidence
- **Mechanism 1**: Low - unbiasedness claim needs empirical validation with finite samples and neural network approximations
- **Mechanism 2**: Medium - error amplification claim plausible but not rigorously proven
- **Mechanism 3**: Medium - cross-compatibility claims lack ablation studies and architecture transfer testing

## Next Checks
1. **Ablation on Score Quality**: Systematically degrade base score function quality and measure impact on covariance estimation accuracy and final sample quality
2. **Off-Diagonal Analysis**: Compare OCM's diagonal covariance against full covariance estimation (where feasible) on small datasets to quantify information loss
3. **Cross-Architecture Transfer**: Train OCM on one architecture (e.g., DDPM) and test direct application to another (e.g., DDIM) without retraining