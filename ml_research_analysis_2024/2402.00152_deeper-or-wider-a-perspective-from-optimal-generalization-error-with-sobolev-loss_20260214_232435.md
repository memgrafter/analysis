---
ver: rpa2
title: 'Deeper or Wider: A Perspective from Optimal Generalization Error with Sobolev
  Loss'
arxiv_id: '2402.00152'
source_url: https://arxiv.org/abs/2402.00152
tags:
- error
- neural
- number
- networks
- denns
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates the generalization error in Sobolev training
  between deeper neural networks (DeNNs) and wider neural networks (WeNNs). The authors
  analyze the optimal generalization error by decomposing it into approximation error
  and sampling error for DeNNs with H k loss functions (k=0,1,2).
---

# Deeper or Wider: A Perspective from Optimal Generalization Error with Sobolev Loss

## Quick Facts
- arXiv ID: 2402.00152
- Source URL: https://arxiv.org/abs/2402.00152
- Authors: Yahong Yang; Juncai He
- Reference count: 40
- This paper investigates the generalization error in Sobolev training between deeper neural networks (DeNNs) and wider neural networks (WeNNs).

## Executive Summary
This paper analyzes the generalization error of deeper neural networks (DeNNs) versus wider neural networks (WeNNs) under Sobolev training with H^k loss functions (k=0,1,2). The authors decompose the generalization error into approximation error and sampling error, finding that DeNNs outperform WeNNs when sample points are abundant but parameters are limited, while WeNNs excel when parameters are abundant but samples are limited. The theoretical analysis provides insights for choosing between deeper and wider architectures based on data availability and model complexity constraints.

## Method Summary
The paper compares DeNNs with width N=O(log L) and depth L to WeNNs with constant or logarithmic depth and width N, both with total parameters W=O(N²L). The generalization error is analyzed using Rademacher complexity and uniform covering numbers for the sampling error component, while approximation error bounds are derived from existing literature on ReLU networks. The Sobolev loss functions (H^k norms) incorporate function values and derivatives up to order k, with ReLU activation for k=0,1 and ReLU squared for k=2. The theoretical framework establishes conditions under which DeNNs or WeNNs achieve lower generalization error based on the trade-off between approximation capability and sampling complexity.

## Key Results
- DeNNs achieve better generalization than WeNNs when the number of sample points is large but the number of parameters is limited
- WeNNs outperform DeNNs when the number of parameters is large but the number of sample points is limited
- As the order of derivatives in the loss function increases, WeNNs may transition to DeNNs, affecting generalization performance

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The optimal generalization error depends on the trade-off between approximation error (model complexity) and sampling error (data availability).
- Mechanism: The paper decomposes the generalization error into approximation error (how well the model fits the target function) and sampling error (how well the learned model generalizes from limited data). Deeper networks excel at approximation with fewer parameters but require more samples for generalization; wider networks need more parameters but generalize better with fewer samples.
- Core assumption: The number of parameters W and number of samples M are the primary determinants of the approximation and sampling errors, respectively.
- Evidence anchors:
  - [abstract]: "DeNNs perform better than WeNNs when the number of sample points is large but the number of parameters is limited. Conversely, WeNNs are superior to DeNNs if the number of parameters is large but the number of sample points is limited."
  - [section]: "ERD,k(θS,k) ≤ RD,k(θD,k) + ERS,k(θD,k) − RD,k(θD,k) + E(RS,k(θS,k) − RS,k(θD,k) + RD,k(θS,k) − RS,k(θS,k))"
  - [corpus]: Weak - related papers discuss Sobolev training but don't directly address the parameter/sample trade-off.
- Break condition: If the regularity of the target function f(x) is not captured by the Sobolev space norms used in the analysis.

### Mechanism 2
- Claim: The order of derivatives in the loss function (k) affects the relative performance of deeper vs. wider networks.
- Mechanism: Higher-order derivatives in the loss function (higher k in H^k norms) require greater regularity in the approximating functions. Deeper networks are better suited for capturing higher regularity, shifting the performance advantage toward them as k increases.
- Core assumption: The regularity of the target function and the ability of the network architecture to capture that regularity are directly related to the order of derivatives in the loss function.
- Evidence anchors:
  - [abstract]: "as the required order of the derivative in the loss function increases, WeNNs may transition to DeNNs"
  - [section]: "Furthermore, as the required order of the derivative in the loss function increases, WeNNs may transition to DeNNs, influencing the performance of NNs in generalization error."
  - [corpus]: Weak - related papers discuss Sobolev training but don't explicitly analyze the impact of derivative order on network architecture choice.
- Break condition: If the target function does not possess the required regularity for the given loss function order.

### Mechanism 3
- Claim: The super-convergence rate of deeper networks allows them to achieve better approximation with fewer parameters, but this advantage is offset by increased sampling error.
- Mechanism: Deeper networks can represent more complex functions and achieve superior approximation rates compared to shallower networks (super-convergence). However, the increased complexity leads to a larger hypothesis space, resulting in higher sampling error that can negate the approximation advantage if data is limited.
- Core assumption: The super-convergence rate of deeper networks is a key factor in their approximation capabilities, but this comes at the cost of increased sampling error.
- Evidence anchors:
  - [section]: "This improved approximation rate is referred to as super-convergence. However, these studies do not explicitly compare the generalization error with shallow neural networks."
  - [section]: "However, in this paper, we are dealing with DeNNs. DeNNs offer significant advantages in approximation, as they can represent more complex functions compared to shallow neural networks, which has been extensively explored in the literature [33, 25, 26]."
  - [corpus]: Weak - related papers discuss Sobolev training but don't directly address the super-convergence phenomenon.
- Break condition: If the number of parameters is not sufficiently constrained, the sampling error can become prohibitive.

## Foundational Learning

- Concept: Sobolev spaces and norms
  - Why needed here: The paper analyzes the generalization error using Sobolev norms (H^k) which incorporate both function values and derivatives. Understanding these spaces is crucial for interpreting the results.
  - Quick check question: What is the difference between L^2 norm and H^1 norm, and why is this distinction important for the analysis?

- Concept: Covering numbers and pseudo-dimension
  - Why needed here: The paper uses covering numbers and pseudo-dimension to bound the generalization error. These concepts are essential for understanding the theoretical analysis.
  - Quick check question: How do covering numbers and pseudo-dimension relate to the complexity of a function class and its generalization ability?

- Concept: Rademacher complexity
  - Why needed here: The paper also uses Rademacher complexity as an alternative method to bound the generalization error. Understanding this concept provides a different perspective on the analysis.
  - Quick check question: How does Rademacher complexity relate to the uniform convergence of empirical risk to true risk?

## Architecture Onboarding

- Component map:
  Input layer -> Hidden layers (L layers with N neurons each) -> Output layer
  Activation: ReLU for k=0,1; ReLU squared for k=2
  Loss function: H^k norm incorporating function values and derivatives

- Critical path:
  1. Define the hypothesis space based on the chosen architecture (WeNN or DeNN)
  2. Calculate the number of parameters W = O(N^2L)
  3. Determine the number of sample points M
  4. Choose the loss function order k (0, 1, or 2)
  5. Apply the theoretical results to determine the optimal architecture

- Design tradeoffs:
  - WeNNs: Simpler training, lower risk of overfitting, but require more parameters for good approximation
  - DeNNs: Superior approximation with fewer parameters, but more complex training and higher sampling error
  - Trade-off: Balance between approximation error (favoring DeNNs) and sampling error (favoring WeNNs)

- Failure signatures:
  - Poor generalization with DeNNs: Likely due to insufficient sample points relative to the complexity of the hypothesis space
  - Overfitting with WeNNs: Likely due to excessive model complexity relative to the available data
  - Suboptimal performance: May indicate an incorrect choice of loss function order k

- First 3 experiments:
  1. Compare the performance of WeNNs and DeNNs on a synthetic dataset with varying numbers of sample points M and fixed number of parameters W.
  2. Investigate the impact of the loss function order k on the relative performance of WeNNs and DeNNs for a fixed number of sample points M and parameters W.
  3. Analyze the generalization error of DeNNs with different depths L and widths N on a real-world dataset, keeping the total number of parameters W constant.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Under what conditions does the generalization error advantage of deeper neural networks (DeNNs) over wider neural networks (WeNNs) become significant in Sobolev training?
- Basis in paper: [explicit] The paper states that DeNNs perform better than WeNNs when the number of sample points is large but the number of parameters is limited, while WeNNs are superior when the number of parameters is large but the number of sample points is limited.
- Why unresolved: The paper does not provide a precise threshold or quantitative relationship between the number of sample points, number of parameters, and the generalization error advantage.
- What evidence would resolve it: Experimental results or theoretical analysis that establishes the exact relationship between sample points, parameters, and the generalization error advantage for DeNNs over WeNNs in various Sobolev loss scenarios.

### Open Question 2
- Question: How does the order of derivatives in the Sobolev loss function affect the generalization error and the choice between DeNNs and WeNNs?
- Basis in paper: [explicit] The paper mentions that as the order of derivatives in the loss function increases, the effective region for DeNNs expands, and WeNNs may transition to DeNNs.
- Why unresolved: The paper does not provide a quantitative analysis of how the derivative order in the loss function influences the generalization error or the choice between DeNNs and WeNNs.
- What evidence would resolve it: Experimental results or theoretical analysis that quantifies the impact of derivative order in the loss function on the generalization error and the relative performance of DeNNs and WeNNs.

### Open Question 3
- Question: What is the generalization error behavior of DeNNs in the overparameterized case, and how does it compare to the underparameterized case?
- Basis in paper: [inferred] The paper focuses on the underparameterized case and mentions that the overparameterized case is considered a topic for future research.
- Why unresolved: The paper does not provide any analysis or results for the overparameterized case, leaving the generalization error behavior of DeNNs in this scenario unknown.
- What evidence would resolve it: Theoretical analysis or experimental results that characterize the generalization error of DeNNs in the overparameterized case and compare it to the underparameterized case.

## Limitations
- The analysis assumes the target function f(x) belongs to a Sobolev space with bounded norms, which may not hold for all practical applications
- The theoretical bounds rely on covering number and Rademacher complexity estimates that may be loose in practice
- The comparison between WeNNs and DeNNs is primarily asymptotic and may not capture finite-sample effects accurately

## Confidence
- High Confidence: The decomposition of generalization error into approximation and sampling components is well-established in statistical learning theory
- Medium Confidence: The super-convergence phenomenon for deeper networks is supported by existing literature but the specific bounds and comparisons in this paper may have tighter constants
- Medium Confidence: The transition behavior as derivative order k increases is theoretically sound but requires empirical validation across diverse function classes

## Next Checks
1. Conduct empirical validation on real-world datasets with varying regularity to test whether the Sobolev space assumptions hold and whether the theoretical predictions match observed performance
2. Implement the proposed framework on a suite of PDE problems using both the deep Ritz method and PINN approach to verify the practical applicability of the theory
3. Extend the analysis to other activation functions (e.g., tanh, sigmoid) and compare whether the WeNN/DeNN trade-off persists across different architectures