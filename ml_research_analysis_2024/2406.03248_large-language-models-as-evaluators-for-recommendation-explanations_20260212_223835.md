---
ver: rpa2
title: Large Language Models as Evaluators for Recommendation Explanations
arxiv_id: '2406.03248'
source_url: https://arxiv.org/abs/2406.03248
tags:
- evaluation
- user
- recommendation
- accuracy
- llms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates whether Large Language Models (LLMs) can
  serve as evaluators for recommendation explanations. The authors use real user feedback
  as ground truth and collect additional third-party annotations and LLM evaluations.
---

# Large Language Models as Evaluators for Recommendation Explanations

## Quick Facts
- arXiv ID: 2406.03248
- Source URL: https://arxiv.org/abs/2406.03248
- Authors: Xiaoyu Zhang; Yishan Li; Jiayin Wang; Bowen Sun; Weizhi Ma; Peijie Sun; Min Zhang
- Reference count: 40
- Key outcome: This paper investigates whether Large Language Models (LLMs) can serve as evaluators for recommendation explanations. The authors use real user feedback as ground truth and collect additional third-party annotations and LLM evaluations. They design a 3-level meta-evaluation strategy to measure correlation between evaluator labels and user feedback. The study finds that certain LLMs, particularly GPT-4, can provide evaluation accuracy comparable to or better than traditional methods like BLEU and ROUGE. The evaluation accuracy varies across different aspects of explanation quality (persuasiveness, transparency, accuracy, satisfaction). The paper also explores enhancing LLM evaluation through one-shot learning with human labels and ensembling multiple heterogeneous LLM evaluators. Overall, the study concludes that LLMs can be a reproducible and cost-effective solution for evaluating recommendation explanation texts with appropriate prompts and settings.

## Executive Summary
This study investigates the feasibility of using Large Language Models (LLMs) as evaluators for recommendation explanations. The authors employ a comprehensive 3-level meta-evaluation strategy to measure how well LLM evaluations align with real user feedback across four aspects: persuasiveness, transparency, accuracy, and satisfaction. Their findings demonstrate that certain LLMs, particularly GPT-4, can provide evaluation accuracy comparable to or better than traditional metrics like BLEU and ROUGE. The study also explores techniques to enhance LLM evaluation performance, including one-shot learning with human labels and ensembling multiple heterogeneous evaluators.

## Method Summary
The authors collect real user feedback on recommendation explanations as ground truth, supplemented by third-party annotations and LLM-generated evaluations. They employ various LLMs (GPT-4, GPT-3.5, Qwen, Llama) to evaluate explanations using zero-shot and one-shot learning approaches. The evaluation is conducted across four aspects: persuasiveness, transparency, accuracy, and satisfaction. A 3-level meta-evaluation strategy measures correlation between LLM evaluations and user feedback at dataset, user, and pair levels using Pearson correlation coefficients. The study also explores personalized one-shot learning with human labels and ensemble methods combining multiple LLM evaluators.

## Key Results
- LLMs, particularly GPT-4, can achieve evaluation accuracy comparable to or better than traditional metrics (BLEU, ROUGE)
- Evaluation accuracy varies across different aspects of explanation quality (persuasiveness, transparency, accuracy, satisfaction)
- Personalized one-shot learning with human labels significantly improves LLM evaluation accuracy by teaching user-specific scoring biases
- Ensembling multiple heterogeneous LLM evaluators improves accuracy and stability by averaging out individual model biases

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLMs can serve as effective evaluators for recommendation explanations by leveraging their instruction-following and common-sense reasoning capabilities.
- Mechanism: LLMs interpret the context of recommendation explanations and generate ratings aligned with user perceptions across multiple aspects (persuasiveness, transparency, accuracy, satisfaction).
- Core assumption: User perceptions of explanation quality can be modeled through natural language prompts and are sufficiently consistent for LLM evaluation.
- Evidence anchors:
  - [abstract] "leveraging LLMs as evaluators presents a promising avenue in Natural Language Processing tasks (e.g., sentiment classification, information extraction), as they perform strong capabilities in instruction following and common-sense reasoning."
  - [section] "We utilize pre-trained LLMs to provide annotations for each explanation text. The large language model receives the movie name and the corresponding explanation text, accompanied by a prompt P to describe the context."
  - [corpus] Weak - no direct corpus evidence supporting LLM evaluation for recommendation explanations specifically.
- Break condition: If user perceptions are too diverse or context-specific for LLMs to capture without extensive fine-tuning.

### Mechanism 2
- Claim: Personalized one-shot learning with human labels improves LLM evaluation accuracy by teaching the model user-specific scoring biases.
- Mechanism: Providing LLM with examples of a specific user's previous ratings helps it learn that user's rating tendencies and preferences, leading to more accurate evaluations.
- Core assumption: Users have consistent rating patterns that can be learned from a small number of examples.
- Evidence anchors:
  - [section] "Incorporating human labels helps align the LLM with user preferences, making GPT4(M) perform as well as or better than third-party human annotators across all aspects simultaneously."
  - [section] "We notice that for zero-shot and non-personalized one-shot, the Pair-Level evaluation accuracy consistently surpasses that of Dataset-Level, but this is not the case for personalized one-shot learning."
  - [corpus] Weak - limited corpus evidence on personalized one-shot learning effectiveness.
- Break condition: If user rating patterns are too inconsistent or if the provided examples don't represent the user's true preferences.

### Mechanism 3
- Claim: Ensembling multiple heterogeneous LLM evaluators improves accuracy and stability by averaging out individual model biases.
- Mechanism: Combining ratings from different LLMs reduces the impact of any single model's weaknesses and provides more robust evaluations.
- Core assumption: Different LLMs have complementary strengths and weaknesses that can be leveraged through ensembling.
- Evidence anchors:
  - [section] "We ensemble results from various LLMs by averaging their ratings to obtain the final scores. In Figure 4, the expectation of evaluation accuracy (mean) increases with #N on all level aspects."
  - [section] "This suggests that ensemble multiple LLMs can mitigate the issue of a single evaluator performing poorly on certain aspects."
  - [corpus] Weak - limited corpus evidence on LLM ensembling for evaluation tasks.
- Break condition: If the ensemble becomes dominated by a few strong models or if the models are too similar to provide meaningful diversity.

## Foundational Learning

- Concept: Correlation metrics (Pearson, Spearman, Kendall)
  - Why needed here: To measure the similarity between LLM evaluations and ground truth user ratings at different levels (dataset, user, pair).
  - Quick check question: What's the difference between Pearson and Spearman correlation, and when would you use each?

- Concept: Prompt engineering for LLM instruction following
  - Why needed here: To guide LLMs in evaluating recommendation explanations across multiple aspects with consistent interpretation.
  - Quick check question: How would you structure a prompt to evaluate explanation quality across multiple aspects versus a single aspect?

- Concept: Meta-evaluation strategies
  - Why needed here: To comprehensively assess the quality of evaluation methods themselves by examining correlations at different granularities.
  - Quick check question: Why might evaluation accuracy differ between dataset-level and pair-level correlations in this context?

## Architecture Onboarding

- Component map: Data collection (user feedback, third-party annotations) → LLM evaluation (zero-shot, one-shot, ensemble) → Meta-evaluation (correlation at 3 levels) → Analysis
- Critical path: Prompt construction → LLM evaluation → Correlation calculation → Result aggregation
- Design tradeoffs: Zero-shot (scalable but potentially less accurate) vs. One-shot (more accurate but requires human labels) vs. Ensemble (most stable but computationally expensive)
- Failure signatures: Poor correlation at certain levels, inconsistent results across aspects, failure to parse LLM outputs
- First 3 experiments:
  1. Compare zero-shot LLM evaluation against human annotations on a small subset
  2. Test one-shot learning with different types of human label examples
  3. Evaluate ensemble performance with varying numbers of LLM evaluators

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can zero-shot LLMs effectively evaluate recommendation explanations for domains beyond movies?
- Basis in paper: [explicit] The study uses movie recommendation data from MovieLens. The authors suggest future work could explore other explanation formats and domains.
- Why unresolved: The experiments are limited to a single domain (movies) and explanation format (text-based). It's unclear if results generalize to other recommendation scenarios like music, products, or news.
- What evidence would resolve it: Conducting similar experiments with recommendation data from different domains and explanation formats (e.g., visual, hybrid) to test LLM evaluation performance across contexts.

### Open Question 2
- Question: How do different LLM architectures (e.g., decoder-only, encoder-decoder, multimodal) compare in their ability to evaluate recommendation explanations?
- Basis in paper: [explicit] The study uses various LLMs (GPT-4, GPT-3.5, Llama2, Qwen) but doesn't deeply analyze architectural differences in evaluation performance.
- Why unresolved: The paper compares different models but doesn't systematically explore how architectural choices impact evaluation quality for recommendation explanations.
- What evidence would resolve it: Comprehensive benchmarking of diverse LLM architectures on the same recommendation explanation dataset, analyzing performance differences and architectural strengths/weaknesses.

### Open Question 3
- Question: What is the optimal ensemble strategy for combining multiple LLM evaluators in recommendation explanation assessment?
- Basis in paper: [explicit] The study explores simple averaging of multiple LLM scores but doesn't investigate advanced ensemble methods or optimal combination strategies.
- Why unresolved: The paper shows that ensembling helps but doesn't explore weighted combinations, model selection strategies, or dynamic ensemble approaches based on aspect or user characteristics.
- What evidence would resolve it: Comparative studies of different ensemble methods (weighted averaging, stacking, boosting) on recommendation explanation datasets, measuring which approaches yield the most accurate and stable evaluations across aspects and user groups.

## Limitations

- The study focuses exclusively on movie recommendation explanations, limiting generalizability to other recommendation domains
- LLM evaluation performance may vary significantly across different user groups and cultural contexts
- The meta-evaluation approach, while comprehensive, may not capture all nuances of subjective explanation quality aspects

## Confidence

- **High Confidence**: The finding that LLMs can provide evaluation accuracy comparable to traditional metrics (BLEU, ROUGE) is well-supported by correlation analysis across multiple evaluation levels.
- **Medium Confidence**: The effectiveness of one-shot learning with human labels, while promising, requires further validation across different domains and user types.
- **Medium Confidence**: The ensemble approach showing improved stability and accuracy is supported by empirical results, but the optimal number and selection of ensemble members needs more investigation.

## Next Checks

1. **Cross-domain validation**: Test the LLM evaluation framework on recommendation explanations from other domains (e.g., product recommendations, news articles) to assess generalizability.
2. **Temporal stability analysis**: Evaluate whether LLM evaluators maintain consistent performance when user preferences evolve over time by conducting longitudinal studies.
3. **Cultural bias assessment**: Investigate how well the evaluation framework performs across different cultural contexts by testing with user feedback from diverse demographic groups.