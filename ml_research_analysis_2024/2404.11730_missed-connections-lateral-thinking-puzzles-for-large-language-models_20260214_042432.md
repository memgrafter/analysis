---
ver: rpa2
title: 'Missed Connections: Lateral Thinking Puzzles for Large Language Models'
arxiv_id: '2404.11730'
source_url: https://arxiv.org/abs/2404.11730
tags:
- word
- words
- puzzle
- group
- connections
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates the ability of large language models (LLMs)
  and sentence embeddings to solve the Connections puzzle from the New York Times,
  which requires finding groups of four words that share a common theme among a set
  of sixteen words. The authors evaluate various sentence embedding baselines and
  LLMs on a dataset of 250 puzzles, measuring their success rates with up to five
  guesses allowed.
---

# Missed Connections: Lateral Thinking Puzzles for Large Language Models

## Quick Facts
- **arXiv ID:** 2404.11730
- **Source URL:** https://arxiv.org/abs/2404.11730
- **Reference count:** 31
- **Primary result:** GPT-4 with chain-of-thought prompting achieves 38.93% success rate on Connections puzzles, significantly outperforming sentence embedding baselines at 11.6%.

## Executive Summary
This paper investigates the ability of large language models and sentence embeddings to solve the Connections puzzle from the New York Times, which requires finding groups of four words that share a common theme among sixteen words. The authors evaluate various sentence embedding baselines and LLMs on 250 puzzles, measuring their success rates with up to five guesses allowed. They find that GPT-4 achieves the highest success rate at 29.2%, which improves to 38.93% with chain-of-thought prompting. The results demonstrate that while LLMs can solve some Connections puzzles, the task remains challenging, suggesting the puzzle could serve as a benchmark for abstract reasoning in NLP models.

## Method Summary
The paper evaluates sentence embeddings and large language models on the Connections puzzle task. The dataset consists of 250 puzzles collected from an online archive. Sentence embeddings from models like BERT, ROBERTA, MPNet, and MiniLM are used as a baseline approach, computing pairwise cosine similarities to identify potential groupings. LLMs including GPT-3.5-Turbo and GPT-4-Turbo are evaluated with both direct and chain-of-thought prompting. The evaluation framework tracks success rates across puzzles and categories, with a focus on the impact of initial guesses and prompting strategies on overall performance.

## Key Results
- GPT-4 with chain-of-thought prompting achieves 38.93% success rate, significantly outperforming the best sentence embedding baseline (MPNet at 11.6%)
- Performance depends heavily on the correctness of the initial guess, with success rates dropping substantially after incorrect first attempts
- Sentence embedding approaches can solve all puzzles within 417 guesses, indicating semantic relationships are captured but require extensive search
- GPT-4-TURBO achieves 29.2% averaged success rate, improving to 38.93% with chain-of-thought prompting (p < 6.77 × 10⁻⁵)

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** GPT-4 with chain-of-thought prompting achieves 38.93% success rate on Connections puzzles, significantly outperforming the baseline MPNet embedding approach (11.6%).
- **Mechanism:** Chain-of-thought prompting causes the model to reason through category membership step-by-step, generating explicit justifications that improve its ability to identify abstract or lateral associations between words.
- **Core assumption:** The model's internal representations contain sufficient semantic information to identify the puzzle categories, but the information retrieval process benefits from explicit reasoning steps.
- **Evidence anchors:**
  - [abstract]: "Chain-of-thought prompting improves GPT-4's performance to 38.93%"
  - [section]: "chain-of-thought prompting improves the performance of the GPT-4-TURBO model both overall (from 29.2% averaged success rate to 38.93% averaged success rate, p < 6.77 ∗ 10−5)"
  - [corpus]: Weak evidence - only mentions related lateral thinking work, no direct Connections puzzle results
- **Break condition:** If the model's internal representations lack the necessary semantic associations for a particular puzzle, even chain-of-thought prompting cannot recover this missing information.

### Mechanism 2
- **Claim:** Sentence embedding approaches can solve all puzzles within 417 guesses, suggesting the embeddings capture weak semantic links but require extensive search.
- **Mechanism:** The sentence embeddings encode semantic relationships between words, allowing the model to identify word groupings through cosine similarity, but the search space is too large for efficient real-time solving.
- **Core assumption:** The sentence embeddings contain enough semantic information to distinguish between correct and incorrect groupings, even if the information is not perfectly organized.
- **Evidence anchors:**
  - [section]: "the straightforward sentence embedding approach solves every one of the 150 puzzles within 417 of the allowed 500 incorrect guesses"
  - [abstract]: "we find that these methods are able to solve the puzzles a sizeable proportion of the time"
  - [corpus]: Weak evidence - mentions sentence transformers library but no direct Connections results
- **Break condition:** If the semantic relationships in the embeddings are too weak or noisy, the search space becomes intractable even with 500 guesses allowed.

### Mechanism 3
- **Claim:** LLM performance depends heavily on the correctness of the initial guess, with success rates dropping substantially after an incorrect or nearly correct first attempt.
- **Mechanism:** An incorrect initial guess leads the model into "rabbit holes" where it expends all remaining guesses attempting to solve a single nearly-correct category rather than exploring other possibilities.
- **Core assumption:** The model's reasoning process becomes anchored to its initial hypothesis, making it difficult to pivot to alternative category assignments.
- **Evidence anchors:**
  - [section]: "performance decreases substantially if the initial guess is incorrect or nearly correct compared to general performance"
  - [abstract]: "we find that these methods are able to solve the puzzles a sizeable proportion of the time, but with far from perfect accuracy"
  - [corpus]: Weak evidence - mentions rabbit holes but no direct Connections results
- **Break condition:** If the model develops better exploration strategies or is given more guesses, it might overcome the initial guess dependency.

## Foundational Learning

- **Concept:** Cosine similarity in high-dimensional embedding space
  - **Why needed here:** The baseline approach relies on computing pairwise cosine similarities between word embeddings to identify potential groupings
  - **Quick check question:** How does cosine similarity measure the relationship between two vectors in embedding space?

- **Concept:** Chain-of-thought prompting
  - **Why needed here:** This technique significantly improves LLM performance by forcing the model to explain its reasoning before providing a final answer
  - **Quick check question:** What is the difference between direct prompting and chain-of-thought prompting in terms of model output?

- **Concept:** Statistical significance testing
  - **Why needed here:** The paper uses Welch's t-test to compare model performance and determine if differences are meaningful
  - **Quick check question:** When would you use Welch's t-test instead of a standard t-test for comparing two groups?

## Architecture Onboarding

- **Component map:** Data collection pipeline -> Puzzle solver interface -> Embedding baseline -> LLM interface -> Evaluation framework
- **Critical path:** Puzzle → LLM prompt generation → API call → Response parsing → Guess validation → Next prompt (if needed) → Success/failure tracking
- **Design tradeoffs:**
  - Using greedy sampling (temperature=0) vs. probabilistic sampling for diversity
  - Limiting to 5 guesses vs. allowing more attempts for better baseline comparison
  - Randomizing word order vs. preserving NYT's original ordering
- **Failure signatures:**
  - LLM produces invalid guesses (words not in puzzle)
  - Model gets stuck in "rabbit holes" after incorrect initial guesses
  - Sentence embeddings require excessive guesses (>500) to solve puzzles
- **First 3 experiments:**
  1. Run baseline embedding approach on 10 puzzles with 50 guesses allowed to verify semantic relationships
  2. Test GPT-4 with basic prompt on 5 puzzles to establish baseline performance
  3. Compare chain-of-thought vs. direct prompting on 3 puzzles to measure improvement magnitude

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How do the semantic embeddings capture lateral or abstract associations between words in Connections puzzles?
- **Basis in paper:** [explicit] The paper discusses the gap between the amount of search required in embedding space versus the amount needed by LLMs or human players.
- **Why unresolved:** The paper identifies a gap but does not provide a detailed analysis of how embeddings represent lateral or abstract associations.
- **What evidence would resolve it:** A detailed analysis of the embeddings' performance on different types of Connections puzzles, including those requiring lateral thinking, would help understand how well embeddings capture abstract associations.

### Open Question 2
- **Question:** What is the impact of using a dedicated training set of Connections puzzles on the performance of LLMs?
- **Basis in paper:** [explicit] The paper suggests that using a dedicated training set could either directly learn a solver or iteratively refine prompts.
- **Why unresolved:** The paper does not explore the impact of training on a dedicated dataset.
- **What evidence would resolve it:** Experiments comparing the performance of LLMs trained on a dedicated dataset versus those without training would provide insights into the benefits of such an approach.

### Open Question 3
- **Question:** How do the word ordering and grouping affect the performance of LLMs in solving Connections puzzles?
- **Basis in paper:** [inferred] The paper mentions that the word ordering in the puzzle presentation affects the performance of LLMs, as evidenced by the discrepancy with prior work.
- **Why unresolved:** The paper does not systematically investigate the impact of word ordering on LLM performance.
- **What evidence would resolve it:** Controlled experiments varying the word ordering and grouping in the puzzles, and measuring the impact on LLM performance, would clarify the role of ordering in solving the puzzles.

## Limitations
- LLM performance remains limited at 38.93% success rate even with chain-of-thought prompting, indicating the task remains challenging
- Results are sensitive to prompt engineering, with small changes potentially affecting performance significantly
- The paper does not explore training LLMs on dedicated Connections puzzle datasets, which could improve performance

## Confidence
- **High confidence**: The sentence embedding baseline performance (11.6% for MPNet) and the general pattern that GPT-4 outperforms other models
- **Medium confidence**: The specific performance improvement from chain-of-thought prompting, as prompt engineering can be sensitive to implementation details
- **Medium confidence**: The claim that initial guesses heavily influence overall success, based on the observation that incorrect first guesses lead to lower success rates

## Next Checks
1. **Prompt sensitivity analysis**: Test 3-5 variations of the GPT-4 prompt to establish performance variance and identify the most robust prompt formulation
2. **Cross-puzzle generalization**: Evaluate the same models on 25 puzzles from a different time period to verify results aren't specific to the collected dataset
3. **Human baseline comparison**: Have 5 humans solve 20 random puzzles to establish whether current LLM performance represents meaningful progress or remains substantially below human capability