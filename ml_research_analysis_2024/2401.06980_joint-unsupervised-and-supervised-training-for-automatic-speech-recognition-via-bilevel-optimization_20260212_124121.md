---
ver: rpa2
title: Joint Unsupervised and Supervised Training for Automatic Speech Recognition
  via Bilevel Optimization
arxiv_id: '2401.06980'
source_url: https://arxiv.org/abs/2401.06980
tags:
- training
- supervised
- bl-just
- unsupervised
- optimization
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes BL-JUST, a bilevel optimization-based training
  approach for joint unsupervised and supervised training of acoustic models for ASR.
  It formulates the training as a bilevel optimization problem with an unsupervised
  loss as the lower-level constraint and a supervised loss as the upper-level objective.
---

# Joint Unsupervised and Supervised Training for Automatic Speech Recognition via Bilevel Optimization

## Quick Facts
- arXiv ID: 2401.06980
- Source URL: https://arxiv.org/abs/2401.06980
- Reference count: 32
- Key outcome: BL-JUST reduces WER by 1-3.8% vs PT+FT on LibriSpeech and TED-LIUM v2

## Executive Summary
This paper proposes BL-JUST, a bilevel optimization framework that jointly trains acoustic models on both labeled and unlabeled speech data. By coupling unsupervised InfoNCE loss with supervised CTC loss in a single training loop, BL-JUST eliminates the need for separate pre-training and fine-tuning stages. Experiments show consistent WER improvements across different model architectures and datasets, with additional efficiency gains from reduced training epochs.

## Method Summary
BL-JUST formulates joint unsupervised and supervised training as a bilevel optimization problem, where unsupervised feature learning (InfoNCE loss) is constrained by supervised performance (CTC loss). The framework uses a penalty-based reformulation to solve this efficiently in a single training loop, updating both backbone and classification parameters simultaneously. This approach eliminates negative transfer issues from sequential pre-training and fine-tuning while reducing overall training time.

## Key Results
- Reduces WER by 1-3.8% compared to conventional pre-training + fine-tuning on LibriSpeech and TED-LIUM v2
- Achieves superior performance across different model architectures (Conformer, CNN-LSTM)
- Improves training efficiency by requiring fewer epochs than the two-stage approach
- Consistent gains across various data splits and supervision levels

## Why This Works (Mechanism)

### Mechanism 1
BL-JUST eliminates negative transfer by coupling unsupervised and supervised training in a single bilevel optimization loop, allowing supervised gradients to guide the feature learning process. The lower-level InfoNCE loss is constrained by the upper-level CTC loss, forming a penalty-based joint objective. Updates to backbone parameters are guided by both labeled and unlabeled data simultaneously.

### Mechanism 2
BL-JUST reduces training time and computational cost compared to PT+FT by integrating pre-training and fine-tuning into a single loop. Both unsupervised and supervised updates occur in the same epoch, eliminating the need to run pre-training for many epochs followed by separate fine-tuning.

### Mechanism 3
The penalty-based reformulation ensures rigorous convergence guarantees and equivalence to the original constrained problem under mild assumptions. With sufficient penalty weight, solutions of the penalized problem are approximately optimal for the original bilevel problem, provided the supervised loss is Lipschitz and the unsupervised loss satisfies the PL inequality.

## Foundational Learning

- Concept: Bilevel optimization
  - Why needed here: Reformulates ASR training as a two-level problem: unsupervised feature learning constrained by supervised performance
  - Quick check question: In bilevel optimization, what is the role of the lower-level problem relative to the upper-level objective?

- Concept: InfoNCE loss for unsupervised speech representation
  - Why needed here: Used as the lower-level objective to learn generic speech representations from unlabeled data
  - Quick check question: How does InfoNCE differ from a simple reconstruction loss in contrastive learning?

- Concept: CTC loss for sequence modeling in ASR
  - Why needed here: Serves as the upper-level supervised objective, optimizing label predictions for transcribed speech
  - Quick check question: Why is CTC preferred over cross-entropy in ASR tasks with variable-length alignments?

## Architecture Onboarding

- Component map: 80-dim logmel features -> Conformer/CNN-LSTM backbone -> 1000 BPE output layer
- Critical path:
  1. Forward pass through backbone to get contextual representations
  2. Compute InfoNCE loss on unlabeled batch (backbone params only)
  3. Compute CTC loss on labeled batch (full model)
  4. Combine losses with penalty γ
  5. Backpropagate and update backbone with both gradients
  6. Update classification head with CTC gradient only

- Design tradeoffs:
  - Single loop vs two-stage: faster training but more complex gradient management
  - Penalty weight γ: too small → weak supervision; too large → overfitting to labeled data
  - Backbone architecture: Conformer better for LibriSpeech, CNN-LSTM for smaller datasets

- Failure signatures:
  - WER plateaus early: check γ schedule or learning rate balance
  - Validation loss diverges: backbone and head gradients may be conflicting; try gradient clipping
  - InfoNCE loss dominates: reduce γ or increase supervised batch size

- First 3 experiments:
  1. Run supervised baseline with labeled data only; record WER
  2. Run PT+FT with same labeled/unlabeled split; compare WER and training time
  3. Run BL-JUST with varying γ growth rates; identify best rate from Table 6 pattern

## Open Questions the Paper Calls Out

- **Open Question 1**: How does BL-JUST scale to larger, real-world ASR datasets with millions of hours of audio?
  - Basis: Experiments limited to datasets up to 960 hours
  - Why unresolved: Paper focuses on moderate-sized datasets without addressing large-scale computational efficiency

- **Open Question 2**: How sensitive is BL-JUST to the choice of unsupervised loss function, and can other losses like masked prediction improve performance?
  - Basis: Paper uses InfoNCE but doesn't explore alternatives like masked prediction
  - Why unresolved: No comparison or analysis of different unsupervised objectives

- **Open Question 3**: Can BL-JUST be extended to multilingual or cross-lingual ASR tasks where labeled data is scarce in some languages?
  - Basis: Paper doesn't address multilingual settings
  - Why unresolved: Experiments limited to monolingual datasets with no discussion of multilingual adaptation

## Limitations
- Theoretical convergence claims not empirically validated beyond validation loss monitoring
- No ablation studies isolate bilevel formulation effects from architectural or data factors
- Penalty weight γ is tuned per experiment without principled schedule
- Choice of InfoNCE loss not compared to alternatives like masked prediction

## Confidence
- **High confidence**: BL-JUST reduces WER compared to supervised-only baselines across architectures
- **Medium confidence**: BL-JUST achieves lower WER than PT+FT while requiring fewer epochs
- **Low confidence**: The bilevel formulation itself is the key driver of performance gains, as opposed to increased model capacity or data usage

## Next Checks
1. Ablation on bilevel constraint: Run BL-JUST with γ=0 (pure supervised) and γ→∞ (pure unsupervised) to confirm the penalty term is essential
2. Alternative unsupervised objectives: Replace InfoNCE with masked reconstruction or contrastive predictive coding and compare WER to baseline
3. Data efficiency test: Train on reduced labeled/unlabeled splits to measure how much supervised data BL-JUST saves compared to PT+FT