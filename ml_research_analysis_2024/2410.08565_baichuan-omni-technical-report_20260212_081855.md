---
ver: rpa2
title: Baichuan-Omni Technical Report
arxiv_id: '2410.08565'
source_url: https://arxiv.org/abs/2410.08565
tags:
- arxiv
- data
- audio
- multimodal
- video
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Baichuan-omni is the first open-source 7B MLLM capable of processing
  text, image, video, and audio inputs concurrently. It addresses the gap in high-performing
  open-source multimodal models by introducing a training framework with omni-modal
  data construction, multimodal alignment pre-training, and multitask fine-tuning.
---

# Baichuan-Omni Technical Report

## Quick Facts
- arXiv ID: 2410.08565
- Source URL: https://arxiv.org/abs/2410.08565
- Reference count: 40
- Key outcome: First open-source 7B MLLM processing text, image, video, and audio concurrently, achieving 76.2% on MMBench and state-of-the-art among open-source models

## Executive Summary
Baichuan-omni is the first open-source 7B multimodal large language model capable of processing text, image, video, and audio inputs concurrently. It addresses the gap in high-performing open-source multimodal models by introducing a training framework with omni-modal data construction, multimodal alignment pre-training, and multitask fine-tuning. The model achieves state-of-the-art performance among open-source models across multiple benchmarks while maintaining competitive language understanding.

## Method Summary
Baichuan-omni uses a progressive training schema starting with a 7B language model and extending through multimodal alignment and multitask fine-tuning. The framework involves constructing omni-modal training data, training a vision-language model using extensive image-text data, then training an audio-language model with ASR data, and finally integrating high-quality images, audio, and video data for comprehensive multimodal alignment. The model employs a Conv-GMLP projector for audio processing and features streaming audio-video interaction with boundary prediction for real-time processing.

## Key Results
- Achieves 76.2% on MMBench, 74.9% on MMBench-CN, and 47.3% on MMMU
- Strong video performance at 60.9% on MVBench
- Excellent audio performance with 7.0% WER on Fleurs zh
- Outperforms prior leading open-source model VITA on most multimodal benchmarks

## Why This Works (Mechanism)

### Mechanism 1
The progressive training approach starting with a 7B LLM and extending through multimodal alignment and multitask fine-tuning achieves high performance. This staged approach prevents catastrophic forgetting while progressively integrating new modalities. A frozen LLM backbone provides a strong foundation for multimodal alignment without requiring retraining from scratch.

### Mechanism 2
Using a Conv-GMLP projector instead of simple pooling preserves more audio information during down-sampling. Convolutional layers reduce sequence length while expanding feature channels, maintaining richer audio detail compared to aggressive pooling that discards information. Preserving audio token sequence information is more important than reducing token count for ASR performance.

### Mechanism 3
Streaming audio-video interaction with boundary prediction enables real-time multimodal processing. The model predicts audio start/end points while simultaneously encoding incoming visual frames into features for attention computation. Audio features are processed only after input completion, allowing concurrent handling. Real-time audio boundary detection coordinates streaming visual encoding without data loss.

## Foundational Learning

- **Multimodal alignment pre-training**: Establishes cross-modal representations before instruction fine-tuning, preventing catastrophic forgetting. Quick check: What happens if you skip visual pre-training and directly fine-tune the LLM on image-text pairs?

- **Streaming multimodal processing**: Enables real-time audio-video interaction by processing visual data while waiting for audio completion. Quick check: How does the model handle cases where audio boundaries are incorrectly predicted?

- **Data filtering and quality control**: High-quality curated datasets prevent model degradation and improve instruction-following capabilities. Quick check: What criteria determine whether a sample is "known" vs "unknown" during SFT data selection?

## Architecture Onboarding

- **Component map**: 7B LLM backbone → Visual encoder (Siglip) → Video encoder (resampled frames) → Audio encoder (Whisper) → Projectors (Conv-GMLP for audio, MLP for visual) → Streaming boundary predictor → Tokenizer
- **Critical path**: Audio boundary prediction → Visual frame encoding (streaming) → LLM attention computation → Audio feature input (post-completion) → Text generation
- **Design tradeoffs**: Progressive training vs. joint training (progressive better for avoiding forgetting), down-sampling rate vs. audio quality (Conv-GMLP allows aggressive down-sampling), fixed vs. dynamic resolution (AnyRes improves document understanding)
- **Failure signatures**: Poor OCR performance indicates visual projector issues, degraded ASR suggests audio projector problems, latency spikes suggest boundary prediction errors
- **First 3 experiments**:
  1. Test audio boundary prediction accuracy on short vs. long audio clips
  2. Compare Conv-GMLP down-sampling rates (2, 4, 8) on ASR benchmarks
  3. Evaluate visual encoder resolution impact using AnyRes vs. fixed 384px on document understanding tasks

## Open Questions the Paper Calls Out

### Open Question 1
How does Baichuan-omni's performance scale with increased model size beyond 7B parameters? The paper mentions VITA uses 8x7B parameters and outperforms Baichuan-omni on some benchmarks, suggesting potential for scaling improvements. Performance comparisons across different parameter scales (e.g., 7B, 13B, 34B) on the same benchmark suite would resolve this.

### Open Question 2
What is the long-term memory retention capability of Baichuan-omni when processing extended video sequences beyond 48 frames? The paper states the model uses a maximum of 48 frames for video input but acknowledges this limitation. Performance evaluation on video understanding tasks with input lengths exceeding 48 frames would resolve this question.

### Open Question 3
How does the Conv-GMLP audio projector's performance compare to traditional pooling methods in real-world noisy environments? While the paper demonstrates Conv-GMLP's robustness in controlled benchmark settings, it doesn't test performance in real-world noisy conditions. ASR and audio comprehension tests in environments with varying levels of background noise would resolve this.

## Limitations

- Evaluation methodology relies heavily on automatic metrics and external human evaluation datasets with limited validation process detail
- Streaming capability claims are based on architectural design rather than empirical latency measurements across diverse hardware
- Report lacks ablation studies isolating contributions of individual components like Conv-GMLP versus alternative down-sampling approaches

## Confidence

**High Confidence (Likelihood > 80%):**
- The model architecture and training pipeline as described can be implemented to produce a working multimodal system
- The benchmark results are correctly reported based on the provided evaluation methodology
- The progressive training approach is technically sound and would prevent catastrophic forgetting

**Medium Confidence (Likelihood 50-80%):**
- The streaming audio-video interaction mechanism works as described for typical use cases
- The Conv-GMLP projector provides meaningful performance gains over simpler pooling approaches
- The model achieves competitive performance on MVBench and Fleurs benchmarks as reported

**Low Confidence (Likelihood < 50%):**
- The real-time performance characteristics under various hardware constraints
- The generalization to completely unseen multimodal tasks beyond the evaluated benchmarks
- The absolute magnitude of improvement over prior open-source approaches when controlling for dataset quality differences

## Next Checks

1. Measure audio boundary prediction accuracy across diverse audio lengths (10s, 30s, 2min) and compare against ground truth timing to quantify real-world streaming performance degradation.

2. Retrain baseline variants using identical datasets but different audio projectors (pooling vs Conv-GMLP at various rates) and evaluate ASR performance on standard benchmarks to isolate projector contribution.

3. Deploy the streaming architecture on different hardware configurations (GPU vs CPU, various memory constraints) and measure end-to-end latency from audio start to text completion to validate real-time claims.