---
ver: rpa2
title: Bayes-Optimal Fair Classification with Linear Disparity Constraints via Pre-,
  In-, and Post-processing
arxiv_id: '2402.02817'
source_url: https://arxiv.org/abs/2402.02817
tags:
- fair
- disparity
- bayes-optimal
- classifier
- ddis
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper develops a unified framework for Bayes-optimal fair
  classification, addressing the challenge of minimizing classification error while
  satisfying group fairness constraints. The authors introduce linear and bilinear
  disparity measures, showing that popular fairness metrics like demographic parity,
  equality of opportunity, and predictive equality fall into these categories.
---

# Bayes-Optimal Fair Classification with Linear Disparity Constraints via Pre-, In-, and Post-processing

## Quick Facts
- arXiv ID: 2402.02817
- Source URL: https://arxiv.org/abs/2402.02817
- Authors: Xianli Zeng; Kevin Jiang; Guang Cheng; Edgar Dobriban
- Reference count: 40
- One-line primary result: Unified framework for Bayes-optimal fair classification with linear disparity constraints, providing pre-, in-, and post-processing algorithms with near-optimal fairness-accuracy tradeoffs

## Executive Summary
This paper develops a unified theoretical framework for Bayes-optimal fair classification that minimizes classification error while satisfying group fairness constraints. The authors introduce linear and bilinear disparity measures, showing that popular fairness metrics like demographic parity, equality of opportunity, and predictive equality fall into these categories. By leveraging a connection with the Neyman-Pearson lemma, they derive explicit forms of Bayes-optimal fair classifiers as group-wise thresholding rules with characterized thresholds. The framework extends to multiple constraints (e.g., equalized odds), multi-class protected attributes, and scenarios where protected attributes cannot be used at prediction time.

## Method Summary
The paper proposes three algorithmic approaches to achieve Bayes-optimal fair classification: Fair Up- and Down-Sampling (pre-processing) perturbs the data distribution to induce fairness; Fair cost-sensitive Classification (in-processing) minimizes a group-wise cost-sensitive risk; and Fair Plug-In Rule (post-processing) adjusts group-wise thresholds on estimated regression functions. These methods directly control disparity levels while maintaining competitive accuracy, and the theoretical framework characterizes the fair Pareto frontier, proving that the tradeoff function between misclassification and disparity is convex for linear measures.

## Key Results
- Achieves near-optimal fairness-accuracy tradeoffs through pre-, in-, and post-processing algorithms
- Demonstrates state-of-the-art performance on synthetic and real datasets (AdultCensus, COMPAS, LawSchool, ACSIncome)
- Pre-processing method particularly effective at low disparity levels while maintaining competitive accuracy
- Theoretical framework characterizes Bayes-optimal fair classifiers under linear and bilinear disparity measures

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The paper establishes that finding Bayes-optimal fair classifiers under linear disparity constraints is equivalent to a generalized Neyman-Pearson optimization problem, enabling explicit threshold-based solutions.
- Mechanism: By expressing fairness constraints as linear functionals of the classifier and leveraging the connection to optimal hypothesis testing, the authors derive group-wise thresholding rules with explicitly characterized thresholds.
- Core assumption: The disparity measure is linear (or bilinear) in the classifier and the class-conditional regression functions ηa(x) have probability density functions.
- Evidence anchors:
  - [abstract] "We find the form of Bayes-optimal fair classifiers under a single linear disparity measure, by uncovering a connection with the Neyman-Pearson lemma."
  - [section 4.1] "We characterize Bayes-optimal classifiers under linear disparity measures (Theorem 4.2), by uncovering a connection with the Neyman-Pearson argument for optimal hypothesis testing."
  - [corpus] Weak - neighboring papers focus on multiple sensitive features or functional data, not the specific Neyman-Pearson connection.
- Break condition: If the disparity measure is not linear/bilinear, or if ηa(x) lacks a density (e.g., discrete features), the explicit thresholding form no longer holds.

### Mechanism 2
- Claim: The proposed FUDS pre-processing method perturbs the data distribution so that an unconstrained classifier on the modified data equals the fair Bayes-optimal classifier on the original data.
- Mechanism: By adjusting the proportions of each demographic group defined by label and protected attribute according to a fairness-inducing factor, the algorithm generates a dataset whose unconstrained Bayes-optimal classifier satisfies the fairness constraint.
- Core assumption: The conditional distribution of features given label and protected attribute remains unchanged during perturbation.
- Evidence anchors:
  - [section 5.1] "Theorem 5.1 designs a fairness-inducing distribution by multiplying the probabilities of each group with a group-wise factor... This enables us to design an easy-to-use Fair Up- and Down-Sampling pre-processing algorithm."
  - [section 5.1.1] "We prove that this perturbation can be obtained by adjusting the proportion of each demographic group defined by the label and protected attribute."
  - [corpus] Missing - neighboring papers do not discuss the specific fairness-inducing factor approach or the connection between perturbed data and fair Bayes-optimality.
- Break condition: If the conditional feature distribution changes with the perturbation, or if the fairness-inducing factor cannot be accurately estimated from finite data.

### Mechanism 3
- Claim: The FCSC in-processing method achieves Bayes-optimal fair classification by minimizing a carefully designed group-wise cost-sensitive risk.
- Mechanism: The fair cost-sensitive risk shifts thresholds for each protected group in opposite directions to reduce disparity, and its unconstrained minimizer equals the fair Bayes-optimal classifier.
- Core assumption: The cost-sensitive risk formulation correctly captures the threshold adjustments needed for fairness.
- Evidence anchors:
  - [section 5.2] "We aim to shift the thresholds used for the protected groups in opposite directions to reduce disparity... We aim to identify a risk function whose unconstrained minimizer is the fair Bayes-optimal classifier."
  - [section 5.2] "Leveraging that cost-sensitive classification achieves a similar effect, we show that the fair Bayes-optimal classifier can be achieved through a carefully designed group-wise cost-sensitive risk."
  - [corpus] Weak - neighboring papers focus on federated learning or minimax optimization, not the specific group-wise cost-sensitive approach.
- Break condition: If the cost-sensitive formulation does not accurately reflect the threshold adjustments, or if the optimization becomes unstable with complex models.

## Foundational Learning

- Concept: Generalized Neyman-Pearson Lemma
  - Why needed here: Provides the theoretical foundation for characterizing optimal classifiers under linear constraints, which directly enables the explicit thresholding rules for fair classification.
  - Quick check question: Can you explain how the Neyman-Pearson lemma extends to optimization with linear constraints on classifiers?

- Concept: Linear and Bilinear Disparity Measures
  - Why needed here: These definitions enable the reduction of fair classification to a tractable optimization problem and allow explicit characterization of Bayes-optimal classifiers.
  - Quick check question: What distinguishes a bilinear disparity measure from a general linear one, and why is this distinction important for deriving explicit classifier forms?

- Concept: Pareto Frontier Analysis
  - Why needed here: Characterizes the optimal tradeoff between accuracy and fairness, providing guidance on how to set appropriate fairness constraints.
  - Quick check question: How does the convexity of the tradeoff function for linear disparity measures affect the marginal cost of increasing fairness?

## Architecture Onboarding

- Component map: Theoretical framework (linear/bilinear disparity measures, Neyman-Pearson connection) -> Algorithmic implementations (FUDS, FCSC, FPIR) -> Empirical evaluation pipeline
- Critical path: Derive explicit fair Bayes-optimal classifier forms -> Implement pre-processing, in-processing, and post-processing algorithms -> Evaluate on synthetic and real datasets -> Analyze Pareto frontier
- Design tradeoffs: Pre-processing (FUDS) is simple but may lose information; in-processing (FCSC) integrates fairness during training but requires careful cost design; post-processing (FPIR) is model-agnostic but depends on accurate probability estimation
- Failure signatures: FUDS may fail if group proportions cannot be accurately estimated; FCSC may become unstable with complex models or poor cost specification; FPIR depends heavily on accurate group-wise probability estimation
- First 3 experiments:
  1. Implement FUDS on a simple synthetic dataset with known Bayes-optimal fair classifier and verify it recovers the expected thresholds.
  2. Apply FCSC to a real dataset and compare fairness-accuracy tradeoff against unconstrained classification.
  3. Use FPIR to post-process an existing classifier and measure disparity reduction while tracking accuracy changes.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the theoretical framework for fair Bayes-optimal classifiers be extended to handle cost-sensitive fairness constraints where the cost of misclassification differs across protected groups?
- Basis in paper: [explicit] The paper mentions cost-sensitive classification in Section 4.3.4 but only considers cost differences between positive and negative outcomes, not across protected groups.
- Why unresolved: The current formulation treats protected groups symmetrically in the fairness constraint, but real-world applications may require differential treatment of groups based on societal impact.
- What evidence would resolve it: Developing and proving theorems for group-specific cost parameters in the disparity measure, along with empirical validation showing improved fairness-accuracy tradeoffs.

### Open Question 2
- Question: How does the performance of the proposed methods scale with the dimensionality of the feature space, particularly for high-dimensional data where the Bayes-optimal classifier may be complex?
- Basis in paper: [inferred] The paper uses synthetic data with p dimensions but doesn't systematically explore the effect of dimensionality on method performance or computational efficiency.
- Why unresolved: The theoretical guarantees assume known population parameters, but finite-sample performance and computational complexity may degrade as dimensionality increases.
- What evidence would resolve it: Empirical studies varying feature dimensions systematically, along with theoretical analysis of sample complexity requirements for each method.

### Open Question 3
- Question: Can the framework be extended to handle fairness constraints that depend on multiple protected attributes simultaneously (e.g., intersectionality)?
- Basis in paper: [explicit] The paper mentions multi-class protected attributes in Section 4.3.3 but only considers single-attribute demographic parity.
- Why unresolved: Real-world fairness concerns often involve multiple intersecting protected characteristics, but the current framework treats each attribute independently.
- What evidence would resolve it: Developing explicit forms of Bayes-optimal classifiers for multi-attribute fairness constraints and proving corresponding algorithmic methods.

### Open Question 4
- Question: What are the implications of using estimated rather than true regression functions ηa in the post-processing method FPIR?
- Basis in paper: [explicit] The FPIR algorithm uses plug-in estimates bηa in Section 5.3, but the paper doesn't analyze the statistical properties of this approach.
- Why unresolved: The theoretical optimality relies on exact knowledge of ηa, but practical implementations must estimate these functions, potentially introducing bias in the fairness adjustment.
- What evidence would resolve it: Finite-sample error analysis showing how estimation error in ηa propagates to fairness disparity and accuracy, along with robustness experiments.

### Open Question 5
- Question: How do the proposed methods perform when the protected attribute is only partially observed or has missing data?
- Basis in paper: [inferred] The paper mentions restricted access to protected attributes in Section 7 but doesn't develop methods for partial observation scenarios.
- Why unresolved: Real-world datasets often have incomplete protected attribute information, yet the current methods assume complete observation during training.
- What evidence would resolve it: Empirical evaluation of methods using datasets with artificially introduced missing protected attribute values, along with theoretical bounds on performance degradation.

## Limitations
- Theoretical framework limited to linear and bilinear disparity measures, potentially excluding some practical fairness definitions
- Performance depends heavily on accurate estimation of population parameters (group proportions, regression functions)
- Empirical evaluation lacks detailed specification of neural network architectures and hyperparameters
- Pre-processing method assumes conditional feature distributions remain unchanged during perturbation

## Confidence

### High confidence: The theoretical derivations connecting Bayes-optimal fair classification to Neyman-Pearson optimization, and the characterization of the fair Pareto frontier for linear disparity measures.

### Medium confidence: The practical implementation of the three algorithmic approaches (FUDS, FCSC, FPIR) and their empirical performance, given the strong theoretical foundation but dependence on accurate parameter estimation.

### Low confidence: Generalization to non-linear disparity measures and scenarios where protected attributes are unavailable during prediction, as these cases receive less theoretical and empirical treatment.

## Next Checks

1. Implement FUDS on synthetic data with known Bayes-optimal fair classifier and verify it recovers the expected thresholds within acceptable error bounds.

2. Conduct sensitivity analysis on parameter estimation quality to quantify the impact of finite sample effects on disparity reduction and accuracy.

3. Test the FCSC method with different cost-sensitive risk formulations to evaluate robustness to hyperparameter choices and model complexity.