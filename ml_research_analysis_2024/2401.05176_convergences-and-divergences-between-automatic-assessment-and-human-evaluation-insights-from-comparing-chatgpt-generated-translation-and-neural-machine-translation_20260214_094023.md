---
ver: rpa2
title: 'Convergences and Divergences between Automatic Assessment and Human Evaluation:
  Insights from Comparing ChatGPT-Generated Translation and Neural Machine Translation'
arxiv_id: '2401.05176'
source_url: https://arxiv.org/abs/2401.05176
tags:
- translation
- human
- chatgpt
- metrics
- evaluation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This study compares the translation quality of ChatGPT and three\
  \ NMT systems using both automated metrics and human evaluation. Automated metrics\
  \ like BLEU and ChrF fail to capture ChatGPT\u2019s strong semantic alignment with\
  \ references, while human evaluation reveals that providing ChatGPT with an example\
  \ or context significantly improves translation quality across all assessed dimensions."
---

# Convergences and Divergences between Automatic Assessment and Human Evaluation: Insights from Comparing ChatGPT-Generated Translation and Neural Machine Translation

## Quick Facts
- arXiv ID: 2401.05176
- Source URL: https://arxiv.org/abs/2401.05176
- Reference count: 27
- Key outcome: Automated metrics like BLEU and ChrF fail to capture ChatGPT’s semantic alignment with references, while human evaluation reveals significant improvements from prompt engineering.

## Executive Summary
This study compares ChatGPT and three NMT systems using automated metrics and human evaluation on a diplomatic corpus. While automated metrics show NMT systems outperforming ChatGPT, human evaluation reveals that providing ChatGPT with examples or context significantly improves translation quality across all assessed dimensions. The weak correlations between automated metrics and human scores demonstrate that human understanding of translation quality diverges from what automated metrics capture, highlighting the need for more nuanced evaluation methods.

## Method Summary
The study constructs a corpus of 6,878 diplomatic texts (17,837 parallel sentences) from the Ministry of Foreign Affairs of China. ChatGPT is used with 0-shot, 1-shot, and context-aware prompts, while three NMT systems (Microsoft Translate, Google Translate, DeepL) generate translations. Automated metrics (BLEU, ChrF, BERTScore, COMET) are computed alongside human evaluation using MQM-DQF error typology and six analytic rubrics. Six annotators each evaluate 100 sentences.

## Key Results
- Automated metrics like BLEU and ChrF fail to capture ChatGPT’s strong semantic alignment with references
- Human evaluation reveals that providing ChatGPT with an example or context significantly improves translation quality across all assessed dimensions
- Weak and non-significant correlations between automated metrics and human scores indicate that human understanding of translation quality diverges from what automated metrics capture

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Automated metrics like BLEU and ChrF fail to capture semantic improvements in ChatGPT translations when prompts are refined.
- Mechanism: These metrics rely on exact n-gram matching and penalize semantic paraphrasing, while ChatGPT uses contextual understanding to generate semantically faithful but lexically divergent translations.
- Core assumption: Semantic fidelity is not well-represented by exact n-gram overlap.
- Evidence anchors:
  - [abstract]: "automated metrics like BLEU and ChrF fail to capture ChatGPT’s strong semantic alignment with references"
  - [section 4.1]: "when measured by BLEU and Chrf, ChatGPT is overshadowed by NMT systems"
- Break condition: If evaluation requires strict adherence to reference phrasing, these metrics would perform adequately.

### Mechanism 2
- Claim: Human evaluation reveals improvements from prompt engineering that automated metrics miss.
- Mechanism: Human raters assess coherence, cultural sensitivity, and pragmatic appropriateness, which are enhanced by providing examples or context to ChatGPT.
- Core assumption: Human judgment captures dimensions of translation quality beyond lexical accuracy.
- Evidence anchors:
  - [abstract]: "human evaluation reveals that providing ChatGPT with an example or context significantly improves translation quality"
  - [section 4.2]: "exposing ChatGPT to contextual information improves its translation quality across all the six rubrics"
- Break condition: If automated metrics were redesigned to capture contextual and pragmatic features, the gap might narrow.

### Mechanism 3
- Claim: Weak correlation between automated metrics and human scores indicates divergent quality constructs.
- Mechanism: Automated metrics quantify formal fidelity (exact matches), while human evaluation weighs semantic and pragmatic fidelity, leading to misalignment.
- Core assumption: Different evaluation methods prioritize different aspects of translation quality.
- Evidence anchors:
  - [abstract]: "weak and non-significant correlations between automated metrics and human scores indicate that human understanding of translation quality diverges from what automated metrics capture"
  - [section 4.3]: "The weak and non-significant results overall demonstrate that human understanding of translation quality is significantly different from what is captured by automated metrics"
- Break condition: If automated metrics incorporated human-like evaluation dimensions, correlations could strengthen.

## Foundational Learning

- Concept: N-gram matching in BLEU/chrF
  - Why needed here: Understanding why these metrics penalize semantic paraphrases is key to interpreting the study's findings.
  - Quick check question: How would BLEU score change if a translation uses synonyms instead of exact words from the reference?

- Concept: Error typology (MQM-DQF)
  - Why needed here: The study uses this framework to categorize translation errors; knowing its structure is essential for interpreting results.
  - Quick check question: What are the primary error types in the MQM-DQF taxonomy, and how do they differ from fluency issues?

- Concept: Correlation coefficient interpretation
  - Why needed here: The study reports weak correlations; understanding what this implies about metric alignment is critical.
  - Quick check question: What does a Pearson correlation of 0.10 between two evaluation methods suggest about their agreement?

## Architecture Onboarding

- Component map:
  Corpus construction -> Translation generation (ChatGPT/NMT) -> Automated metrics calculation -> Human evaluation (error typology + rubric scoring) -> Correlation analysis
- Critical path:
  1. Prepare corpus of diplomatic texts
  2. Generate translations under different prompt conditions
  3. Compute BLEU, chrF, BERTScore, COMET
  4. Conduct human annotation for errors and rubric scores
  5. Calculate correlations between automated and human scores
- Design tradeoffs:
  - Automated metrics: Fast, scalable, but miss semantic/pragmatic nuances
  - Human evaluation: Rich, context-aware, but time-consuming and subjective
- Failure signatures:
  - High automated scores but low human scores → Metrics overfit to lexical accuracy
  - Low automated scores but high human scores → Metrics fail to capture semantic improvements
- First 3 experiments:
  1. Replicate correlation analysis with a different corpus (e.g., literary texts) to test generalizability
  2. Test automated metrics that incorporate semantic similarity (e.g., COMET) against human scores
  3. Compare human evaluation results when raters are blinded vs. aware of translation origin

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What specific aspects of translation quality do automated metrics fail to capture that human evaluators consider essential?
- Basis in paper: [explicit] The paper states that automated metrics like BLEU and ChrF fail to capture ChatGPT's strong semantic alignment with references, while human evaluation reveals that providing ChatGPT with an example or context significantly improves translation quality across all assessed dimensions. The weak and non-significant correlations between automated metrics and human scores indicate that human understanding of translation quality diverges from what automated metrics capture.
- Why unresolved: While the paper demonstrates the divergence between automated metrics and human evaluation, it does not specify the exact dimensions of translation quality that automated metrics fail to capture. The paper mentions that human evaluation considers aspects like coherence, cultural sensitivity, and practicality, but does not provide a detailed analysis of what specific elements of these dimensions are overlooked by automated metrics.
- What evidence would resolve it: A detailed analysis of the error types and severity levels assigned by human evaluators that are not captured by automated metrics, along with examples of translations where automated metrics fail to reflect the improvements made by ChatGPT with example or context prompts.

### Open Question 2
- Question: How can automated metrics be improved to better align with human evaluation and capture the nuances of translation quality?
- Basis in paper: [inferred] The paper highlights the limitations of existing automated metrics in capturing the full range of translation quality aspects considered by human evaluators. It suggests that further exploration is needed to develop more effective evaluation metrics that can better capture the nuances of translation quality, particularly in terms of coherence, clarity, practicality, adherence to norms, cultural sensitivity, and appropriateness of style, tone, and register.
- Why unresolved: While the paper identifies the need for improved automated metrics, it does not provide specific suggestions or guidelines on how to develop such metrics. It does not discuss potential approaches, such as incorporating semantic similarity measures, contextual information, or cultural awareness, that could enhance the alignment between automated metrics and human evaluation.
- What evidence would resolve it: Proposed methods for improving automated metrics, along with experimental results demonstrating their effectiveness in capturing the dimensions of translation quality considered by human evaluators.

### Open Question 3
- Question: What is the impact of prompt engineering on the performance of large language models like ChatGPT in translation tasks, and how can it be optimized?
- Basis in paper: [explicit] The paper shows that providing ChatGPT with an example or context significantly improves its translation quality across all assessed dimensions in human evaluation. However, the automated metrics fail to capture this improvement, suggesting that prompt engineering can have a substantial impact on the performance of large language models in translation tasks.
- Why unresolved: While the paper demonstrates the positive impact of prompt engineering on ChatGPT's translation quality, it does not explore the specific strategies or techniques that can be employed to optimize prompt engineering. It does not discuss the potential trade-offs between different types of prompts (e.g., example-based, context-based) or the optimal level of detail and specificity required in prompts to achieve the best translation results.
- What evidence would resolve it: Comparative studies of different prompt engineering strategies and their impact on the translation quality of large language models, along with guidelines or best practices for optimizing prompt engineering in translation tasks.

## Limitations
- Study relies on a single corpus of diplomatic texts, limiting generalizability
- Exact ChatGPT prompts are not fully specified, making exact replication challenging
- Human evaluation introduces subjectivity that may affect reproducibility

## Confidence
- High Confidence: Automated metrics fail to capture semantic improvements in ChatGPT translations when prompts are refined
- Medium Confidence: Human evaluation reveals improvements from prompt engineering that automated metrics miss
- Low Confidence: Weak correlation between automated metrics and human scores indicates divergent quality constructs

## Next Checks
1. Replicate the study using a different corpus (e.g., literary texts) to test whether the divergence between automated metrics and human evaluation is consistent across domains
2. Test automated metrics that incorporate semantic similarity (e.g., COMET) against human scores to determine if incorporating human-like evaluation dimensions can reduce the gap
3. Compare human evaluation results when raters are blinded vs. aware of translation origin to assess the impact of rater bias on the findings