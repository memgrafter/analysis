---
ver: rpa2
title: Risk-Sensitive Soft Actor-Critic for Robust Deep Reinforcement Learning under
  Distribution Shifts
arxiv_id: '2402.09992'
source_url: https://arxiv.org/abs/2402.09992
tags:
- uni00000013
- uni0000004c
- uni00000003
- uni00000008
- uni00000057
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work introduces discrete Soft Actor-Critic for the entropic
  risk measure, a novel model-free risk-sensitive deep reinforcement learning algorithm
  for discrete actions that improves robustness against distribution shifts in contextual
  multi-stage stochastic combinatorial optimization problems. The method derives a
  Bellman equation for Q-values under the entropic risk measure and establishes policy
  improvement results to enable practical off-policy learning from single trajectories.
---

# Risk-Sensitive Soft Actor-Critic for Robust Deep Reinforcement Learning under Distribution Shifts

## Quick Facts
- arXiv ID: 2402.09992
- Source URL: https://arxiv.org/abs/2402.09992
- Reference count: 40
- Introduces discrete Soft Actor-Critic for the entropic risk measure to improve robustness against distribution shifts

## Executive Summary
This paper introduces a risk-sensitive variant of Soft Actor-Critic (SAC) that optimizes the entropic risk measure to improve robustness against distribution shifts in contextual multi-stage stochastic combinatorial optimization problems. The algorithm derives a Bellman equation for Q-values under the entropic risk measure and establishes policy improvement results to enable practical off-policy learning from single trajectories. Experimental results on a grid-world environment demonstrate that the risk-sensitive algorithm improves robustness against distribution shifts compared to risk-neutral SAC, achieving up to 53% of the upper bound's performance improvement over a greedy baseline, while also improving performance on the training distribution.

## Method Summary
The method introduces discrete Soft Actor-Critic for the entropic risk measure by deriving a version of the Bellman equation for the respective Q-values. Given the Q-values obtained under an old policy, a new policy is obtained as the Boltzmann distribution over the old Q-values, exactly as in the original SAC paper. The computation of the critic target has a log-sum-exp structure, which is rewritten for numerical stability. The resulting algorithm is identical to risk-neutral SAC for discrete actions, except for the adapted critic loss, which gives the intended risk-sensitivity.

## Key Results
- Risk-sensitive SAC improves robustness against distribution shifts compared to risk-neutral SAC
- Achieves up to 53% of the upper bound's performance improvement over a greedy baseline
- Outperforms benchmarks: manipulating training data and entropy regularization

## Why This Works (Mechanism)

### Mechanism 1
Risk-sensitive SAC improves robustness against distribution shifts by optimizing the entropic risk measure, which explicitly penalizes high-variance outcomes. The entropic risk measure $1/\beta \log E[e^{\beta R}]$ transforms the optimization objective to account for uncertainty in returns. For $\beta < 0$, this measure penalizes high-variance returns, leading to policies that perform more consistently across different item distributions.

### Mechanism 2
The policy improvement theorem still holds under the entropic risk measure, allowing practical algorithm development. Despite changing the objective function, the policy improvement step remains the same as standard SAC—minimizing KL divergence between the new policy and a Boltzmann distribution over the old Q-values. This maintains the theoretical foundation while achieving risk-sensitivity.

### Mechanism 3
The log-sum-exp reformulation of the critic target provides numerical stability while maintaining the risk-sensitive objective. By rewriting the critic target using the log-sum-exp trick, the algorithm avoids computing large exponentials directly, preventing numerical overflow while still capturing the exponential utility function's properties.

## Foundational Learning

- Concept: Markov Decision Processes (MDPs) and Bellman equations
  - Why needed here: The entire algorithm is built on MDP theory, and the Bellman equation derivation is central to the method
  - Quick check question: Can you explain why the Bellman equation is necessary for value iteration and how it relates to the recursive structure of MDPs?

- Concept: Risk measures and their properties
  - Why needed here: The entropic risk measure is the core innovation, and understanding its properties (convexity, relationship to exponential utility) is essential
  - Quick check question: What makes the entropic risk measure different from other risk measures like CVaR, and why is it particularly suited for RL applications?

- Concept: Soft Actor-Critic algorithm mechanics
  - Why needed here: The risk-sensitive variant builds directly on SAC, so understanding entropy regularization, the policy loss, and the critic update is crucial
  - Quick check question: How does the entropy regularization term in SAC affect exploration, and why might this be particularly important for robustness?

## Architecture Onboarding

- Component map: Actor network -> Critic network -> Replay buffer -> Target networks -> Environment
- Critical path: 1. Sample batch from replay buffer, 2. Compute target Q-values using log-sum-exp reformulation, 3. Update critic networks with Huber loss, 4. Update actor network using policy gradient, 5. Update target networks
- Design tradeoffs: Risk-sensitivity vs. performance, Numerical stability vs. accuracy, Entropy regularization vs. exploitation
- Failure signatures: Policy collapse to "do not move", Numerical instability with NaN values, Poor robustness with significant performance degradation
- First 3 experiments: 1. Train risk-neutral SAC on gradient 1 distribution and test on all distributions, 2. Train RS-SAC with small |β| values and compare robustness, 3. Test different manipulation strategies (uniform noise, structured noise)

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the risk-sensitive SAC algorithm perform in environments with continuous action spaces?
- Basis in paper: The paper explicitly mentions that future work will develop a version of the risk-sensitive SAC algorithm for continuous actions.
- Why unresolved: The current algorithm is designed for discrete action spaces, and the authors have not yet tested its performance in continuous action space environments.
- What evidence would resolve it: Experimental results comparing the performance of the risk-sensitive SAC algorithm with continuous actions to the risk-neutral SAC and other benchmarks in a continuous action space environment.

### Open Question 2
- Question: What is the impact of varying the entropy coefficient α during training on the robustness of the risk-sensitive SAC algorithm?
- Basis in paper: The paper mentions that the entropy coefficient α can be set as a hyperparameter or learned, and it is used to control the exploration-exploitation tradeoff.
- Why unresolved: The paper does not provide detailed experimental results on how different values of α affect the algorithm's robustness against distribution shifts.
- What evidence would resolve it: A systematic study varying α values during training and evaluating the algorithm's performance on both the training distribution and under distribution shifts.

### Open Question 3
- Question: How does the risk-sensitive SAC algorithm compare to other risk-sensitive algorithms, such as those based on CVaR or mean-variance objectives, in terms of robustness and performance?
- Basis in paper: The paper discusses the use of the entropic risk measure and mentions other risk measures like CVaR but does not compare the performance of the risk-sensitive SAC algorithm to other risk-sensitive algorithms.
- Why unresolved: The paper focuses on the entropic risk measure and does not provide a comprehensive comparison with other risk-sensitive approaches.
- What evidence would resolve it: Experimental results comparing the risk-sensitive SAC algorithm with other risk-sensitive algorithms in terms of robustness and performance on the same test environments.

## Limitations
- The algorithm is limited to discrete action spaces, requiring different techniques for continuous control domains
- Risk-sensitive improvement results assume β is close to zero, which may not hold in practice when strong risk aversion is needed
- Experimental validation is limited to a single grid-world environment, making generalizability unclear

## Confidence

- **High confidence**: The theoretical derivation of the Bellman equation for the entropic risk measure and the policy improvement theorem under this objective function
- **Medium confidence**: The experimental results showing improved robustness against distribution shifts
- **Low confidence**: The claim that this approach offers better general applicability than data manipulation or entropy regularization

## Next Checks
1. **Cross-domain robustness test**: Implement the algorithm on a continuous control benchmark (e.g., MuJoCo tasks) with controlled distribution shifts to verify whether the risk-sensitive approach maintains its advantages beyond discrete action spaces.

2. **β sensitivity analysis**: Systematically vary the risk-sensitivity parameter β across several orders of magnitude to characterize the stability boundaries of the learning process and identify the range where policy improvement guarantees hold.

3. **Distribution shift taxonomy**: Design experiments with different types of distribution shifts (covariate shift, prior probability shift, concept drift) to determine which types the risk-sensitive approach handles best and whether the improvements are consistent across shift categories.