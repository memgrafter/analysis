---
ver: rpa2
title: Can Large Language Models Always Solve Easy Problems if They Can Solve Harder
  Ones?
arxiv_id: '2406.12809'
source_url: https://arxiv.org/abs/2406.12809
tags:
- consistency
- data
- easy
- hard
- answer
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper investigates hard-to-easy inconsistency in large language
  models, where models can solve difficult problems but fail on easier ones. To evaluate
  this, the authors create ConsisEval, a benchmark of paired easy-hard questions across
  math, code, and instruction-following domains.
---

# Can Large Language Models Always Solve Easy Problems if They Can Solve Harder Ones?

## Quick Facts
- arXiv ID: 2406.12809
- Source URL: https://arxiv.org/abs/2406.12809
- Reference count: 40
- Primary result: GPT-4 achieves highest consistency score of 92.2% but still exhibits hard-to-easy inconsistency

## Executive Summary
This paper investigates hard-to-easy inconsistency in large language models (LLMs), where models can solve difficult problems but fail on easier ones. The authors create ConsisEval, a benchmark of paired easy-hard questions across math, code, and instruction-following domains, and propose consistency score (CS) and relative consistency score (RCS) as metrics to quantify this inconsistency. Experiments show that even state-of-the-art models like GPT-4 exhibit inconsistency due to distraction by redundant information, misinterpretation, and computational errors. The study finds that models with stronger capabilities typically show higher consistency, and training on harder data improves consistency for both fine-tuning and in-context learning.

## Method Summary
The authors create ConsisEval benchmark by automatically generating hard data candidates from easy data using GPT-4 with human annotation for selection and revision. They evaluate consistency using CS (conditional probability of correct easy answers given correct hard answers) and RCS (normalized consistency metric). The study tests various models including GPT-4, GPT-3.5, Llama3-8B, and Qwen2-7B across math, code, and instruction-following domains. Models are evaluated under different training conditions with varying ratios of easy and hard data, and in-context learning with different demonstration examples.

## Key Results
- GPT-4 achieves highest consistency score of 92.2% among evaluated models
- Models with stronger capabilities typically exhibit higher consistency, but exceptions exist
- Hard data enhances consistency for both fine-tuning and in-context learning
- Even GPT-4 shows inconsistency due to distraction by redundant information, misinterpretation, and computational errors

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** GPT-4 exhibits hard-to-easy inconsistency due to distraction by redundant information.
- **Mechanism:** When presented with easy problems containing additional conditions, the model processes these extra conditions and performs unnecessary calculations, leading to incorrect answers despite being capable of solving the hard problem.
- **Core assumption:** The model's processing is sequential and cannot filter out irrelevant information once incorporated into the reasoning chain.
- **Evidence anchors:**
  - [section]: Case study shows GPT-4 solving an easy problem about painters' work hours incorrectly by including an unnecessary calculation about "4 painters" that was present in the harder version.
  - [abstract]: "GPT-4 achieves the highest consistency score of 92.2% but is still inconsistent to specific questions due to distraction by redundant information"

### Mechanism 2
- **Claim:** Models with stronger capabilities typically exhibit higher consistency.
- **Mechanism:** As models develop more sophisticated reasoning capabilities, they become better at maintaining logical consistency across problem difficulty levels, reducing the gap between their performance on easy and hard problems.
- **Core assumption:** Capability improvements directly translate to better consistency because underlying reasoning processes become more robust.
- **Evidence anchors:**
  - [section]: "models with stronger capabilities typically exhibit higher consistency, but exceptions also exist"
  - [section]: GPT-4 achieves highest CS of 92.2% among evaluated models
  - [corpus]: "Frontier LLMs Still Struggle with Simple Reasoning Tasks" suggests even advanced models have this issue

### Mechanism 3
- **Claim:** Hard data enhances consistency for both fine-tuning and in-context learning.
- **Mechanism:** Training on harder problems forces the model to develop more robust reasoning patterns that generalize better to easier problems, while in-context learning with harder demonstrations provides better reasoning templates.
- **Core assumption:** The difficulty gradient in training data shapes the model's reasoning development in a way that benefits consistency across difficulty levels.
- **Evidence anchors:**
  - [section]: "hard data enhances consistency for both fine-tuning and in-context learning"
  - [section]: "models show higher consistency when trained under hard data than easy data"
  - [section]: "in-context learning with harder demonstration examples shows better consistency"

## Foundational Learning

- **Concept: Conditional probability**
  - Why needed here: The consistency score metric is fundamentally defined as the conditional probability of correctly answering easy questions given correct answers to hard questions.
  - Quick check question: If a model answers 80 hard questions correctly and 70 easy questions correctly when the hard ones are answered correctly, what is its consistency score?

- **Concept: Maximum likelihood estimation**
  - Why needed here: Probability estimation methods for the consistency score rely on maximum likelihood estimation to determine the probability of correct answers from repeated sampling.
  - Quick check question: Given 15 correct answers out of 20 samples for a question, what is the maximum likelihood estimate of the probability of answering that question correctly?

- **Concept: Relative consistency analysis**
  - Why needed here: The relative consistency score provides insight into the potential for improvement in consistency within a model's current capability level.
  - Quick check question: If a model has a consistency score of 70% and the theoretical maximum for its capability level is 85%, what is its relative consistency score?

## Architecture Onboarding

- **Component map:** Data generation -> Probability estimation -> Metric computation -> Analysis of results and case studies
- **Critical path:** Data generation → Probability estimation → Metric computation → Analysis of results and case studies
- **Design tradeoffs:** Multiple sampling provides more accurate probability estimates but requires more computational resources, while early stopping reduces API costs but may sacrifice some accuracy. The choice between these methods depends on whether models are open-source (local) or proprietary (API-based).
- **Failure signatures:** Low consistency scores despite high accuracy on individual problems indicate inconsistency issues. High variance in probability estimates across samples suggests sampling strategy problems. Cases where hard questions are answered correctly but easy ones are not indicate specific failure modes.
- **First 3 experiments:**
  1. Test GPT-4 on the ConsisEval benchmark using early stopping estimation to establish baseline consistency metrics.
  2. Compare consistency scores across different model sizes (7B, 13B, 70B) using multiple sampling estimation where possible.
  3. Evaluate the impact of training data difficulty by fine-tuning a base model on different ratios of easy-to-hard data and measuring consistency changes.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What specific mechanisms in large language model training lead to hard-to-easy inconsistency, and how do they differ from human learning processes?
- Basis in paper: [explicit] The paper observes that LLMs capable of solving hard problems can fail at easier ones, while humans are inherently consistent reasoners.
- Why unresolved: The paper identifies the phenomenon but does not investigate the underlying causes of this inconsistency in LLMs compared to human reasoning.
- What evidence would resolve it: Comparative studies of attention patterns, internal representations, and reasoning traces between LLMs and humans when solving paired easy-hard problems.

### Open Question 2
- Question: Does the hard-to-easy inconsistency vary across different types of mathematical reasoning (e.g., arithmetic vs. algebra vs. geometry)?
- Basis in paper: [inferred] The paper includes math problems but does not analyze whether inconsistency patterns differ across mathematical domains.
- Why unresolved: The analysis treats math problems generically without examining domain-specific consistency patterns.
- What evidence would resolve it: Granular analysis of consistency scores broken down by mathematical sub-domains and their respective cognitive demands.

### Open Question 3
- Question: How does the sequence length and complexity of context affect hard-to-easy consistency in LLMs?
- Basis in paper: [inferred] The paper mentions distraction by redundant information as a cause of inconsistency but doesn't systematically study context effects.
- Why unresolved: The analysis doesn't vary context length or complexity to determine their impact on consistency.
- What evidence would resolve it: Experiments manipulating problem statement length, number of conditions, and irrelevant information to measure consistency changes.

### Open Question 4
- Question: Can targeted fine-tuning on easy problems improve consistency without degrading performance on hard problems?
- Basis in paper: [explicit] The paper finds that hard data enhances consistency but doesn't explore the opposite approach.
- Why unresolved: The analysis focuses on hard data benefits but doesn't investigate whether easy data can correct inconsistency.
- What evidence would resolve it: Controlled experiments comparing models fine-tuned on easy problems versus hard problems, measuring both consistency and capability trade-offs.

### Open Question 5
- Question: What is the relationship between model size and hard-to-easy consistency, and does it follow the same scaling laws as general capability?
- Basis in paper: [inferred] The paper evaluates models of different sizes but doesn't analyze how consistency scales with parameter count.
- Why unresolved: The analysis correlates capability with consistency but doesn't examine whether consistency improves at the same rate as general performance.
- What evidence would resolve it: Systematic scaling experiments measuring consistency across model sizes while controlling for capability.

## Limitations
- Automatic generation of hard questions using GPT-4 introduces potential bias and may not fully capture difficult variations
- Manual annotation process is labor-intensive and may introduce subjective biases
- Consistency metrics based on limited samples may not fully capture true underlying consistency
- Study focuses on specific domains (math, code, instruction-following) without exploring other areas

## Confidence
- **High Confidence**: The existence of hard-to-easy inconsistency as a measurable phenomenon
- **Medium Confidence**: The proposed mechanisms explaining inconsistency (distraction, capability correlation, and training data effects)
- **Medium Confidence**: The effectiveness of hard data in improving consistency

## Next Checks
1. Cross-validation with human-generated hard questions to validate that observed inconsistency is not an artifact of the generation process
2. Ablation study on redundant information to isolate the distraction mechanism's contribution to inconsistency
3. Longitudinal consistency tracking as models are fine-tuned on progressively harder data to validate the trajectory of improvement