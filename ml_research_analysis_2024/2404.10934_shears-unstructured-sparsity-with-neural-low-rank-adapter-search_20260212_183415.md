---
ver: rpa2
title: 'Shears: Unstructured Sparsity with Neural Low-rank Adapter Search'
arxiv_id: '2404.10934'
source_url: https://arxiv.org/abs/2404.10934
tags:
- shears
- sparsity
- search
- lora
- accuracy
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Shears combines unstructured sparsity with neural low-rank adapter
  search to efficiently fine-tune large language models while achieving high sparsity
  levels and maintaining accuracy. The method first applies a cost-effective sparsification
  algorithm (Wanda) to zero out less important weights, then trains a super-adapter
  network with elastic low-rank adapters, and finally searches for optimal sub-adapter
  configurations using a heuristic followed by hill-climbing.
---

# Shears: Unstructured Sparsity with Neural Low-rank Adapter Search

## Quick Facts
- arXiv ID: 2404.10934
- Source URL: https://arxiv.org/abs/2404.10934
- Authors: J. Pablo Muñoz; Jinjie Yuan; Nilesh Jain
- Reference count: 12
- Primary result: Achieves 40-50% sparsity while improving or maintaining accuracy compared to existing PEFT methods

## Executive Summary
Shears is a novel method that combines unstructured sparsity with neural low-rank adapter search to efficiently fine-tune large language models. The approach first applies the Wanda sparsification algorithm to zero out less important weights, then trains a super-adapter network with elastic low-rank adapters, and finally searches for optimal sub-adapter configurations using a heuristic followed by hill-climbing. Experiments on LLaMA and MPT models across math and commonsense reasoning tasks demonstrate that Shears achieves high sparsity levels while improving or maintaining accuracy compared to standard PEFT methods like LoRA.

## Method Summary
Shears combines unstructured sparsity with neural low-rank adapter search to efficiently fine-tune large language models. The method first applies the Wanda sparsification algorithm to zero out less important weights, then trains a super-adapter network with elastic low-rank adapters, and finally searches for optimal sub-adapter configurations using a heuristic followed by hill-climbing. This approach allows Shears to achieve high sparsity levels (40-50%) while maintaining or improving accuracy compared to existing PEFT methods, requiring only a single GPU for a couple of hours.

## Key Results
- Achieves 40-50% sparsity while improving or maintaining accuracy compared to standard PEFT methods
- Produces models with up to 1.91× fewer non-zero parameters than standard LoRA while maintaining comparable performance
- Reduces training and memory costs by incorporating fewer trainable parameters

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Shears achieves high sparsity while maintaining or improving accuracy by combining unstructured sparsity with neural low-rank adapter search
- Mechanism: The approach first applies Wanda sparsification to zero out less important weights, then trains a super-adapter network with elastic low-rank adapters, and finally searches for optimal sub-adapter configurations using a heuristic followed by hill-climbing
- Core assumption: The neural low-rank adapter search can effectively identify high-performing sub-adapters that compensate for the loss of pruned weights
- Evidence anchors:
  - [abstract]: "Results demonstrate the benefits of Shears compared to other methods, reaching high sparsity levels while improving or with little drop in accuracy"
  - [section 3.3]: "The heuristic strategy... to obtain a reference subnetwork configuration approximately at the center of the search space"
  - [corpus]: Weak - the corpus neighbors discuss various PEFT methods but don't directly address the combination of sparsity and adapter search
- Break condition: If the search space of adapter configurations is too large or the hill-climbing algorithm fails to find good configurations, the approach may not maintain accuracy at high sparsity levels

### Mechanism 2
- Claim: Shears significantly reduces the number of trainable parameters compared to full fine-tuning while maintaining performance
- Mechanism: By freezing the pruned weights and only training the elastic low-rank adapters, Shears reduces the parameter count that needs to be updated during fine-tuning
- Core assumption: Training only the low-rank adapters is sufficient to achieve good performance on downstream tasks
- Evidence anchors:
  - [section 4.3]: "Shears incorporates fewer trainable parameters, reducing training and memory costs"
  - [section 4.4]: "Shears obtains a model with 50% sparsity that contains 1.91× fewer non-zero parameters with minor drops in accuracy"
  - [corpus]: Weak - corpus papers discuss various PEFT methods but don't specifically address the parameter reduction aspect
- Break condition: If the low-rank adapters are insufficient to capture the task-specific knowledge needed, the performance may degrade significantly

### Mechanism 3
- Claim: The Wanda sparsification algorithm provides a cost-effective way to induce sparsity in LLMs
- Mechanism: Wanda computes weight importance based on weights and activations, then leverages this information for unstructured pruning, requiring only a single forward pass of a tiny subset of inputs
- Core assumption: The weight importance scores calculated by Wanda accurately reflect the contribution of each weight to the model's performance
- Evidence anchors:
  - [section 2.1]: "Wanda computes the weight importance S as the element-wise product of the weight magnitude and the norm of input activations"
  - [section 3.1]: "Obtaining Wp for a model with seven billion parameters takes less than five minutes on a single GPU"
  - [corpus]: Weak - corpus papers don't discuss the Wanda algorithm specifically
- Break condition: If the importance scores don't accurately reflect weight contribution, critical weights might be pruned, leading to performance degradation

## Foundational Learning

- Concept: Parameter-Efficient Fine-Tuning (PEFT)
  - Why needed here: Shears builds upon PEFT techniques by adding sparsity and neural architecture search to the adapter configurations
  - Quick check question: What are the main advantages of PEFT methods like LoRA compared to full fine-tuning?

- Concept: Neural Architecture Search (NAS)
  - Why needed here: Shears uses NAS mechanisms to search for optimal low-rank adapter configurations in the super-adapter network
  - Quick check question: How does weight-sharing NAS differ from traditional NAS approaches?

- Concept: Unstructured Pruning
  - Why needed here: Shears employs unstructured pruning to zero out less important weights, creating a sparse model before fine-tuning
  - Quick check question: What are the trade-offs between structured and unstructured pruning in terms of accuracy and hardware efficiency?

## Architecture Onboarding

- Component map: Pre-trained LLM -> Wanda sparsification -> Elastic low-rank adapters -> Super-adapter network -> Neural low-rank adapter search -> Hill-climbing search algorithm

- Critical path:
  1. Apply Wanda sparsification to pre-trained LLM
  2. Generate super-adapter network with elastic LoRA adapters
  3. Fine-tune super-adapter network using NLS
  4. Use heuristic to find initial sub-adapter configuration
  5. Apply hill-climbing search to refine configuration
  6. Deploy sparse, fine-tuned model

- Design tradeoffs:
  - Sparsity level vs. accuracy: Higher sparsity may lead to better efficiency but could degrade performance
  - Search space size vs. search time: Larger search spaces may find better configurations but increase computational cost
  - Number of adapter configurations vs. model complexity: More configurations provide flexibility but increase the super-adapter network size

- Failure signatures:
  - Significant accuracy drop after sparsification: Indicates critical weights were pruned
  - Poor performance of sub-adapters: Suggests the search algorithm failed to find good configurations
  - Slow inference despite high sparsity: May indicate the runtime environment isn't optimized for sparse models

- First 3 experiments:
  1. Apply Wanda sparsification to a small pre-trained model (e.g., GPT-2 small) and verify the sparsity level and inference speed
  2. Implement elastic LoRA adapters on a sparsified model and confirm they can be fine-tuned
  3. Test the hill-climbing search algorithm on a small search space to ensure it can find improved configurations

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of Shears vary when using different unstructured sparsification algorithms beyond Wanda?
- Basis in paper: [explicit] The paper states that "Shears could utilize other algorithms, e.g., movement sparsity (Sanh et al., 2020) or SparseGPT (Frantar and Alisterh, 2023)" and mentions that "The pruned weights Wp are kept frozen throughout the subsequent stages of the overall pipeline."
- Why unresolved: The experiments only use the Wanda algorithm, leaving the performance comparison with other sparsification methods unexplored.
- What evidence would resolve it: Conducting experiments using different sparsification algorithms and comparing their impact on accuracy and sparsity levels would provide insights into the flexibility and performance of Shears with various pruning methods.

### Open Question 2
- Question: What is the impact of the search algorithm choice on the final sub-adapter configuration quality and efficiency?
- Basis in paper: [explicit] The paper mentions that "Identifying an optimal sub-adapter configuration can be an expensive endeavor" and discusses using heuristics and hill-climbing as alternatives to more costly methods like evolutionary search.
- Why unresolved: The paper only provides results using a heuristic followed by hill-climbing but does not explore other search algorithms or compare their effectiveness and efficiency.
- What evidence would resolve it: Comparing the performance of sub-adapter configurations obtained using different search algorithms (e.g., evolutionary search, reinforcement learning-based approaches) would clarify the trade-offs between search quality and computational cost.

### Open Question 3
- Question: How does Shears perform on tasks outside of math reasoning and commonsense reasoning, such as code generation or multi-modal tasks?
- Basis in paper: [inferred] The paper focuses on math reasoning and commonsense reasoning tasks, but does not explore other domains where LLMs are commonly applied.
- Why unresolved: The experiments are limited to specific task types, and there is no evidence of Shears' effectiveness on other types of tasks.
- What evidence would resolve it: Evaluating Shears on a diverse set of tasks, including code generation, summarization, and multi-modal tasks, would demonstrate its generalizability and potential limitations across different domains.

## Limitations

- The effectiveness of the hill-climbing search algorithm in finding optimal sub-adapter configurations is not fully validated, particularly for larger search spaces or more complex tasks
- The trade-off between sparsity level and accuracy across different model architectures and datasets needs more extensive testing
- The computational overhead of the super-adapter training phase versus the benefits gained from the subsequent search process is not quantified

## Confidence

- High Confidence: The sparsification mechanism using Wanda and its implementation details
- Medium Confidence: The overall architecture combining sparsity with adapter search and its ability to maintain accuracy at high sparsity levels
- Low Confidence: The specific effectiveness of the hill-climbing algorithm in finding optimal configurations and its generalizability to different tasks

## Next Checks

1. Test the hill-climbing search algorithm on progressively larger search spaces to determine the point at which it becomes ineffective or computationally prohibitive
2. Conduct ablation studies removing the sparsification step to quantify its contribution to overall performance improvements
3. Evaluate the approach on additional model architectures (e.g., OPT, Bloom) and task types (e.g., text generation, summarization) to assess generalizability