---
ver: rpa2
title: Wavelet-Inspired Multiscale Graph Convolutional Recurrent Network for Traffic
  Forecasting
arxiv_id: '2401.06040'
source_url: https://arxiv.org/abs/2401.06040
tags:
- graph
- traffic
- forecasting
- data
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of traffic forecasting by proposing
  a Wavelet-Inspired Graph Convolutional Recurrent Network (WavGCRN) that combines
  multiscale analysis with deep learning methods. The key innovation is using Discrete
  Wavelet Transformation (DWT) to decompose traffic data into time-frequency components,
  creating a multi-stream input structure.
---

# Wavelet-Inspired Multiscale Graph Convolutional Recurrent Network for Traffic Forecasting

## Quick Facts
- arXiv ID: 2401.06040
- Source URL: https://arxiv.org/abs/2401.06040
- Authors: Qipeng Qian; Tanwi Mallick
- Reference count: 23
- Best results: MAE of 2.67, RMSE of 5.13, MAPE of 6.76% for 3-step ahead forecasting on METR-LA dataset

## Executive Summary
This paper addresses the problem of traffic forecasting by proposing a Wavelet-Inspired Graph Convolutional Recurrent Network (WavGCRN) that combines multiscale analysis with deep learning methods. The key innovation is using Discrete Wavelet Transformation (DWT) to decompose traffic data into time-frequency components, creating a multi-stream input structure. Graph Convolutional Recurrent Networks (GCRNs) are then employed as encoders for each stream to extract spatiotemporal features at different scales. A learnable Inverse DWT (LIDWT) and GCRN are combined as the decoder to fuse information from all streams for traffic metrics reconstruction and prediction. The method also incorporates both road-network-informed graphs and data-driven graph learning to capture spatial correlation. The WavGCRN achieves superior performance compared to state-of-the-art methods.

## Method Summary
The WavGCRN model combines Discrete Wavelet Transformation (DWT) for multi-stream input construction, Graph Convolutional Recurrent Networks (GCRNs) as encoders for each stream, and Learnable Inverse DWT (LIDWT) as a fusion decoder. The model also incorporates graph learning using both road-network-informed graphs and data-driven optimization-based methods. The method uses two real-world datasets: METR-LA and PEMS-BAY, with 5-minute average traffic flow speeds. The model is trained using Mean Absolute Error (MAE) as the loss function, with curriculum learning and scheduled sampling strategies, and evaluated using MAE, RMSE, and MAPE metrics for different forecasting horizons.

## Key Results
- WavGCRN achieves MAE of 2.67, RMSE of 5.13, and MAPE of 6.76% for 3-step ahead forecasting on METR-LA dataset
- Outperforms state-of-the-art methods including Graph WaveNet, STGCN, and DCRNN
- Shows consistent improvement across all evaluation metrics (MAE, RMSE, MAPE) and forecasting horizons (3, 6, and 12 time steps)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: DWT-based multi-stream decomposition improves feature learning by decoupling multiscale traffic patterns
- Mechanism: The Discrete Wavelet Transform (DWT) separates traffic signals into low-frequency (slow-varying) and high-frequency (fine-grained) components, creating multiple streams that allow GCRNs to specialize in different temporal scales
- Core assumption: Traffic data contains distinct multiscale temporal patterns that can be better learned when separated
- Evidence anchors:
  - [abstract]: "DWT-based signal decomposition can decouple traffic information, augmenting the power of data-driven DL and helping to uncover the implicit relationship buried in the data"
  - [section 2.1]: "MSA is a good choice to decouple the spatiotemporal structure by either spatial-multiscale or temporal-multiscale decomposition. In general, the low-frequency signal components are able to capture slow-varying character while the high-frequency ones embody fine-grained changes"
  - [corpus]: Weak evidence - no direct corpus support found for this specific multiscale decomposition claim
- Break condition: If traffic patterns don't exhibit clear multiscale structure, or if DWT decomposition introduces noise that overwhelms signal separation benefits

### Mechanism 2
- Claim: Learnable Inverse DWT (LIDWT) enables effective multi-scale feature fusion
- Mechanism: LIDWT acts as a fusion decoder that aggregates encoded features from different scales while maintaining consistency with the DWT decomposition process, allowing learned coefficients to optimize reconstruction
- Core assumption: The inverse transformation can be parameterized to learn optimal fusion weights while preserving wavelet-like reconstruction properties
- Evidence anchors:
  - [abstract]: "the learnable Inversed DWT and GCRN are combined as the decoder, fusing the information from all streams for traffic metrics reconstruction and prediction"
  - [section 2.3]: "LIDWT aggregates the outputs of all encoder streams... This architecture not only aggregates learned features from different encoder streams but simulates the transformation and inverse transformation processes of wavelet as well"
  - [corpus]: Weak evidence - no direct corpus support found for learnable inverse DWT in traffic forecasting
- Break condition: If learned reconstruction coefficients fail to converge or produce unstable predictions during training

### Mechanism 3
- Claim: Combined road-network-informed and data-driven graph learning captures both physical and learned spatial correlations
- Mechanism: The method combines a prior road-network graph with data-driven learned graphs using NOTEARS optimization, allowing both physics-informed structure and data-adaptive correlations to influence spatial modeling
- Core assumption: Traffic correlations have both physical infrastructure constraints and data-driven patterns that can be jointly modeled
- Evidence anchors:
  - [section 2.5]: "A = γAdata + (1 − γ)Aroad... NOTEARS estimates the structure of a directed acyclic graph (DAG) A by minimize Score (A; X) = 1/2B ||X − XA||2 F + λ||A||1, subject to tr (eA⊗A) − N = 0"
  - [section 2.5]: "we propose a new graph learning method that combines these two kinds of methods together, which simultaneously utilize both physics-informed and data-driven information"
  - [corpus]: Weak evidence - no direct corpus support found for this specific combined graph learning approach
- Break condition: If NOTEARS optimization fails to find meaningful graph structure, or if physical road network constraints dominate and prevent learning adaptive correlations

## Foundational Learning

- Concept: Discrete Wavelet Transform (DWT)
  - Why needed here: DWT provides the multiscale decomposition that enables the multi-stream architecture by separating traffic signals into different frequency components
  - Quick check question: What are the approximate and detailed coefficients in DWT, and how do they relate to low and high frequency components?

- Concept: Graph Convolutional Networks (GCNs)
  - Why needed here: GCNs process spatial dependencies between sensors in the road network, with different graphs learned for each frequency stream
  - Quick check question: How does the K-hop graph convolution formula H(k) = αHin + β ÃH(k-1) propagate information across the graph structure?

- Concept: Graph Neural Networks with Recurrent Networks (GCRN)
  - Why needed here: GCRNs combine spatial graph convolutions with temporal RNNs to capture spatiotemporal features at different scales
  - Quick check question: What modifications are made to the GRU equations to incorporate graph convolutions instead of standard matrix multiplications?

## Architecture Onboarding

- Component map: Input → DWT (multi-stream decomposition) → GCRN encoders (one per stream) → LIDWT fusion decoder → Output prediction
- Critical path: DWT → Multi-stream GCRNs → LIDWT → GCRN decoder → Prediction
- Design tradeoffs: Multiscale decomposition vs. computational complexity, physics-informed vs. data-driven graphs vs. model flexibility
- Failure signatures: Poor DWT decomposition quality, unstable LIDWT learning, graph learning convergence issues, spatiotemporal feature misalignment
- First 3 experiments:
  1. Test single-scale DWT (only one decomposition level) to verify baseline performance before full multiscale implementation
  2. Validate graph learning by comparing performance with only road-network graph vs. learned graph vs. combined graph
  3. Test LIDWT fusion with fixed coefficients vs. learnable coefficients to measure benefit of adaptive fusion

## Open Questions the Paper Calls Out
None

## Limitations
- The method's reliance on wavelet decomposition introduces computational overhead and may not generalize well to non-stationary traffic patterns
- The combined graph learning approach using NOTEARS optimization may be sensitive to hyperparameter tuning, particularly the balance parameter γ
- The model's performance on datasets with different temporal resolutions or sensor densities remains unverified

## Confidence
- **High Confidence**: The core architecture combining multiscale wavelet decomposition with GCRN encoders and decoders is technically sound and the overall methodology is well-established
- **Medium Confidence**: The claims about improved performance over state-of-the-art methods are supported by experimental results, though the specific performance gains may vary with different datasets or configurations
- **Low Confidence**: The effectiveness of the learnable inverse DWT (LIDWT) component and the combined graph learning approach have limited theoretical justification and no direct corpus support

## Next Checks
1. **Ablation Study on Wavelet Decomposition**: Test the model with different wavelet decomposition levels (1-level, 2-level, 3-level) to determine the optimal number of scales and verify that multiscale decomposition provides measurable benefits over single-scale approaches

2. **Graph Learning Sensitivity Analysis**: Conduct experiments varying the balance parameter γ between road-network-informed and data-driven graphs (e.g., γ = 0.0, 0.5, 0.7, 1.0) to quantify the contribution of each graph type and test model robustness to this hyperparameter

3. **Cross-Dataset Generalization**: Evaluate the pretrained model on an entirely different traffic dataset with different characteristics (different city, different temporal resolution, different sensor density) to assess generalization capabilities beyond the METR-LA and PEMS-BAY datasets used in training