---
ver: rpa2
title: 'Thompson Sampling For Combinatorial Bandits: Polynomial Regret and Mismatched
  Sampling Paradox'
arxiv_id: '2410.05441'
source_url: https://arxiv.org/abs/2410.05441
tags:
- have
- regret
- should
- thompson
- answer
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proposes a new variant of Thompson Sampling (TS) for
  linear combinatorial semi-bandits with subgaussian rewards, achieving polynomial
  regret bounds instead of the previously exponential ones. The key innovation is
  introducing a carefully chosen exploration boost that improves finite-time behavior,
  especially when the time horizon is moderate and the decision size is large.
---

# Thompson Sampling For Combinatorial Bandits: Polynomial Regret and Mismatched Sampling Paradox

## Quick Facts
- arXiv ID: 2410.05441
- Source URL: https://arxiv.org/abs/2410.05441
- Reference count: 40
- Achieves polynomial regret bounds for linear combinatorial semi-bandits instead of previously exponential bounds

## Executive Summary
This paper addresses the fundamental challenge of Thompson Sampling (TS) in linear combinatorial semi-bandits, where standard TS algorithms suffer from exponential regret due to premature exploration collapse. The authors propose BG-CTS, a novel TS variant with an exploration boost mechanism that maintains polynomial regret bounds. The key insight is that adding a carefully designed variance term to the Thompson samples prevents the algorithm from committing to suboptimal actions too early, especially in regimes with moderate time horizons and large decision sizes. The paper also reveals a counterintuitive "mismatched sampling paradox" where using an incorrect likelihood (Gaussian) can outperform the correct likelihood (Bernoulli), even when the reward distributions are known.

## Method Summary
The method introduces BG-CTS, which maintains Gaussian posteriors over the unknown parameter vector with an added exploration boost term `gpt` that scales with time. This boost prevents Thompson sample variance from collapsing prematurely while decaying appropriately for long-term exploitation. The algorithm uses a Bayesian inference engine with Gaussian likelihood and improper uniform prior, incorporating observations to update sufficient statistics and adjust the exploration level through the variance boost calculation. The mismatched sampling paradox is demonstrated by comparing BG-CTS (Gaussian likelihood) with natural TS variants using Bernoulli likelihood, showing that the mismatched version can maintain better exploration properties.

## Key Results
- Proposes BG-CTS achieving regret upper bounded by O(σ²d ln m / ∆min ln T + ...), polynomial in problem dimension m
- Demonstrates mismatched sampling paradox where Gaussian TS outperforms Bernoulli TS on Bernoulli rewards
- Shows BG-CTS significantly outperforms Beta-CTS and ESCB on a two-action Bernoulli example with 10^4 time steps
- Provides theoretical analysis proving polynomial regret through clean run arguments with high probability

## Why This Works (Mechanism)

### Mechanism 1
The exploration boost `gpt` in BG-CTS ensures polynomial regret by maintaining sufficient Thompson sample variance, preventing premature convergence to suboptimal actions. The algorithm adds a time-varying variance term `2gpt`σ²V(t) to the Gaussian posterior, where `gpt` asymptotically behaves like a constant but is larger for moderate T. This increased variance keeps the Thompson samples exploratory even when empirical means are close to optimal, avoiding the exponential regret of standard TS. Core assumption: The variance boost `gpt` must be large enough to prevent Thompson samples from collapsing around the empirical mean before sufficient exploration occurs, but small enough to not overwhelm the signal from actual rewards.

### Mechanism 2
The mismatched sampling paradox occurs because mismatched TS (Gaussian likelihood with non-Gaussian rewards) can achieve better regret than natural TS by maintaining appropriate exploration levels. When using a mismatched likelihood (Gaussian) with improper prior, the posterior variance doesn't collapse as quickly as with the correct likelihood (Bernoulli), maintaining exploration longer. This prevents the algorithm from committing to suboptimal actions based on early noisy observations. Core assumption: The mismatched likelihood must provide sufficient posterior variance to maintain exploration without being so mismatched that it completely destroys the signal from the rewards.

### Mechanism 3
Clean runs (events where Thompson samples maintain appropriate confidence bounds) occur with high probability, ensuring the algorithm explores sufficiently. The algorithm defines event At where Thompson samples stay within confidence bounds and optimal action is sampled often enough. Proposition 2 shows these clean runs occur with probability approaching 1 as t increases, by controlling deviations in empirical means, Thompson sample randomness, and exploration levels. Core assumption: The concentration inequalities used to bound deviations (lemma 7, 5, 6) hold for the subgaussian reward assumption and the algorithm's exploration mechanism.

## Foundational Learning

- Concept: Subgaussian random variables and concentration inequalities
  - Why needed here: The regret analysis relies on Hoeffding-style concentration for subgaussian rewards to bound deviations of empirical means from true values
  - Quick check question: Why does the subgaussian assumption allow us to use the concentration inequality from lemma 7 rather than needing a more general bound?

- Concept: Thompson Sampling posterior sampling mechanism
  - Why needed here: Understanding how Thompson Sampling works with different likelihoods and priors is crucial for grasping why mismatched sampling can outperform natural sampling
  - Quick check question: How does the Gaussian likelihood with improper prior maintain exploration differently than the Beta prior with Bernoulli likelihood?

- Concept: Combinatorial semi-bandit feedback structure
  - Why needed here: The regret bounds depend on the structure of the action set (cardinality |A|, maximum action size m) and how observations provide feedback on individual items
  - Quick check question: Why does the regret bound scale polynomially with m in BG-CTS but exponentially in standard TS?

## Architecture Onboarding

- Component map: Bayesian inference engine -> Action selection module -> Observation processing -> Posterior update -> Variance boost calculation -> Next action selection
- Critical path: Action selection → Environment interaction → Observation processing → Posterior update → Variance boost calculation → Next action selection. The variance boost calculation (gpt) is critical as it directly controls the exploration-exploitation tradeoff.
- Design tradeoffs: The algorithm trades off between maintaining sufficient exploration (requiring larger gpt) and efficient learning (requiring gpt to decay appropriately). Too much exploration leads to linear regret, too little leads to exponential regret.
- Failure signatures: (1) Linear regret growth indicates insufficient exploration (gpt too small), (2) High variance in final regret indicates sensitivity to initialization or random seed, (3) Failure to converge suggests mismatched likelihood is too far from true distribution.
- First 3 experiments:
  1. Replicate the two-action Bernoulli example from section 6 to verify polynomial vs exponential regret scaling
  2. Test sensitivity to the λ parameter in gpt by running with different values and measuring regret
  3. Compare BG-CTS with standard TS on a larger combinatorial problem to verify polynomial scaling with m

## Open Questions the Paper Calls Out

### Open Question 1
Under what conditions does the mismatched sampling paradox provide the largest performance gains? The paper demonstrates the paradox theoretically and through numerical experiments, showing that mismatched TS (e.g., Gaussian likelihood for Bernoulli rewards) can outperform natural TS by several orders of magnitude. This remains unresolved because the paper shows the paradox exists but does not systematically characterize when the performance gap is maximized or what factors most influence the effect.

### Open Question 2
Can the BG-CTS algorithm be extended to handle non-linear combinatorial semi-bandits while maintaining polynomial regret bounds? The paper focuses on linear combinatorial semi-bandits with subgaussian rewards. Extending the algorithm and analysis to non-linear settings is not addressed because non-linear reward structures are common in practice, but the current analysis relies heavily on the linearity assumption and associated concentration inequalities.

### Open Question 3
How does the BG-CTS algorithm perform in practice compared to other state-of-the-art algorithms for combinatorial bandits beyond the simple two-action example? The paper includes numerical experiments comparing BG-CTS to Beta-based TS and ESCB on a simple two-action Bernoulli reward example, showing significant improvements. This remains unresolved because the experiments are limited to a specific, simple setting, and it is unclear how BG-CTS scales or compares to other algorithms in more complex or realistic combinatorial bandit problems.

## Limitations

- Limited empirical validation on problems beyond the simple two-action example
- Missing robustness analysis for reward distributions that weakly violate subgaussian assumptions
- No discussion of computational complexity for large-scale combinatorial problems

## Confidence

- High confidence: The polynomial regret bound for BG-CTS under the stated assumptions is well-supported by the theoretical analysis
- Medium confidence: The mismatched sampling paradox is demonstrated on the specific two-action example, but its generalizability to other problem structures requires further validation
- Low confidence: The practical performance advantages of BG-CTS over existing methods on realistic combinatorial bandit problems remain to be demonstrated

## Next Checks

1. Test BG-CTS on combinatorial problems with varying action set sizes (|A| from 10 to 1000) and action sizes (m from 2 to 50) to verify the polynomial scaling with m holds in practice.

2. Evaluate BG-CTS performance when the reward distribution deviates from subgaussian assumptions (e.g., heavy-tailed distributions) and when using different mismatched likelihoods (beyond Gaussian).

3. Systematically vary the exploration boost parameter λ across multiple orders of magnitude to identify regimes where BG-CTS outperforms standard TS and where it fails, establishing practical guidelines for parameter selection.