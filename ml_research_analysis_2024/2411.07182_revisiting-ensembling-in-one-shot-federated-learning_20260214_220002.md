---
ver: rpa2
title: Revisiting Ensembling in One-Shot Federated Learning
arxiv_id: '2411.07182'
source_url: https://arxiv.org/abs/2411.07182
tags:
- fens
- learning
- accuracy
- client
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: FENS introduces a novel federated ensembling scheme that bridges
  the accuracy gap between one-shot federated learning (OFL) and standard federated
  learning (FL) by introducing a trainable prediction aggregator. In the first phase,
  clients locally train models and upload them to the server, similar to OFL.
---

# Revisiting Ensembling in One-Shot Federated Learning

## Quick Facts
- arXiv ID: 2411.07182
- Source URL: https://arxiv.org/abs/2411.07182
- Authors: Youssef Allouah; Akash Dhasade; Rachid Guerraoui; Nirupam Gupta; Anne-Marie Kermarrec; Rafael Pinot; Rafael Pires; Rishi Sharma
- Reference count: 40
- One-line primary result: FENS achieves up to 26.9% higher accuracy than state-of-the-art OFL methods, only 3.1% lower than FL

## Executive Summary
FENS introduces a novel federated ensembling scheme that bridges the accuracy gap between one-shot federated learning (OFL) and standard federated learning (FL) by introducing a trainable prediction aggregator. In the first phase, clients locally train models and upload them to the server, similar to OFL. In the second phase, an aggregator model is trained using FL to combine the client models, enabling better performance under high data heterogeneity. Experiments on multiple datasets show FENS achieves up to 26.9% higher accuracy than state-of-the-art OFL methods, while only being 3.1% lower than FL, with a modest 4.3× communication cost increase over OFL versus 10.9× for FL.

## Method Summary
FENS is a two-phase federated learning approach. In Phase 1, clients locally train models on their data and upload them to the server (one-shot communication). In Phase 2, the server trains a shallow neural network aggregator using federated learning to combine the client models' logits. The final prediction is made by passing the concatenated logits through this aggregator. The method is designed to work well under high data heterogeneity when clients have sufficiently large local datasets. FENS can use either a neural network aggregator or weighted averaging of logits per class per client.

## Key Results
- FENS achieves up to 26.9% higher accuracy than state-of-the-art OFL methods
- FENS is only 3.1% lower in accuracy than standard FL while using 4.3× more communication than OFL
- FENS outperforms FL by up to 1.6% on certain datasets under high heterogeneity

## Why This Works (Mechanism)

### Mechanism 1
The trainable prediction aggregator in FENS effectively bridges the accuracy gap by learning to combine client models' logits in a way that corrects their individual biases. After clients upload their locally trained models, the server trains a shallow neural network (or weighted aggregation) on the concatenated logits from all clients using federated learning. This aggregator learns a non-linear combination that improves ensemble performance beyond simple averaging or voting. The core assumption is that a shallow neural network with few parameters can learn a sufficiently expressive aggregation function to correct for data heterogeneity without overfitting.

### Mechanism 2
FENS achieves high accuracy under high data heterogeneity when clients have large local datasets. Large local datasets enable clients to train high-quality local models. The aggregator then combines these diverse, well-trained models, leveraging their complementary strengths to achieve FL-like accuracy. The performance gain from combining diverse high-quality local models outweighs the loss from not having a shared global model. This mechanism assumes that the diversity of well-trained local models can be effectively leveraged through aggregation.

### Mechanism 3
The two-phase design of FENS provides a favorable accuracy vs. communication cost tradeoff. Phase 1 (OFL) limits communication to one model upload per client. Phase 2 trains only the aggregator parameters via federated learning, which is lightweight due to the small size of the aggregator model. Training a shallow aggregator is much cheaper in communication than training full client models, making the second phase feasible. The communication cost advantage disappears if the aggregator model becomes too large or complex.

## Foundational Learning

- **Concept: Ensemble methods and stacked generalization**
  - Why needed here: FENS builds upon the idea of stacked generalization, where a meta-learner (the aggregator) learns to combine base learners (client models) to improve overall performance.
  - Quick check question: Can you explain how stacked generalization differs from simple model averaging in an ensemble?

- **Concept: Federated learning and communication efficiency**
  - Why needed here: Understanding the communication cost tradeoffs between standard FL (iterative), OFL (one-shot), and FENS (hybrid) is crucial to appreciating the design choices.
  - Quick check question: What is the primary communication bottleneck in standard federated learning, and how does OFL address it?

- **Concept: Data heterogeneity in federated learning**
  - Why needed here: FENS is specifically designed to perform well under high data heterogeneity, where client datasets have different label distributions.
  - Quick check question: How does data heterogeneity affect the performance of standard federated learning algorithms like FedAvg?

## Architecture Onboarding

- **Component map**: Clients (local training -> model upload) -> Server (aggregator training) -> Aggregator (neural network or weighted combination) -> Final prediction

- **Critical path**: 
  1. Clients train local models on their data (Phase 1)
  2. Clients upload models to server
  3. Server trains aggregator using federated learning (Phase 2)
  4. Final global model = Aggregator(local models)

- **Design tradeoffs**:
  - Aggregator complexity vs. communication cost: Deeper aggregators may perform better but increase communication.
  - Local training budget vs. aggregator quality: More local training epochs improve client models but increase Phase 1 cost.
  - Model quantization: Reduces communication and memory but may slightly hurt accuracy.

- **Failure signatures**:
  - Low accuracy despite high communication: Aggregator not learning meaningful combinations.
  - Communication costs similar to FL: Aggregator model too large or complex.
  - Poor performance under low heterogeneity: FENS not needed; standard FL suffices.

- **First 3 experiments**:
  1. Reproduce CIFAR-10 accuracy comparison between FENS and best OFL baseline across heterogeneity levels.
  2. Measure communication cost breakdown between Phase 1 (model upload) and Phase 2 (aggregator training).
  3. Ablation study: Replace neural network aggregator with weighted averaging and observe accuracy drop.

## Open Questions the Paper Calls Out

The paper doesn't explicitly call out open questions, but based on the content, several implicit open questions emerge:

### Open Question 1
- Question: What is the minimum size of the aggregator model that still achieves near-FL performance?
- Basis in paper: [explicit] The paper experiments with different aggregator models including a 2-layer perceptron and per-class per-client weight learning.
- Why unresolved: The paper shows these models work well but doesn't systematically explore the minimum viable model size or architecture.
- What evidence would resolve it: Systematic experiments varying aggregator model size and complexity to find the smallest architecture achieving target accuracy.

### Open Question 2
- Question: How does FENS perform on non-image data types like time series or graphs?
- Basis in paper: [inferred] The paper only evaluates on vision and language datasets (CIFAR, SVHN, AG-News), leaving other data modalities unexplored.
- Why unresolved: The paper doesn't test FENS on non-image/non-language domains where different data structures might affect aggregator performance.
- What evidence would resolve it: Experiments applying FENS to time series, graph, or other non-image datasets with varying heterogeneity levels.

### Open Question 3
- Question: What is the theoretical limit of accuracy improvement achievable through aggregator training in OFL?
- Basis in paper: [explicit] The paper shows empirical improvements but doesn't provide theoretical bounds on how much better ensembles can get with trainable aggregators.
- Why unresolved: No theoretical analysis of the gap between OFL and FL, or bounds on how much aggregator training can close this gap.
- What evidence would resolve it: Theoretical analysis deriving bounds on accuracy achievable through ensemble aggregation in heterogeneous settings.

## Limitations

- **Limited ablation on aggregator design**: While the paper shows neural network aggregators outperform simple weighted averaging, it lacks comparison with other aggregator architectures (e.g., attention mechanisms, deeper networks) or alternative ensembling strategies.
- **Quantization details unclear**: The post-training quantization from FP32 to INT8 is mentioned but lacks implementation specifics (quantization method, precision, library/API), making exact reproduction difficult.
- **Auxiliary dataset specifics**: For FENSdistilled, the server-side distillation process using an auxiliary dataset is not fully specified, including dataset size, composition, or acquisition method.

## Confidence

- **High confidence**: The core claim that FENS achieves state-of-the-art OFL accuracy with modest communication cost increase is well-supported by experimental results across multiple datasets and heterogeneity levels.
- **Medium confidence**: The mechanism explaining why the trainable aggregator works (correcting client model biases) is plausible but not directly validated through extensive ablation or analysis of learned aggregator weights.
- **Low confidence**: The claim about FENS being a practical alternative specifically under "high heterogeneity and sufficiently large local datasets" lacks quantitative thresholds for "large" datasets and rigorous testing across diverse dataset sizes.

## Next Checks

1. **Ablation on aggregator architecture**: Compare FENS with different aggregator designs (e.g., deeper networks, attention mechanisms, or alternative ensembling methods) to isolate the contribution of the neural network aggregator beyond simple weighted averaging.
2. **Communication cost breakdown**: Measure and report the exact communication cost contributions from Phase 1 (model upload) and Phase 2 (aggregator training) separately to verify the claimed 4.3× increase over OFL.
3. **Dataset size sensitivity**: Systematically vary client dataset sizes and quantify the performance threshold where FENS transitions from being beneficial to being comparable to or worse than standard OFL, validating the "sufficiently large local datasets" claim.