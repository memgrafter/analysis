---
ver: rpa2
title: "Advancing Generative AI for Portuguese with Open Decoder Gerv\xE1sio PT*"
arxiv_id: '2402.18766'
source_url: https://arxiv.org/abs/2402.18766
tags:
- language
- portuguese
- gerv
- llama
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "The paper introduces Gerv\xE1sio PT, a fully open, instruction-tuned,\
  \ Transformer-based decoder model for Portuguese. It improves on LLaMA 2 7B using\
  \ new Portuguese instruction datasets."
---

# Advancing Generative AI for Portuguese with Open Decoder Gervásio PT*

## Quick Facts
- **arXiv ID**: 2402.18766
- **Source URL**: https://arxiv.org/abs/2402.18766
- **Reference count**: 0
- **Primary result**: State-of-the-art 7B decoder for Portuguese, outperforming LLaMA 2 and Sabiá-7B on multiple benchmarks

## Executive Summary
This paper introduces Gervásio PT*, a family of fully open, instruction-tuned Transformer-based decoder models for Portuguese, covering both European (PTPT) and Brazilian (PTBR) variants. Built by continuing the training of LLaMA 2 7B with new Portuguese instruction datasets and fine-tuning on GLUE/SuperGLUE tasks translated to Portuguese, Gervásio PT* achieves state-of-the-art performance among open 7B decoders for Portuguese. The models are fully open source under MIT license and can run on consumer hardware, marking the first open decoder of its class for European Portuguese.

## Method Summary
Gervásio PT* is developed by taking LLaMA 2 7B as a starting point and continuing its training with additional Portuguese instruction datasets, including machine-translated GLUE and SuperGLUE tasks and other Portuguese datasets. The model uses supervised fine-tuning with causal language modeling (CLM), applying zero-shot and few-shot instruction templates. Separate models are trained for European Portuguese (PTPT) and Brazilian Portuguese (PTBR), using a Portuguese tokenizer with BPE and 32k vocabulary. Training involves a 512 sequence length, batch size 16, learning rate 2×10⁻⁵, weight decay 0.1, and 2 epochs.

## Key Results
- Gervásio PT* outperforms LLaMA 2 and Sabiá-7B on multiple Portuguese benchmarks including ASSIN2 RTE, ASSIN2 STS, BLUEX, ENEM 2022, and FaQuAD.
- The European Portuguese (PTPT) model is the first open decoder of its class for this variant.
- Both PTPT and PTBR models excel at sentence comparison tasks and demonstrate strong zero/few-shot generalization.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Continued CLM training on Portuguese data improves performance over English-only LLaMA 2.
- Mechanism: LLaMA 2's general architecture retains strong language modeling capability; fine-tuning with Portuguese-only data adapts it to Portuguese linguistic patterns without catastrophic forgetting.
- Core assumption: Portuguese data is sufficient to induce language-specific adaptations while preserving prior knowledge.
- Evidence anchors:
  - [abstract] "a strong LLaMA 2 7B model was used as a starting point, and its further improvement through additional training was done over language resources that include new instruction data sets of Portuguese"
  - [section 2] "Research has shown that when such training is appropriately continued, the performance of the resulting model for that specific language exceeds the performance of the baseline model on that language"
  - [corpus] Weak: corpus shows only 1.8B Portuguese tokens vs 2T English in base model; assumption is that this small proportion is still effective for adaptation.
- Break condition: If Portuguese dataset is too small or too different from training distribution, adaptation fails or causes catastrophic forgetting.

### Mechanism 2
- Claim: Instruction tuning on GLUE/SuperGLUE tasks adapted to Portuguese yields better zero/few-shot performance than standard fine-tuning.
- Mechanism: Instruction-formatted prompts align model outputs with human expectations for downstream tasks, improving generalization to unseen tasks.
- Core assumption: Portuguese translations preserve task semantics and linguistic properties.
- Evidence anchors:
  - [abstract] "its further improvement through additional training was done over language resources that include new instruction data sets of Portuguese"
  - [section 3.1] "we resorted to tasks and respective datasets in the GLUE (Wang et al., 2018) and the SuperGLUE (Wang et al., 2019) collections" and "machine translated into Portuguese"
  - [corpus] Weak: no explicit evaluation of translation quality; assumption is DeepL preserves task semantics.
- Break condition: If translations distort task semantics, instruction tuning becomes ineffective.

### Mechanism 3
- Claim: Separate European and Brazilian Portuguese models outperform a single multilingual model.
- Mechanism: Language variants have distinct vocabulary, grammar, and usage patterns; dedicated models can specialize without interference.
- Core assumption: Portuguese variants differ enough to justify separate models.
- Evidence anchors:
  - [abstract] "covers both European and Brazilian Portuguese variants"
  - [section 2] "The model is available at https://huggingface.co/PORTULAN" and "the model for the European variant it is the first of its class"
  - [corpus] Weak: corpus neighbors show related work on Portuguese variants but no direct comparative evidence for separate vs joint training.
- Break condition: If variants are too similar, separate models waste resources; if too different, joint model suffers from interference.

## Foundational Learning

- Concept: Transformer decoder architecture
  - Why needed here: Gervásio PT* is a decoder-only model; understanding self-attention, causal masking, and autoregressive generation is essential for training and inference.
  - Quick check question: What is the purpose of causal masking in a decoder-only transformer?
- Concept: Instruction tuning vs standard fine-tuning
  - Why needed here: Gervásio uses instruction datasets; knowing how prompts, few-shot examples, and response tokens differ from standard supervised fine-tuning is key to reproducing the training pipeline.
  - Quick check question: How does the zero-out technique change which tokens receive gradients during instruction tuning?
- Concept: Cross-lingual transfer and continued pre-training
  - Why needed here: LLaMA 2 was trained on English; understanding how continued pre-training on a smaller target-language corpus can improve performance is critical for dataset design and training decisions.
  - Quick check question: Why might continued pre-training on 1.8B Portuguese tokens still improve over a 2T English pre-trained model?

## Architecture Onboarding

- Component map: Base LLaMA 2 7B decoder → Portuguese tokenizer (BPE, vocab=32k) → Instruction-formatted training datasets (GLUE/SuperGLUE + custom) → Supervised fine-tuning (CLM) → Separate PTPT and PTBR models
- Critical path: Data preparation (translation + augmentation) → Tokenizer adaptation (padding support) → Training loop (zero-out, 512 seq len, 16 batch, 16 acc steps) → Evaluation (zero/few-shot on held-out tasks)
- Design tradeoffs: Separate models for each variant vs single multilingual model; 512 vs 4096 seq len due to hardware; zero-out vs full-prompt backprop
- Failure signatures: Poor performance on translation tasks indicates dataset/translation issues; large loss variance suggests catastrophic forgetting; slow convergence may mean insufficient Portuguese data
- First 3 experiments:
  1. Run zero-shot inference on a translated MRPC example to verify prompt formatting and tokenization.
  2. Train for 1 epoch on STS-B only and check loss curve for adaptation vs forgetting.
  3. Compare zero-shot vs few-shot performance on a small held-out BoolQ sample to confirm instruction tuning benefits.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of Gervásio PT* models differ when trained on monolingual Portuguese data versus multilingual data that includes English?
- Basis in paper: [inferred] The paper discusses the benefits of continuing LLaMA 2 training with Portuguese data and notes that LLaMA 2 was trained on a majority of English data. It would be valuable to understand if monolingual Portuguese training data leads to better performance than multilingual data.
- Why unresolved: The paper does not compare the performance of Gervásio PT* models trained on monolingual Portuguese data versus multilingual data that includes English.
- What evidence would resolve it: Training and evaluating Gervásio PT* models on both monolingual Portuguese data and multilingual data that includes English, and comparing their performance on various tasks.

### Open Question 2
- Question: What are the main ethical considerations when using Gervásio PT* models, and how do they compare to other open-source models like Mistral?
- Basis in paper: [explicit] The paper mentions that Mistral was excluded as a base model due to a lack of ethical safeguards, and notes that Gervásio PT* includes ethical considerations to the extent possible.
- Why unresolved: The paper does not provide a detailed comparison of the ethical considerations of Gervásio PT* models with other open-source models.
- What evidence would resolve it: A comprehensive analysis of the ethical considerations of Gervásio PT* models, including potential biases, and a comparison with other open-source models like Mistral.

### Open Question 3
- Question: How does the performance of Gervásio PT* models compare to other open-source models when evaluated on tasks that require reasoning or complex inference?
- Basis in paper: [inferred] The paper mentions that Gervásio PT* models excel at tasks involving comparing sentences but may not perform as well on tasks requiring question answering or complex inference.
- Why unresolved: The paper does not provide a detailed comparison of Gervásio PT* models with other open-source models on tasks requiring reasoning or complex inference.
- What evidence would resolve it: Evaluating Gervásio PT* models and other open-source models on a range of tasks that require reasoning or complex inference, and comparing their performance.

## Limitations
- Translation quality of GLUE/SuperGLUE tasks to Portuguese is unverified, creating uncertainty about task preservation for instruction tuning.
- Exact composition and coverage of Portuguese training data beyond GLUE/SuperGLUE is unclear.
- No empirical evidence provided for the claimed benefit of separate European vs Brazilian Portuguese models.

## Confidence

**High Confidence**: The claim that continued pre-training on Portuguese data improves performance over English-only LLaMA 2 is well-supported by established research on cross-lingual transfer and has clear theoretical grounding in catastrophic forgetting avoidance.

**Medium Confidence**: The instruction tuning mechanism for improving zero/few-shot performance is plausible given literature on instruction-tuned models, but the specific effectiveness for Portuguese translation tasks depends on unverified translation quality.

**Low Confidence**: The claim that separate models for European and Brazilian Portuguese outperform a joint multilingual model lacks direct comparative evidence and relies on untested assumptions about variant differences.

## Next Checks
1. **Translation Quality Audit**: Manually evaluate the machine-translated versions of 10-15 GLUE/SuperGLUE tasks (particularly RTE, MRPC, and STS-B) by native Portuguese speakers to verify semantic preservation and task fidelity before instruction tuning.

2. **Variant Similarity Analysis**: Compute perplexity and semantic similarity scores on a held-out corpus for European vs Brazilian Portuguese to quantify actual linguistic differences, then run ablation studies comparing separate vs joint training on a small subset of tasks.

3. **Zero-shot Prompt Robustness**: Test Gervásio PT* on zero-shot Portuguese tasks from multiple domains (legal, medical, conversational) with both few-shot and zero-shot prompts to evaluate generalization beyond the GLUE/SuperGLUE benchmark suite used in training.