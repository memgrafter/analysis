---
ver: rpa2
title: Multiple References with Meaningful Variations Improve Literary Machine Translation
arxiv_id: '2412.18707'
source_url: https://arxiv.org/abs/2412.18707
tags:
- source
- medium
- translation
- training
- dataset
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper explores the impact of using multiple reference translations
  with varying semantic similarity on literary machine translation. It finds that
  when fine-tuning large language models, using reference translations with medium
  and high semantic similarity outperforms an unfiltered dataset, even if the unfiltered
  dataset has more training instances.
---

# Multiple References with Meaningful Variations Improve Literary Machine Translation

## Quick Facts
- arXiv ID: 2412.18707
- Source URL: https://arxiv.org/abs/2412.18707
- Reference count: 16
- Primary result: Using multiple reference translations with medium and high semantic similarity improves literary machine translation quality metrics (BLEU +0.3-0.5, COMET +0.1-0.9, chrF++ +0.17-0.32) compared to unfiltered datasets

## Executive Summary
This paper investigates how the semantic similarity of multiple reference translations affects literary machine translation quality. The authors find that using reference translations with medium and high semantic similarity consistently outperforms using all available references, including those with low similarity. They also demonstrate that when total training instances are held constant, using multiple references per source text provides similar performance gains to doubling the number of unique source texts. The study uses fine-tuning of mT5-large and LLaMA-2-7B models on the Par3 dataset, with semantic similarity measured by PARAGRAM-SP scores.

## Method Summary
The authors fine-tune mT5-large and LLaMA-2-7B models on the Par3 dataset using varying configurations: different numbers of source texts, different numbers of references per source text, and references filtered by semantic similarity levels (low, medium, high). They measure semantic similarity between paraphrases using PARAGRAM-SP scores and experiment with datasets containing only medium similarity references, only high similarity references, and combinations of medium and high similarity references. The models are evaluated using BLEU, COMETDA22, and chrF++ metrics.

## Key Results
- Medium and high similarity references outperform unfiltered datasets (+BLEU 0.3-0.5, +COMET 0.2-0.9, +chrF++ 0.25-0.32)
- When holding total training instances constant, multiple references per source marginally outperforms single references with more source texts
- Training with medium and low similarity references performs worse than unfiltered datasets
- Medium and high similarity references provide the best overall performance

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Filtering paraphrases by medium and high semantic similarity improves translation quality
- Mechanism: Medium and high similarity paraphrases preserve core meaning while introducing controlled variation, which enriches training signals without introducing noise from low-similarity paraphrases
- Core assumption: Semantic similarity measured by PARAGRAM-SP correlates with linguistic quality and translatability
- Evidence anchors:
  - [abstract]: "using paraphrases of medium and high semantic similarity outperforms an unfiltered dataset (+BLEU 0.3-0.5, +COMET 0.2-0.9, +chrF++ 0.25-0.32)"
  - [section]: "Using source texts with medium and high semantic similarity among their references outperform an unfiltered dataset and provide the best performance."
  - [corpus]: Weak. Only found general paraphrasing papers, no direct corpus evidence on similarity filtering

### Mechanism 2
- Claim: Multiple reference translations per source improve performance compared to single reference, when total training instances are held constant
- Mechanism: Multiple references per source expose the model to varied linguistic expressions of the same meaning, improving generalization and robustness
- Core assumption: Training with multiple translations of the same source text provides richer learning signals than training with more unique source texts but only one translation each
- Evidence anchors:
  - [abstract]: "holding the total training instances constant, single-reference but more source texts only marginally outperforms multiple-reference with half of the source texts."
  - [section]: "when one has limited source and target texts, augmenting the number of references per source text can improve performance similarly to increasing the number of source text single target text pairs."
  - [corpus]: Weak. Found papers on paraphrase augmentation but none comparing reference counts under fixed total instance count

### Mechanism 3
- Claim: Including too many low-similarity paraphrases degrades performance
- Mechanism: Low-similarity paraphrases introduce semantic drift or noise that confuses the model during fine-tuning
- Core assumption: Low-similarity paraphrases contain meaningful semantic differences that are not appropriate for the same source text
- Evidence anchors:
  - [section]: "training with medium and low semantic similarity among references will perform worse than an unfiltered dataset."
  - [section]: "Too much divergence among references of a source text may create confusion and hinder the performance."
  - [corpus]: Weak. No direct corpus evidence on low-similarity paraphrase noise; inferred from experimental results

## Foundational Learning

- Concept: Semantic textual similarity scoring (e.g., PARAGRAM-SP)
  - Why needed here: To classify paraphrases into low, medium, and high similarity groups for controlled experimentation
  - Quick check question: How does PARAGRAM-SP score range [-1,1] map to meaningful paraphrase categories?

- Concept: Machine translation evaluation metrics (BLEU, COMET, chrF++)
  - Why needed here: To quantify translation quality improvements when varying paraphrase similarity and reference counts
  - Quick check question: Why might chrF++ be more reliable than BLEU for literary text evaluation?

- Concept: Fine-tuning vs. training from scratch for LLMs
  - Why needed here: The paper uses fine-tuning as a more efficient approach given pre-trained language abilities
  - Quick check question: What are the risks of fine-tuning LLMs with noisy or low-quality data?

## Architecture Onboarding

- Component map: Par3 dataset → semantic similarity filtering → train/val/test split → mT5-large/LLaMA-2-7B → LoRA fine-tuning (LLaMA) or standard fine-tuning (mT5) → evaluation
- Critical path: Semantic similarity computation → dataset construction → model fine-tuning → evaluation
- Design tradeoffs:
  - More references per source → richer signals but higher memory usage
  - LoRA vs. full fine-tuning → parameter efficiency vs. flexibility
  - Medium similarity range selection → balance between variation and noise
- Failure signatures:
  - Performance degrades with too many low-similarity paraphrases
  - Unstable results with LLaMA-2-7B across different batch sizes
  - No improvement over single reference when similarity filtering is ineffective
- First 3 experiments:
  1. Reproduce baseline: Fine-tune mT5-large on unfiltered dataset; record BLEU/COMET/chrF++.
  2. Filter by medium similarity only; compare against baseline.
  3. Combine medium + high similarity; compare against both previous runs.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of machine translation models vary when using synthetically generated paraphrases versus human-generated paraphrases?
- Basis in paper: [inferred] The paper focuses on human-generated paraphrases from the Par3 dataset and does not explore synthetically generated paraphrases
- Why unresolved: The study only uses human-generated paraphrases, leaving the effectiveness of synthetic paraphrases unexplored
- What evidence would resolve it: Comparing the performance of models trained on synthetic versus human-generated paraphrases would provide insights into the effectiveness of each type

### Open Question 2
- Question: To what extent does the observation that multiple references marginally outperform single references hold when applied to languages with limited parallel data?
- Basis in paper: [inferred] The study uses languages with sufficient parallel data, but does not explore low-resource languages
- Why unresolved: The study's findings are based on languages with ample data, and the impact on low-resource languages is unknown
- What evidence would resolve it: Conducting experiments with low-resource languages would determine if the findings apply to languages with limited data

### Open Question 3
- Question: How does the choice of semantic similarity threshold impact the performance of machine translation models?
- Basis in paper: [explicit] The paper uses a specific range [0.45-0.85] for meaningful variations, but does not explore other thresholds
- Why unresolved: The study defines a specific range for meaningful variations but does not test other potential thresholds
- What evidence would resolve it: Experimenting with different semantic similarity thresholds would reveal their impact on model performance

## Limitations
- The correlation between PARAGRAM-SP semantic similarity scores and actual translation quality remains unvalidated on external datasets
- The study doesn't establish whether performance improvements represent true gains versus overfitting to evaluation metrics
- The experimental design compares different similarity thresholds without testing robustness to metric choice

## Confidence

- **High confidence**: The finding that medium and high similarity references outperform unfiltered datasets, as this is directly supported by experimental results across multiple metrics
- **Medium confidence**: The claim that multiple references per source provide richer learning signals than single references when total instances are held constant, as this requires more complex interpretation of the experimental setup
- **Low confidence**: The assertion that too many low-similarity paraphrases necessarily degrade performance, as this conclusion is primarily inferred from negative results rather than positive demonstrations

## Next Checks

1. Validate PARAGRAM-SP semantic similarity scores against human judgments of translation quality to establish the metric's reliability for this task
2. Test model performance with gradually decreasing similarity thresholds to identify optimal cutoff points and assess robustness to metric choice
3. Conduct ablation studies varying reference count and similarity thresholds independently to isolate their individual contributions to performance improvements