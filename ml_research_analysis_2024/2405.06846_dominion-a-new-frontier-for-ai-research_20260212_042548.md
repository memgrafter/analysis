---
ver: rpa2
title: 'Dominion: A New Frontier for AI Research'
arxiv_id: '2405.06846'
source_url: https://arxiv.org/abs/2405.06846
tags:
- dominion
- game
- cards
- card
- player
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper presents Dominion as a new benchmark for AI research,
  introducing the Dominion Online Dataset with over 2 million games and developing
  an RL baseline bot. The game's complexity stems from its unique mechanic where each
  game uses a different set of 10 randomly chosen cards from hundreds of options,
  requiring adaptive strategies.
---

# Dominion: A New Frontier for AI Research

## Quick Facts
- arXiv ID: 2405.06846
- Source URL: https://arxiv.org/abs/2405.06846
- Reference count: 40
- Primary result: Introduces Dominion as an AI benchmark with a DQN bot outperforming heuristic bots and achieving 575/1000 win rate against Provincial

## Executive Summary
This paper establishes Dominion as a new benchmark for AI research, introducing the Dominion Online Dataset with over 2 million games and developing an RL baseline bot. The game's complexity stems from its unique mechanic where each game uses a different set of 10 randomly chosen cards from hundreds of options, requiring adaptive strategies. The authors provide a large dataset of 30,000 high-quality base game records from experienced players and implement a DQN-based bot that outperforms common heuristic-based bots and shows competitive performance against the previous strongest bot, Provincial.

## Method Summary
The authors introduce Dominion as an AI research benchmark and develop an RL baseline bot using DQN with Rainbow. The bot uses a small fully connected neural network (two hidden layers, both size 256) and is trained via self-play for approximately 7000 games. The paper provides a large dataset of 2+ million Dominion games from Dominion Online, with 30,000 high-quality base game records used for training and evaluation.

## Key Results
- DQN-based bot outperforms common heuristic-based bots
- Bot achieves 575 out of 1000 win rate against Provincial
- Demonstrates existing RL methods can excel in Dominion's complex, adaptive environment

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The Dominion benchmark is well-suited for AI research because each game uses a different set of 10 randomly chosen cards from hundreds of options, requiring adaptive strategies.
- Mechanism: The random card selection creates a vast space of possible game setups (~6Ã—10^18), forcing agents to learn generalizable strategies rather than memorizing specific game states.
- Core assumption: The complexity of adapting to different card combinations is a valuable challenge for AI systems to tackle.
- Evidence anchors:
  - [abstract] "The game's complexity stems from its unique mechanic where each game uses a different set of 10 randomly chosen cards from hundreds of options, requiring adaptive strategies."
  - [section] "Since each dominion card has a unique rule printed on it, and the set of 10 cards for a game are randomly picked from among hundreds of cards, no two games of Dominion can be approached the same way."
- Break condition: If the number of possible card combinations becomes computationally intractable, or if the random selection doesn't significantly change gameplay dynamics.

### Mechanism 2
- Claim: The Dominion Online Dataset with over 2 million games provides a valuable resource for training and evaluating AI agents.
- Mechanism: The large dataset of human-played games offers diverse gameplay examples and can be used for pretraining RL agents or as a benchmark for evaluating new approaches.
- Core assumption: The quality and diversity of the dataset is sufficient to capture the complexity of Dominion gameplay.
- Evidence anchors:
  - [abstract] "We also present the Dominion Online Dataset, a collection of over 2,000,000 games of Dominion played by experienced players on the Dominion Online webserver."
  - [section] "One of the most popular ways to play dominion, especially given the high rate of self-isolation in the past year, is on the web server Dominion Online. Fortunately for the research community, the developers save game logs for all the games played on the site (over 2,000,000)."
- Break condition: If the dataset is biased towards certain play styles or if the quality of the games is inconsistent.

### Mechanism 3
- Claim: The DQN-based bot developed in the paper demonstrates that existing RL methods can excel in Dominion, providing a strong baseline for future research.
- Mechanism: By using a DQN approach with a relatively small neural network, the bot outperforms heuristic-based bots and shows competitive performance against the previously strongest bot, Provincial.
- Core assumption: The DQN architecture and training approach are appropriate for the Dominion domain.
- Evidence anchors:
  - [abstract] "Finally, we introduce an RL baseline bot that uses existing techniques to beat common heuristic-based bots, and shows competitive performance against the previously strongest bot, Provincial."
  - [section] "We implement a simple RL-based buyer bot using Rainbow DQN [7], trained to play Dominion via self-play... We see in Table 2 that our DQN-bot beats or ties Provincial almost two thirds of the time."
- Break condition: If more advanced RL techniques significantly outperform the DQN approach, or if the bot's performance is not robust across different card sets.

## Foundational Learning

- Concept: Reinforcement Learning (RL)
  - Why needed here: The paper develops an RL-based bot for Dominion, demonstrating the potential of RL methods in this domain.
  - Quick check question: What is the key difference between supervised learning and reinforcement learning?

- Concept: Game Theory
  - Why needed here: Understanding the strategic aspects of Dominion and how to develop effective playing strategies.
  - Quick check question: How does the concept of Nash equilibrium apply to two-player games like Dominion?

- Concept: Neural Networks
  - Why needed here: The DQN bot uses a neural network to approximate the value function and make decisions.
  - Quick check question: What is the role of the hidden layers in a neural network?

## Architecture Onboarding

- Component map: Game environment -> Neural network -> Training loop -> Evaluation framework

- Critical path:
  1. Initialize the game environment and the neural network.
  2. Play games of Dominion using the bot, collecting experience data.
  3. Update the neural network weights based on the collected experience.
  4. Evaluate the bot's performance against benchmarks.
  5. Iterate steps 2-4 to improve the bot's performance.

- Design tradeoffs:
  - Network architecture: Balancing model complexity and training efficiency.
  - Exploration vs. exploitation: Determining the right balance for effective learning.
  - Training time vs. performance: Allocating sufficient resources for training without overfitting.

- Failure signatures:
  - Poor performance against heuristic bots or the Provincial bot.
  - Inability to adapt to different card sets or game situations.
  - Overfitting to specific game states or strategies.

- First 3 experiments:
  1. Train the DQN bot for a small number of iterations and evaluate its performance against a basic heuristic bot.
  2. Compare the performance of the DQN bot with different network architectures (e.g., varying hidden layer sizes).
  3. Test the bot's ability to adapt to different card sets by training and evaluating it on a diverse set of game configurations.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the performance gap between the DQN-bot and Provincial be closed through enhanced training methods or improved state representations?
- Basis in paper: [explicit] The authors note that the DQN-bot struggles against the Village/Smithy Engine strategy due to insufficient exploration during training, suggesting this problem is "solvable" with more training or a more sophisticated game representation.
- Why unresolved: The paper presents only preliminary results from a basic DQN implementation, and does not explore advanced training techniques or more sophisticated state representations that could address the performance gap.
- What evidence would resolve it: Empirical results showing the DQN-bot's performance against Provincial and various heuristics after implementing enhanced training methods (e.g., curriculum learning, transfer learning) or improved state representations (e.g., using card effect embeddings or autoencoder-based feature learning).

### Open Question 2
- Question: How would the performance of RL-based Dominion agents scale when trained on the full set of 350+ cards rather than just the base game?
- Basis in paper: [explicit] The authors mention that the DQN-bot was trained only on base game cards and struggled with certain strategies, while also noting that Dominion has over 350 possible kingdom cards.
- Why unresolved: The paper only presents results for the base game and does not explore how the RL agents would perform with the full card set, which represents a much larger and more complex state space.
- What evidence would resolve it: Comparative performance metrics of RL agents trained on different subsets of cards (e.g., base game only, first two expansions, full card set) against established benchmarks like Provincial across multiple random card sets.

### Open Question 3
- Question: What is the optimal balance between model complexity and sample efficiency for training effective Dominion agents?
- Basis in paper: [inferred] The paper shows that a simple DQN with two hidden layers of size 256 achieves competitive performance, but notes that more sophisticated approaches might be needed for certain strategies, suggesting a trade-off between model complexity and training requirements.
- Why unresolved: The paper only explores one relatively simple model architecture and does not investigate how different model complexities (e.g., larger networks, transformer-based architectures) affect sample efficiency and final performance.
- What evidence would resolve it: Empirical analysis comparing training time, sample efficiency, and final performance across multiple model architectures (e.g., DQN, Rainbow DQN, transformer-based models) when trained on the same card sets and evaluated against the same benchmarks.

## Limitations

- The paper only compares against a single advanced bot (Provincial) and common heuristic bots, limiting the scope of performance evaluation.
- The DQN bot was trained only on base game cards, leaving open questions about its performance with the full set of 350+ cards.
- The evaluation does not include head-to-head comparisons with top human players, leaving uncertainty about real-world competitiveness.

## Confidence

- High Confidence: The dataset construction and availability (2 million+ games) is well-documented and verifiable.
- Medium Confidence: The DQN bot's performance claims are supported by quantitative results, but the comparison scope is limited.
- Medium Confidence: The claim that Dominion presents a unique AI challenge is well-founded, but the extent of its difficulty relative to other games is not fully explored.

## Next Checks

1. Evaluate the DQN bot against more advanced RL algorithms (e.g., PPO, AlphaZero-style approaches) to determine if DQN is indeed the most effective method for Dominion.
2. Conduct a systematic analysis of the bot's performance across all possible card combinations to assess its generalization capabilities and identify potential weaknesses.
3. Perform a head-to-head comparison between the RL bot and top human players to validate its competitive performance and identify areas for improvement.