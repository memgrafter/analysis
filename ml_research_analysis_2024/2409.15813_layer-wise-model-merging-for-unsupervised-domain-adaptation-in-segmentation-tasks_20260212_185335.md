---
ver: rpa2
title: Layer-wise Model Merging for Unsupervised Domain Adaptation in Segmentation
  Tasks
arxiv_id: '2409.15813'
source_url: https://arxiv.org/abs/2409.15813
tags:
- merging
- segmentation
- different
- training
- semantic
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses Unsupervised Domain Adaptation (UDA) for semantic
  and panoptic segmentation tasks, focusing on the challenge of merging models from
  different datasets and tasks to improve performance without incurring additional
  computational costs. The authors propose a layer-wise model merging method that
  leverages the abundance of freely accessible trained models.
---

# Layer-wise Model Merging for Unsupervised Domain Adaptation in Segmentation Tasks

## Quick Facts
- arXiv ID: 2409.15813
- Source URL: https://arxiv.org/abs/2409.15813
- Reference count: 0
- Primary result: Layer-wise merging improves UDA performance by up to 7% mPQ for cross-task merging and 6.8% mIoU for cross-architecture merging

## Executive Summary
This paper addresses Unsupervised Domain Adaptation (UDA) for semantic and panoptic segmentation by proposing a layer-wise model merging approach. The method leverages pre-trained models from different datasets and tasks, merging their parameters to improve performance without additional computational costs. The core insight is that initial layers encode general features suitable for uniform merging, while final layers preserve task-specific knowledge. Experiments demonstrate substantial UDA improvements across various scenarios including same-architecture merging, cross-architecture merging with shared backbones, and cross-task merging from semantic to panoptic segmentation.

## Method Summary
The method performs layer-wise merging of pre-trained models by uniformly averaging initial layers while preserving the final layers of an anchor model. The approach exploits the observation that initial layers undergo minimal changes during training across different tasks, making them suitable for uniform merging, while final layers encode task-specific information that should be preserved. The merging process involves identifying shared parameter indices across models, selecting an anchor model, applying the layer-wise weight formula to compute merged parameters, and saving the resulting model for inference. This approach is particularly effective for UDA scenarios where source domain labels exist but target domain labels do not.

## Key Results
- Same-architecture models from distinct datasets: up to 2.6% mIoU improvement
- Different-architecture models with shared backbone: 6.8% mIoU improvement
- Merging semantic and panoptic segmentation models: 7% increase in mPQ

## Why This Works (Mechanism)

### Mechanism 1
Initial layers encode general features common across tasks and datasets, making uniform merging effective at reducing variance. Final layers encode task-specific class boundaries, so they should be preserved from the anchor model. This assumes parameter space similarity in initial layers across models trained on different tasks/datasets.

### Mechanism 2
Merging checkpoints or different UDA strategies improves model robustness by integrating knowledge from multiple training states. Each checkpoint captures different aspects of the data distribution, and averaging them reduces noise while reinforcing consistent patterns. This assumes training noise is reduced when averaging models with different seeds, epochs, or strategies.

### Mechanism 3
Cross-task merging transfers knowledge from simpler to more complex tasks without extra training. Shared backbone features learned in semantic segmentation are reused in panoptic segmentation, while the simpler head is replaced by the panoptic head, enriched by semantic features. This assumes feature extraction layers encode useful representations for both tasks.

## Foundational Learning

- Concept: Unsupervised Domain Adaptation (UDA)
  - Why needed here: The paper applies model merging specifically to UDA scenarios where source labels exist but target labels do not.
  - Quick check question: In UDA, are target domain labels used during training? (Answer: No)

- Concept: Model merging via parameter averaging
  - Why needed here: The core proposal relies on averaging model weights layer-wise rather than ensembling predictions.
  - Quick check question: Does isotropic merging require additional inference time compared to a single model? (Answer: No, it loads the merged weights once)

- Concept: Layer-wise weight distribution
  - Why needed here: The method assigns uniform weights to initial layers and decreasing weights to deeper layers to preserve task-specific knowledge.
  - Quick check question: In the proposed scheme, which layers receive the highest weight from the anchor model? (Answer: Final layers)

## Architecture Onboarding

- Component map: Pre-trained models -> Layer-wise merging function -> Merged model checkpoint -> UDA performance evaluation
- Critical path: 1) Load pre-trained models and identify shared parameter indices 2) Select anchor model 3) Apply layer-wise weight formula 4) Save merged model 5) Validate on target domain
- Design tradeoffs: Merging vs. fine-tuning (cost-free vs. peak performance), layer granularity (simpler vs. precise), anchor selection (consistent gains vs. diversity benefits)
- Failure signatures: Large drop in target domain mIoU, model loading failure due to mismatched layer shapes, no performance gain over individual models
- First 3 experiments: 1) Merge two checkpoints of same model on GTA→Cityscapes; measure mIoU vs. individual checkpoints 2) Merge DeepLabV2 models with different UDA strategies on GTA→Cityscapes; compare to best single strategy 3) Merge HRDA (semantic) into EDAPS (panoptic) on Synthia→Cityscapes; measure mPQ improvement

## Open Questions the Paper Calls Out

### Open Question 1
How does layer-wise model merging perform when applied to different segmentation tasks beyond semantic and panoptic segmentation, such as instance segmentation or medical image segmentation? The paper focuses on semantic and panoptic segmentation but mentions potential applications to other visual tasks in the conclusion. This remains unresolved as experiments only evaluate the method on semantic and panoptic segmentation tasks.

### Open Question 2
Can layer-wise model merging be extended to handle models with significantly different architectures, beyond just sharing a backbone? The paper mentions merging models with different architectures that share a backbone, but does not explore merging models with entirely different architectures. This remains unresolved as experiments only consider merging models that share a common backbone.

### Open Question 3
How does the performance of layer-wise model merging scale with the number of models being merged, and is there a point of diminishing returns? The paper mentions merging multiple models but does not explore the effect of increasing the number of models on performance. This remains unresolved as experiments only merge a limited number of models.

## Limitations

- The analysis justifying layer-wise merging is based on observations from a limited set of models (HRNetV2-W48) and may not generalize to other architectures
- The effectiveness for very different architectures (e.g., transformers vs. CNNs) without shared pre-training remains unclear
- The anchor model selection strategy is not thoroughly explored beyond using the best-performing model

## Confidence

- High: Performance improvements on same-architecture same-dataset merging (4.1)
- Medium: Cross-dataset and cross-architecture merging benefits (4.2, 4.3)
- Medium: Cross-task merging from semantic to panoptic segmentation (4.4)
- Low: Generalizability to architectures beyond HRNet and DeepLab variants

## Next Checks

1. Test layer-wise merging on transformer-based segmentation models (e.g., SegFormer) to verify generalizability beyond CNN architectures
2. Conduct ablation studies on anchor model selection criteria, comparing random vs. best-performance vs. ensemble-based approaches
3. Measure actual computational overhead of the merging process (pre-processing time, memory usage) to quantify claimed cost savings