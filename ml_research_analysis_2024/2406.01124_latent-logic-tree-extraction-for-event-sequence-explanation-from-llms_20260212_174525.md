---
ver: rpa2
title: Latent Logic Tree Extraction for Event Sequence Explanation from LLMs
arxiv_id: '2406.01124'
source_url: https://arxiv.org/abs/2406.01124
tags:
- event
- logic
- tree
- llms
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a framework to extract interpretable logic
  trees from large language models (LLMs) to explain event sequences in high-stakes
  systems like healthcare. The core idea is to treat logic trees as latent variables
  and use an amortized expectation-maximization (EM) algorithm with GFlowNets to infer
  them.
---

# Latent Logic Tree Extraction for Event Sequence Explanation from LLMs

## Quick Facts
- arXiv ID: 2406.01124
- Source URL: https://arxiv.org/abs/2406.01124
- Authors: Zitao Song; Chao Yang; Chaojie Wang; Bo An; Shuang Li
- Reference count: 40
- Primary result: Proposed method improves event prediction accuracy by up to 25% over attention-based baselines, especially when using semantic event names.

## Executive Summary
This paper introduces LaTee, a framework that extracts interpretable logic trees from large language models (LLMs) to explain event sequences in high-stakes systems like healthcare. The core innovation treats logic trees as latent variables and uses an amortized expectation-maximization (EM) algorithm with GFlowNets to infer them. The approach significantly improves prediction accuracy compared to attention-based baselines, particularly when event names have semantic meaning. Experiments on synthetic and real-world datasets demonstrate the method's effectiveness and scalability.

## Method Summary
LaTee uses an amortized EM framework where logic trees are treated as latent variables. In the E-step, a learnable GFlowNet generates diverse logic tree samples from the posterior distribution, which combines an LLM prior with a likelihood based on a temporal point process model. The M-step uses these samples to approximate marginalization, updating both event likelihood parameters and the LLM prior. The framework leverages GFlowNets' ability to sample diverse solutions in discrete combinatorial spaces, enabling tractable inference for logic tree extraction.

## Key Results
- Up to 25% improvement in event prediction accuracy over attention-based baselines
- Significant performance gains when using semantic event names versus non-semantic names
- Better scalability with more event types compared to traditional attention-based methods
- GFlowNet fine-tuning outperforms traditional fine-tuning methods like SFT and PPO for logic tree discovery

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Treating logic trees as latent variables and using amortized EM with GFlowNets enables tractable inference in discrete combinatorial spaces.
- Mechanism: The E-step samples logic trees from the posterior distribution using GFlowNets, which are diversity-seeking generative models. The M-step uses these samples to approximate marginalization, updating both event likelihood parameters and LLM priors.
- Core assumption: The posterior distribution over logic trees can be effectively approximated by samples generated through GFlowNets trained with the SubTB loss.
- Evidence anchors:
  - [abstract]: "We propose an amortized Expectation-Maximization (EM) learning framework and treat the logic tree as latent variables...We propose to generate logic tree samples from the posterior using a learnable GFlowNet"
  - [section 3.2]: "In the E-step, we train a GFlowNet to generate logic tree samples from their posterior distribution...The M-step employs the generated logic tree samples to approximate marginalization over the posterior"
- Break condition: If the GFlowNet fails to adequately approximate the posterior distribution, the EM algorithm will converge to suboptimal solutions.

### Mechanism 2
- Claim: Semantic information in event sequences significantly improves the quality of extracted logic trees and prediction accuracy.
- Mechanism: When event names have semantic meaning, LLMs can better understand the relationships between events and generate more accurate logic trees that explain the underlying structure of the event sequences.
- Core assumption: LLMs pretrained on general text data have encoded knowledge about the semantic relationships between events that can be leveraged when event names are meaningful.
- Evidence anchors:
  - [abstract]: "Experiments on synthetic and real-world datasets show the method improves event prediction accuracy by up to 25% over attention-based baselines, especially when using semantic event names"
  - [section 4.2]: "We observe a noteworthy reduction in error rate (approximately 25%) for both EPIC-100 and MIMIC-3 datasets when employing semantic event names for reasoning and inference"
- Break condition: If event names are purely numerical or lack semantic meaning, the improvement from using LLMs will be minimal.

### Mechanism 3
- Claim: The GFlowNet fine-tuning approach is superior to traditional fine-tuning methods like SFT and PPO for logic tree discovery.
- Mechanism: GFlowNet fine-tuning teaches LLMs how to reason about logic trees rather than just predicting outcomes, resulting in more diverse and accurate rule distributions.
- Core assumption: The SubTB loss used in GFlowNet training effectively guides the model to explore diverse reasoning paths that capture the true posterior distribution.
- Evidence anchors:
  - [section 4.2]: "GFN fine-tuning, which focuses on teaching models how to reason rather than predict, enables LLMs to match and even exceed the prediction accuracy of attention-based TPP models"
  - [section 4.2]: "We notice that rule distributions in both SFT and PPO fine-tuning are predominantly concentrated in five regions, whereas GFN fine-tuning exhibits a more diverse spread across the entire rule space"
- Break condition: If the reward function is misspecified or the GFlowNet fails to explore the full space of possible logic trees, the approach will not outperform simpler fine-tuning methods.

## Foundational Learning

- Concept: Temporal Point Processes
  - Why needed here: The framework builds on temporal point processes to model event sequences and evaluate the likelihood of generated logic trees.
  - Quick check question: How does a temporal point process differ from traditional time series models in handling irregularly spaced events?

- Concept: Amortized Inference
  - Why needed here: The approach uses amortized inference to learn a GFlowNet that can efficiently sample logic trees for any given event sequence, rather than performing inference from scratch each time.
  - Quick check question: What is the key advantage of amortized inference over traditional variational inference in this context?

- Concept: GFlowNets
  - Why needed here: GFlowNets are used as the generative model for sampling diverse logic trees from the posterior distribution in the E-step of the EM algorithm.
  - Quick check question: How does the SubTB loss in GFlowNets differ from the traditional maximum likelihood objective?

## Architecture Onboarding

- Component map: Event sequence → LLM prior generation → GFlowNet posterior sampling → TPP likelihood evaluation → Logic tree extraction → Next event prediction
- Critical path: Event sequence → LLM prior generation → GFlowNet posterior sampling → TPP likelihood evaluation → Logic tree extraction → Next event prediction
- Design tradeoffs:
  - Model size vs. inference speed: Larger LLMs may generate better logic trees but increase computational cost
  - Tree depth vs. coverage: Deeper trees can capture more complex relationships but may overfit or be harder to learn
  - Semantic vs. non-semantic events: Semantic events improve performance but may not always be available
- Failure signatures:
  - Poor prediction accuracy: Could indicate issues with logic tree generation or likelihood evaluation
  - Mode collapse in GFlowNet: Suggests the model is not exploring diverse logic trees
  - Slow convergence: May indicate improper hyperparameter tuning or insufficient training data
- First 3 experiments:
  1. Test the framework on a simple synthetic dataset with known logic rules to verify the extraction process
  2. Compare performance with and without semantic event names on a small real-world dataset
  3. Evaluate the impact of different GFlowNet hyperparameters (tree depth, width) on prediction accuracy

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the performance of LaTee scale effectively with larger language models (e.g., GPT-4 or Claude-3) in the M-step for generation tasks?
- Basis in paper: [explicit] The paper mentions that larger language models for generation (M-steps) show more pronounced positive impact compared to larger models for inference (E-steps), suggesting future improvements by using API-based LLMs like GPT-4 and Claude-3.
- Why unresolved: The experiments in the paper used only locally accessible models up to 6.7B parameters. The potential benefits of using larger, API-based models were hypothesized but not tested.
- What evidence would resolve it: Empirical results comparing the performance of LaTee using different sizes of API-based LLMs (e.g., GPT-4, Claude-3) in the M-step for generation tasks, particularly on the real-world datasets with semantic information.

### Open Question 2
- Question: How does the performance of LaTee change when applied to longer event sequences beyond the 40-event limit tested in the paper?
- Basis in paper: [explicit] The paper states that resource constraints limited experiments to models with up to 6.7B parameters and event sequences of a maximum of 40 events, but anticipates findings to be applicable to longer sequences.
- Why unresolved: The paper did not test the scalability of LaTee with longer event sequences due to resource limitations. The impact of sequence length on performance and computational efficiency remains unknown.
- What evidence would resolve it: Experimental results showing the performance and computational efficiency of LaTee on event sequences longer than 40 events, ideally on both synthetic and real-world datasets.

### Open Question 3
- Question: What is the impact of increasing the tree depth versus the tree width on the performance of LaTee, especially for non-semantic event sequences?
- Basis in paper: [explicit] The paper found that increasing tree widths has a more beneficial effect than increasing tree depth or model size on semantic event sequences. However, for non-semantic event sequences, enlarging the model size tends to be more advantageous than increasing tree sizes.
- Why unresolved: The paper only explored tree depths and widths up to 4 and 7, respectively. The optimal balance between depth and width for different types of event sequences is not fully understood.
- What evidence would resolve it: A systematic study varying both tree depth and width independently across a range of values, measuring the performance of LaTee on both semantic and non-semantic event sequences to identify the optimal configuration for each case.

## Limitations

- The framework's performance heavily depends on the semantic meaning of event names, with non-semantic events showing reduced accuracy.
- Scalability to very large event spaces and complex temporal dependencies remains untested due to resource constraints.
- The reliance on LLMs for logic tree generation introduces potential biases from pre-training data.

## Confidence

High confidence in the amortized EM framework with GFlowNets for latent logic tree extraction
Medium confidence in the performance improvement claims, particularly for non-semantic event sequences
Low confidence in the generalization to domains with completely non-semantic event naming

## Next Checks

1. Test the framework on a dataset with mixed semantic and non-semantic events to evaluate robustness
2. Evaluate the approach on datasets with significantly more event types (e.g., 1000+) to assess scalability
3. Implement an ablation study removing the LLM component to quantify its contribution to performance improvements