---
ver: rpa2
title: 'Navigating Uncertainty: Optimizing API Dependency for Hallucination Reduction
  in Closed-Book Question Answering'
arxiv_id: '2401.01780'
source_url: https://arxiv.org/abs/2401.01780
tags:
- search
- answer
- answers
- language
- question
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the problem of reducing hallucinations in Large
  Language Models (LLMs) during closed-book question answering. It proposes a method
  to teach LLMs to self-estimate their confidence in answering questions directly
  or when to request external resources.
---

# Navigating Uncertainty: Optimizing API Dependency for Hallucination Reduction in Closed-Book Question Answering

## Quick Facts
- arXiv ID: 2401.01780
- Source URL: https://arxiv.org/abs/2401.01780
- Reference count: 10
- The model directly provides answers for 78.2% of known queries and opts to search for 77.2% of unknown ones, resulting in the API being utilized only 62% of the time.

## Executive Summary
This paper addresses the challenge of hallucination reduction in Large Language Models (LLMs) during closed-book question answering. The authors propose a novel approach that teaches LLMs to self-estimate their confidence in answering questions directly or when to request external resources. By introducing a hallucination masking mechanism and leveraging parameter-efficient fine-tuning techniques, the model can effectively balance between providing direct answers and utilizing external APIs, resulting in reduced hallucination rates and optimized API usage.

## Method Summary
The paper proposes a method that introduces a hallucination masking mechanism to generate labels using a close book question-answering task. This mechanism allows the model to conserve its ability to answer directly when correct while predicting a special token (⟨search⟩) instead of hallucinating an answer. The authors leverage parameter-efficient fine-tuning techniques, specifically Low-Rank Adaptation (LoRA), to train the model on a small amount of data. The proposed model is fine-tuned on two open-domain CBQA datasets - Natural Questions (NQ) and TriviaQA (TQA) - and evaluated against baselines such as perplexity-threshold strategy and in-context learning models like Mistral-7B.

## Key Results
- The proposed model directly provides answers for 78.2% of known queries and opts to search for 77.2% of unknown ones, resulting in the API being utilized only 62% of the time.
- Model variants using LoRA consistently outperform other approaches in terms of hallucination reduction and correct answer generation.
- The model demonstrates superior performance in detecting in-domain hallucinations compared to out-of-domain ones.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The Hallucination Masking Mechanism (HalM) enables the model to self-detect hallucinations by leveraging the same knowledge used during initial training.
- Mechanism: HalM masks incorrect answers with a special token (⟨search⟩) during a second fine-tuning step. The mechanism relies on the original model's ability to identify its own hallucinations by comparing its outputs against ground truth labels.
- Core assumption: The model's performance on the training data is reliable enough to generate pseudo-labels for self-supervised hallucination detection.
- Evidence anchors:
  - [abstract]: "we propose a new LLM able to self-estimate if it is able to answer directly or needs to request an external tool"
  - [section 2]: "We introduce ψ a Hallucination Masking Mechanism (HalM) allowing to mask wrong answers with ⟨search⟩ tokens"
  - [corpus]: No direct evidence; assumes self-assessment is feasible within-domain
- Break condition: If the model's performance degrades significantly during the second fine-tuning, or if the ground truth labels are too sparse to generate reliable pseudo-labels.

### Mechanism 2
- Claim: Parameter-efficient fine-tuning (LoRA) enables effective adaptation of large models with minimal data and computational cost.
- Mechanism: LoRA modifies the model's weights by learning low-rank matrices that approximate the update to the original weights, allowing for efficient fine-tuning on a small amount of data.
- Core assumption: The low-rank approximation captures the essential changes needed for the model to learn the hallucination detection task.
- Evidence anchors:
  - [abstract]: "we propose to leverage parameter-efficient fine-tuning techniques to train our model on a small amount of data"
  - [section 3.4]: "using Low-Rank Adaptation ( LoRA) (Hu et al., 2022)"
  - [corpus]: No direct evidence; relies on external LoRA literature
- Break condition: If the low-rank approximation is insufficient to capture the necessary changes, or if the model overfits to the small training dataset.

### Mechanism 3
- Claim: The budgeted open QA formulation encourages the model to minimize API calls while maintaining accuracy.
- Mechanism: The objective function (Equation 2) penalizes both hallucinations and API calls, with a hyperparameter λ controlling the relative importance of each.
- Core assumption: The model can effectively trade off between answering directly and calling the API based on the learned objective.
- Evidence anchors:
  - [section 2]: "Training this LLM of parameter θ′ can be seen as a budgeted open QA task in which we minimize the probability of generating a wrong answer P (ˆa /∈ A|θ′, q) and the probability to call an external tool P (ˆa = ⟨search⟩|θ′, q) with a budget λ"
  - [corpus]: No direct evidence; relies on external budgeted learning literature
- Break condition: If the model fails to learn the trade-off effectively, resulting in either excessive API calls or high hallucination rates.

## Foundational Learning

- Concept: Closed-book question answering (CBQA)
  - Why needed here: The paper focuses on CBQA as the base task before introducing the hallucination detection mechanism.
  - Quick check question: What is the difference between open-book and closed-book question answering?

- Concept: Parameter-efficient fine-tuning
  - Why needed here: LoRA is used to adapt large models with minimal computational cost.
  - Quick check question: How does LoRA differ from traditional fine-tuning in terms of parameter updates?

- Concept: Hallucination detection
  - Why needed here: The core problem addressed by the paper is reducing hallucinations in LLM-generated answers.
  - Quick check question: What are some common approaches to hallucination detection in LLMs?

## Architecture Onboarding

- Component map:
  Base LLM (T5-Large or T5-XXL) -> Hallucination Masking Mechanism (HalM) -> Parameter-efficient fine-tuning (LoRA) -> External API (for search)

- Critical path:
  1. Fine-tune base LLM on CBQA dataset
  2. Apply HalM to generate pseudo-labels
  3. Fine-tune with LoRA using pseudo-labels
  4. Evaluate on test set

- Design tradeoffs:
  - Accuracy vs. API usage: Balancing correct answers with minimizing API calls
  - Model size vs. computational cost: Larger models may perform better but require more resources
  - In-domain vs. out-of-domain detection: Focusing on in-domain detection to avoid distribution shifts

- Failure signatures:
  - High hallucination rate: Indicates HalM is not effectively detecting hallucinations
  - Excessive API usage: Suggests the model is not confident in its direct answers
  - Overfitting: Model performs well on training data but poorly on test data

- First 3 experiments:
  1. Compare base LLM performance on CBQA task with and without LoRA fine-tuning
  2. Evaluate hallucination reduction using HalM with different λ values
  3. Assess the impact of model size (T5-Large vs. T5-XXL) on hallucination reduction and API usage

## Open Questions the Paper Calls Out
None explicitly stated in the paper.

## Limitations
- The mechanisms proposed for hallucination detection and reduction are largely theoretical and rely on assumptions that may not hold in practice.
- The effectiveness of the parameter-efficient fine-tuning approach using LoRA is not thoroughly examined.
- The impact of the budgeted open QA formulation on the model's performance is not clearly demonstrated.

## Confidence
- High: The proposed approach shows promise in reducing hallucinations and optimizing API usage.
- Medium: The evidence provided is largely based on the authors' own experiments and does not include external validation or comparison with other state-of-the-art methods.
- Low: The reproducibility of the results may be limited due to the lack of detailed information on the implementation and hyperparameters used.

## Next Checks
1. Conduct an ablation study to assess the individual contributions of the hallucination masking mechanism, parameter-efficient fine-tuning, and budgeted open QA formulation to the overall performance of the model.
2. Compare the proposed approach with other state-of-the-art methods for hallucination detection and reduction in LLMs, using standardized datasets and evaluation metrics.
3. Investigate the robustness of the model to out-of-domain questions and the potential impact of distribution shifts on its performance.