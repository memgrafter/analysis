---
ver: rpa2
title: Repurposing Foundation Model for Generalizable Medical Time Series Classification
arxiv_id: '2410.03794'
source_url: https://arxiv.org/abs/2410.03794
tags:
- datasets
- medts
- time
- formed
- series
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of poor generalizability in
  medical time series classification due to heterogeneity in data structure and domain.
  It proposes FORMED, a framework that repurposes pre-trained foundation models by
  integrating a novel classifier with task-specific channel embeddings and label queries,
  along with a shared decoding attention layer.
---

# Repurposing Foundation Model for Generalizable Medical Time Series Classification

## Quick Facts
- arXiv ID: 2410.03794
- Source URL: https://arxiv.org/abs/2410.03794
- Reference count: 40
- Primary result: Up to 35% absolute F1-score improvement over task-specific baselines

## Executive Summary
This paper addresses the challenge of poor generalizability in medical time series classification due to data heterogeneity across different clinical domains. The authors propose FORMED, a framework that repurposes pre-trained foundation models (specifically TimesFM) by integrating task-specific channel embeddings and label queries with a shared decoding attention layer. This design enables seamless adaptation to new datasets by training only lightweight parameters (0.1% of total), eliminating the need for full fine-tuning. Evaluated on five diverse medical time series datasets, FORMED demonstrates significant performance improvements and establishes a scalable paradigm for foundation model repurposing in healthcare.

## Method Summary
FORMED repurposes pre-trained foundation models by introducing a novel classifier architecture that includes task-specific channel embeddings, label queries, and a shared decoding attention layer. The framework first repurposes the model on a diverse medical time series (MedTS) cohort, then adapts to new datasets by training only the lightweight classifier components while keeping the backbone frozen. This approach leverages pre-learned domain knowledge while maintaining computational efficiency through parameter sharing and minimal adaptation overhead.

## Key Results
- Achieves up to 35% absolute improvement in F1-score over task-specific baselines
- Demonstrates consistent performance across varying channel configurations, time lengths, and clinical tasks
- Enables adaptation to unseen datasets with only 0.1% of parameters trained
- Shows resource efficiency with lightweight label query training eliminating full fine-tuning needs

## Why This Works (Mechanism)

### Mechanism 1
- Claim: FORMED's shared decoding attention (SDA) captures generalizable medical domain knowledge across diverse datasets during repurposing.
- Mechanism: SDA parameters are trained jointly across the MedTS cohort using a common classifier architecture, allowing it to learn task-agnostic interaction patterns between channel-aware features and label queries.
- Core assumption: Multi-channel MedTS data share common temporal and physiological patterns that can be captured by a shared attention layer.
- Evidence anchors:
  - [abstract]: "a shared decoding attention layer, jointly trained across datasets to capture medical domain knowledge through task-agnostic feature-query interactions."
  - [section 4.2]: "The SDA takes all K · k Label Queries from Q as queries, and the flattened, channel-aware feature tokensFlatten(˜H) ∈ R(C·L)×D as keys and values... the parameters within the MultiHeadAttention and the FeedForwardNetwork (collectively θ) are independent of the specific number of input channels C, token length L, or the total number of queries K · k."
- Break condition: If the MedTS cohort lacks diversity or contains datasets with fundamentally different physiological patterns, SDA may overfit to specific dataset characteristics instead of learning generalizable domain knowledge.

### Mechanism 2
- Claim: Lightweight label query training (0.1% of parameters) enables efficient adaptation to new unseen datasets without full fine-tuning.
- Mechanism: During adaptation, only task-specific channel embeddings (CEs) and label queries (LQs) are trained while the backbone and SDA remain frozen, leveraging pre-learned domain knowledge.
- Core assumption: The frozen backbone and SDA contain sufficient generalizable representations to support classification across diverse MedTS tasks with only minimal task-specific adjustments.
- Evidence anchors:
  - [abstract]: "After repurposing, FORMED achieves seamless adaptation to unseen MedTS datasets through lightweight label query training (0.1% of parameters), eliminating the need for full fine-tuning or architectural redesign."
  - [section 3]: "For a new dataset DNew (with potentially different C ′ channels, T ′ time steps, and K ′ classes), the pre-trained backbone f and the shared parameters θ∗ of the classifier hθ∗ remain frozen. Only new Channel Embeddings E′ ∈ RC′×D and Label Queries Q′ ∈ RK′×D are initialized and trained."
  - [section 5.2]: "As seen in Figure 5, FORMED generally outperforms the TimesFM-TSA baseline on these unseen datasets, even with less adaptable parameters (FORMED at k = 16 outperforms TimesFM-TSA with only ∼ 1/6 of the parameter)."
- Break condition: If new datasets have significantly different data characteristics (sampling rates, channel configurations, or physiological patterns) not represented in the repurposing cohort, the frozen components may lack relevant representations.

### Mechanism 3
- Claim: Task-specific channel embeddings enable the model to handle varying numbers of input channels and understand their task-specific relevance.
- Mechanism: CEs are broadcast-added to channel-wise feature tokens, allowing the model to distinguish and weight information from different physiological channels based on the specific diagnostic task.
- Core assumption: Different physiological channels contain task-specific information that should be weighted differently depending on the classification objective.
- Evidence anchors:
  - [section 4.2]: "To enable the model to distinguish information from different physiological channels and understand their task-specific relevance, we introduce learnable Channel Embeddings E ∈ RC×D. For a given dataset with C channels, these embeddings are broadcast-added to the corresponding channel-wise feature tokens H... to produce 'channel-aware' feature tokens ˜H."
  - [section 4.2]: "These CEs are task-specific and learned during both repurposing (part of E in Equation (3)) and adapting (E′ in Equation (4))."
- Break condition: If channel relationships are task-independent and don't require weighting, the additional CE parameters may add unnecessary complexity without performance benefits.

## Foundational Learning

- Concept: Multi-head attention and transformer decoder architecture
  - Why needed here: The SDA mechanism uses transformer decoder layers with multi-head attention to capture complex interactions between label queries and channel-aware features across different scales and perspectives.
  - Quick check question: How does multi-head attention enable the model to capture different types of relationships between features and queries simultaneously?

- Concept: Pre-training and transfer learning principles
  - Why needed here: The framework relies on a pre-trained foundation model (TimesFM) as a feature extractor, requiring understanding of how pre-trained representations can be effectively transferred to new downstream tasks.
  - Quick check question: What are the key differences between fine-tuning the entire model versus freezing the backbone and training only task-specific parameters?

- Concept: Contrastive learning and representation disentanglement
  - Why needed here: The approach implicitly separates domain-invariant representations (captured by the frozen backbone and SDA) from task-specific knowledge (captured by CEs and LQs), which relates to representation disentanglement principles.
  - Quick check question: How does separating domain-invariant and task-specific knowledge help with generalization across different medical time series datasets?

## Architecture Onboarding

- Component map: Input → TimesFM Backbone → Channel Embeddings → Shared Decoding Attention → Label Queries → Classification
- Critical path: Input time series → patching and transformer layers → augmented with channel embeddings → processed by shared attention → weighted by label queries → produces final class logits
- Design tradeoffs:
  - Shared vs. task-specific attention: Using a single shared SDA reduces parameters and enforces domain knowledge sharing but may limit task-specific pattern capture
  - Number of label queries (k): More queries increase capacity for capturing complex class patterns but also increase adaptation parameters
  - Frozen backbone: Preserves pre-trained representations but may limit adaptation to datasets with very different characteristics
- Failure signatures:
  - Poor performance on new datasets: Could indicate insufficient diversity in repurposing cohort or inadequate representation capacity in frozen components
  - Overfitting on small adaptation datasets: May suggest too many task-specific parameters relative to available data
  - Inconsistent performance across different k values: Could indicate sensitivity to query capacity or optimization issues
- First 3 experiments:
  1. Verify channel embedding effectiveness: Compare performance with and without CEs on a dataset with known channel importance patterns
  2. Test SDA sharing benefits: Train separate SDA for each dataset vs. shared SDA, measure performance and parameter efficiency
  3. Evaluate adaptation scaling: Measure performance vs. k (number of queries per class) on a small adaptation dataset to find optimal balance between capacity and overfitting

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How would FORMED perform when repurposed with other time series foundation models beyond TimesFM?
- Basis in paper: [inferred] The paper primarily uses TimesFM as the backbone, but mentions that performance with other diverse time series foundation models warrants future investigation.
- Why unresolved: The study is limited by resource constraints and focuses on validating the efficacy of the FORMED framework using TimesFM, leaving the interaction with other models unexplored.
- What evidence would resolve it: Empirical results comparing FORMED's performance across multiple backbone models (e.g., Time-LLM, UniTime, GPT4TS) would clarify generalizability and model dependency.

### Open Question 2
- Question: How does the composition and scale of the MedTS cohort affect the breadth of domain knowledge captured during repurposing?
- Basis in paper: [explicit] The paper notes that the composition and scale of the MedTS cohort employed during repurposing may influence the breadth of the captured domain knowledge.
- Why unresolved: The current study uses a specific set of five datasets, but does not explore how varying the diversity, size, or balance of the cohort impacts the quality of learned domain knowledge.
- What evidence would resolve it: Systematic experiments varying the MedTS cohort composition (e.g., adding/removing datasets, altering task diversity) and measuring FORMED's adaptation performance would reveal optimal cohort characteristics.

### Open Question 3
- Question: What are the computational and memory requirements for scaling FORMED to larger, more complex MedTS datasets?
- Basis in paper: [inferred] While the paper discusses data-efficient adaptation, it does not provide detailed analysis of computational efficiency or scalability with dataset size.
- Why unresolved: The paper mentions FORMED's resource efficiency but lacks specific metrics on how computational demands scale with input size, channel count, or number of classes.
- What evidence would resolve it: Benchmarking FORMED's memory usage, training/inference time, and parameter growth across progressively larger MedTS datasets would quantify scalability limits and practical deployment constraints.

## Limitations

- The framework's generalizability heavily depends on the diversity and representativeness of the MedTS cohort used for repurposing, with no evidence provided about cohort diversity analysis
- The 0.1% parameter efficiency claim assumes frozen components generalize well, which may not hold for datasets with significantly different characteristics than those in the repurposing cohort
- No ablation studies are provided to confirm the effectiveness of task-specific channel embeddings or demonstrate the impact of removing them

## Confidence

- **High Confidence**: Experimental results showing performance improvements over task-specific baselines on the five evaluated datasets
- **Medium Confidence**: The claim about 0.1% parameter efficiency for adaptation, as it depends on the assumption that frozen components generalize well
- **Medium Confidence**: The mechanism of shared decoding attention capturing generalizable domain knowledge, as the evidence is primarily theoretical rather than empirical

## Next Checks

1. Conduct systematic experiments on datasets with varying degrees of similarity to the MedTS cohort to quantify the limits of the frozen backbone's generalizability
2. Perform ablation studies removing channel embeddings to measure their actual contribution to performance improvements across different tasks
3. Test the framework on synthetic datasets with controlled variations in channel importance and physiological patterns to validate the task-specific channel embedding mechanism