---
ver: rpa2
title: Helpful assistant or fruitful facilitator? Investigating how personas affect
  language model behavior
arxiv_id: '2407.02099'
source_url: https://arxiv.org/abs/2407.02099
tags:
- personas
- person
- persona
- language
- speech
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates how assigning personas to large language
  models (LLMs) affects diverse aspects of model behavior, including task performance,
  biases, attitudes, and refusals. The authors assign 162 personas from 12 categories
  to seven LLMs and prompt them with questions from five datasets covering objective
  and subjective tasks.
---

# Helpful assistant or fruitful facilitator? Investigating how personas affect language model behavior

## Quick Facts
- arXiv ID: 2407.02099
- Source URL: https://arxiv.org/abs/2407.02099
- Authors: Pedro Henrique Luz de Araujo; Benjamin Roth
- Reference count: 40
- One-line primary result: Personas significantly impact LLM task performance, biases, attitudes, and refusals in complex and sometimes arbitrary ways

## Executive Summary
This paper systematically investigates how assigning different personas to large language models affects their behavior across multiple dimensions. The authors test 162 personas from 12 categories across seven LLMs, examining impacts on task performance, social biases, attitudes, and refusal behaviors. Their findings reveal that personas have substantial and sometimes unexpected effects on model outputs, with certain persona patterns showing consistency across different models while others produce highly variable results.

The study challenges the assumption that persona-based prompting is a reliable method for controlling model behavior, particularly in safety-critical applications. By demonstrating that refusals can be arbitrary and potentially discriminatory, the research raises important questions about using personas as a safety mechanism. The comprehensive evaluation across multiple tasks and models provides empirical evidence that persona effects are complex and require careful consideration in practical applications.

## Method Summary
The authors assign 162 personas from 12 categories to seven different LLMs and prompt them with questions from five datasets covering both objective and subjective tasks. They compare persona behaviors against two baseline settings: a control persona setting using paraphrases of "a helpful assistant" and an empty persona setting. The evaluation examines task performance, social biases, attitude values, toxicity annotations, and refusal patterns. The study employs a systematic approach to measure how persona assignment affects various aspects of model behavior, using both automated metrics and human annotations where appropriate.

## Key Results
- Personas significantly impact task performance, with regular personas showing greater variability than control personas
- Some persona rankings are consistent across models (e.g., asexual person and person of atheism belief consistently accurate for trustworthiness)
- Personas affect social biases, with some personas exhibiting lower bias against their own demographic group
- LLM refusals are arbitrary and potentially discriminatory, with disparities across persona categories

## Why This Works (Mechanism)
Personas work by providing contextual framing that influences how language models generate responses. When a model is assigned a specific persona, it adjusts its output generation to align with the characteristics and expectations associated with that persona. This mechanism affects not just factual accuracy but also subjective elements like bias expression and refusal tendencies. The persona acts as a conditioning factor that shapes the model's probability distributions during generation, leading to systematic differences in output characteristics.

## Foundational Learning
- Persona assignment as context conditioning: Why needed - Understanding how contextual information shapes model outputs; Quick check - Test model responses with and without persona prompts on identical queries
- Bias measurement in LLMs: Why needed - Quantifying fairness and discrimination in AI systems; Quick check - Compare bias metrics across different demographic groups
- Refusal pattern analysis: Why needed - Evaluating safety mechanisms and their reliability; Quick check - Measure refusal rates across different prompt types and personas
- Cross-model persona consistency: Why needed - Assessing generalizability of findings; Quick check - Compare persona rankings across multiple model architectures
- Attitude annotation frameworks: Why needed - Quantifying subjective model characteristics; Quick check - Validate annotation consistency across multiple human raters

## Architecture Onboarding

**Component map:** Persona assignment -> Prompt generation -> Model inference -> Output evaluation -> Bias/attitude/refusal analysis

**Critical path:** Persona selection and assignment → Question prompt generation → LLM response generation → Performance evaluation → Bias and attitude measurement → Refusal pattern analysis

**Design tradeoffs:** The study prioritizes comprehensive persona coverage over computational efficiency, testing 162 personas across seven models. This breadth enables robust analysis of persona effects but requires substantial computational resources. The choice to include both objective and subjective tasks provides a holistic view but introduces measurement complexity for subjective evaluations.

**Failure signatures:** Arbitrary refusal patterns indicate potential safety mechanism failures. Inconsistent persona effects across models suggest limitations in generalizability. High variability in regular personas compared to control personas reveals instability in persona-based prompting approaches.

**First experiments:** 1) Test baseline refusal rates without personas; 2) Compare persona-assigned refusals against demographic distributions; 3) Measure performance variance across different persona categories

## Open Questions the Paper Calls Out
None

## Limitations
- Generalizability across different LLM architectures may be limited given only seven models were tested
- Persona assignment relies on human-generated labels from Wikipedia, potentially introducing biases
- Focus on English language prompts limits applicability to multilingual contexts
- Evaluation metrics for subjective tasks may be influenced by annotators' own biases
- Computational resources required may limit reproducibility for researchers with constrained access

## Confidence
- Task performance impacts: High confidence
- Bias measurement findings: Medium confidence
- Attitude and toxicity annotations: Medium confidence
- Refusal patterns: Medium confidence
- Cross-model consistency: Low confidence

## Next Checks
1. Replicate the study using additional LLM architectures beyond the seven tested to assess generalizability
2. Conduct multilingual evaluations to determine if persona effects transfer across languages
3. Implement alternative bias measurement frameworks to validate the consistency of bias findings across different evaluation methodologies