---
ver: rpa2
title: 'Making AI Intelligible: Philosophical Foundations'
arxiv_id: '2406.08134'
source_url: https://arxiv.org/abs/2406.08134
tags:
- what
- about
- have
- content
- some
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Philosophers propose using externalist metasemantics to make AI
  interpretable. They show how Kripkean causal chains, Evansian mental files, and
  teleofunctional role can be abstracted from human-centric cases to apply to AI systems
  like SmartCredit.
---

# Making AI Intelligible: Philosophical Foundations
## Quick Facts
- arXiv ID: 2406.08134
- Source URL: https://arxiv.org/abs/2406.08134
- Reference count: 0
- Philosophers propose using externalist metasemantics to make AI interpretable

## Executive Summary
This paper addresses the challenge of making AI systems interpretable by proposing a philosophical approach using externalist metasemantics. The authors argue that current AI interpretability work is too computationally focused and lacks environmental and sociological explanations. They demonstrate how Kripkean causal chains, Evansian mental files, and teleofunctional role can be abstracted from human-centric cases to apply to AI systems like SmartCredit. The framework reveals that AI outputs can be contentful and addresses fundamental questions about AI meaning and purpose.

## Method Summary
The authors present a philosophical framework that extends externalist metasemantics to AI systems. They apply three specific approaches - Kripkean causal chains, Evansian mental files, and teleofunctional role - to analyze how AI systems like SmartCredit can have meaningful content. The framework moves beyond purely computational explanations to incorporate environmental and sociological factors in understanding AI outputs. They propose knowledge-maximization as a meta-metasemantic principle to guide content attribution to AI systems.

## Key Results
- Demonstrates that AI outputs can be contentful using externalist metasemantics
- Shows how philosophical frameworks can provide richer explanations than computational approaches
- Proposes knowledge-maximization as a guiding principle for content attribution to AI

## Why This Works (Mechanism)
The philosophical approach works by extending established theories of meaning from human language to AI systems. By treating AI outputs as potentially meaningful through externalist metasemantics, the framework provides tools to analyze what AI systems mean and why they produce certain outputs. The three approaches (Kripkean, Evansian, and teleofunctional) offer different but complementary perspectives on AI content, allowing for more nuanced interpretation than purely computational methods.

## Foundational Learning
- Kripkean causal chains: Explains how AI outputs can be causally connected to real-world phenomena
  - Why needed: Provides a basis for understanding AI content in terms of environmental interactions
  - Quick check: Can AI outputs be traced to specific environmental causes?

- Evansian mental files: Shows how AI systems can track and reference real-world objects
  - Why needed: Enables understanding of AI's ability to maintain consistent references
  - Quick check: Does AI maintain consistent tracking of entities across different contexts?

- Teleofunctional role: Demonstrates how AI outputs serve specific functional purposes
  - Why needed: Links AI outputs to their practical applications and goals
  - Quick check: Can AI outputs be explained in terms of their functional roles?

## Architecture Onboarding
- Component map: Kripkean chains -> Evansian files -> Teleofunctional roles
- Critical path: Input processing -> Meaning attribution -> Output interpretation
- Design tradeoffs: Philosophical depth vs. computational practicality
- Failure signatures: Inconsistent meaning attribution, broken causal chains, misaligned functional roles
- First experiments:
  1. Trace causal chains from SmartCredit inputs to outputs
  2. Test consistency of entity tracking across different contexts
  3. Analyze functional alignment between AI outputs and system goals

## Open Questions the Paper Calls Out
The paper acknowledges that its philosophical approach to AI interpretability requires substantial empirical validation. The application to SmartCredit remains largely theoretical without demonstrated implementation or testing. The framework's handling of edge cases and system failures needs further exploration. The proposed knowledge-maximization principle requires additional operationalization and empirical grounding.

## Limitations
- The framework's extension from human concepts to AI systems is largely theoretical
- Limited discussion of practical implementation challenges
- Unclear how the framework handles ambiguous or edge-case outputs
- Need for empirical validation of philosophical interpretations

## Confidence
- High confidence in identifying the philosophical gap in current AI interpretability work
- Medium confidence in the applicability of externalist metasemantics to AI
- Medium confidence in the proposed framework's practical utility
- Low confidence in immediate real-world implementation feasibility

## Next Checks
1. Develop and test a proof-of-concept implementation of the SmartCredit case study using the proposed framework
2. Design empirical studies comparing philosophical interpretability methods against current technical approaches
3. Create benchmark cases testing the framework's handling of ambiguous or edge-case outputs