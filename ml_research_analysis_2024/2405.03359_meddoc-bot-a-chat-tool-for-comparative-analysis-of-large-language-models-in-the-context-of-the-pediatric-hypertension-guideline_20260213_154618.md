---
ver: rpa2
title: 'MedDoc-Bot: A Chat Tool for Comparative Analysis of Large Language Models
  in the Context of the Pediatric Hypertension Guideline'
arxiv_id: '2405.03359'
source_url: https://arxiv.org/abs/2405.03359
tags:
- medical
- responses
- guidelines
- mistral
- pediatric
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This research evaluates four open-source large language models
  (LLMs) - Meditron, MedAlpaca, Mistral, and Llama-2 - for interpreting pediatric
  hypertension guidelines. A Streamlit-based chatbot (MedDoc-Bot) enables users to
  upload PDF documents and query the models for responses.
---

# MedDoc-Bot: A Chat Tool for Comparative Analysis of Large Language Models in the Context of the Pediatric Hypertension Guideline

## Quick Facts
- arXiv ID: 2405.03359
- Source URL: https://arxiv.org/abs/2405.03359
- Reference count: 20
- Key outcome: Llama-2 achieved highest METEOR (0.50) and chrF (0.53) scores for clinical queries in pediatric hypertension guideline interpretation

## Executive Summary
This study evaluates four open-source large language models (Meditron, MedAlpaca, Mistral, and Llama-2) for interpreting pediatric hypertension guidelines using a Streamlit-based chatbot called MedDoc-Bot. The system enables users to upload PDF documents and query the models for responses, with evaluation conducted through human expert ratings and automated metrics (METEOR and chrF). Llama-2 demonstrated the strongest performance in automated metrics for clinical queries, while Mistral excelled in response time for visual elements. The research highlights the potential of open-source LLMs for medical document interpretation while identifying trade-offs between quality and speed.

## Method Summary
The study implements MedDoc-Bot as a Streamlit-based interface for uploading PDF documents and querying four pre-quantized GGUF LLM variants (Meditron, MedAlpaca, Mistral, Llama-2) from Hugging Face. Document processing uses LangChain for chunking and vector embeddings via Sentence Transformers, with FAISS-based semantic search to ground responses in the source material. Evaluation involves 12 expert-curated questions from ESC pediatric hypertension guidelines, assessed through human ratings (fidelity and relevance) and automated metrics (METEOR and chrF scores). Visual elements are transformed into text representations to enhance model interpretation.

## Key Results
- Llama-2 achieved the highest METEOR score (0.50) and chrF score (0.53) for clinical queries
- Mistral demonstrated superior response time performance for queries involving visual elements
- Human evaluation found Mistral, Meditron, and Llama-2 generated responses with reasonable fidelity and relevance
- MedAlpaca consistently performed lower across all datasets and evaluation metrics

## Why This Works (Mechanism)

### Mechanism 1
- Claim: MedDoc-Bot's local deployment of quantized LLMs enables sensitive medical document analysis without exposing patient data to external APIs.
- Mechanism: Quantized models (GGUF format) reduce memory and compute requirements, allowing execution on local hardware while maintaining acceptable performance for clinical queries.
- Core assumption: The performance degradation from quantization is minimal compared to the security benefits of local processing.
- Evidence anchors:
  - [abstract] "The quantized variants from Hugging Face are used to ensure a balance between reduced model size and performance, making the system well-suited for local implementations with limited resources."
  - [section] "The four models are pre-quantized in GGUF (GPT-Generated Unified Format) format introduced by the LLaMA C++ community and retrieved from the Hugging Face repository."

### Mechanism 2
- Claim: LangChain's document processing pipeline enables effective information retrieval from complex medical guidelines.
- Mechanism: Text chunking, vector embedding via Sentence Transformers, and FAISS-based semantic search create a retrieval-augmented generation system that grounds LLM responses in the source document.
- Core assumption: The chunking and embedding approach preserves semantic relationships needed for accurate clinical queries.
- Evidence anchors:
  - [abstract] "The backend utilizes LangChain—a framework designed for language-driven applications for document processing [14]."
  - [section] "The LangChain captures relevant information from the hypertension guideline [2] document by transforming large texts into smaller chunks."

### Mechanism 3
- Claim: Multi-metric evaluation (human, METEOR, chrF) provides comprehensive assessment of LLM performance on medical guidelines.
- Mechanism: Human evaluation captures clinical relevance and fidelity, while automated metrics assess linguistic similarity to reference answers, creating a balanced evaluation framework.
- Core assumption: Human and automated metrics provide complementary insights into different aspects of response quality.
- Evidence anchors:
  - [abstract] "A pediatric expert provides a benchmark for evaluation by formulating questions and responses extracted from the ESC guidelines. The expert rates the model-generated responses based on their fidelity and relevance. Additionally, we evaluated the METEOR and chrF metric scores to assess the similarity of model responses to reference answers."

## Foundational Learning

- Vector embeddings
  - Why needed here: Enables semantic search over medical guidelines by converting text chunks into numerical representations that capture meaning.
  - Quick check question: What library is used to create embeddings for the medical guideline text chunks?

- Language model quantization
  - Why needed here: Allows running large models locally on limited hardware while maintaining acceptable performance for clinical queries.
  - Quick check question: What format is used for the quantized models in this system?

- Retrieval-augmented generation
  - Why needed here: Grounds LLM responses in the actual medical guideline content rather than relying on model's general knowledge.
  - Quick check question: Which framework handles the document processing and retrieval in MedDoc-Bot?

## Architecture Onboarding

- Component map: Streamlit frontend → PDF preprocessing (LangChain) → Vector store (FAISS) → LLM inference (CTransformers) → Response rendering
- Critical path: User query → embedding → vector search → context retrieval → LLM generation → response display
- Design tradeoffs: Local execution provides data security but limits model size; quantization balances performance with resource constraints; vector search adds complexity but improves accuracy
- Failure signatures: Slow responses indicate computational bottlenecks; irrelevant answers suggest embedding or chunking issues; system crashes point to memory constraints
- First 3 experiments:
  1. Test basic PDF upload and text extraction with a simple model
  2. Implement and test the vector search with sample queries
  3. Evaluate response quality with human expert on a small dataset

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do quantized LLMs perform compared to their full-precision counterparts when processing medical guidelines?
- Basis in paper: [inferred] The study used quantized variants of LLMs and noted a balance between reduced model size and performance, but did not directly compare them to full-precision versions.
- Why unresolved: The paper does not provide a direct comparison between quantized and full-precision models, leaving uncertainty about potential performance trade-offs.
- What evidence would resolve it: A comparative study evaluating both quantized and full-precision versions of the same LLMs on the same medical guideline tasks would provide clarity on performance differences.

### Open Question 2
- Question: How generalizable are the evaluated LLMs across different medical specialties beyond pediatric cardiology?
- Basis in paper: [inferred] The study focused on pediatric hypertension guidelines, and while it mentions the tool's versatility for other PDF documents, it does not explore performance across various medical specialties.
- Why unresolved: The paper's evaluation is limited to one specialty, leaving questions about the models' adaptability and accuracy in other medical contexts.
- What evidence would resolve it: Testing the same LLMs on guidelines from diverse medical specialties (e.g., oncology, neurology) and comparing their performance would demonstrate generalizability.

### Open Question 3
- Question: What is the impact of fine-tuning LLMs on domain-specific datasets for medical guideline interpretation?
- Basis in paper: [explicit] The paper mentions ongoing work involving fine-tuning the best-performing models (Llama-2 and Mistral) with a clinical dataset curated by multiple experts.
- Why unresolved: The study does not report results from fine-tuning experiments, leaving uncertainty about the potential improvements in model performance.
- What evidence would resolve it: Conducting fine-tuning experiments and comparing pre- and post-fine-tuning performance metrics (e.g., METEOR, chrF scores) would quantify the impact of domain-specific training.

### Open Question 4
- Question: How do the evaluated LLMs handle multimodal data (e.g., combining text, tables, and figures) in medical guidelines?
- Basis in paper: [explicit] The study mentions transforming figures and tables into textual representations to enhance interpretation, but does not evaluate the models' native handling of multimodal data.
- Why unresolved: The paper's methodology involves preprocessing multimodal data into text, so it does not assess the models' capabilities in directly processing mixed media.
- What evidence would resolve it: Evaluating the LLMs on guidelines with embedded tables and figures (without textual transformation) would reveal their effectiveness in handling multimodal information.

## Limitations
- Narrow scope: Evaluation based on single pediatric hypertension guideline and limited 12-question dataset
- Missing technical details: Exact preprocessing pipeline for visual elements and specific chunking parameters not specified
- Limited generalizability: Results may not extend to other medical specialties or document types

## Confidence
- High Confidence in the core finding that open-source LLMs can effectively process and respond to medical guideline queries
- Medium Confidence in the comparative performance rankings between models given limited dataset and trade-offs between evaluation metrics
- Low Confidence in the generalizability of these results to other medical domains, document types, or clinical decision-making scenarios

## Next Checks
1. **Dataset Expansion Validation**: Replicate the evaluation with multiple medical guidelines across different specialties using the same 12-question benchmark structure to assess domain generalization
2. **Technical Implementation Verification**: Implement the exact preprocessing pipeline for visual elements, test different chunking strategies, and document the full software stack to identify any performance bottlenecks or reproducibility issues
3. **Clinical Impact Assessment**: Conduct a blinded study where clinical experts use the tool to answer medical questions and compare accuracy against their unaided responses, measuring both clinical correctness and time efficiency