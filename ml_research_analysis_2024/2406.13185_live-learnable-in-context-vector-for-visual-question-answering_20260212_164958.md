---
ver: rpa2
title: 'LIVE: Learnable In-Context Vector for Visual Question Answering'
arxiv_id: '2406.13185'
source_url: https://arxiv.org/abs/2406.13185
tags:
- live
- layer
- uni00000013
- shot
- performance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'The paper addresses two challenges in applying In-Context Learning
  (ICL) to Large Multimodal Models (LMMs): increased inference time with more In-Context
  Demonstrations (ICDs) and sensitivity to ICD selection. The authors propose a Learnable
  In-Context Vector (LIVE) to distill essential task information from ICDs into a
  single vector, reducing computational costs and improving robustness.'
---

# LIVE: Learnable In-Context Vector for Visual Question Answering

## Quick Facts
- arXiv ID: 2406.13185
- Source URL: https://arxiv.org/abs/2406.13185
- Reference count: 40
- Primary result: LIVE reduces VQA inference time to 1/24.97 FLOPs of 32-shot ICL while improving accuracy

## Executive Summary
This paper addresses two key challenges in applying In-Context Learning (ICL) to Large Multimodal Models (LMMs): increased inference time with more In-Context Demonstrations (ICDs) and sensitivity to ICD selection. The authors propose a Learnable In-Context Vector (LIVE) that distills essential task information from ICDs into a single vector, reducing computational costs and improving robustness. LIVE is trained to minimize output distribution differences between using ICDs and using LIVE alone. Experiments show that LIVE significantly reduces inference time while improving accuracy on VQA tasks compared to standard ICL approaches.

## Method Summary
LIVE is a learnable vector that replaces ICDs during inference by shifting query representations in a direction similar to what demonstrations would induce. The method trains LIVE to minimize KL divergence between the model's output distributions when using ICDs versus when using LIVE alone. LIVE consists of layer-specific vectors and weights that are added to the model's hidden states at each layer. During training, LIVE is optimized using a combined loss that includes both the KL divergence term and ground truth supervision. The approach is evaluated on VQAv2 and OKVQA datasets using the IDEFICS-9B model.

## Key Results
- LIVE achieves 58.54% accuracy on VQAv2 compared to 56.18% for 32-shot ICL
- Inference time reduced to 1/24.97 FLOPs of 32-shot ICL
- LIVE outperforms LoRA with fewer trainable parameters (131,104 vs 1,155,136)
- LIVE shows robustness to demonstration selection compared to standard ICL

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LIVE captures essential task information by minimizing output distribution differences between demonstrations and LIVE alone.
- Mechanism: During training, LIVE is optimized to align the output distribution P(ˆx|V, α; M) with the distribution obtained using demonstrations P(ˆx|XD; M) via KL divergence minimization. This encourages LIVE to extract the most common task information across different demonstration combinations.
- Core assumption: The task information in demonstrations can be distilled into a single vector that generalizes across different queries.
- Evidence anchors:
  - [abstract]: "LIVE is trained to minimize the output distribution differences between using ICDs and using LIVE alone."
  - [section 3]: "We propose a novel method that involves a Learnable In-Context Vector (LIVE) to simulate the ICL process without actual demonstrations. This approach aims to abstract general task information from demonstrations, enabling it to shift the model's representation toward the direction influenced by the ICDs."
  - [corpus]: Weak - related papers focus on demonstration selection rather than vector distillation.

### Mechanism 2
- Claim: LIVE shifts query representations toward the target direction similar to demonstrations.
- Mechanism: The LIVE vector is added to the model's hidden states at each layer, effectively shifting the query representation in a direction that mimics the effect of demonstrations. This is based on the observation that ICL can be viewed as shifting the latent states of the query.
- Core assumption: The shift direction induced by demonstrations can be learned and approximated by a fixed vector.
- Evidence anchors:
  - [section 3]: "we observe that h(ˆx) is the representation obtained with self-attention over the query ˆx without appending any ICD; h(XD) functions similarly to a 'shift' vector, altering the attention representation h(ˆx) by incorporating contextual information from the ICDs XD."
  - [section 4.4.1]: "Such positive correlation validates the effectiveness of our motivation that a single LIVE can indeed simulate the ICL capability of LMMs by shifting the direction of the query representation."
  - [corpus]: Weak - related papers don't discuss representation shifting mechanisms.

### Mechanism 3
- Claim: Layer-specific LIVE vectors capture fine-grained task information better than single vector approaches.
- Mechanism: Instead of using one vector for all layers, LIVE assigns a unique vector vl and weight factor αl to each layer l. This allows the model to capture task information at different levels of abstraction.
- Core assumption: Different layers in the model play distinct roles in processing demonstrations and queries.
- Evidence anchors:
  - [section 3]: "Additionally, [24] shows that during ICL, different layers of LLM perform a distinct role. Motivated by this, we assume that for LMM, each layer also requires a specific shift direction."
  - [section 4.3]: "Table 20: The performance comparison of using a shared LIVE across all layer (Share LIVE) with LIVE and ICL... This performance drop aligns with the findings in [24] that different Transformer layers play diverse roles in ICL."
  - [corpus]: Weak - related papers don't discuss layer-specific vector approaches.

## Foundational Learning

- Concept: In-Context Learning (ICL)
  - Why needed here: LIVE is designed to replace demonstrations in ICL, so understanding how ICL works is crucial.
  - Quick check question: How does prefixing demonstrations to a query affect the model's output in ICL?

- Concept: Transformer self-attention mechanism
  - Why needed here: LIVE modifies the self-attention outputs by adding shift vectors, so understanding this mechanism is essential.
  - Quick check question: How does the self-attention mechanism combine query, key, and value representations?

- Concept: Kullback-Leibler (KL) divergence
  - Why needed here: KL divergence is used to train LIVE by minimizing the difference between output distributions.
  - Quick check question: What does KL divergence measure between two probability distributions?

## Architecture Onboarding

- Component map:
  Input (query + demonstrations during training, query only during inference) -> LIVE vectors (layer-specific vl and αl) -> LMM (IDEFICS-9B with modified forward pass) -> Output (modified output distribution)

- Critical path:
  1. Training: Compute output distributions with demonstrations and with LIVE, minimize KL divergence
  2. Inference: Add LIVE vectors to hidden states at each layer, generate output without demonstrations

- Design tradeoffs:
  - Pros: Reduced inference time, less sensitive to demonstration selection, fewer trainable parameters
  - Cons: May not capture all task-specific nuances, requires training data, layer-specific vectors increase complexity

- Failure signatures:
  - Poor performance on tasks with highly diverse or complex requirements
  - Overfitting to training data if too few training samples are used
  - Suboptimal performance if layer-specific roles are not well understood

- First 3 experiments:
  1. Compare LIVE performance with and without layer-specific vectors to validate the importance of layer-wise processing
  2. Test LIVE with different numbers of training samples to find the optimal training set size
  3. Evaluate LIVE on tasks with varying complexity to understand its limitations and capabilities

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of LIVE scale with increasing numbers of trainable parameters beyond those tested in the paper?
- Basis in paper: [inferred] The paper compares LIVE with LoRA, which has comparable trainable parameters (131,104 vs. 1,155,136). It would be interesting to see if further increasing LIVE's parameters could lead to even better performance.
- Why unresolved: The paper only tests a specific number of trainable parameters for LIVE and does not explore the impact of scaling these parameters.
- What evidence would resolve it: Experiments varying the number of trainable parameters for LIVE and measuring its performance on VQA tasks.

### Open Question 2
- Question: Can LIVE be effectively applied to other multimodal tasks beyond VQA, such as image captioning or visual reasoning?
- Basis in paper: [explicit] The paper mentions that LIVE is evaluated on VQA tasks and suggests potential application to other multimodal tasks in the conclusion. However, it does not provide experimental results for these tasks.
- Why unresolved: The paper focuses on VQA and does not explore LIVE's performance on other multimodal tasks.
- What evidence would resolve it: Experiments applying LIVE to various multimodal tasks and comparing its performance to other methods like ICL and LoRA.

### Open Question 3
- Question: How does the choice of demonstrations (e.g., random vs. retrieved based on similarity) impact the effectiveness of LIVE during training?
- Basis in paper: [inferred] The paper uses randomly sampled demonstrations during training, but does not explore the impact of using different strategies for selecting demonstrations. It mentions that LIVE trained with RICE samples had lower performance, suggesting that the choice of demonstrations matters.
- Why unresolved: The paper only uses random sampling and does not investigate the impact of other demonstration selection strategies.
- What evidence would resolve it: Experiments training LIVE with different demonstration selection strategies and comparing their performance on VQA tasks.

## Limitations
- Limited empirical validation beyond VQA tasks despite claims of generalizability to other vision-language tasks
- Unclear whether the learned representation shift generalizes across diverse query types or remains task-specific
- Heavy reliance on random demonstration sampling during training without exploring more sophisticated selection strategies

## Confidence

- **High Confidence**: The computational efficiency claims (FLOPs reduction) and baseline comparisons (LIVE vs. LoRA) are well-supported by experimental results and clear methodology.
- **Medium Confidence**: The core mechanism of using KL divergence to train LIVE is sound, but the claim that this effectively captures "essential task information" lacks rigorous validation.
- **Low Confidence**: Claims about LIVE's ability to generalize across different types of vision-language tasks and its robustness to demonstration selection are largely unsupported by evidence.

## Next Checks

1. **Layer Granularity Analysis**: Conduct experiments testing whether LIVE vectors per block (rather than per layer) or per type of layer (attention vs. feed-forward) can achieve similar performance with fewer parameters. This would validate whether the layer-specific approach is necessary or optimal.

2. **Cross-Task Generalization Test**: Evaluate LIVE on non-VQA vision-language tasks such as image captioning, visual reasoning, or referring expression comprehension. This would directly test the paper's claim about generalizability beyond the VQA domain.

3. **Representation Space Analysis**: Perform t-SNE or other visualization of query representations with and without LIVE across different demonstration sets. This would provide direct evidence of whether LIVE consistently shifts representations toward the demonstration-influenced space as claimed.