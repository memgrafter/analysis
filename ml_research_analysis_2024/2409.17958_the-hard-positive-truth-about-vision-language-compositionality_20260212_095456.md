---
ver: rpa2
title: The Hard Positive Truth about Vision-Language Compositionality
arxiv_id: '2409.17958'
source_url: https://arxiv.org/abs/2409.17958
tags:
- hard
- positives
- clip
- caption
- negative
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Current benchmarks for vision-language compositionality only evaluate
  models on hard negatives, ignoring hard positives. This leads to models that are
  overly sensitive to semantic-preserving changes in captions, degrading performance
  by up to 38.7%.
---

# The Hard Positive Truth about Vision-Language Compositionality

## Quick Facts
- **arXiv ID**: 2409.17958
- **Source URL**: https://arxiv.org/abs/2409.17958
- **Reference count**: 24
- **Primary result**: Current vision-language benchmarks over-emphasize hard negatives, causing models to be overly sensitive to semantic-preserving changes in captions, degrading performance by up to 38.7%.

## Executive Summary
Current vision-language compositionality benchmarks evaluate models almost exclusively on hard negatives (captions that semantically differ from images), while ignoring hard positives (semantically equivalent but perturbed captions). This creates a blind spot where models learn to be oversensitive to any perturbation, degrading performance by up to 38.7% on semantic-preserving changes. The authors create a new evaluation set with both hard positives and negatives, and demonstrate that training models with balanced exposure to both types improves compositionality while maintaining performance on standard benchmarks. Their approach achieves up to 12.1% improvement in Augmented Test Accuracy and reduces brittleness by up to 5.7%.

## Method Summary
The authors create a new evaluation benchmark containing both hard positives (semantically equivalent but perturbed captions) and hard negatives (semantically different captions). They generate hard positives using an LLM (LLaMA2 70B-Chat) and hard negatives using the CREPE procedure. They then finetune CLIP ViT-B/32 on a combined dataset of 1,775,259 image-text pairs with balanced hard positives and negatives using a contrastive loss. The finetuned models are evaluated on REPLACE and SWAP benchmarks using Augmented Test Accuracy and Brittleness metrics. The approach shows improved compositionality while maintaining performance on existing benchmarks.

## Key Results
- CLIP's performance degrades by 14.9% on the new benchmark versus standard ones
- Finetuning with only hard negatives causes up to 38.7% decrease in performance on hard positives
- Training with balanced hard positives and negatives improves Augmented Test Accuracy by up to 12.1% and reduces brittleness by up to 5.7%

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Hard negative finetuning improves sensitivity to semantic changes but causes oversensitivity to semantic-preserving changes
- Mechanism: Models learn that any perturbation changes meaning by being penalized for matching perturbed captions to images, ignoring whether the change is semantic or not
- Core assumption: Training signals from hard negatives do not distinguish between semantic and non-semantic changes
- Evidence anchors:
  - [abstract]: "We uncover that including hard positives decreases CLIPâ€™s performance by 12.9%, while humans perform effortlessly at 99%. CLIP finetuned with hard negatives results in an even larger decrease, up to 38.7%."
  - [section]: "Per Section 4.1, to evaluate model brittleness, we calculate the percentage of instances in the benchmark where s(c|i) > s(cn|i) > s(cp|i) or s(cp|i) > s(cn|i) > s(c|i)."
- Break condition: If training includes both hard positives and negatives with balanced weighting, the oversensitivity should reduce

### Mechanism 2
- Claim: Training with both hard positives and hard negatives improves compositional understanding without degrading performance on standard tasks
- Mechanism: Balanced exposure teaches models to distinguish when perturbations change meaning versus when they preserve it, leading to better generalization
- Core assumption: Balanced training signals allow models to learn nuanced semantic boundaries
- Evidence anchors:
  - [abstract]: "By training with both, we see improvements on existing benchmarks while simultaneously improving performance on hard positives, indicating a more robust improvement in compositionality."
  - [section]: "Our experiments reveal that the default CLIP model (Radford et al., 2021) performs14.9% worse on our data versus on existing benchmarks."
- Break condition: If the ratio of hard positives to hard negatives is skewed too far in either direction, performance on the neglected type degrades

### Mechanism 3
- Claim: Oversensitivity to one perturbation type transfers to other perturbation types
- Mechanism: Models learn general perturbation detection rather than perturbation-specific rules, causing brittleness across different perturbation types
- Core assumption: Models do not learn perturbation-type-specific boundaries but rather a general "perturbation equals change" heuristic
- Evidence anchors:
  - [abstract]: "Worse, we test 7 CLIP finetuning approaches... to find even sharper decreases in performance, up to 38.7%."
  - [section]: "We see that models trained onREPLACE hard negatives are still brittle to SWAP hard positives... more so than the original CLIP baseline."
- Break condition: If models are trained on balanced hard positives and negatives for each perturbation type, cross-type brittleness should reduce

## Foundational Learning

- Concept: Compositionality in vision-language models
  - Why needed here: The paper's core contribution is evaluating and improving compositional reasoning in vision-language models
  - Quick check question: What is the difference between a hard negative and a hard positive in the context of vision-language compositionality?

- Concept: Image-text matching (ITM) task
  - Why needed here: The evaluation protocol relies on ITM to measure model performance on hard positives and negatives
  - Quick check question: How does the ITM task differ when evaluating compositionality versus standard retrieval?

- Concept: Contrastive learning and finetuning
  - Why needed here: The paper's methods involve contrastive finetuning with hard positives and negatives to improve model performance
  - Quick check question: What is the role of contrastive loss in vision-language model training?

## Architecture Onboarding

- Component map:
  CLIP model (ViT-B/32) -> Hard negative generation (CREPE) -> Hard positive generation (LLM) -> Finetuning pipeline with balanced hard positives and negatives -> Evaluation benchmarks (REPLACE, SWAP)

- Critical path:
  1. Generate hard positives using LLM
  2. Generate hard negatives using CREPE
  3. Finetune CLIP on balanced dataset
  4. Evaluate on REPLACE and SWAP benchmarks

- Design tradeoffs:
  - Balancing hard positive and negative ratios during finetuning
  - Choosing perturbation types (REPLACE vs SWAP)
  - Deciding on LLM for hard positive generation

- Failure signatures:
  - Oversensitivity to perturbations (high brittleness scores)
  - Degradation in performance on standard benchmarks
  - Failure to generalize across perturbation types

- First 3 experiments:
  1. Evaluate base CLIP model on REPLACE and SWAP benchmarks to establish baseline brittleness
  2. Finetune CLIP with only hard negatives and evaluate on both REPLACE and SWAP to measure oversensitivity
  3. Finetune CLIP with balanced hard positives and negatives and compare performance to previous experiments

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What specific architectural modifications or training objectives could improve CLIP models' understanding of relational semantics without introducing oversensitivity to semantic-preserving perturbations?
- Basis in paper: [explicit] The authors note that even basic text encoders understand semantic relationships better than CLIP, and call for further research into alternative architecture designs and training objectives
- Why unresolved: The paper only shows that current approaches fail and that hard positive finetuning helps, but doesn't explore what architectural changes might fundamentally improve relational understanding
- What evidence would resolve it: Comparative studies of CLIP-like models with modified attention mechanisms, different loss functions, or novel training strategies showing improved relational understanding while maintaining robustness to semantic-preserving changes

### Open Question 2
- Question: How does the oversensitivity problem manifest in vision-language generation models like Flamingo, BLIP, or GPT-4V, and what training strategies could mitigate this issue?
- Basis in paper: [explicit] The authors state that their work is limited to CLIP-style models and that there's a need to evaluate generation models to isolate effects of architecture and training objective
- Why unresolved: The study only examines CLIP-style models using image-text matching, not generation models that produce new text or images
- What evidence would resolve it: Systematic evaluation of generation models on hard positive/negative benchmarks, showing whether they exhibit similar oversensitivity and whether proposed mitigation strategies (like hard positive training) are effective

### Open Question 3
- Question: What is the optimal ratio of hard positive to hard negative loss during training, and how does this ratio affect performance across different types of compositional tasks?
- Basis in paper: [explicit] The authors explore varying this ratio and find a trade-off between test accuracy and brittleness, suggesting careful tuning is needed
- Why unresolved: The paper only tests a few discrete ratios and finds that some hard negatives are needed, but doesn't determine the optimal balance or how it varies by task type
- What evidence would resolve it: Comprehensive ablation studies across multiple compositional benchmarks showing how different ratios affect performance on specific types of compositionality (spatial relations, attributes, actions, etc.)

### Open Question 4
- Question: How well do models trained on hard positives generalize to unseen types of semantic-preserving perturbations that weren't included in the training data?
- Basis in paper: [explicit] The authors note a limitation that their models improve on hard positives but more research is needed to generalize to unseen types
- Why unresolved: The training data only includes specific types of perturbations (replacements and swaps), so it's unclear if the learned robustness transfers to other semantic-preserving changes
- What evidence would resolve it: Testing models on novel perturbation types (e.g., paraphrasing, different word orders, new synonym replacements) that weren't seen during training to measure generalization capability

## Limitations

- Data provenance limitations: The study relies on COCO captions and unspecified image sources, limiting generalizability to other domains
- Training procedure opacity: Key implementation details for CREPE and LLM-based hard positive generation are not fully specified
- Cross-perturbation brittleness mechanism: The underlying reasons for transfer of oversensitivity across perturbation types are not fully explored
- Human baseline comparability: The 99% human performance baseline lacks methodological details for validation

## Confidence

**High confidence** in the core observation that current benchmarks over-represent hard negatives while under-representing hard positives. The empirical evidence showing CLIP performance drops of 14.9% on their benchmark versus standard ones is robust and clearly documented.

**Medium confidence** in the proposed mechanism that balanced training with both hard positives and negatives improves compositional understanding. While the results are compelling (12.1% improvement in Augmented Test Accuracy), the exact causal relationship between training balance and compositional reasoning isn't fully established.

**Low confidence** in the generalizability of findings to models beyond CLIP and to real-world deployment scenarios. The study is limited to CLIP ViT-B/32 and controlled benchmark conditions without testing on naturalistic data or alternative architectures.

## Next Checks

1. **Cross-dataset generalization test**: Evaluate the finetuned models on Flickr30k and Conceptual Captions datasets to verify whether improvements in compositionality transfer beyond the COCO-based training and evaluation data.

2. **Ablation on hard positive generation**: Systematically vary the quality and diversity of LLM-generated hard positives (e.g., using different temperatures, prompt strategies, or smaller models) to determine whether the observed improvements depend on the sophistication of the hard positive generation process.

3. **Real-world brittleness evaluation**: Create a naturalistic test set by applying semantic-preserving transformations to captions from real user queries or diverse image sources, then measure whether the improved compositional understanding translates to better robustness in practical applications.