---
ver: rpa2
title: 'MindSet: Vision. A toolbox for testing DNNs on key psychological experiments'
arxiv_id: '2404.05290'
source_url: https://arxiv.org/abs/2404.05290
tags:
- dataset
- object
- human
- visual
- images
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'MindSet: Vision is a toolbox providing 30 image datasets and scripts
  to test DNNs on psychological phenomena in vision, addressing the gap between observational
  DNN benchmarks and controlled psychological experiments. Each dataset is systematically
  manipulated to test specific hypotheses about human visual perception and object
  recognition.'
---

# MindSet: Vision. A toolbox for testing DNNs on key psychological experiments

## Quick Facts
- arXiv ID: 2404.05290
- Source URL: https://arxiv.org/abs/2404.05290
- Authors: Valerio Biscione; Dong Yin; Gaurav Malhotra; Marin Dujmovic; Milton L. Montero; Guillermo Puebla; Federico Adolfi; Rachel F. Heaton; John E. Hummel; Benjamin D. Evans; Karim Habashy; Jeffrey S. Bowers
- Reference count: 40
- MindSet: Vision is a toolbox providing 30 image datasets and scripts to test DNNs on psychological phenomena in vision, addressing the gap between observational DNN benchmarks and controlled psychological experiments.

## Executive Summary
MindSet: Vision addresses the gap between observational DNN benchmarks and controlled psychological experiments by providing 30 image datasets and scripts to test DNNs on key psychological phenomena in vision. Each dataset is systematically manipulated to test specific hypotheses about human visual perception and object recognition. The toolbox includes pre-generated datasets and code to regenerate them with configurable parameters, along with scripts for three testing methods: similarity judgments, out-of-distribution classification, and decoder method.

Testing ResNet-152 as an example, the authors demonstrate the toolbox's application, finding limited alignment with human perception across various visual phenomena, including Gestalt effects, visual illusions, and shape recognition tasks. The toolbox aims to facilitate rigorous testing of DNN models against key psychological findings to improve understanding of human vision and guide the development of better DNN models.

## Method Summary
MindSet: Vision provides 30 image datasets systematically manipulated to test specific hypotheses about human visual perception. The toolbox includes pre-generated datasets and code to regenerate them with configurable parameters. Three testing methods are provided: similarity judgments comparing pairwise activation patterns to human performance, out-of-distribution classification testing pre-trained DNNs on novel image formats, and decoder method training small networks attached to frozen DNNs to reveal encoded information. The authors demonstrate the toolbox using ResNet-152, showing limited alignment with human perception across various visual phenomena including Gestalt effects, visual illusions, and shape recognition tasks.

## Key Results
- Provides 30 image datasets systematically manipulated to test specific hypotheses about human visual perception
- Includes three testing methods (similarity judgments, OOD classification, decoder method) for comprehensive evaluation
- Demonstrates limited alignment between ResNet-152 and human perception across various visual phenomena
- Offers parameterized dataset generation with configurable parameters for versatile research applications
- Addresses the gap between observational DNN benchmarks and controlled psychological experiments

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Controlled experimental manipulation enables isolation of specific perceptual mechanisms.
- Mechanism: By systematically varying independent variables (e.g., line length, texture, occlusion), the toolbox creates stimuli that isolate specific perceptual phenomena (e.g., Weber's Law, crowding, Gestalt grouping). This allows testing whether DNNs exhibit the same perceptual biases as humans under controlled conditions.
- Core assumption: Perceptual phenomena can be isolated through systematic manipulation of image features.
- Evidence anchors:
  - [abstract] "In all experimental conditions, the stimuli are systematically manipulated to test specific hypotheses regarding human visual perception"
  - [section] "Our stimuli cover aspects of low and mid-level vision (including Gestalt phenomena), visual illusions, and object recognition tasks"
- Break condition: If DNNs do not show similar patterns under controlled manipulation, it indicates a fundamental difference in perceptual processing.

### Mechanism 2
- Claim: Multiple testing methods provide convergent evidence for model-human alignment.
- Mechanism: The toolbox provides three complementary testing methods (similarity judgments, out-of-distribution classification, and decoder method) that each capture different aspects of perceptual processing. Using multiple methods on the same dataset provides more robust conclusions about model behavior.
- Core assumption: Different testing methods capture different aspects of perceptual processing and should converge when models align with human perception.
- Evidence anchors:
  - [abstract] "code to facilitate the testing of DNNs on these image datasets using three different methods (similarity judgments, out-of-distribution classification, and decoder method)"
  - [section] "Each dataset is designed to align with at least one of three methods of testing, but other approaches can be used as well"
- Break condition: If methods yield conflicting results, it suggests the need for additional investigation into which method best captures human-like perception.

### Mechanism 3
- Claim: Parameterized datasets enable systematic investigation across multiple conditions.
- Mechanism: Each dataset can be regenerated with configurable parameters (image size, background color, stroke color, etc.), allowing researchers to test model behavior across a wide range of conditions and identify specific parameter ranges where models align or diverge from human perception.
- Core assumption: Perceptual phenomena are robust across variations in low-level image parameters.
- Evidence anchors:
  - [abstract] "we provide code to regenerate these datasets, offering many configurable parameters which greatly extend the dataset versatility for different research contexts"
  - [section] "To facilitate experimentation across a variety of scenario, each dataset can be easily regenerated across different configurations"
- Break condition: If model behavior changes dramatically with parameter variations, it suggests sensitivity to low-level features rather than higher-level perceptual mechanisms.

## Foundational Learning

- Concept: Controlled experimental manipulation
  - Why needed here: Understanding how systematic variation of image features can isolate specific perceptual phenomena is crucial for interpreting the results from MindSet: Vision
  - Quick check question: Why is it important that the stimuli are "systematically manipulated" rather than naturalistic?

- Concept: Out-of-distribution testing
  - Why needed here: Many datasets use modified versions of ImageNet categories to test whether DNNs can generalize human-like recognition to novel image formats
  - Quick check question: How does out-of-distribution testing differ from standard classification benchmarks?

- Concept: Decoder methods for probing internal representations
  - Why needed here: The decoder approach attaches simple networks to DNN layers to test what information is encoded at different processing stages
  - Quick check question: What does it mean if a decoder trained on scrambled images performs poorly on illusory conditions?

## Architecture Onboarding

- Component map: Dataset generation scripts (parameterized, TOML configuration) -> Pre-generated datasets (5000+ images per condition) -> Testing method scripts (similarity judgment, OOD classification, decoder) -> Configuration files (TOML format) -> Output processors (pandas DataFrames)

- Critical path: Generate dataset → Choose testing method → Configure parameters → Run tests → Analyze results

- Design tradeoffs:
  - Parameter flexibility vs. dataset size (more parameters = more combinations)
  - Pre-generated vs. on-demand generation (convenience vs. customization)
  - Multiple testing methods vs. simplicity (comprehensive vs. streamlined)

- Failure signatures:
  - Empty output directories (configuration errors)
  - NaN values in similarity matrices (numerical issues)
  - Zero classification accuracy (dataset loading problems)
  - Decoder training divergence (learning rate too high)

- First 3 experiments:
  1. Run similarity judgment analysis on Texturized Unfamiliar dataset with default ResNet-152
  2. Test OOD classification on Line Drawings dataset with pre-trained ResNet-152
  3. Apply decoder method to Ebbinghaus Illusion dataset with default parameters

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can DNNs trained on naturalistic images show sensitivity to Weber's Law for image intensity?
- Basis in paper: [explicit] The authors note that Jacob et al. [65] only observed Weber's Law for line lengths in the late convolutional layers of VGG-16, and did not assess whether discrimination was a constant ratio of the original stimulus for image intensity.
- Why unresolved: Previous research has only tested Weber's Law for line length, not image intensity, and the findings were limited to specific layers of the network.
- What evidence would resolve it: Testing DNNs on datasets designed to assess Weber's Law for image intensity, and examining the results across multiple layers of the network.

### Open Question 2
- Question: Can DNNs exhibit human-like perceptual grouping based on texture similarity?
- Basis in paper: [explicit] The authors note that humans can group elements in scenes based on texture, with texture regions defined by the similarity of their elements. This is an example of a Gestalt principle (Similarity) contributing to object recognition [13].
- Why unresolved: While the authors provide datasets to test DNNs on texture-based grouping, they do not report results for this specific aspect of human perception.
- What evidence would resolve it: Testing DNNs on the Texturized Unfamiliar dataset using similarity judgment analysis, and comparing the results to human performance on the same task.

### Open Question 3
- Question: Can DNNs support amodal completion, the process of completing occluded objects in the visual field?
- Basis in paper: [explicit] The authors note that Jacob et al. [65] reported that the DNN VGG-16 network pre-trained on ImageNet failed to show any evidence for amodal completion with the stimuli used in their study.
- Why unresolved: The authors provide a dataset to test DNNs on amodal completion, but do not report results for this specific aspect of human perception.
- What evidence would resolve it: Testing DNNs on the Amodal Completion dataset using similarity judgment analysis, and comparing the results to human performance on the same task.

## Limitations

- The controlled experimental conditions may not fully capture the complexity of human visual perception, which involves dynamic contextual factors, attention mechanisms, and prior knowledge
- The effectiveness of the three testing methods in capturing human-like perception remains uncertain, as different methods may emphasize different aspects of visual processing
- The authors demonstrate limited alignment between ResNet-152 and human perception, suggesting fundamental differences in processing mechanisms that require further investigation

## Confidence

- **High confidence**: The existence and basic functionality of the MindSet: Vision toolbox, including the 30 image datasets and three testing methods
- **Medium confidence**: The claim that systematic manipulation enables isolation of specific perceptual mechanisms, supported by the authors' demonstration with ResNet-152
- **Medium confidence**: The claim that multiple testing methods provide convergent evidence, though the degree of convergence across different perceptual phenomena requires further validation

## Next Checks

1. Conduct a systematic comparison of model performance across all three testing methods on the same datasets to evaluate the consistency and complementarity of the approaches
2. Test a diverse set of DNN architectures beyond ResNet-152 (e.g., Vision Transformers, ConvNeXt) to determine whether alignment with human perception is architecture-dependent
3. Implement a parameter sensitivity analysis by varying image properties (size, color, texture) across multiple orders of magnitude to identify specific conditions where models align or diverge from human perception