---
ver: rpa2
title: Learning User Embeddings from Human Gaze for Personalised Saliency Prediction
arxiv_id: '2403.13653'
source_url: https://arxiv.org/abs/2403.13653
tags:
- user
- saliency
- embeddings
- embedding
- prediction
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a method to learn user embeddings from human
  gaze data for personalised saliency prediction. The method uses a Siamese convolutional
  neural network to contrast image-saliency map pairs from different users, learning
  discriminative embeddings that capture individual viewing behaviour.
---

# Learning User Embeddings from Human Gaze for Personalised Saliency Prediction

## Quick Facts
- **arXiv ID**: 2403.13653
- **Source URL**: https://arxiv.org/abs/2403.13653
- **Authors**: Florian Strohm; Mihai BÃ¢ce; Andreas Bulling
- **Reference count**: 40
- **Primary result**: Learn user embeddings from gaze data for personalised saliency prediction

## Executive Summary
This paper introduces a method to learn user embeddings from human gaze data to improve personalised saliency prediction. The approach uses a Siamese convolutional neural network to contrast image-saliency map pairs from different users, learning discriminative embeddings that capture individual viewing behaviour. These embeddings are then integrated into a saliency prediction network to refine universal saliency maps to individual users. The method is evaluated on two public saliency datasets, demonstrating high discriminative power of the learned embeddings and improved personalised saliency prediction performance compared to baselines that do not use user embeddings.

## Method Summary
The method involves learning user embeddings from gaze data using a Siamese convolutional neural network. The network contrasts image-saliency map pairs from different users, learning discriminative embeddings that capture individual viewing behaviour. These embeddings are then integrated into a saliency prediction network, which refines universal saliency maps to individual users. The approach is trained using triplet margin loss and mean squared error loss, and evaluated on two public saliency datasets where each participant observed all images.

## Key Results
- Learned embeddings demonstrate high discriminative power, enabling effective comparison of users based on their gaze behaviour
- Embeddings generalise well across users and images, leading to improved personalised saliency prediction performance
- Results show the potential of using implicit gaze behaviour to create reusable user embeddings for personalised applications

## Why This Works (Mechanism)
The method works by learning discriminative embeddings that capture individual viewing behaviour from human gaze data. These embeddings are then used to refine universal saliency maps to individual users, improving personalised saliency prediction performance. The Siamese convolutional neural network contrasts image-saliency map pairs from different users, learning embeddings that generalise well across users and images.

## Foundational Learning
- **Saliency prediction**: Predicting where people look in images, essential for understanding visual attention and improving user experience in various applications
- **Siamese networks**: Neural networks that learn to compare inputs, used here to contrast image-saliency map pairs from different users and learn discriminative embeddings
- **Triplet margin loss**: A loss function used to train the Siamese network, encouraging the network to learn embeddings that are closer for similar pairs and farther for dissimilar pairs
- **Mean squared error loss**: A loss function used to train the saliency prediction network, measuring the difference between predicted and ground truth saliency maps

## Architecture Onboarding

### Component Map
Siamese CNN -> User Embedding Layer -> Saliency Prediction Network

### Critical Path
The critical path involves learning user embeddings from gaze data using the Siamese CNN, then integrating these embeddings into the saliency prediction network to refine universal saliency maps to individual users.

### Design Tradeoffs
- Using a Siamese network to learn discriminative embeddings vs. other methods for learning user representations
- Integrating learned embeddings into the saliency prediction network vs. using a separate user-specific model for each user

### Failure Signatures
- Overfitting due to insufficient data or model complexity
- Poor generalisation to unseen users or images

### First 3 Experiments
1. Preprocess the ID and PS datasets to obtain image-saliency map pairs for each user
2. Implement the Siamese CNN architecture for learning user embeddings, training with triplet margin loss
3. Integrate the learned embeddings into a saliency prediction network and train with mean squared error loss

## Open Questions the Paper Calls Out
None

## Limitations
- Lack of architectural details for the Siamese CNN and specific hyperparameter values
- Evaluation focuses on datasets where each participant viewed all images, which may not generalise to more realistic scenarios
- Does not discuss potential biases in the datasets or how the method would handle users with limited gaze data

## Confidence
The paper demonstrates medium confidence in its core claims due to several limitations. The main uncertainties stem from the lack of architectural details for the Siamese CNN and specific hyperparameter values, which are critical for faithful reproduction. The evaluation focuses on datasets where each participant viewed all images, which may not generalise to more realistic scenarios with varying image exposure. Additionally, the paper does not discuss potential biases in the datasets or how the method would handle users with limited gaze data.

## Next Checks
1. Test the learned embeddings on datasets with varying image exposure
2. Evaluate the method's performance with limited gaze data per user
3. Assess the impact of different Siamese CNN architectures on the discriminative power of the embeddings