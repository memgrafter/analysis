---
ver: rpa2
title: Stochastic Q-learning for Large Discrete Action Spaces
arxiv_id: '2405.10310'
source_url: https://arxiv.org/abs/2405.10310
tags:
- stochastic
- action
- q-learning
- actions
- maximization
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a class of value-based reinforcement learning
  methods for large discrete action spaces that replace expensive exact maximization
  operations with stochastic sub-linear alternatives. The key idea is to sample a
  small random subset of actions (size O(log n)) to approximate max and arg max operations,
  significantly reducing per-step computational complexity.
---

# Stochastic Q-learning for Large Discrete Action Spaces

## Quick Facts
- arXiv ID: 2405.10310
- Source URL: https://arxiv.org/abs/2405.10310
- Authors: Fares Fourati; Vaneet Aggarwal; Mohamed-Slim Alouini
- Reference count: 40
- One-line primary result: Stochastic Q-learning methods achieve up to 60x wall time speedup on large discrete action spaces while maintaining competitive performance

## Executive Summary
This paper introduces a class of value-based reinforcement learning methods for large discrete action spaces that replace expensive exact maximization operations with stochastic sub-linear alternatives. The key idea is to sample a small random subset of actions (size O(log n)) to approximate max and arg max operations, significantly reducing per-step computational complexity. The proposed methods include Stochastic Q-learning, Stochastic Double Q-learning, StochDQN, and StochDDQN. Theoretical analysis establishes convergence of Stochastic Q-learning to a modified Bellman optimality operator. Experiments on control tasks show that stochastic methods achieve wall time speedups of up to 60x compared to deterministic counterparts while maintaining competitive or superior performance in terms of rewards.

## Method Summary
The paper proposes replacing exact maximization operations in Q-learning and DQN algorithms with stochastic sampling of O(log n) actions. The core mechanism involves sampling a random subset of actions and computing max/argmax only over this subset, reducing computational complexity from O(n) to O(log n). The method includes both memoryless and memory-based variants, where memory-based sampling can recover exact maximization as the Q-function stabilizes. Theoretical analysis proves convergence of Stochastic Q-learning to a modified Bellman optimality operator. The methods are evaluated on control tasks with discretized action spaces, demonstrating significant wall time improvements while maintaining competitive performance.

## Key Results
- Stochastic methods achieve wall time speedups of up to 60x compared to deterministic counterparts
- On HalfCheetah-v4 with 4096 actions, StochDQN reaches near-optimal returns in 2 hours versus 17 hours for DQN
- The methods maintain competitive or superior performance in terms of rewards while dramatically reducing computation time

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Replacing exact maximization with stochastic sampling of O(log n) actions reduces computational complexity from O(n) to O(log n).
- Mechanism: The stoch max and stoch arg max operations sample a random subset R ⊆ A of size ⌈log(n)⌉ and optionally combine it with memory M of previously exploited actions. This avoids exhaustive search over all n actions while still returning near-optimal values with high probability.
- Core assumption: The sampled subset retains sufficient probability of including near-optimal or optimal actions, especially when combined with memory-based sampling.
- Evidence anchors:
  - [abstract]: "sample a small random subset of actions (size O(log n)) to approximate max and arg max operations"
  - [section 4.1]: "stoch max is the maximum value computed from the union set C = R ∪ M"
  - [corpus]: Weak — none of the 25 related papers explicitly describe logarithmic sampling strategies.
- Break condition: If the action value distribution is highly irregular or has many actions with similar values, the sampled subset may consistently miss the true optimum, leading to degraded performance.

### Mechanism 2
- Claim: Stochastic Q-learning converges to a modified Bellman optimality operator defined over sampled action subsets.
- Mechanism: The update rule uses stoch max over a randomly sampled subset C following distribution P. Theorem 6.2 proves convergence to Q∗ defined by the sampled max operator, with contraction properties preserved under sampling.
- Core assumption: The sampling distribution P is fixed and independent of the learning process; the learning rate conditions (∑ αt = ∞, ∑ αt² < ∞) hold.
- Evidence anchors:
  - [section 6]: "Stochastic Q-learning converges to Q∗, defined in Eq. (11), which recovers the convergence guarantees of Q-learning when the sampling distribution is P(A) = 1"
  - [corpus]: Weak — convergence analysis for stochastic maximization is not addressed in related works.
- Break condition: If the sampling distribution P is time-varying or correlated with Q-function updates, the contraction property may fail, breaking convergence.

### Mechanism 3
- Claim: Memory-based stochastic maximization can recover exact maximization as the Q-function stabilizes.
- Mechanism: As Q-function values converge, the set of optimal actions stabilizes. Once the memory M includes an optimal action, stoch max becomes equivalent to max because the optimal action will be present in C = R ∪ M with high probability.
- Core assumption: The Q-function becomes stable (maximizing action does not change over iterations) before memory-based sampling loses effectiveness.
- Evidence anchors:
  - [section 5.2]: "memory-based stochastic maximization... can become an exact maximization when the Q-function becomes stable"
  - [corpus]: Weak — stability arguments for stochastic maximization are not discussed in related works.
- Break condition: If the environment is non-stationary or the Q-function oscillates, memory-based sampling may never capture the true optimum, and the method remains approximate.

## Foundational Learning

- Concept: Markov Decision Process (MDP) formulation with states S, actions A, transition probabilities P, reward function r, and discount factor γ.
  - Why needed here: All RL algorithms, including stochastic variants, are defined over an MDP framework; understanding this is essential for interpreting updates and convergence.
  - Quick check question: What is the Bellman equation for the state-action value function Qπ(s,a) under policy π?

- Concept: Q-learning update rule and its computational bottleneck.
  - Why needed here: The paper's motivation is to reduce the O(n) complexity of max over actions in Q-learning; understanding the exact update is crucial.
  - Quick check question: In the Q-learning update Q(s,a) ← Q(s,a) + α[r + γ max_b Q(s',b) - Q(s,a)], which term dominates computational cost for large action spaces?

- Concept: Function approximation with neural networks for Q-values.
  - Why needed here: Deep Q-networks (DQN) are used for continuous state spaces; understanding how they output Q-values for all actions is key to seeing why stochastic sampling helps.
  - Quick check question: In a DQN with discretized actions, how many output neurons are needed if there are n discrete actions?

## Architecture Onboarding

- Component map:
  Environment interface (Gymnasium/MuJoCo) -> Action discretization module -> Stochastic sampling engine -> Q-network (or tabular Q-table) -> Update scheduler -> Replay buffer (for DQN variants) -> Target network (for DQN/DDQN)

- Critical path:
  1. Observe state s
  2. Sample random subset R ⊆ A of size ⌈log(n)⌉
  3. Combine with memory M (if enabled) to form C
  4. Compute Q-values for actions in C
  5. Select action via stoch arg max over C
  6. Execute action, observe reward r and next state s'
  7. Sample new subset R' for next-state maximization
  8. Update Q(s,a) using stoch max over R'

- Design tradeoffs:
  - Larger subset size improves approximation but increases computation (logarithmic scaling)
  - Memory inclusion improves convergence but requires tracking last exploited actions
  - Fixed vs. adaptive sampling distribution affects convergence guarantees
  - Neural network architecture must support efficient batch evaluation over sampled actions

- Failure signatures:
  - Slow or no learning: sampled subsets consistently miss high-value actions
  - High variance in returns: stochastic max frequently selects suboptimal actions
  - Memory corruption: incorrect tracking of last exploited actions leads to stale M
  - Divergence: learning rate too high or sampling distribution changes over time

- First 3 experiments:
  1. Implement tabular Stochastic Q-learning on CliffWalking-v0; verify wall time speedup vs standard Q-learning with 256 actions.
  2. Implement StochDQN on InvertedPendulum-v4 with 512 actions; compare average return and wall time to DQN.
  3. Test memory-based vs memoryless stochastic maximization on a small MDP; measure convergence speed and approximation error βt.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of stochastic methods scale with increasing action space size, particularly for very large discrete action spaces beyond the tested range?
- Basis in paper: [inferred] The paper tests methods with up to 4096 actions but does not explore performance in significantly larger action spaces.
- Why unresolved: The paper focuses on demonstrating the feasibility and effectiveness of stochastic methods within a certain range of action space sizes, but does not investigate the upper limits of scalability.
- What evidence would resolve it: Experimental results showing the performance of stochastic methods in environments with action spaces significantly larger than 4096 actions, comparing convergence rates, reward maximization, and computational efficiency.

### Open Question 2
- Question: What is the impact of different memory-based strategies on the performance of stochastic methods, and how do they compare to the specific memory strategy used in the paper?
- Basis in paper: [explicit] The paper uses a specific memory strategy (tracking the two most recent actions for discrete states and a subset of actions from the replay buffer for continuous states) but does not explore alternative memory strategies.
- Why unresolved: While the paper demonstrates the effectiveness of its chosen memory strategy, it does not investigate how other memory-based approaches might affect the performance of stochastic methods.
- What evidence would resolve it: Comparative analysis of different memory strategies (e.g., varying the number of actions stored, using different criteria for selecting actions to store) applied to stochastic methods, evaluating their impact on convergence speed, reward maximization, and computational efficiency.

### Open Question 3
- Question: How do stochastic methods perform in environments with non-stationary or dynamic action spaces, where the set of available actions changes over time?
- Basis in paper: [inferred] The paper focuses on stationary action spaces and does not address the challenges posed by non-stationary or dynamic action spaces.
- Why unresolved: The paper's theoretical analysis and empirical evaluation assume a fixed action space, leaving the behavior of stochastic methods in dynamic environments unexplored.
- What evidence would resolve it: Experimental results demonstrating the performance of stochastic methods in environments where the action space changes over time, comparing their adaptability and convergence to that of deterministic methods in similar settings.

## Limitations

- The effectiveness of stochastic methods depends heavily on the assumption that logarithmic sampling reliably approximates exact maximization in high-variance action-value landscapes
- The memory-based variant's effectiveness depends on accurate tracking of recently exploited actions, introducing potential implementation fragility
- Empirical validation relies on a limited set of environments and action discretizations, limiting generalizability

## Confidence

Medium: The algorithmic framework and theoretical analysis are sound, but empirical validation relies on a limited set of environments and action discretizations. The claimed 60x wall time speedup is impressive but context-dependent on action space size and specific implementation details not fully specified in the paper.

## Next Checks

1. Test stochastic methods on environments with known challenging action-value distributions (e.g., sparse rewards, multiple near-optimal actions) to evaluate approximation quality under stress conditions.
2. Implement ablation studies varying subset size (sub-logarithmic sampling) to quantify the tradeoff between computational savings and performance degradation.
3. Benchmark against alternative action-space reduction techniques (hierarchical action selection, action pruning) to contextualize the relative advantages of stochastic maximization.