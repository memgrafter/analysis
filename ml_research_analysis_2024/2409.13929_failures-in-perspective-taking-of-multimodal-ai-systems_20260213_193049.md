---
ver: rpa2
title: Failures in Perspective-taking of Multimodal AI Systems
arxiv_id: '2409.13929'
source_url: https://arxiv.org/abs/2409.13929
tags:
- spatial
- level
- perspective-taking
- tasks
- human
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study applied cognitive psychology techniques to assess the
  perspective-taking abilities of multimodal AI systems, specifically GPT-4o. The
  research addressed the limitations of current AI spatial cognition assessments by
  using established Level 1 and Level 2 perspective-taking tasks, which are well-studied
  in human development.
---

# Failures in Perspective-taking of Multimodal AI Systems

## Quick Facts
- arXiv ID: 2409.13929
- Source URL: https://arxiv.org/abs/2409.13929
- Reference count: 13
- GPT-4o achieved near-perfect performance on Level 1 perspective-taking tasks but struggled significantly with Level 2 tasks, especially at intermediate angles.

## Executive Summary
This study applied cognitive psychology techniques to assess perspective-taking abilities in multimodal AI systems, specifically testing GPT-4o using Level 1 and Level 2 tasks. The research revealed that while GPT-4o excels at basic visibility judgments (Level 1), it struggles significantly with tasks requiring mental rotation or imagining viewpoints different from its own (Level 2). The findings suggest that current multimodal AI systems rely on propositional representations and language-based reasoning rather than the analog spatial representations used by humans, limiting their ability to perform true perspective-taking.

## Method Summary
The researchers tested GPT-4o on three perspective-taking tasks using 48 images featuring avatars and geometric shapes or letters/numbers at various angles. They employed chain-of-thought prompting for Level 2 spatial tasks and ran 10 iterations per image. The study used established cognitive psychology paradigms to evaluate both Level 1 perspective-taking (knowing if an object is visible) and Level 2 tasks requiring spatial judgments and visual perspective simulation at angular differences ranging from 0° to 315°.

## Key Results
- GPT-4o achieved near-perfect performance on Level 1 perspective-taking tasks across all angles
- Performance dropped significantly on Level 2 tasks, particularly at 90° and 135° angles where mental rotation was required
- Chain-of-thought prompting improved performance on 180° tasks but failed to improve performance at intermediate angles between 90° and 180°

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Multimodal AI systems can achieve high performance on spatial tasks through language-based reasoning alone, without true spatial representation.
- Mechanism: The models use linguistic patterns and associations learned during training to deduce spatial relationships, bypassing the need for genuine mental rotation or perspective-taking processes.
- Core assumption: Spatial reasoning in AI can be approximated through language without requiring the underlying cognitive mechanisms used by humans.
- Evidence anchors:
  - [abstract] "although current models demonstrate a rich understanding of spatial information from images, this information is rooted in propositional representations, which differ from the analog representations employed in human and animal spatial cognition"
  - [section 1.2] "text-only GPT-4 achieves a score of 31.4, while multimodal GPT-4v achieves a score of 42.6 on the spatial understanding category of Meta's openEQA episodic memory task"
  - [section 1.2] "GPT-4v achieved an accuracy of 51.26%, only 13.17% higher than random guessing and 44.44% lower than human performance"

### Mechanism 2
- Claim: AI systems struggle with Level 2 perspective-taking tasks specifically when mental rotation is required, indicating a fundamental difference in spatial reasoning approach.
- Mechanism: The models process spatial information as static image features rather than engaging in dynamic mental rotation processes, leading to failures when viewpoints are misaligned.
- Core assumption: The inability to perform mental rotation is a computational limitation rather than a data limitation, as evidenced by human developmental patterns.
- Evidence anchors:
  - [section 3] "GPT-4o struggled with the task when mental rotation was involved, beginning around a 90° angular difference"
  - [section 4] "this does not necessarily indicate that GPT-4o struggles with or cannot perform mental rotation. Instead, it suggests that GPT-4o likely employs a fundamentally different strategy"
  - [section 4] "GPT-4o often responded with 'E' and '0' for images around a 90° angular difference, where from the image view, an M/W would look like an E, and a 9/6 would look like a 0"

### Mechanism 3
- Claim: Chain-of-thought prompting can improve performance on extreme cases (180°) but fails at intermediate angles, revealing the limits of language-based spatial reasoning.
- Mechanism: While language can help structure reasoning for straightforward transformations, it cannot provide the precise spatial computations needed for intermediate rotations.
- Core assumption: Language alone cannot substitute for the computational processes required for genuine spatial reasoning.
- Evidence anchors:
  - [section 3] "GPT-4o performance significantly improved with chain-of-thought prompting on 180° stimuli... However, this linguistic strategy did not improve the model's ability to handle intermediate rotations between 90° and 180°"
  - [abstract] "Chain-of-thought prompting improved performance on 180° tasks but failed at intermediate angles, indicating that language alone is insufficient for true spatial reasoning"

## Foundational Learning

- Concept: Level 1 vs Level 2 perspective-taking distinction
  - Why needed here: Understanding the difference between knowing if something is visible versus mentally simulating another viewpoint is crucial for interpreting the results
  - Quick check question: What is the key difference between Level 1 and Level 2 perspective-taking tasks in terms of cognitive demands?

- Concept: Mental rotation as a cognitive process
  - Why needed here: The study hinges on understanding how mental rotation affects performance and why its absence in AI models matters
  - Quick check question: How does mental rotation typically manifest in human response times during perspective-taking tasks?

- Concept: Propositional vs analog spatial representations
  - Why needed here: The core finding relates to how AI uses propositional (language-based) representations versus the analog representations humans use
  - Quick check question: What is the fundamental difference between propositional and analog representations of spatial information?

## Architecture Onboarding

- Component map: Multimodal input processing → Visual feature extraction → Language model reasoning → Output generation
- Critical path: Image → Feature extraction → Tokenization → Language model → Response
- Design tradeoffs: Using propositional representations allows efficient processing and language-based reasoning but sacrifices the ability to perform genuine mental rotation and perspective-taking that requires dynamic spatial transformations
- Failure signatures: Performance drops when tasks require mental rotation, especially at intermediate angles (90°-180°); chain-of-thought prompting helps with 180° but not intermediate angles; default responses when uncertainty arises from egocentric perspective
- First 3 experiments:
  1. Test model performance on Level 1 tasks with occluded views to verify near-perfect performance
  2. Evaluate performance on Level 2 spatial tasks at 45° intervals to map the exact angle where performance begins to degrade
  3. Compare chain-of-thought prompting effectiveness across different angle ranges to quantify the gap between 180° success and intermediate angle failures

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What specific neural computations are required for Level 2 perspective-taking that current multimodal models lack?
- Basis in paper: [explicit] The paper discusses how Level 2 perspective-taking develops late in human development (ages 6-10) despite extensive "data" exposure, suggesting the challenge is computational rather than data-driven
- Why unresolved: While the paper suggests these computations occur outside visual and language networks, it doesn't identify the specific neural mechanisms or computational strategies required
- What evidence would resolve it: Neuroimaging studies comparing human and AI processing during perspective-taking tasks, or successful implementation of proposed computational strategies in AI models

### Open Question 2
- Question: Can chain-of-thought prompting be enhanced to enable intermediate-angle mental rotation in multimodal models?
- Basis in paper: [explicit] The paper shows chain-of-thought prompting improved performance on 180° tasks but failed at intermediate angles (90°-180°), suggesting language lacks precision for spatial cognition
- Why unresolved: The paper demonstrates current limitations but doesn't explore alternative prompt engineering approaches or hybrid linguistic-spatial reasoning strategies
- What evidence would resolve it: Development of prompt engineering techniques that successfully enable intermediate-angle perspective-taking, or theoretical framework explaining why linguistic reasoning cannot bridge this gap

### Open Question 3
- Question: What is the computational relationship between mental rotation and perspective-taking in multimodal models versus humans?
- Basis in paper: [explicit] The paper notes GPT-4o's performance decreases on tasks humans solve via mental rotation, but argues this doesn't mean GPT-4o struggles with mental rotation - it likely uses fundamentally different strategies
- Why unresolved: The paper identifies different computational approaches but doesn't characterize how these approaches relate to each other or what makes human mental rotation strategies effective
- What evidence would resolve it: Comparative analysis of computational pathways in human versus AI spatial reasoning, or successful implementation of human-like mental rotation strategies in AI models

### Open Question 4
- Question: How do multimodal models process spatial information differently from propositional representations used in human cognition?
- Basis in paper: [explicit] The paper states that current models use propositional representations that differ from analog representations in human spatial cognition, but doesn't characterize these differences in detail
- Why unresolved: The paper identifies the distinction but doesn't explain the specific computational or representational differences between AI propositional processing and human analog processing
- What evidence would resolve it: Detailed comparative analysis of representation formats, or successful implementation of analog-like representations in AI models that improve spatial reasoning performance

## Limitations
- Findings are limited to GPT-4o and may not generalize to other multimodal AI systems
- The study uses controlled experimental conditions that may not reflect real-world performance
- Does not explore whether fine-tuning on spatial reasoning tasks could overcome the identified limitations

## Confidence
- **High Confidence**: GPT-4o's superior performance on Level 1 tasks versus Level 2 tasks
- **Medium Confidence**: The distinction between propositional and analog spatial representations as the root cause of perspective-taking failures
- **Medium Confidence**: Chain-of-thought prompting's effectiveness at 180° but failure at intermediate angles

## Next Checks
1. Test the same experimental paradigm on other multimodal models (e.g., Claude 3, Gemini, LLaVA) to determine if GPT-4o's performance profile is representative or model-specific.

2. Fine-tune GPT-4o or a similar model on spatial reasoning datasets and retest perspective-taking performance to distinguish between architectural limitations and data-driven deficiencies.

3. Systematically explore alternative prompting strategies beyond chain-of-thought, including few-shot examples, spatial-specific prompting, and hybrid text-visual instruction methods to identify if performance at intermediate angles can be improved.