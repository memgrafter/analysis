---
ver: rpa2
title: 'PMMT: Preference Alignment in Multilingual Machine Translation via LLM Distillation'
arxiv_id: '2410.11410'
source_url: https://arxiv.org/abs/2410.11410
tags:
- translation
- preferences
- dataset
- language
- human
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proposes PMMT, a method for aligning multilingual machine
  translation with human preferences such as tone and style. It addresses the challenge
  of generating large-scale parallel corpora with specific translation preferences
  using large language models (LLMs) and distilling these preferences into smaller
  translation models for efficient online serving.
---

# PMMT: Preference Alignment in Multilingual Machine Translation via LLM Distillation

## Quick Facts
- arXiv ID: 2410.11410
- Source URL: https://arxiv.org/abs/2410.11410
- Reference count: 20
- Key outcome: PMMT achieves competitive general translation accuracy while significantly outperforming baselines in tasks requiring alignment with specific human preferences, with BLEURT scores reaching 0.87 on WMT23 and 0.83 on a customized test set.

## Executive Summary
This paper presents PMMT, a method for aligning multilingual machine translation with human preferences such as tone and style. The approach addresses the challenge of generating large-scale parallel corpora with specific translation preferences using large language models (LLMs) and distilling these preferences into smaller translation models for efficient online serving. The core method involves training a PMMT-P LLM to generate candidate translations from source texts, training a PMMT-J reward model to filter translations based on human preferences, and using the filtered data to train smaller PMMT-B translation models. Experiments on public benchmarks and a customized test set show that PMMT achieves competitive general translation accuracy while significantly outperforming baselines in tasks requiring alignment with specific human preferences.

## Method Summary
PMMT is a two-stage approach that uses LLMs to generate candidate translations and reward models to filter based on human preferences. The method starts by building a diverse seed dataset from e-commerce conversations translated using multiple sources (APIs, AI models, human annotators). A PMMT-P LLM is trained on this seed dataset to generate candidate translations, while a PMMT-J reward model is trained on preference-aligned data to score translations. The filtered candidate translations are then used to train smaller PMMT-B models for efficient online serving. The method can automatically update with new preferences by introducing small amounts of new preference data, updating the seed preference dataset and reward model, and retraining the PMMT-B models.

## Key Results
- PMMT achieves competitive general translation accuracy on public benchmarks (WMT23, Flores200) despite not being trained on these datasets
- Significant improvement in preference alignment tasks with BLEURT scores reaching 0.87 on WMT23 and 0.83 on customized test set
- Outperforms baseline models in tasks requiring specific tone and style alignment while maintaining translation quality

## Why This Works (Mechanism)

### Mechanism 1
- Claim: PMMT achieves superior alignment to human preferences while maintaining competitive general translation accuracy by generating large-scale parallel corpora with specific translation preferences using LLMs and distilling these preferences into smaller translation models.
- Mechanism: The PMMT pipeline uses a two-stage approach. First, a PMMT-P LLM generates candidate translations from source texts. Second, a PMMT-J reward model filters these candidates based on human preferences. The filtered data is then used to train smaller PMMT-B translation models, effectively distilling knowledge from LLMs into more efficient models for online serving.
- Core assumption: LLMs can generate high-quality candidate translations that capture human preferences, and these preferences can be effectively learned and distilled into smaller models.
- Evidence anchors: [abstract] "The core method involves training a PMMT-P LLM to generate candidate translations from source texts, training a PMMT-J reward model to filter translations based on human preferences, and using the filtered data to train smaller PMMT-B translation models." [section] "Firstly, a small seed dataset is built through multiple training resources, on which a translation LLM is trained to generate more candidate translations from the new source texts. Meanwhile, a reward model (RM) is trained on another small dataset which is aligned to human preferences."

### Mechanism 2
- Claim: PMMT-B models can be automatically updated with new preferences by introducing a small amount of new parallel data that reflects these preferences, without the need for extensive retraining.
- Mechanism: When new preferences are introduced, only a small amount of new parallel data reflecting these preferences is required to update the seed preference dataset and the PMMT-J reward model. The PMMT-B models are then automatically updated using the new data generated by the updated PMMT-P and PMMT-J models.
- Core assumption: Small amounts of new preference data are sufficient to update the reward model and trigger effective retraining of the PMMT-B models.
- Evidence anchors: [abstract] "To relieve the cost of alignment to fickle preferences, we propose an approach to automatically distill knowledge from LLMs to smaller models. In this pipeline, only few examples that represent new preferences are required to update the seed preference data and all small translation models will be updated automatically." [section] "Additional preferences can also be added by introducing a small amount of new parallel data which can reflect the new preferences. The seed preference dataset and the PMMT-J model are updated only when the preference varies to maintain a more coherent filtering rule and save cost."

### Mechanism 3
- Claim: PMMT achieves competitive general translation accuracy on public benchmarks despite not being trained on these datasets, demonstrating the generalization ability of the proposed method.
- Mechanism: The seed dataset used to train the PMMT-P model is built from diverse translation resources, including translation APIs, sophisticated AI models, and human annotators. This diversity ensures that the PMMT-P model learns general translation norms, which are then distilled into the PMMT-B models.
- Core assumption: Training on a diverse seed dataset allows the PMMT-P model to learn general translation norms that generalize well to unseen data.
- Evidence anchors: [abstract] "Meanwhile, on popular public benchmarks like WMT and Flores, on which our models were not trained, the proposed method also shows a competitive performance compared to SOTA works." [section] "The seed dataset is used to train the PMMT-P and the source texts are from our online e-commerce conversations. Multiple translation resources are utilized to maintain the diversity of outputs from PMMT-P: translation APIs (Google translation and Damo translation), sophisticated AI models (Brown et al. 2020; OpenAI 2023), and human annotators."

## Foundational Learning

- Concept: Reinforcement Learning from Human Feedback (RLHF)
  - Why needed here: RLHF is a key technique used in the PMMT-J reward model to align translations with human preferences.
  - Quick check question: How does RLHF differ from traditional supervised learning in the context of machine translation?

- Concept: Direct Preference Optimization (DPO)
  - Why needed here: DPO is an alternative to RLHF that is used to align the PMMT-P model with human preferences during training.
  - Quick check question: What are the advantages of DPO over RLHF in terms of training efficiency and effectiveness?

- Concept: Multilingual Machine Translation (MMT)
  - Why needed here: PMMT is a method for aligning multilingual machine translation with human preferences, so understanding the basics of MMT is crucial.
  - Quick check question: What are the main challenges in developing high-quality multilingual translation models, and how does PMMT address these challenges?

## Architecture Onboarding

- Component map: Source texts → PMMT-P → PMMT-J → Filtered data → PMMT-B
- Critical path: Source texts → PMMT-P → PMMT-J → Filtered data → PMMT-B
- Design tradeoffs:
  - Model size vs. efficiency: PMMT-P is a large LLM for generating high-quality candidates, while PMMT-B is a smaller model for efficient online serving.
  - Diversity vs. specificity: The seed dataset is diverse to learn general translation norms, while the preference data is specific to capture human preferences.
- Failure signatures:
  - Poor translation quality: Check if PMMT-P is generating low-quality candidate translations or if PMMT-J is filtering too aggressively.
  - Misalignment with preferences: Check if the preference data is accurately reflecting human preferences or if PMMT-J is not effectively learning these preferences.
- First 3 experiments:
  1. Evaluate PMMT-P on a held-out set of source texts to ensure it generates high-quality candidate translations.
  2. Evaluate PMMT-J on a set of candidate translations to ensure it accurately filters based on human preferences.
  3. Evaluate PMMT-B on a set of source texts to ensure it maintains competitive general translation accuracy while aligning with human preferences.

## Open Questions the Paper Calls Out

- Question: How does the PMMT method perform when applied to non-e-commerce domains with different human preference patterns?
  - Basis in paper: [inferred] The paper mentions that the method is "easily expanded to accommodate other customized preferences" but only demonstrates e-commerce conversation preferences.
  - Why unresolved: The paper only validates the approach on e-commerce data and specific preferences like politeness, but doesn't test how well the framework generalizes to other domains (legal, medical, technical documentation) where human preferences might be very different.
  - What evidence would resolve it: Experiments applying PMMT to multiple diverse domains with their respective preference patterns, showing comparable performance across different types of human preferences.

- Question: What is the impact of using different LLM architectures (beyond LLaMA2-13B) as the PMMT-P model on translation quality and preference alignment?
  - Basis in paper: [explicit] The paper states "The PMMT-P model in this paper is fine-tuned from the pre-trained LLaMA2-13B" without exploring alternatives.
  - Why unresolved: The choice of LLaMA2-13B is presented without justification or comparison to other LLM architectures (GPT models, other LLaMA variants, or specialized translation LLMs) that might perform differently.
  - What evidence would resolve it: Comparative experiments using different LLM architectures for PMMT-P, measuring both general translation accuracy and preference alignment performance.

- Question: How does the computational cost of the PMMT pipeline scale with the number of languages and preference types?
  - Basis in paper: [inferred] The paper discusses automatic updates and large-scale parallel corpus generation but doesn't provide detailed analysis of computational complexity or cost scaling.
  - Why unresolved: While the method claims to be "efficient" and "economical," there's no analysis of how costs (in terms of compute, memory, or time) grow as more languages or preference types are added to the system.
  - What evidence would resolve it: Detailed analysis showing computational resource requirements as a function of language count and preference complexity, including break-even points compared to traditional approaches.

## Limitations
- The scalability of PMMT to handle rapidly evolving preferences in real-world applications remains uncertain, as the method relies on periodic updates rather than continuous adaptation.
- The generalizability of PMMT across diverse domains beyond e-commerce conversations is not fully validated, as the seed dataset and preference data are primarily derived from this specific context.
- The computational overhead of maintaining and updating the PMMT-P and PMMT-J models for multiple language pairs is not quantified, which could impact practical deployment.

## Confidence

- High Confidence: The competitive performance on public benchmarks (WMT23, Flores200) is well-supported by the experimental results, demonstrating the method's ability to maintain general translation accuracy.
- Medium Confidence: The claim that PMMT can automatically update with new preferences using small amounts of data is plausible but requires further validation across diverse preference types and language pairs.
- Medium Confidence: The assertion that PMMT significantly outperforms baselines in preference alignment tasks is supported by the results but may be influenced by the specific evaluation metrics and test sets used.

## Next Checks
1. Evaluate PMMT on diverse domains (e.g., legal, medical, technical) to assess its generalizability beyond e-commerce conversations.
2. Simulate rapid preference changes and measure the time and resources required for PMMT to adapt, validating its real-world applicability.
3. Quantify the computational costs of maintaining and updating the PMMT-P and PMMT-J models for multiple language pairs, ensuring practical scalability.