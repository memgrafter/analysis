---
ver: rpa2
title: Multimodal Unsupervised Domain Generalization by Retrieving Across the Modality
  Gap
arxiv_id: '2402.04416'
source_url: https://arxiv.org/abs/2402.04416
tags:
- conference
- label
- image
- text
- dataset
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses multimodal unsupervised domain generalization
  (MUDG), which aims to train a model to generalize to unseen target tasks using only
  unlabeled, task-agnostic source data and target label names. The core challenge
  is constructing a relevant pseudo-labeled training set from a large, diverse source
  dataset.
---

# Multimodal Unsupervised Domain Generalization by Retrieving Across the Modality Gap

## Quick Facts
- **arXiv ID**: 2402.04416
- **Source URL**: https://arxiv.org/abs/2402.04416
- **Reference count**: 40
- **Primary result**: Achieves 71.1% accuracy on ImageNet-1K and 61.2% on Office Home using ViT-B/16

## Executive Summary
This paper addresses multimodal unsupervised domain generalization (MUDG), where the goal is to train a model to generalize to unseen target tasks using only unlabeled, task-agnostic source data and target label names. The core challenge is constructing a relevant pseudo-labeled training set from a large, diverse source dataset. The authors propose a retrieval-based approach with three main innovations: paired k-means clustering for cross-modal search, adaptive text augmentation, and sample selection via clustering with diversity-preserving loss.

## Method Summary
The proposed method tackles MUDG by retrieving relevant training samples from a large multimodal source dataset. The approach consists of three key innovations: (1) Paired k-means clustering to build a cross-modal search index with better recall by updating centroids in query space instead of image space; (2) Adaptive text augmentation to diversify retrieved images while maintaining classifier prototype variance; (3) Sample selection via clustering and diversity-preserving loss for training. This retrieval-based framework enables the model to construct a pseudo-labeled training set tailored to the target domain using only label names as guidance.

## Key Results
- Achieves 71.1% accuracy on ImageNet-1K (vs. 65.2% ZS baseline) using ViT-B/16
- Achieves 61.2% accuracy on Office Home (vs. 57.6% ZS baseline) using ViT-B/16
- Achieves 75.6% accuracy on ImageNet-1K (vs. 72.1% ZS baseline) using ViT-L/14

## Why This Works (Mechanism)
The method works by leveraging cross-modal alignment between images and text to retrieve relevant samples for unseen target domains. The paired k-means clustering creates a search index that better captures the relationship between visual and textual representations. Adaptive text augmentation ensures diversity in retrieved samples while maintaining alignment with classifier prototypes. The sample selection mechanism ensures that the retrieved set is both relevant and diverse, preventing overfitting to specific source domain characteristics.

## Foundational Learning
- **Cross-modal retrieval**: Understanding how to bridge the modality gap between images and text representations. Why needed: Essential for mapping label names to relevant visual examples.
- **K-means clustering in embedding space**: Clustering algorithms adapted to work with high-dimensional embeddings. Why needed: Forms the basis for efficient search index construction.
- **Domain generalization principles**: Techniques for improving model robustness across unseen domains. Why needed: The core challenge being addressed requires generalization beyond training distributions.

## Architecture Onboarding
**Component Map**: Label names -> Text encoder -> Cross-modal search index -> Retrieved images -> Sample selection -> Training set -> Classifier
**Critical Path**: The bottleneck is the cross-modal search index construction, which requires balancing between search quality and computational efficiency.
**Design Tradeoffs**: Paired k-means vs. single-modality approaches trades computational overhead for improved recall; adaptive augmentation trades diversity for prototype variance preservation.
**Failure Signatures**: Poor performance on domains with significant visual-textual misalignment; degradation with extremely large source datasets due to computational constraints.
**Three First Experiments**: 1) Validate cross-modal search recall on a small dataset; 2) Test adaptive augmentation impact on prototype variance; 3) Measure sample selection effectiveness on a simple classification task.

## Open Questions the Paper Calls Out
None

## Limitations
- Computational scalability concerns for extremely large-scale datasets
- Adaptive text augmentation relies on heuristics that may not generalize
- Assumes task-relevant clusters exist within source data, which may not hold for specialized domains

## Confidence
- **High Confidence**: Core retrieval methodology and effectiveness on standard benchmarks are well-established
- **Medium Confidence**: Cross-modal search index improvements are supported by recall metrics, but component contributions are difficult to disentangle
- **Low Confidence**: Claims about generalizability to truly unseen domains remain speculative

## Next Checks
1. **Scalability Test**: Evaluate the paired k-means approach on a dataset at least 10x larger than current test sets to measure computational overhead and retrieval accuracy degradation.
2. **Domain Robustness**: Test the method on domains with extreme visual-textual misalignment (e.g., medical imaging with technical descriptions) to assess cross-modal search index robustness.
3. **Component Isolation**: Conduct a comprehensive ablation study isolating the contribution of each innovation (paired k-means, adaptive augmentation, sample selection) on a held-out dataset not used in main experiments.