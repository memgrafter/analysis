---
ver: rpa2
title: 'ProbGate at EHRSQL 2024: Enhancing SQL Query Generation Accuracy through Probabilistic
  Threshold Filtering and Error Handling'
arxiv_id: '2404.16659'
source_url: https://arxiv.org/abs/2404.16659
tags:
- unanswerable
- questions
- queries
- data
- answerable
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents ProbGate, a method to filter unanswerable questions
  in text-to-SQL tasks by using token log-probability distributions. The authors fine-tune
  gpt-3.5-turbo on answerable questions and use the lowest log-probability tokens
  as a confidence indicator, setting a threshold to identify unanswerable cases.
---

# ProbGate at EHRSQL 2024: Enhancing SQL Query Generation Accuracy through Probabilistic Threshold Filtering and Error Handling

## Quick Facts
- arXiv ID: 2404.16659
- Source URL: https://arxiv.org/abs/2404.16659
- Reference count: 10
- Primary result: 3rd place in EHRSQL 2024 competition with strong reliability scores (RS metrics)

## Executive Summary
ProbGate introduces a probabilistic threshold filtering method to improve text-to-SQL accuracy by identifying unanswerable questions. The approach fine-tunes gpt-3.5-turbo on answerable questions only, then uses log-probability distributions of generated SQL tokens to set confidence thresholds. Low-confidence tokens indicate potential unanswerability. The method also executes SQL queries to catch grammatical errors, achieving 3rd place in EHRSQL 2024 with strong reliability scores across multiple penalty metrics.

## Method Summary
The method involves fine-tuning gpt-3.5-turbo exclusively on answerable questions from the EHRSQL dataset, then generating SQL for all test questions. ProbGate computes log probabilities for each SQL token, sorts them, and filters out queries with lowest k tokens below a threshold. Reserved SQL words are excluded from this calculation. The filtered queries are then executed against the database to catch grammatical and schema errors. The approach combines confidence-based filtering with execution-based validation to improve reliability while minimizing false answers to unanswerable questions.

## Key Results
- Achieved 3rd place in EHRSQL 2024 competition
- Strong performance across RS(0), RS(5), RS(10), and RS(N) metrics
- Outperformed binary classifier approaches and entropy-based filtering methods
- Demonstrated effectiveness even without direct model access

## Why This Works (Mechanism)

### Mechanism 1
- Claim: ProbGate filters unanswerable questions by thresholding the lowest token log-probabilities in generated SQL.
- Mechanism: After fine-tuning on answerable questions only, the model's generated SQL tokens have higher log-probabilities for answerable cases. Tokens with low log-probabilities signal uncertainty and are likely generated from unanswerable questions. ProbGate reserves SQL syntax tokens, computes average log-probability of the lowest k tokens, and sets a threshold to classify questions as answerable or unanswerable.
- Core assumption: Fine-tuning on answerable data shifts the log-probability distribution for answerable vs. unanswerable queries, making the lowest token log-probabilities a reliable indicator of unanswerability.
- Evidence anchors: [abstract] "we further enhance result quality by filtering low-confidence SQL through log probability-based distribution"; [section] "we calculate the log probabilities of each SQL token in the test set items, sort them by ascending order of average value of its log probability, and consider all items with indices from the first up to the threshold as unanswerable"
- Break condition: If the answerable/unanswerable distribution in the test set differs significantly from training, the threshold k becomes unreliable and false positives/negatives increase.

### Mechanism 2
- Claim: Grammatical Errors Filtering (GEF) removes SQL queries that are syntactically correct but semantically unanswerable by executing them against the database.
- Mechanism: After ProbGate filtering, generated SQL queries are executed. If execution fails due to schema or data mismatches, the query is marked unanswerable. This catches cases where ProbGate could not confidently classify a question but execution reveals it is unanswerable.
- Core assumption: Database execution will fail for unanswerable queries even if they are grammatically valid SQL, thus allowing reliable filtering.
- Evidence anchors: [abstract] "grammatical and schema errors are mitigated by executing queries on the actual database"; [section] "we execute generated answerable SQL queries filtered by ProbGate through given database, if there is an error when executing SQL queries, we consider them unanswerable"
- Break condition: If the database schema or data is incomplete or corrupted, execution may fail for answerable queries, increasing false negatives.

### Mechanism 3
- Claim: Fine-tuning gpt-3.5-turbo only on answerable questions improves the log-probability separation between answerable and unanswerable queries.
- Mechanism: By excluding unanswerable data from fine-tuning, the model's token generation becomes more confident on answerable questions. This increases the log-probability gap between answerable and unanswerable queries, enabling more reliable thresholding.
- Core assumption: Training on answerable data only causes the model to generate higher log-probability tokens for answerable cases, while unanswerable cases produce lower log-probabilities even after fine-tuning.
- Evidence anchors: [abstract] "we fine-tune gpt-3.5-turbo on answerable questions"; [section] "we exclude unanswerable data from training, focusing solely on SQL transformation without considering whether the given questions are answerable or not"
- Break condition: If unanswerable questions share similar structure or semantics with answerable ones, the log-probability gap may shrink, reducing filtering effectiveness.

## Foundational Learning

- Concept: Log-probability distribution analysis for uncertainty estimation.
  - Why needed here: The core filtering relies on using log-probability as a confidence measure; understanding how to interpret and threshold these values is essential.
  - Quick check question: Given a set of SQL tokens with log-probabilities [-0.1, -0.2, -0.05, -0.15], what is the average of the lowest 2 tokens?
    - Answer: (-0.2 + -0.15) / 2 = -0.175.

- Concept: Text-to-SQL fine-tuning strategies and their impact on output quality.
  - Why needed here: The method fine-tunes a large language model on answerable questions only; understanding how fine-tuning affects generation and confidence is key to the approach.
  - Quick check question: If a model is fine-tuned only on answerable questions, what happens to its generated SQL for unanswerable questions?
    - Answer: The generated SQL may still be syntactically valid but with lower confidence (lower log-probabilities), indicating uncertainty.

- Concept: Database schema awareness and execution error handling.
  - Why needed here: The Grammatical Errors Filtering step executes SQL against the database; understanding schema constraints and common execution errors is necessary to interpret failures correctly.
  - Quick check question: If a SQL query references a non-existent table, what type of error would the database return?
    - Answer: A schema error indicating the table does not exist.

## Architecture Onboarding

- Component map: Fine-tuning -> SQL generation -> ProbGate filtering -> GEF execution -> Evaluation
- Critical path: Fine-tuning → SQL generation → ProbGate filtering → GEF execution → Evaluation
- Design tradeoffs:
  - Excluding unanswerable data from fine-tuning improves confidence on answerable cases but may reduce the model's ability to recognize unanswerable patterns
  - Thresholding based on lowest token log-probabilities is simple but sensitive to k value; adaptive thresholds could improve robustness
  - Executing SQL queries for filtering ensures accuracy but introduces latency and dependency on database availability
- Failure signatures:
  - High false negatives: Too many answerable questions marked unanswerable → threshold k too low or fine-tuning insufficient
  - High false positives: Unanswerable questions marked answerable → threshold k too high or database execution failures misclassified
  - Low confidence in filtering: Small gap between answerable/unanswerable log-probability distributions → consider additional filtering signals
- First 3 experiments:
  1. Vary threshold k in ProbGate and observe RS(0), RS(5), RS(10), RS(N) changes; identify optimal k for the dataset
  2. Compare fine-tuning on all data vs. only answerable data; measure impact on log-probability distributions and filtering accuracy
  3. Disable GEF and measure impact on false negatives; quantify how many unanswerable queries are caught only by execution

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How sensitive is the ProbGate threshold to variations in the proportion of unanswerable questions between training and test datasets?
- Basis in paper: [inferred] The authors note that the threshold value has a significant impact on performance and is sensitive to new datasets, particularly when the distribution of unanswerable questions differs between training and test sets.
- Why unresolved: The paper does not provide experiments varying the proportion of unanswerable questions between training and test sets to quantify this sensitivity.
- What evidence would resolve it: Experiments showing performance degradation when training and test sets have different ratios of answerable to unanswerable questions, along with guidelines for threshold adjustment in such cases.

### Open Question 2
- Question: How does ProbGate compare to out-of-distribution detection methods specifically designed for text-to-SQL tasks?
- Basis in paper: [inferred] The authors mention that further research on unanswered question filtering approaches from an out-of-distribution detection perspective is warranted, suggesting this comparison hasn't been made.
- Why unresolved: The paper doesn't benchmark ProbGate against dedicated OOD detection methods for text-to-SQL, instead comparing it to binary classifiers and entropy-based filtering.
- What evidence would resolve it: Direct comparison of ProbGate's performance against established OOD detection methods (like Mahalanobis distance or energy-based methods) on the same text-to-SQL datasets.

### Open Question 3
- Question: Does the exclusion of reserved SQL words from log probability calculation significantly impact the effectiveness of ProbGate compared to including all tokens?
- Basis in paper: [explicit] The authors state they exclude reserved words from log probability calculation, assuming models hallucinate more on entities and attributes than on reserved words.
- Why unresolved: The paper doesn't provide an ablation study showing the performance difference between including and excluding reserved words from the log probability calculation.
- What evidence would resolve it: Results showing the performance difference (in terms of RS metrics) when including all tokens versus excluding reserved words in the log probability calculation.

## Limitations

- The optimal threshold value appears empirically determined and may not generalize across different datasets or domains
- Fine-tuning only on answerable questions may limit the model's ability to recognize unanswerable patterns
- Database execution dependency introduces latency and may fail for answerable queries with incomplete schemas

## Confidence

**High Confidence:**
- The effectiveness of log-probability-based filtering for unanswerable question detection is supported by the 3rd place ranking in EHRSQL 2024 and strong RS metrics
- The general approach of combining confidence-based filtering with execution-based error detection is sound and produces measurable improvements

**Medium Confidence:**
- The specific threshold values used for log-probability filtering are likely dataset-specific and may require adjustment for different domains
- The assumption that database execution reliably identifies unanswerable queries may not hold in all cases, particularly with incomplete schemas

**Low Confidence:**
- The long-term generalizability of the approach to domains beyond medical EHRs without significant parameter tuning
- The impact of excluding unanswerable questions from fine-tuning on the model's ability to recognize unanswerable patterns in diverse query types

## Next Checks

1. **Threshold Sensitivity Analysis**: Systematically vary the log-probability threshold k across a wide range and measure its impact on RS metrics and false positive/negative rates. This will help determine if the threshold is optimally set or if an adaptive threshold approach would be more robust.

2. **Cross-Domain Generalization Test**: Apply the trained model to a different text-to-SQL dataset (such as Spider or WikiSQL) without fine-tuning and measure performance. This will reveal whether the log-probability distribution patterns learned from EHRSQL generalize to other domains.

3. **Ablation Study on Unanswerable Training Data**: Compare models trained with and without unanswerable questions in the training data. Measure not only filtering accuracy but also the model's ability to generate appropriate "unanswerable" responses when questions cannot be answered, assessing whether the exclusion strategy has unintended consequences.