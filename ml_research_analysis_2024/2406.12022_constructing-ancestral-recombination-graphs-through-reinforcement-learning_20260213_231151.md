---
ver: rpa2
title: Constructing Ancestral Recombination Graphs through Reinforcement Learning
arxiv_id: '2406.12022'
source_url: https://arxiv.org/abs/2406.12022
tags:
- sample
- args
- sequences
- state
- each
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a novel approach to constructing ancestral
  recombination graphs (ARGs) using reinforcement learning (RL). The method treats
  ARG construction as a shortest path problem, similar to finding the exit in a maze.
---

# Constructing Ancestral Recombination Graphs through Reinforcement Learning

## Quick Facts
- arXiv ID: 2406.12022
- Source URL: https://arxiv.org/abs/2406.12022
- Authors: Mélanie Raymond; Marie-Hélène Descary; Cédric Beaulac; Fabrice Larribe
- Reference count: 40
- One-line primary result: RL method builds ARGs as short as or shorter than ARG4WG heuristic algorithm

## Executive Summary
This paper introduces a novel approach to constructing ancestral recombination graphs (ARGs) using reinforcement learning (RL). The method treats ARG construction as a shortest path problem, similar to finding the exit in a maze. The agent learns to choose between coalescence, mutation, and recombination events to reach the most recent common ancestor (MRCA) as quickly as possible.

Key contributions include: 1) Using RL to build a distribution of short ARGs for a given sample, 2) Generalizing ARG construction to unseen samples by training on diverse populations, and 3) Developing ensemble methods to improve generalization and eliminate infinite-length genealogies.

## Method Summary
The method frames ARG construction as a Markov Decision Process where states are genetic sequence configurations and actions are genetic events (coalescence, mutation, recombination). The agent learns a value function to estimate the minimum steps to MRCA, using a neural network to approximate this function. A block-based feature vector representation allows the model to handle variable-sized samples. Ensemble methods combine multiple independently trained agents to improve robustness and eliminate infinite-length genealogies.

## Key Results
- RL approach builds ARGs as short as those from heuristic algorithms like ARG4WG, sometimes even shorter
- Method generalizes well to larger sample sizes and produces a wider variety of short ARGs compared to traditional approaches
- Ensemble methods significantly improve performance, with 12-15 agents typically sufficient to eliminate infinite-length genealogies and stabilize average ARG length

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The RL approach learns to construct ancestral recombination graphs (ARGs) by modeling the problem as a shortest path search, where each action (coalescence, mutation, recombination) is a step toward the most recent common ancestor (MRCA).
- Mechanism: The agent receives a reward of -1 at each time step, so it learns to maximize cumulative reward by minimizing the number of steps (events) needed to reach the MRCA. This is analogous to finding the shortest path in a maze.
- Core assumption: The most likely ARG is among the shortest ones, so minimizing steps corresponds to building likely ARGs.
- Evidence anchors:
  - [abstract] "In this paper, we propose a new approach to build short ARGs: Reinforcement Learning (RL). We exploit the similarities between finding the shortest path between a set of genetic sequences and their most recent common ancestor and finding the shortest path between the entrance and exit of a maze, a classic RL problem."
  - [section] "We assume that the most likely graph is among the shortest ones, so we are looking for the shortest path between a set of genetic sequences (maze entry) and their MRCA (maze exit)."
  - [corpus] Weak - corpus neighbors are about ancestral graphs but not RL or ARG construction, so no direct support.
- Break condition: If the shortest path assumption fails (e.g., biologically meaningful ARGs are not short), the learned policy may produce suboptimal ARGs.

### Mechanism 2
- Claim: The feature vector design using overlapping blocks of markers allows the agent to generalize learning across samples of different sizes.
- Mechanism: By representing states with block-based features (e.g., blocks of 3 markers with 1-step overlap), the feature dimension remains independent of the number of sequences, enabling the same model to handle variable-sized samples.
- Core assumption: Local block patterns capture sufficient information about sequence similarity to guide action selection, regardless of sample size.
- Evidence anchors:
  - [section] "To represent each state s, we use a feature vector x(s), which is used as input to our function ˆv(s, w). For building ARGs, we have to find a feature vector whose dimension is independent of the number of sequences in a state s..."
  - [section] "We define xi(s) = mp(bj), for all bj ∈ Bsp, j = 1, 2, ...,3B, and for p = 1, 2, ..., P... The dimension of x(s) is now d = 3BP."
  - [corpus] Missing - no corpus evidence on block-based feature generalization in RL for ARGs.
- Break condition: If block size or overlap is poorly chosen, the feature vector may lose critical sequence information, degrading performance.

### Mechanism 3
- Claim: Ensemble methods stabilize learning and eliminate infinite-length genealogies by aggregating multiple independent agents' policies.
- Mechanism: Multiple agents are trained independently with different initializations and samples. The final policy is derived by averaging value estimates (Mean), majority voting on actions (Majority), or selecting the shortest ARG from each agent (Minimum). This reduces variance and avoids pathological loops.
- Core assumption: Independent agents capture diverse but complementary strategies, and their aggregation yields more robust policies than any single agent.
- Evidence anchors:
  - [section] "To tackle the problem of building infinite-length ARGs and to stabilize learning, we use ensemble methods... We train M independent agents... To estimate the value function, we use the same architecture and the same RL algorithm for each agent, but we have changed the initialization of the parameter vector."
  - [section] "We use the test set to compare the performance of the three approaches... The results obtained are shown in Figure 6. The last method is definitely the best. It builds the shortest genealogy on 97% of the samples in the test set, and is the only one that eliminates the construction of infinite-length genealogies."
  - [corpus] Missing - corpus lacks direct evidence on ensemble methods for RL-based ARG construction.
- Break condition: If agents are too correlated or under-trained, ensemble methods may not improve performance and could even degrade it.

## Foundational Learning

- Concept: Markov Decision Process (MDP)
  - Why needed here: The ARG construction problem is framed as an MDP where states are sequence configurations, actions are genetic events, and rewards are cumulative step penalties.
  - Quick check question: What are the three types of actions the agent can take in the ARG construction MDP?
- Concept: Value Function Approximation
  - Why needed here: Tabular methods are infeasible due to exponential state space growth, so a neural network approximates the value function to generalize across unseen states.
  - Quick check question: Why can't we use tabular methods for large ARG samples?
- Concept: Feature Engineering for Variable-Length Inputs
  - Why needed here: The number of sequences varies during ARG construction, so features must be independent of sample size to allow generalization.
  - Quick check question: How does the block-based feature vector maintain a fixed dimension regardless of sample size?

## Architecture Onboarding

- Component map: RL agent -> NN value function approximator -> block-based feature extractor -> MDP environment (ARG state transitions) -> ensemble aggregator (Mean/Majority/Minimum)
- Critical path: Sample -> Feature extraction -> Value estimation -> Action selection -> State transition -> Reward -> Value update -> Ensemble aggregation
- Design tradeoffs: Block size vs. feature dimensionality (larger blocks capture more context but increase dimensionality); ensemble size vs. computational cost (more agents improve robustness but slow training)
- Failure signatures: Infinite-length genealogies (agent stuck in recombination-coalescence loops); poor generalization (high variance in ARG lengths across test samples); overfitting (excellent training performance but poor validation/test results)
- First 3 experiments:
  1. Train a single agent on a small sample (4 sequences, 4 SNPs) using tabular methods to verify MDP formulation and reward structure
  2. Switch to approximation methods with block-based features on a slightly larger sample (40 sequences, 10 SNPs) to test feature design and NN training stability
  3. Implement ensemble methods with 3-5 agents and compare Mean, Majority, and Minimum aggregation strategies on a validation set to identify the best approach for eliminating infinite genealogies

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the feature vector representation impact the performance of the RL agent in constructing ARGs, and are there more effective representations than the block-based approach?
- Basis in paper: [explicit] The paper mentions that the feature vector's dimension is independent of the number of sequences, using a block-based representation with blocks of 3 markers overlapping by 1 step shift.
- Why unresolved: The paper only discusses one specific feature vector representation and its dimension, but does not explore or compare other possible representations.
- What evidence would resolve it: Experiments comparing the performance of the RL agent using different feature vector representations, such as different block sizes, different overlapping strategies, or entirely different representations like the haplotype or genotype matrix approach mentioned in the discussion.

### Open Question 2
- Question: How does the ensemble method perform when the agents are trained with different RL algorithms or different architectures for the value function approximation?
- Basis in paper: [inferred] The paper mentions that the agents use the same NN architecture and RL algorithm, but suggests that using different RL algorithms could be an area for future exploration.
- Why unresolved: The paper only explores ensemble methods with agents trained using the same RL algorithm and architecture.
- What evidence would resolve it: Experiments comparing the performance of the ensemble method when the agents are trained with different RL algorithms or different architectures for the value function approximation.

### Open Question 3
- Question: What is the impact of the order in which agents are added to the ensemble on the final performance, and is there an optimal order?
- Basis in paper: [explicit] The paper mentions that agents were added to the ensemble as they were trained, but also tried different orders to add them to the ensemble.
- Why unresolved: The paper does not provide a detailed analysis of the impact of the order in which agents are added to the ensemble on the final performance.
- What evidence would resolve it: Experiments analyzing the impact of different orders of adding agents to the ensemble on the final performance, and potentially developing an algorithm to determine the optimal order.

### Open Question 4
- Question: How does the performance of the RL approach scale with the number of SNPs in the sequences, and what are the limitations of the current approach?
- Basis in paper: [explicit] The paper mentions that increasing the number of SNPs is a challenge and suggests transfer learning as a possible approach.
- Why unresolved: The paper does not provide experiments or analysis of the performance of the RL approach with a large number of SNPs.
- What evidence would resolve it: Experiments testing the performance of the RL approach with an increasing number of SNPs, and analysis of the limitations of the current approach and potential solutions.

## Limitations
- The assumption that shortest ARGs are most likely may not hold for all biological scenarios
- Lack of corpus evidence for block-based feature generalization and ensemble methods introduces uncertainty about robustness
- NN architecture and hyperparameters are not fully specified, which could affect reproducibility

## Confidence
- High confidence: The RL formulation as an MDP and the basic mechanism of learning to minimize steps to MRCA are well-supported by the text
- Medium confidence: The feature vector design using blocks of markers is plausible but lacks direct validation in the paper or corpus
- Low confidence: The effectiveness of ensemble methods (Mean/Majority/Minimum) is demonstrated on test data, but the underlying reasons for their success are not fully explained or validated

## Next Checks
1. Validate the block-based feature vector design by testing different block sizes and overlaps on a validation set to determine optimal parameters for generalization
2. Compare the RL method's ARG lengths to those from ARG4WG on a larger, more diverse set of test samples to confirm consistent performance improvements
3. Investigate the impact of ensemble size on eliminating infinite-length genealogies and stabilizing ARG length by training agents with varying numbers of independent agents (e.g., 5, 10, 15, 20) and analyzing the results