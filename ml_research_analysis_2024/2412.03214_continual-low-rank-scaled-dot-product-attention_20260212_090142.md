---
ver: rpa2
title: Continual Low-Rank Scaled Dot-product Attention
arxiv_id: '2412.03214'
source_url: https://arxiv.org/abs/2412.03214
tags:
- continual
- landmarks
- storing
- nystr
- matrices
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper introduces a new formulation of the Scaled Dot-product\
  \ Attention (SDA) based on the Nystr\xF6m approximation that is suitable for Continual\
  \ Inference. The proposed method adapts the Nystr\xF6m-based approximation of the\
  \ matrix multiplication followed by the softmax nonlinearity in SDA for Continual\
  \ Inference settings."
---

# Continual Low-Rank Scaled Dot-product Attention

## Quick Facts
- arXiv ID: 2412.03214
- Source URL: https://arxiv.org/abs/2412.03214
- Reference count: 40
- One-line primary result: Proposed method achieves up to 2 orders of magnitude reduction in operations while maintaining performance

## Executive Summary
This paper introduces Continual Nyströmformer, a new formulation of Scaled Dot-product Attention (SDA) based on the Nyström approximation that is suitable for Continual Inference. The method adapts the Nyström-based approximation for matrix multiplication followed by softmax nonlinearity in SDA, enabling linear computational and memory costs with respect to input tokens. Experiments on Audio Classification, Online Action Detection, and Electricity usage forecasting demonstrate up to three orders of magnitude reduction in operations while retaining competitive performance.

## Method Summary
The paper proposes adapting the Nyström approximation for Scaled Dot-product Attention in continual inference settings by deriving model updates in a continual manner. The approach uses two landmark selection methods: continual landmarks updated every n/m tokens using segment-means, and fixed landmarks determined during training via k-means clustering. The model achieves linear computational complexity O(ndm) instead of quadratic O(n²d) by maintaining cached intermediate attention computations and updating them incrementally as new tokens arrive.

## Key Results
- Up to two orders of magnitude reduction in operations compared to original Transformers
- Linear computational and memory cost with respect to number of input tokens
- Faster inference time compared to competing models with comparable or lower memory requirements
- Performance retention across Audio Classification, Online Action Detection, and Electricity usage forecasting tasks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The continual Nyström approximation reuses cached intermediate attention computations to avoid redundant work in streaming settings.
- Mechanism: By storing and updating the landmark matrices and associated attention terms (B, Γ, ∆) incrementally as new tokens arrive, the method avoids recomputing the full attention matrix from scratch at each step.
- Core assumption: The landmark matrices change slowly enough that recomputing them only every n/m steps is sufficient to maintain accuracy.
- Evidence anchors:
  - [abstract] "The proposed method adapts the Nyström-based approximation of the matrix multiplication followed by the softmax nonlinearity in SDA for Continual Inference settings."
  - [section] "The Continual Nyströmformers share some of the practical aspects of Continual Transformers [31], due to the properties of the involved continual computations..."
  - [corpus] Weak connection to similar token-routing or caching strategies in other low-rank attention works.
- Break condition: If landmark matrices drift too far from their true counterparts, accuracy degrades sharply; the update frequency n/m must be tuned to data rate.

### Mechanism 2
- Claim: Segment-means for landmark selection approximates the full attention with a small set of representative tokens.
- Mechanism: Each landmark is the mean of n/m consecutive tokens, providing a low-rank approximation that captures coarse temporal patterns while reducing O(n²) to O(ndm).
- Core assumption: Tokens within a segment are similar enough that their mean adequately represents them for attention purposes.
- Evidence anchors:
  - [section] "we propose updating the landmark matrices after n/m updates, following the segment-means process used in [19]."
  - [abstract] "This is achieved by deriving the model updates of the Nyström-based SDA in a continual manner and proposing two ways for determining the landmarks used in the SDA approximation."
  - [corpus] Alignment with Nyström method literature that uses clustering or segment-means for landmark selection.
- Break condition: In highly non-stationary data, segments may contain dissimilar tokens, leading to poor landmark quality and reduced accuracy.

### Mechanism 3
- Claim: Fixed landmarks learned from the training data reduce inference cost by eliminating landmark recomputation.
- Mechanism: During training, landmarks are chosen via k-means clustering of Q and K tokens; these fixed landmarks are reused at inference time, so only attention weights need updating per token.
- Core assumption: The learned landmark positions are stable across inference samples and capture sufficient information for approximation.
- Evidence anchors:
  - [section] "The idea of reducing the number of updates can be extended to avoid the landmark computations during inference time."
  - [abstract] "Experiments on Online Audio Classification and Online Action Detection tasks, the proposed Continual Scaled Dot-product Attention can lower the number of operations by up to three orders of magnitude compared to the original Transformers while retaining the predictive performance of competing models."
  - [corpus] Related fixed landmark approaches in kernel methods; however, explicit ablation not shown here.
- Break condition: If test data distribution differs significantly from training, fixed landmarks may no longer represent the input space well, hurting accuracy.

## Foundational Learning

- Concept: Matrix low-rank approximation via Nyström method
  - Why needed here: Enables replacing O(n²d) attention with O(ndm) operations when m ≪ n.
  - Quick check question: What is the trade-off between m (landmark count) and approximation error in the Nyström method?

- Concept: Continual/retroactive computation
  - Why needed here: Allows updating attention only for new tokens rather than recomputing for all tokens in the window.
  - Quick check question: How does the Continual Retroactive Attention formulation in Eq. (2) differ from standard attention in terms of data dependencies?

- Concept: Moore-Penrose pseudo-inverse computation
  - Why needed here: Required for reconstructing the full attention from low-rank components; must be updated when landmarks change.
  - Quick check question: Why can't the pseudo-inverse be updated incrementally in the same way as other matrices?

## Architecture Onboarding

- Component map: Input token stream → Token encoder → Q/K/V projection → Continual Nyström Attention (B, Γ, ∆) → Output features → Landmark updater (fixed or continual) → Memory cache for previous B, Γ, ∆ → Attention computation module

- Critical path: 1. Receive new token 2. Update Q/K/V projections 3. Update landmark matrices if needed 4. Update B, Γ, ∆ matrices incrementally 5. Compute attention output via pseudo-inverse multiplication

- Design tradeoffs:
  - Landmark update frequency vs. accuracy
  - Fixed vs. continual landmarks vs. recomputation cost
  - m (landmark count) vs. memory/computation
  - Single-output vs. retroactive inference mode

- Failure signatures:
  - Accuracy drops with infrequent landmark updates
  - Memory spike if caching is disabled
  - Numerical instability in pseudo-inverse for ill-conditioned Γ

- First 3 experiments:
  1. Benchmark FLOPs and memory for m=2,4,8 on synthetic sequence lengths (n=100,200,400).
  2. Ablate landmark update frequency: update every n/m vs. every step, measure accuracy loss.
  3. Compare fixed vs. continual landmarks on a held-out test set to quantify generalization cost.

## Open Questions the Paper Calls Out
None

## Limitations
- Landmark update frequency must be carefully tuned; infrequent updates lead to accuracy degradation
- Fixed landmarks may not generalize well to data distributions different from training
- Numerical stability issues in pseudo-inverse computation are not thoroughly addressed
- Method effectiveness is highly task-dependent with variable performance impact

## Confidence

- High confidence: The linear computational complexity claim (O(ndm) vs O(n²d)) is mathematically sound and well-established in Nyström method literature
- Medium confidence: The continual update mechanism is correctly derived and the FLOPs reduction measurements appear reliable, though the practical significance depends on specific hardware and implementation details
- Low confidence: The claim that fixed landmarks maintain performance across different data distributions is not sufficiently validated, as the paper doesn't show cross-dataset or domain adaptation experiments

## Next Checks

1. **Landmark stability analysis**: Conduct experiments measuring accuracy degradation when landmark update frequency is systematically reduced (update every 2n/m, n/m, n/2m steps) across all three tasks to establish the practical limits of infrequent updates.

2. **Distribution shift robustness**: Train fixed landmarks on one subset of each dataset and test on held-out data with different characteristics (e.g., different music genres in GTZAN, different action classes in THUMOS14) to quantify performance drop and validate the assumption of stable landmark representation.

3. **Numerical stability benchmarking**: Systematically test pseudo-inverse computation across varying sequence lengths and landmark counts, measuring condition numbers and tracking any NaNs or infinities during training/inference to identify practical limits of the approximation.