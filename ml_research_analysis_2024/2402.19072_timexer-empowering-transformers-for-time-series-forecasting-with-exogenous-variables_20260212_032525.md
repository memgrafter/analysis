---
ver: rpa2
title: 'TimeXer: Empowering Transformers for Time Series Forecasting with Exogenous
  Variables'
arxiv_id: '2402.19072'
source_url: https://arxiv.org/abs/2402.19072
tags:
- exogenous
- variables
- forecasting
- series
- endogenous
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of time series forecasting with
  exogenous variables, where external factors can significantly impact the prediction
  of target series. The proposed method, TimeXer, enhances the canonical Transformer
  architecture by incorporating exogenous information through a novel embedding strategy.
---

# TimeXer: Empowering Transformers for Time Series Forecasting with Exogenous Variables

## Quick Facts
- arXiv ID: 2402.19072
- Source URL: https://arxiv.org/abs/2402.19072
- Reference count: 40
- Authors: Yuxuan Wang, Haixu Wu, Jiaxiang Dong, Guo Qin, Haoran Zhang, Yong Liu, Yunzhong Qiu, Jianmin Wang, Mingsheng Long
- Primary result: State-of-the-art performance on twelve real-world datasets for time series forecasting with exogenous variables

## Executive Summary
TimeXer addresses the challenge of time series forecasting with exogenous variables by enhancing the canonical Transformer architecture. The method uses patch-wise representations for endogenous variables and variate-wise representations for exogenous variables, connected through a learnable global token. This design allows the model to capture both intra-endogenous temporal dependencies and inter-series correlations while handling irregular exogenous data. Experiments demonstrate superior performance across twelve diverse datasets in both univariate and multivariate settings.

## Method Summary
TimeXer extends the Transformer architecture to handle time series forecasting with exogenous variables by introducing a dual embedding strategy. Endogenous series are split into temporal patches and embedded patch-wise, while exogenous series are embedded variate-wise into single tokens. A learnable global token for each endogenous series serves as a bridge, receiving exogenous information through cross-attention and propagating it to patch-level tokens through another cross-attention layer. The model then applies self-attention over concatenated patch and global tokens to capture temporal dependencies and cross-variable correlations in a unified attention computation.

## Key Results
- Achieves state-of-the-art performance on twelve real-world datasets
- Demonstrates effectiveness in both univariate and multivariate settings
- Handles irregular exogenous variables with different frequencies and missing values
- Shows favorable efficiency when dealing with numerous exogenous variables compared to iTransformer

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The global endogenous token bridges causal information from exogenous variables to endogenous temporal patches.
- Mechanism: A learnable global token is created for each endogenous series, serving as a macroscopic representation. It interacts with exogenous variate-level tokens via cross-attention, then propagates that information back to patch-level endogenous tokens through another cross-attention layer.
- Core assumption: Patch-level representations capture intra-series temporal dependencies, while variate-level exogenous tokens can encode cross-series correlations without excessive complexity.
- Evidence anchors: [abstract], [section 3], [corpus] (weak evidence)
- Break condition: If exogenous variables are missing or uninformative, the global token cannot provide useful cross-series context.

### Mechanism 2
- Claim: Variate-level exogenous embeddings handle irregular exogenous series more naturally than patch-level embeddings.
- Mechanism: Each exogenous series is embedded into a single D-dimensional variate token, regardless of look-back length, frequency mismatch, or missing values.
- Core assumption: The information content of an exogenous series can be adequately summarized into a single variate token for the purposes of influencing the endogenous forecast.
- Evidence anchors: [section 3], [section 4.3.1], [corpus] (weak evidence)
- Break condition: If an exogenous series has highly localized, time-dependent patterns that cannot be captured in a single variate token.

### Mechanism 3
- Claim: Patch-wise self-attention over concatenated temporal and global tokens captures both intra-series temporal dependencies and cross-variable correlations in a single attention computation.
- Mechanism: The N patch-level tokens and the global token are concatenated and fed into a single self-attention layer.
- Core assumption: Concatenating patch and global tokens and applying self-attention jointly is sufficient to model both types of dependencies without interference.
- Evidence anchors: [section 3], [section 4.2], [corpus] (weak evidence)
- Break condition: If the attention computation becomes too large (many patches), the efficiency gains may be lost.

## Foundational Learning

- **Attention mechanism and its variants (self-attention, cross-attention)**: TimeXer relies entirely on attention layers to model temporal dependencies and cross-variable correlations without modifying the underlying Transformer architecture. *Quick check: What is the difference between self-attention and cross-attention in the context of TimeXer's architecture?*

- **Embedding strategies for time series (patch-wise vs. variate-wise)**: TimeXer uses patch-wise embeddings for endogenous variables to capture local temporal patterns and variate-wise embeddings for exogenous variables to handle irregularity and reduce complexity. *Quick check: Why does TimeXer use different embedding granularities for endogenous and exogenous variables?*

- **Handling irregular time series data (missing values, frequency mismatch, temporal misalignment)**: Real-world exogenous variables often have different sampling rates and missing data, which TimeXer must handle without explicit imputation. *Quick check: How does TimeXer's variate-level embedding approach handle exogenous series with different look-back lengths or missing values?*

## Architecture Onboarding

- **Component map**: Endogenous series → Patchify → PatchEmbed + Global token → Self-attention (patches+global) → Cross-attention (global←exogenous) → Feed-forward → repeat → Linear projection

- **Critical path**: Endogenous series → Patchify → PatchEmbed + Global token → Self-attention (patches+global) → Cross-attention (global←exogenous) → Feed-forward → repeat → Linear projection

- **Design tradeoffs**: 
  - Patch length: Shorter patches capture finer temporal details but increase computational cost
  - Single global token vs. multiple tokens: A single global token simplifies cross-attention but may limit expressiveness for very long series
  - Variate-level vs. patch-level exogenous: Variate-level is more efficient but may lose localized exogenous patterns

- **Failure signatures**: 
  - Poor performance on datasets with highly localized exogenous patterns
  - Degraded performance when exogenous variables are missing or uninformative
  - Increased memory usage if patch length is set too short

- **First 3 experiments**: 
  1. Run TimeXer on a simple univariate dataset (no exogenous variables) to verify it reduces to a standard Transformer with patch embeddings
  2. Run TimeXer on a dataset with aligned endogenous and exogenous series of equal length to verify the cross-attention mechanism works
  3. Run TimeXer on a dataset where exogenous series have missing values or different frequencies to verify the variate-level embedding handles irregularity

## Open Questions the Paper Calls Out

- **Open Question 1**: How does TimeXer handle the case when exogenous variables have different frequencies compared to the endogenous variables, beyond simple interpolation?
  - Basis: [explicit] The paper demonstrates handling of frequency mismatch on a weather dataset
  - Why unresolved: The paper shows frequency mismatch handling but lacks details on specific mechanisms beyond nearest-neighbor interpolation
  - What evidence would resolve it: Detailed explanation of the frequency mismatch handling mechanism with algorithmic details

- **Open Question 2**: What is the impact of the patch length hyperparameter on TimeXer's performance across different types of time series data?
  - Basis: [explicit] The paper mentions patch lengths of 16 for long-term and 24 for short-term forecasts with some sensitivity analysis
  - Why unresolved: While the paper shows some sensitivity analysis, it doesn't explore the full range of patch lengths or provide guidance on optimal selection
  - What evidence would resolve it: Comprehensive study varying patch lengths across multiple diverse datasets with analysis on different types of time series

- **Open Question 3**: How does TimeXer's performance compare to state-of-the-art models when dealing with high-dimensional exogenous variables (e.g., hundreds or thousands of variables)?
  - Basis: [explicit] The paper mentions favorable efficiency with numerous exogenous variables and demonstrates scalability on a weather dataset with 36 exogenous variables per grid cell
  - Why unresolved: While the paper shows good performance on 36 variables, it doesn't explore scenarios with hundreds or thousands of exogenous variables
  - What evidence would resolve it: Experiments comparing TimeXer against state-of-the-art models on datasets with high-dimensional exogenous variables (100+ variables)

## Limitations
- Limited empirical justification for the dual embedding strategy (patch-wise for endogenous, variate-wise for exogenous) compared to alternative approaches
- Claims of state-of-the-art performance based on literature comparisons rather than controlled ablation studies
- No direct validation of the global token's effectiveness as a bridge between exogenous and endogenous information

## Confidence
- **High Confidence**: The core architectural description of TimeXer is clearly specified with explicit details about patch-wise embeddings, global tokens, and the two-stage attention mechanism
- **Medium Confidence**: The claims about handling irregular exogenous variables through variate-level embeddings are reasonable but not empirically validated against alternatives
- **Low Confidence**: The claims of state-of-the-art performance across all twelve datasets cannot be fully verified without access to exact preprocessing, hyperparameter tuning, and evaluation procedures

## Next Checks
1. **Ablation on Exogenous Embedding Strategy**: Implement a variant of TimeXer using patch-wise embeddings for exogenous variables instead of variate-wise, then evaluate both versions on datasets with irregular exogenous series. Measure the performance difference and computational overhead to validate the claimed benefits of the variate-wise approach.

2. **Global Token Contribution Analysis**: Create a modified version of TimeXer where the global token is replaced with direct cross-attention between all endogenous and exogenous tokens. Compare performance on datasets with and without exogenous variables to quantify the specific contribution of the global token mechanism.

3. **Controlled Component Isolation**: Using a single representative dataset (e.g., Traffic for multivariate or ETTh2 for univariate), systematically disable or modify each of TimeXer's three key innovations (patch-wise embeddings, global tokens, variate-wise exogenous) in isolation. Measure the performance impact of each component to determine which architectural choices drive the reported improvements.