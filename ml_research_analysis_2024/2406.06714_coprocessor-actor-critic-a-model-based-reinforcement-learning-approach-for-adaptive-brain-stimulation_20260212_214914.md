---
ver: rpa2
title: 'Coprocessor Actor Critic: A Model-Based Reinforcement Learning Approach For
  Adaptive Brain Stimulation'
arxiv_id: '2406.06714'
source_url: https://arxiv.org/abs/2406.06714
tags:
- brain
- stimulation
- copac
- learning
- coprocessor
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'The paper introduces Coprocessor Actor Critic (CopAC), a novel
  model-based reinforcement learning (MBRL) approach for adaptive brain stimulation
  in stroke patients. The key insight is to decompose the policy learning into two
  phases: learning optimal world actions using a biomechanical simulator and then
  learning to induce these actions through brain stimulation.'
---

# Coprocessor Actor Critic: A Model-Based Reinforcement Learning Approach For Adaptive Brain Stimulation

## Quick Facts
- arXiv ID: 2406.06714
- Source URL: https://arxiv.org/abs/2406.06714
- Reference count: 40
- One-line primary result: CopAC achieves significantly better sample efficiency and task performance than SAC and MBPO baselines in both standard control tasks and physiologically realistic stroke domains

## Executive Summary
Coprocessor Actor Critic (CopAC) introduces a novel model-based reinforcement learning approach for adaptive brain stimulation in stroke patients. The key insight is decomposing policy learning into two phases: learning optimal world actions using a biomechanical simulator and then learning to induce these actions through brain stimulation. By leveraging a simulator-derived Q-function to guide stimulation sampling during online learning, CopAC focuses on high-value regions of the world-action space. Experimental results demonstrate that CopAC achieves significantly better sample efficiency and task performance compared to state-of-the-art model-free RL (SAC) and MBRL (MBPO) baselines across standard control tasks and a physiologically realistic stroke domain.

## Method Summary
CopAC is a two-phase approach for learning neural coprocessor policies. In the offline phase, a biomechanical simulator is used to learn the optimal Q-function for world actions via SAC. In the online phase, CopAC learns a brain model that maps stimulations to world actions while iteratively updating the Q-function based on the learned brain model. The key innovation is using the Q-function to strategically sample stimulations that produce high-value world actions, focusing model learning on beneficial regions of the action space. This decomposition minimizes patient interaction while maintaining high training reward throughout learning, which is crucial for patient comfort and safety during adaptive brain stimulation.

## Key Results
- CopAC achieves significantly better sample efficiency than SAC and MBPO baselines across all tested tasks
- CopAC maintains high training reward throughout learning, unlike SAC which starts with low reward
- Ablation studies show that both the Q-function update and strategic sampling strategy are crucial for CopAC's performance

## Why This Works (Mechanism)

### Mechanism 1
- Claim: CopAC achieves superior sample efficiency by decomposing policy learning into offline world-action value learning and online brain model learning.
- Mechanism: The biomechanical simulator is used to learn Qψ(s,a) offline, representing the optimal value of world actions. This allows the online phase to focus only on learning the brain model bF_ϕ_brain that maps stimulations to world actions. The Q-function guides stimulation sampling to focus on high-value regions of the action space.
- Core assumption: The optimal world policy is consistent across patients and can be simulated.

### Mechanism 2
- Claim: CopAC maintains high training reward by strategically sampling stimulations that maximize the Q-function given the current brain model.
- Mechanism: The coprocessor policy selects stimulations that produce world actions of maximum value according to Qψ. This strategic sampling ensures the patient experiences helpful stimulation during training, not random or off-task behavior.
- Core assumption: The current estimate of bF_ϕ_brain combined with Qψ can identify high-value stimulation actions.

### Mechanism 3
- Claim: CopAC avoids over-optimistic Q-values by iteratively updating the world-action value model based on the learned brain model.
- Mechanism: The Q-function is updated online to account for the fact that not all world actions are realizable through stimulation. This recalibration prevents the policy from selecting stimulations that would theoretically produce high-value actions but cannot be realized by the injured brain.
- Core assumption: The brain model bF_ϕ_brain provides a reasonable approximation of which world actions can be achieved through stimulation.

## Foundational Learning

- Concept: Biomechanical simulation for motor control
  - Why needed here: Provides a way to learn the optimal world action policy (Qψ) without patient interaction, enabling sample-efficient learning
  - Quick check question: What key assumption allows us to use the same biomechanical simulator for different patients?

- Concept: Model-based reinforcement learning with online model adaptation
  - Why needed here: Enables learning the complex mapping from stimulation to world actions while continuously refining the value function to account for brain injury limitations
  - Quick check question: How does CopAC differ from standard MBRL in how it handles the learned dynamics model?

- Concept: Strategic sampling guided by learned value functions
  - Why needed here: Ensures the patient experiences beneficial stimulation during training, maintaining comfort and safety while improving learning efficiency
  - Quick check question: What would happen to training reward if stimulations were sampled randomly instead of guided by Qψ?

## Architecture Onboarding

- Component map: Biomechanical simulator -> Qψ(s,a) learning -> Online brain model bF_ϕ_brain -> Stimulation policy -> Patient state -> World action -> Reward -> Update bF_ϕ_brain and Qψ

- Critical path:
  1. Initialize bF_ϕ_brain randomly
  2. While in simulator: learn Qψ using SAC on optimal world actions
  3. While interacting with patient: select stimulation using Qψ and bF_ϕ_brain
  4. Update bF_ϕ_brain with new (s, a, r, s') experiences
  5. Update Qψ using the brain model to avoid over-optimism
  6. Repeat steps 3-5 until convergence

- Design tradeoffs:
  - Simulation fidelity vs. model complexity: Higher fidelity simulators enable better Qψ but increase computational cost
  - Brain model capacity vs. sample efficiency: More complex brain models can capture finer stimulation effects but require more patient data
  - Update frequency of Qψ vs. stability: Frequent updates keep Qψ calibrated but may introduce noise; infrequent updates are stable but may lag behind brain model improvements

- Failure signatures:
  - Low training reward despite high evaluation reward: Brain model may be overfitting to suboptimal stimulation patterns
  - Oscillating performance: Qψ updates may be too frequent or brain model updates may be unstable
  - Poor sample efficiency: Qψ may not be transferring well from simulator to patient, or brain model may be too simple to capture stimulation effects

- First 3 experiments:
  1. Test CopAC on a simple continuous control task (e.g., Pendulum) with a known brain model to verify the decomposition approach works
  2. Compare training reward curves between CopAC and SAC on the same task to validate the strategic sampling benefit
  3. Run ablation studies (without Q update, without Q maximization) to identify which components are most critical for performance

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of CopAC compare to other model-based reinforcement learning approaches beyond MBPO, such as model-based meta-learning methods or ensemble-based approaches?
- Basis in paper: The paper only compares CopAC to SAC and MBPO as baselines, leaving room to explore how it fares against other MBRL methods.
- Why unresolved: The authors do not provide any experimental results or theoretical analysis comparing CopAC to a broader range of MBRL techniques.
- What evidence would resolve it: Empirical evaluations comparing CopAC to various MBRL methods on the same tasks and metrics would provide insights into its relative strengths and weaknesses.

### Open Question 2
- Question: What is the impact of the size and quality of the offline dataset used for learning Qψ on the overall performance of CopAC?
- Basis in paper: The authors mention that Qψ can be learned via offline RL on a dataset of human biomechanical rollouts, but do not explore the effects of dataset characteristics.
- Why unresolved: The paper does not provide any experiments or analysis on how different dataset sizes or qualities influence the learned Qψ and subsequent CopAC performance.
- What evidence would resolve it: Systematic studies varying the size and quality of the offline dataset used to train Qψ, along with corresponding performance metrics for CopAC, would shed light on this aspect.

### Open Question 3
- Question: How does CopAC handle the exploration-exploitation trade-off during the online learning phase, and what are the potential safety implications of its exploration strategy?
- Basis in paper: The paper mentions that CopAC aims to select stimulations that produce high-value world actions, but does not explicitly discuss how it balances exploration and exploitation or addresses safety concerns.
- Why unresolved: The authors do not provide any detailed discussion or experimental results on CopAC's exploration strategy or its safety implications during online learning.
- What evidence would resolve it: Analysis of CopAC's exploration behavior during online learning, along with safety metrics and potential mitigation strategies, would help understand this aspect better.

## Limitations
- The approach relies on transferring knowledge from biomechanical simulators to real patients, with uncertain generalization to clinical settings
- The brain model assumes a relatively straightforward mapping from stimulation to world actions, which may not capture the full complexity of neural modulation
- No formal safety analysis or validation of stimulation patterns in actual patients is provided

## Confidence
- **High confidence**: The decomposition approach and strategic sampling mechanism are well-supported by experimental results across multiple control tasks
- **Medium confidence**: Sample efficiency improvements are demonstrated but generalization to complex patient populations is uncertain
- **Medium confidence**: Safety benefits from strategic sampling are theoretically sound but require clinical validation

## Next Checks
1. **Clinical pilot study**: Test CopAC on actual stroke patients with varying injury profiles to validate simulation-to-real transfer and assess safety profiles in real clinical settings
2. **Brain model complexity analysis**: Systematically vary the capacity of the brain model bF_ϕ_brain to determine the minimum complexity required for good performance and identify the point of diminishing returns
3. **Adversarial stimulation testing**: Evaluate CopAC's robustness by introducing controlled perturbations to the stimulation signals to test whether the strategic sampling approach maintains safety and performance under noisy or adversarial conditions