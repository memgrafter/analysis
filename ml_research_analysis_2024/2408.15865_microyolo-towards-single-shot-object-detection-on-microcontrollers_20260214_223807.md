---
ver: rpa2
title: 'microYOLO: Towards Single-Shot Object Detection on Microcontrollers'
arxiv_id: '2408.15865'
source_url: https://arxiv.org/abs/2408.15865
tags:
- yolo
- detection
- object
- tasks
- image
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This work demonstrates the feasibility of deploying single-shot\
  \ object detection models on microcontrollers. By adapting the YOLO architecture\
  \ to create \xB5YOLO, optimized for Cortex-M based microcontrollers, the authors\
  \ achieve real-time object detection with 3.5 FPS on a 128x128 RGB input resolution."
---

# microYOLO: Towards Single-Shot Object Detection on Microcontrollers

## Quick Facts
- arXiv ID: 2408.15865
- Source URL: https://arxiv.org/abs/2408.15865
- Reference count: 10
- Primary result: Real-time object detection (3.5 FPS) on Cortex-M7 microcontrollers with <800 KB Flash and <350 KB RAM using 128x128 input resolution

## Executive Summary
This work demonstrates that single-shot object detection can be deployed on microcontrollers by adapting YOLO architecture to create µYOLO, optimized for resource-constrained Cortex-M platforms. The authors achieve real-time performance (3.5 FPS) on 128x128 RGB input using depthwise separable convolutions and aggressive model compression. The system demonstrates varying accuracy across three tasks: 56.4% mAP on fridge item detection, 27.7% on human detection, and 12.3% on vehicle detection. The performance differences are attributed to scene complexity and object size relative to input resolution. The work establishes that 128x128 is the optimal input resolution, as higher resolutions provide no accuracy benefit while significantly increasing memory consumption.

## Method Summary
µYOLO adapts YOLO architecture for microcontrollers by reducing input resolution to 128×128 pixels and using depthwise separable convolutions to minimize trainable parameters. The model employs a 5×5 grid with 2 bounding boxes per cell, batch normalization, and ReLU activation. Training uses iterative pruning every 20 epochs during the final 100 of 400 total epochs, followed by 8-bit quantization. The backbone is pre-trained on Caltech-256 before fine-tuning on target datasets. Data augmentation includes affine transforms and color adjustments. The model is deployed on Cortex-M7 microcontrollers using a compression pipeline, achieving 3.5 FPS while staying within 800 KB Flash and 350 KB RAM constraints.

## Key Results
- Real-time performance: 3.5 FPS on Cortex-M7 at 480 MHz
- Memory efficiency: <800 KB Flash, <350 KB RAM for 128×128 input
- Task-specific accuracy: 56.4% mAP (fridge items), 27.7% mAP (humans), 12.3% mAP (vehicles)
- Optimal resolution: 128×128 provides best accuracy-memory tradeoff
- Grid configuration: 5×5 grid with 2 bounding boxes per cell

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Depthwise separable convolutions drastically reduce trainable parameters while preserving detection capability on low-resolution inputs.
- Mechanism: By separating spatial filtering from channel mixing, each depthwise convolution applies a single filter per input channel, and the subsequent pointwise convolution combines these features with far fewer parameters than standard convolutions.
- Core assumption: The 128×128 input resolution limits the spatial complexity, so the reduced representational capacity of depthwise convolutions is still sufficient.
- Evidence anchors:
  - [abstract] "We lower the input image resolution to 128 × 128 pixels, significantly reduce the number of trainable parameters of the backbone network"
  - [section] "We choose depth-wise separable convolutions to minimize trainable parameters"
- Break condition: If objects require fine-grained spatial detail beyond what 128×128 can provide, the reduced parameter count may lead to significant accuracy loss.

### Mechanism 2
- Claim: Grid size S=5 and bounding boxes B=2 provide optimal trade-off between detection granularity and memory constraints.
- Mechanism: A 5x5 grid divides the image into 25 cells, each predicting up to 2 bounding boxes. This balances the need to cover object positions without exceeding Flash/RAM limits.
- Core assumption: Objects in the target scenes are large enough relative to the grid cells to be reliably detected within this coarse partitioning.
- Evidence anchors:
  - [section] "Our experiments so far have shown that, given the resource constraints of our target platform and the low spatial resolution of the input images (128), B = 2 and S = 5 is the best compromise"
- Break condition: If objects are smaller than grid cells or require more precise localization, the fixed grid resolution will cause significant misses.

### Mechanism 3
- Claim: Post-training 8-bit quantization maintains accuracy while enabling deployment on microcontrollers.
- Mechanism: Converting weights from floating-point to 8-bit integers reduces memory footprint by 75% with minimal accuracy degradation when the model is already pruned and optimized.
- Core assumption: The dynamic range of weights after pruning fits well within 8-bit representation without excessive quantization noise.
- Evidence anchors:
  - [section] "We apply 8-bit quantization afterwards... we realized early on that for our application it did not provide an improvement over post-training quantization"
- Break condition: If weight distribution is too skewed or sparse, quantization may cause significant accuracy degradation despite pruning.

## Foundational Learning

- Concept: Single-shot object detection architecture
  - Why needed here: Understanding how YOLO's grid-based detection differs from region proposal methods is crucial for grasping µYOLO's design constraints
  - Quick check question: What are the three outputs predicted for each grid cell in YOLO?

- Concept: Depthwise separable convolution mechanics
  - Why needed here: The parameter reduction strategy relies on understanding how spatial and channel operations are separated
  - Quick check question: How many parameters does a depthwise convolution have compared to a standard convolution with the same input/output channels?

- Concept: Memory constraints in embedded systems
  - Why needed here: The entire design philosophy revolves around fitting within 800 KB Flash and 350 KB RAM limits
  - Quick check question: What is the approximate memory difference between a floating-point and 8-bit quantized weight for a single parameter?

## Architecture Onboarding

- Component map:
  Input layer (3×128×128 RGB) -> Single standard convolution (3×64×4×2×0) -> Seven depthwise separable convolutions with max pooling -> Two linear layers (1024→1024→S×S×N+B×5) -> Output layer (grid predictions)

- Critical path:
  Image → Backbone convolutions → Flatten → Detection head → Bounding box predictions → Non-maximum suppression

- Design tradeoffs:
  - Input resolution (128×128) vs accuracy: Lower resolution saves memory but hurts small object detection
  - Grid size (5×5) vs coverage: Larger grids improve localization but increase memory usage
  - Bounding boxes per cell (2) vs object density: More boxes handle multiple nearby objects but increase computation

- Failure signatures:
  - Low mAP with high confidence scores: Likely insufficient grid resolution or too few bounding boxes
  - High memory usage during inference: Check quantization implementation or input resolution settings
  - Slow inference despite small model: Verify that depthwise convolutions are properly optimized for Cortex-M

- First 3 experiments:
  1. Run inference with dummy input to verify memory allocation matches Table 2 specifications
  2. Test different input resolutions (88×88, 128×128, 256×256) to confirm memory constraints from Table 3
  3. Measure FPS at different clock frequencies to validate the 3.5 FPS claim at 480 MHz

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the fundamental limit of input resolution for object detection on microcontrollers given the trade-off between scene complexity and object size relative to image size?
- Basis in paper: [explicit] The authors observe that increasing input resolution beyond 128×128 provides no accuracy benefit while significantly increasing memory consumption, and hypothesize this is due to the relationship between scene complexity and object size relative to image size.
- Why unresolved: The experiments only tested up to 448x448 resolution, and the analysis suggests there may be an optimal resolution threshold that varies based on scene characteristics. The current evidence is limited to specific datasets and tasks.
- What evidence would resolve it: Systematic experiments varying scene complexity and object size distributions across a wider range of input resolutions, testing whether there's a universal optimal resolution or if it's task-dependent.

### Open Question 2
- Question: How does iterative pruning during training affect the final accuracy and robustness of µYOLO compared to post-training quantization alone?
- Basis in paper: [explicit] The authors use iterative pruning during training followed by 8-bit quantization, noting that quantization-aware training was not used to minimize training overhead, and observe slight degradation during pruning but not after quantization.
- Why unresolved: The ablation study comparing pruning-only, quantization-only, and combined approaches is not provided, making it unclear whether pruning provides net benefits over simpler approaches.
- What evidence would resolve it: Controlled experiments comparing models trained with/without pruning, with/without quantization-aware training, and with different pruning schedules to quantify the marginal benefit of each technique.

### Open Question 3
- Question: What is the relationship between detection latency and detection accuracy for real-time applications, and how should this trade-off be optimized for different use cases?
- Basis in paper: [explicit] The authors achieved 3.5 FPS but note this might be too low for some applications and mention they could achieve up to 8 FPS with aggressive pruning at the cost of significantly degraded mAP, suggesting this trade-off warrants further exploration.
- Why unresolved: The paper provides limited analysis of how accuracy degrades with increased FPS and doesn't provide guidance on optimizing this trade-off for different application requirements.
- What evidence would resolve it: Detailed accuracy-FPS curves across different pruning schedules and input resolutions, coupled with application-specific requirements (e.g., safety-critical vs. energy-constrained scenarios) to establish guidelines for optimization.

## Limitations
- Significant performance variance across datasets (56.4% mAP for fridge items vs 12.3% for vehicles) suggests high dataset dependency
- Claims about 128×128 being optimal are based on memory constraints rather than systematic accuracy analysis
- Limited generalization claims given large mAP variance and narrow dataset diversity
- Real-world performance may vary with scene complexity beyond tested scenarios

## Confidence
- High confidence: Memory footprint claims (800 KB Flash, 350 KB RAM) are well-specified and verifiable through implementation
- Medium confidence: The 3.5 FPS claim is plausible given the Cortex-M7 target and reported optimizations, but real-world performance may vary with scene complexity
- Low confidence: Generalization claims across diverse detection tasks are weak given the large mAP variance and limited dataset diversity

## Next Checks
1. **Dataset property analysis**: Characterize the spatial distribution, size, and occlusion patterns in each dataset to explain the mAP variance and identify failure modes
2. **Resolution sweep validation**: Systematically test accuracy vs memory tradeoffs at resolutions between 88×88 and 256×256 to verify 128×128 is truly optimal
3. **Cross-dataset generalization test**: Evaluate µYOLO on a held-out dataset (e.g., COCO) to assess whether the architecture generalizes beyond the three tested scenarios