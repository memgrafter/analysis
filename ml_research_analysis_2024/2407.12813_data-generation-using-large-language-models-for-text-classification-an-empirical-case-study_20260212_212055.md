---
ver: rpa2
title: 'Data Generation Using Large Language Models for Text Classification: An Empirical
  Case Study'
arxiv_id: '2407.12813'
source_url: https://arxiv.org/abs/2407.12813
tags:
- data
- synthetic
- generation
- generate
- label
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigates the effectiveness of using large language
  models (LLMs) to generate synthetic data for text classification tasks. It explores
  various data generation methods, including zero-shot, one-shot, and few-shot in-context
  learning, as well as zero-shot topic generation.
---

# Data Generation Using Large Language Models for Text Classification: An Empirical Case Study

## Quick Facts
- **arXiv ID**: 2407.12813
- **Source URL**: https://arxiv.org/abs/2407.12813
- **Reference count**: 29
- **Primary result**: LLM-generated synthetic data achieves comparable performance with human-annotated data at lower cost, with greatest benefits in low-resource settings.

## Executive Summary
This empirical study investigates using large language models to generate synthetic data for text classification tasks. The research evaluates five in-context learning methods (zero-shot, zero-shot topic, one-shot, few-shot 3, few-shot 5) across six NLP tasks using GPT-3.5-turbo for generation and RoBERTa for classification. The study finds that synthetic data augmentation is most effective in low-resource settings, that mixing synthetic data with raw data during training yields significant improvements, and that synthetic data diversity is crucial for performance. The research also identifies bias in synthetic data generation and demonstrates that rephrasing can mitigate these issues.

## Method Summary
The study generates synthetic data using GPT-3.5-turbo with five in-context learning methods, creating 1,000 synthetic examples per task from 100 raw examples. These synthetic datasets are used to train RoBERTa classification models, with evaluations comparing performance when training on synthetic data alone versus mixing synthetic and raw data. The research systematically examines the relationship between LLM performance, data quality, and downstream classification results across diverse text classification tasks.

## Key Results
- Synthetic data augmentation is most effective in low-resource settings (100 raw examples)
- Mixing synthetic data with raw data during training yields significant improvements over synthetic data alone
- Zero-shot topic generation produces the most diverse synthetic data, improving model performance
- Increasing synthetic data volume beyond 1,000 examples shows diminishing returns
- Bias exists in synthetic data generation, particularly trivial questions in BoolQ task

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Synthetic data augmentation is most effective in low-resource settings.
- **Mechanism**: When raw labeled data is limited, synthetic examples provide substantial marginal benefit as the model hasn't saturated and can generalize better from additional diverse examples.
- **Core assumption**: The task complexity justifies additional training data even in small quantities.
- **Evidence anchors**: Abstract finding on low-resource effectiveness; empirical observation using 100 raw data points as low-resource baseline.
- **Break condition**: If the task is too simple, synthetic data may be redundant when raw data alone suffices.

### Mechanism 2
- **Claim**: Mixing synthetic data with raw data during training yields better performance than using synthetic data alone.
- **Mechanism**: Raw data provides high-quality, task-relevant supervision while synthetic data adds diversity, creating a more robust feature representation through complementary strengths.
- **Core assumption**: Raw data contains unique signal that synthetic data cannot fully capture, and synthetic data adds useful variability without overwhelming the raw signal.
- **Evidence anchors**: Abstract statement on mixing improvements; empirical observation of significant improvements across tasks when incorporating raw data.
- **Break condition**: If synthetic data is extremely noisy or biased, mixing may degrade rather than improve performance.

### Mechanism 3
- **Claim**: Zero-shot topic in-context generation produces more diverse synthetic data than other prompting methods, improving model performance.
- **Mechanism**: Random topic sampling forces the LLM to produce semantically varied examples, reducing repetition and overfitting to narrow data distributions.
- **Core assumption**: Training data diversity improves generalization for classification tasks, especially with small original datasets.
- **Evidence anchors**: Abstract emphasis on data diversity importance; empirical finding that zero-shot topic generation produces most diverse datasets.
- **Break condition**: If topic sampling poorly aligns with task domain, diversity gains may not translate to performance improvements.

## Foundational Learning

- **Concept**: In-context learning (zero/one/few-shot)
  - Why needed here: The paper relies on different in-context learning strategies to generate synthetic data; understanding how few examples bias the LLM's generation is critical to interpreting results.
  - Quick check question: How does increasing the number of in-context examples affect the diversity and quality of generated synthetic data?

- **Concept**: Synthetic data bias detection and mitigation
  - Why needed here: The paper identifies bias in synthetic examples (e.g., BoolQ trivial questions) and shows how rephrasing can reduce it; this is essential for ensuring the utility of generated data.
  - Quick check question: What patterns in synthetic data could indicate unintended bias, and how can they be detected automatically?

- **Concept**: Model performance metrics in imbalanced classification
  - Why needed here: Some tasks (e.g., EMO, RTE) are multi-class or imbalanced, so accuracy alone is insufficient; F1 or macro-F1 are used to capture balanced performance.
  - Quick check question: Why might accuracy be misleading in a multi-class classification task with imbalanced class distributions?

## Architecture Onboarding

- **Component map**: Prompt generator -> GPT-3.5-turbo -> synthetic data pool -> RoBERTa trainer -> evaluation pipeline
- **Critical path**: Prompt design -> data generation -> data filtering (bias check) -> model training -> performance evaluation
- **Design tradeoffs**: Higher synthetic data volume increases diversity but may introduce noise; prompt specificity trades off generation diversity
- **Failure signatures**: Performance plateaus despite more synthetic data; synthetic data shows repetitive patterns or task-specific bias; model overfits to synthetic distribution
- **First 3 experiments**:
  1. Run zero-shot generation on a small raw dataset and evaluate performance with and without mixing in raw data
  2. Compare synthetic data diversity (inter-sample similarity) across zero-shot, one-shot, and few-shot methods
  3. Test the impact of rephrasing synthetic examples to remove bias (e.g., BoolQ trivial questions) and measure performance change

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: What is the optimal amount of synthetic data to generate for text classification tasks using LLMs?
- **Basis in paper**: Explicit statement that 1,000 synthetic data points were chosen as additional synthetic data benefits diminish beyond this point
- **Why unresolved**: The optimal amount likely varies by task, model architecture, and dataset characteristics; paper only tested up to 1,000 data points
- **What evidence would resolve it**: Experiments generating varying amounts (100, 500, 1000, 2000) and measuring resulting model performance across multiple tasks

### Open Question 2
- **Question**: How does the quality of synthetic data generated by LLMs relate to the LLM's performance on the target task?
- **Basis in paper**: Investigation of relationship between LLM performance and data quality finding that zero-shot/few-shot LLM performance doesn't necessarily determine downstream model performance with generated data
- **Why unresolved**: The relationship is complex and may depend on prompt design and task complexity beyond what the paper explores
- **What evidence would resolve it**: Systematic studies comparing LLM task performance with synthetic data quality, controlling for other variables

### Open Question 3
- **Question**: How can we effectively measure and control for bias in synthetic data generated by LLMs?
- **Basis in paper**: Identification of bias in synthetic data generation, specifically terms introducing bias towards false answers in BoolQ task
- **Why unresolved**: Paper only addresses this for one task with limited rephrasing solution; bias detection and mitigation is a broader issue requiring comprehensive solutions
- **What evidence would resolve it**: Development and evaluation of methods to detect and mitigate bias across multiple tasks and domains

## Limitations
- Prompt engineering opacity makes precise replication difficult due to undisclosed exact templates and in-context examples
- Dataset representativeness limited to six selected tasks that may not capture full spectrum of real-world challenges
- Bias mitigation scope limited to manual inspection rather than systematic evaluation across multiple bias dimensions

## Confidence

**High Confidence Claims**:
- Synthetic data augmentation effectiveness in low-resource settings
- Benefits of mixing synthetic and raw data during training
- Identification of bias in synthetic examples and effectiveness of rephrasing

**Medium Confidence Claims**:
- Superiority of zero-shot topic generation for diversity
- Diminishing returns from increasing synthetic data volume
- Cost-effectiveness compared to human annotation

**Low Confidence Claims**:
- Generalizability across all text classification tasks
- Long-term model stability with synthetic data
- Optimal synthetic-to-raw data ratios

## Next Checks

1. **Prompt Sensitivity Analysis**: Systematically vary prompt formulations across the five generation methods while keeping all other factors constant to quantify the impact of prompt design on synthetic data quality and downstream model performance.

2. **Bias Spectrum Evaluation**: Develop automated metrics to measure multiple dimensions of bias in synthetic data (stereotyping, representational balance, topical coverage) and test whether manual rephrasing approaches scale to larger synthetic datasets.

3. **Cross-Domain Transferability**: Apply the methodology to classification tasks from underrepresented domains (medical, legal, technical) to assess whether observed benefits extend beyond the original six tasks, particularly examining whether diversity gains from topic generation remain beneficial in specialized contexts.