---
ver: rpa2
title: 'MuirBench: A Comprehensive Benchmark for Robust Multi-image Understanding'
arxiv_id: '2406.09411'
source_url: https://arxiv.org/abs/2406.09411
tags:
- images
- multi-image
- multimodal
- understanding
- bench
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: MUIR BENCH is a new benchmark for testing how well AI models can
  understand and reason about multiple images together. Unlike existing benchmarks
  that mostly focus on single images, MUIR BENCH includes 12 different types of multi-image
  tasks and 10 categories of relationships between images (like temporal, narrative,
  or multiview).
---

# MuirBench: A Comprehensive Benchmark for Robust Multi-image Understanding

## Quick Facts
- arXiv ID: 2406.09411
- Source URL: https://arxiv.org/abs/2406.09411
- Reference count: 40
- Primary result: Current multimodal models achieve only 49-68% accuracy on MUIR BENCH, well below human performance

## Executive Summary
MUIR BENCH is a new benchmark designed to evaluate AI models' ability to understand and reason about multiple images together. Unlike existing benchmarks that focus on single images, MUIR BENCH includes 12 diverse multi-image tasks and 10 categories of relationships between images. The benchmark contains 2,600 multiple-choice questions across 11,264 images, with each answerable question paired with an unanswerable variant to ensure robust evaluation. When tested on 20 recent multimodal models, even the best-performing ones like GPT-4o and Gemini Pro achieved only 68% and 49% accuracy respectively, demonstrating that current AI models still lack effective multi-image understanding capabilities.

## Method Summary
MUIR BENCH evaluates AI models on multi-image understanding across 12 diverse tasks with 10 categories of multi-image relations. The benchmark uses 11,264 images and 2,600 multiple-choice questions in a pairwise format (answerable + unanswerable variant). Models are evaluated without training using standardized prompts and a rule-based answer extraction tool. The benchmark includes images covering various categories of multi-image relations such as narrative images, ordered pages of documents, and multiview scenes. Human evaluation is conducted by domain experts for comparison.

## Key Results
- Best-performing models (GPT-4o, Gemini Pro) achieved only 68% and 49% accuracy respectively
- Open-source models trained on single images struggled with accuracy below 33%
- All models showed higher error rates on unanswerable instances compared to answerable ones
- Performance varied significantly by image position, with models performing best when images are in options and worst when images are in the middle of questions

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The pairwise design (answerable + unanswerable variants) ensures robust evaluation by testing both knowledge and recognition of uncertainty.
- Mechanism: For each standard instance, an unanswerable variant is created with minimal semantic differences. This forces models to either provide the correct answer or abstain when the query is unanswerable, rather than guessing.
- Core assumption: Models that achieve high accuracy on answerable questions but also make many errors on unanswerable ones are not truly understanding the content, just exploiting patterns.
- Evidence anchors:
  - [abstract] "MUIR BENCH is created in a pairwise manner, where each standard instance is paired with an unanswerable variant that has minimal semantic differences, in order for a reliable assessment."
  - [section 3.1] "MUIR BENCH adopts a pairwise design approach, where each question-answering instance is paired with an expert-annotated unanswerable counterpart [51] featuring minimal differences following Figure 5."
- Break condition: If models consistently guess correctly on unanswerable variants (e.g., always choosing "None of the options"), the evaluation would fail to differentiate true understanding from pattern matching.

### Mechanism 2
- Claim: Including diverse multi-image relations (temporal, narrative, multiview, etc.) tests holistic integration capabilities beyond single-image perception.
- Mechanism: By covering 10 categories of multi-image relations across 12 tasks, the benchmark requires models to synthesize information from varied perspectives, settings, and moments, simulating human multi-image processing.
- Core assumption: Single-image trained models cannot generalize to complex multi-image reasoning without specific multi-image training data and processes.
- Evidence anchors:
  - [abstract] "MUIR BENCH consists of 12 diverse multi-image tasks... that involve 10 categories of multi-image relations (e.g., multiview, temporal relations)."
  - [section 3.1] "MUIR BENCH includes images covering 10 various categories of multi-image relations, such as narrative images conveying stories or ideas, ordered pages of documents and slides providing collective insights..."
- Break condition: If models perform equally well on single-image and multi-image variants of the same task, the diversity of relations may not be adding significant evaluation value.

### Mechanism 3
- Claim: The multiple-choice format with expert-annotated options provides deterministic evaluation while covering diverse reasoning types.
- Mechanism: Questions are either derived from datasets or manually written by experts, with multiple-choice answers ensuring consistent scoring across models while testing various reasoning skills.
- Core assumption: Multiple-choice format can adequately capture the complexity of multi-image understanding without losing nuance in model responses.
- Evidence anchors:
  - [abstract] "Comprising 11,264 images and 2,600 multiple-choice questions..."
  - [section 3.2] "To achieve this goal, we consider three sources of data, including existing datasets, dataset derivations, as well as newly collected data... The questions and choices are either derived from the datasets, or manually written by experts."
- Break condition: If open-ended responses from models reveal understanding that multiple-choice format fails to capture, the evaluation would be incomplete.

## Foundational Learning

- Concept: Visual grounding in multi-image contexts
  - Why needed here: Models must identify specific objects across multiple images and understand their relationships, not just recognize them in isolation
  - Quick check question: Can you explain how a model would determine which car appears in both Image A and Image B when they show different views of the same parking lot?

- Concept: Temporal and sequential reasoning across image sets
  - Why needed here: Many tasks require understanding progression, ordering, or narrative flow across multiple images
  - Quick check question: How would you design a model to correctly order a sequence of images showing a plant growing over time?

- Concept: Attribute similarity across multiple visual examples
  - Why needed here: Tasks like attribute similarity require comparing features across images to find common patterns
  - Quick check question: What features would you extract to determine which vases share the same painted design across multiple images?

## Architecture Onboarding

- Component map: Vision encoder (multiple images) → Cross-attention fusion → Multi-image reasoning module → Answer decoder
- Critical path: Image preprocessing → Feature extraction → Cross-image relationship modeling → Question-image alignment → Answer selection
- Design tradeoffs: Single concatenated image input vs. parallel image processing (parallel allows better spatial relationship modeling but increases computational cost)
- Failure signatures: Random guessing on unanswerable variants, consistent errors on specific image positions, poor performance on narrative/sequential tasks
- First 3 experiments:
  1. Test model performance on answerable vs. unanswerable variants to measure uncertainty recognition
  2. Evaluate accuracy across different image positions (beginning, middle, end, options) to identify positional bias
  3. Compare performance on single-image vs. multi-image versions of the same task to measure generalization capability

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can multimodal models effectively generalize from single-image training to multi-image inference tasks, and what architectural or training modifications are necessary to bridge this gap?
- Basis in paper: [inferred] The paper shows that single-image input models (e.g., LLaVA-NeXT-34B) perform significantly worse on MUIR BENCH compared to multi-image input models (e.g., Mantis-8B-Idefics2), even when the single-image model has more parameters.
- Why unresolved: While the paper demonstrates a performance gap, it does not experimentally test specific architectural changes (e.g., attention mechanisms for cross-image reasoning) or training strategies (e.g., synthetic multi-image data generation) that could improve single-image model generalization.
- What evidence would resolve it: Controlled experiments comparing single-image models trained with added multi-image reasoning components or fine-tuned on curated multi-image datasets against their baseline performance on MUIR BENCH.

### Open Question 2
- Question: How do different types of unanswerable instances (image replacement, question modification, option modification) affect model reliability, and can models be trained to better recognize unanswerable queries?
- Basis in paper: [explicit] The paper shows that all models have higher error rates on unanswerable instances compared to answerable ones, with performance varying by unanswerable type (e.g., GPT-4o performs better on image-replaced instances than option-removed ones).
- Why unresolved: The paper does not explore whether models can be explicitly trained to detect unanswerable queries or whether certain unanswerable types are inherently more challenging for current architectures.
- What evidence would resolve it: Training studies where models are fine-tuned on balanced answerable/unanswerable data, measuring improvements in abstention accuracy and comparing performance across unanswerable types.

### Open Question 3
- Question: Does the position of images within a question (beginning, middle, end, or in options) systematically impact model performance, and what does this reveal about attention mechanisms in multimodal models?
- Basis in paper: [explicit] The paper shows that models perform best when images are in options and worst when images are in the middle of questions, suggesting a correlation between image position and error rate.
- Why unresolved: The paper does not investigate the underlying reasons for this positional bias or test whether architectural changes (e.g., cross-attention mechanisms) can mitigate these differences.
- What evidence would resolve it: Ablation studies varying image positions in training data and testing whether models trained on diverse positional data show reduced positional bias on MUIR BENCH.

## Limitations

- The benchmark relies on multiple-choice format, which may not fully capture the complexity of multi-image reasoning compared to open-ended responses
- Only 20 existing models were evaluated, potentially missing important model variants or newer architectures
- The unanswerable variant design assumes minimal semantic differences, but some variants may still be too easily distinguishable

## Confidence

- **High**: The benchmark's pairwise design (answerable + unanswerable variants) is well-documented and clearly implemented. The evaluation methodology for existing models is transparent and reproducible.
- **Medium**: The claim that current models significantly underperform humans is supported by the experimental results, but the human performance baseline methodology is not fully detailed.
- **Low**: The assertion that the 10 categories of multi-image relations comprehensively cover all relevant types is not empirically validated.

## Next Checks

1. Test additional recent multimodal models (beyond the 20 evaluated) to verify if the performance gap is consistent across the broader model landscape
2. Conduct ablation studies removing specific image relation categories to assess their individual contribution to overall performance
3. Evaluate human performance using the same automated answer extraction tool applied to models to ensure fair comparison