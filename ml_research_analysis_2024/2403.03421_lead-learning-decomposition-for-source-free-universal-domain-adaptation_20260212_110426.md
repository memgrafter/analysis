---
ver: rpa2
title: 'LEAD: Learning Decomposition for Source-free Universal Domain Adaptation'
arxiv_id: '2403.03421'
source_url: https://arxiv.org/abs/2403.03421
tags:
- data
- lead
- domain
- adaptation
- source
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper tackles the problem of Source-free Universal Domain
  Adaptation (SF-UniDA), which aims to transfer knowledge between domains with both
  covariate and label shifts, without access to source data. The main challenge is
  distinguishing between target data belonging to common categories and target-private
  unknown categories.
---

# LEAD: Learning Decomposition for Source-free Universal Domain Adaptation

## Quick Facts
- arXiv ID: 2403.03421
- Source URL: https://arxiv.org/abs/2403.03421
- Authors: Sanqing Qu, Tianpei Zou, Lianghua He, Florian Röhrbein, Alois Knoll, Guang Chen, Changjun Jiang
- Reference count: 40
- Primary result: Proposes LEAD method for SF-UniDA that outperforms existing methods with 75% reduced computational time

## Executive Summary
This paper addresses the challenge of Source-free Universal Domain Adaptation (SF-UniDA), where a model must adapt to a target domain with both covariate and label shifts without access to source data. The key difficulty lies in distinguishing between target data belonging to common categories and target-private unknown categories. The authors propose LEAD (LEArning Decomposition), which uses orthogonal decomposition to separate features into source-known and source-unknown components, then builds instance-level decision boundaries based on distances to target prototypes and source anchors. Extensive experiments show LEAD significantly outperforms existing methods across various UniDA scenarios while reducing computational time.

## Method Summary
LEAD decomposes features into source-known and source-unknown components using orthogonal decomposition based on the pre-trained source classifier weights. For target adaptation, it constructs target prototypes via top-K sampling, collects source anchors from the classifier weights, and computes instance-level decision boundaries using distances to both prototypes and anchors. The method employs a two-component Gaussian Mixture Model to estimate the distribution of source-unknown feature magnitudes, using this to establish adaptive decision boundaries. Training is performed using pseudo-label learning with a combined loss function incorporating cross-entropy loss, feature decomposition regularization, and feature consensus regularization.

## Key Results
- LEAD outperforms GLC by 3.5% in overall H-score on the OPDA scenario of the VisDA dataset
- Reduces computational time by 75% compared to clustering-based methods for deriving pseudo-labeling decision boundaries
- Shows significant improvements when integrated with existing SF-UniDA methods like UMAD and GLC
- Demonstrates effectiveness across multiple UniDA scenarios including PDA, OSDA, and OPDA

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Orthogonal decomposition effectively separates target-private data from common data by isolating the portion of features orthogonal to the source model's classifier weights.
- Mechanism: Features are projected onto the null space of the source classifier weight matrix, producing two orthogonal components: one aligned with source knowledge (source-known) and one independent of it (source-unknown). The magnitude of the source-unknown component is used as a discriminative indicator for target-private data.
- Core assumption: Even under covariate shift, target-private data contain a higher proportion of components from the source-unknown space than common data.
- Evidence anchors:
  - [abstract] "LEAD capitalizes on the orthogonal decomposition to build two orthogonal feature spaces, i.e., source-known and -unknown space. Feature projection on source-unknown space is extracted as the descriptor for private data."
  - [section] "target-private data are expected to encompass more components from the orthogonal complement (source-unknown) space of the pre-trained model."
  - [corpus] Weak. Corpus lacks direct orthogonal decomposition references, but several papers mention feature decomposition or space separation in SF-UniDA.

### Mechanism 2
- Claim: Instance-level decision boundaries, derived from distances to target prototypes and source anchors, enable adaptive separation of common and private data across varying covariate shifts.
- Mechanism: For each instance, a "common score" is computed based on its distance to both target prototypes (for common categories) and source anchors (for source-known categories). This score modulates the decision threshold, allowing per-instance adaptation to category-specific covariate shifts.
- Core assumption: The distance to both target prototypes and source anchors provides complementary information about whether an instance belongs to a common or private category.
- Evidence anchors:
  - [abstract] "LEAD builds instance-level decision boundaries to adaptively identify target-private data."
  - [section] "LEAD considers the distances to both the target prototypes and source anchors to establish instance-level decision boundaries."
  - [corpus] Weak. While corpus mentions adaptive strategies and decision boundaries, it lacks specific details on using both target prototypes and source anchors in SF-UniDA.

### Mechanism 3
- Claim: Two-component Gaussian Mixture Modeling of the source-unknown feature magnitude distribution enables robust estimation of the common/private data boundary.
- Mechanism: The distribution of the norm of source-unknown feature components is modeled as a bimodal GMM, with one component representing common data and the other representing private data. The means of these components (µcom and µpri) are used to parameterize the adaptive decision boundaries.
- Core assumption: The empirical distribution of source-unknown feature magnitudes exhibits a bimodal pattern, with distinct peaks corresponding to common and private data.
- Evidence anchors:
  - [section] "our observations have revealed that the empirical distributions of ∥zt i,unk∥2 exhibit a bimodal pattern, with the presence of two distinct peaks typically indicating the modes of common and private data."
  - [section] "we employ a two-component Gaussian Mixture Model (GMM) to estimate the distribution ∥zt i,unk∥2, with the components featuring a lower mean representing the common data and those with a higher mean corresponding to the private data."
  - [corpus] Weak. Corpus does not directly mention GMM usage for source-unknown feature magnitudes in SF-UniDA, though GMM is mentioned in one related paper.

## Foundational Learning

- Concept: Orthogonal Decomposition (e.g., Singular Value Decomposition)
  - Why needed here: To mathematically construct the source-known and source-unknown feature spaces by projecting features onto the row space and null space of the source classifier weight matrix.
  - Quick check question: Given a classifier weight matrix Wcls ∈ RC×D, how would you use SVD to extract the basis vectors for the source-known and source-unknown spaces?

- Concept: Gaussian Mixture Models (GMMs)
  - Why needed here: To model the bimodal distribution of source-unknown feature magnitudes and estimate the parameters (means) that define the common and private data modes.
  - Quick check question: What are the key assumptions of GMMs, and how do you fit a two-component GMM to a dataset of scalar values?

- Concept: Silhouette Score for Cluster Number Estimation
  - Why needed here: To estimate the number of target categories (ˆCt) for determining the number of target prototypes (K = Nt/ˆCt) using top-K sampling.
  - Quick check question: How does the Silhouette score evaluate the quality of a clustering solution, and how would you use it to select the optimal number of clusters?

## Architecture Onboarding

- Component map: Feature Extractor -> Orthogonal Decomposition Module -> Target Prototype Constructor -> Source Anchor Collector -> Common Score Calculator -> GMM Estimator -> Decision Boundary Generator -> Pseudo-labeler -> Training Loop

- Critical path: Feature extraction → Orthogonal decomposition → Common score calculation → GMM fitting → Decision boundary generation → Pseudo-label assignment → Model optimization

- Design tradeoffs:
  - Using orthogonal decomposition vs. other feature separation methods: Orthogonal decomposition is mathematically rigorous and avoids manual thresholding but may be sensitive to the quality of the source model.
  - Instance-level vs. global decision boundaries: Instance-level boundaries are more adaptive but computationally more expensive.
  - GMM-based boundary estimation vs. fixed thresholds: GMMs are more robust to distribution shifts but require fitting and may fail if the distribution is not truly bimodal.

- Failure signatures:
  - Poor separation of common and private data in t-SNE visualizations.
  - Low pseudo-label accuracy or unstable training curves.
  - High variance in H-score across different runs or datasets.
  - GMM fitting fails to converge or produces unrealistic component means.

- First 3 experiments:
  1. Implement the orthogonal decomposition module and verify that the source-known and source-unknown components are truly orthogonal and that their norms sum to 1.
  2. Implement the GMM fitting on source-unknown feature magnitudes and visualize the resulting bimodal distribution.
  3. Implement the instance-level decision boundary generation and evaluate its performance on a small, controlled dataset where the common/private data separation is known.

## Open Questions the Paper Calls Out

The paper does not explicitly call out specific open questions, but several areas remain unexplored based on the analysis of the proposed method.

## Limitations

- The orthogonal decomposition approach may fail under complex covariate shifts where common categories are severely distorted, causing them to exhibit strong components in the source-unknown space.
- The two-component GMM assumption is a significant simplification that may not hold in practice when dealing with multi-modal distributions or overlapping modes in real-world data.
- The computational overhead of instance-level decision boundaries may become prohibitive for large-scale datasets, despite being more adaptive than global thresholds.

## Confidence

- Mechanism 1 (Orthogonal Decomposition): Medium confidence. The mathematical framework is well-established, but the assumption that source-unknown components reliably indicate target-private data requires more empirical validation across diverse covariate shift scenarios.
- Mechanism 2 (Instance-Level Decision Boundaries): Medium confidence. While the concept of using both target prototypes and source anchors is intuitive, the paper lacks ablation studies demonstrating the necessity of both components, and the computational overhead is not thoroughly analyzed.
- Mechanism 3 (GMM Boundary Estimation): Low confidence. The bimodal assumption is a significant simplification that may not hold in practice, particularly when label noise or complex data distributions are present. The paper provides limited evidence for the reliability of GMM fitting across different datasets.

## Next Checks

1. Systematically test the orthogonal decomposition across datasets with varying levels of covariate shift severity to quantify how often the source-unknown feature magnitude distributions actually exhibit the assumed bimodal pattern. Measure the correlation between component norms and ground-truth common/private labels.

2. Implement timing benchmarks comparing LEAD's instance-level decision boundary computation against baseline methods across different dataset sizes. Identify at what scale the computational cost becomes prohibitive.

3. Design controlled experiments where the source-unknown feature magnitude distribution is intentionally made multi-modal or overlapping, then measure LEAD's performance degradation compared to simpler thresholding approaches.