---
ver: rpa2
title: Automatic Differentiation is Essential in Training Neural Networks for Solving
  Differential Equations
arxiv_id: '2405.14099'
source_url: https://arxiv.org/abs/2405.14099
tags:
- training
- neural
- truncated
- values
- networks
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper demonstrates that automatic differentiation (AD) outperforms
  finite difference (FD) in training neural networks for solving partial differential
  equations (PDEs). The authors introduce truncated entropy as a metric to quantify
  the number of small singular values in system matrices, which are critical to training
  efficiency.
---

# Automatic Differentiation is Essential in Training Neural Networks for Solving Differential Equations

## Quick Facts
- **arXiv ID**: 2405.14099
- **Source URL**: https://arxiv.org/abs/2405.14099
- **Reference count**: 40
- **Primary result**: Automatic differentiation (AD) outperforms finite difference (FD) in training neural networks for solving partial differential equations (PDEs)

## Executive Summary
This paper demonstrates that automatic differentiation (AD) is superior to finite difference (FD) methods when training neural networks for solving partial differential equations. The authors introduce truncated entropy as a metric to quantify small singular values in system matrices, which are critical to training efficiency. Through theoretical analysis and experiments on random feature models and two-layer neural networks, they show that AD preserves fewer small singular values, leading to faster convergence and better accuracy compared to FD. The results are validated across 1D and 2D Poisson equations and a 1D biharmonic equation.

## Method Summary
The study compares AD and FD methods for training neural networks to solve PDEs. AD computes derivatives analytically via the chain rule, while FD approximates derivatives numerically using local points. The analysis uses random feature models (fixed inner layer, trained outer layer) and two-layer neural networks (trained inner and outer layers) with gradient descent optimization. Truncated entropy is introduced as a metric to quantify the number of small singular values in system matrices, serving as an indicator of training speed and residual loss. The authors analyze singular value distributions in the training kernel and system matrix, comparing AD and FD performance across multiple PDE problems.

## Key Results
- AD yields larger truncated entropy and faster training convergence than FD for solving PDEs
- AD retains fewer small singular values in the training kernel, leading to more efficient optimization
- The small singular values of FD are larger than those of AD due to numerical differentiation errors
- AD consistently outperforms FD in terms of training speed and accuracy across 1D/2D Poisson equations and 1D biharmonic equation

## Why This Works (Mechanism)

### Mechanism 1
- AD preserves more small singular values in the training kernel than FD, leading to better optimization dynamics
- Core assumption: The difference in singular value distributions between AD and FD directly correlates with training performance
- Break condition: If numerical differentiation error becomes negligible compared to problem scale, or if the system is well-conditioned such that small singular values don't impact training

### Mechanism 2
- Truncated entropy serves as a reliable metric for quantifying training speed and error
- Core assumption: The relationship between truncated entropy and training performance holds across different neural network architectures and PDEs
- Break condition: If training dynamics are dominated by factors other than singular value distributions, such as learning rate scheduling or architecture-specific optimization challenges

### Mechanism 3
- AD and FD exhibit similar behavior for large singular values but differ significantly for small singular values
- Core assumption: The magnitude of numerical differentiation errors is small enough to not affect large singular values but significant enough to impact small singular values
- Break condition: If numerical differentiation error becomes comparable to problem scale, or if the system is poorly conditioned such that large singular values are also affected

## Foundational Learning

- **Singular value decomposition (SVD)**: Understanding how singular values affect training error and convergence speed
  - Quick check: What happens to the condition number of a matrix when its smallest singular value approaches zero?

- **Automatic differentiation (AD)**: Recognizing how AD computes derivatives analytically and its advantages over numerical methods
  - Quick check: How does AD handle the chain rule differently from symbolic differentiation?

- **Physics-Informed Neural Networks (PINNs)**: Understanding the context in which AD and FD are compared for solving PDEs
  - Quick check: What are the key components of a PINN loss function for solving a PDE?

## Architecture Onboarding

- **Component map**: Random Feature Model (Fixed inner layer) -> Trained outer layer -> Linear system Aa = f
  Two-Layer Neural Network (Trained inner and outer layers) -> Gradient descent optimization
  Loss Functions (AD and FD variants) -> Metrics (Truncated entropy, training error, convergence speed)

- **Critical path**:
  1. Set up PDE problem and neural network architecture
  2. Implement AD and FD loss functions
  3. Train models and monitor convergence
  4. Analyze singular value distributions and truncated entropy
  5. Compare training performance between AD and FD

- **Design tradeoffs**:
  - AD vs FD: Accuracy vs computational cost
  - Model complexity: Random feature model vs two-layer neural network
  - Precision: Double precision for RFM vs float for NN

- **Failure signatures**:
  - AD underperforms FD: Numerical differentiation error is negligible, or problem is well-conditioned
  - Truncated entropy doesn't correlate with training performance: Other factors dominate training dynamics
  - Training diverges: Learning rate too high, or model architecture unsuitable for problem

- **First 3 experiments**:
  1. Replicate 1D Poisson equation results, comparing AD and FD on both RFM and NN
  2. Vary activation functions and dimensions to test robustness of AD advantage
  3. Analyze singular value distributions and truncated entropy for different neural network sizes

## Open Questions the Paper Calls Out

- **How do truncated entropy and effective cut-off number generalize to deeper neural network architectures beyond two-layer networks?**
  - Basis: The paper states future research could explore deeper networks and different architectures like CNNs and Transformers
  - Why unresolved: Current analysis is limited to two-layer networks
  - What evidence would resolve it: Comparative studies applying truncated entropy analysis to deeper networks and different architectures

- **What is the impact of different optimization algorithms beyond gradient descent on the relationship between truncated entropy and training speed?**
  - Basis: The paper mentions various optimization methods exist but analysis is based on gradient descent
  - Why unresolved: Current analysis only considers gradient descent
  - What evidence would resolve it: Empirical studies comparing correlation across multiple optimization algorithms

- **How does the choice of activation function affect the distribution of singular values and resulting truncated entropy in both AD and FD methods?**
  - Basis: The paper conducts experiments with different activation functions but doesn't provide comprehensive analysis
  - Why unresolved: Paper shows activation functions affect singular values but doesn't systematically explore the relationship
  - What evidence would resolve it: Systematic study varying activation functions across different PDE types and dimensions

## Limitations

- Analysis is limited to two-layer neural networks, leaving uncertainty about behavior in deeper architectures
- Truncated entropy metric's generalizability beyond specific PDE problems tested remains unproven
- Sensitivity to problem conditioning and numerical differentiation error magnitude is not fully explored

## Confidence

- **Main claim (AD outperforms FD)**: Medium-High confidence
- **Mechanism (singular value preservation)**: Medium confidence
- **Truncated entropy as metric**: Medium-High confidence

## Next Checks

1. **Cross-Architecture Validation**: Test AD vs FD performance comparison on additional neural network architectures (e.g., deeper networks, convolutional networks) and PDE types to verify robustness of truncated entropy metric and singular value preservation claims.

2. **Numerical Error Analysis**: Systematically vary numerical differentiation step size in FD to quantify its impact on small singular values and training dynamics, establishing clearer relationship between numerical error magnitude and performance degradation.

3. **Condition Number Sensitivity**: Analyze how the ratio of large to small singular values (condition number) affects relative performance of AD vs FD across range of well-conditioned and ill-conditioned PDE problems, identifying thresholds where numerical differentiation errors become significant.