---
ver: rpa2
title: Self-Generated Critiques Boost Reward Modeling for Language Models
arxiv_id: '2411.16646'
source_url: https://arxiv.org/abs/2411.16646
tags:
- reward
- response
- critiques
- critique
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces Critic-RM, a framework that enhances reward
  modeling in large language models (LLMs) by incorporating self-generated critiques.
  The key innovation is a two-stage process: first generating and filtering high-quality
  critiques, then jointly fine-tuning on reward prediction and critique generation
  objectives.'
---

# Self-Generated Critiques Boost Reward Modeling for Language Models

## Quick Facts
- **arXiv ID:** 2411.16646
- **Source URL:** https://arxiv.org/abs/2411.16646
- **Authors:** Yue Yu, Zhengxing Chen, Aston Zhang, Liang Tan, Chenguang Zhu, Richard Yuanzhe Pang, Yundi Qian, Xuewei Wang, Suchin Gururangan, Chao Zhang, Melanie Kambadur, Dhruv Mahajan, Rui Hou
- **Reference count:** 19
- **Primary result:** Critic-RM improves reward modeling accuracy by 3.7%-7.3% compared to standard reward models and LLM judges across benchmarks

## Executive Summary
This paper introduces Critic-RM, a framework that enhances reward modeling in large language models (LLMs) by incorporating self-generated critiques. The key innovation is a two-stage process: first generating and filtering high-quality critiques, then jointly fine-tuning on reward prediction and critique generation objectives. Experiments show that Critic-RM improves reward modeling accuracy by 3.7%-7.3% compared to standard reward models and LLM judges across benchmarks like RewardBench and CrossEval. The approach also demonstrates data efficiency, achieving strong performance with limited training data, and shows particular effectiveness in reasoning-intensive tasks.

## Method Summary
Critic-RM employs a two-stage process to improve reward modeling in LLMs. First, the model generates multiple candidate critiques for each response, filters them based on consistency with human preference labels, then refines through summarization or ranking. Second, the model undergoes joint fine-tuning on both critique generation and reward prediction objectives using a dynamic weight scheduling strategy. The shared LLM backbone has two heads: a language modeling head for critique generation and a randomly initialized reward modeling head. During inference, multiple critique samples can be generated and averaged to produce final scores, particularly beneficial for reasoning tasks.

## Key Results
- Critic-RM achieves 3.7%-7.3% improvement in reward modeling accuracy over baseline approaches on RewardBench and CrossEval
- The framework shows particular effectiveness in reasoning-intensive tasks like Math, Coding, and Safety domains
- Dynamic weight scheduling outperforms both constant weight and reverse scheduling approaches in ablation studies

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Self-generated critiques improve reward modeling by providing interpretable intermediate reasoning that aligns better with human preferences than scalar scores alone.
- Mechanism: The model first generates multiple candidate critiques for each response, filters them based on consistency with human preference labels, then jointly fine-tunes on both critique generation and reward prediction. This dual-objective training leverages the language model's inherent capability to reason about quality while still producing scalar rewards for optimization.
- Core assumption: High-quality critiques generated by the model itself, when properly filtered, serve as better supervision signals than human-written critiques or scalar-only training.
- Evidence anchors: [abstract]: "we propose Critic-RM, a framework that improves reward models using self-generated critiques without extra supervision"; [section 3.2.2]: "we design a generate-then-filter framework to create high-quality supervision signals for critique model training"

### Mechanism 2
- Claim: The dynamic weight scheduling strategy balances the conflicting objectives of diverse critique generation versus overfitting-prone reward modeling.
- Mechanism: During training, the model initially focuses on critique generation loss (λ=1), then gradually transitions to reward prediction loss (λ approaches 0). This allows the model to benefit from diverse critiques in early stages while preventing overfitting in the reward modeling phase.
- Core assumption: Reward models are prone to overfitting when trained with diverse critiques, while critique generation benefits from seeing multiple examples.
- Evidence anchors: [section 3.2.3]: "To resolve this issue, we design a dynamic weight schedule approach, where we add an additional weight λ(t) on Equation 5"; [section 4.5]: "using a constant weight across different rounds, as well as reverse weight scheduling (i.e., prioritizing reward modeling first, followed by critique generation), both negatively impact performance"

### Mechanism 3
- Claim: Inference-time scaling with multiple critique samples improves performance particularly on reasoning-intensive tasks.
- Mechanism: During inference, the model generates multiple critiques (m > 1) with non-zero temperatures, then averages the rewards across these critiques to produce the final score.
- Core assumption: Different critiques capture different aspects of response quality, and averaging across them provides more robust reward estimates.
- Evidence anchors: [abstract]: "Experiments on preference ranking benchmarks including RewardBench and CrossEval show that Critic-RM improves reward modeling accuracy by 3.7%–7.3%"; [section 4.2]: "these gains are most pronounced in reasoning-intensive tasks such as Math, Coding, and Safety"

## Foundational Learning

- **Concept:** Bradley-Terry model for pairwise preference learning
  - Why needed here: The reward model is trained using pairwise logistic loss based on the Bradley-Terry model to predict preferences between response pairs
  - Quick check question: How does the Bradley-Terry model convert scalar reward scores into preference probabilities?

- **Concept:** Knowledge distillation and self-improvement paradigms
  - Why needed here: Critic-RM builds on self-improvement techniques where models refine themselves using their own generated data, similar to recent advances in self-improving language models
  - Quick check question: What distinguishes Critic-RM's self-improvement approach from standard knowledge distillation?

- **Concept:** Kullback-Leibler divergence and its forward/backward variants
  - Why needed here: The critique generation objective uses forward KL divergence to avoid policy and entropy collapses that would occur with backward KL
  - Quick check question: Why does using backward KL divergence in Eq. 5 lead to policy and entropy collapses?

## Architecture Onboarding

- **Component map:** LLM backbone with shared weights -> Language modeling head (inherited) + Reward modeling head (randomly initialized) -> Joint fine-tuning with dynamic weight scheduling
- **Critical path:** Generate candidate critiques → Filter based on score consistency → Refine through summarization/ranking → Jointly fine-tune on critique generation and reward prediction → Inference with optional scaling
- **Design tradeoffs:** The main tradeoff is between inference speed (single critique vs. multiple scaled critiques) and accuracy, as well as the complexity of the two-stage training process versus simpler reward modeling approaches.
- **Failure signatures:** Poor performance on reasoning tasks suggests inadequate critique quality; overfitting on training data indicates incorrect weight scheduling; failure to improve over baseline reward models suggests filtering is too aggressive or refinement is ineffective.
- **First 3 experiments:**
  1. Run ablation on weight scheduling (constant vs. reverse vs. dynamic) to validate the transition mechanism
  2. Test different filtering thresholds to find optimal balance between critique quality and quantity
  3. Compare single vs. multiple critique inference scaling on reasoning benchmarks to quantify performance gains

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of Critic-RM vary across different LLM architectures beyond Llama-3.1-70B-Instruct?
- Basis in paper: [inferred] The paper mentions that Critic-RM requires the base LLM to have a certain level of critique generation ability and suggests testing across different LLM architectures could provide broader insights.
- Why unresolved: The experiments in the paper are conducted primarily using Llama-3.1-70B-Instruct as the backbone, with limited testing on Llama-3-8B. There is no comprehensive evaluation across multiple LLM architectures.
- What evidence would resolve it: Systematic experiments comparing Critic-RM performance across various LLM architectures (e.g., GPT-4, Claude, Mistral) would provide insights into its generalizability and dependency on specific model characteristics.

### Open Question 2
- Question: What is the optimal number of critique generation iterations for iterative training in Critic-RM?
- Basis in paper: [explicit] The paper states that Critic-RM does not incorporate iterative training, where models refine themselves over multiple rounds, and mentions that adding this step could further improve performance.
- Why unresolved: The paper does not explore iterative training approaches, leaving the question of how many iterations would be optimal unanswered.
- What evidence would resolve it: Experiments testing different numbers of iterative training rounds (e.g., 2, 3, 4) with varying amounts of critique refinement would determine the optimal balance between performance gains and computational cost.

### Open Question 3
- Question: How does inference-time scaling affect Critic-RM performance across different task domains?
- Basis in paper: [explicit] The paper observes that inference-time scaling mainly helps for reasoning tasks, with gains most pronounced in math, coding, and safety domains.
- Why unresolved: While the paper identifies this pattern, it doesn't provide a detailed analysis of how scaling affects performance across all task domains or what the underlying mechanisms are.
- What evidence would resolve it: Comprehensive experiments measuring performance gains from inference-time scaling across all evaluated domains (chat, reasoning, safety, cross-eval, etc.) with statistical analysis of the variance in improvement rates would clarify the domain-specific effects.

## Limitations

- The filtering mechanism relies heavily on consistency with human preference labels, but the quality threshold for filtering is not clearly specified
- The dynamic weight scheduling strategy shows improvement over alternatives, but the optimal transition point between phases is not thoroughly explored
- The inference-time scaling benefits are most pronounced on reasoning tasks, suggesting the approach may not generalize equally well to all domains

## Confidence

- **High confidence**: The core claim that Critic-RM improves reward modeling accuracy by 3.7%-7.3% compared to baseline approaches is well-supported by experimental results across multiple benchmarks (RewardBench, CrossEval)
- **Medium confidence**: The mechanism that self-generated critiques provide better supervision than scalar-only training is plausible but relies on assumptions about critique quality and filtering effectiveness that aren't fully validated
- **Medium confidence**: The dynamic weight scheduling approach is shown to work better than alternatives in ablation studies, but the sensitivity to hyperparameter choices (transition point, rate of change) remains unclear

## Next Checks

1. Conduct systematic ablation studies on filtering thresholds to determine the optimal balance between critique quality and quantity for effective training
2. Test the model's performance on out-of-distribution tasks not included in the training data to evaluate generalization beyond the demonstrated benchmarks
3. Analyze the correlation structure of multiple critiques generated during inference to verify that averaging across samples provides genuinely diverse perspectives rather than redundant information