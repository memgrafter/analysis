---
ver: rpa2
title: 'K-QA: A Real-World Medical Q&A Benchmark'
arxiv_id: '2401.14493'
source_url: https://arxiv.org/abs/2401.14493
tags:
- answer
- medical
- statements
- question
- have
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces K-QA, a real-world medical question-answering
  benchmark containing 1,212 deidentified patient questions from actual conversations
  held on the K Health platform. A panel of in-house physicians manually answered
  and decomposed 201 questions into 1,589 self-contained statements, categorized as
  Must Have (clinically crucial) or Nice to Have (supplemental).
---

# K-QA: A Real-World Medical Q&A Benchmark

## Quick Facts
- arXiv ID: 2401.14493
- Source URL: https://arxiv.org/abs/2401.14493
- Reference count: 10
- Primary result: Introduces K-QA benchmark with 1,212 real-world medical questions, evaluating LLM performance on comprehensiveness and hallucination metrics

## Executive Summary
This paper introduces K-QA, a benchmark of 1,212 real-world patient questions from the K Health platform, with 201 questions manually answered and decomposed into 1,589 statements by physicians. The authors develop NLI-based evaluation metrics to measure comprehensiveness (recall of clinically crucial Must Have statements) and hallucination rate (contradictions with ground truth). Experiments with state-of-the-art LLMs show that while larger models and retrieval-augmented generation improve comprehensiveness, hallucinations remain a significant challenge, with the best model achieving 67.7% comprehensiveness and 15.4% hallucination rate.

## Method Summary
The authors collected 1,212 deidentified patient questions from K Health's platform and had physicians manually answer and decompose 201 of these questions into atomic statements. These statements were classified as Must Have (clinically crucial) or Nice to Have (supplemental). They evaluated 7 state-of-the-art models using zero-shot and in-context learning, with and without medical RAG augmentation using MayoClinic and NHS sources. Performance was measured using NLI-based metrics where GPT-4 determined entailment relationships between model responses and ground truth statements.

## Key Results
- GPT-4 with in-context learning and medical RAG achieved the best performance: 67.7% comprehensiveness and 15.4% hallucination rate
- Larger models generally showed improved comprehensiveness but also higher hallucination rates
- Retrieval-augmented generation with domain-specific sources reduced hallucinations compared to general web search
- Only 62.7% of questions received responses from the best-performing model

## Why This Works (Mechanism)

### Mechanism 1
Real-world patient questions are more challenging for LLMs than medical exam-style questions due to ambiguity and need for long-form responses. The benchmark uses actual patient questions from K Health's platform, which contain real-world ambiguity (non-medical jargon, multiple interacting conditions) that typical exam-style datasets lack. Core assumption: Patient questions reflect genuine information needs requiring nuanced, long-form answers. Evidence anchors: [abstract] "real-world questions... can often include various interacting medical conditions... use ambiguous, non-medical jargon... and require long-form, nuanced answers." Break condition: If patient questions were systematically sanitized during dataset creation.

### Mechanism 2
NLI-based evaluation using decomposed statements provides more granular assessment of LLM performance than end-to-end evaluation. Answers are decomposed into atomic statements classified as Must Have or Nice to Have, then evaluated using NLI models to check entailment/contradiction relationships. Core assumption: Medical accuracy can be decomposed into discrete factual statements independently verified. Evidence anchors: [section 4] "We consider a predicted answer as a premise and each ground-truth statement... as a hypothesis." Break condition: If decomposition introduces significant annotation bias.

### Mechanism 3
Retrieval-augmented generation with domain-specific sources reduces hallucinations compared to general web search. Using medically-oriented augmented retrieval (MayoClinic, NHS) provides relevant context that grounds responses in verified medical information. Core assumption: Domain-specific retrieval sources contain necessary medical information to support accurate responses and reduce hallucination risk. Evidence anchors: [section 5.2] "GPT-3.5+ICL+RAG demonstrates the fewest hallucinations while maintaining a comparatively good comprehensiveness score." Break condition: If retrieval sources are incomplete or outdated.

## Foundational Learning

- Concept: Natural Language Inference (NLI) and entailment relationships
  - Why needed here: The evaluation framework relies on determining whether LLM-generated answers entail or contradict ground-truth medical statements
  - Quick check question: What are the three possible NLI relationships between premise and hypothesis in this framework?

- Concept: Information decomposition into atomic semantic units
  - Why needed here: Medical answers must be broken down into discrete statements that can be independently evaluated for completeness and accuracy
  - Quick check question: How does the Must Have/Nice to Have classification affect the comprehensiveness and hallucination metrics?

- Concept: Retrieval-augmented generation (RAG) systems
  - Why needed here: Understanding how domain-specific retrieval affects LLM performance is critical for implementing effective medical Q&A systems
  - Quick check question: What is the difference between the RAG approach used in this work versus the retrieval used by BARD and Bing Chat?

## Architecture Onboarding

- Component map: Question preprocessing pipeline (BERT classifier) → Physician answer annotation (three-stage process) → Statement decomposition (manual + GPT-4 assisted) → NLI evaluation framework (GPT-4 with CoT) → RAG implementation (domain-specific indexing) → Model evaluation interface (comprehensive reporting)

- Critical path: Question → Preprocessing → Physician Answer → Statement Decomposition → NLI Evaluation → Performance Metrics

- Design tradeoffs:
  - Granularity vs. context in statement decomposition
  - Cost vs. accuracy in manual annotation vs. automated evaluation
  - Domain specificity vs. coverage breadth in retrieval sources
  - Metric comprehensiveness vs. evaluation complexity

- Failure signatures:
  - Low comprehensiveness scores indicate models miss critical medical information
  - High hallucination rates suggest models make medically dangerous statements
  - Poor NLI agreement indicates evaluation framework issues
  - Retrieval failures when relevant information is missing from sources

- First 3 experiments:
  1. Test NLI model performance by having physicians classify 50 random question-answer-statement triplets and compare to GPT-4 predictions
  2. Evaluate RAG effectiveness by comparing hallucination rates with and without domain-specific vs. general web retrieval
  3. Analyze statement decomposition quality by having physicians review 100 automatically generated statements for accuracy and completeness

## Open Questions the Paper Calls Out

- How would different NLI model choices affect the evaluation metrics and agreement rates?
- What is the optimal prompt engineering strategy for medical QA in this domain?
- How do domain-specific retrieval sources impact medical QA performance compared to general web search?
- How does model size scaling affect the tradeoff between comprehensiveness and hallucination rate?
- What is the impact of patient demographics on model performance for medical QA?

## Limitations

- Evaluation framework relies heavily on GPT-4 for both reference generation and assessment, potentially introducing bias
- Manual annotation process involved multiple stages with different annotators, raising inter-annotator consistency concerns
- 16.7% of questions where GPT-4 refused to generate reference answers may introduce selection bias

## Confidence

- High confidence: Real-world medical questions are more complex than exam-style questions (well-supported by concrete examples)
- Medium confidence: NLI-based evaluation approach shows promise but has uncertainty due to potential noise propagation
- Low confidence: Domain-specific RAG reduces hallucinations compared to general web search (weakly supported)

## Next Checks

1. Have a third-party physician independently review 100 random question-answer-statement triplets to verify classifications and NLI assessments, comparing results to GPT-4-based annotations.

2. Implement two RAG configurations - one using domain-specific medical sources and another using general web search - and evaluate both on the same questions to directly measure source selection impact on hallucination rates.

3. Analyze the 16.7% of questions where GPT-4 refused to generate reference answers to identify patterns and assess whether missing responses introduce systematic bias in evaluation metrics.