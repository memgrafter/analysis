---
ver: rpa2
title: Inverse decision-making using neural amortized Bayesian actors
arxiv_id: '2409.03710'
source_url: https://arxiv.org/abs/2409.03710
tags:
- cost
- bayesian
- parameters
- neural
- posterior
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work addresses the challenge of inverse decision-making for
  Bayesian actor models in continuous action spaces, where standard analytical methods
  fail and numerical approximations are computationally prohibitive. The core idea
  is to amortize the Bayesian decision-making problem by training a neural network
  to approximate the optimal action using the cost function as an unsupervised training
  objective.
---

# Inverse decision-making using neural amortized Bayesian actors

## Quick Facts
- arXiv ID: 2409.03710
- Source URL: https://arxiv.org/abs/2409.03710
- Authors: Dominik Straub; Tobias F. Niehues; Jan Peters; Constantin A. Rothkopf
- Reference count: 40
- One-line primary result: Amortized neural networks enable efficient gradient-based Bayesian inference of actor model parameters from behavioral data

## Executive Summary
This work addresses the challenge of inverse decision-making for Bayesian actor models in continuous action spaces, where standard analytical methods fail and numerical approximations are computationally prohibitive. The core idea is to amortize the Bayesian decision-making problem by training a neural network to approximate the optimal action using the cost function as an unsupervised training objective. This enables efficient gradient-based Bayesian inference of the model's parameters from behavioral data. The method was validated on synthetic data, showing that inferred posterior distributions closely match those obtained via analytical solutions when available, and accurately recover ground truth parameters otherwise. The approach also reveals identifiability issues between priors and costs in more complex cost functions, which are mitigated when one is fixed. Applied to empirical data from a bean-bag throwing task, the method successfully explains individual behavioral patterns and variability.

## Method Summary
The method trains a neural network to approximate the optimal action for a Bayesian actor model using the cost function as an unsupervised training objective. The network is trained across a wide range of parameter settings using the expected posterior loss, which is minimized via stochastic gradient descent with the reparameterization trick. This pre-trained network then serves as a differentiable approximation of the optimal action within a probabilistic model, enabling efficient gradient-based Bayesian inference of the model's parameters using Hamiltonian Monte Carlo. The approach handles continuous action spaces where analytical solutions are intractable and avoids expensive numerical optimization of optimal actions.

## Key Results
- Neural network approximations achieve low MSE compared to analytical solutions for quadratic costs (0.01 vs 0.0017 for σ0)
- Posterior distributions accurately recover ground truth parameters from synthetic data (R² ≈ 0.98 for σ0)
- Identifiability issues between prior and cost parameters observed in complex cost functions but resolved when fixing one parameter
- Method successfully explains individual behavioral patterns in bean-bag throwing task with posterior predictive distributions matching observed data

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Amortizing Bayesian decision-making with a neural network allows efficient gradient-based inference of model parameters from behavioral data
- Mechanism: The neural network approximates the optimal action for any given parameter setting, making it differentiable and usable as a stand-in within a probabilistic model. This enables Hamiltonian Monte Carlo to sample from the posterior over parameters efficiently
- Core assumption: The neural network can learn to approximate the Bayesian actor's optimal action well enough across the parameter space that downstream inference remains accurate
- Evidence anchors:
  - [abstract]: "we amortize the Bayesian actor using a neural network trained on a wide range of parameter settings in an unsupervised fashion. Using the pre-trained neural network enables performing efficient gradient-based Bayesian inference of the Bayesian actor model’s parameters"
  - [section]: "The neural network implicitly solves the Bayesian decision-making problem... This gradient-based inference algorithm can be used because the neural network is differentiable with respect to the parameters θ and sensory input m"
  - [corpus]: Weak — no direct mentions of amortization or gradient-based inference in corpus neighbors; closest is ASPIRE on amortized posterior inference, but for inverse problems not behavioral modeling
- Break condition: If the neural network fails to generalize across parameter settings or the decision problem is too complex for the network architecture, gradients become unreliable and inference breaks down

### Mechanism 2
- Claim: Training the neural network using the cost function as a stochastic training objective avoids the need for expensive numerical optimization of optimal actions
- Mechanism: The expected posterior loss is minimized via stochastic gradient descent with Monte Carlo samples from the posterior and response distributions, directly optimizing the network to predict optimal actions without supervised data
- Core assumption: The reparameterization trick enables unbiased gradient estimation of the expected loss with respect to the neural network parameters
- Evidence anchors:
  - [section]: "We use the expected posterior loss as a training objective... we need to apply the reparameterization trick and instead take the expectation over a distribution that does not depend on ψ"
  - [abstract]: "train a neural network to approximate the optimal action using the cost function as an unsupervised training objective"
  - [corpus]: Weak — no direct mention of reparameterization or unsupervised training in corpus; closest is LazyDINO using surrogate-driven methods but not unsupervised
- Break condition: If the reparameterization is not differentiable or the Monte Carlo estimates have too high variance, the training objective becomes unstable and the network fails to learn accurate approximations

### Mechanism 3
- Claim: The method reveals identifiability issues between prior and cost parameters in Bayesian actor models
- Mechanism: By inferring both prior and cost parameters jointly from behavioral data, the posterior distributions expose correlations that indicate unidentifiability, which would be hidden if parameters were fixed a priori
- Core assumption: The identifiability issue is a property of the model itself, not the inference method, and can be diagnosed through posterior analysis
- Evidence anchors:
  - [section]: "identifiability problems between priors and costs of Bayesian actor models... we can turn to investigating whether we can tease these different sources of biases apart"
  - [abstract]: "we show that identifiability problems between priors and costs can arise in more complex cost functions"
  - [corpus]: Weak — no direct mention of identifiability or priors/costs in corpus; closest is ASPIRE on inverse problems, but not behavioral modeling
- Break condition: If the experimental design does not include multiple conditions where priors or costs are held fixed, the unidentifiability cannot be resolved even with this method

## Foundational Learning

- Concept: Bayesian decision theory and posterior expected loss
  - Why needed here: The method relies on computing or approximating arg min_a E[p(s|m)][ℓ(a,s)] for optimal actions, and uses this as the training objective for the neural network
  - Quick check question: What is the mathematical expression for the optimal action in a Bayesian actor model with a cost function ℓ(a,s) and posterior p(s|m)?

- Concept: Reparameterization trick in stochastic optimization
  - Why needed here: Allows gradient estimation of the expected loss with respect to neural network parameters by sampling from distributions independent of those parameters
  - Quick check question: How does the reparameterization trick enable backpropagation through stochastic nodes in the network training process?

- Concept: Hamiltonian Monte Carlo and gradient-based inference
  - Why needed here: The differentiable neural network allows use of NUTS (a variant of HMC) to sample efficiently from the posterior over model parameters given behavioral data
  - Quick check question: Why is differentiability of the neural network critical for using gradient-based MCMC algorithms like NUTS?

## Architecture Onboarding

- Component map: Cost function -> Neural network (4-layer MLP with swish activations) -> Perceptual model -> Response model -> Bayesian inference (NUTS)

- Critical path:
  1. Train neural network on wide range of parameter settings using unsupervised cost-based objective
  2. Validate network accuracy on analytical or numerical solutions
  3. Set up probabilistic model with neural network as optimal action approximator
  4. Run NUTS to sample posterior over parameters given behavioral dataset
  5. Analyze posterior for parameter recovery and identifiability

- Design tradeoffs:
  - Wide vs. narrow priors during training: wider ensures better generalization but may reduce accuracy on typical experimental parameters
  - Network depth and width: deeper networks may capture more complex decision problems but risk overfitting or slower training
  - Fixed vs. inferred perceptual uncertainty: fixing improves identifiability but requires separate psychophysics

- Failure signatures:
  - Poor posterior recovery of ground truth parameters
  - High correlation between prior and cost parameters in posterior
  - Training loss plateaus early or diverges
  - NUTS sampler fails to converge (R-hat > 1.05)

- First 3 experiments:
  1. Validate neural network accuracy on synthetic data with known analytical solution (quadratic cost) and compare posterior means and variances to ground truth
  2. Test identifiability by simulating data from a cost function with effort terms, inferring both prior and cost parameters, and checking for posterior correlations
  3. Apply method to real sensorimotor data (e.g., bean-bag throwing) and visualize posterior predictive distributions against observed behavior

## Open Questions the Paper Calls Out

- Can the neural network approximation be extended to handle circular variables or more complex cognitive reasoning tasks where the perceptual model assumptions don't hold?
  - Basis in paper: [inferred] The paper mentions this as a limitation and suggests learning an approximate posterior distribution together with the action network, similar to loss-calibrated inference
  - Why unresolved: The current method requires a perceptual model that allows sampling from the posterior, which may not be suitable for all types of stimuli or cognitive tasks
  - What evidence would resolve it: Demonstrating the method's effectiveness on tasks involving circular variables or complex reasoning would show its broader applicability

- How does the choice of cost function family impact the identifiability of prior and cost parameters?
  - Basis in paper: [explicit] The paper discusses identifiability issues between priors and costs, particularly for more complex cost functions
  - Why unresolved: While the paper shows that fixing either priors or costs can resolve identifiability issues, it doesn't explore how different cost function families affect this problem
  - What evidence would resolve it: Systematic comparison of identifiability across different cost function families would clarify which functions are more prone to this issue

- How does the method perform when the true underlying perceptual and motor noise models differ from the assumed log-normal distributions?
  - Basis in paper: [inferred] The paper assumes log-normal distributions for perceptual and motor noise, but this may not always be the case in real-world scenarios
  - Why unresolved: The paper validates the method using synthetic data generated from the assumed models, but doesn't explore performance under model misspecification
  - What evidence would resolve it: Testing the method on data generated from different noise distributions would reveal its robustness to model assumptions

## Limitations

- Method depends critically on neural network's ability to generalize across parameter space; may struggle with complex cost functions or high-dimensional action spaces
- Identifiability issues between prior and cost parameters represent fundamental limitation of Bayesian actor framework rather than inference method
- Unsupervised training approach lacks stability guarantees of supervised learning and may produce unreliable approximations if training objective is not well-conditioned

## Confidence

- Confidence is High in the core mechanism of using neural networks to amortize Bayesian decision-making
- Medium in the method's ability to handle complex cost functions
- Low regarding its scalability to high-dimensional problems without further architectural innovations

## Next Checks

1. Test the method on cost functions with non-differentiable components or discontinuities to evaluate robustness limits
2. Evaluate performance on high-dimensional action spaces (e.g., reaching tasks with multiple joints) to assess scalability
3. Compare inference accuracy when using different neural network architectures (e.g., convolutional vs. fully connected) on the same decision problems