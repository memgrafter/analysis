---
ver: rpa2
title: 'Windows Agent Arena: Evaluating Multi-Modal OS Agents at Scale'
arxiv_id: '2409.08264'
source_url: https://arxiv.org/abs/2409.08264
tags:
- agent
- windows
- task
- text
- computer
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces WINDOWS AGENTARENA, a new benchmark suite
  for evaluating multi-modal autonomous agents within the Windows operating system.
  The benchmark addresses the challenge of measuring agent performance in realistic
  environments by providing 150+ diverse tasks across representative domains like
  office applications, web browsing, coding, and media.
---

# Windows Agent Arena: Evaluating Multi-Modal OS Agents at Scale

## Quick Facts
- **arXiv ID**: 2409.08264
- **Source URL**: https://arxiv.org/abs/2409.08264
- **Reference count**: 40
- **Primary result**: 19.5% success rate for Navi agent vs 74.5% for humans on 150+ Windows tasks

## Executive Summary
Windows Agent Arena introduces a new benchmark suite for evaluating multi-modal autonomous agents within the Windows operating system. The benchmark addresses the challenge of measuring agent performance in realistic environments by providing 150+ diverse tasks across representative domains like office applications, web browsing, coding, and media. The benchmark is designed to be scalable and deployable in Azure, allowing full evaluations to complete in as little as 20 minutes through parallelization.

The authors introduce a new multi-modal agent called Navi and evaluate its performance on Windows Agent Arena. Navi achieves a 19.5% success rate, significantly lower than the 74.5% success rate of unassisted humans, demonstrating that current generalist agents still have substantial room for improvement. The paper provides extensive quantitative and qualitative analysis of Navi's performance and discusses opportunities for future research in agent development and data generation using Windows Agent Arena.

## Method Summary
The paper introduces Windows Agent Arena as a benchmark for evaluating multi-modal OS agents at scale. The benchmark uses Docker containers to host Windows 11 VMs, allowing tasks to be distributed across multiple Azure compute instances for parallel execution. The Navi agent leverages set-of-marks prompting and combines UIA tree parsing with pixel-based element detectors to create a detailed representation of the screen. The benchmark includes 150+ diverse tasks across representative domains and provides automatic execution-based evaluation criteria that reward task completion rather than following pre-recorded human trajectory demonstrations.

## Key Results
- Navi agent achieves 19.5% success rate on Windows Agent Arena benchmark
- Unassisted humans achieve 74.5% success rate on the same tasks
- Full benchmark evaluation completes in as little as 20 minutes through Azure parallelization
- Common agent failures include inability to correctly align text output with visual understanding of the screen

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Windows Agent Arena provides scalable and reproducible environment through Azure cloud parallelization
- **Mechanism**: Uses Docker containers hosting Windows 11 VMs, distributing tasks across multiple Azure compute instances for parallel execution
- **Core assumption**: Azure VM instances can be efficiently parallelized to handle sequential multi-step tasks without significant overhead
- **Evidence anchors**: Abstract states benchmark can be parallelized in Azure for full evaluation in as little as 20 minutes; section discusses task parallelization to accelerate evaluation
- **Break condition**: If overhead of managing multiple VM instances or cost of Azure resources becomes prohibitive

### Mechanism 2
- **Claim**: Navi achieves competitive performance through set-of-marks prompting and combined UIA tree parsing with pixel-based element detectors
- **Mechanism**: Combines UIA tree parsing to extract visible elements with pixel-based element detectors (OCR, icon detection, image detection) for detailed screen representation
- **Core assumption**: Combination provides more comprehensive and accurate screen representation than either method alone
- **Evidence anchors**: Abstract reports 19.5% success rate; section identifies visual-language misalignment as common failure cause
- **Break condition**: If UIA tree parsing becomes unreliable or pixel-based detectors fail to accurately identify screen elements

### Mechanism 3
- **Claim**: Benchmark designed to be extensible and continuously updated with new tasks and capabilities
- **Mechanism**: Open-source framework allows adding new tasks reflecting common Windows OS user workflows
- **Core assumption**: Open-source nature and continuous task addition ensures benchmark remains relevant as applications and workflows evolve
- **Evidence anchors**: Abstract mentions plans to continuously update with new tasks; section describes scalable Docker container deployment
- **Break condition**: If community fails to contribute new tasks or benchmark becomes too focused on specific task types

## Foundational Learning

- **Concept**: Understanding POMDP framework for modeling agent behavior in Windows Agent Arena
  - **Why needed**: Provides formal way to model agent's decision-making in partially observable Windows OS environment
  - **Quick check**: What are key components of a POMDP and how do they relate to agent's interaction with Windows OS environment?

- **Concept**: Familiarity with Windows UI Automation (UIA) tree and its role in extracting screen elements
  - **Why needed**: UIA tree provides structured representation of UI elements crucial for agent to understand and interact with screen
  - **Quick check**: How does UIA tree differ from other screen element extraction methods like DOM parsing or pixel-based detection?

- **Concept**: Understanding set-of-marks (SoM) prompting and its application in visual grounding for multi-modal agents
  - **Why needed**: SoM prompting allows agent to visually ground understanding by associating text descriptions with visual elements, improving UI interaction
  - **Quick check**: How does SoM prompting differ from traditional image captioning, and what are its advantages in GUI navigation context?

## Architecture Onboarding

- **Component map**: Docker container hosting Windows 11 VM -> client process for task scheduling and configuration -> Navi agent -> evaluation scripts -> Azure infrastructure for parallelization
- **Critical path**: Set up initial task state -> allow agent to interact with environment -> evaluate agent's performance using evaluation scripts
- **Design tradeoffs**: Docker containers and Azure parallelization provide scalability but introduce complexity and potential security concerns; UIA tree parsing vs pixel-based detection involves tradeoff between accuracy and speed
- **Failure signatures**: Agent failing to correctly identify screen elements; evaluation scripts not accurately measuring task completion; issues with Azure parallelization setup
- **First 3 experiments**:
  1. Run simple task (opening text file and typing few words) to verify basic agent-environment interaction
  2. Test parallelization setup by running multiple instances of same task to ensure Azure infrastructure properly distributes workload
  3. Evaluate performance of different combinations of UIA tree parsing and pixel-based element detectors to determine optimal configuration

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: How does Windows Agent Arena's evaluation infrastructure scale to handle thousands of concurrent tasks, and what are bottlenecks in achieving near real-time evaluation?
- **Basis**: Explicit - Paper discusses Azure cloud parallelization and mentions ability to evaluate hundreds of tasks in parallel
- **Why unresolved**: Mentions Azure Machine Learning jobs and Docker containers but doesn't detail exact scaling limits and potential bottlenecks
- **What evidence would resolve**: Detailed performance metrics and benchmarks showing system behavior with increasing concurrent tasks, including CPU, memory, and network utilization data

### Open Question 2
- **Question**: What is impact of different screen parsing models (UIA tree, DOM tree, OCR, icon detection, image detection) on agent performance, and how do these models interact?
- **Basis**: Explicit - Paper discusses use of various screen parsing models and their impact on agent performance
- **Why unresolved**: Mentions use of different models but doesn't provide comprehensive comparison of individual and combined effects
- **What evidence would resolve**: Detailed ablation study comparing performance of agents using different combinations of screen parsing models, along with analysis of individual contributions to task completion

### Open Question 3
- **Question**: How does agent's performance on Windows Agent Arena tasks translate to real-world scenarios, and what are limitations of benchmark in capturing complexity of real-world tasks?
- **Basis**: Inferred - Paper discusses realism of tasks but doesn't provide direct comparison to real-world agent performance or comprehensive analysis of benchmark's limitations
- **Why unresolved**: While tasks are designed to be realistic, may not capture aspects of real-world tasks like dynamic environments, unexpected user interactions, or complex multi-step reasoning
- **What evidence would resolve**: Study comparing agent performance on Windows Agent Arena tasks to real-world scenarios, along with detailed analysis of benchmark's limitations and potential improvements

## Limitations
- Benchmark may not fully represent diversity of real-world Windows usage despite 150+ tasks
- Navi agent relies on proprietary models for Set-of-Marks generation, limiting reproducibility
- Automatic execution-based evaluation criteria may not fully capture true task completion versus superficial success

## Confidence
- **High Confidence**: Windows Agent Arena provides scalable evaluation framework using Azure parallelization - well-supported by technical details and explicit Docker container setup description
- **Medium Confidence**: Reported performance difference between Navi (19.5%) and humans (74.5%) is credible - though methodology for measuring human performance could benefit from more documentation
- **Low Confidence**: Claim about continuous updates and extensibility - stated as goal but lacks concrete evidence of active community or development roadmap

## Next Checks
- Conduct systematic analysis of task coverage by comparing benchmark's task distribution against actual usage statistics from large-scale Windows deployments
- Implement head-to-head comparison using both proprietary and open-sourced Set-of-Marks models to quantify performance gap
- Design and execute user study where human evaluators review agent trajectories to assess whether automatic evaluation metrics accurately reflect meaningful task completion