---
ver: rpa2
title: Unintended Impacts of LLM Alignment on Global Representation
arxiv_id: '2402.15018'
source_url: https://arxiv.org/abs/2402.15018
tags:
- alignment
- language
- country
- english
- starling
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper explores how alignment procedures impact model performance
  across global populations. The authors evaluate three axes of global representation:
  English dialects, multilingualism, and global opinions.'
---

# Unintended Impacts of LLM Alignment on Global Representation

## Quick Facts
- arXiv ID: 2402.15018
- Source URL: https://arxiv.org/abs/2402.15018
- Reference count: 25
- Primary result: Alignment improves English intent prediction for US, Indian, and Nigerian speakers but increases performance disparities between dialects from ~1% to as high as 17.1%.

## Executive Summary
This paper investigates how alignment procedures impact model performance across global populations through three axes: English dialects, multilingualism, and global opinions. The authors find that while alignment improves performance for US, Indian, and Nigerian English speakers, it simultaneously increases performance disparities between these dialects. Multilingual performance improves across most tested languages even with minimal multilingual data in alignment datasets, and reward models show high correlation with US opinions but limited influence on final model preferences. The study highlights the need for transparent reporting of alignment decisions and careful consideration of global representation in alignment procedures.

## Method Summary
The authors evaluate nine open-source 7B parameter language models at different alignment stages using greedy decoding on downstream tasks. They assess dialect performance using the MD3 dataset for US, Indian, and Nigerian English intent detection, multilingual capabilities using TyDiQA and Belebele benchmarks across nine languages, and opinion alignment by comparing model responses to country-specific survey data from 237 countries. The study examines SFT and PT (RLHF/DPO) effects separately where possible, though some models like Llama 2 Chat lack released SFT checkpoints, limiting full disentanglement of alignment stage impacts.

## Key Results
- Alignment improves English intent prediction for US, Indian, and Nigerian speakers but increases dialect performance disparities from ~1% to as high as 17.1%
- Just 13.1% multilingual data in Tülu dataset leads to performance improvements in six out of nine tested languages
- Reward models show 99.4% correlation with US opinions but have limited influence on final model preferences on out-of-distribution opinion tasks

## Why This Works (Mechanism)

### Mechanism 1
Alignment procedures disproportionately improve US English dialect performance compared to other dialects. Alignment steps (SFT and PT) introduce training signals that favor patterns common in US English data, leading to larger accuracy gains for US dialect intent prediction while improving all dialects. Core assumption: Alignment datasets and procedures are biased toward US English linguistic patterns and preferences. Evidence: [abstract] "alignment improves performance on English intent prediction for US, Indian, and Nigerian speakers but increases performance disparities between dialects from ~1% to as high as 17.1%"; [section 4] "Whenever changes to performance are significant (p<0.05), the alignment steps increase US English performance much more significantly than other global Englishes". Break condition: If alignment datasets were balanced across dialects or if PT explicitly corrected dialect disparities.

### Mechanism 2
Small amounts of multilingual data in SFT can significantly improve multilingual model performance. Even unintentional multilingual samples in SFT data expose the model to diverse language patterns, improving cross-lingual transfer capabilities during alignment. Core assumption: Multilingual data in SFT, even if minimal, provides useful inductive bias for multilingual understanding. Evidence: [abstract] "we find that just 13.1% of the Tülu dataset is in any language other than English, and yet this multilingual data leads to performance improvements in six out of nine tested languages"; [section 5] "Despite the stated goal to create English chat assistants, we find gains across most languages after alignment". Break condition: If multilingual samples were too few or too noisy to provide useful learning signals.

### Mechanism 3
Reward models do not significantly influence model preferences on out-of-distribution opinion tasks. When preference-tuning data lacks opinion-based examples, the reward model cannot effectively steer the base model's learned preferences, which remain dominated by pretraining data patterns. Core assumption: Reward models only influence model behavior in domains well-represented in preference-tuning data. Evidence: [abstract] "we find that the open-source Starling reward model, on average, rates 99.4% of all other countries more negatively than the USA. However, this bias does not seem to propagate to the language model preference-tuned with this reward model"; [section 6.1] "Interestingly, Starling RM predictions correlate poorly with all models, including Starling LM, suggesting the preferences were not reflected in the model". Break condition: If preference-tuning data included opinion-based examples or if reward models had broader influence than observed.

## Foundational Learning

- Concept: Dialectal variation in English
  - Why needed here: Understanding how different English varieties (US, Indian, Nigerian) affect model performance and alignment impacts
  - Quick check question: What are the key phonological and lexical differences between US and Nigerian English that might affect intent detection?

- Concept: Multilingual transfer learning
  - Why needed here: Explaining how exposure to multiple languages during training improves performance across language boundaries
  - Quick check question: How does multilingual pretraining data influence a model's ability to perform well in low-resource languages during fine-tuning?

- Concept: Reward modeling and preference optimization
  - Why needed here: Understanding how reward models are trained and how they influence the final aligned model behavior
  - Quick check question: What is the difference between how reward models and direct preference optimization influence model outputs?

## Architecture Onboarding

- Component map: Base LM → SFT → PT (RLHF/DPO) → Final Chat Model
- Critical path:
  1. Collect and curate SFT data (instruction following examples)
  2. Train SFT model on this data
  3. Collect preference data (ranked completion pairs)
  4. Train reward model (RLHF) or apply direct optimization (DPO)
  5. Apply preference tuning to get final chat model
  6. Evaluate on dialect, multilingual, and opinion tasks
- Design tradeoffs:
  - Dataset curation: Broad vs. focused coverage (affects generalization vs. task performance)
  - Feedback source: Human vs. synthetic vs. model-generated (affects cost, quality, bias)
  - Algorithm choice: RLHF vs. DPO (affects training stability, interpretability, reward model influence)
- Failure signatures:
  - Increasing performance gaps between dialects → bias in alignment data/procedures
  - Decreased multilingual performance → insufficient or poor-quality multilingual data
  - Reward model bias not reflected in final model → out-of-distribution preference task
- First 3 experiments:
  1. Replicate dialect performance evaluation with balanced alignment data across US, Indian, and Nigerian English
  2. Systematically vary multilingual data percentage in SFT and measure cross-lingual transfer effects
  3. Test reward model influence by including/excluding opinion-based examples in preference-tuning data

## Open Questions the Paper Calls Out

- Question: What is the relative contribution of SFT vs. PT to the observed dialect performance disparities, given that Llama 2 Chat's SFT model was not released?
- Basis in paper: [inferred] from the discussion of disentangling SFT and RLHF effects and the observation that "We cannot disentangle the effects of SFT and RLHF in the alignment of Llama 2 Chat" due to the unreleased SFT model.
- Why unresolved: The authors cannot directly test individual changes to the alignment procedure and their downstream impacts because they use open checkpoints rather than aligning the models themselves. This limits their ability to isolate the specific effects of SFT versus PT on dialect performance.
- What evidence would resolve it: Releasing the Llama 2 Chat SFT model or conducting a controlled experiment where models undergo only SFT or only PT would allow for a direct comparison of their individual contributions to dialect performance disparities.

- Question: How do the multilingual performance improvements observed in Tülu and Starling translate to other typologically diverse languages not included in the TyDiQA and Belebele benchmarks?
- Basis in paper: [explicit] from the finding that "just 13.1% of the Tülu dataset is in any language other than English, and yet this multilingual data leads to performance improvements in six out of nine tested languages."
- Why unresolved: The authors only tested performance on nine typologically diverse languages from the TyDiQA and Belebele datasets. It remains unclear whether the observed improvements generalize to other languages, especially those with significantly different linguistic features.
- What evidence would resolve it: Evaluating the multilingual performance of Tülu and Starling on a wider range of languages, including those with diverse linguistic features not represented in the current benchmarks, would provide insights into the generalizability of the observed improvements.

- Question: What specific aspects of the base model's pre-training data contribute to its preferences on out-of-distribution opinion-based tasks, as evidenced by the limited influence of reward models?
- Basis in paper: [explicit] from the finding that "the similarity in country preferences is instead mostly consistent between model families" and the conclusion that "the selection of the original SFT data and of PT prompts significantly shape the possible impacts of PT."
- Why unresolved: The authors observe that reward models have little influence on out-of-distribution preferences, suggesting that the base model's pre-training data plays a significant role. However, the specific aspects of this data that contribute to these preferences remain unclear.
- What evidence would resolve it: Analyzing the distribution and content of the base model's pre-training data, particularly in relation to the opinion-based tasks used in the study, could reveal the specific factors that influence the model's preferences. Additionally, comparing the preferences of models trained on different pre-training datasets would provide further insights.

## Limitations

- The study relies heavily on a single multilingual SFT dataset (Tülu) whose composition and quality cannot be fully verified, limiting confidence in the generalizability of multilingual performance improvements.
- The analysis of reward model influence examines correlation rather than causation, and the limited scope to a single model (Starling) prevents generalization to other alignment approaches.
- The lack of systematic dialect balance in alignment datasets makes it difficult to determine whether observed disparities stem from data bias or inherent model tendencies.

## Confidence

- High confidence: Alignment improves English dialect intent prediction across all tested varieties while simultaneously increasing performance disparities, supported by statistically significant results across multiple models.
- Medium confidence: Multilingual performance improvements, as the analysis relies on a single multilingual SFT dataset (Tülu) whose composition and quality cannot be fully verified.
- Low confidence: Reward model analysis, as the study only examines one open-source reward model (Starling) and cannot establish causal relationships between reward model training data and final model preferences.

## Next Checks

1. Conduct controlled experiments varying the proportion of multilingual data in SFT to establish minimum effective thresholds for cross-lingual transfer improvements.

2. Test whether reward model bias persists when preference-tuning data explicitly includes opinion-based examples across multiple cultural contexts.

3. Replicate dialect performance evaluation using balanced alignment datasets where US, Indian, and Nigerian English are represented equally to isolate the source of performance disparities.