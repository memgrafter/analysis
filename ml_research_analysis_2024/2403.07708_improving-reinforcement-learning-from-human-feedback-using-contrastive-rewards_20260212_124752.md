---
ver: rpa2
title: Improving Reinforcement Learning from Human Feedback Using Contrastive Rewards
arxiv_id: '2403.07708'
source_url: https://arxiv.org/abs/2403.07708
tags:
- reward
- human
- contrastive
- learning
- rewards
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of improving reward model quality
  in reinforcement learning from human feedback (RLHF), which is crucial for aligning
  large language models with human preferences. The authors propose a novel method
  called contrastive rewards that introduces a penalty term to the reward function,
  calculated using offline sampling of baseline responses.
---

# Improving Reinforcement Learning from Human Feedback Using Contrastive Rewards

## Quick Facts
- arXiv ID: 2403.07708
- Source URL: https://arxiv.org/abs/2403.07708
- Authors: Wei Shen; Xiaoying Zhang; Yuanshun Yao; Rui Zheng; Hongyi Guo; Yang Liu
- Reference count: 23
- Primary result: Contrastive rewards improves RLHF performance by approximately 20% in human evaluations across multiple tasks

## Executive Summary
This paper addresses the challenge of improving reward model quality in reinforcement learning from human feedback (RLHF), which is crucial for aligning large language models with human preferences. The authors propose a novel method called contrastive rewards that introduces a penalty term to the reward function, calculated using offline sampling of baseline responses. This approach enables the model to penalize reward uncertainty, improve robustness, encourage improvement over baselines, calibrate according to task difficulty, and reduce variance in Proximal Policy Optimization (PPO). Extensive experiments demonstrate that contrastive rewards consistently outperform strong baselines across multiple tasks, with improvements of approximately 20% in human evaluations. The method shows particular effectiveness on challenging prompts and improves benchmark performance on MT-Bench and RED-EVAL tasks.

## Method Summary
The paper proposes contrastive rewards as a two-step process to improve RLHF training. First, during offline sampling, baseline responses are generated for each prompt using the SFT model with temperature sampling, and these are scored by the reward model. Second, during RL training with PPO, the contrastive reward penalty is computed by comparing the reward model's output for the current sample against the baseline responses. The penalty term scales inversely with the reward model's uncertainty and the probability of the true reward being positive, effectively downweighting uncertain or difficult samples. This contrastive reward is then scaled to match the original reward scale and used in the PPO optimization loop to encourage improvement over baseline responses while reducing the impact of noisy reward signals.

## Key Results
- Contrastive rewards improves win rates by approximately 20% in human evaluations compared to standard RLHF
- The method shows consistent improvements across multiple tasks including summarization, safety alignment, and dialogue
- Performance gains are particularly pronounced on challenging prompts where standard RLHF struggles
- Benchmark performance on MT-Bench and RED-EVAL tasks shows significant improvements with contrastive rewards

## Why This Works (Mechanism)

### Mechanism 1: Contrastive Reward Penalizes Uncertainty
The contrastive reward term reduces the influence of samples where the reward model is uncertain by scaling rewards inversely with uncertainty. When the reward model gives inconsistent evaluations (high cx,0 and cx,1), the contrastive reward term (1 - cx,0 - cx,1) shrinks, effectively downweighting those samples during PPO optimization. This works under the assumption that reward model outputs rx,y and rx,ybase are conditionally independent given the true reward r*_x,y.

### Mechanism 2: Self-Improvement Through Comparison
The contrastive reward encourages the policy to generate responses that differ substantially from baseline responses when the baseline is suboptimal. The term Pr(rx,y ≠ rx,ybase) in the expected reward difference rewards the policy for generating responses that the reward model scores differently than the baseline. This mechanism assumes that baseline responses represent a reasonable lower bound on quality that the policy can improve upon.

### Mechanism 3: Task Difficulty Calibration
The contrastive reward automatically downweights difficult prompts where achieving high reward is inherently challenging. The term (2Pr(r*_x,y = 1) - 1) reduces the reward signal for prompts with low probability of receiving high true reward, focusing learning on more tractable instances. This assumes that the distribution of true rewards reflects task difficulty in a meaningful way.

## Foundational Learning

- **Concept: Reinforcement Learning from Human Feedback (RLHF)**
  - Why needed here: This paper builds directly on RLHF as the baseline framework being improved
  - Quick check question: What are the three main stages of the standard RLHF pipeline?

- **Concept: Reward model uncertainty and variance**
  - Why needed here: The paper's contrastive reward mechanism specifically targets reward model imperfections
  - Quick check question: How does reward model uncertainty typically manifest in RLHF training?

- **Concept: Proximal Policy Optimization (PPO)**
  - Why needed here: The contrastive reward is implemented within the PPO optimization loop
  - Quick check question: What role does the KL divergence penalty play in PPO for RLHF?

## Architecture Onboarding

- **Component map**: SFT model -> Reward model -> Contrastive rewards module -> PPO optimizer -> Offline sampling pipeline

- **Critical path**:
  1. Collect prompts for RL training
  2. Generate baseline responses using SFT model
  3. Score baseline responses with reward model
  4. During PPO: compute contrastive reward for each sample
  5. Scale contrastive reward to match original reward scale
  6. Optimize policy with PPO using contrastive rewards

- **Design tradeoffs**:
  - More offline samples → more robust baseline estimates but higher upfront cost
  - Temperature scaling during baseline generation → diversity vs. quality tradeoff
  - Reward scaling factor λ → must balance contrastive signal strength

- **Failure signatures**:
  - High variance in offline rewards → indicates insufficient sampling
  - Degraded policy performance vs. baseline → suggests reward scaling issues
  - Inconsistent GPT-4 evaluations → indicates potential reward model problems

- **First 3 experiments**:
  1. Run with k=1 offline sample to establish baseline performance
  2. Increase to k=3 samples to test improvement curve
  3. Test on prompts with known difficulty to verify calibration effect

## Open Questions the Paper Calls Out

The paper doesn't explicitly call out open questions, but based on the analysis there are several important questions:

### Open Question 1
How does the number of offline samples in contrastive rewards scale with respect to prompt complexity and dataset size? Is there a theoretical bound on the required number of samples for optimal performance?

### Open Question 2
Can the contrastive rewards framework be effectively extended to iterative RLHF processes where the contrastive samples are generated by the current policy rather than the base SFT model?

### Open Question 3
How does the performance of contrastive rewards compare to other uncertainty-aware reward modeling techniques, such as ensemble methods or Bayesian approaches?

## Limitations
- The theoretical analysis relies on the assumption of conditional independence between reward model outputs, which may not hold in practice
- The method requires offline sampling of baseline responses, adding computational overhead to the training process
- Experimental results are based on a restricted subset of prompts (1,000 from each dataset), limiting generalizability

## Confidence

**Confidence: Low** - The paper's theoretical analysis relies on several assumptions that may not hold in practice. The key assumption of conditional independence between reward model outputs for the current sample and baseline responses (rx,y ⊥ rx,ybase | r*_x,y) is not empirically validated. This assumption is critical for the contrastive reward's uncertainty penalization mechanism to work as intended.

**Confidence: Medium** - The effectiveness of the contrastive reward mechanism depends heavily on the quality and representativeness of baseline responses. The paper uses k=3 offline samples per prompt, but the sensitivity analysis for this hyperparameter is limited. Insufficient or unrepresentative baseline samples could lead to ineffective or even harmful contrastive penalties.

**Confidence: High** - The experimental setup uses a restricted subset of prompts (1,000 from each of three datasets) for RL training. This limitation is acknowledged but raises questions about the method's performance when scaled to larger, more diverse datasets. The human evaluation methodology (35 examples per prompt, 6-7 annotators) provides reasonable statistical power, though the exact inter-annotator agreement is not reported.

## Next Checks

1. **Correlation Analysis**: Measure the empirical correlation between reward model outputs for baseline responses and current samples to validate the conditional independence assumption. High correlation would indicate the uncertainty penalization mechanism may not function as theorized.

2. **Hyperparameter Sensitivity**: Systematically vary the number of offline samples (k) and reward scaling factor (λ) to identify optimal ranges and test robustness. This would reveal whether the claimed benefits are consistent across different configurations.

3. **Dataset Scaling Test**: Evaluate the method on a larger, more diverse set of prompts (e.g., 10,000+ examples) to assess whether the performance improvements scale with dataset size and diversity, addressing concerns about the restricted experimental scope.