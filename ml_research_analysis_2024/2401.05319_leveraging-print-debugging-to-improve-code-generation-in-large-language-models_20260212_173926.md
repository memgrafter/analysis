---
ver: rpa2
title: Leveraging Print Debugging to Improve Code Generation in Large Language Models
arxiv_id: '2401.05319'
source_url: https://arxiv.org/abs/2401.05319
tags:
- common
- code
- squares
- elements
- array
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proposes a method to improve code generation in large
  language models by leveraging print debugging. The key idea is to guide LLMs to
  insert print statements into buggy code, execute it to obtain logs, and then analyze
  the logs along with test cases to identify and fix bugs.
---

# Leveraging Print Debugging to Improve Code Generation in Large Language Models

## Quick Facts
- arXiv ID: 2401.05319
- Source URL: https://arxiv.org/abs/2401.05319
- Reference count: 40
- Key outcome: Print debugging improves code generation performance by 1.5% on easy and 17.9% on medium-level Leetcode problems compared to rubber duck debugging

## Executive Summary
This paper proposes a method to enhance code generation in large language models by leveraging print debugging techniques. The approach guides LLMs to insert print statements into buggy code, execute it to capture logs, and then analyze these logs alongside test cases to identify and fix bugs. Evaluated on Leetcode problems using GPT-4, the method shows significant improvements over baseline debugging approaches, particularly for medium-level problems with complex data structures and algorithms. The iterative debugging process demonstrates continuous performance improvement across multiple rounds, highlighting the potential of execution feedback in LLM-assisted programming.

## Method Summary
The method employs an in-context learning approach with GPT-4 to perform print debugging on buggy code. It follows a three-step process: (1) adding strategic print statements to capture variable states and execution flow, (2) executing the code to generate logs showing intermediate values, and (3) analyzing these logs alongside test case explanations to identify inconsistencies and fix bugs. The process iterates until all test cases pass or a stopping criterion is met. The approach is evaluated on a dataset of 211 Leetcode problems (132 easy, 39 medium, 40 hard) using the Leetcode online judging system for automated evaluation.

## Key Results
- Print debugging outperforms rubber duck debugging by 1.5% on easy-level Leetcode problems
- Print debugging shows a 17.9% improvement over rubber duck debugging on medium-level problems
- The method requires up to 7 rounds of iterative debugging to reach optimal performance
- On average, LLMs add 2.51 print statements per debugging session

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Print debugging provides more informative feedback than simple test case results
- Mechanism: By inserting print statements, LLMs capture variable values and execution flow, enabling detailed analysis of program behavior to identify inconsistencies
- Core assumption: LLMs can effectively reason about program execution when provided with intermediate variable values and execution traces
- Evidence anchors: [abstract] 1.5% and 17.9% improvements; [section] Print statements capture changing state crucial for understanding computation
- Break condition: If print statements don't capture relevant state changes or log analysis becomes too complex

### Mechanism 2
- Claim: Iterative debugging with print statements enables continuous performance improvement
- Mechanism: LLMs generate code, add prints, analyze logs, fix bugs, and repeat until convergence or timeout
- Core assumption: LLMs can learn from execution feedback and improve through iterative refinement
- Evidence anchors: [section] Continuous increase in performance requiring up to 7 rounds; [section] Average of 2.51 print statements added
- Break condition: If model fails to identify root causes from logs or gets stuck in local optima

### Mechanism 3
- Claim: Print debugging is particularly effective for problems with complex data structures and algorithms
- Mechanism: Tracing variable values through prints helps LLMs understand complex algorithmic behavior beyond simple input-output relationships
- Core assumption: Complex bugs manifest as subtle inconsistencies in variable states detectable through log analysis
- Evidence anchors: [abstract] Performance remains suboptimal for complex data structures; [section] Print debugging commonly used for intricate algorithms
- Break condition: If problem complexity exceeds LLM's ability to reason about logs

## Foundational Learning

- Concept: Chain-of-thought reasoning
  - Why needed here: Debugging requires step-by-step explanation of test cases and logs to identify inconsistencies
  - Quick check question: Can the LLM generate coherent explanations for both test case logic and program execution logs?

- Concept: In-context learning
  - Why needed here: The method relies on providing demonstrations and instructions within prompts to guide the debugging process
  - Quick check question: Does the LLM follow debugging procedures correctly with one-shot demonstration?

- Concept: Dynamic program analysis
  - Why needed here: Print debugging examines program behavior during execution by capturing variable values and control flow
  - Quick check question: Can the LLM identify relevant variables to print and understand how their values change during execution?

## Architecture Onboarding

- Component map: Problem description input -> Initial code generation -> Print statement insertion -> Code execution with logging -> Log and test case analysis -> Bug identification -> Code correction -> Iterative debugging loop controller

- Critical path: Code generation → Print statement insertion → Execution with logging → Log and test case analysis → Bug identification → Code correction → Repeat until success or timeout

- Design tradeoffs:
  - Number of print statements vs. log complexity (too many prints create overwhelming logs)
  - Number of debugging rounds vs. computational cost (more rounds improve accuracy but increase time)
  - Log truncation vs. completeness (truncating long logs may miss important information)
  - One-shot vs. few-shot prompting (more demonstrations may improve performance but reduce generality)

- Failure signatures:
  - Model gets stuck in infinite loops when adding print statements to iterative constructs
  - Logs exceed token limits, causing truncation and loss of critical information
  - Model fails to identify inconsistencies between test case explanations and logs
  - Debugging process fails to converge after multiple rounds

- First 3 experiments:
  1. Test the basic print debugging pipeline on a simple arithmetic problem with a known bug
  2. Compare performance of print debugging vs. rubber duck debugging on a medium-difficulty problem
  3. Measure the impact of different numbers of print statements (1, 2, 3) on debugging success rate

## Open Questions the Paper Calls Out

- Open Question 1: How does the effectiveness of print debugging vary across different LLM architectures and model sizes?
- Open Question 2: What is the optimal number and placement of print statements for effective debugging across different types of programming problems?
- Open Question 3: How does print debugging performance scale with problem complexity, and is there a complexity threshold beyond which it becomes ineffective?

## Limitations
- The method shows only modest absolute improvements (1.5% and 17.9%) over baseline approaches
- The approach relies heavily on GPT-4, raising questions about scalability to other model architectures
- The study focuses exclusively on Leetcode-style algorithmic problems, limiting generalizability to real-world software development

## Confidence
- High confidence in the basic feasibility of print debugging for code generation improvement
- Medium confidence in the claimed 1.5% and 17.9% performance gains, given the controlled experimental setup
- Medium confidence in the superiority over rubber duck debugging, though the 17.9% margin on medium problems warrants further validation

## Next Checks
1. Test the print debugging method with smaller, more practical model sizes (e.g., GPT-3.5, LLaMA variants) to assess whether performance gains hold without access to expensive models

2. Evaluate the method on actual software engineering tasks from platforms like GitHub, measuring effectiveness on practical bugs rather than algorithmic puzzles

3. Conduct ablation studies varying the number of print statements (1, 2, 3, 4+) and measure the trade-off between log informativeness and LLM's ability to process them effectively within context limits