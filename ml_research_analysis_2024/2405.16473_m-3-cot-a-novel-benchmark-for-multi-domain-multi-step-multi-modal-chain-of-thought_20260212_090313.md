---
ver: rpa2
title: 'M$^3$CoT: A Novel Benchmark for Multi-Domain Multi-step Multi-modal Chain-of-Thought'
arxiv_id: '2405.16473'
source_url: https://arxiv.org/abs/2405.16473
tags:
- reasoning
- multi-modal
- multi-step
- m3cot
- building
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces M3CoT, a new benchmark designed to evaluate
  multi-domain, multi-step, and multi-modal chain-of-thought (CoT) reasoning. The
  authors identify key limitations in existing benchmarks, such as the absence of
  visual reasoning, single-step reasoning, and missing domains like commonsense and
  mathematics.
---

# M$^3$CoT: A Novel Benchmark for Multi-Domain Multi-step Multi-modal Chain-of-Thought

## Quick Facts
- arXiv ID: 2405.16473
- Source URL: https://arxiv.org/abs/2405.16473
- Authors: Qiguang Chen; Libo Qin; Jin Zhang; Zhi Chen; Xiao Xu; Wanxiang Che
- Reference count: 29
- Primary result: Introduces M3CoT, a new benchmark evaluating multi-domain, multi-step, and multi-modal chain-of-thought reasoning with 11,459 samples across science, mathematics, and commonsense domains.

## Executive Summary
This paper introduces M3CoT, a new benchmark designed to evaluate multi-domain, multi-step, and multi-modal chain-of-thought (CoT) reasoning. The authors identify key limitations in existing benchmarks, such as the absence of visual reasoning, single-step reasoning, and missing domains like commonsense and mathematics. M3CoT addresses these issues through a rigorous dataset construction process involving automatic filtering, expert annotation, and domain augmentation. The benchmark includes 11,459 samples spanning science, mathematics, and commonsense, requiring complex multi-step reasoning across text and image modalities. Evaluation of various Vision Large Language Models (VLLMs) reveals significant performance gaps compared to human performance, highlighting the benchmark's difficulty.

## Method Summary
M3CoT is constructed through a three-stage pipeline: automatic filtering removes samples solvable without visual information, manual annotation ensures multi-step reasoning requirements, and domain augmentation adds mathematics and commonsense questions. The dataset consists of 11,459 samples requiring reasoning across text and image modalities. Evaluation involves fine-tuning various VLLMs on the training set using LoRA parameter-efficient methods, then testing with different prompting strategies including CoT, Direct, Desp-CoT, and CCoT. The framework also explores tool usage capabilities and in-context learning approaches for multi-modal reasoning tasks.

## Key Results
- M3CoT achieves significant performance gaps between current VLLMs and human performance (27.77% vs 83.33% accuracy)
- Larger VLLMs (≥13B parameters) show emergent CoT reasoning capabilities, while smaller models do not
- Fine-tuning on M3CoT significantly outperforms zero-shot prompting strategies for multi-modal reasoning tasks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The M3CoT benchmark successfully addresses the limitations of previous MCoT benchmarks by incorporating multi-domain, multi-step, and multi-modal reasoning.
- Mechanism: By systematically removing samples that can be solved without visual information, manually annotating multi-step reasoning samples, and augmenting the dataset with mathematics and commonsense domains, M3CoT creates a more challenging and comprehensive evaluation environment.
- Core assumption: The dataset construction process effectively filters out simple samples and ensures the inclusion of complex, multi-step reasoning tasks across diverse domains.
- Evidence anchors:
  - [abstract] "We identify key limitations in existing benchmarks, such as the absence of visual reasoning, single-step reasoning, and missing domains like commonsense and mathematics."
  - [section 3.1] "To address the first issue, we directly remove samples that could infer the final answer without the need for images."
  - [corpus] Weak - the corpus analysis shows related papers but doesn't directly confirm the effectiveness of the specific construction mechanisms.
- Break condition: If the filtering process fails to remove simple samples or the augmentation introduces noise rather than meaningful complexity, the benchmark would not effectively challenge current models.

### Mechanism 2
- Claim: Larger VLLMs (≥ 13B parameters) exhibit emergent Chain-of-Thought (CoT) reasoning capabilities, while smaller models do not.
- Mechanism: The increased model capacity allows for more complex reasoning processes, enabling the model to generate step-by-step rationales that improve performance on multi-modal reasoning tasks.
- Core assumption: There is a threshold in model size beyond which the ability to perform multi-step reasoning emerges as a capability.
- Evidence anchors:
  - [section 5.2] "VLLM shows CoT emergence phenomenon at the parameter level over 10 billion (≥ 13B)"
  - [section 5.2] "Zero-shot Multi-modal Chain-of-Thought only benefits larger VLLMs"
  - [corpus] Moderate - related papers discuss emergent abilities but don't specifically confirm the 13B parameter threshold for CoT reasoning.
- Break condition: If smaller models can be effectively prompted or fine-tuned to perform multi-step reasoning, the parameter threshold assumption would be invalid.

### Mechanism 3
- Claim: Fine-tuning on M3CoT significantly improves model performance compared to zero-shot prompting strategies.
- Mechanism: By training models on the M3CoT dataset, they learn the specific reasoning patterns and multi-modal interactions required for this benchmark, leading to better performance than approaches relying solely on prompting.
- Core assumption: The M3CoT dataset contains sufficient and representative examples of the reasoning patterns needed for success on the benchmark.
- Evidence anchors:
  - [section 5.4.3] "Finetuning on M3CoT can result better performance"
  - [section 5.4.3] "Finetuning on VLLMs tends to be more effective than on Traditional VLM"
  - [corpus] Moderate - related papers discuss fine-tuning benefits but don't specifically confirm the superiority of fine-tuning over prompting for multi-modal reasoning.
- Break condition: If zero-shot prompting strategies improve significantly or if the fine-tuning process overfits to the M3CoT dataset without generalizing to other tasks, the advantage of fine-tuning would diminish.

## Foundational Learning

- Concept: Multi-modal Chain-of-Thought (MCoT) reasoning
  - Why needed here: Understanding the difference between textual CoT and multi-modal CoT is crucial for appreciating the challenges M3CoT addresses.
  - Quick check question: What are the key differences between textual CoT and multi-modal CoT reasoning?

- Concept: Dataset construction and filtering
  - Why needed here: The methodology used to create M3CoT is central to its effectiveness as a benchmark.
  - Quick check question: How does the manual annotation process ensure that samples require both visual and textual information for reasoning?

- Concept: Vision Large Language Models (VLLMs) and their limitations
  - Why needed here: Understanding the capabilities and shortcomings of current VLLMs is essential for interpreting the results of the M3CoT evaluation.
  - Quick check question: What are the main challenges VLLMs face when performing multi-step, multi-modal reasoning?

## Architecture Onboarding

- Component map: Dataset construction pipeline (automatic filtering -> manual annotation -> domain augmentation) -> Evaluation models (various VLLMs with different prompting strategies) -> Analysis components (performance comparison, error analysis, exploration of different approaches)
- Critical path: The most critical path is the dataset construction process, as the quality and difficulty of the benchmark directly impact its ability to evaluate model capabilities.
- Design tradeoffs: The tradeoff between dataset size and quality (manual annotation is time-consuming but ensures high-quality samples), and the tradeoff between using larger models (better performance but higher computational cost) versus smaller models (more efficient but potentially less capable).
- Failure signatures: Poor performance on M3CoT could indicate either limitations in the benchmark design (if the tasks are too difficult or poorly constructed) or genuine shortcomings in the models' multi-modal reasoning capabilities.
- First 3 experiments:
  1. Evaluate a baseline VLLM on M3CoT using direct prompting to establish a performance baseline.
  2. Apply Chain-of-Thought prompting to the same VLLM and compare performance to identify any emergent reasoning capabilities.
  3. Fine-tune a smaller VLLM on the M3CoT training set and evaluate its performance to assess the impact of fine-tuning on multi-modal reasoning capabilities.

## Open Questions the Paper Calls Out
None

## Limitations
- Dataset construction relies on automated filtering followed by manual annotation, potentially introducing human bias in sample selection
- Evaluation focuses primarily on proprietary models alongside open-source alternatives, creating an incomplete landscape picture
- Performance gaps may partly reflect benchmark difficulty rather than purely model limitations
- Tool usage failure analysis is qualitative rather than quantitative

## Confidence

**High Confidence**: The dataset construction methodology and the resulting benchmark's multi-domain, multi-step, and multi-modal characteristics are well-documented and validated through manual annotation. The performance gaps between different model sizes and prompting strategies are clearly demonstrated with appropriate statistical backing.

**Medium Confidence**: The claim that CoT emergence occurs at the 13B parameter threshold is supported by empirical results but lacks ablation studies to rule out other factors. The superiority of fine-tuning over zero-shot prompting is demonstrated but the analysis doesn't fully explore whether this reflects genuine capability gains or overfitting to the benchmark.

**Low Confidence**: The assertion that current models fundamentally cannot perform multi-modal reasoning is overstated - the paper shows models can perform well with fine-tuning and specific prompting strategies, suggesting the limitation may be more about prompting and training data than inherent capability.

## Next Checks

1. **Cross-dataset Generalization Test**: Evaluate fine-tuned models on M3CoT against their performance on established benchmarks like ScienceQA and MMMU to determine whether improvements reflect genuine multi-modal reasoning capability or overfitting to M3CoT's specific patterns.

2. **Ablation Study on Dataset Construction**: Systematically vary the filtering and augmentation parameters used in M3CoT construction to quantify how each step contributes to benchmark difficulty and model performance, separating genuine reasoning challenges from dataset artifacts.

3. **Tool Usage Implementation Analysis**: Conduct a controlled experiment comparing different tool usage frameworks and interaction patterns to determine whether current failures reflect fundamental limitations in multi-modal reasoning or specific implementation choices in the tool-calling architecture.