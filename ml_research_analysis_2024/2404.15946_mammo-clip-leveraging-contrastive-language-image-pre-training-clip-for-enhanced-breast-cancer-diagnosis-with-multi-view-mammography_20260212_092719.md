---
ver: rpa2
title: 'Mammo-CLIP: Leveraging Contrastive Language-Image Pre-training (CLIP) for
  Enhanced Breast Cancer Diagnosis with Multi-view Mammography'
arxiv_id: '2404.15946'
source_url: https://arxiv.org/abs/2404.15946
tags:
- image
- multi-view
- mammo-clip
- clip
- mammograms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study addresses the challenge of developing multi-view mammogram-based
  computer-aided diagnosis (CAD) schemes for breast cancer detection, which has been
  difficult due to the need to effectively fuse information from multiple mammogram
  views and efficiently fine-tune large models with limited data. The researchers
  introduce Mammo-CLIP, a novel framework that adapts the Contrastive Language-Image
  Pre-training (CLIP) model for multi-view mammogram analysis.
---

# Mammo-CLIP: Leveraging Contrastive Language-Image Pre-training (CLIP) for Enhanced Breast Cancer Diagnosis with Multi-view Mammography

## Quick Facts
- arXiv ID: 2404.15946
- Source URL: https://arxiv.org/abs/2404.15946
- Reference count: 5
- Mammo-CLIP achieves AUC values of 0.841 and 0.837 on two mammography datasets, outperforming state-of-the-art methods by 20.3% and 14.3% in AUC.

## Executive Summary
Mammo-CLIP addresses the challenge of developing effective multi-view mammogram analysis systems for breast cancer detection. By adapting the CLIP model with early-stage multi-view feature fusion and parameter-efficient adapter modules, the framework achieves superior performance in distinguishing malignant from benign cases. The approach processes four mammogram views (CC and MLO views of both breasts) while limiting fine-tuning to approximately 1% of parameters, making it suitable for deployment on limited medical imaging datasets.

## Method Summary
Mammo-CLIP adapts CLIP for multi-view mammogram analysis by inserting adapter modules into both vision and text encoders. The framework processes four mammogram views (LCC, RCC, LMLO, RMLO) by first passing each through local transformer blocks, then concatenating and processing through global blocks for early cross-view fusion. Adapters enable parameter-efficient fine-tuning of only ~1% of parameters while preserving pre-trained knowledge. The model learns to align image and text embeddings in a shared space, using cosine similarity and softmax to classify cases as malignant or benign based on their similarity to positive and negative text prompts.

## Key Results
- Mammo-CLIP achieves AUC values of 0.841 and 0.837 on two independent datasets
- Outperforms state-of-the-art cross-view transformer models by 20.3% and 14.3% in AUC
- Early-stage multi-view fusion strategy captures bilateral asymmetry and ipsilateral correspondence patterns more effectively than late-stage fusion

## Why This Works (Mechanism)

### Mechanism 1
- Early-stage multi-view feature fusion within the vision encoder enables the model to capture bilateral asymmetry and ipsilateral correspondence patterns more effectively than late-stage fusion.
- By splitting transformer blocks into local (view-specific) and global (cross-view) blocks, the model processes individual views independently before fusing multi-view information at earlier abstraction levels, allowing low-level and mid-level features to interact across views.
- Core assumption: Multi-view mammogram analysis benefits from fusing complementary information at multiple abstraction levels rather than only at the final classification stage.

### Mechanism 2
- Parameter-efficient transfer learning through adapter modules enables effective fine-tuning of large CLIP models on small mammography datasets without overfitting.
- Adapter modules inserted within each transformer block of both vision and text encoders allow fine-tuning of only ~1% of parameters while keeping pre-trained weights frozen, reducing overfitting risk on limited data.
- Core assumption: Fine-tuning only a small subset of parameters while preserving pre-trained knowledge is sufficient for adapting CLIP to domain-specific tasks.

### Mechanism 3
- Co-adaptation of both image and text encoders through simultaneous adapter fine-tuning creates stronger multi-modal representations than adapting only one modality.
- By inserting adapters in both vision and text encoders and fine-tuning them concurrently, the model learns better image-text co-adaptation specific to multi-view mammogram analysis, capturing semantic relationships between visual features and textual descriptions.
- Core assumption: Multi-modal learning benefits from simultaneous adaptation of both modalities rather than sequential or single-modality adaptation.

## Foundational Learning

- Multi-view mammogram analysis and the importance of bilateral asymmetry and ipsilateral correspondence: Understanding these domain-specific patterns is crucial for developing effective CAD schemes that mimic radiologist interpretation of CC and MLO views from both breasts. Quick check: What are the two key domain-specific multi-view relationships radiologists consider when analyzing mammograms?

- Vision-Language Models (VLMs) and contrastive learning principles: CLIP's architecture and training approach form the foundation for Mammo-CLIP's ability to integrate image and text information. Quick check: How does CLIP's contrastive learning objective align image and text embeddings in a shared embedding space?

- Parameter-efficient transfer learning and adapter modules: Understanding adapter-based fine-tuning is essential for implementing Mammo-CLIP's approach to adapting large pre-trained models on small datasets. Quick check: What is the key difference between traditional fine-tuning and adapter-based fine-tuning in terms of parameter updates?

## Architecture Onboarding

- Component map: Four mammograms (LCC, RCC, LMLO, RMLO) + case-level text descriptions -> Image Encoder (Transformer blocks with adapters) -> Text Encoder (Transformer blocks with adapters) -> Cosine similarity in shared embedding space -> Probability of malignancy via softmax

- Critical path: 1. Process each mammogram through local transformer blocks, 2. Concatenate features and process through global transformer blocks, 3. Generate image embedding from final global block, 4. Generate text embeddings from text encoder, 5. Compute cosine similarities and apply softmax, 6. Calculate cross-entropy loss for training

- Design tradeoffs: Early vs. late feature fusion: Early fusion captures multi-view relationships at multiple abstraction levels but increases complexity; Adapter integration location: Internal adapters enable deeper co-adaptation but require more careful design than external adapters; Backbone choice: Larger backbones (ViT-L/14) provide better performance but require more computational resources

- Failure signatures: Poor performance on external validation indicates overfitting to training data; Disproportionate performance difference between MLO and CC views suggests view-specific feature extraction issues; Failure to improve over single-view baselines indicates ineffective multi-view fusion

- First 3 experiments: 1. Test single-view performance (CC only, MLO only) to establish baseline and view importance, 2. Compare early fusion (all global blocks) vs. late fusion (all local blocks) configurations, 3. Evaluate image-only vs. text-only vs. co-adapted adapter performance to validate multi-modal benefits

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does Mammo-CLIP's early-stage feature fusion strategy compare to late-stage fusion in terms of capturing bilateral asymmetry and ipsilateral correspondence in mammograms?
- Basis in paper: [explicit] The paper emphasizes Mammo-CLIP's use of early-stage feature fusion within CLIP's vision encoder to capture multi-view relationships and contrasts it with late-stage fusion methods used by existing CAD schemes.
- Why unresolved: While the paper demonstrates Mammo-CLIP's superior performance, a direct comparison with late-stage fusion models using the same backbone and training setup is not provided.
- What evidence would resolve it: A controlled experiment comparing Mammo-CLIP with a late-stage fusion variant of the same model architecture, using identical datasets and evaluation metrics.

### Open Question 2
- Question: Can Mammo-CLIP's performance be further improved by incorporating more detailed and complex textual descriptions of mammograms during the fine-tuning stage?
- Basis in paper: [explicit] The paper acknowledges that Mammo-CLIP currently uses relatively simple textual descriptions and suggests that incorporating more complex descriptions, such as patient history or lesion characteristics, could enhance performance.
- Why unresolved: The study used only basic textual descriptions, leaving the potential impact of richer textual information unexplored.
- What evidence would resolve it: An ablation study comparing Mammo-CLIP's performance with different levels of textual complexity, ranging from simple labels to detailed clinical reports.

### Open Question 3
- Question: How does Mammo-CLIP's performance on high-resolution mammograms compare to its performance on resized images, and what is the impact on detecting small lesions like microcalcifications?
- Basis in paper: [inferred] The paper mentions that resizing mammograms to 224x224 pixels may lose fine-grained details, and it suggests that future studies should focus on adapting VLMs to analyze high-resolution mammograms.
- Why unresolved: The study only evaluated Mammo-CLIP on resized images, and its performance on the original high-resolution images is unknown.
- What evidence would resolve it: A comparative analysis of Mammo-CLIP's performance on both high-resolution and resized mammograms, with a focus on detecting small lesions like microcalcifications.

## Limitations

- External validation dataset is relatively small (354 cases) compared to training dataset (949 cases), potentially limiting generalization assessment
- Adapter architecture details are not fully specified, particularly the exact insertion scheme within transformer blocks
- Study uses only binary classification without considering other clinically relevant outcomes like breast density or BI-RADS categories

## Confidence

**High Confidence**: The superior performance of Mammo-CLIP compared to state-of-the-art methods (20.3% and 14.3% AUC improvement) is well-supported by the experimental results and internal/external validation.

**Medium Confidence**: The effectiveness of early feature fusion and parameter-efficient fine-tuning through adapters is supported by ablation studies, but the specific mechanisms could benefit from more detailed analysis.

**Low Confidence**: The generalization capability to diverse clinical settings and patient populations beyond the two datasets used in this study has not been fully established.

## Next Checks

1. **External Validation on Larger Dataset**: Evaluate Mammo-CLIP on a larger, more diverse external dataset to confirm generalization performance and assess potential dataset-specific biases.

2. **Adapter Architecture Ablation**: Systematically vary adapter dimensions, insertion positions, and numbers to determine optimal configurations and quantify the contribution of each design choice to overall performance.

3. **Clinical Utility Assessment**: Conduct radiologist studies to evaluate whether Mammo-CLIP's outputs improve diagnostic accuracy and efficiency compared to standard CAD systems, including assessment of false positive and false negative rates in clinical practice.