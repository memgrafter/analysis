---
ver: rpa2
title: 'CLongEval: A Chinese Benchmark for Evaluating Long-Context Large Language
  Models'
arxiv_id: '2403.03514'
source_url: https://arxiv.org/abs/2403.03514
tags:
- context
- long
- arxiv
- llms
- long-context
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: CLongEval is a Chinese benchmark for evaluating long-context LLMs,
  addressing the lack of comprehensive evaluation tools for models handling extended
  contexts. It features 7 tasks and 7,267 examples, designed to assess information
  acquisition and reasoning capabilities across varying context lengths (1K-100K tokens).
---

# CLongEval: A Chinese Benchmark for Evaluating Long-Context Large Language Models

## Quick Facts
- arXiv ID: 2403.03514
- Source URL: https://arxiv.org/abs/2403.03514
- Authors: Zexuan Qiu; Jingjing Li; Shijue Huang; Xiaoqi Jiao; Wanjun Zhong; Irwin King
- Reference count: 21
- Primary result: Commercial models outperform open-source ones in long-context tasks, especially in full-context extraction, with performance degrading for middle-context content

## Executive Summary
CLongEval is a comprehensive Chinese benchmark designed to evaluate long-context large language models across varying context lengths (1K-100K tokens). The benchmark features 7 tasks with 7,267 examples, assessing both information acquisition and reasoning capabilities. It employs a three-tiered length design and combines manual annotation with automated construction to ensure quality and scale. Evaluations on 8 models reveal significant performance gaps between commercial and open-source models, particularly in tasks requiring full-text analysis, and demonstrate the "lost in the middle" phenomenon where model performance degrades for content in the middle of long contexts.

## Method Summary
The benchmark consists of 7 tasks designed around two fundamental capabilities: information acquisition and reasoning. It uses a three-tiered length design (1K-16K, 16K-50K, 50K-100K) to evaluate models across different context lengths. The dataset combines manual annotation (2,000+ examples) with automated construction for scale. Evaluation employs matching-based metrics including F1 score, ROUGE-L, accuracy, edit score, and exact match. Models are tested on all tasks with varying context lengths, and performance is analyzed based on answer position within the context.

## Key Results
- Commercial models significantly outperform open-source models, especially in full-context extraction tasks
- Open-source models struggle as context length increases, with some achieving zero accuracy in medium sets
- The "lost in the middle" phenomenon is observed, where performance decreases for content in the middle of long contexts
- Tasks requiring full-text analysis show the largest performance gaps between commercial and open-source models

## Why This Works (Mechanism)

### Mechanism 1
- Claim: CLongEval's three-tiered length design ensures evaluation coverage across the full spectrum of long-context model capabilities
- Mechanism: Stratifying datasets into small, medium, and large subsets isolates performance degradation patterns at different context lengths
- Core assumption: Performance trends in one subset represent behavior in adjacent subsets
- Evidence anchors:
  - [abstract]: "Broad applicability, accommodating to models with context windows size from 1K to 100K"
  - [section 3.1]: "we stratify the benchmark into three subsets: a small set, a medium set, and a large set"
  - [corpus]: Weak evidence - only 5 related papers mention benchmarks but none describe similar tiered length design
- Break condition: If models exhibit non-monotonic performance patterns across length subsets

### Mechanism 2
- Claim: The dual-axis evaluation framework comprehensively captures essential long-context processing capabilities
- Mechanism: Separating full-context vs. partial-context understanding, and extractive vs. abstractive reasoning pinpoints specific capability bottlenecks
- Core assumption: The two dimensions are orthogonal and together cover the full space of long-context processing requirements
- Evidence anchors:
  - [abstract]: "anchored by two fundamental capabilities: information acquisition and reasoning"
  - [section 2]: "Long-Context Information Acquisition...Long-Context Reasoning"
  - [corpus]: Moderate evidence - LongBench Pro uses task diversity but doesn't explicitly frame it around these two dimensions
- Break condition: If a critical long-context capability falls outside this 2×2 framework

### Mechanism 3
- Claim: Mixing automatically constructed labels with manually annotated data ensures both scale and quality
- Mechanism: Automated construction provides volume and diversity, while manual annotation ensures quality control for challenging tasks
- Core assumption: Manual annotation can effectively identify and correct the most problematic cases in automatically generated data
- Evidence anchors:
  - [abstract]: "High quality, with over 2,000 manually annotated question-answer pairs in addition to the automatically constructed labels"
  - [section 3.2]: "2 tasks are human-annotated, 1 task is GPT-4-annotated and 4 tasks are re-constructed from public datasets"
  - [corpus]: Weak evidence - no direct mention of mixed annotation strategies in related works
- Break condition: If manual annotation introduces systematic biases or prevents scaling

## Foundational Learning

- Concept: Context length scaling in transformer architectures
  - Why needed here: Understanding how positional embeddings and attention mechanisms behave as context grows is essential for interpreting benchmark results
  - Quick check question: What happens to attention computation complexity as context length increases in standard transformers?

- Concept: Information extraction vs. reasoning in NLP tasks
  - Why needed here: The benchmark explicitly separates these capabilities, so understanding the distinction is crucial for proper evaluation
  - Quick check question: Can you identify whether a task requires extracting information directly from text or synthesizing new conclusions?

- Concept: Benchmark construction methodologies
  - Why needed here: Knowing how to balance dataset diversity, annotation quality, and scalability is essential for benchmark development
  - Quick check question: What are the tradeoffs between fully manual, fully automatic, and hybrid annotation approaches?

## Architecture Onboarding

- Component map: Data construction → Task categorization → Model evaluation → Result analysis pipeline
- Critical path: Dataset construction (requires domain expertise) → Task alignment with evaluation framework → Automated evaluation script development → Result aggregation and analysis
- Design tradeoffs: Manual annotation quality vs. dataset scale, task diversity vs. coherence, Chinese language specificity vs. generalizability
- Failure signatures: Inconsistent annotation quality, task misalignment with framework, evaluation scripts producing unexpected results, performance gaps that don't align with architectural differences
- First 3 experiments:
  1. Run evaluation on a simple baseline model to verify all evaluation scripts work correctly
  2. Compare manual vs. automatic annotation quality on a small subset to validate the mixed approach
  3. Test the three-tiered length design by running the same model on all three subsets to verify stratification effectiveness

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does CLongEval's framework for evaluating long-context LLMs transfer effectively to other languages with different script systems (e.g., Arabic, Japanese)?
- Basis in paper: [inferred] The paper focuses on Chinese language evaluation and acknowledges limitations in transferability to other languages
- Why unresolved: The paper does not conduct cross-linguistic experiments or comparative analysis with other language benchmarks
- What evidence would resolve it: Cross-linguistic studies applying CLongEval's framework to other languages, or development of similar benchmarks for different scripts with comparative performance analysis

### Open Question 2
- Question: How does the performance degradation in "lost in the middle" phenomenon correlate with specific architectural choices in long-context LLMs (e.g., attention mechanisms, positional encoding schemes)?
- Basis in paper: [explicit] The paper observes "lost in the middle" phenomenon and notes architectural differences between commercial and open-source models
- Why unresolved: The paper does not conduct architectural ablation studies or correlate performance degradation with specific model components
- What evidence would resolve it: Controlled experiments varying attention mechanisms or positional encoding while keeping other factors constant, with performance tracking across different context positions

### Open Question 3
- Question: What is the optimal trade-off between manual annotation quality and automated data generation for constructing long-context evaluation benchmarks?
- Basis in paper: [explicit] The paper uses both manual annotation (2,000+ examples) and automated generation methods for different tasks
- Why unresolved: The paper does not systematically compare the quality, scalability, or cost-effectiveness of different annotation approaches
- What evidence would resolve it: Comparative studies measuring inter-annotator agreement, annotation speed, and downstream model performance differences between manually and automatically generated examples

## Limitations
- The benchmark's 7,267 examples may be insufficient to capture full diversity of real-world long-context scenarios
- Heavy reliance on automatic dataset construction for 5 out of 7 tasks raises concerns about potential annotation artifacts
- The 2×2 categorization framework may not capture all critical long-context capabilities (e.g., temporal reasoning, complex causal inference)

## Confidence

**High Confidence**: Commercial models outperform open-source models in long-context tasks - Supported by direct empirical comparisons across 8 models on standardized tasks with clear metrics.

**Medium Confidence**: The "lost in the middle" phenomenon - Observed in results but could be influenced by task-specific prompt engineering or evaluation methodology.

**Low Confidence**: The three-tiered length design effectively captures capability thresholds - The paper doesn't provide evidence that the boundaries correspond to actual architectural inflection points.

## Next Checks
1. Test whether the commercial vs. open-source performance gap observed in Chinese extends to English long-context tasks, controlling for model architecture differences
2. Compare model performance on manually annotated vs. automatically constructed subsets within the same task to quantify the impact of annotation quality
3. Analyze attention weight patterns in models that exhibit "lost in the middle" behavior to determine whether this reflects positional encoding limitations or other architectural factors