---
ver: rpa2
title: Evaluating and Safeguarding the Adversarial Robustness of Retrieval-Based In-Context
  Learning
arxiv_id: '2405.15984'
source_url: https://arxiv.org/abs/2405.15984
tags:
- attack
- 'false'
- 'true'
- attacks
- question
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper systematically evaluates the robustness of in-context
  learning methods to adversarial perturbations. It shows that retrieval-augmented
  ICL methods are more robust than vanilla ICL to test-sample attacks but less robust
  to demonstration attacks, because they over-rely on the retrieved demonstrations.
---

# Evaluating and Safeguarding the Adversarial Robustness of Retrieval-Based In-Context Learning

## Quick Facts
- arXiv ID: 2405.15984
- Source URL: https://arxiv.org/abs/2405.15984
- Authors: Simon Yu; Jie He; Pasquale Minervini; Jeff Z. Pan
- Reference count: 40
- One-line primary result: Retrieval-augmented ICL improves robustness against test-sample attacks but increases vulnerability to demonstration attacks; DARD defense reduces ASR by 15%.

## Executive Summary
This paper systematically evaluates the adversarial robustness of in-context learning (ICL) methods, particularly focusing on retrieval-augmented approaches. The authors demonstrate that while retrieval-based ICL methods enhance robustness against test-sample attacks by providing semantically related demonstrations, they also exhibit increased vulnerability to demonstration attacks due to over-reliance on retrieved examples. To address these vulnerabilities, the paper introduces DARD (Demonstration Augmentation Retrieval Defences), a training-free defense method that augments the retrieval pool with adversarially perturbed examples. DARD achieves a 15% reduction in attack success rate while maintaining clean accuracy.

## Method Summary
The paper evaluates various ICL methods (vanilla, kNN-ICL, and retrieval-based ICL with BM25, SBERT, and Instructor retrievers) against multiple adversarial attacks (TextBugger, TextFooler, BERT-Attack, AdvICL, Swap-Labels, and Irrelevant Context). The DARD defense method works by adversarially perturbing training examples and reintegrating these perturbed examples into the retrieval pool during inference, effectively performing implicit adversarial training without modifying the model's parameters. The evaluation uses six text classification datasets (SST2, RTE, CR, MR, MNLI-mm, TREC) and measures both clean accuracy and attack success rate.

## Key Results
- Retrieval-augmented ICL methods reduce attack success rate by 4.87% against test-sample attacks compared to vanilla ICL
- Retrieval-based methods show 1-6% increase in ASR for demonstration attacks due to over-reliance on retrieved demonstrations
- DARD achieves 15% reduction in ASR over baseline defenses while maintaining clean accuracy
- DARD with SBERT retriever outperforms adversarially trained models by 2% reduction in ASR

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Retrieval-based ICL methods improve robustness against test-sample attacks by providing semantically related demonstrations that enhance contextual knowledge.
- Mechanism: When adversarial perturbations are applied to test samples, the model's parametric knowledge can be exploited. Retrieval-based ICL mitigates this by retrieving demonstrations semantically similar to the test instance, thereby reducing noise in the LLM's prediction.
- Core assumption: The retrieved demonstrations are sufficiently relevant to the test instance to provide meaningful context that counteracts the adversarial perturbations.
- Evidence anchors:
  - [abstract]: "retrieval-augmented models can enhance robustness against test sample attacks, outperforming vanilla ICL with a 4.87% reduction in Attack Success Rate (ASR)"
  - [section 4.2]: "all three R-ICL methods outperform vanilla ICL and kNN-ICL among all Text Sample Attacks (TB, TF and BA), with a drop in ASR by 4.87% and 2.47%, respectively."

### Mechanism 2
- Claim: Retrieval-based ICL methods are more vulnerable to demonstration attacks due to over-reliance on the retrieved demonstrations.
- Mechanism: When demonstrations are adversarially perturbed, the retrieval-based ICL models place too much confidence in these demonstrations, leading to a decrease in robustness. This is because the models are designed to leverage the retrieved examples, and perturbations in these examples can significantly impact the model's predictions.
- Core assumption: The retrieval-based ICL models are overly reliant on the retrieved demonstrations, and perturbations in these demonstrations can significantly impact the model's predictions.
- Evidence anchors:
  - [abstract]: "they exhibit overconfidence in the demonstrations, leading to a 2% increase in ASR for demonstration attacks"
  - [section 4.2]: "we see an increase in ASR for Retrieval-based ICL when demonstrations are under attack, demonstrating a 1%-6% increase in ASR compared with vanilla ICL"

### Mechanism 3
- Claim: DARD improves robustness by augmenting the retrieval pool with adversarially perturbed examples, effectively performing implicit adversarial training.
- Mechanism: DARD works by adversarially perturbing the training examples and reintegrating these perturbed examples into the training demonstration for retrieval during the inference stage. This approach provides the model with exposure to adversarial examples without the computational cost of explicit adversarial training.
- Core assumption: Reintegrating adversarially perturbed examples into the retrieval pool during inference can improve the model's robustness to adversarial attacks.
- Evidence anchors:
  - [abstract]: "DARD yields improvements in performance and robustness, achieving a 15% reduction in ASR over the baselines"
  - [section 5.2]: "our methods with SBERT outperform [adversarially trained models] by 2% reduction in ASR; both BM25 and SBERT variants of DARD have significant improvement on robustness against No Defence baseline"

## Foundational Learning

- Concept: In-Context Learning (ICL)
  - Why needed here: ICL is the fundamental method being evaluated for robustness. Understanding its mechanics is crucial for grasping how retrieval-based methods and DARD work.
  - Quick check question: How does ICL differ from traditional fine-tuning, and what are its main advantages and disadvantages?

- Concept: Adversarial Attacks
  - Why needed here: The paper evaluates the robustness of ICL methods against various adversarial attacks. Understanding the types of attacks and their mechanisms is essential for interpreting the results.
  - Quick check question: What are the different types of adversarial attacks discussed in the paper, and how do they target ICL methods?

- Concept: Retrieval-Augmented Methods
  - Why needed here: Retrieval-augmented ICL methods are a key focus of the paper. Understanding how these methods work and their advantages over vanilla ICL is crucial for understanding the results.
  - Quick check question: How do retrieval-augmented ICL methods improve upon vanilla ICL, and what are the potential vulnerabilities introduced by these methods?

## Architecture Onboarding

- Component map: ICL Methods (Vanilla ICL, kNN-ICL, R-ICL) -> Attack Methods (Test Sample Attacks, Demonstration Attacks, Datastore Attacks) -> Defense Method (DARD)
- Critical path:
  1. Implement ICL methods with different retrievers
  2. Implement adversarial attack methods
  3. Evaluate the robustness of ICL methods against the attacks
  4. Implement DARD defense method
  5. Evaluate the effectiveness of DARD in improving robustness
- Design tradeoffs:
  - Retrieval-based methods offer improved performance but introduce new vulnerabilities to demonstration attacks
  - DARD provides a training-free defense but may increase computational overhead during inference
  - Different retrievers (BM25, SBERT, Instructor) have varying performance and robustness characteristics
- Failure signatures:
  - Significant drop in accuracy under adversarial attacks
  - Over-reliance on retrieved demonstrations leading to increased vulnerability to demonstration attacks
  - Ineffective defense by DARD due to non-representative adversarial examples
- First 3 experiments:
  1. Evaluate the robustness of vanilla ICL against test sample attacks
  2. Compare the robustness of retrieval-based ICL methods (RBM25-ICL, RSBERT-ICL, RInstructor-ICL) against demonstration attacks
  3. Assess the effectiveness of DARD in improving the robustness of retrieval-based ICL methods

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do retrieval-augmented ICL methods perform against adversarial attacks on larger models beyond LLaMA-2-70B and Mixtral-8×7B, and what is the upper bound of their robustness improvements?
- Basis in paper: [inferred] The paper notes that larger models tend to be more robust but does not test beyond LLaMA-2-70B or Mixtral-8×7B due to computational constraints. It also does not establish a clear upper bound for robustness improvements.
- Why unresolved: Computational limitations prevented testing on even larger models, and the paper does not provide a theoretical or empirical upper bound for robustness improvements.
- What evidence would resolve it: Systematic testing of retrieval-augmented ICL methods on models larger than LLaMA-2-70B and Mixtral-8×7B, coupled with analysis of robustness trends as model size increases.

### Open Question 2
- Question: What specific mechanisms in Mixture of Experts (MoE) models lead to their vulnerability to adversarial attacks, and how can these be mitigated?
- Basis in paper: [explicit] The paper observes that MoE variants of Mistral-7B-Instruct exhibit similar or higher ASR compared to their dense versions, deviating from findings in vision models.
- Why unresolved: The paper identifies the vulnerability but does not investigate the underlying mechanisms or propose specific mitigation strategies.
- What evidence would resolve it: Detailed analysis of how perturbations affect routing decisions in MoE models and experiments testing targeted defenses against these vulnerabilities.

### Open Question 3
- Question: How effective are retrieval-augmented ICL methods against black-box adversarial attacks, and how does their performance compare to white-box attacks?
- Basis in paper: [inferred] The paper focuses on white-box attacks and transferability between models but does not specifically evaluate black-box attack scenarios.
- Why unresolved: The paper's focus on white-box attacks and transferability does not address the unique challenges posed by black-box adversarial scenarios.
- What evidence would resolve it: Comprehensive evaluation of retrieval-augmented ICL methods under black-box attack conditions, including comparison with white-box attack performance.

## Limitations

- Evaluation relies on a fixed set of attack methods and datasets, which may not capture the full spectrum of adversarial scenarios
- Computational overhead of DARD during inference is not quantified, which could be a practical limitation for deployment
- Results are demonstrated on LLaMA-2-7B and similar models, limiting generalizability to larger or differently architected LLMs

## Confidence

- **High Confidence**: The core finding that retrieval-based ICL improves robustness against test-sample attacks but increases vulnerability to demonstration attacks is well-supported by consistent results across multiple datasets and attack types.
- **Medium Confidence**: The effectiveness of DARD in reducing ASR (15% improvement) is demonstrated, but the results depend on specific attack implementations and may vary with different adversarial strategies.
- **Low Confidence**: The claim that DARD performs implicit adversarial training is somewhat speculative, as the mechanism relies on augmenting the retrieval pool rather than modifying the model's internal parameters.

## Next Checks

1. **Generalization Test**: Evaluate DARD's effectiveness against adaptive attacks that specifically target the augmented retrieval pool (e.g., by crafting adversarial examples that evade retrieval).
2. **Scalability Analysis**: Measure the computational overhead of DARD during inference and assess its impact on real-time applications.
3. **Cross-Model Validation**: Test DARD's performance on larger or differently architected LLMs (e.g., GPT-4, Claude) to ensure the defense generalizes beyond LLaMA-2-7B.