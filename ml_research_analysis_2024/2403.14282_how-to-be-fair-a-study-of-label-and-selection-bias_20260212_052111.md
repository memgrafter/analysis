---
ver: rpa2
title: How to be fair? A study of label and selection bias
arxiv_id: '2403.14282'
source_url: https://arxiv.org/abs/2403.14282
tags:
- bias
- fair
- fairness
- label
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a theoretical framework for understanding how
  different types of bias in data (label bias and selection bias) interact with fairness
  interventions. The key idea is to assume a "fair world" distribution and analyze
  how bias introduction processes transform this into the observed biased distribution.
---

# How to be fair? A study of label and selection bias

## Quick Facts
- arXiv ID: 2403.14282
- Source URL: https://arxiv.org/abs/2403.14282
- Reference count: 30
- One-line primary result: A theoretical framework showing how different bias types affect fairness interventions, with mathematical conditions for identifying bias types and guidance on choosing appropriate fairness measures.

## Executive Summary
This paper presents a theoretical framework for understanding how label bias and selection bias in data interact with fairness interventions in machine learning. The authors assume a "fair world" distribution and analyze how different bias introduction processes transform this into the observed biased distribution. They derive mathematical conditions that must hold for a biased distribution to have been generated from a fair world under specific bias types. The framework explains why standard fairness measures like demographic parity difference may not lead to the fairest possible distribution when selection bias is present, providing guidance for practitioners on choosing appropriate interventions based on their assumptions about data bias.

## Method Summary
The paper proposes a theoretical framework that assumes a "fair world" distribution P(y,x,a) and analyzes how bias introduction processes transform this into the observed biased distribution PD(y,x,a). The framework considers two types of bias: label bias (where labels are corrupted by a latent bias variable C) and selection bias (where data points are included or excluded based on a latent variable K). The authors derive mathematical conditions that must hold for PD to have been generated from a fair world under specific bias types and fairness worldviews (statistical parity or WAE). They show that minimizing standard fairness measures doesn't necessarily lead to the fairest distribution when selection bias is present, and provide guidance on choosing appropriate fairness interventions based on assumptions about the type of bias in the data.

## Key Results
- Under label bias, the observed probability PD(y1|x,a) is a linear combination of the fair probability P(y1|x,a) and the opposite label probability P(y0|x,a), weighted by bias parameters.
- Under selection bias, the observed probability maintains the same ratio structure as the fair probability, scaled by a factor δa = P(k1|y1,a)/P(k1|y0,a).
- Minimizing demographic parity difference does not necessarily lead to the fairest possible distribution when selection bias is present, as the demographic parity difference of the recovered fair distribution does not converge to zero.
- The framework provides mathematical conditions that must hold for a biased distribution to have been generated from a fair world under specific bias types, allowing practitioners to identify the type of bias in their data.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Label bias affects the observed distribution linearly, allowing the fair distribution to be recovered through algebraic manipulation.
- Mechanism: When label bias is introduced, the observed probability PD(y1|x,a) becomes a linear combination of the fair probability P(y1|x,a) and the opposite label probability P(y0|x,a), weighted by the bias parameters P(c0|y1,a) and P(c1|y0,a).
- Core assumption: The bias process only affects the label, not the features X or sensitive attribute A, and the latent bias variable C is independent of X given A and Y.
- Evidence anchors:
  - [section] Theorem 5.1 explicitly derives this linear relationship: PD(y1|x,a) = P(c0|y1,a)P(y1|x,a) + P(c1|y0,a)P(y0|x,a).
  - [abstract] The paper states they "derive mathematical conditions that must hold for a biased distribution to have been generated from a fair world under specific bias types."
- Break condition: If the bias process also affects features X or the independence assumption between C and X is violated, the linear relationship no longer holds.

### Mechanism 2
- Claim: Selection bias changes the conditional probabilities in a ratio form, but does not affect the feature distribution P(x,a).
- Mechanism: When selection bias is introduced, the observed probability PD(y1|x,a) maintains the same ratio structure as the fair probability P(y1|x,a)/P(y0|x,a), scaled by a factor δa = P(k1|y1,a)/P(k1|y0,a).
- Core assumption: The selection variable K is independent of X given A and Y, meaning the decision to keep a data point depends only on its label and sensitive attribute.
- Evidence anchors:
  - [section] Theorem 5.2 shows: PD(y1|x,a)/PD(y0|x,a) = δa · P(y1|x,a)/P(y0|x,a).
  - [abstract] The authors "assume a 'fair world' distribution and analyze how bias introduction processes transform this into the observed biased distribution."
- Break condition: If the selection process depends on features X beyond what's explained by A and Y, the ratio relationship breaks down.

### Mechanism 3
- Claim: Minimizing standard fairness measures like demographic parity difference does not necessarily lead to the fairest possible distribution when selection bias is present.
- Mechanism: Under selection bias, even if the fair world satisfies statistical parity, the demographic parity difference of the recovered fair distribution does not converge to zero because the selection bias affects different groups differently.
- Core assumption: The selection bias affects different sensitive groups differently (P(k1|y1,a0) ≤ P(k1|y0,a0) and P(k1|y1,a1) ≥ P(k1|y0,a1)).
- Evidence anchors:
  - [abstract] The paper shows "that minimizing standard fairness measures like demographic parity difference does not necessarily lead to the fairest possible distribution when selection bias is present."
  - [section] Theorem 6.5 proves this result formally: "the calculated disparity of the fair P(y1|x,a) does not converge to zero unless Y |= X | A = a for all a ∈ A where the inequality is strict."
- Break condition: If selection bias affects both groups in exactly the same way (same ratio δa for both groups), the demographic parity difference might converge to zero.

## Foundational Learning

- Concept: Conditional probability manipulation and Bayes' theorem
  - Why needed here: The entire theoretical framework relies on manipulating conditional probabilities to understand how bias processes transform the fair distribution into the observed biased distribution.
  - Quick check question: Can you derive the relationship between PD(y1|x,a) and P(y1|x,a) under label bias using Bayes' theorem and the latent bias variable C?

- Concept: Linear algebra and systems of equations
  - Why needed here: The paper uses linear relationships to characterize how label bias transforms probabilities, and solving systems of equations to find the set of possible fair distributions that could have generated the observed biased distribution.
  - Quick check question: Given the linear system from Theorem 6.6, can you solve for the relationship between PD(y1|x,a0) and PD(y1|x,a1) under the WAE worldview?

- Concept: Jensen's inequality and convexity
  - Why needed here: The paper uses Jensen's inequality to prove that under selection bias, the expected value of the conditional probability Z P(y1|x,a)PD(x|a)dx is bounded relative to P(y1|a), which is crucial for understanding how selection bias affects demographic parity.
  - Quick check question: Can you explain why Jensen's inequality applies to the function fδ(t) = t/(δ + (1-δ)t) when proving Theorem 6.4?

## Architecture Onboarding

- Component map: Fair world distribution P(y,x,a) -> Bias introduction (label bias with latent C or selection bias with latent K) -> Observed biased distribution PD(y,x,a) -> Fairness intervention (statistical parity or WAE) -> Recovered fair distribution

- Critical path: 1. Assume a fairness worldview (statistical parity or WAE) 2. Assume a bias type (label or selection) 3. Derive mathematical conditions that PD must satisfy 4. Check if the observed PD satisfies these conditions 5. Determine if minimizing the corresponding fairness measure on PD recovers the fair distribution

- Design tradeoffs:
  - Generality vs. tractability: Assuming bias is independent of features X makes the math tractable but may not capture all real-world bias scenarios
  - Multiple solutions: For many bias-worldview combinations, there are multiple possible fair distributions that could generate the observed PD, requiring additional assumptions to select the correct one
  - Linear vs. non-linear relationships: Label bias creates linear relationships that are easier to work with, while selection bias creates ratio relationships that are more complex

- Failure signatures:
  - Empty intersection of conditions: If the set of conditions derived from the fairness worldview and bias type has empty intersection with the observed PD, the assumptions are inconsistent
  - Non-convergence of fairness measures: If minimizing demographic parity difference doesn't make it approach zero even after bias correction, selection bias may be the issue
  - Non-linear relationships that don't match theoretical predictions: If the observed relationships between conditional probabilities don't match the theoretical predictions, the bias model may be incorrect

- First 3 experiments:
  1. Generate synthetic data with known fair distribution satisfying statistical parity, introduce label bias with known parameters, verify that the theoretical conditions from Theorem 6.1 are satisfied and that the linear relationship holds.
  2. Generate synthetic data with known fair distribution satisfying WAE, introduce selection bias with known parameters, verify that the ratio relationship from Theorem 6.8 holds and that demographic parity difference doesn't converge to zero.
  3. Take real-world dataset suspected to have label bias, check if it satisfies the conditions from Theorem 6.1, and if so, attempt to recover the fair distribution and verify that demographic parity difference approaches zero.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Under what specific conditions does minimizing demographic parity difference lead to the fairest possible distribution?
- Basis in paper: [explicit] The paper states "we also show that there are situations where minimizing fairness measures does not result in the fairest possible distribution."
- Why unresolved: The paper provides theoretical conditions under which DPD minimization fails but doesn't fully characterize all scenarios where it succeeds or fails.
- What evidence would resolve it: Empirical studies testing DPD minimization across diverse bias types and distributions, or a complete mathematical characterization of when DPD minimization is optimal.

### Open Question 2
- Question: How can we effectively detect and quantify the type of bias (label vs. selection) present in real-world datasets?
- Basis in paper: [inferred] The theoretical framework requires knowing the type of bias to choose appropriate interventions, but the paper acknowledges this is challenging in practice.
- Why unresolved: The paper provides conditions to check for bias types theoretically but doesn't offer practical detection methods for real-world data.
- What evidence would resolve it: Development and validation of statistical tests or machine learning methods that can identify bias type from observational data.

### Open Question 3
- Question: What is the relationship between the intensity of bias injection and the effectiveness of different fairness interventions?
- Basis in paper: [explicit] The experiments show different interventions perform differently under varying levels of label and selection bias.
- Why unresolved: While the paper demonstrates that intervention effectiveness varies with bias intensity, it doesn't provide a comprehensive model of this relationship.
- What evidence would resolve it: Systematic experiments mapping bias intensity to intervention performance across multiple datasets and intervention types.

## Limitations

- Real-world applicability: The theoretical framework assumes idealized bias models where the bias process is independent of features X given Y and A, which may not hold in practice.
- Multiple fair solutions: For many bias-worldview combinations, the framework identifies multiple possible fair distributions, requiring additional assumptions to select the correct one.
- Limited bias types: The analysis focuses on label bias and selection bias, but other important bias types are not considered.

## Confidence

- High confidence: The theoretical derivations connecting fair and biased distributions under the specified bias models (Theorems 5.1, 5.2, 6.1-6.8). These are mathematically rigorous and the proofs appear sound.
- Medium confidence: The claim that minimizing standard fairness measures doesn't necessarily lead to the fairest distribution under selection bias. While the mathematical proof is solid, the practical implications depend on how well the selection bias model matches reality.
- Medium confidence: The experimental results on synthetic data showing the framework's predictions. The experiments appear well-designed but are limited to synthetic scenarios.

## Next Checks

1. **Empirical validation on real-world datasets**: Apply the framework to actual datasets with known bias types (e.g., COMPAS, Adult Income) to test whether the theoretical predictions match empirical observations. This would validate whether the independence assumptions hold in practice.

2. **Sensitivity analysis for feature dependence**: Systematically relax the independence assumptions by introducing feature-dependent bias terms and measure how this affects the accuracy of the bias correction. This would quantify the framework's robustness to assumption violations.

3. **Cross-bias type generalization**: Test whether the framework's insights about label vs. selection bias apply to other bias types (e.g., measurement bias where the measurement process depends on sensitive attributes). This would assess the framework's broader applicability beyond the two specific bias types analyzed.