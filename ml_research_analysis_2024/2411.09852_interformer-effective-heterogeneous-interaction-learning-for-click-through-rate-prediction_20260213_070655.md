---
ver: rpa2
title: 'InterFormer: Effective Heterogeneous Interaction Learning for Click-Through
  Rate Prediction'
arxiv_id: '2411.09852'
source_url: https://arxiv.org/abs/2411.09852
tags:
- information
- sequence
- interaction
- learning
- non-sequence
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces InterFormer, a novel framework for click-through
  rate prediction that addresses two key challenges in heterogeneous information learning:
  insufficient inter-mode interaction and aggressive information aggregation. The
  model employs an interleaving learning style with bidirectional information flows
  between non-sequential and sequential data modes, using separate interaction and
  sequence architectures connected by a cross architecture for effective information
  selection.'
---

# InterFormer: Effective Heterogeneous Interaction Learning for Click-Through Rate Prediction

## Quick Facts
- arXiv ID: 2411.09852
- Source URL: https://arxiv.org/abs/2411.09852
- Reference count: 40
- Key outcome: InterFormer achieves state-of-the-art performance on three public datasets (up to 0.14% AUC improvement) and a large-scale industrial dataset (0.15% Normalized Entropy gain)

## Executive Summary
InterFormer addresses two key challenges in heterogeneous information learning for click-through rate prediction: insufficient inter-mode interaction and aggressive information aggregation. The model employs an interleaving learning style with bidirectional information flows between non-sequential and sequential data modes, using separate interaction and sequence architectures connected by a cross architecture for effective information selection. Experiments show InterFormer achieves state-of-the-art performance on three public datasets and demonstrates 24% QPS improvement when deployed at Meta Ads.

## Method Summary
InterFormer is a novel CTR prediction framework that processes heterogeneous information through three main architectural components: Interaction Arch for behavior-aware non-sequence learning, Sequence Arch for context-aware sequence modeling, and Cross Arch for information exchange between the two. The model uses bidirectional information flow between data modes, maintains separate processing architectures to avoid aggressive aggregation, and employs personalized feature gating for selective information retention. Trained with Adam optimizer and evaluated using AUC, gAUC, LogLoss, and Normalized Entropy metrics.

## Key Results
- Achieves state-of-the-art performance on three public datasets with up to 0.14% AUC improvement
- Deployed at Meta Ads with 24% QPS improvement compared to prior SOTA models
- Shows 0.15% Normalized Entropy gain on large-scale industrial dataset
- Outperforms existing methods on sequence lengths up to 1000

## Why This Works (Mechanism)

### Mechanism 1
Bidirectional information flow between non-sequence and sequence modes improves interaction learning by enabling each mode to inform the other's representation learning simultaneously. This allows user interests to be better captured when both static profile/context information and dynamic behavior sequences inform each other rather than unidirectionally.

### Mechanism 2
Separate interaction and sequence architectures with cross connections prevent aggressive information aggregation by retaining complete information in each mode until final stages. This approach preserves more useful information compared to early aggregation of heterogeneous information.

### Mechanism 3
Personalized feature gating and selective sequence summarization improve information quality by filtering irrelevant information and focusing on the most informative parts. The model uses gating mechanisms to filter irrelevant information from non-sequence embeddings and employs multiple types of sequence summarization.

## Foundational Learning

- **Feature interaction modeling**: Why needed - CTR prediction requires capturing complex relationships between user profiles, item features, and behavior sequences. Quick check - What are the three main types of feature interaction modules mentioned in the paper, and how do they differ in capturing relationships between features?
- **Sequence modeling with attention mechanisms**: Why needed - User behavior sequences contain temporal dependencies that need to be captured to understand evolving user interests. Quick check - How does the Sequence Arch in InterFormer differ from standard Transformer attention in handling non-sequence context?
- **Bidirectional information flow in neural networks**: Why needed - The model relies on information flowing in both directions between different data modes to achieve mutual benefits. Quick check - What specific architectural components enable the bidirectional flow between non-sequence and sequence information in InterFormer?

## Architecture Onboarding

- **Component map**: Interaction Arch (orange) -> Cross Arch (green) -> Sequence Arch (blue), with bidirectional flows between Interaction and Sequence Arches through multiple stacked layers
- **Critical path**: Information flows from input preprocessing through Cross Arch (for summarization), then through Interaction Arch and Sequence Arch in alternating layers, with final predictions made from summarized outputs
- **Design tradeoffs**: Trades computational complexity for better interaction learning by maintaining separate architectures and bidirectional flows versus simpler unidirectional approaches
- **Failure signatures**: Poor performance may indicate issues with bidirectional flow creating conflicting signals, aggressive aggregation despite design, or gating mechanisms filtering out too much information
- **First 3 experiments**:
  1. Compare performance with bidirectional flow enabled vs disabled to validate core mechanism
  2. Test different sequence summarization strategies (CLS only, PMA only, recent only) to understand individual contributions
  3. Evaluate impact of varying number of layers to find optimal depth for specific dataset

## Open Questions the Paper Calls Out

### Open Question 1
How does InterFormer's performance scale with even longer sequences (e.g., length 2000+) compared to existing methods? The paper evaluates sequences up to length 1000 but doesn't explore significantly longer sequences where attention mechanisms typically face computational challenges.

### Open Question 2
What is the optimal number of Interaction Arch layers versus Sequence Arch layers in InterFormer? The paper treats both arch components equally in layer stacking but doesn't explore asymmetric configurations.

### Open Question 3
How does InterFormer's bidirectional information flow compare to other fusion strategies like concatenation or addition? The paper contrasts its approach with naive early aggregation but doesn't directly compare to alternative fusion methods.

### Open Question 4
How sensitive is InterFormer to hyperparameter choices like the number of PMA tokens, CLS tokens, and recent tokens? The paper provides specific values without systematic exploration of their impact.

## Limitations
- Core claims rely heavily on proprietary industrial deployment data rather than fully reproducible benchmarks
- Public dataset improvements (0.14% AUC) are modest and may not justify architectural complexity
- Bidirectional information flow mechanism lacks direct empirical validation through isolation studies
- Computational complexity trade-offs are not fully quantified for real-world deployment scenarios

## Confidence
- **High confidence**: Architectural design and implementation details are clearly specified with mathematical formulations
- **Medium confidence**: Public benchmark results (AUC improvements of 0.14%) are verifiable but represent incremental improvements
- **Low confidence**: Industrial deployment claims (24% QPS, 0.15% NE) cannot be independently verified due to proprietary data

## Next Checks
1. **Bidirectional Flow Validation**: Conduct ablation studies comparing InterFormer with unidirectional information flow to quantify the exact contribution of the bidirectional mechanism
2. **Cross-Architecture Bottleneck Analysis**: Systematically vary the number of layers in the Cross Arch component to identify whether it becomes a computational bottleneck
3. **Deployment Cost-Benefit Analysis**: Implement a framework that quantifies the trade-off between performance improvements and computational overhead across different deployment scenarios