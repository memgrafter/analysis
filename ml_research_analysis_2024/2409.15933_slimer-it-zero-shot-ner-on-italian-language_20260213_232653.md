---
ver: rpa2
title: 'SLIMER-IT: Zero-Shot NER on Italian Language'
arxiv_id: '2409.15933'
source_url: https://arxiv.org/abs/2409.15933
tags:
- zero-shot
- italian
- entity
- slimer-it
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses zero-shot Named Entity Recognition (NER) for
  the Italian language, which has been understudied compared to English. The authors
  propose an evaluation framework for zero-shot NER that tests in-domain, out-of-domain,
  and unseen entity type scenarios.
---

# SLIMER-IT: Zero-Shot NER on Italian Language

## Quick Facts
- arXiv ID: 2409.15933
- Source URL: https://arxiv.org/abs/2409.15933
- Authors: Andrew Zamai; Leonardo Rigutini; Marco Maggini; Andrea Zugarini
- Reference count: 39
- One-line primary result: SLIMER-IT achieves up to 54.7 F1 on unseen Italian entity types using definition and guidelines in prompts

## Executive Summary
This paper addresses the under-researched problem of zero-shot Named Entity Recognition (NER) for Italian, a language with limited annotated resources compared to English. The authors propose SLIMER-IT, an instruction-tuned Italian version of SLIMER that uses enriched prompts containing entity definitions and annotation guidelines. The method is evaluated across three challenging scenarios: in-domain, out-of-domain, and unseen entity types. Results demonstrate that SLIMER-IT significantly outperforms state-of-the-art approaches, achieving up to 54.7 F1 on unseen entity types, with definition and guidelines providing up to 37 absolute F1 improvement.

## Method Summary
SLIMER-IT fine-tunes Italian LLMs using instruction-tuning with prompts that include entity definitions and annotation guidelines. The method tests five different Italian-focused models (Camoscio, LLaMA-2-chat, Mistral-Instruct, LLaMA-3-Instruct, LLaMAntino-3-ANITA) on NERMuD and Multinerd-IT datasets. Each inference call extracts one entity type at a time, using generated definitions and guidelines as reasoning scaffolds. The approach is compared against traditional token classification and generative NER baselines across in-domain, out-of-domain, and unseen entity type scenarios.

## Key Results
- Achieves up to 54.7 F1 on unseen entity types, significantly outperforming competitors
- Definition and guidelines provide up to 37 absolute F1 improvement on unseen entity types
- LLaMAntino-3-ANITA achieves best performance on 3 out of 4 datasets, especially excelling in unseen entity scenarios

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Definition and guidelines in prompts improve model performance, especially on unseen entity types
- Mechanism: The model leverages structured definitions to understand what each entity type means and guidelines to avoid common annotation errors, acting as a reasoning scaffold during generation
- Core assumption: LLMs can effectively use structured prompt components (definitions and guidelines) as reasoning aids for novel entity types
- Evidence anchors:
  - [abstract] "Results show that SLIMER-IT achieves up to 54.7 F1 on unseen entity types, significantly outperforming competitors. The use of definition and guidelines proves particularly beneficial, providing up to 37 absolute F1 improvement on unseen entity types."
  - [section] "In Table 1, we report the results, highlighting the absolute difference in performance between the model steered by D&Gs and the one not using them. Generally, definition and guidelines yield improvements in F1. In particular, the gap is contained when evaluating on in-domain data, whereas it becomes significant in OOD and even more substantial in unseen NEs."

### Mechanism 2
- Claim: Italian-specific LLMs (LLaMAntino-3) outperform general multilingual models on Italian zero-shot NER
- Mechanism: Models fine-tuned on Italian instructions have better understanding of Italian language nuances, idiomatic expressions, and cultural context
- Core assumption: Instruction tuning on target language improves zero-shot generalization on that language
- Evidence anchors:
  - [section] "However, LLaMAntino-3-ANITA reaches the best performance on 3 out of 4 datasets, with a strong gap especially in unseen NEs scenario... Interestingly enough, thanks to their better understanding capabilities, backbones specialized on Italian are particularly effective in the unseen NEs scenario."

### Mechanism 3
- Claim: SLIMER-IT's per-entity inference strategy (one entity type per call) improves focus and accuracy
- Mechanism: By querying for one entity type at a time, the model can dedicate full attention to that specific category without interference from other entity types
- Core assumption: Single-task focus during inference improves accuracy compared to multi-task approaches
- Evidence anchors:
  - [section] "The prompt is designed to extract the occurrences of one entity type per call. While this has the drawback of requiring |NE| inference calls on each input text, it allows the model to better focus on a single NE type at a time."

## Foundational Learning

- Concept: Zero-shot learning vs few-shot learning
  - Why needed here: Understanding the difference is critical for grasping why SLIMER-IT's approach works without training examples
  - Quick check question: What distinguishes zero-shot learning from few-shot learning in NER tasks?

- Concept: Instruction tuning vs traditional fine-tuning
  - Why needed here: SLIMER-IT uses instruction tuning, which is fundamentally different from standard NER fine-tuning approaches
  - Quick check question: How does instruction tuning differ from traditional fine-tuning in terms of data requirements and model behavior?

- Concept: BIO tagging scheme
  - Why needed here: Traditional NER uses BIO (Begin, Inside, Outside) tagging, while SLIMER-IT uses a different generation-based approach
  - Quick check question: What are the limitations of BIO tagging that might motivate a generation-based approach?

## Architecture Onboarding

- Component map:
  - Prompt generator (GPT-3.5-turbo-1106) → creates entity definitions and guidelines
  - SLIMER-IT model (LLM backbone) → performs entity extraction
  - Evaluation framework → measures in-domain, OOD, and unseen NE performance
  - JSON parser → extracts entities from model output

- Critical path:
  1. Generate definitions and guidelines for target entity types
  2. Construct instruction-tuning prompt with entity-specific information
  3. Fine-tune backbone LLM on training data with generated prompts
  4. Evaluate on test sets across different generalization scenarios

- Design tradeoffs:
  - Single-entity inference: Better accuracy but higher computational cost
  - Definition/guidelines: Improved performance on unseen entities but requires additional prompt engineering
  - Italian-specific models: Better language understanding but limited availability

- Failure signatures:
  - Poor performance on unseen entities → likely missing or ineffective definitions/guidelines
  - Inconsistent results across similar inputs → prompt format issues
  - Low overall F1 → backbone model inadequacy or insufficient fine-tuning

- First 3 experiments:
  1. Compare performance with and without definitions in prompts for a single entity type
  2. Test different Italian LLM backbones (Camoscio vs LLaMAntino-3) on the same entity types
  3. Evaluate OOD performance using different training domain subsets (WikiNews vs Fiction)

## Open Questions the Paper Calls Out
- How does SLIMER-IT's performance scale with larger, more diverse Italian datasets containing more entity types?
- What is the optimal balance between definition length and guideline specificity in the instruction-tuning prompt?
- How does SLIMER-IT perform on zero-shot NER for other Romance languages beyond Italian?

## Limitations
- Evaluation relies on a relatively small number of Italian NER datasets
- Per-entity inference strategy requires |NE| inference calls per input text, making it computationally expensive
- Direct comparisons with other recent zero-shot NER methods using similar prompt engineering techniques are limited

## Confidence
- High Confidence:
  - SLIMER-IT achieves significantly higher F1 scores on unseen entity types compared to traditional approaches
  - The use of definitions and guidelines in prompts consistently improves performance across evaluation scenarios
  - Italian-specific LLMs (LLaMAntino-3) outperform general multilingual models on Italian zero-shot NER tasks
- Medium Confidence:
  - The per-entity inference strategy provides better focus and accuracy, though this could benefit from more direct comparative analysis
  - The 37 absolute F1 improvement from definitions and guidelines represents a meaningful contribution, though the exact mechanisms could be further explored

## Next Checks
1. Cross-linguistic validation: Test whether the definition and guidelines mechanism that works so well for Italian zero-shot NER generalizes to other languages, particularly those with different morphological complexity or syntactic structures.
2. Cost-benefit analysis: Conduct a detailed analysis of the computational overhead of the per-entity inference strategy versus the performance gains, including potential optimizations like batching or parallel processing.
3. Robustness testing: Evaluate model performance on adversarial inputs or with entity types that have ambiguous boundaries, to better understand the limits of the definition and guidelines approach in handling edge cases.