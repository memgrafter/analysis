---
ver: rpa2
title: Black-box Uncertainty Quantification Method for LLM-as-a-Judge
arxiv_id: '2410.11594'
source_url: https://arxiv.org/abs/2410.11594
tags:
- uncertainty
- arxiv
- option
- accuracy
- high
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of quantifying uncertainty in
  LLM-as-a-Judge evaluations, where LLMs are used to assess other LLM-generated outputs.
  The proposed method, called confusion-based uncertainty, generates biased assessments
  for each possible output option and constructs a confusion matrix based on token
  probabilities to derive uncertainty labels.
---

# Black-box Uncertainty Quantification Method for LLM-as-a-Judge

## Quick Facts
- arXiv ID: 2410.11594
- Source URL: https://arxiv.org/abs/2410.11594
- Reference count: 33
- One-line primary result: Confusion-based uncertainty method achieves up to 100% accuracy on low uncertainty cases across five benchmarks

## Executive Summary
This paper addresses the challenge of quantifying uncertainty in LLM-as-a-Judge evaluations by introducing a confusion-based uncertainty method. The approach generates biased assessments for each possible output option and constructs a confusion matrix based on token probabilities to derive uncertainty labels. Experiments across five benchmarks demonstrate that low uncertainty labels correlate with higher accuracy, with some models achieving up to 100% accuracy on low uncertainty cases. The method effectively transfers across datasets and models, with larger models producing higher proportions of low uncertainty labels, suggesting it can significantly improve the reliability and consistency of LLM-as-a-Judge evaluations.

## Method Summary
The confusion-based uncertainty method works by first generating biased assessments for each possible output option in a given evaluation task. For each option, the LLM is prompted to create an assessment that assumes that option is correct. These assessments are then combined with all possible options in confusion prompts (n² total prompts for n options), and the LLM generates token probabilities for each option. A confusion matrix is constructed from these probabilities, where each row represents an option and each column represents an assessment. Uncertainty levels are derived by averaging token probabilities across rows and applying a threshold - low uncertainty occurs when exactly one option exceeds the threshold and matches the LLM's choice, while high uncertainty occurs when multiple options exceed the threshold or there's a mismatch.

## Key Results
- Low uncertainty labels correlate with higher accuracy, with some models achieving up to 100% accuracy on low uncertainty cases
- Larger models (like Llama-3-70B-Instruct) produce higher proportions of low uncertainty labels compared to smaller models
- The method effectively transfers across datasets and models, maintaining performance consistency
- Threshold tuning significantly impacts the relationship between accuracy and uncertainty, forming a parabolic effect

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Low uncertainty correlates with higher accuracy in LLM-as-a-Judge evaluations
- Mechanism: The confusion-based uncertainty method identifies cases where the LLM consistently assigns high token probabilities to one option across all biased assessments, indicating strong confidence in that choice
- Core assumption: Consistent high probability assignments across biased assessments reflect genuine model confidence rather than arbitrary selection
- Evidence anchors:
  - [abstract]: "Experiments across five benchmarks demonstrate that low uncertainty labels correlate with higher accuracy, with some models achieving up to 100% accuracy on low uncertainty cases"
  - [section]: "Our results indicate that low uncertainty ratings correlate with higher accuracy, and the method effectively transfers across datasets and models"
  - [corpus]: Weak - neighboring papers discuss uncertainty quantification but don't directly address the correlation between uncertainty labels and accuracy

### Mechanism 2
- Claim: Confusion matrices capture the relationship between biased assessments and output options
- Mechanism: By prompting the LLM with assessments biased toward each possible option and recording token probabilities for all options, the method constructs a confusion matrix that reveals which options the model consistently prefers
- Core assumption: The probability distribution across options in response to biased assessments reveals the model's underlying beliefs about correctness
- Evidence anchors:
  - [section]: "A confusion matrix is constructed from these combinations, and levels of uncertainty are derived from the confusion matrix by looking at the distribution of token probabilities for output options"
  - [section]: "Each row of the matrix, representing the probability of a particular option conditioned on each of the biased assessments, is then averaged to produce an uncertainty label"
  - [corpus]: Weak - while neighboring papers discuss confusion matrices and uncertainty quantification, none specifically describe using them in the LLM-as-a-Judge context

### Mechanism 3
- Claim: Threshold tuning optimizes the balance between uncertainty detection and accuracy
- Mechanism: By adjusting the probability threshold that distinguishes low from high uncertainty, the method can be calibrated to prioritize either more confident predictions or higher overall accuracy
- Core assumption: There exists an optimal threshold that maximizes the correlation between uncertainty labels and actual accuracy
- Evidence anchors:
  - [section]: "Our analysis reveals that threshold tuning significantly impacts the relationship between accuracy and uncertainty, forming a parabolic effect"
  - [section]: "Grid search optimization of the threshold for the Llama-3-70B-Instruct model on the Feedback Collection dataset"
  - [corpus]: Weak - neighboring papers discuss uncertainty quantification methods but don't specifically address threshold optimization in the context of LLM-as-a-Judge

## Foundational Learning

- Concept: Confusion matrices
  - Why needed here: The method relies on constructing and interpreting confusion matrices to quantify uncertainty in LLM evaluations
  - Quick check question: How would you construct a confusion matrix from the probability distributions generated by biased assessments?

- Concept: Token probability analysis
  - Why needed here: The method uses token probabilities from LLM outputs to determine uncertainty levels, requiring understanding of how LLMs generate probabilities
  - Quick check question: What does it mean when an option consistently receives high token probabilities across all biased assessments?

- Concept: Prompt engineering for bias injection
  - Why needed here: The method requires creating prompts that bias the LLM toward treating each option as correct, necessitating skill in crafting effective prompts
  - Quick check question: How would you design a prompt that convinces an LLM to justify why a specific option is correct?

## Architecture Onboarding

- Component map:
  Input -> Assessment generation module -> Confusion prompt generator -> LLM inference engine -> Confusion matrix constructor -> Threshold application module -> Output

- Critical path:
  1. Generate biased assessments for each option (n calls)
  2. Create confusion prompts for all option-assessment combinations (n² calls)
  3. Execute all prompts and collect probabilities
  4. Construct confusion matrix
  5. Apply threshold to label uncertainty
  6. Output labeled evaluations

- Design tradeoffs:
  - Model size vs. computational cost: Larger models (like Llama-3-70B-Instruct) produce better uncertainty labels but require significantly more inference calls
  - Threshold strictness vs. label quantity: Stricter thresholds produce fewer low-uncertainty labels but with higher accuracy
  - Assessment quality vs. generation time: More detailed assessments may improve uncertainty detection but increase processing time

- Failure signatures:
  - Uniform probability distributions across all options in the confusion matrix
  - Threshold values that produce no low-uncertainty labels
  - Inconsistent accuracy between low-uncertainty predictions and actual correctness
  - High variance in uncertainty labels across similar evaluation tasks

- First 3 experiments:
  1. Baseline comparison: Run LLM-as-a-Judge without uncertainty quantification on a small dataset and measure accuracy
  2. Threshold sensitivity analysis: Apply the method with varying thresholds (0.3, 0.5, 0.7) on the same dataset and compare accuracy distributions
  3. Model size comparison: Test the method on different model sizes (8B vs 70B parameters) using identical evaluation tasks and compare low-uncertainty label proportions

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the confusion-based uncertainty method be generalized to tasks beyond multiple-choice or fixed-output options?
- Basis in paper: [inferred] The paper explicitly states the method is designed for "multiple choice settings, or fixed number of output options" but acknowledges the need to investigate generalizability across diverse tasks.
- Why unresolved: The paper only demonstrates the method on discrete-choice tasks (binary classification, 1-5 ratings) and does not explore continuous or open-ended evaluation scenarios.
- What evidence would resolve it: Empirical results showing the method's effectiveness on tasks like regression outputs, free-form text evaluation, or tasks with infinite possible outputs.

### Open Question 2
- Question: What is the optimal threshold for uncertainty labeling across different model architectures and tasks?
- Basis in paper: [explicit] The paper states "Defining an optimal threshold depends on the specific requirements of the use case" and observes a "parabolic effect" where threshold tuning significantly impacts performance.
- Why unresolved: The authors used dataset-specific grid search optimization rather than identifying universal principles for threshold selection.
- What evidence would resolve it: A principled framework or analytical model that predicts optimal thresholds based on model characteristics, task properties, or evaluation context.

### Open Question 3
- Question: Can the confusion matrix be leveraged for both uncertainty quantification and improved option selection?
- Basis in paper: [explicit] The authors note "The confusion matrices contain more information than what is utilized by the current labeling approach" and propose this as a future research direction.
- Why unresolved: The current method only uses the confusion matrix to derive uncertainty labels, not to directly inform which option should be selected.
- What evidence would resolve it: Experimental comparison showing whether using the confusion matrix for direct option selection outperforms the current approach of using it solely for uncertainty labeling.

## Limitations
- The method's effectiveness relies on assumptions about LLM behavior that could break under different prompting strategies or model architectures
- The n² inference cost creates practical deployment barriers that aren't fully addressed
- Claims about generalizability to any LLM-as-a-Judge task are overgeneralized given limited experimental scope
- The method hasn't been tested on continuous or open-ended evaluation scenarios

## Confidence

- **High Confidence**: The experimental results showing correlation between low uncertainty labels and higher accuracy are well-supported by the data across five benchmarks, with clear statistical relationships demonstrated.
- **Medium Confidence**: The mechanism by which confusion matrices capture meaningful uncertainty signals is plausible but relies on assumptions about LLM behavior that could break under different prompting strategies or model architectures.
- **Low Confidence**: Claims about the method's effectiveness across "any" LLM-as-a-Judge task and its ability to "significantly improve reliability" are overgeneralized given the limited experimental scope and lack of ablation studies on prompt quality.

## Next Checks
1. **Prompt Robustness Test**: Systematically vary the wording and structure of biased assessments across 10 different prompt templates while measuring changes in uncertainty label accuracy to determine if the method is sensitive to prompt engineering choices.

2. **Cross-Domain Generalization**: Apply the method to evaluation tasks from completely different domains (e.g., code review, medical diagnosis, creative writing) with no overlap to the original five benchmarks to test transfer claims.

3. **Computation-Accuracy Tradeoff Analysis**: Conduct controlled experiments varying n from 2 to 10 options while measuring both inference cost and accuracy improvement to identify the optimal balance point for practical deployment.