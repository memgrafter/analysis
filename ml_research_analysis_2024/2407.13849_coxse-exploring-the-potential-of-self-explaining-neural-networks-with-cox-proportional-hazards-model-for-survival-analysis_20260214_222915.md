---
ver: rpa2
title: 'CoxSE: Exploring the Potential of Self-Explaining Neural Networks with Cox
  Proportional Hazards Model for Survival Analysis'
arxiv_id: '2407.13849'
source_url: https://arxiv.org/abs/2407.13849
tags:
- coxse
- dataset
- features
- survival
- coxsenam
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes CoxSE and CoxSENAM, two self-explaining neural
  network models for survival analysis that provide both accurate predictions and
  interpretable explanations. CoxSE uses the SENN structure to model locally linear
  log-hazard functions, while CoxSENAM combines SENN with the NAM structure for enhanced
  stability and consistency.
---

# CoxSE: Exploring the Potential of Self-Explaining Neural Networks with Cox Proportional Hazards Model for Survival Analysis

## Quick Facts
- arXiv ID: 2407.13849
- Source URL: https://arxiv.org/abs/2407.13849
- Reference count: 39
- Primary result: Introduces CoxSE and CoxSENAM models that achieve interpretable survival predictions with performance comparable to black-box models while maintaining stable explanations

## Executive Summary
This paper introduces CoxSE and CoxSENAM, two self-explaining neural network models for survival analysis that provide both accurate predictions and interpretable explanations. The models extend the Cox Proportional Hazards framework using the Self-Explaining Neural Network (SENN) structure to model locally-linear log-hazard functions. CoxSE uses pure SENN structure while CoxSENAM combines SENN with Neural Additive Models (NAM) for enhanced stability. The models are evaluated on synthetic and real datasets, demonstrating superior performance compared to existing interpretable survival models while maintaining interpretability through learned feature relevances.

## Method Summary
The authors propose CoxSE and CoxSENAM models that combine the Cox Proportional Hazards framework with Self-Explaining Neural Networks. These models use a locally-linear approximation of the log-hazard function, where feature relevances are learned through a neural network parameterizer. The loss function includes partial likelihood (L1), gradient distance regularization (L2) to encourage local similarity of explanations, and L1 regularization (L3) to promote feature selection. The models are evaluated on three synthetic datasets (Lin, NonLin, NonLinX) with 50,000 subjects each, and two real datasets (FLCHAIN with 7,874 subjects, SEER with 458,117 subjects), using concordance index for performance evaluation and stability metrics.

## Key Results
- CoxSE achieves comparable performance to black-box models like DeepSurv while providing interpretable explanations
- CoxSENAM exhibits the best robustness to noisy features among the compared models
- Both models provide more stable and consistent explanations than CoxNAM, particularly on datasets with feature interactions
- The hybrid CoxSENAM model successfully balances the trade-off between interaction modeling and noise robustness

## Why This Works (Mechanism)

### Mechanism 1
CoxSE achieves comparable performance to black-box models like DeepSurv while providing interpretable explanations. By modeling the log-hazard function as a locally-linear function using the SENN structure, CoxSE can capture feature interactions while maintaining interpretability through learned feature relevances. The core assumption is that the locally-linear approximation is sufficient to capture the true relationship between features and hazard.

### Mechanism 2
CoxSENAM exhibits the best robustness to noisy features among the compared models. The hybrid structure combines NAM's additive noise isolation with SENN's regularization terms, allowing better separation of informative and non-informative features. The core assumption is that additive noise isolation from NAM structure effectively separates signal from noise.

### Mechanism 3
CoxSE and CoxSENAM provide more stable and consistent explanations than CoxNAM. The regularization terms L2 and L3 in the loss function encourage local similarity of explanations and promote feature selection. The core assumption is that regularization terms effectively control explanation stability without sacrificing predictive performance.

## Foundational Learning

- Concept: Cox Proportional Hazards Model
  - Why needed here: All models in this work extend the CoxPH framework, so understanding its assumptions and formulation is crucial.
  - Quick check question: What is the relationship between the hazard function and survival function in the CoxPH model?

- Concept: Self-Explaining Neural Networks (SENN)
  - Why needed here: CoxSE is built on SENN structure, which provides the locally-linear approximation and regularization framework.
  - Quick check question: How does SENN's locally-linear approximation differ from standard neural network outputs?

- Concept: Neural Additive Models (NAM)
  - Why needed here: CoxNAM and CoxSENAM use NAM structure, which models output as additive function of single-feature neural networks.
  - Quick check question: What is the key limitation of NAM models regarding feature interactions?

## Architecture Onboarding

- Component map: Input features → Concept encoder (identity for tabular) → Parameterizer (neural network) → Local relevance weights → Aggregated hazard function → Loss function (PL + L2 + L3)
- Critical path: Feature → Relevance weight computation → Weighted sum → Hazard prediction → Partial likelihood optimization
- Design tradeoffs: SENN provides better feature interaction modeling but is less robust to noise than NAM; hybrid model attempts to balance both.
- Failure signatures: Poor performance on datasets with strong feature interactions (CoxNAM/CoxSENAM fail); instability in explanations when regularization is too weak.
- First 3 experiments:
  1. Train on Lin dataset to verify baseline performance matches CPH
  2. Train on NonLinX dataset to test interaction modeling capability
  3. Train on synthetic noisy dataset to compare robustness across models

## Open Questions the Paper Calls Out

### Open Question 1
How do CoxSE and CoxSENAM perform on datasets with high feature interaction compared to their performance on datasets with low or no feature interaction? The paper mentions that NAM-based models like CoxNAM and CoxSENAM fail to model feature interactions, leading to significantly lower performance on the NonLinX dataset which has interactions between features. This remains unresolved as the paper does not provide detailed experimental results comparing performance across datasets with varying levels of feature interaction.

### Open Question 2
What is the impact of the regularization parameters (α and β) on the stability and consistency of explanations provided by CoxSE and CoxSENAM? The paper mentions that CoxSE and CoxSENAM have two regularization terms (L2 and L3) to encourage stability and consistency of explanations, and that higher α values result in less flexible models and better stability. However, it does not provide a detailed analysis of how varying these parameters affects explanation stability across different datasets and model architectures.

### Open Question 3
How do the explanations provided by CoxSE and CoxSENAM compare to post-hoc explanation methods like SHAP when applied to black-box survival models? The paper mentions that DeepSurv is explained using SHAP, and that CoxSE and CoxSENAM demonstrate better consistency between their intrinsic explanations and SHAP explanations compared to CoxNAM. This comparison remains incomplete as the paper does not provide a direct comparison of explanation quality and interpretability between self-explaining models and SHAP explanations of black-box models.

## Limitations

- Evaluation relies heavily on synthetic datasets, raising questions about external validity
- Robustness claims based on single noisy-feature experiment with fixed noise variance
- Limited human evaluation of explanation quality and interpretability
- No statistical significance testing across multiple runs for performance metrics

## Confidence

- Performance comparison with DeepSurv: **High confidence** - Multiple datasets and clear metrics support this claim
- Explanation stability advantage over CoxNAM: **Medium confidence** - Based on synthetic experiments with limited noise patterns
- Robustness to noisy features: **Medium confidence** - Single experiment design limits generalizability
- Interpretability of learned explanations: **Low-Medium confidence** - No human evaluation of explanation quality

## Next Checks

1. Perform multiple runs of all experiments with different random seeds and report confidence intervals for C-index and stability metrics to establish statistical significance.

2. Replace synthetic noise with realistic noise patterns derived from domain knowledge (e.g., measurement errors in medical data) to validate robustness claims in more realistic scenarios.

3. Train models on one real dataset and evaluate on another to assess whether performance and explanation stability transfer across different domains.