---
ver: rpa2
title: Unsupervised multiple choices question answering via universal corpus
arxiv_id: '2402.17333'
source_url: https://arxiv.org/abs/2402.17333
tags:
- question
- answer
- answering
- questions
- unsupervised
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the unsupervised multiple-choice question
  answering (MCQA) problem, where no labeled data is available in the target domain.
  The authors propose a two-stage framework to generate synthetic MCQA data from a
  universal corpus without relying on manual annotation.
---

# Unsupervised multiple choices question answering via universal corpus
## Quick Facts
- arXiv ID: 2402.17333
- Source URL: https://arxiv.org/abs/2402.17333
- Reference count: 0
- KG-NE method achieves up to 10% higher accuracy than other unsupervised approaches on MCQA datasets

## Executive Summary
This paper introduces an unsupervised approach to multiple-choice question answering (MCQA) that eliminates the need for labeled training data in target domains. The proposed framework, KG-NE, generates synthetic MCQA datasets from a universal corpus using a two-stage process involving question generation from named entities and distractor generation using knowledge graphs. Evaluated across four MCQA datasets, the method outperforms existing unsupervised approaches while demonstrating that fine-tuning large language models with synthetic data can further improve performance.

## Method Summary
The KG-NE framework operates in two stages to create synthetic MCQA data without manual annotation. First, named entities are extracted from context passages and used with unsupervised machine translation to generate relevant questions. Second, plausible distractors are created using a hybrid approach combining named entity-based and knowledge graph-based methods. The framework is evaluated using RoBERTa-base as the backbone model across four MCQA datasets, with additional experiments fine-tuning LLaMA 2-7B using the synthetic data to demonstrate effectiveness.

## Key Results
- KG-NE achieves the highest accuracy among unsupervised MCQA methods, with improvements up to 10% over competitors
- Performance gaps of 10-20% remain compared to supervised approaches on standard benchmarks
- Fine-tuning LLaMA 2-7B with KG-NE synthetic data further improves performance, validating the approach

## Why This Works (Mechanism)
The method leverages unsupervised machine translation for question generation from named entities, allowing scalable creation of question-answer pairs without manual annotation. The hybrid distractor generation approach combines named entity-based and knowledge graph-based methods to produce plausible incorrect options that challenge models while remaining contextually relevant. By building on a universal corpus (primarily Wikipedia and ConceptNet), the framework creates diverse training samples that capture broad knowledge patterns useful for MCQA tasks.

## Foundational Learning
- **Unsupervised Machine Translation**: Used to generate questions from named entities without labeled data; needed for scalable question creation without manual annotation; quick check: verify translation quality and semantic preservation
- **Named Entity Recognition (NER)**: Extracts entities from context for question generation; needed to identify meaningful question targets; quick check: assess NER accuracy on domain-specific text
- **Knowledge Graph Utilization**: Provides structured knowledge for generating plausible distractors; needed to ensure incorrect options are contextually appropriate; quick check: evaluate distractor plausibility via human judgment
- **Synthetic Data Generation**: Creates MCQA samples from scratch; needed to bypass expensive manual annotation; quick check: measure synthetic data quality against real data distributions
- **Universal Corpus Construction**: Aggregates diverse knowledge sources (Wikipedia, ConceptNet); needed for broad coverage of potential question topics; quick check: analyze corpus diversity and domain coverage
- **RoBERTa Fine-tuning**: Adapts pretrained model to MCQA task using synthetic data; needed to learn task-specific patterns from generated samples; quick check: monitor training stability and convergence

## Architecture Onboarding
Component map: Universal Corpus -> NER Extraction -> Unsupervised MT (Question Generation) -> Knowledge Graph Integration -> Distractor Generation -> Synthetic Dataset -> RoBERTa-base Fine-tuning

Critical path: Corpus processing → Entity extraction → Question generation → Distractor creation → Model training. The quality of synthetic data depends critically on the NER accuracy, translation quality, and knowledge graph coverage.

Design tradeoffs: Using Wikipedia and ConceptNet provides broad knowledge coverage but may introduce domain bias; the hybrid distractor approach balances computational efficiency with plausibility but may miss nuanced incorrect options.

Failure signatures: Poor NER accuracy leads to irrelevant questions; translation errors propagate into malformed questions; incomplete knowledge graph coverage results in implausible distractors; insufficient corpus diversity limits generalization.

First experiments: 1) Test NER performance on target domain text to assess extraction quality, 2) Evaluate unsupervised translation outputs for semantic accuracy and fluency, 3) Measure knowledge graph coverage for domain-specific entities.

## Open Questions the Paper Calls Out
None identified in the source material.

## Limitations
- Performance remains 10-20% below supervised approaches on standard benchmarks
- Universal corpus relies heavily on Wikipedia and ConceptNet, potentially introducing domain bias
- Synthetic distractor quality is constrained by knowledge graph coverage and accuracy

## Confidence
- Methodology claims: High
- Comparative results within unsupervised setting: High
- Scalability claims: Medium (limited ablation studies across corpus sizes)
- LLaMA 2-7B fine-tuning results: Medium (single model evaluation)

## Next Checks
1. Evaluate the method on domain-specific corpora to assess generalization beyond Wikipedia-style knowledge
2. Conduct human evaluation of synthetic distractor quality to verify their plausibility compared to real multiple-choice options
3. Test the framework with different backbone models (e.g., DeBERTa, T5) to determine if performance gains are model-agnostic or specific to RoBERTa-base