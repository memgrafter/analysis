---
ver: rpa2
title: Sampling-based Safe Reinforcement Learning for Nonlinear Dynamical Systems
arxiv_id: '2403.04007'
source_url: https://arxiv.org/abs/2403.04007
tags:
- control
- safe
- learning
- safety
- policy
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a safe RL method that ensures hard safety constraints
  during both training and deployment by directly sampling from state-dependent safe
  action sets, rather than using a safety filter. The approach extends policy gradient
  theory to truncated policies, enabling convergence guarantees alongside safety.
---

# Sampling-based Safe Reinforcement Learning for Nonlinear Dynamical Systems

## Quick Facts
- arXiv ID: 2403.04007
- Source URL: https://arxiv.org/abs/2403.04007
- Reference count: 28
- Primary result: Safe RL method ensures hard safety constraints by direct sampling from state-dependent safe action sets, achieving safety during training and deployment on quadcopter obstacle avoidance and inverted pendulum tasks.

## Executive Summary
This paper introduces a sampling-based approach to safe reinforcement learning that guarantees hard safety constraints throughout training and deployment. The method directly samples from state-dependent safe action sets defined by control barrier functions, eliminating the need for a safety filter. By extending policy gradient theory to truncated policies, the approach maintains convergence guarantees while ensuring all actions are safe. Experiments demonstrate that the method achieves both high reward and perfect safety rates in challenging tasks like quadcopter obstacle avoidance, outperforming safety-filter baselines that fail when obstacles block the goal path.

## Method Summary
The method uses Beta distribution-based policies to directly sample actions from state-dependent safe action sets defined by control barrier functions (CBFs). At each state, the algorithm computes the safe control set C(x) and samples actions from the base policy until a safe action is found via rejection sampling. This truncated policy approach ensures only safe actions are ever executed while preserving the Markov property needed for convergence. The policy is trained using Proximal Policy Optimization (PPO) with a clipped objective, and hyperrectangular approximations of the safe set enable efficient sampling. The approach extends policy gradient theory to show that the discounted return is well-defined under these truncated policies and that the induced Markov chain is µ-irreducible, guaranteeing convergence to stationary points.

## Key Results
- Achieves 100% safety rate during training and deployment on quadcopter obstacle avoidance task
- Maintains high reward performance while ensuring safety, unlike safety-filter baselines that fail when obstacles block goal paths
- Provides theoretical guarantees of convergence for truncated policies while satisfying hard safety constraints

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Truncated policies directly sample from the state-dependent safe action set, eliminating the need for a safety filter and preserving convergence guarantees.
- Mechanism: By sampling directly from the truncated policy distribution πC_θ(·|x) = πθ(·|x) / πθ(C(x)|x) for u ∈ C(x), the method ensures that only safe actions are ever executed. This preserves the Markov property and allows standard policy gradient convergence theory to apply, since the truncated policies still satisfy the necessary differentiability and Lipschitz conditions.
- Core assumption: The volume of C(x) is strictly positive for all x ∈ X, and the original policy πθ has full support and is differentiable.
- Evidence anchors:
  - [abstract]: "develop a single-stage, sampling-based approach to hard constraint satisfaction that learns RL controllers enjoying classical convergence guarantees while satisfying hard safety constraints throughout training and deployment."
  - [section]: "our approach is applicable to a wide class of safety constraints including control barrier functions (CBFs) ... and reachability-type constraints"
  - [corpus]: Weak. Most related papers focus on safety filters or shielding; none explicitly show direct sampling from truncated policies preserving convergence.
- Break condition: If the volume of C(x) becomes zero for some state x, rejection sampling fails and the truncated distribution is undefined, breaking the method.

### Mechanism 2
- Claim: The Markov chain induced by the truncated policy on the safe set S is µ-irreducible, ensuring that the objective (3) is well-defined and that ergodic averaging applies.
- Mechanism: Under Assumptions 2, 3, and 4, the proof shows that for any positive-volume subset B of S, there is a strictly positive probability of entering B from any initial state. This µ-irreducibility ensures that the discounted state occupancy measure dC_θ(·) exists and that policy gradient updates converge to stationary points.
- Core assumption: The dynamics map positive-volume control sets to positive-volume state sets (Assumption 2), the policy has full support over C(x) (Assumption 3), and any safe subset is reachable in finite steps (Assumption 4).
- Evidence anchors:
  - [section]: "we show that the objective (3) is well-defined when using truncated policies, even in continuous spaces systems with deterministic dynamics."
  - [section]: "the Markov chain induced by πC_θ on S enters B with strictly positive probability."
  - [corpus]: Missing. No cited work explicitly proves µ-irreducibility for truncated policies in continuous spaces with deterministic dynamics.
- Break condition: If Assumptions 2, 3, or 4 fail—e.g., if some safe state has zero-volume reachable set—the Markov chain may not be irreducible and the objective becomes undefined.

### Mechanism 3
- Claim: CBF-constrained Beta policies enable efficient sampling from hyperrectangular safe action sets, extending Beta policies to state-dependent constraints.
- Mechanism: For a hyperrectangular safe set C(x) = [a(x), b(x)], independent Beta distributions over the unit box [0,1]^n are sampled and then linearly transformed to [a(x), b(x)]. This avoids the bias of clipping infinite-support policies and directly respects the safety constraints at each step.
- Core assumption: The safe set C(x) can be approximated by a hyperrectangle Hc(x) contained within C(x), and the Beta policy parameters map states to valid (α, β) > 0.
- Evidence anchors:
  - [abstract]: "We validate the efficacy of our approach in simulation, including safe control of a quadcopter in a challenging obstacle avoidance problem"
  - [section]: "When actions must be restricted to lie within fixed, predetermined bounds ... the common practice of simply clipping policies with infinite support ... can cause bias and performance issues."
  - [corpus]: Weak. Related papers mention Beta policies but not for state-dependent CBF constraints or hyperrectangular approximations.
- Break condition: If the hyperrectangular approximation Hc(x) poorly covers C(x), the Beta policy may sample actions outside the true safe set, violating safety.

## Foundational Learning

- Concept: Control Barrier Functions (CBFs)
  - Why needed here: CBFs provide a systematic way to define the safe control set C(x) that ensures forward invariance of a safety set S; the method relies on having access to C(x) to perform rejection sampling.
  - Quick check question: Given a nonlinear system ẋ = f(x) + g(x)u and a barrier function h(x), what condition must the control input u satisfy to guarantee safety?
- Concept: Irreducibility of Markov Chains
  - Why needed here: Proving µ-irreducibility of the Markov chain induced by the truncated policy is essential to ensure the discounted return is well-defined and policy gradients converge.
  - Quick check question: What does it mean for a Markov chain to be µ-irreducible on a set S, and why is this property important for RL convergence?
- Concept: Beta Distribution and Policy Parameterization
  - Why needed here: Beta policies provide bounded support, avoiding the bias introduced by clipping unbounded policies, and can be efficiently parameterized to respect state-dependent action constraints.
  - Quick check question: How does sampling from a Beta distribution and then transforming it to [a(x), b(x)] differ from clipping a Gaussian policy in terms of bias and safety?

## Architecture Onboarding

- Component map:
  - Environment dynamics T(x,u) → x'
  - Safety constraint set C(x) (defined via CBF)
  - Base policy πθ(u|x) (e.g., Beta network)
  - Truncation and sampling module → πC_θ(u|x)
  - PPO optimizer with clipped objective
  - Value network for advantage estimation
- Critical path:
  1. At state x, compute C(x) via CBF.
  2. Sample u ~ πθ(·|x) until u ∈ C(x) (rejection sampling).
  3. Execute u, observe reward and next state.
  4. Update policy/value networks via PPO using the sampled safe actions.
- Design tradeoffs:
  - Rejection sampling efficiency vs. complexity of C(x) geometry.
  - Hyperrectangular approximation tightness vs. computational cost of solving the maximal inner rectangle problem.
  - Beta policy flexibility vs. expressiveness compared to Gaussian policies.
- Failure signatures:
  - High rejection rate → C(x) is too narrow or πθ poorly matches C(x).
  - Poor learning → Hyperrectangle approximation Hc(x) too loose, causing the policy to explore unsafe regions.
  - Divergence → Violation of Assumptions 2, 3, or 4 (e.g., C(x) volume → 0).
- First 3 experiments:
  1. Inverted pendulum with simple linear safety constraint; compare Beta vs. clipped Gaussian safety rates.
  2. Quadcopter obstacle avoidance with ECBF; test sampling efficiency as obstacle size varies.
  3. Stress test: make C(x) very small in some states; measure rejection rate and learning slowdown.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of CBF-constrained Beta policies compare to other safe RL methods like those based on control Lyapunov functions or reachability analysis in terms of both safety and reward optimization?
- Basis in paper: [inferred] The paper primarily compares their method to a safety-filter benchmark but doesn't explore other safe RL approaches like CLFs or reachability-based methods.
- Why unresolved: The experimental section focuses on a single benchmark (safety-filter) and doesn't provide a comprehensive comparison with the broader landscape of safe RL methods.
- What evidence would resolve it: Experimental results comparing CBF-constrained Beta policies to CLF-based and reachability-based safe RL methods on similar benchmark tasks.

### Open Question 2
- Question: What is the computational overhead of sampling from the truncated Beta policy distributions compared to standard Gaussian policies, and how does this scale with the dimensionality of the action space?
- Basis in paper: [explicit] The paper mentions that Beta policies can be sampled more efficiently when the safe set has a nice structure, but doesn't provide quantitative comparisons of computational costs.
- Why unresolved: While the paper discusses the theoretical advantages of Beta policies, it doesn't empirically measure the actual computational cost of sampling from these distributions versus Gaussian policies.
- What evidence would resolve it: Benchmarking studies measuring the sampling time and computational complexity of Beta policies versus Gaussian policies across different action space dimensions.

### Open Question 3
- Question: How sensitive is the performance of CBF-constrained Beta policies to the choice of hyperparameters (e.g., learning rates, network architecture) compared to unconstrained RL methods?
- Basis in paper: [inferred] The paper presents hyperparameters used in experiments but doesn't systematically explore their sensitivity or compare it to unconstrained methods.
- Why unresolved: The experimental results show that CBF-constrained Beta policies work well with specific hyperparameters, but it's unclear how robust this performance is to hyperparameter changes compared to unconstrained methods.
- What evidence would resolve it: Sensitivity analysis experiments varying key hyperparameters and comparing the stability of performance between constrained and unconstrained methods.

## Limitations
- The method requires tractable, well-defined safe sets at every state, which may not hold for complex safety constraints
- Performance depends on the quality of hyperrectangular approximations of the true safe set, potentially leading to conservative behavior
- Theoretical guarantees rely on assumptions (positive-volume reachable sets, full support) that may fail in practice

## Confidence
- High: The theoretical framework extending policy gradient theory to truncated policies is well-grounded
- Medium: The experimental results demonstrate the method works in specific scenarios, but robustness to assumption violations is not tested
- Low: The computational overhead of rejection sampling and sensitivity to hyperparameters are not empirically characterized

## Next Checks
1. **Rejection sampling efficiency under varying obstacle sizes**: Systematically vary obstacle size in the quadcopter environment and measure how rejection sampling rates scale; identify the point where sampling becomes infeasible.
2. **Robustness to assumption violations**: Intentionally design scenarios where the volume of C(x) approaches zero or the dynamics map to zero-volume state sets; verify whether the method fails gracefully or produces undefined behavior.
3. **Comparison of hyperrectangular vs. exact safe set sampling**: Implement a method to sample exactly from the true safe set C(x) (e.g., via Hit-and-Run or other MCMC methods) and compare performance and safety rates against the hyperrectangular approximation approach.