---
ver: rpa2
title: Machine Unlearning of Pre-trained Large Language Models
arxiv_id: '2402.15159'
source_url: https://arxiv.org/abs/2402.15159
tags:
- unlearning
- data
- llms
- arxiv
- gradient
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigates machine unlearning for pre-trained large
  language models (LLMs) to address the "right to be forgotten" problem. The authors
  propose a unified framework for LLM unlearning and evaluate seven methods on three
  domains (arXiv, GitHub, books).
---

# Machine Unlearning of Pre-trained Large Language Models

## Quick Facts
- arXiv ID: 2402.15159
- Source URL: https://arxiv.org/abs/2402.15159
- Reference count: 34
- This study investigates machine unlearning for pre-trained LLMs to address "right to be forgotten" problem.

## Executive Summary
This study investigates machine unlearning for pre-trained large language models (LLMs) to address the "right to be forgotten" problem. The authors propose a unified framework for LLM unlearning and evaluate seven methods on three domains (arXiv, GitHub, books). Their approximate retraining baseline enables efficient evaluation without costly full retraining. Results show unlearning methods are over 10^5 times more efficient than retraining, with gradient ascent combined with descent on in-distribution data showing superior hyperparameter robustness.

## Method Summary
The authors propose a unified framework for machine unlearning of pre-trained LLMs, implementing seven unlearning methods including gradient ascent, fine-tuning with random labels, adversarial samples, and hybrid approaches combining descent or KL divergence on retained sets. They evaluate these methods on a pre-trained Yi-6B model using three datasets from arXiv, books, and GitHub. To address computational constraints, they develop an approximate retraining baseline using in-distribution unseen data to simulate the performance of a full retraining baseline, enabling efficient evaluation of unlearning effectiveness.

## Key Results
- Unlearning methods are over 10^5 times more efficient than full retraining while maintaining comparable effectiveness
- Gradient ascent combined with descent on in-distribution data shows superior hyperparameter robustness across learning rates and optimization steps
- Unlearning copyrighted code proves most challenging, with effectiveness highly sensitive to hyperparameter choices
- Membership Inference Attack AUC scores approach 0.5 after unlearning, though the paper notes MIA is inherently weak for LLMs

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Gradient ascent combined with gradient descent on in-distribution data improves hyperparameter robustness.
- **Mechanism:** The dual-objective optimization balances forgetting the forget set (via gradient ascent) with preserving performance on retain set (via gradient descent), stabilizing the learning trajectory across varying hyperparameters.
- **Core assumption:** In-distribution data approximates the distribution of the retain set closely enough that optimizing on it maintains general model utility while unlearning specific data.
- **Evidence anchors:**
  - [abstract] "Our results show that integrating gradient ascent with gradient descent on in-distribution data improves hyperparameter robustness."
  - [section] "Figure 2 shows that the method combining gradient ascent and descent on in-distribution data is notably tolerant to changes in learning rate and number of optimization steps."
  - [corpus] Weak evidence; corpus does not directly address robustness mechanisms.
- **Break condition:** If in-distribution data distribution diverges significantly from retain set, or if forget set contains outliers that gradient ascent amplifies excessively.

### Mechanism 2
- **Claim:** Approximate retraining via in-distribution unseen data provides a computationally efficient baseline for evaluating unlearning.
- **Mechanism:** Instead of retraining on the full retained dataset (D \ U), the method estimates the retrained model's performance on forget set by measuring the vanilla model's performance on unseen domain-specific data from the same distribution.
- **Core assumption:** The vanilla model's generalization gap between trained and unseen data from the same domain is small and consistent, allowing the unseen data performance to proxy the retrained model's behavior.
- **Evidence anchors:**
  - [abstract] "We propose an approximate retraining method using an in-distribution, unseen dataset to simulate the performance of a retraining baseline."
  - [section] "We thus hypothesize that the retrained model will exhibit consistent performance on unseen domain-specific data, albeit inferior to its performance on trained data."
  - [corpus] Weak evidence; corpus neighbors discuss unlearning but do not validate this specific approximation mechanism.
- **Break condition:** Large domain shift between forget set and approximate set, or significant overfitting to training data causing poor generalization.

### Mechanism 3
- **Claim:** Membership Inference Attack (MIA) AUC approaching 0.5 indicates successful unlearning.
- **Mechanism:** MIA attempts to distinguish whether sequences were in the training set. If unlearning is effective, the model should not differentiate between forget set and unseen data, making classification random (AUC ≈ 0.5).
- **Core assumption:** MIA is a reliable indicator of unlearning success, and its AUC score directly reflects the model's ability to "forget" the forget set.
- **Evidence anchors:**
  - [abstract] "Both the MIA AUC scores before and after unlearning are close to 0.5. This result aligns with a recent study... Despite this general difficulty, our results demonstrate a decrease in MIA AUC scores after unlearning."
  - [section] "The effectiveness of MIA was quantitatively assessed using the Area Under Curve (AUC) metric. Notably, a higher AUC indicates that the targeted sequence is still identifiable within the training set, whereas a score approaching 0.5... suggests superior unlearning effectiveness."
  - [corpus] Weak evidence; corpus neighbors mention unlearning but do not validate MIA as a metric.
- **Break condition:** If MIA is inherently weak for LLMs (as noted), AUC changes may not reliably indicate unlearning success.

## Foundational Learning

- **Concept:** Machine Unlearning Fundamentals
  - Why needed here: Understanding the difference between exact (retraining) and approximate unlearning is crucial for grasping the paper's approach and efficiency claims.
  - Quick check question: What is the key difference between exact and approximate unlearning in terms of computational cost and guarantees?

- **Concept:** Gradient-Based Optimization in LLMs
  - Why needed here: The paper adapts gradient ascent/descent methods from image classification to generative LLMs, requiring understanding of how gradients update model parameters for token prediction.
  - Quick check question: How does gradient ascent on the forget set help the model "forget" specific sequences?

- **Concept:** Membership Inference Attack (MIA)
  - Why needed here: MIA is used as an evaluation metric to assess whether unlearning successfully removed traces of the forget set from the model.
  - Quick check question: What does an MIA AUC score of 0.5 signify in the context of unlearning evaluation?

## Architecture Onboarding

- **Component map:** Data preparation -> Model loading -> Unlearning method execution -> Evaluation on forget set, retain set, downstream tasks -> MIA analysis -> Hyperparameter tuning
- **Critical path:** Data preparation → Model loading → Unlearning method execution → Evaluation on forget set, retain set, downstream tasks → MIA analysis → Hyperparameter tuning
- **Design tradeoffs:** Approximate retraining trades exactness for efficiency; gradient ascent vs. descent balance unlearning vs. utility; batch size affects stability vs. efficacy
- **Failure signatures:** High perplexity on retain set (over-unlearning); low accuracy on forget set but high MIA AUC (ineffective unlearning); unstable performance across hyperparameter changes
- **First 3 experiments:**
  1. Apply gradient ascent only on arXiv forget set, evaluate perplexity and MIA
  2. Apply gradient ascent + descent on in-distribution data on GitHub forget set, tune learning rate
  3. Apply adversarial sample unlearning on books forget set, test downstream task performance

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do second-order unlearning methods perform on LLMs when applied to adapter parameters rather than full model weights?
- Basis in paper: [explicit] The paper mentions that second-order methods involving Hessian computation are prohibitively expensive for full LLMs but suggests applying them to adapters or LoRA parameters as a potential direction.
- Why unresolved: The computational complexity of second-order methods remains too high for full LLMs, but applying them to the much smaller adapter/LoRA parameter sets has not been experimentally validated.
- What evidence would resolve it: Comparative experiments showing the privacy guarantees and utility trade-offs of second-order unlearning methods applied to adapter vs. LoRA parameters versus first-order methods on the same LLM unlearning tasks.

### Open Question 2
- Question: What is the optimal partition size K for the SISA framework when applied to LLM fine-tuning unlearning tasks?
- Basis in paper: [explicit] The paper discusses SISA-FC and SISA-A methods for fine-tuning but notes that choosing K involves a trade-off between efficiency and utility, with no clear guidance on optimal selection.
- Why unresolved: The paper acknowledges that K=1 (full retraining) maximizes utility but is inefficient, while larger K improves efficiency but may degrade utility, without providing empirical guidance on optimal K selection.
- What evidence would resolve it: Empirical studies systematically varying K across different fine-tuning dataset sizes and model architectures to identify the optimal trade-off point between unlearning efficiency and retained model utility.

### Open Question 3
- Question: How does unlearning effectiveness vary across different model scales (e.g., 7B vs 70B parameters) and architectural variants (e.g., standard transformers vs mixture-of-experts)?
- Basis in paper: [explicit] The paper acknowledges it only tested the Yi-6B model and recommends future research investigate other model sizes and architectures, noting that most LLMs don't open-source their pre-training data.
- Why unresolved: The current study is limited to a single 6B parameter model, and the authors explicitly call for research on larger models and different architectures to understand scalability of unlearning methods.
- What evidence would resolve it: Systematic experiments comparing unlearning performance across multiple model scales (7B, 13B, 70B) and architectures (standard transformers, MoE) using the same unlearning methods and evaluation metrics.

## Limitations

- The evaluation relies on approximate rather than exact retraining baselines, introducing uncertainty about the true upper bound for unlearning effectiveness
- Membership Inference Attack evaluation shows only modest improvements despite significant perplexity changes, suggesting MIA may be an unreliable metric for LLMs
- Unlearning copyrighted code presents particular challenges with effectiveness highly sensitive to hyperparameter choices

## Confidence

- **High confidence**: Claims about computational efficiency (10^5× faster than retraining) and the general framework structure are well-supported by explicit methodology descriptions and consistent experimental results across domains
- **Medium confidence**: Claims about hyperparameter robustness of the gradient ascent + descent method are supported by Figure 2 showing stable performance across learning rates, though the mechanism explanation could be more detailed
- **Low confidence**: Claims about MIA AUC approaching 0.5 as evidence of successful unlearning are questionable given the paper's own acknowledgment that MIA is inherently weak for LLMs, with only modest improvements observed

## Next Checks

1. **Validate approximate retraining baseline**: Compare approximate retraining results with actual retraining on a small subset of the retain set to quantify the approximation error and establish confidence intervals for the efficiency claims

2. **Stress-test MIA reliability**: Conduct ablation studies varying forget set size, data distribution, and model architecture to determine conditions under which MIA AUC scores correlate with actual unlearning effectiveness versus when they fail to detect unlearning

3. **Hyperparameter sensitivity analysis**: Systematically vary learning rates, batch sizes, and optimization steps across all seven methods on each domain to identify which hyperparameters most affect unlearning efficacy versus utility preservation, and whether these relationships hold consistently