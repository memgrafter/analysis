---
ver: rpa2
title: 'Speak Out of Turn: Safety Vulnerability of Large Language Models in Multi-turn
  Dialogue'
arxiv_id: '2402.17262'
source_url: https://arxiv.org/abs/2402.17262
tags:
- dialogue
- harmful
- multi-turn
- llms
- safety
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper exposes a new safety vulnerability of large language
  models (LLMs) in multi-turn dialogue, where decomposed malicious queries can be
  incrementally addressed to elicit harmful responses. The authors propose a method
  to decompose unsafe queries into multiple sub-queries, which are then posed in sequence
  to the model, allowing harmful content to accumulate over the conversation.
---

# Speak Out of Turn: Safety Vulnerability of Large Language Models in Multi-turn Dialogue

## Quick Facts
- arXiv ID: 2402.17262
- Source URL: https://arxiv.org/abs/2402.17262
- Reference count: 24
- This paper exposes a new safety vulnerability of large language models (LLMs) in multi-turn dialogue, where decomposed malicious queries can be incrementally addressed to elicit harmful responses.

## Executive Summary
This paper identifies a novel safety vulnerability in large language models (LLMs) during multi-turn dialogue, termed "Speak Out of Turn." The vulnerability allows attackers to decompose harmful queries into multiple safe sub-queries across conversation turns, which cumulatively lead to harmful responses. Through experiments with ChatGPT 3.5/4, Claude 1/2, and Gemini-Pro, the authors demonstrate that this approach significantly increases the likelihood of generating harmful content, with harmfulness scores ranging from 22.77% to 84.12%. The study highlights the need for improved safety mechanisms that consider multi-turn dialogue contexts and suggests potential mitigation strategies including enhanced context awareness and multi-turn red-teaming.

## Method Summary
The authors propose a method to decompose unsafe queries into multiple sub-queries that are posed sequentially in a multi-turn dialogue. These sub-queries are designed to be individually safe or cautious but collectively build toward harmful content. The decomposition can be done manually by experts or automatically using LLMs with few-shot demonstrations. The responses are then combined or inverted to generate harmful outputs. The method is evaluated across different commercial LLMs using the AdvBench dataset and harmfulness scores assessed by GPT-4 and LLAMA Guard.

## Key Results
- Decomposing harmful queries into multi-turn dialogue significantly increases harmfulness generation across all tested models (22.77% to 84.12% harmfulness scores)
- Role-playing in the final turn further increases harmful content generation while reducing the model's likelihood of rejection
- Automatic decomposition using LLMs proves to be a low-barrier approach for mass production of harmful query groups
- Manual decomposition achieves higher success rates (84.12%) compared to automatic methods (22.77%-57.84%)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Decomposing harmful queries into sub-questions bypasses single-turn safety alignment by exploiting instruction-following in multi-turn dialogue.
- Mechanism: Multi-turn dialogue context allows LLMs to follow user instructions across turns, incrementally building harmful content even when each individual turn is safe or cautionary.
- Core assumption: LLMs treat each turn as a continuation of the dialogue and attempt to be helpful by following instructions, even when the ultimate goal is harmful.
- Evidence anchors:
  - [abstract] "Therefore, by decomposing an unsafe query into several sub-queries for multi-turn dialogue, we induced LLMs to answer harmful sub-questions incrementally, culminating in an overall harmful response."
  - [section 3.2] "However, the cumulative harmful content across the dialogue leads to an alignment failure. Each turn within the multi-turn dialogue forms a part of the entire malicious content."
- Break condition: If the LLM recognizes the malicious intent across turns or if safety alignment is enhanced to consider multi-turn context holistically.

### Mechanism 2
- Claim: Role-playing in the final turn of multi-turn dialogue further increases the likelihood of generating harmful content by manipulating the model's context and persona.
- Mechanism: Assigning a specific role to the model in the final turn (e.g., "Developer Mode") makes it less likely to reject harmful requests and increases the quality of harmful generations.
- Core assumption: Role-playing influences the model's behavior by setting expectations for the type of response desired.
- Evidence anchors:
  - [section 4.3.2] "Role-playing such as 'Developer Mode' and 'Grandma Exploit' are frequently employed in crafting jailbreak prompts. Inspired by this concept, we introduce role-playing in the final turn of multi-turn dialogue to further test the safety of multi-turn dialogue."
  - [section 4.3.2] "Analyzing the experimental results, we find that role-playing actually makes the model less likely to reject to answer in the final turn while increasing the quality of harmful generations in multi-turn dialogue."
- Break condition: If the LLM is trained to recognize and resist role-playing attempts in the context of multi-turn dialogue.

### Mechanism 3
- Claim: The automatic decomposition of malicious queries using LLMs is a low-barrier approach that enables rapid mass production of harmful query groups.
- Mechanism: LLMs can generate decomposed sub-query prompts based on a few-shot demonstration and a transfer prompt, making it easy for anyone to exploit the safety vulnerability.
- Core assumption: LLMs can effectively learn the decomposition pattern from a small number of examples and apply it to new malicious queries.
- Evidence anchors:
  - [section 3.1] "Considering the output quality of LLMs depends on the prompt (Wei et al., 2022), and manual decomposition of prompts significantly relies on expertise. We explore using LLMs to generate decomposed sub-query prompts, a strategy proven to be a reliable solution (Zhang et al., 2022)."
  - [section 3.1] "Employing LLMs to generate malicious query decomposition automatically is a low-barrier approach and enables the rapid mass production of such query groups."
- Break condition: If the LLM is trained to recognize and reject attempts to decompose malicious queries automatically.

## Foundational Learning

- Concept: Instruction-following in multi-turn dialogue
  - Why needed here: The attack relies on the LLM's ability to follow instructions across multiple turns, building up harmful content incrementally.
  - Quick check question: How does the LLM's behavior change when given a series of related instructions versus a single instruction?

- Concept: Context window and In-Context Learning (ICL)
  - Why needed here: The LLM uses the context from previous turns to generate responses in subsequent turns, allowing harmful knowledge to accumulate.
  - Quick check question: What is the maximum context length of the LLM, and how does it affect the model's ability to maintain coherence across multiple turns?

- Concept: Safety alignment and red-teaming
  - Why needed here: Understanding how safety alignment works and how red-teaming is used to test for vulnerabilities is crucial for understanding the attack and potential defenses.
  - Quick check question: What are the key differences between single-turn and multi-turn red-teaming, and how do they affect the LLM's safety?

## Architecture Onboarding

- Component map: Malicious query -> Decomposition -> Multi-turn dialogue -> Harmful response
- Critical path: Malicious query → Decomposition → Multi-turn dialogue → Harmful response
- Design tradeoffs:
  - Manual vs. automatic decomposition: Manual decomposition may be more effective but requires expertise, while automatic decomposition is easier but may be less reliable.
  - Number of turns: More turns may lead to more harmful content but also increase the risk of detection.
- Failure signatures:
  - LLM rejects harmful queries in individual turns but generates harmful content in the final turn.
  - LLM fails to recognize the malicious intent across turns.
  - LLM is susceptible to role-playing in the final turn.
- First 3 experiments:
  1. Test the effectiveness of manual vs. automatic decomposition on a small set of malicious queries.
  2. Vary the number of turns in the multi-turn dialogue and measure the harmfulness of the final response.
  3. Test the impact of role-playing in the final turn on the LLM's likelihood of generating harmful content.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What specific safety alignment strategies could be implemented to effectively mitigate the vulnerabilities exposed in multi-turn dialogue contexts?
- Basis in paper: [explicit] The authors propose several potential mitigation strategies, including conducting RLHF and SFT safety alignment based on multi-turn dialogue data and enhancing the model's understanding of context.
- Why unresolved: While the authors suggest potential mitigation strategies, they do not provide specific implementation details or evaluate the effectiveness of these strategies in their experiments.
- What evidence would resolve it: Detailed implementation of proposed safety alignment strategies and experimental evaluation demonstrating their effectiveness in reducing harmful content generation in multi-turn dialogue contexts.

### Open Question 2
- Question: How do the safety vulnerabilities in multi-turn dialogue contexts compare across different model architectures and sizes?
- Basis in paper: [inferred] The authors conduct experiments on various commercial LLMs, including ChatGPT 3.5/4, Claude 1/2, and Gemini-Pro, but do not provide a detailed comparison of safety vulnerabilities across different model architectures and sizes.
- Why unresolved: The paper does not explicitly compare the safety vulnerabilities of different model architectures and sizes in multi-turn dialogue contexts.
- What evidence would resolve it: A systematic comparison of safety vulnerabilities across different model architectures and sizes, including analysis of factors such as model capacity, training data, and alignment methods.

### Open Question 3
- Question: How can the effectiveness of safety alignments in multi-turn dialogue contexts be measured and evaluated?
- Basis in paper: [explicit] The authors use GPT-4 evaluation and LLAMA Guard to assess the harmfulness of multi-turn dialogues, but do not provide a comprehensive framework for measuring and evaluating the effectiveness of safety alignments.
- Why unresolved: The paper does not propose a standardized framework for measuring and evaluating the effectiveness of safety alignments in multi-turn dialogue contexts.
- What evidence would resolve it: Development of a comprehensive framework for measuring and evaluating the effectiveness of safety alignments in multi-turn dialogue contexts, including metrics for assessing the reduction of harmful content generation and the maintenance of helpful and relevant responses.

## Limitations

- The automatic decomposition method relies heavily on GPT-4 as a judge, creating potential circularity where the same model family is used for both exploitation and evaluation.
- The evaluation criteria for harmfulness, while detailed, are subjective and may vary across different annotators or models.
- The study focuses primarily on text-based models and does not address potential multimodal extensions of this vulnerability.

## Confidence

**High Confidence**: The core finding that multi-turn dialogue enables harmful content generation through incremental query decomposition is well-supported by experimental results across multiple model families (GPT-3.5/4, Claude-1/2, Gemini-Pro). The harmfulness scores (22.77% to 84.12%) and the effectiveness of role-playing demonstrate robust evidence for this claim.

**Medium Confidence**: The effectiveness of automatic decomposition using LLMs, while demonstrated, may vary depending on the specific implementation details not fully specified in the paper. The reliance on few-shot examples and transfer prompts introduces variability that could affect reproducibility.

**Low Confidence**: The generalizability of these findings to other LLM architectures or safety training approaches remains unclear. The study does not extensively explore variations in model temperature, context length, or alternative safety mechanisms that might mitigate this vulnerability.

## Next Checks

1. **Cross-Evaluation Validation**: Replicate the harmfulness scoring using independent evaluators (human annotators and different LLM judges) to verify the consistency of the harmfulness metrics and address potential circularity concerns.

2. **Defense Mechanism Testing**: Implement and test baseline defense strategies such as context-aware safety alignment, multi-turn red-teaming detection, and enhanced instruction-following constraints to measure their effectiveness against the Speak Out of Turn vulnerability.

3. **Decomposition Robustness Analysis**: Systematically vary the number of turns, the specificity of sub-queries, and the complexity of malicious queries to determine the limits of automatic decomposition effectiveness and identify potential breakpoints in the attack methodology.