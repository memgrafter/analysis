---
ver: rpa2
title: 'CodeS: Towards Building Open-source Language Models for Text-to-SQL'
arxiv_id: '2402.16347'
source_url: https://arxiv.org/abs/2402.16347
tags:
- language
- text-to-sql
- database
- codes
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: CodeS addresses the challenge of building open-source language
  models for text-to-SQL translation, overcoming limitations of closed-source LLMs
  like ChatGPT and GPT-4 such as unclear architectures, data privacy risks, and high
  inference costs. The core method involves incremental pre-training on a SQL-centric
  corpus and enhancing schema linking through strategic prompt construction and bi-directional
  data augmentation.
---

# CodeS: Towards Building Open-source Language Models for Text-to-SQL

## Quick Facts
- arXiv ID: 2402.16347
- Source URL: https://arxiv.org/abs/2402.16347
- Authors: Haoyang Li; Jing Zhang; Hanbing Liu; Ju Fan; Xiaokang Zhang; Jun Zhu; Renjie Wei; Hongyan Pan; Cuiping Li; Hong Chen
- Reference count: 40
- Primary result: Open-source CodeS models (1B-15B parameters) achieve SOTA accuracy on text-to-SQL benchmarks, outperforming much larger closed-source LLMs like ChatGPT.

## Executive Summary
CodeS introduces open-source language models specifically designed for text-to-SQL translation, addressing the limitations of closed-source alternatives like ChatGPT and GPT-4. The approach combines incremental pre-training on SQL-centric data with strategic prompt construction and bi-directional data augmentation to achieve superior performance on challenging benchmarks. CodeS demonstrates that smaller, open-source models can outperform much larger closed-source models while providing transparency, data privacy, and reduced inference costs.

## Method Summary
CodeS builds on StarCoder base models (1B-15B parameters) and employs incremental pre-training on a curated SQL-centric corpus to enhance SQL generation capabilities. The method incorporates database prompt construction with schema filtering and value retrieval to improve schema linking accuracy. For domain adaptation, CodeS uses bi-directional data augmentation to synthesize training data with minimal annotation. The models are fine-tuned using supervised learning or few-shot in-context learning for text-to-SQL inference.

## Key Results
- CodeS-15B achieves 52.15% execution accuracy on BIRD, a 23.20% improvement over ChatGPT
- CodeS models outperform existing SOTA models on nearly all text-to-SQL benchmarks including Spider and BIRD
- CodeS demonstrates superior performance with models 10x to 100x smaller than competing LLMs

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Incremental pre-training on SQL-centric data compensates for limited SQL exposure in general pre-training corpora
- Mechanism: Models are fine-tuned on 11GB SQL-related data, 6GB NL-to-code data, and 4.5GB NL-related data after initial broad training
- Core assumption: General pre-trained models lack sufficient SQL-specific knowledge for accurate text-to-SQL translation
- Evidence anchors: [abstract], [section 5.1]
- Break condition: If curated corpus lacks diversity, model may not generalize to new SQL patterns

### Mechanism 2
- Claim: Database prompt construction with schema filtering and value retrieval reduces noise and improves schema linking accuracy
- Mechanism: Schema filter removes irrelevant tables/columns; value retriever uses BM25 index for rapid value alignment
- Core assumption: Reducing prompt length and providing relevant schema information helps model focus on essential elements
- Evidence anchors: [abstract], [section 6.1]
- Break condition: If schema filter incorrectly removes necessary elements or value retriever fails to find relevant values

### Mechanism 3
- Claim: Bi-directional data augmentation with limited annotation enables rapid domain adaptation
- Mechanism: Real questions are annotated and expanded with GPT-3.5; SQL templates are filled and refined to create training pairs
- Core assumption: Synthesized data reflecting user preferences improves adaptation to new domains
- Evidence anchors: [abstract], [section 7]
- Break condition: If synthesized data doesn't match real query distribution or templates are too rigid

## Foundational Learning

- Concept: Incremental pre-training
  - Why needed here: General pre-trained models lack sufficient SQL-specific knowledge for accurate text-to-SQL translation
  - Quick check question: What is the difference between pre-training and fine-tuning, and why is incremental pre-training used in this context?

- Concept: Schema linking
  - Why needed here: Mapping natural language questions to specific database elements is crucial for generating correct SQL queries
  - Quick check question: What are the main challenges in schema linking, and how does the schema filter and value retriever address them?

- Concept: Data augmentation
  - Why needed here: New domains often lack sufficient labeled training data for effective fine-tuning
  - Quick check question: What are the advantages and limitations of the bi-directional data augmentation approach compared to other methods?

## Architecture Onboarding

- Component map: CodeS models (1B, 3B, 7B, 15B) → Incremental pre-training on SQL-centric corpus → Database prompt construction (schema filter, value retriever, metadata) → Bi-directional data augmentation (question-to-SQL, SQL-to-question) → Supervised fine-tuning or few-shot in-context learning → Text-to-SQL inference
- Critical path: Pre-training → Prompt construction → Inference/Evaluation
- Design tradeoffs: Smaller models are faster and require less memory but may have lower accuracy; bi-directional data augmentation balances authenticity and diversity but relies on GPT-3.5 synthesis
- Failure signatures: Incorrect SQL queries due to poor schema linking, overfitting to training data, inability to adapt to new domains, slow inference with larger models
- First 3 experiments:
  1. Evaluate CodeS-1B, 3B, 7B, and 15B on small text-to-SQL benchmark to assess model size impact
  2. Compare CodeS performance with and without schema filter and value retriever on complex schemas
  3. Test bi-directional data augmentation effectiveness on new domain with limited labeled data

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What mechanisms could further improve schema linking in CodeS for databases with ambiguous schema names and complex structures?
- Basis in paper: [explicit] The paper acknowledges challenges of ambiguous schema names and complex structures but doesn't provide detailed solutions
- Why unresolved: Paper mentions the challenge but doesn't detail specific strategies for handling these cases
- What evidence would resolve it: Testing new schema linking strategies specifically addressing ambiguous names and complex structures

### Open Question 2
- Question: How does CodeS performance compare to other open-source code language models designed for SQL generation?
- Basis in paper: [inferred] Paper compares to StarCoder but doesn't specifically compare to other SQL-optimized code models
- Why unresolved: Focus is on general-purpose language models rather than SQL-specific code models
- What evidence would resolve it: Comparative evaluation against other open-source SQL generation models

### Open Question 3
- Question: How does bi-directional data augmentation perform on domains significantly different from Spider and BIRD benchmarks?
- Basis in paper: [explicit] Paper demonstrates effectiveness on financial and academic domains but acknowledges potential variation for significantly different domains
- Why unresolved: Limited exploration of performance on domains far from benchmark domains
- What evidence would resolve it: Applying approach to diverse domain databases and evaluating performance

## Limitations

- The effectiveness of incremental pre-training depends heavily on the quality and diversity of the SQL-centric corpus
- Schema linking improvements rely on the effectiveness of BM25-based value retrieval and schema filter accuracy
- Bi-directional data augmentation introduces potential distribution shifts between synthetic and real data

## Confidence

- **High Confidence**: Overall experimental results showing SOTA performance with smaller models; 52.15% execution accuracy on BIRD with 23.20% improvement over ChatGPT
- **Medium Confidence**: Individual mechanism effectiveness; combined success demonstrated but limited ablation studies on component contributions
- **Low Confidence**: Scalability and generalization to entirely new domains or database types not represented in training corpus

## Next Checks

1. **Ablation Study on Individual Components**: Systematically evaluate CodeS performance with incremental pre-training removed, schema filter disabled, and bi-directional data augmentation excluded to quantify each mechanism's independent contribution

2. **Robustness Testing on Schema Variations**: Test CodeS on databases with intentionally ambiguous schema names, missing foreign keys, or atypical naming conventions to evaluate schema linking limits

3. **Cross-Domain Generalization Analysis**: Evaluate CodeS on a new domain with significantly different database structure (temporal, graph, or nested schemas) not represented in SQL-centric corpus to test true generalization capabilities