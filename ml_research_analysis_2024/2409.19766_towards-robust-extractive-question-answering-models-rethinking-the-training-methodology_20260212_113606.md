---
ver: rpa2
title: 'Towards Robust Extractive Question Answering Models: Rethinking the Training
  Methodology'
arxiv_id: '2409.19766'
source_url: https://arxiv.org/abs/2409.19766
tags:
- training
- questions
- squad
- default
- unanswerable
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper addresses the lack of robustness in extractive question
  answering (EQA) models trained on datasets with unanswerable questions. The authors
  identify two key issues: (1) existing loss functions incorrectly treat unanswerable
  questions as having an answer span at the [CLS] token, and (2) the implicit single-answer
  assumption in EQA datasets makes models vulnerable to adversarial attacks.'
---

# Towards Robust Extractive Question Answering Models: Rethinking the Training Methodology

## Quick Facts
- arXiv ID: 2409.19766
- Source URL: https://arxiv.org/abs/2409.19766
- Reference count: 38
- Models trained with novel method show 5.7 overall F1 score improvement on out-of-domain datasets

## Executive Summary
This paper addresses the lack of robustness in extractive question answering (EQA) models by identifying two key issues in current training approaches: (1) loss functions incorrectly treat unanswerable questions as having answers at the [CLS] token, and (2) the single-answer assumption in EQA datasets creates vulnerabilities to adversarial attacks. The authors propose a new training methodology that includes a novel loss function treating unanswerable questions with uniform label probability across all tokens, and the introduction of synthetic answer spans in answerable questions to eliminate the single-answer assumption. Experimental results show that models trained with this approach maintain in-domain performance while improving F1 scores on out-of-domain datasets by 5.7 overall and enhancing robustness against adversarial attacks, reducing performance drops by about two-thirds compared to default models.

## Method Summary
The authors propose a new training methodology for EQA models that addresses two key issues. First, they introduce a novel loss function that treats unanswerable questions by assigning uniform label probability to all tokens rather than concentrating it on the [CLS] token. Second, they create synthetic answer spans in a subset of answerable questions to force models to learn that questions can have multiple valid answers. The training combines this with a sequence tagging loss that enables models to naturally signal "unanswerable" predictions by outputting negative span scores. Models are trained using SQuAD 2.0 and SQuAD AGent datasets with specific parameters: batch size of 8, 3 epochs, maximum sequence length of 384 tokens, and AdamW optimizer with initial learning rate of 2 · 10−5.

## Key Results
- Overall F1 score improvement of 5.7 across all testing sets
- Maintained in-domain performance while achieving notable improvement on out-of-domain datasets
- Enhanced robustness against two types of adversarial attacks (AddOneSent and Negation), reducing performance drops by about two-thirds compared to default models

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The novel loss function treats unanswerable questions by assigning uniform label probability to all tokens rather than concentrating it on the [CLS] token.
- Mechanism: This prevents the model from learning that unanswerable questions have a "hidden" answer span, which is what the default Cross Entropy loss does.
- Core assumption: Uniform label distribution over all tokens for unanswerable questions is a better representation of the lack of an answer than a single [CLS] token label.
- Evidence anchors:
  - [section 5.2] "In our approach, since we treat all tokens in these sequences as equally unlikely to be the start or end of an answer, all tokens within an unanswerable sequence are assigned the same label uniformly, represented as ys_k = ye_k = 1/n, where n denotes the sequence length."
  - [abstract] "Our proposed training method includes a novel loss function for the EQA problem and challenges an implicit assumption present in numerous EQA datasets."
- Break condition: If the model still learns to prefer the [CLS] token for unanswerable questions despite uniform labeling, the mechanism fails.

### Mechanism 2
- Claim: Introducing synthetic answer spans in answerable questions forces the model to learn that questions can have multiple valid answers.
- Mechanism: By adding extra answer spans, the model cannot rely on the single-answer assumption, making it more robust to adversarial attacks that try to exploit this assumption.
- Core assumption: The single-answer assumption in most EQA datasets is a learning shortcut that makes models vulnerable to adversarial attacks.
- Evidence anchors:
  - [section 5.3] "In this work, we design new training loss function that naturally treats unanswerable questions as lacking any answer. Second, to overcome the single-answer assumption in most EQA datasets, we create a new 'synthetic' answer span in a number of answerable questions."
  - [abstract] "The assumption that a given question can only have a single answer or no answer introduces a learning shortcut, making EQA models vulnerable to adversarial attacks."
- Break condition: If the model continues to predict only the original answer span and ignores the synthetic ones, the mechanism fails.

### Mechanism 3
- Claim: The sequence tagging loss enables models to naturally signal "unanswerable" predictions by outputting negative span scores.
- Mechanism: This loss encourages the model to learn negative scores for all spans in unanswerable sequences, which the inference pipeline uses to output empty predictions.
- Core assumption: Negative span scores for unanswerable sequences are a more natural way to signal "unanswerable" than the default [CLS] token approach.
- Evidence anchors:
  - [section 5.2] "We enable our models to naturally signal 'unanswerable' predictions by using an inference pipeline that outputs an 'empty' prediction if the maximum span score of si + ej is negative."
  - [section 5.2] "To enable models to output negativesi + ej scores for all spans in unanswerable sequences, we incorporate sequence tagging loss alongside the standard QA loss."
- Break condition: If the model fails to output negative scores for unanswerable sequences, the mechanism fails.

## Foundational Learning

- Concept: Cross Entropy loss and its application in sequence labeling tasks
  - Why needed here: The paper builds a novel loss function based on Cross Entropy, modifying it to handle unanswerable questions differently
  - Quick check question: What is the key difference between the default Cross Entropy loss and the novel loss function proposed in this paper?

- Concept: Adversarial attacks in NLP and their impact on model robustness
  - Why needed here: The paper evaluates robustness against two types of adversarial attacks (AddOneSent and Negation)
  - Quick check question: How do AddOneSent and Negation attacks work, and why are they effective against EQA models?

- Concept: Distribution shift in machine learning and its implications for model generalization
  - Why needed here: The paper measures robustness against distribution shifts from adversarial unanswerable questions to information-seeking unanswerable questions
  - Quick check question: What is the difference between adversarial unanswerable questions and information-seeking unanswerable questions?

## Architecture Onboarding

- Component map:
  Pre-trained language model (BERT/RoBERTa/SpanBERT) -> Two single-layer feed-forward neural networks (start and end position prediction) -> Custom loss function (QA loss + Sequence Tagging Loss) -> Inference pipeline (outputs empty predictions for negative span scores)

- Critical path:
  1. Input question-context pair through pre-trained model
  2. Obtain contextualized embeddings for each token
  3. Feed embeddings through start and end position classifiers
  4. Compute custom loss function during training
  5. During inference, select span with highest score or output empty if all scores are negative

- Design tradeoffs:
  - Using uniform labels for unanswerable questions vs concentrating on [CLS] token
  - Adding synthetic answers increases training complexity but improves robustness
  - Sequence tagging loss adds another component but enables more natural unanswerable predictions

- Failure signatures:
  - Model still predicts [CLS] token for unanswerable questions despite uniform labeling
  - Model ignores synthetic answers and only predicts original answer spans
  - Model fails to output negative scores for unanswerable sequences

- First 3 experiments:
  1. Train model with novel loss function on SQuAD 2.0 and evaluate on in-domain and out-of-domain datasets
  2. Remove synthetic answers and retrain to isolate the effect of the novel loss function
  3. Train model on SQuAD AGent (information-seeking unanswerable questions) and evaluate robustness against adversarial attacks

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions, but several implicit questions arise from the methodology and results.

## Limitations
- Evaluation relies entirely on synthetic unanswerable examples and a narrow set of adversarial attacks (AddOneSent, Negation)
- The approach assumes access to generative models (like ChatGPT-turbo3.5) for creating synthetic answers, which may not be available in all deployment contexts
- Improvements in adversarial robustness are measured against specific attack types; generalizability to other attack strategies is unclear

## Confidence
- High Confidence: The core finding that uniform label assignment for unanswerable questions improves robustness against distribution shifts is well-supported by experimental results
- Medium Confidence: The claim about synthetic answers improving robustness against adversarial attacks has experimental support but relies on a single synthetic answer generation method
- Medium Confidence: The overall F1 score improvement of 5.7 across all testing sets is demonstrated, but the relative contributions of the novel loss function versus synthetic answers are not fully disentangled

## Next Checks
1. Evaluate the proposed method on additional EQA datasets with naturally occurring unanswerable questions to verify that improvements in robustness against distribution shifts generalize beyond the specific datasets used in this study.

2. Test the adversarial robustness claims against a broader range of attack types, including semantic-preserving transformations and more sophisticated adversarial generation methods, to determine whether the improvements against AddOneSent and Negation attacks generalize.

3. Conduct a sensitivity analysis by generating synthetic answers using different methods (e.g., rule-based, different LLMs, or human annotation) to determine how dependent the robustness gains are on the specific synthetic answer generation approach used in this paper.