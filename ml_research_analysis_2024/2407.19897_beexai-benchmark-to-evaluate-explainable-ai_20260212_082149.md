---
ver: rpa2
title: 'BEExAI: Benchmark to Evaluate Explainable AI'
arxiv_id: '2407.19897'
source_url: https://arxiv.org/abs/2407.19897
tags:
- methods
- metrics
- features
- evaluation
- beexai
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces BEExAI, a comprehensive benchmark tool for
  evaluating post-hoc XAI methods on tabular data. BEExAI addresses the challenge
  of quantitatively assessing explainability by providing a standardized pipeline
  that supports 9 evaluation metrics, 8 explainability methods, and a wide range of
  ML models.
---

# BEExAI: Benchmark to Evaluate Explainable AI

## Quick Facts
- arXiv ID: 2407.19897
- Source URL: https://arxiv.org/abs/2407.19897
- Authors: Samuel Sithakoul; Sara Meftah; Clément Feutry
- Reference count: 40
- Key outcome: BEExAI provides a standardized benchmark for evaluating 8 XAI methods across 9 metrics on 50 tabular datasets, with Shapley Value Sampling generally outperforming others in Faithfulness

## Executive Summary
BEExAI is a comprehensive benchmark tool for quantitatively evaluating post-hoc explainable AI (XAI) methods on tabular data. The tool addresses the challenge of inconsistent XAI evaluation by providing a standardized pipeline that supports 9 evaluation metrics, 8 explainability methods, and various ML models across regression, binary classification, and multi-label classification tasks. BEExAI is open-source and includes detailed documentation and examples. The benchmark demonstrates that Shapley Value Sampling generally outperforms other methods in terms of Faithfulness, while Saliency is robust, and LIME and DeepLift offer less complex explanations.

## Method Summary
BEExAI implements a standardized end-to-end pipeline for XAI evaluation, consisting of data preprocessing, model training (XGBoost and 3-layer neural networks), explanation generation using 8 different methods (LIME, SHAP, Integrated Gradients, etc.), and metric computation across 9 evaluation metrics measuring Faithfulness, Robustness, and Complexity. The tool handles task-specific adaptations such as using absolute attribution values for regression tasks and selecting the highest probability label for classification. The benchmark uses 50 widely-used datasets and limits evaluation to 1000 test samples for computational efficiency, computing metrics with multiple random seeds to ensure stability.

## Key Results
- Shapley Value Sampling generally outperforms other methods in Faithfulness metrics across tasks
- Saliency method demonstrates robustness as a gradient-based explainer
- LIME and DeepLift produce explanations with lower complexity scores
- The standardized pipeline enables consistent comparison across 50 datasets and 8 explainability methods

## Why This Works (Mechanism)

### Mechanism 1
- Claim: BEExAI provides a standardized, reproducible evaluation pipeline for post-hoc XAI methods on tabular data
- Mechanism: Fixed sequence of preprocessing, model training, explanation generation, and metric computation ensures consistent and comparable results
- Core assumption: Choice of zero baseline and task-specific attribution handling yield meaningful comparisons
- Evidence anchors: Abstract mentions standardized pipeline supporting 9 metrics and 8 methods; section describes end-to-end pipeline for benchmarks
- Break condition: If baseline choice or attribution handling systematically biases metrics, standardization advantage diminishes

### Mechanism 2
- Claim: Multiple complementary metrics capture faithfulness more robustly than any single metric
- Mechanism: Combining Faithfulness Correlation, Infidelity, Comprehensiveness, Sufficiency, Monotonicity, and AUC-TP reduces reliance on flawed individual metrics
- Core assumption: Selected metrics collectively provide more accurate picture of faithfulness
- Evidence anchors: Section focuses on faithfulness as essential property lacking consensus; discusses selecting highest probability label for target
- Break condition: If metrics are redundant or capture overlapping aspects, computational cost increases without added insight

### Mechanism 3
- Claim: Task-specific handling improves metric interpretability and comparability
- Mechanism: Adapting evaluation metrics to task semantics (absolute values for regression, label selection for classification) ensures scores reflect task-appropriate explanation quality
- Core assumption: Modifications are both necessary and sufficient for valid comparisons
- Evidence anchors: Section discusses using absolute attribution values instead of raw values; adjusting metrics for signed vs absolute differences
- Break condition: If modifications introduce bias or reduce sensitivity, task-specific handling may mislead comparisons

## Foundational Learning

- Concept: Feature ablation and baseline selection in XAI evaluation
  - Why needed here: Many faithfulness metrics require systematically removing or replacing features to measure impact on predictions; baseline choice affects metric values
  - Quick check question: If you replace a feature with the dataset mean instead of zero, how might that change the Faithfulness Correlation score for a regression task?

- Concept: Shannon entropy and Gini index for explanation sparsity
  - Why needed here: Complexity and Sparseness metrics use these measures to quantify how concentrated attribution scores are on few features versus spread evenly
  - Quick check question: Given attributions [0.9, 0.05, 0.05], compute the Shannon entropy and Gini index; what do these values say about explanation sparsity?

- Concept: Gradient-based vs. perturbation-based explainers
  - Why needed here: BEExAI includes both types; understanding their computational trade-offs and model compatibility is essential for interpreting results
  - Quick check question: Why can't Integrated Gradients be applied to a Random Forest model, but LIME can?

## Architecture Onboarding

- Component map: Data preprocessing -> Model training -> Explanation generation -> Metric computation -> Benchmarking aggregation
- Critical path: Preprocessed data -> Trained model -> Attributions -> Metric evaluation -> Benchmark report
- Design tradeoffs: 
  - Zero baseline simplifies computation but may not be optimal for all tasks
  - Limiting to 1000 test samples speeds benchmarks but may reduce statistical robustness
  - Combining multiple faithfulness metrics increases confidence but adds runtime
- Failure signatures:
  - If model R² or accuracy is too low, XAI metrics may be meaningless
  - If feature importance scores are uniformly distributed, Complexity and Sparseness will be low
  - If perturbations are too large, Infidelity and Sensitivity may become unstable
- First 3 experiments:
  1. Run BEExAI on Boston Housing with XGBoost and compare LIME vs. SHAP Faithfulness Correlation
  2. Generate random attributions via sanity check and verify that Faithfulness metrics are near zero
  3. Switch baseline from zero to mean and observe changes in Comprehensiveness and Sufficiency for regression

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal baseline selection method for explainability metrics that require feature ablation?
- Basis in paper: [explicit] The paper discusses the impact of baseline choice on evaluation metrics and mentions various baseline methods
- Why unresolved: Further research needed to accurately measure baselines' choice impact on evaluation metrics' results and their alignment with intended interpretation
- What evidence would resolve it: Comprehensive study comparing different baseline selection methods across datasets and tasks, measuring their impact on evaluation metrics

### Open Question 2
- Question: How should explainability attributions be computed and evaluated differently for regression versus classification tasks?
- Basis in paper: [explicit] The paper highlights significant difference in interpretation of attribution scores between classification and regression tasks
- Why unresolved: Lack of previous research addressing this matter; in-depth research needed to analyze best method for consistent comparisons
- What evidence would resolve it: Thorough investigation comparing different methods of computing and evaluating explainability attributions for regression and classification tasks

### Open Question 3
- Question: How does the predictive performance of machine learning models influence the effectiveness of explainability methods?
- Basis in paper: [explicit] The paper mentions future work could focus on addressing influence of models' predictive capability on explainers' performance
- Why unresolved: Paper does not explore this aspect in detail; suggests it could be a potential direction for future research
- What evidence would resolve it: Systematic study analyzing relationship between ML model performance and effectiveness of various explainability methods

### Open Question 4
- Question: What is the best approach to quantify the plausibility of explanations from a human perspective?
- Basis in paper: [explicit] The paper suggests quantitative analysis should be associated with human assessment of results, studying plausibility of explanations
- Why unresolved: Paper does not provide specific method for quantifying plausibility; suggests it could be a direction for future research
- What evidence would resolve it: Framework for evaluating plausibility of explanations from human perspective, potentially involving user studies or surveys

## Limitations

- Reliance on zero baseline for feature ablation may not be optimal for all tasks and could introduce systematic bias
- Restriction to 1000 test samples may reduce statistical robustness for some datasets
- Lack of corpus evidence for task-specific metric adaptations and multi-metric faithfulness evaluation

## Confidence

- High: Standardized pipeline definition and demonstration across 50 datasets
- Medium: Interpretation of metric results due to known variability and potential redundancy among faithfulness metrics
- Low: No specific claims identified with low confidence

## Next Checks

1. Test sensitivity of BEExAI's metrics to different baselines (mean, median, sampled) to determine if zero baseline choice systematically affects results
2. Evaluate redundancy among faithfulness metrics by computing pairwise correlations and conducting dimensionality reduction to assess whether all 6 faithfulness metrics are necessary
3. Validate BEExAI's results on a subset of datasets using cross-validation and varying random seeds to quantify stability and variance of metric scores across runs