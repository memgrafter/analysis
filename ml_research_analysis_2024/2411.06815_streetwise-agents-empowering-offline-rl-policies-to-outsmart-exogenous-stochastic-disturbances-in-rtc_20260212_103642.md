---
ver: rpa2
title: 'Streetwise Agents: Empowering Offline RL Policies to Outsmart Exogenous Stochastic
  Disturbances in RTC'
arxiv_id: '2411.06815'
source_url: https://arxiv.org/abs/2411.06815
tags:
- offline
- policy
- learning
- policies
- bandwidth
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of deploying offline reinforcement
  learning (RL) policies in real-time communication (RTC) systems, where unseen exogenous
  stochastic disturbances during deployment cause critical failures and generalization
  errors. The authors propose a novel post-deployment policy shaping framework called
  "Streetwise" that adapts policies to handle out-of-distribution (OOD) scenarios
  triggered by these disturbances.
---

# Streetwise Agents: Empowering Offline RL Policies to Outsmart Exogenous Stochastic Disturbances in RTC

## Quick Facts
- arXiv ID: 2411.06815
- Source URL: https://arxiv.org/abs/2411.06815
- Reference count: 40
- Primary result: Achieves up to 18% improvement in video MOS compared to baselines on noisy network profiles

## Executive Summary
This paper addresses the challenge of deploying offline reinforcement learning (RL) policies in real-time communication (RTC) systems where unseen exogenous stochastic disturbances during deployment cause critical failures. The authors propose Streetwise, a novel post-deployment policy shaping framework that adapts policies to handle out-of-distribution (OOD) scenarios triggered by these disturbances. Streetwise uses an LSTM autoencoder to detect OOD states via reconstruction loss and perturbs the original policy's actions using value function gradients weighted by this OOD indicator, achieving significant performance improvements without requiring retraining.

## Method Summary
Streetwise works by first training an offline RL policy (using IQL) and an LSTM autoencoder on limited offline data to characterize normal dynamics. During deployment, the autoencoder detects OOD states through reconstruction loss, and the method perturbs the original policy's actions using gradients from the value function weighted by this OOD indicator. This post-deployment shaping allows the policy to adapt to unseen disturbances without retraining, making it suitable for safety-critical applications where online training is impractical.

## Key Results
- Achieves up to 18% improvement in video Mean Opinion Score (MOS) compared to baselines on noisy network profiles
- Generalizes to standard MuJoCo tasks with viscosity disturbances, outperforming baselines by 2-6% on average
- Demonstrates robustness while maintaining performance on stable networks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LSTM autoencoder detects out-of-distribution (OOD) states via reconstruction loss
- Mechanism: The LSTM autoencoder is trained on normal offline trajectories to learn typical transition dynamics. During deployment, it reconstructs incoming sequences; when exogenous disturbances alter the dynamics, the reconstruction error increases, signaling an OOD state.
- Core assumption: The reconstruction error from the LSTM autoencoder reliably correlates with the presence and magnitude of exogenous disturbances in the transition dynamics.
- Evidence anchors:
  - [abstract] "uses the autoencoder to detect OOD states via reconstruction loss"
  - [section] "The LSTM-AE serves as an OOD detector and characterizer. It tries to reconstruct the most recent sequence while the agent is acting in the environment, and returns some reconstruction error. The reconstruction loss is a valid proxy for the metric highlighted in Equation 1"
- Break condition: If exogenous disturbances affect states but not transition dynamics (or vice versa), the reconstruction loss may not capture the disturbance effectively.

### Mechanism 2
- Claim: Policy shaping with value function gradients adapts actions in OOD regions
- Mechanism: When OOD states are detected, the method perturbs the original policy's actions using gradients from the value function weighted by the reconstruction loss. This steers actions toward higher-value regions that the original policy might not explore due to its behavior cloning constraints.
- Core assumption: The value function gradient ‚àáùëéùëÑ (ùë†, ùëé) provides useful directional information for improving actions in OOD regions, and this perturbation improves rather than degrades performance.
- Evidence anchors:
  - [abstract] "perturbs the original policy's actions using gradients from the value function weighted by this OOD indicator"
  - [section] "To shape the policy in these regions, we use the ùëÑ-function, which extrapolates better in these unknown subspaces. Specifically, we take the gradient of the ùëÑ-function with respect to ùëé ‚àº ùúã ‚àóD, which is defined as ‚àáùëéùëÑ (ùë†, ùëé). This is likely to point towards actions with a higher value than those sampled from the policy"
- Break condition: If the value function's extrapolation in OOD regions is poor or misleading, the gradient-based perturbation could worsen performance.

### Mechanism 3
- Claim: Post-deployment shaping allows adaptation without retraining
- Mechanism: Instead of learning a generalized robust policy within the support of offline data (which is theoretically impossible for handling unknown disturbances), Streetwise shapes the deployed policy at runtime based on real-time OOD characterization, enabling adaptation to unseen disturbances.
- Core assumption: It is impossible to learn a generalized robust policy within the support of limited offline samples that can handle all possible unseen exogenous disturbances.
- Evidence anchors:
  - [abstract] "As it is impossible to learn generalized offline policies within the support of offline data that are robust to these unseen exogenous disturbances, we propose a novel post-deployment shaping of policies"
  - [section] "It is theoretically and practically infeasible to train generalized policies, within the support of the offline data, that are robust to such OOD regions in production"
- Break condition: If the disturbance space can be characterized and included in offline training (e.g., through extensive data collection), post-deployment shaping may be unnecessary.

## Foundational Learning

- Concept: Offline Reinforcement Learning
  - Why needed here: The paper addresses deploying policies trained on limited offline data in real-time communication systems where online training is impractical.
  - Quick check question: What is the key challenge in offline RL that Streetwise aims to address when policies are deployed in production?

- Concept: Out-of-Distribution (OOD) Detection
  - Why needed here: The method needs to identify when exogenous disturbances cause the environment to shift to OOD states during deployment.
  - Quick check question: How does the LSTM autoencoder in Streetwise detect OOD states?

- Concept: Policy Shaping/Perturbation
  - Why needed here: Instead of retraining, the method adjusts the deployed policy's actions at runtime based on detected OOD states.
  - Quick check question: What mathematical operation does Streetwise use to modify the original policy's actions when OOD states are detected?

## Architecture Onboarding

- Component map:
  - Offline RL Policy Training (IQL)
  - LSTM Autoencoder for OOD Detection
  - Post-Deployment Policy Shaping Module
  - Value Function (for gradient computation)

- Critical path:
  1. Train offline RL policy and value function on clean data
  2. Train LSTM autoencoder on same trajectories
  3. During deployment: detect OOD via reconstruction loss
  4. Compute value function gradient and shape action
  5. Execute shaped action

- Design tradeoffs:
  - Using reconstruction loss as OOD metric trades off detection sensitivity with false positives
  - Gradient-based shaping trades off computational overhead with potential performance gains
  - Sequence length k for LSTM affects detection temporal resolution vs. computational cost

- Failure signatures:
  - Poor reconstruction loss correlation with actual disturbances
  - Value function gradients leading to degraded actions in OOD regions
  - Computational overhead causing latency issues in real-time systems

- First 3 experiments:
  1. Test reconstruction loss correlation with known synthetic disturbances on validation data
  2. Compare shaped vs. original policy actions in simulated OOD scenarios
  3. Benchmark Streetwise vs. IQL+OPEX on stable vs. disturbed network profiles

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can Streetwise be extended to handle non-stationary reward functions in addition to transition dynamics disturbances?
- Basis in paper: [explicit] The paper notes that in MuJoCo experiments with viscosity disturbances, there was an unintended side-effect of non-stationary reward functions, and neither Streetwise nor OPEX are designed for this scenario.
- Why unresolved: The current framework focuses on characterizing and adapting to changes in transition dynamics through reconstruction loss, but does not explicitly address scenarios where the reward function itself changes over time.
- What evidence would resolve it: Experiments demonstrating Streetwise's effectiveness on domains with both transition dynamics and reward function disturbances, along with theoretical analysis of how to incorporate reward uncertainty into the shaping potential.

### Open Question 2
- Question: What are the precise stability conditions for the Streetwise framework in terms of exogenous disturbance bounds?
- Basis in paper: [explicit] Section 4.5 provides a discussion on stability using Lyapunov functions, but notes that "Deriving the exact conditions on the manifold for the exogenous disturbance with respect to the normal behavior is future work."
- Why unresolved: While the paper establishes that bounded disturbances lead to bounded system behavior, it does not provide explicit mathematical conditions or thresholds for when the system remains stable versus unstable.
- What evidence would resolve it: Mathematical proofs or empirical studies establishing explicit bounds on exogenous disturbances