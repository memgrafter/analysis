---
ver: rpa2
title: Distilling Privileged Multimodal Information for Expression Recognition using
  Optimal Transport
arxiv_id: '2401.15489'
source_url: https://arxiv.org/abs/2401.15489
tags:
- student
- privileged
- proposed
- teacher
- modality
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a privileged knowledge distillation method
  using optimal transport (PKDOT) for multimodal expression recognition. The method
  addresses the challenge of missing modalities at test time by distilling knowledge
  from a multimodal teacher network to a unimodal student network using privileged
  information available only during training.
---

# Distilling Privileged Multimodal Information for Expression Recognition using Optimal Transport

## Quick Facts
- **arXiv ID**: 2401.15489
- **Source URL**: https://arxiv.org/abs/2401.15489
- **Reference count**: 40
- **Primary result**: Introduces PKDOT, achieving 78.76% accuracy on Biovid and 0.43 valence/0.56 arousal on Affwild2 for multimodal expression recognition

## Executive Summary
This paper addresses the challenge of missing modalities at test time in multimodal expression recognition through privileged knowledge distillation. The proposed PKDOT method distills knowledge from a multimodal teacher network to a unimodal student network using privileged information available only during training. By capturing local structures in the teacher representation space through cosine similarity matrices and applying entropy-regularized optimal transport, PKDOT effectively transfers structural dark knowledge. The method employs a T-Net to hallucinate privileged modality features at test time, enabling robust performance even when modalities are unavailable during inference.

## Method Summary
PKDOT leverages privileged information during training to bridge the modality gap at test time. The approach consists of a multimodal teacher network that processes all available modalities and a unimodal student network that learns to mimic the teacher's behavior. A key innovation is the use of cosine similarity matrices to capture local structures in the teacher's representation space, which are then transferred to the student through entropy-regularized optimal transport. Additionally, a T-Net is trained to hallucinate features of the privileged modality, allowing the student to perform well even when that modality is absent during testing.

## Key Results
- Achieves 78.76% accuracy on pain estimation task using Biovid dataset
- Achieves 0.43 valence and 0.56 arousal scores on Affwild2 dataset for arousal-valence prediction
- Outperforms state-of-the-art privileged knowledge distillation methods across different modalities and fusion architectures

## Why This Works (Mechanism)
PKDOT works by leveraging the structural information in the teacher's representation space that is typically lost in traditional distillation methods. The cosine similarity matrices capture local relationships between samples, preserving the geometry of the data manifold. Entropy-regularized optimal transport then provides a principled way to align these structures between teacher and student networks, ensuring that the student learns not just individual predictions but the underlying data relationships. The hallucination component (T-Net) further enhances performance by generating plausible features for the missing modality, effectively bridging the gap between training and testing conditions.

## Foundational Learning
- **Optimal Transport**: A mathematical framework for comparing probability distributions, needed to align teacher and student representations while preserving local structures. Quick check: Verify that the transport plan maintains sample-level relationships.
- **Knowledge Distillation**: A model compression technique where a smaller network (student) learns from a larger pre-trained network (teacher). Quick check: Ensure student performance approaches teacher performance on available modalities.
- **Cosine Similarity Matrices**: Used to capture local geometric structures in the representation space. Quick check: Validate that similarity matrices preserve neighborhood relationships.
- **Privileged Information**: Data available only during training but not at test time. Quick check: Confirm that performance degrades gracefully when privileged information is removed.

## Architecture Onboarding

**Component Map**: Input Modalities → Teacher Network → Cosine Similarity Matrix → Optimal Transport → Student Network; T-Net (Hallucination) → Student Network

**Critical Path**: Input → Teacher → Similarity Matrix → Optimal Transport → Student → Output

**Design Tradeoffs**: 
- Balances between preserving local structures (through cosine similarity) and global alignment (through optimal transport)
- Uses hallucination to compensate for missing modalities but adds complexity and training time
- Employs entropy regularization to ensure smooth transport plans but may lose some fine-grained structure

**Failure Signatures**:
- Poor performance on unseen data distributions due to overfitting to privileged information
- Suboptimal hallucination leading to noisy or unrealistic feature generation
- Suboptimal transport plan alignment causing structural misalignment between teacher and student

**First Experiments**:
1. Test PKDOT on additional expression recognition datasets with different modalities to verify modality-agnostic claims
2. Evaluate performance degradation when privileged information is partially corrupted during training
3. Compare against state-of-the-art unimodal methods to quantify the actual benefit of multimodal training

## Open Questions the Paper Calls Out
None

## Limitations
- Performance evaluation is limited to two specific datasets (Biovid and Affwild2), raising questions about generalizability
- Reliance on privileged information during training may limit practical deployment scenarios
- Experimental validation of model-agnostic and modality-agnostic claims is limited to two fusion architectures and three modalities

## Confidence

**High Confidence**:
- Core methodology of using optimal transport for privileged knowledge distillation is technically sound

**Medium Confidence**:
- Experimental results showing superior performance over existing methods are promising but based on limited datasets
- Claims about model-agnostic and modality-agnostic properties are supported by experiments but require more extensive validation

## Next Checks
1. Test PKDOT on additional expression recognition datasets with different modalities to verify modality-agnostic claims and assess cross-dataset robustness
2. Evaluate the method's performance when privileged information is partially corrupted or noisy during training to assess real-world applicability
3. Compare PKDOT against state-of-the-art unimodal expression recognition methods to quantify the actual benefit of multimodal training when privileged information is unavailable