---
ver: rpa2
title: Multi-Agent Causal Discovery Using Large Language Models
arxiv_id: '2407.15073'
source_url: https://arxiv.org/abs/2407.15073
tags:
- causal
- graph
- discovery
- data
- metadata
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces the Multi-Agent Causal Discovery Framework
  (MAC), the first work to explore multi-agent LLMs for causal discovery. MAC addresses
  the limitations of traditional methods by integrating statistical approaches with
  multi-agent LLM reasoning.
---

# Multi-Agent Causal Discovery Using Large Language Models

## Quick Facts
- arXiv ID: 2407.15073
- Source URL: https://arxiv.org/abs/2407.15073
- Authors: Hao Duong Le; Xin Xia; Zhang Chen
- Reference count: 35
- Multi-agent LLM framework achieves state-of-the-art causal discovery performance

## Executive Summary
This paper introduces MAC (Multi-Agent Causal Discovery Framework), the first comprehensive approach to leverage multi-agent Large Language Models for causal discovery. The framework addresses key limitations of traditional statistical methods and single-LLM approaches by integrating debate-based reasoning with statistical causal discovery techniques. MAC demonstrates superior performance across five benchmark datasets, with particular success on the Earthquake dataset where it achieved perfect results. The approach represents a novel direction in causal discovery by combining LLM reasoning capabilities with established statistical methods.

## Method Summary
MAC employs a two-module architecture: the Debate-Coding Module (DCM) and the Meta-Debate Module (MDM). DCM selects and executes appropriate statistical causal discovery methods based on input data characteristics, while MDM uses a multi-agent debating framework to refine the causal structure through iterative reasoning and critique. The framework processes observational data through these modules sequentially, with the debate agents critically examining and improving initial causal hypotheses. The approach represents a significant methodological advancement by introducing collaborative LLM reasoning into the causal discovery pipeline.

## Key Results
- MAC using GPT-4o achieved best performance in 7 out of 15 evaluation points
- Perfect performance achieved on Earthquake dataset
- Outperformed traditional statistical causal discovery methods and existing LLM-based approaches
- Demonstrated effectiveness across five different benchmark datasets

## Why This Works (Mechanism)
MAC leverages the reasoning capabilities of LLMs through structured debate, allowing for more nuanced exploration of causal relationships than traditional methods. The multi-agent framework enables diverse perspectives on causal structures, while the integration with statistical methods ensures empirical grounding. The debating process helps identify and correct potential errors in causal hypotheses through critical examination by multiple agents.

## Foundational Learning

1. Causal Discovery Concepts
   - Why needed: Understanding fundamental causal inference principles
   - Quick check: Can distinguish between correlation and causation

2. Statistical Causal Discovery Methods
   - Why needed: Provides baseline techniques for comparison
   - Quick check: Familiarity with PC algorithm, FCI, and other standard methods

3. Multi-Agent Systems
   - Why needed: Core architectural principle of MAC
   - Quick check: Understanding of agent collaboration and debate frameworks

4. Large Language Model Capabilities
   - Why needed: Determines what tasks LLMs can effectively perform
   - Quick check: Knowledge of LLM limitations and strengths in reasoning tasks

## Architecture Onboarding

Component Map:
Input Data -> DCM -> Initial Causal Structure -> MDM -> Refined Causal Structure

Critical Path:
Data processing through DCM selection of statistical method, followed by MDM refinement through debate rounds

Design Tradeoffs:
- LLM reasoning vs. computational efficiency
- Number of debate agents vs. consensus quality
- Statistical method selection vs. framework flexibility

Failure Signatures:
- Inconsistent debate outcomes across runs
- Computational timeouts during agent deliberation
- Statistical method selection errors by DCM

First Experiments:
1. Test MAC on synthetic datasets with known causal structures
2. Compare single-agent vs. multi-agent debate performance
3. Evaluate different statistical method selections within DCM

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal number of debate rounds needed in the MDM module for achieving the best causal discovery results?
- Basis in paper: [inferred] The paper shows that increasing debate rounds from 1 to 5 does not consistently improve performance and can even worsen results on some datasets.
- Why unresolved: The experiments only tested up to 5 rounds, and the optimal number may vary depending on dataset characteristics or domain.
- What evidence would resolve it: Systematic experiments testing more debate rounds across diverse datasets, potentially including a theoretical analysis of debate convergence.

### Open Question 2
- Question: How does MAC perform when applied to interventional data compared to purely observational data?
- Basis in paper: [explicit] The paper explicitly states that MAC relies solely on observational data and lacks mechanisms for interventional validation.
- Why unresolved: The current framework has not been tested or extended to handle interventional scenarios.
- What evidence would resolve it: Experiments comparing MAC's performance on datasets with known interventional data against its observational-only performance.

### Open Question 3
- Question: What is the computational overhead of MAC compared to traditional statistical causal discovery methods, and how does it scale with dataset size?
- Basis in paper: [explicit] The paper mentions computational overhead as a major challenge, noting that the multi-agent debating process requires high processing power.
- Why unresolved: The paper does not provide detailed computational complexity analysis or scaling experiments.
- What evidence would resolve it: Empirical studies measuring runtime and resource usage of MAC versus traditional methods across datasets of varying sizes and complexities.

## Limitations
- Limited evaluation metrics (15 total across 5 datasets) may not fully capture performance
- Potential computational overhead from multi-agent debating process
- Current framework limited to observational data without interventional validation mechanisms

## Confidence

| Claim | Confidence Level |
|-------|------------------|
| Technical feasibility of multi-agent framework | High |
| Comparative performance claims | Medium |
| Practical applicability to real-world problems | Medium |

## Next Checks

1. Conduct comprehensive ablation study to isolate contributions of DCM and MDM modules
2. Validate framework performance on real-world datasets with known causal structures
3. Test framework robustness to noise and missing data, and scalability to high-dimensional problems