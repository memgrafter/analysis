---
ver: rpa2
title: RelayAttention for Efficient Large Language Model Serving with Long System
  Prompts
arxiv_id: '2402.14808'
source_url: https://arxiv.org/abs/2402.14808
tags:
- system
- prompt
- attention
- relayattention
- length
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the inefficiency of serving large language
  models (LLMs) with long system prompts, which causes throughput and latency bottlenecks
  due to redundant memory accesses. The authors propose RelayAttention, an attention
  algorithm that eliminates these redundancies by reading cached hidden states of
  system prompts from DRAM exactly once per batch.
---

# RelayAttention for Efficient Large Language Model Serving with Long System Prompts

## Quick Facts
- arXiv ID: 2402.14808
- Source URL: https://arxiv.org/abs/2402.14808
- Authors: Lei Zhu; Xinjiang Wang; Wayne Zhang; Rynson W. H. Lau
- Reference count: 13
- Key outcome: Up to 2.2× higher sustainable request rate and 2.0× throughput for chatbot workloads with long system prompts

## Executive Summary
This paper addresses the inefficiency of serving large language models (LLMs) with long system prompts, which causes throughput and latency bottlenecks due to redundant memory accesses. The authors propose RelayAttention, an attention algorithm that eliminates these redundancies by reading cached hidden states of system prompts from DRAM exactly once per batch. RelayAttention is based on a mathematical reformulation of causal attention and maintains generation quality without requiring model retraining. Experimental results show significant improvements when integrating RelayAttention into the vLLM serving system.

## Method Summary
RelayAttention eliminates redundant memory accesses by reformulating causal attention into a convex combination of two scaled attention terms—one for system prompts and one for context. The algorithm reads cached key-value pairs of system prompts from DRAM exactly once per batch, reducing the number of memory transfers from O(N²) to O(N) where N is the system prompt length. This mathematical reformulation preserves the exact same output distribution as standard causal attention while significantly reducing memory bandwidth requirements. The method requires a separate system KV cache and modifies the attention interface to return log-sum-exp values.

## Key Results
- Achieves up to 2.2× higher sustainable request rate and 2.0× throughput for chatbot workloads
- Improvements are more pronounced with longer system prompts (64→2048 tokens)
- Maintains generation quality without model retraining or architectural modifications

## Why This Works (Mechanism)

### Mechanism 1
- Claim: RelayAttention eliminates redundant memory accesses by reading cached key-value pairs of system prompts exactly once per batch.
- Mechanism: The algorithm reformulates causal attention into a convex combination of two scaled attention terms—one for system prompt and one for context—allowing system key-value pairs to be accessed only once.
- Core assumption: System prompts are shared across all requests in a batch and their key-value pairs are static during inference.
- Evidence anchors:
  - [abstract] "RelayAttention, an attention algorithm that allows reading these hidden states from DRAM exactly once for a batch of input tokens."
  - [section] "The key idea of RelayAttention is to group the matrix-vector multiplications corresponding to the system prompt into matrix-matrix multiplications, which allow loading the hidden states of the system prompt from DRAM exactly once for all request tokens in a batch."
- Break condition: If system prompts vary per request or are updated during inference, the static cache assumption fails.

### Mechanism 2
- Claim: RelayAttention maintains generation quality without model retraining.
- Mechanism: It uses a mathematical reformulation of causal attention, not a modification of model weights or architecture.
- Core assumption: The reformulation preserves the exact same output distribution as standard causal attention.
- Evidence anchors:
  - [abstract] "RelayAttention is a free lunch: it maintains the generation quality while requiring no model retraining, as it is based on a mathematical reformulation of causal attention."
  - [section] "As a result, the attention inference latency grows much slower than PagedAttention w.r.t. the length of system prompt."
- Break condition: If the convex combination coefficients are computed with numerical instability or approximation errors.

### Mechanism 3
- Claim: RelayAttention scales performance gains with longer system prompts.
- Mechanism: The reduction in redundant memory accesses becomes more significant as system prompt length increases relative to context length.
- Core assumption: Memory bandwidth, not compute, is the bottleneck in causal attention with cached key-value pairs.
- Evidence anchors:
  - [abstract] "The improvements are even more profound with longer system prompts."
  - [section] "Specifically, while the system prompt is shared by all requests, its hidden states (i.e., key-value pairs) of system prompts are transferred from off-chip DRAM to on-chip SRAM multiple times, each corresponding to an individual request."
- Break condition: When context length dominates system prompt length, the benefit diminishes.

## Foundational Learning

- Concept: Memory-bound vs compute-bound operations
  - Why needed here: Understanding why redundant memory access is the primary bottleneck in causal attention.
  - Quick check question: Given an operator with arithmetic intensity I and ratio r = I × tc / tm, what condition makes it memory-bound?

- Concept: Key-value cache mechanism in transformers
  - Why needed here: To understand how cached hidden states are reused and why they can be shared across requests.
  - Quick check question: In causal attention with KV cache, which parts of the computation are cached and reused across time steps?

- Concept: Convex combination of attention outputs
  - Why needed here: To grasp how RelayAttention mathematically reformulates attention without changing model behavior.
  - Quick check question: How does the convex combination in RelayAttention preserve the same attention distribution as standard causal attention?

## Architecture Onboarding

- Component map: System attention -> Context attention -> Relay fusion
- Critical path: System attention and context attention can be computed in parallel, then fused via relay fusion
- Design tradeoffs: Separate system KV cache increases memory overhead but eliminates redundant accesses; convex combination adds small computation overhead but enables significant memory savings
- Failure signatures: Degraded throughput with short system prompts, increased latency with varying system prompts, numerical instability in log-sum-exp computation
- First 3 experiments:
  1. Benchmark standalone attention latency with and without RelayAttention for varying system prompt lengths
  2. Measure memory bandwidth utilization during attention computation with standard vs RelayAttention
  3. Test generation quality (e.g., perplexity) to verify no degradation from the mathematical reformulation

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the theoretical limit of RelayAttention's speedup in practical LLM serving scenarios with varying batch sizes and sequence lengths?
- Basis in paper: [explicit] The paper provides a theoretical speedup formula (Eq. 10) and mentions that speedup increases with larger batch sizes, but doesn't explore practical limits or how speedup varies with different sequence lengths and batch sizes in real-world scenarios.
- Why unresolved: The paper focuses on demonstrating significant improvements over existing methods but doesn't provide a comprehensive analysis of the speedup limits under various practical conditions.
- What evidence would resolve it: Empirical results showing RelayAttention's performance across a wide range of batch sizes, sequence lengths, and different LLM architectures would help establish practical speedup limits.

### Open Question 2
- Question: How does RelayAttention perform with different types of system prompts, such as those containing multiple knowledge documents versus single-instruction prompts?
- Basis in paper: [inferred] The paper discusses system prompts that may include instructions, examples, and knowledge documents, but doesn't explicitly test performance variations based on different types of system prompts.
- Why unresolved: While the paper shows consistent improvements across various models and datasets, it doesn't explore how the complexity or type of system prompt content affects RelayAttention's efficiency gains.
- What evidence would resolve it: Benchmarking RelayAttention with different categories of system prompts (e.g., knowledge-heavy vs. instruction-heavy) would reveal if certain types of prompts yield better performance improvements.

### Open Question 3
- Question: What are the potential bottlenecks when integrating RelayAttention with other LLM optimization techniques like speculative decoding or quantization?
- Basis in paper: [explicit] The paper mentions that RelayAttention can work together with other techniques like speculative sampling and quantization, but doesn't explore potential interactions or bottlenecks that might arise from such integrations.
- Why unresolved: The paper focuses on RelayAttention's standalone benefits and integration with vLLM, but doesn't investigate how it performs when combined with other optimization strategies.
- What evidence would resolve it: Performance benchmarks and analysis of RelayAttention when integrated with other optimization techniques would identify any potential bottlenecks or synergistic effects.

### Open Question 4
- Question: How does RelayAttention scale when hosting multiple applications with different system prompts in a single serving process?
- Basis in paper: [explicit] The paper mentions that hosting multiple applications requires engineering efforts and suggests tagging requests with application IDs, but doesn't provide any analysis of the scalability or performance implications of such a setup.
- Why unresolved: While the paper addresses single-application scenarios well, it only briefly mentions the multi-application case without exploring its practical implications or limitations.
- What evidence would resolve it: Performance metrics and analysis of RelayAttention in a multi-application hosting scenario would reveal its scalability limits and any potential performance trade-offs.

## Limitations

- The static system prompt assumption may not hold for dynamic prompt scenarios or multi-tenant serving environments
- Numerical stability of log-sum-exp computation could affect generation quality in edge cases
- Relative benefits might diminish for future hardware generations with improved memory bandwidth

## Confidence

**High Confidence**: The throughput and latency improvements demonstrated in the experimental results are well-supported by the fundamental memory access reduction mechanism.

**Medium Confidence**: The claim that generation quality is fully preserved without retraining relies on the assumption that the convex combination exactly replicates standard attention behavior.

**Low Confidence**: The scalability analysis with longer system prompts assumes memory bandwidth remains the primary bottleneck.

## Next Checks

1. Conduct stress tests with extreme system prompt values and edge cases to verify the numerical stability of the log-sum-exp computation across different GPU architectures and precision settings.

2. Test RelayAttention in scenarios where system prompts vary between requests or are updated during inference to identify performance degradation points and establish operational boundaries.

3. Evaluate generation quality beyond perplexity metrics by testing on diverse benchmarks including creative writing, code generation, and factual question answering to ensure no subtle quality regressions.