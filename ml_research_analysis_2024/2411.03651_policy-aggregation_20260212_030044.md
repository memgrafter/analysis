---
ver: rpa2
title: Policy Aggregation
arxiv_id: '2411.03651'
source_url: https://arxiv.org/abs/2411.03651
tags:
- policy
- agents
- agent
- return
- borda
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper formalizes the problem of aligning AI models with multiple
  individuals as policy aggregation, where the goal is to identify a collective policy
  that respects each agent's values represented by different reward functions in a
  shared Markov decision process. The key insight is that social choice methods can
  be reinterpreted by identifying ordinal preferences with volumes of subsets of the
  state-action occupancy polytope.
---

# Policy Aggregation

## Quick Facts
- arXiv ID: 2411.03651
- Source URL: https://arxiv.org/abs/2411.03651
- Reference count: 40
- One-line primary result: This paper formalizes AI alignment with multiple individuals as policy aggregation using volumetric interpretation of state-action occupancy polytope

## Executive Summary
This paper introduces policy aggregation as a framework for aligning AI systems with multiple individuals who have different reward functions in a shared Markov decision process. The authors bridge social choice theory and multi-objective MDPs by reinterpreting ordinal preferences as volumes of subsets within the state-action occupancy polytope. They develop new fairness notions (quantile fairness and proportional veto core) and voting rules (α-approval voting and Borda count) adapted to this volumetric setting. The work provides theoretical guarantees showing that quantile fairness achieves stronger fairness outcomes than previously known methods, along with efficient algorithms for computing fair policies.

## Method Summary
The authors formalize policy aggregation in multi-objective MDPs where multiple agents with different reward functions must have their optimal policies aggregated into a collective policy. They employ a volumetric interpretation of the state-action occupancy polytope to extend social choice methods to this setting. The approach involves sampling random policies to compute expected returns for all agents, fitting cumulative distribution functions (CDFs) for each agent's expected return, and implementing mixed-integer linear programming (MILP) formulations for α-approval and Borda count rules. For quantile fairness, they develop binary search algorithms to find policies that satisfy fairness constraints across the distribution of possible outcomes.

## Key Results
- Quantile fairness yields stronger fairness guarantees than previously known methods in discrete settings
- Max-quantile and Borda count rules produce fairer outcomes compared to utilitarian and egalitarian approaches on Gini index metrics
- MILP formulations provide exact solutions for α-approval and Borda count rules while remaining computationally tractable for moderate problem sizes
- Experimental results on dynamic attention allocation demonstrate significant improvements in fairness metrics

## Why This Works (Mechanism)
The volumetric interpretation creates a bridge between social choice theory and MDPs by mapping ordinal preferences to volumes within the state-action occupancy polytope. This enables the application of voting rules and fairness notions that were previously confined to discrete preference settings. By treating policies as points in this continuous space and using distributional fairness criteria, the framework can capture nuanced trade-offs between agents' utilities rather than relying on point-wise comparisons.

## Foundational Learning
- State-action occupancy polytope: The convex hull of all possible state-action distributions achievable by policies in an MDP; needed because it provides the continuous space where policies can be compared and aggregated
- Quick check: Verify that the polytope is non-empty and bounded for the given MDP structure

- Cumulative distribution functions for expected returns: Fitted distributions showing how likely each agent is to achieve different utility levels under random policies; needed because they enable quantile-based fairness definitions
- Quick check: Confirm that fitted CDFs are monotonically increasing and bounded between 0 and 1

- Mixed-integer linear programming for social choice: MILP formulations that encode voting rules as optimization problems; needed because they provide exact computational methods for finding fair policies
- Quick check: Validate that the MILP formulations correctly encode the intended voting rules through small-scale test cases

## Architecture Onboarding
- Component map: MDP environment -> State-action occupancy polytope -> Volumetric fairness functions -> MILP/Binary search optimization -> Aggregated policy
- Critical path: Compute occupancy polytope volumes → Evaluate fairness metrics → Solve optimization → Select aggregated policy
- Design tradeoffs: Exact MILP solutions vs computational tractability; volumetric approximation vs discrete preference methods; theoretical guarantees vs practical implementation complexity
- Failure signatures: Solver timeouts for large problems; poor CDF fitting leading to suboptimal policy selection; approximation errors in volumetric calculations
- First experiments: 1) Implement and validate the dynamic attention allocation environment with synthetic data, 2) Test MILP formulations on small MDPs to verify correctness, 3) Compare quantile fairness against traditional utilitarian/egalitarian approaches on benchmark problems

## Open Questions the Paper Calls Out
### Open Question 1
- Question: What is the computational complexity of finding the exact Borda count winner in policy aggregation?
- Basis in paper: [explicit] The paper states that the computational complexity of the Borda count rule remains an interesting open question, though they make progress on two fronts (approximation algorithms and MILP formulations)
- Why unresolved: While the paper provides approximation methods and a MILP formulation, it does not establish whether finding the exact Borda count winner is NP-hard or in P
- What evidence would resolve it: A formal proof showing either that computing the exact Borda count winner is NP-hard (through a reduction from a known NP-hard problem) or that it can be solved in polynomial time

### Open Question 2
- Question: Can the volumetric approach be extended to continuous state or action spaces in MDPs?
- Basis in paper: [inferred] The paper mentions this as a limitation, stating "an interesting future direction is to apply these rules within continuous state or action spaces"
- Why unresolved: The current work focuses on discrete MDPs where the state-action occupancy polytope is well-defined. Extending this to continuous spaces would require different mathematical tools and definitions
- What evidence would resolve it: A formal extension of the volumetric interpretation and fairness notions to continuous MDPs, along with algorithms for computing fair policies in such settings

### Open Question 3
- Question: How does the volumetric approach behave under strategic manipulation when reward functions are learned through inverse reinforcement learning?
- Basis in paper: [explicit] The paper discusses this as a potential limitation, noting that "if reward functions are obtained through inverse reinforcement learning, successful manipulation would be difficult"
- Why unresolved: While the paper conjectures that strategic behavior would be difficult, it does not provide a formal analysis of the incentive compatibility of the volumetric approach under IRL
- What evidence would resolve it: A formal game-theoretic analysis showing either that the volumetric approach is incentive compatible under IRL (no agent can benefit from misreporting their behavior) or identifying conditions under which manipulation is possible

## Limitations
- The framework relies heavily on well-behaved continuous distributions for agent preferences, which may not hold in all real-world scenarios
- Experimental validation is limited to a single synthetic dynamic attention allocation environment rather than diverse real-world multi-agent settings
- Practical implementation details for computing occupancy polytope volumes in high-dimensional spaces remain underdeveloped
- Computational scalability of exact MILP formulations for larger MOMDPs is not fully characterized

## Confidence
- High: The mathematical formalization of policy aggregation as a social choice problem and the theoretical guarantees for quantile fairness
- Medium: The empirical results showing improved fairness metrics for max-quantile and Borda count rules compared to traditional approaches
- Medium: The MILP formulations for α-approval and Borda count rules, though computational scalability needs further validation

## Next Checks
1. Test the aggregation algorithms on more complex, real-world multi-agent environments with heterogeneous reward structures beyond the synthetic warehouse allocation task
2. Conduct ablation studies to verify that the volumetric interpretation of ordinal preferences is essential to the performance gains, not just a theoretical construct
3. Implement and validate the piecewise linear approximation method for concave Fi functions in Borda count to ensure numerical stability and solution quality