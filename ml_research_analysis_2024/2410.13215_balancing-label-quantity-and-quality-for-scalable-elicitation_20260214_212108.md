---
ver: rpa2
title: Balancing Label Quantity and Quality for Scalable Elicitation
arxiv_id: '2410.13215'
source_url: https://arxiv.org/abs/2410.13215
tags:
- weak
- labels
- label
- arxiv
- high-quality
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper addresses the problem of balancing label quantity and
  quality when training language models, particularly in domains where high-quality
  labels are expensive or difficult to obtain. The authors explore three regimes of
  eliciting classification knowledge from pretrained models: quantity-dominant (when
  only cheap low-quality labels are affordable), quality-dominant (when enough high-quality
  labels can be afforded), and a mixed regime where combining both types of labels
  yields optimal performance.'
---

# Balancing Label Quantity and Quality for Scalable Elicitation

## Quick Facts
- arXiv ID: 2410.13215
- Source URL: https://arxiv.org/abs/2410.13215
- Reference count: 38
- One-line primary result: Using a combination of low-quality and high-quality labels can improve accuracy by up to 5 percentage points at a fixed labeling budget compared to using either type alone.

## Executive Summary
This paper addresses the fundamental challenge of balancing label quantity and quality when training language models for classification tasks. The authors propose a systematic approach to scalable elicitation that identifies three distinct regimes based on available budget: quantity-dominant (only cheap low-quality labels affordable), quality-dominant (enough high-quality labels can be afforded), and a mixed regime where combining both types yields optimal performance. Through extensive experiments across multiple datasets and model scales, they demonstrate that sequential supervised fine-tuning—first on weak labels then on high-quality labels—combined with uncertainty sampling and few-shot prompting can achieve Pareto-optimal trade-offs between labeling cost and classifier accuracy.

## Method Summary
The core method involves sequential supervised fine-tuning (SFT) where models are first trained on low-quality labels generated by finetuned small language models, then refined on high-quality human-labeled examples. The approach includes variants like few-shot prompting with in-context examples and uncertainty sampling to select the most informative high-quality examples. A log-confidence auxiliary loss is used during the weak label training stage to improve calibration. The method is evaluated across multiple binary NLP classification tasks (BoolQ, Hellaswag, SciQ, GLUE Cola, CosmosQA, QuAIL, SocialIQA, MMLU) with varying budget levels ($16-$4096) and different cost assumptions for weak ($0.01-$0.50) versus high-quality ($1) labels. The results establish a Pareto frontier of scalable elicitation methods that optimally trade off labeling cost and classifier performance.

## Key Results
- Using a combination of low-quality and high-quality labels can improve accuracy by up to 5 percentage points at a fixed labeling budget compared to using either type alone
- The optimal allocation of labels depends on the budget size, with mixed strategies performing best for moderate budgets
- Adding few-shot prompts can make SFT more sample-efficient
- The three regimes (quantity-dominant, quality-dominant, mixed) persist across different model scales, with larger models requiring fewer examples to achieve high accuracy

## Why This Works (Mechanism)
Assumption: The sequential fine-tuning approach works because initial training on weak labels provides a calibration baseline that improves the model's ability to learn from high-quality examples. The log-confidence auxiliary loss during weak training helps the model develop better uncertainty estimates, making the uncertainty sampling in the high-quality stage more effective. Larger models require fewer examples because they have more representational capacity to extract information from each training instance.

## Foundational Learning
Assumption: This work builds on foundational concepts in semi-supervised learning and active learning, where weak supervision provides coverage while high-quality labels provide precision. The Pareto-optimal trade-off analysis draws from economic theory applied to machine learning resource allocation. The sequential fine-tuning methodology relates to curriculum learning principles where easier (weak) examples precede harder (high-quality) ones.

## Architecture Onboarding
### Component Map
Sequential SFT -> Weak Label Generation -> High-Quality Label Integration -> Pareto Frontier Analysis

### Critical Path
1. Generate weak labels using small finetuned LM on 8k ground-truth examples
2. Sequentially fine-tune with LoRA: weak labels first, then high-quality labels
3. Apply uncertainty sampling to select most informative high-quality examples
4. Establish Pareto frontier by testing across multiple budget levels

### Design Tradeoffs
- Weak vs high-quality labels: Cost vs accuracy trade-off
- Sequential training order: Weak first improves calibration for high-quality stage
- Budget allocation: Mixed strategy optimal for moderate budgets
- Model scale: Larger models more sample-efficient but increase compute costs

### Failure Signatures
- Poor few-shot prompting performance: Check context formatting and class label consistency
- Uncertainty sampling ineffective: Verify model calibration after weak training
- Weak labels not improving results: Check temperature and finetuning hyperparameters

### 3 First Experiments
1. Replicate sequential SFT on BoolQ with $64 budget to verify 5-point accuracy improvement
2. Test few-shot prompting variants with in-context examples to confirm sample efficiency gains
3. Implement uncertainty sampling and compare against random selection on validation set

## Open Questions the Paper Calls Out
Assumption: The paper does not explicitly call out open questions in the text, but implicit questions remain about the optimal temperature for weak label generation, the impact of different weak label source architectures, and whether the three-regime framework generalizes to multi-class classification beyond the binary tasks studied.

## Limitations
- Weak label generation details are underspecified (architecture, hyperparameters, temperature)
- Log-confidence auxiliary loss implementation details are incomplete
- Dataset binarization process is not described, potentially affecting results
- Numerical improvements may vary due to unspecified implementation details

## Confidence
- High Confidence: Core methodology of sequential SFT and Pareto frontier establishment
- Medium Confidence: Specific results on optimal label allocation at different budget levels
- Low Confidence: Exact numerical improvements (e.g., "up to 5 percentage points")

## Next Checks
1. Reproduce weak label generation using specified Qwen-1.5 0.5B finetuning on 8,000 examples and verify label quality consistency
2. Implement sequential SFT with LoRA and log-confidence auxiliary loss on BoolQ subset to verify claimed accuracy improvements
3. Test budget-level optimizations ($16-$4096) to validate optimal label allocation between weak and high-quality examples