---
ver: rpa2
title: 'Supporting Student Decisions on Learning Recommendations: An LLM-Based Chatbot
  with Knowledge Graph Contextualization for Conversational Explainability and Mentoring'
arxiv_id: '2401.08517'
source_url: https://arxiv.org/abs/2401.08517
tags:
- chatbot
- user
- learning
- information
- context
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper presents an LLM-based chatbot for providing conversational
  explanations of learning recommendations, addressing the need for students to understand
  why recommendations are made and how to modify them. The approach uses a knowledge
  graph (KG) to contextualize the LLM''s prompt, defining four categories of information:
  roles, definitions, rules, and additional content.'
---

# Supporting Student Decisions on Learning Recommendations: An LLM-Based Chatbot with Knowledge Graph Contextualization for Conversational Explainability and Mentoring

## Quick Facts
- arXiv ID: 2401.08517
- Source URL: https://arxiv.org/abs/2401.08517
- Reference count: 14
- Primary result: LLM-based chatbot with KG contextualization achieves 88% intent classification accuracy and high user satisfaction (4.7/5 design, 4.4/5 answer quality, 4.6/5 speed)

## Executive Summary
This paper presents an LLM-based chatbot designed to provide conversational explanations for learning recommendations, addressing the critical need for students to understand why specific learning materials are suggested and how to modify recommendations. The system uses a knowledge graph to contextualize LLM prompts across four categories: roles, definitions, rules, and additional content, while implementing intent classification with re-prompting and human mentor fallback. A user study with 9 participants demonstrated high satisfaction with design (4.7/5), answer quality (4.4/5), and response speed (4.6/5), though the system requires improvement for handling out-of-scope queries.

## Method Summary
The system implements a dialogue manager that limits chatbot tasks to seven predefined intents, using intent classification to route queries appropriately. When queries fall outside scope, the system re-prompts users up to three times before suggesting connection to human mentors. Knowledge graph contextualization provides structured educational content about learning materials and their relationships, integrated into LLM prompts to improve accuracy and reduce hallucination. The architecture includes a group chat feature connecting students with mentors, where the chatbot acts as a mediator that can be @-mentioned for explanations. Evaluation involved intent classification accuracy testing on 182 labeled user requests and a user study with 9 participants using 8 test scenarios.

## Key Results
- Intent classification accuracy of 88% across 7 intent categories
- User satisfaction scores: design (4.7/5), answer quality (4.4/5), speed (4.6/5)
- Effective handling of in-scope questions with clear escalation paths for out-of-scope queries
- Knowledge graph contextualization improves LLM response relevance and accuracy

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Knowledge Graph contextualization improves LLM prompt relevance and reduces hallucination risk
- Mechanism: KG provides structured, human-curated information about learning materials and relationships, integrated into LLM prompts through roles, definitions, rules, and additional content categories
- Core assumption: Structured knowledge from KG is more reliable than unstructured text and effectively constrains LLM behavior
- Evidence anchors: KG represents human-curated data structure providing comprehensive learning material information; KG-LLM accuracy improvements reported in literature
- Break condition: If KG data is incomplete, outdated, or poorly aligned with curriculum, contextualization may introduce noise

### Mechanism 2
- Claim: Intent classification with re-prompting and fallback to human mentors improves user satisfaction and system reliability
- Mechanism: System limits chatbot to 7 predefined intents; out-of-scope queries trigger re-prompting up to 3 times, then suggest mentor connection
- Core assumption: Restricting LLM to well-defined tasks and providing clear escalation paths reduces user frustration
- Evidence anchors: Dialogue manager requests rephrasing for vague queries; suggests mentor support after 3 failed re-prompts
- Break condition: If re-prompting cycles are too long or mentors are unavailable, user experience degrades

### Mechanism 3
- Claim: Group chat architecture with chatbot as mediator enables hybrid human-AI support while maintaining educational quality
- Mechanism: Sessions connect students with mentors where both can chat directly and @-mention chatbot for explanations
- Core assumption: Combining LLM explanations with human mentorship provides better outcomes than either alone
- Evidence anchors: Group chat approach connects students with human mentors on demand or when chatbot reaches task limits
- Break condition: If chatbot responses are too frequent or disruptive, they may interfere with natural mentor-student dialogue flow

## Foundational Learning

- Concept: Knowledge Graph structure and querying
  - Why needed here: Essential for designing effective contextualization strategies and debugging retrieval issues
  - Quick check question: How would you represent a course-to-topic relationship in a KG triple format?

- Concept: Intent classification and dialogue state management
  - Why needed here: Chatbot relies on accurate intent classification to route queries correctly and manage conversation states
  - Quick check question: What happens when a user query matches multiple intent classes with similar confidence scores?

- Concept: LLM prompt engineering and context management
  - Why needed here: Effective contextualization requires balancing prompt size limits with sufficient information to guide LLM responses
  - Quick check question: How do you determine the optimal amount of KG context to include without exceeding LLM context window limits?

## Architecture Onboarding

- Component map: Web Application (Angular) -> Chatbot Module -> Knowledge Graph Database -> Recommendation System -> LLM API (GPT-4) -> Session Manager -> Mentor Registry
- Critical path: User query → Intent classifier → Dialogue manager → Prompt builder (KG + context) → LLM API → Response → Display
- Design tradeoffs:
  - Prompt size vs. LLM context limits: Larger context improves accuracy but risks truncation
  - Intent restriction vs. user flexibility: Narrow scope reduces errors but may frustrate users with unsupported queries
  - Human fallback vs. automation: Mentors improve quality but increase operational costs
- Failure signatures:
  - High "other" intent classification rate indicates scope definition issues
  - Slow response times suggest KG query optimization problems
  - Mentor unavailability breaks group chat functionality
- First 3 experiments:
  1. Test intent classifier accuracy on edge cases where user queries are ambiguous or span multiple categories
  2. Measure response quality difference with and without KG contextualization using human evaluation
  3. Simulate mentor unavailability to verify chatbot fallback mechanisms work correctly

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the long-term impact of conversational explainability chatbots on student learning outcomes compared to traditional textual explanations?
- Basis in paper: Authors state they are designing a larger-scale A/B test to compare conversation explainability against single-step textual explanation modality
- Why unresolved: Current study is only proof-of-concept with small sample size (n=9) and short duration (60-90 minutes)
- What evidence would resolve it: Longitudinal study with large, diverse student population comparing learning outcomes, retention rates, and knowledge transfer over extended period

### Open Question 2
- Question: How does the phrasing and structure of contextual information in LLM prompts affect the quality and relevance of generated explanations?
- Basis in paper: Authors acknowledge context phrasing may vary even with same information and requires another test for context phrasing effects
- Why unresolved: Current implementation uses different phrasing for each intent without systematic evaluation of impact on explanation quality
- What evidence would resolve it: Controlled experiments testing multiple phrasing variations of identical contextual information across different intents

### Open Question 3
- Question: What is the optimal balance between automated chatbot responses and human mentor intervention in educational settings?
- Basis in paper: Authors implement group chat system connecting students with human mentors as fallback strategy but express concerns about when human intervention adds value
- Why unresolved: Study doesn't systematically evaluate when human intervention is most beneficial or how to optimize transition between chatbot and mentor support
- What evidence would resolve it: Detailed analysis of chat transcripts categorizing question types and measuring learning outcomes based on intervention timing

## Limitations

- Small evaluation sample size (n=9) limits generalizability of user satisfaction findings
- Knowledge graph implementation details remain unspecified, making it difficult to assess completeness and accuracy
- System performance with diverse student populations and different educational domains has not been validated

## Confidence

- High confidence: Technical architecture description (intent classification, dialogue management, KG contextualization) is well-specified and implementable
- Medium confidence: User satisfaction metrics reported but based on small sample size, limiting generalizability
- Low confidence: Claims about KG's effectiveness in reducing hallucination and improving accuracy lack direct empirical validation

## Next Checks

1. Scale validation study: Test system with 50+ participants across multiple educational domains to verify satisfaction metrics hold at scale and identify emerging limitations
2. Controlled KG effectiveness test: Conduct A/B testing comparing LLM responses with identical prompts, one with KG contextualization and one without, measuring hallucination rates and accuracy
3. Edge case stress test: Systematically test intent classifier and re-prompting mechanisms with ambiguous queries that could belong to multiple intent categories to identify classification failure patterns and optimize threshold settings