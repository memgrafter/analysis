---
ver: rpa2
title: Span-Aggregatable, Contextualized Word Embeddings for Effective Phrase Mining
arxiv_id: '2405.07263'
source_url: https://arxiv.org/abs/2405.07263
tags:
- phrase
- context
- dataset
- embeddings
- word
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of phrase mining in noisy contexts
  where target phrases are embedded within longer sentences. The authors propose a
  novel method called SLICE (Span-aggregated Late Interaction Contextualized Embeddings)
  that produces contextualized word embeddings which can be aggregated for arbitrary
  word spans while maintaining the span's semantic meaning.
---

# Span-Aggregatable, Contextualized Word Embeddings for Effective Phrase Mining

## Quick Facts
- arXiv ID: 2405.07263
- Source URL: https://arxiv.org/abs/2405.07263
- Authors: Eyal Orbach; Lev Haikin; Nelly David; Avi Faizakof
- Reference count: 25
- Primary result: SLICE outperforms equivalent methods on STS-B-Context dataset without significant compute increase

## Executive Summary
This paper addresses the challenge of phrase mining in noisy contexts where target phrases are embedded within longer sentences. The authors propose SLICE (Span-aggregated Late Interaction Contextualized Embeddings), a method that produces contextualized word embeddings which can be aggregated for arbitrary word spans while maintaining semantic meaning. SLICE modifies the common contrastive loss to encourage word embeddings to have this span-aggregation property, and is trained on the noisy MS-MARCO dataset. The method is evaluated on a newly introduced STS-B-Context dataset and shows superior performance compared to equivalent methods with similar compute requirements.

## Method Summary
SLICE modifies the contrastive loss used in sentence embedding models to optimize for span-aggregatable word embeddings. The method trains on MS-MARCO triplets using BERT-base as the encoder, generating all possible n-grams between MIN_SIZE and MAX_SIZE in context passages. For each span, token embeddings are mean-pooled to create span representations. The loss function maximizes similarity between a query and its best-matching span in positive passages while minimizing similarity with spans in negative passages. During inference, the model computes similarities between the query and each possible span in the context separately, identifying the best matching span without being overwhelmed by irrelevant context.

## Key Results
- SLICE achieves Pearson correlation of 0.677 and Spearman correlation of 0.669 on STS-B-Context dataset
- Outperforms equivalent methods (BM25, GloVe, BERT, SBERT-nli, SBERT-msmarco, PhraseBERT, DensePhrases) of similar compute utilization
- Maintains performance advantage across MAX_SIZE values between 15-35, with advantage smaller when value is 10 or lower
- Single forward pass setup achieves competitive results while significantly reducing compute requirements

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Span-aggregatable embeddings maintain semantic meaning when mean-pooled over arbitrary spans
- Mechanism: Modified contrastive loss explicitly optimizes for span-aggregation property by maximizing similarity between query and best-matching span in positive passage while minimizing similarity with spans in negative passages
- Core assumption: Mean-pooling over contextualized token embeddings preserves semantic meaning of original span
- Evidence anchors: [abstract] "contextualized word embeddings that can be aggregated for arbitrary word spans while maintaining the span's semantic meaning"; [section] "we produce token embeddings by a single forward pass of the entire sentence, and aggregate them by mean-pooling, for each span between MIN_SIZE and MAX_SIZE"
- Break condition: If mean-pooling destroys semantic relationships in span (e.g., for spans with very different semantic structures), method fails

### Mechanism 2
- Claim: Training on MS-MARCO as noisy dataset still produces useful span representations
- Mechanism: Max-pooling over all possible spans in passage ensures model learns to identify and represent spans that are semantically similar to query
- Core assumption: MS-MARCO passages labeled as relevant contain at least one span highly similar to query
- Evidence anchors: [section] "many of the passages labeled as best match for a query, contain a single span with high similarity to the query"; [abstract] "training with this method, even on a noisy labeled dataset, produces a model that outperforms equivalent methods"
- Break condition: If MS-MARCO passages rarely contain spans similar to queries, training signal becomes too weak to learn useful representations

### Mechanism 3
- Claim: Late interaction between query and context spans is more effective than full-context encoding
- Mechanism: Computing similarities between query and each possible span in context separately allows model to identify best matching span without being overwhelmed by irrelevant context
- Core assumption: Semantic relationship between query and target span is independent of surrounding context
- Evidence anchors: [abstract] "when target phrases reside inside noisy context, representing the full sentence with a single dense vector, is not sufficient for effective phrase retrieval"; [section] "representing multiple, sub-sentence, consecutive word spans, each with its own dense vector. We show that this technique is much more effective for phrase mining"
- Break condition: If surrounding context is semantically crucial for understanding target span, ignoring it in encoding would degrade performance

## Foundational Learning

- Concept: Contrastive learning for sentence similarity
  - Why needed here: Paper builds on SBERT's approach but modifies it for span-level representations
  - Quick check question: How does the contrastive loss in SBERT differ from the loss proposed in SLICE?

- Concept: Token-level contextualized embeddings
  - Why needed here: Understanding how models like BERT produce different embeddings for same word in different contexts is crucial for grasping why span-aggregation works
  - Quick check question: What information does BERT use to generate contextualized embeddings for each token?

- Concept: Mean-pooling operations on embeddings
  - Why needed here: Effectiveness of method depends on mean-pooling preserving semantic information
  - Quick check question: When would mean-pooling of token embeddings fail to represent semantic meaning of a span?

## Architecture Onboarding

- Component map: Encoder (BERT-base) -> Token embeddings -> Span generation (all n-grams MIN_SIZE-MAX_SIZE) -> Mean-pooling -> Span representations -> Similarity computation -> Loss function

- Critical path:
  1. Forward pass through encoder to get token embeddings
  2. Generate all possible spans in context
  3. Mean-pool token embeddings for each span
  4. Compute max similarity between query and spans in positive/negative contexts
  5. Apply loss function and backpropagate

- Design tradeoffs:
  - Span size range (MIN_SIZE/MAX_SIZE) vs. computational cost
  - Using MS-MARCO (noisy but large) vs. cleaner but smaller datasets
  - Mean-pooling vs. other aggregation methods (max, attention)

- Failure signatures:
  - Poor performance on STS-B-Context despite good training loss
  - Degradation when context becomes too long or too short
  - Sensitivity to hyperparameter choices for span sizes

- First 3 experiments:
  1. Verify that mean-pooling token embeddings preserves span semantics on small handcrafted dataset
  2. Test modified loss function on simplified version with fixed span sizes
  3. Compare performance with and without max-pooling over spans during training

## Open Questions the Paper Calls Out

The paper identifies three main open questions:

1. How SLICE performs on real-world context rather than auto-generated context (paper uses GPT-3.5-Turbo generated context due to inability to share private data)

2. How choice of MAX_SIZE for spans affects performance across different types of phrases and contexts (authors experimented with values 10-35 but didn't provide comprehensive analysis)

3. How SLICE compares to other state-of-the-art models when trained on datasets other than MS-MARCO (authors only explored performance when trained on MS-MARCO)

## Limitations

- Reliance on mean-pooling may not preserve semantic meaning for all span types and contexts
- Performance depends on assumption that MS-MARCO passages contain at least one span highly similar to queries
- Evaluation uses auto-generated context rather than real-world data, raising questions about generalizability

## Confidence

**High Confidence Claims:**
- SLICE can be implemented as described and produces span representations that can be mean-pooled
- Method outperforms equivalent compute methods on STS-B-Context dataset as measured by Pearson and Spearman correlations
- Computational requirements are comparable to existing methods with single forward pass setup

**Medium Confidence Claims:**
- Span-aggregation property is universally useful for phrase mining tasks
- Training on noisy datasets like MS-MARCO produces representations as effective as training on cleaner data
- Modified contrastive loss is optimal approach for learning span-aggregatable embeddings

**Low Confidence Claims:**
- Mean-pooling preserves semantic meaning for all types of spans and contexts
- Improvements would generalize to other phrase mining tasks beyond STS-B-Context
- Method would maintain performance advantages with significantly longer contexts

## Next Checks

1. **Semantic preservation validation**: Create controlled test suite with handcrafted examples covering various span types (negations, contrasts, idiomatic expressions, technical terms) to systematically evaluate when mean-pooling preserves vs. destroys semantic meaning.

2. **Noise tolerance analysis**: Quantitatively measure proportion of MS-MARCO passages that contain truly relevant spans for their queries, and analyze how this affects training dynamics and final model performance across different noise levels.

3. **Context dependency study**: Design experiments that systematically vary context length and semantic relationship to target span, measuring performance degradation as context becomes more semantically crucial for understanding target phrase.