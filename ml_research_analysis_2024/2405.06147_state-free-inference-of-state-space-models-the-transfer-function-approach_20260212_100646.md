---
ver: rpa2
title: 'State-Free Inference of State-Space Models: The Transfer Function Approach'
arxiv_id: '2405.06147'
source_url: https://arxiv.org/abs/2405.06147
tags:
- inference
- function
- state-space
- transfer
- sequence
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Rational Transfer Functions (RTF), a state-space
  model (SSM) parametrization that enables state-free parallel inference, meaning
  its computational and memory costs do not scale with state size. This is achieved
  by casting SSM parameters onto sequence length via a single Fast Fourier Transform
  (FFT).
---

# State-Free Inference of State-Space Models: The Transfer Function Approach

## Quick Facts
- **arXiv ID:** 2405.06147
- **Source URL:** https://arxiv.org/abs/2405.06147
- **Reference count:** 40
- **Primary result:** RTF achieves 35% faster training speed than S4 on Long Range Arena while delivering state-of-the-art performance among attention-free models

## Executive Summary
This paper introduces Rational Transfer Functions (RTF), a state-space model parametrization that enables state-free parallel inference. By representing SSMs in the frequency domain and leveraging FFT computation, RTF eliminates the memory and computational costs that typically scale with state size. The approach achieves state-of-the-art performance on the Long Range Arena benchmark among attention-free models while providing 35% faster training than S4. RTF also demonstrates improved language modeling perplexity when replacing convolutional filters in a Hyena baseline.

## Method Summary
The RTF method parameterizes state-space models in the frequency domain as rational transfer functions, enabling direct computation of convolutional kernels via FFT. This eliminates the need to materialize states over time, achieving state-free inference where memory and computation don't scale with state size. The approach uses zero initialization and enforces Montel's constraint for training stability. RTF layers compute output sequences by evaluating the transfer function at roots of unity and applying inverse FFT, achieving O(ℓ) memory complexity regardless of state size.

## Key Results
- 35% faster training speed than S4 on Long Range Arena benchmark
- State-of-the-art performance among attention-free models on LRA
- Improved language modeling perplexity over convolutional Hyena baseline when replacing filters
- State-free inference with O(ℓ) memory complexity independent of state size

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** RTF enables state-free parallel inference by computing the convolutional kernel's spectrum via a single FFT
- **Mechanism:** The frequency-domain representation allows direct computation of the kernel's spectrum without explicit state evolution
- **Core assumption:** Frequency-domain representation enables direct kernel spectrum computation
- **Evidence anchors:** Abstract states state-free inference doesn't incur memory/computational cost with increased state size; section describes achieving this through frequency domain transfer function parametrization
- **Break condition:** FFT computation bottleneck for extremely long sequences or loss of expressiveness for certain signal types

### Mechanism 2
- **Claim:** RTF is coordinate-invariant, capturing complete functional space of any LTI SSM
- **Mechanism:** Transfer function parameters remain unchanged under invertible state-space coordinate changes
- **Core assumption:** Transfer function is complete system representation independent of coordinates
- **Evidence anchors:** Section states transfer function is invariant under state-space coordinate changes; claims any dense matrix representation can be described with 2n+1 parameters
- **Break condition:** Nonlinear behavior or time-varying characteristics beyond linear time-invariant models

### Mechanism 3
- **Claim:** Zero initialization with Montel's constraint provides better training stability
- **Mechanism:** Zero initialization positions parameters far from violating Montel's constraint (sum of absolute coefficients ≤ 1)
- **Core assumption:** Zero initialization provides stable starting point avoiding short-term bias
- **Evidence anchors:** Section proposes zero initialization to position coefficients as far as possible from violating Montel's constraint; ablation tests show enhanced stability
- **Break condition:** Zero initialization too conservative for tasks requiring immediate complex dynamics

## Foundational Learning

- **Concept:** Z-transform and frequency domain analysis
  - Why needed here: Core innovation relies on moving between time and frequency domains using Z-transform
  - Quick check question: What is the relationship between the Z-transform of an impulse response and the transfer function of an LTI system?

- **Concept:** State-space representation of LTI systems
  - Why needed here: Understanding state-space models represents convolutions through recurrence relations
  - Quick check question: How does the state-space representation (3) relate to the convolutional operation in (1)?

- **Concept:** Fast Fourier Transform (FFT) algorithm
  - Why needed here: Efficiency gains rely on FFT for parallel inference and kernel generation
  - Quick check question: What are the time and space complexities of FFT, and why is it suitable for this application?

## Architecture Onboarding

- **Component map:** Input → Encoder → RTF layer (FFT computation) → Activation → Output projection → Skip connection → Next layer
- **Critical path:** Input → Encoder → RTF layer (FFT computation) → Activation → Output projection → Skip connection → Next layer
- **Design tradeoffs:**
  - RTF vs diagonal SSMs: RTF offers better expressiveness but requires more careful initialization
  - State-free vs state-additive: RTF has O(ℓ) memory vs O(ℓ+n) for state-additive methods
  - Multi-SISO vs MIMO: RTF uses independent SISO filters per channel with post-mixing
- **Failure signatures:**
  - Training instability: Issues with initialization or Montel constraint enforcement
  - Poor performance on synthetic tasks: Insufficient model capacity or inappropriate state size
  - Memory issues: Unlikely with RTF due to state-free nature, but possible with very long sequences
- **First 3 experiments:**
  1. Implement RTF and compare against S4 on ListOps with n=64 to verify 35% speedup claim
  2. Test RTF with increasing state sizes (64, 128, 256, 512) on Retrieval task to confirm state-free memory scaling
  3. Apply RTF to new LRA task (e.g., Image classification) to verify consistent performance improvements

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does RTF performance scale with sequence length beyond 1024 to 16384?
- Basis in paper: Experiments conducted across sequence lengths 1024 to 16384, but no analysis beyond this range
- Why unresolved: Paper lacks data or analysis for sequence lengths outside tested range
- What evidence would resolve it: Experimental results showing RTF performance on significantly longer or shorter sequences

### Open Question 2
- Question: What is the impact of different state sizes on expressiveness and generalization in real-world applications?
- Basis in paper: RTF achieved state-of-the-art results on LRA tasks but failed on Path-X with n=64, requiring increase to 2048
- Why unresolved: Paper doesn't explore relationship between state size, expressiveness, and generalization in detail
- What evidence would resolve it: Empirical studies comparing RTF performance with varying state sizes on diverse real-world tasks

### Open Question 3
- Question: How does RTF compare to Transformers in computational efficiency and performance?
- Basis in paper: RTF achieved state-of-the-art performance on LRA and improved perplexity over Hyena baseline, but lacks direct Transformer comparison
- Why unresolved: Paper doesn't include comprehensive comparison with Transformers across tasks
- What evidence would resolve it: Experimental results comparing RTF and Transformer performance across various tasks with efficiency metrics

## Limitations
- Expressiveness vs. efficiency trade-off boundaries not thoroughly explored
- Zero initialization sensitivity and comparison to other strategies not comprehensively analyzed
- Scalability to extremely long sequences and potential FFT bottlenecks not adequately addressed

## Confidence

**High Confidence:** Core algorithmic contribution of using FFT for state-free inference is mathematically sound with well-documented experimental speedup results.

**Medium Confidence:** Claims about expressiveness equivalence with dense SSMs are theoretically plausible but need more empirical validation; training stability improvements from zero initialization are demonstrated but not comprehensively analyzed.

**Low Confidence:** Cross-domain applicability claims based on limited experiments; theoretical guarantees about coordinate invariance don't fully address practical considerations.

## Next Checks

1. **Expressiveness Boundary Testing:** Systematically test RTF's ability to represent increasingly complex impulse responses compared to dense SSMs across multiple signal types, measuring both accuracy and parameter efficiency.

2. **Initialization Ablation Study:** Conduct comprehensive study comparing zero initialization against other strategies (random, He, Xavier) across different state sizes and tasks to quantify claimed stability benefits.

3. **Scaling Analysis:** Evaluate RTF's performance and computational characteristics on sequences of increasing length (10K, 100K, 1M tokens) to identify potential FFT bottlenecks and numerical stability issues at scale.