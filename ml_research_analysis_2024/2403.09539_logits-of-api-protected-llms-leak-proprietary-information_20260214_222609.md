---
ver: rpa2
title: Logits of API-Protected LLMs Leak Proprietary Information
arxiv_id: '2403.09539'
source_url: https://arxiv.org/abs/2403.09539
tags:
- outputs
- output
- image
- full
- bmax
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper demonstrates that it is possible to learn significant
  proprietary information about large language models (LLMs) from their API outputs,
  despite limited access. The key observation is that LLM outputs are constrained
  to a low-dimensional subspace due to the softmax bottleneck.
---

# Logits of API-Protected LLMs Leak Proprietary Information

## Quick Facts
- arXiv ID: 2403.09539
- Source URL: https://arxiv.org/abs/2403.09539
- Authors: Matthew Finlayson; Xiang Ren; Swabha Swayamdipta
- Reference count: 14
- Key outcome: API outputs from LLMs leak significant proprietary information due to softmax bottleneck constraints, enabling embedding size discovery, model identification, update detection, and full vocabulary recovery.

## Executive Summary
This paper demonstrates that API-protected LLMs leak significant proprietary information through their outputs due to a fundamental softmax bottleneck. The key insight is that LLM outputs are constrained to a low-dimensional linear subspace, which enables attackers to efficiently discover model properties like embedding size, identify the specific LLM producing outputs, detect model updates, and recover full vocabulary outputs with far fewer API calls than previously thought possible. The authors estimate OpenAI's gpt-3.5-turbo has an embedding size of approximately 4096 using their method.

## Method Summary
The authors exploit the softmax bottleneck constraint that forces LLM outputs into a d-dimensional subspace of the full vocabulary space. By collecting d linearly independent outputs from the API and solving for unbiased probabilities using biased token queries, they can efficiently recover the LLM's embedding size and full vocabulary outputs. The method leverages the fact that different LLMs have largely disjoint output spaces, making their "images" unique signatures for model identification and update detection.

## Key Results
- Successfully estimated gpt-3.5-turbo's embedding size as approximately 4096
- Demonstrated full vocabulary recovery from API calls in O(d) time instead of O(v)
- Showed LLM outputs can be used as unique signatures to identify source models with high accuracy
- Proved the method can detect model updates between different checkpoints

## Why This Works (Mechanism)

### Mechanism 1
The LLM output probability distributions are constrained to a low-dimensional linear subspace due to the softmax bottleneck. The softmax matrix W, which projects embeddings from Rd to Rv, has rank at most d, meaning all LLM outputs lie within a d-dimensional subspace of the full output space.

### Mechanism 2
The low-rank output space allows efficient recovery of full vocabulary outputs from API responses. By adding bias to specific tokens, we can isolate and solve for unbiased probabilities of tokens, reducing the number of calls from O(v) to O(d).

### Mechanism 3
The LLM's image acts as a unique signature, allowing identification of the source model and detection of updates. Different LLMs have largely disjoint output spaces, and comparing residuals of least-squares solutions across models can identify which model produced the output.

## Foundational Learning

- **Linear algebra - vector spaces, linear maps, rank, image of a linear map**: Understanding how the softmax matrix projects embeddings to logits and how this projection's rank constraint limits the output space. Quick check: What is the dimension of the image of a linear map defined by a matrix W in Rv×d?

- **Probability theory - probability distributions, simplex, log-probabilities**: The LLM outputs are probability distributions over vocabulary tokens, and the softmax function maps logits to probabilities. Quick check: How does the softmax function ensure that the output is a valid probability distribution?

- **Machine learning - transformer architecture, embedding layers, output layers**: The LLM architecture, specifically the transformer model, generates embeddings that are projected to logits via the softmax matrix. Quick check: What role does the embedding size play in the transformer architecture?

## Architecture Onboarding

- **Component map**: Transformer model -> Softmax matrix W -> Softmax function -> API interface
- **Critical path**: Query API with biased tokens → Solve for unbiased probabilities → Collect d linearly independent outputs → Use image for various analyses
- **Design tradeoffs**: Accuracy vs. cost for full output recovery; security vs. transparency for API access to log-probabilities
- **Failure signatures**: Insufficient linearly independent outputs, numerical instability in probability solving, API lacking required information
- **First 3 experiments**: 
  1. Collect outputs from known model to verify image dimensionality matches embedding size
  2. Test full output recovery algorithm on API with log-probabilities
  3. Attempt model identification using image comparison across different models

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the exact embedding size of gpt-3.5-turbo?
- Basis in paper: The paper estimates the embedding size to be between 4,096 and 4,650, with 4,096 being most likely.
- Why unresolved: The authors note their estimate might be slightly overestimated due to potential errors in the data collection process.
- What evidence would resolve it: A precise determination through a more rigorous and error-free methodology.

### Open Question 2
- Question: Can the softmax matrix of an API-protected LLM be exactly recovered from its outputs?
- Basis in paper: The paper discusses the possibility of approximating the softmax matrix but doesn't provide a definitive answer.
- Why unresolved: The authors hypothesize that embeddings lie on a hypersphere but acknowledge this needs verification.
- What evidence would resolve it: A successful or failed attempt to recover the exact softmax matrix with rigorous validation.

### Open Question 3
- Question: How can LLM providers effectively mitigate the information leakage described in the paper?
- Basis in paper: The paper discusses potential mitigation strategies including removing log-prob access and changing model architecture.
- Why unresolved: Each mitigation strategy has drawbacks and none is a perfect solution.
- What evidence would resolve it: Development and implementation of an effective mitigation strategy without significantly impacting API utility.

## Limitations
- Assumes softmax matrix has full rank, which may not hold for all implementations
- Depends on specific API behaviors (log-probabilities, top-k tokens) that may vary across providers
- Requires collecting d linearly independent outputs, which may be affected by prompt engineering
- Solving for unbiased probabilities may suffer from numerical instability

## Confidence

- **High Confidence**: The softmax bottleneck mechanism and relationship between embedding dimension and output space dimensionality are well-established mathematical facts
- **Medium Confidence**: The uniqueness of LLM signatures and effectiveness of model identification across checkpoints is empirically supported but may not hold universally
- **Medium Confidence**: The efficiency gains for full vocabulary recovery depend on API implementation details that may vary in practice

## Next Checks

1. **Cross-Model Signature Robustness**: Test the model identification algorithm across diverse LLM families and multiple checkpoints to assess sensitivity to fine-tuning and architectural variations.

2. **API Interface Generalization**: Implement full vocabulary recovery against multiple API configurations to identify minimal requirements for the attack to succeed.

3. **Prompt Sensitivity Analysis**: Systematically vary prompts while collecting basis vectors to determine how prompt engineering affects the dimensionality and stability of the LLM's image space, including testing adversarial prompts.