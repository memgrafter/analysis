---
ver: rpa2
title: Near-Optimal Solutions of Constrained Learning Problems
arxiv_id: '2403.11844'
source_url: https://arxiv.org/abs/2403.11844
tags:
- dual
- learning
- best
- function
- problem
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper analyzes the convergence of primal iterates obtained
  from dual ascent methods in constrained learning problems. The key insight is that
  these non-convex problems can be viewed as parametrizations of benign functional
  programs.
---

# Near-Optimal Solutions of Constrained Learning Problems

## Quick Facts
- arXiv ID: 2403.11844
- Source URL: https://arxiv.org/abs/2403.11844
- Reference count: 40
- One-line primary result: Dual ascent methods yield near-optimal and near-feasible primal solutions in constrained learning problems by leveraging the connection to benign functional programs.

## Executive Summary
This paper analyzes the convergence of primal iterates obtained from dual ascent methods in constrained learning problems. The key insight is that non-convex constrained learning problems can be viewed as parametrizations of benign functional programs. The authors leverage this connection to characterize the sub-optimality and infeasibility of Lagrangian minimizers associated with optimal dual variables. They show that for sufficiently rich parametrizations, solutions obtained by dual algorithms closely approximate the functional solutions in terms of both objective value and constraint satisfaction.

## Method Summary
The paper studies constrained learning problems where a parametrized function approximator (e.g., neural network) must optimize an objective while satisfying constraints. The method uses dual ascent with stochastic supergradients and approximate primal minimization. In the stochastic setting, the algorithm maintains a running best dual iterate and associated primal solution. The theoretical analysis shows that the best iterate converges to near-optimality and near-feasibility bounds determined by the approximation error of the parametrization and the quality of the primal solver.

## Key Results
- Non-convex constrained learning problems can be seen as parametrizations of convex functional programs
- Dual ascent methods yield primal iterates that are both near-optimal and near-feasible
- The final iterates perform well in practice without randomization, as demonstrated in fair learning tasks
- The bounds on sub-optimality and infeasibility depend on the approximation error ν of the parametrization

## Why This Works (Mechanism)

### Mechanism 1
Non-convex constrained learning problems can be approximated by parametrizations of benign functional programs, enabling near-optimal primal solutions from dual methods. The paper shows that a non-convex, finite-dimensional constrained learning problem (parametrized by θ) is a restriction of a convex functional optimization problem (over a hypothesis space F). When the parametrization is sufficiently rich (small ν in Assumption 3.3), solutions to the dual problem of the parametrized case closely approximate those of the functional problem in both objective value and constraint satisfaction. If the parametrization is not sufficiently rich (ν large), the approximation error bounds degrade, and dual iterates may no longer yield near-feasible solutions.

### Mechanism 2
The curvature of the unparametrized dual function gu determines how closely the dual variables λ⋆_p and λ⋆_u align, which in turn controls feasibility of the associated primal solutions. Under Assumptions 3.1–3.4, the dual function gu is strongly concave and smooth along the segment Hλ connecting λ⋆_u and λ⋆_p. This allows bounding the distance between optimal dual variables and, via the gradient relationship, bounding the difference in constraint violations between the associated Lagrangian minimizers. If the constraints are not smooth or the objective is not strongly convex, the strong concavity and smoothness constants degrade, inflating the feasibility bound.

### Mechanism 3
Stochastic supergradient updates with approximate primal minimization can converge to a near-optimal dual point whose associated primal iterate is near-feasible. Algorithm 1 is relaxed to allow ρ-approximate Lagrangian minimization and stochastic constraint estimates. Lemma 4.1 shows that the best dual iterate converges almost surely to within ηS²/2 + ρ of D⋆_p. Proposition 4.1 then extends the near-feasibility bounds to this near-optimal λbest. If the stochastic estimates have unbounded variance or the approximate primal solver fails to find low-ρ solutions, the convergence rate and feasibility bounds degrade.

## Foundational Learning

- **Concept**: Convex analysis and duality (Fenchel conjugates, subgradients, strong duality)
  - **Why needed here**: The paper's main results hinge on viewing the parametrized problem as a restriction of a convex functional problem, then applying duality theory to transfer properties from the functional case to the parametrized case
  - **Quick check question**: What is the relationship between the perturbation function P⋆(ϵ) and the dual function gu(λ) under Fenchel conjugation?

- **Concept**: Parametrization theory for neural networks and reproducing kernel Hilbert spaces
  - **Why needed here**: Assumption 3.3 requires that the model class Fθ can approximate any function in the convex hypothesis space F arbitrarily well; this is standard for two-layer NNs with sigmoid activations or RKHSs
  - **Quick check question**: For a two-layer NN with K sigmoid units, how does the approximation error ν scale with K?

- **Concept**: Stochastic optimization and martingale convergence
  - **Why needed here**: Section 4 replaces exact primal solves and exact supergradients with approximate and stochastic versions; martingale convergence tools are used to show best-iterate convergence
  - **Quick check question**: Why does a stochastic supergradient method converge to a neighborhood rather than the exact optimum when using constant step size?

## Architecture Onboarding

- **Component map**: Function approximator fθ(·) -> Oracle (arg min_θ L(fθ, λ(t))) -> Statistics estimator (mini-batch ˆℓ_i(f†_θ(t))) -> Dual variables λ(t+1)

- **Critical path**: Each iteration requires (1) sampling a mini-batch, (2) running the oracle to get f†_θ(t), (3) computing stochastic constraint estimates, (4) updating λ(t), (5) storing f†_θ(t) for best-iterate selection

- **Design tradeoffs**:
  - Approximation error ρ vs. runtime: Smaller ρ yields tighter feasibility bounds but costs more oracle calls
  - Step size η vs. stability: Larger η accelerates dual convergence but can cause oscillations in primal feasibility
  - Model capacity vs. ν: Richer models reduce ν but increase training variance and memory use

- **Failure signatures**:
  - Persistent infeasibility across iterations → check whether ν is too large (model too weak) or η is too aggressive
  - Divergence of dual variables → verify that S² is finite and step size respects smoothness constants
  - Best iterate stuck at early poor point → ensure storage of all iterates and proper comparison of gp values

- **First 3 experiments**:
  1. **Feasibility sweep**: Run Algorithm 1 with varying model widths (10, 100, 1000 hidden units) and record max constraint violation of last iterate; confirm O(ν) decay
  2. **Dual convergence test**: Plot gp(t) vs. iteration with/without stochastic estimates; verify almost-sure approach to D⋆_p within ηS²/2 + ρ
  3. **Approximation tolerance**: Fix a strong model and vary ρ in the oracle; measure impact on final constraint violation to confirm ρ enters the bound

## Open Questions the Paper Calls Out

### Open Question 1
How do the approximation bounds scale with the number of constraints in the constrained learning problem? While the paper provides bounds involving m (number of constraints) in the form of √m, it does not explicitly analyze how the complexity of the problem scales with the number of constraints. A detailed analysis of the bounds as a function of m, possibly with empirical results on problems with varying numbers of constraints, would clarify the scalability of the approach.

### Open Question 2
Can the assumptions required for the theoretical guarantees (e.g., Assumption 3.6 on the Jacobian of the constraints) be relaxed or replaced with weaker conditions? The current theoretical guarantees rely on Assumption 3.6, which may not hold in all practical scenarios. Proving the theoretical results under weaker assumptions or providing empirical evidence that the guarantees hold even when Assumption 3.6 is not satisfied would address this question.

### Open Question 3
How does the choice of the step size in the dual ascent algorithm affect the convergence and the quality of the final solution? While the paper provides some guidance on the step size, a more thorough analysis of its impact on the algorithm's performance would be valuable for practical applications. A detailed study of the convergence rate and solution quality as a function of the step size, possibly with empirical results on different problems, would clarify the role of the step size in the algorithm's performance.

## Limitations
- The theoretical framework relies on the existence of a "benign" convex functional problem whose restriction yields the non-convex parametrized problem, which may not hold for all real-world applications
- The curvature-based feasibility bounds depend critically on strong concavity and smoothness of the dual function, which may not hold for all constraint structures
- The stochastic extension assumes bounded variance and unbiased estimates, but the paper does not explore how sensitive the results are to violations of these conditions

## Confidence
- **High confidence**: The theoretical framework connecting dual iterates to primal feasibility via functional approximation is mathematically sound and well-supported by the proofs
- **Medium confidence**: The practical applicability to fair learning depends on whether real datasets and model classes satisfy the required assumptions, which is not thoroughly validated
- **Medium confidence**: The stochastic extension provides useful robustness, but the bounds may be loose in practice given only constant step sizes are considered

## Next Checks
1. **Assumption validation**: For the COMPAS dataset and the chosen NN architecture, measure the approximation error ν empirically and verify it decreases with model capacity as predicted
2. **Robustness to assumption violations**: Test Algorithm 1 with non-smooth or non-strongly-convex objectives to identify when the feasibility bounds break down
3. **Best-iterate vs. last-iterate**: Compare the constraint violation of the best-iterate (as analyzed in Section 4) versus the last iterate in practice, and quantify the improvement margin