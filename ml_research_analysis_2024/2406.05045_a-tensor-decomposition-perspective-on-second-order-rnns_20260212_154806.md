---
ver: rpa2
title: A Tensor Decomposition Perspective on Second-order RNNs
arxiv_id: '2406.05045'
source_url: https://arxiv.org/abs/2406.05045
tags:
- rank
- rnns
- hidden
- latexit
- cprnns
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper formally studies CPRNN, a second-order RNN model that
  leverages CP decomposition to compress the second-order weight tensor. The authors
  analyze how the rank of the CP decomposition and the hidden size interact to control
  the model's expressiveness.
---

# A Tensor Decomposition Perspective on Second-order RNNs

## Quick Facts
- arXiv ID: 2406.05045
- Source URL: https://arxiv.org/abs/2406.05045
- Authors: Maude Lizaire; Michael Rizvi-Martel; Marawan Gamal Abdel Hameed; Guillaume Rabusseau
- Reference count: 30
- Key outcome: CPRNNs leverage CP decomposition to compress second-order weight tensors, achieving better expressiveness-efficiency trade-offs than RNNs, 2RNNs, and MIRNNs on character-level language modeling.

## Executive Summary
This paper introduces CPRNNs, a second-order RNN model that uses CP decomposition to compress the second-order weight tensor. The authors analyze how the rank of the CP decomposition and hidden size interact to control model expressiveness, establishing theoretical results showing CPRNNs are strictly more expressive than MIRNNs and equivalent to 2RNNs at high rank. Empirical experiments on Penn Treebank demonstrate CPRNNs outperform RNNs, 2RNNs, and MIRNNs in bits-per-character for fixed parameter budgets.

## Method Summary
The paper studies CPRNNs which use CP decomposition to approximate the second-order weight tensor in RNNs. The model computes hidden states using a combination of first-order terms (standard RNN computation) and a second-order term computed via tensor contraction with the CP factors. The theoretical analysis establishes how rank and hidden size affect expressiveness, while empirical experiments compare CPRNN performance to RNNs, MIRNNs, and 2RNNs on character-level language modeling with fixed parameter budgets.

## Key Results
- CPRNNs with rank below hidden size strictly increase expressiveness with higher rank
- CPRNNs are strictly more expressive than MIRNNs for ranks exceeding hidden size
- CPRNNs outperform RNNs, 2RNNs, and MIRNNs on Penn Treebank for fixed parameter budgets

## Why This Works (Mechanism)

### Mechanism 1
Increasing the rank of the CP decomposition in CPRNNs strictly increases model expressive power up to the maximal typical CP rank. The CP rank controls the dimension of the pre-activation space, with higher rank directly increasing the dimensionality of the first hidden state space. This holds when the activation function is invertible or a homeomorphism, preserving dimensionality of the pre-activation manifold.

### Mechanism 2
CPRNNs with sufficient rank are strictly more expressive than MIRNNs because MIRNNs constrain the third factor matrix to be diagonal. In MIRNNs, each component of the second-order term is computed by a rank-one matrix, while CPRNNs with rank greater than hidden size allow the third factor matrix to have full rank, enabling more complex interactions.

### Mechanism 3
CPRNNs with fixed parameter budgets always outperform RNNs, 2RNNs, and MIRNNs by optimally trading off rank and hidden size. The parameter count formula (O(Rn + nd) for CPRNNs) allows flexible allocation between rank and hidden size, enabling interpolation between first-order (RNN-like) and second-order (2RNN-like) expressiveness.

## Foundational Learning

- Concept: Tensor decomposition (specifically CP decomposition)
  - Why needed here: The paper's main contribution is analyzing how CP decomposition rank affects CPRNN expressiveness
  - Quick check question: What is the difference between the CP rank of a tensor and the rank hyperparameter used in CPRNNs?

- Concept: Recurrent Neural Networks (RNNs) and their variants
  - Why needed here: The paper compares CPRNNs to RNNs, 2RNNs, and MIRNNs
  - Quick check question: How does a 2RNN differ from a standard RNN in terms of hidden state computation?

- Concept: Formal language theory and automata
  - Why needed here: The paper mentions connections between 2RNNs and weighted finite automata
  - Quick check question: What class of languages can be recognized by linear 2RNNs?

## Architecture Onboarding

- Component map: Input (d) -> Hidden state (n) -> CP factors A(n×R), B(d×R), C(n×R) -> First-order weights U(n×d), V(n×n), bias b(n) -> Activation (tanh)

- Critical path:
  1. Compute second-order term: [[A,B,C]]×1 ht-1 ×2 xt
  2. Add first-order terms: Vht-1 + Uxt + b
  3. Apply activation: ht = σ(second-order + first-order)

- Design tradeoffs:
  - Higher rank → more expressive but more parameters
  - Larger hidden size → more expressive but more parameters
  - Balancing rank and hidden size is key for fixed parameter budgets

- Failure signatures:
  - Underfitting: Low rank or hidden size, especially when R < n
  - Overfitting: Very high hidden size with fixed parameters, leading to small rank

- First 3 experiments:
  1. Fix hidden size, vary rank from 1 to 100, measure bits-per-character on Penn Treebank
  2. Fix parameter budget, sweep combinations of rank and hidden size, find optimal configuration
  3. Compare CPRNN performance to RNN, MIRNN, and 2RNN at equal parameter counts

## Open Questions the Paper Calls Out

### Open Question 1
What is the exact maximal CP rank (Rmax) for the specific tensor dimensions used in the experiments? The paper states that the saturation of CPRNN expressivity occurs at Rmax but does not compute the exact value for the experimental setup (n=64, 128, 256, 1024; d=101).

### Open Question 2
Does the strict inclusion result in Theorem 1 (HCPBIRNN(R, n) ⊊ HCPBIRNN(R + 1, n) for R < Rtyp−max) hold for all common activation functions (e.g., tanh, ReLU)? The paper conjectures this but only proves it for real analytic invertible activation functions and linear activation functions.

### Open Question 3
How does the performance of CPRNNs compare to other second-order RNN variants like Kronecker-based models or Tensor-Train based models? The paper mentions these as potential future work but does not compare CPRNNs to them experimentally.

## Limitations

- Theoretical analysis assumes invertibility of activation function, which may not hold for common choices like ReLU
- Diagonal constraint analysis for MIRNNs relies on specific structural assumptions that may not generalize
- Empirical validation limited to a single dataset (Penn Treebank), raising questions about generalization

## Confidence

- **High confidence**: The theoretical relationship between CPRNN rank and expressiveness, and the equivalence to 2RNNs at high rank
- **Medium confidence**: The strict superiority of CPRNNs over MIRNNs for rank > hidden size, given the diagonal constraint assumption
- **Medium confidence**: The empirical performance gains, though limited by single-dataset validation

## Next Checks

1. Verify the theoretical analysis with non-invertible activation functions (ReLU, GELU) to test the robustness of the expressiveness results
2. Extend empirical evaluation to multiple language modeling datasets (WikiText, enwik8) and tasks (sequence modeling, time series) to test generalization
3. Implement and compare CPRNNs with other second-order RNN variants that use different tensor decompositions (Tucker, Tensor Train) to isolate the effects of CP decomposition specifically