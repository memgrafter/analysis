---
ver: rpa2
title: Tackling Data Corruption in Offline Reinforcement Learning via Sequence Modeling
arxiv_id: '2407.04285'
source_url: https://arxiv.org/abs/2407.04285
tags:
- corruption
- data
- offline
- learning
- reward
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates the robustness of offline reinforcement
  learning methods under data corruption, focusing on sequence modeling approaches.
  The authors find that traditional temporal difference learning methods underperform
  Decision Transformer (DT) under data corruption, especially with limited data.
---

# Tackling Data Corruption in Offline Reinforcement Learning via Sequence Modeling

## Quick Facts
- arXiv ID: 2407.04285
- Source URL: https://arxiv.org/abs/2407.04285
- Reference count: 40
- Primary result: Robust Decision Transformer (RDT) outperforms baseline methods under data corruption across MuJoCo, Kitchen, and Adroit tasks

## Executive Summary
This paper investigates the robustness of offline reinforcement learning methods under data corruption, focusing on sequence modeling approaches. The authors find that traditional temporal difference learning methods underperform Decision Transformer (DT) under data corruption, especially with limited data. To improve DT's robustness, they propose Robust Decision Transformer (RDT) with three key techniques: embedding dropout, Gaussian weighted learning, and iterative data correction. Experiments on MuJoCo, Kitchen, and Adroit tasks show that RDT outperforms baseline methods under both random and adversarial data corruption, achieving state-of-the-art performance across most settings. RDT also demonstrates superior robustness to observation perturbations during testing. The results highlight the potential of sequence modeling for reliable offline RL in real-world scenarios with noisy or corrupted data.

## Method Summary
The paper proposes Robust Decision Transformer (RDT), an extension of Decision Transformer designed to handle data corruption in offline RL. RDT incorporates three key techniques: embedding dropout to improve robustness during training, Gaussian weighted learning to reduce the impact of corrupted labels by down-weighting samples with high prediction errors, and iterative data correction to identify and replace corrupted data points using distributional statistics of prediction errors. The method is evaluated on MuJoCo, Kitchen, and Adroit tasks under various corruption scenarios, comparing RDT against baseline methods including behavioral cloning, conservative Q-learning, and other offline RL approaches.

## Key Results
- RDT outperforms baseline methods under both random and adversarial data corruption
- DT exhibits inherent robustness to data corruption even without specialized modifications
- RDT achieves state-of-the-art performance across most settings while maintaining robustness to observation perturbations during testing
- Performance degradation is observed when RDT encounters state corruption with high rates, indicating room for improvement

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Sequence modeling with return-to-go conditioning provides robustness to state corruption in limited data regimes.
- Mechanism: Decision Transformer's architecture conditions predictions on return-to-go, which summarizes future reward expectations. When states are corrupted, the return-to-go embedding helps the model maintain performance by focusing on reward structure rather than exact state values.
- Core assumption: Return-to-go embedding captures sufficient information to compensate for corrupted state inputs.
- Evidence anchors:
  - [abstract] "we discover that vanilla sequence modeling methods, such as Decision Transformer, exhibit robustness against data corruption, even without specialized modifications"
  - [section] "DT outperforms prior methods under state attack scenarios"
  - [corpus] Weak - no direct comparison in corpus papers
- Break condition: If return-to-go itself becomes heavily corrupted or if state corruption patterns are highly structured and adversarial to the return-to-go encoding.

### Mechanism 2
- Claim: Gaussian weighted learning reduces the impact of corrupted labels by down-weighting samples with high prediction errors.
- Mechanism: The Gaussian weight function w ∝ e^(-β·δ²) exponentially reduces the contribution of samples where the model's prediction error is large, assuming these errors indicate corrupted labels.
- Core assumption: Corrupted labels produce larger prediction errors than clean labels during training.
- Evidence anchors:
  - [section] "we use the value of the sample-wise loss to adjust the weight for the DT loss"
  - [section] "erroneous actions can directly influence the model through backpropagation"
  - [corpus] Weak - no direct evidence in corpus papers
- Break condition: If corrupted data produces low prediction errors (e.g., if corruption patterns align with the model's inductive biases) or if clean data happens to produce high errors.

### Mechanism 3
- Claim: Iterative data correction identifies and replaces corrupted data points using distributional statistics of prediction errors.
- Mechanism: The algorithm tracks the mean and variance of prediction errors during training, then uses z-scores to detect outliers that likely represent corrupted data, replacing them with model predictions.
- Core assumption: The prediction error distribution for clean data remains stable during training, allowing outlier detection.
- Evidence anchors:
  - [section] "Our hypothesis is that the prediction error δ between predicted and clean label actions should exhibit consistency after sufficient training"
  - [section] "we calculate the z-score, denoted by z(i) = δ(i)−µδ/σδ, for each sampled action"
  - [corpus] Weak - no direct evidence in corpus papers
- Break condition: If the prediction error distribution shifts significantly due to learning dynamics, or if corrupted data produces error distributions similar to clean data.

## Foundational Learning

- Concept: Sequence modeling for decision-making
  - Why needed here: The paper reframes RL as a supervised sequence prediction problem, which requires understanding how transformer architectures can model sequential decision processes.
  - Quick check question: How does Decision Transformer's return-to-go conditioning differ from standard supervised learning approaches?

- Concept: Data corruption detection and mitigation
  - Why needed here: The robust techniques (Gaussian weighting, iterative correction) rely on understanding how to identify corrupted data based on statistical properties.
  - Quick check question: What assumptions about prediction error distributions enable the iterative data correction mechanism?

- Concept: Transformer architecture and positional embeddings
  - Why needed here: The embedding dropout technique requires understanding how transformer embeddings encode information and how dropping dimensions affects learning.
  - Quick check question: How does embedding dropout in transformers differ from standard dropout in MLPs?

## Architecture Onboarding

- Component map:
  Input (Return-to-go, state, action sequences) -> Linear projection layers -> Positional embeddings -> Transformer blocks -> Action/reward prediction -> Gaussian weighted loss -> Backpropagation -> Parameter update

- Critical path: Data → Projection → Transformer → Prediction → Loss (with Gaussian weights) → Backpropagation → Parameter update

- Design tradeoffs:
  - Sequence length vs. computational cost: Longer sequences capture more context but increase memory usage
  - Embedding dropout rate vs. robustness: Higher rates increase robustness but may hurt clean performance
  - Gaussian weight coefficient β vs. sensitivity: Higher values make the model less sensitive to potential corruption but may discard useful information

- Failure signatures:
  - Performance degradation on clean data suggests over-regularization
  - Sensitivity to state corruption indicates insufficient return-to-go conditioning
  - Poor convergence suggests inappropriate Gaussian weight settings

- First 3 experiments:
  1. Verify baseline DT performance on clean data vs. corrupted data to establish the robustness gap
  2. Test embedding dropout ablation to determine optimal dropout rate
  3. Validate Gaussian weighted learning by comparing prediction error distributions for clean vs. corrupted data

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does RDT's performance degrade as corruption rates exceed 0.5 or scales exceed 2.0?
- Basis in paper: [inferred] The paper states "we did observe a decline in RDT's performance when it encountered state corruption with high rates" and suggests future work on enhancing robustness to high corruption rates and scales.
- Why unresolved: The experiments only tested corruption rates up to 0.5 and scales up to 2.0. The paper explicitly identifies this as an area for future improvement.
- What evidence would resolve it: Additional experiments testing RDT with corruption rates > 0.5 and scales > 2.0, measuring performance degradation curves.

### Open Question 2
- Question: Can RDT's state corruption correction be improved beyond the current z-score detection method?
- Basis in paper: [explicit] The paper states "Our hypothesis is that the prediction error δ between predicted and clean label actions should exhibit consistency after sufficient training" and "Therefore, if erroneous label actions are encountered, δ will deviate from the mean µδ, behaving like outliers. This deviation essentially enables us to detect and correct the corrupted action using the distributional information of δ."
- Why unresolved: The paper acknowledges that "better data correction methods for states" are left for future work, suggesting the current z-score method may not be optimal.
- What evidence would resolve it: Comparison of RDT's z-score correction method with alternative state corruption detection and correction techniques.

### Open Question 3
- Question: How does RDT perform under more complex data corruption scenarios, such as correlated corruptions across multiple elements?
- Basis in paper: [inferred] While the paper tests random and adversarial corruption on individual elements, it doesn't explore scenarios where corruptions might be correlated or structured in more complex ways.
- Why unresolved: The experiments focus on independent corruptions of states, actions, and rewards. Real-world data corruption might involve more complex patterns.
- What evidence would resolve it: Experiments testing RDT under correlated or structured corruption patterns, such as element-wise correlations or temporal dependencies in corruption.

## Limitations

- The assumption that prediction error distributions remain stable for outlier detection may not hold in complex environments
- The effectiveness of Gaussian weighted learning depends on the premise that corrupted labels consistently produce higher prediction errors, which isn't universally true
- Ablation studies for individual RDT components are incomplete, making it difficult to isolate which mechanism drives the most improvement

## Confidence

- **High Confidence**: The empirical observation that sequence modeling methods like DT show better robustness than temporal difference methods under data corruption is well-supported by experimental results.
- **Medium Confidence**: The three proposed mechanisms (embedding dropout, Gaussian weighted learning, iterative data correction) appear effective based on experiments, though the theoretical justification could be stronger.
- **Low Confidence**: The claim that RDT achieves "state-of-the-art" performance across most settings should be interpreted cautiously given the limited scope of comparisons and lack of ablation studies for individual components.

## Next Checks

1. Conduct ablation studies to determine the individual and combined contributions of embedding dropout, Gaussian weighted learning, and iterative data correction to overall performance.
2. Test RDT's robustness when corrupted data produces low prediction errors (opposite of the assumed pattern) to validate the Gaussian weighted learning mechanism's assumptions.
3. Evaluate RDT on additional offline RL benchmarks beyond MuJoCo, Kitchen, and Adroit to assess generalizability across different task types and data distributions.