---
ver: rpa2
title: Dual Traits in Probabilistic Reasoning of Large Language Models
arxiv_id: '2412.11009'
source_url: https://arxiv.org/abs/2412.11009
tags:
- answer
- please
- your
- following
- posterior
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This study investigates how large language models (LLMs) evaluate
  posterior probabilities, revealing two distinct judgment modes: normative (following
  Bayes'' rule) and representativeness-based (relying on similarity to prototypes).
  Through three experiments with varying information availability, the authors found
  that state-of-the-art LLMs primarily use representativeness-based judgment, assigning
  higher posterior probabilities to more representative descriptions regardless of
  base rates.'
---

# Dual Traits in Probabilistic Reasoning of Large Language Models

## Quick Facts
- **arXiv ID**: 2412.11009
- **Source URL**: https://arxiv.org/abs/2412.11009
- **Reference count**: 40
- **Primary result**: LLMs exhibit dual modes of probabilistic reasoning, with representativeness-based judgment dominating over normative (Bayesian) reasoning in most conditions

## Executive Summary
This study investigates how large language models evaluate posterior probabilities, revealing two distinct judgment modes: normative (following Bayes' rule) and representativeness-based (relying on similarity to prototypes). Through three experiments with varying information availability, the authors found that state-of-the-art LLMs primarily use representativeness-based judgment, assigning higher posterior probabilities to more representative descriptions regardless of base rates. The models also struggled to recall base rate information and showed inconsistent behavior with prompt engineering. These findings suggest that LLMs develop dual traits in probabilistic reasoning, analogous to human System 1 and System 2 thinking, likely due to contrastive loss functions in reinforcement learning from human feedback. The study highlights the need for caution when deploying LLMs in critical fields and suggests incorporating base rates explicitly in prompts to improve reliability.

## Method Summary
The researchers conducted three experiments to assess how LLMs handle probabilistic reasoning tasks. They tested six state-of-the-art commercial models including GPT-4, Claude, Gemini, and others. The first experiment presented scenarios where representativeness information was available but base rates were omitted. The second experiment provided both representativeness information and base rates, then tested whether models could recall base rates accurately. The third experiment explored how different prompt engineering approaches affected reasoning outcomes. Each experiment used carefully constructed scenarios designed to test specific aspects of probabilistic judgment, with responses evaluated against normative Bayesian calculations and human cognitive biases.

## Key Results
- LLMs predominantly used representativeness-based judgment, giving higher posterior probabilities to more representative descriptions regardless of base rates
- Models demonstrated poor base rate recall accuracy, particularly when base rates were initially omitted from scenarios
- Prompt engineering with base rate information showed inconsistent results, sometimes improving but often failing to activate normative reasoning
- The dual-trait phenomenon was observed across all tested models, suggesting it may be a fundamental characteristic of current LLM architectures

## Why This Works (Mechanism)
The dual-trait phenomenon emerges from how LLMs are trained on human-generated data that contains both systematic probabilistic reasoning and heuristic shortcuts. During training, models learn to associate patterns and similarities (representativeness) because these are frequently used in human communication, while explicit Bayesian calculations are less common in training data. The contrastive loss functions used in reinforcement learning from human feedback may reinforce this pattern by rewarding outputs that match human-like responses, which often include representativeness-based judgments rather than purely normative reasoning.

## Foundational Learning
- **Bayesian probability**: Understanding how to update beliefs based on evidence is crucial for evaluating LLM reasoning against normative standards
- **Representativeness heuristic**: This cognitive bias, where similarity to prototypes influences probability judgments, provides the framework for understanding the alternative reasoning mode
- **System 1 vs System 2 thinking**: The dual-process theory of cognition offers an analogy for understanding how LLMs might switch between different reasoning modes
- **Base rate neglect**: This common human error in probability judgment provides context for why LLMs might struggle with incorporating prior probabilities
- **Contrastive loss functions**: Understanding this training mechanism helps explain why models might learn to favor human-like (but potentially biased) reasoning patterns
- **Prompt engineering effects**: Knowledge of how different prompt formulations influence model outputs is essential for interpreting the inconsistent results observed

## Architecture Onboarding

**Component Map**: User Prompt -> Language Model -> Response Generation -> Output

**Critical Path**: The reasoning pathway involves initial pattern matching based on training data, followed by either representativeness-based or normative evaluation, with the final response determined by which mode dominates in the given context.

**Design Tradeoffs**: The dual-trait system represents a tradeoff between computational efficiency (using quick representativeness judgments) and accuracy (requiring more complex Bayesian calculations). This mirrors the tradeoff in human cognition between fast, heuristic-based decisions and slower, more deliberative reasoning.

**Failure Signatures**: Key failure modes include base rate neglect when representativeness information is available, inconsistent behavior under different prompt formulations, and difficulty switching to normative reasoning even when explicitly prompted to do so.

**First Experiments**: 1) Test model responses to scenarios with varying levels of base rate information to map the conditions under which each reasoning mode activates. 2) Examine how different model architectures (transformers, recurrent networks, etc.) exhibit these dual traits to identify architectural influences. 3) Investigate whether fine-tuning on explicit probability reasoning examples can shift the balance toward more normative judgment.

## Open Questions the Paper Calls Out
None identified in the source material.

## Limitations
- The study tested only six commercial LLMs, limiting generalizability across the broader model ecosystem
- Experimental scenarios, while carefully constructed, represent only a subset of possible probability judgment contexts
- The interpretation of results through human cognitive frameworks remains speculative and may not accurately reflect LLM internal mechanisms
- Prompt engineering results were inconsistent, suggesting potential methodological limitations in how reasoning modes were activated
- Base rate recall assessment may not fully capture models' true capabilities in probabilistic reasoning

## Confidence
- **Medium**: LLMs primarily use representativeness-based judgment across conditions
- **High**: Dual reasoning modes exist and can be activated under different circumstances
- **Low**: The connection between these findings and reinforcement learning from human feedback mechanisms

## Next Checks
1. Replicate experiments across a broader range of LLMs, including open-source models with varying architectures and training datasets, to assess generalizability of the dual-trait phenomenon
2. Conduct ablation studies to isolate effects of different training objectives (contrastive loss, supervised fine-tuning) on probabilistic reasoning modes
3. Design experiments testing LLMs' ability to deliberately switch between judgment modes, exploring whether explicit prompting can reliably activate normative reasoning