---
ver: rpa2
title: Diffusion Boosted Trees
arxiv_id: '2406.01813'
source_url: https://arxiv.org/abs/2406.01813
tags:
- diffusion
- card
- learning
- gradient
- boosting
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces Diffusion Boosted Trees (DBT), a novel supervised
  learning framework that combines denoising diffusion probabilistic models with gradient
  boosting. DBT parameterizes diffusion models using decision trees (one tree per
  diffusion timestep), creating a model that learns conditional distributions without
  parametric assumptions.
---

# Diffusion Boosted Trees

## Quick Facts
- arXiv ID: 2406.01813
- Source URL: https://arxiv.org/abs/2406.01813
- Authors: Xizewen Han; Mingyuan Zhou
- Reference count: 40
- Key outcome: DBT outperforms deep neural network-based diffusion models on piecewise functions and categorical features while handling missing data robustly

## Executive Summary
Diffusion Boosted Trees (DBT) introduces a novel supervised learning framework that combines denoising diffusion probabilistic models with gradient boosting. The method parameterizes diffusion models using decision trees (one tree per diffusion timestep), creating a model that learns conditional distributions without parametric assumptions. DBT demonstrates strong performance on both regression and classification tasks, particularly excelling on piecewise-defined functions and datasets with many categorical features while handling missing data without imputation.

## Method Summary
DBT combines denoising diffusion probabilistic models with gradient boosting by using decision trees as weak learners at each diffusion timestep. The method involves pre-training a conditional mean estimator, then sequentially training one decision tree per timestep to approximate the forward process posterior mean. Unlike amortized neural network approaches, DBT uses separate trees for each timestep, enabling timestep-specific feature importance analysis and handling heterogeneous data types naturally. The method is particularly effective for tabular data with categorical features and piecewise-defined functions.

## Key Results
- Outperforms CARD (deep neural network-based diffusion models) on piecewise-defined functions and datasets with many categorical features
- Matches CARD's performance on UCI regression benchmarks while significantly outperforming other baseline methods
- Improves classification accuracy from 69.58% to 76.68% (or 78.59% with stricter thresholds) by learning to defer uncertain predictions
- Handles missing data robustly without requiring imputation, maintaining competitive performance across various metrics

## Why This Works (Mechanism)

### Mechanism 1
DBT learns conditional distributions without parametric assumptions by combining diffusion models with gradient boosting. Each diffusion timestep uses a separate decision tree as a weak learner, trained sequentially to approximate the forward process posterior mean. This sequential training aligns the computational graph during training and sampling, addressing exposure bias. The core assumption is that a weak learner at each timestep is sufficient because the posterior mean computation depends predominantly on the current noisy sample rather than the predicted target for most timesteps.

### Mechanism 2
Trees outperform deep neural networks on tabular data with many categorical features because decision trees naturally handle heterogeneous data types (numerical and categorical) and piecewise-defined functions without requiring data normalization or imputation. The discrete subregion partitioning of decision trees better captures the structure of piecewise-defined functions than the smooth interpolation of neural networks.

### Mechanism 3
DBT handles missing data without imputation while maintaining robust performance because LightGBM's native handling of missing values allows DBT to train on incomplete data without preprocessing. The gradient boosting framework can effectively learn from data with missing values without explicit imputation, avoiding potential biases introduced by imputation methods.

## Foundational Learning

- Concept: Denoising Diffusion Probabilistic Models (DDPM)
  - Why needed here: DBT builds directly on the DDPM framework, using the forward process posterior distributions to guide tree training
  - Quick check question: What is the relationship between the forward process posterior mean formula and the noise prediction loss in CARD?

- Concept: Gradient Boosting and Weak Learners
  - Why needed here: DBT applies gradient boosting principles to diffusion models, using sequential tree training instead of amortized neural networks
  - Quick check question: How does the sequential training paradigm in DBT differ from standard gradient boosting, and why is this difference necessary?

- Concept: SHAP Values and Feature Importance
  - Why needed here: DBT's use of different trees at each timestep enables timestep-specific feature importance analysis through SHAP values
  - Quick check question: What insight does the variation in SHAP values across timesteps provide about the diffusion sampling process?

## Architecture Onboarding

- Component map: Conditional mean estimator fϕ(x) -> Tree ensemble {fθt}T_t=1 -> Forward process sampling q(yt|y0,x) -> Posterior sampling q(yt-1|yt,y0,x)

- Critical path:
  1. Pre-train fϕ(x) to estimate E[y|x]
  2. For t=T to 1: Sample yt+1, train fθt to predict y0
  3. For inference: Start with prior noise, sequentially apply trees to generate samples

- Design tradeoffs:
  - Single amortized model vs. separate trees: Amortized models require deep networks but train in parallel; separate trees are simpler but require sequential training
  - Number of trees per timestep: Current implementation uses 1 tree for simplicity, but more trees may improve performance near sampling completion
  - Handling of missing data: LightGBM handles missing values natively, but this ties DBT to specific implementations

- Failure signatures:
  - Poor performance on smooth, continuous functions: Trees may create artificial breakpoints
  - High memory usage: Duplicating dataset for noise samples can be prohibitive for large datasets
  - Slow inference: Sequential nature prevents parallelization during sampling

- First 3 experiments:
  1. Replicate toy examples (linear, multimodal, piecewise) to verify DBT captures different distributional characteristics
  2. Compare DBT vs CARD on tabular dataset with many categorical features to validate categorical handling advantage
  3. Test DBT with 10% missing data on UCI datasets to confirm robust performance without imputation

## Open Questions the Paper Calls Out

### Open Question 1
How does the number of trees per timestep (fθt) affect DBT's performance during the reverse diffusion process? The paper notes "We argue that model performance could potentially be improved by using more trees when γ0 surges near the end of the generation process" but states "We defer this attempt for future iterations of the algorithm." This remains unresolved as the authors deliberately chose to use only 1 tree per timestep in their experiments.

### Open Question 2
How does DBT's sequential training paradigm compare to parallel training approaches for diffusion models with tree-based parameterization? The authors discuss "exposure bias" in CARD and propose sequential training to address this, but acknowledge "Recent versions of XGBoost offer the data iterator functionality for memory-efficient training" which could enable parallel approaches. This comparison remains unexplored due to the memory constraints of LightGBM.

### Open Question 3
What is the optimal balance between model complexity and runtime efficiency for DBT in practical applications? The authors note DBT cannot be parallelized during inference due to the sequential nature of reverse process sampling, and that training requires significant memory due to dataset duplication. Runtime optimization is explicitly stated as future research direction.

## Limitations
- Sequential training paradigm prevents parallelization, limiting computational efficiency
- Requires duplicating dataset for noise samples, which may be prohibitive for large datasets
- Performance on high-dimensional continuous data beyond tabular datasets is unexplored
- Single tree per timestep chosen without systematic evaluation against alternatives

## Confidence
- High: DBT's ability to handle missing data without imputation and superior performance on piecewise-defined functions with categorical features
- Medium: DBT's competitive performance on UCI regression benchmarks compared to CARD
- Medium: DBT's ability to learn deferral in classification tasks, though improvement is modest

## Next Checks
1. Benchmark runtime efficiency against parallelized neural diffusion models on datasets of varying sizes
2. Test DBT on high-dimensional continuous datasets (e.g., image or audio data) to assess generalization beyond tabular data
3. Systematically evaluate the impact of using multiple trees per timestep versus the current single-tree approach