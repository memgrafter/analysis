---
ver: rpa2
title: Exploring Self-Supervised Multi-view Contrastive Learning for Speech Emotion
  Recognition with Limited Annotations
arxiv_id: '2406.07900'
source_url: https://arxiv.org/abs/2406.07900
tags:
- speech
- data
- fine-tuning
- pre-training
- wav2vec
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proposes Pairwise-CL, a multi-view contrastive self-supervised
  learning (SSL) framework for speech emotion recognition (SER) under limited annotation
  conditions. The method pre-trains view-level encoders on multiple speech representations
  (wav2vec 2.0, mel spectrograms, eGeMAPS-88) using pairwise contrastive loss, aligning
  their latent representations.
---

# Exploring Self-Supervised Multi-view Contrastive Learning for Speech Emotion Recognition with Limited Annotations

## Quick Facts
- arXiv ID: 2406.07900
- Source URL: https://arxiv.org/abs/2406.07900
- Authors: Bulat Khaertdinov; Pedro Jeuris; Annanda Sousa; Enrique Hortal
- Reference count: 0
- One-line primary result: Pairwise-CL improves UAR by up to 10% compared to supervised training from scratch when fine-tuning with 2-5% of data.

## Executive Summary
This paper proposes Pairwise-CL, a multi-view contrastive self-supervised learning framework for speech emotion recognition (SER) under limited annotation conditions. The method pre-trains view-level encoders on multiple speech representations (wav2vec 2.0, mel spectrograms, eGeMAPS-88) using pairwise contrastive loss, aligning their latent representations. Experiments on IEMOCAP-4 show that Pairwise-CL significantly improves Unweighted Average Recall (UAR) by up to 10% compared to supervised training from scratch when fine-tuning with extremely sparse annotations (2-5% of data).

## Method Summary
Pairwise-CL pre-trains three view-level encoders (wav2vec 2.0, mel spectrograms, eGeMAPS-88) using pairwise contrastive loss with NT-Xent objective. The wav2vec 2.0 model is frozen during pre-training and used as a feature extractor. After pre-training on IEMOCAP-10 dataset, the encoders are fine-tuned on IEMOCAP-4 with limited annotations using linear classifiers. The method employs temperature τ=0.5 and early stopping after 30 epochs of no improvement. Cross-validation is performed using leave-one-session-out strategy across 5 folds.

## Key Results
- Pairwise-CL achieves up to 10% UAR improvement over supervised training from scratch when fine-tuning with 2-5% of labeled data
- PWCCA alignment scores show significantly higher representation alignment after Pairwise-CL pre-training compared to supervised training
- eGeMAPS-88 handcrafted features benefit most from Pairwise-CL, achieving 3% UAR improvement over supervised baseline
- Fine-tuning wav2vec 2.0 view-level encoder yields 8% UAR improvement over frozen encoder

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Pre-training with Pairwise-CL aligns representations of different speech views (wav2vec 2.0, mel spectrograms, eGeMAPS-88) in a shared latent space, enabling the model to generalize better from limited annotations.
- Mechanism: Pairwise contrastive loss maximizes cosine similarity between projected representations of the same utterance across views while minimizing similarity to other utterances, forcing the encoders to learn a common structure.
- Core assumption: Different speech views contain complementary information about emotional content, and aligning them in latent space improves downstream task performance.
- Evidence anchors:
  - [abstract] "The method pre-trains view-level encoders on multiple speech representations ... using pairwise contrastive loss, aligning their latent representations."
  - [section 2.1] "The proposed loss function aims to maximize the similarities for multi-view representations ... corresponding to the same l-th instance."
  - [corpus] Weak corpus support for this specific mechanism; related papers focus on SSL and SER but not multi-view contrastive alignment.
- Break condition: If views do not contain complementary information or are noisy, alignment may degrade rather than improve performance.

### Mechanism 2
- Claim: Freezing the large pre-trained wav2vec 2.0 model during pre-training and fine-tuning reduces overfitting risk and focuses learning on the view-level encoders.
- Mechanism: By keeping wav2vec 2.0 frozen, the method avoids catastrophic forgetting of general speech features while the view-level encoders adapt to emotion-specific patterns.
- Core assumption: wav2vec 2.0 already encodes rich, generalizable speech representations that are useful for emotion recognition.
- Evidence anchors:
  - [section 2.2] "During pre-training, the wav2vec 2.0 model is frozen and used as a feature extraction method."
  - [section 4.1] Fine-tuning wav2vec 2.0 view-level encoder yields 8% UAR improvement over frozen encoder.
  - [corpus] No direct corpus evidence; assumption based on general SSL practices.
- Break condition: If wav2vec 2.0 features are not sufficiently rich for emotion, fine-tuning the base model may be necessary.

### Mechanism 3
- Claim: PWCCA alignment scores correlate with downstream SER performance, validating the effectiveness of Pairwise-CL pre-training.
- Mechanism: PWCCA quantifies shared structure between view-level representations; higher scores indicate better alignment, which improves classification accuracy under limited labels.
- Core assumption: Better alignment of representations in latent space leads to better generalization and classification.
- Evidence anchors:
  - [section 4.3] "We utilize a projection-weighted Canonical Correlation Analysis (PWCCA) to quantify their alignment."
  - [section 4.3] Table 3 shows higher PWCCA scores after Pairwise-CL compared to supervised training.
  - [corpus] Weak corpus support; no direct citation for PWCCA in SER context.
- Break condition: If alignment does not capture emotion-relevant features, PWCCA scores may not correlate with performance.

## Foundational Learning

- Concept: Contrastive learning and NT-Xent loss
  - Why needed here: Pairwise-CL relies on NT-Xent to align views by maximizing similarity of positive pairs and minimizing similarity of negative pairs.
  - Quick check question: What is the difference between NT-Xent and simple cosine similarity loss?

- Concept: Multi-view learning and feature complementarity
  - Why needed here: The method assumes different views (acoustic, spectral, paralinguistic) capture complementary emotional cues.
  - Quick check question: Why might mel spectrograms outperform eGeMAPS-88 in some data regimes?

- Concept: Transfer learning and fine-tuning strategies
  - Why needed here: The paper compares frozen vs fine-tuned encoders to understand when to adapt pre-trained representations.
  - Quick check question: When would freezing encoders be preferable to fine-tuning them?

## Architecture Onboarding

- Component map:
  Input: Raw speech signals (16 kHz, 15 s) -> View encoders: wav2vec 2.0 (frozen feature extractor + CNN), mel spectrogram CNN, eGeMAPS-88 MLP -> Projection heads: Separate MLPs per view -> Contrastive loss: NT-Xent between all view pairs -> Downstream: Linear classifier on top of encoder output -> Training: Pre-train with Pairwise-CL, fine-tune on labeled data

- Critical path:
  Feature extraction → Projection → Contrastive loss computation → Encoder update
  Pre-training (unlabeled data) → Fine-tuning (labeled data)

- Design tradeoffs:
  Freezing vs tuning wav2vec 2.0: Frozen avoids overfitting but may miss emotion-specific features.
  Temperature τ: Controls hardness of negative samples; τ=0.5 optimal in experiments.
  View selection: Combining multiple views improves robustness but increases compute.

- Failure signatures:
  Low PWCCA scores: Views may not be complementary or model not aligning them.
  Fine-tuning from scratch outperforms pre-training: Pre-training failed to learn useful representations.
  Large gap between validation and test: Overfitting to pre-training data distribution.

- First 3 experiments:
  1. Run Pairwise-CL with τ=0.5 on IEMOCAP-10, then fine-tune on IEMOCAP-4 with 5% labels; compare to supervised baseline.
  2. Compare frozen vs fine-tuned view-level encoders after pre-training on same data.
  3. Vary pre-training data distribution (target vs out-of-distribution emotions) and measure impact on downstream performance.

## Open Questions the Paper Calls Out
None

## Limitations
- Exact CNN architecture details for spectrogram view-level encoder are not fully specified, potentially affecting reproducibility
- The mechanism of weighted averaging for wav2vec 2.0 features lacks detailed implementation guidance
- While PWCCA analysis shows improved alignment, the direct causal relationship between alignment scores and emotion recognition performance needs more rigorous validation across diverse datasets

## Confidence
- High confidence: The Pairwise-CL framework improves UAR by 10% under 2-5% annotation regimes, as demonstrated through multiple experimental conditions and cross-validation folds.
- Medium confidence: The claim that multi-view alignment improves generalization is supported by PWCCA scores and ablation studies, though the correlation between alignment and emotion recognition could be stronger.
- Medium confidence: Freezing wav2vec 2.0 during pre-training prevents overfitting while preserving useful features, based on the 8% UAR improvement when fine-tuning the encoder.

## Next Checks
1. **Architecture Specification**: Implement the exact CNN architecture for spectrogram view-level encoder (layer count, kernel sizes, activation functions) to verify reported performance gains are reproducible.

2. **Cross-Dataset Validation**: Test Pairwise-CL on out-of-domain emotion datasets (e.g., Emo-DB, RAVDESS) to validate generalization claims and assess robustness to different emotional distributions.

3. **Ablation on View Selection**: Systematically remove individual views (wav2vec 2.0, mel spectrograms, eGeMAPS-88) to quantify the contribution of each representation type and verify the complementarity assumption.