---
ver: rpa2
title: 'RED: Residual Estimation Diffusion for Low-Dose PET Sinogram Reconstruction'
arxiv_id: '2411.05354'
source_url: https://arxiv.org/abs/2411.05354
tags:
- diffusion
- reconstruction
- noise
- process
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes RED, a residual estimation diffusion model
  for low-dose PET sinogram reconstruction. RED addresses the problem of information
  loss in low-dose PET sinograms by using the residual between low-dose and full-dose
  sinograms instead of Gaussian noise in the diffusion process.
---

# RED: Residual Estimation Diffusion for Low-Dose PET Sinogram Reconstruction

## Quick Facts
- arXiv ID: 2411.05354
- Source URL: https://arxiv.org/abs/2411.05354
- Reference count: 40
- Key outcome: RED achieves up to 8.08 dB PSNR improvement in low-dose PET sinogram reconstruction compared to traditional methods

## Executive Summary
This paper introduces RED, a residual estimation diffusion model that addresses information loss in low-dose PET sinograms by replacing Gaussian noise with the residual between low-dose and full-dose sinograms during the diffusion process. The method employs a drift correction strategy to maintain data consistency during reconstruction. Experimental results demonstrate that RED effectively improves low-dose sinogram quality and reconstruction results, achieving state-of-the-art performance across multiple evaluation metrics.

## Method Summary
RED operates by treating the low-dose sinogram as an eroded version of the full-dose sinogram and iteratively estimating and removing the residual to reconstruct high-quality data. The method uses a cascaded architecture with a Residual Estimation Network (REN) that predicts the residual at each time step, followed by a Drift Correction Network (DCN) that corrects accumulated prediction errors. The model is trained in two stages: first training REN to predict residuals using MSE and SSIM loss, then training DCN using drifted samples. The approach maintains data consistency through deterministic reconstruction without injecting additional noise.

## Key Results
- RED achieves PSNR improvements of up to 8.08 dB compared to traditional methods
- The method demonstrates strong generalization capabilities, maintaining effectiveness even with added Gaussian noise
- State-of-the-art performance across PSNR, SSIM, and NRMSE metrics at DRF 4, 20, and 100

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Replacing Gaussian noise with the residual between low-dose and full-dose sinograms avoids introducing artifacts and preserves original information.
- Mechanism: The low-dose sinogram is treated as an already eroded version of the full-dose sinogram, so no additional degradation is needed. Instead of diffusing from clean data toward noise, the model reconstructs from the low-dose state toward the full-dose state by iteratively estimating and removing the residual.
- Core assumption: The residual between low-dose and full-dose sinograms is a stable and learnable quantity that can be accurately predicted by a neural network.
- Evidence anchors:
  - [abstract] "RED uses the residual between sinograms to replace Gaussian noise in diffusion process"
  - [section III.B] "Instead of adding Gaussian noise to data, RED directly uses low-dose sinogram as the starting point and full-dose sinogram as the endpoint of reverse process"

### Mechanism 2
- Claim: Drift correction during reverse iterations mitigates accumulated prediction errors that arise from imperfect residual estimation.
- Mechanism: After each residual prediction step, a second network (DCN) estimates the drift‚Äîthe difference between the current prediction and the true sinogram state‚Äîand corrects the intermediate result before proceeding.
- Core assumption: Prediction errors accumulate linearly over time steps and can be approximated and corrected in a cascaded fashion.
- Evidence anchors:
  - [abstract] "RED introduces a drift correction strategy to reduce accumulated prediction errors during the reverse process"
  - [section III.C] "Denoting ùõø‡Øú as the predicted error at time step ùëñ...Calibrating the errors that arise during the iterative process helps improve the data consistency"

### Mechanism 3
- Claim: Training with staged loss functions (MSE + SSIM) improves both accuracy and perceptual quality of the reconstructed residuals.
- Mechanism: The residual estimation network is trained to minimize both pixel-wise error (MSE) and structural similarity (SSIM), encouraging the model to preserve edge and texture fidelity in addition to intensity accuracy.
- Core assumption: Combining MSE and SSIM losses yields better perceptual reconstruction than either alone, especially for medical imaging where structure preservation is critical.
- Evidence anchors:
  - [section III.D] "the total loss and objective function can be written... ‚ÑíùëÖùê∏ùëÅ= ‚ÑíùëÄùëÜùê∏+ ‚ÑíùëÜùëÜùêºùëÄ"
  - [section IV.A] "RED achieved the best performance across all evaluation metrics"

## Foundational Learning

- Concept: Diffusion models and their forward/reverse processes
  - Why needed here: Understanding how noise is progressively added and removed is essential to adapt the mechanism to non-Gaussian residuals.
  - Quick check question: In a standard DDPM, what is the role of the variance schedule ùõΩ‡Øß, and how does it differ conceptually from RED's residual schedule ùõº‡Øß?

- Concept: Sinogram geometry and PET reconstruction pipeline
  - Why needed here: RED operates directly in the sinogram domain, so familiarity with line-of-response geometry and back-projection is required to interpret intermediate results and failure modes.
  - Quick check question: How does undersampling in the sinogram domain manifest in the reconstructed image, and why might residual-based reconstruction be more robust to it?

- Concept: Data consistency and drift in iterative reconstruction
  - Why needed here: Drift correction relies on modeling and correcting accumulated errors; without this understanding, the cascaded network design may seem arbitrary.
  - Quick check question: What is the mathematical relationship between accumulated prediction error ùõø and the drift correction term ùõæ in RED?

## Architecture Onboarding

- Component map: Low-dose sinogram ‚Üí Residual Estimation Network (REN) ‚Üí Predicted residual ‚Üí Intermediate sinogram update ‚Üí Drift Correction Network (DCN) ‚Üí Drift correction ‚Üí Output sinogram (repeat for T steps)
- Critical path: Low-dose sinogram ‚Üí REN ‚Üí residual estimate ‚Üí sinogram update ‚Üí DCN ‚Üí drift correction ‚Üí next iteration (loop)
- Design tradeoffs: Deterministic reconstruction (no injected noise) vs. reduced generative diversity; added DCN complexity vs. stability gain; staged training vs. end-to-end simplicity
- Failure signatures: Persistent blurring despite high PSNR (DCN underfitting); unstable outputs with oscillating PSNR (overly aggressive drift correction); reconstruction collapse under extreme dose (residual too sparse to model)
- First 3 experiments:
  1. Train REN only (no DCN) and compare drift magnitude across time steps on validation data.
  2. Train full RED and visualize intermediate sinograms at each reverse step to confirm monotonic quality improvement.
  3. Test RED on sinograms with added Gaussian noise to verify robustness as claimed in Section V.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of RED change when using different types of non-Gaussian noise as degradation operators beyond the residual-based approach?
- Basis in paper: [explicit] The paper mentions that RED offers promising directions for future research to expand its application by combining it with different types of noise, such as blurring and masking, as suggested by the Cold Diffusion method.
- Why unresolved: The current study focuses on the residual-based approach and does not explore the impact of other non-Gaussian noise types on RED's performance.
- What evidence would resolve it: Conducting experiments with RED using various non-Gaussian noise types (e.g., blurring, masking) and comparing the results with the residual-based approach would provide insights into the optimal noise type for different imaging scenarios.

### Open Question 2
- Question: What is the impact of the drift correction strategy on the reconstruction quality of RED in scenarios with varying levels of data sparsity?
- Basis in paper: [explicit] The paper introduces a drift correction strategy to reduce accumulated prediction errors during the reverse process, but it does not extensively analyze its impact across different levels of data sparsity.
- Why unresolved: The study provides ablation results showing the importance of drift correction but does not explore its effectiveness across a spectrum of data sparsity levels.
- What evidence would resolve it: Performing experiments with RED on datasets with varying levels of