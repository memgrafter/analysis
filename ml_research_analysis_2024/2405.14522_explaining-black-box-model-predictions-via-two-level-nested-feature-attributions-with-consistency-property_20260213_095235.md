---
ver: rpa2
title: Explaining Black-box Model Predictions via Two-level Nested Feature Attributions
  with Consistency Property
arxiv_id: '2405.14522'
source_url: https://arxiv.org/abs/2405.14522
tags:
- lofas
- hifas
- feature
- proposed
- estimated
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of explaining predictions from
  black-box machine learning models that take inputs with a nested structure (high-level
  features composed of low-level features). The key idea is to introduce a consistency
  property between high-level feature attributions (HiFAs) and low-level feature attributions
  (LoFAs), where the HiFA of a high-level feature should equal the sum of the LoFAs
  of its constituent low-level features.
---

# Explaining Black-box Model Predictions via Two-level Nested Feature Attributions with Consistency Property

## Quick Facts
- arXiv ID: 2405.14522
- Source URL: https://arxiv.org/abs/2405.14522
- Reference count: 19
- Key outcome: C2FA method achieves higher NDCG/AUROC scores and better consistency between HiFAs and LoFAs with fewer queries to black-box models

## Executive Summary
This paper addresses the challenge of explaining black-box model predictions for nested-structured inputs by proposing a method that estimates consistent high-level and low-level feature attributions (HiFAs and LoFAs). The key innovation is introducing a consistency property where HiFA of a high-level feature equals the sum of LoFAs of its constituent low-level features. The proposed C2FA method jointly estimates both attribution levels using ADMM optimization with consistency constraints, producing explanations that are both faithful to the model's behavior and consistent with each other while requiring fewer perturbations than baseline methods.

## Method Summary
The C2FA method solves a joint optimization problem to estimate HiFAs and LoFAs for nested-structured inputs. It uses ADMM to enforce consistency constraints between the two attribution levels while maintaining faithfulness to the black-box model. The method generates perturbations for both high-level and low-level features, obtains predictions from the black-box model, and solves weighted least squares problems with ℓ2 regularization. The consistency property bridges the gap between surrogate models, compensating for insufficient queries and mitigating missingness bias effects.

## Key Results
- C2FA achieves higher NDCG and AUROC scores compared to baseline methods (LIME, MILLI variants) on image and text classification tasks
- The method demonstrates better consistency between HiFAs and LoFAs, measured by ∥α - Mβ†∥² and MIHL agreement
- C2FA requires fewer queries to the black-box model while maintaining or improving explanation quality
- On image MIL task, C2FA shows superior deletion and insertion metrics compared to baselines

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Consistency constraints improve estimation accuracy when perturbation budgets are limited
- Mechanism: ADMM optimization enforces αj = sum of LoFAs for high-level feature j, creating shared structure that regularizes both levels and reduces independent queries needed
- Core assumption: Black-box model behavior is approximately linear in local region and respects nested feature decomposition
- Evidence anchors: [abstract] "produces HiFAs and LoFAs that are both faithful... using a smaller number of queries"; [section] "consistency property bridges gap between surrogate models"
- Break condition: If model behavior violates nested decomposition assumption, consistency constraints may introduce bias

### Mechanism 2
- Claim: Joint estimation mitigates missingness bias in perturbation-based attribution methods
- Mechanism: Optimizing both levels together with consistency constraints reduces variance from different masking behaviors
- Core assumption: Missingness bias affects high-level and low-level attribution estimation differently, joint estimation can compensate
- Evidence anchors: [section] "missingness bias [Jain et al., 2022]"; [section] "consistency constraints bridge gap... mitigate negative effects"
- Break condition: If missingness bias is so severe that model behavior is fundamentally non-additive, constraints may enforce incorrect relationships

### Mechanism 3
- Claim: Better faithfulness to black-box model's decision boundaries compared to separate estimation
- Mechanism: Simultaneous optimization with consistency constraints creates surrogate models that better approximate local behavior
- Core assumption: Linear surrogate models with consistency constraints can approximate black-box behavior more accurately than separate models
- Evidence anchors: [abstract] "HiFAs and LoFAs... are accurate, faithful to behaviors"; [section] "faithful to black-box models and consistent"
- Break condition: If black-box model's decision boundary is highly non-linear or discontinuous, even consistency-constrained surrogates may fail

## Foundational Learning

- Concept: Alternating Direction Method of Multipliers (ADMM)
  - Why needed here: Solves joint optimization problem with consistency constraints between HiFAs and LoFAs
  - Quick check question: What are the three update steps in ADMM for this problem, and how do they enforce the consistency constraint?

- Concept: Feature attribution methods (LIME, Kernel SHAP)
  - Why needed here: Builds on these approaches by extending to nested feature structures with consistency constraints
  - Quick check question: How does perturbation generation differ from standard LIME when dealing with nested features?

- Concept: Multiple Instance Learning (MIL) and set functions
  - Why needed here: Nested feature structure modeled as set function where high-level features are instances and low-level features are elements
  - Quick check question: In MIL setting, what is relationship between instance attributions (HiFAs) and bag-level prediction?

## Architecture Onboarding

- Component map: Input -> Mask functions ϕH/ϕL -> Black-box model f -> Surrogate models eH/eL -> ADMM optimizer -> Consistent HiFAs and LoFAs

- Critical path:
  1. Generate perturbations for both high-level and low-level features
  2. Obtain black-box predictions for all perturbations
  3. Initialize ADMM variables (α, β, auxiliary variables, Lagrange multipliers)
  4. Iteratively update α, β, and auxiliary variables until convergence
  5. Return consistent HiFAs and LoFAs

- Design tradeoffs:
  - Query efficiency vs. accuracy: Joint estimation reduces queries but may introduce bias if consistency assumption violated
  - Regularization strength: Balancing between fitting black-box model and enforcing consistency
  - Hyperparameter tuning: Requires validation data to set λH, λL, and µ1, µ2

- Failure signatures:
  - High loss values in surrogate models indicate poor approximation of black-box behavior
  - Large ∥α - Mβ†∥² suggests consistency constraints not being satisfied
  - Degraded performance compared to separate estimation methods on validation data

- First 3 experiments:
  1. Compare C2FA with LIME and MILLI on simple MIL dataset with ground truth labels, measuring NDCG and consistency scores
  2. Test sensitivity to perturbation budget by varying NH and NL and measuring faithfulness metrics
  3. Evaluate on text classification with language models, comparing deletion/insertion scores and MIHL agreement across methods

## Open Questions the Paper Calls Out

- Question: How does C2FA perform on more complex nested structures like hierarchical or multi-level nested inputs?
  - Basis in paper: [inferred] Paper mentions future work includes extending method to handle more complex nested structures
  - Why unresolved: Current implementation focuses on two-level nested structures, acknowledges need to explore more complex structures
  - What evidence would resolve it: Experimental results comparing C2FA's performance on multi-level nested structures against baseline methods

- Question: What is impact of different types of regularizations and constraints (sparsity, non-negativity) on estimated HiFAs and LoFAs?
  - Basis in paper: [explicit] Paper mentions ADMM allows various regularizations but doesn't explore these in detail
  - Why unresolved: Briefly mentions possibility of incorporating different regularizations but doesn't investigate their impact
  - What evidence would resolve it: Experiments comparing C2FA with different regularizations and constraints, with qualitative analysis of resulting explanations

## Limitations

- The consistency assumption between HiFAs and LoFAs may not hold for all model architectures, particularly those with complex interactions across feature levels
- Method's performance is sensitive to hyperparameter choices (λH, λL, ADMM convergence criteria) though paper lacks extensive sensitivity analysis
- Experimental validation limited to two specific tasks (image MIL and text classification) may not generalize to other nested data structures or model types

## Confidence

- High: Mathematical formulation and optimization procedure are sound and well-specified
- Medium: Empirical improvements over baselines are demonstrated but could benefit from more extensive ablation studies
- Medium: Theoretical justification for consistency constraints is reasonable but lacks rigorous proof of optimality

## Next Checks

1. Conduct sensitivity analysis across different regularization parameter settings and perturbation budgets to establish robustness
2. Test the method on additional nested data structures (hierarchical time series or graph-based features) to assess generalizability
3. Perform ablation studies to quantify the specific contribution of consistency constraints versus other methodological choices