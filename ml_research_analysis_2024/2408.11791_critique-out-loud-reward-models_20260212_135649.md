---
ver: rpa2
title: Critique-out-Loud Reward Models
arxiv_id: '2408.11791'
source_url: https://arxiv.org/abs/2408.11791
tags:
- reward
- response
- cloud
- information
- critiques
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Critique-out-Loud (CLoud) reward models,
  which combine language generation with preference modeling by first generating a
  critique of a response and then predicting a scalar reward. Compared to classic
  reward models, CLoud models improve pairwise preference classification accuracy
  on RewardBench by 4.65 and 5.84 percentage points for 8B and 70B models respectively,
  and achieve Pareto improvements in Best-of-N win rates on ArenaHard.
---

# Critique-out-Loud Reward Models

## Quick Facts
- arXiv ID: 2408.11791
- Source URL: https://arxiv.org/abs/2408.11791
- Reference count: 36
- Key outcome: CLoud reward models improve pairwise preference classification accuracy on RewardBench by 4.65-5.84 percentage points and achieve Pareto improvements on ArenaHard

## Executive Summary
This paper introduces Critique-out-Loud (CLoud) reward models, which improve upon classic reward models by explicitly generating critiques of responses before predicting scalar rewards. By enabling step-by-step reasoning similar to Chain-of-Thought prompting, CLoud models address the limitation of classic reward models that cannot explicitly reason about response quality. The study demonstrates significant performance improvements on preference classification benchmarks and explores the importance of on-policy training and self-consistency decoding for reasoning tasks.

## Method Summary
The CLoud reward model architecture consists of a base LLM (Llama-3-8B or 70B) with preserved LM and reward heads. The method involves three key steps: first, training the LM head via supervised fine-tuning on oracle critiques to generate quality critiques; second, replacing oracle critiques with self-generated critiques to create a new training dataset; and third, training the reward head conditioned on these self-generated critiques using a joint loss combining SFT and preference modeling. The approach is designed to minimize distribution shift between training and inference while enabling explicit reasoning about response quality.

## Key Results
- CLoud reward models improve pairwise preference classification accuracy on RewardBench by 4.65 percentage points (8B) and 5.84 percentage points (70B) compared to classic reward models
- CLoud models achieve Pareto improvements in Best-of-N win rates on ArenaHard, winning more comparisons without losing any
- Self-consistency decoding provides benefits specifically for reasoning tasks with short horizons

## Why This Works (Mechanism)

### Mechanism 1
- Claim: CLoud reward models improve performance by allowing explicit reasoning before scoring, overcoming the implicit reasoning limitation of classic reward models.
- Mechanism: The model generates a natural language critique of the response before predicting a scalar reward, enabling step-by-step reasoning similar to Chain-of-Thought prompting.
- Core assumption: The critique generation process provides valuable intermediate reasoning that improves the accuracy of the final reward prediction.
- Evidence anchors:
  - [abstract] "To enable reward models to reason explicitly about the quality of a response, we introduce Critique-out-Loud (CLoud) reward models."
  - [section 1] "We hypothesize that this limits the performance of classic reward models as they cannot explicitly reason about the quality of the response in a Chain-of-Thought (CoT) (Wei et al., 2022) like manner."
  - [corpus] Found 25 related papers (using 8). Average neighbor FMR=0.385, average citations=0.0.
- Break condition: If the critique generation process fails to capture relevant quality signals or introduces noise that degrades reward prediction accuracy.

### Mechanism 2
- Claim: On-policy training is essential for CLoud reward models because it ensures the reward head sees critiques in the same distribution as during inference.
- Mechanism: The model is trained by first generating critiques and then training the reward head on these self-generated critiques, matching the training and inference distributions.
- Core assumption: Distribution mismatch between oracle critiques (used in training) and self-generated critiques (used in inference) degrades performance.
- Evidence anchors:
  - [section 2.2] "We choose to train the reward head on self-generated critiques as to minimizes the distribution shift in the critiques seen by the reward head between training and inference when oracle critiques are not available."
  - [section 3.2] "We train an off-policy CLoud reward model by training on oracle critiques instead of self-generated critiques."
  - [corpus] Found 25 related papers (using 8). Average neighbor FMR=0.385, average citations=0.0.
- Break condition: If the critique generation quality is poor, on-policy training may amplify errors rather than improve performance.

### Mechanism 3
- Claim: Self-consistency decoding improves CLoud reward model performance for reasoning tasks by marginalizing over multiple critique samples to provide a better reward estimate.
- Mechanism: Multiple critiques are sampled and the final reward is computed as the mean of rewards predicted from each critique, reducing variance in the reward estimation.
- Core assumption: Different critique samples capture complementary aspects of response quality, and averaging their rewards provides a more robust estimate.
- Evidence anchors:
  - [abstract] "we explore how to exploit the dynamic inference compute capabilities of CLoud reward models by performing self-consistency decoding for reward prediction."
  - [section 3.3] "We find that reasoning is the only category that benefits from additional inference compute in the form of self-consistency."
  - [corpus] Found 25 related papers (using 8). Average neighbor FMR=0.385, average citations=0.0.
- Break condition: If the critiques are highly correlated or if averaging introduces bias that outweighs the variance reduction benefit.

## Foundational Learning

- Concept: Bradley-Terry preference modeling
  - Why needed here: The reward head is trained using the Bradley-Terry model to predict higher rewards for preferred responses.
  - Quick check question: What does the Bradley-Terry model assume about pairwise comparisons between items?

- Concept: Supervised finetuning for language generation
  - Why needed here: The LM head is trained via SFT on oracle critiques to learn how to generate quality critiques of responses.
  - Quick check question: How does supervised finetuning differ from reinforcement learning in terms of the training signal provided?

- Concept: Distribution matching in training vs inference
  - Why needed here: The on-policy training approach ensures the reward head sees critiques in the same distribution as during inference, avoiding distribution shift issues.
  - Quick check question: What problems can arise when training and inference data come from different distributions?

## Architecture Onboarding

- Component map:
  Base LLM (Llama-3-8B or 70B) -> LM head (preserved and trained for critique generation) -> Reward head (trained to predict scalar rewards conditioned on critiques) -> Oracle critique generator (Llama-3.1-405B-Instruct for dataset construction)

- Critical path:
  1. Train LM head on oracle critiques via SFT
  2. Generate self-critiques using finetuned LM head
  3. Train reward head on prompts, responses, and self-critiques using Bradley-Terry loss
  4. At inference: generate critique â†’ predict reward conditioned on critique

- Design tradeoffs:
  - Complexity vs performance: CLoud adds complexity but improves accuracy by 4.65-5.84 percentage points
  - Inference time vs quality: Self-consistency improves accuracy but increases inference compute
  - Data requirements: Requires oracle critiques for training, which may be expensive to obtain

- Failure signatures:
  - Poor critique quality leading to inaccurate rewards
  - Overfitting to specific critique patterns
  - Distribution shift between training and inference critiques
  - Self-consistency not improving performance for non-reasoning tasks

- First 3 experiments:
  1. Compare classic vs CLoud reward model accuracy on a small validation set to verify the core improvement
  2. Test on-policy vs off-policy training by training two models with different critique distributions
  3. Evaluate self-consistency with different numbers of critique samples on reasoning vs non-reasoning tasks

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do CLoud reward models perform when trained on human-generated critiques instead of model-generated ones?
- Basis in paper: [explicit] The paper mentions that oracle critiques are generated by Llama-3.1-405B-Instruct to approximate human critiques, and that self-generated critiques are used during training to minimize distribution shift.
- Why unresolved: The study uses model-generated critiques as a proxy for human critiques and does not compare the performance when using actual human critiques.
- What evidence would resolve it: A comparison of CLoud reward model performance when trained on human critiques versus model-generated critiques, measured by preference classification accuracy and BoN win rates.

### Open Question 2
- Question: How does the performance of CLoud reward models scale with larger base models (e.g., Llama-3-405B)?
- Basis in paper: [inferred] The paper only experiments with Llama-3-8B and 70B base models and notes improvements at these scales.
- Why unresolved: The study does not explore the performance of CLoud reward models with significantly larger base models.
- What evidence would resolve it: Training and evaluating CLoud reward models using larger base models like Llama-3-405B and comparing their performance metrics to those of smaller models.

### Open Question 3
- Question: Can CLoud reward models be effectively integrated with more complex preference modeling objectives, such as those handling intransitive preferences or multi-objective rewards?
- Basis in paper: [explicit] The paper states that improvements of CLoud reward models are orthogonal to preference modeling objectives and suggests future work should explore this composition.
- Why unresolved: The study focuses on Bradley-Terry preference modeling and does not investigate integration with other preference modeling methods.
- What evidence would resolve it: Implementing CLoud reward models with different preference modeling objectives and evaluating their performance to see if they maintain or improve upon the gains observed with Bradley-Terry modeling.

## Limitations

- Reliance on oracle critiques generated by a much stronger model (Llama-3.1-405B-Instruct), which may not be practically accessible to all researchers
- Self-consistency decoding benefits appear narrowly focused on reasoning tasks, suggesting limited generalizability across different response types
- Distribution shift between oracle and self-generated critiques remains a concern, particularly for off-policy training variants

## Confidence

**High Confidence Claims:**
- CLoud reward models outperform classic reward models on RewardBench (4.65-5.84 percentage point improvements)
- On-policy training provides clear benefits over off-policy training with oracle critiques
- The architecture design of separating critique generation from reward prediction is sound

**Medium Confidence Claims:**
- Self-consistency decoding improves reasoning task performance
- The mechanism of explicit reasoning through critique generation is the primary driver of improvements
- Pareto improvements on ArenaHard translate to real-world preference alignment

**Low Confidence Claims:**
- The scalability of CLoud models to larger datasets and more diverse tasks
- The robustness of performance when oracle critiques are unavailable or of lower quality
- The general applicability of self-consistency decoding across all task categories

## Next Checks

1. **Distribution Shift Analysis**: Systematically measure the quality gap between oracle and self-generated critiques across different domains and response types, using automated metrics and human evaluation to quantify the distribution shift impact.

2. **Scalability Test**: Evaluate CLoud models on datasets significantly larger than RewardBench (e.g., 10x more samples) to assess whether the performance improvements hold at scale and identify any degradation patterns.

3. **Resource Efficiency Benchmark**: Compare the inference-time compute overhead of CLoud models with their performance gains, measuring whether the dynamic inference compute capability provides favorable cost-benefit tradeoffs across different deployment scenarios.