---
ver: rpa2
title: Evaluating the Performance of Large Language Models via Debates
arxiv_id: '2406.11044'
source_url: https://arxiv.org/abs/2406.11044
tags:
- llama-3-70b
- side
- mixtral-8x7b
- llama-2-70b
- gpt-4
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proposes an automated framework to evaluate and rank
  large language models (LLMs) using debates between models judged by another LLM.
  The method addresses the scalability and flexibility issues of existing evaluation
  approaches that rely on fixed benchmarks or human input.
---

# Evaluating the Performance of Large Language Models via Debates

## Quick Facts
- arXiv ID: 2406.11044
- Source URL: https://arxiv.org/abs/2406.11044
- Authors: Behrad Moniri; Hamed Hassani; Edgar Dobriban
- Reference count: 40
- Primary result: Automated debate framework ranks LLMs with 0.0833 normalized Kendall tau distance to human-based rankings

## Executive Summary
This paper introduces a novel framework for evaluating and ranking large language models (LLMs) using automated debates between models judged by another LLM. The approach addresses key limitations of existing evaluation methods that rely on fixed benchmarks or human input. In this framework, pairs of LLMs debate predefined topics over multiple rounds, with a judge LLM scoring their arguments based on criteria such as clarity, factuality, persuasiveness, and coherence. The overall winner is determined by the highest number of wins across debates, eliminating the need for costly human crowdsourcing while producing rankings that closely align with human-based evaluations.

## Method Summary
The proposed method involves selecting pairs of LLMs to debate on predefined topics, with each debate consisting of four rounds. The models take turns making arguments, supporting them with logic and facts. A judge LLM (typically GPT-4) evaluates each debate based on seven criteria, scoring arguments from 1-10. To eliminate bias, debates are conducted in both directions with models switching roles. The framework aggregates results to determine winners and generate final rankings. The approach was tested with state-of-the-art LLMs including Llama-2 variants, Llama-3, Vicuna, Mixtral, and GPT models.

## Key Results
- The framework achieved a normalized Kendall tau distance of 0.0833 when compared to Chatbot Arena human-based rankings
- Rankings remained consistent when using Llama-3-70b as an alternative judge instead of GPT-4
- The approach successfully evaluated models across diverse debate topics spanning science, philosophy, and politics
- Computational cost scales quadratically with the number of models being evaluated

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The debate framework leverages the judge LLM's ability to evaluate argumentative quality across multiple criteria (clarity, factuality, persuasiveness, etc.)
- Mechanism: By structuring debates as multi-round interactions with clear evaluation criteria, the judge LLM can assess model performance in a controlled, repeatable way
- Core assumption: The judge LLM can reliably evaluate argumentative quality based on predefined criteria
- Evidence anchors: [abstract] "a judge LLM scores their arguments based on criteria such as clarity, factuality, persuasiveness, and coherence"
- Break condition: If the judge LLM cannot consistently evaluate argumentative quality, the framework fails to produce reliable rankings

### Mechanism 2
- Claim: Running debates in both directions (role reversal) eliminates bias from debate structure and judge preferences
- Mechanism: By conducting two debates per topic with models switching sides, any systematic advantage from going first or second is cancelled out
- Core assumption: The debate structure itself doesn't inherently favor one side over another
- Evidence anchors: [abstract] "We run two debates because the two sides of the argument might not be equally hard to argue for"
- Break condition: If certain debate topics inherently favor one side regardless of model capability, the role reversal won't fully eliminate bias

### Mechanism 3
- Claim: The debate framework tests multiple relevant skills beyond simple knowledge recall
- Mechanism: Debates require models to demonstrate argumentative reasoning, inconsistency recognition, and prioritization skills, not just domain knowledge
- Core assumption: These skills are relevant and necessary for real-world LLM applications
- Evidence anchors: [abstract] "This method assesses not only domain knowledge, but also skills such as argumentative reasoning and inconsistency recognition"
- Break condition: If the target application doesn't require these skills, the framework may overvalue debate performance

## Foundational Learning

- Concept: LLM evaluation methodologies
  - Why needed here: Understanding different evaluation approaches (static benchmarks, human-based, game-based) is crucial for contextualizing the debate framework
  - Quick check question: What are the main limitations of static benchmarks that the debate framework aims to address?

- Concept: Debate structure and argumentation theory
  - Why needed here: The framework relies on structured debate mechanics and understanding what makes arguments persuasive
  - Quick check question: What are the key criteria used by the judge LLM to evaluate debate performance?

- Concept: Kendall tau distance and ranking correlation
  - Why needed here: The paper validates its rankings against human-based rankings using normalized Kendall tau distance
  - Quick check question: What does a normalized Kendall tau distance of 0.0833 indicate about the framework's alignment with human rankings?

## Architecture Onboarding

- Component map: Topic selection module (human input) -> Debating model pool (multiple LLMs) -> Judge LLM (GPT-4 or alternative) -> Debate orchestration engine -> Result aggregation and ranking system

- Critical path:
  1. Load debate topics
  2. For each model pair, conduct debates in both directions
  3. Judge evaluates each debate
  4. Aggregate results to determine winners
  5. Generate final rankings

- Design tradeoffs:
  - Judge strength vs. evaluation criteria: Stronger judges may better assess factuality but may be less scalable
  - Number of rounds vs. computational cost: More rounds provide richer evaluation but increase cost
  - Topic selection vs. coverage: Broader topics increase applicability but may reduce depth

- Failure signatures:
  - Rankings inconsistent across different judge LLMs
  - Systematic advantage for certain model families
  - Judge consistently unable to distinguish between strong/weak arguments

- First 3 experiments:
  1. Verify role reversal eliminates bias: Compare rankings when debates are only run in one direction
  2. Judge consistency test: Run same debates with different judge LLMs and measure ranking correlation
  3. Skill isolation test: Design topics that specifically test one skill (e.g., inconsistency recognition) to validate framework captures intended abilities

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the debate framework be adapted to evaluate LLMs on tasks requiring high factual accuracy, given that the judge LLM may be influenced by the persuasiveness of arguments rather than their factual correctness?
- Basis in paper: [explicit] The paper acknowledges that when using weaker LLMs as judges, the winner may be determined by the ability to convince the judge rather than by providing factual answers, and notes this limits applicability to tasks where factuality is crucial.
- Why unresolved: The paper does not propose specific modifications to the debate framework to prioritize factual accuracy over persuasiveness when the judge's factual evaluation capability is limited.
- What evidence would resolve it: Experimental results comparing rankings generated by debate frameworks with different judge models (e.g., those specialized in factuality vs. general-purpose) on tasks requiring high factual accuracy.

### Open Question 2
- Question: What is the optimal number of debate topics and rounds needed to generate reliable and stable LLM rankings, and how does this scale with the number of models being compared?
- Basis in paper: [inferred] The paper uses 25 topics and 4 rounds for debates but does not systematically explore how the number of topics or rounds affects ranking stability or reliability.
- Why unresolved: The paper does not provide a systematic analysis of how the number of topics and rounds impacts the consistency of rankings across different experimental runs or model sets.
- What evidence would resolve it: Empirical studies showing ranking stability and consistency across different numbers of topics and rounds, and analysis of how these parameters affect computational cost and scalability.

### Open Question 3
- Question: How can the debate framework be extended to multilingual settings, particularly for languages with complex morphology, to ensure fair and accurate evaluation of LLMs across different languages?
- Basis in paper: [explicit] The paper notes that the method has been primarily evaluated on English, which has limited morphological complexity, and acknowledges this could restrict applicability to other languages.
- Why unresolved: The paper does not explore or propose specific adaptations of the debate framework for multilingual evaluation or address challenges posed by languages with complex morphology.
- What evidence would resolve it: Experimental results comparing LLM rankings generated by the debate framework across multiple languages, including those with complex morphology, and analysis of any systematic differences or biases observed.

## Limitations
- The framework's computational cost scales quadratically with the number of models being evaluated
- Reliance on a single judge LLM (GPT-4) raises concerns about judge-dependent biases
- Topic selection, while diverse, may not fully capture the breadth of real-world LLM applications

## Confidence
- **High Confidence**: The core mechanism of using debates to evaluate LLMs and the role reversal approach to reduce bias
- **Medium Confidence**: The effectiveness of the judge LLM in reliably scoring debates across different topics and model pairs
- **Medium Confidence**: The alignment with human-based rankings (Chatbot Arena) as measured by normalized Kendall tau distance

## Next Checks
1. Test judge consistency by running the same debates with multiple judge LLMs and measuring ranking correlation
2. Conduct ablation studies by varying the number of debate rounds to determine optimal trade-off between evaluation quality and computational cost
3. Validate the framework on specialized domains (e.g., medical or legal) where factuality and argumentative reasoning are critical