---
ver: rpa2
title: 'MultiChartQA: Benchmarking Vision-Language Models on Multi-Chart Problems'
arxiv_id: '2410.14179'
source_url: https://arxiv.org/abs/2410.14179
tags:
- chart
- question
- charts
- arxiv
- reasoning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces MultiChartQA, a new benchmark designed to
  evaluate multimodal large language models (MLLMs) on complex reasoning tasks involving
  multiple charts. The benchmark includes 2,000 questions spanning four categories:
  direct question answering, parallel question answering, comparative reasoning, and
  sequential reasoning.'
---

# MultiChartQA: Benchmarking Vision-Language Models on Multi-Chart Problems

## Quick Facts
- arXiv ID: 2410.14179
- Source URL: https://arxiv.org/abs/2410.14179
- Authors: Zifeng Zhu; Mengzhao Jia; Zhihan Zhang; Lang Li; Meng Jiang
- Reference count: 25
- MultiChartQA evaluates MLLMs on complex multi-chart reasoning with 2,000 questions across 4 categories

## Executive Summary
MultiChartQA is a new benchmark designed to evaluate multimodal large language models (MLLMs) on complex reasoning tasks involving multiple charts. The benchmark includes 2,000 questions spanning four categories: direct question answering, parallel question answering, comparative reasoning, and sequential reasoning. These questions are derived from real-world multi-chart articles and require advanced capabilities such as multi-image encoding, precise information localization, and multi-hop reasoning. The evaluation of 20 MLLMs, including both closed-source and open-source models, reveals significant performance gaps compared to humans, particularly in sequential reasoning tasks.

## Method Summary
The benchmark uses 1,370 charts collected from diverse real-world sources, including ArXiv, OECD, Our World in Data, and others. Each question is paired with multiple semantically related charts from the same article. The evaluation metrics include relaxed accuracy for numerical answers and exact match for non-numeric answers. For questions with multiple sub-questions, accuracy is calculated based on the proportion of correctly answered sub-questions. The benchmark evaluates 20 mainstream MLLMs, including 4 closed-source models, 13 open-source general-purpose models, and 3 chart-domain specialized models. The evaluation includes settings with and without Chain-of-Thought (CoT) reasoning and with merged charts.

## Key Results
- MLLMs perform significantly worse than humans on sequential reasoning tasks
- Models exhibit performance degradation on parallel questions compared to direct ones
- Chain-of-Thought reasoning significantly improves MLLMs' performance on MultiChartQA

## Why This Works (Mechanism)

### Mechanism 1
- Claim: MultiChartQA exposes fundamental limitations in MLLMs' ability to integrate information across multiple charts, particularly in sequential reasoning tasks.
- Mechanism: By presenting semantically related chart sets with questions requiring multi-hop reasoning, the benchmark forces models to extract and combine information across charts, revealing weaknesses in cross-chart integration.
- Core assumption: Real-world chart comprehension often involves multiple charts that need to be analyzed together, and current benchmarks inadequately capture this complexity.
- Evidence anchors:
  - [abstract]: "Current benchmarks primarily focus on single-chart tasks, neglecting the multi-hop reasoning required to extract and integrate information from multiple charts, which is essential in practical applications."
  - [section 4.2]: "Models perform poorly on sequential reasoning among four categories. The sequential reasoning questions present significant challenges for MLLMs."
- Break condition: If models achieve human-level performance on sequential reasoning tasks, indicating they can effectively integrate information across multiple charts.

### Mechanism 2
- Claim: The design of question categories in MultiChartQA progressively increases the complexity of chart comprehension tasks, revealing different capabilities and limitations of MLLMs.
- Mechanism: The four question types (direct, parallel, comparative, sequential) require increasingly complex reasoning abilities, from simple chart identification to multi-hop reasoning across multiple charts.
- Core assumption: Different types of chart comprehension tasks require distinct cognitive abilities, and a comprehensive benchmark should evaluate all these abilities.
- Evidence anchors:
  - [section 3.2]: "We concentrate on four key capabilities in multi-chart tasks and design four types of questions, i.e., direct questions, parallel questions, comparative reasoning, and sequential reasoning."
  - [section 4.2]: "Models exhibit performance degradation on parallel questions compared to direct ones. Although the difficulty of each sub-question in parallel questions is comparable to that of direct questions, the model's performance drops significantly on parallel questions."
- Break condition: If models perform equally well across all question categories, suggesting the distinction between task complexities is not meaningful.

### Mechanism 3
- Claim: Chain-of-Thought reasoning significantly improves MLLMs' performance on MultiChartQA, highlighting the importance of explicit reasoning steps in complex multi-chart tasks.
- Mechanism: By encouraging models to generate intermediate reasoning steps before providing final answers, CoT helps models organize their thought process and perform multi-step reasoning more effectively.
- Core assumption: Complex reasoning tasks benefit from explicit decomposition into intermediate steps, which helps models avoid errors in multi-step reasoning.
- Evidence anchors:
  - [section 4.3]: "Most MLLMs experience a significant performance drop in the w/o CoT setting, especially among closed-source models."
  - [section 4.3]: "One possible explanation is that chain-of-thought, as an emergent ability (Kojima et al., 2022), does not result in performance gains for smaller models ( âˆ¼ 7B parameters)."
- Break condition: If models achieve similar performance with and without CoT, suggesting that explicit reasoning steps are not necessary for this task.

## Foundational Learning

- Concept: Multi-hop reasoning
  - Why needed here: MultiChartQA requires models to perform reasoning across multiple charts, often involving several steps of information extraction and integration.
  - Quick check question: Can you explain how you would answer a question that requires information from Chart 1 and Chart 2, where the answer to the first chart is needed to understand the second chart?

- Concept: Information localization in multi-image contexts
  - Why needed here: Models need to accurately identify which chart contains the relevant information for each question, especially when multiple charts are presented simultaneously.
  - Quick check question: How would you design a system to help a model quickly locate the relevant information in a set of charts when asked a specific question?

- Concept: Chart structure understanding
  - Why needed here: Understanding the structural elements of charts (axes, legends, subplots) is crucial for answering both direct and comparative reasoning questions.
  - Quick check question: What are the key structural elements you would look for in a chart to answer questions about its organization and content?

## Architecture Onboarding

- Component map:
  Chart preprocessing -> Visual feature extraction -> Information localization -> Cross-chart integration -> Reasoning engine -> Answer generation

- Critical path:
  1. Input: Multiple semantically related charts + question
  2. Chart preprocessing: Merge or maintain separate chart representations
  3. Visual feature extraction: Extract chart elements and content
  4. Information localization: Identify relevant chart regions for each question
  5. Cross-chart integration: Combine information from multiple charts as needed
  6. Reasoning: Apply chain-of-thought or similar approach
  7. Answer generation: Format and validate the final answer

- Design tradeoffs:
  - Single vs. separate chart processing: Merging charts simplifies input but may lose semantic distinctions
  - End-to-end vs. modular approach: Modular systems may be more interpretable but harder to train
  - Chain-of-thought vs. direct reasoning: CoT improves performance but increases computational cost

- Failure signatures:
  - Perceptual errors: Misidentification of chart elements or OCR failures
  - Reasoning errors: Correct chart understanding but incorrect logical conclusions
  - Instruction understanding errors: Misinterpretation of question constraints

- First 3 experiments:
  1. Compare performance with and without chart references to evaluate information localization capabilities
  2. Test different chart preprocessing approaches (merged vs. separate) to find optimal input format
  3. Evaluate the impact of chain-of-thought reasoning on different question categories to identify where it's most beneficial

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can MultiChartQA be scaled up efficiently without compromising annotation quality?
- Basis in paper: [inferred] The paper explicitly states that the key limitation is the cost of manual annotation, resulting in a limited sample size, and suggests semi-automated methods as future work.
- Why unresolved: Scaling up requires balancing annotation efficiency with quality control, and no specific semi-automated approach has been proposed or tested yet.
- What evidence would resolve it: Demonstration of a semi-automated pipeline that maintains high annotation accuracy while significantly increasing sample size.

### Open Question 2
- Question: What specific architectural improvements could enhance MLLMs' performance on sequential reasoning tasks in MultiChartQA?
- Basis in paper: [explicit] The paper highlights that sequential reasoning questions present significant challenges for MLLMs, with performance lagging notably behind humans.
- Why unresolved: While the paper identifies the performance gap, it does not propose specific architectural modifications or training strategies to address this weakness.
- What evidence would resolve it: Implementation and evaluation of specialized architectures or training methods that show marked improvement on sequential reasoning tasks.

### Open Question 3
- Question: How does the removal of chart references impact MLLMs' ability to perform information localization across different chart types?
- Basis in paper: [explicit] The paper includes experiments showing performance drops when chart references are removed, indicating challenges with information localization.
- Why unresolved: The experiments only demonstrate the existence of the problem, not the underlying mechanisms or potential solutions for improving reference-free information localization.
- What evidence would resolve it: Analysis of how different chart types affect localization difficulty and development of methods to improve reference-free chart understanding.

## Limitations
- Benchmark relies on human-generated questions, which may introduce subtle biases
- Merging multiple charts into a single composite image could influence model performance
- Evaluation lacks detailed breakdowns of model errors (perceptual vs. reasoning vs. instruction-following failures)

## Confidence
- High confidence: MLLMs perform significantly worse than humans on sequential reasoning tasks
- Medium confidence: Chain-of-thought reasoning universally improves performance
- Low confidence: Four question categories capture all relevant dimensions of multi-chart reasoning

## Next Checks
1. Conduct an ablation study to isolate the impact of chart merging on model performance by comparing results with and without merged inputs
2. Perform a detailed error analysis to categorize model failures (perceptual, reasoning, or instruction-following) and identify dominant failure modes
3. Test model performance on a subset of questions with varying chart complexity to determine which factors most influence accuracy