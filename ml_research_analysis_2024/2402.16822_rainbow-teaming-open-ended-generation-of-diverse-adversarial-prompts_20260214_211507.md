---
ver: rpa2
title: 'Rainbow Teaming: Open-Ended Generation of Diverse Adversarial Prompts'
arxiv_id: '2402.16822'
source_url: https://arxiv.org/abs/2402.16822
tags:
- teaming
- prompts
- adversarial
- llama
- archive
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Rainbow Teaming uses quality-diversity search to automatically
  generate diverse adversarial prompts for LLMs. It treats prompt generation as an
  open-ended optimization problem, producing an archive of effective attacks across
  safety, question answering, and cybersecurity domains.
---

# Rainbow Teaming: Open-Ended Generation of Diverse Adversarial Prompts

## Quick Facts
- arXiv ID: 2402.16822
- Source URL: https://arxiv.org/abs/2402.16822
- Reference count: 19
- Primary result: Quality-diversity search generates diverse adversarial prompts with >90% attack success rate across safety, QA, and cybersecurity domains.

## Executive Summary
Rainbow Teaming introduces a novel approach to adversarial prompt generation using quality-diversity search. By treating prompt generation as an open-ended optimization problem, it produces diverse and effective adversarial prompts that significantly improve the robustness of large language models when used for synthetic data fine-tuning. The method achieves strong cross-model transfer and outperforms existing black-box attack methods in both effectiveness and diversity.

## Method Summary
Rainbow Teaming uses MAP-Elites quality-diversity search to generate adversarial prompts. It maintains an archive where each cell contains the highest-fitness adversarial prompt for a specific combination of feature descriptors. New prompts are generated by mutating existing prompts, with mutations guided by target feature descriptors. A Judge LLM performs pairwise comparisons to evaluate effectiveness, avoiding reward hacking issues associated with score-based metrics. The system targets different domains using domain-specific feature descriptors and demonstrates effectiveness across safety, question answering, and cybersecurity applications.

## Key Results
- Generated hundreds of adversarial prompts with over 90% attack success rate on Llama 2-chat models
- Synthetic data fine-tuning with Rainbow Teaming prompts significantly improves model robustness
- Achieved strong cross-model transfer, with archives generated for one model size effective against others
- Demonstrated superior diversity compared to existing black-box attack methods

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Quality-diversity search efficiently explores the adversarial prompt space by balancing effectiveness and diversity.
- Mechanism: MAP-Elites maintains an archive where each cell contains the highest-fitness adversarial prompt for a specific combination of feature descriptors. New prompts are generated by mutating existing prompts, with mutations guided by target feature descriptors. The preference model evaluates effectiveness through pairwise comparisons.
- Core assumption: The fitness function correlates with true adversarial effectiveness.
- Evidence anchors: Abstract states Rainbow Teaming uses quality-diversity search for effective and diverse prompts; section 4 discusses attack success rate measures.

### Mechanism 2
- Claim: Directed mutations with feature descriptor specification maintain diversity and avoid mode collapse.
- Mechanism: Instead of randomly mutating prompts and classifying results, Rainbow Teaming specifies target feature descriptors in advance. The mutator LLM generates prompts targeting specific feature combinations, ensuring coverage across the entire feature space.
- Core assumption: The mutator LLM can generate diverse prompts when given specific feature targets.
- Evidence anchors: Section 3.2 discusses benefits of sampling feature descriptors in advance; section 4.5 mentions similarity filtering for diversity.

### Mechanism 3
- Claim: Preference model comparison is more robust than score-based evaluation and enables open-ended search.
- Mechanism: The Judge LLM compares two responses pairwise to determine which is more harmful, avoiding reward hacking associated with fixed-scale score maximization. This enables continuous improvement since there's no upper bound on "more harmful."
- Core assumption: Pairwise comparison better approximates human judgment than score-based metrics.
- Evidence anchors: Section 4.4 shows LLM pairwise comparisons have higher agreement with humans than single-answer grading.

## Foundational Learning

- Concept: Quality-Diversity (QD) Search
  - Why needed here: Rainbow Teaming needs to generate not just effective adversarial prompts but also a diverse set covering different attack strategies and risk categories.
  - Quick check question: What is the key difference between traditional optimization and quality-diversity search?
  - Answer: Traditional optimization finds a single best solution, while QD search finds a collection of high-performing solutions that are diverse across specified feature dimensions.

- Concept: MAP-Elites Algorithm
  - Why needed here: This specific QD algorithm provides the archive-based approach that allows Rainbow Teaming to maintain and evolve a diverse set of adversarial prompts.
  - Quick check question: How does MAP-Elites decide whether to replace an existing prompt in the archive?
  - Answer: It replaces the existing prompt if the new prompt has higher fitness (effectiveness) for the same feature descriptor.

- Concept: Feature Descriptors and Archive Structure
  - Why needed here: The feature descriptors define what diversity means in the context of adversarial prompts and structure the archive accordingly.
  - Quick check question: Why does Rainbow Teaming use categorical and numerical features rather than just one type?
  - Answer: Categorical features allow discrete coverage of different attack types, while numerical features enable continuous variation in aspects like prompt length.

## Architecture Onboarding

- Component map:
  - Archive: K-dimensional grid storing elite adversarial prompts
  - Mutator LLM: Generates new prompts by applying K mutations targeting specified feature descriptors
  - Target LLM: The model being attacked (e.g., Llama 2-chat)
  - Judge LLM: Compares responses to determine which prompt is more effective
  - Feature Descriptors: Define archive dimensions (e.g., Risk Category, Attack Style)
  - Fitness Function: Measures adversarial effectiveness (e.g., Llama Guard probability)

- Critical path:
  1. Sample prompt from archive
  2. Specify target feature descriptor
  3. Mutator LLM generates candidate prompt
  4. Target LLM generates response
  5. Judge LLM compares candidate vs existing prompt
  6. Update archive if candidate wins
  7. Repeat

- Design tradeoffs:
  - Using LLMs for all components vs rule-based alternatives: LLMs provide flexibility but are expensive and may have biases
  - Archive size vs coverage: Larger archives provide better coverage but require more computation
  - Fitness function choice: Should balance attack success rate with diversity and transferability

- Failure signatures:
  - Archive fills with similar prompts despite diversity features: Mutator isn't generating diverse enough prompts
  - ASR plateaus quickly: Fitness function isn't correlating with true adversarial effectiveness
  - Certain archive cells never get populated: Mutator can't generate prompts for those feature combinations

- First 3 experiments:
  1. Run Rainbow Teaming with minimal features (1-2 dimensions) to verify basic functionality and measure ASR
  2. Test different fitness functions (GPT-4 vs Llama Guard) to see impact on prompt diversity and effectiveness
  3. Measure transferability by applying archives generated for one model size to different model sizes

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the diversity of adversarial prompts generated by Rainbow Teaming compare to those generated by existing black-box methods like PAIR and TAP across different domains and model sizes?
- Basis in paper: The paper notes existing methods exhibit a lack of diversity but only compares to a score-based baseline within the same domain.
- Why unresolved: The paper doesn't directly compare to other black-box methods like PAIR and TAP.
- What evidence would resolve it: Comprehensive comparison of prompt diversity across multiple domains and model sizes using quantitative metrics.

### Open Question 2
- Question: How does the robustness of LLMs fine-tuned on Rainbow Teaming-generated synthetic data compare to those fine-tuned on data from other sources across different attack types and model architectures?
- Basis in paper: The paper demonstrates effectiveness of Rainbow Teaming-generated data for fine-tuning but doesn't compare to other data sources.
- Why unresolved: The paper only evaluates Rainbow Teaming-generated data without comparison to alternatives.
- What evidence would resolve it: Empirical comparison of robustness against various attack types and across different model architectures.

### Open Question 3
- Question: How does the choice of feature descriptors in Rainbow Teaming impact the diversity and effectiveness of the generated adversarial prompts?
- Basis in paper: The paper uses different feature descriptors for each domain but doesn't explore the impact of different choices.
- Why unresolved: The paper doesn't systematically investigate how feature descriptor choices affect results.
- What evidence would resolve it: Systematic study varying feature descriptors across domains and evaluating impact on diversity and effectiveness.

## Limitations
- Heavy reliance on LLM-based judgment for fitness scoring and pairwise comparisons may introduce bias
- Limited to Llama 2-chat models, restricting generalizability to other LLM architectures
- Requires access to powerful mutator LLM, creating computational cost and reproducibility barriers
- Long-term effectiveness of synthetic data fine-tuning remains uncertain as models evolve

## Confidence
**High Confidence**: Core quality-diversity search mechanism and MAP-Elites implementation are well-established; reported metrics are empirically measured; cross-model transferability is quantitatively validated.

**Medium Confidence**: Preference model effectiveness depends on LLM consistency; filtering strategies may need domain-specific tuning; safety transfer learning improvements could vary with different safety models.

**Low Confidence**: Real-world applicability without access to specific mutation templates; long-term robustness of synthetic data fine-tuning; computational overhead and practical deployment feasibility at scale.

## Next Checks
1. **Replication with Alternative Fitness Functions**: Validate Rainbow Teaming's effectiveness using different safety evaluation models beyond Llama Guard to test robustness against reward hacking.

2. **Cross-Architecture Transferability Test**: Apply Rainbow Teaming archives to non-Llama models (e.g., GPT-4, Claude) to assess generalizability and measure both ASR and diversity preservation.

3. **Cost-Benefit Analysis of Synthetic Data Fine-Tuning**: Quantify the trade-off between computational cost of generating diverse adversarial prompts and robustness improvement achieved through fine-tuning, comparing against alternative defense mechanisms.