---
ver: rpa2
title: Regulation of Language Models With Interpretability Will Likely Result In A
  Performance Trade-Off
arxiv_id: '2412.12169'
source_url: https://arxiv.org/abs/2412.12169
tags:
- concept
- performance
- which
- data
- user
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the challenge of regulating large language
  models (LLMs) for sensitive applications, such as insurance liability assessment,
  by making them use human-defined, interpretable concepts. The core method involves
  constraining an LLM to use only a subset of human-understandable, legally permissible
  features (regulatable features) for decision-making, achieved by integrating human-labeled
  concept data and prototypes into the model architecture.
---

# Regulation of Language Models With Interpretability Will Likely Result In A Performance Trade-Off

## Quick Facts
- arXiv ID: 2412.12169
- Source URL: https://arxiv.org/abs/2412.12169
- Authors: Eoin M. Kenny; Julie A. Shah
- Reference count: 6
- Primary result: Constraining LLMs to use only human-defined interpretable concepts reduces classification accuracy by 7.34% on average.

## Executive Summary
This paper addresses the challenge of regulating large language models (LLMs) for sensitive applications like insurance liability assessment by making them use human-defined, interpretable concepts. The core method involves constraining an LLM to use only a subset of human-understandable, legally permissible features (regulatable features) for decision-making, achieved by integrating human-labeled concept data and prototypes into the model architecture. The primary results show that this approach reduces classification accuracy by an average of 7.34% compared to unconstrained models, confirming a trade-off between regulation and performance. However, a user study with insurance adjusters demonstrated that the regulated model still improved human task performance speed and appropriate confidence, highlighting its practical utility despite the performance drop.

## Method Summary
The method involves integrating human-labeled concept data into an LLM architecture by creating prototypes for each concept and computing similarity scores between sentence embeddings and these prototypes. The model uses these similarity scores (weighted by human-defined importance) to make predictions instead of relying on black-box features. Specifically, the approach uses BERT-based encoders to generate sentence embeddings, computes similarity to human-labeled concept prototypes, and applies a weight matrix to determine classification logits. The model is trained with a combined loss function that includes both classification accuracy and concept-label prediction.

## Key Results
- Constraining LLMs to use only human-defined regulatable concepts reduces classification accuracy by 7.34% on average compared to unconstrained models.
- The regulated model improved human task performance speed and appropriate confidence in a user study with insurance adjusters, despite lower accuracy.
- Context-aware sentence encoding performed better for classification while context-unaware encoding worked better for concept classification tasks.

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Constraining an LLM to use only a subset of human-defined regulatable concepts forces the model to abandon optimal black-box features, resulting in a measurable accuracy drop.
- **Mechanism:** The regulatable feature set (R) is a strict subset of the interpretable feature set (I), which itself is a subset of the full latent space (L). The original black-box feature set (B) occupies a different subspace optimized for classification. By forcing the model to operate only within R, it loses access to features that minimize loss on the original task.
- **Core assumption:** The regulatable concepts do not perfectly overlap with the features the model originally learned to optimize classification performance.
- **Evidence anchors:**
  - [abstract] "The primary results show that this approach reduces classification accuracy by an average of 7.34% compared to unconstrained models"
  - [section] "it is necessarily a subset of I, given such constraints, a model relying only on R is guaranteed to have a performance equal to, or less than B or I"
- **Break condition:** If the regulatable concepts happen to perfectly align with the model's learned optimal features, no performance drop would occur.

### Mechanism 2
- **Claim:** Using human-labeled concept data to create prototypes enables the model to classify inputs based on interpretable, domain-specific features while maintaining some classification capability.
- **Mechanism:** The model computes similarity scores between sentence embeddings and concept prototypes, then uses these scores (weighted by human-defined importance) to make predictions. This replaces the original black-box decision process with one based on human-understandable concepts.
- **Core assumption:** The human-labeled concept data accurately captures the important features for the classification task and that averaging across labeled instances creates meaningful prototypes.
- **Evidence anchors:**
  - [section] "we employed two separate vendors to label parts of their sentences with important concepts for assessing liability that were defined by a domain expert"
  - [section] "there were eight labels (i.e., concepts) we asked them to assign"
- **Break condition:** If the human-labeled concepts are poorly chosen or the prototypes poorly represent the concept space, classification performance would degrade significantly beyond the expected 7.34% drop.

### Mechanism 3
- **Claim:** The regulation performance trade-off enables human-AI collaboration benefits despite lower model accuracy, as humans can understand and appropriately trust the model's reasoning.
- **Mechanism:** When humans can see which interpretable concepts the model used to make decisions, they can better calibrate their trust and make faster, more confident decisions, even if the model itself is less accurate than a black-box alternative.
- **Core assumption:** Human understanding of the model's reasoning process leads to appropriate trust calibration and improved collaboration, not just awareness of the reasoning.
- **Evidence anchors:**
  - [abstract] "a user study with insurance adjusters demonstrated that the regulated model still improved human task performance speed and appropriate confidence"
  - [section] "such systems actually improve human task performance speed and appropriate confidence in a realistic deployment setting compared to no AI assistance"
- **Break condition:** If humans misinterpret the explanations or if the explanations don't actually reflect the model's reasoning, trust calibration could fail and collaboration benefits would not materialize.

## Foundational Learning

- **Concept: Subset relationships between feature sets (R ⊆ I ⊆ L)**
  - Why needed here: Understanding why constraining to R necessarily reduces performance requires grasping how R relates to the full feature space and the original optimal features.
  - Quick check question: If R is a proper subset of I, and I is a proper subset of L, what can we say about the performance of a model constrained to R versus one using the full L?

- **Concept: Prototype-based classification**
  - Why needed here: The method relies on creating concept prototypes from human-labeled data and using similarity to these prototypes for classification, replacing the original black-box mechanism.
  - Quick check question: How does using a single prototype per concept compare to the original model's learned feature representations in terms of representational capacity?

- **Concept: Human-AI collaboration and appropriate trust**
  - Why needed here: The surprising result that humans benefit from interacting with a less accurate model depends on understanding how interpretable explanations affect human trust and decision-making.
  - Quick check question: What is the difference between raw accuracy improvement and appropriate confidence improvement in human-AI collaboration?

## Architecture Onboarding

- **Component map:**
  Original LLM encoder (frozen) -> Sentence encoder (context-aware or context-unaware) -> Concept transformation networks (MLPs) -> Similarity function (custom monotonic decreasing function) -> Human-defined weight matrix (W') -> Loss function (classification loss + concept classification loss)

- **Critical path:**
  1. Input text → sentence segmentation
  2. Sentence embeddings via LLM encoder
  3. Compute similarity to each concept prototype
  4. Select maximum similarity per concept
  5. Element-wise product with W' to get logits
  6. Classification + concept label prediction

- **Design tradeoffs:**
  - Context-aware vs context-unaware sentence embeddings: context-aware better for classification, context-unaware better for concept classification
  - Using human-labeled data vs learning prototypes from scratch: human data enables regulation but reduces performance, learned prototypes improve performance but lose interpretability
  - Number of concept prototypes: more prototypes increase representational capacity but may reduce interpretability

- **Failure signatures:**
  - Low concept classification accuracy despite high class accuracy: prototypes poorly represent concepts
  - Performance close to original black-box: regulatable concepts overlap significantly with original optimal features
  - Human users not benefiting from explanations: explanations not trusted or misunderstood

- **First 3 experiments:**
  1. Train with Human Labels=Yes vs Human Labels=No on a small dataset to verify the 7.34% performance drop
  2. Compare context-aware vs context-unaware settings to identify which works better for each task
  3. Test with randomized W' (no human bias) to verify human-defined weights improve appropriate trust

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the regulation performance trade-off scale with model size or complexity? The paper only tested one LLM architecture (BERT) and reported a specific performance drop, but it's unclear if larger or smaller models would experience proportionally similar or different trade-offs.
- Basis in paper: [explicit] The paper states "we tried a grid search of several other architectures such as DeBERTa, RoBERTa, DistilBERT etc., but none showed a significant improvement, so we used BERT" and reports a specific 7.34% average drop for their tested setup.
- Why unresolved: The study used a single encoder-only architecture without systematically varying model size or complexity to understand how the trade-off scales.
- What evidence would resolve it: Systematic experiments varying model architecture (encoder-only vs decoder-only), model size (number of parameters), and complexity while measuring both classification accuracy and concept classification accuracy across multiple datasets.

### Open Question 2
- Question: What is the optimal balance between regulatable concept granularity and model performance? The paper used high-level concepts like "IV Liable" rather than specific ones like "IV ran a red light," but the relationship between concept specificity and performance degradation remains unexplored.
- Basis in paper: [explicit] "high-level concepts were chosen such as e.g. 'IV Liable' rather than 'IV ran a red light' to maximize the generalizability of the concepts during training."
- Why unresolved: The paper made a design choice for concept granularity but didn't explore how varying this granularity affects the performance trade-off or model utility.
- What evidence would resolve it: Comparative experiments using concepts at different levels of specificity (from very general to very specific) while measuring both classification performance and human-AI collaboration outcomes.

### Open Question 3
- Question: How do different user types respond to regulatable LLMs over longer periods of use? The user study showed immediate reactions but couldn't determine whether initial skepticism toward the AI would persist or whether users who initially benefit would continue to do so with extended use.
- Basis in paper: [explicit] "how people respond to the AI assistant is quite individual, but if those users who benefit can be identified pre-hoc, the system's potential utility increases" and the study was limited to single-session interactions.
- Why unresolved: The pilot study was brief and within-subjects, making it impossible to determine if user responses would change with repeated exposure or over longer time periods.
- What evidence would resolve it: Longitudinal studies tracking the same users over weeks or months of regular use, measuring changes in speed, confidence, and accuracy as they become more familiar with the regulatable system.

## Limitations
- The primary dataset is proprietary, limiting independent verification of the 7.34% accuracy drop.
- The user study involved only six insurance adjusters, which constrains statistical power and may not represent broader practitioner populations.
- The human-labeled concept dataset (2,000 statements) is relatively small compared to the full 150,000-entry insurance dataset.

## Confidence

- **High confidence**: The existence of a performance trade-off when forcing LLMs to use only interpretable concepts is well-supported by the mathematical argument that regulatable features (R) are a strict subset of interpretable features (I), which are themselves a subset of the full latent space (L). The empirical evidence of a 7.34% accuracy drop provides concrete validation of this theoretical expectation.

- **Medium confidence**: The mechanism by which human-labeled concept prototypes enable classification while maintaining interpretability is supported by the methodology description, but the specific implementation details (e.g., exact prototype computation, similarity function parameters) are not fully specified, limiting reproducibility.

- **Medium confidence**: The claim that regulated models improve human task performance speed and appropriate confidence despite lower accuracy is based on a small user study (n=6) and may not generalize to other domains or larger populations. The distinction between "appropriate confidence" and raw accuracy improvement needs clearer operationalization.

## Next Checks
1. **Dataset accessibility validation**: Attempt to reproduce the 7.34% accuracy drop using an open-source dataset with similar characteristics (multi-class classification with interpretable concepts) to verify the core performance trade-off claim independently.

2. **User study replication**: Conduct a larger-scale user study (minimum n=30 participants) across multiple domains to validate whether regulated models consistently improve human task performance speed and appropriate confidence, controlling for individual differences in AI literacy and domain expertise.

3. **Concept space completeness analysis**: Systematically vary the number and coverage of human-defined regulatable concepts to quantify how the size of R relative to I affects the performance trade-off, testing whether certain concept subsets minimize accuracy loss while maintaining interpretability.