---
ver: rpa2
title: 'How Deep Networks Learn Sparse and Hierarchical Data: the Sparse Random Hierarchy
  Model'
arxiv_id: '2404.10727'
source_url: https://arxiv.org/abs/2404.10727
tags:
- figure
- networks
- learning
- sample
- different
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work studies how deep networks learn structured, high-dimensional
  data by introducing sparsity into hierarchical generative models. The Sparse Random
  Hierarchy Model (SRHM) captures both hierarchical feature composition and stability
  to smooth transformations through spatial sparsity of informative features.
---

# How Deep Networks Learn Sparse and Hierarchical Data: the Sparse Random Hierarchy Model

## Quick Facts
- arXiv ID: 2404.10727
- Source URL: https://arxiv.org/abs/2404.10727
- Authors: Umberto Tomasini; Matthieu Wyart
- Reference count: 40
- Key outcome: Deep networks learn sparse hierarchical data with polynomial sample complexity, beating curse of dimensionality while simultaneously acquiring hierarchical representations and transformation invariance

## Executive Summary
This work introduces the Sparse Random Hierarchy Model (SRHM) to study how deep networks learn structured, high-dimensional data. By incorporating sparsity into hierarchical generative models, the authors demonstrate that spatial sparsity naturally leads to stability against smooth transformations. The key insight is that sparse features make the class label insensitive to discrete diffeomorphisms. Empirically, both CNNs and LCNs learn SRHM with polynomial sample complexity in input dimension, while simultaneously acquiring hierarchical representations and invariance to diffeomorphisms and synonyms at the same sample complexity.

## Method Summary
The method involves training convolutional and locally connected networks on data generated from the Sparse Random Hierarchy Model. The SRHM uses hierarchical production rules with one-hot encoded features and introduces sparsity through a parameter s0. Networks are trained using SGD with cross-entropy loss until reaching a target test error threshold. Sample complexity is measured as the number of training points required to achieve this threshold. Sensitivity to diffeomorphisms and synonym exchanges is measured using discrete operators that capture these transformations, allowing researchers to correlate transformation stability with learning progress.

## Key Results
- CNNs and LCNs learn SRHM with sample complexity polynomial in input dimension, beating curse of dimensionality
- Hierarchical representations and invariance to diffeomorphisms/synonyms are acquired at the same sample complexity
- Sparsity naturally leads to insensitivity to discrete versions of smooth transformations
- Sample complexity for CNNs scales quadratically with hierarchy depth L, while LCNs scale exponentially

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Sparsity in hierarchical generative models naturally leads to insensitivity to discrete versions of diffeomorphisms.
- Mechanism: When features are spatially sparse, small transformations of their positions (discrete diffeomorphisms) do not alter the class label because the informative features are few and their exact spatial arrangement is not critical for classification.
- Core assumption: The class label depends only on the presence of informative features, not their precise spatial arrangement.
- Evidence anchors:
  - [abstract] "We show that by introducing sparsity to generative hierarchical models of data, the task acquires insensitivity to spatial transformations that are discrete versions of smooth transformations."
  - [section] "Our key insight is that spatial sparsity of features implies stability to diffeomorphisms."
- Break condition: If the class label depends critically on the exact spatial arrangement of features, then sparsity will not confer invariance to diffeomorphisms.

### Mechanism 2
- Claim: Hierarchical representations are learned precisely when insensitivity to diffeomorphisms is achieved.
- Mechanism: The same training set size that allows the network to learn to group together equivalent representations (synonyms) also allows it to learn invariance to transformations of those representations (diffeomorphisms).
- Core assumption: Learning synonyms and learning diffeomorphism invariance are two sides of the same coin, both requiring the network to recognize that different representations encode the same feature.
- Evidence anchors:
  - [abstract] "we observe and rationalize that a hierarchical representation mirroring the hierarchical model is learnt precisely when such insensitivity is learnt"
  - [section] "the insensitivities to diffeomorphisms and synonyms exchange are acquired for the same training set size, precisely when the network learns the task."
- Break condition: If the network can learn the task without learning to group synonyms or without learning diffeomorphism invariance, then the correlation will not hold.

### Mechanism 3
- Claim: The sample complexity of learning the sparse random hierarchy model (SRHM) is polynomial in the input dimension, beating the curse of dimensionality.
- Mechanism: The sample complexity depends on both the sparsity and hierarchical structure of the task, and is reduced by weight sharing in CNNs compared to LCNs.
- Core assumption: The network can efficiently detect correlations between features and output when features are sparse and hierarchically organized.
- Evidence anchors:
  - [abstract] "Empirically, CNNs and LCNs learn the SRHM with sample complexity polynomial in input dimension, beating the curse of dimensionality."
  - [section] "We quantify how the number of training points required to learn the SRHM for both CNNs and for Locally Connected Networks (LCNs), a version of CNNs without weight sharing."
- Break condition: If the task is not sparse or not hierarchical, or if weight sharing does not reduce sample complexity, then the polynomial scaling may not hold.

## Foundational Learning

- Concept: Hierarchical generative models
  - Why needed here: The SRHM is a hierarchical generative model, so understanding how such models work is crucial for understanding how the network learns.
  - Quick check question: What is the key property of hierarchical generative models that allows them to be learned efficiently by deep networks?

- Concept: Invariance to transformations
  - Why needed here: The paper shows that sparsity leads to invariance to diffeomorphisms, so understanding what invariance means and why it is useful is important.
  - Quick check question: Why is invariance to smooth transformations useful for image classification tasks?

- Concept: Sample complexity
  - Why needed here: The paper quantifies how the sample complexity of learning the SRHM depends on various factors, so understanding what sample complexity means and how it is measured is necessary.
  - Quick check question: How is sample complexity typically defined in the context of learning theory?

## Architecture Onboarding

- Component map:
  Input layer (one-hot encoded features) -> Hidden layers (convolutional or locally connected) -> Output layer (linear)

- Critical path:
  1. Generate training data from the SRHM
  2. Train the network on the training data using SGD
  3. Evaluate the network's performance on a test set
  4. Measure the network's sensitivity to diffeomorphisms and synonyms
  5. Analyze how the network's performance and sensitivities change with training set size

- Design tradeoffs:
  - Depth vs width: Deeper networks may be better at learning hierarchical representations, but wider networks may have more capacity to represent complex functions
  - Filter size: Larger filters may capture more context, but smaller filters may be more efficient and lead to better generalization
  - Weight sharing: CNNs with weight sharing have lower sample complexity than LCNs, but may be less flexible in representing certain functions

- Failure signatures:
  - If the network fails to learn the task, the test error will remain high even with large training sets
  - If the network learns the task but not the hierarchical representation, it may not show insensitivity to diffeomorphisms or synonyms
  - If the network overfits to the training data, the test error may be low but the sensitivities may be high

- First 3 experiments:
  1. Train a CNN on the SRHM with s0 = 0 (no sparsity) and measure the sample complexity and learned representation
  2. Train a CNN on the SRHM with s0 > 0 (sparsity) and compare the sample complexity and learned representation to the s0 = 0 case
  3. Train an LCN on the SRHM with s0 > 0 and compare the sample complexity and learned representation to the CNN case

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the sample complexity of CNNs compare to that of LCNs when learning the SRHM, and what explains this difference?
- Basis in paper: [explicit] The paper states that for CNNs, the sample complexity is P*_CNN ~ C1(s0 + 1)^2ncmL, while for LCNs it is P*_LCN ~ C0(s, L)(s0 + 1)^L ncmL.
- Why unresolved: The paper provides empirical evidence for this difference but does not offer a theoretical explanation for why weight sharing in CNNs leads to a sample complexity that is only quadratic in L, while LCNs have an exponential dependence on L.
- What evidence would resolve it: A theoretical analysis showing how weight sharing in CNNs reduces the effective dimensionality of the problem, leading to a lower sample complexity.

### Open Question 2
- Question: How do the sample complexities for CNNs and LCNs scale with the number of classes (nc) and the number of synonyms per feature (m)?
- Basis in paper: [explicit] The sample complexity equations for both CNNs and LCNs include factors of nc and mL.
- Why unresolved: The paper does not provide a detailed analysis of how changes in nc and m affect the sample complexity.
- What evidence would resolve it: Experiments systematically varying nc and m while keeping other parameters constant, and observing how the sample complexity changes.

### Open Question 3
- Question: Can the SRHM be extended to handle continuous transformations instead of just discrete diffeomorphisms?
- Basis in paper: [inferred] The paper mentions that sparsity leads to stability to discrete versions of diffeomorphisms, but does not explore continuous transformations.
- Why unresolved: The current SRHM model is based on discrete production rules and discrete transformations, making it challenging to directly extend to continuous cases.
- What evidence would resolve it: A modified SRHM model that incorporates continuous transformations and experiments showing how well it captures the stability properties observed in continuous data like images.

## Limitations

- Theoretical claims about polynomial sample complexity are supported by empirical observations but lack rigorous proofs for all parameter regimes
- The SRHM model may not fully represent the complexity of real-world data distributions
- Discrete diffeomorphism operators used in experiments are approximations of smooth transformations, and the relationship between discrete and continuous cases remains theoretical

## Confidence

**High Confidence**: The empirical correlation between hierarchical representation learning and transformation invariance across architectures and training set sizes is well-supported by experimental results.

**Medium Confidence**: The mechanism linking sparsity to diffeomorphism invariance is theoretically sound but relies on idealized assumptions about feature encoding.

**Medium Confidence**: The polynomial sample complexity scaling is observed empirically but lacks complete theoretical justification for all parameter combinations.

## Next Checks

1. **Generalization Test**: Evaluate network performance on SRHM variants with different hierarchical depths and sparsity levels not seen during training to test robustness of the learned representations.

2. **Theoretical Bounds**: Derive explicit sample complexity bounds as a function of all SRHM parameters (L, s, s0, v, m, nc) to verify the polynomial scaling claims.

3. **Real Data Validation**: Apply the SRHM-inspired architecture and training methodology to real image datasets with known hierarchical structure (e.g., faces with hierarchical facial features) to test practical applicability beyond synthetic data.