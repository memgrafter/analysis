---
ver: rpa2
title: 'IceFormer: Accelerated Inference with Long-Sequence Transformers on CPUs'
arxiv_id: '2405.02842'
source_url: https://arxiv.org/abs/2405.02842
tags:
- attention
- iceformer
- keys
- time
- which
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of inefficient inference for long-sequence
  Transformers on CPUs due to quadratic time and space complexity of self-attention.
  The authors propose IceFormer, a retraining-free method that uses k-nearest neighbor
  search (k-NNS) to identify the most important keys for each query, approximating
  the original attention matrix with only those keys.
---

# IceFormer: Accelerated Inference with Long-Sequence Transformers on CPUs

## Quick Facts
- arXiv ID: 2405.02842
- Source URL: https://arxiv.org/abs/2405.02842
- Authors: Yuzhen Mao; Martin Ester; Ke Li
- Reference count: 31
- Key outcome: IceFormer achieves 2.73x - 7.63x speedup compared to Transformer while retaining 98.6% - 99.6% of original accuracy on long-sequence tasks.

## Executive Summary
This paper addresses the challenge of inefficient inference for long-sequence Transformers on CPUs due to the quadratic time and space complexity of self-attention. The authors propose IceFormer, a retraining-free method that uses k-nearest neighbor search (k-NNS) to identify the most important keys for each query, approximating the original attention matrix without computing all attention weights. IceFormer embeds keys and queries into a higher-dimensional space to enable k-NNS in the general case of non-normalized keys. Experiments on the LRA, ZeroSCROLLS, and LongEval benchmarks demonstrate that IceFormer significantly accelerates inference while maintaining high accuracy, outperforming other efficient Transformer methods in terms of the accuracy-speed tradeoff.

## Method Summary
IceFormer is a retraining-free method that accelerates inference for long-sequence Transformers on CPUs by approximating the self-attention mechanism. It uses k-nearest neighbor search (k-NNS) to identify the most important keys for each query, embedding keys and queries into a higher-dimensional space to enable k-NNS in the general case of non-normalized keys. IceFormer employs the Prioritized DCI algorithm for fast and accurate k-NNS, constructing a database from the embedded keys and querying it to find the k nearest neighbors for each query. The attention weights are then computed only for these k nearest neighbors, significantly reducing the computational complexity compared to the vanilla Transformer. IceFormer is evaluated on various benchmarks and achieves substantial speedup while retaining high accuracy.

## Key Results
- IceFormer achieves a speedup of 2.73x - 7.63x compared to the Transformer while retaining 98.6% - 99.6% of the original accuracy on LRA, ZeroSCROLLS, and LongEval benchmarks.
- IceFormer outperforms other efficient Transformers in terms of the accuracy-speed tradeoff, providing a better balance between accuracy and computational efficiency.
- IceFormer is effective across a wide range of sequence lengths (up to 16k tokens) and various natural language processing tasks, demonstrating its versatility and generalizability.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: k-nearest neighbor search (k-NNS) on embedded keys identifies the most important keys for each query without computing all attention weights.
- Mechanism: Keys and queries are embedded into a higher-dimensional space using functions TK and TQ. In this space, k-NNS can identify the k keys with highest attention weights by finding those closest to the query embedding.
- Core assumption: The embedding functions TK and TQ preserve the relative ordering of attention weights when computed in the original space.
- Evidence anchors:
  - [abstract] "IceFormer embeds keys and queries into a higher dimensional space to enable k-NNS in the general case of non-normalized keys."
  - [section] "Instead of applying k-NNS to the original keys directly, we will first embed the keys and queries into a higher dimensional space."
  - [corpus] Weak - no direct evidence in corpus.
- Break condition: If the embedding functions do not preserve the relative ordering of attention weights, k-NNS will not identify the correct keys.

### Mechanism 2
- Claim: Ranking-based k-NNS algorithms are better suited than bucketing-based algorithms for identifying important keys in attention.
- Mechanism: Ranking-based algorithms compare the rankings of different keys relative to the query and search over highly ranked keys, while bucketing-based algorithms place keys into discrete buckets and search over buckets that contain the query.
- Core assumption: Attention weights depend on how different keys compare to one another, rather than an absolute evaluation of each key against a fixed threshold.
- Evidence anchors:
  - [section] "For accelerating attention, we posit that ranking-based algorithms are better suited than bucketing-based algorithms, because attention weights depend on how different keys compare to one another, rather than an absolute evaluation of each key against a fixed threshold."
  - [corpus] Weak - no direct evidence in corpus.
- Break condition: If attention weights do not depend on the relative comparison of keys, bucketing-based algorithms may be equally effective.

### Mechanism 3
- Claim: Prioritized DCI is efficient at both the construction and querying stages of k-NNS, making it suitable for accelerating attention in Transformers.
- Mechanism: Prioritized DCI uses random projections to rank keys relative to the query and searches over highly ranked keys. It can return exact k-nearest neighbors with high probability within approximately O(dkp/̃dm^(1-p/̃d)) time.
- Core assumption: The number of random projection directions p is nearly as large as the intrinsic dimensionality of the data d' and the number of nearest neighbors k to look for is small.
- Evidence anchors:
  - [section] "Fortunately, Prioritized DCI is efficient at both the construction and querying stages. If the number of random projection directions p is nearly as large as the intrinsic dimensionality of the data d' ≥ 1 and the number of nearest neighbours k to look for is small, Prioritized DCI can return the exact k-nearest neighbours for a query with high probability within approximately O(dkp/̃dm^(1-p/̃d)) time."
  - [corpus] Weak - no direct evidence in corpus.
- Break condition: If the assumptions about p, d', and k are not met, Prioritized DCI may not be efficient or accurate.

## Foundational Learning

- Concept: Self-attention mechanism in Transformers
  - Why needed here: Understanding how self-attention works is crucial for understanding how IceFormer accelerates it.
  - Quick check question: What are the three matrices that the attention operation takes as input?

- Concept: Quadratic time and space complexity of self-attention
  - Why needed here: The inefficiency of self-attention is the problem that IceFormer aims to solve.
  - Quick check question: What is the time and space complexity of self-attention in terms of sequence length?

- Concept: k-nearest neighbor search (k-NNS)
  - Why needed here: k-NNS is the key technique used by IceFormer to identify important keys without computing all attention weights.
  - Quick check question: What is the main difference between ranking-based and bucketing-based k-NNS algorithms?

## Architecture Onboarding

- Component map:
  Transformer model with self-attention layers -> IceFormer module (replacing self-attention) -> Prioritized DCI k-NNS algorithm -> Key and query embedding functions TK and TQ

- Critical path:
  1. Input keys and queries to IceFormer
  2. Embed keys and queries using TK and TQ
  3. Construct Prioritized DCI database from embedded keys
  4. Query database to find k nearest neighbors for each query
  5. Compute attention weights only for the k nearest neighbors
  6. Combine values using the computed attention weights

- Design tradeoffs:
  - Accuracy vs. speed: Larger k leads to higher accuracy but slower inference
  - Memory vs. speed: Prioritized DCI database construction requires additional memory but enables faster querying
  - Generality vs. performance: IceFormer works with any pretrained Transformer but may not be as fast as specialized architectures

- Failure signatures:
  - Low accuracy: k is too small, or the embedding functions do not preserve relative ordering of attention weights
  - High memory usage: Prioritized DCI database is too large for the available memory
  - Slow inference: Prioritized DCI querying is not efficient enough for the given data

- First 3 experiments:
  1. Measure inference time and accuracy of IceFormer with different values of k on a small dataset
  2. Compare the approximation quality of IceFormer to other efficient Transformers on a larger dataset
  3. Evaluate the scalability of IceFormer on very long sequences and measure the impact on inference time and accuracy

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of IceFormer scale with increasing sequence lengths beyond the benchmarks tested in the paper?
- Basis in paper: [inferred] The paper demonstrates IceFormer's effectiveness on benchmarks with sequence lengths up to 16k tokens, but does not explore performance beyond this range.
- Why unresolved: The authors did not conduct experiments with sequence lengths exceeding 16k tokens, leaving uncertainty about IceFormer's scalability to extremely long sequences.
- What evidence would resolve it: Experiments on benchmarks with sequence lengths significantly longer than 16k tokens, such as those with sequences in the range of hundreds of thousands or millions of tokens, would provide evidence of IceFormer's scalability.

### Open Question 2
- Question: How does the choice of k in k-nearest neighbor search (k-NNS) affect the trade-off between accuracy and speed for different types of tasks and input sequences?
- Basis in paper: [explicit] The paper mentions that the number of returned candidates (k) in k-NNS affects the trade-off between accuracy and speed, but does not provide a detailed analysis of how this trade-off varies across different tasks and input sequences.
- Why unresolved: The authors did not conduct a comprehensive study on the impact of k on the accuracy-speed trade-off for various tasks and input sequences.
- What evidence would resolve it: A systematic study that evaluates IceFormer's performance with different values of k on a wide range of tasks and input sequences, including both long and short sequences, would provide insights into the optimal choice of k for different scenarios.

### Open Question 3
- Question: How does IceFormer's performance compare to other efficient Transformer methods that were not included in the experiments, such as those based on low-rank approximations or sparse attention patterns?
- Basis in paper: [inferred] The paper compares IceFormer to several efficient Transformer methods, but does not include all possible approaches, leaving uncertainty about its relative performance.
- Why unresolved: The authors did not conduct experiments with all available efficient Transformer methods, making it difficult to determine IceFormer's overall performance relative to the entire landscape of efficient Transformer techniques.
- What evidence would resolve it: Experiments that compare IceFormer to a comprehensive set of efficient Transformer methods, including those based on low-rank approximations and sparse attention patterns, would provide a more complete picture of its performance relative to other approaches.

## Limitations

- The exact implementation details of the Prioritized DCI algorithm, particularly the data structure construction and querying procedures, are not fully specified in the paper, which could impact the faithful reproduction of the results.
- The specific hyperparameter values for IceFormer on different tasks and benchmarks are not explicitly mentioned, requiring further tuning and experimentation to achieve optimal performance.
- The paper does not discuss potential failure modes or limitations of the method in detail, such as its performance on very long sequences or with highly non-normalized keys.

## Confidence

- High confidence: The core mechanism of using k-nearest neighbor search (k-NNS) to approximate self-attention and accelerate inference on CPUs is well-supported by the paper's theoretical analysis and experimental results.
- Medium confidence: The choice of Prioritized DCI as the k-NNS algorithm and its efficiency compared to other methods is supported by the paper's analysis, but the exact implementation details are not fully specified.
- Low confidence: The generalizability of IceFormer to other Transformer architectures and tasks beyond the ones evaluated in the paper is not extensively discussed, leaving some uncertainty about its broader applicability.

## Next Checks

1. Reproduce the approximation quality of IceFormer: Implement the method and measure the L2-norm of the difference between the attention module outputs of IceFormer and the vanilla Transformer on a small dataset. Compare the results with the values reported in the paper.

2. Evaluate the scalability of IceFormer: Run experiments on very long sequences (e.g., 32k tokens) and measure the impact on inference time and accuracy. Compare the results with the vanilla Transformer and other efficient Transformers to validate the scalability claims.

3. Investigate the sensitivity to hyperparameters: Perform a systematic study of the impact of different values of k in k-NNS and the embedding function parameters (m, L) on the accuracy and speed of IceFormer. Identify the optimal hyperparameter settings for different tasks and sequence lengths.