---
ver: rpa2
title: Local Explanations and Self-Explanations for Assessing Faithfulness in black-box
  LLMs
arxiv_id: '2409.13764'
source_url: https://arxiv.org/abs/2409.13764
tags:
- faithfulness
- context
- answer
- keywords
- explanations
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of assessing faithfulness in
  large language models (LLMs) through local perturbations and self-explanations.
  The authors propose an efficient explainability technique inspired by the leave-one-out
  approach to identify sufficient and necessary parts of context for correct LLM answers.
---

# Local Explanations and Self-Explanations for Assessing Faithfulness in black-box LLMs

## Quick Facts
- arXiv ID: 2409.13764
- Source URL: https://arxiv.org/abs/2409.13764
- Authors: Christos Fragkathoulas; Odysseas S. Chlapanis
- Reference count: 40
- One-line primary result: Proposed explainability method achieves faithfulness scores of 0.691 for GPT-4o and 0.653 for GPT-3.5 on 62 successfully explained samples from Natural Questions dataset

## Executive Summary
This paper addresses the challenge of assessing faithfulness in large language models (LLMs) through local perturbations and self-explanations. The authors propose an efficient explainability technique inspired by the leave-one-out approach to identify sufficient and necessary parts of context for correct LLM answers. They introduce a faithfulness metric that compares these crucial parts with the model's self-explanations. Using the Natural Questions dataset, they demonstrate their method's effectiveness in explaining model decisions and assessing faithfulness.

## Method Summary
The method identifies sufficient and necessary parts of context for correct LLM answers through local perturbations. Context is split into p equal parts (p=3), and each part's sufficiency is tested by masking others. Necessary keywords are found by masking groups of words (q=5) within sufficient regions. The faithfulness score compares these identified parts with the model's own keywords. The approach uses chain-of-thought prompting to elicit both reasoning and keyword extraction from the model, then evaluates answer correctness using a hybrid metric combining exact match, normalized exact match, fuzzy match, embedding similarity, and date transformations.

## Key Results
- GPT-4o achieves higher faithfulness score (0.691) compared to GPT-3.5 (0.653) on a common subset of 62 successfully explained samples
- Method requires 16 API calls per sample with 45% explainability success rate
- The approach offers a more efficient alternative to existing methods with constant number of API calls per sample

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Local perturbations combined with self-explanations can isolate sufficient and necessary parts of context for correct LLM answers
- Mechanism: The approach splits context into equal parts (p=3), tests each part's sufficiency by masking others, then masks groups of words (q=5) within sufficient regions to find necessary keywords. The faithfulness score compares these identified parts with the model's own keywords
- Core assumption: The model's correct answer depends on specific context parts, and masking these parts will cause incorrect answers
- Evidence anchors:
  - [abstract] "Using this approach, we identify the sufficient and necessary parts for the LLM to generate correct answers, serving as explanations."
  - [section 3.5.2] "Instead of masking a single word or a predetermined number of words as in traditional LOO approaches, we mask q groups with equal number of words each"
  - [corpus] Found 25 related papers; average neighbor FMR=0.396, but only 0 citations - weak external validation of perturbation effectiveness
- Break condition: If the model relies on internal knowledge rather than context, or if the context is too short to split into p parts, the method fails

### Mechanism 2
- Claim: A hybrid evaluation metric combining exact match, normalized exact match, fuzzy match, embedding similarity, and date transformations provides robust assessment of answer correctness
- Mechanism: The metric combines multiple string comparison methods to handle natural language variability, semantic equivalence, and format variations in answers
- Core assumption: Different answer formats can be semantically equivalent, and multiple comparison methods capture these equivalences better than single methods
- Evidence anchors:
  - [section 3.3] "Our hybrid metric addresses these issues by capturing semantic equivalence and format variations, offering a more robust evaluation of the answers of the model."
  - [section 3.3] "Exact match often fails due to natural language variability, such as different formatting of names or dates."
  - [corpus] Weak evidence - corpus neighbors focus on faithfulness evaluation but don't validate this specific hybrid approach
- Break condition: If the answer format variations are too complex for the defined metrics to capture, or if the model consistently uses novel formats

### Mechanism 3
- Claim: Using Retrieval-Hard subset ensures fair evaluation by excluding samples the model can answer without context
- Mechanism: Filters dataset to include only samples where the model fails without context, ensuring the model must rely on provided context for correct answers
- Core assumption: Proprietary LLMs have unknown pretraining corpora, so filtering ensures context is actually needed
- Evidence anchors:
  - [section 3.2] "Models might rely on internal knowledge rather than context, leading to unfair comparisons. To address this, we use the Retrieval-Hard subset"
  - [section 4] "Currently, we have used 790 samples, only 311 of them are in the Retrieval-Hard subset"
  - [corpus] No direct evidence in neighbors about Retrieval-Hard subset methodology
- Break condition: If the model's pretraining corpus happens to overlap significantly with the context snippets, or if the filtering criteria are too strict

## Foundational Learning

- Concept: Leave-One-Out (LOO) approach
  - Why needed here: Forms the basis for identifying sufficient regions and necessary keywords through systematic masking
  - Quick check question: What does the LOO approach do in the context of model explanation?

- Concept: Chain-of-Thought (CoT) prompting
  - Why needed here: Used to elicit both reasoning and keyword extraction from the model
  - Quick check question: How does CoT prompting structure help in getting both thought process and keywords?

- Concept: Faithfulness vs. Plausibility
  - Why needed here: Distinguishes between explanations that align with actual model behavior versus those that sound reasonable but aren't accurate
  - Quick check question: What's the difference between faithfulness and plausibility in model explanations?

## Architecture Onboarding

- Component map: Dataset loader -> Context splitter -> API interface -> Evaluation metrics -> Faithfulness calculator -> Visualization formatter
- Critical path:
  1. Load QA triplet
  2. Split context into parts
  3. Generate answers for each part to identify sufficient regions
  4. Mask word groups in sufficient regions to find necessary keywords
  5. Compare model keywords with identified parts using faithfulness metric
  6. Calculate and report faithfulness score

- Design tradeoffs: The trade-off between explainability success rate and API call efficiency, balancing perturbation granularity with computational cost
- Failure signatures: 
  - 0% explainability success rate indicates model answers correctly even with masked context
  - Low faithfulness score despite high explainability success suggests poor keyword extraction from model
  - High API call failure rate indicates context splitting or masking approach needs adjustment

- First experiments:
  1. Test the method on a small subset of Natural Questions to verify basic functionality
  2. Compare faithfulness scores across different LLM models (GPT-3.5 vs GPT-4o) on identical samples
  3. Evaluate sensitivity of faithfulness scores to different values of p (context splits) and q (word groups)

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the choice of hyperparameter p (number of context splits) and q (number of word groups) affect the explainability success rate and the faithfulness score?
- Basis in paper: [explicit] The authors mention a trade-off between API calls/cost and explainability success, and they selected p=3 and q=5 for their experiments, resulting in 16 API calls per sample with a 45% explainability success rate
- Why unresolved: The paper does not provide a systematic exploration of how varying p and q affects the performance of the method
- What evidence would resolve it: Conducting experiments with different combinations of p and q, and analyzing their impact on the explainability success rate and faithfulness score

### Open Question 2
- Question: How does the proposed method compare to existing perturbation-based explanation methods for text inputs, such as SHAP-based approaches?
- Basis in paper: [explicit] The authors mention that their approach offers a more efficient alternative with a constant number of API calls per sample compared to Shapley-based approaches that involve numerous attempts
- Why unresolved: The paper does not provide a direct comparison of the proposed method with existing perturbation-based explanation methods
- What evidence would resolve it: Conducting experiments comparing the proposed method with SHAP-based approaches on the same datasets, evaluating both the efficiency and effectiveness of the explanations

### Open Question 3
- Question: Can the proposed method be extended to handle longer context inputs and generate explanations at different levels of granularity?
- Basis in paper: [explicit] The authors mention that their algorithm extends the Leave One Out (LOO) method, which is a powerful baseline in previous work. However, they do not discuss the limitations of their method when dealing with longer contexts or generating explanations at different levels of granularity
- Why unresolved: The paper does not explore the limitations of the proposed method when handling longer contexts or generating explanations at different levels of granularity
- What evidence would resolve it: Conducting experiments with longer context inputs and analyzing the performance of the method at different levels of granularity, such as word-level, phrase-level, and sentence-level explanations

## Limitations
- Small sample size (62 samples) in final comparison between GPT-3.5 and GPT-4o limits statistical significance
- Reliance on proprietary OpenAI API introduces cost constraints and reproducibility issues
- Method's effectiveness depends heavily on quality of chain-of-thought prompting, which is not fully specified
- Limited evaluation to question-answering task, generalizability to other domains uncertain

## Confidence

**High Confidence**: The core methodology of using local perturbations to identify sufficient and necessary context parts, and the faithfulness metric design comparing these parts with self-explanations

**Medium Confidence**: The claim that GPT-4o achieves higher faithfulness than GPT-3.5 based on preliminary experiments with 62 samples

**Low Confidence**: The generalizability of the approach to other domains beyond question answering, and the assumption that p=3 and q=5 are optimal parameters

## Next Checks

1. **Statistical Validation**: Perform significance testing on the faithfulness scores between GPT-3.5 and GPT-4o using larger sample sizes to verify the preliminary findings are not due to chance

2. **Domain Generalization**: Test the approach on non-QA datasets (e.g., summarization, classification) to assess its effectiveness across different NLP tasks and identify any domain-specific limitations

3. **Alternative Perturbation Strategies**: Experiment with different values of p and q (number of context splits and word groups) to determine optimal parameters and assess sensitivity to these choices