---
ver: rpa2
title: 'ReSi: A Comprehensive Benchmark for Representational Similarity Measures'
arxiv_id: '2408.00531'
source_url: https://arxiv.org/abs/2408.00531
tags:
- similarity
- representations
- measures
- uni00000013
- test
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper presents ReSi, the first comprehensive benchmark for
  evaluating representational similarity measures across graph, language, and vision
  domains. It includes six carefully designed tests grounded by prediction or design,
  24 different similarity measures, 14 neural network architectures, and seven datasets.
---

# ReSi: A Comprehensive Benchmark for Representational Similarity Measures

## Quick Facts
- arXiv ID: 2408.00531
- Source URL: https://arxiv.org/abs/2408.00531
- Reference count: 40
- Key outcome: No single similarity measure consistently outperforms others across all domains and tests

## Executive Summary
ReSi is the first comprehensive benchmark for evaluating representational similarity measures across graph, language, and vision domains. It tests 24 different similarity measures on 14 neural network architectures across seven datasets using six carefully designed tests. The benchmark demonstrates that preprocessing choices significantly impact measure performance and that no single measure dominates across all domains, highlighting the need for careful measure selection based on specific applications.

## Method Summary
The benchmark evaluates representational similarity measures by training groups of models with systematically varied conditions (label randomization, shortcut features, augmentation levels) and measuring how well similarity measures can identify higher similarity within groups than between groups. It includes 24 different similarity measures applied to representations from 14 neural network architectures across graph, language, and vision domains, using seven datasets. The evaluation framework computes correlations between similarity measures and ground truth (accuracy differences, output differences, or group separation) across six different test types.

## Key Results
- No single similarity measure consistently outperforms others across all domains and tests
- Jaccard similarity performed surprisingly well in vision despite limited prior use
- Established measures like CKA showed domain-specific strengths (first in Language, eighth in Vision)
- Preprocessing choices significantly impact measure performance
- Runtime varies dramatically across measures, with some taking orders of magnitude longer than others

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Grounding representational similarity by design creates measurable behavioral differences that can be detected by similarity measures
- Mechanism: The benchmark deliberately engineers groups of models with distinct training conditions that produce systematically different representations. When similarity measures correctly identify higher similarity within groups than between groups, this demonstrates their ability to capture meaningful representational differences
- Core assumption: Behavioral differences induced by training modifications are faithfully reflected in the learned representations
- Evidence anchors:
  - [abstract]: "The benchmark demonstrates that no single similarity measure consistently outperforms others across all domains and tests"
  - [section]: "We train groups of five models each, for which r ∈ {25%, 50%, 75%, 100%} of all labels are randomized during training"
  - [corpus]: Weak evidence - no direct citations about label randomization grounding found
- Break condition: If models learn to ignore training modifications or if representations become invariant to the designed differences, the grounding fails

### Mechanism 2
- Claim: Preprocessing transformations significantly impact which representational differences similarity measures can detect
- Mechanism: Different preprocessing approaches (like unit norm scaling in Orthogonal Procrustes vs no scaling in Procrustes Size-and-Shape-Distance) implicitly encode assumptions about which representation properties matter. The benchmark reveals that seemingly minor preprocessing differences lead to substantial performance variations across tests
- Core assumption: Preprocessing transformations preserve or remove specific semantic information from representations
- Evidence anchors:
  - [abstract]: "The benchmark also reveals that preprocessing choices significantly impact measure performance"
  - [section]: "Orthogonal Procrustes (OrthProc) and Procrustes Size-and-Shape Distance (ProcDist), which differ only in the way the representations are preprocessed"
  - [corpus]: Weak evidence - no direct citations about preprocessing impact found
- Break condition: If preprocessing consistently removes all semantic information or if all measures become invariant to preprocessing choices

### Mechanism 3
- Claim: No single similarity measure can capture all meaningful aspects of representational differences across domains
- Mechanism: The benchmark tests 24 measures across graph, language, and vision domains with different grounding approaches. Performance rankings vary dramatically by domain and test, showing that each measure captures different aspects of representational similarity
- Core assumption: Representational differences are multidimensional and context-dependent
- Evidence anchors:
  - [abstract]: "no single similarity measure consistently outperforms others across all domains and tests"
  - [section]: "CKA (Kornblith et al., 2019) ranks first in Language, eighth in Vision, and 11th in Graphs"
  - [corpus]: Moderate evidence - related papers discuss domain-specific performance but lack comprehensive multi-domain benchmarks
- Break condition: If a measure is discovered that performs well across all domains and grounding approaches, or if representational differences are found to be fundamentally one-dimensional

## Foundational Learning

- Concept: Representational similarity measures
  - Why needed here: The entire benchmark is built around evaluating how different measures quantify representational similarity, so understanding what these measures are and how they work is fundamental
  - Quick check question: What is the difference between CKA and RSA in how they compute representational similarity?

- Concept: Ground truth establishment in similarity measurement
  - Why needed here: The benchmark uses two distinct approaches to establish ground truth (prediction-based and design-based), and understanding these approaches is crucial for interpreting results
  - Quick check question: How does grounding by design differ from grounding by prediction in terms of what assumptions are made about model behavior?

- Concept: Domain-specific neural network architectures
  - Why needed here: The benchmark spans graph, language, and vision domains, each with distinct architectures (GNNs, Transformers, CNNs/ViTs), and understanding these differences is important for interpreting domain-specific results
  - Quick check question: Why might neighborhood-based measures perform better in the graph domain compared to the vision domain?

## Architecture Onboarding

- Component map: The benchmark consists of six test modules (correlation to accuracy difference, correlation to output difference, label randomization, shortcut affinity, augmentation, layer monotonicity), 24 similarity measure implementations, data loaders for seven datasets, and evaluation scripts that aggregate results
- Critical path: Implement similarity measure → run all six tests → collect results → generate rankings → analyze domain-specific performance
- Design tradeoffs: Comprehensive coverage vs. computational efficiency (RSM-based measures scale quadratically), breadth of measures vs. depth of analysis per measure, fixed preprocessing vs. measure-specific optimization
- Failure signatures: NaN values in results (numerical instability), extremely long runtimes (memory issues), poor separation between model groups (invalid grounding assumptions), correlation values near zero (measure insensitivity)
- First 3 experiments:
  1. Run all similarity measures on Cora dataset with GCN models for Test 1 (correlation to accuracy difference) to verify basic functionality
  2. Compare preprocessing effects by running OrthProc and ProcDist on the same representations to observe performance differences
  3. Test all measures on ImageNet100 with ResNet18 for Test 3 (label randomization) to validate domain-specific behavior

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Why do some similarity measures like Jaccard perform unexpectedly well in certain domains (e.g., vision) despite limited prior use in those contexts?
- Basis in paper: [explicit] The paper notes that "Jaccard similarity emerged as the best-performing measure in the vision domain" and that this "challenges popular opinion and suggests that rarely used measures...may possess unique properties."
- Why unresolved: The paper identifies this as an unexpected finding but does not investigate the underlying reasons for Jaccard's strong performance in vision, such as whether it captures specific geometric properties of vision representations or is less affected by certain preprocessing steps.
- What evidence would resolve it: Systematic ablation studies comparing Jaccard with other measures across different vision architectures, datasets, and preprocessing choices, along with theoretical analysis of what aspects of vision representations Jaccard captures that other measures miss.

### Open Question 2
- Question: How do preprocessing choices like normalization and scaling fundamentally affect what aspects of model similarity similarity measures capture?
- Basis in paper: [explicit] The paper states that "Orthogonal Procrustes (OrthProc) and Procrustes Size-and-Shape Distance (ProcDist), which differ only in the way the representations are preprocessed...this seemingly small difference has a substantial impact on what properties of models similarity measures capture."
- Why unresolved: While the paper observes that preprocessing matters significantly, it does not investigate what specific properties of representations are affected by different preprocessing approaches or develop guidelines for when different preprocessing methods should be used.
- What evidence would resolve it: Controlled experiments varying preprocessing steps systematically across different types of representations and similarity measures, combined with interpretability analysis to understand what semantic aspects of representations are preserved or lost under different preprocessing transformations.

### Open Question 3
- Question: What specific geometric or topological properties of neural representations make certain similarity measures more effective for particular tasks or domains?
- Basis in paper: [explicit] The paper observes that "neighborhood-based measures appear favorable" for graphs, "CKA or distance correlation" for language, and "Jaccard similarity" for vision, but does not explain why these measure types align with these domains.
- Why unresolved: The paper establishes empirical correlations between measure types and domains but does not investigate the underlying representational characteristics (e.g., manifold structure, neighborhood relationships, or statistical distributions) that make certain measures more suitable.
- What evidence would resolve it: Detailed analysis of the geometric and topological properties of representations across domains (e.g., using manifold learning techniques, topological data analysis, or geometric statistics) to identify which properties each measure type is sensitive to and how these align with domain-specific representation characteristics.

## Limitations

- Grounding by design assumes engineered training modifications produce systematic representational differences, but this assumption has weak corpus support
- Computational cost of some similarity measures (particularly RSM-based approaches) limits scalability to larger datasets
- The benchmark does not investigate the underlying geometric or topological properties that make certain measures more effective for particular domains

## Confidence

- **High Confidence**: Claims about preprocessing choices impacting measure performance and the observation that no single measure dominates across all domains and tests are directly supported by the benchmark results and show consistent patterns
- **Medium Confidence**: Claims about domain-specific measure performance are moderately supported by the corpus, though the comprehensive multi-domain comparison in ReSi provides novel evidence not previously established
- **Low Confidence**: Claims about the effectiveness of grounding by design rely on assumptions about model behavior that have weak corpus support and limited empirical validation beyond the benchmark itself

## Next Checks

1. **Test grounding assumptions**: Systematically vary the strength of label randomization and shortcut features across a wider range of model architectures to verify that representational differences scale predictably with the degree of modification

2. **Validate preprocessing sensitivity**: Compare similarity measure rankings when using alternative preprocessing pipelines (e.g., different normalization approaches) to determine if observed performance differences are robust to preprocessing choices

3. **Extend to additional domains**: Apply the benchmark framework to reinforcement learning or multimodal architectures to test whether the observed domain-specific patterns generalize beyond the current graph, language, and vision domains