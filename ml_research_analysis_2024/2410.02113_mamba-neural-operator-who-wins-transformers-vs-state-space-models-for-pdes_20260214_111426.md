---
ver: rpa2
title: 'Mamba Neural Operator: Who Wins? Transformers vs. State-Space Models for PDEs'
arxiv_id: '2410.02113'
source_url: https://arxiv.org/abs/2410.02113
tags:
- mamba
- neural
- operator
- pdes
- attention
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces the Mamba Neural Operator (MNO), a novel
  framework that connects state-space models (SSMs) with neural operators for solving
  partial differential equations (PDEs). MNO leverages the structured design of SSMs
  to address the limitations of Transformers in representing continuous dynamics and
  long-range interactions.
---

# Mamba Neural Operator: Who Wins? Transformers vs. State-Space Models for PDEs

## Quick Facts
- arXiv ID: 2410.02113
- Source URL: https://arxiv.org/abs/2410.02113
- Authors: Chun-Wun Cheng; Jiahao Huang; Yi Zhang; Guang Yang; Carola-Bibiane Schönlieb; Angelica I Aviles-Rivero
- Reference count: 36
- Primary result: MNO outperforms Transformer-based methods on PDE tasks across RMSE, nRMSE, and RL2 metrics

## Executive Summary
This paper introduces the Mamba Neural Operator (MNO), a novel framework that connects state-space models (SSMs) with neural operators for solving partial differential equations (PDEs). MNO leverages the structured design of SSMs to address the limitations of Transformers in representing continuous dynamics and long-range interactions. Through extensive analysis, the authors show that MNO significantly boosts the expressive power and accuracy of neural operators, making it a superior framework for PDE-related tasks. The performance of MNO is evaluated on three PDEs—Darcy Flow, Shallow Water 2D, and Diffusion Reaction 2D—outperforming Transformer-based methods across all metrics, including RMSE, nRMSE, and RL2. Visualizations and frequency response analyses further demonstrate MNO's ability to capture complex dynamics and maintain stability, highlighting its potential for efficient and accurate PDE modeling.

## Method Summary
The Mamba Neural Operator (MNO) framework integrates structured state-space models (SSMs) with neural operators by replacing self-attention and cross-attention in Transformer models with S6 and Cross S6 blocks derived from SSMs. The architecture follows a three-stage pipeline: Bi-Directional Scan Expand unfolds input data into sequences along two distinct paths, S6/Cross S6 Block processes each patch sequence, and Bi-Directional Scan Merge reshapes and merges processed sequences back together to generate the output map. The framework establishes a theoretical connection between neural operator layers and time-varying SSMs, demonstrating that their hidden space updates align with the iterative process in neural operators. MNO was evaluated on three PDE datasets from PDEBench: Darcy Flow (β=1.0), Shallow Water 2D, and Diffusion Reaction 2D, using 9,000 training samples and 1,000 test samples for Darcy Flow, and 900/100 splits for SW2D and DR2D.

## Key Results
- MNO outperforms Transformer-based methods (GNOT, Galerkin Transformer, OFormer) across all three PDE tasks
- Achieved lower RMSE, nRMSE, and RL2 metrics compared to baseline models
- Demonstrated superior ability to capture complex spatial correlations and maintain stability through frequency response analysis
- Showed better accuracy in representing continuous dynamics without requiring excessively small step sizes

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Mamba Neural Operator (MNO) effectively captures long-range dependencies and continuous dynamics in PDEs better than Transformers by leveraging the structured design of state-space models (SSMs).
- Mechanism: MNO uses the zero-order hold discretisation method from SSMs, which is equivalent to the Euler method but retains higher-order terms from the Taylor series, leading to better accuracy without requiring excessively small step sizes.
- Core assumption: The zero-order hold discretisation method provides higher accuracy than the Euler method for solving PDEs, especially in capturing continuous dynamics.
- Evidence anchors:
  - [abstract] "MNO establishes a formal theoretical connection between structured state-space models (SSMs) and neural operators, offering a unified structure that can adapt to diverse architectures, including Transformer-based models."
  - [section] "Proposition 1. The zero-order hold discretisation method, as in (6), is equivalent to the Euler method in SSM when the Taylor series expansion of the exponential function is truncated to its first-order term."
  - [corpus] Weak evidence. The corpus mentions related works like 'Latent Mamba Operator for Partial Differential Equations' and 'GeoMaNO: Geometric Mamba Neural Operator for Partial Differential Equations', but lacks direct evidence on the accuracy claims.
- Break condition: If the Taylor series expansion is truncated beyond the first-order term, the equivalence to the Euler method no longer holds, potentially reducing accuracy.

### Mechanism 2
- Claim: MNO bridges the gap between efficient representation and accurate solution approximation for PDE-related tasks.
- Mechanism: MNO replaces self-attention and cross-attention in Transformer models with S6 and Cross S6 blocks, which are derived from SSMs, leading to improved performance in capturing complex spatial correlations.
- Core assumption: Replacing self-attention and cross-attention with S6 and Cross S6 blocks derived from SSMs improves the model's ability to capture complex spatial correlations in PDEs.
- Evidence anchors:
  - [abstract] "Through extensive analysis, we show that MNO significantly boosts the expressive power and accuracy of neural operators, making it not just a complement but a superior framework for PDE-related tasks."
  - [section] "As depicted in Figure 1, the data processing pipeline in our Mamba Neural Operator (MNO) is composed of three key stages: Bi-Directional Scan Expand, S6/Cross S6 Block, and Bi-Directional Scan Merge."
  - [corpus] Weak evidence. The corpus mentions 'State-space models are accurate and efficient neural operators for dynamical systems', but lacks direct evidence on the performance claims for PDE-related tasks.
- Break condition: If the S6 and Cross S6 blocks are not properly implemented or if the SSMs are not suitable for the specific type of PDE, the model's performance may degrade.

### Mechanism 3
- Claim: MNO provides a theoretical understanding that neural operator layers share a comparable structural framework with time-varying SSMs.
- Mechanism: MNO demonstrates that the hidden space updates in time-varying SSMs align with the iterative process in neural operator layers, providing a new perspective on their underlying principles.
- Core assumption: The iterative process in neural operator layers can be aligned with the hidden space updates in time-varying SSMs, providing a theoretical foundation for the MNO framework.
- Evidence anchors:
  - [abstract] "MNO establishes a formal theoretical connection between structured state-space models (SSMs) and neural operators, offering a unified structure that can adapt to diverse architectures, including Transformer-based models."
  - [section] "Proposition 2. The hidden space in time-varying state-space models demonstrates a structural similarity to neural operator layers."
  - [corpus] Weak evidence. The corpus mentions 'State-space models are accurate and efficient neural operators for dynamical systems', but lacks direct evidence on the theoretical understanding claims.
- Break condition: If the structural similarity between neural operator layers and time-varying SSMs cannot be established for a specific type of PDE, the theoretical foundation of MNO may be compromised.

## Foundational Learning

- Concept: Partial Differential Equations (PDEs)
  - Why needed here: PDEs are the core mathematical framework that MNO is designed to solve, and understanding their structure and properties is essential for designing effective neural operators.
  - Quick check question: What are the key differences between elliptic, parabolic, and hyperbolic PDEs, and how do these differences affect the choice of numerical methods for solving them?

- Concept: State-Space Models (SSMs)
  - Why needed here: SSMs provide the underlying structure for MNO, and understanding their properties and discretisation methods is crucial for implementing and optimising the MNO framework.
  - Quick check question: How does the zero-order hold discretisation method in SSMs differ from the Euler method, and what are the implications for accuracy and computational efficiency?

- Concept: Neural Operators
  - Why needed here: Neural operators are the general framework that MNO extends, and understanding their structure and properties is essential for comparing and contrasting different approaches to solving PDEs.
  - Quick check question: How do neural operators differ from traditional neural networks in their ability to learn mappings between function spaces, and what are the implications for solving PDEs?

## Architecture Onboarding

- Component map: Input → Bi-Directional Scan Expand → S6/Cross S6 Block → Bi-Directional Scan Merge → Output

- Critical path:
  - Input → Bi-Directional Scan Expand → S6/Cross S6 Block → Bi-Directional Scan Merge → Output

- Design tradeoffs:
  - MNO vs. Transformers: MNO offers better accuracy and efficiency for PDEs, but may require more complex implementation and tuning.
  - S6 vs. Cross S6 blocks: S6 blocks are simpler and faster, but Cross S6 blocks may offer better performance for certain types of PDEs.

- Failure signatures:
  - If the model fails to converge or produces unstable results, it may indicate issues with the discretisation method or the SSM parameters.
  - If the model overfits or underfits the data, it may indicate issues with the architecture or the training process.

- First 3 experiments:
  1. Implement a simple MNO model for the Darcy Flow equation and compare its performance to a baseline Transformer model.
  2. Experiment with different discretisation methods and SSM parameters to optimise the MNO model for a specific type of PDE.
  3. Evaluate the MNO model's performance on a larger and more complex PDE dataset, such as the Shallow Water 2D equation.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the Mamba Neural Operator's performance scale with increasing spatial and temporal resolution compared to Transformers?
- Basis in paper: [inferred] The paper evaluates MNO on specific resolutions (128² spatial, 101 temporal steps) but does not explore performance at higher resolutions or analyze scaling behavior.
- Why unresolved: The experiments focus on fixed grid sizes, and the authors do not discuss how MNO's efficiency or accuracy changes with increased resolution, which is critical for real-world applications requiring high fidelity.
- What evidence would resolve it: Systematic experiments varying spatial and temporal resolution, with runtime and accuracy comparisons to Transformers, would clarify MNO's scalability advantages.

### Open Question 2
- Question: Can the theoretical connection between neural operator layers and time-varying SSMs be extended to other neural operator architectures beyond the ones tested?
- Basis in paper: [explicit] The authors claim the connection is "applicable to diverse architectures, including Transformer-based models" but only validate it on GNOT, Galerkin Transformer, and OFormer.
- Why unresolved: The theoretical framework is presented as general, but empirical validation is limited to three architectures. It's unclear if the connection holds for newer or more complex neural operator designs.
- What evidence would resolve it: Applying the theoretical framework to additional architectures (e.g., FNO variants, DeepONet) and demonstrating consistent alignment between their layers and SSM structures would confirm broader applicability.

### Open Question 3
- Question: What are the limitations of Mamba Neural Operator in handling irregular domains or unstructured grids, and how can these be addressed?
- Basis in paper: [inferred] The experiments use uniform grids, and while the authors mention that irregular grids are being addressed in related works, they do not explore MNO's performance on such domains.
- Why unresolved: Real-world PDEs often involve complex geometries, and the paper's focus on structured grids leaves open questions about MNO's adaptability to unstructured data.
- What evidence would resolve it: Testing MNO on datasets with irregular domains or unstructured grids, and comparing its performance to specialized methods for such cases, would reveal its limitations and potential improvements.

## Limitations

- Implementation complexity: MNO requires careful implementation of S6/Cross S6 blocks and their integration with neural operators, which may present challenges for practitioners without extensive experience in both SSMs and neural operators.

- Computational requirements: While MNO shows improved efficiency over Transformers, the computational demands for training on complex PDEs may still be significant, particularly for large-scale problems or when using high-resolution discretizations.

- Generalization uncertainty: The evaluation focuses on three specific PDE types from PDEBench, and the framework's performance on other PDE classes, especially those with different mathematical properties (e.g., nonlinear, time-dependent, or multi-physics problems), remains uncertain.

## Confidence

- Mechanism 1 (MNO captures long-range dependencies better than Transformers): Medium
- Mechanism 2 (MNO bridges efficiency and accuracy): Medium
- Mechanism 3 (Theoretical understanding of structural similarity): Medium

## Next Checks

1. **Cross-PDE Validation**: Evaluate MNO's performance on a broader range of PDE types, including nonlinear and time-dependent equations, to assess its generalizability and identify potential limitations.

2. **Scalability Analysis**: Test MNO's performance and computational efficiency on larger-scale problems with higher spatial and temporal resolutions to determine its practical applicability for real-world PDE modeling tasks.

3. **Ablation Studies**: Conduct systematic ablation studies to isolate the contributions of the S6/Cross S6 blocks and the SSM integration, quantifying their individual impact on accuracy and efficiency.