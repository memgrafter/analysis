---
ver: rpa2
title: End-to-End Reinforcement Learning of Curative Curtailment with Partial Measurement
  Availability
arxiv_id: '2405.03262'
source_url: https://arxiv.org/abs/2405.03262
tags:
- grid
- power
- learning
- curtailment
- flow
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proposes a novel deep reinforcement learning (DRL) approach
  for curative curtailment in distribution grids with partial measurement availability.
  The method learns to curtail power and set appropriate reactive power to resolve
  grid congestions using only a subset of observable buses, addressing the challenge
  of limited grid observability in real-world scenarios.
---

# End-to-End Reinforcement Learning of Curative Curtailment with Partial Measurement Availability

## Quick Facts
- arXiv ID: 2405.03262
- Source URL: https://arxiv.org/abs/2405.03262
- Reference count: 23
- One-line primary result: Achieves 100% voltage violation resolution and 98.8% line overload resolution on a real low-voltage grid using only 7% observability

## Executive Summary
This paper presents a novel deep reinforcement learning approach for curative curtailment in distribution grids with limited measurement availability. The method addresses the challenge of partial grid observability by training an RL agent to make curtailment decisions using only a subset of observable buses. The approach achieves significant computational speedup compared to optimal power flow while maintaining high performance on real-world grid scenarios.

## Method Summary
The authors propose a DDPG-based RL agent trained end-to-end to perform curative curtailment under partial observability. The agent receives observations (P, Q, V) from a subset of buses and outputs curtailment actions for controllable nodes. The environment simulates grid dynamics using power flow computation, providing rewards based on violation resolution and curtailment costs. Data augmentation techniques are employed to increase representation of critical grid states during training.

## Key Results
- Achieves 100% resolution of voltage band violations on a real low-voltage grid
- Resolves 98.8% of asset overloads while using only 7% observability
- Demonstrates 300x faster computation time compared to optimal power flow methods

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The RL agent can learn to make curtailment decisions under sparse observability by iteratively adjusting actions based on observed feedback.
- Mechanism: The agent receives partial grid state observations and outputs curtailment actions. The environment simulates full grid dynamics via power flow, providing a reward based on violations and curtailment costs. Over training, the agent learns a mapping from partial observations to effective curtailment decisions that resolve grid violations.
- Core assumption: The partial observations contain sufficient information to infer the critical state of the grid and make effective curtailment decisions.
- Evidence anchors:
  - [abstract]: "The DRL agent is trained end-to-end to make decisions under sparse information, achieving 100% resolution of voltage band violations and 98.8% resolution of asset overloads on a real low-voltage grid."
  - [section]: "The RL agent receives the observation and determines the