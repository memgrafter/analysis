---
ver: rpa2
title: 'RWKV-TS: Beyond Traditional Recurrent Neural Network for Time Series Tasks'
arxiv_id: '2401.09093'
source_url: https://arxiv.org/abs/2401.09093
tags:
- time
- rwkv-ts
- series
- forecasting
- maemse
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents RWKV-TS, a recurrent neural network model designed
  to overcome the limitations of traditional RNNs for time series tasks. RWKV-TS introduces
  a novel architecture with linear time and space complexity, enhanced long-term sequence
  modeling, and improved computational efficiency.
---

# RWKV-TS: Beyond Traditional Recurrent Neural Network for Time Series Tasks

## Quick Facts
- arXiv ID: 2401.09093
- Source URL: https://arxiv.org/abs/2401.09093
- Authors: Haowen Hou; F. Richard Yu
- Reference count: 20
- This paper presents RWKV-TS, a recurrent neural network model designed to overcome the limitations of traditional RNNs for time series tasks.

## Executive Summary
This paper introduces RWKV-TS, a recurrent neural network architecture designed to handle long time series sequences with linear time and space complexity. RWKV-TS combines a novel WKV operator for efficient long-range memory with a token-shift mechanism for short-term context preservation. Through extensive experiments on six major time series tasks across eight real-world datasets, RWKV-TS demonstrates competitive performance compared to state-of-the-art models while requiring significantly less computational resources.

## Method Summary
RWKV-TS is a recurrent neural network model that achieves O(L) time and space complexity through its WKV operator, which replaces traditional self-attention. The architecture uses instance normalization before patching, followed by a linear projection to tokens. The core RWKV backbone consists of stacked residual blocks containing time-mixing and channel-mixing sub-blocks. The model is trained using AdamW optimizer with learning rate 0.0001 and cosine decay schedule, with MSE loss for regression tasks and cross-entropy for classification.

## Key Results
- RWKV-TS achieves competitive performance on six major time series tasks compared to state-of-the-art models
- The model demonstrates significantly reduced latency and memory utilization due to its linear complexity
- RWKV-TS shows improved ability to capture long-term sequence information compared to traditional RNNs

## Why This Works (Mechanism)

### Mechanism 1
RWKV-TS achieves linear time and space complexity through its WKV operator, which avoids the quadratic cost of traditional self-attention. The WKV operator uses a recurrence relation where the state st accumulates weighted dot products of keys and values from all previous time steps, scaled by a learnable decay vector w. This allows each token to attend to the entire history in O(L) time and space.

### Mechanism 2
The token-shift mechanism in the time-mixing sub-block preserves short-term context while still allowing long-range propagation via the WKV state. Each gate (g, r, k, v) is computed as a linear interpolation between the current token xt and the previous token xt-1, weighted by learnable parameters µg, µr, µk, µv. This injects immediate temporal context into the gating signals, while the WKV state carries information across longer horizons.

### Mechanism 3
Instance normalization before patching reduces domain shift effects and improves generalization across different time series. Each univariate time series is normalized to zero mean and unit variance before patching, then the normalization parameters are reapplied to the predictions, effectively centering and scaling inputs per-instance.

## Foundational Learning

- Concept: Recurrent neural network architecture and vanishing/exploding gradient problem
  - Why needed here: RWKV-TS is an RNN-based model; understanding why traditional RNNs fail on long sequences explains the motivation for the design choices.
  - Quick check question: What mechanism do LSTM and GRU use to mitigate the vanishing gradient problem, and why might they still struggle when L > 100?

- Concept: Self-attention and its computational complexity
  - Why needed here: RWKV-TS replaces self-attention with a WKV operator; knowing how attention works and why it is O(L^2) clarifies the efficiency gain.
  - Quick check question: In a standard Transformer, how many key-value pairs must be computed for a sequence of length L, and how does this scale?

- Concept: Normalization techniques (InstanceNorm vs BatchNorm)
  - Why needed here: RWKV-TS uses Instance Normalization before patching; understanding the difference explains why this choice helps with time series.
  - Quick check question: How does Instance Normalization differ from Batch Normalization in terms of the statistics it uses, and why is this important for per-instance time series?

## Architecture Onboarding

- Component map: Input module (Instance normalization → patching → linear projection) -> RWKV backbone (stacked residual blocks) -> Output module (Flatten → linear head)
- Critical path: Input → patch projection → time-mixing → channel-mixing → output
- Design tradeoffs:
  - Linear complexity vs. quadratic attention: RWKV-TS trades the full pairwise interactions of self-attention for efficient recurrence, at the cost of potentially less expressive interactions.
  - Token shift vs. positional encoding: The interpolation approach injects short-term context but does not explicitly encode absolute positions, relying instead on the recurrence.
  - Instance normalization vs. batch statistics: Normalizing per instance improves robustness to distribution shift but may lose cross-instance patterns.
- Failure signatures:
  - Degraded long-range performance: If the decay vector w is not learned properly, the WKV state may lose historical information too quickly.
  - Poor short-term modeling: If µg, µr, µk, µv are near 0 or 1, the model may fail to capture local dynamics.
  - Numerical instability: If w is not constrained to (0,1), the recurrence may explode.
- First 3 experiments:
  1. Verify linear complexity: Run RWKV-TS on sequences of increasing length and measure runtime and memory; confirm O(L) scaling.
  2. Ablate token shift: Set µg = µr = µk = µv = 0 or 1 and compare performance to baseline; confirm importance of short-term context.
  3. Test normalization impact: Remove instance normalization and retrain; compare generalization on held-out series with different statistics.

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions, but several remain regarding the scalability of RWKV-TS for very long sequences beyond those tested, the robustness of the time-decay mechanism across diverse time series domains, and the need for additional ablation studies for the token-shift parameters and alternative normalization strategies.

## Limitations
- Empirical scalability for very long sequences (L > 10,000) remains untested
- The optimal values for token-shift parameters (µg, µr, µk, µv) are not established through extensive ablation
- Generalization to domains beyond the tested datasets (weather, traffic, electricity, etc.) is not demonstrated

## Confidence
- **High**: RWKV-TS achieves linear time and space complexity via the WKV operator; instance normalization before patching reduces domain shift.
- **Medium**: RWKV-TS improves long-term sequence modeling relative to traditional RNNs; the token-shift mechanism effectively preserves short-term context.
- **Low**: RWKV-TS outperforms all baselines on every task; the model is robust to all forms of distribution shift.

## Next Checks
1. **Scalability test**: Train and evaluate RWKV-TS on synthetic sequences up to L = 20,000, measuring runtime and memory, to confirm O(L) scaling holds in practice.
2. **Ablation of token shift**: Systematically vary µg, µr, µk, µv (including setting them to 0 or 1) and retrain on the main forecasting tasks; quantify impact on both short- and long-range performance.
3. **Domain robustness**: Apply RWKV-TS to a held-out dataset from a different domain (e.g., stock prices or medical time series) without retraining; assess whether instance normalization alone is sufficient to handle domain shift.