---
ver: rpa2
title: Reward Fine-Tuning Two-Step Diffusion Models via Learning Differentiable Latent-Space
  Surrogate Reward
arxiv_id: '2411.15247'
source_url: https://arxiv.org/abs/2411.15247
tags:
- reward
- image
- arxiv
- lasro
- fine-tuning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of fine-tuning two-step diffusion
  models (DMs) for ultra-fast image generation with arbitrary rewards, including non-differentiable
  ones. The authors identify limitations of existing policy-based reinforcement learning
  (RL) methods like PPO and Diffusion-DPO when applied to step-distilled DMs due to
  hard exploration, degenerated objectives, and non-smooth mappings.
---

# Reward Fine-Tuning Two-Step Diffusion Models via Learning Differentiable Latent-Space Surrogate Reward

## Quick Facts
- arXiv ID: 2411.15247
- Source URL: https://arxiv.org/abs/2411.15247
- Reference count: 40
- Key outcome: LaSRO significantly outperforms existing RL methods like DDPO, Diffusion-DPO, and GORS-LCM across different reward objectives while maintaining image fidelity.

## Executive Summary
This paper addresses the challenge of fine-tuning two-step diffusion models (DMs) for ultra-fast image generation with arbitrary rewards, including non-differentiable ones. The authors identify limitations of existing policy-based reinforcement learning (RL) methods like PPO and Diffusion-DPO when applied to step-distilled DMs due to hard exploration, degenerated objectives, and non-smooth mappings. To overcome these issues, they propose LaSRO, a framework that learns a differentiable surrogate reward in the latent space of pre-trained DMs. This surrogate reward converts arbitrary rewards into differentiable ones, enabling efficient reward gradient guidance without relying on policy gradient estimation. LaSRO leverages off-policy exploration and connects to value-based RL, providing theoretical insights. Experiments show LaSRO significantly outperforms existing RL methods across different reward objectives while maintaining image fidelity.

## Method Summary
LaSRO is a two-stage framework that first pretrains a surrogate reward model in the latent space of pre-trained diffusion models, then uses this model for reward optimization. The method learns a differentiable surrogate reward function using a Bradley-Terry model to rank winning and losing samples according to the target reward. This allows arbitrary rewards (including non-differentiable ones) to be converted into differentiable signals for direct reward gradient guidance. The framework uses off-policy exploration in the latent space to address hard exploration problems, sampling multiple initial noise vectors and injected noise to collect diverse samples. Training alternates between diffusion model fine-tuning and online surrogate adaptation with normalization and clipping functions for stability.

## Key Results
- LaSRO achieves higher Image Reward scores than DDPO, Diffusion-DPO, and GORS-LCM on two-step diffusion models
- LaSRO improves Attribute Binding Score accuracy by significant margins while maintaining image quality
- LaSRO demonstrates better Text Alignment Score performance compared to baseline RL methods
- The method maintains image fidelity as measured by FID scores while optimizing for various reward objectives

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The learned surrogate reward converts arbitrary (including non-differentiable) rewards into differentiable signals, enabling direct reward gradient guidance without policy gradient estimation.
- Mechanism: The surrogate reward model R_ψ is trained using a Bradley-Terry model in the latent space, learning to rank winning and losing samples according to the target reward r. This learned ranking function provides differentiable gradients that guide the diffusion model update.
- Core assumption: The latent space of pre-trained diffusion models contains sufficient information to predict arbitrary rewards through a learned function.
- Evidence anchors:
  - [abstract]: "learns surrogate reward models in the latent space of SDXL to convert arbitrary rewards into differentiable ones for effective reward gradient guidance"
  - [section 5.1]: "we propose to learn a differentiable surrogate reward that efficiently provides accurate reward gradient guidance for two-step image generation"
- Break condition: If the latent space lacks sufficient discriminative information about the target reward, the surrogate model cannot learn accurate gradients.

### Mechanism 2
- Claim: Off-policy exploration in the latent space solves the hard exploration problem for two-step diffusion models.
- Mechanism: Instead of sampling only from the current policy p_θ(x_τ^H | x_τ^0, c), LaSRO samples multiple initial noise vectors x_τ^0 and injected noise Z to explore the latent space, collecting diverse samples for both surrogate reward learning and diffusion model fine-tuning.
- Core assumption: The latent space exploration via multiple noise samples provides sufficient coverage to learn
- Evidence anchors:
  - [section 4.2]: "Off-Policy Exploration" section describes the sampling strategy
  - [section 5.3]: "LaSRO benefits from off-policy exploration for hard exploration"

## Foundational Learning

### Concept 1: Two-Step Diffusion Models
- Why needed: Understanding the unique challenges of two-step models (LCM, LCM-X) is crucial as they present specific difficulties for RL fine-tuning that LaSRO addresses.
- Quick check: Verify that two-step models use a single denoising step followed by a deterministic step, creating non-smooth reward landscapes.

### Concept 2: Bradley-Terry Model
- Why needed: This pairwise ranking model is the foundation for learning the surrogate reward in latent space.
- Quick check: Confirm that the Bradley-Terry loss compares pairs of samples to learn relative quality rankings.

### Concept 3: Latent Space Exploration
- Why needed: LaSRO's key innovation is exploring the latent space rather than the image space for more efficient reward learning.
- Quick check: Verify that exploration happens through sampling multiple noise vectors in the latent space rather than in pixel space.

## Architecture Onboarding

### Component Map
Pre-trained DM -> Surrogate Reward Model (latent UNet + CNN head) -> Fine-tuned DM

### Critical Path
1. Pretrain surrogate reward model using Bradley-Terry loss on latent space pairs
2. Fine-tune diffusion model using differentiable surrogate reward gradients
3. Alternate between model updates and online surrogate adaptation

### Design Tradeoffs
- Pros: Converts non-differentiable rewards to differentiable, avoids policy gradient estimation, enables off-policy exploration
- Cons: Adds computational overhead of surrogate model, requires careful normalization/clipping for stability

### Failure Signatures
- Training instability: Check gradient norms and use smaller learning rates
- Reward over-optimization: Monitor FID scores alongside reward metrics
- Poor exploration: Verify off-policy sampling is working by checking diversity of initial noise samples

### First 3 Experiments
1. Test surrogate reward pretraining on simple pairwise ranking tasks
2. Validate differentiable reward gradients on a toy diffusion model
3. Compare exploration efficiency between on-policy and off-policy sampling

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does LaSRO's performance scale with different numbers of initial noise samples (Ns) during the surrogate reward pre-training stage?
- Basis in paper: [explicit] The paper mentions Ns as a hyperparameter and shows it affects training (e.g., Ns=1 for Image Reward, Ns=6 otherwise) but doesn't systematically explore its impact on final performance.
- Why unresolved: The paper provides specific Ns values for different reward objectives but lacks ablation studies showing how varying Ns affects convergence speed, final reward optimization, or sample efficiency.
- What evidence would resolve it: Systematic experiments varying Ns across multiple orders of magnitude while measuring both convergence speed and final reward scores for different reward objectives.

### Open Question 2
- Question: Can LaSRO be effectively applied to three-step or higher-step diffusion models, and how would its performance compare to two-step applications?
- Basis in paper: [inferred] The paper focuses exclusively on two-step models and analyzes why two-step models present unique challenges, but doesn't explore whether these insights extend to three-step or higher-step models.
- Why unresolved: The paper establishes LaSRO's effectiveness for two-step models but doesn't investigate whether the latent-space surrogate reward approach provides similar benefits for models with more steps, where the non-smoothness and exploration challenges might be less severe.
- What evidence would resolve it: Experiments applying LaSRO to three-step and four-step models while comparing performance improvements, convergence behavior, and computational requirements relative to two-step applications.

### Open Question 3
- Question: What is the theoretical relationship between the learned surrogate reward model Rψ and the true underlying value function for different reward signal structures?
- Basis in paper: [explicit] The paper establishes a connection between LaSRO and value-based RL, showing TD learning loss equivalence to surrogate reward loss, but doesn't fully characterize when and why this approximation is good.
- Why unresolved: While the paper shows functional equivalence between surrogate reward learning and value function approximation, it doesn't provide theoretical bounds on approximation error or analyze how this relationship varies with reward signal properties.
- What evidence would resolve it: Theoretical analysis providing error bounds between Rψ and true value functions under different reward structures, plus empirical validation showing how these bounds relate to actual performance on various reward objectives.

## Limitations

- The paper claims LaSRO outperforms existing methods "by a significant margin" without statistical tests confirming the differences or defining what constitutes "significant"
- The generalizability to truly arbitrary rewards remains unproven, as only three specific reward types were tested
- Computational overhead compared to existing methods is not quantified, though the method claims efficiency through off-policy exploration

## Confidence

**High Confidence**: The theoretical framework connecting latent space exploration to value-based RL is well-established, and the mechanism for converting non-differentiable rewards through surrogate learning is sound.

**Medium Confidence**: The experimental results showing LaSRO outperforming existing methods on the tested reward functions are compelling, though the lack of statistical significance testing and limited reward diversity reduces confidence in generalizability.

**Low Confidence**: Claims about LaSRO's effectiveness for "arbitrary rewards" are not fully supported by the experimental evidence, which only tests three specific reward types.

## Next Checks

1. **Statistical validation**: Conduct t-tests or bootstrap confidence intervals on the reward score differences between LaSRO and baseline methods across multiple runs to establish statistical significance of the performance improvements.

2. **Reward diversity testing**: Evaluate LaSRO on additional reward types beyond the three tested, particularly rewards that require understanding of abstract concepts or long-range dependencies to assess true generality.

3. **Computational efficiency analysis**: Measure and compare wall-clock training time, GPU memory usage, and total computational cost of LaSRO versus existing RL fine-tuning methods to quantify the claimed efficiency gains from off-policy exploration.