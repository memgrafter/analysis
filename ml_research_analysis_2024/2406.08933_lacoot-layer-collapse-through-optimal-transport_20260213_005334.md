---
ver: rpa2
title: 'LaCoOT: Layer Collapse through Optimal Transport'
arxiv_id: '2406.08933'
source_url: https://arxiv.org/abs/2406.08933
tags:
- lacoot
- performance
- distance
- neural
- layers
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: LaCoOT is an optimal transport-based method that reduces the depth
  of over-parameterized deep neural networks by minimizing the Max-Sliced Wasserstein
  distance between intermediate feature distributions. The approach introduces a regularization
  strategy during training that encourages layers to act as identity functions, enabling
  their complete removal post-training with minimal performance loss.
---

# LaCoOT: Layer Collapse through Optimal Transport

## Quick Facts
- arXiv ID: 2406.08933
- Source URL: https://arxiv.org/abs/2406.08933
- Reference count: 40
- One-line primary result: LaCoOT reduces deep network depth with minimal performance loss using Max-Sliced Wasserstein distance regularization

## Executive Summary
LaCoOT is an optimal transport-based method that reduces the depth of over-parameterized deep neural networks by minimizing the Max-Sliced Wasserstein distance between intermediate feature distributions. The approach introduces a regularization strategy during training that encourages layers to act as identity functions, enabling their complete removal post-training with minimal performance loss. Tested across three architectures (ResNet-18, Swin-T, MobileNetV2) and seven datasets for image classification, LaCoOT achieved better performance/depth trade-offs than existing methods. For example, it reduced ResNet-18's critical path length while maintaining comparable accuracy, achieving practical benefits including 40% reduction in inference time and 53% reduction in MACs. The method was also successfully extended to a DiT-XL/2 generative model, demonstrating halved FID scores when two blocks were removed.

## Method Summary
LaCoOT introduces a regularization strategy based on the Max-Sliced Wasserstein distance to minimize the distance between intermediate feature distributions during training. By encouraging layers to behave as identity functions through this distributional constraint, the method enables complete removal of entire layers after training without significant performance degradation. The approach requires only a single training phase and compares favorably against existing depth reduction methods like Layer Folding, EGP, NEPENTHE, and EASIER. LaCoOT was validated across three architectures (ResNet-18, Swin-T, MobileNetV2) and seven datasets for image classification, demonstrating consistent improvements in the performance/depth tradeoff.

## Key Results
- Achieved better performance/depth trade-offs than existing methods across three architectures and seven datasets
- Reduced ResNet-18's critical path length while maintaining comparable accuracy, with 40% reduction in inference time and 53% reduction in MACs
- Successfully extended to DiT-XL/2 generative model, halving FID scores when two blocks were removed

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Minimizing the Max-Sliced Wasserstein distance between consecutive block feature distributions encourages layers to behave as identity functions.
- Mechanism: The Max-Sliced Wasserstein distance measures distributional similarity between inputs and outputs of network blocks. When minimized, it forces the block transformation to preserve the statistical properties of its input, effectively reducing its contribution beyond identity mapping.
- Core assumption: A layer acting as an identity function can be removed without performance degradation because it doesn't introduce meaningful transformation.
- Evidence anchors:
  - [abstract]: "we propose a new regularization strategy based on the Max-Sliced Wasserstein distance to minimize the distance between the intermediate feature distributions"
  - [section 4.1]: "we will incorporate a penalization of the distance between two consecutive blocks during training... yields, after training, to identifying blocks that can be removed from the architecture without impacting the performance"
  - [corpus]: Weak correlation - no direct mention of Max-Sliced Wasserstein in neighbor papers
- Break condition: If the Max-Sliced Wasserstein distance cannot be computed due to mismatched dimensions between consecutive layers, or if the distance threshold ε is set too high, allowing non-identity layers to be incorrectly pruned.

### Mechanism 2
- Claim: The regularization acts as a soft 1-Lipschitz constraint on the network, preventing arbitrary amplification of small differences and distributional changes.
- Mechanism: By constraining the Jacobian of each block transformation to be orthogonal (as derived in section 4.3), the method ensures that the network maintains gradient norms and avoids excessive distributional shifts between layers.
- Core assumption: A 1-Lipschitz constraint does not limit the expressiveness of neural networks for classification tasks while improving generalization.
- Evidence anchors:
  - [section 4.3]: "The constraint µk = νk can be interpreted as an orthogonality constraint on the Jacobian of the block transformation... This implies a block-wise soft Lipschitz constraint"
  - [section 4.3]: "a 1-Lipschitz constraint does not limit the expressiveness... for classification tasks"
  - [corpus]: Weak correlation - no direct mention of Lipschitz constraints in neighbor papers
- Break condition: If the regularization strength λ is too high, the network may underfit by being overly constrained, failing to learn necessary transformations.

### Mechanism 3
- Claim: The Max-Sliced Wasserstein distance provides a reliable importance metric for layer removal, outperforming random and performance-based methods.
- Mechanism: Layers with lower Max-Sliced Wasserstein distances are more likely to be close to identity functions and thus removable. The method compares favorably against random removal and block influence (performance-based) approaches.
- Core assumption: The Max-Sliced Wasserstein distance between input and output distributions of a block correlates with its functional importance to the overall network.
- Evidence anchors:
  - [section 5.4]: "we study the impact ofλ... we compare it with two alternative methods for selecting which layers to remove... LaCoOT achieves a better performance/compression trade-off"
  - [section 4.3]: "Without our regularization, the blocks operate changes on the intermediate features' distribution, which is unnecessary"
  - [corpus]: Weak correlation - no direct mention of Max-Sliced Wasserstein distance in neighbor papers
- Break condition: If the network architecture is already parameter-efficient, the Max-Sliced Wasserstein distance may not effectively identify removable layers, as all layers may be functionally important.

## Foundational Learning

- Concept: Optimal Transport and Wasserstein Distance
  - Why needed here: LaCoOT relies on computing the Max-Sliced Wasserstein distance between feature distributions to measure layer importance
  - Quick check question: What is the computational advantage of using sliced Wasserstein distance over standard Wasserstein distance in high-dimensional spaces?

- Concept: Jacobian Analysis and Lipschitz Constraints
  - Why needed here: The method interprets its regularization as imposing orthogonality constraints on Jacobians, effectively creating a soft Lipschitz constraint
  - Quick check question: How does constraining the Jacobian of a layer transformation relate to the layer behaving as an identity function?

- Concept: Neural Network Pruning and Compression Techniques
  - Why needed here: LaCoOT is compared against existing depth reduction methods like Layer Folding, EGP, NEPENTHE, and EASIER
  - Quick check question: What is the key difference between unstructured pruning and structured pruning in terms of practical deployment benefits?

## Architecture Onboarding

- Component map: Input -> Network blocks (groups of layers separated by skip connections) -> Classifier head
- Critical path: Data flows through network blocks sequentially, with skip connections bypassing certain blocks. Critical path length equals number of sequential blocks.
- Design tradeoffs:
  - Regularization strength λ: Higher λ allows more aggressive pruning but risks underfitting
  - Batch size: Larger batches improve Max-Sliced Wasserstein distance estimation but require more memory
  - Number of projections: More projections give better Max-Sliced Wasserstein distance approximation but increase computation
- Failure signatures:
  - Performance degradation after layer removal indicates too aggressive pruning
  - Training instability with high λ values
  - Inconsistent Max-Sliced Wasserstein distance rankings across runs with small batch sizes
- First 3 experiments:
  1. Implement Max-Sliced Wasserstein distance computation between consecutive ResNet block outputs on CIFAR-10, measure baseline distances without regularization
  2. Add LaCoOT regularization with small λ, verify that Max-Sliced Wasserstein distances decrease and compare performance to baseline
  3. Test layer removal with increasing λ values, measure performance degradation and MAC reduction to find optimal tradeoff

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does LaCoOT's performance scale with extremely large over-parameterized models beyond DiT-XL/2, such as those with billions of parameters?
- Basis in paper: [explicit] The paper mentions extending LaCoOT to a DiT-XL/2 generative model and notes that it "required only a few fine-tuning steps" and achieved "twice as low" FID when removing two blocks, suggesting scalability but without exploring larger models
- Why unresolved: The paper only tested on a single large model (DiT-XL/2) and doesn't examine whether the method maintains effectiveness or computational efficiency when scaling to foundation models with orders of magnitude more parameters
- What evidence would resolve it: Experiments applying LaCoOT to models like GPT-4, Claude, or other multi-billion parameter models showing consistent performance preservation and practical benefits in terms of inference speedup and resource reduction

### Open Question 2
- Question: What is the optimal balance between regularization strength (λ) and model complexity for different architectural families (e.g., residual networks vs. transformers)?
- Basis in paper: [explicit] The paper shows that "the higher λ, the more blocks can be removed without harming performance" and that λ selection involves a "trade-off between performance and complexity," but only demonstrates this on three architectures
- Why unresolved: The paper provides limited architectural diversity and doesn't establish whether there are systematic patterns in how λ should be tuned based on architectural characteristics like depth, width, or connectivity patterns
- What evidence would resolve it: A comprehensive study across diverse architectural families (CNNs, transformers, MLPs, hybrid architectures) showing how optimal λ varies with architectural properties and task complexity

### Open Question 3
- Question: How does LaCoOT compare to other compression methods when combined sequentially (e.g., LaCoOT followed by quantization or unstructured pruning)?
- Basis in paper: [inferred] The paper mentions that structured pruning methods "operate at different levels of granularity" and suggests they are "complementary," noting that "applying DepGraph on top of LaCoOT yields even greater latency gains," but doesn't explore full combinations with other compression techniques
- Why unresolved: The paper only briefly mentions potential complementarity with structured pruning and doesn't systematically investigate how LaCoOT's reduced-depth models perform when further compressed with quantization, unstructured pruning, or other techniques
- What evidence would resolve it: Empirical studies showing performance, inference speed, and memory usage trade-offs when applying LaCoOT followed by various compression pipelines, compared to applying these methods individually or in different orders

## Limitations
- The relationship between Max-Sliced Wasserstein distance and layer importance may not hold for all architectures or tasks
- Handling of dimension mismatches between layers is mentioned but not fully detailed
- Comparison with other compression methods may not account for all deployment scenarios

## Confidence
- **High Confidence**: The computational benefits (40% inference time reduction, 53% MAC reduction) are directly measurable and well-documented across experiments
- **Medium Confidence**: The theoretical justification for using Max-Sliced Wasserstein distance as a regularization objective is sound, but its practical effectiveness compared to other metrics needs further validation
- **Medium Confidence**: The single-phase training approach is promising, but the long-term stability of compressed models across different tasks and datasets requires more extensive testing

## Next Checks
1. Apply LaCoOT to additional architectures beyond ResNet, Swin, and MobileNet (e.g., Vision Transformers, EfficientNet) to verify cross-architecture effectiveness
2. Evaluate the compressed models on downstream tasks (transfer learning, fine-tuning) to assess whether the pruning preserves useful representations
3. Conduct systematic ablation studies on λ (regularization strength) and batch size to determine their impact on the Max-Sliced Wasserstein distance estimation accuracy and final performance