---
ver: rpa2
title: Interactive Machine Teaching by Labeling Rules and Instances
arxiv_id: '2409.05199'
source_url: https://arxiv.org/abs/2409.05199
tags:
- rules
- learning
- rule
- active
- instances
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the challenge of efficient expert supervision
  in weakly supervised learning for text classification. The authors analyze existing
  expert-designed rules and find that rule precision is more important than coverage
  across datasets.
---

# Interactive Machine Teaching by Labeling Rules and Instances

## Quick Facts
- arXiv ID: 2409.05199
- Source URL: https://arxiv.org/abs/2409.05199
- Authors: Giannis Karamanolakis; Daniel Hsu; Luis Gravano
- Reference count: 28
- The paper proposes INTERVAL, an interactive learning framework that outperforms state-of-the-art weakly supervised approaches by 7% in F1 across 6 datasets, achieving high F1 values with as few as 10 expert feedback queries.

## Executive Summary
This paper addresses the challenge of efficient expert supervision in weakly supervised learning for text classification. The authors analyze existing expert-designed rules and find that rule precision is more important than coverage across datasets. They then propose INTERVAL, an interactive learning framework that automatically extracts candidate rules using rich patterns (e.g., prompting language models) and solicits expert feedback on both rules and instances. INTERVAL demonstrates significant performance improvements over state-of-the-art weakly supervised approaches while requiring substantially fewer expert interactions than traditional active learning methods.

## Method Summary
The paper presents INTERVAL, an interactive machine teaching framework that combines rule-based weak supervision with active learning. The method uses a Teacher-Student co-training framework where a teacher model generates weak labels using automatically extracted rules, and a student model learns from both labeled data and weak labels. The framework iteratively selects informative instances and extracts candidate rules using rich predicates (n-grams, linguistic features, prompt-based features), then queries experts for feedback on both. The approach aims to maximize learning efficiency by leveraging the fact that each accepted rule can generate weak labels for multiple instances simultaneously.

## Key Results
- INTERVAL outperforms state-of-the-art weakly supervised approaches by 7% in F1 across 6 datasets
- Achieves high F1 values with as few as 10 expert feedback queries, outperforming active learning methods that require 100 queries
- Rule feedback leads to 8% performance improvement over instance feedback only
- Automatic rules outperform expert rules and n-gram rules (77.5 vs 76.8 vs 64.6 F1)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Precision of labeling rules matters more than coverage for effective weak supervision
- Mechanism: High-precision rules provide cleaner signal for the student model, reducing noise in the weak labels and improving generalization
- Core assumption: The student model can effectively learn from a smaller set of high-quality weak labels rather than a larger set of noisy ones
- Evidence anchors:
  - [abstract] "we find that rule precision is more important than coverage across datasets"
  - [section 5.1] Analysis shows precision-coverage weights consistently favor precision (0.54-0.80 vs 0.20-0.46) across all datasets
  - [corpus] No direct corpus evidence found
- Break condition: When the student model cannot effectively denoise or when high-precision rules become too scarce to cover the data space

### Mechanism 2
- Claim: Interactive feedback on automatically extracted rules is more efficient than instance labeling alone
- Mechanism: Each accepted rule generates weak labels for multiple instances simultaneously, amplifying the value of each expert interaction
- Core assumption: Experts can reliably evaluate rule quality and domain experts can assess whether a rule captures a meaningful pattern
- Evidence anchors:
  - [abstract] "INTERVAL outperforms state-of-the-art weakly supervised approaches by 7% in F1" and "requires as few as 10 queries for expert feedback to reach F1 values that existing active learning methods cannot match even with 100 queries"
  - [section 5.3] Rule feedback leads to 8% performance improvement over instance feedback only
  - [corpus] No direct corpus evidence found
- Break condition: When rules become too complex for experts to evaluate efficiently or when the cost of rule evaluation approaches instance labeling cost

### Mechanism 3
- Claim: Rich predicate rules (n-grams, linguistic features, prompt-based features) are more effective than simple n-gram rules
- Mechanism: Rich predicates capture higher-level semantic patterns that may not be expressible through n-grams alone, improving rule precision and coverage
- Core assumption: Pre-trained language models can identify meaningful semantic patterns through prompt-based features
- Evidence anchors:
  - [section 5.2] "Our approach extracts common patterns across instances that might not even share any n-gram features"
  - [section 5.2] Table 5 shows automatic rules outperform expert rules and n-gram rules (77.5 vs 76.8 vs 64.6 F1)
  - [corpus] No direct corpus evidence found
- Break condition: When prompt-based features become unreliable or when linguistic features don't align with domain-specific patterns

## Foundational Learning

- Concept: Weak supervision and rule-based labeling
  - Why needed here: The paper's core contribution relies on understanding how noisy labeling rules can be aggregated to train models
  - Quick check question: What are the key challenges in training models with noisy labeling rules, and how do methods like Snorkel address them?

- Concept: Active learning and instance selection strategies
  - Why needed here: The paper compares rule-based feedback with instance-based active learning, requiring understanding of selection criteria
  - Quick check question: How does hierarchical sampling in active learning differ from uncertainty-based sampling, and when is each more effective?

- Concept: Prompt engineering and zero-shot classification
  - Why needed here: The rule extraction method uses prompt-based features, requiring understanding of how prompts can elicit useful features from pre-trained models
  - Quick check question: How does prompt-based feature extraction differ from direct zero-shot classification, and what are the advantages of using it for rule creation?

## Architecture Onboarding

- Component map:
  Teacher-Student framework -> Instance clustering module -> Rule extraction pipeline -> Interactive feedback loop

- Critical path:
  1. Train initial Teacher-Student using available labeled data and rules
  2. Select diverse instances based on entropy and cluster informativeness
  3. Extract candidate rules using rich predicates
  4. Query expert for instance and rule labels
  5. Update labeled data and rule set
  6. Repeat until budget exhausted

- Design tradeoffs:
  - Rule complexity vs interpretability: Rich predicates improve accuracy but may be harder for experts to evaluate
  - Interaction budget allocation: Balancing instance vs rule queries based on relative costs and effectiveness
  - Prompt template selection: Choosing templates that elicit useful semantic features without introducing bias

- Failure signatures:
  - Student performance plateaus despite additional feedback (rule extraction or instance selection may be ineffective)
  - High rejection rate of candidate rules (precision thresholds too strict or rule extraction method misaligned with domain)
  - Slow convergence (budget allocation between instances and rules may be suboptimal)

- First 3 experiments:
  1. Baseline: Run with no interactive feedback (just Teacher-Student co-training) to establish performance floor
  2. Instance-only feedback: Test active learning baseline to compare against rule+instance approach
  3. Rule extraction validation: Extract rules without expert feedback to verify automatic rule quality before adding interaction overhead

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal balance between precision and coverage for labeling rules in different text classification tasks?
- Basis in paper: [explicit] The authors found that rule precision is more important than coverage across datasets, but the optimal balance may vary by task.
- Why unresolved: The paper provides evidence that precision is generally more important, but doesn't specify how this balance should be tuned for specific tasks or datasets.
- What evidence would resolve it: A comprehensive study across diverse text classification tasks, systematically varying the precision-coverage trade-off and measuring the resulting student model performance.

### Open Question 2
- Question: How does the cost-effectiveness of interactive rule labeling compare to active learning across different types of text classification tasks and labeling budgets?
- Basis in paper: [explicit] The authors show that interactive rule labeling can outperform active learning, but the relative cost-effectiveness varies by dataset and task.
- Why unresolved: The paper demonstrates that rule labeling can be more effective than active learning in some cases, but doesn't provide a general framework for determining when and how much to invest in rule labeling vs. instance labeling.
- What evidence would resolve it: A large-scale empirical study across diverse text classification tasks, varying the labeling budget and measuring the cost-effectiveness of different interactive learning strategies.

### Open Question 3
- Question: How can we automatically determine the optimal number of rules to label per instance in interactive machine teaching?
- Basis in paper: [inferred] The authors show that labeling up to one rule per instance gives strong boosts, but labeling up to two rules gives further improvements in some tasks. They suggest that future research could dynamically determine this threshold.
- Why unresolved: The paper provides some evidence on the optimal number of rules to label per instance, but doesn't provide a general method for automatically determining this number.
- What evidence would resolve it: A method that can automatically determine the optimal number of rules to label per instance based on task characteristics and labeling costs, validated across diverse text classification tasks.

## Limitations

- The analysis showing precision matters more than coverage relies on weight analysis rather than controlled experiments isolating these variables
- The effectiveness of rich predicate rules depends on prompt-based features whose domain-specific reliability isn't validated
- The rule-based feedback advantage doesn't account for the actual cognitive load and time costs of rule evaluation versus instance labeling

## Confidence

- **High Confidence**: The quantitative performance claims (7% F1 improvement, 10-query efficiency vs 100-query active learning) are well-supported by experimental results across 6 datasets
- **Medium Confidence**: The mechanism explaining why precision matters more than coverage is plausible but relies on weight analysis rather than controlled experiments isolating these variables
- **Low Confidence**: The claim that rich predicate rules are universally more effective lacks sufficient evidence, as the analysis depends on prompt-based features whose domain-specific effectiveness isn't validated

## Next Checks

1. **Precision vs Coverage Isolation**: Design an experiment that holds coverage constant while varying precision (or vice versa) to directly test which factor drives the performance gains, rather than relying on weight analysis.

2. **Rule Evaluation Cost-Benefit Analysis**: Measure the actual time and cognitive load experts spend evaluating rules versus labeling instances, and test whether the rule feedback advantage persists when accounting for these costs.

3. **Domain Transfer Validation**: Test the rule extraction method on domains where linguistic features and prompt-based patterns are known to be less reliable (e.g., highly technical or domain-specific terminology) to validate the robustness of rich predicate rules.