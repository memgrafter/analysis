---
ver: rpa2
title: 'LArctan-SKAN: Simple and Efficient Single-Parameterized Kolmogorov-Arnold
  Networks using Learnable Trigonometric Function'
arxiv_id: '2410.19360'
source_url: https://arxiv.org/abs/2410.19360
tags:
- larctan-skan
- functions
- accuracy
- networks
- rkan
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces LArctan-SKAN, a novel single-parameterized
  Kolmogorov-Arnold Network using learnable trigonometric functions. The method constructs
  SKAN base functions from trigonometric functions, developing three variants: LSin-SKAN,
  LCos-SKAN, and LArctan-SKAN.'
---

# LArctan-SKAN: Simple and Efficient Single-Parameterized Kolmogorov-Arnold Networks using Learnable Trigonometric Function

## Quick Facts
- arXiv ID: 2410.19360
- Source URL: https://arxiv.org/abs/2410.19360
- Authors: Zhijie Chen; Xinglin Zhang
- Reference count: 8
- Primary result: Achieves 0.93-2.99% higher test accuracy on MNIST than existing KAN variants while providing 49.55-535.01% faster training

## Executive Summary
This paper introduces LArctan-SKAN, a novel single-parameterized Kolmogorov-Arnold Network that uses learnable trigonometric functions as basis functions. The method constructs SKAN base functions from trigonometric functions, developing three variants: LSin-SKAN, LCos-SKAN, and LArctan-SKAN. Tested on MNIST, LArctan-SKAN significantly outperforms all pure KAN variants and mixed MLP-based models in accuracy while demonstrating remarkable computational efficiency. The approach confirms the effectiveness of trigonometric functions in SKAN construction, providing both superior accuracy and computational performance.

## Method Summary
LArctan-SKAN develops three SKAN variants using learnable trigonometric functions with single trainable parameters. The method leverages trigonometric function properties to create efficient, learnable functions. Experiments conducted on MNIST with linear search for learning rate optimization across 30 epochs, using five independent runs with best result selected. The model architecture uses 784 input neurons, 100 hidden units, and 10 output classes.

## Key Results
- LArctan-SKAN achieves 0.93-2.99% higher test accuracy than existing pure KAN variants on MNIST
- Training speed increases of 49.55-535.01% compared to MLP-based baselines
- Outperforms all pure KAN variants (FourierKAN, LSS-SKAN, Spl-KAN) and mixed MLP-based models (MLP+rKAN, MLP+fKAN)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LArctan-SKAN achieves higher accuracy than pure KAN variants due to trigonometric functions' superior approximation properties
- Mechanism: Trigonometric functions provide periodic, smooth basis functions that can approximate complex relationships more efficiently than spline-based functions
- Core assumption: The periodic nature of trigonometric functions allows for better representation of underlying data patterns
- Evidence anchors:
  - [abstract] "LArctan-SKAN significantly improves test set accuracy over existing models, outperforming all pure KAN variants compared"
  - [section III] "Specifically, LArctan-SKAN achieves test accuracy improvements of 0.93% (over FourierKAN), 0.53% (over LSS-SKAN), 0.14% (over MLP+rKAN), 0.19% (over MLP+fKAN), 1.94% (over Spl-KAN), 2.99% (over FastKAN), 0.50% (over WavKAN), 0.28% (over LSin-SKAN), and 2.01% (over LCos-SKAN)"
- Break condition: If the underlying data relationships are not periodic or smooth, the trigonometric approximation advantage diminishes

### Mechanism 2
- Claim: LArctan-SKAN achieves computational efficiency through single-parameter parameterization
- Mechanism: Single-parameter trigonometric functions reduce the number of trainable parameters while maintaining expressive power, leading to faster training
- Core assumption: Fewer parameters with proper function design can maintain or improve performance while reducing computation
- Evidence anchors:
  - [abstract] "LArctan-SKAN exhibits remarkable computational efficiency, with a training speed increase of 535.01% and 49.55% compared to MLP+rKAN and MLP+fKAN, respectively"
  - [section II] "Single-Parameterized Function (SFunc) constructed from trigonometric functions"
- Break condition: If the single parameter becomes a bottleneck for learning complex patterns, additional parameters may be needed

### Mechanism 3
- Claim: Learnable trigonometric functions provide better convergence behavior than fixed basis functions
- Mechanism: Learnable parameters in trigonometric functions allow the network to adapt the basis functions to the specific problem, leading to faster and more stable convergence
- Core assumption: Adaptive basis functions can better match the underlying data structure than fixed functions
- Evidence anchors:
  - [section III] "Analysis of training processes at optimal performance... LSin-SKAN and LArctan-SKAN exhibit optimal convergence, achieving fast and stable training"
  - [section II] "Learnable parameters were added externally, forming the Learnable Sine (LSin) and Learnable Cosine (LCos) functions"
- Break condition: If the learning rate is poorly chosen or the optimization landscape is highly non-convex, convergence benefits may be lost

## Foundational Learning

- Concept: Kolmogorov-Arnold Representation Theorem
  - Why needed here: The paper builds on KAN architecture which is based on this theorem
  - Quick check question: What does the Kolmogorov-Arnold theorem state about multivariate function representation?

- Concept: Single-parameter parameterization
  - Why needed here: LArctan-SKAN uses a single learnable parameter in its trigonometric functions
  - Quick check question: How does reducing parameters affect model capacity and training efficiency?

- Concept: Trigonometric function properties
  - Why needed here: The paper specifically uses sine, cosine, and arctangent functions as basis functions
  - Quick check question: What are the key mathematical properties of trigonometric functions that make them useful for function approximation?

## Architecture Onboarding

- Component map: 784 input neurons -> Multiple SKAN layers with learnable trigonometric basis functions -> 10 output neurons

- Critical path:
  1. Data preprocessing (MNIST normalization)
  2. Forward pass through SKAN layers with learnable trigonometric functions
  3. Loss computation (cross-entropy)
  4. Backpropagation with gradient updates to the single learnable parameter
  5. Repeat for multiple epochs

- Design tradeoffs:
  - Accuracy vs. parameter efficiency: Single-parameter design reduces parameters but may limit expressiveness
  - Computational speed vs. convergence stability: Trigonometric functions are fast but may oscillate more than splines
  - Model interpretability vs. performance: Single-parameter design maintains interpretability but may sacrifice some performance

- Failure signatures:
  - Oscillating training loss (especially with LCos-SKAN variant)
  - Slow convergence despite single-parameter efficiency
  - Accuracy plateauing below baseline MLP performance

- First 3 experiments:
  1. Baseline comparison: Implement and train all three variants (LSin-SKAN, LCos-SKAN, LArctan-SKAN) on MNIST and compare accuracy to FourierKAN and LSS-SKAN
  2. Parameter sensitivity: Test different initial values for the learnable parameter and observe convergence behavior
  3. Computational efficiency: Measure training time per epoch for each variant and compare to MLP-based baselines

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do LArctan-SKAN and other trigonometric SKAN variants perform on datasets beyond MNIST, particularly on more complex image classification tasks or other types of data?
- Basis in paper: [inferred] The paper only tests on MNIST, a relatively simple dataset, leaving questions about generalization to other domains.
- Why unresolved: The paper focuses exclusively on MNIST results, without exploring performance on diverse datasets or tasks.
- What evidence would resolve it: Systematic testing of LArctan-SKAN variants on multiple benchmark datasets (e.g., CIFAR-10, CIFAR-100, ImageNet) and other data types (e.g., tabular, time series) with performance comparisons to existing models.

### Open Question 2
- Question: What is the theoretical foundation explaining why trigonometric functions, particularly in the LArctan-SKAN formulation, lead to superior performance compared to other basis functions in KANs?
- Basis in paper: [inferred] While the paper leverages Fourier transform theory and constructs SKAN using trigonometric functions, it does not provide a rigorous theoretical explanation for the observed performance advantages.
- Why unresolved: The paper demonstrates empirical success but lacks theoretical analysis connecting trigonometric properties to network performance improvements.
- What evidence would resolve it: Mathematical proofs or theoretical analysis showing how trigonometric function properties (e.g., periodicity, range, differentiability) contribute to better approximation capabilities and training dynamics in KAN architectures.

### Open Question 3
- Question: How do the LArctan-SKAN variants scale to deeper networks with multiple hidden layers, and what are the limitations in terms of network depth and width?
- Basis in paper: [inferred] The experiments use shallow architectures with a single hidden layer, raising questions about scalability to deeper networks.
- Why unresolved: The paper only tests relatively simple network architectures, leaving unanswered questions about performance in deeper or wider configurations.
- What evidence would resolve it: Experiments testing LArctan-SKAN on networks with varying depths (e.g., 2-10 hidden layers) and widths, analyzing performance trends, convergence behavior, and potential limitations or failure modes at different scales.

## Limitations
- Limited to MNIST dataset only; generalization to other tasks remains unproven
- No ablation studies on different network depths or widths
- Computational efficiency claims based on training speed only, not inference latency
- Single hyperparameter search (learning rate) without exploration of other critical parameters
- No comparison with state-of-the-art deep learning models on MNIST

## Confidence

- **High confidence** in computational efficiency improvements due to measurable training speed metrics
- **Medium confidence** in accuracy improvements as results are from single dataset and limited hyperparameter tuning
- **Low confidence** in generalizability across different domains and architectures

## Next Checks
1. Replicate experiments on additional datasets (CIFAR-10, Fashion-MNIST) to verify generalization claims
2. Conduct comprehensive hyperparameter sensitivity analysis beyond learning rate
3. Compare against modern CNN architectures and attention-based models on same tasks to establish relative performance boundaries