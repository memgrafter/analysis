---
ver: rpa2
title: 'ClinicalBench: Can LLMs Beat Traditional ML Models in Clinical Prediction?'
arxiv_id: '2411.06469'
source_url: https://arxiv.org/abs/2411.06469
tags:
- labels
- other
- prediction
- patient
- llms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Large Language Models (LLMs) have shown promise in medical text
  processing and licensing exams, but their effectiveness in clinical prediction remains
  unclear. ClinicalBench benchmarked 14 general-purpose and 8 medical LLMs against
  11 traditional ML models on three clinical prediction tasks (length-of-stay, mortality,
  readmission) using two real-world databases.
---

# ClinicalBench: Can LLMs Beat Traditional ML Models in Clinical Prediction?

## Quick Facts
- arXiv ID: 2411.06469
- Source URL: https://arxiv.org/abs/2411.06469
- Reference count: 40
- Large Language Models (LLMs) underperform traditional ML models in clinical prediction tasks

## Executive Summary
ClinicalBench benchmarked 14 general-purpose and 8 medical LLMs against 11 traditional ML models on three clinical prediction tasks (length-of-stay, mortality, readmission) using real-world databases. The comprehensive evaluation revealed that traditional ML models consistently outperformed LLMs across all tasks, even when LLMs were fine-tuned or prompted with different strategies. While fine-tuning improved some LLMs' performance, they still lagged behind traditional models. These findings challenge the assumption that LLMs' success in medical text processing and licensing exams translates to superior clinical prediction capabilities, suggesting potential limitations in their clinical reasoning abilities.

## Method Summary
The study evaluated 14 general-purpose LLMs (including GPT-3.5, GPT-4, Claude, and others) and 8 medical LLMs (including GPT-4-Med, BioMedLM, and others) on three clinical prediction tasks using MIMIC-III and eICU databases. Eleven traditional ML models (including logistic regression, random forests, and gradient boosting) served as baselines. Models were assessed on length-of-stay prediction, in-hospital mortality prediction, and unplanned readmission prediction. The evaluation included zero-shot, few-shot, and fine-tuned settings for LLMs, with performance measured using appropriate clinical metrics.

## Key Results
- Traditional ML models consistently outperformed all LLMs across all three clinical prediction tasks
- Fine-tuning improved LLM performance but still fell short of traditional ML models
- No clear advantage was observed for medical-specific LLMs over general-purpose LLMs
- Performance differences persisted across different dataset scales and prompting strategies

## Why This Works (Mechanism)
The study's mechanism centers on comparing LLM capabilities against traditional ML approaches in structured clinical prediction tasks. The evaluation design captures how LLMs handle numerical data, temporal relationships, and medical coding - core requirements for clinical prediction. The consistent underperformance suggests fundamental limitations in LLMs' ability to process structured clinical data compared to traditional ML models optimized for these specific tasks.

## Foundational Learning
- **Clinical prediction tasks**: Why needed - core healthcare decision support; Quick check - length-of-stay, mortality, readmission prediction
- **MIMIC-III/eICU databases**: Why needed - large-scale clinical data for validation; Quick check - ICU patient records with structured features
- **Zero-shot vs fine-tuning**: Why needed - evaluate generalization vs task-specific adaptation; Quick check - performance differences across settings
- **Traditional ML baselines**: Why needed - establish performance benchmarks; Quick check - logistic regression, random forests, gradient boosting
- **Clinical metrics**: Why needed - measure prediction quality appropriately; Quick check - accuracy, AUC, F1-score

## Architecture Onboarding

Component map: Clinical data -> Feature extraction -> Model prediction -> Performance evaluation -> Comparison

Critical path: The study's critical path involves preprocessing clinical data, extracting relevant features, running predictions through various models (LLMs and traditional ML), evaluating performance metrics, and comparing results. The bottleneck appears to be the LLMs' processing of structured clinical data.

Design tradeoffs: The study prioritized comprehensive benchmarking over computational efficiency, testing multiple models across different settings. The tradeoff between model complexity (LLMs) and task specificity (traditional ML) was central to the findings.

Failure signatures: LLMs failed to match traditional ML performance particularly in handling numerical clinical features, temporal dependencies, and medical coding patterns. The failure was consistent across different prompting strategies and fine-tuning approaches.

First experiments:
1. Run baseline traditional ML models on a subset of MIMIC-III data
2. Test a single LLM (GPT-4) on zero-shot length-of-stay prediction
3. Compare performance of fine-tuned vs zero-shot LLM on mortality prediction

## Open Questions the Paper Calls Out
None

## Limitations
- Evaluation focused on only three specific clinical prediction tasks
- Used publicly available datasets that may not represent all clinical scenarios
- Limited number of medical-specific LLMs tested compared to general-purpose models
- Computational costs and environmental impact of fine-tuning not discussed

## Confidence
- High confidence: LLMs underperform traditional ML models in tested clinical prediction tasks
- Medium confidence: Current medical LLMs show no clear advantage over general-purpose LLMs for clinical prediction
- Medium confidence: Fine-tuning improves LLM performance but insufficient to match traditional ML

## Next Checks
1. Replicate the benchmark using additional clinical datasets from different healthcare systems to test generalizability
2. Test newer medical-specific LLMs (beyond the 8 evaluated) including those trained on larger clinical corpora
3. Evaluate model performance on more complex clinical reasoning tasks beyond prediction, such as treatment recommendation or differential diagnosis