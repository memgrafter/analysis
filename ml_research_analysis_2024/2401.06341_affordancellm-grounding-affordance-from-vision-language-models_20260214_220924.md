---
ver: rpa2
title: 'AffordanceLLM: Grounding Affordance from Vision Language Models'
arxiv_id: '2401.06341'
source_url: https://arxiv.org/abs/2401.06341
tags:
- affordance
- object
- image
- objects
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces AffordanceLLM, a method for grounding object
  affordances using large-scale vision language models (VLMs). The key innovation
  is leveraging the rich world knowledge embedded in VLMs to improve generalization
  to novel objects and actions.
---

# AffordanceLLM: Grounding Affordance from Vision Language Models

## Quick Facts
- arXiv ID: 2401.06341
- Source URL: https://arxiv.org/abs/2401.06341
- Authors: Shengyi Qian; Weifeng Chen; Min Bai; Xiong Zhou; Zhuowen Tu; Li Erran Li
- Reference count: 40
- Primary result: Significant performance gains on AGD20K benchmark, especially for novel objects and actions

## Executive Summary
This paper introduces AffordanceLLM, a method for grounding object affordances using large-scale vision language models (VLMs). The key innovation is leveraging the rich world knowledge embedded in VLMs to improve generalization to novel objects and actions. The approach extends a VLM backbone (LLaVA) with a mask decoder and a special token to predict affordance maps, and incorporates depth maps as 3D input to elicit geometric reasoning. Experiments on the AGD20K benchmark demonstrate significant performance gains over state-of-the-art methods, especially on the harder split where test objects are semantically different from training objects. Qualitative results show the model can generalize to random internet images with novel objects and even novel actions.

## Method Summary
AffordanceLLM extends the LLaVA VLM by introducing a special <mask_token> that the LLM predicts based on image and text inputs. The hidden state of this token is then used as a query to a mask decoder that generates dense affordance maps. The method also incorporates pseudo-depth maps as additional input to elicit geometric reasoning about object functionality. The model is trained on the AGD20K dataset using a mask reconstruction loss that encourages the generated affordance maps to align with ground truth annotations.

## Key Results
- Significant performance improvements on AGD20K benchmark compared to state-of-the-art methods
- Better generalization to novel objects, especially on the harder split where test objects are semantically different from training objects
- Qualitative results show the model can handle random internet images with novel objects and actions
- Depth input consistently improves performance across different evaluation metrics

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The VLM backbone (LLaVA) provides rich world knowledge about object affordances that enables generalization to novel objects.
- Mechanism: The LLaVA model has been trained on large-scale image-text pairs, learning semantic correspondences between visual concepts and language descriptions of how objects are used. This knowledge transfer allows the model to reason about affordances even for objects not seen during fine-tuning.
- Core assumption: The world knowledge embedded in VLMs includes common-sense understanding of object functionality and human-object interactions that is relevant for affordance reasoning.
- Evidence anchors:
  - [abstract] "by taking the advantage of the rich world, abstract, and human-object-interaction knowledge from pretrained large-scale vision language models"
  - [section 1] "Large language models (LLMs) trained on trillions of tokens contain even richer world knowledge and are capable of answering common-sense questions"
  - [corpus] Weak evidence - corpus shows related papers using VLMs for affordance but doesn't directly confirm world knowledge transfer
- Break condition: If the VLM's pretraining data lacks sufficient coverage of object-function relationships, or if the fine-tuning task is too far removed from the pretraining distribution.

### Mechanism 2
- Claim: The special <mask_token> and mask decoder architecture enables the model to generate dense affordance maps from language reasoning.
- Mechanism: The LLM is trained to predict a special token that represents the affordance answer, and the hidden state of this token is used as a query to the mask decoder to generate the final heatmap. This creates a bridge between language understanding and pixel-level predictions.
- Core assumption: The LLM's reasoning about affordances can be effectively encoded in the hidden state of a single token and decoded into spatial information.
- Evidence anchors:
  - [section 3.1] "We propose to treat affordance as an implicit text token predicted from the LLM, which could be further decoded into a 2D map"
  - [section 3.1] "We train the LLM to predict a special token <mask token>, the hidden state of which is first projected into a query embedding q and then fed into a Decoder to generate a dense affordance map"
- Break condition: If the mask decoder cannot effectively transform the language embedding into spatial affordance information, or if the <mask_token> prediction becomes too abstract.

### Mechanism 3
- Claim: Adding pseudo-depth maps as 3D input elicits geometric reasoning that improves affordance prediction.
- Mechanism: Depth information provides geometric cues about object functionality (e.g., cylindrical shapes for handles, flat surfaces for sitting). The model learns to associate 3D geometry with action affordances, bypassing the need to memorize appearance variations.
- Core assumption: 3D geometry is more strongly correlated with object functionality than 2D appearance, and VLMs can effectively integrate depth information for reasoning.
- Evidence anchors:
  - [abstract] "we introduce depth maps as 3D input to elicit geometric reasoning"
  - [section 1] "Another novel factor we introduce to improve affordance reasoning is 3D geometry, as it holds rich information of object functionality"
  - [section 3.1] "We also include a pseudo depth map as additional inputs to the large language model"
- Break condition: If the depth estimation is too noisy or inaccurate, or if the VLM cannot effectively reason about 3D geometry.

## Foundational Learning

- Concept: Large-scale pretraining of VLMs on image-text pairs
  - Why needed here: The world knowledge and semantic understanding required for affordance reasoning comes from this pretraining
  - Quick check question: What are the key differences between CLIP, LLaVA, and other VLMs in terms of their pretraining objectives and data?

- Concept: Vision transformers and their attention mechanisms
  - Why needed here: The image encoder (OWL-ViT) uses transformer architecture to extract spatial features that are crucial for affordance mapping
  - Quick check question: How does the attention mechanism in vision transformers help capture spatial relationships in images?

- Concept: 3D geometry and depth estimation
  - Why needed here: Depth maps provide geometric information about object shapes that correlates with functionality
  - Quick check question: What are the limitations of pseudo-depth estimation compared to real depth sensors?

## Architecture Onboarding

- Component map: Image encoder (OWL-ViT) → Text encoder (tokenizer) → LLM (LLaVA) → <mask_token> prediction → Mask decoder → Affordance map
- Critical path: The flow from image and text input through the LLM to the mask decoder is the core inference path that must be optimized
- Design tradeoffs: Using OWL-ViT instead of CLIP-ViT increases resolution but requires more memory; adding depth improves performance but requires additional computation
- Failure signatures: Poor affordance maps may indicate issues with the LLM's reasoning, the mask decoder's ability to translate embeddings to spatial information, or the quality of depth estimation
- First 3 experiments:
  1. Test the base LLaVA model on affordance reasoning with the standard prompt to verify world knowledge transfer
  2. Validate the mask decoder architecture with a synthetic affordance dataset to ensure it can decode embeddings to spatial maps
  3. Evaluate the impact of depth input by training with and without depth maps on a subset of the data

## Open Questions the Paper Calls Out
None

## Limitations
- Limited empirical validation of world knowledge transfer from VLMs versus learning during fine-tuning
- Reliance on pseudo-depth estimation with potential quality issues for real-world deployment
- Unclear generalization to truly novel actions not present in training data at all

## Confidence
**High Confidence**: The architectural framework combining VLMs with mask decoders is technically sound and the implementation details are well-specified. The performance improvements on the AGD20K benchmark are measurable and significant.

**Medium Confidence**: The claim that VLMs provide rich world knowledge for affordance reasoning is plausible but not definitively proven. The improvements could be attributed to multiple factors including better optimization, improved geometric reasoning, or the combination of both.

**Low Confidence**: The extent of generalization to truly novel actions and the robustness of the method in diverse real-world conditions with varying depth quality and lighting conditions.

## Next Checks
1. **Ablation Study on Knowledge Transfer**: Train a version of the model without pretraining (random initialization) and compare performance on novel objects versus the full VLM-based approach. This would isolate the contribution of world knowledge from other improvements.

2. **Depth Quality Sensitivity Analysis**: Systematically degrade depth map quality (add noise, reduce resolution, simulate different depth sensing modalities) and measure the impact on affordance prediction accuracy. This would quantify the method's robustness to depth estimation errors.

3. **Novel Action Evaluation**: Create a benchmark split with entirely novel actions (not present in training data at all) and evaluate the model's ability to generalize. This would test the true limits of the VLM's reasoning capabilities for affordance grounding.