---
ver: rpa2
title: 'Leveraging LLMs for Legacy Code Modernization: Challenges and Opportunities
  for LLM-Generated Documentation'
arxiv_id: '2411.14971'
source_url: https://arxiv.org/abs/2411.14971
tags:
- code
- comments
- legacy
- documentation
- metrics
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates the use of large language models (LLMs)
  for generating documentation of legacy software written in MUMPS and IBM mainframe
  Assembly Language Code (ALC). A prompting strategy for line-wise code comment generation
  is proposed, along with a rubric to evaluate the completeness, readability, usefulness,
  and hallucination of the generated comments.
---

# Leveraging LLMs for Legacy Code Modernization: Challenges and Opportunities for LLM-Generated Documentation

## Quick Facts
- arXiv ID: 2411.14971
- Source URL: https://arxiv.org/abs/2411.14971
- Reference count: 40
- Large language models can generate hallucination-free documentation for legacy code with expert validation

## Executive Summary
This paper investigates using large language models to generate documentation for legacy software systems written in MUMPS and IBM mainframe Assembly Language Code. The researchers developed a prompting strategy for line-wise code comment generation and created a rubric to evaluate the completeness, readability, usefulness, and hallucination potential of the generated comments. Human experts evaluated both LLM-generated and ground-truth comments, comparing their assessments against automated metrics like code complexity. The study found that LLM-generated comments are generally hallucination-free and useful, though ALC presents greater challenges than MUMPS. Notably, no automated metrics showed strong correlation with comment quality, highlighting the need for better evaluation methods in this domain.

## Method Summary
The researchers developed a prompting strategy for line-wise code comment generation using large language models. They created a comprehensive rubric to evaluate generated comments across four dimensions: completeness, readability, usefulness, and hallucination potential. Human experts evaluated comments from both MUMPS (69 methods) and ALC (66 methods) codebases, comparing LLM-generated comments against ground-truth documentation. The evaluation process included automated metrics such as code complexity measurements and reference-based metrics, though these showed no strong correlation with human assessment of comment quality.

## Key Results
- LLM-generated comments are hallucination-free and generally complete, readable, and useful
- Assembly Language Code (ALC) poses greater challenges for LLM documentation generation compared to MUMPS
- No automated metrics tested (including code complexity) show strong correlation with human assessments of comment quality

## Why This Works (Mechanism)
The approach works by leveraging LLMs' pattern recognition capabilities to understand legacy code syntax and semantics, then generating natural language explanations that align with human understanding of the code's purpose and functionality.

## Foundational Learning
- Prompt engineering for code documentation: Why needed - to guide LLMs toward generating accurate, context-appropriate comments; Quick check - verify prompt effectiveness across different legacy languages
- Code complexity metrics: Why needed - to understand how code structure affects documentation quality; Quick check - measure correlation between cyclomatic complexity and comment accuracy
- Hallucination detection in AI-generated documentation: Why needed - to ensure generated comments accurately reflect code behavior; Quick check - validate comments against actual code execution paths
- Legacy language syntax understanding: Why needed - to bridge the gap between modern LLMs and older programming paradigms; Quick check - test LLM comprehension of MUMPS and ALC constructs
- Multi-expert evaluation protocols: Why needed - to reduce individual bias in quality assessments; Quick check - measure inter-rater reliability scores
- Automated evaluation metrics for documentation: Why needed - to scale assessment beyond human expert capacity; Quick check - test semantic similarity measures against human judgment

## Architecture Onboarding
- Component map: LLM model -> Prompt engineering -> Code parsing -> Comment generation -> Expert evaluation -> Quality assessment
- Critical path: Prompt design → Code input → LLM processing → Comment output → Expert validation
- Design tradeoffs: Granular line-by-line comments vs. holistic documentation; automated vs. human evaluation
- Failure signatures: Hallucinations in generated comments; correlation failures between automated metrics and human assessment
- First experiments: 1) Test different prompt templates across legacy languages, 2) Compare multi-expert consensus vs. individual assessment, 3) Evaluate semantic similarity metrics for comment quality

## Open Questions the Paper Calls Out
None

## Limitations
- Small sample sizes (MUMPS: 69 methods; ALC: 66 methods) may not capture full legacy system complexity
- Heavy reliance on three expert participants introduces potential subjectivity and limits statistical power
- Study focuses only on line-wise comment generation, not addressing broader documentation needs

## Confidence
- LLM-generated comments are hallucination-free: High confidence based on expert evaluation
- Comments are complete, readable, and useful: Medium confidence due to small sample size and limited expert pool
- ALC is more challenging than MUMPS for LLMs: Medium confidence, though the study doesn't explore underlying reasons
- No automated metrics correlate with comment quality: High confidence for the specific metrics tested, but limited scope

## Next Checks
1. Expand evaluation to include additional legacy languages (COBOL, PL/I) and larger codebases to test generalizability
2. Implement multi-expert consensus protocols with 5+ domain experts to reduce individual bias in quality assessments
3. Test alternative automated evaluation metrics including semantic similarity measures and task-based completion tests to identify potential correlations with human judgment