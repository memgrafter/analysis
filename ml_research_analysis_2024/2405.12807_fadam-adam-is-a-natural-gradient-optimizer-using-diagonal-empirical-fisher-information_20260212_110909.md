---
ver: rpa2
title: 'FAdam: Adam is a natural gradient optimizer using diagonal empirical Fisher
  information'
arxiv_id: '2405.12807'
source_url: https://arxiv.org/abs/2405.12807
tags:
- adam
- gradient
- loss
- natural
- empirical
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper establishes a mathematical foundation for the Adam optimizer
  by connecting it to natural gradient descent through Riemannian and information
  geometry. The analysis reveals that Adam approximates natural gradient using diagonal
  empirical Fisher information, which explains the use of square root in the update
  rule.
---

# FAdam: Adam is a natural gradient optimizer using diagonal empirical Fisher information

## Quick Facts
- arXiv ID: 2405.12807
- Source URL: https://arxiv.org/abs/2405.12807
- Authors: Dongseong Hwang
- Reference count: 40
- Primary result: Establishes Adam as natural gradient optimizer; proposes FAdam with improved performance across multiple domains

## Executive Summary
This paper reveals that Adam implicitly performs natural gradient descent by approximating diagonal empirical Fisher information, explaining its empirical success through information geometry. The authors identify critical flaws in Adam's implementation including improper momentum calculation, bias correction, epsilon handling, and weight decay application. Based on these insights, they propose Fisher Adam (FAdam) which incorporates theoretically sound modifications such as invariant momentum averaging, adaptive epsilon, gradient clipping, and refined weight decay. FAdam demonstrates superior performance across diverse domains including large language models, automatic speech recognition, and image generation, achieving state-of-the-art results in ASR with a 600M parameter Conformer model.

## Method Summary
The paper establishes that Adam approximates natural gradient descent using diagonal empirical Fisher information matrix, connecting optimization theory with information geometry. FAdam builds on this foundation by fixing implementation flaws: momentum now averages invariant natural gradients rather than raw gradients, bias correction is adjusted to match natural gradient theory, epsilon is made adaptive to prevent division by zero, gradient clipping is incorporated for stability, and weight decay is reformulated based on natural gradient principles. The method replaces standard Adam with these modifications while maintaining existing hyperparameters, validated across LLM training, ASR, and image generation tasks.

## Key Results
- FAdam achieves SOTA results on LibriSpeech ASR with 600M parameter Conformer model
- The square root in Adam's update rule is explained as creating invariant natural gradients in Euclidean space
- FAdam demonstrates superior performance across LLM training, ASR, and VQ-VAE tasks
- The paper identifies that Adam excels with discrete distributions but struggles with continuous ones

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Adam implicitly uses natural gradient descent by approximating the diagonal empirical Fisher information matrix.
- **Mechanism**: The square root in Adam's update rule comes from constructing an invariant gradient in Euclidean space that matches the length of the natural gradient vector, which varies across the statistical manifold.
- **Core assumption**: The diagonal Fisher information matrix provides sufficient curvature information for effective optimization.
- **Evidence anchors**:
  - [abstract]: "Adam approximates natural gradient using diagonal empirical Fisher information"
  - [section 3.4.1]: "We refer to it as the invariant natural gradient... This is the reason why Adam uses the square root"
  - [corpus]: Weak evidence - only one neighbor paper mentions Fisher diagonal, but doesn't explain the square root mechanism
- **Break condition**: If off-diagonal Fisher information terms become significant (as in models with insufficient layer sizes), the diagonal approximation may fail to capture important curvature information.

### Mechanism 2
- **Claim**: Using log-probability loss functions (cross-entropy) is essential for Adam's effectiveness with empirical Fisher information.
- **Mechanism**: Discrete probability distributions concentrate probability mass on few logits, enabling accurate empirical Fisher estimation from samples, while continuous distributions require integration over density functions.
- **Core assumption**: Empirical Fisher estimation works reliably only with discrete distributions.
- **Evidence anchors**:
  - [section 3.3.1]: "Adam excels when dealing with discrete distributions... it may encounter difficulties when handling continuous distributions"
  - [abstract]: "advocating for the use of log probability functions as loss, which should be based on discrete distributions"
  - [corpus]: Weak evidence - neighbor papers mention Fisher information but don't discuss the discrete vs continuous distinction
- **Break condition**: If training involves continuous outputs (like regression tasks) without proper discretization, Adam's empirical Fisher estimation becomes unreliable.

### Mechanism 3
- **Claim**: FAdam's modifications (adaptive epsilon, gradient clipping, invariant momentum) stabilize optimization by addressing empirical Fisher estimation noise.
- **Mechanism**: The adaptive epsilon prevents division by zero in FIM, gradient clipping prevents explosive updates from noisy FIM estimates, and invariant momentum averages natural gradients rather than raw gradients.
- **Core assumption**: Empirical Fisher estimation from minibatches is inherently noisy and requires stabilization techniques.
- **Evidence anchors**:
  - [section 3.4.3]: "To prevent division by zero before clipping, ϵ is added"
  - [section 3.4.2]: "momentum should average the invariant natural gradient rather than the raw gradient"
  - [section B.3]: "gradient clipping is essential for Adam... is now used as a standard practice"
- **Break condition**: If FIM estimation becomes sufficiently accurate (through larger batch sizes or better approximation methods), some stabilization mechanisms may become unnecessary.

## Foundational Learning

- **Riemannian Geometry and Information Geometry**
  - Why needed here: The paper builds its mathematical foundation on differential geometry concepts to explain Adam's behavior through the lens of natural gradient descent on statistical manifolds.
  - Quick check question: Can you explain why the Fisher information matrix serves as a Riemannian metric for the statistical manifold?

- **Fisher Information Matrix Properties**
  - Why needed here: Understanding FIM properties (positive semi-definiteness, relationship to Hessian, expected value interpretation) is crucial for grasping why natural gradient descent works and how Adam approximates it.
  - Quick check question: What is the relationship between the Fisher information matrix and the Hessian of the log-likelihood function?

- **Empirical Risk Minimization vs True Risk**
  - Why needed here: The paper distinguishes between using true data distribution and empirical distribution for Fisher information estimation, which is central to understanding Adam's approximation approach.
  - Quick check question: Why does using empirical distribution instead of true distribution for FIM estimation potentially lead to issues with continuous distributions?

## Architecture Onboarding

- **Component map**:
  State: momentum vector (m), Fisher information vector (f), time step (t)
  Hyperparameters: β1, β2, epsilon, weight decay λ, learning rate η
  Core operations: gradient computation, FIM estimation via EMA, invariant gradient construction, momentum update, weight decay application

- **Critical path**:
  1. Compute gradient g = ∇θ log P(x|θ)
  2. Update FIM estimate: f = β2·f + (1-β2)·g²
  3. Construct invariant gradient: ¯g = g/√(f + ε²)
  4. Apply gradient clipping: ¯g = ¯g/max(1, RMS(¯g)/c)
  5. Update momentum: m = β1·m + (1-β1)·¯g
  6. Apply weight decay: θ = θ - η(m + λ·θ/√(f + ε²))
  7. Return updated parameters

- **Design tradeoffs**:
  - Diagonal vs full FIM: Diagonal approximation reduces memory complexity from O(N²) to O(N) but loses covariance information
  - Empirical vs true FIM: Empirical estimation enables practical implementation but introduces noise, especially for continuous distributions
  - Square root vs reciprocal: Square root creates invariant gradients but deviates from pure natural gradient; reciprocal would be more theoretically sound but less stable

- **Failure signatures**:
  - Training instability or divergence: Likely due to epsilon being too small or FIM estimate becoming zero
  - Poor convergence with continuous outputs: Likely due to inadequate FIM estimation from continuous distributions
  - Suboptimal performance compared to SGD: Likely due to insufficient layer sizes making diagonal FIM approximation inadequate

- **First 3 experiments**:
  1. Replace Adam with FAdam in a pre-existing LLM training pipeline and compare loss curves
  2. Test FAdam on a simple regression task with continuous outputs to verify the discrete distribution hypothesis
  3. Vary the FIM exponent (ρ) from 0.3 to 1.0 on a small model to confirm the optimal value of 0.5

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal FIM exponent value for different model architectures and domains beyond the tested range of 0.3 to 1.0?
- Basis in paper: [explicit] Section 3.4.1 discusses the square root (0.5) as optimal through ablation studies
- Why unresolved: The ablation study only tested a limited range of exponents, and the authors note that exponents below 0.3 show sharp decline while those above 0.5 exhibit relative stability
- What evidence would resolve it: Systematic experiments testing FIM exponents across diverse architectures (CNNs, Transformers, RNNs) and domains (NLP, vision, speech) to identify optimal values and potential patterns

### Open Question 2
- Question: How would incorporating off-diagonal elements of the Fisher Information Matrix affect Adam's performance compared to the current diagonal approximation?
- Basis in paper: [explicit] Section 3.2 mentions that Adam variants have explored low-rank approximations and Kronecker-factored approximations but applying off-diagonal FIM to Adam is left for future study
- Why unresolved: The paper acknowledges that Adam's success suggests loss of covariance information might not be detrimental, but the potential benefits of incorporating off-diagonal elements remain unexplored
- What evidence would resolve it: Comparative experiments between Adam/FAdam and variants using off-diagonal FIM approximations across multiple domains, measuring both performance and computational efficiency

### Open Question 3
- Question: Can the empirical Fisher approximation be improved for continuous distributions without requiring discrete representations?
- Basis in paper: [explicit] Section 3.3.1 highlights that Adam struggles with continuous distributions and proposes using categorical cross-entropy instead of L2 loss
- Why unresolved: The paper demonstrates that Adam works well with discrete distributions but encounters difficulties with continuous ones, suggesting fundamental limitations in how empirical FIM is estimated
- What evidence would resolve it: Development and testing of alternative methods for estimating FIM in continuous domains that don't rely on discretization, comparing performance to the discrete approach proposed in the paper

## Limitations
- Empirical evidence primarily confined to discrete distribution tasks with limited validation on continuous distribution problems
- Diagonal Fisher approximation may not capture important off-diagonal curvature information in larger models
- Performance improvements demonstrated on specific tasks but not systematically validated across diverse scenarios

## Confidence
- **High Confidence**: The mathematical derivation connecting Adam's update rule to natural gradient descent through invariant gradient construction
- **Medium Confidence**: The discrete vs continuous distribution hypothesis for Adam's effectiveness - supported by theoretical arguments but limited empirical validation
- **Medium Confidence**: The proposed FAdam modifications improving optimization stability - demonstrated on specific tasks but not systematically validated across diverse scenarios

## Next Checks
1. Test FAdam on continuous regression tasks (e.g., physics simulations, time series prediction) to validate the discrete distribution hypothesis and assess whether the adaptive epsilon and gradient clipping modifications sufficiently address continuous distribution challenges
2. Systematically vary model depth and width to identify when diagonal Fisher approximation breaks down, comparing FAdam performance against full Fisher methods or SGD baselines
3. Conduct ablation studies removing individual FAdam modifications (adaptive epsilon, gradient clipping, invariant momentum) to quantify their independent contributions to performance improvements