---
ver: rpa2
title: Buffer-based Gradient Projection for Continual Federated Learning
arxiv_id: '2409.01585'
source_url: https://arxiv.org/abs/2409.01585
tags:
- learning
- buffer
- task
- tasks
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces Fed-A-GEM, a federated adaptation of A-GEM,
  to mitigate catastrophic forgetting in continual federated learning. Fed-A-GEM leverages
  global buffer gradients aggregated from clients and applies local gradient projection
  to align updates with previously learned knowledge.
---

# Buffer-based Gradient Projection for Continual Federated Learning

## Quick Facts
- arXiv ID: 2409.01585
- Source URL: https://arxiv.org/abs/2409.01585
- Reference count: 40
- Primary result: Fed-A-GEM mitigates catastrophic forgetting in continual federated learning using global buffer gradients and local gradient projection, improving accuracy by up to 27% on sequential-CIFAR100 without increasing communication overhead.

## Executive Summary
Fed-A-GEM is a federated adaptation of A-GEM designed to address catastrophic forgetting in continual federated learning. It leverages global buffer gradients aggregated from clients and applies local gradient projection to align updates with previously learned knowledge. The method tackles challenges posed by limited buffer sizes and non-IID data distributions across clients, operating without requiring task boundaries. Extensive experiments on benchmark datasets (rotated-MNIST, permuted-MNIST, sequential-CIFAR10/100, and sequential-YahooQA) demonstrate consistent accuracy improvements and reduced forgetting, especially when combined with existing CFL techniques.

## Method Summary
Fed-A-GEM integrates gradient projection with federated learning by maintaining global buffers that aggregate representative gradients from clients. During each update, local client gradients are projected onto the space that minimally interferes with these global buffer gradients, preserving previously acquired knowledge. The method is designed to function effectively under non-IID data distributions and without explicit task boundaries, making it suitable for realistic, asynchronous continual learning scenarios. Fed-A-GEM is compatible with various base federated learning algorithms and does not increase communication overhead.

## Key Results
- Fed-A-GEM improves accuracy by up to 27% on sequential-CIFAR100 in task-incremental learning.
- The method consistently reduces forgetting across benchmark datasets, including rotated-MNIST, permuted-MNIST, and sequential-YahooQA.
- Fed-A-GEM performs well in asynchronous task boundaries and varying numbers of tasks and clients without increasing communication overhead.

## Why This Works (Mechanism)
Fed-A-GEM mitigates catastrophic forgetting by using global buffer gradients to guide local updates. The gradient projection ensures that new learning does not overwrite previously acquired knowledge, as represented by the buffer. This approach is robust to non-IID data and does not require explicit task boundaries, allowing it to function in more realistic, asynchronous continual learning settings.

## Foundational Learning
- **Gradient Projection**: Projects gradients to minimize interference with past knowledge, needed to prevent catastrophic forgetting in sequential learning; quick check: verify projection step aligns with buffer gradients.
- **Buffer Management**: Aggregates representative gradients across clients, needed to maintain global knowledge under non-IID data; quick check: ensure buffer updates are representative and efficient.
- **Federated Learning**: Coordinates updates across distributed clients, needed to scale to many devices; quick check: confirm compatibility with base FL algorithms and no added communication overhead.

## Architecture Onboarding
- **Component Map**: Clients -> Local Buffer Aggregation -> Global Buffer -> Gradient Projection -> Model Update
- **Critical Path**: Local gradient computation → buffer gradient aggregation → projection step → federated update
- **Design Tradeoffs**: Larger buffers improve performance but increase storage; asynchronous operation increases robustness but may slow convergence
- **Failure Signatures**: Degraded accuracy with highly non-IID data; increased forgetting if buffer synchronization fails
- **First Experiments**:
  1. Run Fed-A-GEM on rotated-MNIST with varying buffer sizes to assess storage-performance tradeoff.
  2. Evaluate performance under asynchronous task boundaries on permuted-MNIST.
  3. Test scalability by increasing the number of clients on sequential-CIFAR10.

## Open Questions the Paper Calls Out
None

## Limitations
- Performance on real-world, non-stationary, and multimodal data streams is uncertain due to focus on controlled image classification benchmarks.
- Buffer synchronization and management in heterogeneous client environments are not addressed.
- Scalability to thousands of clients and robustness to client dropouts or adversarial behavior are not evaluated.

## Confidence
- **High Confidence**: The core gradient projection mechanism and integration with federated learning are well-justified; improvements on standard benchmarks are substantial and consistent.
- **Medium Confidence**: Effectiveness in asynchronous task boundary scenarios is supported, but generalization to highly dynamic and non-IID environments is uncertain; communication overhead claims are plausible but unverified under varying conditions.
- **Low Confidence**: Scalability to thousands of clients and robustness to client dropouts or adversarial behavior are speculative.

## Next Checks
1. Evaluate Fed-A-GEM on real-world non-IID data streams (e.g., sensor or text data) to assess robustness beyond controlled benchmarks.
2. Assess scalability and communication efficiency by conducting experiments with hundreds of clients and measuring actual overhead under varying network conditions.
3. Analyze buffer synchronization in heterogeneous environments by simulating scenarios with diverse client capabilities and evaluating impact on performance.