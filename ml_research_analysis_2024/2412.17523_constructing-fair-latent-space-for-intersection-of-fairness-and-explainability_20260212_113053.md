---
ver: rpa2
title: Constructing Fair Latent Space for Intersection of Fairness and Explainability
arxiv_id: '2412.17523'
source_url: https://arxiv.org/abs/2412.17523
tags:
- latent
- information
- space
- fairness
- fair
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a method to construct a fair latent space for
  generative models by disentangling and redistributing information associated with
  labels and sensitive attributes. The approach trains an invertible neural network
  (INN) on a pre-trained generative model's latent space, transforming it into a fair
  latent space without retraining the entire model.
---

# Constructing Fair Latent Space for Intersection of Fairness and Explainability

## Quick Facts
- arXiv ID: 2412.17523
- Source URL: https://arxiv.org/abs/2412.17523
- Reference count: 30
- Primary result: 86.8% reduction in Equalized Odds, 52.0% reduction in Demographic Parity, and 39.9% increase in Worst-Group-Accuracy compared to baseline methods

## Executive Summary
This paper proposes a method to construct a fair latent space for generative models by disentangling and redistributing information associated with labels and sensitive attributes. The approach trains an invertible neural network (INN) on a pre-trained generative model's latent space, transforming it into a fair latent space without retraining the entire model. Theoretical analysis using information bottleneck principles and mutual information maximization guides the design of loss functions that diagonalize and equalize the covariance matrix while minimizing distances between opposite sensitive attribute groups. Experiments on CelebA, CelebAHQ, and UTK Face datasets demonstrate significant improvements in fairness metrics while maintaining the ability to generate counterfactual explanations that reveal decision factors and verify fairness.

## Method Summary
The method trains an invertible neural network (Glow) on a pre-trained generative model's latent space to create a fair representation. The INN transforms the original latent space into a new space where label-related information and sensitive attribute information are disentangled into separate dimensions. Four loss functions guide this transformation: Ldg diagonalizes the covariance matrix, Leq equalizes variance across dimensions, Ldi maximizes mutual information between opposite sensitive attribute groups through contrastive learning, and Lg Gaussianizes the distributions. This approach avoids retraining the entire generative model while achieving significant fairness improvements through theoretical information bottleneck principles.

## Key Results
- 86.8% reduction in Equalized Odds compared to baseline methods
- 52.0% reduction in Demographic Parity while maintaining classification accuracy
- 39.9% increase in Worst-Group-Accuracy on CelebA dataset
- Near-elimination of gender bias in generated counterfactual images as verified by CLIP gender classifier
- Maintained attribute enhancement capability while achieving fairness improvements

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Information bottleneck principle enables disentangling sensitive attributes from labels by maximizing label-relevant information in latent dimensions.
- Mechanism: The INN transforms the generative model's latent space using a loss function derived from mutual information maximization. This forces label-related information into specific dimensions while separating sensitive attribute information into others.
- Core assumption: The latent representation Z_Y follows a Gaussian distribution, allowing entropy to be expressed via the covariance matrix determinant.
- Evidence anchors:
  - [abstract] "The fair latent space is constructed by disentangling and redistributing labels and sensitive attributes"
  - [section 3.1] "Our objective is to maximize the information associated with each assigned attribute within its respective dimension"
  - [corpus] Weak - related work focuses on fairness/explainability but doesn't discuss information bottleneck for disentanglement
- Break condition: If the Gaussian assumption fails or mutual information cannot be effectively maximized, the disentanglement breaks down and fairness improvements disappear.

### Mechanism 2
- Claim: Diagonalizing and equalizing the covariance matrix creates maximally informative, non-redundant dimensions for labels and sensitive attributes.
- Mechanism: The Ldg loss subtracts off-diagonal elements of the covariance matrix while Leq constrains variance across dimensions, transforming the covariance into an identical diagonal matrix.
- Core assumption: Diagonalizing the covariance matrix maximizes the determinant, which corresponds to maximizing information content under the Gaussian assumption.
- Evidence anchors:
  - [section 3.2] "To diagonalize the obtained covariance matrix, we subtract the term that is element-wise multiplied by the identity matrix"
  - [section 3.1] "equality holds when all values are equal as λi(CZY ) = λj(CZY ) for ∀j ∈ [1, · · · , d]"
  - [corpus] Weak - contrastive learning papers discuss decorrelation but not specifically for fairness/explainability intersection
- Break condition: If the diagonalization process fails to maintain distinct dimensions for labels vs sensitive attributes, information becomes entangled again.

### Mechanism 3
- Claim: Minimizing L2 distance between opposite sensitive attribute groups while maximizing distance between different labels enhances fairness without losing discriminative power.
- Mechanism: The Ldi loss uses contrastive learning principles to pull together representations with same labels but different sensitive attributes while pushing apart representations with different labels.
- Core assumption: Maximizing mutual information between opposite sensitive attribute groups can be approximated by minimizing their L2 distance.
- Evidence anchors:
  - [section 3.1] "Our strategy necessitates numerically increasing g(xy_s0,i, xy_s1,i) while concurrently reducing g(xy_s0,i, xy_s1,j)"
  - [section 3.2] "L2(xy_s0, xy_s1 ), which is a feasible strategy for maximizing I(X_y_s0, X_y_s1)"
  - [corpus] Weak - FSCL paper discusses group similarity but not specifically for counterfactual explanations
- Break condition: If the contrastive loss overwhelms the information bottleneck loss, the model may collapse to trivial solutions or lose label discrimination.

## Foundational Learning

- Concept: Information bottleneck theory
  - Why needed here: Provides theoretical foundation for how to extract label-relevant information while discarding sensitive attribute information
  - Quick check question: How does the information bottleneck principle relate to maximizing mutual information between labels and latent representations?

- Concept: Gaussian distribution properties
  - Why needed here: Enables expressing entropy via covariance matrix determinant, which is crucial for the diagonalization objective
  - Quick check question: Why does assuming Gaussian distribution allow us to use the determinant of the covariance matrix as a proxy for information content?

- Concept: Contrastive learning loss functions
  - Why needed here: Provides the mechanism for maximizing mutual information between opposite sensitive attribute groups
  - Quick check question: How does the contrastive loss in Ldi differ from standard supervised contrastive learning objectives?

## Architecture Onboarding

- Component map:
  Pre-trained generative model (DiffAE) → Encoder produces E
  → Invertible Neural Network (Glow) → Transforms E to Z
  → Fair representation Z = [Z_Y, Z_S] with separate dimensions
  → Classifiers on Z_Y and Z_S for labels and sensitive attributes
  → Loss functions: Ldg (diagonalize), Leq (equalize), Ldi (contrastive), Lg (Gaussianizing)

- Critical path:
  1. Input image → Pre-trained encoder → Latent representation E
  2. E → INN → Fair latent representation Z
  3. Z → Separate classifiers for labels and sensitive attributes
  4. Compute losses and backpropagate through INN only

- Design tradeoffs:
  - Training only the INN vs retraining entire generative model (computation vs flexibility)
  - Gaussian assumption vs more complex distributions (simplicity vs accuracy)
  - Separate vs entangled representations (fairness vs potential information loss)

- Failure signatures:
  - Loss collapse: All dimensions become identical, losing discriminative power
  - Mode collapse: Generated counterfactuals become unrealistic or repetitive
  - Fairness metrics plateau: Additional training doesn't improve EO/DP scores

- First 3 experiments:
  1. Ablation study: Train with only Ldg vs only Leq vs both to verify diagonalization importance
  2. Sensitivity analysis: Vary λdg, λeq, λdi hyperparameters to find optimal balance
  3. Counterfactual validation: Generate counterfactuals and verify gender bias elimination using CLIP gender classifier

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the proposed fair latent space construction method generalize to other generative model architectures beyond Diffusion Autoencoders?
- Basis in paper: [explicit] The paper states "our model does not involve training the entire model but rather acts as a module added to a pre-trained generative model" and mentions training on a pretrained generative model, but only demonstrates results with DiffAE.
- Why unresolved: The experiments only validate the approach on DiffAE, leaving uncertainty about performance with other architectures like VAEs, GANs, or other diffusion models.
- What evidence would resolve it: Systematic experiments comparing the method across multiple generative model architectures (VAEs, GANs, different diffusion models) with consistent evaluation metrics.

### Open Question 2
- Question: How does the proposed method perform when dealing with datasets containing more than two sensitive attributes or complex intersectional biases?
- Basis in paper: [explicit] The paper mentions experiments on CelebA with "two sensitive attributes" but doesn't extensively explore scenarios with more attributes or complex intersectional scenarios.
- Why unresolved: The theoretical framework and experiments focus primarily on binary sensitive attributes, leaving questions about scalability to multi-attribute scenarios.
- What evidence would resolve it: Experiments on datasets with multiple sensitive attributes (race, gender, age simultaneously) and analysis of intersectional fairness metrics across these combinations.

### Open Question 3
- Question: What is the impact of the fair latent space construction on downstream tasks beyond facial attribute classification?
- Basis in paper: [inferred] The paper focuses on facial attribute classification tasks and mentions the module "acts as a module added to a pre-trained generative model" but doesn't explore applications to other downstream tasks.
- Why unresolved: The evaluation is limited to classification tasks, and it's unclear how the fair latent space affects tasks like image generation quality, reconstruction fidelity, or other vision tasks.
- What evidence would resolve it: Experiments evaluating the fair latent space on various downstream tasks including image generation quality metrics, reconstruction error, and performance on non-classification tasks like segmentation or object detection.

## Limitations
- Gaussian assumption for latent representations may not hold for real-world data distributions
- Scalability challenges with higher-dimensional data or more than two sensitive attribute groups
- Binary sensitive attribute assumption limits applicability to complex demographic variables

## Confidence

- **High Confidence**: The mechanism for diagonalizing the covariance matrix and the experimental results showing significant improvements in fairness metrics (EO reduction of 86.8%, DP reduction of 52.0%, WGA increase of 39.9%) are well-supported by the mathematical framework and empirical evidence.

- **Medium Confidence**: The information bottleneck-based disentanglement mechanism works as described under the Gaussian assumption, but its effectiveness may vary with non-Gaussian distributions or more complex sensitive attribute scenarios.

- **Low Confidence**: The scalability of the contrastive learning approach to multi-class sensitive attributes and the method's generalization to domains beyond face images remain unproven.

## Next Checks

1. **Distribution Validation**: Test the method's performance when the latent space follows non-Gaussian distributions (e.g., Laplacian or mixture models) to assess robustness of the diagonalization approach.

2. **Multi-class Extension**: Implement and evaluate the framework with multi-class sensitive attributes (e.g., race/ethnicity categories beyond binary classification) to test scalability.

3. **Domain Transfer**: Apply the method to non-image domains (e.g., text or tabular data) to validate generalizability beyond generative image models.