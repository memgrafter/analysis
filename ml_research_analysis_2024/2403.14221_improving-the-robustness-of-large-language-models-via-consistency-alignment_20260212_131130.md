---
ver: rpa2
title: Improving the Robustness of Large Language Models via Consistency Alignment
arxiv_id: '2403.14221'
source_url: https://arxiv.org/abs/2403.14221
tags:
- training
- arxiv
- instructions
- llms
- consistency
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the robustness issue in large language models
  (LLMs) caused by inconsistent responses to semantically equivalent but differently
  verbalized instructions. The authors propose a two-stage training framework consisting
  of instruction-augmented supervised fine-tuning and consistency alignment training.
---

# Improving the Robustness of Large Language Models via Consistency Alignment

## Quick Facts
- **arXiv ID**: 2403.14221
- **Source URL**: https://arxiv.org/abs/2403.14221
- **Reference count**: 0
- **Primary result**: A two-stage training framework (SFT IA + CAT) improves LLM robustness by addressing inconsistent responses to semantically equivalent but differently verbalized instructions, with Vicuna-13B surpassing GPT-4 in consistency metrics.

## Executive Summary
This paper addresses a critical robustness issue in large language models where semantically equivalent but differently verbalized instructions can produce inconsistent responses. The authors propose a two-stage training framework that first augments instructions with paraphrased versions to improve generalization, then uses self-rewards to train the model to differentiate between similar responses based on their alignment with human expectations. The approach achieves significant improvements in consistency metrics and ROUGE scores without requiring external human preference resources. Experiments demonstrate that Vicuna-13B with the proposed training method surpasses GPT-4 in consistency metrics, with human evaluation confirming the superiority of the approach in generating more aligned responses.

## Method Summary
The method consists of a two-stage training framework: (1) Instruction-augmented supervised fine-tuning (SFT IA) that improves generalization by exposing the model to semantically equivalent but syntactically diverse instructions during fine-tuning, and (2) Consistency alignment training (CAT) that uses self-rewards inferred from the model itself to enable preference learning without external human preference resources. In the first stage, the original instruction set is augmented with paraphrased versions using LLMs like Vicuna or ChatGPT. In the second stage, the model generates multiple responses for each paraphrased instruction, self-rewards assess answer type and correctness, and ranking loss is used to optimize the model based on these preferences. The training process is accomplished without external human preference resources.

## Key Results
- Vicuna-13B + SFT(IA) + CAT surpasses GPT-4 in consistency metrics
- Significant improvements in consistency metrics (CR, MCR) and ROUGE scores compared to baseline methods
- Human evaluation confirms the superiority of the proposed approach in generating more aligned responses
- The training process is accomplished without external human preference resources

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Instruction augmentation with paraphrased instructions improves model generalization and robustness.
- Mechanism: By exposing the model to semantically equivalent but syntactically diverse instructions during fine-tuning, the model learns to map different linguistic expressions to the same underlying task, reducing inconsistency.
- Core assumption: Paraphrased instructions are semantically equivalent and maintain task intent.
- Evidence anchors:
  - [abstract]: "augment the original instruction set with paraphrased versions to improve generalization"
  - [section 4.1]: "Similar instructions are the instructions that convey the same task but are verbalized differently."
  - [corpus]: Weak - no corpus evidence directly supports semantic equivalence of paraphrases.
- Break condition: If paraphrased instructions introduce semantic drift or alter task intent, the model may learn incorrect mappings.

### Mechanism 2
- Claim: Self-rewards inferred from the model itself enable preference learning without external human preference resources.
- Mechanism: The model generates multiple responses for each paraphrased instruction, and self-rewards assess answer type and correctness. Response pairs are constructed with higher-reward responses as "good" and lower-reward as "bad," enabling preference learning through ranking loss.
- Core assumption: The model can accurately assess its own responses for answer type and correctness.
- Evidence anchors:
  - [abstract]: "The training process is accomplished by self-rewards inferred from the trained model at the first stage without referring to external human preference resources."
  - [section 4.2]: "We prompt the trained model to give a reward r_i for each generated response yi."
  - [corpus]: Weak - no corpus evidence supports model's ability to self-assess.
- Break condition: If the model's self-assessment is inaccurate, the preference learning process will reinforce incorrect response patterns.

### Mechanism 3
- Claim: Combining instruction-augmented supervised fine-tuning with consistency alignment training produces superior robustness improvements.
- Mechanism: SFT (IA) improves generalization on following instructions, while CAT improves the model's ability to differentiate subtle differences in similar responses and align with human expectations. The combination addresses both instruction understanding and response consistency.
- Core assumption: The two training stages are complementary and address different aspects of the robustness problem.
- Evidence anchors:
  - [abstract]: "two-stage training framework consisting of instruction-augmented supervised fine-tuning and consistency alignment training"
  - [section 4]: "our training framework sequentially performs the following two training stages"
  - [section 5.2]: "Vicuna-13B + SFT (IA) + CAT surpasses the SOTA LLM GPT-4 in our setting"
- Break condition: If the two training stages interfere with each other or if one stage dominates the other, the combined approach may not yield additional benefits.

## Foundational Learning

- Concept: Semantic equivalence and paraphrase detection
  - Why needed here: The method relies on generating and using paraphrased instructions that convey the same meaning as the original.
  - Quick check question: Can you explain how semantic equivalence is maintained when paraphrasing instructions?

- Concept: Self-assessment and reward modeling
  - Why needed here: The consistency alignment training uses the model itself to assess the quality of its own responses.
  - Quick check question: What are the potential challenges of using a model to assess its own responses?

- Concept: Preference learning and ranking loss
  - Why needed here: The consistency alignment training uses ranking loss to optimize the model based on response preferences.
  - Quick check question: How does ranking loss differ from traditional cross-entropy loss in terms of what it optimizes?

## Architecture Onboarding

- Component map: Paraphrase generation -> SFT IA fine-tuning -> CAT fine-tuning -> Evaluation
- Critical path: The critical path is: generate paraphrases → SFT IA fine-tuning → CAT fine-tuning → evaluation. Each stage depends on the output of the previous stage.
- Design tradeoffs: The main tradeoff is between instruction diversity (more paraphrases) and training efficiency (more data to process). Another tradeoff is between self-assessment accuracy and the need for external human preference data.
- Failure signatures: If the model shows increased inconsistency after training, it may indicate issues with paraphrase quality or self-assessment accuracy. If the model's performance on original instructions degrades, it may indicate overfitting to paraphrased instructions.
- First 3 experiments:
  1. Train the model with SFT IA only and evaluate consistency metrics to verify the impact of instruction augmentation.
  2. Train the model with CAT only (using original instructions) to verify the impact of consistency alignment training.
  3. Train the model with SFT IA + CAT and compare consistency metrics to the baseline to verify the combined approach's effectiveness.

## Open Questions the Paper Calls Out

- **Question**: How does the proposed consistency alignment training (CAT) perform when applied to smaller or weaker large language models?
  - **Basis in paper**: [explicit] The paper acknowledges the limitation that the CAT relies on a model's self-rewards and plans to conduct experiments on smaller and weaker models in the future.
  - **Why unresolved**: The paper does not provide empirical results or a detailed analysis of how CAT performs on smaller models.
  - **What evidence would resolve it**: Conducting experiments on smaller models and comparing their performance with larger models would provide insights into the scalability and effectiveness of CAT.

- **Question**: What is the impact of the diversity of verbalized instructions on the effectiveness of the proposed training framework?
  - **Basis in paper**: [explicit] The paper mentions that the diversity of verbalized instructions may be limited compared to end-users and plans to collect instructions from a wide range of end-users in the future.
  - **Why unresolved**: The paper does not provide empirical evidence or analysis on how the diversity of instructions affects the performance of the proposed training framework.
  - **What evidence would resolve it**: Conducting experiments with varying levels of instruction diversity and comparing the performance of the proposed framework would provide insights into the importance of instruction diversity.

- **Question**: How does the proposed training framework perform on tasks outside the scope of the Super Natural Instructions dataset?
  - **Basis in paper**: [explicit] The paper uses the Super Natural Instructions dataset for experiments, but does not provide evidence or analysis on the performance of the proposed framework on other datasets or tasks.
  - **Why unresolved**: The paper does not explore the generalization ability of the proposed framework to other datasets or tasks.
  - **What evidence would resolve it**: Conducting experiments on other datasets or tasks and comparing the performance of the proposed framework with other methods would provide insights into its generalization ability.

## Limitations

- The self-reward mechanism's reliability remains uncertain without comparison to human-annotated preference data
- Semantic equivalence of paraphrased instructions is assumed rather than rigorously validated
- The paper does not explore how the proposed framework performs on tasks outside the Super Natural Instructions dataset
- The diversity of verbalized instructions may be limited compared to instructions from a wide range of end-users

## Confidence

- **High Confidence**: The overall training methodology and experimental setup are clearly described and reproducible. The improvements in consistency metrics and ROUGE scores compared to baseline methods are empirically demonstrated.
- **Medium Confidence**: The claim that Vicuna-13B + SFT(IA) + CAT surpasses GPT-4 in consistency metrics is supported by the reported results, but the evaluation methodology for consistency metrics is not fully transparent.
- **Low Confidence**: The assertion that self-rewards inferred from the trained model can effectively replace external human preference resources is not adequately validated. The paper does not compare the model's self-assessment accuracy against human judgments.

## Next Checks

1. Conduct human evaluation of the model's self-rewards by having human annotators assess the same response pairs used for training, to measure alignment between self-assessment and human preferences.
2. Perform ablation studies on the paraphrasing process by systematically varying the quality and diversity of paraphrased instructions to quantify their impact on final model performance.
3. Test the model's robustness on out-of-distribution instructions that were not seen during training to evaluate whether the consistency improvements generalize beyond the specific instruction set used.