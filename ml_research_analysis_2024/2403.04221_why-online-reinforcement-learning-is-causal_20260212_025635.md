---
ver: rpa2
title: Why Online Reinforcement Learning is Causal
arxiv_id: '2403.04221'
source_url: https://arxiv.org/abs/2403.04221
tags:
- causal
- policy
- learning
- state
- variables
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper argues that online reinforcement learning is inherently
  causal: conditional probabilities estimated from online data are also causal probabilities.
  The key insight is that when an agent learns from its own experience, there are
  no unobserved confounders influencing both the agent''s actions and the rewards
  it receives.'
---

# Why Online Reinforcement Learning is Causal

## Quick Facts
- arXiv ID: 2403.04221
- Source URL: https://arxiv.org/abs/2403.04221
- Authors: Oliver Schulte; Pascal Poupart
- Reference count: 11
- Primary result: Online RL agents can estimate causal effects directly from conditional probabilities because they observe all causes of their actions (action sufficiency), with no unobserved confounders.

## Executive Summary
This paper argues that online reinforcement learning is inherently causal: conditional probabilities estimated from online data are also causal probabilities. The key insight is that when an agent learns from its own experience, there are no unobserved confounders influencing both the agent's actions and the rewards it receives. The authors formalize this argument using causal models and prove that under action sufficiency (all causes of the agent's actions are observed), causal effects and what-if counterfactuals can be estimated from conditional probabilities in online RL. For offline RL, where an agent may learn from the experience of others, the authors describe methods for leveraging causal models, including support for counterfactual queries.

## Method Summary
The authors leverage causal models and Pearl's do-calculus to prove that under action sufficiency, conditional probabilities equal interventional probabilities in online RL. They formalize this using structural causal models (SCMs) where action variables are exogenous and noise variables capture all causes of actions. The temporal ordering ensures that past states and actions don't confound future actions and rewards. For counterfactuals, they distinguish between what-if queries (which reduce to conditional probabilities under action sufficiency) and hindsight counterfactuals (which require full causal models even in online settings).

## Key Results
- Online RL satisfies action sufficiency, making conditional probabilities equivalent to causal effects
- What-if counterfactuals can be evaluated using conditional probabilities in online RL
- Hindsight counterfactuals require causal models beyond conditional probabilities, even in online RL
- Offline RL can estimate causal effects when observation-equivalence holds between behavioral and learned policies

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Online RL agents can estimate causal effects directly from conditional probabilities because they have access to all causes of their own actions.
- Mechanism: When an agent learns from its own experience, it observes all factors that influence its decisions (like health, location, etc.). This observation-equivalence means there are no unobserved confounders between the agent's actions and the rewards it receives, making conditional probabilities equivalent to causal effects.
- Core assumption: The agent's observation signal includes all causes of its actions (action sufficiency).
- Evidence anchors:
  - [abstract] "when an agent learns from their own experience, there are no unobserved confounders that influence both the agent's own exploratory actions and the rewards they receive"
  - [section] "Our main argument is that in online learning, conditional probabilities are causal"
- Break condition: If the agent's observation signal misses any causes of its actions (partial observability), confounders can exist and conditional probabilities become confounded.

### Mechanism 2
- Claim: Online RL can compute what-if counterfactuals using conditional probabilities, but cannot compute hindsight counterfactuals even in online settings.
- Mechanism: What-if queries ask "what if I had chosen action A1 instead of A0?" Since online RL satisfies action sufficiency, these reduce to conditional probabilities. Hindsight queries ask "given I received reward R1 after choosing A0, what if I had chosen A1 instead?" These require knowing the outcome to infer latent state information, which cannot be reduced to conditional probabilities.
- Core assumption: Action sufficiency holds for online RL (all causes of actions are observed).
- Evidence anchors:
  - [abstract] "what-if counterfactuals can be evaluated using conditional probabilities, whereas hindsight counterfactuals require a causal model beyond conditional probabilities, even in online RL"
  - [section] "under action sufficiency, what-if counterfactuals are equivalent to conditional probabilities"
- Break condition: If we observe outcomes that reveal latent state information, hindsight counterfactuals require the full causal model even in online RL.

### Mechanism 3
- Claim: Offline RL with observation-equivalence (like on-policy learning) can also estimate causal effects from conditional probabilities.
- Mechanism: When the behavioral policy and learned policy share the same observation signal, the learning agent has access to all causes of the behavioral agent's actions. This satisfies observation-equivalence, which implies action sufficiency, making conditional probabilities equivalent to causal effects.
- Core assumption: Observation-equivalence between behavioral and learned policies.
- Evidence anchors:
  - [section] "The gist of our analysis is that if the learning setting satisfies observation-equivalence, as it does in online RL, then causal effects and what-if counterfactuals can be estimated from conditional probabilities"
  - [section] "Another sufficient condition is complete observability, where the environment is completely observable for both the behavioral and the learning agent"
- Break condition: If observation-equivalence is violated (e.g., learning from an expert's data with different observation signal), confounders can exist.

## Foundational Learning

- Concept: Causal sufficiency vs. action sufficiency
  - Why needed here: The paper distinguishes between general causal sufficiency (no shared latent causes) and action sufficiency (all causes of agent's actions are observed). This distinction is crucial for understanding when conditional probabilities equal causal effects.
  - Quick check question: In the hockey example, why is player health a confounder in offline learning but not in online learning?

- Concept: Observation-equivalence
  - Why needed here: This concept explains when offline RL can still benefit from conditional probabilities. It's the condition where behavioral and learned policies share the same observation signal.
  - Quick check question: In the self-driving car example, why does the offline learner have a different observation signal than the expert demonstrator?

- Concept: What-if vs. hindsight counterfactuals
  - Why needed here: The paper shows these require different causal machinery. What-if can be computed from conditional probabilities under action sufficiency, while hindsight requires the full structural causal model.
  - Quick check question: Why does observing the actual outcome (as in hindsight queries) require more causal machinery than what-if queries?

## Architecture Onboarding

- Component map:
  - Observation signal processing -> Conditional probability estimator -> Belief state updater -> Value function evaluator -> Counterfactual query processor

- Critical path:
  1. Agent takes action in environment
  2. Environment returns state and reward
  3. Observation signal is processed to capture all action causes
  4. Conditional probabilities are updated
  5. Value functions are evaluated for policy improvement
  6. (Optional) Counterfactual queries are processed if needed

- Design tradeoffs:
  - Completeness vs. simplicity: Complete observation signals ensure action sufficiency but may be complex to process
  - Online vs. offline: Online learning simplifies causal inference but may be slower; offline can be faster but requires careful handling of observation signals
  - Conditional vs. causal models: Conditional models are simpler but cannot handle hindsight counterfactuals

- Failure signatures:
  - High variance in value estimates despite stable environment
  - Poor performance when transferring to similar but slightly different environments
  - Inability to correctly answer "what if I had done X instead" type questions
  - Failure to improve policy despite extensive training data

- First 3 experiments:
  1. Implement a simple gridworld where the agent's actions are influenced by both visible and hidden state variables. Compare policy performance when using conditional probabilities vs. when confounders are introduced.
  2. Create a POMDP version of the hockey example. Implement both online and offline learning versions, comparing their ability to correctly estimate causal effects of shooting actions.
  3. Implement what-if and hindsight counterfactual query processing. Show that what-if queries can be answered using conditional probabilities in online settings, while hindsight queries require the full structural causal model even in online settings.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Under what conditions can offline RL agents learn causal models that are as accurate as online RL agents?
- Basis in paper: [inferred] The paper argues that online RL agents have access to all causes of their actions (action sufficiency), making conditional probabilities equivalent to causal effects. Offline RL agents may face confounders, suggesting a need for accurate causal models.
- Why unresolved: The paper doesn't specify the exact conditions or assumptions required for offline agents to learn accurate causal models comparable to online agents.
- What evidence would resolve it: Empirical studies comparing the performance of offline RL agents using causal models versus online RL agents, with varying degrees of observability and confounding.

### Open Question 2
- Question: How can causal models be leveraged to improve data augmentation in offline RL?
- Basis in paper: [explicit] The paper mentions that hindsight counterfactuals require causal models and could be used for data augmentation, but doesn't elaborate on specific methods.
- Why unresolved: The paper only briefly mentions the potential of using causal models for data augmentation through hindsight counterfactuals, without providing concrete algorithms or empirical results.
- What evidence would resolve it: Development and evaluation of algorithms that use causal models to generate virtual experiences or augment existing data in offline RL settings, with measurable improvements in policy performance.

### Open Question 3
- Question: What are the limitations of using causal models in partially observable environments, and how can they be addressed?
- Basis in paper: [explicit] The paper discusses the challenges of confounders in offline RL with partial observability and mentions that even online RL requires causal models for hindsight counterfactuals in such environments.
- Why unresolved: While the paper identifies the challenges, it doesn't provide a comprehensive analysis of the limitations of causal models in partially observable environments or potential solutions.
- What evidence would resolve it: Theoretical analysis of the limitations of causal models in partially observable environments, along with empirical studies evaluating the performance of causal models under varying levels of observability and confounding.

## Limitations
- The analysis focuses on discrete actions, with unclear extension to continuous action spaces
- The paper assumes the agent's observation signal captures all causes of its actions, which may not hold in partially observable environments
- While what-if counterfactuals can be computed from conditional probabilities, hindsight counterfactuals require full causal models even in online settings

## Confidence
- High: Online RL settings where action sufficiency holds, as the paper provides formal proofs using causal models and Pearl's do-calculus
- Medium: Offline RL scenarios, as the paper relies on sufficient conditions (observation-equivalence, complete observability) that may not always hold in practice

## Next Checks
1. Test the action sufficiency assumption empirically in a partially observable gridworld where hidden confounders are introduced
2. Implement both online and offline versions of a POMDP task (like the hockey example) to verify the claimed differences in causal effect estimation
3. Validate the what-if vs. hindsight counterfactual distinction by implementing both query types and showing that what-if queries can be answered using conditional probabilities while hindsight queries require the full structural causal model