---
ver: rpa2
title: Are Compressed Language Models Less Subgroup Robust?
arxiv_id: '2403.17811'
source_url: https://arxiv.org/abs/2403.17811
tags:
- multinli
- compression
- performance
- subgroup
- subgroups
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigates the subgroup robustness of compressed language
  models across 18 different compression methods and settings on BERT. Experiments
  on MultiNLI, CivilComments, and SCOTUS datasets reveal that worst-group performance
  does not solely depend on model size but also on the compression method used.
---

# Are Compressed Language Models Less Subgroup Robust?

## Quick Facts
- arXiv ID: 2403.17811
- Source URL: https://arxiv.org/abs/2403.17811
- Authors: Leonidas Gee; Andrea Zugarini; Novi Quadrianto
- Reference count: 40
- Key outcome: Model compression can improve worst-group accuracy in datasets where base models easily overfit, and subgroup robustness depends on compression method beyond model size

## Executive Summary
This study investigates whether compressed language models are less subgroup robust by evaluating 18 different compression methods on BERT across three datasets (MultiNLI, CivilComments, SCOTUS). Contrary to common assumptions, the research finds that model compression does not always worsen minority subgroup performance - in some cases, it improves worst-group accuracy, particularly when the base model overfits. The key insight is that subgroup robustness depends not only on model size but also on the specific compression method used, as different methods induce varying weight initialization patterns that affect performance across subgroups.

## Method Summary
The paper evaluates 18 compression methods on BERT models across three datasets, measuring average accuracy, worst-group accuracy (WGA), and model size. Models are fine-tuned using empirical risk minimization (ERM) with early stopping, and experiments are conducted across multiple random initializations. The compression methods include distillation, pruning, quantization, and vocabulary transfer techniques. The study systematically compares how different compression approaches affect subgroup robustness beyond simple parameter reduction.

## Key Results
- Worst-group performance depends on compression method, not just model size
- Model compression can improve worst-group accuracy in overfit datasets like CivilComments
- Compressed models with identical parameter counts can exhibit varying subgroup robustness due to different weight initialization patterns

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Subgroup robustness depends not only on model size but also on the compression method used.
- Mechanism: Different compression methods induce different weight initialization patterns after compression, leading to varying estimation errors even with identical model sizes.
- Core assumption: The choice of compression method affects weight initialization in a way that influences subgroup performance.
- Evidence anchors:
  - [abstract]: "worst-group performance does not depend on model size alone, but also on the compression method used."
  - [section]: "although two models may have an equal number of parameters (approximation error), their difference in weight initialization after compression (estimation error) as determined by the compression method used will lead to varying performance."
- Break condition: If compression methods are modified to explicitly control weight initialization patterns, this mechanism may no longer hold.

### Mechanism 2
- Claim: Model compression can improve worst-group accuracy in datasets where the base model easily overfits.
- Mechanism: Reducing model size acts as a form of regularization, helping the model generalize better across subgroups by preventing overfitting to majority subgroups.
- Core assumption: The base model overfits the training data, leading to poor generalization on minority subgroups.
- Evidence anchors:
  - [abstract]: "model compression does not always worsen the performance on minority subgroups. In some cases, it improves worst-group accuracy, particularly in datasets where the base model easily overfits."
  - [section]: "we hypothesize that this is due to CivilComments being a dataset that BERTBase easily overfits on. As such, a reduction in model size serves as a form of regularization for generalizing better across subgroups."
- Break condition: If the base model does not overfit the training data, compression may not improve worst-group accuracy.

### Mechanism 3
- Claim: Model compression does not necessarily maintain overall performance by sacrificing minority subgroup performance.
- Mechanism: Compression can improve performance across all subgroups, not just maintain overall accuracy by sacrificing minority subgroups.
- Core assumption: The relationship between overall performance and subgroup performance is not necessarily zero-sum.
- Evidence anchors:
  - [abstract]: "model compression does not always worsen the performance on minority subgroups."
  - [section]: "we observe that model compression does not always maintain overall performance by sacrificing the minority subgroups. In MultiNLI, a decreasing model size reduces the accuracy on minority subgroups... Conversely, most compressed models improve in accuracy on the minority subgroups (2 and 3) in CivilComments."
- Break condition: If the dataset is highly imbalanced and the model is forced to choose between overall performance and subgroup performance, this mechanism may not hold.

## Foundational Learning

- Concept: Subgroup robustness
  - Why needed here: The paper investigates how different compression methods affect the model's ability to perform well on minority subgroups.
  - Quick check question: What is the definition of subgroup robustness according to the paper?

- Concept: Model compression methods
  - Why needed here: The paper analyzes 18 different compression methods and their effects on subgroup robustness.
  - Quick check question: What are the main categories of model compression methods discussed in the paper?

- Concept: Empirical Risk Minimization (ERM)
  - Why needed here: The paper uses ERM to train the compressed models and discusses its limitations in handling minority subgroups.
  - Quick check question: What is the main drawback of using ERM for training models on imbalanced datasets?

## Architecture Onboarding

- Component map: Data pipeline -> Model loading -> Compression methods -> Training loop -> Evaluation
- Critical path: Load and preprocess datasets -> Load pre-trained models and apply compression -> Fine-tune models using ERM -> Evaluate models on test set -> Analyze results and draw conclusions
- Design tradeoffs:
  - Model size vs. subgroup robustness: Smaller models may have better worst-group accuracy but lower overall accuracy
  - Compression method choice: Different methods lead to varying subgroup robustness even with the same model size
  - Training time vs. performance: More epochs or larger batch sizes may improve performance but increase training time
- Failure signatures:
  - WGA collapses to 0 for distilled models with fewer than 6 layers on SCOTUS
  - Significant drops in WGA for MultiNLI and SCOTUS as sparsity increases in unstructured pruning
  - TinyBERT6 shows unusually high WGA on MultiNLI compared to other compressed models
- First 3 experiments:
  1. Fine-tune BERTBase on MultiNLI, CivilComments, and SCOTUS to establish baseline performance
  2. Apply structured pruning with varying levels of sparsity and evaluate subgroup robustness
  3. Compare the performance of DistilBERT and TinyBERT6 on MultiNLI to investigate the impact of weight initialization

## Open Questions the Paper Calls Out

The paper identifies three main open questions:
1. What specific properties of datasets make model compression improve subgroup robustness rather than harm it?
2. How does the order and combination of different compression methods affect subgroup robustness?
3. What role does the initialization strategy play in determining subgroup robustness of compressed models with identical parameter counts?

## Limitations

- Study examines only BERT-family models, limiting generalizability to other architectures
- Analysis relies on three datasets, which may not represent full diversity of subgroup bias patterns
- Does not explore whether compression methods could be explicitly designed to optimize for subgroup robustness

## Confidence

- **High confidence**: Model compression does not always worsen minority subgroup performance; specific compression methods can improve worst-group accuracy
- **Medium confidence**: The mechanism linking compression method choice to weight initialization patterns affecting subgroup robustness, based on observed correlations rather than direct causal evidence
- **Medium confidence**: The regularization hypothesis explaining improved subgroup performance through reduced overfitting, though this requires further validation across different overfitting scenarios

## Next Checks

1. Test whether explicitly controlling weight initialization patterns during compression can predictably influence subgroup robustness outcomes
2. Evaluate the same compression methods on non-BERT architectures (e.g., RoBERTa, DeBERTa) to assess generalizability
3. Conduct ablation studies varying dataset imbalance levels to determine conditions under which compression improves vs. harms subgroup robustness