---
ver: rpa2
title: 'AttackGNN: Red-Teaming GNNs in Hardware Security Using Reinforcement Learning'
arxiv_id: '2402.13946'
source_url: https://arxiv.org/abs/2402.13946
tags:
- adversarial
- circuits
- security
- circuit
- hardware
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: AttackGNN is the first RL-based red-team attack on GNN techniques
  for hardware security problems. It generates adversarial circuits that fool five
  different GNN-based techniques for IP piracy detection, HT detection/localization,
  reverse engineering, and hardware obfuscation, achieving 100% success rate against
  all tested GNNs.
---

# AttackGNN: Red-Teaming GNNs in Hardware Security Using Reinforcement Learning

## Quick Facts
- arXiv ID: 2402.13946
- Source URL: https://arxiv.org/abs/2402.13946
- Reference count: 40
- Key outcome: AttackGNN is the first RL-based red-team attack on GNN techniques for hardware security problems

## Executive Summary
AttackGNN introduces a reinforcement learning framework that generates adversarial circuits to fool graph neural network (GNN)-based hardware security detectors. The system overcomes challenges of effectiveness, scalability, and generality by using functionality-preserving circuit perturbations, sparse rewards for faster training, and multi-task learning for attacking multiple GNN architectures simultaneously. The approach achieves 100% success rate against five different GNN-based techniques for IP piracy detection, HT detection/localization, reverse engineering, and hardware obfuscation.

## Method Summary
AttackGNN employs a reinforcement learning agent that learns to generate adversarial circuit examples by applying functionality-preserving transformations. The agent uses Proximal Policy Optimization (PPO) to solve a contextual Markov decision process where each context corresponds to a different target GNN. The action space consists of 10 different standard cell selection strategies that alter circuit structure while maintaining functionality. The system uses sparse rewards computed at the end of each episode and employs multi-task learning to generate adversarial examples against multiple GNN architectures with a single agent.

## Key Results
- Achieves 100% success rate against all tested GNNs across four hardware security applications
- Successfully fools GNN4IP (IP piracy detection), TrojanSAINT (HT detection), GNN-RE (reverse engineering), and OMLA (hardware obfuscation) techniques
- Demonstrates generality by using a single RL agent to attack multiple GNN architectures simultaneously
- Shows effectiveness on circuits with thousands of gates and edges from various benchmark datasets

## Why This Works (Mechanism)

### Mechanism 1
- Claim: RL agent learns effective circuit perturbation sequences that preserve functionality while fooling GNN-based hardware security detectors.
- Mechanism: The agent iteratively explores a large design space using reinforcement learning, where each action applies a functionality-preserving transformation to the circuit graph. The reward signal is only computed at the end of each episode, creating sparse rewards that accelerate training. The agent learns a policy that maps circuit states to sequences of transformations that minimize the GNN detector's accuracy.
- Core assumption: Functionality-preserving transformations exist that can sufficiently alter the circuit graph's structural features to evade GNN detection while maintaining semantic equivalence.
- Evidence anchors:
  - [abstract] "We overcome three challenges related to effectiveness, scalability, and generality to devise a potent RL agent."
  - [section] "These transformations change the circuit's structure but not the function."
  - [corpus] Weak evidence - the corpus neighbors don't directly discuss this mechanism, though "InF-ATPG" discusses circuit representation guided RL which is tangentially related.
- Break condition: If no sequence of functionality-preserving transformations can alter the graph features sufficiently to evade detection, or if the state space is too large for the agent to explore effectively within reasonable training time.

### Mechanism 2
- Claim: Multi-task learning via contextual MDP allows a single RL agent to generate adversarial examples against multiple different GNN architectures.
- Mechanism: The problem is formulated as a contextual Markov decision process where each context corresponds to a different target GNN. The agent learns a single policy that works across all contexts by optimizing a combined reward function that aggregates individual GNN performance metrics. This avoids training separate agents for each GNN.
- Core assumption: The state and action spaces are sufficiently similar across different GNN tasks that knowledge can transfer between them, and the combined reward function provides appropriate gradients for all tasks.
- Evidence anchors:
  - [abstract] "we devise a formulation that allows a single RL agent to generate successful adversarial examples against all GNNs"
  - [section] "we perform multi-task learning using contextual MDPs, i.e., a single RL agent to generate successful adversarial examples against all GNNs"
  - [corpus] Weak evidence - the corpus neighbors don't directly discuss multi-task RL for adversarial examples, though "Towards a General Recipe for Combinatorial Optimization with Multi-Filter GNNs" is tangentially related.
- Break condition: If the different GNN tasks are too dissimilar (e.g., different input spaces or reward structures), the agent may not learn effective policies for all tasks simultaneously.

### Mechanism 3
- Claim: Custom functionality-preserving actions based on standard cell selection strategies are more effective than traditional synthesis transformations for generating adversarial examples.
- Mechanism: Instead of using generic synthesis transformations, the agent selects from 10 different standard cell strategies that constrain which gate types can be used. This creates more diverse circuit structures while maintaining functionality, and these actions are compatible across different synthesis tools.
- Core assumption: Constraining gate types in specific patterns creates sufficient diversity in circuit structure to evade GNN detection while still allowing synthesis tools to find functional equivalents.
- Evidence anchors:
  - [abstract] "we devise functionality-preserving circuit perturbations"
  - [section] "These novel actions are the 10 different gate type (also called 'standard cell') selection strategies shown in Table 2"
  - [corpus] Weak evidence - the corpus neighbors don't directly discuss this mechanism, though "NetDeTox" discusses adversarial netlist rewrites which is related.
- Break condition: If the standard cell constraints are too restrictive and prevent synthesis tools from finding functional equivalents, or if they don't create sufficient structural diversity to fool the GNNs.

## Foundational Learning

- Concept: Graph Neural Networks (GNNs) and their application to hardware security
  - Why needed here: The entire attack framework targets GNN-based hardware security techniques, so understanding how GNNs work on circuit graphs is fundamental to designing effective adversarial examples.
  - Quick check question: What are the typical input features and aggregation methods used by GNNs when processing circuit graphs for hardware security tasks?

- Concept: Reinforcement Learning and Markov Decision Processes
  - Why needed here: The attack framework uses RL to solve an MDP