---
ver: rpa2
title: 'Growing Q-Networks: Solving Continuous Control Tasks with Adaptive Control
  Resolution'
arxiv_id: '2404.04253'
source_url: https://arxiv.org/abs/2404.04253
tags:
- control
- action
- learning
- arxiv
- continuous
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces Growing Q-Networks (GQN), a simple critic-only
  reinforcement learning algorithm that adapts control resolution from coarse to fine
  during training. GQN combines decoupled Q-learning with an adaptive action masking
  mechanism, allowing the agent to autonomously increase control resolution as training
  progresses.
---

# Growing Q-Networks: Solving Continuous Control Tasks with Adaptive Control Resolution

## Quick Facts
- arXiv ID: 2404.04253
- Source URL: https://arxiv.org/abs/2404.04253
- Reference count: 19
- Primary result: GQN achieves competitive performance on continuous control tasks by adaptively increasing control resolution during training

## Executive Summary
Growing Q-Networks (GQN) introduces a novel approach to continuous control by combining decoupled Q-learning with adaptive action masking. The method starts with coarse discretization of the action space and progressively increases resolution as training progresses, allowing the agent to balance exploration and smooth control. GQN demonstrates strong performance on action-penalized tasks from DeepMind Control Suite, MetaWorld, and MyoSuite benchmarks, outperforming both discrete and continuous state-of-the-art baselines in several cases.

## Method Summary
GQN is a critic-only reinforcement learning algorithm that adapts control resolution from coarse to fine during training. It employs decoupled Q-learning with an adaptive action masking mechanism, where the agent begins with a small discretized action space and autonomously expands it based on performance thresholds. The network architecture represents the full discretized action space from the start but constrains the active set during training via masking, enabling newly activated actions to benefit from learned shared representations.

## Key Results
- GQN achieves competitive performance against D4PG, DMPO, and DecQN baselines on continuous control tasks
- Particularly strong results on action-penalized tasks where it outperforms stationary discretization methods
- Provides smoother control signals compared to continuous actor-critic methods while maintaining exploration benefits of discrete approaches
- Scales to high-dimensional action spaces up to dim(A) = 38, though network capacity becomes limiting

## Why This Works (Mechanism)

### Mechanism 1
- Claim: GQN enables efficient exploration in high-dimensional continuous control by starting with coarse discretization and gradually increasing resolution
- Core assumption: Early coarse discretization provides sufficient exploration to discover good policies while reducing curse of dimensionality
- Evidence: Abstract states GQN "adapts control resolution from coarse to fine during training"
- Break condition: Initial discretization too restrictive or expansion schedule poorly tuned

### Mechanism 2
- Claim: Decoupled Q-learning with growing action spaces enables scalability to high-dimensional action spaces
- Core assumption: Additive decomposition of Q-values captures action dimension interactions sufficiently
- Evidence: Paper shows strong performance on tasks up to dim(A) = 38
- Break condition: Strong non-linear interactions between action dimensions not captured by additive decomposition

### Mechanism 3
- Claim: Action masking allows warm-start capabilities for newly activated actions
- Core assumption: Shared network torso learns transferable representations
- Evidence: Network architecture "accommodates the full discretized action space from the start and constrains the active set via action masking"
- Break condition: Shared representations don't transfer well to newly activated actions

## Foundational Learning

- Concept: Markov Decision Processes (MDPs)
  - Why needed here: Framework for reinforcement learning algorithms
  - Quick check question: What are the five components of an MDP tuple {S, A, T, R, γ}?

- Concept: Value Decomposition in Multi-Agent RL
  - Why needed here: Enables scalability through additive decomposition of joint Q-values
  - Quick check question: How does the additive decomposition Q(s,a) = (1/M) Σ Qj(s,aj) help avoid exponential complexity?

- Concept: Action Masking in Reinforcement Learning
  - Why needed here: Constrains active action set while maintaining full representation
  - Quick check question: What is the benefit of maintaining complete action space representation while optimizing over subset?

## Architecture Onboarding

- Component map: State → Shared torso [512, 512 MLP] → Univariate utility heads Qj(s,aj) → Additive combination → Q-value prediction
- Critical path: State → Shared torso → Univariate utility heads → Additive combination → Q-value prediction
- Design tradeoffs:
  - Fixed vs. growing discretization: Simplicity vs. adaptive exploration
  - Network capacity: Performance vs. computational cost
  - Expansion schedule: Linear simplicity vs. adaptive efficiency
- Failure signatures:
  - Poor performance on action-penalized tasks: Insufficient resolution
  - Slow learning or high variance: Poor expansion schedule
  - Instability in high-dimensional tasks: Overestimation bias or insufficient capacity
- First 3 experiments:
  1. Pendulum swing-up with 2→9 growing discretization vs. stationary DecQN
  2. Different expansion schedules on basic locomotion task
  3. Humanoid stand with increased network capacity for scalability testing

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does GQN scale to action spaces beyond dim(A)=38, particularly in robotics with dozens of joints?
- Basis: Paper shows performance degradation on MyoSuite tasks despite increased network capacity
- Why unresolved: Fundamental scalability limits not fully characterized
- Evidence needed: Systematic evaluation across progressively larger action dimensions (50-100+)

### Open Question 2
- Question: What is the optimal schedule for expanding action space resolution?
- Basis: Paper introduces both linear and adaptive schedules but provides preliminary comparison only
- Why unresolved: No systematic comparison of parameterizations, growth rates, or timing strategies
- Evidence needed: Ablation studies measuring sample efficiency and final performance across task families

### Open Question 3
- Question: How does GQN compare to continuous actor-critic methods without action penalties?
- Basis: Evaluation only considers action-penalized variations
- Why unresolved: Comparison incomplete for standard control tasks
- Evidence needed: Head-to-head comparisons on standard continuous control benchmarks measuring both performance and smoothness

## Limitations

- Scalability constraints: Exponential growth in discrete action space size limits applicability to extremely high-dimensional problems
- Additional hyperparameters: Expansion thresholds and growth schedules require task-specific tuning
- Limited comparative analysis: Relatively small set of benchmark tasks evaluated against baselines

## Confidence

- Core claims: Medium
  - Empirical results show competitive performance but comparative analysis is limited
  - Adaptive masking mechanism introduces additional hyperparameters with unclear sensitivity
- Scalability claims: Low-Medium
  - Demonstrated up to dim(A) = 38 but performance degrades at this limit
  - Fundamental exponential complexity not addressed

## Next Checks

1. **Scalability Validation**: Test GQN on tasks with 40+ action dimensions to identify precise performance collapse points and determine whether limits are due to network capacity, coordination complexity, or discrete action space growth

2. **Transfer Learning Evaluation**: Pretrain GQN on simpler tasks and fine-tune on complex ones to evaluate whether shared torso representations effectively transfer to new action dimensions

3. **Comparison with Continuous Baselines on Action-Penalized Tasks**: Conduct extensive comparison between GQN and continuous actor-critic methods specifically on action-penalized tasks, measuring both performance metrics and control smoothness to quantify claimed advantages