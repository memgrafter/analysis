---
ver: rpa2
title: Is Our Chatbot Telling Lies? Assessing Correctness of an LLM-based Dutch Support
  Chatbot
arxiv_id: '2411.00034'
source_url: https://arxiv.org/abs/2411.00034
tags:
- answer
- chatbot
- https
- support
- context
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study defines and evaluates correctness for an LLM-based Dutch
  support chatbot. Using insights from shadowing support staff and analyzing chatbot
  rejections, correctness is characterized by truthfulness, relatedness, and completeness.
---

# Is Our Chatbot Telling Lies? Assessing Correctness of an LLM-based Dutch Support Chatbot

## Quick Facts
- arXiv ID: 2411.00034
- Source URL: https://arxiv.org/abs/2411.00034
- Authors: Herman Lassche; Michiel Overeem; Ayushi Rastogi
- Reference count: 28
- Primary result: 55% accuracy in detecting incorrect chatbot responses, with 0.28-0.37 Spearman correlation with human ratings

## Executive Summary
This study defines and evaluates correctness for an LLM-based Dutch support chatbot by modeling the human decision process of support staff. Through shadowing and analysis of rejection reports, correctness is characterized by truthfulness, relatedness, and completeness. The authors develop a decision tree to model the support team's evaluation process and derive automated features from natural language generation and automated grading literature. The resulting approach achieves 55% accuracy in detecting wrong messages and shows moderate correlation with human ratings. The study reveals that translated English text outperforms Dutch text for automated evaluation, highlighting challenges for regional languages.

## Method Summary
The authors collected Dutch chatbot responses with user questions and context documents, along with human ratings on truthfulness. They constructed a decision tree modeling the support team's assessment process to derive heuristics for truthfulness. Automated features were implemented from literature and combined into a scoring system that outputs scores on a 1-5 scale. The system was validated against human ratings using Spearman correlation. The approach focuses on binary and instruction-type messages, using both Dutch and English features (with translation for certain NLP-dependent features).

## Key Results
- Automated approach achieves 55% accuracy in detecting incorrect messages
- Spearman correlation with human ratings ranges from 0.28-0.37
- Translated English text outperforms Dutch text for automated evaluation
- Focus on binary and instruction-type messages enables effective automated scoring

## Why This Works (Mechanism)

### Mechanism 1
The study achieves automated detection of incorrect chatbot responses by defining correctness as a combination of truthfulness, relatedness, and completeness. The authors build a decision tree modeling the support team's evaluation process, then derive heuristics from this tree to create automated features for scoring responses. This works because the support team's manual evaluation process can be effectively modeled as a decision tree that captures the essential criteria for correctness.

### Mechanism 2
Translating Dutch chatbot responses to English improves automated evaluation performance due to better NLP tool support. The authors translate Dutch text to English for features that rely on external NLP packages, while keeping company-specific features in Dutch. This works because external NLP tools (like dependency parsers, word embeddings) perform better on English text than Dutch, leading to more accurate feature extraction.

### Mechanism 3
Focusing on binary and instruction-type messages allows for more effective automated scoring due to overlapping heuristics. The authors analyze message types and find that binary and instruction messages share sufficient heuristics for truthfulness evaluation, while other types require different approaches. This works because binary and instruction message types represent a significant portion of support queries and share enough characteristics to be evaluated with the same set of heuristics.

## Foundational Learning

- Concept: Decision trees as models of human decision processes
  - Why needed here: The study uses decision trees to model how support staff evaluate chatbot responses, which then guides the creation of automated features
  - Quick check question: How does a decision tree represent sequential decision-making, and why is this suitable for modeling human evaluation criteria?

- Concept: Spearman correlation for ordinal rating comparison
  - Why needed here: The study uses Spearman correlation to measure agreement between automated scores (continuous) and human ratings (ordinal 1-5 scale)
  - Quick check question: Why is Spearman correlation more appropriate than Pearson correlation when comparing ordinal human ratings with continuous automated scores?

- Concept: Feature ablation studies for metric selection
  - Why needed here: The study uses ablation to determine which features contribute most to correlation with human evaluation, removing those that don't improve performance
  - Quick check question: How does removing a feature and observing changes in correlation help identify which features are truly valuable for the automated scoring system?

## Architecture Onboarding

- Component map: Data Collection -> Message Type Classification -> Feature Extraction -> Scoring Engine -> Evaluation Module
- Critical path: Message → Type Classification → Feature Extraction → Scoring → Evaluation
- Design tradeoffs:
  - Dutch vs English processing: Better NLP tools for English but loss of company-specific nuance
  - Simple vs complex features: Simple features are more interpretable but may miss subtle errors
  - Generic vs custom features: Custom features capture domain knowledge but reduce generalizability
- Failure signatures:
  - Low correlation with human ratings: Features don't capture what humans consider important
  - High false positive rate: Scoring system marks correct answers as incorrect
  - Type classification errors: Wrong messages not being evaluated due to misclassification
- First 3 experiments:
  1. Test message type classification accuracy on a held-out set of messages
  2. Compare Dutch-only vs English-translation feature performance on the same dataset
  3. Run ablation study to identify which features contribute most to correlation with human ratings

## Open Questions the Paper Calls Out

### Open Question 1
How does the automated correctness metric perform on question types beyond binary and instruction, such as error resolution or cause-and-effect reasoning? The paper states that only binary and instruction types were evaluated due to data constraints, and acknowledges that other types may require different heuristics. This remains unresolved because the study focused on 58% of the dataset, leaving the other 42% untested.

### Open Question 2
Does translating Dutch text to English for automated evaluation consistently improve performance across different Dutch dialects or specialized industry jargon? The paper found that translated English text outperformed Dutch text, suggesting potential benefits for regional languages. This remains unresolved because the study only tested with general Dutch text and did not explore variations in dialects or specialized jargon.

### Open Question 3
How can the automated metric be adapted to capture nuanced correctness issues, such as superior solutions or context-specific terminology, that are difficult for humans to detect? The paper identifies edge cases where even human annotators struggle, such as superior solutions or context-specific terminology, and suggests a knowledge base or neural network could help. This remains unresolved because the current metric relies on heuristics that may not capture all nuances.

## Limitations
- Approach limited to binary and instruction-type messages (82% of test set)
- Performance drops significantly when evaluated on broader analysis set (35% accuracy)
- Use of translated English text may introduce semantic distortions
- Results based on single company's support chatbot may not generalize

## Confidence

- **High Confidence:** Decision tree modeling approach for capturing human evaluation criteria is well-grounded and supported by direct observation
- **Medium Confidence:** Superiority of translated English text for automated evaluation is demonstrated but may be context-dependent
- **Low Confidence:** Generalizability of approach to other chatbot domains or message types

## Next Checks
1. Apply the automated scoring system to chatbot responses from a different company or domain to test generalizability of the decision tree and feature set
2. Compare performance of Dutch-only features against translated English features without mixing the two approaches to isolate the translation effect
3. Develop and test automated scoring approaches for unspecified and other message types to increase coverage beyond the current 82% of the test set