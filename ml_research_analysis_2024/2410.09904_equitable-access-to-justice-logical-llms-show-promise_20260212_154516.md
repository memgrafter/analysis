---
ver: rpa2
title: 'Equitable Access to Justice: Logical LLMs Show Promise'
arxiv_id: '2410.09904'
source_url: https://arxiv.org/abs/2410.09904
tags:
- policy
- query
- prolog
- legal
- time
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Large language models (LLMs) struggle with legal reasoning due
  to the need for System 2 logical reasoning in legal contexts. This paper explores
  using LLMs to generate logic programs from legal texts to improve access to justice.
---

# Equitable Access to Justice: Logical LLMs Show Promise

## Quick Facts
- arXiv ID: 2410.09904
- Source URL: https://arxiv.org/abs/2410.09904
- Reference count: 40
- Large language models with System 2 reasoning capabilities can effectively encode legal rules into logic programs, with OpenAI o1-preview correctly answering 7.5 out of 9 queries on average versus GPT-4o's 2.4

## Executive Summary
This paper addresses the challenge of using large language models for legal reasoning, which requires System 2 logical reasoning capabilities that standard LLMs lack. The authors propose translating legal texts into logic programs using LLMs, then applying these programs to specific cases. An experiment translating a health insurance contract into Prolog code demonstrated that OpenAI o1-preview, with its advanced reasoning capabilities, significantly outperformed GPT-4o in generating correct Prolog encodings and answering legal queries. This approach shows promise for improving access to justice by making legal reasoning more scalable and interpretable.

## Method Summary
The method involves prompting LLMs to translate legal texts (specifically a simplified Chubb Hospital Cash Benefit insurance policy) into Prolog rules. The generated Prolog code is then used to answer specific yes/no queries about coverage through a Prolog query interface. The experiment compares OpenAI o1-preview against GPT-4o by measuring how many of nine sample queries each model's Prolog encoding can answer correctly. The Prolog code is executed in SWISH to evaluate query responses.

## Key Results
- OpenAI o1-preview correctly answered 7.5 out of 9 queries on average, compared to GPT-4o's 2.4
- GPT-4o failed to generate syntactically valid Prolog code in 4 out of 10 trials
- o1-preview successfully encoded temporal logic conditions (e.g., wellness visits within 6 months with confirmation within 7 months) using structured Prolog rules

## Why This Works (Mechanism)

### Mechanism 1
- Claim: OpenAI o1-preview outperforms GPT-4o in generating Prolog code that correctly encodes legal contract conditions due to its advanced System 2 reasoning capabilities.
- Mechanism: o1-preview uses step-by-step logical decomposition to map legal text conditions into structured Prolog rules with correct temporal logic.
- Core assumption: Legal contract conditions are expressible in formal logic and o1-preview can parse and translate these without loss of semantic meaning.
- Evidence anchors: [abstract] states o1-preview "correctly answered 7.5 out of 9 queries on average compared to GPT-4o's 2.4"; [section] describes o1-preview encoding wellness visit condition using temporal variables and explicit logical relationships.
- Break condition: If legal text contains ambiguity or context-dependent judgment beyond formal logic, o1-preview may fail to encode accurately.

### Mechanism 2
- Claim: The integration of LLMs with logic programming creates interpretable and scalable legal reasoning systems that combine probabilistic understanding with deterministic verification.
- Mechanism: LLMs generate initial Prolog representations from legal texts, which are then used as structured logic programs to answer specific queries with consistency and transparency.
- Core assumption: Generated Prolog code is sufficiently accurate to be used without manual rewriting for query answering.
- Evidence anchors: [abstract] describes the objective "to translate laws and contracts into logic programs that can be applied to specific legal cases"; [section] shows GPT-4o's failure to encode the wellness visit condition correctly while o1-preview succeeds.
- Break condition: If LLM-generated logic contains subtle errors or omissions, the entire reasoning chain fails even if the code is syntactically valid.

### Mechanism 3
- Claim: Fine-tuning language models using logic-based explanations (e.g., STaR method) can enhance legal reasoning capabilities without requiring large annotated datasets.
- Mechanism: Iterative rationale generation and rationalization improves model reasoning by providing step-by-step logical explanations that are then used to correct errors.
- Core assumption: Logic-based explanations can be generated and evaluated automatically to improve model performance.
- Evidence anchors: [section] proposes "fine-tuning language models using logic-based explanations" citing STaR method; [abstract] notes current LLMs "lack this capability to a sufficient degree" for legal reasoning.
- Break condition: If logic explanations are too complex for the model to generate or evaluate, the fine-tuning process may not converge.

## Foundational Learning

- Concept: System 1 vs System 2 reasoning
  - Why needed here: The paper explicitly contrasts the probabilistic, fast System 1 reasoning of standard LLMs with the deliberate, logical System 2 reasoning needed for legal applications
  - Quick check question: What type of reasoning (System 1 or 2) is required for encoding legal contract conditions into formal logic?

- Concept: Logic programming fundamentals
  - Why needed here: The entire approach relies on translating natural language legal text into Prolog rules that can be queried deterministically
  - Quick check question: What Prolog construct would you use to represent a condition that must be satisfied within a specific time window?

- Concept: Temporal logic in legal reasoning
  - Why needed here: Legal contracts often contain time-based conditions (e.g., "no later than the 7th month anniversary") that require precise temporal encoding
  - Quick check question: How would you represent "within 6 months" versus "exactly at 6 months" in Prolog temporal logic?

## Architecture Onboarding

- Component map: Legal text input -> LLM (o1-preview) -> Prolog code generation -> Logic program -> Query interface -> Answer output
- Critical path: Legal text -> LLM encoding -> Prolog query execution -> Answer validation
  The LLM encoding step is the bottleneck and primary source of errors
- Design tradeoffs:
  - Accuracy vs. scalability: More accurate models (o1-preview) are slower and more expensive than faster models (GPT-4o)
  - Interpretability vs. completeness: More detailed Prolog code is easier to audit but may be harder to maintain
  - Human review vs. automation: Expert review ensures quality but reduces scalability
- Failure signatures:
  - Syntax errors in generated Prolog code (GPT-4o failed this in 4 of 10 trials)
  - Semantic errors where logic is valid but doesn't match contract intent
  - Temporal logic errors in encoding time-based conditions
  - Missing edge cases in contract conditions
- First 3 experiments:
  1. Run the Chubb Hospital Cash Benefit policy through GPT-4o and o1-preview to compare Prolog output quality
  2. Generate 10 different legal contract snippets and measure LLM accuracy in encoding complex conditions
  3. Test query answering accuracy using SWISH Prolog engine with both correct and incorrect encodings

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the performance ceiling for LLMs with System 2 reasoning capabilities in encoding legal texts into logic programs?
- Basis in paper: [explicit] The paper demonstrates that OpenAI o1-preview outperforms GPT-4o significantly in encoding a health insurance contract into Prolog code, but does not explore the limits of this capability.
- Why unresolved: The experiment only compares two models and uses a single, simplified contract. It is unclear how these models would perform with more complex legal texts or how much further improvements could be made with even more advanced LLMs.
- What evidence would resolve it: Testing a range of LLMs with System 2 reasoning capabilities on a variety of complex legal texts and comparing their performance in encoding these texts into logic programs.

### Open Question 2
- Question: How can human feedback be systematically integrated into the process of generating logic programs from legal texts using LLMs?
- Basis in paper: [inferred] The paper mentions that incorporating human feedback, specifically from expert attorneys, could enhance the quality of the generated logic, but does not provide a detailed mechanism for this integration.
- Why unresolved: The paper suggests the importance of human feedback but does not explore how to effectively implement this feedback loop, particularly at scale, without compromising the efficiency gains from using LLMs.
- What evidence would resolve it: Developing and testing a framework for integrating human feedback into the LLM-generated logic programs, evaluating its impact on accuracy and efficiency.

### Open Question 3
- Question: What are the limitations of using Prolog as the primary logic programming language for encoding legal texts, and are there alternative languages that could be more effective?
- Basis in paper: [explicit] The paper uses Prolog to encode a health insurance contract, but acknowledges that logic programming languages like Prolog have inherent limitations in flexibility and scalability.
- Why unresolved: While Prolog is chosen for its logical reasoning capabilities, the paper does not explore other logic programming languages or frameworks that might better handle the complexities and nuances of legal texts.
- What evidence would resolve it: Comparing the performance of different logic programming languages in encoding various legal texts, assessing their ability to handle complexity, scalability, and interpretability.

## Limitations
- The study relies on a single, simplified insurance contract, limiting generalizability to complex real-world legal documents
- Ground truth answers for the nine queries are not explicitly verified, introducing potential circular validation
- The comparison between o1-preview and GPT-4o uses average scores (7.5 vs 2.4) but doesn't report variance or statistical significance

## Confidence

- **High confidence**: o1-preview generates more accurate Prolog encodings than GPT-4o for the tested insurance contract
- **Medium confidence**: Logic programming approach can scale to real-world legal reasoning with appropriate LLM support
- **Low confidence**: Fine-tuning with logic-based explanations will significantly improve legal reasoning capabilities

## Next Checks
1. **Legal Expert Review**: Have practicing attorneys review the generated Prolog encodings against the original contract to verify semantic accuracy, not just syntactic correctness
2. **Cross-Contract Testing**: Test the same LLM models on 5-10 additional legal contracts of varying complexity (employment agreements, lease contracts, etc.) to assess generalizability
3. **Error Pattern Analysis**: Systematically analyze the types of errors GPT-4o makes (syntax vs. semantics vs. temporal logic) to identify whether o1-preview's improvements are fundamental or task-specific