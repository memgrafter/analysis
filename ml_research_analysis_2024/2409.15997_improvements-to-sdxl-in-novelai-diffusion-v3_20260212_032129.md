---
ver: rpa2
title: Improvements to SDXL in NovelAI Diffusion V3
arxiv_id: '2409.15997'
source_url: https://arxiv.org/abs/2409.15997
tags:
- image
- diffusion
- noise
- training
- sdxl
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces several key improvements to SDXL for better
  anime image generation. They switch from epsilon-prediction to v-prediction parameterization
  to support Zero Terminal SNR (ZTSNR), which trains the model to handle pure noise
  by using a noise schedule that extends to infinite noise levels.
---

# Improvements to SDXL in NovelAI Diffusion V3

## Quick Facts
- arXiv ID: 2409.15997
- Source URL: https://arxiv.org/abs/2409.15997
- Authors: Juan Ossa; Eren DoÄŸan; Alex Birch; F. Johnson
- Reference count: 37
- Key outcome: Introduces technical improvements to SDXL for anime image generation including v-prediction, ZTSNR, aspect-ratio bucketing, and tag-based loss weighting

## Executive Summary
This paper presents several key improvements to the SDXL model specifically for anime image generation. The authors implement a switch from epsilon-prediction to v-prediction parameterization to support Zero Terminal SNR (ZTSNR), which enables training the model to handle pure noise by using a noise schedule that extends to infinite noise levels. They also increase the maximum sigma value to improve coherence in high-resolution generation, implement aspect-ratio bucketing for training efficiency, and use tag-based loss weighting to balance rare and common concepts. The model is trained on a large anime dataset and demonstrates improved performance at lower CFG scales compared to standard SDXL.

## Method Summary
The paper introduces multiple technical improvements to SDXL for anime generation. The key change is switching from epsilon-prediction to v-prediction parameterization, which supports Zero Terminal SNR (ZTSNR) by allowing the noise schedule to extend to infinite levels. The authors also raise the maximum sigma value to improve high-resolution generation coherence, implement aspect-ratio bucketing to enhance training efficiency, and employ tag-based loss weighting to balance rare and common concepts in the dataset. The model is trained on a large anime dataset using extensive compute resources. The paper also provides practical solutions for implementing ZTSNR in the EDM framework and shares dataset statistics for latent scaling.

## Key Results
- Model produces coherent images at lower CFG scales (3.5-5) compared to standard SDXL (7.5)
- Implementation of ZTSNR in EDM framework successfully handles pure noise through extended noise schedules
- Dataset statistics provided for latent scaling applications
- Aspect-ratio bucketing improves training efficiency

## Why This Works (Mechanism)
The improvements work by addressing specific limitations in the standard SDXL architecture for anime generation. The v-prediction parameterization enables ZTSNR, which trains the model to handle extreme noise conditions by extending the noise schedule to infinite levels. This creates more robust denoising capabilities. The increased maximum sigma value allows better handling of high-frequency details in anime images. Aspect-ratio bucketing optimizes the training process by grouping similar aspect ratios together, reducing computational overhead. Tag-based loss weighting ensures balanced representation of both common and rare concepts in the training data, preventing the model from overfitting to dominant patterns.

## Foundational Learning

**Zero Terminal SNR (ZTSNR)**: Training the model to handle pure noise by extending noise schedules to infinite levels
- Why needed: Enables generation from complete randomness without degradation
- Quick check: Verify model can generate coherent images from high sigma values

**V-prediction parameterization**: Alternative to epsilon-prediction that better handles extreme noise conditions
- Why needed: Provides mathematical framework for ZTSNR implementation
- Quick check: Compare image quality at high sigma values between v-prediction and epsilon-prediction

**Aspect-ratio bucketing**: Grouping training samples by aspect ratio to improve computational efficiency
- Why needed: Reduces memory overhead and speeds up training
- Quick check: Measure training time and memory usage with and without bucketing

**Tag-based loss weighting**: Adjusting loss contributions based on concept frequency in training data
- Why needed: Prevents model bias toward common concepts
- Quick check: Analyze concept representation in generated images

## Architecture Onboarding

Component Map: V-prediction + ZTSNR noise schedule -> Aspect-ratio bucketing -> Tag-based loss weighting -> Training pipeline -> Image generation

Critical Path: The core innovation follows the path from v-prediction parameterization through ZTSNR implementation to final image generation, with aspect-ratio bucketing and tag-based loss weighting serving as supporting optimizations.

Design Tradeoffs: The switch to v-prediction enables ZTSNR but requires more complex mathematical handling. Higher sigma values improve detail but increase computational cost. Aspect-ratio bucketing improves efficiency but requires preprocessing overhead.

Failure Signatures: Poor image quality at high sigma values suggests ZTSNR implementation issues. Training instability may indicate problems with v-prediction parameterization. Imbalanced concept representation points to inadequate tag-based loss weighting.

First Experiments:
1. Generate images from pure noise using ZTSNR to verify implementation
2. Compare image quality at various sigma levels between v-prediction and epsilon-prediction
3. Test aspect-ratio bucketing impact on training efficiency

## Open Questions the Paper Calls Out
None

## Limitations
- Technical details of ZTSNR implementation in EDM framework are not fully elaborated
- Specific composition and curation methodology of the anime dataset are not detailed
- The relationship between CFG scale and output quality could reflect factors beyond label quality

## Confidence
High confidence: Technical implementation claims (epsilon-prediction to v-prediction switch, aspect-ratio bucketing, tag-based loss weighting)
Medium confidence: ZTSNR implementation success, dataset statistics and composition, qualitative improvements in image generation
Low confidence: CFG scale interpretation

## Next Checks
1. Replicate the ZTSNR implementation in a controlled environment to verify the claimed benefits and ensure proper handling of infinite noise levels

2. Conduct ablation studies on the anime dataset to isolate the contribution of each improvement (v-prediction, sigma value changes, aspect-ratio bucketing, loss weighting)

3. Compare model outputs using identical CFG scales across both models while varying other parameters to better understand the relationship between CFG scale and output quality in this context