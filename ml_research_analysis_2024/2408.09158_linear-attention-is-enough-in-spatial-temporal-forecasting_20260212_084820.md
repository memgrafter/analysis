---
ver: rpa2
title: Linear Attention is Enough in Spatial-Temporal Forecasting
arxiv_id: '2408.09158'
source_url: https://arxiv.org/abs/2408.09158
tags:
- traffic
- forecasting
- spatial-temporal
- attention
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of spatial-temporal forecasting
  in traffic prediction, where existing methods struggle with dynamic road network
  topologies, message passing issues, and over-smoothing in graph neural networks.
  The proposed approach treats each sensor at different time steps as independent
  spatial-temporal tokens, feeding them into a vanilla Transformer to learn complex
  spatial-temporal patterns.
---

# Linear Attention is Enough in Spatial-Temporal Forecasting

## Quick Facts
- arXiv ID: 2408.09158
- Source URL: https://arxiv.org/abs/2408.09158
- Authors: Xinyu Ning
- Reference count: 23
- Key outcome: Achieves SOTA results on METR-LA and PEMS-BAY traffic forecasting datasets using vanilla Transformer with linear attention approximation

## Executive Summary
This paper addresses the challenge of spatial-temporal forecasting in traffic prediction by treating each sensor at different time steps as independent spatial-temporal tokens and feeding them into a vanilla Transformer. The proposed STformer model achieves state-of-the-art performance on standard benchmarks, while its variant NSTformer using Nyström-based linear attention approximation surprisingly shows slightly better performance in some cases. The approach simplifies the problem formulation compared to existing methods that struggle with dynamic road network topologies, message passing issues, and over-smoothing in graph neural networks.

## Method Summary
The paper proposes treating traffic forecasting as a sequence-to-sequence problem where each sensor-time pair is treated as an independent token, similar to language modeling. This formulation allows direct application of vanilla Transformer architectures without the complexity of graph convolutions or specialized attention mechanisms. The key insight is that the spatial-temporal dependencies in traffic data can be effectively captured by standard self-attention when properly formulated as a sequence problem. To address the quadratic complexity of self-attention, the NSTformer variant uses the Nyström method to approximate self-attention with linear complexity, achieving both computational efficiency and maintaining or improving performance.

## Key Results
- STformer achieves SOTA performance on METR-LA and PEMS-BAY datasets
- NSTformer (linear attention variant) slightly outperforms STformer in some cases
- For 15-minute horizons on METR-LA: MAE of 2.58, RMSE of 4.87, MAPE of 6.69%
- For 15-minute horizons on PEMS-BAY: MAE of 1.14, RMSE of 2.31, MAPE of 2.33%

## Why This Works (Mechanism)
The paper demonstrates that spatial-temporal patterns in traffic forecasting can be effectively captured by treating sensor-time pairs as independent tokens in a sequence-to-sequence framework. By reformulating the problem as a standard sequence prediction task, the complex spatial-temporal dependencies can be learned through standard self-attention mechanisms without requiring specialized graph neural network components. The surprising effectiveness of linear attention approximation suggests that the full quadratic attention matrix may contain redundant information that can be efficiently captured through Nyström approximation.

## Foundational Learning
- **Spatial-Temporal Forecasting**: Predicting future traffic conditions based on historical sensor data across space and time - needed because traffic networks exhibit complex dependencies that traditional methods struggle to capture
- **Nyström Method**: A technique for approximating large kernel matrices using a subset of columns - needed to reduce the quadratic complexity of self-attention to linear complexity
- **Transformer Architecture**: The original self-attention based model for sequence-to-sequence tasks - needed as the base architecture for processing spatial-temporal tokens
- **Traffic Network Topology**: The structure and connectivity of road sensors - needed context for understanding spatial dependencies in traffic data
- **Message Passing in GNNs**: Information propagation between graph nodes - needed to understand why traditional approaches face challenges with over-smoothing
- **Quadratic Attention Complexity**: The O(n²) computational requirement of standard self-attention - needed to appreciate the motivation for linear attention approximation

## Architecture Onboarding

Component Map:
Input Time Series -> Tokenization (sensor-time pairs) -> Transformer Encoder -> Self-Attention (or Nyström Approximation) -> Prediction Layer -> Output

Critical Path:
Sensor data -> Time series processing -> Token formation -> Self-attention computation -> Feature transformation -> Output prediction

Design Tradeoffs:
- Standard Transformer vs. specialized graph architectures: Simpler formulation but requires treating each sensor-time pair as independent
- Full attention vs. linear attention: Accuracy vs. computational efficiency tradeoff, though results show linear attention may be sufficient
- Token-based vs. graph-based representation: Easier to implement but may lose explicit spatial relationships

Failure Signatures:
- Missing or irregular sensor data could break the fixed token structure
- Varying network topologies not handled well by fixed token approach
- Potential loss of explicit spatial relationships when treating tokens independently

First Experiments:
1. Compare STformer vs. NSTformer performance on small subset of data to verify linear attention approximation quality
2. Test with artificially missing sensor data to evaluate robustness
3. Compare attention patterns learned by full vs. linear attention to understand what information is preserved

## Open Questions the Paper Calls Out
None identified in the source material.

## Limitations
- The fixed token structure assumes consistent network topology and may not generalize well to varying sensor counts or missing data
- The claim that linear attention "is enough" is primarily empirical with underdeveloped theoretical justification
- The counterintuitive result of NSTformer outperforming STformer needs further investigation for generalization
- Computational complexity analysis doesn't fully account for memory overhead of processing all sensor-time pairs

## Confidence

**Major claim clusters confidence:**
- STformer achieving SOTA performance: High confidence (well-validated on standard benchmarks)
- Linear attention sufficiency: Medium confidence (empirical results strong but theoretical foundation weak)
- NSTformer outperforming STformer: Medium confidence (counterintuitive result needs further investigation)

## Next Checks
1. Test model performance with missing sensor data and varying network topologies to assess robustness beyond fixed sensor configurations
2. Conduct ablation studies comparing learned spatial-temporal patterns between STformer and NSTformer to understand what information the Nyström approximation may be preserving or losing
3. Evaluate computational memory requirements and scaling behavior on larger road networks or longer prediction horizons to verify practical advantages of linear attention