---
ver: rpa2
title: Interpretable Preferences via Multi-Objective Reward Modeling and Mixture-of-Experts
arxiv_id: '2406.12845'
source_url: https://arxiv.org/abs/2406.12845
tags:
- arxiv
- reward
- preprint
- preference
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a method to improve interpretability of reward
  models used in reinforcement learning from human feedback (RLHF). The authors address
  the issue that conventional reward models act as black boxes without providing human-interpretable
  explanations for their outputs.
---

# Interpretable Preferences via Multi-Objective Reward Modeling and Mixture-of-Experts

## Quick Facts
- arXiv ID: 2406.12845
- Source URL: https://arxiv.org/abs/2406.12845
- Reference count: 23
- State-of-the-art performance on RewardBench benchmark

## Executive Summary
This paper addresses the interpretability problem in reward models used for reinforcement learning from human feedback (RLHF). Traditional reward models act as black boxes without providing human-interpretable explanations for their outputs. The authors propose a two-stage approach that first trains an Absolute-Rating Multi-Objective Reward Model (ArmoRM) using multi-dimensional absolute ratings across interpretable objectives like honesty and verbosity, then employs a Mixture-of-Experts (MoE) strategy with a gating network to automatically select the most suitable reward objectives based on context. The resulting model, ArmoRM-Llama3-8B, achieves state-of-the-art performance on RewardBench, surpassing LLM-as-a-judge methods and approaching the performance of much larger models.

## Method Summary
The authors propose a two-stage approach: (1) Train an ArmoRM with multi-dimensional absolute ratings across human-interpretable objectives using a frozen Llama-3 8B backbone and a linear regression layer, (2) Employ a Mixture-of-Experts (MoE) strategy with a gating network (shallow MLP) that takes prompt features and outputs probability distributions over reward objectives to dynamically adjust objective weighting based on context. The model addresses verbosity bias by subtracting a scaled version of the verbosity objective from other objectives during training.

## Key Results
- ArmoRM-Llama3-8B achieves state-of-the-art performance on RewardBench benchmark
- Model surpasses LLM-as-a-judge methods with GPT-4 judges
- Performance approaches that of the much larger Nemotron-4 340B reward model

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The gating network enables context-dependent objective weighting, improving alignment quality over static linear combinations.
- Mechanism: The gating network takes the prompt feature fθ(x) and outputs a probability distribution over reward objectives, allowing the model to dynamically adjust which objectives matter most for a given context.
- Core assumption: The prompt feature fθ(x) contains sufficient information to predict which reward objectives should be emphasized for optimal performance.
- Evidence anchors: [abstract] "employ a Mixture-of-Experts (MoE) strategy with a gating network that automatically selects the most suitable reward objectives based on the context"

### Mechanism 2
- Claim: Multi-objective absolute ratings provide richer training signal than binary pairwise preferences.
- Mechanism: By training on continuous absolute ratings across multiple dimensions rather than just binary pairwise comparisons, the model learns more nuanced distinctions between responses and can capture fine-grained human preferences.
- Core assumption: The continuous rating scales contain meaningful information beyond what can be captured in binary preferences.
- Evidence anchors: [section 3.1] "The binarization process discards some fine-grained information... It is not justified that discarding the fine-grained preference information is beneficial"

### Mechanism 3
- Claim: Adjusting reward objectives for verbosity correlation prevents the model from inheriting verbosity bias.
- Mechanism: By subtracting a scaled version of the verbosity objective from other objectives, the model removes the confounding effect of response length on other quality dimensions.
- Core assumption: Verbosity correlates with other reward objectives in a way that systematically biases the model.
- Evidence anchors: [section 3.2] "most reward objectives are highly correlated with verbosity, which indicates a strong verbosity bias"

## Foundational Learning

- Concept: Bradley-Terry model for pairwise preference learning
  - Why needed here: Understanding the baseline approach that this work improves upon
  - Quick check question: What is the key limitation of the Bradley-Terry model for complex human preferences?

- Concept: Multi-objective regression and loss functions
  - Why needed here: The ArmoRM is trained using regression loss on multi-dimensional absolute ratings
  - Quick check question: How does multi-objective regression differ from the Bradley-Terry pairwise preference approach?

- Concept: Mixture-of-Experts (MoE) architecture and gating mechanisms
  - Why needed here: The gating network implements MoE to dynamically select which reward objectives to emphasize
  - Quick check question: What is the role of the gating network in the MoE architecture?

## Architecture Onboarding

- Component map: Prompt → Backbone (Llama-3 8B) → Feature extraction → Gating network (MLP) → Weighted objective combination → Scalar reward score

- Critical path: Prompt → Backbone → Feature extraction → Gating network → Weighted objective combination → Scalar reward score

- Design tradeoffs:
  - Frozen backbone for efficiency vs. fine-tuning for potentially better performance
  - Number of objectives vs. model complexity and risk of objective conflicts
  - Simplicity of gating network vs. more complex architectures

- Failure signatures:
  - Gating coefficients collapse to uniform distribution
  - Performance matches or is worse than baseline Bradley-Terry model
  - Model overfits to training objectives without generalizing

- First 3 experiments:
  1. Train ArmoRM with regression loss on multi-objective data and evaluate on RewardBench without MoE
  2. Add gating network with uniform weights to verify scalarization works correctly
  3. Train gating network with frozen backbone and compare performance to baseline Bradley-Terry model

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the choice of reference dataset for correlation metric calculation in Eq. (3) affect the performance and bias correction of the ArmoRM model?
- Basis in paper: [explicit] The paper mentions using UltraFeedback as the reference dataset for the Spearman correlation metric in Eq. (3), but does not explore other options.

### Open Question 2
- Question: What is the impact of using different architectures for the gating network (e.g., more complex MLPs, transformer-based models) on the ArmoRM's performance and interpretability?
- Basis in paper: [inferred] The paper uses a simple 3-layer ReLU MLP for the gating network but does not explore alternative architectures.

### Open Question 3
- Question: How does the ArmoRM model's performance on specialized domains (e.g., legal, medical) compare to its performance on general language tasks, and what modifications might be necessary for domain-specific applications?
- Basis in paper: [inferred] The paper evaluates ArmoRM on general language tasks but does not explore its performance on specialized domains.

## Limitations

- The gating network's ability to meaningfully distinguish between different prompt contexts and adjust reward objective weighting is not directly validated through ablation studies.
- The verbosity correlation adjustment mechanism assumes a stable global correlation pattern that may not hold across diverse domains.
- The claim that multi-objective absolute ratings provide richer training signal than binary preferences is asserted but not empirically validated through direct comparison.

## Confidence

- **High Confidence**: Technical feasibility of the two-stage training approach and basic implementation details
- **Medium Confidence**: State-of-the-art performance claims on RewardBench
- **Low Confidence**: Mechanism-level claims about context-dependent objective weighting and verbosity bias mitigation

## Next Checks

1. Conduct an ablation study of gating network contribution by training versions with uniform gating weights versus learned gating weights to quantify the actual performance improvement attributable to context-sensitive objective selection.

2. Test the verbosity adjustment mechanism across different prompt categories and domains to verify that the assumed correlation pattern is consistent and that the global adjustment coefficients remain appropriate.

3. Conduct a controlled experiment comparing models trained on the same data but using either absolute ratings versus binary preferences to directly measure the claimed benefit of multi-objective training signal.