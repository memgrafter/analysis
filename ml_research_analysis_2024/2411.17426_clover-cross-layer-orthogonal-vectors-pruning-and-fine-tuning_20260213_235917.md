---
ver: rpa2
title: 'CLOVER: Cross-Layer Orthogonal Vectors Pruning and Fine-Tuning'
arxiv_id: '2411.17426'
source_url: https://arxiv.org/abs/2411.17426
tags:
- clover
- arxiv
- pruning
- vectors
- orthogonal
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: CLOVER introduces a method for compressing decoder-only models
  by treating Q-K and V-O pairs in attention heads as low-rank decompositions. Through
  SVD, CLOVER orthogonalizes attention vectors without adding extra transformation
  matrices, enabling higher pruning ratios and full-rank fine-tuning.
---

# CLOVER: Cross-Layer Orthogonal Vectors Pruning and Fine-Tuning

## Quick Facts
- arXiv ID: 2411.17426
- Source URL: https://arxiv.org/abs/2411.17426
- Authors: Fanxu Meng; Pingzhi Tang; Fan jiang; Muhan Zhang
- Reference count: 34
- Key outcome: CLOVER achieves 10x improvement in pruning efficiency and removes 46.42% of parameters from Whisper without fine-tuning while preserving performance

## Executive Summary
CLOVER introduces a novel method for compressing decoder-only models by treating Q-K and V-O pairs in attention heads as low-rank decompositions. Through SVD, CLOVER orthogonalizes attention vectors without adding extra transformation matrices, enabling higher pruning ratios and full-rank fine-tuning. It outperforms existing methods like LoRA, DoRA, HiRA, and PiSSA in commonsense reasoning tasks while using comparable or fewer parameters.

## Method Summary
CLOVER works by applying SVD to the Q-K and V-O pairs within attention heads, decomposing these matrices into orthogonal bases and singular values. During initialization, CLOVER orthogonalizes attention vectors to eliminate linear redundancy, allowing safe pruning of near-zero singular vectors. For fine-tuning, only the singular values matrix is updated while orthogonal bases remain fixed, enabling full-rank updates with fewer parameters than full fine-tuning. This approach can be combined with any other pruning technique since it only modifies initialization.

## Key Results
- Achieves 10x improvement in pruning efficiency over vanilla methods
- Removes 46.42% of parameters from Whisper without fine-tuning while preserving performance
- Outperforms LoRA, DoRA, HiRA, and PiSSA in commonsense reasoning tasks (BoolQ, PIQA, SIQA, HellaSwag, WinoGrande, ARC-e, ARC-c, OBQA)

## Why This Works (Mechanism)

### Mechanism 1
CLOVER enables higher pruning ratios by eliminating linear redundancy through orthogonal decomposition. It treats Q-K and V-O pairs as low-rank matrices and applies SVD to orthogonalize attention vectors, enabling safe pruning of near-zero vectors. The core assumption is that linear redundancy exists in attention vectors, and orthogonal decomposition can separate meaningful directions from redundant ones.

### Mechanism 2
CLOVER achieves full-rank updates by learning linear combinations of all orthogonal vectors. It fine-tunes only the singular values matrix while keeping orthogonal bases fixed, enabling updates across all directions. The core assumption is that the singular values matrix contains sufficient degrees of freedom to learn linear combinations that span the full rank space.

### Mechanism 3
CLOVER avoids "intrusive dimensions" that degrade model performance. It updates only the vector combinations (singular values) while keeping orthogonal bases fixed, preventing the introduction of new random directions. The core assumption is that the "intrusive dimensions" phenomenon occurs when PEFT methods introduce new random directions that precede original singular vectors.

## Foundational Learning

- **Concept: Singular Value Decomposition (SVD)**
  - Why needed here: SVD is the core mathematical operation that enables CLOVER's orthogonal decomposition and pruning mechanism
  - Quick check question: What is the relationship between the rank of a matrix and the number of non-zero singular values after SVD?

- **Concept: Low-rank matrix approximation**
  - Why needed here: CLOVER treats attention weight matrices as low-rank approximations, which justifies the use of SVD for compression
  - Quick check question: How does treating a matrix as a low-rank approximation enable parameter-efficient fine-tuning?

- **Concept: Linear algebra and vector spaces**
  - Why needed here: Understanding how orthogonal bases span vector spaces is crucial for grasping how CLOVER eliminates redundancy
  - Quick check question: What is the relationship between the number of orthogonal bases and the dimensionality of the vector space they span?

## Architecture Onboarding

- **Component map:**
  - Input layer → Q-K pair → SVD decomposition → singular values matrix → Output layer
  - Input layer → V-O pair → SVD decomposition → singular values matrix → Output layer
  - Orthogonal bases (fixed) ←→ singular values matrix (trainable)

- **Critical path:**
  1. Apply SVD to Q-K and V-O pairs to obtain orthogonal bases and singular values
  2. Prune small singular values and their corresponding vectors
  3. Fine-tune singular values matrix while keeping orthogonal bases fixed
  4. Merge singular values back into orthogonal bases for inference

- **Design tradeoffs:**
  - Higher pruning ratios vs. potential loss of information in discarded vectors
  - Full-rank updates vs. computational overhead of maintaining larger singular values matrix
  - Orthogonal bases stability vs. flexibility in adapting to new tasks

- **Failure signatures:**
  - Performance degradation after pruning indicates insufficient singular values were retained
  - Slow convergence during fine-tuning suggests the singular values matrix lacks sufficient degrees of freedom
  - Catastrophic forgetting indicates "intrusive dimensions" may have been introduced

- **First 3 experiments:**
  1. Apply CLOVER to a small transformer model and verify that SVD successfully orthogonalizes attention vectors
  2. Compare pruning ratios and performance between CLOVER and vanilla pruning on a toy dataset
  3. Fine-tune the singular values matrix on a simple task and verify that full-rank updates are achieved

## Open Questions the Paper Calls Out

### Open Question 1
How does CLOVER's performance scale with model size beyond the tested LLaMA-3-8B, particularly for very large models like GPT-4 or LLaMA-405B? The paper only tested on moderate-sized models (7B-13B range), leaving questions about effectiveness at extreme scales where memory constraints are most severe.

### Open Question 2
What is the theoretical upper bound on pruning ratios achievable with CLOVER, and how does this bound vary across different model architectures? The paper demonstrates significant pruning (up to 46.42% in Whisper) but doesn't establish theoretical limits or provide architecture-specific analysis.

### Open Question 3
How does CLOVER's orthogonalization interact with other compression techniques like quantization, and what is the optimal combination strategy? The paper mentions CLOVER could be combined with quantization but doesn't explore this combination or analyze interaction effects.

### Open Question 4
Does CLOVER maintain its effectiveness when applied to multimodal models beyond LLaMA-3.2-11B-Vision, such as vision-language models or text-to-image generators? The paper only examines a narrow subset of multimodal architectures, leaving questions about generalizability to other cross-modal tasks.

## Limitations
- The paper does not specify exact SVD truncation threshold values used across different model sizes and tasks
- Evaluation focuses primarily on commonsense reasoning tasks, leaving performance on other critical tasks unknown
- Computational overhead of SVD operations during initialization and maintaining singular values matrix is not detailed

## Confidence

- **High Confidence**: CLOVER achieves higher pruning ratios than vanilla methods by eliminating linear redundancy through orthogonal decomposition
- **Medium Confidence**: CLOVER enables full-rank updates during fine-tuning while using fewer parameters than full fine-tuning
- **Low Confidence**: CLOVER avoids "intrusive dimensions" that degrade model performance

## Next Checks
1. Systematically vary the SVD truncation threshold across a range of values and measure the trade-off between pruning ratio and performance degradation on multiple tasks
2. Measure wall-clock time and memory usage for CLOVER initialization and fine-tuning compared to LoRA and full fine-tuning on the same hardware
3. Apply CLOVER to models trained on diverse tasks beyond commonsense reasoning and measure performance retention after pruning