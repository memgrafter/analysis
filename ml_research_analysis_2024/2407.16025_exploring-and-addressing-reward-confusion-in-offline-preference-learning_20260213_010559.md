---
ver: rpa2
title: Exploring and Addressing Reward Confusion in Offline Preference Learning
arxiv_id: '2407.16025'
source_url: https://arxiv.org/abs/2407.16025
tags:
- learning
- reward
- preference
- impec
- rollouts
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses reward confusion in offline RLHF, where reward
  models learn spurious correlations from training data, leading to incorrect policies.
  The authors create a benchmark, Confusing Minigrid (CMG), with six tasks testing
  different types of spurious correlations (extra observations, distributional shifts).
---

# Exploring and Addressing Reward Confusion in Offline Preference Learning

## Quick Facts
- **arXiv ID**: 2407.16025
- **Source URL**: https://arxiv.org/abs/2407.16025
- **Reference count**: 33
- **Key outcome**: IMPEC outperforms standard RLHF and two active learning baselines on most CMG tasks, achieving higher mean returns and lower failure rates.

## Executive Summary
This paper addresses reward confusion in offline RLHF, where reward models learn spurious correlations from training data, leading to incorrect policies. The authors create a benchmark, Confusing Minigrid (CMG), with six tasks testing different types of spurious correlations (extra observations, distributional shifts). They propose IMPEC, an algorithm that uses Bayesian neural networks with information gain as an acquisition function to select rollouts, builds a complete preference ordering using transitivity, and maintains a preference chain to reduce uncertainty. IMPEC outperforms standard RLHF and two active learning baselines on most CMG tasks, achieving higher mean returns and lower failure rates. The method shows promise in mitigating reward confusion by leveraging model uncertainty and preference transitivity.

## Method Summary
IMPEC addresses reward confusion in offline RLHF by using a Bayesian neural network to model reward uncertainty and an information gain acquisition function to select preference queries. Instead of learning from partial pairwise comparisons, IMPEC builds a complete preference ordering through transitivity constraints. The algorithm maintains a preference chain and uses fast-guess insertion based on model uncertainty to reduce the number of expensive human queries needed. This approach aims to prevent the reward model from learning spurious correlations by ensuring global consistency in preferences.

## Key Results
- IMPEC outperforms standard RLHF and two active learning baselines on most CMG tasks
- Achieves higher mean returns and lower failure rates compared to baseline methods
- Shows improved sample efficiency while reducing reward confusion

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Using information gain to select preference queries reduces epistemic uncertainty about the reward function faster than random selection.
- **Mechanism**: The algorithm samples weight distributions from a Bayesian neural network posterior and selects the rollout whose ranking would maximally reduce uncertainty over these weights. This prioritizes queries that resolve the most ambiguous reward predictions.
- **Core assumption**: Reward function uncertainty is well-captured by the variance in Bayesian neural network weights, and reducing this variance leads to better alignment with true human preferences.
- **Evidence anchors**:
  - [abstract]: "It involves two stages of training: First, we use information gain as the acquisition function to select comparison rollouts that reduce uncertainty about the reward function."
  - [section]: "The information gain over network weights θ by selecting a rollout ξ ∈ Ξ for ranking is I(θ; ψTξ | T, ξ) = H(θ|T, ξ) − H(θ|ψTξ , T, ξ)"
  - [corpus]: Weak evidence - related work focuses on preference optimization but doesn't specifically address uncertainty reduction through information gain.
- **Break condition**: If the Bayesian network posterior becomes too peaked or if the information gain estimates become unreliable due to model misspecification.

### Mechanism 2
- **Claim**: Maintaining a complete preference ordering through transitivity constraints reduces reward confusion by preventing spurious correlations from dominating the reward model.
- **Mechanism**: Instead of learning from partial pairwise comparisons, the algorithm builds a full preference chain where transitivity allows deriving additional preference constraints. This constrains the reward function to be consistent with the complete ordering.
- **Core assumption**: Human preferences are transitive, and enforcing this transitivity in the learned reward function prevents the model from exploiting spurious correlations that would violate global consistency.
- **Evidence anchors**:
  - [abstract]: "Second, we form a complete preference ordering over the set of selected rollouts, rather than just a partial ordering as in traditional RLHF."
  - [section]: "Our experiments show that these techniques together improve sample efficiency while reducing reward confusion."
  - [corpus]: Weak evidence - related work mentions preference chains but doesn't demonstrate their effectiveness against reward confusion specifically.
- **Break condition**: If human preferences are actually intransitive in the domain, or if the transitivity constraints become too restrictive and prevent learning the true reward function.

### Mechanism 3
- **Claim**: Using model uncertainty to guide fast insertion reduces the number of expensive human queries needed to maintain the preference chain.
- **Mechanism**: Before querying humans for the exact position of a new rollout, the algorithm uses the Bayesian network's uncertainty estimates to predict where the rollout might belong, then only queries humans for comparisons near that predicted position.
- **Core assumption**: The Bayesian neural network's uncertainty estimates are correlated with its prediction accuracy, allowing it to make useful fast-guesses about rollout rankings.
- **Evidence anchors**:
  - [section]: "we use an interval [ˆRξnew − ϵ, ˆRξnew + ϵ] to fast-guess where ξnew may belong. IMPEC queries human preferences of two pairs (ξnew, ξl) and (ξnew, ξu)."
  - [corpus]: No direct evidence - this specific technique doesn't appear in related work.
- **Break condition**: If the uncertainty estimates become miscalibrated or if the fast-guess interval becomes too wide, requiring nearly full insertion sort.

## Foundational Learning

- **Concept**: Bayesian neural networks for uncertainty quantification
  - Why needed here: The algorithm needs to estimate epistemic uncertainty over reward function parameters to guide active learning
  - Quick check question: How does a Bayesian neural network represent uncertainty differently from a standard neural network?

- **Concept**: Information gain as an acquisition function
  - Why needed here: To select which rollout comparisons will most reduce uncertainty about the reward function
  - Quick check question: What's the mathematical definition of information gain in the context of Bayesian inference?

- **Concept**: Preference transitivity in ranking
  - Why needed here: To derive additional preference constraints from the complete ordering, reducing the number of queries needed
  - Quick check question: If A ≻ B and B ≻ C, what can we infer about A and C under the transitivity assumption?

## Architecture Onboarding

- **Component map**: Bayesian neural network (BNN) reward model -> Information gain acquisition function -> Preference chain data structure -> Fast-guess insertion mechanism -> Transitivity-based preference derivation system

- **Critical path**: 
  1. Train BNN on existing preference data
  2. Use information gain to select next rollout for querying
  3. Use BNN uncertainty to fast-guess rollout position
  4. Query humans for exact position via insertion sort
  5. Update preference chain and derive transitive preferences
  6. Repeat until convergence or budget exhausted

- **Design tradeoffs**:
  - Active learning vs. random sampling: Active learning reduces queries but requires more computation
  - Complete ordering vs. partial ordering: Complete ordering reduces reward confusion but requires more queries
  - BNN uncertainty vs. point estimates: Uncertainty helps guide learning but increases model complexity

- **Failure signatures**:
  - High variance in final policy returns across seeds
  - BNN uncertainty estimates don't correlate with prediction accuracy
  - Preference chain becomes too large to maintain efficiently
  - Transitivity constraints lead to contradictions

- **First 3 experiments**:
  1. Run baseline RLHF with same query budget but random sampling to establish performance floor
  2. Run with information gain but without maintaining complete preference chain to isolate active learning effect
  3. Run with complete preference chain but without active learning to isolate transitivity effect

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does IMPEC's performance scale with different levels of noise in human preference labels?
- Basis in paper: [inferred] The authors mention "Our results suggest a deeper connection between the quality of preference datasets and the efficiency of preference learning algorithms" and note that "IMPEC is its potential sensitivity to noise in preferences."
- Why unresolved: The paper keeps a "relatively low noise level" in experiments but doesn't explore how performance degrades with higher noise levels or what noise thresholds make the method ineffective.
- What evidence would resolve it: Experiments varying the noise level in preference labels across a spectrum from 0% to 50%+ error rates, measuring how this affects IMPEC's performance relative to baseline methods.

### Open Question 2
- Question: What is the theoretical relationship between graph-theoretic properties of preference datasets and learning performance?
- Basis in paper: [explicit] The authors analyze "several graph-theoretic qualities" like clustering coefficient and efficiency, observing that IMPEC has higher clustering coefficient and efficiency than baseline, and note this "raises an interesting question: Is the assumption in network theory, where the distance of nodes influences information efficiency, also applicable to preference learning?"
- Why unresolved: While the paper presents preliminary observations about graph properties, it explicitly states "We do not have matured results establishing connections between data connectivity in RLHF and the training performance yet."
- What evidence would resolve it: A formal theoretical framework linking specific graph metrics (diameter, betweenness centrality, community structure) to convergence rates or final policy performance in preference-based RLHF.

### Open Question 3
- Question: Can the information gain acquisition function be improved by incorporating the model's epistemic uncertainty about preference transitivity?
- Basis in paper: [inferred] The authors use Bayesian neural networks to capture epistemic uncertainty and information gain for selecting rollouts, but their transitivity derivation assumes perfect transitivity once a ranking is established.
- Why unresolved: The paper doesn't investigate whether incorporating uncertainty about transitivity violations (e.g., from noisy preferences or inconsistent human judgments) could improve active selection.
- What evidence would resolve it: Experiments comparing IMPEC's standard information gain to variants that weight information gain by uncertainty about transitivity consistency, measuring whether this reduces failures in cases where human preferences violate strict transitivity.

### Open Question 4
- Question: What is the optimal query budget allocation between pairwise comparisons and insertion sort ranking in IMPEC?
- Basis in paper: [explicit] The authors use 2:1 ratio (150 pairwise comparisons + 300 insertion queries for simpler tasks), noting "IMPEC requires additional queries to precisely rank each sampled rollout within the candidate list" and that "with the limited query budget for human preferences, we first find out the rollout ξ whose ranking ψT ξ on chain T will provide the most information gain."
- Why unresolved: The paper uses fixed allocation ratios but doesn't explore whether different task complexities or dataset characteristics warrant different budget splits.
- What evidence would resolve it: Systematic experiments varying the pairwise:ranking query ratio across tasks, measuring performance and identifying whether optimal allocation depends on task difficulty, dataset size, or presence of spurious correlations.

## Limitations
- The empirical evidence is limited to a single benchmark (CMG) with six tasks
- Scalability to more complex environments and real-world preference learning scenarios remains uncertain
- The paper doesn't adequately address potential computational costs of maintaining complete preference chains

## Confidence
- **High Confidence**: The theoretical framework for using Bayesian neural networks and information gain is well-established in active learning literature. The concept of reward confusion in offline RLHF is also well-documented.
- **Medium Confidence**: The specific implementation of IMPEC shows promising results on the CMG benchmark, but generalization to other domains needs validation.
- **Low Confidence**: The claim that maintaining complete preference orderings significantly reduces reward confusion compared to partial orderings needs more extensive empirical validation across diverse task distributions.

## Next Checks
1. **Generalization Test**: Evaluate IMPEC on additional RLHF benchmarks beyond CMG, particularly those with more naturalistic spurious correlations and longer horizon tasks.
2. **Ablation Study**: Systematically test the contribution of each component (information gain, complete ordering, fast-guess insertion) by removing them individually to quantify their marginal impact on reward confusion.
3. **Computational Analysis**: Measure the wall-clock time and computational resources required for IMPEC compared to standard RLHF across different dataset sizes and model complexities.