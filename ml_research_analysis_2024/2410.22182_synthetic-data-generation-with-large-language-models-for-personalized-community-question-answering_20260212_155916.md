---
ver: rpa2
title: Synthetic Data Generation with Large Language Models for Personalized Community
  Question Answering
arxiv_id: '2410.22182'
source_url: https://arxiv.org/abs/2410.22182
tags:
- data
- synthetic
- answers
- personalized
- question
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates the use of large language models (LLMs)
  to generate synthetic data for personalized community question answering. The authors
  propose a new dataset, Sy-SE-PQA, based on the SE-PQA dataset, which contains questions
  and answers from StackExchange communities.
---

# Synthetic Data Generation with Large Language Models for Personalized Community Question Answering

## Quick Facts
- **arXiv ID**: 2410.22182
- **Source URL**: https://arxiv.org/abs/2410.22182
- **Reference count**: 40
- **Primary result**: Synthetic data generated by LLMs can achieve comparable or better performance than human-written training data for personalized community question answering, with up to 24% improvement in P@1.

## Executive Summary
This paper investigates using large language models to generate synthetic data for personalized community question answering (cQA). The authors create Sy-SE-PQA by generating synthetic answers using GPT-3.5 and Phi-3 models with different prompt techniques (basic, personalized, contextual) based on the SE-PQA dataset from StackExchange. They fine-tune DistilBERT on this synthetic data and evaluate against human-annotated test sets. Results show that models trained on synthetic data can match or exceed performance of those trained on human-written data, with improvements up to 24% in P@1 and 18% in NDCG@10. However, the study finds that over 35% of generated answers contain hallucinations, highlighting the need for continued refinement of LLMs.

## Method Summary
The authors generate synthetic answers for the SE-PQA dataset using three different prompt strategies: basic, personalized (incorporating user interests via tags), and contextual. They employ GPT-3.5, Phi-3-mini, and Phi-3-medium to create synthetic answers for questions from StackExchange communities. The synthetic data is then used to fine-tune DistilBERT using triplet margin loss, with BM25 serving as a first-stage ranker. The combined BM25+DistilBERT model is evaluated on a human-annotated test set using standard IR metrics (P@1, NDCG@3, NDCG@10, MAP@100).

## Key Results
- Models trained on synthetic data achieve comparable or better performance than those trained on human-written data, with improvements up to 24% in P@1 and 18% in NDCG@10
- More than 35% of generated answers contain hallucinations, raising concerns about reliability
- Personalized and contextual prompts lead to better retrieval performance than basic prompts
- Higher lexical overlap between questions and synthetic answers does not guarantee better IR performance

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Synthetic answers generated by LLMs can replace human-written training data while preserving or improving IR model performance.
- Mechanism: Fine-tuning DistilBERT on synthetic answers enables the model to learn representations that align well with human-annotated relevance judgments, even though the synthetic answers contain hallucinations.
- Core assumption: The neural re-ranker can generalize from synthetic data to human-written test data if the synthetic answers preserve relevant semantic patterns.
- Evidence anchors:
  - [abstract] "The synthetic data can replace human-written training data, even if the generated data may contain incorrect information."
  - [section] "models trained on synthetic data can achieve comparable or even better performance than models trained on human-written data, with improvements of up to 24% in P@1 and 18% in NDCG@10."
  - [corpus] Found related work on synthetic data generation for IR tasks, but no direct comparison of hallucination-impact on IR model training (weak).
- Break condition: If synthetic answers diverge too far from human relevance patterns or hallucination rate exceeds ~50%, performance degrades.

### Mechanism 2
- Claim: Personalization and contextual prompts lead to better retrieval performance than basic prompts.
- Mechanism: Incorporating user interests (via tags) or community context into the prompt guides the LLM to generate answers that are more aligned with user intent, which improves downstream IR relevance modeling.
- Core assumption: The additional prompt context is encoded in the synthetic answers in a way that the fine-tuned IR model can exploit.
- Evidence anchors:
  - [section] "training the model on Personalized answers generated by both Phi mini and medium is able to improve remarkably over the model trained on the real answer."
  - [section] "Even without adding contextual or personal information into the prompt, the model trained on Basic synthetic data achieves similar performances to the model trained on Real Answers."
  - [corpus] No direct evidence in neighbors about prompt-context effect on IR; only general synthetic data studies (weak).
- Break condition: If personalization or context does not improve BLEU/chrF diversity scores, it likely will not improve IR performance.

### Mechanism 3
- Claim: Higher lexical overlap between questions and synthetic answers does not guarantee better IR performance.
- Mechanism: LLMs tend to repeat question terms in answers, inflating lexical overlap metrics, but IR models evaluate based on semantic relevance captured in embeddings rather than exact token overlap.
- Core assumption: DistilBERT learns semantic similarity from synthetic data regardless of lexical overlap.
- Evidence anchors:
  - [section] "We suspect that this higher lexical overlap compared to the human answers happens because LLMs often repeat the question or query in the response."
  - [section] "It is worth noting that lexical overlap is not the best indicator of answer quality for fine-tuning effective encoders."
  - [corpus] No direct evidence in neighbors about lexical overlap vs IR performance (weak).
- Break condition: If IR performance strongly correlates with lexical overlap, this mechanism is invalid.

## Foundational Learning

- Concept: Personalized Information Retrieval (PIR) and re-ranking architecture.
  - Why needed here: The paper evaluates personalized cQA using a two-stage BM25 + DistilBERT re-ranker; understanding PIR is essential to interpret results.
  - Quick check question: What is the difference between query expansion and results re-ranking in PIR?

- Concept: Large Language Models (LLMs) and hallucination.
  - Why needed here: Synthetic data is generated by LLMs, and the paper explicitly measures hallucination rates; understanding LLM limitations is key.
  - Quick check question: What is a hallucination in LLM outputs, and why is it problematic for IR tasks?

- Concept: Evaluation metrics in IR (P@1, NDCG@3, NDCG@10, MAP@100).
  - Why needed here: These metrics are used to compare models trained on synthetic vs human data; knowing what they measure is critical.
  - Quick check question: How does NDCG@10 differ from P@1 in evaluating retrieval quality?

## Architecture Onboarding

- Component map: SE-PQA dataset -> LLM prompt templates (Basic, Personalized, Contextual) -> synthetic answers -> Triplet margin loss fine-tuning -> DistilBERT re-ranker -> BM25 first-stage retriever -> Combined BM25 + DistilBERT scoring -> final ranking -> Human-annotated test set -> evaluation

- Critical path: Data generation -> fine-tune DistilBERT -> re-rank BM25 output -> evaluate on human test set

- Design tradeoffs:
  - More complex prompts increase answer diversity but also hallucination risk.
  - Higher synthetic data volume improves model robustness but increases computational cost.
  - Using only in-batch negatives simplifies training but may limit negative sampling quality.

- Failure signatures:
  - Low IR performance despite high BLEU/chrF -> synthetic answers not semantically aligned.
  - High hallucination rate (>40%) -> prompts need refinement or RAG integration.
  - Overfitting to synthetic data -> poor generalization to human test set.

- First 3 experiments:
  1. Generate synthetic answers with all three prompt types using Phi-mini; fine-tune DistilBERT; evaluate on SE-PQA test set.
  2. Compare IR performance of models trained on synthetic vs human-written answers.
  3. Measure BLEU/chrF diversity between prompt types and analyze correlation with IR performance.

## Open Questions the Paper Calls Out
- The paper does not explicitly call out open questions beyond noting the need for continued refinement of LLMs to ensure accuracy and reliability given the high hallucination rates.

## Limitations
- Significant hallucination rates (>35%) in generated answers challenge reliability of the approach
- Lack of detailed specifications for LLM configuration parameters (temperature, max tokens)
- Manual evaluation process for detecting hallucinations not fully specified
- Results rely on a single dataset without cross-validation across different domains

## Confidence
- **High**: Synthetic data can achieve comparable IR performance to human-written data (supported by quantitative results)
- **Medium**: Personalization prompts improve performance (supported by results but lacking ablation studies)
- **Low**: Lexical overlap is not indicative of IR performance (based on single observation without statistical validation)

## Next Checks
1. Conduct a comprehensive statistical analysis correlating hallucination rates with IR performance degradation across different prompt types
2. Implement cross-dataset validation using SE-PQA plus at least one additional personalized cQA dataset to test generalizability
3. Design controlled experiments varying LLM generation parameters (temperature, max tokens) to optimize the hallucination-IR performance tradeoff