---
ver: rpa2
title: Convergence of Sharpness-Aware Minimization Algorithms using Increasing Batch
  Size and Decaying Learning Rate
arxiv_id: '2409.09984'
source_url: https://arxiv.org/abs/2409.09984
tags:
- gsam
- learning
- batch
- rate
- increasing
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper analyzes the convergence of the gap-guided sharpness-aware
  minimization (GSAM) algorithm under increasing batch sizes and decaying learning
  rates. GSAM aims to find flat local minima of the empirical loss function, which
  is believed to improve generalization in deep neural networks.
---

# Convergence of Sharpness-Aware Minimization Algorithms using Increasing Batch Size and Decaying Learning Rate

## Quick Facts
- arXiv ID: 2409.09984
- Source URL: https://arxiv.org/abs/2409.09984
- Reference count: 40
- Key outcome: GSAM with increasing batch sizes or decaying learning rates achieves ϵ-approximation of SAM problem and finds flatter minima with better generalization

## Executive Summary
This paper analyzes the convergence of the gap-guided sharpness-aware minimization (GSAM) algorithm under increasing batch sizes and decaying learning rates. GSAM aims to find flat local minima of the empirical loss function, which is believed to improve generalization in deep neural networks. The authors provide theoretical bounds on the search direction noise between GSAM and gradient descent, showing that this noise decreases as batch size increases or learning rate decays. They prove that GSAM with increasing batch sizes and decaying learning rates achieves ϵ-approximation of the SAM problem.

## Method Summary
The paper analyzes GSAM algorithm convergence by examining how search direction noise changes with batch size and learning rate. GSAM uses mini-batch gradients to solve the SAM problem, which finds flat local minima by considering perturbations around each point. The authors prove theoretical bounds showing that search direction noise decreases with larger batch sizes (Theorem 2.1, 2.2) and smaller learning rates. These results extend to prove that GSAM with increasing batch sizes (Theorem 2.3) and decaying learning rates (Theorem 2.4) achieves ϵ-approximation of the SAM problem. Numerical experiments on CIFAR100 using ResNet-18, Wide-ResNet-28-10, and ViT-Tiny models demonstrate that GSAM with increasing batch size or cosine-annealing learning rate finds flatter local minima than using constant batch size and learning rate.

## Key Results
- GSAM with increasing batch sizes or decaying learning rates achieves ϵ-approximation of the SAM problem
- Search direction noise between GSAM and gradient descent decreases as batch size increases or learning rate decays
- Numerical experiments show GSAM with varying batch size/learning rate finds flatter minima with better generalization than constant configurations

## Why This Works (Mechanism)

### Mechanism 1
- Claim: GSAM with increasing batch sizes or decaying learning rates achieves ϵ-approximation of the SAM problem.
- Mechanism: As batch size increases or learning rate decays, the search direction noise between GSAM and GD decreases, causing GSAM to behave more like GD and converge to flatter minima.
- Core assumption: The norm of the search direction noise is inversely proportional to the batch size and directly proportional to the learning rate.
- Evidence anchors:
  - [abstract] "The authors provide theoretical bounds on the search direction noise between GSAM and gradient descent, showing that this noise decreases as batch size increases or learning rate decays."
  - [section] "Equation (10) indicates that the full gradient ∇ˆf SAM S,ρ(x0) substantially differs from ∇ˆf SAM S0,ρ(x0) with a small batch size b0 or a large learning rate η0."
  - [corpus] "Weak or missing" - no direct corpus evidence for this specific mechanism.
- Break condition: If the noise norm doesn't decrease with larger batch sizes or smaller learning rates, the approximation guarantee fails.

### Mechanism 2
- Claim: Increasing batch sizes and decaying learning rates help GSAM find flatter local minima than constant batch size and learning rate.
- Mechanism: Larger batch sizes and smaller learning rates reduce the search direction noise, causing GSAM to converge to flatter minima with better generalization performance.
- Core assumption: Flatter minima found by GSAM correspond to better generalization performance.
- Evidence anchors:
  - [abstract] "Numerical experiments on CIFAR100 dataset using ResNet-18, Wide-ResNet-28-10, and ViT-Tiny models demonstrate that GSAM with increasing batch size or cosine-annealing learning rate finds flatter local minima than using constant batch size and learning rate, resulting in better generalization performance."
  - [section] "Table 2 summarizes the mean values of the test errors and the worst-case ℓ∞ adaptive sharpness... SAM+B had the highest test accuracy and the lowest sharpness, which implies that SAM+B approximated a flatter local minimum."
  - [corpus] "Weak or missing" - no direct corpus evidence for this specific mechanism.
- Break condition: If flatter minima don't correspond to better generalization, or if the noise reduction doesn't lead to flatter minima.

### Mechanism 3
- Claim: The search direction noise is approximately zero when batch size equals n and α is small.
- Mechanism: When using the full batch (bt = n) and small α, the stochastic gradient becomes the true gradient, eliminating the search direction noise.
- Core assumption: The perturbation amplitude ρ is small enough to make the approximation in equation (3) valid.
- Evidence anchors:
  - [section] "From the definition of the perturbed empirical loss fS,ρ, the SAM problem is specialized to finding flat local minima of the empirical loss fS, which may lead to a better generalization capability than finding sharp minima."
  - [section] "Let us consider the case of GSAM with bt = n and α ≠ 0, we have that ηtωt = ηt(∇ˆf SAM S,ρ(xt)−∇ˆf SAM S,ρ(xt) + α∇fS⊥(xt)) = ηtα∇fS⊥(xt). Hence, an upper bound of E[ηt∥ωt∥2] is ηt|α|G⊥(Theorem 2.1 (bt = n))."
  - [corpus] "Weak or missing" - no direct corpus evidence for this specific mechanism.
- Break condition: If the perturbation amplitude ρ is too large, the approximation breaks down and the noise doesn't become zero.

## Foundational Learning

- Concept: Sharpness-aware minimization (SAM) problem
  - Why needed here: GSAM is a variant of SAM, and understanding SAM is crucial for understanding GSAM's objectives and convergence properties.
  - Quick check question: What is the main goal of the SAM problem, and how does it differ from standard empirical risk minimization?

- Concept: Mini-batch gradient descent and its variance
  - Why needed here: GSAM uses mini-batch gradients, and understanding their variance properties is essential for analyzing the search direction noise.
  - Quick check question: How does the variance of a mini-batch gradient scale with the batch size, and why is this important for GSAM?

- Concept: Lyapunov functions and convergence analysis
  - Why needed here: The convergence proofs for GSAM use Lyapunov functions to establish the approximation guarantee.
  - Quick check question: What is a Lyapunov function, and how is it used to prove convergence in optimization algorithms?

## Architecture Onboarding

- Component map: GSAM algorithm -> Increasing batch size scheduler or Decaying learning rate scheduler -> Flatter local minima -> Better generalization performance

- Critical path: GSAM algorithm → Increasing batch size or decaying learning rate scheduler → Flatter local minima → Better generalization performance

- Design tradeoffs:
  - Larger batch sizes lead to lower noise but higher computational cost
  - Smaller learning rates lead to lower noise but slower convergence
  - Choice of scheduler (increasing batch size vs. decaying learning rate) affects convergence speed and final performance

- Failure signatures:
  - If GSAM doesn't converge to flatter minima, check if batch size is increasing sufficiently or learning rate is decaying appropriately
  - If generalization performance doesn't improve, check if the loss function is properly minimizing sharpness
  - If convergence is too slow, check if learning rate is too small or batch size is increasing too slowly

- First 3 experiments:
  1. Implement GSAM with constant batch size and learning rate, compare performance with standard SGD
  2. Implement GSAM with increasing batch size scheduler, compare performance with constant batch size
  3. Implement GSAM with decaying learning rate scheduler, compare performance with constant learning rate

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do different increasing batch size schedules (linear, exponential, polynomial) compare in their effectiveness at finding flatter minima?
- Basis in paper: [inferred] The paper demonstrates benefits of using an increasing batch size schedule, but only tests a specific doubling pattern.
- Why unresolved: The paper only explores one specific increasing batch size schedule (doubling every 40 epochs). The impact of different scheduling strategies remains untested.
- What evidence would resolve it: Systematic comparison of different increasing batch size schedules (linear, exponential, polynomial) on the same models and datasets, measuring both convergence speed and generalization performance.

### Open Question 2
- Question: How does the effectiveness of increasing batch size versus decaying learning rate vary across different model architectures and dataset sizes?
- Basis in paper: [explicit] The paper notes that their experiments were limited to specific models (ResNet-18, Wide-ResNet-28-10, ViT-Tiny) and one dataset (CIFAR100).
- Why unresolved: The paper acknowledges its limited experimental scope and calls for broader testing across more models and datasets.
- What evidence would resolve it: Experiments testing both increasing batch size and decaying learning rate strategies across a diverse range of model architectures (CNNs, transformers, etc.) and dataset sizes, comparing their relative effectiveness.

### Open Question 3
- Question: What is the theoretical relationship between the search direction noise bounds and the actual generalization performance in practice?
- Basis in paper: [inferred] The paper provides theoretical bounds on search direction noise and shows empirical improvements in generalization, but doesn't establish a direct theoretical link.
- Why unresolved: While the paper proves convergence properties and shows empirical generalization improvements, it doesn't establish a theoretical connection between the noise bounds and generalization performance.
- What evidence would resolve it: A theoretical framework connecting the search direction noise bounds to generalization bounds, supported by empirical validation across different scenarios.

## Limitations

- Limited experimental scope to one dataset (CIFAR100) and three model architectures
- Theoretical analysis assumes small perturbation amplitude ρ, which may not hold in practice
- Hyperparameter sensitivity to α and ρ values not fully explored across different tasks

## Confidence

- Theoretical convergence analysis: Medium
- Empirical validation results: Medium
- Generalization of findings to other architectures/datasets: Low

## Next Checks

1. Replicate the theoretical analysis with different perturbation amplitudes ρ to verify the robustness of the convergence bounds
2. Test GSAM with increasing batch sizes and decaying learning rates on additional datasets (e.g., CIFAR10, ImageNet) and architectures to assess generalizability
3. Compare the computational efficiency of GSAM with varying schedules against standard training methods to quantify the practical benefits beyond improved generalization