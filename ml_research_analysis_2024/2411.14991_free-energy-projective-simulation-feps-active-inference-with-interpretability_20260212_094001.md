---
ver: rpa2
title: 'Free Energy Projective Simulation (FEPS): Active inference with interpretability'
arxiv_id: '2411.14991'
source_url: https://arxiv.org/abs/2411.14991
tags:
- agent
- belief
- state
- states
- environment
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Free Energy Projective Simulation (FEPS),
  an interpretable active inference model that combines Projective Simulation with
  the free energy principle to create agents that learn without external rewards.
  FEPS agents build a world model using internal rewards based on prediction accuracy,
  then derive policies by minimizing expected free energy.
---

# Free Energy Projective Simulation (FEPS): Active inference with interpretability

## Quick Facts
- arXiv ID: 2411.14991
- Source URL: https://arxiv.org/abs/2411.14991
- Authors: JosÃ©phine Pazem; Marius Krumm; Alexander Q. Vining; Lukas J. Fiderer; Hans J. Briegel
- Reference count: 40
- Key outcome: FEPS agents successfully learn to navigate partially observable environments without external rewards, achieving near-optimal performance through internal prediction-based learning

## Executive Summary
This paper introduces Free Energy Projective Simulation (FEPS), an interpretable active inference model that combines Projective Simulation with the free energy principle. FEPS agents build world models using internal rewards based on prediction accuracy, then derive policies by minimizing expected free energy. The model uses a clone-structured architecture where belief states are represented as graph vertices, allowing for disambiguation of ambiguous observations through superposition techniques. The approach is validated on two biologically-inspired tasks: a timed response task and a navigation task in a partially observable grid world.

## Method Summary
FEPS implements active inference using a Projective Simulation framework where agents learn world models through a wandering phase without task-specific rewards. During this phase, agents explore their environment and build a transition graph representing belief states as vertices connected by edges with h-values (strengths) and confidence attributes. The world model is constructed using clone clips - multiple representations of the same observation state that enable disambiguation through superposition when observations are ambiguous. After the wandering phase, agents compute look-ahead preference distributions to perform specific tasks by minimizing expected free energy, which balances prediction accuracy with information gain about belief states.

## Key Results
- FEPS agents achieved near-optimal performance in the timed response task, with average trajectory lengths approaching the theoretical maximum of 15 time steps
- In the navigation task, agents successfully mapped belief states to specific grid locations and adapted their policies to different target observations without additional environmental interaction
- The wandering phase proved crucial for building accurate world models, enabling longer prediction horizons and more flexible task performance compared to agents without this exploratory learning phase

## Why This Works (Mechanism)
FEPS works by combining the interpretability of Projective Simulation's graph-based representations with the predictive power of active inference's free energy minimization. The clone-structured world model allows agents to maintain multiple hypotheses about their current state when observations are ambiguous, resolving uncertainty through active exploration. By using internal rewards based on prediction accuracy rather than external task rewards, FEPS agents develop generalizable world models that can be flexibly applied to different tasks. The look-ahead preference distribution calculation enables task-specific behavior while maintaining the agent's ability to explore and learn from prediction errors.

## Foundational Learning
- **Projective Simulation**: A reinforcement learning framework using episodic memory represented as a directed graph where vertices are situations and edges are actions with associated strengths (h-values). Why needed: Provides interpretable, graph-based representations that can be combined with active inference principles. Quick check: Can you describe how h-values are updated during learning?
- **Active Inference**: A framework where agents minimize free energy to maintain homeostasis and achieve goals. Why needed: Provides the theoretical foundation for prediction-based learning and policy optimization. Quick check: What is the relationship between variational free energy and prediction accuracy?
- **Belief State Estimation in Superposition**: Technique for handling ambiguous observations by maintaining multiple candidate belief states simultaneously. Why needed: Enables agents to disambiguate uncertain situations through active exploration. Quick check: How does superposition differ from simple probability distributions over states?
- **Wandering Phase**: Initial exploration phase where agents learn without task-specific rewards, building a world model based on prediction accuracy. Why needed: Allows agents to develop generalizable models before task-specific optimization. Quick check: What happens to EFE during the wandering phase when using marginal preference distributions?
- **Expected Free Energy (EFE)**: Objective function balancing prediction accuracy with information gain about hidden states. Why needed: Guides policy selection by quantifying the trade-off between exploitation and exploration. Quick check: How does EFE decompose into pragmatic and epistemic value components?
- **Clone Clips**: Multiple representations of the same observation state used to enable disambiguation. Why needed: Provides the structural basis for belief state superposition and disambiguation. Quick check: Why might too many clones per observation be problematic for training?

## Architecture Onboarding
**Component Map**: World Model (transition graph + emission graph) -> Policy Graph -> Belief State Estimation -> Action Selection -> Environment
**Critical Path**: Wandering Phase (build world model) -> Look-ahead Preference Distribution Calculation -> EFE Minimization -> Policy Derivation -> Task Performance
**Design Tradeoffs**: The clone-structured architecture provides interpretability and disambiguation capabilities but scales poorly with state space size. The wandering phase requirement adds training time but enables more flexible task performance. The superposition technique handles ambiguity but increases computational complexity during belief state estimation.
**Failure Signatures**: Poor prediction accuracy indicates inadequate world model building during wandering phase. Short trajectory lengths suggest the agent is not effectively disambiguating observations. High variational free energy persistence indicates the agent is not converging to optimal policies.
**First Experiments**:
1. Implement a minimal FEPS agent on a simple two-state environment to verify the basic learning loop
2. Test belief state disambiguation with ambiguous observations in a controlled setting
3. Compare performance with and without the wandering phase on a simple navigation task

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions, but several emerge from the analysis:

### Open Question 1
- Question: How would the FEPS architecture perform in non-deterministic environments where multiple transitions are possible from the same belief state?
- Basis in paper: [inferred] The paper extensively discusses deterministic environments and derives asymptotic limits for expected free energy in such cases, suggesting the model may not be fully tested in stochastic settings.
- Why unresolved: The current mathematical derivations and simulations focus on deterministic transitions, and the authors note this is a limitation when deriving EFE limits. The model's behavior with probabilistic transitions remains unexplored.
- What evidence would resolve it: Testing FEPS agents in environments with stochastic transitions and comparing their learning curves and performance metrics to those in deterministic settings would provide empirical evidence.

### Open Question 2
- Question: What is the optimal number of clone clips per observation for balancing model complexity and learning efficiency?
- Basis in paper: [explicit] The paper tests with 2 clones for the timed response task and 3 clones for the navigation task, but does not systematically explore the trade-offs of different clone numbers.
- Why unresolved: The choice of clone numbers appears arbitrary in the current experiments, and the authors note that providing too many clones can lead to redundancy issues that complicate training.
- What evidence would resolve it: Systematic experiments varying the number of clones per observation across multiple environment types and measuring learning speed, final model accuracy, and computational efficiency would identify optimal configurations.

### Open Question 3
- Question: How does the wandering phase's effectiveness change with different preference distributions that don't match the marginal of the world model?
- Basis in paper: [explicit] The paper derives that using the marginal of the world model over actions makes the EFE equal to information gain about belief states, but doesn't explore alternative preference distributions.
- Why unresolved: The authors only test one specific preference distribution during wandering and derive its theoretical properties, leaving open whether other distributions might yield better exploration or faster learning.
- What evidence would resolve it: Testing multiple alternative preference distributions during the wandering phase and comparing the resulting learning curves, model accuracy, and trajectory lengths would reveal whether the marginal distribution is optimal.

## Limitations
- Computational complexity scales poorly with state and action space size due to the clone-structured architecture, making large-scale applications challenging
- The wandering phase requirement adds an additional training step that may limit practical applicability in fast-changing environments
- The model is currently tested only in deterministic environments, with performance in stochastic settings unexplored

## Confidence
- **High confidence**: The core mathematical formulation of FEPS combining free energy minimization with Projective Simulation is well-established and internally consistent
- **Medium confidence**: The experimental results on the two tested tasks, as the sample sizes are relatively small (100 and 30 agents) and the tasks are highly controlled
- **Low confidence**: The generalizability of FEPS to more complex, real-world environments with larger state spaces and more diverse action sets

## Next Checks
1. Test FEPS on a larger navigation task with increased grid size (e.g., 20x20) and more complex obstacle configurations to assess scalability
2. Implement a comparative study against standard active inference approaches without the Projective Simulation components to isolate the benefits of the combined architecture
3. Evaluate the sensitivity of FEPS performance to the wandering phase duration and look-ahead preference parameters across different task complexities