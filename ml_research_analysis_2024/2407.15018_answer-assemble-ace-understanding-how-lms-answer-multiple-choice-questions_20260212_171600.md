---
ver: rpa2
title: 'Answer, Assemble, Ace: Understanding How LMs Answer Multiple Choice Questions'
arxiv_id: '2407.15018'
source_url: https://arxiv.org/abs/2407.15018
tags:
- answer
- layer
- https
- conference
- instruct
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates how transformer language models perform
  formatted multiple-choice question answering (MCQA), a common evaluation format
  in LLM benchmarks. The authors use vocabulary projection and activation patching
  methods to analyze the internal mechanisms of three model families (Olmo, Llama,
  and Qwen) across three datasets (MMLU, HellaSwag, and a synthetic copying task).
---

# Answer, Assemble, Ace: Understanding How LMs Answer Multiple Choice Questions

## Quick Facts
- arXiv ID: 2407.15018
- Source URL: https://arxiv.org/abs/2407.15018
- Authors: Sarah Wiegreffe; Oyvind Tafjord; Yonatan Belinkov; Hannaneh Hajishirzi; Ashish Sabharwal
- Reference count: 40
- Key outcome: Transformer language models use sparse network components, particularly attention heads in middle layers, to perform formatted multiple-choice question answering (MCQA)

## Executive Summary
This paper investigates how transformer language models perform formatted multiple-choice question answering (MCQA) using vocabulary projection and activation patching methods. The authors analyze three model families (Olmo, Llama, and Qwen) across three datasets (MMLU, HellaSwag, and a synthetic copying task) to understand the internal mechanisms of answer symbol prediction. They find that answer selection is driven by sparse network components, particularly attention heads in middle layers, and identify complex two-stage processing for unusual answer symbols. A synthetic task is introduced to disentangle formatted MCQA performance from dataset-specific knowledge and pinpoint when models learn this skill during training.

## Method Summary
The paper employs vocabulary projection to inspect when and how models build up answer symbol predictions in vocabulary space by projecting hidden states to the unembedding matrix. Activation patching is used to causally attribute answer symbol prediction to specific layers and attention heads by replacing hidden states from instances with different answer formats. A synthetic "Colors" copying task isolates the symbol binding component by providing correct answers in context, forcing models to only perform symbol binding rather than reasoning. The study analyzes three model families (Olmo, Llama, Qwen) across three datasets (MMLU, HellaSwag, and the synthetic task) to understand how transformers build up answer symbol predictions through their internal representations.

## Key Results
- Answer symbol prediction is driven by sparse network components, particularly attention heads in middle layers
- Middle layers encode relevant information for answer selection while later layers increase probability of predicted answer symbols
- Some models show complex two-stage processing for unusual answer symbols (initially considering A/B/C/D before switching to actual prompt symbols)
- A synthetic task can disentangle formatted MCQA performance from dataset-specific knowledge and pinpoint when models learn this skill during training

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Answer symbol prediction is driven by sparse network components, particularly attention heads in middle layers
- Mechanism: Middle layers (around layer 24 for Olmo 7B) encode the information needed to predict the correct answer symbol, while subsequent layers increase the probability of that symbol in vocabulary space through sparse attention heads
- Core assumption: The answer selection process can be localized to specific layers and components rather than being distributed throughout the network
- Evidence anchors:
  - [abstract] "answer symbol prediction is driven by sparse network components, particularly attention heads"
  - [section] "The prediction of a specific answer symbol is causally attributed to a few middle layers, and specifically their multi-head self-attention mechanisms"
  - [corpus] Weak - related papers focus on MCQA robustness but not internal mechanisms
- Break condition: If experiments show that answer symbol prediction requires broad network activation across many layers, this mechanism would fail

### Mechanism 2
- Claim: Models sometimes use a two-stage processing for unusual answer symbols
- Mechanism: When presented with non-standard answer symbols (like Q/Z/R/X), models initially assign high logit values to expected symbols (A/B/C/D) before switching to the correct symbols in later layers
- Core assumption: Models have learned expectations about answer symbol formats from training data
- Evidence anchors:
  - [abstract] "some models show complex two-stage processing for unusual answer symbols (e.g., initially assigning high values to expected symbols like A/B/C/D before switching to actual prompt symbols)"
  - [section] "Olmo and Qwen models first assign non-negligible probability to labels that are more likely or expected (A, B, C, D) even though they are not included in the prompt, before making an abrupt switch to the correct symbols"
  - [corpus] Weak - corpus doesn't provide direct evidence for this specific two-stage mechanism
- Break condition: If models consistently predict unusual symbols correctly without first considering standard symbols, this mechanism would be invalid

### Mechanism 3
- Claim: A synthetic task can disentangle formatted MCQA performance from dataset-specific knowledge
- Mechanism: The "Colors" copying task isolates the symbol binding component by providing the correct answer in context, forcing the model to only perform symbol binding rather than reasoning
- Core assumption: Performance on a simple symbol binding task reflects the model's ability to perform formatted MCQA regardless of domain knowledge
- Evidence anchors:
  - [abstract] "a synthetic task can disentangle formatted MCQA performance from dataset-specific performance, and allows us to narrow down a point during training at which Olmo 7B learns formatted MCQA"
  - [section] "The dataset consists of instances such as x=Corn is , y=yellow. We include y in the context, so the model's only task is to produce the symbol associated with that answer choice"
  - [corpus] Weak - related papers don't discuss synthetic tasks for isolating MCQA skills
- Break condition: If performance on the synthetic task doesn't correlate with formatted MCQA performance on real datasets, this mechanism would fail

## Foundational Learning

- Concept: Transformer architecture with residual connections
  - Why needed here: The paper relies on understanding how information flows through transformer layers and how residual connections enable iterative refinement of predictions
  - Quick check question: How do residual connections affect the propagation of information through transformer layers?

- Concept: Attention mechanisms and multi-head self-attention
  - Why needed here: The paper specifically identifies attention heads as the sparse components responsible for answer symbol prediction
  - Quick check question: What is the difference between single-head and multi-head attention in transformers?

- Concept: Vocabulary projection and logit space
  - Why needed here: The paper uses vocabulary projection to understand how models build up predictions in the vocabulary space defined by the unembedding matrix
  - Quick check question: How does vocabulary projection help interpret what transformers are "thinking" at intermediate layers?

## Architecture Onboarding

- Component map: Input -> Embedding -> Alternating layers of MHSA and MLP (with residual connections and layer normalization) -> Final layer output -> Unembedding matrix projection to vocabulary space -> Logits
- Critical path: For MCQA, the critical path involves: (1) encoding the question and answer choices in middle layers (around layer 24), (2) promoting the correct answer symbol through sparse attention heads in subsequent layers, and (3) refining the prediction in the final layers
- Design tradeoffs: The model balances between encoding sufficient information for answer selection in middle layers versus refining predictions in later layers. Using sparse attention heads for promotion allows for efficient computation but may limit flexibility
- Failure signatures: If models show inconsistent performance across different answer choice orders or symbols, this indicates issues with the symbol binding mechanism. Poor performance on the synthetic task despite good performance on reasoning tasks suggests the model hasn't learned the MCQA format
- First 3 experiments:
  1. Run vocabulary projection on a correctly answering model to identify which layers show increased logit values for the correct answer symbol
  2. Perform activation patching from a correctly formatted prompt to a differently formatted one to identify which layers causally affect answer selection
  3. Test the same model on the synthetic copying task to determine if it can perform symbol binding independent of domain knowledge

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do different instruction-tuning strategies affect the emergence of symbol binding capabilities in language models, and can we predict which models will develop robust MCQA performance based on their training objectives?
- Basis in paper: [explicit] The paper mentions that instruction-tuned models perform well on formatted MCQA tasks and observes differences between base and instruction-tuned versions of the same models (Olmo 0724 7B vs Olmo 0724 7B Instruct)
- Why unresolved: The paper doesn't directly compare instruction-tuned vs base models beyond noting performance differences, nor does it analyze how different instruction-tuning approaches might lead to different MCQA capabilities
- What evidence would resolve it: Systematic comparison of multiple models with different instruction-tuning strategies, analyzing their MCQA performance and internal mechanisms, could reveal patterns linking training objectives to symbol binding capabilities

### Open Question 2
- Question: What specific attention head configurations are most critical for successful symbol binding across different model architectures, and can we design more efficient architectures that optimize these components?
- Basis in paper: [explicit] The paper identifies sparse sets of attention heads responsible for promoting answer symbols and notes that MHSA mechanisms dominate over MLPs in this process, with 1-4 attention heads per layer being particularly important
- Why unresolved: While the paper identifies key attention heads, it doesn't explore whether these configurations are universal across architectures or how they might be optimized for better performance
- What evidence would resolve it: Detailed analysis of attention head configurations across multiple model families, followed by architectural modifications targeting these components, could reveal optimal configurations for symbol binding

### Open Question 3
- Question: How does the ability to handle unusual answer choice symbols (Q/Z/R/X, 1/2/3/4) develop during training, and what specific training interventions could accelerate this capability?
- Basis in paper: [explicit] The paper observes that some models initially assign high values to expected symbols (A/B/C/D) before switching to actual prompt symbols for unusual formats, suggesting a complex two-stage processing that develops later in training
- Why unresolved: The paper identifies this behavior but doesn't explore the training dynamics that lead to it or potential interventions to improve handling of unusual symbols
- What evidence would resolve it: Analysis of model checkpoints during training specifically focused on unusual symbol handling, combined with targeted training interventions (like data augmentation with unusual symbols), could reveal how this capability develops and how to accelerate it

## Limitations

- The mechanistic claims rely heavily on vocabulary projection and activation patching methods, which may not capture the full complexity of transformer decision-making
- The analysis focuses primarily on the final token position, potentially missing intermediate reasoning processes
- The synthetic task, while useful for isolating symbol binding, may not fully represent the complexity of real-world MCQA scenarios where multiple reasoning steps are involved

## Confidence

- **High confidence**: The finding that middle layers encode relevant information for answer selection while later layers refine predictions
- **Medium confidence**: The identification of specific attention heads as the primary drivers of answer symbol prediction
- **Medium confidence**: The observation of two-stage processing for unusual answer symbols

## Next Checks

1. **Cross-dataset validation**: Test the identified sparse attention heads on additional MCQA datasets (e.g., RACE, ARC) to verify whether the same components consistently drive answer symbol prediction across diverse domains and question types

2. **Ablation study**: Perform targeted ablations of the identified middle-layer attention heads to confirm their causal role in answer symbol prediction, measuring both performance degradation and changes in the two-stage processing behavior

3. **Training dynamics analysis**: Track the development of the symbol binding skill during model training using the synthetic task, correlating the emergence of this skill with changes in the identified attention head patterns and two-stage processing behavior