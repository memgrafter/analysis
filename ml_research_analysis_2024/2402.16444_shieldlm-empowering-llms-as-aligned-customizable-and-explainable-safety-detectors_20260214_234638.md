---
ver: rpa2
title: 'ShieldLM: Empowering LLMs as Aligned, Customizable and Explainable Safety
  Detectors'
arxiv_id: '2402.16444'
source_url: https://arxiv.org/abs/2402.16444
tags:
- safety
- shieldlm
- rules
- response
- unsafe
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: ShieldLM is an LLM-based safety detector designed to align with
  human safety standards, support customizable detection rules, and provide explanations
  for its decisions. It addresses the challenge of detecting safety issues in LLM-generated
  responses by leveraging a large bilingual dataset and training an LLM to understand
  and apply diverse safety rules.
---

# ShieldLM: Empowering LLMs as Aligned, Customizable and Explainable Safety Detectors

## Quick Facts
- arXiv ID: 2402.16444
- Source URL: https://arxiv.org/abs/2402.16444
- Reference count: 40
- Key outcome: LLM-based safety detector achieving state-of-the-art performance with explainability and customizability

## Executive Summary
ShieldLM is an LLM-based safety detector designed to align with human safety standards, support customizable detection rules, and provide explanations for its decisions. It addresses the challenge of detecting safety issues in LLM-generated responses by leveraging a large bilingual dataset and training an LLM to understand and apply diverse safety rules. ShieldLM achieves state-of-the-art performance across four test sets, outperforming strong baselines like GPT-4. It demonstrates remarkable customizability by adapting to various safety standards and provides explainable analyses for its decisions. In real-world applications, ShieldLM effectively evaluates the safety of LLMs, offering accurate and tailored detection results.

## Method Summary
ShieldLM fine-tunes the Qwen-14B-Chat model on a bilingual dataset of 14,387 annotated query-response pairs using safety rules and generated explanations. During training, irrelevant rules are injected to help the model learn to identify the correct rule for each sample. The model takes a prompt containing the query-response pair and safety rules, and outputs a safety label with an explanation. ShieldLM's architecture emphasizes customizability and explainability through rule-based input and GPT-4-generated natural language analyses.

## Key Results
- Achieves state-of-the-art performance across four test sets (ID and OOD)
- Outperforms strong baselines like GPT-4 on safety detection tasks
- Demonstrates remarkable customizability by adapting to various safety standards
- Provides explainable analyses for its decisions, enhancing transparency

## Why This Works (Mechanism)

### Mechanism 1
- Claim: ShieldLM's training with irrelevant rules improves its ability to recognize the correct rule for a given sample.
- Mechanism: By incorporating irrelevant rules during training, ShieldLM learns to identify the relevant rule from a list of rules provided in the input, mimicking real-world scenarios where developers define multiple rules for their system.
- Core assumption: The model can effectively learn to differentiate between relevant and irrelevant rules through the loss function.
- Evidence anchors:
  - [section] "To achieve this objective, we sample irrelevant rules and integrate them with the original rule at training time. This enables ShieldLM to discern the effective rule via the label prediction loss and subsequent loss in generating the analysis based on the identified rule."
  - [corpus] Weak evidence; no direct corpus citation for this specific mechanism.
- Break condition: If the model cannot effectively differentiate between relevant and irrelevant rules, its performance will degrade.

### Mechanism 2
- Claim: ShieldLM's fine-tuning on a large bilingual dataset enhances its alignment with human safety standards.
- Mechanism: The dataset, consisting of 14,387 query-response pairs annotated for safety, provides diverse examples for ShieldLM to learn from, improving its understanding of safety issues in both English and Chinese.
- Core assumption: The annotated dataset accurately represents the range of safety issues and human judgment in safety detection.
- Evidence anchors:
  - [abstract] "To train ShieldLM, we compile a large bilingual dataset comprising 14,387 query-response pairs, annotating the safety of responses based on various safety standards."
  - [corpus] Weak evidence; no direct corpus citation for this specific mechanism.
- Break condition: If the dataset does not adequately represent the diversity of safety issues or human judgment, ShieldLM's alignment will be compromised.

### Mechanism 3
- Claim: The natural language analyses generated by GPT-4 enhance ShieldLM's explainability.
- Mechanism: GPT-4 generates explanations that align with human labels and safety rules, providing insights into ShieldLM's decision-making process and increasing transparency.
- Core assumption: GPT-4 can generate high-quality, consistent explanations that align with human judgment and the provided rules.
- Evidence anchors:
  - [section] "According to the manual evaluation in Appendix D, although GPT-4 doesn't fully align with human safety standards, we find that it can generate faithful and reasonable analysis when provided with human judgements and rules."
  - [corpus] Weak evidence; no direct corpus citation for this specific mechanism.
- Break condition: If GPT-4 fails to generate consistent and high-quality explanations, ShieldLM's explainability will be limited.

## Foundational Learning

- Concept: Safety detection in LLMs
  - Why needed here: Understanding the nuances of safety issues in LLM-generated responses is crucial for developing effective detection methods.
  - Quick check question: What are the main categories of safety issues considered in ShieldLM's training data?

- Concept: Customizable safety rules
  - Why needed here: ShieldLM's ability to adapt to different safety standards is a key feature, requiring an understanding of how to define and apply customizable rules.
  - Quick check question: How does ShieldLM incorporate irrelevant rules during training to improve its adaptability?

- Concept: Explainable AI
  - Why needed here: ShieldLM's explainability is a key differentiator, requiring an understanding of how to generate and interpret natural language explanations for model decisions.
  - Quick check question: What is the role of GPT-4 in generating explanations for ShieldLM's decisions?

## Architecture Onboarding

- Component map:
  Dataset -> Qwen-14B-Chat -> Fine-tuning with irrelevant rules -> Inference with prompts

- Critical path:
  1. Compile and annotate the bilingual dataset
  2. Generate natural language explanations using GPT-4
  3. Fine-tune the base model on the dataset with irrelevant rules
  4. Evaluate and validate the model's performance on test sets

- Design tradeoffs:
  - Dataset size vs. annotation quality: Larger datasets may improve performance but require more resources for annotation.
  - Rule customization vs. model complexity: More complex rules may improve accuracy but increase the model's complexity.

- Failure signatures:
  - Poor performance on out-of-distribution test sets
  - Inability to adapt to new safety rules
  - Inconsistent or low-quality explanations

- First 3 experiments:
  1. Evaluate ShieldLM's performance on the in-distribution test set to assess its overall effectiveness.
  2. Test ShieldLM's adaptability by providing new safety rules and evaluating its performance on out-of-distribution test sets.
  3. Analyze the quality and consistency of ShieldLM's explanations to assess its explainability.

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions. However, it acknowledges potential limitations and future work directions, such as extending ShieldLM to specialized domains requiring professional knowledge and exploring the impact of including controversial cases in the test set.

## Limitations
- Limited dataset size (14,387 pairs) may not capture full diversity of safety issues
- Reliance on GPT-4 for explanations introduces potential single point of failure
- Lack of extensive analysis on truly out-of-distribution data and edge cases
- No detailed analysis of computational costs and inference latency for real-world deployment

## Confidence
- **High Confidence**: ShieldLM's ability to achieve state-of-the-art performance on the specified test sets (ID and OOD)
- **Medium Confidence**: The claim that ShieldLM provides "explainable analyses" for its decisions
- **Medium Confidence**: The claim of "remarkable customizability" for various safety standards

## Next Checks
1. **Ablation Study on Irrelevant Rule Integration**: Conduct an ablation study to quantify the contribution of irrelevant rule injection during training. Train a baseline model without irrelevant rules and compare its performance on both ID and OOD test sets to assess whether this mechanism truly improves rule differentiation and overall detection accuracy.

2. **Robustness Evaluation on Out-of-Distribution Data**: Design and test ShieldLM on a comprehensive set of out-of-distribution scenarios, including adversarial examples, novel safety categories not present in the training data, and samples with ambiguous or conflicting safety rules. Measure performance degradation and identify failure patterns to assess real-world robustness.

3. **Explainability Quality Assessment**: Implement a systematic evaluation of ShieldLM's explanations using human evaluators and automated metrics. Assess explanation consistency across similar samples, alignment with human judgment, and sensitivity to changes in input rules. Compare the quality of explanations generated by ShieldLM against those from baseline models to quantify the practical value of the explainability feature.