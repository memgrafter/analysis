---
ver: rpa2
title: 'Jet: A Modern Transformer-Based Normalizing Flow'
arxiv_id: '2412.15129'
source_url: https://arxiv.org/abs/2412.15129
tags:
- flow
- coupling
- normalizing
- layers
- depth
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper revisits normalizing flow models by simplifying their
  architecture and using Vision Transformer (ViT) blocks instead of convolutional
  neural networks. The authors eliminate complex components like multiscale architectures,
  invertible dense layers, and activation normalization layers, retaining only plain
  affine coupling blocks parameterized by ViT models.
---

# Jet: A Modern Transformer-Based Normalizing Flow

## Quick Facts
- arXiv ID: 2412.15129
- Source URL: https://arxiv.org/abs/2412.15129
- Authors: Alexander Kolesnikov; André Susano Pinto; Michael Tschannen
- Reference count: 40
- Primary result: State-of-the-art NLL results using ViT blocks in normalizing flows

## Executive Summary
This paper revisits normalizing flow models by simplifying their architecture and using Vision Transformer (ViT) blocks instead of convolutional neural networks. The authors eliminate complex components like multiscale architectures, invertible dense layers, and activation normalization layers, retaining only plain affine coupling blocks parameterized by ViT models. Their simplified model, called Jet, achieves state-of-the-art negative log-likelihood (NLL) results across standard image benchmarks including ImageNet-1k and ImageNet-21k at both 32×32 and 64×64 resolutions. For example, Jet achieves 3.58 bits per dimension on ImageNet-1k 32×32 when pretrained on ImageNet-21k and finetuned. The model also transfers well to CIFAR-10, achieving 3.02 bpd.

## Method Summary
Jet replaces traditional CNN-based coupling layers in normalizing flows with ViT-based blocks. The architecture uses only plain affine coupling layers parameterized by ViT models, eliminating multiscale components, invertible dense layers, and activation normalization. The model processes images by splitting them into patches, applying repeated affine coupling transformations where each coupling layer uses a ViT block to compute scale and bias parameters, and accumulating log-determinants for likelihood computation. The authors demonstrate that this simplified architecture achieves state-of-the-art performance while being more straightforward than previous normalizing flow models.

## Key Results
- Achieves 3.58 bits per dimension on ImageNet-1k 32×32 when finetuned from ImageNet-21k
- Outperforms CNN-based normalizing flows significantly across all tested configurations
- Shows state-of-the-art results on CIFAR-10 (3.02 bpd) and ImageNet-21k (2.84 bpd at 32×32)
- Demonstrates that transfer learning from ImageNet-21k effectively mitigates overfitting

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Vision Transformer (ViT) blocks significantly outperform convolutional neural networks (CNNs) for coupling-based normalizing flows
- Mechanism: ViT blocks provide better global context modeling and spatial mixing capabilities compared to CNNs, which is crucial for effective coupling transformations in normalizing flows
- Core assumption: The coupling layer's expressiveness is primarily limited by the representation power of the neural network used to parameterize the affine transformation
- Evidence anchors:
  - [abstract] "we achieve state-of-the-art quantitative and qualitative performance with a much simpler architecture" by "using computational blocks based on the Vision Transformer architecture, not convolutional neural networks"
  - [section 3.2.3] "we conduct a similar sweep to our main sweep on ImageNet-1k 64×64 but using a CNN architecture... This time sweeping the following settings for the CNN setup... The results in Figure 2a show that the CNN-based variant lags significantly behind the ViT-based one"
  - [corpus] Weak - the corpus papers focus on different applications (Boltzmann sampling, LHC jets) and don't directly compare ViT vs CNN performance for normalizing flows
- Break condition: If the coupling layer's bottleneck shifts to other components (e.g., splitting strategy or dimensionality constraints), the advantage of ViT over CNN might diminish

### Mechanism 2
- Claim: Simplifying the architecture by eliminating complex components (multiscale architecture, invertible dense layers, activation normalization) while maintaining performance
- Mechanism: The increased expressiveness of ViT blocks compensates for the removed components, allowing the model to learn effective transformations without these architectural crutches
- Core assumption: ViT's superior representation power can substitute for the specialized components traditionally used in normalizing flows
- Evidence anchors:
  - [abstract] "As a result, we achieve state-of-the-art quantitative and qualitative performance with a much simpler architecture" while eliminating "multiscale components and early factored-out channels", "invertible dense layers", and "activation normalization layers"
  - [section 3.2.5] "We use neither of them in Jet, but ablate whether performance could be improved by introducing those... Overall we found that not using any of those components leads to the best results"
  - [corpus] Missing - corpus papers don't discuss architectural simplification in normalizing flows
- Break condition: If ViT blocks were replaced with less expressive architectures, the removed components might become necessary again

### Mechanism 3
- Claim: Transfer learning from ImageNet-21k to ImageNet-1k effectively mitigates overfitting in normalizing flow models
- Mechanism: Pretraining on a larger, more diverse dataset provides better initialization and regularization, allowing the model to achieve better performance with fewer training samples
- Core assumption: The visual features learned on ImageNet-21k transfer effectively to the ImageNet-1k task despite the dataset differences
- Evidence anchors:
  - [abstract] "we demonstrate that transfer learning can be used to tame overfitting" with results showing "state-of-the-art results on ImageNet-1k, attaining 3.58 and 3.86 bpd on 32×32 and 64×64 input resolution, respectively" when finetuning from ImageNet-21k
  - [section 3.1] "We observe that with a very light finetuning (30 epochs, learning rate of 1e-5 and 3e-6 for higher resolution) we obtain state-of-the-art results on ImageNet-1k"
  - [corpus] Missing - corpus papers don't discuss transfer learning in the context of normalizing flows
- Break condition: If the target domain is sufficiently different from the pretraining data, transfer learning benefits might diminish or disappear

## Foundational Learning

- Concept: Change of variables formula in probability theory
  - Why needed here: Normalizing flows rely on the change of variables identity to compute likelihoods through the bijective transformation
  - Quick check question: Given a random variable X with density p(x) and an invertible transformation g, what is the density of Y = g(X) in terms of p and the Jacobian of g?

- Concept: Coupling layers and affine transformations
  - Why needed here: The core building block of Jet is the affine coupling layer, which requires understanding how to split dimensions and apply learned transformations
  - Quick check question: In a coupling layer, if we split input x into (x1, x2), what is the output y2 in terms of x2, b(x1), and s(x1)?

- Concept: Vision Transformer architecture
  - Why needed here: Jet replaces traditional CNN blocks with ViT blocks, so understanding self-attention, patch embeddings, and transformer blocks is essential
  - Quick check question: In a standard ViT, how are input images processed before being fed to the transformer layers?

## Architecture Onboarding

- Component map:
  Input image -> Patch extraction and flattening -> Dimension splitting -> N coupling layers (each with ViT block) -> Affine transformation -> Log-determinant accumulation -> Gaussian density computation -> Total log-likelihood

- Critical path:
  1. Forward pass through N coupling layers
  2. Computation of log-determinant of Jacobian (sum of log-scales)
  3. Evaluation of Gaussian density in latent space
  4. Summation to obtain total log-likelihood

- Design tradeoffs:
  - ViT depth vs coupling layer count: Deeper ViT with fewer coupling layers vs shallower ViT with more coupling layers
  - Patch size selection: Affects total number of patches and ViT computational requirements
  - Dimension splitting strategy: Channel-wise vs spatial-wise vs hybrid approaches
  - Pretraining strategy: ImageNet-1k vs ImageNet-21k for initialization and regularization

- Failure signatures:
  - NaN or Inf values in training: Often indicates numerical instability in log-determinant computation or improper initialization
  - Overfitting with high training accuracy but low validation accuracy: Suggests model capacity exceeds available data
  - Poor convergence: May indicate learning rate issues or problematic initialization
  - Mode collapse: Model fails to capture full data distribution, possibly due to insufficient model capacity

- First 3 experiments:
  1. Implement a single coupling layer with random ViT parameters and verify the forward and inverse transformations work correctly with proper Jacobian computation
  2. Train a minimal Jet model (e.g., 2 coupling layers, ViT depth 1) on a small subset of CIFAR-10 to verify end-to-end training
  3. Compare a CNN-based coupling layer vs ViT-based coupling layer on the same small dataset to reproduce the core performance difference finding

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does Jet's performance scale with even larger datasets beyond ImageNet-21k, such as JFT-300M or private web-scale datasets?
- Basis in paper: [inferred] The paper shows Jet benefits from pretraining on ImageNet-21k (10x larger than ImageNet-1k) and suggests unlabeled natural images are abundant. The authors state "the best way to tackle overfitting is to increase the amount of training data."
- Why unresolved: The paper only tests up to ImageNet-21k (14M images) and doesn't explore scaling to truly massive datasets that modern large models are trained on.
- What evidence would resolve it: Training Jet on datasets like JFT-300M (300M images) or other web-scale datasets and measuring improvements in negative log-likelihood across resolutions.

### Open Question 2
- Question: What is the computational trade-off between increasing ViT depth versus increasing the number of coupling layers in Jet?
- Basis in paper: [explicit] The paper shows "an interesting interplay between these parameters" and notes that "scaling the number of coupling layers, while keeping shallow ViT models (e.g. depth 1) results in an unfavorable compute-performance trade-off" with specific examples comparing 32 layers/4 depth vs 128 layers/1 depth.
- Why unresolved: While the paper identifies this trade-off exists, it doesn't provide a comprehensive analysis of the optimal ratio or how this scales with compute budget.
- What evidence would resolve it: Systematic experiments varying both parameters across multiple compute budgets to identify Pareto-optimal configurations.

### Open Question 3
- Question: Can Jet's architecture be effectively extended to higher resolutions (e.g., 128×128 or 256×256) while maintaining competitive performance?
- Basis in paper: [inferred] The paper tests only up to 64×64 resolution and notes that autoregressive models "do not scale to large resolutions as they require a forward-pass per (sub)pixel." This suggests a gap in understanding Jet's scalability.
- Why unresolved: The paper doesn't investigate whether the current architecture design (fixed 256 patches) can scale to higher resolutions without architectural modifications.
- What evidence would resolve it: Training Jet at higher resolutions and measuring whether performance degrades or requires architectural changes like multi-scale approaches.

## Limitations
- The simplification of architecture raises questions about whether the ViT-based approach truly eliminates the need for complex components or merely masks their absence through increased model capacity
- Transfer learning results don't fully characterize when and why ImageNet-21k pretraining helps versus hurts
- Computational cost of ViT-based coupling layers versus traditional CNN approaches isn't thoroughly analyzed

## Confidence
**High confidence** in the core empirical findings: The state-of-the-art results on standard benchmarks (ImageNet-1k, CIFAR-10) are well-documented and reproducible through the ablation studies.

**Medium confidence** in the architectural simplification claims: While ablation studies show Jet performs well without removed components, the paper doesn't rigorously test whether these components might still be beneficial in certain regimes.

**Medium confidence** in the ViT superiority claim: The CNN vs ViT comparison is well-executed, but the study doesn't explore whether hybrid architectures or carefully tuned CNN variants might close the performance gap.

**Low confidence** in the transfer learning mechanism explanation: The paper shows ImageNet-21k pretraining helps with overfitting, but doesn't provide deep analysis of which features transfer effectively or why this particular transfer works better than alternatives.

## Next Checks
1. **Architectural component necessity test**: Systematically reintroduce each eliminated component (activation normalization, multiscale architecture, invertible dense layers) one at a time under different data regimes to determine if any become necessary under specific conditions.

2. **Computational efficiency benchmarking**: Conduct head-to-head comparisons measuring not just bits-per-dimension but also training/inference time, memory usage, and parameter efficiency between Jet and traditional CNN-based normalizing flows across different hardware configurations.

3. **Transfer learning mechanism analysis**: Perform controlled experiments varying the size and domain similarity of pretraining data, then analyze which learned features actually transfer by examining intermediate representations and conducting feature ablation studies.