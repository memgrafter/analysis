---
ver: rpa2
title: Challenges and Future Directions of Data-Centric AI Alignment
arxiv_id: '2410.01957'
source_url: https://arxiv.org/abs/2410.01957
tags:
- human
- feedback
- response
- user
- gold
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper identifies and analyzes sources of unreliability in
  human feedback used for aligning large language models (LLMs). The authors find
  that over 25% of human feedback in the widely-used Anthropic-HH dataset disagrees
  with gold reward models (RMs), indicating significant quality concerns.
---

# Challenges and Future Directions of Data-Centric AI Alignment

## Quick Facts
- arXiv ID: 2410.01957
- Source URL: https://arxiv.org/abs/2410.01957
- Reference count: 39
- Primary result: Over 25% of human feedback in Anthropic-HH dataset disagrees with gold reward models, addressed through Source-Aware Cleaning achieving 77% win-tie rate

## Executive Summary
This paper identifies significant reliability issues in human feedback data used for aligning large language models, finding that more than 25% of preferences in the widely-used Anthropic-HH dataset disagree with gold reward models. Through qualitative analysis, the authors identify six key sources of unreliability including labeling errors, subjective preferences, and misinformation. They propose Source-Aware Cleaning (SAC), a method that automatically cleans the dataset based on gold reward model voting patterns and identified unreliability sources. When applied to Llama-3-8B, models trained on the cleaned dataset achieve substantially better alignment performance with a 77% win-tie rate against models trained on the original dataset.

## Method Summary
The authors assess human feedback reliability using a committee of eight gold reward models to categorize data into agreement groups (DNoAgree, DLowAgree, DHighAgree, DAllAgree). They conduct qualitative analysis by having annotators examine disagreeing samples to identify six sources of unreliability. Based on these insights, they develop Source-Aware Cleaning (SAC) that flips labels in DNoAgree group and removes problematic samples from DLowAgree group in harmless split. The cleaned dataset is then used to train DPO models, which are evaluated against the original dataset using GPT-4o pairwise comparison, reward model scoring, and preference prediction accuracy.

## Key Results
- Over 25% of human feedback in Anthropic-HH dataset shows low or no agreement with gold reward models
- Source-Aware Cleaning (SAC) improves alignment performance, achieving 77% win-tie rate against original dataset model
- Six key sources of unreliability identified: labeling errors, subjective preferences, differing evaluation criteria, varying thresholds, harmful suggestions in both responses, and misinformation in both responses

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Using a committee of gold reward models as reference points creates a more reliable signal for identifying unreliable human feedback than using a single model.
- Mechanism: The gold reward models serve as idealized evaluators trained on diverse, high-quality datasets. By comparing human feedback against multiple independently trained models, individual biases and errors are mitigated through voting patterns.
- Core assumption: Gold reward models accurately represent idealized human preferences across various domains including conversational ability, instruction following, and safety.
- Evidence anchors:
  - [abstract] "We assess feedback reliability using a committee of gold reward models (RMs), revealing that over 25% of the dataset shows low or no agreement with these models"
  - [section 2] "A committee of gold reward models consists of multiple independently trained models, each contributing to a collective judgment that mitigates individual biases and errors"
- Break condition: If gold reward models themselves become misaligned with true human preferences or if they overfit to specific dataset characteristics.

### Mechanism 2
- Claim: Qualitative analysis of disagreement patterns reveals specific sources of unreliability that can be systematically addressed through targeted data cleaning.
- Mechanism: By having multiple annotators label data points and analyze their thought processes, six distinct sources of unreliability are identified. These sources are then mapped to specific voting patterns from the gold reward models committee.
- Core assumption: Annotators can reliably identify and categorize different types of unreliability when given appropriate instructions and context.
- Evidence anchors:
  - [abstract] "Through qualitative analysis, we identify six key sources of unreliability: human labeling errors, subjective preferences, differing evaluation criteria, varying thresholds for quality assessment, harmful suggestions in both responses, and misinformation in both responses"
  - [section 3.2] "We thoroughly examine each type to identify their underlying sources of unreliability. Specifically, for data in DNoAgree, we focus on cases where the majority vote among three annotators opposed the original label"
- Break condition: If the identified sources of unreliability are not comprehensive or if the mapping between sources and voting patterns is inaccurate.

### Mechanism 3
- Claim: Source-aware cleaning based on identified unreliability patterns significantly improves alignment performance compared to heuristic-based approaches.
- Mechanism: The cleaning method removes or flips labels based on gold RM voting patterns and the identified sources of unreliability. Data where humans made clear mistakes (DNoAgree group) has labels flipped, while data with both responses being harmful or irrelevant is removed.
- Core assumption: The identified sources of unreliability are the primary factors affecting alignment performance, and addressing them directly will improve results.
- Evidence anchors:
  - [abstract] "When applied to Llama-3-8B, models trained on the cleaned dataset (HH-Clean) achieve a 77% win-tie rate against models trained on the original dataset"
  - [section 4.1] "Our key insight is that these sources of unreliability are closely linked to the groups automatically categorized by gold reward models"
- Break condition: If other factors beyond the identified sources are more significant for alignment performance, or if the cleaning introduces new biases.

## Foundational Learning

- Concept: Gold reward models as idealized evaluators
  - Why needed here: Understanding that gold RMs serve as reference points for assessing human feedback reliability is crucial for grasping the paper's methodology
  - Quick check question: Why use multiple gold RMs instead of just one when assessing human feedback reliability?

- Concept: Qualitative analysis methodology
  - Why needed here: The approach of having annotators provide explanations and identify sources of unreliability is central to the paper's contributions
  - Quick check question: How does asking annotators to provide explanations improve the quality of the qualitative analysis?

- Concept: Data cleaning as targeted intervention
  - Why needed here: The paper's novel contribution is not just identifying problems but creating a systematic way to address them
  - Quick check question: What distinguishes source-aware cleaning from traditional heuristic-based data cleaning approaches?

## Architecture Onboarding

- Component map: Anthropic-HH dataset -> Gold reward models committee -> Agreement categorization -> Qualitative analysis -> Source-Aware Cleaning -> DPO training -> Evaluation

- Critical path: The sequence is: (1) Use gold RMs to categorize human feedback reliability, (2) Conduct qualitative analysis on unreliable samples, (3) Develop source-aware cleaning based on insights, (4) Apply cleaning and train models, (5) Evaluate performance improvements.

- Design tradeoffs: Using gold RMs introduces computational overhead but provides more reliable assessment; qualitative analysis requires human effort but yields actionable insights; targeted cleaning preserves data diversity while removing problematic samples.

- Failure signatures: Poor performance improvements despite cleaning may indicate incomplete identification of unreliability sources, misalignment of gold RMs with true preferences, or overfitting during model training.

- First 3 experiments:
  1. Compare agreement levels between human feedback and gold RMs on a small sample to validate the committee approach
  2. Conduct qualitative analysis on a subset of disagreeing samples to identify potential sources of unreliability
  3. Apply simple cleaning rules (flip labels in DNoAgree) and measure performance impact before implementing full source-aware approach

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of Source-Aware Cleaning (SAC) compare to other data-cleaning methods when applied to human feedback datasets from domains other than helpfulness and harmlessness (e.g., factual accuracy, creativity, or emotional support)?
- Basis in paper: [inferred] The paper only evaluates SAC on the Anthropic-HH dataset, which focuses on helpfulness and harmlessness. The authors acknowledge that their analytical framework can be applied to other human feedback datasets but do not test it.
- Why unresolved: The paper does not explore how SAC performs on datasets with different annotation criteria or domains. Different domains may have unique sources of unreliability that SAC might not address effectively.
- What evidence would resolve it: Experiments applying SAC to human feedback datasets from various domains, comparing its performance against baselines and measuring improvements in model alignment quality across these domains.

### Open Question 2
- Question: What is the optimal balance between human annotation costs and data quality when implementing human-AI collaboration strategies for feedback collection, as suggested in the Discussion section?
- Basis in paper: [explicit] The authors discuss potential human-RM collaboration strategies in Section 6, including having annotators recheck labels that conflict with all RMs or assigning additional annotators to harder cases (low inter-RM agreement).
- Why unresolved: The paper proposes these strategies but does not empirically test their effectiveness or determine the optimal allocation of human resources to maximize data quality while minimizing costs.
- What evidence would resolve it: A controlled study comparing different human-RM collaboration strategies, measuring both the improvement in data quality (e.g., reduced mislabeling, increased agreement with gold RMs) and the associated annotation costs.

### Open Question 3
- Question: How does the inclusion of a "both are bad" option in human feedback annotation affect the downstream alignment performance of language models compared to traditional binary preference annotation?
- Basis in paper: [explicit] The authors recommend adding a "both are bad" option during annotation in Section 6, citing their qualitative analysis showing that many instances of unreliability arise from cases where both response options are of poor quality.
- Why unresolved: While the authors suggest this approach based on their analysis, they do not experimentally validate whether including this option actually improves alignment performance compared to traditional methods.
- What evidence would resolve it: An experiment comparing language models trained on datasets with and without the "both are bad" option, measuring differences in alignment quality (e.g., win-tie rates against baseline models, preference prediction accuracy) and model safety metrics.

## Limitations
- The findings are based on a single dataset (Anthropic-HH) and specific set of gold reward models, which may not generalize to other alignment datasets or reward model architectures.
- The assumption that gold reward models represent "idealized" human preferences is a significant limitation, as these models may themselves be imperfect or biased.
- The qualitative analysis relies on human annotators' judgments, introducing subjectivity and potential inconsistencies in identifying sources of unreliability.

## Confidence
- **High Confidence**: The quantitative finding that over 25% of human feedback disagrees with gold reward models is well-supported by the data and methodology. The experimental results showing performance improvements from cleaned data (77% win-tie rate) are also robust.
- **Medium Confidence**: The identification of six specific sources of unreliability is based on qualitative analysis, which while thorough, depends on annotator interpretation and may not capture all possible sources.
- **Low Confidence**: The claim that gold reward models represent "idealized" human preferences is an assumption that requires further validation, as these models may themselves be imperfect or biased.

## Next Checks
1. Test the Source-Aware Cleaning method on multiple alignment datasets beyond Anthropic-HH to verify generalizability of the approach and identified unreliability sources.
2. Conduct human evaluation studies to validate whether gold reward model preferences align with actual human preferences in cases where human feedback disagreed with gold RMs.
3. Implement ablation studies removing each identified source of unreliability from the cleaning process to determine which factors contribute most significantly to performance improvements.