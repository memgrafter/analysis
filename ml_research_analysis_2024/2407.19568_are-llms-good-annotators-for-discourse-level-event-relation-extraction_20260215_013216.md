---
ver: rpa2
title: Are LLMs Good Annotators for Discourse-level Event Relation Extraction?
arxiv_id: '2407.19568'
source_url: https://arxiv.org/abs/2407.19568
tags:
- event
- timex
- relations
- prompt
- temporal
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper evaluates large language models (LLMs) on discourse-level
  event relation extraction (ERE) tasks involving coreference, temporal, causal, and
  subevent relations. Experiments with GPT-3.5 and LLaMA-2 show that LLMs significantly
  underperform a supervised baseline model, even with supervised fine-tuning.
---

# Are LLMs Good Annotators for Discourse-level Event Relation Extraction?

## Quick Facts
- arXiv ID: 2407.19568
- Source URL: https://arxiv.org/abs/2407.19568
- Reference count: 40
- Key outcome: LLMs (GPT-3.5, LLaMA-2) significantly underperform supervised baselines on discourse-level event relation extraction, struggling with long-distance relations, transitivity rules, and frequently hallucinating events

## Executive Summary
This paper evaluates large language models (GPT-3.5 and LLaMA-2) on discourse-level event relation extraction tasks involving coreference, temporal, causal, and subevent relations using the MAVEN-ERE dataset. The study compares LLMs against a supervised baseline model trained with RoBERTa, testing various prompt patterns including bulk, iterative, event ranking, and pairwise approaches. Results show that LLMs underperform the supervised baseline across all relation types, with temporal relations being easiest and subevent relations most difficult. Both models struggle with capturing long-distance relations, transitivity rules, and dense event contexts, while also frequently hallucinating non-existent event mentions.

## Method Summary
The study uses the MAVEN-ERE dataset containing 4480 English Wikipedia documents, evaluating on 857 test documents. GPT-3.5 and LLaMA-2 are prompted using Iterative Prediction patterns with varying demonstration sizes (whole document, 1-shot through 10-shot). The supervised baseline uses RoBERTa fine-tuned on the same dataset. Four relation types are evaluated: coreference, temporal, causal, and subevent. Performance is measured using F1 score, precision, and recall for each relation type and overall macro-average. LLaMA-2 undergoes supervised fine-tuning with SFT, though exact hyperparameters are not fully specified.

## Key Results
- LLMs significantly underperform the supervised baseline model across all relation types and evaluation metrics
- Temporal relations are easiest for LLMs, while subevent relations are most difficult
- LLMs frequently hallucinate event mentions that don't exist in the source text
- Both models struggle with capturing long-distance relations and applying transitivity rules
- LLaMA-2 requires approximately twice as much training data as the supervised baseline to reach comparable performance

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLMs fail on discourse-level ERE because they cannot reliably capture long-distance event relations.
- Mechanism: The model struggles to maintain context across multiple sentences when identifying relations between events that are far apart in the text.
- Core assumption: Discourse-level ERE requires understanding relations that span multiple sentences or even entire documents.
- Evidence anchors:
  - [section]: "LLMs encounter challenges in capturing long distance event relations and inter-sentence event relations."
  - [abstract]: "Our quantitative and qualitative analysis shows that LLMs have several weaknesses when applied for extracting event relations, including... failures to capture transitivity rules among relations, detect long distance relations..."
  - [corpus]: Weak. The corpus contains related papers on event relation extraction but does not directly address LLM limitations on long-distance relations.
- Break condition: When the number of sentences between related events exceeds the model's context window or attention span.

### Mechanism 2
- Claim: LLMs underperform because they tend to hallucinate event mentions that do not exist in the text.
- Mechanism: The model generates false event mentions or relations when it cannot find sufficient evidence in the provided context.
- Core assumption: LLMs will fabricate information when faced with insufficient or ambiguous input.
- Evidence anchors:
  - [abstract]: "Our quantitative and qualitative analysis shows that LLMs have several weaknesses when applied for extracting event relations, including a tendency to fabricate event mentions..."
  - [section]: "We notice that both models create events or event relations that do not exist in text."
  - [corpus]: Weak. No direct evidence in corpus about hallucination in ERE tasks.
- Break condition: When the model's confidence threshold for generating content is too low, leading to excessive generation.

### Mechanism 3
- Claim: LLMs cannot learn transitivity rules for event relations from examples.
- Mechanism: The model fails to infer indirect relations from directly stated ones, violating transitivity (if A before B and B before C, then A before C).
- Core assumption: Transitivity is a fundamental property of temporal and causal relations that should be learnable from examples.
- Evidence anchors:
  - [section]: "By manually examine the output of GPT-3.5 on the 10 validation documents, we notice that this model failed to learn the transitivity rules from the provided examples."
  - [abstract]: "Furthermore, LLMs display inconsistencies in adhering to the provided prompts... failures to capture transitivity rules among relations..."
  - [corpus]: Weak. Corpus papers discuss relation extraction but not specifically LLM handling of transitivity.
- Break condition: When the model is trained on examples that explicitly demonstrate transitivity, yet still fails to apply it.

## Foundational Learning

- Concept: Transitivity in event relations
  - Why needed here: Understanding why LLMs fail requires knowing what transitivity is and why it matters for temporal and causal relations.
  - Quick check question: If Event A happened before Event B, and Event B happened before Event C, what can you conclude about the relation between Event A and Event C?

- Concept: Context window limitations
  - Why needed here: LLMs have fixed context windows, which limits their ability to process long documents needed for discourse-level ERE.
  - Quick check question: What happens to an LLM's performance when the relevant information for a task is spread across more sentences than its context window can hold?

- Concept: Prompt engineering techniques
  - Why needed here: Different prompt patterns (bulk, iterative, event ranking, pairwise) were tested to optimize LLM performance on ERE tasks.
  - Quick check question: How might providing a full document as context versus just a few sentences affect an LLM's ability to identify relations between events?

## Architecture Onboarding

- Component map: Document → Prompt generation → LLM API call → Relation extraction → Evaluation metrics
- Critical path: Document → Prompt generation → LLM API call → Relation extraction → Evaluation metrics
- Design tradeoffs: Model size vs. cost vs. performance; context length vs. number of examples in prompt
- Failure signatures: Hallucinated events, violation of transitivity rules, poor performance on inter-sentence relations
- First 3 experiments:
  1. Test different prompt patterns (bulk, iterative, event ranking, pairwise) on a small validation set to identify most effective approach
  2. Evaluate performance on intra-sentence vs. inter-sentence event relations to identify distance limitations
  3. Measure hallucination rate by comparing predicted events to ground truth mentions in the text

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What specific architectural or training differences between LLMs and smaller supervised models (like RoBERTa) account for LLMs' inability to capture long-distance event relations in discourse-level ERE?
- Basis in paper: [explicit] The paper states LLMs "encounter challenges in capturing long distance event relations" and "LLaMA-2 typically requires twice more training data to reach the same overall performance as the smaller supervised baseline model"
- Why unresolved: The paper identifies the performance gap but doesn't investigate the underlying architectural reasons why transformer-based LLMs struggle with this specific task compared to smaller supervised models.
- What evidence would resolve it: Controlled experiments comparing different model architectures (attention mechanisms, positional encodings) or training strategies (curriculum learning focusing on long-distance relations) could reveal whether the issue stems from model capacity, training data distribution, or architectural limitations.

### Open Question 2
- Question: How do specific types of hallucinations (non-existent events, inconsistent relation predictions) vary across different LLM architectures and prompting strategies for ERE tasks?
- Basis in paper: [explicit] The paper notes LLMs "display inconsistencies in adhering to the provided prompts" and have "a tendency to fabricate event mentions"
- Why unresolved: While the paper observes hallucination issues, it doesn't systematically categorize or compare the types and frequencies of hallucinations across different models (GPT-3.5 vs LLaMA-2) or prompt patterns.
- What evidence would resolve it: Detailed error analysis categorizing hallucination types (non-existent events, relation violations, format inconsistencies) across different models and prompts would clarify whether certain architectures or prompting strategies are more prone to specific error types.

### Open Question 3
- Question: What is the minimum effective training data size for SFT to make LLMs competitive with supervised baselines for discourse-level ERE tasks?
- Basis in paper: [explicit] The paper shows "LLaMA-2 typically requires twice more training data to reach the same overall performance as the smaller supervised baseline model" but doesn't identify the break-even point
- Why unresolved: The experiments show performance curves but don't pinpoint where SFT on LLMs becomes as effective as training smaller supervised models, or whether there's a threshold beyond which LLMs become superior.
- What evidence would resolve it: Extended scaling experiments mapping performance curves for both approaches across a wider range of training data sizes would identify the crossover point where SFT becomes competitive or superior.

## Limitations
- The evaluation relies heavily on a single dataset (MAVEN-ERE), which may not fully represent the diversity of discourse-level ERE tasks across different domains
- The study uses GPT-3.5 and LLaMA-2 but does not explore other contemporary LLMs that might perform differently
- The qualitative analysis is based on manual examination of only 10 validation documents, which may not capture the full spectrum of LLM failure modes

## Confidence
- High Confidence: LLMs significantly underperform supervised baselines on discourse-level ERE tasks, particularly for complex relation types like subevents
- Medium Confidence: LLMs struggle specifically with long-distance relations, transitivity rules, and dense event contexts
- Medium Confidence: LLMs frequently hallucinate event mentions and relations

## Next Checks
1. Conduct a domain transfer experiment using the best-performing prompt pattern from MAVEN-ERE to evaluate LLM performance on a different ERE dataset (e.g., TimeBank or MATRES) to assess generalizability
2. Implement a systematic hallucination detection framework that automatically compares predicted event mentions against ground truth mentions, quantifying hallucination rates across different relation types and prompt patterns
3. Test additional contemporary LLMs (such as GPT-4, Claude, or PaLM) using the same evaluation protocol to determine whether performance limitations are model-specific or inherent to LLM approaches for discourse-level ERE