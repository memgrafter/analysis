---
ver: rpa2
title: 'The Mamba in the Llama: Distilling and Accelerating Hybrid Models'
arxiv_id: '2408.15237'
source_url: https://arxiv.org/abs/2408.15237
tags:
- arxiv
- mamba
- linear
- attention
- preprint
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of converting large pretrained
  Transformer models to efficient linear RNN models for faster inference while maintaining
  performance. The authors propose distilling Transformers into hybrid Mamba models
  by reusing attention weights and show this approach can remove many original attention
  layers with minimal performance loss.
---

# The Mamba in the Llama: Distilling and Accelerating Hybrid Models

## Quick Facts
- arXiv ID: 2408.15237
- Source URL: https://arxiv.org/abs/2408.15237
- Authors: Junxiong Wang; Daniele Paliotta; Avner May; Alexander M. Rush; Tri Dao
- Reference count: 40
- Primary result: Distilled Llama3-8B-Instruct achieves 29.61 length-controlled win rate on AlpacaEval 2 against GPT-4

## Executive Summary
This paper introduces a novel approach to convert large pretrained Transformer models into efficient linear RNN models (specifically Mamba) while preserving performance. The authors propose a distillation method that reuses attention weights to initialize Mamba blocks, enabling the removal of most attention layers with minimal performance degradation. They demonstrate this on Zephyr-7B and Llama-3 8B-Instruct, showing that hybrid models with only 25-50% attention layers can match or exceed the original models' performance. Additionally, they develop a hardware-aware speculative decoding algorithm specifically optimized for Mamba and hybrid architectures, achieving over 300 tokens/second throughput on various hardware configurations.

## Method Summary
The approach involves a three-stage distillation process: progressive distillation, supervised fine-tuning (SFT), and directed preference optimization (DPO). The key innovation is the attention-to-Mamba initialization, where linear projection weights from attention layers are reused to initialize Mamba blocks, providing a strong starting point for distillation. The authors also introduce hybrid architectures combining Mamba and attention layers, with varying ratios (12.5%, 25%, 50%) to balance efficiency and performance. A hardware-aware speculative decoding algorithm is developed specifically for these models, using a draft-verifier setup optimized for different GPU architectures. The method is evaluated on chat and general benchmarks including MT-Bench, AlpacaEval 2, and LM Evaluation Harness.

## Key Results
- Distilled Llama3-8B-Instruct achieves 29.61 length-controlled win rate on AlpacaEval 2 against GPT-4
- Hybrid models with 25% attention layers maintain performance close to full Transformer baselines
- Hardware-aware speculative decoding achieves 344 tokens/second throughput on H100 (80GB) GPUs
- Strong length extrapolation capability, maintaining accuracy at 20x the training length
- Outperforms similarly sized pretrained Mamba models on chat and general benchmarks

## Why This Works (Mechanism)
The success of this approach stems from the observation that attention layers in pretrained Transformers contain rich linear projection information that can be effectively transferred to Mamba blocks. By reusing these weights during initialization, the distilled models start from a strong position that captures much of the original model's reasoning capabilities. The hybrid architecture allows retaining critical attention layers where they provide the most value while replacing others with efficient Mamba blocks. The hardware-aware speculative decoding is specifically optimized for the computational characteristics of Mamba, leveraging its linear-time complexity and parallelizable operations to achieve significant speedups.

## Foundational Learning
- **Attention-to-Mamba initialization**: Converting attention weights to Mamba parameters provides a strong starting point for distillation, reducing the amount of training needed to recover performance
  - Why needed: Direct random initialization of Mamba blocks would require significantly more training to achieve comparable performance
  - Quick check: Verify that linear projection weights from attention layers correctly map to Mamba parameters through weight reshaping

- **Progressive distillation curriculum**: Gradually removing attention layers during training helps the model adapt to the new architecture
  - Why needed: Abrupt removal of attention layers would cause catastrophic performance degradation
  - Quick check: Monitor validation loss during progressive distillation to ensure smooth transitions between stages

- **Hardware-aware optimization**: Speculative decoding algorithms must be tailored to the specific computational characteristics of the target architecture
  - Why needed: Generic speculative decoding approaches may not leverage the unique properties of Mamba models
  - Quick check: Measure speedup ratio on different GPU architectures to verify hardware-specific optimizations

## Architecture Onboarding

**Component map:** Pretrained Transformer -> Attention-to-Mamba initialization -> Progressive distillation (3 stages) -> Hybrid architecture (Mamba + attention) -> Hardware-aware speculative decoding

**Critical path:** The most critical components are the attention-to-Mamba initialization and the three-stage distillation process. The initialization provides the foundation, while the progressive distillation ensures smooth transition to the new architecture. Hardware-aware speculative decoding is critical for achieving the claimed inference speedups.

**Design tradeoffs:** The main tradeoff is between efficiency and performance. Removing more attention layers increases efficiency but risks performance degradation. The hybrid approach (25-50% attention) represents a balanced compromise. Another tradeoff involves speculative decoding where larger draft models provide better suggestions but reduce the overall speedup.

**Failure signatures:** 
- Poor initialization: Large performance gap between teacher and student models from the start
- Distillation collapse: Performance degradation during training, particularly in early stages
- Speculative decoding inefficiency: Low acceptance rates or minimal speedup despite multiple GPUs

**3 first experiments:**
1. Implement attention-to-Mamba initialization on a small pretrained Transformer and verify weight mapping correctness
2. Train a hybrid model with 50% attention layers and evaluate on a subset of MT-Bench
3. Benchmark speculative decoding throughput with different draft-to-verifier ratios on a single GPU

## Open Questions the Paper Calls Out

**Open Question 1:** What is the theoretical limit of how many attention layers can be removed through distillation while maintaining performance? The paper only experiments with up to 50% attention removal; further experiments with fewer attention layers are needed.

**Open Question 2:** How does the proposed speculative decoding algorithm scale to different hardware architectures beyond H100 and 3090 GPUs? The paper only evaluates on two specific GPU architectures.

**Open Question 3:** What is the impact of distillation on model robustness and safety, particularly for models trained on sensitive or biased data? The paper focuses on performance metrics but does not address safety or robustness considerations.

## Limitations
- The specific architectural details of the Mamba2 variant used in experiments are not fully specified
- The hardware-aware speculative decoding algorithm lacks complete implementation details
- The exact hyperparameters for the three-stage distillation process are not fully enumerated
- Safety and robustness evaluations are not included despite being critical for deployed models

## Confidence
- Claims about distillation effectiveness and performance preservation: High confidence
- Claims about speculative decoding speedup: High confidence  
- Claims about length extrapolation capabilities: High confidence
- Claims about hybrid architecture benefits: Medium confidence

## Next Checks
1. Replicate the attention-to-Mamba initialization process using a pretrained Llama-3 8B model and verify that the extracted linear projection weights correctly initialize Mamba parameters
2. Implement the hybrid architecture with 25% and 50% attention layers and evaluate performance degradation on MT-Bench compared to full Transformer baseline
3. Benchmark the hardware-aware speculative decoding throughput on NVIDIA A100 (80GB) GPU with draft-to-verifier ratio of 2.4:1 and verify the claimed 344 tokens/second throughput