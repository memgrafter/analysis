---
ver: rpa2
title: How Good is Zero-Shot MT Evaluation for Low Resource Indian Languages?
arxiv_id: '2406.03893'
source_url: https://arxiv.org/abs/2406.03893
tags:
- languages
- data
- metrics
- evaluation
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The authors collect a dataset of 1000 human MQM and DA annotations
  for Assamese, Kannada, Maithili, and Punjabi to meta-evaluate machine translation
  metrics. They find that learned metrics like COMET-Kiwi show poor correlation with
  human judgments on these low-resource languages, with Kendall Tau and Pearson correlations
  as low as 0.32 and 0.45.
---

# How Good is Zero-Shot MT Evaluation for Low Resource Indian Languages?

## Quick Facts
- arXiv ID: 2406.03893
- Source URL: https://arxiv.org/abs/2406.03893
- Reference count: 14
- The authors collect a dataset of 1000 human MQM and DA annotations for Assamese, Kannada, Maithili, and Punjabi to meta-evaluate machine translation metrics.

## Executive Summary
This study examines the performance of machine translation evaluation metrics on four low-resource Indian languages (Assamese, Kannada, Maithili, Punjabi). The authors collect 1000 human annotations using MQM and DA frameworks and use this data to meta-evaluate various automatic metrics. They find that even learned metrics like COMET-Kiwi, which are known for zero-shot performance, show poor correlation with human judgments (Kendall Tau up to 0.32, Pearson up to 0.45). The study explores several approaches to improve performance, including fine-tuning on related languages, using IndicBERT as backbone, and synthetic data augmentation, but finds only modest improvements across these methods.

## Method Summary
The authors collect 250 human MQM annotations and DA scores per language across four low-resource Indian languages. They meta-evaluate various MT metrics (BLEU, chrF++, TER, embeddings, BERTScore variants, COMET variants) using these human judgments. The study then explores improvement strategies including fine-tuning COMET models on related Indic languages (Hindi, Gujarati, Marathi, Tamil, Malayalam), replacing the COMET backbone with IndicBERT v2, and integrating synthetic data generated based on error type/severity distributions. Different training configurations are tested including single-stage and two-stage approaches for synthetic data integration.

## Key Results
- Learned metrics like COMET-Kiwi show poor correlation with human judgments on low-resource languages (Kendall Tau up to 0.32, Pearson up to 0.45)
- Fine-tuning on related languages, using IndicBERT as backbone, and adding synthetic data provide only modest improvements
- Synthetic data augmentation shows mixed results, with performance declining when larger proportions of synthetic data are added
- All tested approaches fall short of achieving strong correlation with human judgments on these low-resource languages

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Fine-tuning learned MT evaluation metrics on related high-resource Indic languages improves zero-shot performance on low-resource Indian languages.
- Mechanism: Transfer learning allows the model to adapt to linguistic patterns shared across related languages, improving generalization.
- Core assumption: Related languages share sufficient linguistic and structural similarities for effective transfer.
- Evidence anchors:
  - [abstract] "We observe that even for learned metrics, which are known to exhibit zero-shot performance, the Kendall Tau and Pearson correlations with human annotations are only as high as 0.32 and 0.45."
  - [section 4.2] "We find that it also enhances performance of COMET-DA ("Indic-COMET-DA" row) on the low-resource languages belonging to the same or a close language family."
  - [corpus] Weak: No direct evidence of transfer learning effectiveness for MT evaluation metrics specifically.
- Break condition: If the related languages do not share sufficient linguistic similarities or the evaluation metric architecture is not well-suited for transfer learning.

### Mechanism 2
- Claim: Using a multilingual language model pre-trained on the target low-resource languages as the backbone for MT evaluation metrics improves performance.
- Mechanism: Pre-training on the target languages provides the model with exposure to linguistic features specific to those languages, leading to better representation learning.
- Core assumption: The pre-trained model captures sufficient linguistic knowledge of the target languages.
- Evidence anchors:
  - [section 4.2.1] "Comparing with non-fine-tuned as well as fine-tuned COMET variants, latter being Indic-COMET, we find that fine-tuning with an Indic-languages-specific base model like IndicBERT v2, which has prior exposure to these languages, leads to an improvement in performance."
  - [abstract] "Fine-tuning on related languages, using IndicBERT as the backbone, and adding synthetic data provide only modest improvements."
  - [corpus] Weak: No direct evidence of the effectiveness of using a multilingual language model pre-trained on target languages for MT evaluation.
- Break condition: If the pre-trained model does not capture sufficient linguistic knowledge of the target languages or the evaluation metric architecture is not compatible with the pre-trained model.

### Mechanism 3
- Claim: Synthetic data augmentation can improve the performance of MT evaluation metrics on low-resource languages.
- Mechanism: Synthetic data provides additional training examples, allowing the model to learn from a wider range of linguistic patterns and error types.
- Core assumption: The synthetic data accurately represents the linguistic characteristics and error distributions of the target low-resource languages.
- Evidence anchors:
  - [section 3.2] "We generate synthetic examples, we utilized BPCC-seed dataset containing data in all these languages without any overlap with the FLORES test set."
  - [section 4.3] "The Single-Stage approach shows modest improvement when equal proportions of real and synthetic data are used. However, the performance declines on adding more amount of synthetic data."
  - [corpus] Weak: No direct evidence of the effectiveness of synthetic data augmentation for MT evaluation metrics specifically.
- Break condition: If the synthetic data does not accurately represent the linguistic characteristics and error distributions of the target low-resource languages or the evaluation metric architecture is not well-suited for synthetic data augmentation.

## Foundational Learning

- Concept: Transfer learning
  - Why needed here: To leverage knowledge from high-resource related languages to improve zero-shot performance on low-resource languages.
  - Quick check question: What are the key considerations when applying transfer learning for MT evaluation metrics on low-resource languages?

- Concept: Multilingual language models
  - Why needed here: To provide language-specific representations for low-resource languages, improving the performance of MT evaluation metrics.
  - Quick check question: How does the choice of multilingual language model impact the performance of MT evaluation metrics on low-resource languages?

- Concept: Synthetic data augmentation
  - Why needed here: To address the scarcity of human-annotated data for low-resource languages, improving the robustness and generalization of MT evaluation metrics.
  - Quick check question: What are the challenges and best practices for generating synthetic data for MT evaluation metrics on low-resource languages?

## Architecture Onboarding

- Component map: Data collection (MQM/DA annotations) -> Model training (fine-tuning on related languages/synthetic data) -> Evaluation (meta-evaluation with human judgments)

- Critical path:
  1. Collect human annotations for low-resource languages
  2. Fine-tune existing MT evaluation metrics on related languages
  3. Evaluate the fine-tuned metrics using the collected annotations

- Design tradeoffs:
  - Using related languages vs. synthetic data for fine-tuning
  - Choosing the appropriate backbone model (e.g., XLM-Roberta vs. IndicBERT)
  - Balancing the amount of real and synthetic data for training

- Failure signatures:
  - Poor correlation between metric scores and human judgments
  - Overfitting to the training data (related languages or synthetic data)
  - Inability to generalize to unseen low-resource languages

- First 3 experiments:
  1. Fine-tune COMET-DA on related languages and evaluate on low-resource languages
  2. Replace the COMET backbone with IndicBERT and fine-tune on related languages
  3. Generate synthetic data for low-resource languages and fine-tune COMET-DA using different proportions of real and synthetic data

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What specific linguistic features or phenomena cause the significant performance drop of learned metrics like COMET-Kiwi on low-resource languages compared to high-resource languages?
- Basis in paper: [explicit] The paper states that learned metrics show poor correlation with human judgments on low-resource languages, with Kendall Tau and Pearson correlations as low as 0.32 and 0.45, despite showing decent to good performance in other languages.
- Why unresolved: The paper identifies the performance gap but doesn't analyze the underlying linguistic causes, such as morphological complexity, syntactic differences, or lack of training data.
- What evidence would resolve it: Comparative analysis of error types, linguistic feature distributions, and model attention patterns between low-resource and high-resource language evaluations.

### Open Question 2
- Question: What is the optimal ratio of synthetic to real training data for improving MT evaluation metrics on low-resource languages?
- Basis in paper: [explicit] The paper experiments with different proportions of synthetic data (5k, 10k, 20k) added to real data but finds mixed results without identifying an optimal ratio.
- Why unresolved: The experiments show that adding more synthetic data doesn't consistently improve performance, suggesting the relationship between synthetic and real data ratios is non-linear and context-dependent.
- What evidence would resolve it: Systematic ablation studies varying synthetic/real data ratios across different low-resource language families and evaluation metrics.

### Open Question 3
- Question: How does the performance of MT evaluation metrics vary across different error types (e.g., omission, mistranslation, grammatical errors) in low-resource languages?
- Basis in paper: [explicit] The paper mentions that synthetic data was created to reflect various error types and severities, but doesn't analyze how metrics perform on different error categories.
- Why unresolved: The aggregated correlation scores mask potential variations in metric performance across different error types, which could guide targeted improvements.
- What evidence would resolve it: Detailed error-type-level correlation analysis showing which types of errors metrics detect well or poorly in low-resource languages.

### Open Question 4
- Question: What architectural modifications to existing evaluation metrics could better handle the linguistic characteristics of low-resource Indian languages?
- Basis in paper: [inferred] The paper shows that simply fine-tuning on related languages or changing the backbone model provides only modest improvements, suggesting fundamental architectural limitations.
- Why unresolved: The experiments with IndicBERT and related language fine-tuning suggest that current metric architectures may not adequately capture the specific linguistic features of low-resource languages.
- What evidence would resolve it: Comparative studies of metric architectures with language-specific components (e.g., character-level processing, morphology-aware representations) on low-resource languages.

### Open Question 5
- Question: How does the quality of synthetic data generation impact the effectiveness of training MT evaluation metrics for low-resource languages?
- Basis in paper: [explicit] The paper uses synthetic data generation following Geng et al. (2023) but notes that the results are mixed, questioning the effectiveness of larger quantities of synthetic data.
- Why unresolved: The paper doesn't analyze whether the quality or diversity of synthetic errors affects metric performance, or whether alternative synthetic data generation methods might be more effective.
- What evidence would resolve it: Comparative analysis of different synthetic data generation strategies and their impact on metric performance across various low-resource language families.

## Limitations
- The study focuses on only four low-resource Indian languages, limiting generalizability to other language pairs
- The synthetic data generation process lacks full procedural detail, making exact replication challenging
- The study does not explore alternative metric architectures or more extensive synthetic data generation strategies that might yield better results
- The modest performance gains suggest fundamental challenges in cross-lingual evaluation persist

## Confidence
**High Confidence:** The core finding that learned MT evaluation metrics show poor correlation with human judgments on low-resource Indian languages is well-supported by the experimental evidence (Kendall Tau up to 0.32, Pearson up to 0.45).

**Medium Confidence:** The observation that fine-tuning on related languages, using IndicBERT, and adding synthetic data provides only modest improvements is reasonably supported, though the exact magnitude of these improvements could vary with different experimental conditions or language pairs.

**Low Confidence:** The specific effectiveness of the synthetic data generation approach and its optimal integration strategy (single-stage vs. two-stage) are less certain due to limited comparative analysis and the lack of detailed procedural documentation.

## Next Checks
1. **Cross-lingual Transfer Validation:** Test the fine-tuning approach on additional low-resource language pairs beyond the four Indian languages studied, particularly focusing on languages with different linguistic families to assess the generalizability of transfer learning benefits.

2. **Synthetic Data Quality Assessment:** Conduct a detailed analysis of the synthetic data quality by comparing its linguistic characteristics and error distributions with authentic low-resource language data, and evaluate whether more sophisticated synthetic data generation methods could yield better results.

3. **Alternative Metric Architecture Evaluation:** Explore and evaluate alternative metric architectures specifically designed for low-resource languages, such as adapter-based approaches or multilingual models with stronger cross-lingual alignment capabilities, to determine if better performance can be achieved beyond the current COMET-based approaches.