---
ver: rpa2
title: Enhancing Foundation Models for Time Series Forecasting via Wavelet-based Tokenization
arxiv_id: '2412.05244'
source_url: https://arxiv.org/abs/2412.05244
tags:
- uni00000045
- uni00000049
- uni00000012
- uni00000014
- uni00000053
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces WaveToken, a wavelet-based tokenizer for
  time series forecasting with foundation models. By decomposing input time series
  into sparse wavelet coefficients, the method learns directly in the space of time-localized
  frequencies, yielding a compact vocabulary of 1024 tokens.
---

# Enhancing Foundation Models for Time Series Forecasting via Wavelet-based Tokenization

## Quick Facts
- arXiv ID: 2412.05244
- Source URL: https://arxiv.org/abs/2412.05244
- Reference count: 39
- Primary result: WaveToken achieves best average rank across 42 datasets using wavelet-based tokenization

## Executive Summary
This paper introduces WaveToken, a wavelet-based tokenizer for time series forecasting with foundation models. By decomposing input time series into sparse wavelet coefficients, the method learns directly in the space of time-localized frequencies, yielding a compact vocabulary of 1024 tokens. Empirical results on 42 real-world datasets show that WaveToken outperforms other foundation models and task-specific deep learning models across three complementary metrics (WQL, MASE, VRSE) while achieving the best average rank. It also excels at capturing complex temporal patterns like trends, spikes, and non-stationary signals, demonstrating superior generalization in both in-domain and zero-shot settings.

## Method Summary
WaveToken first scales and decomposes time series using maximally decimated Discrete Wavelet Transform (DWT), then thresholds and quantizes wavelet coefficients into discrete tokens using the Freedman-Diaconis rule. These tokens are fed into a T5 encoder-decoder architecture pre-trained for autoregressive next-token prediction. During inference, the model generates wavelet coefficients for the forecast horizon, which are then reconstructed via inverse DWT and inverse scaling to produce final forecasts. The method leverages the inherent sparsity of wavelet representations to achieve compact vocabularies while preserving essential temporal structures across multiple scales.

## Key Results
- WaveToken achieves best average rank across 42 datasets on WQL, MASE, and VRSE metrics
- Outperforms both foundation models (Chronos, Moirai, TimesFM, Lag-Llama) and task-specific models (DeepAR, TFT, PatchTST)
- Demonstrates superior generalization in zero-shot settings across diverse domains including energy, transport, and finance
- Captures complex temporal patterns like trends, spikes, and non-stationary signals more effectively than baseline methods

## Why This Works (Mechanism)

### Mechanism 1
Wavelets provide a compact yet expressive vocabulary for time series by concentrating signal energy into a few coefficients. The discrete wavelet transform decomposes a signal into approximation (low-frequency) and detail (high-frequency) coefficients. Because wavelets localize both time and frequency, they capture sharp transitions and varying frequencies with fewer coefficients than raw samples. Core assumption: Time series exhibit sparse representations in the wavelet domain (most coefficients near zero).

### Mechanism 2
Learning autoregressive models directly on wavelet coefficients exposes a multi-scale hierarchy that the transformer can exploit. After DWT, coefficients are grouped by decomposition level (approximation, detail level 1, level 2, …). The transformer learns to attend across these groups, mapping coarse-to-fine structures for forecasting. Core assumption: The autoregressive model can learn meaningful dependencies across different frequency bands.

### Mechanism 3
Thresholding detail coefficients reduces noise and improves generalization by removing small, less informative coefficients. Applying hard/soft thresholding (e.g., VisuShrink or FDRC) sets small coefficients to zero before quantization, effectively denoising the representation. Core assumption: Small wavelet coefficients are mostly noise and can be safely discarded without losing essential signal content.

## Foundational Learning

- Concept: Discrete Wavelet Transform (DWT) and its sparsity properties
  - Why needed here: Understanding how DWT decomposes signals into sparse coefficients is essential for grasping why WaveToken is efficient
  - Quick check question: What property of the DWT makes it suitable for compressing time series with sharp spikes?

- Concept: Multi-resolution analysis and the multi-scale structure of wavelets
  - Why needed here: The transformer leverages different decomposition levels; knowing the multi-scale nature explains how it captures coarse and fine patterns
  - Quick check question: How does the arrangement of approximation and detail coefficients enable the model to forecast coarse-to-fine structures?

- Concept: Quantization and discretization of continuous wavelet coefficients
  - Why needed here: WaveToken maps real-valued coefficients to discrete tokens; understanding this step is crucial for the tokenizer design
  - Quick check question: Why does the Freedman-Diaconis rule help choose optimal bin sizes for quantization?

## Architecture Onboarding

- Component map: Data → Z-score normalization → DWT decomposition → Thresholding → Quantization → Token sequence → T5 encoder-decoder → Autoregressive output → IDWT + inverse scaling
- Critical path: Data → Wavelet tokenizer → Model inference → IDWT reconstruction
- Design tradeoffs:
  - Vocabulary size vs. reconstruction fidelity (1024 chosen for balance)
  - Decomposition level vs. model complexity (single-level preferred)
  - Thresholding vs. noise sensitivity (no thresholding often better)
- Failure signatures:
  - Poor accuracy → check coefficient sparsity, vocabulary size, or decomposition level
  - Unstable training → verify scaling/normalization or threshold choice
  - Reconstruction artifacts → inspect IDWT implementation or boundary handling
- First 3 experiments:
  1. Compare WQL/MASE with and without wavelet decomposition (raw vs. wavelet tokens)
  2. Vary vocabulary size (512, 1024, 2048) and measure impact on accuracy and compression
  3. Test different thresholding methods (VisuShrink, FDRC, none) on a held-out validation set

## Open Questions the Paper Calls Out

### Open Question 1
How does the choice of wavelet family affect the trade-off between model accuracy and computational efficiency across different time series domains? The paper explores the impact of different wavelet families but does not systematically analyze computational costs or domain-specific effectiveness.

### Open Question 2
Can wavelet-based tokenization be effectively integrated with patch-based transformer architectures to improve both accuracy and inference speed? The paper mentions slower decoding times compared to patch-based models but does not explore hybrid approaches.

### Open Question 3
What is the optimal vocabulary size for wavelet-based tokenization across different time series characteristics, and how does it scale with series length and complexity? The paper shows 1024 tokens are optimal but does not analyze how optimal size varies with series characteristics.

## Limitations
- Performance depends critically on wavelet sparsity assumptions, which may not hold for all time series types
- The biorthogonal-2.2 wavelet family and single-level decomposition may not generalize optimally to all time series characteristics
- The extensive empirical comparison does not include recent state-of-the-art specialized models that may have emerged

## Confidence

**High Confidence:**
- WaveToken achieves superior average ranking across the 42 datasets compared to both foundation and task-specific models
- The wavelet-based tokenizer provides a compact vocabulary (1024 tokens) that effectively captures essential time series patterns
- The method demonstrates strong generalization in zero-shot settings across diverse domains

**Medium Confidence:**
- Wavelets inherently simplify learning by concentrating signal energy into fewer coefficients
- The multi-scale hierarchy learned by the transformer directly contributes to forecasting accuracy
- Thresholding consistently improves model performance across datasets

**Low Confidence:**
- The superiority of VisuShrink thresholding over other methods is dataset-dependent and not universally optimal
- The specific choice of biorthogonal-2.2 wavelet family is optimal for all time series types
- The 1024 vocabulary size represents the ideal balance between compression and fidelity for all applications

## Next Checks

1. **Robustness to Noise and Non-Sparsity**: Systematically evaluate WaveToken on synthetic datasets with controlled noise levels and varying degrees of sparsity in the wavelet domain. Measure performance degradation as the assumption of sparse representations breaks down, and compare against alternative tokenization methods (e.g., BPE, random projections) under identical conditions.

2. **Cross-Domain Wavelet Sensitivity**: Conduct ablation studies using different wavelet families (Daubechies, Symlets, Coiflets) and decomposition levels across the 42 datasets. Identify whether certain wavelet types consistently outperform others for specific domains (e.g., energy vs. healthcare) and whether the single-level decomposition is optimal or if multi-level approaches provide additional benefits.

3. **Attention Pattern Analysis and Interpretability**: Visualize and analyze the cross-attention weights in the decoder layers to confirm that the model is indeed leveraging the multi-scale structure as intended. Quantify how attention patterns differ across approximation and detail coefficient groups, and correlate these patterns with forecasting accuracy to validate the hypothesized mechanism of coarse-to-fine learning.