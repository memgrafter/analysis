---
ver: rpa2
title: 'Explaining GPTs'' Schema of Depression: A Machine Behavior Analysis'
arxiv_id: '2411.13800'
source_url: https://arxiv.org/abs/2411.13800
tags:
- gpt-4
- symptoms
- depression
- were
- symptom
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper uses measurement theory to infer how GPT-4 internally
  represents and relates depressive symptoms. The authors had GPT-4 analyze open-ended
  depression essays, estimating severity for each PHQ-9 symptom, and then used these
  estimates to uncover GPT-4's "schema" of depression.
---

# Explaining GPTs' Schema of Depression: A Machine Behavior Analysis

## Quick Facts
- arXiv ID: 2411.13800
- Source URL: https://arxiv.org/abs/2411.13800
- Authors: Adithya V Ganesan; Vasudha Varadarajan; Yash Kumar Lal; Veerle C. Eijsbroek; Katarina Kjell; Oscar N. E. Kjell; Tanuja Dhanasekaran; Elizabeth C. Stade; Johannes C. Eichstaedt; Ryan L. Boyd; H. Andrew Schwartz; Lucie Flek
- Reference count: 40
- Primary result: GPT-4's symptom-symptom relationships largely aligned with established depression literature, except for two differences: underemphasizing suicidality's relationship with other symptoms while overemphasizing psychomotor symptoms

## Executive Summary
This study uses measurement theory to infer how GPT-4 internally represents and relates depressive symptoms. The authors had GPT-4 analyze open-ended depression essays, estimating severity for each PHQ-9 symptom, and then used these estimates to uncover GPT-4's "schema" of depression. The approach demonstrates a novel method for understanding how language models conceptualize mental health conditions.

## Method Summary
The study analyzed 955 participants' depression essays using GPT-4 to estimate PHQ-9 symptom severity scores. GPT-4 was prompted with step-by-step instructions to first identify explicitly mentioned symptoms with severity scores, then infer implicit symptoms with rationales, and finally provide total PHQ-9 score and severity category. The results were validated against self-reported PHQ-9 scores and expert annotations for 209 samples, then analyzed for symptom relationships and differential item functioning.

## Key Results
- GPT-4's estimates showed strong convergence with both expert judgments (r = 0.81) and self-reported PHQ-9 scores (r = 0.70)
- The symptom-symptom relationships GPT-4 inferred largely aligned with established depression literature, except for two differences: GPT-4 underemphasized suicidality's relationship with other symptoms while overemphasizing psychomotor symptoms
- GPT-4 was more precise in estimating explicitly mentioned symptoms (r = 0.57 average) than implicitly inferred ones (r = 0.39 average)
- When symptoms weren't explicitly mentioned, GPT-4 relied heavily on the cardinal symptoms (depressed mood and anhedonia) to infer other symptoms

## Why This Works (Mechanism)

### Mechanism 1
- Claim: GPT-4's depression assessments show convergent validity with both self-reported and expert judgments.
- Mechanism: By analyzing open-ended depression essays, GPT-4 estimates symptom severity scores that correlate strongly with established depression measures (PHQ-9) and expert annotations.
- Core assumption: GPT-4's symptom-level estimates reflect meaningful mental health constructs rather than surface linguistic patterns.
- Evidence anchors: [abstract] GPT-4 demonstrated high agreement with self-report (r = 0.70) and experts-judgments (avg r = 0.81) on total PHQ-9 ratings.

### Mechanism 2
- Claim: GPT-4's symptom-symptom relationships align with established depression literature, with specific exceptions.
- Mechanism: GPT-4's inferred symptom inter-correlations reveal its internal "schema" of depression, showing how symptoms relate to each other in its understanding.
- Core assumption: Symptom inter-correlations in GPT-4 reflect genuine symptom relationships rather than statistical artifacts.
- Evidence anchors: [abstract] GPT-4's symptom-symptom relationships largely aligned with established depression literature, except for two differences: underemphasizing suicidality's relationship with other symptoms while overemphasizing psychomotor symptoms.

### Mechanism 3
- Claim: GPT-4 is more precise in estimating explicitly mentioned symptoms than implicitly inferred ones.
- Mechanism: When symptoms are directly mentioned in text, GPT-4's estimates converge more strongly with self-reported scores than when symptoms must be inferred from context.
- Core assumption: Explicit symptom mentions provide clearer signals for GPT-4 than contextual inference.
- Evidence anchors: [abstract] GPT-4 was more precise in estimating explicitly mentioned symptoms (r = 0.57 average) than implicitly inferred ones (r = 0.39 average).

## Foundational Learning

- Concept: Measurement Theory and Validity
  - Why needed here: The study uses measurement theory to evaluate whether GPT-4's symptom assessments represent valid constructs of depression.
  - Quick check question: What is the difference between convergent validity and internal consistency in measurement theory?

- Concept: Item Response Theory (IRT)
  - Why needed here: IRT is used to analyze how well individual depression symptoms discriminate between different levels of overall depression severity.
  - Quick check question: In IRT, what does a higher item location rank indicate about a symptom's ability to distinguish depression severity levels?

- Concept: Differential Item Functioning (DIF)
  - Why needed here: DIF analysis compares how symptoms function differently across different assessment methods (GPT-4 vs self-report).
  - Quick check question: What does it mean if a symptom shows positive DIF when comparing GPT-4 assessments to self-report?

## Architecture Onboarding

- Component map: Data collection (human essays) -> GPT-4 analysis (symptom severity estimates) -> Validation (self-report and expert comparison) -> Schema inference (symptom relationships) -> Language analysis (linguistic cues)
- Critical path: Prompt design -> GPT-4 response generation -> Data cleaning and validation -> Statistical analysis of symptom relationships -> Expert comparison
- Design tradeoffs: Using a single model (GPT-4) provides consistency but limits generalizability; explicit symptom extraction is more reliable than inference but may miss nuanced presentations
- Failure signatures: Low convergence with self-report (<0.50 Pearson correlation), inconsistent symptom relationships across validation methods, GPT-4 relying heavily on non-symptom linguistic cues
- First 3 experiments:
  1. Replicate symptom convergence analysis with different GPT-4 versions or other LLM models
  2. Test GPT-4's performance on depression essays from different cultural contexts or languages
  3. Compare GPT-4's implicit symptom estimation accuracy when varying the prominence of cardinal symptoms in input text

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does GPT-4's schema of depression vary based on the specific instructions given to it for assessment?
- Basis in paper: [explicit] The paper notes that "given that the information present in the instructions can affect the performance of these models [63â€“66], we instructed GPT-4 to estimate symptoms of depression before estimating the severity of overall depression."
- Why unresolved: The study only tested one specific instruction format, and the authors acknowledge that model performance can be affected by instructions.

### Open Question 2
- Question: How does GPT-4's depression assessment schema change when analyzing longitudinal data versus cross-sectional data?
- Basis in paper: [explicit] The paper states "Accurate mental health assessments would require taking longitudinal symptom trajectories into account as well [35] to overcome the limitations of cross-sectional evaluations"
- Why unresolved: The current study only used cross-sectional data from a single time point, and the authors explicitly identify this as a limitation for capturing the dynamic nature of symptom progression.

### Open Question 3
- Question: What linguistic features outside of explicitly mentioned symptoms does GPT-4 use to infer implicit symptoms, and how reliable are these inferences?
- Basis in paper: [inferred] While the paper found "little evidence that it relied on linguistic cues outside its explanations of explicit phrases used to estimate PHQ-9 scores," it also notes that "Language is an information-rich medium compared to rating scales [61] which can provide avenues for making hypotheses about predisposing, precipitating, and perpetuating factors of psychopathology."
- Why unresolved: The study primarily focused on comparing explicit symptom mentions and their influence on implicit symptoms, but the complex interplay between implicit linguistic cues and symptom inference remains underexplored.

## Limitations

- The study is limited to a single language model (GPT-4) and cannot establish whether findings generalize to other LLMs or mental health conditions
- The analysis is based on English-language essays, potentially missing cultural variations in how depression is expressed or understood
- The study cannot determine whether GPT-4's symptom estimates represent "ground truth" depression severity or merely correlate with existing measurement methods

## Confidence

**High Confidence**: GPT-4 demonstrates convergent validity with both self-reported PHQ-9 scores (r = 0.70) and expert annotations (r = 0.81); GPT-4 shows higher precision in estimating explicitly mentioned symptoms (r = 0.57) compared to implicitly inferred ones (r = 0.39); symptom-symptom relationships align with established depression literature with specific, documented exceptions

**Medium Confidence**: GPT-4's reliance on cardinal symptoms for inferring other symptoms when not explicitly mentioned; the specific exceptions in symptom relationships represent genuine differences in GPT-4's schema versus established literature

**Low Confidence**: The clinical implications of GPT-4's schema differences for real-world mental health applications; whether GPT-4's symptom relationships would remain stable across different versions, architectures, or training datasets

## Next Checks

1. **Cross-model replication**: Test the same methodology with multiple LLMs (GPT-3.5, Claude, LLaMA) to determine if the observed symptom relationships and differential item functioning are model-specific or represent generalizable patterns in how LLMs conceptualize depression.

2. **Cultural validation**: Apply the GPT-4 analysis to depression essays from different cultural contexts or translated versions of the same essays to assess whether the reliance on cardinal symptoms and the specific symptom-symptom relationships hold across cultural variations in depression expression.

3. **Temporal stability assessment**: Evaluate whether GPT-4's depression schema remains stable over time by running the same analysis on essays collected at different time periods or by comparing results from different GPT-4 model versions as they are updated, to ensure clinical applications can rely on consistent symptom interpretations.