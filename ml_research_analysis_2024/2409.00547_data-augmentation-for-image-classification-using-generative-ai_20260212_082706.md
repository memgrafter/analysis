---
ver: rpa2
title: Data Augmentation for Image Classification using Generative AI
arxiv_id: '2409.00547'
source_url: https://arxiv.org/abs/2409.00547
tags:
- data
- image
- images
- augmentation
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper presents AGA (Automated Generative Data Augmentation),
  a framework designed to improve image classification by generating diverse training
  data while preserving subject authenticity. The key innovation is using segmentation
  models to isolate subjects, then generating varied backgrounds via LLM-generated
  captions and diffusion models, finally merging them with affine-transformed subjects.
---

# Data Augmentation for Image Classification using Generative AI

## Quick Facts
- arXiv ID: 2409.00547
- Source URL: https://arxiv.org/abs/2409.00547
- Reference count: 40
- Improves image classification accuracy by 15.6% on in-distribution data and 23.5% on out-of-distribution data

## Executive Summary
This paper introduces AGA (Automated Generative Data Augmentation), a framework that improves image classification by generating diverse training data while preserving subject authenticity. The key innovation is using segmentation models to isolate subjects, then generating varied backgrounds via LLM-generated captions and diffusion models, finally merging them with affine-transformed subjects. AGA addresses the challenge of maintaining subject integrity during augmentation, which previous text-to-image or inpainting methods often fail to do. Evaluated on ImageNet10, CUB, and iWildCam datasets, AGA demonstrates significant improvements in both accuracy and model explainability metrics.

## Method Summary
AGA is a data augmentation framework that isolates image subjects using object detection (GroundingDINO) and segmentation (SAM), generates diverse backgrounds through LLM-generated captions and Stable Diffusion, and merges these elements with affine-transformed subjects. The method uses structured prompt generation with instruction, background, and temporal modality sets to create varied yet relevant backgrounds. The framework is evaluated by training ResNet variants from scratch on both original and augmented data, measuring performance on in-distribution (ImageNet10) and out-of-distribution (ImageNet-V2, ImageNet-Sketch) datasets.

## Key Results
- 15.6% improvement in classification accuracy on in-distribution data (ImageNet10) compared to baseline models
- 23.5% improvement in classification accuracy on out-of-distribution data (ImageNet-V2, ImageNet-Sketch)
- 64.3% improvement in SIC score, demonstrating better model explainability and generalization
- Benefits observed with up to 10X augmentation without performance degradation

## Why This Works (Mechanism)

### Mechanism 1
Segmentation-guided foreground isolation preserves subject authenticity during augmentation. AGA uses object detection to locate subjects and segmentation to create precise masks, ensuring only the background is modified while the subject remains untouched. Core assumption: Object detection and segmentation models can accurately isolate subjects even in fine-grained classification datasets.

### Mechanism 2
LLM-generated prompts with combinatorial complexity create diverse backgrounds without corrupting the subject. AGA's prompt generation engine samples from instruction, background, and temporal modality sets, then uses Llama to generate detailed captions that guide Stable Diffusion to create varied backgrounds while avoiding subject-related words. Core assumption: LLM-generated captions can produce diverse yet relevant background prompts when given structured input.

### Mechanism 3
Affine transformations on isolated subjects increase diversity without semantic corruption. AGA applies flipping, rotating, and scaling to the masked subject before merging with generated backgrounds, creating additional variation while preserving subject meaning. Core assumption: Affine transformations maintain the semantic content of the subject while providing sufficient diversity.

## Foundational Learning

- **Segmentation and object detection**
  - Why needed here: AGA relies on accurate subject isolation to prevent foreground corruption during augmentation
  - Quick check question: What are the key differences between object detection and semantic segmentation, and why does AGA use both?

- **Prompt engineering for generative models**
  - Why needed here: AGA's effectiveness depends on generating diverse yet relevant background prompts for Stable Diffusion
  - Quick check question: How does prompt decomposition with instruction, background, and temporal sets increase prompt diversity combinatorially?

- **Diffusion model image synthesis**
  - Why needed here: AGA uses Stable Diffusion to generate diverse backgrounds based on LLM-generated prompts
  - Quick check question: What are the key components of the diffusion model architecture that enable text-guided image generation?

## Architecture Onboarding

- **Component map**: Image and class name → GroundingDINO → SAM → Mask creation → Instruction/Background/Temporal sets → Llama → Background captions → Stable Diffusion XL → Background images → Affine transformations → Masked image + Background → Augmented image → Classification model training

- **Critical path**: Segmentation → Prompt generation → Background generation → Merging → Classification model training

- **Design tradeoffs**: Using pre-trained models vs. fine-tuning (AGA uses off-the-shelf models for practicality but may be limited by their capabilities); Background diversity vs. relevance (more diverse prompts may generate less relevant backgrounds); Augmentation scale vs. performance (benefits observed up to 10X augmentation)

- **Failure signatures**: Poor subject isolation (incorrect bounding boxes or segmentation masks); Irrelevant backgrounds (LLM-generated prompts that don't match subject context); Artifact introduction (merging process that doesn't properly integrate subject with background); Semantic corruption (affine transformations that distort the subject)

- **First 3 experiments**:
  1. Test segmentation accuracy on ImageNet10 with GroundingDINO and SAM - measure IoU and check for false positives/negatives
  2. Evaluate prompt generation diversity - generate 100 prompts for a sample image and analyze vocabulary and variation
  3. Assess background relevance - generate 10 backgrounds for a sample image and have humans rate relevance and diversity

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions but several remain unresolved: How does AGA's performance scale to the full ImageNet dataset and other large-scale classification tasks? What is the computational cost compared to other state-of-the-art augmentation methods? How do different segmentation or object detection models affect performance? Can AGA be effectively applied to other computer vision domains beyond image classification?

## Limitations
- Specific prompt engineering methodology and modality sets are not fully detailed, making exact replication challenging
- Evaluation focuses primarily on accuracy metrics without deeper analysis of how augmentation affects different failure modes or edge cases
- No comparison with alternative generative augmentation approaches that might achieve similar results through different mechanisms

## Confidence
- **High Confidence**: Core mechanism of using segmentation to preserve subject integrity during augmentation is well-established and experimental improvements in accuracy are directly measurable
- **Medium Confidence**: LLM-generated prompts providing combinatorially diverse backgrounds is plausible but specific implementation details are limited
- **Low Confidence**: Claim that AGA specifically improves model explainability (SIC score) requires more detailed analysis of what aspects of explainability are being measured and why

## Next Checks
1. **Segmentation Robustness Test**: Evaluate AGA's subject isolation performance across diverse datasets with varying object sizes, occlusion levels, and background complexity to quantify failure rates and error types

2. **Prompt Diversity Analysis**: Generate 1000 background prompts for sample images and perform statistical analysis of vocabulary diversity, semantic relevance, and potential bias patterns in the generated prompts

3. **Ablation Study**: Systematically disable each component of AGA (segmentation, LLM prompts, affine transforms) to quantify the marginal contribution of each mechanism to the overall performance improvements observed