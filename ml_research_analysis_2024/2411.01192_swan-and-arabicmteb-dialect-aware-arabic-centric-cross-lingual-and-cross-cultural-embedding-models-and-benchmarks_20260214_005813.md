---
ver: rpa2
title: 'Swan and ArabicMTEB: Dialect-Aware, Arabic-Centric, Cross-Lingual, and Cross-Cultural
  Embedding Models and Benchmarks'
arxiv_id: '2411.01192'
source_url: https://arxiv.org/abs/2411.01192
tags:
- arabic
- retrieval
- datasets
- arabicmteb
- text
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Swan introduces Arabic-centric embedding models designed for both
  small-scale and large-scale use cases, addressing the limitations of existing multilingual
  models in capturing Arabic linguistic and cultural nuances. The family includes
  Swan-Small, based on ARBERTv2, and Swan-Large, built on ArMistral, a pretrained
  Arabic large language model.
---

# Swan and ArabicMTEB: Dialect-Aware, Arabic-Centric, Cross-Lingual, and Cross-Cultural Embedding Models and Benchmarks

## Quick Facts
- arXiv ID: 2411.01192
- Source URL: https://arxiv.org/abs/2411.01192
- Authors: Gagan Bhatia; El Moatez Billah Nagoudi; Abdellah El Mekki; Fakhraddin Alwajih; Muhammad Abdul-Mageed
- Reference count: 40
- Key outcome: Swan-Large achieves state-of-the-art results, outperforming Multilingual-E5-large in most Arabic tasks, while Swan-Small consistently surpasses Multilingual-E5-base with 75% smaller size and 83% lower inference costs.

## Executive Summary
Swan introduces a family of Arabic-centric embedding models designed to address the limitations of existing multilingual models in capturing Arabic linguistic and cultural nuances. The models include Swan-Small (based on ARBERTv2) and Swan-Large (built on ArMistral), both trained with contrastive learning using InfoNCE loss and synthetic data augmentation. A comprehensive benchmark, ArabicMTEB, was developed to evaluate cross-lingual, multi-dialectal, multi-domain, and multi-cultural Arabic text embedding performance across eight tasks and 94 datasets.

## Method Summary
The Swan models are trained using InfoNCE contrastive learning with hard negatives, leveraging synthetic data generation via Command R+ for MSA, dialectal, and domain-specific content. Swan-Small uses ARBERTv2 as base while Swan-Large employs ArMistral with LoRA for parameter-efficient fine-tuning. The training pipeline includes Polydedupe for data deduplication and sequential fine-tuning from MSA to dialectal data. Evaluation is conducted on ArabicMTEB, a benchmark covering 94 datasets across eight tasks including retrieval, classification, clustering, and cross-lingual retrieval.

## Key Results
- Swan-Large achieves state-of-the-art performance, scoring 60.42 average across tasks and setting new standards in domain-specific tasks (82.49 average).
- Swan-Small consistently outperforms Multilingual-E5-base while being 75% smaller and 83% more cost-efficient.
- Both models demonstrate strong cross-lingual retrieval capabilities, with Swan-Large achieving 77.8 on mMARCO.
- Synthetic data training significantly improves performance, with Swan-Small increasing from 32.46 to 48.42 average score when trained with synthetic MSA data.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Synthetic data improves model performance across multiple Arabic NLP tasks
- Mechanism: Training on synthetic MSA, dialectal, and domain-specific data fills gaps in real-world Arabic datasets, especially for low-resource dialects and specialized domains
- Core assumption: Synthetic data generation via LLMs preserves linguistic quality and diversity
- Evidence anchors:
  - [abstract]: "Swan-Small achieves state-of-the-art results, outperforming Multilingual-E5-large in most Arabic tasks"
  - [section]: "When comparing the initial Swan-Small (average score of 32.46) to its version trained with synthetic MSA data, we observe a significant increase in average performance to 48.42"
  - [corpus]: "Weak - corpus analysis shows related papers but no direct citations yet for synthetic data impact"

### Mechanism 2
- Claim: Hard negatives improve contrastive learning effectiveness
- Mechanism: Including closely related but incorrect documents forces the model to learn finer-grained distinctions in embedding space
- Core assumption: Hard negatives are properly identified and don't introduce noise
- Evidence anchors:
  - [abstract]: "Our extensive evaluations show that Swan models are both dialectally and culturally aware"
  - [section]: "Swan-Large achieves its peak performance of 60.42 when trained with seven hard negatives"
  - [corpus]: "Weak - no direct corpus evidence for hard negative impact on Arabic embeddings"

### Mechanism 3
- Claim: Domain-specific and cultural awareness improves real-world Arabic NLP performance
- Mechanism: Training and evaluation on culturally relevant datasets (country-specific Wikipedia, domain-specific retrieval) enables models to capture regional knowledge and specialized vocabulary
- Core assumption: Cultural datasets accurately represent diverse Arabic-speaking regions
- Evidence anchors:
  - [abstract]: "Our models and benchmark are available at our GitHub page: https://github.com/UBC-NLP/swan"
  - [section]: "Swan-Large sets a new standard in domain-specific tasks, scoring 82.49 on average"
  - [corpus]: "Moderate - related papers exist for Arabic cultural NLP but no direct citations yet"

## Foundational Learning

- Concept: Contrastive learning with InfoNCE loss
  - Why needed here: Enables learning effective text embeddings by maximizing similarity between related pairs and minimizing similarity between unrelated pairs
  - Quick check question: What is the key difference between InfoNCE loss and traditional cross-entropy loss in embedding training?

- Concept: Cross-lingual and multilingual training
  - Why needed here: Allows models to handle Arabic queries retrieving documents in other languages and vice versa
  - Quick check question: How does multilingual training improve zero-shot performance on low-resource languages?

- Concept: Dialectal variation in Arabic
  - Why needed here: Arabic has significant dialectal differences that standard MSA models often fail to capture
  - Quick check question: What are the main challenges in building models that handle both MSA and dialectal Arabic?

## Architecture Onboarding

- Component map: Training data preprocessing (Polydedupe) → Model training with InfoNCE loss and hard negatives → Synthetic data augmentation → Evaluation on ArabicMTEB
- Critical path: Data preprocessing → Model training with hard negatives → Synthetic data augmentation → Evaluation on ArabicMTEB
- Design tradeoffs: Smaller model for efficiency vs larger model for performance; synthetic vs real data quality; dialect coverage vs model complexity
- Failure signatures: Poor performance on dialectal tasks suggests insufficient dialectal training data; high latency indicates optimization opportunities
- First 3 experiments:
  1. Train Swan-Small baseline without synthetic data, measure performance drop
  2. Vary hard negative count (1, 7, 15) and measure impact on retrieval metrics
  3. Test cross-lingual retrieval performance on mMARCO to validate multilingual capabilities

## Open Questions the Paper Calls Out
No open questions were explicitly called out in the paper.

## Limitations
- Evaluation methodology heavily skewed toward retrieval tasks with limited coverage of other critical NLP applications
- Cross-lingual evaluation relies entirely on simulated Arabic queries that may not reflect real-world patterns
- Absence of human evaluation for cultural and dialectal awareness claims

## Confidence

**High Confidence**: Performance improvements over Multilingual-E5 models are well-supported by ArabicMTEB benchmark results with clear numerical comparisons.

**Medium Confidence**: Dialectal and cultural awareness claims are supported by benchmark performance but lack direct human validation.

**Low Confidence**: Cross-lingual retrieval capabilities are primarily based on synthetic query experiments without real-world validation.

## Next Checks

1. **Human Evaluation of Cultural Nuance**: Conduct blind human evaluations comparing Swan model outputs against baseline models on culturally-specific queries to verify claimed cultural awareness beyond benchmark metrics.

2. **Real-World Cross-Lingual Testing**: Deploy the models in an actual cross-lingual retrieval system using real user query logs that mix Arabic with other languages, measuring both retrieval accuracy and user satisfaction.

3. **Bias and Dialect Coverage Analysis**: Perform systematic analysis of model performance across different Arabic dialect regions using sociolinguistic expertise to identify potential biases or blind spots in dialect representation.