---
ver: rpa2
title: Model Debiasing by Learnable Data Augmentation
arxiv_id: '2408.04955'
source_url: https://arxiv.org/abs/2408.04955
tags:
- samples
- bias
- data
- biased
- unbiased
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper addresses the problem of learning unbiased models from
  biased training data in the unsupervised setting, where the bias is unknown. The
  authors propose a two-stage approach: first, they split the training data into biased
  and unbiased subsets using a pseudo-labeling method based on prediction history.'
---

# Model Debiasing by Learnable Data Augmentation

## Quick Facts
- arXiv ID: 2408.04955
- Source URL: https://arxiv.org/abs/2408.04955
- Authors: Pietro Morerio; Ruggero Ragonesi; Vittorio Murino
- Reference count: 40
- Key outcome: Two-stage method for unsupervised debiasing achieves state-of-the-art accuracy on unbiased test samples by learning to mix biased and unbiased data

## Executive Summary
This paper addresses unsupervised debiasing where the bias is unknown by proposing a two-stage approach. First, it splits training data into biased and unbiased subsets using a prediction history method that exploits the faster learning rate of biased samples. Second, it employs a learnable data augmentation strategy that mixes these subsets, with mixing parameters learned through an adversarial mechanism. The method is evaluated on synthetic and realistic biased datasets, showing superior performance particularly on unbiased test samples.

## Method Summary
The method operates in two stages: (1) Data splitting using prediction history - samples are classified as biased or unbiased based on their prediction correctness across training epochs, with biased samples showing higher prediction accuracy earlier in training. (2) Learnable mixup with adversarial training - the model learns Beta distribution parameters (α, β) that control the mixing coefficient λ for combining biased and unbiased samples, creating synthetic examples that break spurious correlations. A Gradient Reversal Layer updates the mixing parameters to maximize classification loss while the classifier minimizes it, creating challenging examples that improve generalization.

## Key Results
- Achieves state-of-the-art accuracy on unbiased test samples across multiple benchmark datasets
- Demonstrates effectiveness even when no explicit bias is present, improving generalization
- Shows particular strength on the Waterbirds and CelebA datasets with varying bias ratios
- Outperforms standard ERM and other debiasing methods in most evaluation scenarios

## Why This Works (Mechanism)

### Mechanism 1: Prediction History for Bias Identification
The method separates biased and unbiased samples through prediction history, exploiting the faster learning rate of biased samples. By tracking correct/incorrect predictions across training epochs, samples consistently classified correctly are deemed biased (easier to fit), while those rarely or never correctly classified are deemed unbiased. Core assumption: Biased samples dominate the dataset and are thus learned faster, leading to a correlation between correct predictions and bias.

### Mechanism 2: Learnable Data Augmentation
Mixing biased and unbiased samples through a learnable augmentation strategy regularizes training and reduces the impact of spurious correlations. The model learns parameters (α, β) of a Beta distribution that controls the mixing coefficient λ for combining biased (ˆx1, ˆy1) and unbiased (ˆx2, ˆy2) samples. This creates synthetic "neutral" examples that break the spurious correlations present in the original data.

### Mechanism 3: Adversarial Learning Component
The adversarial learning component optimizes the mixing parameters to generate challenging examples that force the model to learn more robust features. A Gradient Reversal Layer (GRL) is used to update the parameters ψ of the mixing module hψ. The goal is to maximize the classification loss on the mixed samples while the classifier minimizes it, leading to the generation of difficult examples that improve generalization.

## Foundational Learning

- **Empirical Risk Minimization (ERM)** - Why needed: Understanding ERM is crucial as the paper starts with ERM as a baseline and shows its limitations in biased datasets, motivating the need for debiasing methods. Quick check: What is the main limitation of ERM when training on biased datasets?

- **Mixup data augmentation** - Why needed: The paper builds upon Mixup by making the mixing parameters learnable, so understanding the original Mixup method and its benefits is essential. Quick check: How does Mixup regularize the training process, and why is it beneficial for debiasing?

- **Adversarial learning and Gradient Reversal Layers (GRL)** - Why needed: The adversarial component is central to learning effective mixing parameters that create challenging examples. Quick check: How does a Gradient Reversal Layer enable adversarial training in this context?

## Architecture Onboarding

**Component Map:** Data → ERM Baseline → Prediction History Splitter → Biased/Unbiased Sets → Learnable Mixup Module → Adversarial Training → Debiased Model

**Critical Path:** Data splitting via prediction history → Learnable mixup with adversarial training → Classification model training

**Design Tradeoffs:** The method trades increased computational complexity (two-stage process, adversarial training) for improved generalization on unbiased samples. The adversarial component adds training instability risk but potentially greater debiasing effectiveness.

**Failure Signatures:** Poor bias identification leading to noisy splits; adversarial training instability causing convergence issues; mixing parameters failing to generate meaningful synthetic examples.

**3 First Experiments:**
1. Verify prediction history correlation assumption by computing Pearson correlation between correct predictions and ground-truth bias labels at different training epochs
2. Test the mixing mechanism by visualizing synthetic examples generated from biased/unbiased combinations
3. Evaluate model performance sensitivity to the proportion of biased vs. unbiased samples in the training split

## Open Questions the Paper Calls Out
None

## Limitations
- Core assumption that biased samples are learned faster may not hold for all bias types or dataset distributions
- Adversarial mixing mechanism's effectiveness depends heavily on the quality of the biased/unbiased split, which could be noisy in practice
- Method's generalization to real-world datasets beyond tested benchmarks remains unproven

## Confidence

**High confidence:** Experimental results on benchmark datasets and the basic two-stage pipeline architecture
**Medium confidence:** Theoretical justification for using prediction history to identify bias, as this relies on strong assumptions about learning dynamics
**Medium confidence:** Adversarial mixing mechanism's ability to create truly "neutral" samples that break spurious correlations

## Next Checks

1. Test the prediction history splitting method on datasets with different bias types (e.g., background vs. foreground bias) to verify the correlation assumption holds across scenarios
2. Evaluate model performance sensitivity to different levels of noise in the biased/unbiased split to understand robustness limits
3. Conduct ablation studies removing the adversarial component to quantify its specific contribution versus standard mixup augmentation