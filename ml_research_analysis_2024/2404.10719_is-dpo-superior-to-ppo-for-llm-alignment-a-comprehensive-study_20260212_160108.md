---
ver: rpa2
title: Is DPO Superior to PPO for LLM Alignment? A Comprehensive Study
arxiv_id: '2404.10719'
source_url: https://arxiv.org/abs/2404.10719
tags:
- reward
- dataset
- preference
- arxiv
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The study investigates whether Direct Preference Optimization (DPO)
  is truly superior to Proximal Policy Optimization (PPO) for aligning large language
  models with human preferences. Theoretical analysis reveals that DPO may produce
  biased solutions by exploiting out-of-distribution responses, while PPO can leverage
  prompt-only data and benefit from KL regularization.
---

# Is DPO Superior to PPO for LLM Alignment? A Comprehensive Study

## Quick Facts
- **arXiv ID**: 2404.10719
- **Source URL**: https://arxiv.org/abs/2404.10719
- **Reference count**: 40
- **Primary result**: PPO consistently outperforms DPO across dialogue and code generation tasks when using optimized implementations with large batch sizes and advantage normalization

## Executive Summary
This comprehensive study investigates whether Direct Preference Optimization (DPO) truly surpasses Proximal Policy Optimization (PPO) for aligning large language models with human preferences. Through theoretical analysis and extensive empirical experiments on dialogue and code generation tasks, the authors demonstrate that PPO consistently outperforms DPO. The research reveals that DPO's performance degrades when the preference dataset distribution shifts from the model's output distribution, while PPO benefits from KL regularization that prevents exploitation of out-of-distribution responses. With optimized implementation including large batch sizes, advantage normalization, and exponential moving average for reference model updates, PPO achieves state-of-the-art results in challenging code competitions.

## Method Summary
The study compares DPO and PPO using various datasets including HH-RLHF for dialogue tasks and APPS/CodeContest for code generation. DPO is implemented with Î²=0.1 and learning rate 1e-6, while PPO incorporates advantage normalization, large batch size (512), and exponential moving average for reference model updates. The authors conduct experiments on multiple base models including Llama2-7B and CodeLlama-13B/34B, evaluating performance using OpenAssistant rewards, win rates, safety rates for dialogue, and pass@k metrics for code generation. The implementation leverages DeepSpeed-Chat and is available in the ReaLHF GitHub repository.

## Key Results
- PPO consistently outperforms DPO across all tested dialogue and code generation tasks
- DPO performance degrades significantly when preference dataset distribution shifts from model output distribution
- Large batch sizes (512) and advantage normalization provide substantial performance improvements for PPO
- PPO achieves state-of-the-art results on challenging code competitions, surpassing DPO by significant margins

## Why This Works (Mechanism)

### Mechanism 1
- Claim: DPO's performance degrades when the preference dataset distribution shifts from the model's output distribution
- Mechanism: DPO directly optimizes the policy on preference pairs without considering the reference model's regularization, leading to exploitation of out-of-distribution responses that may not reflect true human preferences
- Core assumption: The preference dataset has limited coverage and doesn't represent the full distribution of model outputs
- Evidence anchors:
  - [abstract] "Empirical experiments on dialogue and code generation tasks demonstrate that PPO consistently outperforms DPO across all cases"
  - [section 4.3] "The performance of DPO could be improved by mitigating the distribution shift between the model and the preference dataset"
  - [corpus] "Found 25 related papers... Average neighbor FMR=0.551" (weak evidence)
- Break condition: When preference dataset perfectly covers all possible model outputs or when reference model regularization is incorporated into DPO

### Mechanism 2
- Claim: PPO's KL regularization provides implicit regularization on out-of-distribution samples
- Mechanism: PPO uses KL divergence between the learned policy and reference model as a regularization term, which constrains the policy from deviating too far from the reference model on unseen samples
- Core assumption: The reference model captures reasonable priors about human preferences that should be preserved
- Evidence anchors:
  - [abstract] "PPO can leverage prompt-only data and benefit from KL regularization"
  - [section 4.1] "PPO can leverage prompt-only data and generate responses beyond the preference dataset distribution"
  - [section 5] "updating the parameters of the reference model with exponential moving average"
- Break condition: When KL regularization is disabled or when the reference model is poorly aligned with human preferences

### Mechanism 3
- Claim: Large batch sizes and advantage normalization stabilize PPO training
- Mechanism: Larger batch sizes provide more stable gradient estimates, while advantage normalization reduces variance in the policy gradient updates
- Core assumption: PPO training is sensitive to batch size and advantage scaling
- Evidence anchors:
  - [abstract] "With optimized implementation including advantage normalization, large batch size, and exponential moving average for reference model updates"
  - [section 5] "The most significant benefit is brought by using a large batch size, especially on code generation tasks"
  - [section 5] "Advantage normalization stabilizes PPO training and improves the performance of PPO"
- Break condition: When batch size is too small to capture sufficient diversity or when advantage normalization is improperly implemented

## Foundational Learning

- **Concept**: Reinforcement Learning from Human Feedback (RLHF)
  - Why needed here: The paper compares two RLHF methods (PPO and DPO) and analyzes their strengths/weaknesses
  - Quick check question: What are the two main components of RLHF and how do they differ between PPO and DPO approaches?

- **Concept**: Proximal Policy Optimization (PPO)
  - Why needed here: PPO is one of the two main algorithms being compared, and the paper investigates its key implementation factors
  - Quick check question: How does PPO's KL regularization term help prevent policy collapse during training?

- **Concept**: Direct Preference Optimization (DPO)
  - Why needed here: DPO is the other main algorithm being compared, and the paper demonstrates its fundamental limitations
  - Quick check question: What is the closed-form solution that DPO uses to derive its policy update rule?

## Architecture Onboarding

- **Component map**: Base LLM (SFT model) -> Reward model (for PPO) -> Reference model (for KL regularization) -> Preference dataset -> PPO vs DPO training loops

- **Critical path**:
  1. Start with SFT-trained LLM
  2. For PPO: train reward model, then run PPO with KL regularization
  3. For DPO: directly optimize policy on preference pairs
  4. Evaluate on downstream tasks

- **Design tradeoffs**:
  - PPO: More stable but requires reward model training; computationally expensive
  - DPO: Simpler but sensitive to distribution shift; no reward model needed
  - Large batch size: Better stability but higher memory requirements
  - Advantage normalization: Reduces variance but adds complexity

- **Failure signatures**:
  - PPO: Reward hacking, policy collapse without KL regularization
  - DPO: Exploitation of OOD responses, performance degradation with distribution shift
  - Both: Overfitting to preference dataset, poor generalization

- **First 3 experiments**:
  1. Compare PPO vs DPO on HH-RLHF with default settings
  2. Test PPO with varying batch sizes on APPS dataset
  3. Evaluate DPO with different reference models on SafeRLHF dataset

## Open Questions the Paper Calls Out

- **Open Question 1**: How can we effectively train a robust reward model for RLHF that mitigates the reward misspecification issue?
  - Basis in paper: The paper mentions that the reward model is significant in both PPO and DPO-Iter training processes but states they haven't delved into discussing how to effectively train a robust reward model.
  - Why unresolved: The paper identifies the importance of the reward model but doesn't provide specific methods or techniques for improving its robustness, leaving this as an open area for research.
  - What evidence would resolve it: A comprehensive study comparing different reward model training techniques (e.g., data augmentation, ensemble methods, adversarial training) on their ability to reduce reward misspecification and improve RLHF performance across various tasks.

- **Open Question 2**: What is the optimal balance between collecting new preference data through model generation and using existing preference data for iterative DPO training?
  - Basis in paper: The paper suggests using iterative DPO to mitigate distribution shift but notes that excessively discarding high-quality data could be detrimental. It also mentions the need for careful annotation of model-generated samples.
  - Why unresolved: The paper demonstrates the benefits of iterative DPO but doesn't provide specific guidelines on how much new data to collect versus using existing data, or how to efficiently annotate generated samples.
  - What evidence would resolve it: A systematic study examining the trade-offs between data collection methods (e.g., model-generated vs. human-annotated) and annotation strategies on DPO performance, including analysis of annotation cost vs. performance improvement.

- **Open Question 3**: How do DPO and PPO perform on more diverse types of human feedback beyond pairwise preferences, such as multi-preference comparisons or scalar ratings?
  - Basis in paper: The paper focuses on pairwise preference data and doesn't explore other feedback formats that might be used in RLHF.
  - Why unresolved: The study's scope is limited to pairwise preferences, leaving open questions about how DPO and PPO would adapt to different feedback structures that might better capture nuanced human preferences.
  - What evidence would resolve it: Empirical comparisons of DPO and PPO performance using various feedback formats (e.g., multi-preference comparisons, Likert scale ratings, free-form text feedback) across multiple tasks, with analysis of how each method handles different feedback structures.

## Limitations

- The distribution shift analysis relies on synthetic preference datasets that may not fully capture real-world preference data characteristics
- Performance improvements from large batch sizes and advantage normalization require substantial computational resources
- The analysis assumes the reference model captures reasonable priors but doesn't explore cases where the reference model itself is poorly aligned

## Confidence

**High Confidence**: PPO consistently outperforming DPO on both dialogue and code tasks, the importance of KL regularization in preventing policy collapse, and the effectiveness of large batch sizes and advantage normalization for PPO stability.

**Medium Confidence**: The theoretical analysis of distribution shift exploitation in DPO, as it relies on simplified assumptions about preference dataset coverage. The superiority of PPO on code generation tasks, as code evaluation metrics can be sensitive to implementation details.

**Low Confidence**: The claim that DPO's performance can be improved by incorporating reference model regularization, as this would essentially make it equivalent to PPO and wasn't empirically validated.

## Next Checks

1. **Distribution Shift Robustness**: Test DPO and PPO on preference datasets with varying levels of distribution shift from the base model to quantify the impact on performance degradation.

2. **Computational Efficiency Analysis**: Compare the wall-clock training time and memory requirements for PPO with large batch sizes versus DPO across different hardware configurations.

3. **Reference Model Quality Impact**: Evaluate how the quality and alignment level of the reference model affects both PPO and DPO performance, including cases where the reference model is poorly aligned with human preferences.