---
ver: rpa2
title: Subgoal Discovery Using a Free Energy Paradigm and State Aggregations
arxiv_id: '2412.16687'
source_url: https://arxiv.org/abs/2412.16687
tags:
- state
- learning
- states
- space
- environment
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces a novel method for subgoal discovery in reinforcement
  learning using a free energy paradigm and state aggregations. The authors propose
  detecting bottleneck states by monitoring model changes between neighboring states
  using a free energy-based approach.
---

# Subgoal Discovery Using a Free Energy Paradigm and State Aggregations

## Quick Facts
- arXiv ID: 2412.16687
- Source URL: https://arxiv.org/abs/2412.16687
- Reference count: 13
- Primary result: A novel method for subgoal discovery using free energy paradigm and state aggregations that identifies bottleneck states in stochastic environments up to 50% action failure probability

## Executive Summary
This paper introduces a novel approach to subgoal discovery in reinforcement learning that leverages free energy minimization and state aggregations to identify bottleneck states. The method detects subgoals by monitoring changes in optimal policy space selection between neighboring states, where model changes indicate unpredictability and thus potential subgoals. Unlike trajectory-based methods, this approach directly observes policy space selection dynamics, making it robust to high levels of environmental stochasticity. Experimental results on grid-world navigation tasks demonstrate that the method can identify bottleneck states without prior knowledge and outperforms frequency-based methods in stochastic environments.

## Method Summary
The method implements SARSA learning with two parallel representations: a main space (identity mapping) and an aggregation space (neighboring states within distance L). At each state, free energy is computed for both spaces using Thompson sampling policy estimates, and the space with lower free energy is selected for learning. Bottleneck states are detected by counting model changes - instances where the optimal space selection flips between neighboring states. The method applies Otsu's thresholding on the model changes matrix and uses non-maximum suppression to identify bottleneck states. The approach extends to continuous state spaces by discretizing states and computing expected free energy over the neighborhood.

## Key Results
- Successfully identifies bottleneck states in grid-world navigation tasks without prior knowledge
- Demonstrates robustness to high environmental stochasticity (up to 50% action failure probability)
- Outperforms frequency-based methods in stochastic environments
- Extends to continuous state spaces with modified implementation

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Bottleneck states are identified by tracking changes in the optimal policy space selection between neighboring states.
- Mechanism: When an agent moves from one state to a bottleneck state, the optimal space selection flips from aggregation to main space due to increased uncertainty in aggregated representations. This flip is counted as a model change, and high model change counts indicate bottleneck states.
- Core assumption: Subgoal/bottleneck states have inherently higher unpredictability in their aggregated representations compared to regular states.
- Evidence anchors:
  - [abstract] "The model changes from neighboring states to a given state shows the unpredictability of a given state, and therefore it is used in this paper for subgoal discovery."
  - [section] "If by entering a state from its neighboring states, there is a change between the best spaces, we count up the value of the model change of that particular state"
- Break condition: If the aggregation space doesn't capture policy uncertainty effectively (e.g., in environments with uniform optimal policies), model changes won't differentiate bottlenecks from regular states.

### Mechanism 2
- Claim: Free energy minimization selects the most predictable space (main or aggregation) for learning at each state.
- Mechanism: The method calculates free energy for both spaces using Thompson sampling policy estimates and selects the space with lower free energy. This selection process inherently captures which representation is more predictable at each state.
- Core assumption: The free energy formulation (combining complexity and accuracy terms) provides a reliable measure of predictability for policy learning.
- Evidence anchors:
  - [abstract] "This is achieved by using free energy to select between two spaces, the main space and an aggregation space."
  - [section] "Following (Ghorbani et al. 2025), the best space between the main and the aggregation space is the one that minimizes the free energy given by (5)"
- Break condition: If the free energy formulation doesn't properly balance exploration-exploitation (incorrect α, β parameters), the space selection becomes unreliable.

### Mechanism 3
- Claim: Model changes are robust to high environmental stochasticity (up to 50% action failure probability).
- Mechanism: Unlike trajectory-based methods that rely on graph construction from agent experiences, model changes directly observe policy space selection dynamics, making them less sensitive to stochastic transitions.
- Core assumption: The policy space selection process (based on free energy) remains stable enough to detect systematic changes even in highly stochastic environments.
- Evidence anchors:
  - [abstract] "Our proposed method is also robust to the stochasticity of environments."
  - [section] "As shown in Figure 8, our algorithm successfully identified bottleneck states in environments with a high probability of action failure p = 50%"
- Break condition: If stochasticity exceeds the method's tolerance threshold (e.g., >50% failure probability), the model changes may become too noisy to reliably identify bottlenecks.

## Foundational Learning

- Concept: Free Energy Principle
  - Why needed here: Forms the theoretical foundation for selecting between main and aggregation spaces based on predictability
  - Quick check question: What are the two components of the free energy formulation used in this method?

- Concept: Thompson Sampling for Uncertainty Estimation
  - Why needed here: Provides approximate belief distributions for state-action values needed to compute free energy
  - Quick check question: How does the method approximate Thompson sampling when exact belief distributions are computationally intractable?

- Concept: State Aggregation and Perceptual Aliasing
  - Why needed here: Understanding how aggregating states can create uncertainty that reveals bottleneck states
  - Quick check question: What problem arises when aggregated states have different optimal policies, and how does the method address it?

## Architecture Onboarding

- Component map:
  - Main Space (mM main) -> Identity function mapping states to themselves
  - Aggregation Space (mAgg) -> Aggregates neighboring states within distance L
  - Free Energy Evaluator -> Computes free energy for both spaces
  - Model Change Counter -> Tracks flips in optimal space selection
  - Bottleneck Detector -> Applies Otsu's thresholding and non-maximum suppression

- Critical path: Agent interaction → Q-value updates → Thompson sampling estimation → Free energy evaluation → Space selection → Model change counting → Bottleneck detection

- Design tradeoffs:
  - Aggregation distance L: Larger L captures broader context but may blur bottleneck signals
  - Stochasticity tolerance: Method trades off between sensitivity and robustness
  - Computational complexity: O(|S||A|²) for free energy calculation per state

- Failure signatures:
  - No model changes detected: Aggregation distance too small or policy too uniform
  - False positives in hallways: Aggregation distance too large, capturing multiple rooms
  - Sensitivity to hyperparameters: α and β values not tuned properly

- First 3 experiments:
  1. Test on simple 2-room environment with varying action failure probabilities (0%, 33%, 50%)
  2. Vary aggregation distance L and observe effect on bottleneck detection accuracy
  3. Compare model changes vs. frequency-based methods in high-stochasticity environments

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the model change approach perform in continuous action spaces, not just continuous state spaces?
- Basis in paper: [explicit] The paper mentions the method extends to continuous state spaces with modifications, but does not address continuous action spaces.
- Why unresolved: The authors only discuss extending their method to continuous state spaces, leaving open whether the model change approach would work similarly for continuous actions.
- What evidence would resolve it: Experiments demonstrating model change detection in environments with continuous action spaces, showing whether the approach requires significant modifications or performs similarly to discrete actions.

### Open Question 2
- Question: What is the optimal value for the maximum neighborhood distance parameter L in the aggregation space, and how sensitive is the method to this choice?
- Basis in paper: [explicit] The paper uses L=2 for all experiments but notes that careful parameter tuning could lead to different subgoal discoveries, particularly in the 1-room with hallway environment.
- Why unresolved: The authors do not systematically explore how different values of L affect subgoal discovery performance or whether there is an optimal value that generalizes across environments.
- What evidence would resolve it: A parameter sensitivity analysis showing subgoal discovery performance across different values of L, identifying whether there is an optimal range and how performance degrades as L deviates from optimal.

### Open Question 3
- Question: How does the proposed method compare to model-based approaches that explicitly learn environment transitions when discovering subgoals?
- Basis in paper: [inferred] The paper focuses on model-free RL and mentions model changes as a key concept, but does not compare to approaches that build explicit transition models for subgoal discovery.
- Why unresolved: The authors only compare to experience-based methods and graph-theoretic approaches, leaving open whether incorporating explicit model learning would improve or degrade subgoal discovery.
- What evidence would resolve it: Experiments comparing the proposed method to a model-based subgoal discovery approach that learns transition probabilities, measuring both subgoal quality and computational efficiency.

## Limitations

- The method requires careful tuning of aggregation distance parameter L, which may not generalize across environments
- Extension to continuous state spaces relies on discretization, potentially losing important state information
- The approach has not been validated on non-grid environments or real-world robotics applications

## Confidence

- High: Free energy-based space selection mechanism, bottleneck detection via model changes
- Medium: Robustness to stochasticity, performance vs frequency-based methods
- Low: Extension to continuous state spaces, generalization to non-grid environments

## Next Checks

1. Test the method on stochastic environments with action failure probabilities exceeding 50% to determine the upper bounds of its robustness.
2. Compare the free energy-based approach against multiple state aggregation strategies (different L values, alternative aggregation functions) to assess sensitivity to design choices.
3. Validate the continuous state space extension on benchmark RL environments (e.g., MuJoCo, PyBullet) to test real-world applicability beyond grid-worlds.