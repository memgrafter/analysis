---
ver: rpa2
title: Guiding Catalogue Enrichment with User Queries
arxiv_id: '2406.07098'
source_url: https://arxiv.org/abs/2406.07098
tags:
- triplets
- guidance
- query
- pairs
- queries
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of low precision in knowledge
  graph completion methods due to the huge search space of possible triplets and varying
  relevance of candidate facts to users. The authors propose a method to guide missing
  triplet prediction using user query logs, extracting entity-predicate pairs from
  SPARQL queries that are more likely to be correct and relevant.
---

# Guiding Catalogue Enrichment with User Queries

## Quick Facts
- arXiv ID: 2406.07098
- Source URL: https://arxiv.org/abs/2406.07098
- Reference count: 37
- One-line primary result: Query guidance improves KG completion precision from 0.345 to 0.950 and relevance from 0.220 to 0.895 on DBpedia

## Executive Summary
This paper tackles the challenge of low precision in knowledge graph completion (KGC) due to the vast search space of possible triplets and varying relevance of candidate facts to users. The authors propose using user query logs to guide missing triplet prediction by extracting entity-predicate pairs from SPARQL queries. Their approach significantly reduces the search space and improves both correctness and relevance of predictions. Evaluated on DBpedia and YAGO 4, query guidance outperforms alternatives like KG metadata or KGE scores, identifying entity-predicate pairs at least two times more likely to be correct.

## Method Summary
The method extracts entity-predicate pairs from user SPARQL SELECT queries and uses them to guide KGE-based prediction of missing triplets. After preprocessing the KGs to remove irrelevant entities and filtering extracted pairs against the KG schema, the system predicts 10 million triplets not in the training or development sets using rejection sampling based on RotatE KGE scores. Evaluation combines automatic metrics (precision on test set) with human annotation for correctness and relevance. The approach focuses only on SELECT queries and filters entity-predicate pairs by KG metadata compatibility to improve precision.

## Key Results
- Query guidance improves correctness from 0.345 to 0.950 and relevance from 0.220 to 0.895 on DBpedia
- Query guidance outperforms metadata and KGE score-based guidance, identifying entity-predicate pairs at least two times more likely to be correct
- On DBpedia, Pair Precision improves from 0.0106 (baseline) to 0.3467 (query guidance)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Query logs reduce the KG completion search space from |E| × |P| × |E| to |E| by providing relevant entity-predicate pairs.
- Mechanism: SPARQL SELECT queries naturally expose entity-predicate pairs that users expect to be valid. These pairs are extracted and used as guidance for KGE models, restricting prediction to the object entity given the subject and predicate.
- Core assumption: User queries reflect correct and relevant entity-predicate relationships, i.e., their frequency signals both correctness and relevance.
- Evidence anchors:
  - [abstract] "We extract entity-predicate pairs from user queries, which are more likely to be correct and relevant"
  - [section] "The existence of queries can help as a heuristic to evaluate the correctness and relevance of properties"
  - [corpus] No direct corpus evidence supporting this claim; paper relies on internal empirical evaluation.
- Break condition: If query logs contain many irrelevant or malformed queries, or if the KG contains entities/predicates not covered by the logs, the reduction in search space and the correctness signal will degrade.

### Mechanism 2
- Claim: Query guidance improves both correctness and relevance of predicted triplets, outperforming guidance based on KG metadata or KGE scores.
- Mechanism: By filtering entity-predicate pairs through user query patterns, the system biases predictions toward triplets users actually care about, not just structurally possible ones. This leads to higher human-annotated correctness (0.950 vs 0.345) and relevance (0.895 vs 0.220) on DBpedia.
- Core assumption: There is a strong correlation between query frequency and both the logical correctness and the user-perceived relevance of a triplet.
- Evidence anchors:
  - [section] "query guidance drastically improve the quality of missing triplet prediction" (Table 1: Pair Precision 0.3467 vs 0.0106)
  - [section] Human evaluation shows "query guidance can improve both correctness and relevance of the predictions by a large margin"
  - [corpus] No direct external corpus evidence; relies on the paper's own evaluation.
- Break condition: If user queries are sparse, biased, or not representative of actual KG usage, the method will not outperform metadata or score-based guidance.

### Mechanism 3
- Claim: Query guidance enables better prediction of the second entity (object) by providing a strong prior on the predicate.
- Mechanism: Even when the entity-predicate pair itself is correct, the KGE model benefits from the query-based prior, leading to higher precision in predicting the object entity (as seen in the smaller gap between RS and QG in Pair Precision vs #Hit Triplets).
- Core assumption: KGE models, when given a high-quality entity-predicate pair, can more accurately predict the object entity.
- Evidence anchors:
  - [section] "KGE models predict the second entities more accurately based on entity-predicates extracted from user queries, compared with based on the ones that are randomly sampled"
  - [section] "QG not only offers more correct and relevant entity-predicate pairs, but also helps KGE models predict better"
  - [corpus] No external validation of this mechanism; internal evaluation only.
- Break condition: If the KGE model's ability to predict objects is poor regardless of the predicate, or if query-based priors are misleading, this benefit will not materialize.

## Foundational Learning

- Concept: Knowledge Graph Embeddings (KGE)
  - Why needed here: The method uses RotatE embeddings to score and predict missing triplets; understanding KGE is essential to grasp how the rejection sampling and scoring works.
  - Quick check question: In RotatE, what does the score function sr(h, t) = γ - ||h ◦ r - t|| measure?

- Concept: SPARQL SELECT queries and RDF triples
  - Why needed here: The method extracts entity-predicate pairs from SPARQL queries, which are central to the guidance mechanism.
  - Quick check question: What part of a SPARQL SELECT query corresponds to the entity and predicate in an RDF triple?

- Concept: Rejection sampling
  - Why needed here: The baseline and the guided method both rely on rejection sampling to generate predictions from unnormalized KGE scores.
  - Quick check question: In the context of KGE, what is the role of the margin γ in the acceptance probability p(accept) = esr(h,t)/eγ?

## Architecture Onboarding

- Component map:
  SPARQL query log parser -> entity-predicate pair extractor -> query guidance filter
  KGE model (RotatE) -> rejection sampler -> prediction generator
  Human evaluation pipeline (for correctness/relevance annotation)
  Automatic evaluation pipeline (for precision on test set)

- Critical path:
  1. Parse SPARQL logs to extract entity-predicate pairs
  2. Filter pairs against KG schema (entities/predicates exist)
  3. Use pairs as guidance for KGE-based rejection sampling
  4. Generate predictions and evaluate

- Design tradeoffs:
  - Using only SELECT queries simplifies guidance but ignores other query types that might contain useful signals.
  - Filtering by KG metadata compatibility can improve precision but may discard correct pairs if metadata is incomplete.
  - Rejection sampling is simple but may be inefficient if the score distribution is flat.

- Failure signatures:
  - Low precision despite high query coverage -> possible query bias or poor KGE model
  - High variance in human evaluation -> possible ambiguity in correctness/relevance definitions
  - No improvement over RS baseline -> guidance extraction or filtering may be ineffective

- First 3 experiments:
  1. Verify entity-predicate extraction from SPARQL logs against a small, manually curated set.
  2. Compare rejection sampling output with and without query guidance on a toy KG.
  3. Run human evaluation on a small sample of predictions to validate correctness/relevance annotation guidelines.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the proposed query-guided approach scale when applied to commercial KGs with billions of triples compared to the evaluated DBpedia and YAGO 4 datasets?
- Basis in paper: [explicit] The authors mention that commercial KGs and query logs are usually confidential and they evaluate on public general-purpose KGs instead. They also state their approach can be "easily adapted to commercial KGs" but do not provide experimental validation.
- Why unresolved: The paper only evaluates on relatively smaller general-purpose KGs (DBpedia900K with 1.35M triples and YAGO5M with 12.92M triples) and does not demonstrate performance on large-scale commercial KGs.
- What evidence would resolve it: Experiments showing the approach's effectiveness on commercial KGs with billions of triples, including runtime and resource usage comparisons with the baseline.

### Open Question 2
- Question: What is the optimal frequency threshold for query logs to be considered meaningful guidance for KG completion?
- Basis in paper: [inferred] The authors extract entity-predicate pairs from user queries and use them for guidance, but they do not specify any frequency threshold or discuss how to filter out noisy or infrequent queries that might not represent meaningful user interests.
- Why unresolved: The paper uses all SELECT queries from the logs without discussing whether all queries should be treated equally or if some filtering based on frequency or other criteria would be beneficial.
- What evidence would resolve it: Empirical studies showing how different frequency thresholds for query inclusion affect the precision and relevance of the predictions.

### Open Question 3
- Question: How would the approach perform if applied to other types of KGs beyond general-purpose and product catalogs, such as scientific or biomedical KGs?
- Basis in paper: [explicit] The authors state their approach "can be easily adapted to commercial KGs" and mention various types of KGs (encyclopedia, domain-specific, task-specific) but only evaluate on encyclopedia KGs.
- Why unresolved: The paper only tests on DBpedia and YAGO 4 (both general-purpose encyclopedia KGs) and does not explore whether the approach generalizes to other KG types with different characteristics and query patterns.
- What evidence would resolve it: Experiments applying the approach to domain-specific KGs (e.g., scientific literature KGs, biomedical KGs) and comparing performance across different KG types.

## Limitations

- Limited external validation of the core assumption that query frequency correlates with correctness and relevance
- No comparison against alternative guidance sources like external knowledge bases or user feedback
- Human evaluation methodology not fully described, making annotation reliability difficult to assess

## Confidence

- Confidence in core claims: Medium
  - Empirical results are strong but mechanisms are largely asserted rather than independently verified
  - Comparison to alternatives is limited, and no ablation studies or sensitivity analyses provided

## Next Checks

1. Verify that the extracted entity-predicate pairs from SPARQL logs actually appear as valid, correct relationships in the KG by manual spot-checking a random sample.
2. Test the robustness of the method by varying the coverage threshold of query guidance (e.g., only use pairs that appear in >k queries) and measuring the impact on precision.
3. Evaluate the method on a KG and query log from a different domain (e.g., not DBpedia/YAGO) to assess generalizability and the dependence on query log quality.