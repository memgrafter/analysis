---
ver: rpa2
title: 'Weight-based Decomposition: A Case for Bilinear MLPs'
arxiv_id: '2406.03947'
source_url: https://arxiv.org/abs/2406.03947
tags:
- eigenvectors
- bilinear
- features
- decomposition
- layer
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The authors propose a method to decompose bilinear MLPs into interpretable
  eigenvector features, leveraging the linear structure of bilinear layers. By decomposing
  the third-order tensor representing the bilinear interactions, they derive a sparse
  set of orthonormal eigenvectors that capture the model's computations.
---

# Weight-based Decomposition: A Case for Bilinear MLPs

## Quick Facts
- arXiv ID: 2406.03947
- Source URL: https://arxiv.org/abs/2406.03947
- Reference count: 40
- The authors propose decomposing bilinear MLPs into interpretable eigenvector features, showing preliminary interpretability results on MNIST and Tiny Stories, and demonstrate finetuning pretrained models to bilinear variants with minimal data loss.

## Executive Summary
This paper introduces a method to improve the interpretability of neural networks by leveraging the linear structure of bilinear MLPs. The authors show that bilinear layers can be decomposed into interpretable eigenvector features by analyzing the third-order tensor representing their interactions. Through preliminary experiments on shallow image classifiers and small language models, they demonstrate that the top eigenvectors reveal meaningful patterns in model behavior. The work also shows that pretrained models can be converted to bilinear variants with minimal performance loss, opening possibilities for applying this interpretability technique to existing architectures.

## Method Summary
The core method involves decomposing bilinear MLPs by analyzing the third-order tensor that represents their interactions. For any output direction u, this tensor can be reduced to a symmetric matrix Q = u ·out B, which admits an eigendecomposition Q = P^T Λ P. The eigenvectors form an orthogonal basis with sparse interactions, where each eigenvector only interacts with itself. The authors apply regularization techniques like latent noise (adding dense Gaussian noise between layers) and weight decay to improve the interpretability of these eigenvector features. They demonstrate their approach on shallow MNIST and Fashion-MNIST classifiers, a single-layer transformer for language modeling, and show that pretrained models like TinyLlama-1.1B can be finetuned to bilinear variants.

## Key Results
- The eigendecomposition of bilinear layers yields interpretable eigenvector features that capture the model's computations
- Regularization techniques (latent noise and weight decay) improve the visual interpretability of eigenvectors on MNIST
- Pretrained language models can be finetuned to bilinear variants with minimal performance degradation
- Larger pretrained language models can be converted to bilinear form, suggesting applicability beyond simple architectures

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Bilinear MLPs can be decomposed into interpretable eigenvector features using the symmetric structure of their interaction matrices
- Mechanism: The third-order tensor representing bilinear interactions can be reduced to a symmetric matrix Q = u ·out B for any output direction u. This symmetric matrix admits an eigendecomposition Q = P^T Λ P, where eigenvectors form an orthogonal basis with sparse interactions (each eigenvector only interacts with itself).
- Core assumption: The interaction matrix derived from the tensor is symmetric, enabling standard eigendecomposition techniques.
- Evidence anchors:
  - [abstract] "By decomposing the third-order tensor representing the bilinear interactions, they derive a sparse set of orthonormal eigenvectors that capture the model's computations."
  - [section] "Since Q can be taken as symmetric (see subsection 3.3), the spectral theorem provides a decomposition of the form Q = Σ λ_i v_i v_i^T"
  - [corpus] Weak evidence - only 1 relevant paper mentions "bilinear" and "decomposition" together, but not specifically eigenvector decomposition

### Mechanism 2
- Claim: Regularization techniques like latent noise and weight decay improve the interpretability of eigenvector features.
- Mechanism: Latent noise (dense Gaussian noise added between layers) encourages sparse feature representations by forcing features to be robust enough to overcome random activations. Weight decay reduces fine-scale noisiness in the features.
- Core assumption: Adding noise between layers promotes the emergence of sparse, interpretable features similar to sparse autoencoders.
- Evidence anchors:
  - [section] "Latent noise regularization adds dense Gaussian noise to the inputs of each layer... We hypothesize that adding dense noise between each layer encourages a better handoff of information from one layer to the next"
  - [section] "Figure 5 shows that the features without regularization are noisy and have large values on the periphery where pixels are rarely 'on' which may be due to overfitting"
  - [corpus] Weak evidence - no direct corpus evidence supporting latent noise for interpretability

### Mechanism 3
- Claim: Bilinear MLPs can be finetuned from existing models with minimal data loss, enabling application to pretrained architectures.
- Mechanism: Since Swishβ interpolates between common activation functions (SiLU at β=1 and linear at β=0), a gated Swish can be used to gradually transition from SiLU to bilinear activations during finetuning.
- Core assumption: Swishβ provides a smooth interpolation between activation functions, allowing gradual transition during finetuning.
- Evidence anchors:
  - [abstract] "Application of our method may not be limited to pretrained bilinear models since we find that language models such as TinyLlama-1.1B can be finetuned into bilinear variants"
  - [section] "Since Swishβ interpolates between common activation functions including SiLU (β=1) and linear (β=0) we can use a gated Swish... to finetune between the gated SiLU and bilinear activations"
  - [corpus] Weak evidence - no corpus papers discussing finetuning from SiLU to bilinear

## Foundational Learning

- Concept: Tensor decompositions and higher-order SVD
  - Why needed here: The bilinear layer is represented as a third-order tensor, requiring tensor decomposition techniques to extract interpretable features
  - Quick check question: How does flattening a third-order tensor into a matrix affect the ability to extract interpretable features compared to the reduction approach described?

- Concept: Eigendecomposition of symmetric matrices
  - Why needed here: The interaction matrices are symmetric, enabling the spectral theorem to provide an orthonormal basis of eigenvectors
  - Quick check question: Why does the symmetry of the interaction matrix guarantee that eigenvalues are real and eigenvectors are orthonormal?

- Concept: Regularization and noise injection in neural networks
  - Why needed here: Latent noise and weight decay are used to improve feature interpretability, requiring understanding of how regularization affects learned representations
  - Quick check question: How does adding noise between layers differ from traditional dropout or L1/L2 regularization in terms of its effect on feature sparsity?

## Architecture Onboarding

- Component map: Input → embedding → bilinear MLP (tensor computation) → unembedding → output
- Critical path: Input → embedding → bilinear MLP (tensor computation) → unembedding → output
- Design tradeoffs: Bilinear MLPs have 33% more parameters than standard MLPs but enable weight-based interpretability; regularization improves interpretability but may affect performance
- Failure signatures: Non-symmetric interaction matrices preventing eigendecomposition; poor performance after truncation to top eigenvectors; failure to converge during finetuning from SiLU to bilinear
- First 3 experiments:
  1. Verify eigendecomposition works on a simple bilinear layer with known weights by checking that reconstructed weights match original
  2. Test effect of different regularization strengths on MNIST eigenvector interpretability by visualizing features with/without latent noise
  3. Attempt finetuning of a small SiLU-based model to bilinear by gradually reducing β parameter while monitoring loss

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do regularization techniques like latent noise and weight decay affect the interpretability of eigenvectors in different model architectures beyond MNIST?
- Basis in paper: [explicit] The paper shows that latent noise and weight decay improve the interpretability of MNIST eigenvectors.
- Why unresolved: The study only examined these effects on shallow MNIST models. It is unclear whether similar improvements would occur in deeper networks, language models, or other domains.
- What evidence would resolve it: Experiments applying latent noise and weight decay to a variety of architectures (e.g., deeper CNNs, transformer-based language models, multimodal models) and measuring eigenvector interpretability using standardized metrics.

### Open Question 2
- Question: Can the eigenvector decomposition be combined with sparse autoencoders (SAEs) to produce sparse, monosemantic features while preserving the interpretability benefits of the eigenvector basis?
- Basis in paper: [inferred] The paper discusses limitations of eigenvector polysemanticity and suggests combining eigendecomposition with SAEs as a potential solution.
- Why unresolved: The paper does not implement or test this combination, leaving the effectiveness and trade-offs unknown.
- What evidence would resolve it: Experiments where eigenvectors are used as an initialization or feature basis for SAE training, comparing interpretability and sparsity metrics against SAEs trained directly on activations.

### Open Question 3
- Question: Does the finetuning approach for converting pretrained models to bilinear variants preserve task-specific knowledge and performance across diverse downstream tasks?
- Basis in paper: [explicit] The paper shows finetuning TinyLlama-1.1B to a bilinear variant on FineWeb data with minimal loss increase.
- Why unresolved: The finetuning was only tested on one dataset and one model size. It is unclear whether this approach generalizes to other architectures, tasks, or whether it preserves fine-tuned capabilities.
- What evidence would resolve it: Finetuning experiments on multiple pretrained models (different sizes, architectures) across diverse downstream tasks (e.g., classification, generation, reasoning) while measuring performance retention and task adaptation speed.

## Limitations

- The interpretability claims are primarily qualitative, relying on visual inspection rather than quantitative metrics
- The regularization effects are hypothesized but not rigorously validated through controlled ablation studies
- Finetuning results are preliminary with limited evaluation of functional capabilities beyond loss metrics

## Confidence

- Medium: The tensor decomposition mechanism is mathematically valid, but the connection to mechanistic interpretability is primarily observational
- Medium: Regularization improves visual quality of features, but causal links to interpretability are not established
- Low: Finetuning results are preliminary with limited quantitative evaluation of the converted models' capabilities

## Next Checks

1. Conduct quantitative tests comparing eigenvector-based feature reconstructions against random projections to establish whether the decomposition captures more information than chance
2. Implement controlled ablation studies testing different regularization schemes (L1, dropout, latent noise) to determine which most effectively improves interpretability without sacrificing performance
3. Evaluate finetuned bilinear models on held-out tasks to verify they maintain functional capabilities beyond just low loss values on training data