---
ver: rpa2
title: 'Beyond Text-Visual Attention: Exploiting Visual Cues for Effective Token Pruning
  in VLMs'
arxiv_id: '2412.01818'
source_url: https://arxiv.org/abs/2412.01818
tags:
- attention
- tokens
- token
- visual
- distribution
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the computational inefficiency of large vision-language
  models (LVLMs), which process significantly more visual tokens than textual ones.
  The authors analyze text-visual attention in LVLMs and find it is not an ideal indicator
  for token pruning due to positional bias and attention dispersion.
---

# Beyond Text-Visual Attention: Exploiting Visual Cues for Effective Token Pruning in VLMs

## Quick Facts
- **arXiv ID**: 2412.01818
- **Source URL**: https://arxiv.org/abs/2412.01818
- **Reference count**: 40
- **Primary result**: VisPruner reduces LLaVA-1.5-7B FLOPs by 91% and latency by 75% without training, outperforming text-visual attention-based methods

## Executive Summary
This paper addresses the computational inefficiency of large vision-language models (LVLMs) by proposing VisPruner, a plug-and-play method that exploits visual cues for effective token pruning. The authors demonstrate that traditional text-visual attention is not an ideal indicator for token pruning due to positional bias and attention dispersion. VisPruner instead uses visual encoder attention to select important tokens and removes duplicates based on similarity to preserve diverse visual information. The method achieves significant computational savings (91% FLOPs reduction) while maintaining model performance across various VLM architectures and benchmarks.

## Method Summary
VisPruner is a pre-processing method that prunes visual tokens before they enter the language model. It first uses visual encoder attention to select a limited number of significant tokens, focusing on important foreground objects. Then, it removes duplicate tokens from the remaining ones based on cosine similarity, preserving diverse tokens that cover different image regions. This two-stage approach combines important tokens (from visual attention) with diverse tokens (from similarity-based removal) to maximize visual information retention while achieving substantial computational savings. The method is architecture-agnostic and requires no training.

## Key Results
- VisPruner reduces LLaVA-1.5-7B FLOPs by 91% and inference latency by 75% without any training
- Outperforms existing text-visual attention-based pruning methods across various VLM architectures
- Maintains comparable performance on multiple vision-language benchmarks including VQAv2, GQA, and TextVQA
- Achieves 94.4% reduction (retaining only 5.6% of tokens) while preserving model functionality

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Text-visual attention in language models is not an ideal indicator for token pruning due to positional bias and attention dispersion.
- **Mechanism**: The positional bias arises from the long-term decay property of rotary position embedding, causing text tokens to pay more attention to visual tokens physically closer to them (lower parts of images). Attention dispersion results in a uniform attention distribution across many visual tokens, making it difficult to identify truly important ones.
- **Core assumption**: The attention scores from text tokens to visual tokens can reliably indicate visual token importance if positional bias and dispersion are not present.
- **Evidence anchors**:
  - [abstract] "However, in this study, we first analyze the text-visual attention in the language model and find that this score is not an ideal indicator for token pruning."
  - [section] "We conduct an in-depth analysis of text-visual attention in the language model and make two findings: (1) Text-visual attention shift... (2) Text-visual attention dispersion..."
  - [corpus] "TransPrune: Token Transition Pruning for Efficient Large Vision-Language Model" suggests similar concerns about attention-based pruning.

### Mechanism 2
- **Claim**: Visual encoder attention provides a more concentrated and reliable signal for identifying important visual tokens compared to text-visual attention.
- **Mechanism**: The [CLS] token in the visual encoder attends more concentratedly to key foreground objects, while text-visual attention in the language model is more dispersed across the entire image.
- **Core assumption**: The visual encoder's attention mechanism is better at identifying semantically important visual regions than the language model's text-visual attention.
- **Evidence anchors**:
  - [abstract] "We propose VisPruner, a plug-and-play method that utilizes visual cues for more effective token pruning in LVLMs. Specifically, we first use visual attention to select a limited number of significant tokens."
  - [section] "To our surprise, in the shallower layers of the model, visual tokens located in the central region of the image maintain the highest performance, rather than those positioned later, which received the most text attention."
  - [corpus] "PLPHP: Per-Layer Per-Head Vision Token Pruning for Efficient Large Vision-Language Models" also uses visual encoder attention for pruning decisions.

### Mechanism 3
- **Claim**: Combining important tokens with diverse tokens through similarity-based removal preserves both key information and background context.
- **Mechanism**: First, important tokens are selected based on visual encoder attention. Then, among remaining tokens, duplicates with high similarity are removed to retain diverse tokens that cover different image regions.
- **Core assumption**: Visual encoder attention focuses primarily on foreground objects, while background information is equally important for comprehensive understanding.
- **Evidence anchors**:
  - [abstract] "Then, we remove duplicate tokens from the remaining ones based on their similarity. By retaining diverse tokens alongside the initially selected important tokens, we maximally preserve the visual information of the input image."
  - [section] "Considering that attention in visual encoders often focuses more on foreground objects [6, 13], to prevent performance degradation on background-related questions after pruning, we also retain a set of diverse tokens as a complement."
  - [corpus] "FrameFusion: Combining Similarity and Importance for Video Token Reduction on Large Vision Language Models" suggests similar diversity preservation strategies.

## Foundational Learning

- **Concept**: Rotary Position Embedding (RoPE) and its long-term decay property
  - Why needed here: Understanding why text tokens pay more attention to lower-positioned visual tokens requires knowledge of how RoPE affects attention patterns.
  - Quick check question: How does the RoPE matrix cause tokens later in the sequence to have higher attention scores for tokens physically closer to them?

- **Concept**: Attention mechanisms in Vision Transformers vs. Language Transformers
  - Why needed here: The paper exploits differences between global attention in visual encoders and causal attention in language models for effective pruning.
  - Quick check question: What are the key architectural differences between how visual encoders and language models compute attention, and how do these differences affect token pruning strategies?

- **Concept**: Token similarity metrics and diversity preservation
  - Why needed here: The method removes duplicate tokens based on cosine similarity to retain diverse visual information.
  - Quick check question: How does cosine similarity between token embeddings relate to visual diversity, and why is preserving diversity important for maintaining model performance after pruning?

## Architecture Onboarding

- **Component map**: Image → Visual Encoder → VisPruner (pruning) → Projector → Language Model → Output
- **Critical path**: The pruning decision happens immediately after the visual encoder and before the projector, making it independent of language model attention patterns.
- **Design tradeoffs**:
  - Early pruning (before language model) vs. late pruning (within language model): Early pruning enables compatibility with FlashAttention but may miss task-specific importance signals.
  - Visual attention vs. text-visual attention: Visual attention is more concentrated but may miss task-specific importance; text-visual attention is task-aware but suffers from positional bias.
  - Important tokens vs. diverse tokens: Balancing foreground focus with background coverage is crucial for comprehensive understanding.
- **Failure signatures**:
  - Performance degradation on tasks requiring background context: Indicates insufficient diversity in retained tokens.
  - Sensitivity to input resolution changes: May indicate that pruning ratios need to be adaptive rather than fixed.
  - Incompatibility with certain VLM architectures: Some models may require different attention extraction methods.
- **First 3 experiments**:
  1. Compare performance of VisPruner using [CLS] attention vs. average visual token attention on a simple VQA task to validate the concentration hypothesis.
  2. Test different ratios of important vs. diverse tokens (e.g., 50/50, 70/30, 90/10) to find the optimal balance for a specific benchmark.
  3. Measure inference time with and without FlashAttention compatibility to quantify the efficiency gains from early pruning.

## Open Questions the Paper Calls Out

- **Question**: How does VisPruner's attention-based important tokens selection perform across different visual encoder architectures beyond CLIP and SigLIP?
- **Basis in paper**: [explicit] The paper mentions that VisPruner uses visual encoder attention to select important tokens, specifically noting CLIP's [CLS] attention and SigLIP's average attention across all tokens. However, it doesn't test this across a broader range of visual encoders.
- **Why unresolved**: The paper only tests with CLIP and SigLIP, leaving uncertainty about performance with other visual encoders like ConvNeXt, DINO, or other self-supervised models.
- **What evidence would resolve it**: Systematic testing of VisPruner across multiple visual encoder architectures (ConvNeXt, DINO, MAE, etc.) showing consistent performance improvements.

## Limitations

- The method's effectiveness across diverse VLM architectures beyond the tested ones (LLaVA-1.5, LLaVA-NeXT, Video-LLaVA) remains unverified, potentially limiting generalizability.
- The fixed pruning ratios may not be optimal for all input types, as image complexity and content diversity could require adaptive thresholds for best performance.
- While VisPruner maintains overall performance, detailed analysis of task-specific performance degradation on fine-grained visual reasoning tasks is lacking.

## Confidence

- **High confidence**: The core finding that text-visual attention suffers from positional bias and dispersion is well-supported by the analysis and aligns with known properties of RoPE.
- **Medium confidence**: The claim that visual encoder attention provides more reliable pruning signals is plausible but requires broader validation across diverse VLM architectures.
- **Medium confidence**: The effectiveness of combining important and diverse tokens is demonstrated empirically but lacks ablation studies isolating the contribution of each component.

## Next Checks

1. **Architecture ablation study**: Systematically test VisPruner across all mentioned VLM architectures (LLaVA, LLaVA-NeXT, Video-LLaVA, Qwen-VL, InternVL, CogVLM) on a standardized benchmark to quantify performance variation and identify architectural characteristics that affect pruning effectiveness.

2. **Task-type sensitivity analysis**: Perform detailed per-task performance analysis on the benchmarks, particularly comparing performance on tasks requiring fine-grained visual detail (e.g., TextVQA, POPE) versus those requiring only high-level understanding (e.g., GQA, VizWiz) to identify failure modes.

3. **Adaptive threshold validation**: Implement and test an adaptive pruning mechanism that adjusts the important/diverse token ratio based on image complexity metrics (visual entropy, object count, resolution) and evaluate whether this improves performance on the full range of input types.