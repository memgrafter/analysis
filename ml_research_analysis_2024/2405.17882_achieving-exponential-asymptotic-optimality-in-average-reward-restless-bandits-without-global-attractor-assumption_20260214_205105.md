---
ver: rpa2
title: Achieving Exponential Asymptotic Optimality in Average-Reward Restless Bandits
  without Global Attractor Assumption
arxiv_id: '2405.17882'
source_url: https://arxiv.org/abs/2405.17882
tags:
- arms
- optimal
- lemma
- policy
- state
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proposes the first policy that achieves exponential asymptotic
  optimality for average-reward restless bandits without assuming the strong global
  attractor property. The Two-Set Policy maintains two dynamic subsets of arms, applying
  Optimal Local Control to one subset and Exact Proportional Control to the other,
  gradually merging them.
---

# Achieving Exponential Asymptotic Optimality in Average-Reward Restless Bandits without Global Attractor Assumption

## Quick Facts
- arXiv ID: 2405.17882
- Source URL: https://arxiv.org/abs/2405.17882
- Reference count: 40
- Achieves exponential asymptotic optimality for average-reward restless bandits without global attractor assumption

## Executive Summary
This paper addresses the long-standing challenge of achieving exponential asymptotic optimality in average-reward restless bandits without requiring the strong global attractor property. The authors propose the Two-Set Policy, which maintains two dynamic subsets of arms and gradually merges them using Optimal Local Control and Exact Proportional Control. Under mild assumptions of aperiodic-unichain, non-degeneracy, and local stability, the policy achieves an optimality gap of O(exp(-CN)). The paper also establishes a lower bound showing that local stability is fundamental for exponential asymptotic optimality.

## Method Summary
The Two-Set Policy maintains two dynamic subsets of arms, applying Optimal Local Control to one subset and Exact Proportional Control to the other. The policy gradually merges these subsets over time, allowing it to achieve exponential convergence without requiring the global attractor property. The approach leverages three key assumptions: aperiodic-unichain (ensuring ergodicity), non-degeneracy (preventing singular transition structures), and local stability (ensuring stability of optimal actions). The policy parameters C and N control the convergence rate, with the optimality gap bounded by O(exp(-CN)).

## Key Results
- First policy achieving exponential asymptotic optimality without global attractor assumption
- Optimality gap of O(exp(-CN)) under aperiodic-unichain, non-degeneracy, and local stability assumptions
- Lower bound proof showing local stability is fundamental for exponential asymptotic optimality
- Three assumptions claimed to be easier to verify than global attractor property

## Why This Works (Mechanism)
The Two-Set Policy works by maintaining two dynamic subsets of arms and gradually merging them over time. This approach avoids the need for the global attractor property by instead relying on local stability of optimal actions. The policy uses Optimal Local Control on one subset and Exact Proportional Control on the other, allowing it to balance exploration and exploitation while ensuring convergence. The exponential convergence rate emerges from the careful design of the merging process and the control mechanisms applied to each subset.

## Foundational Learning
- Restless bandits: Sequential decision-making problems where arms evolve independently even when not played. Needed for understanding the problem domain and why standard multi-armed bandit solutions don't apply.
- Average-reward criterion: Evaluates policies based on long-term average reward rather than finite-horizon objectives. Quick check: Verify that the policy's performance is measured appropriately for the infinite-horizon setting.
- Local stability: Property ensuring that optimal actions remain stable under small perturbations. Needed to replace the global attractor assumption. Quick check: Confirm that local stability holds for the specific problem instance.
- Aperiodic-unichain: Ensures ergodicity of the Markov chain underlying the bandit problem. Needed to guarantee convergence. Quick check: Verify that all states communicate and the chain is aperiodic.

## Architecture Onboarding

**Component Map**
Two-Set Policy -> Optimal Local Control -> Subset A
                  -> Exact Proportional Control -> Subset B
                  -> Gradual Merging Mechanism

**Critical Path**
Initialization -> Subset Division -> Control Application -> Gradual Merging -> Convergence

**Design Tradeoffs**
The policy trades off between computational complexity and convergence rate by choosing the merging speed. Faster merging may lead to better exploration but potentially slower convergence. The choice of parameters C and N directly impacts the optimality gap and convergence speed.

**Failure Signatures**
- If local stability assumption is violated, the policy may fail to converge
- Incorrect parameter choices (C, N) can lead to suboptimal performance
- The aperiodic-unichain assumption ensures ergodicity; its violation prevents convergence

**First Experiments**
1. Implement Two-Set Policy on a small-scale restless bandit problem with known optimal solution to verify convergence
2. Test sensitivity to parameter choices (C and N) on benchmark problems
3. Compare performance against baseline policies that require global attractor assumption

## Open Questions the Paper Calls Out
The paper notes that the Three Assumptions (aperiodic-unichain, non-degeneracy, and local stability) are claimed to be "easier to verify" than global attractor, but doesn't provide concrete examples or complexity analysis comparing verification difficulty. The O(exp(-CN)) optimality gap depends critically on the constant C and parameter N, but the paper doesn't discuss how these scale with problem size or how to choose them in practice. The lower bound proof showing local stability is "fundamental" assumes specific problem structures that may not generalize.

## Limitations
- Three assumptions claimed easier to verify than global attractor, but no complexity analysis provided
- O(exp(-CN)) optimality gap depends on constants C and N with unclear scaling properties
- Lower bound proof assumes specific problem structures that may not generalize
- No concrete examples demonstrating verification ease of proposed assumptions

## Confidence
**High confidence**: The correctness of the Two-Set Policy algorithm and its basic structure
**Medium confidence**: The O(exp(-CN)) optimality gap proof, as it relies on technical conditions that may be tight
**Medium confidence**: The lower bound proof, as it assumes specific problem structures

## Next Checks
1. Implement the Two-Set Policy on benchmark restless bandit problems and empirically verify the exponential convergence rate across different problem sizes
2. Develop concrete examples demonstrating that the Three Assumptions are indeed easier to verify than global attractor in practice, with complexity analysis
3. Test the sensitivity of the optimality gap to parameter choices (C and N) and identify practical guidelines for parameter selection