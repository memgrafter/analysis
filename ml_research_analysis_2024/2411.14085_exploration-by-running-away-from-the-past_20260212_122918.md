---
ver: rpa2
title: Exploration by Running Away from the Past
arxiv_id: '2411.14085'
source_url: https://arxiv.org/abs/2411.14085
tags:
- exploration
- ramp
- state
- reward
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of exploration in reinforcement
  learning by framing it as maximizing the Shannon entropy of the state occupation
  measure. The proposed method, RAMP (Running Away from the Past), encourages exploration
  by iteratively maximizing the divergence between an agent's current behavior and
  its past experiences.
---

# Exploration by Running Away from the Past

## Quick Facts
- **arXiv ID**: 2411.14085
- **Source URL**: https://arxiv.org/abs/2411.14085
- **Reference count**: 35
- **Primary result**: RAMPW (Wasserstein variant) achieves up to 6x better state coverage than baselines in Ant locomotion task and over 40% improvement when combined with extrinsic rewards

## Executive Summary
This paper addresses the challenge of exploration in reinforcement learning by framing it as maximizing the Shannon entropy of the state occupation measure. The proposed method, RAMP (Running Away from the Past), encourages exploration by iteratively maximizing the divergence between an agent's current behavior and its past experiences. Two versions are proposed: RAMPKL using Kullback-Leibler divergence and RAMPW using Wasserstein distance. The algorithm is evaluated on mazes, locomotion tasks, and robotic manipulation tasks, demonstrating competitive performance against state-of-the-art baselines. RAMPW particularly excels in high-dimensional environments, achieving up to 6x better state coverage than baselines in the Ant locomotion task. When combined with extrinsic rewards, both RAMP variants show strong performance, with RAMPW achieving over 40% improvement in the Ant environment compared to the second-best baseline.

## Method Summary
RAMP addresses exploration by maximizing the Shannon entropy of the state occupation measure through iterative divergence maximization. The algorithm uses either Kullback-Leibler divergence or Wasserstein distance to measure divergence between current and past state distributions, encouraging the agent to visit novel states. RAMPKL uses a contrastive classifier to estimate the ratio between current and past distributions, while RAMPW uses temporal distance to encourage exploration of states temporally distant from past experiences. Both variants use Soft Actor-Critic (SAC) as the underlying RL method, maintaining replay buffers for current experiences and past mixture states. The agent collects new experiences using the current policy, updates the divergence estimator based on current vs past states, uses the divergence-based intrinsic reward to update the policy, and updates the past experience buffer with new experiences.

## Key Results
- RAMPW achieves up to 6x better state coverage than baselines in the Ant locomotion task
- When combined with extrinsic rewards, RAMPW achieves over 40% improvement in the Ant environment compared to the second-best baseline
- Both RAMP variants demonstrate competitive performance against state-of-the-art baselines across mazes, locomotion tasks, and robotic manipulation tasks
- RAMPW particularly excels in high-dimensional environments while RAMPKL shows stronger performance in lower-dimensional maze tasks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: RAMP achieves exploration by maximizing the Shannon entropy of the state occupation measure through iterative divergence maximization.
- Mechanism: The algorithm maximizes the divergence between current and past state distributions, using either KL divergence or Wasserstein distance, which forces the agent to visit novel states.
- Core assumption: High Shannon entropy of state occupation correlates with effective exploration in the environment.
- Evidence anchors:
  - [abstract]: "cast exploration as a problem of maximizing the Shannon entropy of the state occupation measure"
  - [section]: "Optimizing the diversity of occupied states at epoch n, by maximizing Hn, is an appealing objective for promoting exploration"
- Break condition: If the environment requires structured exploration (like goal-directed behavior) rather than pure coverage, entropy maximization alone may be insufficient.

### Mechanism 2
- Claim: KL divergence-based RAMP uses a contrastive classifier to estimate the ratio between current and past distributions.
- Mechanism: The classifier learns to distinguish states from the current policy versus states from the past mixture, producing a reward that encourages visiting states dissimilar to past experiences.
- Core assumption: The classifier can effectively estimate the log ratio between two distributions through contrastive learning.
- Evidence anchors:
  - [section]: "estimating the log of the ratio between two different distributions can be seen as a contrastive learning problem"
  - [section]: "by solving this simple classification problem, the output of the neural network (without the sigmoid activation) is exactly rDKL"
- Break condition: In high-dimensional spaces where past and present distributions overlap significantly, the classifier may struggle to distinguish them effectively.

### Mechanism 3
- Claim: Wasserstein distance-based RAMP uses temporal distance to encourage exploration of states temporally distant from past experiences.
- Mechanism: The Wasserstein reward function is approximated using a neural network trained with temporal distance constraints, encouraging exploration of states far from previously visited states in terms of environment steps.
- Core assumption: Temporal distance is a meaningful metric for characterizing "distance" between states in terms of exploration progress.
- Evidence anchors:
  - [section]: "using the Temporal Distance dtemp(s1, s2)...which represents the minimum number of steps that must be performed in a Markov chain in order to reach state s1 from s2"
  - [section]: "The agent quickly learns to move away from the initial distribution...ultimately resulting in a uniform distribution over the XY coordinates"
- Break condition: If the environment has symmetries or periodic structures, temporal distance may not capture meaningful exploration progress.

## Foundational Learning

- Concept: Shannon entropy maximization
  - Why needed here: Provides the theoretical foundation for why maximizing distribution diversity leads to exploration
  - Quick check question: Why does maximizing entropy encourage the agent to visit diverse states rather than focusing on high-reward states?

- Concept: Kullback-Leibler divergence
  - Why needed here: Used as one method to measure divergence between current and past state distributions
  - Quick check question: What happens to KL divergence when the support of two distributions becomes disjoint?

- Concept: Wasserstein distance and optimal transport
  - Why needed here: Alternative divergence measure that considers the geometry of the state space
  - Quick check question: How does Wasserstein distance differ from KL divergence in terms of what it measures about distribution differences?

## Architecture Onboarding

- Component map:
  Main policy network (πθ) -> Divergence estimator network (fϕ) -> SAC optimizer for policy updates -> Classifier/critic optimizer for divergence estimator

- Critical path:
  1. Collect new experiences using current policy
  2. Update divergence estimator based on current vs past states
  3. Use divergence-based intrinsic reward to update policy
  4. Update past experience buffer with new experiences

- Design tradeoffs:
  - KL divergence: Simpler implementation, may struggle in high-dimensional spaces
  - Wasserstein distance: More geometrically meaningful, computationally more expensive
  - Buffer update rate β: Trade-off between exploration freshness and stability

- Failure signatures:
  - KL variant: Agent explores by manipulating irrelevant state variables rather than meaningful exploration
  - Wasserstein variant: High computational cost with diminishing returns in simple environments
  - Both: Policy converges to exploiting intrinsic reward without discovering extrinsic rewards

- First 3 experiments:
  1. Verify KL classifier learns to distinguish current vs past states in a simple gridworld
  2. Test both variants on Ant locomotion task, comparing state coverage
  3. Add extrinsic reward and measure performance improvement in locomotion tasks

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the choice of β (mixing proportion between current and past experiences) affect the exploration efficiency and final performance of RAMP in different environments?
- Basis in paper: [explicit] The paper states "In practice, a sample of µn can be maintained using a replay buffer. At each epoch, the replay buffer is updated by retaining a proportion (1 − β) from the previous buffer and incorporating a proportion β from the new distribution" and discusses β ∈ (0, 1).
- Why unresolved: The paper uses fixed β values (e.g., β = 7 × 10^-3) but doesn't explore how varying β affects exploration dynamics or performance across different environments.
- What evidence would resolve it: Systematic experiments varying β across a range of values (e.g., 0.001 to 0.5) in different environments (mazes, locomotion tasks, robotic manipulation) to identify optimal β ranges for different task types.

### Open Question 2
- Question: How would RAMP perform with alternative density estimators beyond the simple classifier approach used in the current implementation?
- Basis in paper: [inferred] The paper mentions that "finding a relevant density estimator for an environment can be challenging" and compares with APT which uses a different density estimator, showing APT outperforms RAMP in some robotic control tasks.
- Why unresolved: The paper uses a simple classifier for density estimation but acknowledges this might not be optimal for all environments, particularly high-dimensional robotic control tasks.
- What evidence would resolve it: Comparative experiments using RAMP with different density estimators (e.g., particle-based methods like APT, or other neural network architectures) across various environments to measure performance differences.

### Open Question 3
- Question: How does RAMP compare to hierarchical exploration methods that combine skill discovery with intrinsic motivation?
- Basis in paper: [explicit] The paper states "RAMP takes a different approach by incrementally improving the Shannon entropy of the agent's state distribution over time, rather than directly maximizing it" and mentions that "This new perspective on exploration, driven by maximizing the divergence between successive distributions, has the potential to advance both theoretical insights and practical algorithms for exploration in reinforcement learning."
- Why unresolved: While RAMP is compared to non-hierarchical baselines, the paper acknowledges that "This new perspective on exploration... has the potential to advance both theoretical insights and practical algorithms" but doesn't explore how it might integrate with or compare to hierarchical methods.
- What evidence would resolve it: Experiments combining RAMP with skill-based hierarchical approaches or direct comparison with hierarchical exploration algorithms like DIAYN in challenging environments requiring multi-level exploration strategies.

## Limitations
- Performance gains primarily demonstrated on continuous control tasks where exploration is relatively well-defined
- Method's effectiveness in more complex, sparse-reward environments or those requiring hierarchical exploration remains unclear
- Computational overhead of maintaining and updating divergence estimators, particularly for the Wasserstein variant, could limit scalability

## Confidence
- **Exploration effectiveness claims**: High confidence - Well-supported by empirical results across multiple environments and clear comparison against baselines
- **Theoretical foundations**: Medium confidence - The entropy maximization framing is theoretically sound, but practical benefits over alternative exploration methods need further validation
- **Computational efficiency**: Low confidence - The paper does not provide detailed analysis of the computational overhead introduced by the divergence estimators and additional buffer management

## Next Checks
1. **Scalability test**: Evaluate RAMP on a more complex, high-dimensional environment with sparse rewards (e.g., Atari games or 3D navigation tasks) to verify if the state coverage advantages persist in truly challenging exploration scenarios.

2. **Ablation study**: Systematically remove components of the RAMP algorithm (e.g., test without the past buffer update, or with fixed divergence estimators) to quantify the contribution of each element to overall performance.

3. **Generalization analysis**: Test whether an agent trained with RAMP on one task can adapt more quickly to a related task compared to baselines, examining if the exploration strategy learned leads to better transfer learning capabilities.