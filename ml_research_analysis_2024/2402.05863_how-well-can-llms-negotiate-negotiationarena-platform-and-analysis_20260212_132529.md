---
ver: rpa2
title: How Well Can LLMs Negotiate? NegotiationArena Platform and Analysis
arxiv_id: '2402.05863'
source_url: https://arxiv.org/abs/2402.05863
tags:
- player
- game
- negotiation
- agents
- resources
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces NegotiationArena, a framework for evaluating
  and probing the negotiation abilities of large language model (LLM) agents. The
  framework implements three types of negotiation scenarios - resource exchange, ultimatum
  games, and seller-buyer scenarios - allowing for flexible multi-turn dialogues between
  LLM agents.
---

# How Well Can LLMs Negotiate? NegotiationArena Platform and Analysis

## Quick Facts
- arXiv ID: 2402.05863
- Source URL: https://arxiv.org/abs/2402.05863
- Reference count: 33
- Primary result: GPT-4 is the best LLM negotiator, and agents can boost outcomes by 20% through behavioral tactics like pretending to be desperate

## Executive Summary
This paper introduces NegotiationArena, a framework for evaluating LLM negotiation abilities across three scenarios: resource exchange, ultimatum games, and seller-buyer interactions. The framework uses structured XML-like communication tags to track multi-turn negotiations between LLM agents. Experiments with GPT-4, GPT-3.5, Claude-2, and Claude-2.1 reveal that GPT-4 consistently outperforms other models, and that agents can significantly improve their negotiation outcomes by employing behavioral tactics such as acting desperate or aggressive. The study also identifies several irrational negotiation behaviors in LLMs that mirror human biases, including anchoring effects and suboptimal counteroffers.

## Method Summary
The researchers created NegotiationArena to test LLM negotiation abilities using three game types with structured multi-turn dialogues. They employed four LLM models (GPT-4, GPT-3.5, Claude-2, Claude-2.1) and ran 60 negotiations for each ordered agent pair in each scenario. Agents communicated using XML-like tags to explicitly state resources, goals, and proposed trades, enabling accurate parsing and state tracking. The framework measured win rates (percentage of games where one agent obtained more resources) and average payoffs (average resources obtained after trade). Behavioral tactics were tested by priming agents with specific personas like "desperate" or "cunning" to observe their impact on negotiation outcomes.

## Key Results
- GPT-4 consistently outperforms GPT-3.5, Claude-2, and Claude-2.1 across all negotiation scenarios
- Agents going second or having final decision-making power gain significant advantages through information asymmetry
- Behavioral tactics like pretending to be desperate can improve LLM payoffs by 20% and increase win rates
- LLMs exhibit irrational negotiation behaviors including anchoring bias and suboptimal counteroffers that mirror human biases

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Structured communication format improves negotiation tracking and outcome analysis
- Mechanism: XML-like tags force agents to explicitly state their resources, goals, and proposed trades in a machine-readable format, enabling accurate parsing and state tracking
- Core assumption: Agents can be reliably trained to follow structured communication protocols
- Evidence anchors:
  - [abstract] "Agents use a structured conversation format to communicate"
  - [section 2.2] "Agents are supposed to communicate using XML-like tags... These XML-like tags are used at runtime to extract information from the text, store it, and send it to the other agent"
- Break condition: Agents fail to follow the structured format or introduce unstructured elements that break parsing

### Mechanism 2
- Claim: Role order and turn-taking significantly impact negotiation outcomes
- Mechanism: The agent going second or having the final decision-making power gains advantage through information asymmetry and anchoring effects
- Evidence anchors:
  - [section 3.1] "Overall, the agent going second tends to beat the first agent" and "In all the scenarios we tested, the order in which an agent goes and its role matters in the result"
  - [section 5.1] "Anchoring affects final prices... there is a strong correlation between the final accepted price and the initial price proposal"
- Break condition: When both agents have equal access to information and decision power, or when the game structure eliminates turn-based advantages

### Mechanism 3
- Claim: Behavioral tactics can significantly improve negotiation outcomes
- Mechanism: Agents primed with specific personas (desperate, cunning) achieve better outcomes by exploiting opponent vulnerabilities and creating psychological pressure
- Evidence anchors:
  - [abstract] "LLM agents can significantly boost their negotiation outcomes by employing certain behavioral tactics... by pretending to be desolate and desperate, LLMs can improve their payoffs by 20%"
  - [section 4] "In all the games tested, the social behaviors led to an increase in average payoff and win rate"
- Break condition: When opponents recognize and counter the behavioral tactics, or when the tactics backfire by causing the opponent to walk away

## Foundational Learning

- Concept: Game theory and rational decision-making
  - Why needed here: Understanding baseline rational strategies provides reference points for evaluating LLM behavior and identifying irrational deviations
  - Quick check question: In a simple ultimatum game, what should the proposer rationally offer and what should the responder rationally accept?

- Concept: Structured data parsing and state management
  - Why needed here: The framework relies on accurately tracking multi-turn negotiations with complex state transitions
  - Quick check question: How would you design a parser to extract resource exchanges from unstructured negotiation dialogue?

- Concept: Experimental design and controlled comparisons
  - Why needed here: Evaluating LLM negotiation abilities requires careful control of variables and systematic comparison across different scenarios
  - Quick check question: What factors would you control when comparing negotiation performance across different LLM models?

## Architecture Onboarding

- Component map: Game Engine -> Parser -> Agent Interface -> Data Logger
- Critical path: Agent sends structured message → Parser extracts information → Game Engine updates state and determines next turn → Agent receives updated context → Loop continues until termination condition
- Design tradeoffs: Structured communication vs natural dialogue (structured enables better analysis but may constrain agent behavior), detailed state tracking vs computational efficiency (comprehensive logging enables deeper analysis but increases storage requirements), fixed scenarios vs flexibility (predefined scenarios ensure consistency but limit exploration)
- Failure signatures: Parsing failures (agents deviate from structured format), State inconsistencies (tracking errors or conflicting information), Communication breakdowns (agents fail to understand each other or the rules), Irrational behavior patterns (systematic deviations from expected strategies)
- First 3 experiments:
  1. Run identical negotiations with different LLM models to establish baseline performance comparisons
  2. Test the impact of role reversal by running negotiations with swapped player positions
  3. Implement behavioral priming and measure its effect on negotiation outcomes compared to baseline

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the observed anchoring bias in LLMs during negotiation be eliminated or reduced through specific training or prompting strategies?
- Basis in paper: [explicit] The paper identifies a strong correlation between the initial proposed price and the final accepted price in the seller and buyer game, suggesting an anchoring effect.
- Why unresolved: The paper demonstrates the existence of the bias but does not explore methods to mitigate it. It's unclear if this bias is inherent to the LLM's training or if it can be influenced by specific prompts or fine-tuning.
- What evidence would resolve it: Experiments testing different training methods, prompting strategies, or fine-tuning approaches to reduce the impact of the initial proposal on the final outcome would provide insights into whether the anchoring bias can be mitigated.

### Open Question 2
- Question: How do different levels of desperation or cunning in negotiation prompts affect the long-term success of LLM agents in multi-round negotiations?
- Basis in paper: [explicit] The paper shows that prompting GPT-4 to be desperate or cunning can significantly increase its win rate and payoff in single-round negotiations.
- Why unresolved: The experiments only consider single-round negotiations. It's unknown how these social behaviors would impact the agents' performance and reputation in longer, more complex negotiations where trust and strategy evolve over time.
- What evidence would resolve it: Simulations of multi-round negotiations where agents with different social behaviors interact repeatedly would reveal whether these behaviors are sustainable strategies or if they lead to diminishing returns or negative consequences in the long run.

### Open Question 3
- Question: Can LLMs be trained to recognize and exploit the irrational behaviors of other LLM agents in negotiation scenarios?
- Basis in paper: [explicit] The paper identifies various irrational behaviors in LLM agents, such as anchoring bias and poor counteroffers, which can be detrimental to their negotiation outcomes.
- Why unresolved: The paper focuses on identifying these irrational behaviors but does not explore whether LLMs can be trained to detect and take advantage of them. This would require developing strategies for the agents to adapt their negotiation tactics based on the observed behavior of their opponents.
- What evidence would resolve it: Experiments where LLM agents are trained to recognize patterns of irrational behavior in their opponents and adjust their strategies accordingly would demonstrate whether LLMs can learn to exploit these weaknesses for improved negotiation outcomes.

## Limitations
- Limited scope of negotiation scenarios (only three types tested)
- Potential overfitting to the structured communication format
- Lack of human negotiation baselines for comparison
- No systematic exploration of the behavioral tactics' mechanisms

## Confidence
- Cross-model performance comparisons: Medium
- Behavioral tactic effectiveness: Medium
- Irrational behavior observations: Low-Medium (qualitative evidence only)
- Game theory alignment claims: Low (limited theoretical analysis)

## Next Checks
1. Conduct human negotiation experiments using the same NegotiationArena framework to establish baseline performance and compare human vs. LLM negotiation patterns

2. Systematically vary the behavioral tactic prompts (different wordings, intensities) to measure the robustness and limits of these effects across all three game types

3. Implement additional negotiation scenarios that test edge cases like multi-party negotiations, incomplete information games, or scenarios with conflicting objectives to stress-test the framework's generality