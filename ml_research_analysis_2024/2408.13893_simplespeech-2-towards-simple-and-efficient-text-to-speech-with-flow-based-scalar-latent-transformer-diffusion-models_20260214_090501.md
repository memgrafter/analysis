---
ver: rpa2
title: 'SimpleSpeech 2: Towards Simple and Efficient Text-to-Speech with Flow-based
  Scalar Latent Transformer Diffusion Models'
arxiv_id: '2408.13893'
source_url: https://arxiv.org/abs/2408.13893
tags:
- speech
- audio
- duration
- diffusion
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: SimpleSpeech 2 presents a non-autoregressive text-to-speech framework
  that achieves high-quality speech synthesis by leveraging a flow-based scalar latent
  transformer diffusion model. The system uses a scalar quantization-based audio codec
  (SQ-Codec) to compress speech into a finite latent space, which is more suitable
  for generative modeling than continuous representations.
---

# SimpleSpeech 2: Towards Simple and Efficient Text-to-Speech with Flow-based Scalar Latent Transformer Diffusion Models

## Quick Facts
- arXiv ID: 2408.13893
- Source URL: https://arxiv.org/abs/2408.13893
- Authors: Dongchao Yang; Rongjie Huang; Yuanyuan Wang; Haohan Guo; Dading Chong; Songxiang Liu; Xixin Wu; Helen Meng
- Reference count: 40
- Primary result: Achieves MOS 4.28 vs 3.97 for ground truth, 7.5 WER, 0.933 speaker similarity with RTF 0.25

## Executive Summary
SimpleSpeech 2 introduces a non-autoregressive TTS framework using flow-based scalar latent transformer diffusion models. The system compresses speech into a finite latent space using scalar quantization (SQ-Codec) and controls speech length via sentence duration rather than phoneme-level alignment. Experiments show it outperforms autoregressive and non-autoregressive baselines in naturalness, robustness, and inference speed, achieving higher MOS scores than ground truth recordings while maintaining competitive speaker similarity and low WER on English and multilingual datasets.

## Method Summary
The system uses SQ-Codec to compress speech into a finite scalar latent space through quantization, avoiding the infinite complexity of continuous representations. A flow-based diffusion model with Time-MoE architecture predicts these latents from text and speaker embeddings, using sentence duration for length control instead of fine-grained phoneme alignment. The model is trained on 7k hours of English speech with ASR-generated transcriptions, and evaluates on multiple test sets using MOS, WER, speaker similarity, and RTF metrics.

## Key Results
- Achieves MOS 4.28 (vs 3.97 for ground truth recordings)
- Maintains 7.5 WER and 0.933 speaker similarity on English data
- Real-time factor of 0.25 vs 1.6-10.2 for baseline models
- Competes effectively on multilingual tasks with 5.3 WER on Chinese

## Why This Works (Mechanism)

### Mechanism 1
Scalar quantization replaces vector quantization to create a finite, compact latent space. By mapping continuous features into 2*S+1 discrete values (S=9 gives 19 values), it reduces search space complexity from infinite to finite, making generative modeling more stable and removing codebook collapse issues.

### Mechanism 2
Flow-based diffusion uses velocity field prediction instead of noise prediction. By linearly interpolating between noise and data along straight paths, it directly maps noise to data without multi-step denoising, requiring fewer inference steps while maintaining generation quality.

### Mechanism 3
Sentence duration control eliminates phoneme alignment requirements. Using coarse sentence-level duration (sum of phoneme durations or ChatGPT prediction) to scale latent sequence length simplifies data preparation while preserving prosody control without explicit phoneme boundaries.

## Foundational Learning

- **Diffusion probabilistic models (DDPM)** - Understanding DDPM vs flow matching differences is critical for grasping why fewer inference steps are needed. *Quick check: What is the main computational difference between noise prediction in DDPM and velocity prediction in flow matching?*

- **Variational Autoencoders (VAEs) vs audio codecs** - Recognizing why VAEs force data into normal distributions while codecs preserve reconstruction quality explains SQ-Codec's design. *Quick check: How does the KL penalty in VAEs affect reconstruction compared to direct reconstruction loss in codecs?*

- **Scalar quantization vs vector quantization** - Knowing how scalar quantization reduces search space from infinite to finite explains generative model stability improvements. *Quick check: What is the search space size for scalar quantization with S=9, and how does that compare to RVQ with 2 codebooks of size 512?*

## Architecture Onboarding

- **Component map**: Text Encoder (ByT5) → Text latent → Transformer diffusion → SQ → Decoder → Waveform; Speaker Encoder (FACodec) → Speaker prompt → Transformer diffusion

- **Critical path**: Text/Speaker → Transformer diffusion (velocity prediction) → SQ → Decoder → Waveform

- **Design tradeoffs**: SQ reduces search space but may limit expressiveness; more latent dimensions improve reconstruction but slow generation; flow matching avoids multi-step prediction but requires accurate velocity field learning

- **Failure signatures**: Poor SQ-Codec reconstruction → low PESQ/STOI/MCD; unstable velocity field → artifacts/dropped words; inaccurate duration prediction → mismatched length/garbled prosody

- **First 3 experiments**: 1) Compare SQ-Codec vs SoundStream reconstruction metrics on LibriTTS; 2) Run flow-based diffusion with 5, 10, 25 steps and measure RTF vs MOS trade-off; 3) Replace sentence duration predictor with ground truth and measure WER/MOS change

## Open Questions the Paper Calls Out

1. **Audio tokenizer completeness and compactness** - How do different tokenizers' completeness and compactness levels impact TTS quality? While SQ-Codec shows advantages, the paper doesn't fully explore trade-offs across a wider range of tokenizer designs.

2. **ASR transcriptions with high WER** - Can noisy ASR transcriptions (>10% WER) effectively train high-quality TTS systems? The paper provides theoretical analysis but only validates with Whisper's low WER performance.

3. **Optimal sentence duration prediction architecture** - What is the best architecture for sentence duration prediction in non-autoregressive TTS? The paper compares four approaches but doesn't investigate underlying reasons for architectural differences.

## Limitations
- Claims of MOS 4.28 exceeding ground truth (3.97) appear inconsistent with established benchmarks
- Choice of S=9 and d=32 parameters lacks ablation studies demonstrating optimality
- Flow matching efficiency claims lack direct comparisons against DDPM with varying step counts
- Reliance on ChatGPT for duration prediction introduces external dependency without exploring failure modes

## Confidence
- **High**: Technical soundness of scalar quantization theory
- **Medium**: Empirical evidence for SQ-Codec superiority over alternatives
- **Medium**: Flow matching approach claims without comprehensive ablation studies
- **Medium**: Sentence duration control effectiveness without cross-domain validation

## Next Checks
1. Reproduce SQ-Codec reconstruction metrics (PESQ, STOI, SSIM) on LibriTTS using S=9, d=32 configuration
2. Conduct ablation study on flow matching steps (5, 10, 25, 50) measuring RTF vs MOS trade-off
3. Test ChatGPT-based duration prediction on out-of-domain datasets to assess robustness and identify failure patterns