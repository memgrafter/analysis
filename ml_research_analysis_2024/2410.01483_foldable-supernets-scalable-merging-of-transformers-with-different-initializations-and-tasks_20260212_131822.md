---
ver: rpa2
title: 'Foldable SuperNets: Scalable Merging of Transformers with Different Initializations
  and Tasks'
arxiv_id: '2410.01483'
source_url: https://arxiv.org/abs/2410.01483
tags:
- fs-merge
- merging
- original
- features
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: FS-Merge is a method for merging neural networks trained on different
  tasks from distinct initializations. It uses a "Foldable SuperNet" that contains
  the original models with frozen weights and is trained using a feature reconstruction
  objective.
---

# Foldable SuperNets: Scalable Merging of Transformers with Different Initializations and Tasks

## Quick Facts
- arXiv ID: 2410.01483
- Source URL: https://arxiv.org/abs/2410.01483
- Reference count: 40
- Primary result: FS-Merge achieves state-of-the-art results when merging MLPs and transformers across various sizes, tasks, modalities, and distribution shifts, especially in low-data scenarios

## Executive Summary
FS-Merge is a novel method for merging neural networks trained on different tasks from distinct initializations. It introduces a "Foldable SuperNet" architecture that contains frozen source models with learnable merge matrices, trained using a feature reconstruction objective. After training, the SuperNet is folded to produce a model with the same size as the original models. The method demonstrates superior performance compared to traditional merging techniques, particularly when data is limited, and achieves state-of-the-art results across diverse scenarios including different model sizes, tasks, modalities, and distribution shifts.

## Method Summary
FS-Merge works by constructing a Foldable SuperNet that contains the original models with frozen weights, plus learnable matrices M and U that transform between the source and target feature spaces. The SuperNet is trained using a feature reconstruction objective where it learns to reconstruct the output features of the source models from their merged representation. After training, the SuperNet is "folded" by applying the learned matrices to the source model weights to produce a single merged model. The method supports both full-rank and low-rank parameterizations of M and U matrices, allowing for a tradeoff between expressivity and parameter efficiency. For practical implementation, FS-Merge uses a "first" initialization strategy where M and U are initialized to select only the first model's weights, which has proven effective in practice.

## Key Results
- FS-Merge outperforms traditional merging methods (RegMean, Git Re-Basin) across diverse tasks and model sizes
- Achieves state-of-the-art results when merging transformers on ImageNet-1k subsets with limited data (100 original + 1000 augmented images per dataset)
- Demonstrates superior performance in low-data regimes compared to Knowledge Distillation, especially when data is limited

## Why This Works (Mechanism)

### Mechanism 1
FS-Merge can reconstruct arbitrary target MLP weights given invertible source weights by learning matrices M and U such that W = M(WA 0; 0 WB)U⁻¹, where WA and WB are source weights. If WA is invertible, choosing M = (W*WA⁻¹ 0; 0 0) and U = (I I) yields the target weight W*. This mechanism relies on at least one source model having invertible weights at each layer.

### Mechanism 2
FS-Merge generalizes both RegMean and Git Re-Basin merging methods by expressing their solutions as specific instances of the FS-Merge framework. Any solution from RegMean or Git Re-Basin can be expressed as M = (½I ½P) and U = (I P⁻¹) for Git Re-Basin, or M = (fA(fA)ᵀ+fB(fB)ᵀ)⁻¹(fA(fA)ᵀWA+fB(fB)ᵀWB) and U = (I I) for RegMean. This generalization provides superior expressiveness compared to the traditional methods.

### Mechanism 3
FS-Merge handles feature selection and permutation patterns that RegMean and Git Re-Basin cannot by learning matrices M and U that select different feature subsets and apply permutations independently for each source model. This allows the method to reconstruct original features from the merged representation even when source models share a common backbone but select different feature subsets, addressing the symmetry problem in neural network merging.

## Foundational Learning

- Concept: Low-rank feature representations in neural networks
  - Why needed here: FS-Merge leverages low-rank structure to reduce learnable parameters while maintaining expressivity
  - Quick check question: If neural network features are effectively low-rank, can we use low-rank weight matrices to achieve the same input-output behavior?

- Concept: Linear layer reconstruction through matrix factorization
  - Why needed here: FS-Merge reconstructs target weights as products of source weights and learned matrices M and U
  - Quick check question: If WA is invertible, can we express any target weight W* as M(WA 0; 0 WB)U⁻¹ for some matrices M and U?

- Concept: Feature space alignment and permutation symmetries
  - Why needed here: Different initializations can permute neurons, and FS-Merge handles this through learned alignment matrices
  - Quick check question: If two models have the same function but with permuted neurons, can we find permutation matrices to align their features?

## Architecture Onboarding

- Component map: Source models (frozen) -> M matrices -> Merged representation -> U matrices -> Reconstructed features -> Folding -> Merged model
- Critical path: Data → Feature extraction → M,U optimization → Folding → Merged model
- Design tradeoffs:
  - Full-rank vs. low-rank M,U matrices (expressivity vs. parameter efficiency)
  - Local vs. global optimization (simplicity vs. better reconstruction)
  - Random vs. smart initialization (convergence vs. performance)
- Failure signatures:
  - Random initialization fails to converge (especially for transformers)
  - Low-rank approximation insufficient for complex merging tasks
  - Local optimization fails for architectures with skip connections or attention
- First 3 experiments:
  1. Merge two MLPs with different initializations on MNIST halves using local FS-Merge
  2. Merge two ViT models on CIFAR100 vs. EuroSAT using global FS-Merge with low-rank M,U
  3. Compare FS-Merge vs. distillation on small datasets with varying amounts of training data

## Open Questions the Paper Calls Out

### Open Question 1
How does FS-Merge's performance scale when merging a large number of models (e.g., 10 or more) compared to the sequential variant? The paper introduces FS-Merge seq. as a more efficient variant for merging many models, but only demonstrates it with 4-8 models. Experiments showing per-task and joint accuracy when merging 10+ models using both FS-Merge and FS-Merge seq., comparing resource usage and accuracy against distillation, would resolve this question.

### Open Question 2
Can FS-Merge be extended to handle models with different depths or architectures, such as merging a ViT with a ResNet? The authors explicitly state that "one cannot naively merge two models of different depths using our method" and suggest this could be solved in future work. Demonstrations of FS-Merge successfully merging models with different depths (e.g., ViT-B-16 with ViT-B-32) or different architectures (e.g., ViT with ResNet), showing per-task and joint accuracy, would resolve this question.

### Open Question 3
How does the choice of initialization method for the Foldable SuperNet (e.g., "first" vs. "average" vs. random) affect FS-Merge's performance across different model sizes and tasks? The paper shows that "first" initialization works best for ViTs, while the ablation study shows random initialization fails to converge. Systematic experiments comparing different initialization strategies across various model sizes (small MLPs to large ViTs), different tasks, and different numbers of models would identify which initialization works best under which conditions.

## Limitations

- Theoretical assumptions about invertibility of source model weights may not hold in practice, limiting the method's applicability
- Scalability to larger transformer architectures remains untested, particularly for very large models
- Limited evaluation on distribution shifts beyond the specific datasets tested, leaving robustness questions unanswered

## Confidence

- Mechanism 1 (invertibility-based reconstruction): Medium - supported by theoretical proof but relies on strong assumptions about source model weights
- Mechanism 2 (generalization of existing methods): High - proven theoretically with clear mathematical relationships
- Mechanism 3 (handling feature selection patterns): Medium - theoretical support exists but empirical validation is limited
- Low-data performance claims: Medium - demonstrated across multiple datasets but sample size is limited

## Next Checks

1. Test FS-Merge with singular source weight matrices to validate the invertibility assumption and identify failure modes
2. Evaluate FS-Merge on larger transformer architectures (e.g., ViT-Large) and diverse distribution shifts (domain adaptation tasks)
3. Conduct ablation studies comparing FS-Merge with RegMean and Git Re-Basin across the full range of expressible functions to verify the theoretical generalization claims empirically