---
ver: rpa2
title: LLMs Can't Plan, But Can Help Planning in LLM-Modulo Frameworks
arxiv_id: '2402.01817'
source_url: https://arxiv.org/abs/2402.01817
tags:
- llms
- planning
- llm-modulo
- arxiv
- framework
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper argues that large language models (LLMs) cannot plan
  or self-verify on their own due to their autoregressive nature, but can be valuable
  as approximate knowledge sources in planning tasks. The authors propose an "LLM-Modulo
  Framework" that combines LLMs with external model-based verifiers in a tighter bi-directional
  interaction.
---

# LLMs Can't Plan, But Can Help Planning in LLM-Modulo Frameworks

## Quick Facts
- arXiv ID: 2402.01817
- Source URL: https://arxiv.org/abs/2402.01817
- Reference count: 20
- LLMs cannot plan alone but can help planning in LLM-Modulo frameworks

## Executive Summary
This paper argues that large language models (LLMs) cannot plan or self-verify due to their autoregressive nature, but can serve as valuable approximate knowledge sources in planning tasks. The authors propose the "LLM-Modulo Framework" that combines LLMs with external model-based verifiers in a tighter bi-directional interaction. This framework leverages LLMs for generating candidate plans, translating plans into specialized representations, helping end users flesh out incomplete specifications, and assisting domain experts in acquiring domain models. The approach uses a generate-test-critique loop where LLMs generate candidates and external critics evaluate them for correctness and style, providing soundness guarantees that LLMs alone cannot offer.

## Method Summary
The LLM-Modulo Framework implements a generate-test-critique loop where LLMs generate candidate plans and external verifiers/critics evaluate them. The framework includes a Meta Controller that coordinates backprompting and feedback aggregation, reformulators that convert plans to verifier-specific representations, and a bank of critics that evaluate both correctness (hard critics) and style/preferences (soft critics). The method involves implementing this architecture with an LLM as the candidate plan generator, integrating model-based verifiers like V AL as hard critics for soundness guarantees, and testing the framework on planning problem instances while comparing performance with baselines.

## Key Results
- LLMs can generate approximate domain knowledge and candidate plans but cannot guarantee correctness
- External verifiers provide correctness guarantees that compensate for LLM limitations
- The LLM-Modulo framework extends expressiveness of planning beyond traditional PDDL constraints

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLMs can generate approximate domain knowledge and candidate plans but cannot guarantee correctness.
- Mechanism: LLMs act as approximate knowledge sources, generating candidate plans and reformulating them into specialized representations for external verifiers.
- Core assumption: LLMs' n-gram training enables them to produce plausible but unverified plans and knowledge representations.
- Evidence anchors: [abstract] "LLMs should be viewed as universal approximate knowledge sources that have much more meaningful roles to play in planning/reasoning tasks beyond simple front-end/back-end format translators."
- Break condition: If the LLM fails to generate plausible candidates, the entire framework loses its generate-test-critique loop.

### Mechanism 2
- Claim: External verifiers provide correctness guarantees, compensating for LLM limitations.
- Mechanism: Model-based verifiers or simulators validate candidate plans generated by LLMs, ensuring soundness.
- Core assumption: External verifiers can be implemented with formal correctness guarantees independent of LLM output.
- Evidence anchors: [abstract] "The LLM-Modulo Framework that combines the strengths of LLMs with external model-based verifiers in a tighter bi-directional interaction regime."
- Break condition: If verifiers are too computationally expensive or incomplete, the framework's correctness guarantees fail.

### Mechanism 3
- Claim: LLM-Modulo framework extends expressiveness of planning beyond traditional PDDL constraints.
- Mechanism: By combining LLM flexibility with verifier soundness, the framework handles more flexible knowledge, problem, and preference specifications.
- Core assumption: LLMs can represent and manipulate knowledge beyond strict symbolic planning languages.
- Evidence anchors: [abstract] "The LLM-Modulo Framework provides a better neuro-symbolic approach that offers tighter integration between LLMs and symbolic components, extending the scope of model-based planning/reasoning regimes towards more flexible knowledge, problem and preference specifications."
- Break condition: If LLM approximations become too unreliable, the framework loses its advantage in handling flexible specifications.

## Foundational Learning

- Concept: Difference between System 1 and System 2 cognitive processes
  - Why needed here: Understanding why LLMs (System 1) cannot perform true reasoning/planning (System 2)
  - Quick check question: What fundamental difference between LLMs and traditional planners makes the latter capable of correctness guarantees while the former cannot?

- Concept: Generate-Test-Critique loop architecture
  - Why needed here: The LLM-Modulo framework relies on this pattern to combine LLM creativity with verifier soundness
  - Quick check question: In the LLM-Modulo framework, what role does the Meta Controller play in coordinating the generate-test-critique loop?

- Concept: Approximate knowledge vs verified knowledge
  - Why needed here: Understanding when to trust LLM outputs vs when external verification is required
  - Quick check question: When implementing the LLM-Modulo framework, which components should provide soundness guarantees versus which can remain approximate?

## Architecture Onboarding

- Component map: LLM -> Reformulator -> Verifier -> Meta Controller -> LLM (loop until accepted or timeout)

- Critical path: LLM generates plan → Reformulator converts representation → Verifier validates → Meta Controller aggregates feedback → LLM refines plan (loop until accepted or timeout)

- Design tradeoffs:
  - Expressiveness vs. computational cost: More flexible specifications increase LLM burden and verifier complexity
  - Number of critics vs. iteration time: More critics provide better validation but increase backprompting cycles
  - LLM model size vs. responsiveness: Larger models may generate better candidates but slow the iteration loop

- Failure signatures:
  - LLM generates nonsensical plans: Check prompting strategy and domain knowledge encoding
  - Verifiers reject all plans: Verify verifier implementation and problem specification completeness
  - Meta Controller gets stuck: Monitor feedback aggregation logic and backprompting diversity

- First 3 experiments:
  1. Implement a simple Blocks World domain with V AL verifier, testing basic LLM-Modulo loop
  2. Add soft critics for plan style evaluation, measuring impact on iteration count
  3. Test obfuscated domain names to verify LLM dependency on symbolic representations vs semantic understanding

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can LLMs be trained to improve their plan verification abilities through self-critique and iterative refinement?
- Basis in paper: [explicit] The paper discusses studies showing that LLMs cannot verify plans or self-improve through iterative prompting, but notes that this may be due to lack of training on "corrections data."
- Why unresolved: While the paper demonstrates current limitations, it does not rule out the possibility of training LLMs specifically for plan verification and self-improvement.
- What evidence would resolve it: Experiments showing improved plan verification and self-improvement capabilities of LLMs after training on datasets that include corrections and feedback on plan correctness.

### Open Question 2
- Question: How can the LLM-Modulo framework be extended to handle more complex planning problems with larger state spaces and longer horizons?
- Basis in paper: [inferred] The paper mentions the potential for applying the framework to other planning and reasoning tasks, but does not discuss specific strategies for scaling to more complex problems.
- Why unresolved: The paper does not provide details on how the framework would handle the increased computational complexity and search space of larger planning problems.
- What evidence would resolve it: Demonstrations of the LLM-Modulo framework successfully solving planning problems with significantly larger state spaces and longer horizons than those discussed in the paper.

### Open Question 3
- Question: What are the limits of LLMs as approximate knowledge sources, and how can these limitations be mitigated in the LLM-Modulo framework?
- Basis in paper: [explicit] The paper argues that LLMs can be valuable as approximate knowledge sources but acknowledges that this knowledge is not guaranteed to be correct.
- Why unresolved: The paper does not provide a comprehensive analysis of the types of knowledge that LLMs are most likely to get wrong, nor does it discuss strategies for mitigating these limitations.
- What evidence would resolve it: Studies quantifying the accuracy of LLM-generated knowledge across different domains and tasks, along with techniques for filtering or refining this knowledge to improve its reliability in the LLM-Modulo framework.

## Limitations
- LLM approximation reliability: The framework's success depends on LLMs generating plausible candidate plans, but there's limited empirical evidence about how often LLM-generated plans fall outside the verifier's correction capability
- Computational overhead: The generate-test-critique loop with multiple critics and backprompting introduces substantial computational overhead without detailed timing or cost analysis
- Verifier completeness: The paper emphasizes soundness guarantees but doesn't address completeness - whether the verifier can always find flaws when they exist

## Confidence
- High Confidence: The core observation that LLMs alone cannot guarantee planning correctness due to their autoregressive nature
- Medium Confidence: The LLM-Modulo framework architecture and its potential benefits
- Low Confidence: Claims about extending expressiveness beyond PDDL constraints without providing concrete metrics or comparisons

## Next Checks
1. **Error Tolerance Analysis**: Systematically measure the percentage of LLM-generated plans that require more than 3-5 iteration cycles to be accepted by verifiers, and identify patterns in which types of errors LLMs struggle to self-correct.

2. **Computational Cost Benchmarking**: Compare the total time and computational resources required by LLM-Modulo versus pure symbolic planners across domains of increasing complexity, including a cost-benefit analysis of the expressivity gains.

3. **Generalization Testing**: Test the framework on obfuscated or novel domain names (e.g., "Colored Blocks World" instead of "Blocks World") to empirically verify whether LLMs are truly leveraging semantic understanding versus pattern matching on symbolic representations.