---
ver: rpa2
title: A framework for annotating and modelling intentions behind metaphor use
arxiv_id: '2407.03952'
source_url: https://arxiv.org/abs/2407.03952
tags:
- metaphor
- metaphors
- intention
- intentions
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a novel taxonomy of intentions commonly attributed
  to metaphor use, comprising 9 categories. The authors annotate a dataset of metaphors
  from the VU Amsterdam Metaphor Corpus with these intentions and analyze the relationship
  between intentions, metaphor types, genres, and novelty.
---

# A framework for annotating and modelling intentions behind metaphor use

## Quick Facts
- arXiv ID: 2407.03952
- Source URL: https://arxiv.org/abs/2407.03952
- Reference count: 13
- This paper presents a novel taxonomy of intentions commonly attributed to metaphor use, comprising 9 categories.

## Executive Summary
This paper introduces a comprehensive framework for annotating and modeling the intentions behind metaphor use in language. The authors develop a taxonomy of 9 distinct metaphor intention categories and apply it to annotate a dataset of metaphors from the VU Amsterdam Metaphor Corpus. They then evaluate the ability of state-of-the-art language models (GPT-4 and Llama2 variants) to infer these intentions, finding that while models can perform above random chance, the task remains challenging with the best model achieving only 44.68% accuracy.

## Method Summary
The authors propose a two-step annotation procedure where metaphors are first classified as lexicalized or non-lexicalized, then annotated with one of 9 intention categories (Concreteness, Persuasiveness, Lexicalized, Explanation, Humour, Artistic, Social interaction, Heuristic reasoning, Other). They extract metaphors from the VU Amsterdam Metaphor Corpus using the MetaNet approach, resulting in 988 annotated metaphors. The framework is then evaluated using zero-shot and five-shot prompting with GPT-4 and Llama2-Chat models, measuring accuracy and F1 scores for intention inference.

## Key Results
- GPT-4 achieves highest accuracy at 42.99% (zero-shot) and 44.68% (five-shot) for inferring metaphor intentions
- Llama2-13B and Llama2-70B perform significantly worse, with accuracies around 24-27%
- All models perform above random baseline (11.11%) but well below human-level performance
- Persuasiveness, Explanation, Humour, and Artistic metaphors tend to be more original, while Lexicalized metaphors, Social interaction, and Heuristic reasoning are more conventional

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The taxonomy captures a broad range of metaphor intentions because it builds on prior fragmented research and operationalizes them systematically.
- Mechanism: The authors aggregate intention categories from literature (Roberts & Kreuz 1994, Steen 2008, Gibbs 1999, etc.) and formalize them into 9 discrete labels with clear definitions and examples.
- Core assumption: That prior findings on individual intentions can be unified into a single taxonomy without losing specificity.
- Evidence anchors:
  - [abstract] "we propose a novel taxonomy of intentions commonly attributed to metaphor, which comprises 9 categories"
  - [section 3] "We introduce the individual intention categories, motivating them through theoretical considerations, previous literature and examples"
- Break condition: If any intention category overlaps significantly with another, or if new intention types emerge in future metaphor use that fall outside the taxonomy.

### Mechanism 2
- Claim: The annotation procedure successfully distinguishes between lexicalized and non-lexicalized metaphors.
- Mechanism: Step 1 asks annotators to judge whether a metaphor is unavoidable; if it can be paraphrased without loss, it is non-lexicalized and proceeds to Step 2.
- Core assumption: Annotators can reliably judge paraphrase equivalence and metaphoricity based on contextual meaning vs basic dictionary meaning.
- Evidence anchors:
  - [section 4.2] "The annotator should distinguish lexicalized metaphors from other types of metaphors. If they perceive some intention behind the metaphor other than pure communication of information to the receiver, then they shall move on to step 2."
  - [section 4.3] "Inter-annotator reliability... agreement in terms of Krippendorff’s α (Artstein and Poesio, 2008) is 0.77, indicating moderate-to-fair agreement."
- Break condition: If annotators disagree frequently on whether a metaphor is lexicalized, the procedure may not be robust.

### Mechanism 3
- Claim: GPT-4 outperforms Llama2 models in inferring metaphor intentions due to its larger scale and more nuanced understanding.
- Mechanism: In zero-shot and few-shot settings, GPT-4 achieves higher accuracy (42.99% zero-shot, 44.68% five-shot) than Llama2-13B (24.79%, 26.75%) and Llama2-70B (27.29%, 22.49%).
- Core assumption: Larger model size and pretraining data correlate with better performance on intention inference tasks.
- Evidence anchors:
  - [abstract] "The best-performing model, GPT-4, reaches an average accuracy of 42.99% in the zero-shot setting and 44.68% in the five-shot setting"
  - [section 6] "All three models reach accuracies that are above the random baseline in the zero-shot experiments, although the accuracies are still relatively low"
- Break condition: If the task inherently requires multimodal or embodied reasoning that LLMs cannot simulate, model size may not help.

## Foundational Learning

- Concept: Conceptual Metaphor Theory (CMT)
  - Why needed here: CMT underpins the idea that metaphors map from concrete SOURCE to abstract TARGET domains, which is central to understanding metaphor use.
  - Quick check question: In "time is money," what is the SOURCE domain and what is the TARGET domain?

- Concept: Deliberate Metaphor Theory (DMT)
  - Why needed here: DMT distinguishes between deliberate metaphors (intentionally used) and non-deliberate ones, which informs the taxonomy and annotation.
  - Quick check question: Why might "the sky is the limit" be considered a deliberate metaphor in a motivational speech?

- Concept: Inter-annotator reliability metrics
  - Why needed here: To validate the annotation scheme, especially since metaphor interpretation can be subjective.
  - Quick check question: What does a Krippendorff’s α of 0.77 indicate about the reliability of the metaphor intention annotations?

## Architecture Onboarding

- Component map:
  - VUAMC corpus extraction -> Metaphor filtering -> Two-step annotation procedure -> LLM evaluation harness -> Statistical analysis dashboard

- Critical path:
  1. Corpus extraction → filtering → annotation
  2. LLM prompt generation → model inference → result aggregation
  3. Statistical analysis → interpretation

- Design tradeoffs:
  - Using MRW vs phrase as annotation unit: MRW is consistent with VUAMC but less natural for intention assignment.
  - Single intention vs multi-label output for LLMs: Simpler for models but loses nuance.
  - Manual vs automated exclusion of idioms: Manual ensures quality but is slower.

- Failure signatures:
  - Low inter-annotator agreement (<0.7) → taxonomy too vague or examples insufficient
  - LLM accuracy near random baseline → task too hard or prompts ineffective
  - Genre/novelty correlations missing → dataset not representative or analysis flawed

- First 3 experiments:
  1. Re-run LLM evaluation with 5-shot-short prompts to test reliance on explanations.
  2. Compute per-intention F1 scores to identify hardest categories for models.
  3. Filter corpus to only direct metaphors and re-analyze intention distribution.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the proposed taxonomy of metaphor intentions generalize across different languages and cultures?
- Basis in paper: [inferred] The authors note that their taxonomy was developed based on analysis of English metaphors from the VU Amsterdam Metaphor Corpus and that certain intention categories correlate with specific genres (Fiction, News, Academic, Conversation). They also mention that genre can help track most common uses but not all uses.
- Why unresolved: The study is limited to English metaphors and does not explore cross-linguistic or cross-cultural validity. The taxonomy's applicability to other languages and cultures remains untested.
- What evidence would resolve it: Applying the taxonomy to annotate metaphors in multiple languages and cultures, then analyzing whether the same intention categories emerge and whether their distributions vary systematically across different linguistic and cultural contexts.

### Open Question 2
- Question: What specific linguistic features or markers reliably distinguish deliberate metaphors from non-deliberate ones in practice?
- Basis in paper: [explicit] The authors discuss Deliberate Metaphor Theory (DMT) and note that direct metaphors are considered more deliberate as they "overtly introduce a referent from a SOURCE domain." They also mention challenges identified by Gibbs (2011) regarding identifying deliberate metaphors without specific linguistic markers.
- Why unresolved: While the paper acknowledges theoretical distinctions between deliberate and non-deliberate metaphors, it doesn't empirically test what specific linguistic features can reliably identify deliberate metaphors in real text.
- What evidence would resolve it: Empirical studies analyzing large corpora of metaphors to identify consistent linguistic markers (e.g., specific syntactic constructions, lexical choices, or contextual cues) that distinguish deliberate from non-deliberate metaphors, validated through inter-annotator reliability testing.

### Open Question 3
- Question: How does metaphor intention interact with metaphor novelty across different discourse genres?
- Basis in paper: [explicit] The authors analyze novelty scores across intention categories, finding that Persuasiveness, Explanation, Humour, and Artistic metaphor tend to be more original, while Lexicalized metaphor, Social interaction, and Heuristic reasoning are more conventional. They also show that intentions correlate with genres.
- Why unresolved: The paper analyzes novelty and genre separately but doesn't systematically examine how intention-type interactions with novelty vary across genres. The relationship between these three factors remains unexplored.
- What evidence would resolve it: A comprehensive analysis mapping intention categories, novelty scores, and genres simultaneously, examining whether certain intention-genre combinations consistently produce novel or conventional metaphors, and whether this relationship holds across multiple corpora.

## Limitations
- The taxonomy may not capture all possible metaphor intentions, particularly emerging ones from evolving language use
- Moderate inter-annotator reliability (0.77) suggests some subjectivity in intention attribution
- LLM evaluation shows significant performance gaps, with best model achieving only 44.68% accuracy
- The corpus is limited to specific English genres (academic, news, fiction, conversation) limiting generalizability

## Confidence

**High confidence**: The taxonomy construction methodology and its grounding in established metaphor theories (CMT, DMT)

**Medium confidence**: The annotation procedure's reliability and the correlation analyses between intentions, genres, and novelty

**Low confidence**: The LLM performance results and their interpretation, given the challenging nature of the task and potential evaluation limitations

## Next Checks

1. Replicate the LLM evaluation using a stratified sample of metaphors across all genres and novelty levels to verify the reported accuracy differences between models
2. Conduct a blind annotation study with independent experts to validate the intention assignments and identify any systematic biases in the current annotations
3. Test the taxonomy's coverage by having annotators label metaphors from domains not represented in the VUAMC (e.g., technical documentation, social media) to identify potential gaps or new intention categories