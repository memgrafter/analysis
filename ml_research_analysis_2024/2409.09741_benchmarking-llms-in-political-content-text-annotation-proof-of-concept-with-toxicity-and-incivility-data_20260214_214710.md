---
ver: rpa2
title: 'Benchmarking LLMs in Political Content Text-Annotation: Proof-of-Concept with
  Toxicity and Incivility Data'
arxiv_id: '2409.09741'
source_url: https://arxiv.org/abs/2409.09741
tags:
- llms
- perspective
- open-source
- toxicity
- hermes
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study benchmarks large language models (LLMs) for annotating
  political content on social media, focusing on toxicity and incivility detection.
  Using a dataset of over three million Twitter interactions from Argentina and Chile,
  the research compares OpenAI's GPTs, Google's Perspective API, and open-source LLMs
  against human-annotated ground truth.
---

# Benchmarking LLMs in Political Content Text-Annotation: Proof-of-Concept with Toxicity and Incivility Data

## Quick Facts
- arXiv ID: 2409.09741
- Source URL: https://arxiv.org/abs/2409.09741
- Reference count: 12
- Key outcome: Open-source LLMs like Nous Hermes 2 and Mistral OpenOrca achieve competitive performance in political content annotation with fewer parameters, offering cost-effective alternatives to proprietary models

## Executive Summary
This study benchmarks large language models (LLMs) for annotating political content on social media, focusing on toxicity and incivility detection. Using a dataset of over three million Twitter interactions from Argentina and Chile, the research compares OpenAI's GPTs, Google's Perspective API, and open-source LLMs against human-annotated ground truth. Key findings reveal that Perspective API with a relaxed threshold, GPT-4o, and Nous Hermes 2 Mixtral outperform other models in zero-shot classification tasks. Notably, smaller open-source models like Nous Hermes 2 and Mistral OpenOrca demonstrate strong performance with fewer parameters, offering cost-effective alternatives. Ancillary experiments show that while GPTs excel in speed and reliability, only open-source models ensure full reproducibility when temperature settings are minimized. This work highlights the potential of LLMs in automating political content annotation while addressing challenges in reproducibility and computational efficiency.

## Method Summary
The study employed a comprehensive benchmarking approach using a dataset of over three million Twitter interactions from Argentina and Chile, annotated by human experts for toxicity and incivility. Multiple LLMs were evaluated through zero-shot classification tasks, including OpenAI's GPTs, Google's Perspective API, and various open-source models. Performance was measured against the human-annotated ground truth using standard metrics: accuracy, precision, recall, and F1-score. A consistent prompt strategy was applied across all models to ensure fair comparison. The evaluation considered both computational efficiency and reproducibility, with particular attention to temperature settings affecting open-source model outputs.

## Key Results
- Perspective API with relaxed threshold, GPT-4o, and Nous Hermes 2 Mixtral achieved the highest performance in zero-shot classification
- Open-source models like Nous Hermes 2 and Mistral OpenOrca demonstrated strong performance with fewer parameters
- Open-source models ensured full reproducibility when temperature settings were minimized
- GPTs showed superior speed and reliability compared to open-source alternatives

## Why This Works (Mechanism)
The study's effectiveness stems from its systematic comparison of multiple LLMs under identical conditions, revealing that smaller, open-source models can achieve competitive performance with appropriate configuration. The focus on reproducibility through temperature control and the evaluation of both proprietary and open-source solutions provides a comprehensive understanding of the trade-offs between performance, cost, and practical deployment considerations.

## Foundational Learning
- **Zero-shot classification**: Why needed - to evaluate models without task-specific training; Quick check - models can classify without examples
- **Temperature settings**: Why needed - controls output randomness and reproducibility; Quick check - lower values increase consistency
- **Performance metrics**: Why needed - standard evaluation framework for classification tasks; Quick check - accuracy, precision, recall, F1-score
- **Prompt engineering**: Why needed - influences model behavior and output quality; Quick check - consistent prompts across models
- **Computational efficiency**: Why needed - balances performance with resource requirements; Quick check - parameter count vs. accuracy trade-off
- **Reproducibility**: Why needed - ensures consistent results across different runs; Quick check - temperature settings and random seeds

## Architecture Onboarding
**Component Map**: Dataset -> Preprocessing -> Prompt Engineering -> Model Execution -> Evaluation -> Results Analysis
**Critical Path**: Dataset preparation and prompt engineering are critical for ensuring fair model comparison
**Design Tradeoffs**: Proprietary models offer better performance and reliability but at higher cost, while open-source models provide cost-effectiveness and reproducibility
**Failure Signatures**: Inconsistent results from open-source models typically indicate improper temperature settings; poor performance may suggest inadequate prompt engineering
**First Experiments**:
1. Run zero-shot classification with minimum temperature settings on open-source models
2. Compare performance metrics across different prompt variations
3. Evaluate cost-effectiveness by measuring inference time and resource usage

## Open Questions the Paper Calls Out
### Open Question 1
- Question: How do different prompt variations and additional context information impact the performance of LLMs in toxicity classification tasks?
- Basis in paper: [explicit] The paper mentions that using additional information to generate prompt variation could improve LLMs' performance by 20-30% for hate speech detection.
- Why unresolved: The study primarily focused on zero-shot classification with a basic prompt strategy, without exploring the impact of different prompt variations or additional context information on model performance.
- What evidence would resolve it: Comparative analysis of model performance using various prompt strategies, including different levels of context information and prompt variations, would provide insights into the optimal approach for toxicity classification tasks.

### Open Question 2
- Question: What is the long-term reliability and consistency of LLMs in toxicity classification tasks across different datasets and time periods?
- Basis in paper: [inferred] The study tested model performance on a specific dataset and did not examine the long-term reliability or consistency of LLMs across different datasets or time periods.
- Why unresolved: The paper focused on benchmarking models on a single dataset and did not address the potential changes in model performance over time or across different datasets.
- What evidence would resolve it: Longitudinal studies tracking model performance on various datasets over time would provide insights into the long-term reliability and consistency of LLMs in toxicity classification tasks.

### Open Question 3
- Question: How do ensemble approaches combining multiple LLMs perform compared to individual models in toxicity classification tasks?
- Basis in paper: [explicit] The paper mentions that Mistral OpenOrca and Hermes 3 showed intermediate performance and could open avenues for ensemble annotations and the use of stacking classifiers.
- Why unresolved: The study did not explore ensemble approaches or compare their performance to individual models in toxicity classification tasks.
- What evidence would resolve it: Comparative analysis of ensemble approaches, such as model stacking or weighted averaging, against individual models would provide insights into the potential benefits of combining multiple LLMs for toxicity classification tasks.

## Limitations
- Regional specificity of the dataset (Argentina and Chile) may limit broader applicability
- Computing infrastructure specifications for open-source model deployment remain unspecified
- Temperature settings' impact on reproducibility needs more systematic investigation

## Confidence
- **High confidence**: Comparative model performance rankings are well-supported by reported metrics
- **Medium confidence**: Cost-effectiveness conclusions depend on unprovided infrastructure details
- **Medium confidence**: Reproducibility claims are supported but contingent on specific deployment settings

## Next Checks
1. Replicate the classification tasks using identical temperature settings (minimum) across all open-source models to verify reproducibility claims
2. Test model performance on politically diverse datasets from different regions to assess generalizability beyond Argentina and Chile
3. Conduct a systematic comparison of zero-shot versus few-shot learning approaches to determine optimal annotation strategies for political content