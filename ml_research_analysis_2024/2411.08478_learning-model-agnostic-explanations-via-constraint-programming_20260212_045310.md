---
ver: rpa2
title: Learning Model Agnostic Explanations via Constraint Programming
arxiv_id: '2411.08478'
source_url: https://arxiv.org/abs/2411.08478
tags:
- explanations
- learning
- rule
- precision
- explanation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of providing concise and precise
  explanations for predictions made by opaque classifiers such as ensemble models,
  kernel methods, or neural networks. The authors frame the task of finding model-agnostic
  explanations as a Constraint Optimization Problem (COP), where the constraint solver
  seeks an explanation of minimum error and bounded size for an input data instance
  and a set of samples generated by the black box.
---

# Learning Model Agnostic Explanations via Constraint Programming

## Quick Facts
- arXiv ID: 2411.08478
- Source URL: https://arxiv.org/abs/2411.08478
- Reference count: 35
- Key outcome: Constraint programming approach achieves empirical precision errors under 20% on 20 of 25 benchmarks while providing PAC-style generalization guarantees

## Executive Summary
This paper addresses the challenge of providing concise and precise explanations for predictions made by opaque classifiers such as ensemble models, kernel methods, or neural networks. The authors frame the task of finding model-agnostic explanations as a Constraint Optimization Problem (COP), where the constraint solver seeks an explanation of minimum error and bounded size for an input data instance and a set of samples generated by the black box. The proposed approach offers PAC-style guarantees for the output explanation. Empirically, the method outperforms the state-of-the-art heuristic Anchors method in terms of precision while using a reasonable amount of time for the solver.

## Method Summary
The paper proposes learning model-agnostic explanations by formulating the problem as a Constraint Optimization Problem (COP) over monotone k-monomials. The approach generates samples from the black-box model, encodes the explanation search as a COP using channeling constraints between feature selection and sample classification, and solves for the minimum-error explanation within a size bound. Two constraint encodings (Cop with all constraints and Sat without positive constraints) are implemented using the CP-SAT solver from OR-Tools. The method is evaluated on 25 OpenML tabular datasets converted to interpretable binary features using K-bins discretization, comparing against the Anchors method across precision error metrics.

## Key Results
- Constraint encodings Cop and Sat achieve empirical precision errors under 20% for 20 out of 25 benchmarks
- 9 benchmarks achieve errors under 10%, outperforming Anchors method in precision
- Sat encoding is faster than Cop while maintaining similar precision performance
- The approach provides PAC-style guarantees with theoretical sample complexity bounds

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Constraint optimization formulation enables PAC-style guarantees on explanation quality.
- Mechanism: The paper formulates explanation generation as a Constraint Optimization Problem (COP) over a restricted hypothesis class (monotone k-monomials). The COP solver returns an optimal solution within the class, and the empirical loss on a finite sample set bounds the true loss with high probability.
- Core assumption: The hypothesis class is expressive enough to approximate the black-box model locally, and the sample complexity is manageable for the COP solver.
- Evidence anchors:
  - [abstract] "From a theoretical perspective, this constraint programming approach offers PAC-style guarantees for the output explanation."
  - [section] "Theorem 3. Let D be a probability distribution over {0, 1}^d and f : {0, 1}^d → {0, 1} be a black-box model... a k-explanation S for x and f satisfying, with probability at least 1 − δ, Ex∼D [ϵf,x(S)] ≤ min_r∈Rf,x,k Lf (r) + ε can be found using O(k ln(d) + ln(1/δ))/ε² calls to ex(f, D)."
  - [corpus] Weak. No direct citation of PAC learning guarantees in neighbor papers.

### Mechanism 2
- Claim: Mapping explanation problem to monotone monomial learning preserves solution quality.
- Mechanism: The paper shows a bijection between explanations and monotone k-monomials. Learning the best monotone k-monomial in the empirical risk minimization sense yields a k-explanation whose precision error is bounded by the empirical loss of the learned monomial.
- Core assumption: The bijection between rules in R_f,x,k and monotone k-monomials is lossless, and empirical risk minimization over this class is tractable via COP.
- Evidence anchors:
  - [section] "Lemma 2. For a black-box model f and an instance x... min_{c∈C_k} ˆL(c) = min_{r∈Rf,x,k} ˆLf (r)"
  - [section] "Based on these considerations, we are now in a position to provide a constraint encoding of our learning problem."
  - [corpus] Weak. No direct citation of monomial learning in neighbor papers.

### Mechanism 3
- Claim: Encoding explanations as SAT/CP constraints enables efficient search over large feature spaces.
- Mechanism: The COP uses channeling constraints to link feature selection variables with sample classification correctness, and a cardinality constraint to enforce the size limit. The solver minimizes the number of misclassified samples, yielding the best explanation within the class.
- Core assumption: The constraint encoding is correct and complete; the solver can handle the encoding size within time limits.
- Evidence anchors:
  - [section] "The COP associated with this optimization task is described in Table 1... the empirical loss of the rule is encoded in the objective function (vii)."
  - [section] "We used the CP-SAT solver from OR-Tools [25], with a 60-second timeout per explanation task."
  - [corpus] Weak. No direct citation of CP/SAT for explanations in neighbor papers.

## Foundational Learning

- Concept: PAC learning and generalization bounds.
  - Why needed here: The paper's main theoretical contribution relies on showing that the COP solution generalizes from a finite sample to the true distribution with high probability.
  - Quick check question: If you have m samples and want error ε with probability 1−δ, what is the minimum m required for a finite hypothesis class of size H?

- Concept: Constraint optimization and channeling constraints.
  - Why needed here: The paper uses channeling constraints to link feature selection with sample classification, enabling the COP to minimize empirical loss.
  - Quick check question: In a channeling constraint between X and Y, if X=1 forces Y=1, how is this encoded in SAT?

- Concept: Monotone monomials and rule representations.
  - Why needed here: The paper shows that explanations can be represented as monotone monomials, enabling the reduction to a known learning problem.
  - Quick check question: What is the difference between a monotone monomial and a general monomial in Boolean logic?

## Architecture Onboarding

- Component map: Black-box model f -> Sample generator (ex(f, D)) -> Constraint encoder -> CP-SAT solver -> Explanation S
- Critical path: Generate samples → Encode COP → Solve COP → Return S
- Design tradeoffs:
  - Sample size m vs. runtime: More samples improve generalization but increase solver load
  - Encoding size (Cop vs. Sat): Cop includes all constraints, Sat excludes positive constraints for speed
  - Timeout setting: Longer timeouts may yield better solutions but increase wall-clock time
- Failure signatures:
  - Timeout on solver → Suboptimal explanation returned
  - High empirical precision error → Sample size too small or hypothesis class too weak
  - Solver memory error → Encoding too large for available resources
- First 3 experiments:
  1. Run Cop and Sat encodings on a small dataset (e.g., iris) with k=1, m=100, compare runtime and precision
  2. Vary m from 100 to 1000 on the same dataset, plot precision vs. sample size for both encodings
  3. Fix m=1000, vary k from 1 to 7, plot precision vs. explanation size for Cop, Sat, and Anchors

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the constraint-based approach scale with the dimensionality of the input data?
- Basis in paper: [explicit] The paper mentions that the average dimension of interpretable instances is 60, ranging from 12 to 352, but does not explore the scalability of the approach beyond this range.
- Why unresolved: The paper does not provide experiments or analysis on how the constraint-based approach performs as the number of features increases significantly.
- What evidence would resolve it: Additional experiments on datasets with much higher dimensionality, or theoretical analysis of the computational complexity as a function of the number of features, would help understand the scalability limits.

### Open Question 2
- Question: How sensitive are the explanations to the choice of the size limit k?
- Basis in paper: [explicit] The paper uses k values ranging from 1 to 7 and shows that the precision of explanations improves with k, but does not explore the impact of different k values on the quality and interpretability of explanations.
- Why unresolved: The paper does not provide a systematic study on how the choice of k affects the trade-off between precision and conciseness of explanations.
- What evidence would resolve it: A detailed analysis of the precision and conciseness of explanations for a wide range of k values, along with a study on the interpretability of explanations for different k values, would help understand the sensitivity to k.

### Open Question 3
- Question: How does the performance of the constraint-based approach compare to other model-agnostic explanation methods beyond Anchors?
- Basis in paper: [explicit] The paper compares the constraint-based approach to the Anchors method, but does not explore its performance against other model-agnostic explanation methods like Lime, SHAP, or counterfactual explanations.
- Why unresolved: The paper focuses on comparing the constraint-based approach to a single state-of-the-art method, leaving the comparison to other methods unexplored.
- What evidence would resolve it: Experiments comparing the constraint-based approach to other model-agnostic explanation methods on the same datasets and metrics would help assess its relative performance and identify its strengths and weaknesses.

## Limitations
- The constraint programming approach suffers from scalability issues, with Cop encoding showing timeouts on several benchmarks
- Explanations may be "too imprecise" when the sample size is small or the black-box model is too complex for the restricted hypothesis class
- The paper acknowledges that the monotone monomial restriction may limit expressiveness for certain black-box models

## Confidence

**High**: The bijection between explanations and monotone monomials, and the empirical comparison methodology showing precision improvements over Anchors

**Medium**: The PAC-style generalization guarantees, given the theoretical assumptions about sample complexity and hypothesis class expressiveness

**Low**: The scalability claims for the Sat encoding, as the paper shows it's faster but provides limited evidence of consistent performance across all benchmarks

## Next Checks

1. **Sample Complexity Validation**: Systematically vary the sample size m from 100 to 1000 on representative datasets and measure how precision error decreases, comparing observed scaling with theoretical predictions from PAC bounds

2. **Timeout Analysis**: For benchmarks where Cop encoding times out, measure the partial solutions quality and runtime distribution to determine if early termination policies could provide practical tradeoffs

3. **Hypothesis Class Expressiveness**: Test whether increasing k beyond 7 on select datasets improves precision, validating whether the monotone monomial restriction is indeed the limiting factor for certain black-box models