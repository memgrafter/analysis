---
ver: rpa2
title: 'Classification Metrics for Image Explanations: Towards Building Reliable XAI-Evaluations'
arxiv_id: '2406.05068'
source_url: https://arxiv.org/abs/2406.05068
tags:
- saliency
- methods
- metrics
- dataset
- images
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work introduces new saliency evaluation metrics based on feature
  importance classification, extending prior work by Arias-Duart et al. The metrics
  leverage mosaics of images from different classes and classify positive/negative
  feature importance into true/false positives/negatives.
---

# Classification Metrics for Image Explanations: Towards Building Reliable XAI-Evaluations

## Quick Facts
- arXiv ID: 2406.05068
- Source URL: https://arxiv.org/abs/2406.05068
- Reference count: 40
- Primary result: Introduces classification-based metrics for saliency map evaluation with psychometric reliability testing, showing ResNet50 outperforms VGG11 in metric consistency.

## Executive Summary
This paper addresses the critical need for reliable evaluation metrics in explainable AI by introducing a novel framework that treats saliency maps as feature importance classifiers. The approach constructs mosaic images from different classes and evaluates how well saliency methods attribute importance to correct versus incorrect image regions using standard classification metrics. To validate these metrics without ground truth, the authors apply psychometric reliability tests including Krippendorff's alpha for inter-rater reliability and Spearman's rank correlation for inter-method reliability. Experiments on ImageNet and synthetic datasets demonstrate that the metrics are reliable and that no single saliency method excels across all metrics, highlighting the importance of multi-metric evaluation approaches.

## Method Summary
The proposed method constructs 2×2 mosaic images containing two images from a target class and two from non-target classes. Saliency methods (LIME, LRP, IntGrad, Grad-CAM, Grad-CAM++, SHAP, B-cos) are applied to these mosaics, and feature importance (FI) values are classified into true/false positives/negatives based on whether they highlight correct (target class) or incorrect (non-target class) regions. Standard classification metrics including precision, recall, F1-score, specificity, and accuracy are then computed for each saliency method. The framework is validated using psychometric reliability tests: Krippendorff's alpha measures inter-rater reliability across different images, while Spearman's rank correlation measures inter-method reliability between different saliency methods. Experiments are conducted on ImageNet using pretrained ResNet50 and VGG11 models.

## Key Results
- ResNet50 outperforms VGG11 in metric consistency and reliability across all tested datasets
- B-cos achieves high precision but low specificity due to providing minimal negative FI values
- No single saliency method excels across all metrics, demonstrating the necessity of multi-metric evaluation
- High inter-method correlation indicates joint failures among saliency methods on difficult datasets
- Psychometric reliability tests confirm the metrics are consistent across different images and methods

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Mosaic-based evaluation works because it creates controlled class separation while preserving realistic image statistics.
- Mechanism: The 2×2 grid ensures that true positive FI comes from the target class images, while false positives come from the non-target class images. This setup enables calculation of classification-style metrics for saliency maps.
- Core assumption: The underlying model can distinguish between the classes in the mosaic, so FI distribution reflects actual class-specific features.
- Evidence anchors:
  - [abstract] "The metrics leverage mosaics of images from different classes and classify positive/negative feature importance into true/false positives/negatives."
  - [section 3.1.1] "The mosaics used to test and evaluate various saliency methods are constructed of four images: two from the assigned target class and two from different classes within the same dataset."
  - [corpus] Weak - no direct corpus evidence, but the concept is novel.
- Break condition: If the underlying model cannot distinguish classes (e.g., difficult-to-distinguish dog breeds), the FI distribution will be random, making metrics unreliable.

### Mechanism 2
- Claim: Psychometric reliability metrics provide an objective way to validate XAI evaluation metrics without ground truth.
- Mechanism: Krippendorff's α measures inter-rater reliability by checking if different images (raters) rank saliency methods consistently. Spearman's ρ measures inter-method reliability by checking if different saliency methods agree on image difficulty.
- Evidence anchors:
  - [abstract] "To validate the metrics, psychometric reliability tests (Krippendorff's α for inter-rater reliability, Spearman's ρ for inter-method reliability) are applied."
  - [section 3.2] "When the ranking of saliency methods remains consistent across all (or most) test images, it is highly likely that the ranking for new images will be the same as well."
  - [corpus] Weak - limited corpus evidence, but the approach is sound given the lack of ground truth.
- Break condition: If saliency methods produce highly correlated failures (as seen with Grad-CAM on difficult datasets), the reliability metrics may falsely indicate good performance.

### Mechanism 3
- Claim: Multiple metrics are necessary because no single metric captures all aspects of saliency method quality.
- Mechanism: Different metrics (precision, recall, F1-score, etc.) measure different properties like contrastivity and correctness. B-cos may excel at precision but fail at specificity due to imbalance in positive/negative FI.
- Evidence anchors:
  - [abstract] "No single saliency method excels across all metrics, highlighting the need for multi-metric evaluation."
  - [section 5.3] "B-cos provides a high precision, but a low specificity, because it barely provides any negative FI-values."
  - [corpus] Weak - the paper itself argues for this, but external validation is limited.
- Break condition: If all metrics consistently rank methods the same way, the need for multiple metrics may be overstated.

## Foundational Learning

- Concept: Confusion matrix and classification metrics (precision, recall, F1-score, specificity)
  - Why needed here: The saliency evaluation framework mimics classification metrics by treating FI as predictions and actual class regions as ground truth.
  - Quick check question: What's the difference between precision and recall in the context of saliency maps?

- Concept: Psychometric testing (reliability vs validity)
  - Why needed here: Provides a methodology to validate evaluation metrics when ground truth is unavailable.
  - Quick check question: Why is reliability a necessary but not sufficient condition for validity?

- Concept: Spearman's rank correlation
  - Why needed here: Used to measure inter-method reliability between different saliency methods.
  - Quick check question: What does it mean if Spearman's ρ is close to 1 between two saliency methods?

## Architecture Onboarding

- Component map: Mosaic generation (2×2 grid of images from target and non-target classes) -> Saliency method execution (produces FI maps for each mosaic) -> Metric calculation (converts FI into classification-style metrics) -> Reliability testing (Krippendorff's α and Spearman's ρ)

- Critical path:
  1. Generate mosaics from dataset
  2. Run each saliency method on each mosaic
  3. Calculate all proposed metrics for each method
  4. Compute reliability statistics
  5. Analyze results and make recommendations

- Design tradeoffs:
  - Mosaic size vs. runtime: Larger mosaics provide more data but increase computation time
  - Class difficulty vs. metric reliability: Easy-to-distinguish classes yield more reliable metrics
  - Positive vs. negative FI: Some methods only provide positive FI, limiting metric options

- Failure signatures:
  - Low inter-rater reliability (Krippendorff's α near 0): Metrics are inconsistent across images
  - High inter-method correlation: Saliency methods may be jointly failing
  - Imbalanced positive/negative FI: Some metrics may be misleading

- First 3 experiments:
  1. Run on Cars/Cats dataset with ResNet50 - expect high reliability and clear metric separation
  2. Run on Mountain Dogs dataset with ResNet50 - expect low reliability and joint failures
  3. Compare ResNet50 vs VGG11 on ImageNet - expect model-dependent metric behavior

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do the proposed metrics perform on datasets with highly overlapping features between classes?
- Basis in paper: [explicit] The paper tests on a Mountain Dogs dataset with similar-looking classes but does not extensively explore datasets with highly overlapping features.
- Why unresolved: The paper only briefly mentions the challenge of similar features but does not provide a comprehensive evaluation on datasets designed to test this scenario.
- What evidence would resolve it: Extensive testing on datasets with known feature overlap and comparison of metric performance across these datasets.

### Open Question 2
- Question: How sensitive are the metrics to the choice of mosaic construction (e.g., number of images, arrangement)?
- Basis in paper: [inferred] The paper constructs 2×2 mosaics but does not explore alternative configurations or their impact on metric reliability.
- Why unresolved: The paper does not experiment with different mosaic sizes or arrangements, leaving the robustness of the metrics to these choices untested.
- What evidence would resolve it: Systematic experiments varying mosaic size and arrangement, showing how these changes affect metric consistency and reliability.

### Open Question 3
- Question: How do the metrics compare to user study evaluations in terms of reliability and validity?
- Basis in paper: [explicit] The paper mentions user studies as a gold standard but does not directly compare their metrics to user study results.
- Why unresolved: The paper does not include user studies to validate their metrics against human judgments.
- What evidence would resolve it: Conducting user studies alongside metric evaluations and comparing the agreement between human judgments and metric scores.

### Open Question 4
- Question: How do the metrics perform when the model's ability to distinguish classes is imperfect?
- Basis in paper: [inferred] The paper assumes models can distinguish classes but does not explore scenarios where this assumption is violated.
- Why unresolved: The paper does not test the metrics on models with varying levels of classification accuracy.
- What evidence would resolve it: Experiments using models with different accuracies on the same dataset and analyzing how this affects the metric scores and reliability.

## Limitations
- The reliability metrics are validated through internal consistency rather than external ground truth, making it unclear whether consistent rankings translate to meaningful quality differences
- The framework assumes that FI distribution patterns are stable enough across different images for reliability testing to be meaningful
- Some saliency methods (particularly B-cos) only provide positive FI, potentially biasing certain metrics

## Confidence
- **High confidence**: The classification metrics framework is well-defined and mathematically sound
- **Medium confidence**: The psychometric reliability validation approach is reasonable given the constraints, but lacks external validation
- **Low confidence**: The claim that "no single saliency method excels across all metrics" may reflect metric design choices rather than fundamental differences in method quality

## Next Checks
1. Test the framework on a simpler dataset (e.g., MNIST) where ground truth explanations are more straightforward to establish, allowing comparison between the classification metrics and direct ground truth evaluation
2. Analyze the correlation between Krippendorff's α values and actual performance differences on held-out test sets to verify that reliability predicts validity
3. Investigate whether the joint failure patterns observed in difficult datasets (Mountain Dogs) persist across different model architectures and whether this indicates fundamental limitations in current saliency methods