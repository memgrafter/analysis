---
ver: rpa2
title: Fake or Compromised? Making Sense of Malicious Clients in Federated Learning
arxiv_id: '2403.06319'
source_url: https://arxiv.org/abs/2403.06319
tags:
- clients
- attack
- fake
- data
- hybrid
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the lack of clarity in federated learning
  (FL) poisoning research by presenting a spectrum of adversary models, ranging from
  fake clients (easy to inject, low impact) to compromised clients (high impact, expensive)
  and introducing a hybrid model in between. The hybrid adversary first compromises
  a small number of real clients, uses their data to train a generative model (DDPM
  or DCGAN), and generates synthetic data to mount stronger attacks against robust
  aggregation rules.
---

# Fake or Compromised? Making Sense of Malicious Clients in Federated Learning

## Quick Facts
- arXiv ID: 2403.06319
- Source URL: https://arxiv.org/abs/2403.06319
- Reference count: 40
- Key outcome: Hybrid attacks achieve attack impacts close to compromised attacks at much lower cost by combining compromised client data with synthetic data generation.

## Executive Summary
This paper addresses the lack of clarity in federated learning poisoning research by presenting a spectrum of adversary models ranging from fake clients (easy to inject, low impact) to compromised clients (high impact, expensive) and introducing a hybrid model in between. The hybrid adversary first compromises a small number of real clients, uses their data to train a generative model (DDPM or DCGAN), and generates synthetic data to mount stronger attacks against robust aggregation rules. Experiments on CIFAR10 and FEMNIST show that hybrid attacks achieve attack impacts close to compromised attacks at a much lower cost (e.g., 30% attack impact at $500 vs $30,000), with DDPM generating more realistic synthetic data than DCGAN. This clarifies threat landscapes for FL system design and identifies the need for versatile defenses.

## Method Summary
The paper evaluates federated learning poisoning attacks under three adversary models: fake clients, compromised clients, and a hybrid approach. The hybrid attack uses compromised client data to train generative models (DDPM or DCGAN) that create synthetic data for fake clients. The attack optimization uses DYN-OPT framework against various robust aggregation rules including Median, Norm-Bounding, Trimmed-Mean, and Multi-Krum. Experiments run on CIFAR10 and FEMNIST with non-IID data distribution (Dirichlet β=0.5), using 2000/1000 FL rounds respectively with 25 clients per round. Attack impact is measured as the reduction in global model accuracy compared to benign training.

## Key Results
- Hybrid attacks achieve attack impacts close to compromised attacks at much lower cost (30% impact at $500 vs $30,000)
- DDPM generates more realistic synthetic data than DCGAN, especially with limited and noisy datasets
- Fake clients can cause significant attack impact when their updates have larger norms than benign updates after Norm-Bounding aggregation
- Compromised clients provide higher attack impact but at prohibitive costs compared to hybrid approaches

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Hybrid adversary achieves higher attack impact with lower cost by combining compromised and fake clients.
- Mechanism: Compromised clients provide real data to train generative models (DDPM/DCGAN), which then generate synthetic data to create more impactful fake client updates.
- Core assumption: The synthetic data generated by DDPM/DCGAN sufficiently mimics the distribution of benign client data.
- Evidence anchors: [abstract] "hybrid attacks achieve attack impacts close to compromised attacks at a much lower cost (e.g., 30% attack impact at $500 vs $30,000)..."; [section V] "In general, the combination of technical challenges and the high cost of compromising genuine clients in FL makes it a difficult task for an attacker to launch a successful model poisoning attack using only compromised clients."

### Mechanism 2
- Claim: DDPM generates more realistic synthetic data than DCGAN, leading to higher attack impact.
- Mechanism: DDPM's diffusion process iteratively exchanges information between data points to reveal underlying structure, capturing complex distributions better than DCGAN's adversarial approach.
- Core assumption: The quality of synthetic data directly correlates with attack impact against robust aggregation rules.
- Evidence anchors: [abstract] "DDPM generating more realistic synthetic data than DCGAN..."; [section II-B] "DDPM is based on the idea of diffusion, which is a process of iterative exchange of information between the data points in order to reveal their underlying structure."

### Mechanism 3
- Claim: Fake clients can cause significant attack impact when their updates have larger norms than benign updates after Norm-Bounding aggregation.
- Mechanism: As global model converges, benign updates have smaller norms that fall below threshold, while malicious updates are scaled to threshold, making them proportionally larger in aggregation.
- Core assumption: The Norm-Bounding threshold τ is set too high, allowing malicious updates to dominate.
- Evidence anchors: [section VI-B] "We formulate the DYN-OPT attack against AGR bound to Norm using the original framework proposed in [2]."; [section VII-A] "Figure 6 shows the L2 norm of the updates... when the global model starts to converge, the L2 norm of the local updates from benign updates becomes smaller than the threshold."

## Foundational Learning

- Concept: Federated Learning (FL) fundamentals
  - Why needed here: Understanding how clients train locally and send updates to server is crucial for grasping poisoning attack vectors.
  - Quick check question: What is the difference between data poisoning and model poisoning in FL?

- Concept: Generative Adversarial Networks (GANs) and Diffusion Models
  - Why needed here: The paper compares DCGAN and DDPM for generating synthetic data - understanding both architectures is essential.
  - Quick check question: How does DDPM's diffusion process differ from GAN's adversarial training approach?

- Concept: Robust Aggregation Rules (AGRs) and Byzantine tolerance
  - Why needed here: The paper evaluates attacks against various AGRs - understanding how they work is crucial for interpreting results.
  - Quick check question: What is the key difference between adaptive and agnostic robust AGRs?

## Architecture Onboarding

- Component map: Client-side model training → Server-side aggregation rules → Attack infrastructure (compromised data collection → generative model training → synthetic data generation → fake client injection) → Evaluation pipeline (test accuracy measurement → attack impact calculation)

- Critical path: Compromised client → Data collection → Generative model training → Synthetic data generation → Fake client update creation → Model poisoning attack → Server aggregation → Attack impact measurement

- Design tradeoffs:
  - Generative model complexity vs. attack effectiveness
  - Number of compromised clients vs. synthetic data quality
  - Attack cost vs. impact on global model accuracy
  - Detection risk vs. attack potency

- Failure signatures:
  - Synthetic data detected by robust AGRs
  - Generative models produce low-quality samples
  - Attack impact plateaus despite increased resources
  - Server implements effective defense mechanisms

- First 3 experiments:
  1. Baseline: Evaluate FedAvg without attacks to establish normal convergence behavior
  2. Simple attack: Inject fake clients with random updates against different AGRs
  3. Hybrid attack: Compromise 1% of clients, generate synthetic data with DDPM, evaluate against adaptive AGRs

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do different generative models (e.g., DDPM, DCGAN, StyleGAN) compare in terms of their effectiveness in generating realistic synthetic data for hybrid attacks in federated learning?
- Basis in paper: [explicit] The paper mentions using DDPM and DCGAN for generating synthetic data in hybrid attacks and notes that DDPM generates more realistic synthetic data than DCGAN, especially with limited and noisy datasets.
- Why unresolved: The paper only compares DDPM and DCGAN, leaving the effectiveness of other generative models unexplored.
- What evidence would resolve it: Empirical experiments comparing the attack impact of hybrid attacks using different generative models (e.g., DDPM, DCGAN, StyleGAN) on various datasets and aggregation rules.

### Open Question 2
- Question: How does the data distribution of compromised clients affect the quality of synthetic data generated by DDPM in hybrid attacks?
- Basis in paper: [explicit] The paper discusses collecting data from compromised clients and generating synthetic data using DDPM, but does not explicitly analyze how the data distribution of compromised clients impacts the quality of generated data.
- Why unresolved: The paper focuses on the overall effectiveness of DDPM in generating synthetic data but does not delve into the specific impact of the data distribution of compromised clients.
- What evidence would resolve it: Experiments varying the data distribution of compromised clients (e.g., using different Dirichlet parameters) and analyzing the resulting quality of synthetic data and attack impact.

### Open Question 3
- Question: How do hybrid attacks perform against more advanced robust aggregation rules that are specifically designed to handle data poisoning attacks?
- Basis in paper: [explicit] The paper evaluates hybrid attacks against standard robust aggregation rules (e.g., Median, Norm-Bounding, Trimmed-Mean, Multi-Krum) but does not explore their performance against more advanced defenses designed for data poisoning.
- Why unresolved: The paper focuses on evaluating hybrid attacks against common robust aggregation rules, leaving the effectiveness against more specialized defenses unexplored.
- What evidence would resolve it: Experiments testing hybrid attacks against advanced robust aggregation rules (e.g., CRFL, FoolsGold) and analyzing their performance in terms of attack impact and detection rates.

## Limitations

- The paper's core contribution depends heavily on the assumption that generative models can produce realistic synthetic data that evades detection, which hasn't been thoroughly validated against state-of-the-art robust aggregation defenses
- Attack cost analysis relies on external economic estimates that may not reflect actual market conditions
- The generalizability of results to other datasets and model architectures remains unclear

## Confidence

- **High Confidence:** The characterization of adversary models (fake, compromised, hybrid) and their relative costs and capabilities; the experimental methodology for measuring attack impact against specific aggregation rules
- **Medium Confidence:** The comparative effectiveness of DDPM vs DCGAN for generating synthetic data; the specific attack cost estimates ($500 vs $30,000) based on compromised client percentages
- **Low Confidence:** The scalability of hybrid attacks to larger, more complex datasets; the robustness of synthetic data generation against emerging detection techniques

## Next Checks

1. Test hybrid attacks against additional robust aggregation rules (e.g., FedNova, SCAFFOLD) and adaptive defenses to verify the claimed advantages over simple fake client attacks
2. Conduct ablation studies varying the percentage of compromised clients (beyond 0.1%, 0.3%, 0.5%) to determine the minimum threshold needed for effective synthetic data generation
3. Evaluate attack transferability across different model architectures (CNNs, transformers) and datasets to assess generalizability beyond CIFAR10 and FEMNIST