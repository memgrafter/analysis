---
ver: rpa2
title: 'ICCV23 Visual-Dialog Emotion Explanation Challenge: SEU_309 Team Technical
  Report'
arxiv_id: '2407.09760'
source_url: https://arxiv.org/abs/2407.09760
tags:
- emotion
- language
- explanation
- visual
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a multi-modal approach to generating emotion
  explanations from visual-dialog interactions in art discussions, combining Language
  Models (LM) and Large Vision Language Models (LVLM). The method uses BLIP2 for image
  captioning in the LM-based approach and LLaVA for direct image-text processing in
  the LVLM-based approach, with fine-tuning and ensemble techniques.
---

# ICCV23 Visual-Dialog Emotion Explanation Challenge: SEU_309 Team Technical Report

## Quick Facts
- arXiv ID: 2407.09760
- Source URL: https://arxiv.org/abs/2407.09760
- Authors: Yixiao Yuan; Yingzhe Peng
- Reference count: 15
- Primary result: Hybrid LM+LVLM approach achieved 52.36 weighted F1 and 0.26 BLEU on visual-dialog emotion explanation task

## Executive Summary
This paper presents a multi-modal approach to generating emotion explanations from visual-dialog interactions in art discussions, combining Language Models (LM) and Large Vision Language Models (LVLM). The method uses BLIP2 for image captioning in the LM-based approach and LLaVA for direct image-text processing in the LVLM-based approach, with fine-tuning and ensemble techniques. The hybrid strategy achieved top performance in the ICCV23 Visual-Dialog Emotion Explanation Challenge with a weighted F1 score of 52.36 and BLEU score of 0.26, demonstrating the effectiveness of combining LM and LVLM strengths for accurate emotion classification and nuanced explanation generation.

## Method Summary
The team employed a hybrid approach combining LM-based and LVLM-based methods. For the LM-based approach, they used BLIP2 for image captioning, concatenated with conversation text, and fine-tuned BART-large with 5-fold cross-validation. For the LVLM-based approach, they used LLaVA with LoRA fine-tuning (rank=96, alpha=192). The hybrid strategy combined ensemble voting for emotion classification from the LM models with LVLM-generated explanations. The final output used hard voting for emotion classification and LVLM explanations, achieving the highest performance on the challenge leaderboard.

## Key Results
- Hybrid approach achieved 52.36 weighted F1 score for emotion classification
- Generated explanations achieved 0.26 BLEU score
- Ensemble method improved emotion classification performance by 0.827 weighted F1 compared to single models
- LVLM-based approach showed superior explanation generation quality compared to LM-based approach

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Ensemble of multiple LM models improves emotion classification accuracy by reducing model-specific bias.
- Mechanism: The team trains five separate LM models on different folds of the data, then uses majority voting to aggregate their predictions for emotion classification. This approach reduces the impact of any single model's bias or overfitting on the training data.
- Core assumption: Different models trained on different subsets of data will make different types of errors, and majority voting can cancel out these errors.
- Evidence anchors:
  - [abstract] "For the emotion classification part, we can utilize model ensemble to reduce classification bias. Specifically, we divide the dataset into 5 folds and train a language model for each fold. During the final prediction, we use the voting method for emotion classification."
  - [section] "Tables 4 show the CV performance and the performance in leaderboard. For BLEU in ensemble model, we use the explanation generated by Fold 4. We find that ensembling can greatly improve the performance of emotion classification. On the validation set, the Ensemble model show an improvement of 0.827 in weighted F1 compared to a single model (Fold 5)."
  - [corpus] Weak - no direct corpus evidence for ensemble voting in visual-dialog emotion tasks.
- Break condition: If the models are too similar or the dataset is too small, the ensemble may not provide significant improvement over individual models.

### Mechanism 2
- Claim: LVLM models provide superior explanation generation compared to LM models by directly processing image-text pairs.
- Mechanism: The LVLM approach (LLaVA) processes both the image and text conversation simultaneously through a vision encoder and language model, allowing it to generate more contextually relevant explanations that incorporate visual details directly.
- Core assumption: Direct multimodal processing captures more nuanced visual information than converting images to text first.
- Evidence anchors:
  - [abstract] "In the LVLM-based method, we use the LLA V A [10] to combine image and text input. It's a novel end-to-end trained large multimodal model that combines a vision encoder and Vicuna for general-purpose visual and language understanding."
  - [section] "Table 5 presents our leaderboard performance. Owing to computational resource constraints, we were unable to employ k-fold validation on the LVLM-based method." - This suggests they recognized the LVLM approach as distinct and valuable.
  - [corpus] Weak - no direct corpus evidence comparing LVLM vs LM for explanation quality in visual dialog tasks.
- Break condition: If the vision encoder is poor quality or the LVLM is undertrained, the direct processing may introduce more errors than the two-stage LM approach.

### Mechanism 3
- Claim: Hybrid approach combining LM and LVLM strengths achieves optimal performance by leveraging their complementary capabilities.
- Mechanism: The team uses the ensemble voting from LM models for emotion classification (where LM excels) and the LVLM model for explanation generation (where LVLM excels), creating a hybrid system that outperforms either approach alone.
- Core assumption: The strengths of LM (classification) and LVLM (generation) are complementary and can be effectively combined.
- Evidence anchors:
  - [abstract] "Our comparative analysis indicates that the LM-based models excel in emotion classification, as evidenced by their superior Weighted F1 scores, while LVLM-based models show a notable advantage in explanation generation, demonstrated by higher BLEU scores. To leverage the strengths of both approaches, we propose a hybrid strategy."
  - [section] "Consequently, this hybrid approach achieves a final performance with a Weighted F1 score of 52.361 and a BLEU score of 0.2641, effectively harnessing the complementary strengths of both methodologies."
  - [corpus] Weak - no direct corpus evidence for hybrid approaches in visual-dialog emotion explanation tasks.
- Break condition: If the two approaches are not sufficiently complementary or if the integration introduces significant overhead, the hybrid approach may not outperform the best individual model.

## Foundational Learning

- Concept: Multimodal learning fundamentals
  - Why needed here: Understanding how to integrate visual and textual information is crucial for this task that requires processing both images and dialog conversations about art.
  - Quick check question: What are the key differences between early fusion and late fusion approaches in multimodal learning?

- Concept: Ensemble methods and their bias-variance tradeoff
  - Why needed here: The team uses ensemble voting for emotion classification, which requires understanding how combining multiple models affects overall performance.
  - Quick check question: How does increasing the number of models in an ensemble affect bias and variance in classification tasks?

- Concept: Vision-language model architecture
  - Why needed here: Understanding the LVLM architecture (vision encoder + language model) is essential for implementing and debugging the direct image-text processing approach.
  - Quick check question: What are the key components of a vision-language model like LLaVA and how do they interact?

## Architecture Onboarding

- Component map: Image preprocessing → BLIP2 (LM-based) or CLIP (LVLM-based) → Text processing → Bart-Large (LM-based) or Vicuna (LVLM-based) → Ensemble voting module (LM-based only) → Output selection logic (hybrid approach) → Training pipeline with LoRA adapters (LVLM-based)

- Critical path: Image-to-text conversion (BLIP2) or direct multimodal input (LLaVA) → Text embedding and dialog processing → Emotion classification (ensemble voting for LM-based) → Explanation generation (LVLM-based output) → Final output assembly (hybrid approach)

- Design tradeoffs:
  - LM-based: More interpretable, requires image captioning step, better at classification
  - LVLM-based: End-to-end learning, better at generation, requires more computational resources
  - Ensemble: Improves classification but increases inference time and complexity

- Failure signatures:
  - Low BLEU scores: LVLM explanation generation issues, poor image-text alignment
  - Low F1 scores: Classification model confusion, poor ensemble coordination
  - High variance across folds: Dataset issues, insufficient training data

- First 3 experiments:
  1. Test individual LM models on validation set to establish baseline performance
  2. Implement and test ensemble voting mechanism with LM models
  3. Compare LVLM model output quality with LM-based approach on sample inputs

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do different emotion classification ensembles compare in performance, such as weighted voting vs. confidence-based selection?
- Basis in paper: [explicit] The paper mentions using voting method for emotion classification but does not explore alternative ensemble strategies
- Why unresolved: The paper only implements basic voting ensemble for emotion classification without comparing it to other ensemble techniques
- What evidence would resolve it: Systematic comparison of different ensemble methods (weighted voting, confidence-based selection, stacking) on the same validation set

### Open Question 2
- Question: What is the optimal ratio of LM-based to LVLM-based contributions in the hybrid approach for different evaluation metrics?
- Basis in paper: [inferred] The paper uses a binary hard voting mechanism and direct LVLM explanations without exploring weighted combinations
- Why unresolved: The current hybrid approach uses fixed equal contributions from both models rather than optimizing the weighting
- What evidence would resolve it: Ablation studies varying the contribution weights of LM and LVLM components across different evaluation metrics

### Open Question 3
- Question: How does the performance scale with larger model sizes or different architectural choices for both LM and LVLM components?
- Basis in paper: [explicit] The paper uses specific model sizes (Bart-Large, LLaVA 7B) without exploring larger alternatives
- Why unresolved: Computational resource constraints limited experimentation with larger models
- What evidence would resolve it: Comparative results using larger model variants (e.g., LLaVA-13B, GPT-4V) under similar training conditions

### Open Question 4
- Question: What is the impact of different image captioning models on the LM-based approach's performance?
- Basis in paper: [explicit] The paper uses BLIP2 for image captioning but does not compare it with alternative captioning models
- Why unresolved: Only one image captioning model (BLIP2) was evaluated in the LM-based approach
- What evidence would resolve it: Performance comparison using different image captioning models (e.g., BLIP, Flamingo) with the same LM architecture

### Open Question 5
- Question: How do different fine-tuning strategies (LoRA configurations, learning rates) affect the LVLM-based model's performance?
- Basis in paper: [explicit] The paper uses specific LoRA parameters (rank 96, alpha 192) without exploring alternatives
- Why unresolved: Limited hyperparameter search due to computational constraints
- What evidence would resolve it: Systematic evaluation of different LoRA configurations and learning rates on the validation set

## Limitations

- Lack of detailed dataset information and preprocessing steps limits reproducibility
- Computational constraints prevented comprehensive validation of LVLM approach
- Missing ablation studies and statistical significance testing for performance claims
- Insufficient error analysis to understand systematic weaknesses of the approach

## Confidence

- **High Confidence**: The general methodology of using ensemble voting for emotion classification and hybrid LM/LVLM approaches is sound and well-established in the literature.
- **Medium Confidence**: The reported performance metrics (52.36 weighted F1, 0.26 BLEU) are plausible given the complexity of the task, but lack of detailed ablation studies reduces confidence in the magnitude of improvements.
- **Low Confidence**: The specific implementation details necessary for exact reproduction are insufficient, and the absence of comprehensive error analysis limits understanding of where the approach succeeds or fails.

## Next Checks

1. **Ablation Study**: Perform systematic evaluation of the LM-only, LVLM-only, and hybrid approaches on a held-out test set to quantify the contribution of each component and verify that the hybrid approach provides statistically significant improvements.

2. **Dataset Analysis**: Conduct detailed analysis of the training and validation data distribution, including emotion label balance, conversation length distributions, and image characteristics to understand potential biases and limitations in the evaluation.

3. **Error Case Investigation**: Generate and analyze specific examples where the model succeeds vs fails, particularly focusing on edge cases in emotion classification and explanation generation to identify systematic weaknesses and guide future improvements.