---
ver: rpa2
title: Automatic Question-Answer Generation for Long-Tail Knowledge
arxiv_id: '2403.01382'
source_url: https://arxiv.org/abs/2403.01382
tags:
- knowledge
- entities
- datasets
- question
- llms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work addresses the problem of evaluating and improving large
  language models (LLMs) for answering questions about long-tail knowledge entities,
  which are entities that rarely appear in common knowledge bases like Wikipedia.
  The authors propose an automatic approach to generate specialized QA datasets for
  tail entities using Wikidata knowledge graphs, where tail entities are defined based
  on their degree in the graph.
---

# Automatic Question-Answer Generation for Long-Tail Knowledge

## Quick Facts
- arXiv ID: 2403.01382
- Source URL: https://arxiv.org/abs/2403.01382
- Reference count: 27
- Primary result: LLMs show significantly worse performance on long-tail knowledge entities, with accuracy dropping from 84.5% on common knowledge to 47.1% on tail entities

## Executive Summary
This paper addresses the challenge of evaluating and improving large language models (LLMs) on long-tail knowledge entities - those that rarely appear in common knowledge bases like Wikipedia. The authors propose an automatic approach to generate specialized QA datasets for tail entities using Wikidata knowledge graphs, where tail entities are defined based on their degree in the graph rather than Wikipedia frequency. Experiments with GPT-3 on these generated datasets demonstrate that LLMs struggle significantly with long-tail knowledge compared to common knowledge, and while augmentation with external resources like Wikipedia and Wikidata can improve performance, effectiveness is limited by retrieval model capabilities.

## Method Summary
The authors developed an automatic pipeline for generating QA datasets focused on long-tail entities from Wikidata. They define tail entities based on degree information in the knowledge graph, extracting triplets (subject, property, object) for entities with low connectivity. GPT-3 is used to generate questions from these triplets, with difficulty controlled by balancing property types. The generated datasets are evaluated with and without external resource augmentation using Dense Passage Retrieval (DPR) from Wikipedia and knowledge graph-based re-ranking. The approach enables systematic evaluation of LLM performance on long-tail knowledge while addressing challenges like noisy triplet filtering and question generation quality control.

## Key Results
- GPT-3 accuracy drops significantly on tail datasets: from 84.5% on existing common knowledge datasets to 47.1% on coarse-tail (15-100 degree) and 22.8% on fine-tail (<3 degree) entities
- DPR retrieval performs worse on tail datasets (63-66% top-100 accuracy) compared to head datasets (85% accuracy)
- KG+DPR augmentation improves GPT-3 performance on tail datasets from 22.1% to 30.95% accuracy
- Error analysis shows GPT-3 struggles to extract correct answers even when provided with relevant context from retrieved passages

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The degree-based tail entity definition in Wikidata captures true long-tail entities better than Wikipedia frequency-based approaches
- Mechanism: Wikidata degree (number of connected triplets) reflects entity engagement with general knowledge; low-degree entities are rarely connected and thus represent true long-tail knowledge
- Core assumption: Entity degree in a knowledge graph correlates with entity prevalence in pre-training data
- Evidence anchors:
  - [abstract]: "We propose a novel approach to defining tail entities based on their degree information in Wikidata, as opposed to [7] relying on Wikipedia"
  - [section 3.2.1]: "In this study, we propose a novel approach to defining tail entities based on their degree information in Wikidata"
  - [corpus]: Weak evidence - no direct corpus data comparing degree-based vs frequency-based tail definitions
- Break condition: If entity degree doesn't correlate with pre-training data frequency, this mechanism fails

### Mechanism 2
- Claim: GPT-3's performance on long-tail knowledge is fundamentally limited by the retrieval model's ability to find relevant information
- Mechanism: When augmenting GPT-3 with external resources, the retrieval model (DPR) fails to find relevant passages for long-tail entities, providing irrelevant context that confuses the LLM
- Core assumption: The quality of retrieved passages directly impacts LLM performance on tail entities
- Evidence anchors:
  - [section 4.3]: "DPR performs consistently worse on our tail datasets than the existing datasets... DPR's retrieval often leads to irrelevant passages on long-tail knowledge"
  - [section 4.3]: "Consequently, the presence of these additional contexts confuses GPT-3 and adversely affects its performance"
  - [corpus]: Strong evidence from Table 3 showing DPR Top-100 accuracy drops from 85% on head datasets to 63-66% on tail datasets
- Break condition: If retrieval model improves to handle long-tail entities effectively, this mechanism breaks

### Mechanism 3
- Claim: Joint learning from multiple external resources (Wikipedia + Wikidata) improves LLM performance on long-tail knowledge
- Mechanism: Re-ranking DPR-retrieved passages using knowledge graph triplets improves retrieval accuracy, which in turn improves GPT-3 performance
- Core assumption: Knowledge graph triplets provide complementary information that helps identify relevant Wikipedia passages
- Evidence anchors:
  - [section 4.4]: "DPR retrieval accuracy is improved by up to 6% with the help of knowledge graphs"
  - [section 4.4]: "The improvement in DPR retrieval accuracy leads to the improvement of GPT-3's QA performance... from 22.1% to 30.95%"
  - [corpus]: Moderate evidence from Table 5 showing improvement in both DPR accuracy and GPT-3 performance
- Break condition: If knowledge graph provides no additional discriminative signal for passage selection

## Foundational Learning

- Concept: Knowledge graph triplet structure (subject, property, object)
  - Why needed here: The entire dataset generation process relies on extracting and manipulating triplets from Wikidata
  - Quick check question: Given a triplet [s1, property, s2], which element becomes the answer in the generated QA pair?

- Concept: Entity degree in graph theory
  - Why needed here: Tail entity definition is based on node degree, which determines which entities are considered "long-tail"
  - Quick check question: If an entity has degree 2, how many triplets have this entity as the subject?

- Concept: Dense Passage Retrieval (DPR) architecture
  - Why needed here: DPR is used to retrieve relevant passages from Wikipedia to augment LLM performance
  - Quick check question: What is the primary limitation of DPR when dealing with long-tail knowledge entities?

## Architecture Onboarding

- Component map: GPT-3 (question generation and answering) ←→ Wikidata KG (triplet extraction) ←→ Wikipedia (DPR retrieval) ←→ Evaluation pipeline
- Critical path: Wikidata triplet extraction → GPT-3 question generation → GPT-3 answering → Evaluation
- Design tradeoffs: Automated dataset generation (fast, scalable) vs. quality control (noisy questions, granularity issues)
- Failure signatures: Low accuracy on tail datasets, DPR failing to retrieve relevant passages, GPT-3 providing incorrect answers despite correct context
- First 3 experiments:
  1. Generate QA pairs using different GPT-3 prompts with complete vs. incomplete triplets
  2. Measure DPR retrieval accuracy on head vs. tail entity datasets
  3. Test GPT-3 performance with and without DPR-retrieved passages on the same question set

## Open Questions the Paper Calls Out
None

## Limitations

- The degree-based tail entity definition lacks direct empirical validation comparing it to frequency-based approaches
- The automated dataset generation introduces quality control challenges with noisy questions and granularity issues
- The proposed solutions show modest absolute performance improvements (22.1% to 30.95%), suggesting fundamental limitations in current approaches

## Confidence

- Tail entity definition mechanism: Medium - Limited empirical validation comparing degree-based vs frequency-based approaches
- Retrieval model limitation: High - Strong evidence from consistent DPR performance drops on tail datasets
- KG+DPR augmentation effectiveness: Medium - Statistically significant but modest absolute improvements

## Next Checks

1. **Tail Definition Validation**: Conduct a controlled experiment comparing degree-based vs. frequency-based tail entity definitions using the same generation pipeline to empirically validate which better predicts LLM performance gaps.

2. **Retrieval Quality Analysis**: Perform detailed error analysis on DPR failures specifically for tail entities - measure whether failures stem from missing passages, poor retrieval embeddings, or entity name ambiguity.

3. **Cross-LLM Generalization**: Test whether the observed performance patterns hold across different LLMs (e.g., Claude, LLaMA) to determine if limitations are specific to GPT-3's training or represent fundamental challenges for the approach.