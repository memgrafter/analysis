---
ver: rpa2
title: 'FuseMoE: Mixture-of-Experts Transformers for Fleximodal Fusion'
arxiv_id: '2402.03226'
source_url: https://arxiv.org/abs/2402.03226
tags:
- modalities
- each
- experts
- data
- gating
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper introduces FuseMoE, a mixture-of-experts (MoE) framework\
  \ for multimodal fusion of FlexiModal data\u2014data that may be incomplete, irregularly\
  \ sampled, and involve many modalities. The core method uses sparsely gated MoE\
  \ layers with a novel Laplace gating function and per-modality routers to handle\
  \ missing modalities and irregular temporal patterns."
---

# FuseMoE: Mixture-of-Experts Transformers for Fleximodal Fusion

## Quick Facts
- arXiv ID: 2402.03226
- Source URL: https://arxiv.org/abs/2402.03226
- Authors: Xing Han; Huy Nguyen; Carl Harris; Nhat Ho; Suchi Saria
- Reference count: 40
- Primary result: Achieves state-of-the-art performance across MIMIC-IV, PAM, CMU-MOSI/MOSEI, and CIFAR-10 benchmarks for FlexiModal data fusion

## Executive Summary
FuseMoE introduces a mixture-of-experts framework for multimodal fusion of FlexiModal data, which may be incomplete, irregularly sampled, and involve many modalities. The method uses sparsely gated MoE layers with a novel Laplace gating function and per-modality routers to handle missing modalities and irregular temporal patterns. Theoretical analysis shows that Laplace gating offers improved convergence rates over standard softmax gating, while empirical results demonstrate state-of-the-art performance across diverse benchmarks.

## Method Summary
FuseMoE employs a mixture-of-experts architecture with per-modality routers and Laplace gating function to handle FlexiModal data. The core innovation is the Laplace gating function, which computes similarity via L2 distance between token representations and expert embeddings, providing better convergence properties than softmax gating. Per-modality routers allow specialized handling of missing modalities, while entropy regularization ensures balanced expert utilization across modalities. The method includes an mTAND module for handling irregular temporal patterns in time series data.

## Key Results
- Achieves state-of-the-art performance on MIMIC-IV ICU prediction tasks
- Outperforms existing methods on PAM activity monitoring and CMU-MOSI/MOSEI sentiment analysis
- Demonstrates robust handling of missing modalities across all tested benchmarks
- Shows scalability to many input types while maintaining performance

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The Laplace gating function improves parameter estimation convergence over softmax gating
- Mechanism: Laplace gating computes similarity via L2 distance between token representations and expert embeddings, which is less prone to representation collapse and magnitude bias compared to inner product softmax
- Core assumption: The FlexiModal data has heterogeneous distributions across modalities and missingness patterns
- Evidence anchors:
  - [abstract]: "Theoretically, our unique gating function contributes to enhanced convergence rates, leading to better performance in multiple downstream tasks."
  - [section]: "The Laplace gating function, characterized by its Euclidean term exp(−∥W − x∥2), is less prone to converge towards extreme weight distributions due to the bounded nature of this term."
  - [corpus]: Weak - corpus neighbors focus on gating mechanism improvements but not specifically Laplace gating
- Break condition: If input modalities are homogeneous or if representation collapse is not a concern

### Mechanism 2
- Claim: Per-modality routers handle missing modalities more effectively than joint routing
- Mechanism: Each modality has its own router that assigns embeddings to experts; when a modality is missing, the router assigns lower weights to experts responsible for that modality while still utilizing available modalities
- Core assumption: Missing modalities are common in FlexiModal data and need specialized handling
- Evidence anchors:
  - [section]: "This allows FuseMoE to effectively handle scenarios with missing modalities by dynamically adjusting the influence of experts primarily responsible for the absent data."
  - [section]: "The per-modality approach can better separate the present and missing modalities, reducing the influence of experts responsible for processing the absent inputs."
  - [corpus]: Weak - corpus neighbors don't discuss missing modality handling strategies
- Break condition: If all modalities are always present or if joint routing performs equally well

### Mechanism 3
- Claim: Entropy regularization ensures balanced expert utilization across modalities
- Mechanism: An entropy loss term encourages diverse expert selection across modalities while stabilizing expert preferences within each modality
- Core assumption: Expert utilization should be balanced to prevent domination by a few experts and to leverage cross-modal complementarity
- Evidence anchors:
  - [section]: "We implement an entropy regularization loss to ensure balanced and stable expert utilization... It maximizes the mutual information between modalities and experts."
  - [section]: "This allows FuseMoE to effectively handle scenarios with missing modalities by dynamically adjusting the influence of experts primarily responsible for the absent data."
  - [corpus]: Weak - corpus neighbors mention load balancing but not entropy regularization specifically
- Break condition: If expert utilization naturally balances without regularization or if regularization hurts performance

## Foundational Learning

- Concept: Mixture of Experts (MoE) architecture
  - Why needed here: MoE allows specialized processing for different modality types and handles variable input combinations
  - Quick check question: How does MoE differ from standard neural networks in terms of parameter sharing?

- Concept: Gating functions and routing mechanisms
  - Why needed here: Routing determines which experts process which inputs; Laplace gating provides better convergence properties
  - Quick check question: What is the key difference between softmax and Laplace gating in terms of similarity computation?

- Concept: Temporal irregularity handling in time series
  - Why needed here: FlexiModal data often has irregularly sampled time series that need discretization
  - Quick check question: How does the mTAND module handle irregular sampling compared to simple imputation?

## Architecture Onboarding

- Component map:
  Input modalities → Per-modality encoders → Irregularity encoder (mTAND) → MoE fusion layer → Output
  Per-modality routers + entropy loss + Laplace gating function

- Critical path: Data preprocessing → Per-modality encoding → Irregularity discretization → MoE routing → Expert processing → Weighted fusion → Task prediction

- Design tradeoffs:
  - Joint vs per-modality routers: Simplicity vs specialized handling
  - Number of experts: Model capacity vs computational cost
  - Top-K gating: Sparsity vs routing accuracy

- Failure signatures:
  - Poor performance on missing modalities: Check per-modality router configuration
  - Representation collapse: Verify Laplace gating implementation and expert diversity
  - Slow convergence: Check gating function parameters and regularization strength

- First 3 experiments:
  1. Baseline comparison: Replace Laplace gating with softmax gating while keeping all other components constant
  2. Router ablation: Compare joint router vs per-modality router performance on datasets with missing modalities
  3. Expert count scaling: Test performance with 8, 16, and 32 experts to find optimal tradeoff point

## Open Questions the Paper Calls Out
None

## Limitations
- Theoretical validation of convergence rate improvements is not fully detailed or rigorously proven
- Performance generalization to other FlexiModal domains beyond tested applications remains untested
- Computational overhead trade-offs between performance gains and cost are not quantified

## Confidence
- Mechanism 1 (Laplace gating convergence): Medium
- Mechanism 2 (Per-modality routers): High
- Mechanism 3 (Entropy regularization): Medium

## Next Checks
1. Implement a controlled experiment comparing Laplace gating vs softmax gating on synthetic data with known convergence properties, measuring both convergence speed and final performance across multiple random seeds
2. Test FuseMoE on a modified version of benchmark datasets where modalities are randomly dropped at varying rates (0-90%) to quantify robustness boundaries
3. Apply FuseMoE to a new FlexiModal domain (e.g., multimodal autonomous driving data) to evaluate whether performance gains transfer beyond tested domains