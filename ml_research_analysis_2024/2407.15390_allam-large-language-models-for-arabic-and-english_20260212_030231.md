---
ver: rpa2
title: 'ALLaM: Large Language Models for Arabic and English'
arxiv_id: '2407.15390'
source_url: https://arxiv.org/abs/2407.15390
tags:
- language
- arabic
- data
- english
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces ALLaM, a series of large language models designed
  to support Arabic Language Technologies (ALT) while maintaining strong performance
  in English. The authors address the challenge of expanding language capabilities
  in low-resource languages like Arabic without degrading performance in the original
  language.
---

# ALLaM: Large Language Models for Arabic and English

## Quick Facts
- arXiv ID: 2407.15390
- Source URL: https://arxiv.org/abs/2407.15390
- Reference count: 35
- Large language models that achieve state-of-the-art Arabic performance while maintaining/improving English capabilities

## Executive Summary
The paper introduces ALLaM, a series of large language models designed to support Arabic Language Technologies while maintaining strong performance in English. The key challenge addressed is expanding language capabilities in low-resource languages like Arabic without degrading performance in the original language. The authors demonstrate that through careful pretraining on balanced Arabic-English data, tokenizer expansion, and vocabulary learning, their models achieve state-of-the-art performance on Arabic benchmarks while also improving English performance compared to the original Llama-2 model.

## Method Summary
The authors develop ALLaM through a systematic approach that includes tokenizer expansion specifically designed for Arabic, vocabulary learning to handle language-specific tokens, and balanced pretraining on a mixture of Arabic and English data. The methodology emphasizes language alignment and transfer, with extensive alignment with human preferences to enhance overall performance. The training process involves careful consideration of the ratio and quality of data from both languages to ensure neither is compromised.

## Key Results
- State-of-the-art performance in Arabic benchmarks including MMLU Arabic, ACV A, and Arabic Exams
- Larger models (70B) simultaneously improve in both Arabic and English
- Smaller models may trade off between Arabic and English performance
- ALLaM improves English performance compared to the original Llama-2 model

## Why This Works (Mechanism)
The mechanism behind ALLaM's success lies in the careful balance between language-specific adaptation and general language model capabilities. By expanding the tokenizer to handle Arabic morphology and incorporating vocabulary learning, the model can better represent Arabic linguistic features. The balanced pretraining ensures that neither language dominates the learning process, allowing for effective cross-lingual transfer. The alignment with human preferences further refines the model's ability to handle both languages appropriately.

## Foundational Learning
- **Tokenizer Expansion**: Needed to handle Arabic morphology; Quick check: Evaluate tokenization accuracy on morphologically rich Arabic words
- **Vocabulary Learning**: Required for language-specific token representation; Quick check: Measure vocabulary coverage for Arabic-specific tokens
- **Balanced Pretraining**: Essential to prevent language dominance; Quick check: Monitor loss convergence for both languages during training
- **Language Alignment**: Critical for cross-lingual transfer; Quick check: Test performance on code-switching tasks
- **Human Preference Alignment**: Improves overall model quality; Quick check: Compare human ratings before and after alignment

## Architecture Onboarding
**Component Map**: Tokenizer Expansion -> Vocabulary Learning -> Balanced Pretraining -> Human Preference Alignment
**Critical Path**: The critical path is the sequential integration of tokenizer expansion followed by vocabulary learning, as these foundational components enable effective pretraining on both languages.
**Design Tradeoffs**: The main tradeoff is between model size and performance balance - larger models (70B) show simultaneous improvement in both languages, while smaller models require careful calibration to avoid performance trade-offs.
**Failure Signatures**: Performance degradation in one language when the other is emphasized during training, or inability to handle Arabic-specific morphological variations.
**First Experiments**: 1) Evaluate tokenizer performance on Arabic morphological variations, 2) Test vocabulary learning effectiveness on unseen Arabic words, 3) Compare balanced vs unbalanced pretraining on cross-lingual transfer

## Open Questions the Paper Calls Out
None

## Limitations
- Evaluation methodology lacks ablation studies to isolate contributions of individual components
- English performance improvements may be dataset-specific rather than generalizable
- Limited validation of Arabic dialectal variant performance beyond Modern Standard Arabic

## Confidence
High confidence: ALLaM achieves state-of-the-art Arabic performance across multiple standardized benchmarks
Medium confidence: Tokenizer expansion and vocabulary learning are key enablers, but direct experimental validation is lacking
Low confidence: English performance improvements are demonstrated but may be influenced by pretraining dataset characteristics

## Next Checks
1. Conduct ablation studies to isolate the impact of tokenizer expansion, vocabulary learning, and balanced pretraining
2. Perform cross-dataset validation to verify English performance improvements are not dataset-specific
3. Evaluate model performance on low-resource Arabic dialectal variants beyond Modern Standard Arabic