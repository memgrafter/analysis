---
ver: rpa2
title: Large Language Models Are Cross-Lingual Knowledge-Free Reasoners
arxiv_id: '2406.16655'
source_url: https://arxiv.org/abs/2406.16655
tags:
- reasoning
- knowledge
- languages
- language
- cross-lingual
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates cross-lingual reasoning capabilities in
  large language models by decomposing reasoning tasks into knowledge retrieval and
  knowledge-free reasoning components. Through experiments on adapted commonsense
  reasoning datasets and a newly constructed knowledge-free reasoning dataset (KFRD),
  the authors find that knowledge-free reasoning capability transfers nearly perfectly
  across languages (often exceeding 90% cross-lingual transfer ratio), while knowledge
  retrieval significantly hinders cross-lingual transfer.
---

# Large Language Models Are Cross-Lingual Knowledge-Free Reasoners

## Quick Facts
- arXiv ID: 2406.16655
- Source URL: https://arxiv.org/abs/2406.16655
- Reference count: 33
- Large language models show nearly perfect cross-lingual transfer (>90%) for knowledge-free reasoning tasks

## Executive Summary
This paper investigates cross-lingual reasoning capabilities in large language models by decomposing reasoning tasks into knowledge retrieval and knowledge-free reasoning components. The authors find that knowledge-free reasoning capability transfers nearly perfectly across languages (often exceeding 90% cross-lingual transfer ratio), while knowledge retrieval significantly hinders cross-lingual transfer. Through experiments on adapted commonsense reasoning datasets and a newly constructed knowledge-free reasoning dataset (KFRD), the study demonstrates that models share similar neural mechanisms for knowledge-free reasoning across languages, particularly in middle reasoning layers.

## Method Summary
The study performs LoRA fine-tuning on selected 7B-parameter LLMs (LLaMA-2-7B-Chat, BLOOMZ-MT-7B, Mistral-7B-Instruct-v0.1, Qwen-1.5-7B-Chat) in English and evaluates on 10 languages. Models are fine-tuned on adapted commonsense reasoning datasets with varying knowledge retrieval demand (StrategyQA, QASC) and on the newly constructed Knowledge-Free Reasoning Dataset (KFRD) containing arithmetic, symbolic, and logical reasoning tasks. Cross-lingual transfer performance is measured using the Cross-lingual Transfer Ratio (XLTR) metric. Interpretability analysis examines hidden states and neuron activation patterns using Cosine Similarity of Hidden States (CS) and Neuron Activation Overlap (NAO) metrics to compare computational similarity across languages.

## Key Results
- Knowledge-free reasoning tasks show nearly perfect cross-lingual transfer (>90% XLTR) across 10 languages
- Knowledge retrieval significantly hinders cross-lingual transfer, with lower XLTR values as knowledge retrieval demand increases
- Interpretability analysis reveals higher similarity in hidden representations and neuron activation patterns for knowledge-free reasoning tasks across languages, particularly in middle reasoning layers
- Target language proficiency impacts cross-lingual transfer effectiveness, with low-resource languages benefiting from continued pretraining

## Why This Works (Mechanism)
Knowledge-free reasoning tasks rely on universal logical and mathematical principles that transcend language barriers, allowing models to apply the same reasoning mechanisms across different languages. The middle layers of transformer models, which handle the core reasoning process, appear to develop language-agnostic computational patterns for these tasks. In contrast, knowledge retrieval requires access to language-specific factual information stored in different ways across languages, leading to poor cross-lingual transfer. The shared neural mechanisms for reasoning suggest that these models develop abstract reasoning capabilities that operate independently of the specific language used for input and output.

## Foundational Learning
- Cross-lingual Transfer Ratio (XLTR): Measures cross-lingual transferability by comparing model performance after fine-tuning on source language versus target language; needed to quantify how well reasoning capabilities transfer across languages
- Knowledge-Free Reasoning: Reasoning tasks that don't require external knowledge (arithmetic, symbolic, logical); needed as a baseline to understand pure reasoning capabilities separate from knowledge retrieval
- Neuron Activation Overlap (NAO): Measures overlap in activated neurons between source and target languages; needed to identify shared neural mechanisms for reasoning across languages
- LoRA fine-tuning: Parameter-efficient fine-tuning method that modifies attention mechanisms; needed for efficient adaptation of large models to different reasoning tasks
- Interpretability metrics (CS, NAO): Tools to analyze model internals and understand how reasoning differs between knowledge retrieval and knowledge-free tasks

## Architecture Onboarding

**Component map:** LoRA fine-tuning -> Cross-lingual evaluation -> Interpretability analysis (CS, NAO) -> XLTR calculation

**Critical path:** Fine-tuning on source language → evaluation on target languages → XLTR calculation → interpretability analysis of hidden states and neuron activations

**Design tradeoffs:** 7B-parameter models offer efficient computation but may limit generalizability to larger models; LoRA fine-tuning provides parameter efficiency but may constrain full model adaptation

**Failure signatures:** Low XLTR values indicate poor cross-lingual transfer, particularly for knowledge retrieval tasks; unexpected CS and NAO patterns suggest issues with neuron activation analysis or threshold selection

**Three first experiments:**
1. Fine-tune selected LLM on KFRD arithmetic tasks and evaluate cross-lingual transfer across all 10 languages
2. Measure CS and NAO on middle layers for knowledge-free versus knowledge retrieval tasks
3. Compare XLTR for low-resource languages (Arabic, Hebrew) using different model proficiency levels (vanilla, SFT, CPT, CPT+SFT)

## Open Questions the Paper Calls Out

**Open Question 1:** What specific neurons or neural pathways are responsible for knowledge-free reasoning versus knowledge retrieval in multilingual models? The paper suggests that knowledge-free reasoning shares similar neurons across languages while knowledge storage is language-specific, but lacks precise identification of specific reasoning or knowledge neurons.

**Open Question 2:** How does the depth of reasoning (number of reasoning steps) affect cross-lingual transfer ratios for knowledge-free reasoning tasks? The study uses datasets with depth-1 reasoning but doesn't systematically vary reasoning depth to measure its impact on cross-lingual transfer.

**Open Question 3:** What is the relationship between the model's attention patterns during cross-lingual knowledge-free reasoning versus knowledge retrieval tasks? The paper analyzes hidden states and neuron activation but doesn't examine attention mechanisms, which could provide additional insight into how models process reasoning differently across languages.

## Limitations

- Analysis relies on 7B-parameter models, limiting generalizability to larger models where architectural differences might affect cross-lingual transfer patterns
- KFRD dataset construction methodology depends on GPT-4-generated examples that may introduce subtle biases in reasoning task difficulty across languages
- Neuron Activation Overlap metric's sensitivity to threshold selection remains a methodological uncertainty that could affect interpretability results

## Confidence

- Cross-lingual knowledge-free reasoning transfer exceeds 90% with High confidence
- Knowledge retrieval negatively impacts cross-lingual transfer with High confidence
- Interpretability findings showing higher computational similarity in knowledge-free tasks with Medium confidence (threshold sensitivity uncertainty)
- Target language proficiency effects on transfer with Medium confidence (limited language sample)

## Next Checks

1. Replicate key findings using 13B and 70B parameter models to test scalability of cross-lingual reasoning transfer patterns
2. Conduct threshold sensitivity analysis for Neuron Activation Overlap metric across 0.1-0.9 thresholds to verify robustness of interpretability conclusions
3. Expand low-resource language testing beyond Arabic and Hebrew to include languages from different families (e.g., Swahili, Hindi) to validate target language proficiency effects