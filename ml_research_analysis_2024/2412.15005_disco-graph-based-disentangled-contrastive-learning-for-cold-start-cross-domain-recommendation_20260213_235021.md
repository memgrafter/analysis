---
ver: rpa2
title: 'DisCo: Graph-Based Disentangled Contrastive Learning for Cold-Start Cross-Domain
  Recommendation'
arxiv_id: '2412.15005'
source_url: https://arxiv.org/abs/2412.15005
tags:
- user
- domain
- target
- information
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes DisCo, a graph-based disentangled contrastive
  learning framework for cold-start cross-domain recommendation. The key challenge
  addressed is the negative transfer problem where similar user preferences in one
  domain may not transfer to another domain, leading to irrelevant information being
  incorporated.
---

# DisCo: Graph-Based Disentangled Contrastive Learning for Cold-Start Cross-Domain Recommendation

## Quick Facts
- arXiv ID: 2412.15005
- Source URL: https://arxiv.org/abs/2412.15005
- Reference count: 16
- Key outcome: DisCo achieves significant improvements in recommendation accuracy on cold-start cross-domain recommendation tasks, outperforming state-of-the-art baselines on four benchmark datasets

## Executive Summary
This paper introduces DisCo, a graph-based disentangled contrastive learning framework designed to address the negative transfer problem in cold-start cross-domain recommendation. The key insight is that similar user preferences in one domain may not transfer to another domain, leading to irrelevant information being incorporated. DisCo employs a multi-channel graph encoder to capture diverse user intents in each domain, constructs affinity graphs, and performs multi-step random walks to obtain high-order user similarities. The framework then uses an intent-wise contrastive learning approach to retain user similarity information in the target domain while explicitly identifying the rationale between domains to avoid negative transfer. Experiments on four benchmark datasets demonstrate that DisCo consistently outperforms existing state-of-the-art baselines.

## Method Summary
DisCo addresses cold-start cross-domain recommendation by using a multi-channel graph encoder to capture diverse user intents in each domain. The framework constructs affinity graphs and performs multi-step random walks to capture high-order user similarity relationships. It then employs a disentangled intent-wise contrastive learning approach, guided by user similarity, to refine the bridging of user intents across domains. The method preserves domain-specific information while transferring relevant source domain information to the target domain. The approach is evaluated on four domain pairs from the Amazon dataset, demonstrating significant improvements in recommendation accuracy for cold-start users.

## Key Results
- DisCo consistently outperforms existing state-of-the-art baselines on four benchmark datasets
- Achieves significant improvements in recommendation accuracy for cold-start cross-domain recommendation
- Effectively mitigates negative transfer by preserving target domain-specific user preferences while transferring relevant source domain information

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Multi-channel graph encoder captures diverse user intents, mitigating negative transfer by learning domain-specific and domain-shared representations separately.
- Mechanism: The multi-channel graph encoder processes user-item interactions in each domain independently, extracting intent-specific embeddings (Z_k) through separate GNN channels. This allows the model to distinguish between domain-shared preferences and domain-specific preferences before any cross-domain transfer.
- Core assumption: User intents can be meaningfully disentangled across channels such that each channel represents a coherent latent preference factor.
- Evidence anchors:
  - [abstract]: "we use a multi-channel graph encoder to capture diverse user intents."
  - [section]: "we leverage GNNs as the backbone to capture the fine-grained user intents in each domain... we iteratively update the representations of user and item nodes through a message-passing mechanism."
- Break condition: If user intents are not separable into orthogonal channels, the disentanglement assumption fails and negative transfer cannot be prevented.

### Mechanism 2
- Claim: Affinity graph construction with multi-step random walks generates high-order user similarity relationships that guide contrastive learning.
- Mechanism: Affinity graphs are built from the embedding space of each domain, and multi-step random walks are performed to obtain high-order user similarity matrices (T_k). These matrices serve as pseudo-labels for contrastive learning, ensuring that similar users are aligned while dissimilar ones are pushed apart.
- Core assumption: High-order random walks on the affinity graph capture meaningful similarity relationships that reflect true user preference structures.
- Evidence anchors:
  - [abstract]: "construct the affinity graph in the embedding space and perform multi-step random walks to capture high-order user similarity relationships."
  - [section]: "By treating user similarity as the edge weight between users, a fully connected user affinity graph R could be constructed... Considering a d step random walks (Lu et al. 2024), the high-order user similarity under intent k could be: T_k = αI + (1 − α) ˜R^d."
- Break condition: If the affinity graph does not reflect actual user similarity (e.g., due to noisy embeddings), the random walk will propagate incorrect similarity signals.

### Mechanism 3
- Claim: Intent-wise contrastive learning with cross-domain decoder explicitly identifies rationale between domains, preserving target domain-specific user preferences while transferring relevant source domain information.
- Mechanism: The model performs both intra-domain and inter-domain contrastive learning. Intra-domain contrastive loss retains user similarity within each domain, while inter-domain contrastive loss uses a cross-domain decoder to map source domain embeddings to target domain space, guided by user similarity in the target domain. This ensures only relevant information is transferred.
- Core assumption: The cross-domain decoder can effectively map relevant source domain information to the target domain while filtering out irrelevant collaborative information.
- Evidence anchors:
  - [abstract]: "propose a disentangled intent-wise contrastive learning approach, guided by user similarity, to refine the bridging of user intents across domains."
  - [section]: "we introduce a user representation decoder that could preserve the domain-specific information while capturing the domain-shared information between the source and target domains."
- Break condition: If the decoder fails to distinguish relevant from irrelevant information, negative transfer will still occur despite the contrastive learning framework.

## Foundational Learning

- Concept: Graph Neural Networks (GNNs) for user-item interaction modeling
  - Why needed here: GNNs can capture multi-hop neighborhood and high-order interaction information, which is crucial for understanding complex user preferences in recommendation systems.
  - Quick check question: What is the key difference between GNNs and traditional matrix factorization approaches in capturing user-item interactions?

- Concept: Contrastive learning for representation learning
  - Why needed here: Contrastive learning helps the model learn discriminative representations by pulling similar samples together and pushing dissimilar ones apart, which is essential for mitigating negative transfer in cross-domain recommendation.
  - Quick check question: How does contrastive learning differ from traditional supervised learning approaches in representation learning?

- Concept: Disentangled representation learning
  - Why needed here: Disentangled representation learning separates underlying explanatory factors within observed data, allowing the model to distinguish between domain-shared and domain-specific user preferences, which is critical for addressing negative transfer.
  - Quick check question: What are the key challenges in learning disentangled representations for recommendation systems?

## Architecture Onboarding

- Component map: Multi-channel graph encoder -> Affinity graph construction -> Multi-step random walks -> Intent-wise contrastive learning -> Cross-domain decoder -> User intent adaptation and prediction

- Critical path: Multi-channel graph encoder → Affinity graph construction → Multi-step random walks → Intent-wise contrastive learning → Cross-domain decoder → User intent adaptation and prediction

- Design tradeoffs:
  - Number of GNN layers vs. computational complexity
  - Number of latent intents (K) vs. model capacity and overfitting risk
  - Trade-off between self and user pair similarities (α) vs. capturing user preference structures
  - Number of random walk steps (d) vs. capturing high-order user similarities
  - Weights of contrastive loss (β) and orthogonality loss (γ) vs. preserving user similarity and disentanglement

- Failure signatures:
  - Negative transfer still occurs despite contrastive learning
  - Model performance degrades significantly on target domain
  - User similarity matrices (T_k) do not reflect actual user preference structures
  - Cross-domain decoder fails to effectively map source domain information to target domain

- First 3 experiments:
  1. Ablation study: Remove cross-domain decoder to assess its impact on preserving domain-specific information
  2. Parameter sensitivity: Vary the number of latent intents (K) to find the optimal value for different domains
  3. Negative transfer analysis: Compare model performance on overlapping users with and without contrastive learning to demonstrate its effectiveness in mitigating negative transfer

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does DisCo perform when dealing with scenarios involving more than two domains?
- Basis in paper: [explicit] The paper mentions that DisCo is designed for two-domain recommendation tasks, but it doesn't explore its performance in multi-domain settings.
- Why unresolved: The paper only tests DisCo on two-domain scenarios, leaving its performance in more complex, multi-domain situations unknown.
- What evidence would resolve it: Experiments on datasets with more than two domains to compare DisCo's performance with other multi-domain recommendation methods.

### Open Question 2
- Question: How does DisCo handle cases where the source and target domains have significantly different data distributions or user preferences?
- Basis in paper: [inferred] The paper highlights the negative transfer problem, where similar user preferences in one domain may not transfer to another. However, it doesn't specifically address cases with large distribution differences.
- Why unresolved: The paper focuses on mitigating negative transfer in general but doesn't explore how DisCo performs when the source and target domains have vastly different characteristics.
- What evidence would resolve it: Experiments on datasets with deliberately introduced domain differences to assess DisCo's robustness and adaptability.

### Open Question 3
- Question: How does DisCo perform in cold-start scenarios where users have no interactions in the source domain either?
- Basis in paper: [inferred] The paper focuses on cold-start users who have interactions in the source domain but not in the target domain. It doesn't address cases where users have no interactions in either domain.
- Why unresolved: The paper's experiments assume that users have some interactions in the source domain, leaving the performance of DisCo in the most challenging cold-start scenarios unexplored.
- What evidence would resolve it: Experiments on datasets where users have no interactions in either domain, comparing DisCo's performance with other methods designed for extreme cold-start scenarios.

## Limitations

- The paper's effectiveness is primarily validated on Amazon datasets with limited domain diversity
- The number of latent intents (K) is set to 10 for all experiments, but optimal values may vary across different domain pairs
- The paper does not provide detailed analysis of computational complexity, particularly for the multi-step random walks and affinity graph construction

## Confidence

- **High confidence**: The multi-channel graph encoder architecture and affinity graph construction are well-defined and technically sound
- **Medium confidence**: The disentangled contrastive learning framework is theoretically justified, but real-world effectiveness depends on dataset characteristics
- **Low confidence**: Claims about superiority over all baselines may not generalize to domains beyond e-commerce product recommendations

## Next Checks

1. **Cross-domain generalization**: Test DisCo on non-Amazon datasets (e.g., MovieLens + BookCrossing) to validate domain transferability
2. **Cold-start user diversity**: Analyze performance across different cold-start user groups based on activity levels in source domain
3. **Negative transfer analysis**: Conduct ablation studies specifically measuring negative transfer reduction compared to non-contrastive baselines