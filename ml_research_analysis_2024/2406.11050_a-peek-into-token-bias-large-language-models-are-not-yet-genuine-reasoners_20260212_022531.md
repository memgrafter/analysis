---
ver: rpa2
title: 'A Peek into Token Bias: Large Language Models Are Not Yet Genuine Reasoners'
arxiv_id: '2406.11050'
source_url: https://arxiv.org/abs/2406.11050
tags:
- 'true'
- 'false'
- arxiv
- reasoning
- llms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study introduces a hypothesis-testing framework to evaluate
  whether large language models (LLMs) possess genuine reasoning abilities or primarily
  rely on token bias. We systematically design synthetic datasets for conjunction
  fallacy and syllogistic fallacy problems, then perform controlled token perturbations
  while keeping underlying logic intact.
---

# A Peek into Token Bias: Large Language Models Are Not Yet Genuine Reasoners

## Quick Facts
- arXiv ID: 2406.11050
- Source URL: https://arxiv.org/abs/2406.11050
- Authors: Bowen Jiang; Yangxinyu Xie; Zhuoqun Hao; Xiaomeng Wang; Tanwi Mallick; Weijie J. Su; Camillo J. Taylor; Dan Roth
- Reference count: 40
- Primary result: Most LLMs exhibit significant token bias, performing poorly when superficial tokens are altered despite unchanged underlying logic

## Executive Summary
This study introduces a hypothesis-testing framework to evaluate whether large language models (LLMs) possess genuine reasoning abilities or primarily rely on token bias. The researchers systematically design synthetic datasets for conjunction fallacy and syllogistic fallacy problems, then perform controlled token perturbations while keeping underlying logic intact. Using McNemar's test with the Benjamini-Hochberg procedure for statistical validation, they test six hypotheses about token bias across various LLMs (GPT-3.5, GPT-4, Claude, Llama, Mistral). Results show that most LLMs exhibit significant token biasâ€”performance drops when altering superficially relevant tokens, celebrity names, quantifiers, or when removing explicit hints.

## Method Summary
The researchers created synthetic datasets for conjunction fallacy and syllogistic fallacy problems, then systematically perturbed superficial tokens while preserving underlying logic. They applied McNemar's test with Benjamini-Hochberg correction to validate six hypotheses about token bias across multiple LLMs. The framework examines how performance changes when altering superficial tokens, celebrity names, quantifiers, and removing explicit hints.

## Key Results
- Most LLMs show significant performance drops when superficial tokens are altered, despite unchanged underlying logic
- Celebrity names and explicit hints heavily influence LLM reasoning performance
- Token bias affects models differently based on their architecture and training data

## Why This Works (Mechanism)
Token bias occurs because LLMs learn statistical patterns from training data rather than genuine reasoning capabilities. When encountering specific token combinations, models rely on memorized associations rather than logical deduction. This manifests as performance degradation when superficial elements are changed while preserving the logical structure.

## Foundational Learning

**McNemar's Test**: Statistical test for paired binary data to determine if changes in one variable affect another. Why needed: To validate whether token perturbations significantly impact model performance. Quick check: Apply to any paired dataset to verify test assumptions.

**Benjamini-Hochberg Procedure**: Method for controlling false discovery rate in multiple hypothesis testing. Why needed: To handle multiple comparisons when testing six hypotheses simultaneously. Quick check: Verify p-value ranking and threshold calculation.

**Conjunction Fallacy**: Logical error where specific conditions are judged more probable than general ones. Why needed: Classic reasoning test to evaluate genuine logical capabilities. Quick check: Verify that specific conditions cannot be more probable than general ones.

## Architecture Onboarding

**Component Map**: Dataset Creation -> Token Perturbation -> Model Evaluation -> Statistical Validation -> Hypothesis Testing

**Critical Path**: The statistical validation phase is critical, as it determines whether observed performance changes are significant or due to random variation.

**Design Tradeoffs**: Synthetic datasets enable controlled experiments but may not capture real-world complexity. The framework prioritizes statistical rigor over ecological validity.

**Failure Signatures**: Performance drops when superficial tokens are altered indicate token bias rather than genuine reasoning. Consistent performance across perturbations suggests true reasoning capabilities.

**First Experiments**:
1. Test conjunction fallacy with celebrity names vs. generic names
2. Evaluate syllogistic reasoning with and without explicit quantifiers
3. Measure performance changes when removing superficial hints

## Open Questions the Paper Calls Out
None

## Limitations
- Synthetic datasets may not fully represent real-world reasoning complexity
- Limited set of fallacies and perturbations may not capture all token bias forms
- Statistical validation relies on specific assumptions that may not always hold
- Does not explore fine-tuning or domain adaptation effects on token bias

## Confidence

| Claim | Confidence |
|-------|------------|
| Token bias significantly affects LLM performance | High |
| Findings generalize to real-world reasoning tasks | Medium |
| Token bias can be mitigated through fine-tuning | Low |

## Next Checks

1. Expand dataset complexity to include more diverse and complex reasoning tasks
2. Investigate impact of fine-tuning and domain adaptation on token bias
3. Test hypotheses across broader range of LLMs including open-source models