---
ver: rpa2
title: Object and Contact Point Tracking in Demonstrations Using 3D Gaussian Splatting
arxiv_id: '2411.03555'
source_url: https://arxiv.org/abs/2411.03555
tags:
- object
- pose
- tracking
- objects
- contact
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a method to enhance Interactive Imitation Learning
  by extracting touch interaction points and tracking object movement from video demonstrations.
  The approach combines 3D Gaussian Splatting for scene reconstruction with FoundationPose
  for 6-DoF tracking, enabling robots to better understand and manipulate objects
  in dynamic environments.
---

# Object and Contact Point Tracking in Demonstrations Using 3D Gaussian Splatting

## Quick Facts
- **arXiv ID**: 2411.03555
- **Source URL**: https://arxiv.org/abs/2411.03555
- **Reference count**: 33
- **Primary result**: 17% high-accuracy and 42% low-accuracy success rates for 6-DoF pose tracking on 12 household objects

## Executive Summary
This paper presents a method to enhance Interactive Imitation Learning (IIL) by extracting touch interaction points and tracking object movement from video demonstrations. The approach combines 3D Gaussian Splatting for scene reconstruction with FoundationPose for 6-DoF tracking, enabling robots to better understand and manipulate objects in dynamic environments. The pipeline processes RGB-D videos of both the scene and demonstrations, using optical flow, segmentation, and depth-based contact point estimation. Experiments on 12 objects show success rates of 17% for high-accuracy pose tracking and 42% for low-accuracy tracking. The method is most effective on large, textured objects but faces challenges with reflective surfaces, occlusions, and articulated objects. While computational constraints limit real-time application, the approach lays groundwork for improved robotic manipulation through video-based learning.

## Method Summary
The method processes paired RGB-D videos - one capturing a dynamic scene and another showing a human demonstration of object manipulation. It uses 3D Gaussian Splatting to reconstruct the scene, RAFT optical flow and SAM 2 for object segmentation in demonstrations, GS2Mesh and SAGS for mesh generation, FoundationPose for 6-DoF tracking, and depth differencing with hand masks for contact point estimation. The pipeline enables robots to learn from demonstrations by identifying where and how humans interact with objects, though current implementation requires several minutes of processing per video, limiting real-time application.

## Key Results
- 17% success rate for high-accuracy 6-DoF pose tracking (both position and rotation accurate)
- 42% success rate for low-accuracy pose tracking (position only)
- Method most effective on large, textured objects but struggles with reflective surfaces, occlusions, and articulated objects
- Processing takes several minutes per video, limiting real-time application

## Why This Works (Mechanism)

### Mechanism 1
- Claim: 3D Gaussian Splatting reconstruction provides a dense, accurate 3D scene representation that enables robust object tracking.
- Mechanism: Gaussian Splatting models the scene as a collection of anisotropic Gaussians, each with position, orientation, and scale. This representation captures both geometry and appearance, allowing for accurate pose estimation of objects in dynamic scenes.
- Core assumption: The Gaussian Splatting reconstruction accurately captures the object geometry and appearance needed for reliable pose tracking.
- Evidence anchors:
  - [abstract] "By leveraging cutting-edge techniques such as 3D Gaussian Splatting and FoundationPose for tracking, this method allows robots to better understand and manipulate objects in dynamic environments."
  - [section 3] "These estimations are then combined with the original 3D Gaussian Splatting training to reconstruct a Gaussian Splatting model of the environment, including the object of interest"
  - [corpus] Weak - corpus neighbors focus on tracking and reconstruction but don't directly support the specific combination with contact point estimation
- Break condition: If the object has textureless or reflective surfaces, the Gaussian Splatting reconstruction quality degrades, leading to inaccurate pose tracking.

### Mechanism 2
- Claim: Optical flow combined with clustering identifies the manipulated object in demonstration videos.
- Mechanism: RAFT optical flow detects motion between frames, which is used to generate bounding boxes around moving objects. Clustering these bounding boxes over time identifies the most consistent object being manipulated.
- Core assumption: The manipulated object has sufficient motion distinct from background or human movement to be detected by optical flow.
- Evidence anchors:
  - [section 3] "We first find a bounding box of the manipulated object using RAFT optical flow [2], as shown in Figure A.2 in the Appendix. We accumulate potential bounding boxes over the video and use a clustering mechanism to find the most probable bounding box."
  - [section 4] "While 3D Gaussian Splatting combined with Spectacular AI's pose estimation is generally robust, light reflections on surfaces, such as on the jug and the mug, can introduce artifacts in the texture of the mesh, which in turn affect the accuracy of the 6DoF pose estimation."
  - [corpus] Weak - corpus neighbors focus on object tracking but don't address the specific combination with optical flow and clustering
- Break condition: If the object has minimal movement or if human hands obscure the object throughout the manipulation, the optical flow-based detection fails.

### Mechanism 3
- Claim: Depth image differencing combined with hand masks identifies contact points on objects.
- Mechanism: By comparing depth images between the scene and demonstration, and applying hand masks, the algorithm identifies where the hand touches the object surface.
- Core assumption: The hand mask accurately identifies the human hand in the demonstration video, and depth differences correspond to actual contact points.
- Evidence anchors:
  - [section 3] "By utilizing depth images and hand mask data from the demonstration video, along with simple distance thresholding, contact points on the object can be identified, as seen in Figure 2."
  - [section 4] "We accumulate these points for up to 10 frames of contact and select the most occurring points as the final contact points"
  - [corpus] Weak - corpus neighbors focus on object tracking but don't address contact point detection specifically
- Break condition: If the hand occludes large portions of the object or if there are depth estimation errors, the contact point detection becomes inaccurate.

## Foundational Learning

- Concept: 3D Gaussian Splatting fundamentals
  - Why needed here: The entire pipeline relies on accurate 3D scene reconstruction using Gaussian Splatting as the foundation for object tracking and contact point estimation.
  - Quick check question: How does 3D Gaussian Splatting represent 3D scenes differently from point clouds or meshes?

- Concept: Optical flow and motion detection
  - Why needed here: RAFT optical flow is used to detect the manipulated object in demonstration videos by identifying regions of motion.
  - Quick check question: What are the limitations of optical flow in detecting object motion when there's significant camera movement or textureless surfaces?

- Concept: Depth image processing and differencing
  - Why needed here: Contact point estimation relies on comparing depth images between the scene and demonstration to identify where the hand touches the object.
  - Quick check question: How does depth image differencing handle occlusions and sensor noise when identifying contact points?

## Architecture Onboarding

- Component map: RGB-D Scene Video -> 3D Gaussian Splatting Reconstruction -> Scene 3D Model; RGB-D Demo Video -> RAFT Optical Flow -> SAM 2 Segmentation -> Object Mask -> COLMAP Alignment -> Demo 3D Model -> GS2Mesh Mesh Generation -> FoundationPose Tracking -> 6-DoF Pose Estimation -> Depth Differencing with Hand Masks -> Contact Point Estimation

- Critical path: The most critical path is from object masking through pose tracking to contact point estimation. Errors in object masking propagate through pose tracking and directly affect contact point accuracy.

- Design tradeoffs: The system trades computational efficiency for accuracy by using per-frame pose estimation (11x slower) versus standard tracking. It also trades robustness for simplicity by using basic depth differencing for contact points rather than more sophisticated methods.

- Failure signatures: Common failure modes include: (1) tracking loss for small or textureless objects, (2) incorrect object masks due to SAM 2 failures, (3) mesh artifacts from GS2Mesh on reflective surfaces, and (4) contact point misalignment due to hand occlusion.

- First 3 experiments:
  1. Test the complete pipeline on a simple object (e.g., a textured box) to verify basic functionality and identify bottlenecks.
  2. Test object masking accuracy by comparing RAFT+SAM 2 results with manual annotations on various object types.
  3. Test contact point estimation by manually verifying predicted contact points against ground truth for different hand-object interaction scenarios.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How would the system perform if trained on more diverse objects, including smaller, textureless, and deformable objects, or if objects were partially occluded during demonstrations?
- Basis in paper: [explicit] The paper discusses challenges with reflective surfaces, textureless objects, occlusions, and articulated objects, noting that the approach is most effective on large, textured objects but faces difficulties with reflective surfaces, occlusions, and articulated objects.
- Why unresolved: The current experimental results are based on 12 objects with specific characteristics. The paper acknowledges limitations with certain object types but does not provide data on how the system would handle a broader range of objects or more severe occlusion scenarios.
- What evidence would resolve it: Testing the system on a larger dataset with diverse objects, including smaller, textureless, deformable objects, and scenes with varying degrees of occlusion, would provide concrete evidence of its generalizability and robustness.

### Open Question 2
- Question: What are the computational requirements and performance implications for real-time application on mobile robots with limited processing power?
- Basis in paper: [explicit] The paper mentions that the processing takes several minutes per video, making real-time application challenging, and notes that improvements in algorithmic efficiency or access to more powerful hardware would be necessary for real-time application on mobile robots.
- Why unresolved: While the paper acknowledges the computational constraints and processing time, it does not provide detailed analysis of how the system would perform on hardware with different specifications or what optimizations would be needed for real-time operation.
- What evidence would resolve it: Profiling the system on various hardware configurations, including mobile robot processors, and identifying bottlenecks or optimization opportunities would clarify the feasibility of real-time application.

### Open Question 3
- Question: How would the system handle dynamic environments where objects are frequently moved or rearranged between the scene capture and demonstration phases?
- Basis in paper: [explicit] The paper discusses challenges related to changes in lighting, reflective surfaces, and cluttered scenes, and mentions that variations in the placement of background objects between the real-world scene and the 3DGS environment do not affect tracking performance as long as an accurate demonstration camera pose is maintained.
- Why unresolved: The paper does not explore scenarios where the primary objects of interest are moved or rearranged, which is a common occurrence in real-world environments. It's unclear how such changes would impact the system's ability to track and manipulate objects accurately.
- What evidence would resolve it: Conducting experiments where the scene is modified between capture and demonstration phases, and measuring the system's accuracy in tracking and manipulating objects under these conditions, would provide insights into its robustness in dynamic environments.

## Limitations
- Computational requirements limit real-time application - processing takes several minutes per video
- Limited success rates (17% high-accuracy, 42% low-accuracy) on current evaluation set
- Performance degrades significantly on reflective surfaces, textureless objects, and articulated objects

## Confidence

- **High Confidence**: The fundamental approach of combining 3D Gaussian Splatting with FoundationPose for object tracking is technically sound and the basic pipeline architecture is valid.
- **Medium Confidence**: The success rate claims (17% high-accuracy, 42% low-accuracy) are plausible given the stated limitations, but the evaluation methodology lacks detail on ground truth generation and success criteria.
- **Low Confidence**: The contact point estimation accuracy and its direct applicability to robotic manipulation learning is uncertain due to limited quantitative validation.

## Next Checks

1. **Ground Truth Validation**: Conduct controlled experiments with known object poses and contact points to establish quantitative accuracy metrics for both pose tracking and contact point estimation across diverse object categories.

2. **Computational Profiling**: Perform detailed analysis of computational bottlenecks in the pipeline to identify optimization opportunities for real-time implementation on standard robotic hardware.

3. **Generalization Testing**: Test the method on a broader range of objects including small items, transparent materials, and articulated objects to assess performance boundaries and identify failure patterns.