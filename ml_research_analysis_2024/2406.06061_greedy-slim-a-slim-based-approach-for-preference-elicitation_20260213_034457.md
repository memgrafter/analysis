---
ver: rpa2
title: 'Greedy SLIM: A SLIM-Based Approach For Preference Elicitation'
arxiv_id: '2406.06061'
source_url: https://arxiv.org/abs/2406.06061
tags:
- items
- slim
- user
- which
- qgslim
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the cold-start problem in recommender systems
  by proposing a new preference elicitation method based on SLIM. The core idea is
  Greedy SLIM, a novel training technique that iteratively selects items to minimize
  the SLIM loss, thereby ordering items by importance for the questionnaire.
---

# Greedy SLIM: A SLIM-Based Approach For Preference Elicitation

## Quick Facts
- arXiv ID: 2406.06061
- Source URL: https://arxiv.org/abs/2406.06061
- Authors: Claudius Proissl; Amel Vatic; Helmut Waldschmidt
- Reference count: 10
- This paper proposes Greedy SLIM, a SLIM-based method for preference elicitation that outperforms QBandit in both offline experiments and user studies.

## Executive Summary
This paper addresses the cold-start problem in recommender systems by proposing a new preference elicitation method based on SLIM. The core idea is Greedy SLIM, a novel training technique that iteratively selects items to minimize the SLIM loss, thereby ordering items by importance for the questionnaire. The authors compare their approach with QBandit, a popular latent factor model-based method, through offline experiments and a user study. Results show that Greedy SLIM outperforms QBandit, especially with longer questionnaires, and performs remarkably well in the user study, receiving higher positive feedback and engagement. The paper concludes that SLIM-based questionnaires are a promising alternative to existing methods, despite the computational cost of the training phase.

## Method Summary
The Greedy SLIM method constructs a SLIM matrix row by row, iteratively selecting items that minimize the SLIM loss function. This greedy training approach orders items by importance for the questionnaire, avoiding the recommendation degradation seen in traditional SLIM-based methods. The resulting static questionnaire allows for computationally expensive preprocessing while maintaining online efficiency during user interaction. The method was evaluated using MovieLens-25M and Netflix datasets, comparing against QBandit through NDCG, Precision, and Recall metrics.

## Key Results
- Greedy SLIM outperforms QBandit in NDCG@10, with a 16% improvement for 20 questions on MovieLens-25M
- Greedy SLIM's questionnaire performance does not degrade with longer questionnaires, unlike QBandit
- User study results show Greedy SLIM received higher positive feedback and engagement than QBandit

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Greedy SLIM improves questionnaire effectiveness by selecting items that minimize the SLIM loss iteratively, thus maximizing information gain per question.
- Mechanism: The algorithm starts with an empty SLIM matrix and iteratively fills rows, each time choosing the item that minimizes the SLIM loss function. This directly orders items by importance for the questionnaire.
- Core assumption: Minimizing the SLIM loss during training correlates with selecting items that best reveal user preferences.
- Evidence anchors:
  - [abstract] "Our approach mainly consists of a new training technique for SLIM, which we call Greedy SLIM. This technique iteratively selects items for the training in order to minimize the SLIM loss greedily."
  - [section 5] "Our method constructs the SLIM matrix W row by row, each time selecting the item i âˆˆ I that minimizes the SLIM loss."
- Break condition: If the assumption that minimizing SLIM loss equals maximizing information gain is false, the greedy ordering would not be optimal for questionnaires.

### Mechanism 2
- Claim: Greedy SLIM avoids the recommendation degradation seen in traditional SLIM-based questionnaires by not restricting recommendations to items outside the questionnaire.
- Mechanism: Traditional SLIM questionnaires suffer when popular items are asked about because recommendations are restricted to non-question items. Greedy SLIM's ordering ensures questions are meaningful without this restriction.
- Core assumption: The restriction that recommendations cannot include questionnaire items harms performance, and Greedy SLIM avoids this by its item ordering.
- Evidence anchors:
  - [section 4] "The NDCG of QBandit converges or even decreases after ten questions... QGSLIM does not suffer from this restriction."
  - [section 6.5] "QGSLIM does not suffer from this restriction. On the contrary, even after 20 questions there is no sign of convergence or overfitting."
- Break condition: If the performance drop in traditional methods is due to other factors (e.g., overfitting) rather than the restriction, Greedy SLIM's advantage may be overstated.

### Mechanism 3
- Claim: Greedy SLIM's static questionnaire design allows for computationally expensive preprocessing while maintaining online efficiency during user interaction.
- Mechanism: The SLIM matrix is computed offline using the greedy training method, so the questionnaire can be predetermined. During user interaction, only fast matrix operations are needed to compute recommendations.
- Core assumption: The computational cost of preprocessing can be amortized over many users, making the expensive training phase acceptable.
- Evidence anchors:
  - [section 6.3] "As our questionnaire is static, we can compute the SLIM matrix in a preprocessing step and do not have to worry too much about the run time."
  - [section 6.3] "For our purpose it suffices to compute the number of rows that equals the size of the questionnaire, which is 20 in our experiments."
- Break condition: If the preprocessing cost is prohibitive even with amortization (e.g., frequent updates needed), the static approach becomes impractical.

## Foundational Learning

- Concept: SLIM (Sparse Linear Methods) and its loss function formulation
  - Why needed here: Understanding how SLIM works and its loss function is crucial to grasp why Greedy SLIM's iterative training approach is effective.
  - Quick check question: What is the purpose of the non-negativity constraint in SLIM, and how does it affect the learned item-item relationships?

- Concept: Preference elicitation and the cold-start problem in recommender systems
  - Why needed here: The entire paper addresses how to select items for new users to rate in order to generate accurate recommendations.
  - Quick check question: How does preference elicitation differ from traditional collaborative filtering, and why is it particularly useful for cold-start scenarios?

- Concept: Evaluation metrics for top-N recommendation (NDCG, Precision, Recall)
  - Why needed here: The paper uses these metrics to compare Greedy SLIM with baseline methods, so understanding them is essential for interpreting results.
  - Quick check question: Why might NDCG be preferred over RMSE when evaluating top-N recommendation performance?

## Architecture Onboarding

- Component map:
  - Data preprocessing -> Greedy SLIM training -> Questionnaire generation -> Recommendation computation -> Evaluation

- Critical path:
  1. Load and preprocess data (user-item interactions)
  2. Train Greedy SLIM model (preprocessing phase)
  3. Generate static questionnaire from trained model
  4. For each test user: collect ratings, compute recommendations, evaluate performance

- Design tradeoffs:
  - Static vs. dynamic questionnaires: Static allows expensive preprocessing but less flexibility; dynamic requires online computation but can adapt to user responses
  - Computational cost: Greedy SLIM training is expensive but yields better performance; simpler methods are faster but less effective
  - Questionnaire length: Longer questionnaires improve accuracy but may reduce user engagement

- Failure signatures:
  - Poor NDCG scores despite training: Could indicate issues with data quality, parameter settings, or that the greedy approach isn't suitable for the dataset
  - Extremely long training times: May suggest the need for optimization or that the dataset is too large for this approach
  - User study results don't match offline experiments: Could indicate differences between simulated and real user behavior

- First 3 experiments:
  1. Verify SLIM training with standard coordinate descent to establish baseline performance
  2. Implement and test Greedy SLIM on a small dataset to ensure the greedy algorithm works correctly
  3. Compare Greedy SLIM questionnaire performance against a simple popularity-based baseline on the full dataset

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the Greedy SLIM approach be successfully adapted to dynamic questionnaires that adjust questions based on user responses?
- Basis in paper: [explicit] The authors state "We believe that our ideas can be easily extended to the dynamic setting" but do not provide evidence or implementation details.
- Why unresolved: The paper only evaluates static questionnaires where the same questions are asked to all users, leaving the dynamic extension theoretical.
- What evidence would resolve it: A user study or offline experiment comparing dynamic Greedy SLIM questionnaires against static versions and baseline dynamic methods.

### Open Question 2
- Question: Does the Greedy SLIM approach generalize to other types of questions beyond absolute ratings, such as pairwise comparisons?
- Basis in paper: [explicit] The authors mention "There are many possible extensions to QGSLIM...it would be worthwhile investigating if the presented ideas can be applied to other types of questions such as pairwise questions."
- Why unresolved: The paper only considers absolute ratings and does not test or analyze the effectiveness of Greedy SLIM for other question types.
- What evidence would resolve it: Implementation and evaluation of Greedy SLIM using pairwise comparison questions in both offline experiments and user studies.

### Open Question 3
- Question: Can the Greedy SLIM training algorithm be optimized to reduce its computational cost while maintaining recommendation accuracy?
- Basis in paper: [explicit] The authors acknowledge that "Improving these times is a promising open problem" and note that the training phase is computationally expensive.
- Why unresolved: The paper focuses on the effectiveness of Greedy SLIM rather than computational efficiency, and the training times are described as prohibitive for large-scale applications.
- What evidence would resolve it: An optimized version of Greedy SLIM with significantly reduced training time (e.g., by parallelizing computations or using sampling techniques) that achieves comparable or better accuracy than the original method.

## Limitations
- The computational expense of the training phase remains a significant limitation, though the authors argue this is acceptable for static questionnaires
- The user study involved only 20 participants, limiting generalizability of engagement findings
- Exact parameter settings for regularization were not fully specified, requiring assumptions in reproduction attempts

## Confidence
- Greedy SLIM's superior NDCG performance: **High** (strong experimental evidence across multiple metrics)
- Computational cost justification: **Medium** (theoretical argument but no real-world scaling analysis)
- User study conclusions: **Medium** (positive results but small sample size and potential bias)

## Next Checks
1. Reproduce the Greedy SLIM training process on a subset of MovieLens data to verify computational requirements and parameter sensitivity
2. Implement a dynamic questionnaire baseline to test whether the static approach is truly optimal for all cold-start scenarios
3. Conduct a larger-scale user study with diverse participant demographics to validate the engagement findings