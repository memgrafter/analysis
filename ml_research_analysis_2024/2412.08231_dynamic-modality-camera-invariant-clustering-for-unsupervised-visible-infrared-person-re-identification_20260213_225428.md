---
ver: rpa2
title: Dynamic Modality-Camera Invariant Clustering for Unsupervised Visible-Infrared
  Person Re-identification
arxiv_id: '2412.08231'
source_url: https://arxiv.org/abs/2412.08231
tags:
- clustering
- person
- learning
- training
- unsupervised
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of unsupervised visible-infrared
  person re-identification (USL-VI-ReID), where the goal is to match individuals across
  visible and infrared images without relying on manual annotations. The key problem
  is that existing clustering-based methods suffer from identity splitting due to
  cross-modality and cross-camera discrepancies, which degrades the quality of pseudo
  labels and limits model performance.
---

# Dynamic Modality-Camera Invariant Clustering for Unsupervised Visible-Infrared Person Re-identification

## Quick Facts
- arXiv ID: 2412.08231
- Source URL: https://arxiv.org/abs/2412.08231
- Authors: Yiming Yang; Weipeng Hu; Haifeng Hu
- Reference count: 40
- Key outcome: Achieves state-of-the-art performance on SYSU-MM01 (60.26% Rank-1, 57.82% mAP) and RegDB (86.31% Rank-1, 81.36% mAP) datasets for unsupervised visible-infrared person re-identification

## Executive Summary
This paper addresses the challenge of unsupervised visible-infrared person re-identification (USL-VI-ReID) by proposing a Dynamic Modality-Camera Invariant Clustering (DMIC) framework. The key innovation lies in addressing cross-modality and cross-camera discrepancies that cause identity splitting during clustering. DMIC integrates three components: Modality-Camera Invariant Expansion (MIE) to create invariant embeddings, Dynamic Neighborhood Clustering (DNC) with adaptive hyperparameter scheduling, and Hybrid Modality Contrastive Learning (HMCL) with real-time cluster memory updates. The method significantly outperforms existing unsupervised approaches on standard benchmarks.

## Method Summary
DMIC tackles USL-VI-ReID through a three-stage framework. First, MIE fuses inter-modal and inter-camera distance coding to create modality-camera invariant embeddings for clustering. Second, DNC employs dynamic search strategies that adjust clustering hyperparameters (eps and k2) during training - narrowing them early to improve discriminability, then broadening them later to enhance generalization. Third, HMCL performs both instance-level and cluster-level contrastive learning, updating cluster memories using randomly selected cross-modal samples to learn robust modality-invariant representations. The method is trained in two phases: intra-modality training followed by inter-modality training.

## Key Results
- On SYSU-MM01: Achieves 60.26% Rank-1 and 57.82% mAP, significantly outperforming PGMAL (57.27% Rank-1) and GUR (63.51% Rank-1)
- On RegDB: Sets new state-of-the-art with 86.31% Rank-1 and 81.36% mAP
- Demonstrates effective reduction of performance gap between unsupervised and supervised methods
- Shows robust performance across different evaluation metrics including Rank-10, Rank-20, mAP, and mINP

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Cross-modality and cross-camera discrepancies cause identity splitting during clustering, leading to noisy pseudo-labels.
- Mechanism: MIE fuses inter-modal and inter-camera distance coding to create modality-camera invariant embeddings that prevent excessive splitting of identities in clustering.
- Core assumption: Distance encoding fusion can effectively bridge cross-modality and cross-camera gaps.
- Evidence anchors: [abstract] "they ignore cross-camera differences, leading to noticeable issues with excessive splitting of identities"; [section II.C] "we introduce a simple and effective modal-camera invariant expansion (MIE) to improve the clustering algorithm..."
- Break condition: If distance coding fusion doesn't effectively bridge the gaps, identity splitting will persist and degrade performance.

### Mechanism 2
- Claim: Dynamic adjustment of clustering hyperparameters improves model discriminability and cross-modal/cross-camera generalizability.
- Mechanism: DNC employs two dynamic search strategies that adjust eps and k2 during training - narrowing them early to improve discriminability, then broadening them later to enhance generalization.
- Core assumption: The model's discriminative capability evolves during training in a predictable way that can be leveraged by dynamic hyperparameter adjustment.
- Evidence anchors: [abstract] "DNC employs two dynamic search strategies to refine the network's optimization objective, transitioning from improving discriminability to enhancing cross-modal and cross-camera generalizability"; [section III.C] "we employ distinct dynamic strategies for these two stages of training..."
- Break condition: If the model's learning trajectory doesn't follow the assumed pattern, dynamic adjustment may introduce instability or suboptimal convergence.

### Mechanism 3
- Claim: Hybrid contrastive learning at both instance and cluster levels, with real-time cluster memory updates, enables robust modality-invariant representation learning.
- Mechanism: HMCL performs intra-modality and inter-modality contrastive learning, updating cluster representations using randomly selected cross-modal samples for real-time exploration of modality-invariant representations.
- Core assumption: Real-time cluster memory updates using randomly selected samples can effectively capture and maintain modality-invariant representations.
- Evidence anchors: [abstract] "Memories for intra-modality and inter-modality training are updated using randomly selected samples, facilitating real-time exploration of modality-invariant representations"; [section III.D] "we introduce cluster-level and instance-level losses to refine global and partial distributions...update the memory by randomly selected instances..."
- Break condition: If random sample selection fails to capture representative cluster characteristics, the learned representations may become unstable or biased.

## Foundational Learning

- Concept: Distance encoding and Jaccard distance for clustering
  - Why needed here: Forms the basis for MIE module to create modality-camera invariant embeddings
  - Quick check question: How does distance encoding help capture neighborhood relationships in clustering?

- Concept: Dynamic scheduling of hyperparameters
  - Why needed here: Core mechanism of DNC to improve model performance through adaptive clustering
  - Quick check question: Why would you want to adjust eps and k2 differently during different training stages?

- Concept: Contrastive learning with memory banks
  - Why needed here: Foundation for HMCL module to learn modality-invariant representations
  - Quick check question: What's the advantage of updating cluster memories with randomly selected samples versus momentum updates?

## Architecture Onboarding

- Component map:
  Backbone (AGW) → MIE (distance coding fusion) → DNC (dynamic clustering) → HMCL (contrastive learning) → Pseudo labels
  Intra-modality training path: Visible/IR separately → Inter-modality training path: Visible+IR together

- Critical path: MIE → DNC → HMCL
  - MIE must run first to create invariant embeddings
  - DNC depends on MIE outputs for clustering
  - HMCL depends on DNC outputs for pseudo labels

- Design tradeoffs:
  - Static vs dynamic clustering hyperparameters (DNC adds complexity but improves performance)
  - Momentum vs random memory updates (random adds exploration but may be noisier)
  - Single vs dual-stream architecture (dual-stream enables modality-specific processing)

- Failure signatures:
  - Poor Rank-1/mAP performance indicates MIE not creating effective invariant embeddings
  - High variance in cluster numbers across epochs suggests DNC scheduling issues
  - Degraded performance when switching between intra/inter-modality training indicates HMCL implementation problems

- First 3 experiments:
  1. Verify MIE creates modality-camera invariant embeddings by visualizing t-SNE plots of clustering inputs vs outputs
  2. Test DNC dynamic scheduling by plotting eps/k2 values over epochs and measuring their impact on cluster stability
  3. Validate HMCL contrastive learning by measuring instance-to-cluster similarity distributions before/after training

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of DMIC scale with dataset size and diversity, particularly for datasets with a higher number of cameras and more complex backgrounds?
- Basis in paper: [inferred] The paper evaluates DMIC on SYSU-MM01 and RegDB datasets, but does not explore its performance on datasets with varying sizes, camera numbers, or background complexities.
- Why unresolved: The paper focuses on demonstrating the effectiveness of DMIC on two specific datasets without exploring its scalability to different dataset characteristics.
- What evidence would resolve it: Conducting experiments on datasets with varying numbers of cameras, different background complexities, and larger scales would provide insights into DMIC's scalability and robustness.

### Open Question 2
- Question: What is the impact of the choice of backbone architecture on the performance of DMIC, and how does it compare to other state-of-the-art backbones for VI-ReID?
- Basis in paper: [explicit] The paper uses AGW [3] as the backbone network for DMIC, but does not explore the impact of using different backbone architectures on its performance.
- Why unresolved: The choice of backbone architecture can significantly influence the performance of VI-ReID models. The paper does not investigate how DMIC's performance is affected by different backbone choices.
- What evidence would resolve it: Evaluating DMIC with different backbone architectures, such as ResNet-101, SENet, or other specialized VI-ReID backbones, and comparing its performance with the current results would reveal the impact of backbone choice on DMIC's effectiveness.

### Open Question 3
- Question: How does DMIC handle the challenge of domain shift when applied to datasets from different domains or with different acquisition conditions?
- Basis in paper: [inferred] The paper focuses on unsupervised learning for VI-ReID and does not explicitly address the challenge of domain shift.
- Why unresolved: Domain shift is a common challenge in VI-ReID, where models trained on one dataset may not generalize well to datasets from different domains or with varying acquisition conditions.
- What evidence would resolve it: Conducting experiments on datasets from different domains or with varying acquisition conditions would reveal how well DMIC handles domain shift. Additionally, exploring techniques to improve DMIC's domain adaptation capabilities would provide insights into its robustness to domain variations.

## Limitations
- Relies on dynamic hyperparameter scheduling that may not generalize across different datasets
- Random memory update strategy in HMCL could introduce instability in cluster representation learning
- Performance depends heavily on quality of initial pseudo labels, which may degrade with significant appearance variations

## Confidence
- **High Confidence**: The core architecture combining MIE, DNC, and HMCL is well-grounded in clustering and contrastive learning literature
- **Medium Confidence**: The specific dynamic scheduling strategies for eps and k2 are theoretically sound but may require dataset-specific tuning
- **Medium Confidence**: The hybrid contrastive learning approach is promising but the random memory update mechanism needs further validation

## Next Checks
1. **Ablation Study on Dynamic Scheduling**: Systematically test static vs. dynamic eps and k2 values to quantify the exact contribution of DNC to overall performance
2. **Memory Update Strategy Comparison**: Compare random vs. momentum-based memory updates in HMCL to assess stability vs. exploration tradeoff
3. **Cross-Dataset Generalization**: Evaluate DMIC on additional visible-infrared datasets to test robustness across different camera setups and environmental conditions