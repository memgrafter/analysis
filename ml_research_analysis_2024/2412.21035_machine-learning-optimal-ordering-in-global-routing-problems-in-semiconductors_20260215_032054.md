---
ver: rpa2
title: Machine Learning Optimal Ordering in Global Routing Problems in Semiconductors
arxiv_id: '2412.21035'
source_url: https://arxiv.org/abs/2412.21035
tags:
- routing
- latexit
- layer
- sha1
- base64
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a machine learning approach to optimize net
  ordering in global routing for semiconductor packages, improving upon heuristic-based
  methods. The proposed method trains deep neural networks on feature vectors extracted
  from 2D routing solutions to predict optimal net orderings, which significantly
  reduce routing congestion and improve layer assignment.
---

# Machine Learning Optimal Ordering in Global Routing Problems in Semiconductors

## Quick Facts
- arXiv ID: 2412.21035
- Source URL: https://arxiv.org/abs/2412.21035
- Authors: Heejin Choi; Minji Lee; Chang Hyeong Lee; Jaeho Yang; Rak-Kyeong Seong
- Reference count: 38
- Primary result: ML approach predicts optimal net orderings from 1-layer routing features, achieving 10x accuracy improvements over heuristics

## Executive Summary
This paper introduces a machine learning approach to optimize net ordering in global routing for multilayered semiconductor packages. The method trains deep neural networks on feature vectors extracted from 2D routing solutions to predict optimal net orderings, which significantly reduce routing congestion and improve layer assignment. Experiments demonstrate superior performance over traditional heuristics, with accuracy improvements of 10x for complex scenarios. The method is transferable to larger routing environments without significant performance loss, offering potential for further optimization in EDA workflows.

## Method Summary
The proposed method generates 1-layer routing solutions using Kruskal's algorithm or Steiner Tree algorithm on 5x5 grid graphs with 15 pins per layer. Feature vectors are extracted from these solutions including pin counts, vertex counts, overflow, branch vertices, and bounding rectangle dimensions. Three deep learning models (MLP-based) are trained to predict optimal net ordering based on these features, using MSE or cross-entropy loss. The models are evaluated on their ability to minimize total overflow, maximum overflow, and optimality score (wirelength × (1 + runtime)).

## Key Results
- Deep learning models achieve 10x accuracy improvements over heuristics for complex routing scenarios
- Reduced feature sets maintain competitive accuracy while decreasing training time
- Models demonstrate high transferability to routing environments with increasing numbers of layers without significant performance degradation

## Why This Works (Mechanism)

### Mechanism 1
The model outperforms heuristics because it learns an optimality ranking across many routing problems, not just static features. The network is trained to predict the rank of a net ordering based on the actual 3D routing result (overflow, wirelength, runtime) after layer assignment, giving feedback that heuristics lack.

### Mechanism 2
Using a reduced feature set (pins, vertices, overflow) still captures the most predictive signals for net ordering. Feature selection based on correlation analysis removes noise and focuses learning on features that directly influence 3D routing congestion.

### Mechanism 3
Transferability works because the model learns general patterns of congestion that hold across different layer counts. Training on datasets with k=2 and k=5 layers allows the model to capture rules of thumb that generalize to k=10.

## Foundational Learning

- Grid graph compression and mapping between 3D and 2D representations
  - Why needed here: The net ordering problem is defined on the 1-layer compressed graph; misunderstanding the mapping corrupts feature extraction
  - Quick check question: If a pin is at layer 3 in the 3D problem, where does it appear in the compressed 1-layer graph?

- Minimum spanning tree (MST) construction and edge orientation
  - Why needed here: Layer assignment relies on MST orientation to define parent/children relationships and compute via costs
  - Quick check question: In an MST with 4 vertices, how many edges are oriented away from the source pin?

- Overflow calculation and constraint satisfaction
  - Why needed here: The optimality metric is dominated by overflow; miscomputing overflow invalidates the ranking
  - Quick check question: If an edge has capacity 3 and demand 5, what is its overflow?

## Architecture Onboarding

- Component map: Dataset generator → Feature extractor → Deep learning model (MLP) → Optimality rank predictor → Layer assignment → 3D routing evaluator
- Critical path: Dataset generation → Feature vector construction → Model training → Accuracy evaluation → Transferability testing
- Design tradeoffs: Full feature vector improves accuracy but increases training time; reduced vector speeds training but may lose edge cases; MSE loss vs cross-entropy affects convergence stability
- Failure signatures: Training loss plateaus early (underfitting), high variance across folds (overfitting), or accuracy below 20% (model not learning)
- First 3 experiments:
  1. Train Model 1 on Data 1, evaluate on training set, plot loss curve for 50 epochs
  2. Train Model 3 on Data 5 with reduced features, compare accuracy vs full feature version
  3. Transfer Model 2 from k=2 to k=10, measure accuracy drop per layer increase

## Open Questions the Paper Calls Out

### Open Question 1
How would the proposed deep learning approach perform when applied to routing environments with more than 10 layers or more than 5 nets per layer? The authors acknowledge this limitation and express interest in exploring more complex scenarios in future work.

### Open Question 2
How would different deep learning architectures, such as convolutional neural networks (CNN) or graph neural networks (GNN), perform compared to the multilayer perceptron (MLP) used in this study? The authors expect that further optimization can be achieved with other architectures.

### Open Question 3
How can the proposed deep learning approach be extended to optimize other steps in the global routing process, such as the 2D routing step or the routing environment discretization step? The authors emphasize that their work does not optimize other steps in global routing.

## Limitations
- Absence of direct experimental evidence for the correlation between 1-layer features and 3D routing outcomes
- Claim of "10x accuracy improvements" lacks baseline comparison details and statistical significance reporting
- Transferability across layer counts is demonstrated empirically but without theoretical justification

## Confidence

High confidence in basic ML framework implementation and feature extraction methodology
Medium confidence in optimality ranking system and its stability across different routing scenarios
Low confidence in transferability claims without additional validation on layer counts beyond those tested

## Next Checks

1. Verify feature correlation stability by testing reduced feature set performance degradation across different net complexities
2. Conduct ablation studies comparing model performance with and without overflow-based ranking to isolate the ranking mechanism's contribution
3. Test transferability beyond k=10 layers to identify the upper bound of the model's generalization capability