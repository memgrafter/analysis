---
ver: rpa2
title: 'ToolSandbox: A Stateful, Conversational, Interactive Evaluation Benchmark
  for LLM Tool Use Capabilities'
arxiv_id: '2408.04682'
source_url: https://arxiv.org/abs/2408.04682
tags:
- tool
- user
- tools
- agent
- state
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces ToolSandbox, a comprehensive evaluation benchmark
  for large language models' (LLMs) tool-use capabilities. Unlike previous benchmarks
  that focus on either stateless web services or predefined dialog trajectories, ToolSandbox
  includes stateful tool execution, implicit state dependencies between tools, a built-in
  user simulator supporting on-policy conversational evaluation, and a dynamic evaluation
  strategy for intermediate and final milestones over arbitrary trajectories.
---

# ToolSandbox: A Stateful, Conversational, Interactive Evaluation Benchmark for LLM Tool Use Capabilities

## Quick Facts
- arXiv ID: 2408.04682
- Source URL: https://arxiv.org/abs/2408.04682
- Reference count: 16
- Key outcome: Introduces ToolSandbox benchmark revealing significant performance gaps between open source and proprietary models, with complex tasks remaining challenging even for state-of-the-art LLMs

## Executive Summary
ToolSandbox is a comprehensive evaluation benchmark designed to assess large language models' (LLMs) tool-use capabilities in realistic, stateful environments. Unlike previous benchmarks that focus on stateless web services or predefined dialog trajectories, ToolSandbox incorporates stateful tool execution, implicit state dependencies between tools, and a built-in user simulator supporting on-policy conversational evaluation. The benchmark contains 1032 test cases organized into categories like State Dependency, Canonicalization, and Insufficient Information, with human-authored milestones and minefields for evaluation.

The evaluation framework reveals significant performance gaps between open source and proprietary models, with complex tasks like state dependencies, canonicalization, and insufficient information remaining challenging even for the most capable state-of-the-art LLMs. ToolSandbox provides a more comprehensive and flexible evaluation framework by using milestones and minefields rather than static trajectories, allowing for dynamic evaluation of any trajectory against predefined milestones (required events) and minefields (forbidden events). This approach offers richer intermediate and final execution signals that provide deeper insights into model performance.

## Method Summary
ToolSandbox evaluates LLM tool-use capabilities through a stateful execution context with Python tools that can modify and depend on shared world state. Models interact with a simulated user and execution environment, issuing tool calls and consuming responses to complete tasks. The evaluation uses milestone-based similarity measures to assess whether actual trajectories match predefined required and forbidden events. The benchmark includes 1032 test cases across categories like State Dependency, Canonicalization, and Insufficient Information, with human-authored milestones and minefields for evaluation.

## Key Results
- Significant performance gap between open source and proprietary models, with Hermes (best open source) lagging more than 20 points behind the second-to-last proprietary model
- Complex tasks like State Dependency, Canonicalization, and Insufficient Information remain challenging even for state-of-the-art LLMs
- Models struggle with stateful tool execution, implicit state dependencies, and canonicalization of arguments in insufficient information scenarios
- Tool schema representations (distraction tools, scrambled names/descriptions) significantly impact model accuracy

## Why This Works (Mechanism)

### Mechanism 1
- Claim: ToolSandbox enables more accurate evaluation of LLM tool-use capabilities by incorporating stateful tool execution and implicit state dependencies.
- Mechanism: By modeling stateful tools that can modify and depend on a shared world state, ToolSandbox captures realistic multi-turn interactions where tool calls are interdependent and the agent must reason about the current state before making decisions.
- Core assumption: Stateful tool interactions and state dependencies are essential for evaluating true LLM tool-use capabilities in realistic scenarios.
- Evidence anchors:
  - [abstract] "ToolSandbox includes stateful tool execution, implicit state dependencies between tools, a built-in user simulator supporting on-policy conversational evaluation and a dynamic evaluation strategy for intermediate and final milestones over an arbitrary trajectory."
  - [section 2] "Stateful tool execution, implicit state dependencies between tools, a built-in user simulator supporting on-policy conversational evaluation and a dynamic evaluation strategy for intermediate and final milestones over an arbitrary trajectory."
- Break condition: If stateful tools are not implemented correctly or state dependencies are not properly modeled, the evaluation results may not accurately reflect LLM capabilities.

### Mechanism 2
- Claim: ToolSandbox provides a more comprehensive and flexible evaluation framework compared to existing benchmarks by using milestones and minefields.
- Mechanism: Instead of relying on static trajectories and turn-wise metrics, ToolSandbox allows for dynamic evaluation of any trajectory against predefined milestones (required events) and minefields (forbidden events), providing intermediate and final signals that offer deeper insights into model performance.
- Core assumption: Milestone and minefield based evaluation can accurately assess LLM tool-use capabilities across diverse and complex trajectories.
- Evidence anchors:
  - [abstract] "a dynamic evaluation strategy for intermediate and final milestones over arbitrary trajectories"
  - [section 2.3] "we developed an evaluation strategy based on Milestones and Minefields, which defines key events that must or must not happen in a trajectory, allowing us to evaluate any trajectory with rich intermediate and final execution signals"
- Break condition: If milestones and minefields are not carefully designed or cover all relevant aspects of tool-use scenarios, the evaluation may miss important capabilities or overemphasize others.

### Mechanism 3
- Claim: ToolSandbox reveals significant performance gaps between open source and proprietary models, and identifies challenging scenarios even for state-of-the-art LLMs.
- Mechanism: By providing a comprehensive benchmark with diverse and complex tool-use scenarios, ToolSandbox exposes the limitations of current models, showing that even top proprietary models struggle with state dependencies, canonicalization, and insufficient information tasks.
- Core assumption: The tool-use scenarios in ToolSandbox are representative and challenging enough to reveal meaningful differences in model capabilities.
- Evidence anchors:
  - [abstract] "We show that open source and proprietary models have a significant performance gap, and complex tasks like State Dependency, Canonicalization and Insufficient Information defined in ToolSandbox are challenging even the most capable SOTA LLMs"
  - [section 4] "Significant performance gap between open source and proprietary models, with the best performing open source model Hermes lagging more than 20 points behind the second to last proprietary model"
- Break condition: If the benchmark scenarios are not diverse or challenging enough, the performance gaps and limitations revealed may not be representative of real-world tool-use capabilities.

## Foundational Learning

- Concept: Stateful tool execution and state dependencies
  - Why needed here: Understanding how stateful tools and state dependencies work is crucial for designing and evaluating LLM tool-use capabilities in realistic multi-turn scenarios.
  - Quick check question: How do stateful tools and state dependencies differ from stateless tool execution, and why are they important for evaluating LLM tool-use capabilities?

- Concept: Milestone and minefield based evaluation
  - Why needed here: Knowing how to design and interpret milestones and minefields is essential for creating comprehensive benchmarks and understanding model performance in diverse tool-use scenarios.
  - Quick check question: What are the advantages of using milestones and minefields over static trajectories and turn-wise metrics for evaluating LLM tool-use capabilities?

- Concept: Tool schema representation and its impact on model accuracy
  - Why needed here: Understanding how different tool schema representations (e.g., tool names, descriptions, argument types) affect model accuracy is important for designing effective tools and evaluating model robustness.
  - Quick check question: How can different tool schema representations impact the accuracy of LLM tool-use models, and what are some strategies for mitigating these effects?

## Architecture Onboarding

- Component map: ToolSandbox consists of an execution context (world state), tools (stateful Python functions), a message bus (communication between roles), and a user simulator (LLM-based simulated user). The evaluation is based on milestones and minefields defined for each test scenario.
- Critical path: The critical path for evaluating an LLM's tool-use capabilities in ToolSandbox involves the agent receiving a user request, deciding whether to prompt the user or call a tool, the tool executing and potentially modifying the world state, and the agent using the results to decide the next step until the task is completed or deemed unsolvable.
- Design tradeoffs: ToolSandbox trades off simplicity and scalability for more realistic and comprehensive evaluation by incorporating stateful tools, state dependencies, and a user simulator. This makes the benchmark more challenging to implement and evaluate but provides more meaningful insights into model capabilities.
- Failure signatures: Common failure modes in ToolSandbox include failing to resolve state dependencies, incorrectly canonicalizing arguments, hallucinating tool calls or arguments in insufficient information scenarios, and struggling with tool schema representations (e.g., distraction tools, scrambled names/descriptions).
- First 3 experiments:
  1. Evaluate a simple single-tool-call scenario to verify the basic functionality of the ToolSandbox framework.
  2. Evaluate a multi-tool-call scenario with state dependencies to test the agent's ability to reason about the world state and make sequential tool calls.
  3. Evaluate a scenario with tool schema augmentations (e.g., distraction tools, scrambled names) to assess the agent's robustness to different tool representations.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal strategy for automatically generating milestones and minefields to scale ToolSandbox?
- Basis in paper: [explicit] The paper mentions that authoring milestones and minefields requires deep knowledge and many iterations, hindering scalability.
- Why unresolved: Manual annotation is time-consuming and doesn't scale well with increasing test scenarios.
- What evidence would resolve it: A method that can automatically generate milestones and minefields with comparable accuracy to human annotators would demonstrate a scalable solution.

### Open Question 2
- Question: How can we improve the tool-assisted user simulator to reduce hallucination and improve instruction following?
- Basis in paper: [explicit] The paper notes that the user simulator is still subject to non-negligible hallucination and instruction following errors, and suggests expanding its tool set.
- Why unresolved: The current user simulator, even with knowledge boundaries and demonstrations, still produces errors that could affect evaluation reliability.
- What evidence would resolve it: A tool-assisted user simulator that achieves significantly lower error rates than the current implementation would validate this approach.

### Open Question 3
- Question: What is the best approach for handling mandatory confirmation and authentication in tool-use scenarios?
- Basis in paper: [explicit] The paper identifies this as an interesting problem not currently addressed in ToolSandbox, noting that confirmation is typically left to model discretion.
- Why unresolved: Current tool-use designs leave confirmation decisions to the model, which may lead to inconsistent or inappropriate behavior.
- What evidence would resolve it: An orchestration-level solution that enforces appropriate confirmation patterns while maintaining natural conversation flow would demonstrate a viable approach.

## Limitations

- The paper does not fully specify the exact implementation details of the milestone and minefield evaluation metrics, which could affect reproducibility.
- Specific prompt templates and configurations used for the LLM user simulator are not completely detailed, potentially impacting consistency of results across different implementations.
- Manual annotation of milestones and minefields is time-consuming and doesn't scale well with increasing test scenarios.

## Confidence

- **High confidence**: The overall framework design and its distinction from existing benchmarks (stateful vs stateless evaluation) is well-established and clearly explained.
- **Medium confidence**: The performance gap findings between open-source and proprietary models are supported by experimental results, though the exact magnitude may vary with different evaluation configurations.
- **Medium confidence**: The identified challenging scenarios (State Dependency, Canonicalization, Insufficient Information) are demonstrated through experimental results, but the relative difficulty across different model families could be further validated.

## Next Checks

1. Implement a simplified version of the milestone and minefield evaluation metrics using the provided similarity measure description to verify the scoring mechanism.
2. Conduct a controlled experiment comparing tool-use performance with and without state dependencies to quantify their specific impact on model capabilities.
3. Test model robustness by evaluating the same scenarios with systematically varied tool schema representations (e.g., scrambled names, distraction tools) to confirm the reported sensitivity to tool description formats.