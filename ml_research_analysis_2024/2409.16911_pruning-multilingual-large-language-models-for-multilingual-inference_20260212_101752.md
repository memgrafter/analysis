---
ver: rpa2
title: Pruning Multilingual Large Language Models for Multilingual Inference
arxiv_id: '2409.16911'
source_url: https://arxiv.org/abs/2409.16911
tags:
- translation
- language
- features
- demonstrations
- performance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigates enhancing the zero-shot performance of
  Multilingual Large Language Models (MLLMs) in non-English languages by leveraging
  their alignment capabilities between English and non-English. The research identifies
  large magnitude features that are predominantly active during few-shot translation
  demonstrations and demonstrates their relevance for translation performance.
---

# Pruning Multilingual Large Language Models for Multilingual Inference

## Quick Facts
- arXiv ID: 2409.16911
- Source URL: https://arxiv.org/abs/2409.16911
- Authors: Hwichan Kim; Jun Suzuki; Tosho Hirasawa; Mamoru Komachi
- Reference count: 33
- Primary result: Pruning MLLMs to retain weights associated with large magnitude translation features improves zero-shot performance in non-English languages for XGLM and mGPT, with refined pruning needed for BLOOM.

## Executive Summary
This study investigates enhancing the zero-shot performance of Multilingual Large Language Models (MLLMs) in non-English languages by leveraging their alignment capabilities between English and non-English. The research identifies large magnitude features that are predominantly active during few-shot translation demonstrations and demonstrates their relevance for translation performance. By pruning MLLMs to retain weights associated with these features, the study shows improved zero-shot performance in non-English languages for XGLM and mGPT, but not for BLOOM. To address this, the study refines the pruning strategy to eliminate weights related to programming language generation, leading to enhanced performance in BLOOM as well. The approach effectively utilizes English inference capabilities for non-English tasks, as evidenced by improved cross-lingual consistency.

## Method Summary
The study analyzes hidden state features during translation demonstrations to identify large magnitude features critical for translation performance. Using the Wanda pruning method, weights associated with these high-magnitude translation features are retained while others are pruned. For BLOOM, which was trained on both natural and programming languages, an additional refinement eliminates weights related to programming language generation to reduce noise. The pruned models are evaluated on multilingual inference tasks (XNLI and MARC) and translation tasks, with cross-lingual consistency measured using RankC scores.

## Key Results
- Pruning XGLM and mGPT to retain large magnitude translation features improves zero-shot performance in non-English languages
- Initial pruning approach fails for BLOOM due to its programming language training, requiring refined pruning strategy
- Cross-lingual consistency (RankC) improves after pruning, indicating better alignment between English and non-English predictions
- Refined pruning eliminating programming language generation features successfully improves BLOOM performance

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Large magnitude features are preferentially active during translation demonstrations, and pruning non-translation weights forces the model to rely on these features for non-English tasks.
- Mechanism: By identifying high-magnitude hidden state features during few-shot translation demonstrations and pruning weights that do not contribute to these features, the model is constrained to use translation-relevant feature patterns even in unrelated tasks, thus enhancing cross-lingual alignment.
- Core assumption: Translation demonstrations activate a specific subset of high-magnitude features that are critical for alignment between languages.
- Evidence anchors:
  - [abstract]: "we first analyze the behavior of MLLMs when performing translation and reveal that there are large magnitude features that play a critical role in the translation process"
  - [section 4]: "we analyzed magnitudes of features of MLLMs when inputting few-shot monolingual and translation demonstrations...specific features are predominantly active only when inputting translation demonstrations"
  - [corpus]: Weak support; no direct pruning or feature activation studies found.
- Break condition: If large magnitude features are not language-specific or are task-agnostic, pruning based on translation features will degrade general performance.

### Mechanism 2
- Claim: Programming language generation introduces noise that interferes with natural language inference performance.
- Mechanism: BLOOM is trained on both natural and programming language data; its ability to generate code activates certain high-magnitude features during pruning. By reformulating the pruning score to penalize weights active during code generation, noise from programming tasks is reduced.
- Core assumption: Programming language generation features are distinct from and detrimental to natural language task performance.
- Evidence anchors:
  - [section 5.3]: "BLOOM was trained by both multilingual natural language and programming language texts...BLOOM has the ability to generate programming language text comparable to models such as PolyCoder"
  - [section 5.3]: "If the ability for programming language generation persists within the pruned model, it may introduce undesired noise into the model's predictions"
  - [corpus]: Weak support; no direct comparative pruning studies on code vs. natural language features found.
- Break condition: If programming language features are not separable from natural language features, or if code generation aids general reasoning, this pruning will harm rather than help.

### Mechanism 3
- Claim: Using English inference capability for non-English tasks increases cross-lingual consistency.
- Mechanism: Pruning with translation demonstrations encourages the model to align English and non-English representations. This alignment manifests as higher RankC scores, indicating that the model's predictions are consistent across languages.
- Core assumption: Models naturally have better inference in English and can transfer this capability to other languages when alignment is enforced.
- Evidence anchors:
  - [abstract]: "Our contributions...we conducted multilingual zero-shot learning...The results indicated that the pruning enhances performance in XGLM and mGPT"
  - [section 5.3]: "we measured Ranking based Consistency (RankC)...models pruned using translation demonstrations (θFr−En, θEs−En, and θZh−En) achieve superior scores relative to the original model θ"
  - [corpus]: Weak support; no direct RankC studies in pruning literature found.
- Break condition: If cross-lingual consistency does not correlate with task performance, or if the model cannot effectively transfer English inference, pruning will not improve non-English results.

## Foundational Learning

- Concept: Hidden state feature magnitudes and their role in neural network behavior.
  - Why needed here: Pruning decisions depend on identifying and ranking features by magnitude; misunderstanding this could lead to ineffective pruning.
  - Quick check question: If a feature's magnitude is large during translation but small during monolingual inference, should it be retained or pruned?

- Concept: Pruning by activation importance (e.g., Wanda method).
  - Why needed here: The study applies Wanda-style pruning to retain weights associated with high-magnitude features; misapplication would destroy model capacity.
  - Quick check question: In Wanda, what does a high importance score for a weight imply about that weight's contribution to the model's predictions?

- Concept: Cross-lingual consistency metrics (RankC).
  - Why needed here: Evaluating whether pruning improves alignment between English and non-English predictions requires understanding RankC computation.
  - Quick check question: If RankC between English and French increases after pruning, what does that say about the model's cross-lingual behavior?

## Architecture Onboarding

- Component map:
  Input -> Template & Verbalizer -> Cloze-style prompt -> MLLM layers -> Hidden states -> Feature magnitude analysis -> Pruning mask -> Pruned MLLM -> Output prediction
  Calibration datasets: translation demos, monolingual demos, programming code (for BLOOM)

- Critical path:
  1. Construct few-shot translation and monolingual demonstrations
  2. Compute hidden state magnitudes for each layer
  3. Rank features by magnitude and identify top/bottom sets
  4. Apply Wanda-based pruning to retain high-magnitude translation features
  5. Evaluate zero-shot performance on non-English tasks
  6. Measure cross-lingual consistency (RankC)

- Design tradeoffs:
  - Pruning ratio: Too high → catastrophic performance loss; too low → minimal benefit
  - Demonstration quality: Poorly constructed demos → wrong features selected
  - Language selection: High-resource languages give more stable pruning signals; low-resource languages may mislead pruning

- Failure signatures:
  - BLEU/accuracy drops sharply after pruning → wrong features retained or too aggressive pruning
  - RankC unchanged or decreased → alignment not improved despite pruning
  - BLOOM performance unchanged → programming language features not separable

- First 3 experiments:
  1. Reproduce the magnitude heatmap for a single layer to verify translation-specific features exist
  2. Apply pruning with a small ratio (e.g., 10%) and test XNLI accuracy gain on one language pair
  3. Compare RankC before/after pruning to confirm consistency improvement

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of pruning-based methods compare to instruction-tuned or RLHF models for multilingual zero-shot learning?
- Basis in paper: [explicit] The paper notes that "Recent developments have shown that models fine-tuned through instruction tuning (Wei et al., 2022) and Reinforcement Learning from Human Feedback (RLHF) (Ouyang et al., 2022) can produce outputs more aligned with human preferences, and it is such models that end users are likely to use. Demonstrating the utility of pruning on these types of models is considered important."
- Why unresolved: The paper only evaluates pruning on pre-trained models without instruction tuning or RLHF, leaving the effectiveness of pruning on these more aligned models unknown.
- What evidence would resolve it: Experiments comparing pruning performance on instruction-tuned and RLHF models versus their pre-trained counterparts across multiple multilingual tasks.

### Open Question 2
- Question: What is the optimal pruning ratio α for maximizing multilingual performance, and how does it vary across different MLLM architectures?
- Basis in paper: [explicit] The paper states "In the experiments, the pruning ratio α was fixed to 0.3, and no experiments were conducted with varying ratios."
- Why unresolved: The paper used a fixed pruning ratio without exploring how performance changes with different ratios, making it unclear if 0.3 is optimal.
- What evidence would resolve it: Systematic experiments varying α across different models and tasks to identify optimal pruning ratios and their relationship to model architecture.

### Open Question 3
- Question: How sensitive are pruning-based multilingual improvements to the quality and composition of translation demonstrations?
- Basis in paper: [explicit] The paper mentions "Previous studies (Vilar et al., 2023; Chitale et al., 2024) have demonstrated that the quality of few-shot demonstrations significantly impacts the translation performance of multilingual large language models (MLLMs). However, our research did not analyze how changes in the translation demonstrations for the pruning affect the performance."
- Why unresolved: The paper used fixed demonstration parameters without analyzing sensitivity to demonstration quality or composition.
- What evidence would resolve it: Experiments varying demonstration quality, shot numbers, and language pairs to quantify their impact on pruning effectiveness.

### Open Question 4
- Question: How do architectural differences (like BLOOM's ALiBi) influence the effectiveness of pruning-based multilingual enhancement?
- Basis in paper: [explicit] The paper states "In §5.3, our focus was on the differences in pre-training data. However, an analysis based on architectural differences was not conducted. Notably, BLOOM's architecture is distinctive, especially due to its use of ALiBi, which operates directly on attention scores influenced by token positions."
- Why unresolved: The paper did not investigate how architectural features affect pruning outcomes, particularly why BLOOM showed different trends from other models.
- What evidence would resolve it: Comparative analysis of pruning effectiveness across architectures with different attention mechanisms and positional encoding schemes.

## Limitations

- Pruning effectiveness varies significantly across different MLLM architectures, with initial approach failing for BLOOM due to its programming language training
- The assumption that translation features are distinct from general language features lacks strong empirical validation
- The causal mechanism by which pruning improves performance is not rigorously proven beyond correlation with RankC improvements

## Confidence

- **High confidence**: The identification of large magnitude features during translation demonstrations is methodologically sound and supported by empirical analysis. The pruning mechanism for XGLM and mGPT is clearly demonstrated.
- **Medium confidence**: The refined pruning strategy for BLOOM (removing programming language features) is theoretically motivated but lacks direct ablation studies showing programming features specifically degrade non-English inference.
- **Low confidence**: The claim that pruning "forces" the model to rely on translation features for non-English tasks is speculative; the actual causal mechanism by which pruning improves performance is not rigorously proven.

## Next Checks

1. **Feature ablation study**: Conduct controlled experiments removing only translation features vs. general high-magnitude features to isolate whether translation-specific features are necessary for non-English performance gains.

2. **Cross-lingual transfer validation**: Test pruned models on non-English languages not seen during pruning calibration to verify that improvements generalize beyond the pruned language pairs.

3. **Architecture generalization test**: Apply the same pruning methodology to a different MLLM architecture (e.g., mT0 or NLLB) to determine whether the approach is architecture-dependent or broadly applicable.