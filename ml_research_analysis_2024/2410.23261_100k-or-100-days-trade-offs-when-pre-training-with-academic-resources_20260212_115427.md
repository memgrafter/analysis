---
ver: rpa2
title: '$100K or 100 Days: Trade-offs when Pre-Training with Academic Resources'
arxiv_id: '2410.23261'
source_url: https://arxiv.org/abs/2410.23261
tags:
- training
- gpus
- pythia
- time
- zero
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Academic researchers can pre-train large models with 3x less compute
  than original reports by using optimal combinations of efficient training methods,
  reducing Pythia-1B from 192 to 72 GPU-days. A survey of 50 AI researchers found
  common access to 1-8 GPUs for days to weeks, with most relying on on-premises hardware
  rather than cloud.
---

# $100K or 100 Days: Trade-offs when Pre-Training with Academic Resources

## Quick Facts
- arXiv ID: 2410.23261
- Source URL: https://arxiv.org/abs/2410.23261
- Reference count: 40
- Primary result: Academic labs can pre-train large models in 3x less compute than originally reported using optimal combinations of efficient training methods

## Executive Summary
Academic researchers face significant challenges when pre-training large language and vision models due to limited compute resources. This paper presents a comprehensive survey of 50 AI researchers combined with empirical benchmarking to identify optimal configurations that make large-scale pre-training feasible on academic hardware. By systematically searching among free-lunch and memory-saving methods, the authors demonstrate that training times can be reduced by up to 71% compared to naive settings, enabling previously infeasible experiments on typical academic GPU allocations of 1-8 GPUs for days to weeks.

## Method Summary
The authors developed a benchmark system that measures training times for various model sizes (160M to 6.9B parameters) on different GPU configurations. The methodology combines a survey of academic researchers to understand typical hardware availability with empirical measurement of training times across multiple optimization combinations. The benchmark explores 22 different configurations of efficient training methods including model compilation, custom kernels, mixed precision, activation checkpointing, model sharding, and offloading. Cost-benefit analysis is performed to compare different GPU types and configurations for optimal resource allocation.

## Key Results
- Pythia-1B model training reduced from 192 to 72 GPU-days through optimal method combinations
- H100 GPUs offer 3x faster training than A100 at similar cost per experiment
- 50 AI researchers survey found common access to 1-8 GPUs for days to weeks
- 2,000 GPU-hours of empirical benchmarks identified configurations enabling previously infeasible training
- Optimal hardware configuration identified as 4 H100 GPUs over 8 A100 GPUs for certain models

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Academic labs can train large models in 3x less compute than originally reported by using optimal combinations of efficient training methods.
- Mechanism: By systematically searching among free-lunch and memory-saving methods (like activation checkpointing, model sharding, offloading, compilation, and custom kernels), the benchmark identifies configurations that reduce training time by up to 71% compared to naive settings.
- Core assumption: The search space of efficiency methods is large enough that optimal combinations exist and can be found within a reasonable benchmark cost.
- Evidence anchors:
  - [abstract] "Empirical benchmarks on 2,000 GPU-hours identified configurations enabling training previously deemed infeasible."
  - [section 3.2.1] "We consider several optimizations... up to 22 combinations in our experiments."
  - [corpus] Weak evidence; no neighbor papers directly address this specific search methodology for training time reduction.

### Mechanism 2
- Claim: Optimal hardware configurations can significantly reduce both training time and cost compared to naive hardware choices.
- Mechanism: Cost-benefit analysis shows that H100 GPUs offer 3x faster training than A100 at similar cost per experiment, enabling training previously deemed infeasible on academic hardware.
- Core assumption: Hardware cost and performance scaling is predictable enough to make informed purchasing decisions.
- Evidence anchors:
  - [abstract] "Cost-benefit analysis shows H100 GPUs offer 3x faster training than A100 at similar cost per experiment."
  - [section 4.1] "A machine with 4 H100 GPUs ($130k) than 8 A100 GPUs ($160k)—even with half the memory—as both train this model in 8–9 days."
  - [corpus] Weak evidence; no neighbor papers provide similar cost-performance hardware comparison data.

### Mechanism 3
- Claim: Academic researchers typically have access to limited but sufficient compute resources (1-8 GPUs for days to weeks) to conduct meaningful pre-training experiments.
- Mechanism: Survey data shows that most academics rely on on-premises hardware rather than cloud, with 1-8 GPUs available for short-term allocations, making pre-training feasible with optimization.
- Core assumption: The survey accurately represents the broader academic compute landscape and that short-term allocations are sufficient for pre-training.
- Evidence anchors:
  - [section 2] "We find that the typical academic researcher can use 4 GPUs for days at a time."
  - [section 2] "Our survey suggests a common range for what constitutes 'academic hardware' today: 1–8 GPUs—especially RTX 3090s, A6000s, and A100s—for days (typically) or weeks (at the higher-end) at a time."
  - [corpus] Weak evidence; neighbor papers discuss computing resources but don't specifically address academic GPU availability patterns.

## Foundational Learning

- Concept: GPU memory management and optimization techniques (activation checkpointing, model sharding, offloading)
  - Why needed here: These techniques directly impact whether large models can fit in GPU memory and how quickly they can be trained
  - Quick check question: What happens to training time when you enable activation checkpointing versus model sharding?

- Concept: Hardware cost-performance analysis and procurement strategy
  - Why needed here: Understanding the trade-offs between different GPU types and configurations is essential for making cost-effective decisions
  - Quick check question: How does the cost per experiment compare between 4 H100 GPUs versus 8 A100 GPUs for training the same model?

- Concept: Empirical benchmarking methodology and experimental design
  - Why needed here: The paper's approach relies on systematic measurement rather than theoretical estimation to determine training times
  - Quick check question: Why did the analytic approach overestimate training speed by a factor of 6x compared to empirical measurements?

## Architecture Onboarding

- Component map: Survey -> Benchmark Setup -> Method Combination Search -> Result Analysis -> Hardware Recommendation
- Critical path: Survey findings inform benchmark design, which identifies optimal configurations, leading to hardware recommendations based on cost-benefit analysis
- Design tradeoffs: Between exhaustive search of all method combinations (22 options) versus targeted optimization, and between accuracy of measurements versus benchmark runtime
- Failure signatures: Infeasible configurations show as memory allocation errors or timeouts; poor optimization combinations show as slower-than-baseline training times
- First 3 experiments:
  1. Run naive training of a small model (160M Pythia) on a single GPU to establish baseline timing
  2. Enable free-lunch methods only and measure speed improvement on the same configuration
  3. Enable all method combinations for a medium model (410M Pythia) on 2 GPUs to identify optimal memory-saving configuration

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do different inter-node networking configurations (e.g., Ethernet vs Infiniband at various bandwidths) quantitatively affect pre-training time for different model sizes?
- Basis in paper: [explicit] The paper notes that 33% of respondents reported having Ethernet and 24% reported having Infiniband inter-node connections, and that multi-node support varies significantly across institutions.
- Why unresolved: The paper focuses on single-node experiments and only mentions networking in the context of survey results, not empirical benchmarking.
- What evidence would resolve it: Empirical measurements of pre-training times across different networking configurations (1Gbps, 10Gbps, 25Gbps Ethernet and QDR/FDR/EDR/HDR Infiniband) for various model sizes and GPU counts.

### Open Question 2
- Question: What is the optimal balance between activation checkpointing depth and model sharding stage for different model architectures (Transformers vs Mamba vs ConvNeXt) when training on academic hardware?
- Basis in paper: [explicit] The paper explores various combinations of memory-saving methods including activation checkpointing (with multiple stages) and model sharding (stages 0-3), noting that the optimal configuration varies.
- Why unresolved: While the paper finds that optimal configurations exist, it doesn't provide a systematic analysis of how these methods interact differently across model architectures.
- What evidence would resolve it: Comparative analysis showing training time vs memory trade-offs for different combinations of checkpointing depth and sharding stages across multiple model architectures.

### Open Question 3
- Question: How do the performance gains from the proposed optimization methods scale with model size beyond 7B parameters, and at what point do they become insufficient to make large-scale training feasible on academic hardware?
- Basis in paper: [explicit] The paper states "We don't consider models with more than 7B parameters, because we find that Pythia-6.9B already requires 30 days to train on the very best 'academic' hardware (8 H100 GPUs)."
- Why unresolved: The paper explicitly stops at 7B parameters and doesn't explore the scalability limits of the proposed optimizations.
- What evidence would resolve it: Benchmarking results for models in the 8B-100B parameter range, showing when the optimization methods no longer provide sufficient speedups to make training feasible within academic resource constraints.

## Limitations

- The empirical benchmark (2,000 GPU-hours) may not generalize across all model architectures and hardware configurations
- Survey methodology has unclear response rate and may not represent full diversity of academic computing environments
- Hardware cost-performance analysis assumes stable pricing and availability of H100 GPUs
- Benchmark focus on specific models (Pythia family) and datasets limits applicability to other domains

## Confidence

**High Confidence**: The survey findings about typical academic GPU availability (1-8 GPUs for days to weeks) are well-supported by the data and align with known patterns in academic computing. The mechanism of using memory-saving techniques (activation checkpointing, model sharding, offloading) to reduce memory requirements is technically sound and well-established in the literature.

**Medium Confidence**: The claim that H100 GPUs offer 3x faster training than A100 at similar cost requires careful interpretation - this appears to be based on specific model sizes and configurations. The benchmark methodology for identifying optimal training settings is rigorous but may not capture all real-world constraints like network bottlenecks or system noise.

**Low Confidence**: The extrapolation from 2,000 GPU-hours of empirical data to general conclusions about academic pre-training feasibility involves assumptions about model architecture transferability and hardware scaling that warrant further validation.

## Next Checks

1. **Cross-Architecture Validation**: Test the benchmark methodology on non-Pythia architectures (e.g., Llama, OPT) to verify that the identified optimal configurations transfer across different model families and whether the 3x reduction claim holds.

2. **Long-Term Resource Availability**: Conduct follow-up surveys with academic institutions over 6-12 months to track changes in GPU availability, wait times, and actual usage patterns, particularly as demand for large model training increases.

3. **Cost Fluctuation Sensitivity**: Re-run the hardware cost-benefit analysis with updated pricing data for H100, A100, and potential future GPU generations to assess the stability of the recommended configurations and identify breaking points where the cost-performance trade-offs change significantly.