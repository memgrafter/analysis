---
ver: rpa2
title: Weak-to-Strong Jailbreaking on Large Language Models
arxiv_id: '2401.17256'
source_url: https://arxiv.org/abs/2401.17256
tags:
- arxiv
- attack
- language
- weak-to-strong
- jailbreaking
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Weak-to-Strong Jailbreaking on Large Language Models exposes critical
  safety vulnerabilities in aligned LLMs by showing that safe and unsafe models share
  significant token distribution overlap, especially in later generations. The proposed
  weak-to-strong attack leverages small, unsafe models to guide larger, aligned models
  toward generating harmful content by modifying decoding probabilities during inference.
---

# Weak-to-Strong Jailbreaking on Large Language Models

## Quick Facts
- arXiv ID: 2401.17256
- Source URL: https://arxiv.org/abs/2401.17256
- Reference count: 40
- One-line primary result: Exposes critical safety vulnerabilities in aligned LLMs by showing that safe and unsafe models share significant token distribution overlap, especially in later generations

## Executive Summary
Weak-to-Strong Jailbreaking on Large Language Models exposes critical safety vulnerabilities in aligned LLMs by showing that safe and unsafe models share significant token distribution overlap, especially in later generations. The proposed weak-to-strong attack leverages small, unsafe models to guide larger, aligned models toward generating harmful content by modifying decoding probabilities during inference. This approach achieves over 99% attack success rates on the AdvBench and MaliciousInstruct datasets, outperforming existing methods and producing more harmful outputs than weak models alone. The attack is computationally efficient, requiring only one forward pass per query, and is effective across diverse model families and languages. Experiments confirm that outputs differ from those of weak unsafe models, ruling out simple copying. A gradient ascent defense reduces attack success by 20%-40%. The findings reveal that current LLM safety measures are superficial and highlight the urgent need for deeper alignment strategies.

## Method Summary
The weak-to-strong attack modifies a large safe model's decoding probabilities using two smaller models: a safe reference model and an unsafe model. The attack computes modified token probabilities using the formula M+(yt|q, y<t) × (ˆM−(yt|q,y<t)/M−(yt|q,y<t))α, where M+ is the target large safe model, M− is the weak safe reference model, and ˆM− is the weak unsafe model. The amplification factor α controls the influence of the unsafe model. The method requires one forward pass through all three models, making it computationally efficient. The attack is evaluated on AdvBench and MaliciousInstruct datasets using Attack Success Rate (ASR), Harm Score, and GPT-4 Score metrics, and compared against baselines including GCG, Prefix Injection, SelfCipher, DeepInception, Adversarial Decoding, and Adversarial Fine-tuning.

## Key Results
- Achieved over 99% attack success rates on AdvBench and MaliciousInstruct datasets
- Outperformed existing methods including GCG, Prefix Injection, and Adversarial Fine-tuning
- Generated outputs more harmful than weak unsafe models alone, ruling out simple copying
- Demonstrated effectiveness across diverse model families and languages
- Gradient ascent defense reduced attack success by 20%-40%

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Safe and unsafe models share significant token distribution overlap, especially in later generations
- Mechanism: During decoding, the divergence between safe and unsafe models decreases over time. Safe models may initially refuse harmful prompts, but once the prefix contains harmful content, they tend to follow similar distributions to unsafe models
- Core assumption: The safety alignment is primarily superficial, affecting mainly initial token refusals rather than fundamental changes to the model's generation behavior
- Evidence anchors:
  - [abstract] "safe and unsafe models only differ in their initial decoding distributions"
  - [section] "the average KL divergence for 500 samples decreases over time, suggesting later positions in the decoding of the safe and unsafe models have less distributional shift when conditioning on the same prefix"

### Mechanism 2
- Claim: Weak unsafe models can guide strong safe models through log probability algebra
- Mechanism: The attack uses two smaller models (safe and unsafe) to adversarially modify a larger safe model's decoding probabilities using the formula: M+(yt|q, y<t) × (ˆM−(yt|q,y<t)/M−(yt|q,y<t))α
  This amplifies the mismatch between the unsafe and safe small models, steering the large model toward harmful outputs
- Core assumption: The log probability manipulation can effectively transfer harmful knowledge from small models to large ones
- Evidence anchors:
  - [abstract] "The weak-to-strong attack's key technical insight is using two smaller models (a safe and an unsafe one) to adversarially modify a significantly larger safe model's decoding probabilities"
  - [section] "The intuition is that the log probability algebra transfers harmful knowledge from the small model to the large one"

### Mechanism 3
- Claim: Attack effectiveness diminishes as generation length increases
- Mechanism: As the generation length increases, the prediction mismatch term (ˆM−(yt|q,y<t)/M−(yt|q,y<t)) converges closer to 1, reducing the influence of the weak jailbroken model and increasing reliance on the large model's own capabilities
- Core assumption: The attack's steering power is strongest at the beginning of generation and weakens over time
- Evidence anchors:
  - [abstract] "As the generation length increases, the prediction mismatch term converges closer to 1 based on the evidence in Section 3.1"
  - [section] "Consequently, the influence of the weak jailbroken model diminishes, and the generation increasingly relies on the large strong model's capabilities"

## Foundational Learning

- Concept: KL divergence between probability distributions
  - Why needed here: Understanding how token distributions differ between safe and unsafe models is central to the attack's effectiveness
  - Quick check question: If two models have identical token distributions for a given prompt, what would their KL divergence be?

- Concept: Log probability manipulation and probability algebra
  - Why needed here: The attack fundamentally relies on manipulating log probabilities using the formula M+(yt|q, y<t) × (ˆM−(yt|q,y<t)/M−(yt|q,y<t))α
  - Quick check question: What happens to the probability distribution if the amplification factor α is set to 0?

- Concept: Token overlap and top-K sampling
  - Why needed here: The attack exploits the fact that safe and unsafe models share many top-ranked tokens, making it easier to steer the safe model
  - Quick check question: If two models share 90% of their top-10 tokens for a given context, what does this imply about their similarity in that context?

## Architecture Onboarding

- Component map: Input prompt -> Forward pass through M+, M−, and ˆM− -> Probability manipulation using log algebra -> Modified token distribution -> Decoding of output
- Critical path: Input prompt → Forward pass through M+, M−, and ˆM− → Probability manipulation using log algebra → Modified token distribution → Decoding of output
- Design tradeoffs: The attack trades minimal computational overhead (one forward pass) for high effectiveness, but requires access to model logits and shared vocabulary
- Failure signatures: Low attack success rate, outputs that closely resemble the weak unsafe model (indicating simple copying rather than knowledge transfer), or models that maintain strong resistance throughout generation
- First 3 experiments:
  1. Measure KL divergence between safe and unsafe models across different generation steps to confirm the decreasing divergence pattern
  2. Test different amplification factor values (α) to find the optimal balance between attack success rate and harmfulness
  3. Evaluate attack effectiveness across different model families and sizes to verify generalizability

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the weak-to-strong jailbreaking attack perform against larger models (e.g., 405B parameters) where computational costs are significantly higher?
- Basis in paper: [explicit] The paper mentions that performing existing attacks on much larger models remains challenging due to extreme computational costs.
- Why unresolved: The study primarily focuses on models up to 70B parameters, leaving the effectiveness on much larger models unexplored.
- What evidence would resolve it: Experiments testing the attack on models significantly larger than 70B, measuring attack success rates and computational costs.

### Open Question 2
- Question: Can the weak-to-strong jailbreaking attack be adapted to work with closed-source models that do not provide full token logits?
- Basis in paper: [explicit] The paper discusses that closed-source models might not fully disclose their full token logits, but logit extraction techniques can be applied to recover them.
- Why unresolved: The study focuses on open-source models for reproducibility, leaving the attack's effectiveness on closed-source models unverified.
- What evidence would resolve it: Successful application of the attack on closed-source models, demonstrating its feasibility and effectiveness in such environments.

### Open Question 3
- Question: What are the long-term effects of the gradient ascent defense on model alignment and performance?
- Basis in paper: [explicit] The paper proposes a gradient ascent defense that reduces attack success rates by 20%-40% and mentions minimal impact on overall capability as evaluated by TruthfulQA.
- Why unresolved: The study does not explore the long-term effects of the defense on model alignment and performance over extended periods or with more comprehensive evaluations.
- What evidence would resolve it: Long-term studies measuring the defense's impact on model alignment, performance, and robustness over time and with diverse datasets.

## Limitations

- The attack requires access to model logits and shared vocabulary between models, which may not hold in all deployment scenarios
- The defense evaluation uses gradient ascent on 200 harmful instruction-answer pairs, but generalizability to other attack vectors is unclear
- The study focuses on English-language datasets and model families, potentially limiting applicability to other languages and architectures
- The attack's effectiveness may be sensitive to specific model architectures and training procedures

## Confidence

**High Confidence:** The core finding that safe and unsafe models share significant token distribution overlap, especially in later generations, is well-supported by KL divergence measurements across 500 samples. The computational efficiency claim (one forward pass) is straightforward to verify.

**Medium Confidence:** The attack effectiveness across diverse model families and languages is demonstrated but may be sensitive to specific model architectures and training procedures. The gradient ascent defense reducing attack success by 20%-40% is measured but may not generalize to all attack variations.

**Low Confidence:** The claim that outputs differ from weak unsafe models (ruling out simple copying) is supported but the statistical evidence could be more robust. The generalizability of findings to proprietary models like GPT-4 and Claude is asserted but not empirically verified.

## Next Checks

1. **Test the attack on proprietary models:** Evaluate whether the weak-to-strong attack transfers to closed models like GPT-4 and Claude through API access or other black-box methods to verify the claimed generalizability.

2. **Evaluate attack stability across longer generations:** Systematically measure how attack success rate and harm score degrade as generation length increases beyond the tested range to validate the prediction mismatch convergence mechanism.

3. **Test defense robustness against attack variants:** Evaluate the gradient ascent defense against modified attack strategies (different α values, alternative probability manipulation methods) to assess whether it addresses the fundamental vulnerability or only this specific attack vector.