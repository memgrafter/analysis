---
ver: rpa2
title: Large Language Models for Relevance Judgment in Product Search
arxiv_id: '2406.00247'
source_url: https://arxiv.org/abs/2406.00247
tags:
- relevance
- item
- training
- data
- human
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper explores leveraging large language models (LLMs) for
  automating relevance judgment in product search. The authors propose fine-tuning
  LLMs with and without low-rank adaptation (LoRA) on a unique dataset of millions
  of query-item pairs (QIPs) annotated by human evaluators.
---

# Large Language Models for Relevance Judgment in Product Search

## Quick Facts
- arXiv ID: 2406.00247
- Source URL: https://arxiv.org/abs/2406.00247
- Reference count: 40
- Key outcome: LLMs achieve F1 scores exceeding 88% for automated product search relevance judgment

## Executive Summary
This paper investigates the use of large language models for automating relevance judgment in product search, addressing a traditionally costly human evaluation process. The authors fine-tune LLMs on millions of query-item pairs annotated by human evaluators, exploring the impact of different fine-tuning strategies (full vs. LoRA) and feature inclusion (item attributes). Their experiments demonstrate that LoRA fine-tuning outperforms full fine-tuning while being more parameter-efficient, and that including item descriptions significantly improves performance for larger models. The best models achieve micro F1 scores of 88.8% and 89.6% on three-class relevance tasks, showing promise for scalable evaluation of product search systems.

## Method Summary
The authors fine-tune LLMs using a proprietary dataset of 16 million query-item pairs from an e-commerce platform, where items were annotated by human evaluators for relevance. They experiment with both full fine-tuning and LoRA approaches across different model sizes, systematically varying the inclusion of item attributes in prompts. The study evaluates model performance using micro F1 scores on three-class relevance tasks and assesses the models' ability to evaluate product search features by comparing their judgments against human evaluators. The research focuses on optimizing the trade-off between performance and computational efficiency while maintaining high accuracy in relevance assessment.

## Key Results
- LoRA fine-tuning consistently outperforms full fine-tuning while using fewer parameters
- Including item descriptions improves performance for larger models
- Best models achieve micro F1 scores of 88.8% and 89.6% on three-class relevance tasks
- Fine-tuned LLMs match human evaluators in 85-89% of cases when evaluating product search features

## Why This Works (Mechanism)
The success of LLMs in relevance judgment leverages their strong natural language understanding capabilities and ability to capture semantic relationships between queries and items. By fine-tuning on large-scale human-annotated data, the models learn to map query-item pairs to relevance judgments in a way that generalizes across different product categories. The LoRA approach allows efficient adaptation of pre-trained models to the specific task while preserving their general language understanding capabilities. Including item descriptions provides additional context that helps the models make more informed relevance assessments, particularly for larger models with greater capacity to process and integrate this information.

## Foundational Learning
- **Relevance Judgment Fundamentals**: Understanding how queries and items are mapped to relevance scores is essential for evaluating search quality. Quick check: Can the model distinguish between exact matches, relevant alternatives, and irrelevant items?
- **Fine-tuning vs. LoRA**: Parameter-efficient fine-tuning methods like LoRA reduce computational costs while maintaining performance. Quick check: Compare parameter counts and training times between full fine-tuning and LoRA approaches.
- **Multi-class Classification**: The three-class relevance task requires models to distinguish between multiple levels of relevance rather than binary decisions. Quick check: Analyze confusion matrices to understand where models struggle with fine-grained distinctions.
- **Prompt Engineering**: The way item attributes and descriptions are presented to the model significantly impacts performance. Quick check: Test different prompt formats to identify optimal information presentation.

## Architecture Onboarding

**Component Map**: Query-Item Pairs -> LLM (Fine-tuned with LoRA) -> Relevance Classification

**Critical Path**: Data Preparation (QIPs + Annotations) -> Model Fine-tuning (LoRA) -> Evaluation (F1 Score) -> Feature Assessment Comparison

**Design Tradeoffs**: The choice between LoRA and full fine-tuning represents a key tradeoff between computational efficiency and model adaptation capacity. Including item descriptions improves accuracy but increases input length and processing requirements. The three-class task provides nuanced relevance assessment but is more challenging than binary classification.

**Failure Signatures**: Models may struggle with ambiguous queries, items with limited descriptions, or domain-specific terminology not well-represented in training data. Performance degradation may occur when evaluating features outside the training distribution or when item descriptions are missing or misleading.

**First Experiments**: 1) Compare LoRA vs. full fine-tuning on a validation set to confirm efficiency gains; 2) Test model performance with and without item descriptions on a held-out test set; 3) Evaluate model agreement with human judgments across different product categories to identify potential biases.

## Open Questions the Paper Calls Out
None

## Limitations
- The study relies on a proprietary dataset from a single e-commerce platform, raising questions about generalizability across domains
- The research doesn't explore alternative parameter-efficient fine-tuning methods beyond LoRA
- Limited discussion of model performance on rare or ambiguous query-item pairs

## Confidence
- **High Confidence**: LoRA outperforms full fine-tuning; item descriptions improve performance for larger models; achieved F1 scores above 88%
- **Medium Confidence**: LLMs can accurately evaluate product search features; potential for cost reduction and efficiency improvement
- **Low Confidence**: Generalizability across domains; long-term stability of fine-tuned models; comparison with other relevance judgment approaches

## Next Checks
1. Test fine-tuned models on product search datasets from different e-commerce platforms to assess cross-domain generalizability
2. Conduct temporal stability analysis to evaluate model performance as product catalogs and user behavior evolve over time
3. Perform detailed comparison of model judgments with multiple human evaluators, including inter-annotator agreement metrics and systematic difference analysis