---
ver: rpa2
title: 'Search Engines in an AI Era: The False Promise of Factual and Verifiable Source-Cited
  Responses'
arxiv_id: '2410.22349'
source_url: https://arxiv.org/abs/2410.22349
tags:
- answer
- sources
- engines
- participants
- information
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper presents a sociotechnical audit of answer engines\u2014\
  LLM-based generative search systems that synthesize and cite sources in response\
  \ to user queries. Through a user study with 21 participants and an automated evaluation\
  \ of three popular systems (You.com, Perplexity.ai, BingChat), the authors identified\
  \ 16 key limitations, including frequent hallucinations, inaccurate citations, and\
  \ one-sided answers on debate queries."
---

# Search Engines in an AI Era: The False Promise of Factual and Verifiable Source-Cited Responses

## Quick Facts
- arXiv ID: 2410.22349
- Source URL: https://arxiv.org/abs/2410.22349
- Reference count: 40
- Primary result: Answer engines frequently generate unsupported statements (30%+), provide one-sided answers on debates (50-80%), and cite incorrect sources

## Executive Summary
This paper presents a comprehensive sociotechnical audit of LLM-based answer engines that synthesize and cite sources in response to user queries. Through a user study with 21 participants and automated evaluation of three popular systems (You.com, Perplexity.ai, BingChat), the authors identified 16 key limitations including frequent hallucinations, inaccurate citations, and biased responses on debate queries. They developed 16 design recommendations linked to 8 quantitative metrics and implemented these in an Answer Engine Evaluation (AEE) benchmark. The evaluation revealed that all systems struggle with factual support and citation accuracy, with Perplexity performing worst overall and You.com showing slightly better performance. The authors release their evaluation framework to facilitate transparent assessment of these sociotechnical systems.

## Method Summary
The study employed a mixed-methods approach combining qualitative user research and quantitative automated evaluation. The qualitative component involved a user study with 21 participants using think-aloud protocols to interact with three answer engines. The quantitative component developed an Answer Engine Evaluation (AEE) benchmark with 8 metrics measuring factual support, citation accuracy, and answer balance. The benchmark was applied to 100 queries across three answer engines, using LLM-based factual support evaluation with human verification on a subset of responses.

## Key Results
- All evaluated systems generate unsupported statements in approximately 30%+ of cases
- One-sided answers on debate queries occur in 50-80% of responses across all systems
- Citation accuracy is particularly poor, with Perplexity showing more than half of citations being inaccurate
- You.com demonstrates slightly better performance than other evaluated systems

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** RAG-based answer engines still generate substantial proportions of unsupported statements even when citing sources
- **Mechanism:** The retrieval component provides relevant documents, but the LLM generator can synthesize information that is not explicitly present in the retrieved sources, creating unsupported statements
- **Core assumption:** The LLM has access to pre-training knowledge that it combines with retrieved documents, leading to confabulation
- **Evidence anchors:**
  - [abstract]: "all systems frequently generate unsupported statements (30%+)"
  - [section]: "the RAG framework is advertised to solve the hallucinatory behavior of LLMs by enforcing that an LLM generates an answer grounded in source documents, yet the results show that RAG-based answer engines still generate answers containing a large proportion of statements unsupported by the sources they provide"
  - [corpus]: Weak - corpus doesn't directly address hallucination mechanisms
- **Break condition:** When the LLM is properly constrained to only use information from retrieved sources, or when retrieval consistently provides comprehensive coverage of all answer elements

### Mechanism 2
- **Claim:** Citation accuracy is low because answer engines frequently cite incorrect sources even when correct sources exist
- **Mechanism:** The system maps statements to sources based on relevance or proximity rather than factual support, leading to misattribution
- **Core assumption:** The citation selection process uses simple heuristics (e.g., first relevant source) rather than verifying factual support
- **Evidence anchors:**
  - [abstract]: "frequently generate unsupported statements (30%+), one-sided answers on debates (50-80%), and often cite incorrect sources"
  - [section]: "all answer engines struggle to back their statements with the sources that support them...Perplexity performs worse with more than half of its citations being inaccurate"
  - [corpus]: Weak - corpus doesn't provide evidence about citation mapping mechanisms
- **Break condition:** When citation mapping incorporates explicit factual support verification before establishing source-statement relationships

### Mechanism 3
- **Claim:** One-sided answers emerge from systems aligning with query bias rather than presenting balanced perspectives
- **Mechanism:** The LLM interprets query formulation as preference signal and generates confirmatory rather than comprehensive answers
- **Core assumption:** The system prioritizes user satisfaction (agreement) over informational completeness
- **Evidence anchors:**
  - [abstract]: "frequently generate unsupported statements (30%+), one-sided answers on debates (50-80%)"
  - [section]: "answer engines frequently aligned with the bias implied in the question, neglecting to present diverse perspectives...reinforcing user biases"
  - [corpus]: Moderate - corpus shows related work on bias in generative search engines
- **Break condition:** When the system is explicitly trained or prompted to present multiple perspectives regardless of query framing

## Foundational Learning

- **Concept:** RAG (Retrieval-Augmented Generation) systems
  - Why needed here: The paper evaluates RAG-based answer engines, so understanding how retrieval and generation components interact is fundamental
  - Quick check question: What are the two main components of a RAG system and how do they interact to produce answers?

- **Concept:** Factual support evaluation
  - Why needed here: The evaluation framework relies on determining whether sources factually support statements, which is a core challenge in NLP
  - Quick check question: What makes factual support evaluation different from simple semantic similarity between statements and source text?

- **Concept:** User study methodology (think-aloud protocol)
  - Why needed here: The paper's qualitative findings come from a usability study using think-aloud protocol, which shapes how insights are gathered
  - Quick check question: How does the think-aloud protocol help researchers understand user interactions with answer engines compared to post-hoc interviews?

## Architecture Onboarding

- **Component map:** User Interface -> Retriever -> Generator -> Citation Mapper -> User Interface
- **Critical path:** User Query → Retriever → Generator → Citation Mapper → User Interface → User Interaction → Evaluation Metrics
- **Design tradeoffs:**
  - Speed vs. accuracy: More thorough source verification improves accuracy but slows response time
  - Source comprehensiveness vs. conciseness: Including more sources provides better coverage but may overwhelm users
  - Confidence vs. nuance: High-confidence answers are more satisfying but may oversimplify complex topics
- **Failure signatures:**
  - High unsupported statement ratio indicates generation problems
  - Low citation accuracy indicates mapping issues
  - One-sided answers indicate bias in generation or citation selection
  - High uncited sources indicates retrieval-coverage mismatch
- **First 3 experiments:**
  1. Measure the correlation between number of retrieved sources and percentage of unsupported statements to test if more sources reduce hallucination
  2. Compare citation accuracy when using different factual support evaluation methods (LLM vs. human) to validate the evaluation framework
  3. Test whether adding explicit instructions to present multiple perspectives reduces one-sided answers on debate queries

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can answer engines be designed to effectively balance providing concise answers with the need for objective detail and comprehensive coverage of complex topics?
- Basis in paper: [inferred] ... The paper identifies a key limitation where answer engines often lack objective detail and contextual depth in generated responses, leading to overly generic and insufficient summaries.
- Why unresolved: While the paper proposes design recommendations, it does not provide concrete solutions or metrics to measure the effectiveness of these recommendations in achieving a balance between conciseness and comprehensiveness.
- What evidence would resolve it: Comparative studies evaluating user satisfaction and information retention when interacting with answer engines that prioritize either conciseness or detail, and the development of new metrics that quantify the trade-off between these two aspects.

### Open Question 2
- Question: How can answer engines be designed to avoid reinforcing user biases and present balanced viewpoints on debate queries, especially when the query itself is biased?
- Basis in paper: [explicit] ... The paper highlights that answer engines frequently align with the bias implied in the question, neglecting to present diverse perspectives available from the retrieved sources.
- Why unresolved: The paper identifies the problem but does not offer a clear methodology or metric to quantify the balance of viewpoints presented in answers or a mechanism to actively counteract the influence of biased queries.
- What evidence would resolve it: Evaluation of answer engines using metrics that measure the proportion of pro, con, and neutral statements in answers to debate queries, and user studies comparing the impact of biased versus balanced answers on user opinions.

### Open Question 3
- Question: What are the long-term societal implications of relying on answer engines for information retrieval, particularly in terms of critical thinking skills and the ability to independently verify information?
- Basis in paper: [explicit] ... The paper discusses concerns about the erosion of critical thinking skills and the increased reliance on answer engines for information, potentially leading to a decrease in users' ability to independently verify information.
- Why unresolved: While the paper raises these concerns, it does not provide empirical evidence or long-term studies to quantify the impact of answer engines on critical thinking skills or user behavior in information verification.
- What evidence would resolve it: Longitudinal studies tracking changes in critical thinking skills and information verification habits among users who frequently use answer engines compared to those who rely on traditional search methods.

## Limitations
- The evaluation framework relies on LLM-based factual support assessment, which introduces potential circularity and bias
- The user study sample size (n=21) limits generalizability of qualitative findings
- The benchmark covers only three answer engines, which may not represent the full landscape of RAG-based systems

## Confidence

**High confidence**: The existence of significant unsupported statements (30%+) and incorrect citations across all evaluated systems - this is directly measured and consistently observed

**Medium confidence**: The claim that Perplexity performs worst overall - based on limited sample of systems and queries

**Medium confidence**: The design recommendations are grounded in observed limitations but their effectiveness requires future validation

**Low confidence**: The generalizability of findings to all RAG-based answer engines given the limited system sample

## Next Checks
1. **Cross-validation of factual support evaluation**: Re-run the automated benchmark using human annotators for a subset of 20 queries to measure inter-annotator agreement and validate LLM-based assessment accuracy
2. **System diversity expansion**: Evaluate 5 additional RAG-based answer engines (including open-source implementations) to test whether observed limitations generalize beyond the initial three systems
3. **Temporal robustness testing**: Re-run the benchmark quarterly for one year to assess whether observed limitations persist as these systems evolve and claim improvements