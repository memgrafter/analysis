---
ver: rpa2
title: 'INCLUDE: Evaluating Multilingual Language Understanding with Regional Knowledge'
arxiv_id: '2411.19799'
source_url: https://arxiv.org/abs/2411.19799
tags:
- language
- implicit
- languages
- arxiv
- include
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces INCLUDE, a new multilingual benchmark designed
  to evaluate large language models (LLMs) across diverse linguistic and cultural
  contexts. The key problem addressed is the lack of high-quality evaluation resources
  for languages other than English, which limits the effective deployment of LLMs
  in many regions.
---

# INCLUDE: Evaluating Multilingual Language Understanding with Regional Knowledge

## Quick Facts
- arXiv ID: 2411.19799
- Source URL: https://arxiv.org/abs/2411.19799
- Authors: Angelika Romanou, Negar Foroutan, Anna Sotnikova, Zeming Chen, Sree Harsha Nelaturu, Shivalika Singh, Rishabh Maheshwary, Micol Altomare, Mohamed A. Haggag, Snegha A, Alfonso Amayuelas, Azril Hafizi Amirudin, Viraat Aryabumi, Danylo Boiko, Michael Chang, Jenny Chim, Gal Cohen, Aditya Kumar Dalmia, Abraham Diress, Sharad Duwal, Daniil Dzenhaliou, Daniel Fernando Erazo Florez, Fabian Farestam, Joseph Marvin Imperial, Shayekh Bin Islam, Perttu Isotalo, Maral Jabbarishiviari, Börje F. Karlsson, Eldar Khalilov, Christopher Klamm, Fajri Koto, Dominik Krzemiński, Gabriel Adriano de Melo, Syrielle Montariol, Yiyang Nan, Joel Niklaus, Jekaterina Novikova, Johan Samir Obando Ceron, Debjit Paul, Esther Ploeger, Jebish Purbey, Swati Rajwal, Selvan Sunitha Ravi, Sara Rydell, Roshan Santhosh, Drishti Sharma, Marjana Prifti Skenduli, Arshia Soltani Moakhar, Bardia Soltani Moakhar, Ran Tamir, Ayush Kumar Tarun, Azmine Toushik Wasi, Thenuka Ovin Weerasinghe, Serhan Yilmaz, Mike Zhang, Imanol Schlag, Marzieh Fadaee, Sara Hooker, Antoine Bosselut
- Reference count: 40
- Primary result: Introduces INCLUDE, a multilingual benchmark with 197,243 MCQA from local exam sources across 44 languages, revealing significant performance gaps in LLMs for region-specific knowledge

## Executive Summary
This paper introduces INCLUDE, a new multilingual benchmark designed to evaluate large language models (LLMs) across diverse linguistic and cultural contexts. The key problem addressed is the lack of high-quality evaluation resources for languages other than English, which limits the effective deployment of LLMs in many regions. To tackle this, the authors collect 197,243 multiple-choice questions (MCQA) from local exam sources across 44 languages, capturing regional and cultural knowledge. The benchmark includes two subsets: INCLUDE-BASE (22,635 samples) for comprehensive evaluation and INCLUDE-LITE (10,770 samples) for resource-constrained settings. Experiments with 15 models, including GPT-4o and various open-source models, show significant performance variance across languages, particularly for questions requiring regional knowledge. The results highlight the need for improved multilingual capabilities in LLMs, especially for region-specific understanding. The benchmark is publicly released to support future research and development in multilingual AI.

## Method Summary
The authors construct INCLUDE by collecting multiple-choice questions from local exam sources across 44 languages and 15 scripts, covering academic, professional, and occupational license exams. The dataset is stratified by academic domains and regional knowledge types, with questions categorized as region-agnostic or region-specific (further divided into region-explicit, cultural, and implicitly regional). Two benchmark subsets are created: INCLUDE-BASE with 22,635 samples for comprehensive evaluation and INCLUDE-LITE with 10,770 samples for resource-constrained settings. Models are evaluated using in-language and English prompts, with and without regional prefixes, and zero-shot or 5-shot Chain-of-Thought prompting. The evaluation leverages the Harness-Eval framework and considers generation output length variations to account for formatting differences across languages.

## Key Results
- INCLUDE-BASE and INCLUDE-LITE provide the first comprehensive multilingual evaluation covering 44 languages and 15 scripts with region-specific knowledge assessment.
- Models show significant performance variance across languages, with lower accuracy on questions requiring regional knowledge, especially in professional certification and cultural history domains.
- Increasing generation output length from 50 to 512 tokens improves performance by 3.1% overall, with some low-resource languages benefiting more (e.g., Uzbek +17.2%, Armenian +13.1%, Malayalam +12.9%).

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Creating evaluation benchmarks from native regional exam sources improves multilingual LLM performance by grounding assessments in culturally and linguistically relevant contexts.
- **Mechanism**: Regional exam sources are developed by native speakers within their linguistic and cultural environments, inherently embedding local knowledge and context. By using these native materials instead of translated datasets, evaluation captures genuine regional understanding and reduces biases introduced by translationese artifacts.
- **Core assumption**: Native exam content accurately reflects the regional and cultural knowledge necessary for practical deployment of LLMs in those regions.
- **Evidence anchors**:
  - [abstract] "collect 197,243 multiple-choice questions (MCQA) from local exam sources across 44 languages, capturing regional and cultural knowledge"
  - [section 2] "Language represents regional knowledge... regional knowledge as the specific information, culture, and practices related to a local environment"
  - [corpus] Weak evidence: limited peer-reviewed validation of exam source authenticity across all 44 languages.
- **Break condition**: If native exam sources are outdated, biased, or do not represent current linguistic and cultural norms, the benchmark fails to reflect realistic deployment contexts.

### Mechanism 2
- **Claim**: Stratified sampling across academic domains and regional knowledge types enables targeted assessment of LLM capabilities beyond general language proficiency.
- **Mechanism**: By categorizing questions into region-agnostic, region-explicit, cultural, and implicitly regional groups, the benchmark isolates different types of knowledge and reasoning skills. This stratification allows detection of performance gaps tied specifically to regional knowledge rather than language comprehension alone.
- **Core assumption**: Performance differences across these categories reliably indicate model strengths and weaknesses in handling regional versus universal knowledge.
- **Evidence anchors**:
  - [section 3.2] "categorize exam questions into two major groups: region-agnostic and region-specific knowledge... further divided into three sub-categories"
  - [section 5.3] "lower performance in certain languages is often linked to questions requiring regional knowledge"
  - [corpus] Weak evidence: coarse-grained labeling at the exam source level may misclassify individual question regionality.
- **Break condition**: If regional labels do not align with actual question content, performance analysis misattributes errors to regional knowledge gaps.

### Mechanism 3
- **Claim**: Varying generation output lengths during evaluation mitigates format inconsistencies across languages, improving reliability of multilingual assessment.
- **Mechanism**: Multilingual LLMs may generate verbose or differently formatted responses depending on language and script. Allowing longer output windows captures correct answers that might otherwise be truncated or misformatted, especially for low-resource or structurally distinct languages.
- **Core assumption**: Extended generation windows recover otherwise missed correct responses without introducing noise from unrelated content.
- **Evidence anchors**:
  - [section 5.4] "model shows a 3.1% performance improvement when increasing the generation length window from 50 to 512 tokens"
  - [section 5.4] "some languages experience significant improvements, such as Uzbek (+17.2%), Armenian (+13.1%), and Malayalam (+12.9%)"
  - [corpus] Moderate evidence: quantitative improvement but limited qualitative explanation of why some languages benefit more.
- **Break condition**: If longer generations introduce irrelevant content or increase evaluation noise, gains may not reflect true model capability.

## Foundational Learning

- **Concept**: Regional knowledge classification
  - **Why needed here**: To distinguish between questions requiring universal knowledge and those dependent on specific cultural or geographical context, enabling targeted evaluation.
  - **Quick check question**: Can you explain the difference between region-explicit and cultural question types?

- **Concept**: Multilingual data contamination detection
  - **Why needed here**: To assess whether models inadvertently learned from the evaluation data, which would invalidate performance results.
  - **Quick check question**: What method does the paper use to estimate training data contamination rates?

- **Concept**: Chain-of-thought prompting in multilingual settings
  - **Why needed here**: To understand how reasoning prompts affect performance across languages and whether language of instruction matters.
  - **Quick check question**: Does the paper find significant benefits to using in-language versus English prompts?

## Architecture Onboarding

- **Component map**: Data ingestion pipeline → Native exam parsing and quality control → Regionality annotation → Stratified sampling → Model evaluation harness → Results aggregation and analysis
- **Critical path**: Data collection and parsing → Accurate regional knowledge labeling → Balanced stratified sampling → Consistent evaluation across languages → Detailed performance analysis stratified by language and knowledge type
- **Design tradeoffs**: Broader language coverage vs. depth of regional knowledge assessment per language; Native source authenticity vs. ease of data curation and standardization; Longer output windows for accuracy vs. increased computational cost and potential noise
- **Failure signatures**: Performance disparities unexplained by language proficiency may indicate regional knowledge gaps; High formatting errors in certain languages suggest generation length or instruction language issues; Inconsistent results across prompting settings signal sensitivity to evaluation configuration
- **First 3 experiments**:
  1. Run GPT-4o on INCLUDE-BASE with 5-shot in-language prompting; record per-language accuracy and regional knowledge breakdowns.
  2. Repeat with extended output window (512 tokens) to quantify improvement for low-resource languages.
  3. Test the same model with zero-shot chain-of-thought prompting in English; compare performance shifts and formatting errors.

## Open Questions the Paper Calls Out
- The paper does not explicitly call out open questions but identifies several areas for future work, including deeper analysis of format errors across languages, investigation of factors contributing to performance variance in region-specific questions, and exploration of how academic domain distribution within languages influences evaluation results.

## Limitations
- The regional knowledge annotation process relies on coarse-grained categorization at the exam source level rather than individual question analysis, which may introduce noise in the regionality classification.
- The contamination check methodology is not fully detailed, raising questions about potential training data overlap despite authors' efforts.
- The performance improvements from extended generation windows, while quantitatively significant, lack qualitative analysis of why certain languages benefit more than others.

## Confidence
- High confidence: The benchmark construction methodology and dataset release are well-documented and reproducible.
- Medium confidence: The core finding that regional knowledge affects performance is supported but could benefit from finer-grained analysis.
- Medium confidence: The contamination check methodology, while present, lacks sufficient detail for full validation.

## Next Checks
1. Conduct a fine-grained manual annotation of a random sample of questions to verify the accuracy of regional knowledge classification at the individual question level.
2. Perform an ablation study varying generation window sizes systematically across all languages to identify threshold effects and language-specific patterns.
3. Test the benchmark with newer, larger multilingual models (e.g., GPT-4 Turbo, Claude 3) to establish whether observed performance gaps persist with state-of-the-art systems.