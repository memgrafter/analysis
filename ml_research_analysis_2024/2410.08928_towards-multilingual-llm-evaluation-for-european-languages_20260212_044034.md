---
ver: rpa2
title: Towards Multilingual LLM Evaluation for European Languages
arxiv_id: '2410.08928'
source_url: https://arxiv.org/abs/2410.08928
tags:
- languages
- table
- b-instruct
- accuracy
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of evaluating Large Language
  Models (LLMs) across multiple European languages. The authors propose a multilingual
  evaluation framework using translated versions of five widely-used benchmarks (MMLU,
  HellaSwag, ARC, TruthfulQA, GSM8K) into 20 European languages.
---

# Towards Multilingual LLM Evaluation for European Languages

## Quick Facts
- **arXiv ID**: 2410.08928
- **Source URL**: https://arxiv.org/abs/2410.08928
- **Reference count**: 40
- **Primary result**: Translated benchmarks can serve as a reliable proxy for human preference in evaluating LLM performance across 20 European languages

## Executive Summary
This paper addresses the critical challenge of evaluating Large Language Models across multiple European languages by proposing a multilingual evaluation framework using translated versions of five widely-used benchmarks. The authors translate MMLU, HellaSwag, ARC, TruthfulQA, and GSM8K into 20 European languages and evaluate 40 different LLMs, finding that translated benchmarks effectively correlate with human preferences. The study reveals that language family significantly impacts LLM performance, with Romance and Germanic languages showing higher accuracy than Slavic languages, and identifies Gemma-2-27b-Instruct and Meta-Llama-3.1-70B-Instruct as top-performing models across most languages and tasks.

## Method Summary
The authors create multilingual benchmarks by translating five English-language evaluation datasets (MMLU, HellaSwag, ARC, TruthfulQA, GSM8K) into 20 European languages using DeepL. They evaluate 40 different LLMs using the LM Evaluation Harness framework, computing accuracy scores across all languages and tasks. The evaluation includes correlation analysis with human preference data from LMSYS Chatbot Arena (Elo scores) to validate the effectiveness of translated benchmarks, as well as translation quality assessment using COMET-KIWI scores to compare different translation services.

## Key Results
- Translated benchmarks show strong correlation with human Elo scores across all 20 languages, with Pearson correlations ranging from 0.616 to 0.895
- Gemma-2-27b-Instruct and Meta-Llama-3.1-70B-Instruct demonstrate the highest overall performance across most tasks and languages
- Language family significantly affects performance, with Romance and Germanic languages showing 3-4% higher accuracy than Slavic languages
- Translation quality (COMET-KIWI scores 0.82-0.82) is sufficient to maintain benchmark reliability across languages

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Translated benchmarks can serve as a reliable proxy for human preference in evaluating LLM performance across European languages.
- Mechanism: Human preference data from LMSYS Chatbot Arena (Elo scores) strongly correlates with model accuracy on translated benchmarks, indicating that these benchmarks effectively capture the same performance characteristics that humans value.
- Core assumption: The Elo score accurately reflects human preference across multiple languages and tasks, and the translated benchmarks preserve the semantic and task-specific content needed for accurate evaluation.
- Evidence anchors:
  - [abstract]: "The results show that translated benchmarks can serve as a reliable proxy for human preference in various languages"
  - [section]: "The high correlations across all languages highlight the strong relationship between Elo scores and the average benchmark accuracies. The highest Pearson and Spearman correlation is in Polish ( r = .895, ρ = .858). Portuguese shows similar values (r = .811, ρ = .806), while English exhibits lower correlations (Pearson r = .716, p = .00062; Spearman ρ = .618, p = .00412)."
  - [corpus]: Weak evidence; no direct corpus comparison provided for human preference correlation.

### Mechanism 2
- Claim: Language family influences LLM performance, with higher accuracy on Romance and Germanic languages compared to Slavic languages.
- Mechanism: LLMs trained on CommonCrawl data show performance patterns that correlate with the proportion of training data available for each language family, with Romance and Germanic languages having more representation than Slavic languages.
- Core assumption: The distribution of languages in the CommonCrawl dataset directly influences model performance on those languages, and this relationship is consistent across different task types.
- Evidence anchors:
  - [section]: "Examining the average accuracies across the three language families, we observe that models generally achieve higher performance on Romance and Germanic languages compared to Slavic languages. For instance, the Gemma-2-27b-Instruct model attains average accuracies of 0.727 on Germanic languages, 0.716 on Romance languages, and 0.694 on Slavic languages."
  - [section]: "Both EU21-ARC and EU21-Hellaswag are benchmarks that focus on reasoning, commonsense knowledge, and context understanding. These tasks might require more language specific knowledge, which may not be as well represented in languages with fewer speakers or less CommonCrawl data."
  - [corpus]: Moderate evidence; the corpus shows some papers discussing multilingual evaluation but limited direct evidence about language family performance differences.

### Mechanism 3
- Claim: Translation service choice (DeepL vs ChatGPT) impacts benchmark quality and model evaluation results.
- Mechanism: Different translation services produce varying quality translations that affect model performance measurements, with some services better preserving task-specific nuances and semantic content.
- Core assumption: Translation quality directly impacts model evaluation accuracy, and different translation services have systematic differences in how they handle task-specific content and linguistic nuances.
- Evidence anchors:
  - [section]: "In this section, we present a correlation analysis with a focus on the EU20-ARC, EU20-HellaSwag, and EU20-MMLU tasks, alongside a COMET score (Rei et al., 2023) evaluation to assess the translation quality across both datasets."
  - [section]: "Both our and OKAPI datasets have a relatively high average score of 0.8208 and 0.8190, respectively, across all tasks and translations."
  - [corpus]: Weak evidence; no direct corpus comparison provided for translation service impact.

## Foundational Learning

- Concept: Multilingual evaluation methodology
  - Why needed here: Understanding how to create and evaluate benchmarks across multiple languages is fundamental to this work's approach and findings.
  - Quick check question: What are the key challenges in creating multilingual benchmarks that can fairly evaluate LLM performance across different language families?

- Concept: Correlation analysis and statistical significance
  - Why needed here: The paper relies heavily on correlation coefficients between Elo scores and benchmark accuracies to validate the effectiveness of translated benchmarks.
  - Quick check question: How do Pearson and Spearman correlation coefficients differ, and what do their values indicate about the relationship between human preference and benchmark performance?

- Concept: Language resource distribution and its impact on model performance
  - Why needed here: The paper discusses how the proportion of languages in training corpora (like CommonCrawl) affects model performance across different language families.
  - Quick check question: How does the availability of training data for different languages in a corpus like CommonCrawl influence the performance of LLMs on those languages?

## Architecture Onboarding

- Component map: Translated benchmark datasets (EU20-MMLU, EU20-HellaSwag, EU20-ARC, EU20-TruthfulQA, EU20-GSM8K) -> Model evaluation using LM Evaluation Harness -> Correlation analysis with LMSYS Elo scores -> Translation quality assessment using COMET scores -> Benchmark release

- Critical path: Data translation → Model evaluation → Correlation analysis → Quality assessment → Benchmark release. The most critical step is ensuring translation quality, as this directly impacts all downstream evaluation and correlation results.

- Design tradeoffs: Using translated benchmarks enables scalable multilingual evaluation but may introduce quality issues compared to human-generated benchmarks. The choice of translation service (DeepL vs ChatGPT) affects translation quality and evaluation consistency. Focusing on European languages limits generalizability but allows deeper analysis of language family effects.

- Failure signatures: Low correlation between Elo scores and benchmark accuracies indicates translation quality issues or fundamental differences in what humans value vs what benchmarks measure. Performance discrepancies between language families suggest imbalanced training data distribution. Inconsistent results across translation services indicate sensitivity to translation quality.

- First 3 experiments:
  1. Replicate correlation analysis using different subsets of models to verify robustness of findings
  2. Compare model performance on original vs translated benchmarks for a subset of languages to quantify translation impact
  3. Analyze performance differences between language families using models trained on different data distributions to isolate language family effects from training data effects

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do translated benchmarks perform compared to human-annotated benchmarks across different task types and language families?
- Basis in paper: [explicit] The authors mention that translated benchmarks can serve as a reliable proxy for human preference in various languages, but note that some tasks like WinoGrande become easier due to grammatical signals in gendered languages
- Why unresolved: The paper doesn't provide a comprehensive comparison between translated and human-annotated benchmarks across all language families and task types, only mentioning specific problematic examples
- What evidence would resolve it: A systematic comparison of translated vs. human-annotated benchmarks across all language families and task types, with statistical analysis of performance differences

### Open Question 2
- Question: How does translation quality impact LLM performance across different languages, and can this impact be quantified?
- Basis in paper: [explicit] The authors use COMET-KIWI scores to assess translation quality and note that ambiguous or incorrect translations can negatively affect model performance, but don't establish a quantitative relationship
- Why unresolved: While translation quality is assessed using COMET-KIWI, the paper doesn't establish a clear quantitative relationship between translation quality scores and model performance
- What evidence would resolve it: A regression analysis showing the relationship between COMET-KIWI scores and model performance across different languages and tasks

### Open Question 3
- Question: How do different translation services (DeepL vs. ChatGPT) affect LLM performance, and what are the specific linguistic challenges each service faces?
- Basis in paper: [explicit] The authors compare DeepL and ChatGPT translations and note differences in performance, but don't provide a detailed linguistic analysis of each service's strengths and weaknesses
- Why unresolved: The paper identifies differences in performance between translation services but doesn't provide a detailed linguistic analysis of the specific challenges each service faces with different language families
- What evidence would resolve it: A linguistic analysis of translation quality for different language families, identifying specific challenges each translation service faces with different language structures

### Open Question 4
- Question: What is the optimal balance between model size and multilingual performance, and how does this vary across language families?
- Basis in paper: [explicit] The authors observe that Gemma-2-27b-Instruct performs well across most languages and tasks, but don't systematically analyze the relationship between model size and multilingual performance
- Why unresolved: While the paper identifies that larger models generally perform better, it doesn't systematically analyze how this relationship varies across different language families or identify an optimal model size for multilingual tasks
- What evidence would resolve it: A comprehensive analysis of model size vs. multilingual performance across all language families, identifying optimal model sizes for different linguistic groups

### Open Question 5
- Question: How do cultural and linguistic nuances in translations affect LLM performance, and can these be systematically addressed?
- Basis in paper: [inferred] The authors mention the importance of cultural localization in translations but don't provide a systematic approach to addressing cultural and linguistic nuances
- Why unresolved: The paper acknowledges the importance of cultural localization but doesn't provide a systematic framework for addressing cultural and linguistic nuances in translations
- What evidence would resolve it: A framework for systematically identifying and addressing cultural and linguistic nuances in translations, with empirical evidence of its impact on LLM performance

## Limitations

- Translation quality dependency: The core finding that translated benchmarks correlate with human preference relies heavily on translation quality, which is only assessed using COMET-KIWI scores without human validation
- Data distribution bias: The paper assumes training data distribution in CommonCrawl is the primary driver of language family performance differences without direct evidence linking these factors
- Benchmark representativeness: The focus on academic-style tasks may not capture performance differences that would emerge in practical multilingual use cases

## Confidence

**High Confidence**: The finding that Gemma-2-27b-Instruct and Meta-Llama-3.1-70B-Instruct show strong performance across most tasks and languages is well-supported by the evaluation results across multiple benchmarks.

**Medium Confidence**: The claim that translated benchmarks serve as a reliable proxy for human preference has moderate support from correlation analysis, but this confidence is tempered by the lack of direct human evaluation of translation quality.

**Low Confidence**: The assertion that language family performance differences are primarily due to training data distribution in CommonCrawl is speculative and lacks direct evidence.

## Next Checks

1. **Human Translation Validation Study**: Conduct a small-scale human evaluation comparing translations of 50-100 benchmark items across different language families to quantify translation quality differences and their impact on model performance.

2. **Training Data Analysis**: Perform a detailed analysis of the actual training data distribution for the evaluated models, mapping the proportion of each language family in their pretraining corpora.

3. **Cross-Benchmark Consistency Test**: Evaluate model performance on human-generated multilingual benchmarks (where available) alongside the translated benchmarks to assess whether performance patterns remain consistent across different evaluation methodologies.