---
ver: rpa2
title: 'How much data do you need? Part 2: Predicting DL class specific training dataset
  sizes'
arxiv_id: '2403.06311'
source_url: https://arxiv.org/abs/2403.06311
tags:
- training
- dataset
- size
- data
- number
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a method for predicting deep learning model
  performance by considering class-specific training dataset sizes, rather than just
  the overall dataset size. The authors address the combinatorial challenge of selecting
  which class-count combinations to evaluate by introducing an algorithm motivated
  by space-filling mixture designs.
---

# How much data do you need? Part 2: Predicting DL class specific training dataset sizes

## Quick Facts
- arXiv ID: 2403.06311
- Source URL: https://arxiv.org/abs/2403.06311
- Reference count: 29
- Primary result: Proposes class-specific training dataset size prediction using space-filling mixture designs

## Executive Summary
This paper addresses the challenge of predicting deep learning model performance by considering class-specific training dataset sizes rather than just overall dataset size. The authors introduce an algorithm motivated by space-filling mixture designs to generate diverse training subsets, allowing for fitting non-linear models that incorporate class counts and epochs as predictors. Experiments on CIFAR10 and EMNIST datasets demonstrate that the proposed class-specific models outperform standard total-size-only approaches, achieving higher R² values (up to ~96% for CIFAR10) and better interpretability of class importance.

## Method Summary
The method generates diverse training subsets using a space-filling mixture design algorithm, ensuring each subset has a different distribution of class counts while maintaining fixed total size. Non-linear models (powerlaw and arctan functions) are fitted to these subsets, incorporating class-specific parameters that weight each class's contribution to overall performance. For datasets with many classes, forward selection is used to iteratively add predictive features. The approach captures how individual class counts affect model performance and allows estimation of class-specific importance.

## Key Results
- Class-specific models outperform total-size-only approaches, achieving R² up to ~96% on CIFAR10
- The arctan model with class-specific parameters provides the best performance prediction
- Forward selection effectively reduces dimensionality for high-class-count datasets like EMNIST
- Class-specific modeling reveals differential importance of training images from each class

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Class-specific training data allocation can be predicted using space-filling mixture designs to generate diverse training subsets.
- Mechanism: The algorithm creates constrained mixture designs that ensure each generated training subset has a different distribution of class counts while maintaining a fixed total size. This allows fitting models that capture how individual class counts affect model performance.
- Core assumption: The performance function g(·) depends on individual class counts in a way that can be approximated by linear combinations of class-specific effects plus epoch effects.
- Evidence anchors:
  - [abstract] "an algorithm is suggested which is motivated from special cases of space filling design of experiments"
  - [section] "an algorithm is suggested in the following. It takes motivation from a special case of statistical design of experiments, notably constrained space filling mixture designs"
  - [corpus] Weak evidence - no directly related papers found on space-filling mixture designs for ML
- Break condition: When the relationship between class counts and performance becomes too nonlinear for the chosen model family (powerlaw, arctan, etc.)

### Mechanism 2
- Claim: Non-linear models with class-specific parameters outperform total-size-only approaches for predicting model performance.
- Mechanism: Models like powerlaw and arctan functions are extended to include class-specific parameters (βc) that weight each class's contribution to overall performance. This captures class-specific difficulty and importance.
- Core assumption: Different classes contribute differently to model performance, and this differential importance can be captured by fitting individual parameters for each class.
- Evidence anchors:
  - [abstract] "the proposed models (especially those using class-specific counts with arctan effects) outperform standard total-size-only approaches"
  - [section] "Hence here an approach is taken, which allows to estimate the importance of training images from each class individually"
  - [corpus] No strong evidence found in neighboring papers about class-specific performance modeling
- Break condition: When class distributions are highly imbalanced or when classes are highly correlated in their difficulty

### Mechanism 3
- Claim: Forward selection can effectively reduce dimensionality when modeling datasets with many classes.
- Mechanism: For datasets with many classes (like EMNIST with 47 classes), forward selection is used to iteratively add features that minimize residual sum of squares, starting from an empty model.
- Core assumption: Only a subset of all possible class-specific features (including interactions) are actually predictive of model performance.
- Evidence anchors:
  - [section] "Hence, an interactive modeling approach has been chosen here, specifically a forward selection approach using a residual sum of squares as selection criterion"
  - [corpus] No direct evidence in neighboring papers about forward selection for high-dimensional class features
- Break condition: When important features are highly correlated, causing forward selection to miss some predictive interactions

## Foundational Learning

- Concept: Space-filling design of experiments
  - Why needed here: To generate diverse training subsets with different class distributions while maintaining fixed total size
  - Quick check question: What is the main goal of space-filling designs in experimental design?

- Concept: Generalized linear models
  - Why needed here: To extend standard performance prediction models to include class-specific parameters
  - Quick check question: How does replacing total dataset size with a linear combination of class counts change the model structure?

- Concept: Non-linear least squares fitting
  - Why needed here: To fit the proposed performance prediction models (powerlaw, arctan, etc.) to the generated data
  - Quick check question: What optimization method is used to fit the non-linear models in this paper?

## Architecture Onboarding

- Component map:
  - Data generation module (space-filling mixture design algorithm)
  - Model fitting module (non-linear regression with class-specific parameters)
  - Evaluation module (prediction plots, R² calculation)
  - Forward selection module (for high-dimensional class problems)

- Critical path:
  1. Generate diverse training subsets using mixture design algorithm
  2. Train models on each subset and collect performance metrics
  3. Fit non-linear models with class-specific parameters
  4. Evaluate model predictions against held-out data

- Design tradeoffs:
  - More diverse subsets → better model fitting but higher computational cost
  - More class-specific parameters → better performance prediction but risk of overfitting
  - Forward selection → reduces dimensionality but may miss important interactions

- Failure signatures:
  - Poor R² values on test data indicate model misspecification
  - Unstable parameter estimates suggest overfitting
  - Forward selection failing to reduce residual sum of squares indicates all features may be needed

- First 3 experiments:
  1. Run the mixture design algorithm with small subset sizes to verify diverse class distributions are generated
  2. Fit a simple powerlaw model with only total dataset size to establish baseline performance
  3. Fit the extended model with class-specific parameters and compare R² to baseline

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do the proposed class-specific prediction models generalize to datasets with significantly more classes than CIFAR10 and EMNIST (e.g., 100+ classes)?
- Basis in paper: [inferred] The paper tested the method on CIFAR10 (10 classes) and EMNIST (47 classes), but the scalability to much larger class counts is not explored.
- Why unresolved: The paper does not provide empirical evidence or theoretical analysis for datasets with many more classes, and the forward selection method used for EMNIST may become computationally prohibitive.
- What evidence would resolve it: Experiments on datasets with 100+ classes showing model performance, computational cost, and prediction accuracy would clarify scalability.

### Open Question 2
- Question: Can the proposed algorithm for generating diverse training subsets be adapted to handle highly imbalanced datasets effectively?
- Basis in paper: [explicit] The discussion section mentions that the method could be modified for unbalanced datasets, but no such adaptation or evaluation is provided.
- Why unresolved: The current algorithm assumes balanced or nearly balanced datasets, and the impact of class imbalance on the design and model fitting is unexplored.
- What evidence would resolve it: Application of the algorithm to real-world imbalanced datasets with evaluation of both design diversity and prediction performance would demonstrate feasibility.

### Open Question 3
- Question: How sensitive are the model predictions to the choice of epochs at which performance is evaluated (e.g., every 5 epochs vs. other intervals)?
- Basis in paper: [inferred] The paper evaluates performance at fixed epoch intervals (e.g., every 5 epochs), but does not analyze how this choice affects prediction accuracy or model fitting.
- Why unresolved: The impact of evaluation frequency on capturing learning dynamics and prediction quality is not discussed or tested.
- What evidence would resolve it: Systematic experiments varying the evaluation interval and measuring resulting prediction accuracy and model fit would clarify sensitivity.

## Limitations
- The space-filling mixture design algorithm implementation details are not fully specified, making faithful reproduction difficult
- The method assumes balanced or nearly balanced datasets, with no evaluation on highly imbalanced data
- Critical parameter settings (ndoe, ncmax, nopt, ncandidate) for the mixture design algorithm are omitted

## Confidence

**Mechanism 1 (Space-filling designs): Medium** - algorithm motivation is clear but implementation details are missing
**Mechanism 2 (Class-specific parameters): Medium** - empirical results show improvement but theoretical justification is limited  
**Mechanism 3 (Forward selection): Low** - method is mentioned but not deeply explored or validated

## Next Checks

1. Implement the space-filling mixture design algorithm and verify it generates diverse class distributions across multiple runs
2. Compare R² performance of class-specific models against total-size-only models on held-out data using cross-validation
3. Test forward selection stability by running it multiple times with different random seeds and measuring feature selection consistency