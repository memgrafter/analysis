---
ver: rpa2
title: Optimizing Query Generation for Enhanced Document Retrieval in RAG
arxiv_id: '2407.12325'
source_url: https://arxiv.org/abs/2407.12325
tags:
- query
- retrieval
- queries
- score
- document
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses hallucinations in Retrieval-Augmented Generation
  (RAG) systems by optimizing query generation through query expansion and LLM-based
  refinement. The proposed QOQA method generates concrete, precise queries by leveraging
  a top-k averaged query-document alignment score to refine queries using LLMs, enhancing
  precision and computational efficiency in document retrieval.
---

# Optimizing Query Generation for Enhanced Document Retrieval in RAG

## Quick Facts
- arXiv ID: 2407.12325
- Source URL: https://arxiv.org/abs/2407.12325
- Reference count: 10
- Key result: 1.6% average gain in nDCG@10 scores for document retrieval accuracy

## Executive Summary
This paper addresses hallucinations in Retrieval-Augmented Generation (RAG) systems by optimizing query generation through query expansion and LLM-based refinement. The proposed QOQA method generates concrete, precise queries by leveraging a top-k averaged query-document alignment score to refine queries using LLMs, enhancing precision and computational efficiency in document retrieval. Experiments on SciFact, Trec-Covid, and FiQA datasets demonstrate improved document retrieval accuracy with an average gain of 1.6% in nDCG@10 scores. The approach significantly reduces hallucinations by ensuring more accurate and relevant document retrieval.

## Method Summary
The Query Optimization using Query expAnsion (QOQA) method optimizes queries for RAG systems by expanding the original query with top-N retrieved documents and iteratively refining it using LLM-based rephrasing. The process involves generating R0 rephrased queries, scoring them based on their alignment with retrieved documents using BM25, dense, or hybrid scores, and selecting the top-K scored queries for further refinement. This iterative process continues for i steps, progressively improving query precision and reducing hallucinations by ensuring more relevant document retrieval.

## Key Results
- Achieved an average accuracy gain of 1.6% in nDCG@10 scores across SciFact, Trec-Covid, and FiQA datasets
- Demonstrated significant reduction in hallucinations through improved document retrieval precision
- Ablation studies confirmed the importance of both query expansion and optimization components

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Optimizing queries via top-k averaged query-document alignment score reduces hallucinations by ensuring retrieved documents are more relevant to the user's intent.
- Mechanism: The method retrieves N documents using the original query, then generates R0 rephrased queries with an LLM. These rephrased queries are scored based on their alignment with the retrieved documents using BM25, dense, or hybrid scores. The top K scored queries are used to refine the prompt for the next optimization step, generating Ri queries iteratively.
- Core assumption: Higher query-document alignment scores indicate better retrieval performance and reduced hallucination risk.
- Evidence anchors:
  - [abstract] "Experiments have shown that our approach improves document retrieval, resulting in an average accuracy gain of 1.6%."
  - [section] "This approach is computationally efficient and improves the precision of document retrieval, thereby reducing hallucinations."
  - [corpus] Weak evidence; related works mention query expansion and retrieval-augmented generation but lack direct evidence for hallucination reduction via alignment scoring.
- Break condition: If the alignment score does not correlate with actual relevance, or if the LLM generates misleading rephrased queries that do not improve alignment.

### Mechanism 2
- Claim: Query expansion using top-k averaged alignment scores enhances retrieval precision by providing more context to the LLM for rephrasing.
- Mechanism: The original query is expanded with the top N retrieved documents, creating a richer context. This expanded query is used to generate rephrased queries that are more precise and concrete, improving keyword matching with answer documents.
- Core assumption: Including retrieved documents in the query context helps the LLM generate more relevant and specific rephrased queries.
- Evidence anchors:
  - [section] "We concatenate the original query with the topN retrieved documents to create an expanded query, which is then sent to the LLM to generate R0 rephrased queries."
  - [section] "When searching for the answer document, queries generated with our QOQA method include precise keywords, such as 'nano' or 'molecular evidence,' to retrieve the most relevant documents."
  - [corpus] Weak evidence; related works discuss query expansion but do not specifically address the use of alignment scores for context enrichment.
- Break condition: If the expanded query introduces noise or irrelevant information, leading to less effective rephrased queries.

### Mechanism 3
- Claim: Iterative optimization using top-k scores progressively refines queries to maximize retrieval performance.
- Mechanism: After generating initial rephrased queries, the top K scored queries are used to update the prompt for the next iteration. This process continues for i iterations, each time generating Ri queries that are expected to have higher scores than previous iterations.
- Core assumption: Iterative refinement based on alignment scores leads to progressively better queries and improved retrieval accuracy.
- Evidence anchors:
  - [section] "We update the prompt template with the original query, the retrieved documents, and the top K rephrased queries... In the later optimization steps i, based on the scores, we generate a Ri rephrased query and add it to the query bucket."
  - [section] "The optimization step improves the search for better rephrased queries."
  - [corpus] Weak evidence; related works mention iterative processes but lack direct evidence for the effectiveness of top-k score-based refinement.
- Break condition: If the optimization process converges prematurely or generates queries that do not improve retrieval performance.

## Foundational Learning

- Concept: Query-document alignment scoring (BM25, dense, hybrid)
  - Why needed here: Understanding how different scoring methods evaluate the relevance between queries and documents is crucial for implementing and tuning the QOQA method.
  - Quick check question: What are the differences between BM25, dense, and hybrid scoring methods, and when would you choose one over the others?
- Concept: Large Language Model (LLM) prompt engineering
  - Why needed here: Crafting effective prompts for the LLM is essential for generating high-quality rephrased queries that improve retrieval performance.
  - Quick check question: How does the inclusion of retrieved documents and top-k scored queries in the prompt influence the LLM's output?
- Concept: Retrieval-augmented generation (RAG) and hallucination mitigation
  - Why needed here: Understanding the role of RAG in reducing hallucinations and the importance of accurate query generation is fundamental to appreciating the QOQA method's contributions.
  - Quick check question: How does improving query precision contribute to reducing hallucinations in RAG systems?

## Architecture Onboarding

- Component map: Original query -> Retriever -> Query Expander -> LLM Optimizer -> Alignment Scorer -> Query Bucket -> Iterative refinement -> Final optimized query
- Critical path: Original query → Retriever → Query Expander → LLM Optimizer → Alignment Scorer → Query Bucket → Iterative refinement → Final optimized query
- Design tradeoffs:
  - Computational cost vs. retrieval accuracy: More iterations and larger N/K values improve accuracy but increase computational cost.
  - Score choice (BM25, dense, hybrid): Each has strengths and weaknesses depending on the dataset and query type.
  - LLM selection: Different LLMs may generate varying quality rephrased queries, affecting performance.
- Failure signatures:
  - Low alignment scores despite high relevance: Indicates a mismatch between the scoring method and actual relevance.
  - Degrading performance over iterations: Suggests the optimization process is not converging correctly.
  - Hallucinations persist: Implies that the query optimization is not sufficiently improving retrieval accuracy.
- First 3 experiments:
  1. Baseline comparison: Run the original query through the retriever without any optimization and measure nDCG@10 scores.
  2. Single iteration test: Apply one iteration of the QOQA method and compare performance to the baseline.
  3. Ablation study: Remove the expansion component and the optimization component separately to assess their individual contributions to performance.

## Open Questions the Paper Calls Out
- Open Question 1: How does the choice of alignment score (BM25, Dense, or Hybrid) impact the overall performance of the QOQA method across different domains and query types?
- Open Question 2: What is the optimal number of optimization iterations (i) and the size of the query bucket (N) for maximizing retrieval performance without incurring excessive computational costs?
- Open Question 3: How does the QOQA method perform when integrated with more advanced query refinement techniques, such as continuous learning or adaptive query expansion?

## Limitations
- The method's performance improvement (1.6% nDCG@10 gain) may not be sufficient to conclusively demonstrate significant hallucination reduction in real-world applications.
- The paper does not provide detailed implementation specifics for the LLM optimization step, which could hinder reproducibility.
- The generalizability of the method across different datasets and query types is not extensively explored.

## Confidence
- High Confidence: The core mechanism of using LLM-based query refinement to improve document retrieval precision is well-supported by the described iterative process and alignment scoring.
- Medium Confidence: The claim that the method significantly reduces hallucinations is supported by the improved retrieval accuracy, but the paper does not provide direct evidence linking query optimization to hallucination reduction.
- Low Confidence: The generalizability of the method across different datasets and query types is not extensively explored.

## Next Checks
1. Conduct a controlled study to measure the reduction in hallucinations before and after applying the QOQA method, using human evaluators to assess the relevance and accuracy of retrieved documents in response to user queries.
2. Perform an analysis to determine the correlation between the top-k averaged query-document alignment scores and actual document relevance, particularly in cases where the scoring method may not capture semantic nuances.
3. Test the QOQA method on a broader range of datasets, including those from different domains and with varying query complexities, to assess its robustness and adaptability to diverse retrieval tasks.