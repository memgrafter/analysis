---
ver: rpa2
title: 'nEMO: Dataset of Emotional Speech in Polish'
arxiv_id: '2404.06292'
source_url: https://arxiv.org/abs/2404.06292
tags:
- emotional
- speech
- dataset
- nemo
- emotion
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work presents nEMO, a novel emotional speech dataset in Polish
  with over 3 hours of audio recordings from 9 actors portraying 6 emotions (anger,
  fear, happiness, sadness, surprise, neutral) across 90 phonetically diverse sentences.
  The dataset aims to fill the research gap in emotional speech corpora for Slavic
  languages.
---

# nEMO: Dataset of Emotional Speech in Polish

## Quick Facts
- arXiv ID: 2404.06292
- Source URL: https://arxiv.org/abs/2404.06292
- Authors: Iwona Christop
- Reference count: 0
- Primary result: 83.95% accuracy using Random Forest classifier with MFCC features

## Executive Summary
This paper introduces nEMO, a novel emotional speech dataset for Polish containing over 3 hours of audio from 9 actors portraying 6 emotions across 90 phonetically diverse sentences. The dataset addresses the scarcity of emotional speech resources for Slavic languages and aims to support research in speech emotion recognition (SER). The authors evaluated the dataset using traditional machine learning classifiers (SVM, Logistic Regression, Random Forest) with MFCC features, achieving up to 83.95% accuracy. The dataset is publicly available under a Creative Commons license.

## Method Summary
The nEMO dataset was created by recording 9 actors (both professional and non-professional) speaking 90 phonetically diverse sentences in 6 emotional states: anger, fear, happiness, sadness, surprise, and neutral. The linguistic content was carefully selected to represent uncommon Polish phonemes. Recordings were made in home environments using professional microphones with noise reduction. The data was normalized to 0 dB peak amplitude and downsampled to 24 kHz with 16 bits per sample. The authors used SVM, Logistic Regression, and Random Forest classifiers with MFCC features for evaluation, employing an 80:20 random split of the data for training and testing.

## Key Results
- Dataset contains 3 hours of audio recordings from 9 actors
- Achieved 83.95% accuracy using Random Forest classifier
- Successfully distinguishes 6 emotional states in Polish speech
- Human evaluation confirmed high quality of emotional expressions

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Professional actors with specific instructions to avoid exaggerated emotional expressions improves naturalness and authenticity
- Mechanism: Professional actors can convey genuine emotional states through controlled vocal modulation, reducing artificial distortion
- Core assumption: Non-professional actors can deliver authentic emotional expressions with proper guidance
- Evidence anchors: Analysis revealed no significant differences between professionals and amateurs, both audibly and in spectral data analysis

### Mechanism 2
- Claim: Careful selection of phonetically diverse sentences ensures adequate representation of Polish phonetics
- Mechanism: By identifying 90 uncommon phonemes and creating sentences including them, the dataset captures wide phonetic variations
- Core assumption: Selected sentences are semantically correct and suitable for everyday conversations
- Evidence anchors: Linguistic content was prepared to sufficiently represent the phonetics of the Polish language

### Mechanism 3
- Claim: MFCC features and basic ML classifiers demonstrate dataset effectiveness for SER tasks
- Mechanism: MFCCs capture essential speech characteristics, and chosen classifiers can learn patterns associated with different emotional states
- Core assumption: Extracted MFCC features are sufficient to distinguish between six emotional states
- Evidence anchors: Random Forest achieved highest accuracy at 83.95%, with confusion matrices showing classifier performance

## Foundational Learning

- Concept: Emotional speech and its importance in human-computer interaction
  - Why needed here: Understanding the role of emotional speech in dialogue systems helps contextualize the need for nEMO
  - Quick check question: What are some potential applications of speech emotion recognition mentioned in the paper?

- Concept: Dataset creation and evaluation methods
  - Why needed here: Knowing the steps involved in creating a speech dataset is crucial for understanding nEMO's development process
  - Quick check question: What are the three categories of emotional speech datasets based on the method used to acquire recordings?

- Concept: Machine learning techniques for speech emotion recognition
  - Why needed here: Familiarity with the machine learning methods used to evaluate the dataset is essential for interpreting results
  - Quick check question: Which classifier achieved the highest accuracy in the evaluation of the nEMO dataset?

## Architecture Onboarding

- Component map: Recording setup -> Data collection (9 actors, 90 sentences, 6 emotions) -> Data preprocessing (normalization, downsampling) -> ML pipeline (MFCC extraction + SVM/LogReg/Random Forest) -> Evaluation (80:20 split)

- Critical path: The recording setup and data collection process is critical, as it directly impacts the quality and authenticity of emotional speech samples

- Design tradeoffs:
  - Professional vs. non-professional actors to balance naturalness and control over emotional expressions
  - Simulated approach vs. natural/semi-natural approaches to ensure full control over copyrights and emotional states
  - Basic machine learning methods vs. deep learning techniques for interpretable results and comparison with existing datasets

- Failure signatures:
  - Low classifier accuracy indicating insufficient distinction between emotional states or poor quality of recordings
  - High confusion between specific emotional states (e.g., surprise) suggesting issues with corresponding samples or feature extraction
  - Inconsistent emotional portrayals across actors or recordings, pointing to problems with guidance during data collection

- First 3 experiments:
  1. Replicate the evaluation using the same machine learning methods and feature extraction techniques to verify reported results
  2. Perform a speaker-independent split of the data and evaluate classifier performance to assess dataset's generalization capabilities
  3. Compare the performance of the nEMO dataset with other existing emotional speech datasets using the same evaluation setup to benchmark its effectiveness

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do non-professional actors' emotional portrayals in the nEMO dataset compare to professional actors in terms of classification accuracy?
- Basis in paper: The paper states "Although concerns were raised about the adequacy of emotional expression by non-actors, analysis revealed no significant differences between professionals and amateurs, both audibly and in spectral data analysis."
- Why unresolved: The paper mentions the evaluation but doesn't provide specific performance metrics comparing professional vs non-professional actors
- What evidence would resolve it: Classification accuracy results broken down by actor type (professional vs non-professional)

### Open Question 2
- Question: Would speaker-independent train-test splits significantly impact the classification performance of the nEMO dataset?
- Basis in paper: The paper states "It is important to note that the splits were not speaker-independent."
- Why unresolved: The paper acknowledges this limitation but doesn't explore how speaker-independent splits would affect results
- What evidence would resolve it: Classification accuracy results using speaker-independent train-test splits

### Open Question 3
- Question: How does the nEMO dataset perform on emotions with similar arousal levels, and what specific acoustic features contribute to these classification challenges?
- Basis in paper: The paper states "The errors made by both models were mainly related to misclassification between emotions with similar levels of arousal."
- Why unresolved: While the paper identifies this challenge, it doesn't analyze which specific acoustic features might help distinguish these similar emotions
- What evidence would resolve it: Detailed confusion matrix analysis with feature importance rankings for emotion pairs with similar arousal levels

## Limitations

- Dataset size is relatively small (3 hours, 9 actors) compared to larger multilingual emotional speech corpora
- All recordings were made in home environments, introducing uncontrolled acoustic variability
- Evaluation did not include speaker-independent testing, limiting assessment of generalization
- No comparison with other emotional speech datasets to benchmark relative performance

## Confidence

- High confidence: The dataset creation methodology and phonetic diversity claims, supported by detailed procedural descriptions and clear evidence anchors
- Medium confidence: The effectiveness of the dataset for SER tasks, as the evaluation used relatively basic methods that may not capture the full potential of the data
- Medium confidence: The claim about professional vs. non-professional actor performance being equivalent, based on the authors' spectral analysis and subjective assessment

## Next Checks

1. **Speaker-independent evaluation**: Re-split the dataset to ensure speakers in training and test sets are completely disjoint, then re-evaluate classifier performance to assess true generalization capability

2. **Deep learning baseline**: Apply modern deep learning approaches (CNNs, Transformers) to establish whether the dataset's performance ceiling exceeds the 83.95% baseline achieved with traditional ML methods

3. **Cross-corpus validation**: Test whether classifiers trained on nEMO can successfully recognize emotions in other emotional speech datasets, and vice versa, to evaluate domain transferability