---
ver: rpa2
title: 'Survey of Bias In Text-to-Image Generation: Definition, Evaluation, and Mitigation'
arxiv_id: '2404.01030'
source_url: https://arxiv.org/abs/2404.01030
tags:
- bias
- gender
- arxiv
- biases
- images
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This survey provides the first comprehensive review of bias in
  text-to-image (T2I) generation models, covering 36 papers across three dimensions:
  gender, skintone, and geo-cultural bias. The review reveals that while gender and
  skintone biases are well-studied, geo-cultural bias remains under-explored.'
---

# Survey of Bias In Text-to-Image Generation: Definition, Evaluation, and Mitigation

## Quick Facts
- arXiv ID: 2404.01030
- Source URL: https://arxiv.org/abs/2404.01030
- Authors: Yixin Wan; Arjun Subramonian; Anaelia Ovalle; Zongyu Lin; Ashima Suvarna; Christina Chance; Hritik Bansal; Rebecca Pattichis; Kai-Wei Chang
- Reference count: 40
- Key outcome: First comprehensive review of bias in text-to-image generation models, revealing fragmented evaluation methods and ineffective mitigation strategies, while calling for human-centric approaches.

## Executive Summary
This survey provides the first comprehensive review of bias in text-to-image (T2I) generation models, covering 36 papers across three dimensions: gender, skintone, and geo-cultural bias. The review reveals that while gender and skintone biases are well-studied, geo-cultural bias remains under-explored. Most studies focus on occupational associations, neglecting other aspects like power dynamics and non-binary identities. Evaluation methods are fragmented, with no unified framework, and current mitigation strategies are often ineffective and lack robustness. The survey highlights the need for human-centric approaches to bias definition, evaluation, and mitigation to build fair and trustworthy T2I systems.

## Method Summary
The authors conducted a literature review of 36 papers collected through Google Scholar and arXiv, focusing on bias in text-to-image generation models. Papers were categorized by bias dimension (gender, skintone, geo-cultural), conceptualizations, evaluation methods, and mitigation strategies. The analysis identified research gaps, limitations, and proposed future directions for human-centric approaches to bias definition, evaluation, and mitigation in T2I systems.

## Key Results
- Gender and skintone biases are well-studied, but geo-cultural bias remains under-explored in T2I models
- Most bias studies focus on occupational associations, neglecting other aspects like power dynamics and non-binary identities
- Current mitigation strategies are often ineffective and lack robustness, with no unified evaluation framework existing

## Why This Works (Mechanism)

### Mechanism 1
- Claim: T2I models perpetuate gender and skintone stereotypes by default because they are trained on biased image-caption datasets
- Mechanism: Models learn spurious correlations between demographic descriptors and visual features from training data, leading to skewed default generations
- Core assumption: Training datasets contain disproportionate representations of certain demographics and occupations, which the model memorizes and reproduces
- Evidence anchors: "models magnify occupational gender bias in society" and "depict over 80% of 'inmates' with dark skin"

### Mechanism 2
- Claim: Bias evaluation is fragmented because no unified framework exists, leading to inconsistent measurement across studies
- Mechanism: Researchers use varied handcrafted prompts, classifiers, and metrics, making cross-study comparisons difficult and potentially missing certain bias dimensions
- Core assumption: Lack of standardization in bias definitions, datasets, and metrics prevents systematic understanding of model bias
- Evidence anchors: "evaluation datasets and metrics are scattered, with no unified framework for measuring biases"

### Mechanism 3
- Claim: Current mitigation strategies are ineffective because they are either not robust or not controllable
- Mechanism: Prompt-based interventions can "overshoot" or fail to follow instructions, while training-based approaches risk catastrophic forgetting or degraded quality
- Core assumption: Simple intervention prompts or fine-tuning without careful alignment can introduce new biases or reduce model performance
- Evidence anchors: "prompt-based approaches suffer from a lack of robustness and controllability, resulting in unstable behaviors"

## Foundational Learning

- Concept: Social bias and its impact
  - Why needed here: Understanding the types of harm (allocational and representational) caused by biased T2I outputs is essential for defining what needs to be measured and mitigated
  - Quick check question: What are the two main types of societal harms that bias in AI systems can cause, according to the survey?

- Concept: Text-to-image generation pipeline
  - Why needed here: Knowing how T2I models work (text encoder, diffusion, etc.) helps understand where bias can be introduced and how mitigation might be applied
  - Quick check question: What are the two main stages in the T2I generation process where bias can be addressed, as described in the survey?

- Concept: Evaluation metrics and their limitations
  - Why needed here: Recognizing the strengths and weaknesses of different evaluation approaches (classification vs. embedding-based, human vs. automated) is crucial for designing effective bias studies
  - Quick check question: What are the two main types of bias evaluation metrics identified in the survey, and what is a key limitation of classification-based methods?

## Architecture Onboarding

- Component map: Prompt → Text Encoder → Latent Space → Diffusion Sampling → Image Output → Evaluation (human or automated)
- Critical path: Input prompt flows through text encoder, latent space transformation, diffusion sampling, and generates output image for evaluation
- Design tradeoffs:
  - Debiasing during training vs. inference: Training is more thorough but resource-intensive; inference is lighter but less robust
  - Explicit vs. implicit demographic control: Explicit control is more controllable but may reduce naturalness; implicit control is subtler but less predictable
  - Automated vs. human evaluation: Automated is scalable but may inherit biases; human is accurate but slow and subjective
- Failure signatures:
  - Persistent demographic skew in default generations
  - Inconsistent responses to fairness prompts ("overshooting")
  - Degraded image quality or prompt alignment after debiasing
  - High variance in bias measurements across different evaluation methods
- First 3 experiments:
  1. Generate images from gender-neutral prompts and analyze demographic distribution using a standard classifier; compare to real-world statistics
  2. Apply a simple prompt intervention (e.g., "diverse and inclusive") to biased prompts and measure changes in demographic skew and image quality
  3. Fine-tune the text encoder with a fairness loss on a balanced dataset and evaluate bias reduction and any drop in text-image alignment

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can we develop a unified evaluation framework for measuring bias in T2I models that accounts for all dimensions (gender, skintone, geo-cultural) and conceptualizations?
- Basis in paper: The paper notes that "evaluation datasets and metrics are scattered, with no unified framework for measuring biases"
- Why unresolved: Current evaluation methods are fragmented, using different handcrafted prompts, datasets, and metrics
- What evidence would resolve it: Development and validation of a standardized benchmark dataset with diverse prompts covering all bias dimensions

### Open Question 2
- Question: What are the most effective mitigation strategies for addressing intersectional biases (e.g., gender and skintone) in T2I models, and how can we ensure these strategies are robust and controllable?
- Basis in paper: The paper states that "current mitigation methods fail to resolve biases comprehensively"
- Why unresolved: Current mitigation approaches are often limited in scope, focusing on single dimensions of bias and lacking robustness and controllability
- What evidence would resolve it: Empirical studies comparing the effectiveness of different mitigation strategies on intersectional biases

### Open Question 3
- Question: How can we incorporate diverse human and community perspectives into the development and evaluation of T2I models to ensure fair and inclusive representation?
- Basis in paper: The paper emphasizes the need for "human-centric approaches" and highlights the limitations of current methods
- Why unresolved: Current approaches to bias evaluation and mitigation often lack diverse perspectives, relying on limited conceptualizations of identity
- What evidence would resolve it: Studies that involve diverse communities in the development of bias definitions and evaluation datasets

## Limitations
- The survey relies on literature published up to 2023, potentially missing more recent advancements in bias mitigation techniques
- The categorization of papers may have overlooked emerging bias dimensions beyond gender, skintone, and geo-cultural aspects
- The effectiveness of proposed human-centric approaches remains theoretical, with limited empirical validation

## Confidence

- **High Confidence**: Claims about the dominance of gender and skintone bias research over geo-cultural bias, supported by explicit counts of papers in each category
- **Medium Confidence**: Assertions about evaluation fragmentation and mitigation ineffectiveness, based on survey of existing literature
- **Low Confidence**: Predictions about future research directions and the superiority of human-centric approaches, as these are forward-looking statements

## Next Checks

1. **Validation of Paper Coverage**: Conduct a citation network analysis to verify that the 36 papers represent the core literature in T2I bias research
2. **Empirical Comparison of Mitigation Strategies**: Implement a controlled experiment comparing multiple bias mitigation approaches on the same T2I model to assess relative effectiveness and robustness
3. **Human Evaluation Study**: Design and conduct a human study to validate the survey's claims about representational harms, measuring participants' perceptions of bias in T2I outputs across different demographics and contexts