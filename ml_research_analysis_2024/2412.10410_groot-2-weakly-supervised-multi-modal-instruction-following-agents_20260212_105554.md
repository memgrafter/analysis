---
ver: rpa2
title: 'GROOT-2: Weakly Supervised Multi-Modal Instruction Following Agents'
arxiv_id: '2412.10410'
source_url: https://arxiv.org/abs/2412.10410
tags:
- latent
- learning
- language
- data
- policy
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of training multimodal instruction-following
  agents with limited labeled data. The authors propose GROOT-2, a latent variable
  model that combines weak supervision with constrained self-imitation learning to
  align the latent space with human intentions.
---

# GROOT-2: Weakly Supervised Multi-Modal Instruction Following Agents

## Quick Facts
- arXiv ID: 2412.10410
- Source URL: https://arxiv.org/abs/2412.10410
- Reference count: 39
- Key outcome: GROOT-2 achieves robust performance on multimodal instruction following across diverse environments, with notable improvements over baselines in complex tasks requiring object-centric understanding.

## Executive Summary
This paper addresses the challenge of training multimodal instruction-following agents with limited labeled data. GROOT-2 combines weak supervision with constrained self-imitation learning to align the latent space with human intentions. By using a small set of labeled demonstrations alongside large amounts of unlabeled data, the method learns to follow instructions across diverse environments including video games and robotic manipulation tasks. Experiments on Minecraft, Language Table, Simpler Env, and Atari demonstrate that GROOT-2 achieves robust performance, with notable improvements over baselines, particularly in complex tasks requiring object-centric understanding.

## Method Summary
GROOT-2 uses a VAE-based latent variable model that learns from both labeled and unlabeled demonstrations. The method combines two key components: constrained self-imitation learning, which utilizes large amounts of unlabeled demonstrations to enable the policy to learn diverse behaviors, and human intention alignment, which uses a smaller set of labeled demonstrations to ensure the latent space reflects human intentions. The model encodes instructions from all modalities (text, video, returns) as distributions over the latent space, allowing the policy to follow instructions from any modality by sampling the corresponding latent distribution. Training involves optimizing a combined loss function that includes both demonstration-only loss and labeled loss, with hyperparameters β1=0.1, β2=0.1, and learning rate=0.0000181.

## Key Results
- Achieves strong performance on Minecraft, Language Table, Simpler Env, and Atari environments
- Demonstrates improved object-centric understanding in complex tasks compared to baselines
- Shows effective scaling with additional unlabeled data, particularly in Minecraft
- Performs well with limited labeled demonstrations (30% labeled data)

## Why This Works (Mechanism)

### Mechanism 1
Weak supervision aligns the latent space with human intentions by using labeled demonstrations to guide the latent distribution. The method maximizes the likelihood of the latent space distribution inferred from labeled demonstrations being consistent with the distribution from corresponding video demonstrations through an MLE-based alignment objective.

### Mechanism 2
Constrained self-imitation learning enables the policy to learn diverse behaviors from large amounts of unlabeled demonstrations. The method uses a VAE to model trajectory data, with posterior and prior encoders generating distributions over the latent space, and an auxiliary KL divergence term limits information in the latent space to encourage reliance on environmental feedback.

### Mechanism 3
The combination of weak supervision and latent variable models enables the policy to follow multimodal instructions by unifying them in the same latent space. Instructions from all modalities are encoded as distributions over the latent space, allowing the policy to follow any instruction modality by sampling the corresponding latent distribution.

## Foundational Learning

- **Concept: Variational Autoencoder (VAE)**
  - Why needed here: The VAE models trajectory data and learns a latent space that can represent diverse behaviors
  - Quick check question: What are the three main components of a VAE and what is the role of each component?

- **Concept: Semi-supervised learning**
  - Why needed here: The method combines weak supervision with latent variable models, requiring understanding of how to leverage both labeled and unlabeled data
  - Quick check question: What is the difference between supervised, unsupervised, and semi-supervised learning?

- **Concept: Multimodal learning**
  - Why needed here: The method aims to follow instructions from different modalities, requiring integration of information from different sources
  - Quick check question: What are some common challenges in multimodal learning and how can they be addressed?

## Architecture Onboarding

- **Component map**: Vision backbone (ViT/32) -> Image representations; BERT encoder -> Text representations; MLP -> Return representations; Transformer encoder -> Multimodal instructions; Transformer-XL decoder -> Actions; VAE components (posterior encoder, prior encoder, policy decoder)

- **Critical path**: Extract representations from different modalities → Encode multimodal instructions using shared Transformer encoder → Sample latent variable from instruction-conditioned distribution → Decode actions using causal Transformer-XL decoder conditioned on latent variable and observations

- **Design tradeoffs**: Using shared Transformer encoder for multimodal instructions vs. separate encoders; Using VAE vs. other generative models; Using causal Transformer-XL decoder vs. other sequence models

- **Failure signatures**: Poor performance on following instructions from specific modality; Inability to generalize to new tasks; High variance in performance across runs

- **First 3 experiments**: 1) Evaluate single-modality instruction following on simple task; 2) Evaluate multi-modality instruction following on simple task; 3) Evaluate multi-modality instruction following on complex task (Minecraft)

## Open Questions the Paper Calls Out

### Open Question 1
How does the model handle partial observability in Minecraft when following video instructions? The paper mentions Minecraft is a partially observable open-ended environment and that past observations are essential for decision-making, but doesn't provide quantitative analysis of how this affects performance specifically in partially observable scenarios versus fully observable ones.

### Open Question 2
What is the optimal ratio of labeled to unlabeled data for different types of tasks? The paper shows performance plateaus at 50-80% labeled data for Language Table but also demonstrates scaling benefits with more unlabeled data in Minecraft, without establishing a general principle for how this ratio might vary across different task types.

### Open Question 3
How does the model generalize to completely unseen tasks that weren't represented in the training data? While the paper demonstrates strong performance across multiple diverse environments and mentions cross-environment knowledge transfer, it doesn't specifically test zero-shot generalization to novel tasks requiring fundamentally different skill sets.

## Limitations

- The method assumes labeled demonstrations are sufficiently representative of the task space to guide latent alignment effectively
- Performance depends heavily on the quality and diversity of unlabeled demonstrations
- The paper doesn't address potential catastrophic forgetting when fine-tuning on specific instruction modalities

## Confidence

- **High Confidence**: Core architecture combining VAE-based latent variable models with weak supervision is technically sound
- **Medium Confidence**: Claims about performance improvements over baselines are supported by experimental results, but lack definitive ablation studies
- **Low Confidence**: Broad claims about versatility across diverse environments lack analysis of failure modes or conditions under which performance degrades

## Next Checks

1. Conduct ablation study varying labeled/unlabeled data ratios (1%, 5%, 10%, 30%) to determine minimum effective supervision needed

2. Test cross-modal generalization by training on subset of modalities and evaluating zero-shot/few-shot transfer to held-out modality

3. Perform detailed latent space analysis using t-SNE/UMAP visualization to verify semantic capture vs. low-level action sequences and modality mapping