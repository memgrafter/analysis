---
ver: rpa2
title: 'ViSpeR: Multilingual Audio-Visual Speech Recognition'
arxiv_id: '2406.00038'
source_url: https://arxiv.org/abs/2406.00038
tags:
- speech
- dataset
- visper
- languages
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work addresses the scarcity of multilingual audio-visual speech
  recognition (AVSR) datasets beyond English by developing a data pipeline to collect
  and process large-scale datasets for Chinese, Arabic, Spanish, and French. The ViSpeR
  dataset comprises nearly 3.2 million clips totaling over 3,600 hours, collected
  from diverse online sources and filtered using keyword searches and active speaker
  detection.
---

# ViSpeR: Multilingual Audio-Visual Speech Recognition

## Quick Facts
- arXiv ID: 2406.00038
- Source URL: https://arxiv.org/abs/2406.00038
- Reference count: 21
- Primary result: 3.2M clips, 3.6K hours multilingual AVSR dataset with competitive performance across Chinese, Arabic, Spanish, French

## Executive Summary
This work addresses the scarcity of multilingual audio-visual speech recognition (AVSR) datasets beyond English by developing a comprehensive data pipeline to collect and process large-scale datasets for Chinese, Arabic, Spanish, and French. The ViSpeR dataset comprises nearly 3.2 million clips totaling over 3,600 hours, collected from diverse online sources and filtered using keyword searches and active speaker detection. The authors train multilingual baseline AVSR and VSR models using an encoder-decoder architecture, achieving competitive performance on newly established benchmarks for each language. Results show a significant performance gap between AVSR and VSR, with AVSR outperforming VSR by a large margin across all languages due to the integration of audio cues. The dataset and models are released to the community to foster further research in this increasingly important area.

## Method Summary
The authors develop a pipeline to collect multilingual AVSR data through YouTube keyword searches, followed by binary classification to filter non-relevant content, scene change detection for shot segmentation, face detection and tracking, active speaker filtering using SyncNet, and Whisper-based transcription for clip segmentation. The resulting dataset contains 3.2M clips (3.6K hours) across Chinese, Arabic, Spanish, and French. They train multilingual baseline AVSR and VSR models using an encoder-decoder architecture with 12-layer encoder and 6-layer decoder, trained in multilingual setting using AdamW optimizer for 150 epochs on 64 Nvidia A100 GPUs.

## Key Results
- ViSpeR dataset contains nearly 3.2 million clips totaling over 3,600 hours across four languages
- AVSR models outperform VSR models by a large margin across all languages due to audio-visual integration
- Competitive performance achieved on newly established benchmarks for Chinese, Arabic, Spanish, and French

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The hybrid keyword + curated source approach yields higher-quality, more diverse clips than pure TED-only or pure YouTube search.
- Mechanism: By filtering initial video candidates through a trained binary classifier (using first 100 frames), the pipeline discards non-speech or non-relevant content early, reducing downstream processing load and noise.
- Core assumption: A simple classifier can reliably detect active speaker frames without full transcription or face tracking.
- Evidence anchors: "Thus, we reduce the search to approximately only 20 % of the initial pool." and "The training set used to create the classiﬁer was built by doing a ﬁrst pass and then recursively tracking the videos that did not output any clips, followed by labeling the data accordingly."
- Break condition: If classifier recall drops below ~80%, too many valid clips are discarded and the diversity gain disappears.

### Mechanism 2
- Claim: Using Whisper for both language detection and transcription produces sufficient alignment accuracy for large-scale VSR dataset creation.
- Mechanism: Whisper's word-level timestamps enable precise clip segmentation (2–16 s), ensuring visual and textual data are temporally aligned.
- Core assumption: Whisper's language ID and transcription quality are adequate for non-English languages in the dataset's domain (TEDx, interviews, discussions).
- Evidence anchors: "Utilizing word timestamps from Whisper outputs, tracks are segmented into clips ranging from 2.0 to 16 seconds in duration." and "We expect a higher level of label-noise in non-Latin languages given the fact that Whisper performance on these languages isn't on par with the Latin ones."
- Break condition: If label error rate > ~30%, model training signal becomes too noisy, hurting downstream performance.

### Mechanism 3
- Claim: Training a single multilingual encoder-decoder model (ViSpeR) on all four languages yields competitive performance due to shared representation learning.
- Mechanism: A unified tokenizer (21k vocab) and shared encoder allow cross-lingual transfer, improving low-resource language performance.
- Core assumption: Shared encoder can handle linguistic differences between Latin and non-Latin scripts without catastrophic interference.
- Evidence anchors: "Our model, ViSpeR, is trained in a multi-lingual setting, resulting in competitive performance on newly established benchmarks for each language." and "Both VSR and A VSR models are trained on the full set (TedX+Wild)."
- Break condition: If token-level mixing causes >10% performance drop in one language, multilingual training may need language-specific modules.

## Foundational Learning

- Concept: Scene change detection for shot segmentation
  - Why needed here: Ensures each clip contains stable speaker context and avoids abrupt visual transitions that confuse face tracking.
  - Quick check question: What algorithm is used to detect scene changes in this pipeline?

- Concept: Active speaker detection (SyncNet) for face-track filtering
  - Why needed here: Guarantees that the video segment corresponds to the audio content, a critical requirement for audio-visual alignment.
  - Quick check question: Which model is used to isolate face tracks with active speakers?

- Concept: Word-level timestamp alignment for clip segmentation
  - Why needed here: Allows extraction of temporally precise audio-visual-text triplets, avoiding misalignment that would hurt ASR training.
  - Quick check question: How are clips' start/end times determined in the pipeline?

## Architecture Onboarding

- Component map: YouTube search API → keyword filtering → binary classifier → scene segmentation → face detection (YOLOv5n0.5-Face) → face tracking → active speaker filtering (SyncNet) → Whisper transcription → clip segmentation (2–16 s) → dataset split (TedX/Wild)
- Critical path: Keyword filtering → classifier → face tracking → SyncNet → Whisper → clip creation
- Design tradeoffs: Larger classifier recall improves dataset size but risks more noise; smaller clip duration reduces misalignment but may lose context
- Failure signatures:
  - High WER on TedX vs. Wild: likely overfitting to high-quality visuals or label noise in TedX
  - Low clip yield: classifier or SyncNet too strict; consider relaxing thresholds
- First 3 experiments:
  1. Vary classifier threshold to measure yield vs. quality trade-off
  2. Test multi-stage speaker filtering (SyncNet + rule-based duration) to improve clip purity
  3. Compare single-language vs. multilingual training WER to quantify transfer gains

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal vocabulary size for tokenizers when training multilingual AVSR models, and how does it affect model performance across different languages?
- Basis in paper: [explicit] The authors mention that the unigram tokenizers are learned for all languages combined and have a vocabulary size of 21k, but they do not explore the impact of different vocabulary sizes on model performance.
- Why unresolved: The paper does not provide a systematic study on how varying the vocabulary size affects the model's ability to handle multiple languages, particularly those with different writing systems (e.g., Latin vs. non-Latin).
- What evidence would resolve it: A controlled experiment comparing model performance with different vocabulary sizes (e.g., 10k, 21k, 50k) across multiple languages would provide insights into the optimal vocabulary size for multilingual AVSR.

### Open Question 2
- Question: How can we effectively handle token switching in multilingual AVSR models when the model mixes languages in the prediction?
- Basis in paper: [explicit] The authors raise the question of how to avoid tokens switching when the model mixes languages in the prediction, indicating that this is a known challenge in multilingual AVSR.
- Why unresolved: The paper does not provide a solution or methodology to address this issue, which is critical for ensuring accurate and coherent predictions in multilingual settings.
- What evidence would resolve it: Developing and evaluating techniques such as language-specific token embeddings, dynamic vocabulary adjustment, or post-processing strategies to correct token switching would provide a solution to this problem.

### Open Question 3
- Question: How can we leverage the ViSpeR dataset for training models to translate visual speech from one language to another (e.g., from French to English)?
- Basis in paper: [explicit] The authors mention that an interesting future direction involves using the dataset to train models for translating visual speech from one language to another, highlighting the potential of the ViSpeR dataset for this task.
- Why unresolved: The paper does not explore the feasibility or methodology for implementing such a translation system, which could significantly enhance cross-linguistic communication.
- What evidence would resolve it: Conducting experiments to train and evaluate multilingual AVSR models with an additional translation component, and measuring their performance on translating visual speech between different language pairs, would demonstrate the viability of this approach.

## Limitations
- The dataset creation pipeline relies heavily on pre-trained models (Whisper, SyncNet, YOLOv5) without detailed implementation specifications
- Performance metrics show significant WER variation across languages without statistical significance testing or error analysis
- Model architecture details are sparse with critical hyperparameters omitted
- The work assumes Whisper's language identification quality is sufficient for non-English languages without empirical validation

## Confidence
**High Confidence Claims:**
- The ViSpeR dataset exists and contains the reported scale of multilingual video clips across four languages
- The AVSR models outperform VSR models across all tested languages
- The dataset and model weights are available for community use

**Medium Confidence Claims:**
- The reported WER/CER scores accurately reflect model performance on the test splits
- The multilingual training approach provides competitive performance relative to monolingual alternatives
- The dataset collection methodology produces sufficiently clean data for training

**Low Confidence Claims:**
- The magnitude of AVSR vs VSR performance gap is precisely quantified
- The specific architectural choices are optimal for multilingual AVSR
- The dataset quality is uniformly high across all languages and sources

## Next Checks
1. Perform detailed WER breakdown by language family, error type, and test split (TedX vs Wild) to identify systematic weaknesses and determine whether performance gaps reflect model limitations or dataset quality differences.

2. Systematically disable or modify key pipeline components (classifier threshold, SyncNet filtering, Whisper-based segmentation) to quantify their individual contributions to dataset quality and downstream model performance.

3. Evaluate Whisper's language ID accuracy on a held-out sample of the dataset to verify the assumption that language detection is reliable enough for large-scale dataset construction, particularly for non-Latin scripts.