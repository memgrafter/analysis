---
ver: rpa2
title: Functional Acceleration for Policy Mirror Descent
arxiv_id: '2407.16602'
source_url: https://arxiv.org/abs/2407.16602
tags:
- policy
- optimization
- lemma
- acceleration
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper applies functional acceleration to Policy Mirror Descent
  (PMD), a family of reinforcement learning algorithms. The key idea is to use momentum
  in the dual policy space to accelerate learning on long ravines or decelerate at
  sharp curvatures at the functional level of the policy optimization objective.
---

# Functional Acceleration for Policy Mirror Descent

## Quick Facts
- arXiv ID: 2407.16602
- Source URL: https://arxiv.org/abs/2407.16602
- Authors: Veronica Chelu; Doina Precup
- Reference count: 40
- Primary result: Functional acceleration applies momentum in the dual policy space to accelerate Policy Mirror Descent algorithms, leading to faster convergence especially on ill-conditioned optimization landscapes

## Executive Summary
This paper introduces functional acceleration for Policy Mirror Descent (PMD) algorithms in reinforcement learning. The key innovation is applying momentum at the functional policy representation level rather than the parameter level, using temporal-difference errors between consecutive policy evaluations. This approach is independent of policy parametrization and particularly effective on ill-conditioned optimization landscapes with "long ravines" or "sharp curvatures." The authors provide theoretical analysis and conduct numerical studies showing accelerated convergence and robustness to critic approximation errors.

## Method Summary
The method extends PMD by adding a momentum term that is the temporal-difference error between consecutive policy evaluation updates. This momentum is applied in the dual policy space (functional representation π) rather than the parameter space (θ). The algorithm operates as a proximal method on the policy simplex, making it independent of how policies are parameterized. The approach uses an approximate inner-loop optimization procedure with gradient descent to update policy parameters, and includes adaptive step-size mechanisms based on divergence and policy changes.

## Key Results
- Functional acceleration leads to faster convergence on ill-conditioned policy optimization landscapes
- The method shows robustness to critic approximation errors up to a certain threshold before overshooting occurs
- Higher-quality inner-loop policy approximation (larger k values) enhances the acceleration effect
- The approach maintains parametrization independence while covering previous momentum methods as special cases

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Momentum in the dual policy space accelerates learning on long ravines or decelerates at sharp curvatures
- Mechanism: Adds temporal-difference error between consecutive evaluation updates to current policy update
- Core assumption: Optimization landscape has ill-conditioned features where momentum-based acceleration is beneficial
- Evidence anchors: Abstract and section statements about using momentum to accelerate on long ravines or decelerate at sharp curvatures
- Break condition: If optimization landscape is well-conditioned or critic approximation error becomes too large

### Mechanism 2
- Claim: Functional acceleration leads to faster convergence on ill-conditioned policy optimization landscapes
- Mechanism: Applies momentum at functional policy representation level rather than parameter level
- Core assumption: Transformation from parameters θ to functional representation πθ is complex and problem-dependent
- Evidence anchors: Abstract statement about parametrization independence and functional vs. parameter-level optimization
- Break condition: When policy approximation quality is poor (small k values), defaulting to classic parameter-level momentum

### Mechanism 3
- Claim: Method is independent of policy parametrization and applicable to large-scale optimization
- Mechanism: Operates directly on policy simplex rather than requiring specific parameter forms
- Core assumption: Policy optimization can be formulated as proximal algorithm on policy simplex
- Evidence anchors: Abstract and section statements about functional route independence from parametrization
- Break condition: When policy class cannot be expressed as Bregman policy class or mirror map cannot be properly defined

## Foundational Learning

- Concept: Policy Mirror Descent (PMD) as a proximal algorithm
  - Why needed here: Fundamental to understanding how functional acceleration extends PMD with momentum
  - Quick check question: What is the key difference between PMD and standard policy gradient methods in terms of how they handle the policy update?

- Concept: Bregman divergences and mirror maps
  - Why needed here: Algorithm uses Bregman divergences to measure distance in policy space and requires understanding of mirror maps
  - Quick check question: How does the choice of mirror map affect the geometry of the policy optimization problem?

- Concept: Policy optimization landscape and ill-conditioning
  - Why needed here: Effectiveness of functional acceleration depends on presence of ill-conditioned features
  - Quick check question: What are "long ravines" and "sharp curvatures" in policy optimization, and why do they matter for acceleration?

## Architecture Onboarding

- Component map: Policy representation (π) -> Mirror map (h) -> Critic (Q) -> Momentum term -> Step size adaptation

- Critical path:
  1. Initialize policy and critic
  2. Compute current policy gradient
  3. Add momentum term (temporal-difference error)
  4. Update policy using proximal perspective
  5. Update critic approximation
  6. Repeat with adaptive step sizes

- Design tradeoffs:
  - Functional vs. parameter-level acceleration: Functional provides parametrization independence but may be more complex to implement
  - Exact vs. approximate critic: Exact critics provide better convergence but are computationally expensive
  - Momentum strength: Too much momentum can cause overshooting with inexact critics

- Failure signatures:
  - Oscillations in policy updates: Indicates critic error too large for effective momentum
  - Slow convergence: May indicate ill-conditioning not severe enough to benefit from acceleration
  - Parameter explosion: Suggests step sizes too large relative to momentum magnitude

- First 3 experiments:
  1. Compare PMD vs. PMD(+mom) on simple 2-state MDP with varying discount factors to observe acceleration effects
  2. Test sensitivity to critic approximation error by varying error magnitude and observing performance degradation
  3. Sweep over policy approximation quality (k values) to find sweet spot where functional acceleration is most effective

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the impact of using stochastic gradient descent with mini-batches versus full-batch updates on convergence and performance of functional acceleration for PMD?
- Basis in paper: [explicit] Paper used full-batch updates to showcase higher-level optimization and remove confounding effects from exploration, acknowledging this as a limitation
- Why unresolved: Only explores full-batch updates, which may not be practical for large-scale problems; impact of stochastic gradients on acceleration mechanism is unknown
- What evidence would resolve it: Empirical studies comparing convergence rate, final performance, and stability of functional acceleration with mini-batch SGD versus full-batch updates on various MDPs

### Open Question 2
- Question: How does functional acceleration for PMD perform with function approximation (e.g., neural networks) compared to the tabular case?
- Basis in paper: [explicit] Paper focuses on tabular setting and mentions translation to practical large-scale implementations and deep RL remains for further investigation
- Why unresolved: Theoretical analysis and empirical studies limited to tabular case; effectiveness with complex function approximators is unknown
- What evidence would resolve it: Empirical studies comparing performance of functional acceleration with neural network function approximation versus tabular methods on benchmark RL tasks like Atari or MuJoCo

### Open Question 3
- Question: What is the theoretical convergence rate of functional acceleration for PMD with general policy parametrization and stochastic gradients?
- Basis in paper: [inferred] Paper provides convergence rates for exact tabular case with full gradients; acknowledges additional guarantees for general parametrization and stochastic setting are deferred for future work
- Why unresolved: Current theoretical analysis limited to exact tabular case; extending analysis to general parametrization and stochastic gradients is open problem
- What evidence would resolve it: Theoretical analysis proving convergence rates for functional acceleration with general policy parametrization and stochastic gradients

## Limitations
- Theoretical analysis is primarily focused on exact tabular case with full gradients
- Limited empirical validation on complex RL benchmarks beyond simple MDPs
- The mechanism by which functional acceleration provides benefits over parameter-level momentum remains somewhat abstract
- Impact of stochastic gradients and mini-batch updates on the acceleration mechanism is unexplored

## Confidence

- Mechanism 1 (Momentum in dual policy space): Medium - Theoretical foundation is sound but empirical evidence is limited to ablation studies on simple MDPs
- Mechanism 2 (Faster convergence on ill-conditioned landscapes): Medium - Supported by theory but needs validation on more complex RL benchmarks
- Mechanism 3 (Parametrization independence): High - Mathematical formulation clearly demonstrates this property, though practical implications need exploration

## Next Checks

1. **Landscape Visualization**: Generate visualizations of policy optimization landscape for simple problems to empirically demonstrate "long ravines" and "sharp curvatures" and how functional acceleration navigates them differently than parameter-level momentum

2. **Scaling Experiments**: Test the algorithm on continuous control benchmarks (e.g., MuJoCo tasks) to verify that parametrization independence and large-scale applicability claims hold in practice

3. **Critic Error Sensitivity**: Conduct more systematic study of how different magnitudes and types of critic approximation errors affect acceleration mechanism, including both value-based and policy gradient critic approximations