---
ver: rpa2
title: Revisiting Catastrophic Forgetting in Large Language Model Tuning
arxiv_id: '2406.04836'
source_url: https://arxiv.org/abs/2406.04836
tags:
- open-platypus
- alpaca
- performance
- llms
- dataset
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The authors propose a novel method to mitigate catastrophic forgetting
  (CF) in large language model (LLM) fine-tuning by revealing a link between the flatness
  of the model's loss landscape and the extent of CF. They show that flatter loss
  landscapes correlate with reduced CF, and introduce sharpness-aware minimization
  (SAM) to flatten the loss landscape during optimization.
---

# Revisiting Catastrophic Forgetting in Large Language Model Tuning

## Quick Facts
- arXiv ID: 2406.04836
- Source URL: https://arxiv.org/abs/2406.04836
- Reference count: 40
- Shows Sharpness-Aware Minimization (SAM) reduces catastrophic forgetting in LLM fine-tuning

## Executive Summary
This paper establishes a novel connection between catastrophic forgetting (CF) in large language model (LLM) fine-tuning and the flatness of the model's loss landscape. The authors demonstrate that flatter loss landscapes correlate with reduced CF and introduce Sharpness-Aware Minimization (SAM) as a method to explicitly flatten the loss landscape during optimization. Through experiments across three datasets (Alpaca, ShareGPT52K, and MetaMathQA) and multiple model sizes (1B, 7B, 13B), they show SAM significantly reduces CF while maintaining or improving general task performance. The method is shown to complement existing anti-forgetting techniques like Wise-FT and rehearsal, providing incremental benefits beyond what these approaches achieve individually.

## Method Summary
The authors propose using Sharpness-Aware Minimization (SAM) to mitigate catastrophic forgetting during LLM fine-tuning by targeting loss landscape flatness. SAM employs a two-step gradient descent process that minimizes both the loss value and its sensitivity to weight perturbations, effectively seeking parameters in flatter regions of the loss landscape. During fine-tuning, SAM simultaneously optimizes for the target task while maintaining the pre-trained knowledge encoded in flatter regions. The method is evaluated across three instruction-tuning datasets (Alpaca with 52K instructions, ShareGPT52K with 52K conversations, and MetaMathQA with 395K instructions) using multiple model sizes (1B, 7B, and 13B parameters). The evaluation includes general task performance on MMLU benchmark, domain knowledge across STEM, Humanities, Social Sciences, and Other categories, reasoning abilities measured through SuperGLUE, Hellaswag, Boolq, and Siqa, understanding assessed via RACE, Openbookqa-fact, and Csl-dev, and exams performance on ARC-c.

## Key Results
- SAM reduces catastrophic forgetting across all tested model sizes (1B, 7B, 13B) with consistent improvements
- The method achieves significant performance gains compared to baseline fine-tuning approaches
- SAM demonstrates compatibility with existing anti-forgetting techniques like Wise-FT and rehearsal, providing incremental benefits when combined

## Why This Works (Mechanism)
The paper establishes that catastrophic forgetting occurs when fine-tuning pushes model parameters into sharp regions of the loss landscape that are sensitive to perturbations. By using SAM to find parameters in flatter regions, the model maintains better generalization to both the pre-trained distribution and the fine-tuning task. The two-step optimization process of SAM explicitly penalizes sharp minima, creating a regularization effect that preserves previously learned knowledge while acquiring new capabilities.

## Foundational Learning
- **Loss landscape flatness**: The curvature of the loss function around optimal parameters; flatter regions generalize better and are more robust to perturbations. Needed to understand why some minima lead to better retention of pre-trained knowledge.
- **Sharpness-Aware Minimization (SAM)**: An optimization method that minimizes both loss value and its sensitivity to weight perturbations through a two-step gradient process. Needed as the core mechanism for finding flatter minima during fine-tuning.
- **Catastrophic forgetting**: The phenomenon where neural networks lose previously learned information when trained on new tasks. Needed as the target problem being addressed.
- **Instruction fine-tuning**: The process of adapting LLMs to follow instructions using curated datasets. Needed to contextualize the experimental setup and evaluation metrics.
- **Parameter perturbation**: Small changes to model weights used to measure loss landscape sensitivity. Needed to understand how SAM evaluates sharpness.

## Architecture Onboarding

**Component map**: Pre-trained LLM -> SAM optimizer -> Loss landscape analysis -> Fine-tuned model

**Critical path**: Pre-training → SAM fine-tuning → Evaluation (MMLU + domain-specific tasks) → CF measurement

**Design tradeoffs**: SAM requires two forward passes per update (computational overhead) vs. improved CF mitigation and generalization

**Failure signatures**: 
- High CF despite SAM usage suggests improper perturbation radius (ρ) configuration
- Degraded performance on target task indicates overly conservative flatness constraints
- Computational inefficiency may result from inappropriate batch size or learning rate settings

**First experiments**:
1. Baseline: Fine-tune without SAM using epochs=2, lr=5e-6, batch_size=128
2. SAM implementation: Fine-tune with SAM using epochs=1, lr=5e-6, batch_size=128, ρ=2
3. Combined approach: Apply SAM + Wise-FT to test complementarity claims

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions, but several emerge from the work:
- How does SAM's effectiveness scale to larger models beyond 13B parameters?
- What is the optimal perturbation radius (ρ) for different dataset characteristics?
- How does SAM compare to other sharpness-aware optimization methods?
- Does SAM's performance depend on the similarity between pre-training and fine-tuning tasks?
- What is the computational overhead of SAM relative to its anti-forgetting benefits?

## Limitations
- The study uses only 1 epoch of fine-tuning with SAM, raising questions about generalization to standard training durations
- The causal mechanism linking loss landscape flatness to reduced forgetting is demonstrated empirically but not fully established theoretically
- The incremental benefits of combining SAM with existing methods like Wise-FT are shown but relative contributions aren't clearly isolated

## Confidence
- High confidence: SAM reduces catastrophic forgetting across multiple datasets and model sizes
- Medium confidence: The theoretical connection between loss landscape flatness and catastrophic forgetting
- Medium confidence: SAM complements existing anti-forgetting techniques

## Next Checks
1. Conduct ablation study comparing SAM's effectiveness across different fine-tuning durations (1, 2, and 4 epochs)
2. Isolate SAM's contribution by comparing against standard AdamW with equivalent hyperparameter configurations
3. Design experiments to directly test whether artificial manipulation of loss landscape geometry independently affects catastrophic forgetting