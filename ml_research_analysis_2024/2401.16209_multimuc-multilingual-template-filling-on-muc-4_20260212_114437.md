---
ver: rpa2
title: 'MultiMUC: Multilingual Template Filling on MUC-4'
arxiv_id: '2401.16209'
source_url: https://arxiv.org/abs/2401.16209
tags:
- language
- template
- target
- conference
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: MULTIMUC introduces the first multilingual parallel corpus for
  template filling, translating the classic MUC-4 benchmark into Arabic, Chinese,
  Farsi, Korean, and Russian. The corpus includes automatic translations with manually
  projected annotations and human-corrected translations for dev and test splits.
---

# MultiMUC: Multilingual Template Filling on MUC-4

## Quick Facts
- arXiv ID: 2401.16209
- Source URL: https://arxiv.org/abs/2401.16209
- Reference count: 40
- Primary result: Bilingual training (English plus target language) outperforms monolingual approaches for template filling, with best models achieving 25-41 F1 on CEAF-REE

## Executive Summary
MULTIMUC introduces the first multilingual parallel corpus for template filling, translating the classic MUC-4 benchmark into Arabic, Chinese, Farsi, Korean, and Russian. The corpus includes automatic translations with manually projected annotations and human-corrected translations for dev and test splits. Experiments with state-of-the-art models show that bilingual training (English plus target language) outperforms monolingual approaches, with the best supervised models achieving 25-41 F1 on CEAF-REE. Few-shot ChatGPT baselines perform below supervised models but remain competitive, especially when English examples are included in prompts. The resource enables research on multilingual document-level information extraction and highlights the value of English training data for lower-resource languages.

## Method Summary
The MULTIMUC corpus is created by translating the English MUC-4 documents into five target languages using machine translation, then projecting entity annotations through word alignment and manual correction. Two model architectures are evaluated: GTT (BERT encoder-decoder) and ITER X (T5 encoder with span classification). Models are trained under three settings: target language only with automatic translations (TGT AUTO), target language only with human-corrected translations (TGT MAN), and bilingual training with both English and target language data (BIMAN). Performance is measured using CEAF-REE and CEAF-RME F1 scores.

## Key Results
- Bilingual training outperforms monolingual approaches across all languages and model architectures
- GTT achieves 25-41 F1 on CEAF-REE, while ITER X achieves 21-37 F1
- Few-shot ChatGPT baselines achieve 12-27 F1, with English examples improving performance
- Template recall is more challenging than precision, with missing role fillers accounting for most errors

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Automatic word alignment followed by manual correction improves entity mention alignment accuracy
- Mechanism: Automatic alignments are obtained using a fine-tuned XLM-R encoder trained on large parallel corpora. These alignments are then manually inspected and corrected by native speaker annotators, resulting in higher-quality projected annotations.
- Core assumption: The combination of strong automatic alignment tools and human expertise yields better results than either alone
- Evidence anchors:
  - [abstract]: "obtain automatic translations from a strong multilingual machine translation system and manually project the original English annotations into each target language"
  - [section 3.3]: "We use Awesome-align...which derives word alignments via comparison of word embeddings"
  - [section 3.4]: "Between one and four annotators worked on each language, with tasks distributed based on availability"
- Break Condition: If the automatic alignment quality is very low or if human annotators make systematic errors

### Mechanism 2
- Claim: Bilingual training (English + target language) outperforms monolingual approaches for template filling
- Mechanism: Models trained on both English and target language data learn to handle cross-lingual entity mentions and benefit from the larger amount of English training data
- Core assumption: English training data provides valuable signal for lower-resource languages
- Evidence anchors:
  - [abstract]: "bilingual training (English plus target language) outperforms monolingual approaches"
  - [section 4.2]: "for nearly all languages, both models obtain their strongest performance when trained jointly on English and target language data (BIMAN)"
  - [section 4.2]: "This is consistent with past findings in IE establishing the value of English training data for lower-resource target languages"
- Break Condition: If the English and target language data have significantly different distributions

### Mechanism 3
- Claim: Template filling models struggle with template recall more than precision
- Mechanism: The models tend to miss entire templates or role fillers more often than they incorrectly predict them, leading to a precision-focused behavior
- Core assumption: The models have difficulty individuating events and determining which role fillers belong to which template
- Evidence anchors:
  - [section 5.1]: "we find that, across languages and settings, missing role fillers account for a majority of the errors"
  - [section 5.1]: "models tend to struggle significantly with template recall, perhaps due to difficulty in individuating events"
  - [section 4.2]: "GTT's extractions heavily favor precision"
- Break Condition: If the models are modified to focus more on recall or if the evaluation metric changes to reward recall

## Foundational Learning

- Concept: Template filling task structure
  - Why needed here: Understanding the input/output format and evaluation metrics is crucial for implementing and evaluating models
  - Quick check question: What are the two main components of the template filling task and how are they evaluated?

- Concept: Cross-lingual projection
  - Why needed here: The dataset creation process relies on transferring annotations from English to target languages
  - Quick check question: What are the two main steps in cross-lingual projection and what tools are used?

- Concept: Sequence-to-sequence modeling
  - Why needed here: The GTT model uses a BERT encoder-decoder architecture for template filling
  - Quick check question: How does the GTT model generate linearized templates and what decoding strategy does it use?

## Architecture Onboarding

- Component map: Preprocessing English documents → Machine Translation → Word Alignment → Annotation Projection → Manual Correction → Model training and evaluation
- Critical path: Preprocessing English documents → Obtaining high-quality translations → Accurate word alignments → Projected annotations → Model training and evaluation
- Design tradeoffs: Using machine translation vs. human translation for the full dataset, monolingual vs. bilingual training, different model architectures
- Failure signatures: Low alignment quality leading to incorrect projected annotations, poor translation quality affecting downstream model performance, models struggling with template recall more than precision
- First 3 experiments:
  1. Evaluate the impact of alignment correction on entity extraction performance
  2. Compare monolingual vs. bilingual training for template filling
  3. Investigate the error patterns of the best performing model on the test set

## Open Questions the Paper Calls Out
- How do the proposed models perform on other multilingual parallel corpora for template filling beyond the MUC-4 dataset?
- What is the impact of different translation quality on the performance of the models?
- How do the proposed models handle template filling in languages with different word orders and grammatical structures?

## Limitations
- Reliance on machine translation for the full dataset introduces quality variations that may affect model performance
- Automatic word alignment step requires manual correction and may introduce residual errors
- Few-shot ChatGPT experiments use an unspecified prompt template, making results difficult to reproduce

## Confidence
- **High Confidence**: Bilingual training outperforms monolingual approaches; models struggle more with recall than precision
- **Medium Confidence**: English training data provides valuable signal for lower-resource languages; few-shot ChatGPT performance
- **Low Confidence**: None identified

## Next Checks
1. Replicate bilingual vs. monolingual comparison on held-out automatically translated data to verify bilingual advantage without human corrections
2. Manually examine sample predictions for Korean to confirm missing role fillers constitute majority of errors
3. Correlate automatic translation quality scores with downstream model performance across languages to quantify translation quality impact