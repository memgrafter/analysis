---
ver: rpa2
title: Simulating User Agents for Embodied Conversational-AI
arxiv_id: '2410.23535'
source_url: https://arxiv.org/abs/2410.23535
tags:
- user
- dialogue
- actions
- observe
- embodied
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces an LLM-based user agent to simulate human
  behavior in embodied conversational AI interactions. Given a user goal, the agent
  decides at each step whether to observe or speak, generating dialogue acts in response
  to robot actions.
---

# Simulating User Agents for Embodied Conversational-AI

## Quick Facts
- arXiv ID: 2410.23535
- Source URL: https://arxiv.org/abs/2410.23535
- Authors: Daniel Philipov; Vardhan Dongre; Gokhan Tur; Dilek Hakkani-Tür
- Reference count: 12
- Key outcome: LLM-based user agent achieves 42% F-measure with zero-shot prompting in predicting when to speak in embodied conversations

## Executive Summary
This paper introduces an LLM-based user agent to simulate human behavior in embodied conversational AI interactions. Given a user goal, the agent decides at each step whether to observe or speak, generating dialogue acts in response to robot actions. This addresses the challenge of creating large-scale, diverse datasets for training and evaluating embodied agents, which are typically expensive and time-consuming to collect. The approach demonstrates that LLMs can effectively predict user behavior with zero-shot prompting, achieving reasonable performance while enabling scalable dataset generation for embodied AI systems.

## Method Summary
The method employs an LLM-based user simulator that predicts when a user should speak versus observe during embodied interactions. Using the TEACh dataset, the approach implements zero-shot and few-shot prompting with GPT-4 and Llama 3.1 models to predict user actions, and fine-tunes Llama 3.1 and RoBERTa-base models on the training subset. The simulator takes interaction history and user goals as input, generating dialogue acts or "observe" decisions at each step. Experiments compare performance across different prompting strategies and fine-tuning approaches using Speak-F1 and DA-F1 metrics to evaluate prediction accuracy.

## Key Results
- LLM-based user agent achieves 42% F-measure with zero-shot prompting and 43.4% with few-shot prompting in predicting when to speak
- Fine-tuning improves dialogue act generation accuracy from 51.1% to 62.5%
- Excluding move actions from the dataset significantly improves Speak-F1 scores (42.03% to 43.39% vs 27.04% to 26.79% with moves)
- GPT-4 consistently outperforms Llama 3.1, with few-shot implementation showing highest F1 scores across most dialogue acts

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The LLM-based user agent can predict when to speak by analyzing the history of robot actions and dialogue acts.
- Mechanism: At each time step, the model observes the interaction history and uses either zero-shot or few-shot prompting to decide whether the user should "observe" or produce a dialogue act. The prompt structure includes a goal description and optionally in-context examples to guide the LLM's response.
- Core assumption: The LLM has sufficient understanding of the task domain and dialogue structure from pre-training to make reasonable predictions without fine-tuning.
- Evidence anchors: [abstract] "the LLM-based user agent can achieve an F-measure of 42% in mimicking human speaking behavior with simple zero-shot prompting"; [section] "the input to the user simulator for step i, xi, is the sequence (s1, a1), ..., (si−1, ai−1), and the goal of simulation is to predict yi ∈ {''observe''} ∪ D"

### Mechanism 2
- Claim: Fine-tuning the LLM on the TEACh dataset significantly improves the accuracy of dialogue act generation.
- Mechanism: By adapting a pre-trained LLM (Llama 3.1 8B) to the specific dialogue act distribution and task types in TEACh, the model learns to generate more appropriate and context-sensitive responses.
- Core assumption: The task-specific patterns in TEACh are learnable and generalizable enough to improve model performance through fine-tuning.
- Evidence anchors: [abstract] "Through fine-tuning, we achieved similar success in deciding when to speak but much greater success in deciding what to say, from 51.1% to 62.5%"; [section] "Fine-tuning was applied to Llama 3.1 8B and RoBERTa-base models over multiple epochs"

### Mechanism 3
- Claim: Excluding move actions from the dataset improves the model's ability to predict speaking turns.
- Mechanism: Move actions introduce noise that confuses the model's decision-making process for speech prediction. When these actions are removed, the model can focus on the conversational context and make more accurate predictions.
- Core assumption: Move actions do not provide meaningful context for determining when a user should speak.
- Evidence anchors: [section] "When move actions are included, the Speak F1 scores remain relatively low and consistent between zero-shot (27.04%) and few-shot (26.79%) approaches... However, the exclusion of move actions leads to a dramatic improvement in Speak F1 scores, jumping to 42.03% for zero-shot and 43.39% for few-shot scenarios"

## Foundational Learning

- Concept: Dialogue acts and their role in task-oriented conversations
  - Why needed here: The user simulator must predict dialogue acts to generate realistic user behavior, so understanding the different types of dialogue acts and their functions is crucial.
  - Quick check question: What is the difference between an "Instruction" dialogue act and a "RequestForInstruction" dialogue act?

- Concept: Embodied AI and situated dialogue
  - Why needed here: The user simulator is designed for embodied agents that operate in physical environments, so understanding how natural language interactions work in this context is essential.
  - Quick check question: Why might a user need to provide feedback or corrections to an embodied agent during a task?

- Concept: Zero-shot and few-shot learning
  - Why needed here: The paper uses both zero-shot and few-shot prompting to evaluate the LLM's ability to simulate user behavior, so understanding these learning paradigms is important.
  - Quick check question: What is the main difference between zero-shot and few-shot prompting in the context of this paper?

## Architecture Onboarding

- Component map: User Goal -> LLM-based User Agent -> Prompt Generator -> TEACh Dataset -> Interaction History -> Embodied Agent -> Virtual Environment (AI2Thor)

- Critical path: 1. Receive user goal and interaction history; 2. Generate prompt with goal description and optional examples; 3. LLM predicts user action (observe or dialogue act); 4. If dialogue act, convert to natural language using templates; 5. Output user action to embodied agent

- Design tradeoffs: Zero-shot vs. few-shot prompting (simplicity vs. accuracy); Move actions inclusion (context vs. noise); Fine-tuning vs. prompting (performance vs. flexibility)

- Failure signatures: Low Speak-F1 score (inaccurate speaking prediction); Low DA-F1 score (inappropriate dialogue acts); High false positive rate (speaking too often); High false negative rate (not speaking when needed)

- First 3 experiments: 1. Evaluate zero-shot prompting performance on Speak-F1 and DA-F1 metrics; 2. Evaluate few-shot prompting performance and compare with zero-shot results; 3. Fine-tune LLM on TEACh dataset and evaluate improvement in DA-F1 score

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does incorporating visual information impact the accuracy of user simulation models in embodied AI?
- Basis in paper: [explicit] The paper mentions that the current model does not incorporate visual information, which is a crucial aspect of embodied AI, and future work could explore integrating visual cues into the user simulator.
- Why unresolved: The paper does not include experiments or analysis involving visual information, leaving the potential impact on user simulation accuracy unexplored.
- What evidence would resolve it: Experimental results comparing user simulation accuracy with and without visual input, ideally using visual LLMs like GPT-4V or LLaVa.

### Open Question 2
- Question: What is the effect of selective removal of move actions on the performance of user simulation models?
- Basis in paper: [explicit] The paper includes an analysis of the impact of move actions and their selective removal, noting that in some cases selective removal performed noticeably worse than excluding move turns entirely.
- Why unresolved: The paper does not fully explain why selective removal of move actions sometimes leads to worse performance, nor does it explore the underlying reasons for this outcome.
- What evidence would resolve it: Detailed analysis of the contexts in which selective removal of move actions improves or worsens performance, including qualitative insights into the types of interactions affected.

### Open Question 3
- Question: How do different dialogue acts perform under various prompting strategies (zero-shot vs. few-shot) in user simulation models?
- Basis in paper: [explicit] The paper includes an analysis of F1 scores across different dialogue acts, showing that GPT-4 consistently outperforms Llama 3.1, with its few-shot implementation showing the highest F1 scores across most dialogue acts.
- Why unresolved: The paper does not explore the reasons behind the variance in performance across dialogue acts or identify specific strategies to improve performance for challenging acts.
- What evidence would resolve it: Comparative analysis of performance across dialogue acts under different prompting strategies, identifying patterns or characteristics of acts that are more or less challenging for the models.

## Limitations

- Dataset representation concerns: The study relies entirely on the TEACh dataset, which may not capture the full diversity of real-world embodied interactions.
- Evaluation methodology constraints: The paper uses simulated evaluation rather than human judgment to assess the quality of generated user behavior.
- LLM capability assumptions: The approach assumes that pre-trained LLMs contain sufficient world knowledge to simulate embodied interactions without extensive fine-tuning.

## Confidence

**High confidence**: The experimental methodology is clearly described and reproducible. The use of established datasets (TEACh) and standard evaluation metrics provides solid grounding. The distinction between zero-shot, few-shot, and fine-tuned approaches is well-defined.

**Medium confidence**: The core claims about LLM-based user simulation are supported by experimental results, but the relatively low performance metrics (42-43% F-measure) indicate that the approach, while feasible, is not yet highly reliable. The paper's conclusions about the "feasibility" of this approach are appropriately modest given the results.

**Low confidence**: Claims about the broader applicability of this approach to different embodied AI domains are not empirically tested. The paper doesn't address how well the method would generalize to different types of tasks, environments, or robot capabilities beyond what's represented in TEACh.

## Next Checks

1. **Human evaluation study**: Conduct a user study where human participants interact with the simulated user agent and rate the naturalness and appropriateness of responses. This would validate whether the quantitative metrics (Speak-F1, DA-F1) actually correlate with perceived quality of interaction.

2. **Cross-dataset generalization test**: Apply the trained user simulator to a different embodied dialogue dataset (e.g., from a different domain or with different robot capabilities) to assess how well the approach generalizes beyond the training distribution. This would reveal whether the method captures fundamental principles of embodied interaction or merely memorizes patterns from TEACh.

3. **Error analysis on challenging cases**: Systematically analyze the 57-58% of cases where the model fails to predict correct speaking behavior. Categorize errors by type (e.g., missing contextual cues, misunderstanding physical affordances, timing issues) to identify specific weaknesses that could guide targeted improvements to the approach.