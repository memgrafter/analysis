---
ver: rpa2
title: A Survey on the Real Power of ChatGPT
arxiv_id: '2405.00704'
source_url: https://arxiv.org/abs/2405.00704
tags:
- chatgpt
- tasks
- arxiv
- language
- generation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper surveys recent studies evaluating ChatGPT\u2019s real-world\
  \ NLP performance across seven task categories, reviewing its capabilities and limitations\
  \ in classification, generation, sequence labeling, information retrieval, parsing,\
  \ reasoning, and multilingual tasks. Key findings include: (1) ChatGPT performs\
  \ well in zero-shot settings but underperforms fine-tuned models; (2) Its generalization\
  \ ability is limited on newly collected data; (3) Performance degrades over time;\
  \ (4) Prompt engineering introduces instability and reproducibility issues."
---

# A Survey on the Real Power of ChatGPT

## Quick Facts
- arXiv ID: 2405.00704
- Source URL: https://arxiv.org/abs/2405.00704
- Reference count: 7
- Primary result: Survey of ChatGPT's NLP performance across seven task categories, finding strong zero-shot capabilities but limitations in reasoning, domain specialization, and reproducibility

## Executive Summary
This paper surveys recent studies evaluating ChatGPT's real-world NLP performance across seven task categories: classification, generation, sequence labeling, information retrieval, parsing, reasoning, and multilingual tasks. The survey reveals that ChatGPT achieves strong zero-shot performance but consistently underperforms fine-tuned models on specialized tasks. Key findings include temporal performance degradation, limited generalization to newly collected data, and significant variability introduced by prompt engineering. The paper emphasizes that ChatGPT is strongest in text generation and information retrieval while struggling with complex reasoning and specialized domains.

## Method Summary
The authors conducted a systematic review of recent empirical studies that directly evaluated ChatGPT on specific NLP tasks. They categorized these studies across seven task types and synthesized findings regarding performance, limitations, and methodological challenges. The survey approach involved analyzing published results, identifying patterns across studies, and highlighting reproducibility issues, temporal variability, and generalization challenges. The paper also reviewed social implications and safety considerations while emphasizing evaluation challenges and future opportunities.

## Key Results
- ChatGPT achieves high zero-shot performance on NLP tasks but underperforms fine-tuned models on specialized domains
- Performance degrades over time due to model updates and concept drift
- Prompt engineering introduces significant instability and reproducibility issues across evaluations

## Why This Works (Mechanism)

### Mechanism 1
- Claim: ChatGPT achieves high zero-shot performance on NLP tasks due to pretraining on large-scale diverse text corpora
- Mechanism: The model has seen a wide variety of language patterns during pretraining, enabling it to generalize without task-specific fine-tuning
- Core assumption: The pretraining corpus included examples similar to evaluation tasks
- Evidence anchors:
  - [abstract] "ChatGPT performs well in zero-shot settings but underperforms fine-tuned models"
  - [section 3.1] "ChatGPT's performance is good under the zero shot classification setting but still fall behind the supervised models"
  - [corpus] Weak evidence - no direct mention of pretraining data overlap
- Break condition: If evaluation datasets were not part of pretraining corpus, zero-shot performance drops significantly

### Mechanism 2
- Claim: ChatGPT's performance degrades over time due to model version updates and concept drift
- Mechanism: Model versions change without public disclosure, and evaluation benchmarks become stale as language usage evolves
- Core assumption: Model updates occur between evaluation studies
- Evidence anchors:
  - [abstract] "Performance degrades over time"
  - [section 5] "the behavior of GPT-3.5 and GPT-4 has varied significantly over a relatively short amount of time"
  - [corpus] No corpus evidence available
- Break condition: If model versions are frozen for evaluation periods, performance degradation should not occur

### Mechanism 3
- Claim: Prompt engineering introduces variability and reproducibility issues in ChatGPT evaluations
- Mechanism: Different prompt formulations lead to substantially different model outputs despite equivalent semantic content
- Core assumption: ChatGPT's response generation is sensitive to prompt wording and structure
- Evidence anchors:
  - [abstract] "Prompt engineering introduces instability and reproducibility issues"
  - [section 3.1] "ChatGPT cannot conduct knowledge transfer from in-domain demonstrations and generalize it to out-of-domain tasks"
  - [corpus] No corpus evidence available
- Break condition: If standardized prompts are used across all evaluations, performance variability should decrease

## Foundational Learning

- Concept: Zero-shot vs few-shot learning
  - Why needed here: The paper distinguishes between ChatGPT's zero-shot and few-shot capabilities
  - Quick check question: What's the key difference between zero-shot and few-shot learning in the context of large language models?

- Concept: Fine-tuning vs in-context learning
  - Why needed here: The paper compares ChatGPT (in-context learning) with fine-tuned models
  - Quick check question: How does in-context learning differ from traditional fine-tuning approaches?

- Concept: Concept drift in language models
  - Why needed here: The paper notes performance degradation over time
  - Quick check question: What factors might cause concept drift in large language models deployed over time?

## Architecture Onboarding

- Component map: User prompt → tokenization → attention layers → output generation → post-processing
- Critical path: User prompt → tokenization → attention layers → output generation → post-processing
- Design tradeoffs: Large model size provides better generalization but reduces deployment flexibility; closed-source nature prevents direct analysis but enables commercial applications
- Failure signatures: Inconsistent responses to semantically equivalent prompts; performance degradation on tasks requiring domain-specific knowledge; sensitivity to prompt phrasing
- First 3 experiments:
  1. Compare zero-shot performance across multiple semantically equivalent prompts for the same task
  2. Test performance consistency across different model versions (if accessible)
  3. Evaluate cross-domain generalization by testing on tasks outside pretraining distribution

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does ChatGPT's performance degradation over time indicate fundamental limitations in continual learning for large language models?
- Basis in paper: [explicit] The paper states "The performance of ChatGPT degrades with time" and notes this as a key finding
- Why unresolved: While the paper identifies performance degradation, it doesn't determine whether this is due to data drift, model updates, or inherent limitations in how LLMs handle evolving information
- What evidence would resolve it: Longitudinal studies comparing ChatGPT's performance on static benchmarks over time, coupled with analysis of model updates and training data changes, could determine the root cause of performance degradation

### Open Question 2
- Question: Can prompt engineering techniques be standardized to achieve consistent and reproducible results across different NLP tasks?
- Basis in paper: [explicit] The paper highlights that "prompt engineering introduces instability and reproducibility issues" as a key challenge
- Why unresolved: While the paper identifies prompt engineering as problematic, it doesn't provide concrete methods for standardizing prompts or establishing best practices that ensure consistent performance
- What evidence would resolve it: Empirical studies comparing various prompt engineering approaches across multiple tasks, with statistical analysis of variance in results, could identify which techniques are most reliable

### Open Question 3
- Question: What architectural modifications would enable ChatGPT to perform better on specialized domains without extensive fine-tuning?
- Basis in paper: [inferred] The paper notes that ChatGPT struggles with specialized domains and underperforms fine-tuned models, but doesn't explore architectural solutions
- Why unresolved: The paper identifies the problem but doesn't investigate whether changes to the underlying architecture (beyond prompt engineering) could improve domain performance
- What evidence would resolve it: Comparative studies testing modified architectures with different specialization mechanisms, evaluated on domain-specific benchmarks, could reveal effective architectural improvements

## Limitations

- The closed-source nature of ChatGPT prevents direct analysis of internal mechanisms, limiting understanding of performance patterns
- The rapidly evolving nature of ChatGPT means findings from early 2023 may not generalize to current model versions
- The survey cannot definitively separate prompt engineering issues from fundamental model limitations due to their intertwined nature in empirical studies

## Confidence

- **High Confidence:** Claims about ChatGPT's general zero-shot performance capabilities across NLP tasks are well-supported by multiple independent studies
- **Medium Confidence:** Claims about performance degradation over time are supported by anecdotal evidence but lack systematic longitudinal studies
- **Low Confidence:** Specific claims about ChatGPT's generalization limitations on newly collected data lack quantitative bounds

## Next Checks

1. Conduct systematic replication studies using standardized prompts across multiple model versions to isolate the impact of prompt engineering versus model capabilities
2. Perform controlled experiments testing ChatGPT's performance on datasets explicitly excluded from pretraining corpora to measure true generalization boundaries
3. Establish a longitudinal evaluation framework tracking specific NLP tasks over time to quantify the nature and extent of performance degradation across model updates