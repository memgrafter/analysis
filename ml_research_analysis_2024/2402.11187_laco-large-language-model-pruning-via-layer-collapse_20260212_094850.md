---
ver: rpa2
title: 'LaCo: Large Language Model Pruning via Layer Collapse'
arxiv_id: '2402.11187'
source_url: https://arxiv.org/abs/2402.11187
tags:
- laco
- pruning
- table
- layer
- layers
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: LaCo is a layer-wise structured pruning method for LLMs that collapses
  rear model layers into prior layers without training. It outperforms state-of-the-art
  structured pruning methods, maintaining average task performance above 80% at pruning
  ratios of 25-30%.
---

# LaCo: Large Language Model Pruning via Layer Collapse

## Quick Facts
- arXiv ID: 2402.11187
- Source URL: https://arxiv.org/abs/2402.11187
- Authors: Yifei Yang; Zouying Cao; Hai Zhao
- Reference count: 26
- Primary result: Layer-wise pruning method achieving 25-30% compression with >80% task performance

## Executive Summary
LaCo is a novel layer-wise structured pruning method for large language models that collapses rear model layers into prior layers without training. The method leverages the observation that adjacent layers in LLMs have highly similar parameters and outputs, allowing layer merging without significant performance loss. LaCo outperforms state-of-the-art structured pruning methods while maintaining the original model structure, enabling seamless integration into existing applications.

## Method Summary
LaCo uses a Reserving-Differences-while-Seeking-Common (RDSC) Layer Merge approach that computes differences between parameters of consecutive layers and merges them into earlier layers. The method iteratively merges layers from top to bottom, calculating cosine similarity of output representations to ensure minimal performance impact. Key hyperparameters include C (layers merged per operation), I (iteration count), and T (similarity threshold). The method preserves model structure by maintaining intermediate dimensions while achieving significant compression ratios.

## Key Results
- Achieves 25-30% layer-wise compression with average task performance above 80%
- Outperforms state-of-the-art structured pruning methods across multiple benchmarks
- Requires minimal post-training to recover performance, achieving convergence with one-thousandth of the cost of alternative methods

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Merging parameter differentials between adjacent layers preserves model representation.
- Mechanism: LaCo uses RDSC Layer Merge where differences between parameters of consecutive layers (θl+1 - θl) are computed and added to the earlier layer (θl), then later layers are discarded. This works because adjacent layers have highly similar parameters and outputs.
- Core assumption: Adjacent layers in LLMs have minimal parameter differences and highly correlated output representations.
- Evidence anchors: [abstract] "merging the parameter differentials...often does not significantly impact model performance"; [section] "changes in parameters and output representations between adjacent layers...are not particularly significant"; [corpus] Weak validation from related papers
- Break condition: If adjacent layers have divergent representations or large parameter differences, merging would cause significant performance degradation.

### Mechanism 2
- Claim: Preserving model structure during pruning maintains compatibility with existing systems.
- Mechanism: LaCo collapses layers without changing the model's internal structure (like intermediate dimensions), unlike other structured pruning methods that remove modules and alter architecture.
- Core assumption: Maintaining the original model architecture is more important for practical deployment than aggressive parameter reduction.
- Evidence anchors: [abstract] "LaCo preserves the internal structure...pruned models can be seamlessly integrated into existing applications"; [section] "LaCo maintains the model structure...does not have this issue" (decreased inference speed); [corpus] Weak emphasis on structural preservation in related work
- Break condition: If preserved structure includes redundant components that don't contribute to performance, the method becomes less efficient than structure-altering alternatives.

### Mechanism 3
- Claim: LaCo enables effective post-training recovery of pruned models.
- Mechanism: Pruned models retain sufficient parameter information from the original model, allowing them to rapidly converge to original performance levels with minimal post-training data and compute resources.
- Core assumption: Parameter inheritance from original models is sufficient for quick performance recovery.
- Evidence anchors: [abstract] "post-training experiments confirm LaCo effectively inherits parameters...requires only minimal training to restore...loss convergence level"; [section] "pruned models have effectively inherited parameters...enabling them to rapidly recover performance with minimal post-training"; [corpus] Weak discussion of post-training recovery in related papers
- Break condition: If pruned model loses critical parameter information, post-training would require extensive data and resources similar to training from scratch.

## Foundational Learning

- Concept: Layer-wise similarity in neural networks
  - Why needed here: LaCo relies on the observation that adjacent layers have similar parameters and outputs, which enables effective merging without significant performance loss
  - Quick check question: What metric is used to measure similarity between adjacent layer outputs in LaCo?

- Concept: Parameter differencing and merging
  - Why needed here: The core mechanism of LaCo involves computing differences between consecutive layer parameters and merging them into earlier layers
  - Quick check question: How does LaCo compute the merged parameters for a layer that absorbs subsequent layers?

- Concept: Structured pruning vs non-structured pruning
  - Why needed here: Understanding the difference helps explain why LaCo's approach of preserving structure while removing layers is advantageous
  - Quick check question: What is the main structural difference between LaCo and traditional structured pruning methods?

## Architecture Onboarding

- Component map: Input -> Original LLM model, few-shot calibration samples, hyperparameters (C, I, T) -> RDSC Layer Merge function, similarity calculation, iterative pruning loop -> Pruned LLM model with preserved structure -> Optional post-training for performance recovery

- Critical path:
  1. Initialize model and hyperparameters
  2. Iteratively merge layers from top down using RDSC
  3. Calculate cosine similarity of output representations
  4. Compare similarity to threshold and decide whether to keep merge
  5. Output final pruned model

- Design tradeoffs:
  - Higher C (layers merged per operation) → faster pruning but potentially more performance loss
  - Lower T (similarity threshold) → more aggressive pruning but lower performance
  - Fewer calibration samples → faster pruning but potentially less stable results

- Failure signatures:
  - Rapid drop in cosine similarity below threshold indicates too aggressive merging
  - Performance degradation on specific benchmarks suggests loss of task-specific representations
  - Memory errors during merging suggest incorrect parameter dimension handling

- First 3 experiments:
  1. Run LaCo with C=4, I=2, T=0.65 on Llama2-7B using 10 Wikipedia sentences as calibration data; measure final layer count and performance on BoolQ
  2. Test layer merging with m=1 (simple drop instead of merge) on same model; compare performance to full LaCo
  3. Vary T from 0.85 to 0.45 in increments; plot compression ratio vs performance to find optimal threshold

## Open Questions the Paper Calls Out

None

## Limitations

- The RDSC merging mechanism's effectiveness may vary across different LLM architectures beyond Llama2
- The calibration process using few-shot samples may not generalize well when calibration data differs significantly from test data
- The claim about seamless integration into existing applications is largely theoretical without concrete deployment case studies

## Confidence

**High Confidence**: Structural preservation claim is well-supported with robust experimental results showing 25-30% compression with >80% average task performance.

**Medium Confidence**: RDSC merging mechanism's effectiveness depends on layer similarity assumption, which is empirically observed but may not generalize universally across all architectures.

**Low Confidence**: Seamless integration claim is largely theoretical without concrete deployment case studies or performance comparisons in production environments.

## Next Checks

1. **Architecture Generalization Test**: Apply LaCo to diverse LLM architectures including GPT-3, OPT, and BLOOM models of varying sizes. Measure whether the layer similarity assumption holds and quantify performance degradation when it doesn't.

2. **Calibration Robustness Analysis**: Systematically vary the calibration data quality and quantity (from 5 to 500 samples, from clean to noisy data) and measure the impact on final pruned model performance across all tested benchmarks.

3. **Production Integration Case Study**: Deploy a LaCo-pruned model in a real application scenario (e.g., chat system, code completion tool) and measure actual inference speed, memory usage, and any integration issues compared to the original model and other pruning methods.