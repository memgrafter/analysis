---
ver: rpa2
title: 'Learning to Compress: Local Rank and Information Compression in Deep Neural
  Networks'
arxiv_id: '2410.07687'
source_url: https://arxiv.org/abs/2410.07687
tags:
- rank
- neural
- information
- local
- networks
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces the concept of local rank as a measure of
  the dimensionality of feature manifolds learned by deep neural networks. The authors
  theoretically and empirically demonstrate that local rank decreases during the terminal
  phase of training, indicating compression of the learned representations.
---

# Learning to Compress: Local Rank and Information Compression in Deep Neural Networks

## Quick Facts
- arXiv ID: 2410.07687
- Source URL: https://arxiv.org/abs/2410.07687
- Reference count: 25
- Key outcome: Introduces local rank as a measure of feature manifold dimensionality, demonstrating that deep networks compress information by reducing local rank during training

## Executive Summary
This paper introduces local rank as a measure of the dimensionality of feature manifolds learned by deep neural networks. The authors theoretically and empirically demonstrate that local rank decreases during the terminal phase of training, indicating compression of learned representations. They connect this phenomenon to Information Bottleneck theory, arguing that rank reduction correlates with mutual information compression between inputs and intermediate layers. Experiments on synthetic and real-world datasets show that as the Information Bottleneck trade-off parameter β increases, local rank increases while accuracy decreases.

## Method Summary
The method involves computing the Jacobian of pre-activation values with respect to input data and measuring its expected rank. For robust estimation, an ε-rank is computed by counting singular values above a threshold ε. The study uses MLPs and Deep VIB (Variational Information Bottleneck) models trained with Adam optimizer. Local rank is tracked during training across different datasets (synthetic Gaussian, MNIST, Fashion-MNIST) and varying β parameters in the VIB framework.

## Key Results
- Local rank decreases during the terminal phase of training on all tested datasets
- Higher β values in Deep VIB models lead to increased local rank and decreased accuracy
- The rank reduction phenomenon is consistent across different network architectures and datasets

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Local rank acts as a measurable proxy for manifold dimensionality, allowing us to track implicit compression during training.
- Mechanism: At each layer, the Jacobian's rank reflects how many independent directions the input can influence the pre-activation. As training progresses, gradient descent implicitly reduces this rank by pushing singular values toward zero, compressing the feature manifold.
- Core assumption: The Jacobian rank correlates with the intrinsic dimensionality of the learned representation.

### Mechanism 2
- Claim: The Information Bottleneck (IB) theory provides a principled explanation for why rank reduction correlates with mutual information compression.
- Mechanism: IB seeks representations that retain maximal predictive power about the target while discarding irrelevant input information. Reducing local rank enforces a bottleneck by limiting the degrees of freedom in the representation, which mathematically aligns with compressing mutual information between input and intermediate layers.
- Core assumption: Lower local rank implies lower mutual information with the input, given a fixed predictive capacity.

### Mechanism 3
- Claim: Implicit regularization during gradient descent drives networks toward low-rank solutions without explicit constraints.
- Mechanism: Gradient flow with exponential-tailed losses converges to KKT points of a norm-minimization problem subject to classification constraints. This process favors solutions where weight matrices have small operator norms, which in turn leads to low-rank Jacobians and reduced local rank.
- Core assumption: Gradient descent on overparameterized networks implicitly biases toward low-rank weight matrices.

## Foundational Learning

- Concept: Jacobian rank and singular value decomposition (SVD)
  - Why needed here: Local rank is defined via the expected rank of the Jacobian matrix; understanding SVD is essential to interpret rank and its approximation via singular values.
  - Quick check question: If a Jacobian has singular values [5.2, 3.1, 0.01, 0.0003] and ε = 0.1, what is rank_ε?
    - Answer: 2 (only the first two singular values exceed ε).

- Concept: Information Bottleneck theory
  - Why needed here: The paper connects local rank reduction to mutual information compression, which is the core of IB theory.
  - Quick check question: In IB, what happens to I(T;X) as β increases?
    - Answer: I(T;X) increases because the penalty on compression is reduced, allowing more information to pass through.

- Concept: Implicit regularization in deep learning
  - Why needed here: The theoretical analysis relies on understanding how gradient descent implicitly biases networks toward low-norm, low-rank solutions.
  - Quick check question: Why does minimizing weight norm lead to low-rank solutions?
    - Answer: Smaller weight norms often correspond to configurations where some directions in parameter space are "turned off," reducing effective rank.

## Architecture Onboarding

- Component map: Data pipeline → MLP model (with configurable depth/width) → Local rank computation module → Loss function (MSE/CrossEntropy/VIB) → Optimizer (Adam) → Training loop → Rank tracking/plotting

- Critical path:
  1. Forward pass to compute pre-activations
  2. Jacobian estimation (finite differences or analytic if possible)
  3. Rank computation (exact or ε-rank)
  4. Backpropagation with chosen loss
  5. Weight update
  6. Rank logging for analysis

- Design tradeoffs:
  - Exact vs. approximate Jacobian: Exact requires O(n²) storage; finite differences scale better but introduce noise
  - ε threshold choice: Too large → undercount rank; too small → noisy estimates and computational cost
  - Network depth/width: Deeper networks may show stronger rank reduction; wider networks may dilute the effect per layer

- Failure signatures:
  - Local rank plateaus early → either the network has already compressed maximally or the learning rate is too low to induce further compression
  - Rank increases during training → possible optimization issues or insufficient regularization
  - No correlation between β and rank in VIB experiments → potential bugs in the encoder or incorrect β scheduling

- First 3 experiments:
  1. Train a 3-layer MLP on synthetic Gaussian data and plot local rank vs. epoch for each layer
  2. Train a Deep VIB model on MNIST, vary β, and plot local rank and accuracy to confirm the inverse relationship
  3. Compare local rank trajectories between ReLU and linear activations to isolate the effect of nonlinearity

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the local rank reduction phenomenon generalize to other network architectures beyond MLPs, such as convolutional neural networks or transformers?
- Basis in paper: [inferred] The paper focuses on MLPs but mentions in the discussion that future work could "extend the analysis to other network architectures."
- Why unresolved: The empirical evidence is limited to MLPs on synthetic and real-world datasets (MNIST, Fashion-MNIST), leaving open whether the phenomenon is specific to fully connected networks.
- What evidence would resolve it: Experiments measuring local rank reduction during training of CNNs, transformers, and other architectures on comparable datasets.

### Open Question 2
- Question: What is the precise relationship between local rank reduction and generalization performance across different learning tasks and data distributions?
- Basis in paper: [explicit] The paper states "Understanding the behavior of local rank has implications for model compression, generalization, and the design of neural network architectures."
- Why unresolved: While the paper demonstrates local rank reduction correlates with information compression, it doesn't establish a clear link to generalization bounds or task performance beyond the observed accuracy decrease with increasing β.
- What evidence would resolve it: Systematic experiments varying task complexity, dataset characteristics, and network architectures to measure how local rank reduction relates to test accuracy, robustness, and other generalization metrics.

### Open Question 3
- Question: How does the local rank reduction phenomenon manifest in the presence of different activation functions and training procedures?
- Basis in paper: [inferred] The paper uses ReLU activation and Adam optimizer but doesn't explore how different choices affect local rank dynamics.
- Why unresolved: The theoretical analysis assumes ReLU networks, and the empirical validation uses a specific optimizer, leaving uncertainty about the phenomenon's robustness to architectural choices.
- What evidence would resolve it: Comparative experiments training networks with different activation functions (ReLU, tanh, sigmoid) and optimization methods (SGD, AdamW, etc.) while tracking local rank changes.

## Limitations
- Theoretical connection between local rank reduction and mutual information compression relies on assumptions about data distribution and network architecture
- Empirical evidence limited to relatively simple datasets (MNIST, Fashion-MNIST, synthetic Gaussian)
- Mechanism linking implicit regularization to low-rank solutions is primarily theoretical with limited empirical validation

## Confidence
- **High confidence**: Local rank can be computed and decreases during training on tested datasets
- **Medium confidence**: The connection between local rank reduction and Information Bottleneck theory
- **Low confidence**: The claim that gradient descent implicitly regularizes toward low-rank solutions through norm minimization

## Next Checks
1. Test local rank dynamics on more complex datasets (CIFAR-10, ImageNet) to verify if the compression phenomenon holds beyond simple datasets
2. Conduct ablation studies comparing local rank trajectories between networks with and without explicit low-rank regularization to isolate implicit effects
3. Measure actual mutual information (not just rank) between inputs and representations across training to directly validate the IB connection