---
ver: rpa2
title: 'In Case You Missed It: ARC ''Challenge'' Is Not That Challenging'
arxiv_id: '2412.17758'
source_url: https://arxiv.org/abs/2412.17758
tags:
- options
- separation
- llama
- other
- evaluation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: ARC Challenge evaluation setup, which presents answer choices in
  isolation, falsely inflates perceived difficulty by up to 35%, as it prevents models
  from directly comparing options. A fairer setup showing all options together reduces
  this gap and reveals models perform much better, even surpassing human performance
  on OpenBookQA.
---

# In Case You Missed It: ARC 'Challenge' Is Not That Challenging

## Quick Facts
- arXiv ID: 2412.17758
- Source URL: https://arxiv.org/abs/2412.17758
- Authors: Åukasz Borchmann
- Reference count: 11
- ARC Challenge evaluation setup falsely inflates perceived difficulty by up to 35%

## Executive Summary
The ARC Challenge evaluation setup, which presents answer choices in isolation, creates an artificial barrier that falsely inflates the perceived difficulty of questions. By forcing models to evaluate each option separately rather than seeing all choices together, the setup introduces unnecessary complexity that doesn't reflect how humans naturally approach multiple-choice problems. This evaluation artifact can make strong models appear significantly weaker than they actually are.

The author demonstrates that a fairer evaluation setup showing all answer choices together reduces this artificial difficulty gap and reveals models perform much better than previously thought, even surpassing human performance on certain benchmarks like OpenBookQA. The recommendation is clear: multiple-choice evaluations should always present all answer choices together to accurately reflect model reasoning capabilities and enable fair comparisons across different approaches.

## Method Summary
The study compares two evaluation setups for multiple-choice questions: one where answer options are presented in isolation (standard lm_eval configuration) and another where all options are shown together (using arc_utils.doc_to_text function). The author evaluates base model variants (Llama, Mistral, Qwen, Yi, Gemma) using Hugging Face implementations and likelihood-based scoring. The analysis focuses on ARC Challenge, ARC Easy, SIQA, and OpenBookQA datasets, measuring how accuracy scores differ between the two evaluation approaches.

## Key Results
- ARC Challenge evaluation setup falsely inflates perceived difficulty by up to 35%
- A fairer setup showing all options together reduces performance gaps (e.g., on SIQA)
- Models can achieve superhuman results on OpenBookQA when all options are presented together

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Presenting all answer choices together allows models to directly compare and contextualize options, leading to better accuracy.
- Mechanism: The model can leverage comparative reasoning to identify the best answer by evaluating options against each other rather than in isolation.
- Core assumption: Many questions in ARC datasets require comparative evaluation to determine the correct answer.
- Evidence anchors:
  - [abstract]: "a fairer setup showing all options together reduces this gap and reveals models perform much better"
  - [section]: "Questions like 'Which of these most likely has the greatest mass?' and the option 'puppy' cannot be determined without comparing the mass of the 'puppy' to the masses of all other provided options"
  - [corpus]: Weak - no direct corpus evidence for this specific mechanism
- Break condition: If questions do not require comparative reasoning, the benefit of showing all options together diminishes.

### Mechanism 2
- Claim: The separation setup falsely implies reasoning deficits by creating unnecessary obstacles that don't reflect natural human reasoning processes.
- Mechanism: By forcing models to evaluate each option in isolation, the setup introduces artificial difficulty that doesn't test actual reasoning capabilities.
- Core assumption: The natural way humans approach multiple-choice questions is to see all options simultaneously.
- Evidence anchors:
  - [abstract]: "evaluation shapes perceived difficulty"
  - [section]: "The separation setup is unnecessarily complicated and not consistent with how humans would approach the multi-choice problem"
  - [corpus]: Weak - limited corpus evidence on human reasoning processes in multiple-choice settings
- Break condition: If the separation setup is the standard for human evaluation, this mechanism breaks down.

### Mechanism 3
- Claim: The evaluation method creates artifacts that misrepresent model capabilities, particularly on benchmarks like SIQA and OpenBookQA.
- Mechanism: Changing the evaluation setup can dramatically improve scores, suggesting that previous interpretations of model performance were flawed.
- Core assumption: The current evaluation setup is not optimal for measuring true model capabilities.
- Evidence anchors:
  - [abstract]: "dramatically reduce performance gaps (e.g. on SIQA) and even yield superhuman results (OpenBookQA)"
  - [section]: "The fact that strong LLMs perform 30% worse than humans on SIQA doesn't mean they are deficient in social reasoning if under options the difference largely disappears"
  - [corpus]: Weak - limited corpus evidence on the impact of evaluation methods on perceived model capabilities
- Break condition: If the evaluation method is changed to options, the perceived performance gap should decrease significantly.

## Foundational Learning

- Concept: Multiple-choice question evaluation
  - Why needed here: Understanding how different evaluation setups affect model performance is crucial for accurate assessment of model capabilities.
  - Quick check question: What are the two main conventions for evaluating multiple-choice questions in LLMs?

- Concept: Comparative reasoning in language models
  - Why needed here: The paper argues that many questions require comparing options to determine the correct answer.
  - Quick check question: How does presenting all options together enable comparative reasoning?

- Concept: Evaluation artifacts and measurement bias
  - Why needed here: The study demonstrates how evaluation methodology can create misleading impressions of model performance.
  - Quick check question: What are common sources of bias in AI model evaluation?

## Architecture Onboarding

### Component Map
Evaluation Setup -> Model Performance -> Benchmark Comparison -> Human Performance

### Critical Path
1. ARC Challenge isolation evaluation setup creates artificial difficulty
2. Models appear weaker than actual reasoning capabilities
3. Options setup reveals true performance levels
4. Performance gaps dramatically reduce or reverse

### Design Tradeoffs
- Isolation evaluation: Easier to implement, but creates artificial difficulty
- Options evaluation: More complex setup, but provides fairer assessment of reasoning capabilities
- Human vs. model comparison: Standard isolation setup may disadvantage models unfairly

### Failure Signatures
- Performance gaps that disappear when all options are shown together
- Models scoring significantly below human levels on isolation setup but exceeding human performance on options setup
- Benchmarks showing consistent underperformance across multiple models using isolation evaluation

### Three First Experiments
1. Run ARC Challenge with both isolation and options setups on the same model to quantify the performance gap
2. Test whether the effect generalizes to other multiple-choice benchmarks beyond ARC, SIQA, and OpenBookQA
3. Evaluate human performance under both isolation and options conditions to verify the comparative advantage

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What specific proportion of ARC questions require comparative reasoning that cannot be answered in isolation?
- Basis in paper: Explicit - The paper states "such 'hardly

## Limitations

- The analysis relies on limited direct corpus evidence for some mechanisms, particularly regarding human reasoning processes
- Uncertainty about whether performance differences are due solely to evaluation setup versus other factors like prompt formatting
- Limited generalization to other benchmarks beyond ARC, SIQA, and OpenBookQA
- Assumption that showing all options together better reflects natural human reasoning lacks strong corpus support

## Confidence

- Core claim about evaluation setup impact: High confidence
- Mechanism explaining why comparison helps: Medium confidence
- Assumption about human-like evaluation: Low confidence
- Generalization to other benchmarks: Medium confidence

## Next Checks

1. **Replicate with standardized prompts**: Run the evaluation using exactly the same prompt templates and normalization methods as the original technical reports to isolate the effect of evaluation setup from formatting differences.

2. **Test with human evaluators**: Conduct a controlled study where humans answer the same questions under both isolation and options conditions to verify whether the performance gap exists for human reasoning as well.

3. **Cross-dataset validation**: Test the evaluation setup effect across a broader range of multiple-choice datasets beyond ARC, SIQA, and OpenBookQA to determine if the phenomenon generalizes or is dataset-specific.