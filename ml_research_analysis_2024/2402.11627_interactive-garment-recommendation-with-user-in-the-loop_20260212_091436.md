---
ver: rpa2
title: Interactive Garment Recommendation with User in the Loop
arxiv_id: '2402.11627'
source_url: https://arxiv.org/abs/2402.11627
tags:
- user
- recommendation
- feedback
- bottom
- garment
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces an interactive garment recommendation system
  that learns user preferences in real-time without prior knowledge. The core method
  uses reinforcement learning to recommend clothing items iteratively, integrating
  user feedback at each step to refine suggestions.
---

# Interactive Garment Recommendation with User in the Loop

## Quick Facts
- arXiv ID: 2402.11627
- Source URL: https://arxiv.org/abs/2402.11627
- Reference count: 40
- Primary result: RL-based system learns user preferences in real-time without prior knowledge

## Executive Summary
This paper introduces an interactive garment recommendation system that learns user preferences in real-time without prior knowledge. The core method uses reinforcement learning to recommend clothing items iteratively, integrating user feedback at each step to refine suggestions. A proxy model (GP-BPR) simulates user feedback during training, enabling the system to personalize recommendations effectively. Experiments on the IQON3000 dataset show that the reinforcement learning agent outperforms non-reinforcement models, achieving higher compatibility scores and recommending diverse items tailored to user preferences. The system demonstrates the importance of exploration during training for effective personalization.

## Method Summary
The proposed system employs a reinforcement learning framework where an agent learns to recommend garments based on user feedback. During training, a proxy GP-BPR model simulates user responses to create a training signal. The RL agent iteratively recommends items, receives feedback, and updates its policy to better match user preferences. The system emphasizes exploration during training to discover diverse preferences, and the interactive loop allows continuous refinement of recommendations as user preferences evolve over time.

## Key Results
- RL agent outperforms non-reinforcement models on IQON3000 dataset
- System achieves higher compatibility scores with user preferences
- Demonstrates ability to recommend diverse items tailored to individual users
- Exploration during training proves crucial for effective personalization

## Why This Works (Mechanism)
The system works by treating garment recommendation as a sequential decision-making problem. The RL agent learns a policy that maps user interaction history to garment recommendations. At each step, the agent observes the current state (user preferences and interaction history), takes an action (recommends a garment), and receives feedback that updates the state. Through repeated interactions, the agent learns which types of recommendations lead to positive feedback and adjusts its policy accordingly. The GP-BPR proxy model provides a consistent training signal by simulating how users would respond to different recommendations.

## Foundational Learning
1. **Reinforcement Learning Basics** - Why needed: Core framework for learning from sequential interactions; Quick check: Agent learns optimal policy through reward signals
2. **Collaborative Filtering** - Why needed: Understanding baseline recommendation approaches; Quick check: Can explain user-item interaction patterns
3. **Fashion Compatibility Modeling** - Why needed: Domain-specific knowledge for garment recommendations; Quick check: Can identify compatible clothing combinations
4. **Exploration vs Exploitation** - Why needed: Balance between trying new recommendations and optimizing known preferences; Quick check: System explores sufficiently during training
5. **Proxy Modeling** - Why needed: Enables training without real user interactions; Quick check: Proxy accurately simulates user behavior
6. **Sequential Decision Making** - Why needed: Each recommendation affects future user state; Quick check: System maintains and updates user state appropriately

## Architecture Onboarding

**Component Map:** User Interaction -> RL Agent -> Garment Recommendation -> Proxy GP-BPR Feedback -> Policy Update -> User Interaction

**Critical Path:** User provides initial preferences → RL agent recommends garment → Proxy model generates feedback → Agent updates policy → New recommendation cycle

**Design Tradeoffs:** 
- Real-time interaction vs. recommendation quality (favoring interactivity)
- Exploration vs. exploitation balance (favoring exploration during training)
- Proxy model accuracy vs. training efficiency (accepting proxy limitations)
- Model complexity vs. computational cost (prioritizing effectiveness)

**Failure Signatures:**
- Poor recommendations indicate insufficient exploration during training
- Inconsistent feedback suggests proxy model misalignment
- Slow convergence implies suboptimal reward structure
- Lack of diversity suggests over-specialization to training data

**3 First Experiments:**
1. Test RL agent with synthetic user feedback to verify learning capability
2. Compare recommendation diversity with and without exploration
3. Validate proxy GP-BPR model accuracy against real user data

## Open Questions the Paper Calls Out
None

## Limitations
- Evaluation relies on simulated user feedback rather than real human responses
- Single dataset (IQON3000) limits generalizability across different fashion domains
- Potential biases in dataset not addressed for diverse demographic groups
- Computational costs of real-time RL interaction not discussed

## Confidence
- High confidence in technical RL framework implementation
- Medium confidence in GP-BPR proxy model's ability to simulate user preferences
- Low confidence in real-world performance without user studies

## Next Checks
1. Conduct user study with real participants to validate against simulated feedback results
2. Test system on multiple fashion datasets to assess generalizability
3. Evaluate computational efficiency and scalability in production environments