---
ver: rpa2
title: Checkpoint Merging via Bayesian Optimization in LLM Pretraining
arxiv_id: '2403.19390'
source_url: https://arxiv.org/abs/2403.19390
tags:
- merging
- performance
- checkpoints
- checkpoint
- optimization
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the computational burden of pretraining large
  language models (LLMs) by proposing a checkpoint merging strategy that leverages
  intermediate checkpoints to improve model performance without requiring additional
  pretraining. The authors conduct pilot experiments to identify optimal checkpoint
  combinations and develop a Bayesian optimization framework to find the best merging
  weights.
---

# Checkpoint Merging via Bayesian Optimization in LLM Pretraining

## Quick Facts
- arXiv ID: 2403.19390
- Source URL: https://arxiv.org/abs/2403.19390
- Authors: Deyuan Liu; Zecheng Wang; Bingning Wang; Weipeng Chen; Chunshan Li; Zhiying Tu; Dianhui Chu; Bo Li; Dianbo Sui
- Reference count: 40
- Primary result: Achieves up to 1.01% improvement on evaluation metrics through Bayesian-optimized checkpoint merging

## Executive Summary
This paper addresses the computational burden of pretraining large language models (LLMs) by proposing a checkpoint merging strategy that leverages intermediate checkpoints to improve model performance without requiring additional pretraining. The authors conduct pilot experiments to identify optimal checkpoint combinations and develop a Bayesian optimization framework to find the best merging weights. Experiments demonstrate that their method consistently outperforms existing checkpoint merging approaches across multiple model architectures and benchmarks, achieving improvements of up to 1.01% on evaluation metrics. The merged models also maintain strong generalization across diverse domains and model sizes ranging from 70M to 6.9B parameters.

## Method Summary
The method utilizes intermediate checkpoints from the same pretraining trajectory and employs Bayesian optimization to find optimal merging weights. The approach begins with pilot experiments to identify beneficial checkpoint combinations, then uses Gaussian Process regression to model the performance landscape as a function of merging weights. The GP-hedge acquisition strategy balances exploration and exploitation to efficiently search the weight space. The framework is grounded in PAC-Bayesian generalization bounds, showing that merging reduces KL divergence between posterior and prior distributions, leading to tighter generalization bounds. The method is particularly effective for adjacent checkpoints in the pretraining trajectory and can be applied to various model architectures including Baichuan2, DeepSeek, and Pythia models.

## Key Results
- Consistent improvement over baseline merging methods (Uniform Soup, Greedy Soup, Fisher Weighted Averaging, RegMean) across multiple model architectures
- Performance gains up to 1.01% on benchmark datasets (C-Eval, CMMLU, MMLU, GSM8K, PIQA, WinoGrande, SciQ, ARC-Easy)
- Effective across diverse model sizes from 70M to 6.9B parameters
- Strong generalization maintained across different domains after merging
- Bayesian optimization efficiently identifies optimal merging weights without exhaustive search

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Checkpoint merging improves model performance by leveraging intermediate checkpoints from the same pretraining trajectory.
- Mechanism: Linearly combining checkpoints smooths the loss landscape, allowing the model to settle into flatter minima that generalize better.
- Core assumption: The loss landscape is smooth and locally quadratic near checkpoints, enabling beneficial interpolation between parameter states.
- Evidence anchors:
  - [abstract] "This method utilizes intermediate checkpoints with shared training trajectories, and is rooted in an extensive search space exploration for the best merging weight via Bayesian optimization."
  - [section 3.1] "The merged checkpoint eΘt can outperform the most recent checkpoint Θt when an optimal or near-optimal λt is selected."
  - [corpus] Weak - no direct evidence in corpus papers about this specific mechanism.
- Break condition: If checkpoints are too far apart in training trajectory, destructive interference occurs, leading to performance degradation.

### Mechanism 2
- Claim: Bayesian optimization efficiently finds optimal merging weights by modeling the performance function as a Gaussian Process.
- Mechanism: GP regression captures uncertainty and smoothness of the performance landscape, allowing acquisition functions to balance exploration and exploitation.
- Core assumption: The performance function f(λt) is smooth and can be modeled as a GP with appropriate kernel.
- Evidence anchors:
  - [section 3.2] "We model the objective function f(λt) using Gaussian process (GP) regression (Rasmussen, 2003), which provides a probabilistic framework for modeling unknown functions."
  - [section 3.3] "Using the PAC-Bayesian generalization bound (McAllester, 1998), it holds with probability at least 1 − δ: EΘ∼Q [LD(Θ)] ≤ LS(Q) + ..."
  - [corpus] Weak - corpus papers focus on checkpoint merging but not on Bayesian optimization for weight selection.
- Break condition: If the performance landscape is highly non-smooth or multimodal with narrow peaks, GP modeling may fail to capture the true optimum.

### Mechanism 3
- Claim: Merging checkpoints reduces the KL divergence between posterior and prior distributions, leading to tighter generalization bounds.
- Mechanism: By averaging parameters, merging effectively regularizes the model, moving it closer to the prior distribution and reducing overfitting risk.
- Core assumption: The prior distribution P = N(Θt-1, σ²P I) and posterior Q = N(eΘt, σ²Q I) are well-defined Gaussian distributions.
- Evidence anchors:
  - [section 3.3] "Since λt ≤ 1, the KL divergence in (Eq. (11)) is reduced compared to using Θt alone. This leads to a tighter generalization bound in (Eq. (12)), indicating that checkpoint merging can improve generalization by effectively regularizing the model."
  - [section D.3.4] "Since λt ≤ 1, it follows that: DKL(Q ∥ P) = λ²t DKL(QΘt ∥ P) ≤ DKL(QΘt ∥ P)."
  - [corpus] Weak - no direct evidence in corpus papers about PAC-Bayesian generalization bounds for checkpoint merging.
- Break condition: If the variance parameters σ²P and σ²Q are not appropriately chosen, the KL divergence calculation and generalization bound may not hold.

## Foundational Learning

- Concept: Gaussian Processes and Bayesian Optimization
  - Why needed here: GP modeling captures uncertainty in the performance function, and Bayesian optimization efficiently searches for optimal merging weights without exhaustive evaluation.
  - Quick check question: What is the role of the acquisition function in Bayesian optimization, and how does it balance exploration vs. exploitation?

- Concept: PAC-Bayesian Framework
  - Why needed here: Provides theoretical justification for why checkpoint merging improves generalization by reducing KL divergence between posterior and prior distributions.
  - Quick check question: How does the KL divergence between posterior and prior distributions relate to generalization performance in the PAC-Bayesian framework?

- Concept: Neural Network Loss Landscape Properties
  - Why needed here: Understanding smoothness, local convexity, and Hessian bounds is crucial for analyzing checkpoint merging effects on convergence and generalization.
  - Quick check question: What assumptions about the loss landscape are necessary for the quadratic approximation used in the convergence analysis?

## Architecture Onboarding

- Component map: Checkpoint Storage -> Bayesian Optimization Engine -> Merging Module -> Evaluation Pipeline -> Generalization Analysis
- Critical path:
  1. Load available checkpoints from pretraining run
  2. Initialize GP with initial observations (e.g., λ=0.5 and λ=1.0)
  3. Select next λ using acquisition function (GP-hedge strategy)
  4. Merge checkpoints with selected weight
  5. Evaluate merged model on held-out dataset
  6. Update GP posterior with new observation
  7. Repeat until convergence or budget exhausted
  8. Select best λ and generate final merged model
- Design tradeoffs:
  - Search space size (α parameter) vs. computational efficiency
  - Number of GP observations vs. accuracy of optimization
  - Held-out dataset size vs. generalization reliability
  - Acquisition function complexity (GP-hedge vs. single function) vs. performance
- Failure signatures:
  - Poor performance improvement despite optimization → Checkpoints too far apart or inappropriate α value
  - GP uncertainty remains high → Insufficient observations or inappropriate kernel choice
  - Optimization converges slowly → Acquisition function not balancing exploration/exploitation well
  - Generalization gap remains large → PAC-Bayesian assumptions violated or variance parameters incorrect
- First 3 experiments:
  1. Test merging two adjacent checkpoints with varying λ values (grid search) to visualize performance landscape
  2. Apply Bayesian optimization with GP-hedge acquisition to find optimal λ for a known good checkpoint pair
  3. Evaluate generalization by testing merged model on out-of-domain datasets after optimizing on in-domain data

## Open Questions the Paper Calls Out
None

## Limitations
- Theoretical analysis relies on assumptions about loss landscape smoothness and local convexity that may not hold for complex LLMs
- Effectiveness demonstrated primarily on Chinese LLMs (Baichuan2) with limited validation on other architectures
- Computational overhead of Bayesian optimization not fully quantified against pretraining time saved
- Optimal choice of variance parameters σ²P and σ²Q for PAC-Bayesian bounds not specified

## Confidence
**High Confidence:** The experimental results showing consistent improvements over baseline merging methods across multiple model architectures and benchmarks.

**Medium Confidence:** The theoretical justification using PAC-Bayesian generalization bounds, as practical applicability depends on specific choice of variance parameters.

**Low Confidence:** The claim that the method is particularly effective for adjacent checkpoints, as systematic exploration of checkpoint spacing was not conducted.

## Next Checks
1. Conduct empirical analysis of the loss landscape smoothness and Hessian bounds for the Baichuan2 model to validate theoretical assumptions.

2. Systematically evaluate the impact of checkpoint spacing on merging effectiveness, testing both adjacent and distant checkpoints.

3. Apply the checkpoint merging method to non-Chinese LLM architectures (e.g., Llama, Mistral) and different pretraining objectives to assess generality.