---
ver: rpa2
title: Cooperative Strategic Planning Enhances Reasoning Capabilities in Large Language
  Models
arxiv_id: '2410.20007'
source_url: https://arxiv.org/abs/2410.20007
tags:
- reasoning
- agent
- planning
- policy
- hint
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces CoPlanner, a cooperative multi-agent framework
  for enhancing reasoning in large language models by decomposing complex problems
  into high-level planning and focused reasoning steps. It uses two agents: a planning
  agent that selects meta-strategies (e.g., deduction, induction) to guide reasoning
  and a reasoning agent that executes step-wise solutions.'
---

# Cooperative Strategic Planning Enhances Reasoning Capabilities in Large Language Models

## Quick Facts
- arXiv ID: 2410.20007
- Source URL: https://arxiv.org/abs/2410.20007
- Reference count: 15
- LLaMA-3-8B-based CoPlanner outperforms previous best by 9.94% on LogiQA and 3.09% on BBH

## Executive Summary
This paper introduces CoPlanner, a cooperative multi-agent framework that enhances reasoning in large language models by decomposing complex problems into high-level planning and focused reasoning steps. The framework uses two specialized agents: a planning agent that selects meta-strategies to guide reasoning, and a reasoning agent that executes step-wise solutions. By training the planning agent's policy through Proximal Policy Optimization (PPO) based on interactive reasoning processes, CoPlanner achieves significant performance improvements on LogiQA and BBH benchmarks compared to previous methods.

## Method Summary
CoPlanner implements a two-agent cooperative framework where a planning agent provides high-level strategic hints through meta-strategy selection, while a reasoning agent follows these hints to generate step-wise solutions. The planning agent's policy is trained using PPO, initialized with behavioral cloning on filtered training data. The framework includes difficulty-aware curriculum filtering to improve training efficiency by focusing on problems with moderate difficulty that provide meaningful feedback signals.

## Key Results
- LLaMA-3-8B-based CoPlanner outperforms previous best method by 9.94% on LogiQA benchmark
- Mistral-7B-based CoPlanner achieves 3.09% improvement over prior work on BBH benchmark
- CoPlanner with PPO-based training shows significant advantage over baselines including direct prompting, few-shot prompting, and Chain-of-Thought methods

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Separating high-level planning from step-by-step execution allows each agent to focus on its strengths.
- Mechanism: The planning agent selects meta-strategies (deduction, induction, etc.) while the reasoning agent follows those instructions without handling the overall problem decomposition.
- Core assumption: LLMs can effectively follow concrete instructions but struggle with strategic planning on their own.
- Evidence anchors:
  - [abstract] "CoPlanner consists of two LLM agents: a planning agent and a reasoning agent. The planning agent provides high-level strategic hints, while the reasoning agent follows these hints and infers answers."
  - [section] "The reasoning agent's sole responsibility is to accurately follow the immediate instruction provided by the planning agent, while the planning agent handles the higher-level planning and decomposition of the overall problem into a sequence of steps."

### Mechanism 2
- Claim: PPO training on interactive reasoning processes enables the planning agent to learn effective hint selection policies.
- Mechanism: The planning agent receives rewards based on the reasoning agent's final answer, updates its policy network via PPO to maximize expected rewards, and generates better meta-strategy hints over time.
- Core assumption: The reward signal (correct answer = 1, incorrect = -1) provides sufficient feedback for policy improvement.
- Evidence anchors:
  - [abstract] "By training the planning agent's policy through the interactive reasoning process via Proximal Policy Optimization (PPO), the LLaMA-3-8B-based CoPlanner outperforms the previous best method by 9.94% on LogiQA and 3.09% on BBH."
  - [section] "We use Proximal Policy Optimization (PPO) (Schulman et al., 2017) to train the policy network... The updates are based on the observed rewards and states, allowing the planning agent to adapt and improve its hints over time."

### Mechanism 3
- Claim: Difficulty-aware curriculum filtering improves training efficiency by focusing on informative problems.
- Mechanism: Problems are filtered based on success rate during behavior cloning initialization, removing both too-easy (always correct) and too-hard (always incorrect) problems to stabilize PPO training.
- Core assumption: Problems with moderate difficulty provide more meaningful feedback signals for policy updates.
- Evidence anchors:
  - [section] "We observe that simple problems contribute less to the RL training because they get a reward of 1 most of the time. Meanwhile, the most challenging ones always receive negative rewards, hindering the training. Therefore, we sort the training problems by their difficulty and filter the easiest and hardest ones."
  - [section] "By focusing on problems that provide meaningful feedback and opportunities for improvement, we can optimize the interaction between the reasoning agent and the planning agent, leading to more effective policy updates and enhanced overall performance."

## Foundational Learning

- Concept: Proximal Policy Optimization (PPO)
  - Why needed here: PPO provides stable policy updates for the planning agent by constraining policy changes through clipping, preventing destructive updates from noisy reward signals.
  - Quick check question: What is the main advantage of PPO's clipping mechanism compared to vanilla policy gradient methods?

- Concept: Behavioral Cloning Initialization
  - Why needed here: Initializing the planning agent's policy with successful trajectories from behavior cloning provides a good starting point before PPO training, reducing the exploration space and improving training stability.
  - Quick check question: How does behavior cloning help address the sparse reward problem in reinforcement learning for reasoning tasks?

- Concept: Meta-strategy Selection
  - Why needed here: Pre-defining meta-strategies (deduction, induction, etc.) provides a structured action space for the planning agent, making the policy learning problem more tractable than generating arbitrary hints.
  - Quick check question: Why might a discrete meta-strategy pool be more effective than allowing the planning agent to generate arbitrary hints?

## Architecture Onboarding

- Component map:
  Planning Agent -> Reasoning Agent -> Reward Computation -> Policy Update
  (Policy Network + Value Network + LLM) -> (LLM) -> (Correct/Incorrect) -> (PPO)

- Critical path: Planning agent selects meta-strategy → Generates concrete hint → Reasoning agent follows hint → Produces answer → Compute reward → Update planning agent policy

- Design tradeoffs:
  - Number of interaction rounds: More rounds allow complex reasoning but increase training difficulty and inference time
  - Meta-strategy pool size: Larger pools provide more diverse hints but increase policy learning complexity
  - LLM model size: Larger models may perform better but increase computational costs

- Failure signatures:
  - Planning agent gets stuck in local optima selecting only one meta-strategy
  - Reasoning agent fails to follow hints despite correct meta-strategy selection
  - Training diverges after initial success due to poor reward signals

- First 3 experiments:
  1. Run CoPlanner with only 1 interaction round to establish baseline performance and verify basic cooperation
  2. Test different meta-strategy pool sizes (5 vs 10) to find optimal diversity vs. learning complexity tradeoff
  3. Compare PPO training with and without behavior cloning initialization to measure its impact on convergence speed and final performance

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does CoPlanner's performance scale with larger language models and more complex reasoning tasks beyond multi-choice questions?
- Basis in paper: [inferred] The paper notes they focused on 7B models due to computational limitations and primarily tested on multi-choice reasoning tasks.
- Why unresolved: The study didn't explore larger models or diverse problem types, leaving uncertainty about generalizability.
- What evidence would resolve it: Experiments testing CoPlanner with larger models (e.g., 70B+ parameters) on diverse reasoning tasks like mathematical proofs, scientific reasoning, or real-world planning scenarios.

### Open Question 2
- Question: What is the optimal number of interactive rounds between planning and reasoning agents for different types of reasoning problems?
- Basis in paper: [explicit] The paper shows performance varies with different round numbers (Figure 3) but doesn't provide a systematic analysis across problem types.
- Why unresolved: The study only tested up to 4 rounds and didn't analyze whether optimal round numbers vary by problem complexity or type.
- What evidence would resolve it: A comprehensive study mapping problem characteristics (complexity, reasoning type) to optimal round numbers, potentially revealing patterns or heuristics.

### Open Question 3
- Question: How does the meta-strategy selection mechanism perform compared to more dynamic approaches that generate hints directly without pre-defined strategies?
- Basis in paper: [explicit] The authors tested both "Pick Meta-strategy" and "Pick Hint" approaches, with the former performing better, but didn't explore more sophisticated dynamic generation methods.
- Why unresolved: The study only compared two basic approaches and didn't investigate more advanced methods for generating context-aware hints.
- What evidence would resolve it: Comparative experiments testing CoPlanner against methods that use learned prompt engineering, retrieval-augmented generation, or other advanced techniques for hint generation.

## Limitations
- Implementation details for meta-strategy pool definitions and concrete hint generation process are not specified
- PPO hyperparameters and training configuration lack precise description
- Evaluation only compares against relatively simple baselines without examining more sophisticated reasoning methods

## Confidence

- **High Confidence**: The fundamental two-agent architecture (planning + reasoning) is well-specified and the reported benchmark improvements (9.94% on LogiQA, 3.09% on BBH) are clearly stated with appropriate methodology description.
- **Medium Confidence**: The PPO training methodology and the general concept of difficulty-aware curriculum filtering are described, but implementation details are sparse enough to create uncertainty in reproduction.
- **Low Confidence**: The specific meta-strategy definitions, exact behavioral cloning initialization process, and precise filtering criteria are insufficiently detailed for reliable reproduction.

## Next Checks

1. **Component Isolation Test**: Run CoPlanner with only behavior cloning (no PPO fine-tuning) to establish the baseline contribution of the curriculum-filtered initialization and measure the isolated impact of the meta-strategy approach.

2. **Meta-strategy Ablation Study**: Systematically test CoPlanner with different meta-strategy pool sizes (2, 5, 10, 15 strategies) to empirically determine the optimal tradeoff between strategy diversity and policy learning complexity, while measuring the impact on training stability and final performance.

3. **Cooperation Quality Analysis**: Implement a detailed logging system to track the planning agent's meta-strategy selection patterns over training, the reasoning agent's hint-following success rate, and the correlation between specific meta-strategy types and problem-solving success to identify potential failure modes in the cooperative process.