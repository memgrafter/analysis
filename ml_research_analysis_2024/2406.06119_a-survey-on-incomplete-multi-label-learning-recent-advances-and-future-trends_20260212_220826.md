---
ver: rpa2
title: 'A Survey on Incomplete Multi-label Learning: Recent Advances and Future Trends'
arxiv_id: '2406.06119'
source_url: https://arxiv.org/abs/2406.06119
tags:
- labels
- inmll
- multi-label
- learning
- label
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper provides a comprehensive survey of incomplete multi-label
  learning (InMLL), which addresses the challenge of learning from data with missing
  labels. InMLL is motivated by the practical difficulty of obtaining complete and
  accurate labels in real-world applications.
---

# A Survey on Incomplete Multi-label Learning: Recent Advances and Future Trends

## Quick Facts
- arXiv ID: 2406.06119
- Source URL: https://arxiv.org/abs/2406.06119
- Reference count: 40
- One-line primary result: Comprehensive survey of incomplete multi-label learning methods and open problems

## Executive Summary
This paper provides a comprehensive survey of incomplete multi-label learning (InMLL), addressing the challenge of learning from data with missing labels. The survey categorizes InMLL approaches based on data-oriented perspectives (label type and missing type) and algorithm-oriented perspectives (techniques employed). The paper identifies four open problems: non-random missing labels, weakly semi-supervised InMLL, InMLL open-set, and InMLL with noisy labels. It also suggests three future technical directions: leveraging multimodal large language models, addressing label imbalance, and applying noisy label learning techniques. The survey aims to provide a foundation for innovative research in InMLL by summarizing current progress and identifying unresolved challenges.

## Method Summary
The paper surveys existing InMLL methods and provides a taxonomy based on data-oriented and algorithm-oriented perspectives. Data-oriented categorization considers label type (binary, continuous, ordinal) and label missing type (missing completely at random, missing at random, missing not at random). Algorithm-oriented categorization includes low-rank based, graph-model based, probabilistic-model based, and loss design based methods. The survey provides formal objective functions and intuitions for each approach, discussing their assumptions and limitations. The paper identifies open problems and future directions, suggesting potential research avenues for the InMLL community.

## Key Results
- InMLL methods can be categorized into data-oriented and algorithm-oriented approaches
- Four open problems are identified: non-random missing labels, weakly semi-supervised InMLL, InMLL open-set, and InMLL with noisy labels
- Three future technical directions are suggested: leveraging multimodal large language models, addressing label imbalance, and applying noisy label learning techniques
- The survey provides a foundation for innovative research in InMLL by summarizing current progress and identifying unresolved challenges

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Low-rank assumptions enable effective label correlation capture in InMLL by exploiting the inherent structure in label relationships.
- Mechanism: By assuming the label matrix (or its related forms like coefficient matrix or label correlation matrix) is low-rank, InMLL methods can infer missing labels through matrix completion techniques. The low-rank structure implies that labels are not independent but have strong correlations, which can be exploited to fill in missing values.
- Core assumption: The label matrix has a low-rank structure globally or locally, meaning that the labels share underlying patterns or dependencies.
- Evidence anchors:
  - [abstract] The paper discusses low-rank based InMLL methods that assume the incomplete label matrix, coefficient matrix, or label correlation matrix is low-rank.
  - [section] The formal objective functions for low-rank based InMLL methods are provided, showing how minimizing rank (or nuclear norm) is used to infer missing labels.
  - [corpus] The corpus neighbors include a paper on "Label Distribution Learning with Biased Annotations by Learning Multi-Label Representation" which also uses low-rank assumptions.
- Break condition: If the label matrix is not actually low-rank (globally or locally), the low-rank based methods may not perform well. The paper mentions that labels are "just locally low-rank but globally high-rank" in practice, which could violate the global low-rank assumption.

### Mechanism 2
- Claim: Graph-model based methods effectively capture sample and label correlations in InMLL by constructing and utilizing similarity graphs.
- Mechanism: InMLL methods based on graph models construct graphs where vertices represent samples or labels, and edges represent their similarities. These graphs are then used to propagate label information and infer missing labels based on the assumption that similar samples should have similar labels or that labels with similar semantics should have similar label vectors.
- Core assumption: There exists a meaningful similarity structure among samples or labels that can be captured by a graph.
- Evidence anchors:
  - [abstract] The paper categorizes graph-model based InMLL methods as a major type of algorithm-oriented approach.
  - [section] The formal definition of sample correlation and label correlation graphs is provided, along with the intuition that similar samples should have similar labels.
  - [corpus] The corpus neighbors include a paper on "Multi-label Learning with Global and Local Label Correlation" which also explores label correlations.
- Break condition: If the similarity structure among samples or labels is not meaningful or does not reflect the true label correlations, the graph-model based methods may not perform well.

### Mechanism 3
- Claim: Probabilistic-model based methods effectively handle uncertainty in missing labels by modeling them as latent variables and incorporating prior knowledge.
- Mechanism: InMLL methods based on probabilistic models, particularly Bayesian models, treat missing labels as latent variables and estimate their posterior distributions. They can incorporate prior knowledge about the data distribution and label correlations to facilitate the learning process.
- Core assumption: The data follows a certain probabilistic distribution that can be modeled, and prior knowledge about the data can be incorporated.
- Evidence anchors:
  - [abstract] The paper discusses probabilistic-model based InMLL methods as one of the major algorithm-oriented approaches.
  - [section] The use of Bayesian models to explicitly model uncertainty using probability distributions and incorporate prior knowledge is explained.
  - [corpus] The corpus neighbors do not include any directly related papers on probabilistic models for InMLL, so the evidence is weak here.
- Break condition: If the assumed probabilistic distribution does not match the true data distribution, or if the prior knowledge is not accurate or helpful, the probabilistic-model based methods may not perform well.

## Foundational Learning

- Concept: Multi-label learning (MLL)
  - Why needed here: InMLL is a variant of MLL, so understanding the basics of MLL is crucial to grasp the challenges and solutions in InMLL.
  - Quick check question: What is the key difference between multi-class and multi-label learning?

- Concept: Matrix completion
  - Why needed here: Many InMLL methods use matrix completion techniques, particularly those based on low-rank assumptions, to infer missing labels.
  - Quick check question: How does the low-rank assumption help in matrix completion?

- Concept: Graph-based learning
  - Why needed here: Graph-model based InMLL methods rely on constructing and utilizing similarity graphs to capture label correlations.
  - Quick check question: What is the intuition behind using graph-based methods for label correlation learning?

## Architecture Onboarding

- Component map: InMLL methods can be broadly categorized into data-oriented (based on label type and missing type) and algorithm-oriented (based on techniques employed) approaches. The algorithm-oriented approaches include low-rank based, graph-model based, probabilistic-model based, and loss design based methods.
- Critical path: The critical path in InMLL is to effectively infer missing labels while leveraging the available label correlations and minimizing the performance gap with complete MLL.
- Design tradeoffs: InMLL methods need to balance between making reasonable assumptions about the data (e.g., low-rank, graph structure) and not over-constraining the model. They also need to consider the tradeoff between capturing label correlations and computational complexity.
- Failure signatures: InMLL methods may fail if the assumptions about the data are violated (e.g., label matrix is not low-rank, graph structure is not meaningful) or if the model is too complex and overfits to the limited available labels.
- First 3 experiments:
  1. Implement a simple low-rank based InMLL method using matrix completion and evaluate its performance on a standard InMLL dataset.
  2. Construct a sample correlation graph for a multi-label dataset and use it to infer missing labels, comparing the results with the low-rank method.
  3. Implement a probabilistic-model based InMLL method using a Bayesian approach and compare its performance with the previous two methods.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can non-random missing label patterns be effectively modeled in incomplete multi-label learning?
- Basis in paper: [explicit] The paper explicitly identifies "non-random missing multi-label problem" as an open problem, noting that existing methods assume uniformly random missing labels, while in reality, labels that are difficult to identify or require professional knowledge are more likely to be missing.
- Why unresolved: Current InMLL methods rely on assumptions of uniform label missingness, which doesn't reflect real-world annotation behavior where certain labels are systematically more likely to be missing based on difficulty or expertise requirements.
- What evidence would resolve it: Development and validation of InMLL algorithms that explicitly model non-random missingness patterns, tested on datasets with known annotation difficulty hierarchies (e.g., medical images with varying levels of annotation expertise).

### Open Question 2
- Question: How can weakly semi-supervised InMLL be effectively implemented when only a small proportion of labels are provided for some samples while others are completely unlabeled?
- Basis in paper: [explicit] The paper identifies "weakly semi-supervised InMLL problem" as an open problem, noting that it requires less label information than traditional semi-supervised learning but increases difficulty due to incomplete supervision.
- Why unresolved: Traditional semi-supervised learning algorithms assume complete labels for some samples, making them inappropriate for this setting where even labeled samples have missing information. The challenge involves both handling missing labels and utilizing unlabeled samples effectively.
- What evidence would resolve it: Novel algorithms specifically designed for weakly semi-supervised InMLL that demonstrate improved performance over baseline methods on benchmark datasets with varying levels of label incompleteness.

### Open Question 3
- Question: How can incomplete multi-label learning be effectively combined with noisy label learning techniques to handle both missing and incorrect labels simultaneously?
- Basis in paper: [explicit] The paper identifies "InMLL with noisy labels problem" as an open challenge, noting that missing labels are often assumed to be negative (creating false negatives) while annotators may also provide incorrect positive labels.
- Why unresolved: Existing InMLL methods focus primarily on missing labels while neglecting the issue of noisy labels, and existing noisy label learning techniques are not well-suited to the specific challenges of InMLL including severe label imbalance and insufficient supervision.
- What evidence would resolve it: Development and validation of robust InMLL algorithms that can simultaneously handle both missing and noisy labels, demonstrating performance improvements over methods that address only one of these issues.

## Limitations

- The paper relies heavily on categorizing existing approaches without providing extensive quantitative comparisons between different methods.
- The confidence in the mechanisms described is Medium for low-rank and graph-based approaches but Low for probabilistic models due to limited direct evidence in related literature.
- The discussion of future trends involving multimodal large language models is speculative and lacks concrete implementation details or validation.

## Confidence

- Mechanism 1 (Low-rank assumptions): Medium
- Mechanism 2 (Graph-based methods): Medium  
- Mechanism 3 (Probabilistic models): Low

## Next Checks

1. Conduct a controlled experiment comparing low-rank, graph-based, and probabilistic InMLL methods on the same benchmark dataset with varying degrees of label incompleteness.

2. Implement and evaluate the suggested future direction of leveraging multimodal large language models for InMLL on a real-world dataset with visual and textual features.

3. Design an experiment to test the proposed "non-random missing labels" open problem by artificially creating structured missing patterns and measuring their impact on different InMLL approaches.