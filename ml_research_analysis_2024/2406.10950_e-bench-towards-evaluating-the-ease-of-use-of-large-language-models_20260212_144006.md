---
ver: rpa2
title: 'E-Bench: Towards Evaluating the Ease-of-Use of Large Language Models'
arxiv_id: '2406.10950'
source_url: https://arxiv.org/abs/2406.10950
tags:
- prompt
- llms
- language
- arxiv
- wang
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a new benchmark E-Bench to evaluate the ease-of-use
  of LLMs by simulating real-world scenarios where users may use synonymous expressions
  or make typos in their prompts. The benchmark constructs perturbations through paraphrasing,
  simplification, colloquialism, and typing attacks, then manually reviews them to
  ensure semantic invariance.
---

# E-Bench: Towards Evaluating the Ease-of-Use of Large Language Models
## Quick Facts
- arXiv ID: 2406.10950
- Source URL: https://arxiv.org/abs/2406.10950
- Authors: Zhenyu Zhang; Bingguang Hao; Jinpeng Li; Zekai Zhang; Dongyan Zhao
- Reference count: 8
- Primary result: All tested LLMs experience performance degradation after prompt perturbation, with larger models generally performing better under synonymous perturbation but no clear scaling law for typing attacks.

## Executive Summary
This paper introduces E-Bench, a new benchmark designed to evaluate the ease-of-use of large language models by simulating real-world scenarios where users may use synonymous expressions or make typos in their prompts. The benchmark constructs perturbations through paraphrasing, simplification, colloquialism, and typing attacks, then manually reviews them to ensure semantic invariance. Experiments on 6 representative LLMs (Llama 2-chat, Vicuna, GPT-3.5, GPT-4) demonstrate that all models experience performance degradation after prompt perturbation, with larger models generally performing better under synonymous perturbation but no clear scaling law for typing attacks. The results indicate that improving LLMs' ease-of-use remains a significant challenge, and training data might be key to addressing this issue.

## Method Summary
The E-Bench benchmark evaluates LLM ease-of-use by testing robustness to four types of prompt perturbations: paraphrasing, simplification, colloquialism, and typing attacks. The method uses the AlpacaEval dataset as a base, applies automatic perturbation generation tools (GPT-4, GAN, TextBugger, PWWS), and conducts manual review to ensure semantic invariance. The evaluation framework measures performance degradation using a winning rate metric comparing original versus perturbed prompts, with GPT-4 serving as the judge. The study tests 6 representative LLMs including Llama 2-chat, Vicuna, GPT-3.5, and GPT-4, analyzing how different perturbation types affect model performance.

## Key Results
- All tested LLMs show significant performance degradation under prompt perturbations, with degradation rates ranging from 6.25% to 13.98% across different perturbation types
- Larger models generally perform better under synonymous perturbation, with GPT-4 showing the best overall performance
- No clear scaling law exists between model size and performance degradation under typing attacks
- Different models exhibit distinct sensitivities to perturbation types, suggesting training data composition influences robustness

## Why This Works (Mechanism)
### Mechanism 1
- Claim: Prompt perturbation causes performance degradation because LLMs fail to recognize synonymous expressions due to training data bias.
- Mechanism: The model's attention mechanisms become disrupted when encountering paraphrased or simplified prompts, causing it to lose focus on key task-relevant information.
- Core assumption: Training data contains limited coverage of synonymous expressions, making the model overly sensitive to prompt variations.
- Evidence anchors:
  - [abstract] "most large language models (LLMs) are sensitive to prompts, and another synonymous expression or a typo may lead to unexpected results"
  - [section] "Different models have different areas of expertise. For example, GPTs perform well in paraphrasing, while Vicunas perform well in colloquial setting. We attribute this to the influence of training data"
  - [corpus] Weak evidence - related papers focus on robustness but not specifically on training data coverage of synonymous expressions
- Break condition: When models are trained with diverse prompt variations covering all perturbation types

### Mechanism 2
- Claim: Typing attacks degrade performance because models lack robustness to character-level perturbations in their tokenization process.
- Mechanism: Misspellings cause the tokenizer to split words differently, breaking the attention flow and making it difficult for the model to parse instruction semantics correctly.
- Core assumption: LLMs are primarily trained on correctly spelled text and have not learned to handle common typing errors gracefully.
- Evidence anchors:
  - [section] "Typing errors are widespread in practice scenarios; one may mistakenly touch the surrounding keys when typing, or optical character recognition (OCR) errors may result in similar-looking characters"
  - [section] "Most language models are trained on a large amount of correct text data. Typographic perturbations can deviate from the model's training data distribution"
  - [corpus] Weak evidence - related papers mention robustness but don't specifically address character-level perturbation handling
- Break condition: When models are trained with adversarial typing examples or OCR error simulations

### Mechanism 3
- Claim: Larger models perform better under synonymous perturbation due to their capacity to learn more nuanced semantic relationships.
- Mechanism: Increased model size allows for more complex attention patterns and better generalization across different phrasings of the same concept.
- Core assumption: Model capacity directly correlates with ability to handle semantic invariance across prompt variations.
- Evidence anchors:
  - [section] "The larger the model, the better the stability (roughly) under synonymous perturbation"
  - [section] "GPT-4 has the best overall performance, comparable to Vicuna-v1.5 13b, and is sensibly ahead of Llama 2-chat models in all aspects"
  - [corpus] Weak evidence - related papers discuss model size but not specifically in the context of semantic invariance
- Break condition: When model size reaches a point where additional capacity doesn't improve semantic understanding

## Foundational Learning
- Concept: Semantic invariance in NLP
  - Why needed here: Understanding how models should ideally handle different phrasings of the same meaning is central to evaluating ease-of-use
  - Quick check question: What's the difference between semantic equivalence and syntactic similarity in natural language?

- Concept: Attention mechanisms in transformers
  - Why needed here: The degradation patterns observed are directly linked to how attention flows change when prompts are perturbed
  - Quick check question: How does the attention head visualization help diagnose where the model's understanding breaks down?

- Concept: Tokenization and subword processing
  - Why needed here: Understanding how typing attacks affect tokenization is crucial for interpreting why character-level errors cause such significant degradation
  - Quick check question: What happens to the token IDs when "cookies" is misspelled as "cookeis" versus "cookes"?

## Architecture Onboarding
- Component map: AlpacaEval dataset -> Perturbation generators (paraphrasing, simplification, colloquialism, typing attack) -> Manual review pipeline -> Evaluation framework using AlpacaEval as base -> LLM-as-a-judge system
- Critical path: Generate perturbations → Manual review → Run on target models → Automatic evaluation with GPT-4 judge → Analyze degradation patterns
- Design tradeoffs: Automation vs. quality (more automated generation risks semantic drift), perturbation diversity vs. evaluation tractability (too many variations becomes unmanageable)
- Failure signatures: Semantic drift in perturbed prompts, inconsistent evaluation scores across similar perturbations, over-reliance on single perturbation type
- First 3 experiments:
  1. Run E-Bench on a small set of models and manually verify semantic invariance of all perturbed prompts
  2. Test correlation between manual and automatic evaluation scores using a subset of data
  3. Analyze attention head patterns for a specific degradation case to validate the proposed mechanism

## Open Questions the Paper Calls Out
### Open Question 1
- Question: Why do larger LLMs perform better under synonymous perturbation but show no clear scaling law for typing attacks?
- Basis in paper: [explicit] The paper states "larger models perform better under synonymous perturbation, while there is no clear scaling law for model size and performance degradation under typing attack."
- Why unresolved: The paper acknowledges this phenomenon but does not provide a clear explanation for the different scaling behaviors between synonymous and typing perturbations.
- What evidence would resolve it: Detailed ablation studies comparing different model architectures, attention mechanisms, and tokenization strategies across model sizes for both perturbation types.

### Open Question 2
- Question: How does the training data composition affect LLMs' sensitivity to different types of prompt perturbations?
- Basis in paper: [inferred] The paper notes "Different models have different areas of expertise... We attribute this to the influence of training data" and "training data might be the secret key to improving ease-of-use."
- Why unresolved: While the paper suggests training data is important, it doesn't provide concrete evidence or analysis of how different training data characteristics affect perturbation robustness.
- What evidence would resolve it: Systematic experiments varying training data composition, analyzing how different domains, writing styles, and error patterns in training data correlate with perturbation robustness.

### Open Question 3
- Question: What causes the specific error patterns (challenge, safety, refusal) in Llama 2-chat models under synonymous perturbations?
- Basis in paper: [explicit] The paper identifies these error patterns and states "Llama 2-chat models might have a certain degree of over-fitting to the original AlpacaEval, or the safety mechanisms have a severe problem of over-recall."
- Why unresolved: The paper provides two possible explanations but doesn't conclusively determine which factor is more significant or explore other potential causes.
- What evidence would resolve it: Comparative analysis of model behavior on perturbed vs. original prompts, examination of safety mechanism implementation, and testing on out-of-distribution data to distinguish overfitting from safety mechanism issues.

## Limitations
- The benchmark relies heavily on GPT-4 as both perturbation generator and evaluator, creating potential circularity
- Manual review for semantic invariance introduces subjectivity and scalability constraints
- Study only examines 6 models and one base dataset, limiting generalizability
- Typing attack perturbations may not capture full diversity of real-world typing errors

## Confidence
- **High Confidence**: Core finding that LLMs degrade under prompt perturbations is well-supported by experimental results
- **Medium Confidence**: Attribution of degradation to training data bias and model size relationship with synonymous perturbation handling are plausible but require further validation
- **Low Confidence**: Specific claims about why certain models perform better under particular perturbation types are speculative and not sufficiently supported

## Next Checks
1. Test E-Bench on multiple datasets beyond AlpacaEval to verify whether observed degradation patterns hold across different domains and task types
2. Implement secondary evaluation framework using human judges or multiple LLM evaluators to assess reliability of automatic evaluation pipeline
3. Conduct controlled experiments isolating individual perturbation types to determine their relative contribution to performance degradation