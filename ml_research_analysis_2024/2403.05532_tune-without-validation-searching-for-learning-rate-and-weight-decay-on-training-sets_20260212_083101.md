---
ver: rpa2
title: 'Tune without Validation: Searching for Learning Rate and Weight Decay on Training
  Sets'
arxiv_id: '2403.05532'
source_url: https://arxiv.org/abs/2403.05532
tags:
- twin
- training
- learning
- validation
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Twin eliminates the need for validation sets when tuning learning
  rate and weight decay by leveraging a theoretical framework of learning phases.
  It performs a grid search over the LR-WD space, segments the region with best training
  loss, and selects the configuration with smallest parameter norm as a proxy for
  generalization.
---

# Tune without Validation: Searching for Learning Rate and Weight Decay on Training Sets

## Quick Facts
- **arXiv ID**: 2403.05532
- **Source URL**: https://arxiv.org/abs/2403.05532
- **Reference count**: 40
- **Primary result**: Twin eliminates validation sets for LR-WD tuning by selecting configurations with smallest parameter norm among those with best training loss

## Executive Summary
Twin is a hyperparameter optimization method that eliminates the need for validation sets when tuning learning rate and weight decay. It performs a grid search over the LR-WD space, segments the region with best training loss using Quickshift, and selects the configuration with smallest parameter norm as a proxy for generalization. The method achieves strong performance across 20 datasets and multiple architectures, scoring 1.3% MAE against an Oracle baseline that selects hyperparameters from the test set.

## Method Summary
Twin performs a grid search over learning rate and weight decay hyperparameters using an early-stopping scheduler. After training, it segments the loss matrix using Quickshift image segmentation to identify regions of low training loss, then selects the configuration with the smallest parameter norm within that region. The method leverages the theoretical framework that configurations with low training loss can be distinguished by parameter norm, where lower norms correlate with better generalization. Twin supports both from-scratch and transfer learning scenarios and is robust to different grid densities and early stopping schedules.

## Key Results
- Achieves 1.3% MAE against Oracle baseline that selects HPs from test set
- Works across 20 datasets including small datasets, medical imaging, and natural images
- Performs well with multiple architectures including ConvNets, transformers, and MLPs
- Eliminates need for validation sets while maintaining competitive performance

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Training loss alone can identify underfitting configurations but not distinguish between memorizing and generalizing high-performance solutions.
- Mechanism: The paper leverages a four-phase learning framework where configurations with low training loss eliminate confusion (underfitting) but still include both memorization (overfitting) and comprehension/grokking (generalization) phases.
- Core assumption: The training loss reliably correlates with task performance when IID assumptions hold.
- Evidence anchors: [abstract] "Twin performs a grid search of trials according to an early-/non-early-stopping scheduler and then segments the region that provides the best results in terms of training loss."
- Break condition: Distribution shifts or severe class imbalance that decouple training loss from generalization performance.

### Mechanism 2
- Claim: Weight norm correlates strongly with generalization among configurations that achieve low training loss.
- Mechanism: After filtering out underfitting configurations using training loss, the parameter norm serves as a proxy for regularization strength, where lower norms indicate better generalization.
- Core assumption: The regularization strength captured by weight norm is the primary differentiator between memorizing and generalizing solutions.
- Evidence anchors: [abstract] "Among these trials, the weight norm strongly correlates with predicting generalization."
- Break condition: When other regularization techniques (dropout, data augmentation) dominate over weight decay's effect on generalization.

### Mechanism 3
- Claim: Quickshift segmentation reliably identifies the region of LR-WD space containing generalizing configurations without requiring validation data.
- Mechanism: The training loss matrix is treated as an image and segmented using Quickshift to find clusters of low-loss configurations, then the parameter norm within this region selects the best generalizing solution.
- Core assumption: The loss landscape across LR-WD space forms coherent regions that Quickshift can identify.
- Evidence anchors: [abstract] "Twin performs a grid search of trials according to an early-/non-early-stopping scheduler and then segments the region that provides the best results in terms of training loss."
- Break condition: Highly irregular loss landscapes where Quickshift fails to identify coherent regions.

## Foundational Learning

- **Concept**: Phase diagrams in neural network training
  - Why needed here: The paper's theoretical foundation relies on understanding four learning phases (comprehension, grokking, memorization, confusion) across hyperparameter space
  - Quick check question: What distinguishes the comprehension phase from memorization in terms of both training and test performance?

- **Concept**: Scale invariance in neural networks
  - Why needed here: The paper specifically addresses scale-invariant architectures where weight decay affects effective learning rate rather than model complexity
  - Quick check question: Why does weight decay have a different effect on scale-invariant networks compared to traditional networks?

- **Concept**: Generalization gap and cross-validation theory
  - Why needed here: Understanding why validation sets are typically needed and how their absence affects hyperparameter selection
  - Quick check question: How does the expected prediction error on a validation set scale with the number of validation samples?

## Architecture Onboarding

- **Component map**: Grid search -> Trial scheduling (FIFO/HyperBand) -> Training with logging -> Quickshift segmentation -> Parameter norm selection -> Configuration selection
- **Critical path**: Grid search → training with logging → loss segmentation → norm filtering → configuration selection
- **Design tradeoffs**: Grid density vs computational cost, early stopping aggressiveness vs segmentation quality, segmentation parameters vs robustness
- **Failure signatures**: Poor segmentation when loss landscape is noisy, incorrect configuration when regularization effects are dominated by other factors, failure when IID assumptions break down
- **First 3 experiments**:
  1. Run Twin with a simple grid (2x2) on CIFAR-10 with a small ResNet to verify basic functionality
  2. Compare Twin's selected configuration against validation-based selection on a dataset with known optimal hyperparameters
  3. Test Quickshift segmentation sensitivity by varying kernel_size and max_dist on a synthetic loss landscape

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions, but based on the methodology and results, several natural extensions and investigations emerge.

## Limitations
- Relies on IID data assumptions that may not hold in real-world scenarios with distribution shifts
- Quickshift segmentation parameters are not fully specified, affecting reproducibility
- Performance on non-image domains and larger-scale problems remains unexplored

## Confidence
- **High confidence**: Twin's ability to eliminate validation sets while maintaining competitive performance against Oracle baselines (MAE 1.3%)
- **Medium confidence**: The theoretical framework linking training loss phases to generalization behavior
- **Medium confidence**: Quickshift's reliability for identifying low-loss regions across diverse datasets

## Next Checks
1. Test Twin's robustness to distribution shift by evaluating on out-of-distribution samples or corrupted datasets
2. Vary Quickshift segmentation parameters systematically to determine their impact on final configuration selection
3. Compare Twin against validation-based methods on a larger-scale problem (e.g., ImageNet) to assess scalability limitations