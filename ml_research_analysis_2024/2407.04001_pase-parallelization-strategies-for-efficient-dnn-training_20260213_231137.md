---
ver: rpa2
title: 'PaSE: Parallelization Strategies for Efficient DNN Training'
arxiv_id: '2407.04001'
source_url: https://arxiv.org/abs/2407.04001
tags:
- strategies
- parallelism
- strategy
- data
- best
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the challenge of efficiently parallelizing
  deep neural network (DNN) training across multiple devices by automatically finding
  optimal parallelization strategies. The core method uses dynamic programming to
  compute efficient parallelization strategies from a DNN's computation graph, considering
  both data and parameter parallelism.
---

# PaSE: Parallelization Strategies for Efficient DNN Training

## Quick Facts
- arXiv ID: 2407.04001
- Source URL: https://arxiv.org/abs/2407.04001
- Reference count: 26
- The proposed method achieves speedups of up to 1.85× and 4× over data parallelism on 1080Ti and 2080Ti GPU clusters respectively.

## Executive Summary
This paper addresses the challenge of efficiently parallelizing deep neural network training across multiple devices by automatically finding optimal parallelization strategies. The core method uses dynamic programming to compute efficient parallelization strategies from a DNN's computation graph, considering both data and parameter parallelism. The approach orders vertices to minimize dependent set sizes, enabling efficient computation of optimal strategies within seconds for various DNNs. Experimental results demonstrate that the proposed method significantly outperforms data parallelism, expert-designed strategies, and state-of-the-art approaches like FlexFlow.

## Method Summary
The paper presents a dynamic programming approach to find optimal parallelization strategies for DNN training. The method represents DNNs as computation graphs where vertices are layers and edges are tensor dependencies. A key innovation is the vertex ordering algorithm that minimizes dependent set sizes, reducing the search space from K^M to K^(M+1) where M is the maximum dependent set size. The algorithm computes strategies by considering both data parallelism (splitting input data across devices) and parameter parallelism (splitting model parameters across devices). The cost function approximates training time by summing layer computation costs and communication costs, normalized to FLOPs, while ignoring inter-layer pipeline parallelism to simplify optimization.

## Key Results
- Strategies found by the method achieve speedups of up to 1.85× and 4× over data parallelism on 1080Ti and 2080Ti GPU clusters respectively
- The approach outperforms expert-designed strategies and FlexFlow across multiple DNN types including CNNs, RNNs, and Transformers
- The algorithm finds efficient strategies within a few seconds for various DNNs, making it practical for real-world use
- Hybrid parallelism combining data and parameter parallelism outperforms pure data parallelism for most DNN layers

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Dynamic programming efficiently finds optimal parallelization strategies by exploiting the dependency structure of DNN computation graphs.
- Mechanism: The algorithm orders vertices to minimize dependent set sizes, enabling exponential reduction in search space from K^M to K^(M+1), where M is the maximum dependent set size.
- Core assumption: DNN computation graphs are mostly sparse with a few high-degree nodes, allowing vertex ordering to effectively reduce dependent set sizes.
- Evidence anchors:
  - [abstract] "Experimental results show that our algorithm finds efficient strategies within a few seconds for various DNNs."
  - [section] "Since the computational complexity of the recurrence is exponential in terms of the sizes of dependent sets, using recurrence (4) instead of (2) exponentially reduces the computation time"
  - [corpus] Weak - no direct mention of DP or dependent sets in related papers
- Break condition: If DNN graphs become uniformly dense (like DenseNet), no vertex ordering can effectively reduce M, making the algorithm computationally expensive.

### Mechanism 2
- Claim: Hybrid parallelism combining data and parameter parallelism outperforms pure data parallelism for most DNN layers.
- Mechanism: Each layer is parallelized differently using a combination of data and parameter parallelism, with the algorithm automatically selecting the optimal configuration for each layer based on computational and communication costs.
- Core assumption: Different DNN layers have different optimal parallelization strategies depending on their computation characteristics and tensor dimensions.
- Evidence anchors:
  - [abstract] "Our results show that the strategies found using our approach outperform the baseline data parallelism strategy in all the cases."
  - [section] "The algorithm suggests to use data parallelism for modules A-D, but for the module E, the algorithm suggests a hybrid of data+parameter parallelism"
  - [corpus] Weak - related papers mention various parallelism strategies but don't specifically discuss automatic hybrid strategy selection
- Break condition: If all layers in a DNN are uniform (like simple path graphs), hybrid parallelism may not provide significant benefits over data parallelism.

### Mechanism 3
- Claim: The cost function accurately ranks parallelization strategies without requiring precise runtime predictions.
- Mechanism: The cost function approximates training time by summing layer computation costs and communication costs, normalized to FLOPs, while ignoring inter-layer pipeline parallelism to simplify the optimization problem.
- Core assumption: The relative ordering of strategies based on the simplified cost function matches their actual performance ranking.
- Evidence anchors:
  - [abstract] "Our simplifying assumptions affect costs of all the strategies more or less alike, preserving most of the relative ordering."
  - [section] "Our simplifying assumptions affect costs of all the strategies more or less alike, preserving most of the relative ordering."
  - [corpus] Weak - no direct discussion of cost function design in related papers
- Break condition: If inter-layer pipeline parallelism opportunities become significant in certain DNN architectures, ignoring them in the cost function could lead to suboptimal strategy selection.

## Foundational Learning

- Concept: Dynamic programming with optimal substructure
  - Why needed here: The problem of finding optimal parallelization strategies has optimal substructure where optimal solutions to subproblems can be combined to form optimal solutions to larger problems
  - Quick check question: What property allows us to use dynamic programming instead of exhaustive search for this problem?

- Concept: Computation graph representation of DNNs
  - Why needed here: Understanding how DNNs can be represented as computation graphs with vertices as layers and edges as tensor dependencies is crucial for applying graph algorithms to parallelization
  - Quick check question: How does the computation graph representation help in identifying parallelization opportunities?

- Concept: Hybrid parallelism strategies
  - Why needed here: Knowledge of how data parallelism and parameter parallelism work, and when each is advantageous, is necessary to understand why hybrid approaches can outperform pure strategies
  - Quick check question: What are the main advantages and disadvantages of data parallelism versus parameter parallelism?

## Architecture Onboarding

- Component map:
  - Vertex ordering algorithm (GENERATE SEQ) -> Dynamic programming solver (FIND BEST STRATEGY) -> Cost function evaluator -> Graph representation module -> Strategy extraction component

- Critical path:
  1. Parse DNN model into computation graph
  2. Generate vertex ordering using GENERATE SEQ
  3. Run dynamic programming to find optimal strategy
  4. Extract and format the best parallelization strategy

- Design tradeoffs:
  - Simplified cost function vs. accuracy: Ignoring inter-layer pipeline parallelism significantly speeds up computation but may miss some optimization opportunities
  - Vertex ordering complexity vs. dependent set reduction: More sophisticated ordering algorithms could potentially reduce dependent sets further but would increase pre-processing time
  - Memory usage vs. computation time: The DP approach trades memory for speed, storing intermediate results for all dependent set configurations

- Failure signatures:
  - High runtime on uniformly dense graphs (algorithm complexity becomes prohibitive)
  - Suboptimal performance when inter-layer pipeline parallelism is significant
  - Memory exhaustion when K^M becomes too large for available memory

- First 3 experiments:
  1. Run algorithm on AlexNet (path graph) to verify basic functionality and compare with BF ordering performance
  2. Test on InceptionV3 to observe vertex ordering effectiveness on graphs with high-degree nodes
  3. Compare performance against data parallelism baseline on 1080Ti GPU cluster

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the dynamic programming approach scale with graph density beyond DNNs, and are there alternative formulations that could handle uniformly dense graphs more efficiently?
- Basis in paper: [explicit] The paper notes limitations with dense graphs like DenseNet and computational complexity O(|V|²K M+1).
- Why unresolved: The current algorithm's complexity becomes prohibitive for uniformly dense graphs where M cannot be reduced.
- What evidence would resolve it: Empirical testing on various dense graph structures and comparison with alternative algorithms.

### Open Question 2
- Question: How would incorporating inter-layer pipeline parallelism into the cost function affect the quality of the computed parallelization strategies?
- Basis in paper: [explicit] The paper acknowledges ignoring inter-layer pipeline parallelism and notes this prevents capturing overlapping computation and communication.
- Why unresolved: The current cost function only captures data and parameter parallelism, potentially missing optimization opportunities.
- What evidence would resolve it: Performance comparison between current strategies and those incorporating pipeline parallelism on DNNs with inherent pipeline opportunities.

### Open Question 3
- Question: What would be the impact of including cache effects and other low-level hardware details in the cost model?
- Basis in paper: [explicit] The paper mentions ignoring cache effects for simplicity and notes this is not an inherent limitation.
- Why unresolved: The current cost model is a simplified approximation that may not accurately reflect real-world performance.
- What evidence would resolve it: Performance comparison between strategies using simplified vs. detailed cost models across different hardware architectures.

## Limitations
- The approach's effectiveness depends heavily on DNN computation graphs being mostly sparse with a few high-degree nodes
- The simplified cost function that ignores inter-layer pipeline parallelism could lead to suboptimal strategies when such opportunities are significant
- Experimental evaluation focuses on specific DNNs and hardware configurations (1080Ti and 2080Ti GPUs), limiting generalizability

## Confidence
- High confidence: The dynamic programming approach and vertex ordering mechanism - well-specified algorithm with clear computational complexity analysis
- Medium confidence: The effectiveness of hybrid parallelism strategies - demonstrated on specific DNNs but limited generalization
- Low confidence: The accuracy of the simplified cost function - relies on unproven assumption about relative ordering preservation

## Next Checks
1. Test the algorithm on DenseNet or other uniformly dense DNN architectures to verify the break condition where no vertex ordering can effectively reduce dependent set sizes
2. Implement and evaluate the cost function on a new DNN architecture not covered in the paper to test its generalizability and identify any cases where ignoring pipeline parallelism leads to suboptimal strategies
3. Compare the memory usage and computation time of the proposed approach against FlexFlow on the same hardware to quantify the practical trade-offs between the two methods