---
ver: rpa2
title: Reinforcement Learning for Conversational Question Answering over Knowledge
  Graph
arxiv_id: '2401.08460'
source_url: https://arxiv.org/abs/2401.08460
tags:
- knowledge
- question
- graph
- answering
- conversational
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proposes a reinforcement learning approach to address
  conversational question answering over law knowledge bases, where input questions
  are often noisy and unclear. The method employs an iterative reinforcement learning
  framework, using a pre-trained BERT model and LSTM to encode the question and conversation
  history.
---

# Reinforcement Learning for Conversational Question Answering over Knowledge Base

## Quick Facts
- arXiv ID: 2401.08460
- Source URL: https://arxiv.org/abs/2401.08460
- Authors: Mi Wu
- Reference count: 8
- One-line primary result: Proposed RL method outperforms baselines on certain metrics like Hit@5 for noisy conversational QA over law knowledge bases

## Executive Summary
This paper introduces a reinforcement learning approach for conversational question answering over law knowledge bases, addressing the challenge of noisy and unclear input questions. The method uses a pre-trained BERT model and LSTM to encode questions and conversation history, then employs a policy network to navigate the knowledge graph and find answers. Experiments on real-world datasets demonstrate effectiveness in handling conversational ambiguity through iterative reinforcement learning.

## Method Summary
The approach employs an iterative reinforcement learning framework where a policy network navigates a law knowledge graph to find answers to conversational questions. The method encodes questions using pre-trained BERT for contextual embeddings, processes conversation history with LSTM, and uses these representations to guide graph traversal actions. The REINFORCE algorithm trains the policy through trial-and-error interactions with the knowledge graph environment, receiving binary rewards for correct answers.

## Key Results
- Outperforms existing baselines on Hit@5 metric for conversational QA over law knowledge bases
- Demonstrates effectiveness in handling noisy and unclear input questions
- Shows improved performance through iterative reinforcement learning framework

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The iterative reinforcement learning agent can find correct answers in law knowledge bases even when input questions are noisy and unclear.
- Mechanism: The agent learns a policy to navigate the knowledge graph by taking actions (following edges) based on the current entity, question embedding, and search history. It receives rewards only when it reaches the correct answer, encouraging it to learn paths that lead to answers despite noisy input.
- Core assumption: The law knowledge base contains sufficient paths from relevant entities to answers, and the policy can learn to distinguish useful paths from noise through trial and error.
- Evidence anchors:
  - [abstract] states the method "outperforms existing baselines on certain metrics, such as Hit@5, demonstrating its effectiveness in handling noisy and unclear input questions."
  - [section] describes the reward function: "The model receives a reward of Rb(st) = 1 if the current location is the correct answer, and 0 otherwise."
  - [corpus] shows related work on ConvQA using RL, supporting the general approach.
- Break condition: If the knowledge graph lacks sufficient connectivity between entities and answers, or if noise is too severe to distinguish signal from noise through learning.

### Mechanism 2
- Claim: Encoding conversational history with LSTM allows the model to handle incomplete and ungrammatical natural language inputs.
- Mechanism: The LSTM encoder processes the sequence of question embeddings from the conversation history, producing a context vector that captures the dialogue flow. This context vector is concatenated with the current question embedding and used by the policy network.
- Core assumption: The LSTM can capture relevant context from the conversation history that helps disambiguate incomplete current questions.
- Evidence anchors:
  - [section] states: "To tackle this, an LSTM is employed to encode the entire conversational history: lqi = LSTM(hqi)"
  - [abstract] mentions the method handles "noisy and unclear input questions" which are common in conversational settings.
  - [corpus] evidence is weak - no direct mention of LSTM for ConvQA in related papers.
- Break condition: If the conversation history is too long or complex for the LSTM to effectively capture relevant context, or if the current question provides insufficient information despite history.

### Mechanism 3
- Claim: Using BERT to encode questions provides rich contextual embeddings that help the agent understand the current query.
- Mechanism: The question is processed with BERT to get token embeddings, then a feedforward network produces a fixed-size question embedding. This embedding captures the semantic meaning of the question, including entity references and relations.
- Core assumption: BERT's pre-training on large text corpora enables it to produce meaningful embeddings even for domain-specific law questions.
- Evidence anchors:
  - [section] describes: "we feed the processed question context into a pre-trained BERT model (Devlin et al., 2019) for extracting contextual embeddings for each token."
  - [abstract] mentions using "a pre-trained BERT model and LSTM to encode the question and conversation history."
  - [corpus] evidence is weak - related papers don't mention BERT for ConvQA over KGs.
- Break condition: If BERT embeddings don't capture domain-specific legal terminology effectively, or if the question structure is too different from BERT's pre-training data.

## Foundational Learning

- Concept: Reinforcement Learning basics (states, actions, rewards, policy)
  - Why needed here: The entire approach is built on RL framework where the agent learns to navigate KG through trial and error.
  - Quick check question: What is the difference between a state and an action in an RL context?

- Concept: LSTM networks for sequence processing
  - Why needed here: LSTM encodes conversational history and search history, which are sequential data requiring memory of previous states.
  - Quick check question: How does an LSTM cell maintain information across long sequences compared to a simple RNN?

- Concept: Knowledge Graph structure and traversal
  - Why needed here: The agent navigates a KG by following edges between entities, requiring understanding of graph topology.
  - Quick check question: What are the key components of a knowledge graph triple, and how do they relate to graph traversal?

## Architecture Onboarding

- Component map:
  - BERT encoder: Converts raw question text to contextual embeddings
  - LSTM encoder: Processes conversation history and search history
  - Policy network: Maps (current entity, question embedding, search history) to action probabilities
  - Knowledge Graph environment: Provides state transitions and rewards
  - REINFORCE training loop: Updates policy parameters based on rewards

- Critical path: Input question → BERT → LSTM history → Policy network → Action selection → KG traversal → Reward → Policy update

- Design tradeoffs:
  - BERT vs simpler encoders: Better semantic understanding vs computational cost
  - LSTM vs transformer for history: Better for long sequences vs more parallelizable
  - Dense vs sparse rewards: Faster learning vs more informative feedback

- Failure signatures:
  - Policy never finds correct answers: Check reward signal and KG connectivity
  - Policy gets stuck in loops: Check for self-loop handling and exploration strategy
  - Poor performance on new questions: Check BERT generalization and training data coverage

- First 3 experiments:
  1. Single-turn QA baseline: Remove conversation history, test if policy can answer standalone questions
  2. Ablation study: Replace LSTM with mean pooling, test if history encoding helps
  3. Reward shaping: Add intermediate rewards for reaching relevant entities, test if learning speeds up

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the model handle noise in real-world legal questions that deviate from training data patterns?
- Basis in paper: [explicit] The paper discusses noisy and unclear input questions as a challenge, but does not specify how the model adapts to unseen noise patterns.
- Why unresolved: The paper focuses on evaluation metrics but lacks detailed analysis of robustness to diverse noise types.
- What evidence would resolve it: Experiments testing the model on datasets with varied noise distributions or synthetic noise injection.

### Open Question 2
- Question: What is the impact of conversation history length on model performance?
- Basis in paper: [inferred] The model uses LSTM to encode conversation history, but the paper does not explore how varying history lengths affects accuracy.
- Why unresolved: No ablation studies or analysis of history length vs. performance trade-offs are provided.
- What evidence would resolve it: Comparative experiments with different history truncation lengths.

### Open Question 3
- Question: How does the reinforcement learning reward function influence answer quality beyond Hit@5?
- Basis in paper: [explicit] The reward is binary (1 for correct answer, 0 otherwise), but the paper does not analyze if this encourages optimal exploration.
- Why unresolved: No discussion of reward shaping or alternative reward structures to improve learning efficiency.
- What evidence would resolve it: Experiments comparing different reward functions (e.g., step penalties, path efficiency rewards).

## Limitations

- Limited discussion of robustness to diverse noise patterns beyond basic evaluation metrics
- No ablation studies to isolate contributions of BERT, LSTM, and RL components
- Weak related work section with zero citations, limiting context for design choices

## Confidence

- Confidence: Low on claims about handling "noisy and unclear" questions. While the paper mentions this capability, the evidence is weak: the related papers section contains zero citations, and the corpus analysis shows limited direct evidence supporting the noise-handling claims.
- Confidence: Medium on the RL framework effectiveness. The method follows established RL patterns (REINFORCE algorithm, reward shaping), but the specific design choices for ConvQA over KGs lack direct precedent in the related work.
- Confidence: Medium on BERT and LSTM contributions. The paper describes these components but doesn't provide ablation studies or comparative analysis against simpler alternatives, making it difficult to assess their relative importance.

## Next Checks

1. **Ablation Study**: Remove BERT and LSTM components to test their individual contributions to performance. Compare against a baseline using only word embeddings and simple history tracking.

2. **Noise Robustness Test**: Create controlled datasets with varying levels of question noise (typos, grammatical errors, incomplete sentences) and measure performance degradation compared to clean inputs.

3. **Knowledge Graph Connectivity Analysis**: Analyze the graph structure to verify sufficient paths exist between entities and answers. Test whether the RL agent can learn in graphs with varying connectivity levels.