---
ver: rpa2
title: Better-than-KL PAC-Bayes Bounds
arxiv_id: '2402.09201'
source_url: https://arxiv.org/abs/2402.09201
tags:
- bounds
- bound
- divergence
- have
- inequality
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a new high-probability PAC-Bayes bound using
  a divergence tighter than KL. The key technical contribution is deriving a concentration
  inequality via a change-of-measure argument based on the potential function $x \mapsto
  e^{x^2/2}$, which yields a bound scaling with $\sqrt{\mathrm{DKL} \cdot \mathrm{DTV}}$
  rather than $\sqrt{\mathrm{DKL}}$.
---

# Better-than-KL PAC-Bayes Bounds

## Quick Facts
- arXiv ID: 2402.09201
- Source URL: https://arxiv.org/abs/2402.09201
- Authors: Ilja Kuzborskij; Kwang-Sung Jun; Yulian Wu; Kyoungseok Jang; Francesco Orabona
- Reference count: 40
- Primary result: Introduces a new high-probability PAC-Bayes bound using a divergence tighter than KL

## Executive Summary
This paper introduces a novel high-probability PAC-Bayes bound using the Zhang-Cutkosky-Paschalidis (ZCP) divergence, which is strictly tighter than the traditional KL divergence. The key technical contribution is deriving a concentration inequality via a change-of-measure argument based on the potential function $x \mapsto e^{x^2/2}$, which yields a bound scaling with $\sqrt{\mathrm{DKL} \cdot \mathrm{DTV}}$ rather than $\sqrt{\mathrm{DKL}}$. This new divergence is never worse than KL and can be strictly better in discrete and continuous cases. The analysis demonstrates that KL is suboptimal for PAC-Bayes complexity measures and opens the way to study optimal rates in this framework.

## Method Summary
The paper presents a new PAC-Bayes concentration inequality using the ZCP divergence, derived through a change-of-measure argument with a specific potential function. The method involves analyzing the regret of online betting algorithms to bound the expected maximal log-wealth, which unifies various concentration bounds. The approach leverages Ville's inequality for martingales and the Fenchel-Young inequality with the potential function $x \mapsto e^{x^2/2}$ to obtain the improved divergence. The theoretical framework provides both Hoeffding-type and Bernstein-type bounds with potentially tighter complexity measures than traditional KL-based approaches.

## Key Results
- Introduces the ZCP divergence that scales as $\sqrt{\mathrm{DKL} \cdot \mathrm{DTV}}$ instead of $\sqrt{\mathrm{DKL}}$
- Proves the ZCP divergence is never worse than KL divergence and can be strictly better in discrete and continuous cases
- Derives new Hoeffding-style and Bernstein-type PAC-Bayes bounds using the ZCP divergence
- Demonstrates through theoretical examples that the new bounds can provide tighter generalization guarantees

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The new bound replaces KL divergence with a better-than-KL divergence that combines KL and total variation (TV) distances.
- Mechanism: The ZCP divergence scales as $\sqrt{\mathrm{DKL} \cdot \mathrm{DTV}}$ instead of $\sqrt{\mathrm{DKL}}$, and since DTV ≤ 1, this yields tighter bounds in cases where DTV is small.
- Core assumption: The change-of-measure inequality based on the potential function $f(x) = |x-1|\sqrt{\ln(1 + c^2|x-1|^2)}$ holds for all relevant distributions.
- Evidence anchors:
  - [abstract] "We demonstrate new high-probability PAC-Bayes bounds with a novel and better-than-KL divergence that is inspired by Zhang et al. (2022)."
  - [section 4] "The main interesting property of this divergence is that it is controlled simultaneously by KL-divergence and TV distance, namely: Theorem 2..."
- Break condition: If the TV distance approaches 1, the advantage over KL disappears.

### Mechanism 2
- Claim: The optimal log-wealth $\ln W^*_n$ unifies various concentration bounds (Hoeffding, Bernstein, etc.).
- Mechanism: By analyzing the regret of online betting algorithms, we can bound $\int\ln W^*_n(\theta)dP_n(\theta)$ which then translates to different PAC-Bayes inequalities through lower-bounding techniques.
- Core assumption: The wealth process $W_n(\theta)$ forms a nonnegative martingale when the coin outcomes have conditional zero mean.
- Evidence anchors:
  - [section 5] "The central quantity in the coming result will be the expected maximal log-wealth $\int\ln W^*_n(\theta)dP_n(\theta)$ — it was recently shown by Jang et al. (2023) that through lower-bounding $\ln W^*_n$ term we can obtain many known PAC-Bayes bounds."
  - [section 3.1] "Ville's inequality will be the main tool to leverage regret guarantees to construct our concentration inequalities."
- Break condition: If the betting outcomes are not conditionally mean-zero, Ville's inequality no longer applies.

### Mechanism 3
- Claim: The Fenchel-Young inequality with the specific potential function $x \mapsto \exp(x^2/2)$ enables the change-of-measure argument that produces the ZCP divergence.
- Mechanism: This potential function's convex conjugate creates a bound that naturally incorporates both KL and TV terms, leading to the DZCP divergence.
- Core assumption: The function $f^*(x) = b\cdot\exp(x^2/(2a))$ satisfies the conditions of Lemma 15 for appropriate choices of $a$ and $b$.
- Evidence anchors:
  - [section 7] "By Fenchel-Young inequality, for a convex $F : R \to R$, $(i) \leq \int F^*(\Delta(\theta)) dP_0(\theta) + \int F(dP_n/dP_0 - 1)dP_0(\theta)$."
  - [section 2] "In this paper, we focus on a particular $f$, whose convex conjugate is a function $f^*(y) = \delta \exp(y^2/(2n))$."
- Break condition: If the potential function is changed to something incompatible with the Fenchel-Young framework, the divergence relationship breaks down.

## Foundational Learning

- Concept: PAC-Bayes bounds and their relationship to generalization error
  - Why needed here: The entire paper builds on understanding how PAC-Bayes bounds can be tightened beyond the traditional KL-based approach
  - Quick check question: What is the key difference between standard generalization bounds and PAC-Bayes bounds?

- Concept: f-divergences and their properties (KL, TV, χ², etc.)
  - Why needed here: The ZCP divergence is an f-divergence, and understanding its relationship to other divergences is crucial
  - Quick check question: How does the ZCP divergence relate to both KL and TV divergences according to Theorem 2?

- Concept: Change-of-measure arguments and Fenchel-Young inequality
  - Why needed here: The core proof technique relies on applying Fenchel-Young with a specific potential function
  - Quick check question: What is the role of the potential function $f(x) = |x-1|\sqrt{\ln(1 + c^2|x-1|^2)}$ in the change-of-measure argument?

## Architecture Onboarding

- Component map: ZCP divergence definition -> Change-of-measure framework -> Optimal log-wealth analysis -> PAC-Bayes inequalities
- Critical path: To implement a new bound, one must: (a) compute the ZCP divergence between prior and posterior, (b) apply the appropriate concentration inequality, (c) optimize over the posterior
- Design tradeoffs: The ZCP approach offers tighter bounds in some cases but requires computing DTV, which can be expensive for continuous distributions
- Failure signatures: Bounds becoming worse than KL-based ones typically indicates DTV is large or the potential function choice is inappropriate
- First 3 experiments:
  1. Implement the ZCP divergence calculation for discrete distributions and verify it's always ≤ KL divergence
  2. Test the Hoeffding-style bound (Theorem 6) on a simple Bernoulli example and compare to traditional KL-based bounds
  3. Implement the empirical Bernstein bound (Corollary 12) and test on a synthetic dataset with small variance

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the factor of $D_{TV}^{1/2}(P_n, P_0)$ in the empirical Bernstein inequality bound be removed to match the Hoeffding-style bound's complexity term?
- Basis in paper: The paper notes a factor of $D_{TV}^{1/2}(P_n, P_0)$ gap between the empirical Bernstein bound and the Hoeffding-style bound in Remark 13
- Why unresolved: The paper explicitly states this as an open problem and leaves it for future work
- What evidence would resolve it: A proof showing that the empirical Bernstein bound can be tightened to remove the $D_{TV}^{1/2}(P_n, P_0)$ factor while maintaining the same probability guarantees

### Open Question 2
- Question: Are there Pareto-optimal divergences for PAC-Bayes bounds beyond the ZCP divergence that can achieve strictly better bounds in certain regimes?
- Basis in paper: The conclusion states "A tantalizing open problem is whether our bounds can be further improved. It would be interesting to see if it is possible to establish some (Pareto) optimalities for PAC-Bayes bounds."
- Why unresolved: The paper only introduces the ZCP divergence and shows it is strictly better than KL in certain cases, but doesn't explore the full space of possible divergences
- What evidence would resolve it: Identification and proof of specific divergences that provide better bounds than ZCP in certain learning scenarios, with clear Pareto optimality conditions

### Open Question 3
- Question: Can the asymptotic bound in Theorem 8 be made non-asymptotic with explicit constants for practical applications?
- Basis in paper: Theorem 8 provides an asymptotic result (Corollary 9) with a universal constant $c > 0$, but the proof only shows existence of such a constant
- Why unresolved: The proof technique uses Borel-Cantelli lemma and asymptotic analysis which doesn't provide explicit constants
- What evidence would resolve it: Derivation of explicit non-asymptotic bounds with computable constants that match the asymptotic behavior for practical sample sizes

### Open Question 4
- Question: How does the ZCP divergence perform in PAC-Bayes bounds for non-convex loss functions or unbounded domains?
- Basis in paper: All theoretical results assume bounded losses in $[0,1]$ and convex parameter spaces
- Why unresolved: The paper only considers the bounded case, and extending to unbounded domains would require different techniques
- What evidence would resolve it: Theoretical analysis and empirical validation of ZCP-based PAC-Bayes bounds for unbounded losses or non-convex domains

## Limitations

- The computational tractability of the ZCP divergence for complex continuous distributions is not fully addressed
- The paper's empirical validation is limited to theoretical examples rather than comprehensive empirical studies
- The analysis relies heavily on the specific choice of potential function, and the sensitivity to alternative choices remains unexplored

## Confidence

- **High Confidence**: The mathematical derivation of the ZCP divergence and its relationship to KL and TV divergences (Theorem 2) is rigorous and well-supported
- **Medium Confidence**: The claim that ZCP bounds are never worse than KL bounds holds theoretically, but practical advantages may be limited when DTV is large
- **Medium Confidence**: The extension to various PAC-Bayes inequalities through log-wealth analysis is sound, but the generality of this approach requires further exploration

## Next Checks

1. **Computational Complexity Analysis**: Systematically compare the computational cost of calculating ZCP divergence versus KL divergence for various distribution families (Gaussian, Dirichlet, discrete distributions)
2. **Empirical Performance Study**: Implement the ZCP-based bounds in a machine learning setting (e.g., Bayesian neural network generalization) and compare against traditional KL-based PAC-Bayes bounds on real datasets
3. **Robustness to Potential Function Choice**: Test alternative potential functions in the Fenchel-Young framework to determine how sensitive the ZCP divergence is to this choice and whether other potential functions might yield even tighter bounds