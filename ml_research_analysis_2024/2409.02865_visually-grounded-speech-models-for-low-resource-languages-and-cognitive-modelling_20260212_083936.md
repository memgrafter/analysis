---
ver: rpa2
title: Visually Grounded Speech Models for Low-resource Languages and Cognitive Modelling
arxiv_id: '2409.02865'
source_url: https://arxiv.org/abs/2409.02865
tags:
- image
- speech
- word
- spoken
- few-shot
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This dissertation explores visually grounded speech (VGS) models
  for low-resource languages and cognitive modelling. We introduce a new task called
  visually prompted keyword localisation to detect and localise keywords in speech
  using images.
---

# Visually Grounded Speech Models for Low-resource Languages and Cognitive Modelling

## Quick Facts
- arXiv ID: 2409.02865
- Source URL: https://arxiv.org/abs/2409.02865
- Authors: Leanne Nortje
- Reference count: 40
- Primary result: Introduces visually prompted keyword localisation and demonstrates VGS models' effectiveness in few-shot learning for low-resource languages like Yoruba

## Executive Summary
This dissertation explores visually grounded speech (VGS) models for low-resource languages and cognitive modelling. The work introduces a novel task called visually prompted keyword localisation (VPKL) to detect and localise keywords in speech using images. The research demonstrates the effectiveness of VGS models in few-shot learning scenarios for low-resource languages like Yoruba, showing that models can learn new words from minimal paired examples. Additionally, the study examines the mutual exclusivity bias in VGS models, finding that monolingual models exhibit this cognitive phenomenon, though multilingualism does not affect it similarly to children.

## Method Summary
The research employs visually grounded speech models that learn from paired speech and images without text transcriptions. For low-resource applications, the approach uses few-shot learning with query-by-example systems to mine additional training pairs from unlabelled datasets. The VGS models consist of audio and vision branches with multimodal attention mechanisms, trained using InfoNCE loss. For cognitive modelling, the study investigates mutual exclusivity bias by initializing models with pretrained speech and vision representations and testing their behavior in word-object mapping tasks.

## Key Results
- VPKL models successfully detect and localise keywords in speech using image queries without requiring text transcriptions
- Few-shot approach achieved the best performance for English VPKL models, demonstrating effective vocabulary expansion from limited examples
- Monolingual VGS models exhibit mutual exclusivity bias, but multilingualism does not enhance this bias in the same way observed in children

## Why This Works (Mechanism)

### Mechanism 1: Visually Grounded Speech Models Bridge Modalities for Low-Resource Language Learning
VGS models learn meaningful mappings between spoken words and visual objects during training, creating a shared representation space that enables keyword detection without text. The model can generalise to new keyword-image pairs at test time by using image queries instead of text. Core assumption is that the model learns robust word-object mappings during training, and image queries are sufficiently clear. Break condition occurs if the model fails to learn these mappings or if image queries are too ambiguous.

### Mechanism 2: Few-Shot Learning with Query-by-Example Enables Efficient Vocabulary Expansion
For each keyword, spoken examples are compared to unlabelled speech using query-by-example systems to find matching utterances, while image examples are compared to unlabelled images. The mined pairs are then used to train the VGS model. Core assumption is that the query-by-example system accurately identifies relevant pairs, and the mined pairs are sufficiently accurate for training. Break condition occurs if the system has low accuracy or if unlabelled datasets are too noisy.

### Mechanism 3: InfoNCE Loss and Prior Knowledge Initialization Strengthen Mutual Exclusivity Bias
Initializing the VGS model with pretrained audio (CPC) and vision (AlexNet) models, combined with the InfoNCE loss, results in the strongest mutual exclusivity bias. Pretrained initializations provide prior knowledge of speech and vision representations, while InfoNCE loss shapes the representation space by pushing positive word-image pairs closer together and negative pairs apart. Core assumption is that pretrained models capture relevant features and InfoNCE effectively shapes the representation space. Break condition occurs if pretrained models are not well-suited or if InfoNCE is not appropriate.

## Foundational Learning

- **Multimodal Learning**: VGS models learn from two modalities simultaneously, requiring understanding of how to fuse and align information from both speech and vision. Quick check: How does a VGS model learn to associate a spoken word with a visual object without using text?

- **Few-Shot Learning**: The approach relies on learning new keywords from only a few examples, requiring understanding of how to efficiently learn from limited data. Quick check: How does the query-by-example system identify relevant utterances and images for a given keyword from a large unlabelled dataset?

- **Mutual Exclusivity Bias**: The computational study investigates whether VGS models exhibit the ME bias, requiring understanding of this cognitive phenomenon and how to test for it. Quick check: How does the ME bias influence a child's word learning, and how can we test for it in a computational model?

## Architecture Onboarding

- **Component map**: Audio branch (CPC network + BiLSTM + linear layers) -> Multimodal attention mechanism (matchmap or word-to-image attention) -> Vision branch (AlexNet adaptation) -> Similarity score calculation
- **Critical path**: Data flows from input speech and image through respective branches, the attention mechanism, and the final similarity score calculation
- **Design tradeoffs**: Matchmap attention provides detailed alignment information but is computationally expensive; word-to-image attention is faster but may lose alignment information. Pretrained initializations affect prior knowledge and performance.
- **Failure signatures**: Poor word-object mapping indicates issues with attention mechanism, pretrained initializations, or training data. Lack of ME bias suggests problems with architecture, loss function, or evaluation setup.
- **First 3 experiments**: 1) Train on small ground truth word-image pairs and evaluate novel pair matching, 2) Test query-by-example system accuracy on held-out pairs, 3) Compare performance with different pretrained initializations (CPC vs random, AlexNet vs random)

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: What is the impact of using different loss functions (e.g., InfoNCE vs. hinge) on the ME bias strength across multilingual settings?
- **Basis in paper**: Explicit - The paper tested different loss functions and found InfoNCE yielded the strongest ME bias in monolingual settings, but only tested multilingual models with InfoNCE.
- **Why unresolved**: Monolingual models were tested with multiple loss functions, but multilingual models were only tested with InfoNCE.
- **What evidence would resolve it**: Testing bilingual/trilingual models with hinge and standard contrastive losses and comparing ME bias strength to monolingual models with same losses.

### Open Question 2
- **Question**: How does training data size (number of examples per class) affect ME bias strength differently in monolingual vs multilingual models?
- **Basis in paper**: Explicit - The paper found that as familiar-familiar score increased, ME bias strengthened in monolingual settings, but the opposite trend occurred when comparing monolingual to multilingual models.
- **Why unresolved**: The paper only compared models trained on different total dataset sizes, not varying examples per class systematically across language settings.
- **What evidence would resolve it**: Training monolingual and multilingual models on datasets with identical total number of examples but different distribution across languages/classes.

### Open Question 3
- **Question**: What is the effect of cross-lingual loss terms (e.g., aligning word representations across languages) on ME bias strength in multilingual models?
- **Basis in paper**: Explicit - The paper tested preliminary experiments adding cross-lingual terms to bilingual/trilingual losses and found changes in ME trends, but effects were inconclusive.
- **Why unresolved**: Only one type of cross-lingual term was tested; other types of cross-lingual constraints or combinations were not explored.
- **What evidence would resolve it**: Systematically testing different cross-lingual loss formulations across bilingual/trilingual models and comparing ME bias strength to monolingual models.

## Limitations

- Pair mining approach for few-shot learning relies heavily on query-by-example system accuracy, which may introduce noise for truly low-resource languages
- Mutual exclusivity bias findings are based on controlled experimental conditions that may not capture real-world language acquisition complexity
- Limited evaluation of VPKL performance across different languages and datasets makes generalizability difficult to assess

## Confidence

- **High confidence**: Core methodology for visually grounded speech models and keyword localisation is well-established and supported by experimental results
- **Medium confidence**: Few-shot learning approach and pair mining methodology show promise but require further validation across diverse low-resource languages
- **Medium confidence**: Mutual exclusivity bias findings are methodologically sound but need replication across broader language contexts and model architectures

## Next Checks

1. Evaluate the VPKL system's performance on additional low-resource languages beyond Yoruba, particularly those with different phonetic and morphological structures
2. Conduct ablation studies to quantify the impact of pretrained initializations (CPC and AlexNet) on the mutual exclusivity bias across different model configurations
3. Test the robustness of the pair mining approach by systematically varying the number of support examples and measuring the quality of mined pairs using human evaluation