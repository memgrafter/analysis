---
ver: rpa2
title: 'Chain-of-Instructions: Compositional Instruction Tuning on Large Language
  Models'
arxiv_id: '2402.11532'
source_url: https://arxiv.org/abs/2402.11532
tags:
- instruction
- output
- instructions
- input
- tasks
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Chain-of-Instructions (CoI) addresses the limitation of existing
  instruction-tuned models in handling complex, multi-step tasks. The core method
  involves composing existing single-task instructions into chains where each step's
  output becomes the next step's input, and fine-tuning models on these composed datasets.
---

# Chain-of-Instructions: Compositional Instruction Tuning on Large Language Models

## Quick Facts
- arXiv ID: 2402.11532
- Source URL: https://arxiv.org/abs/2402.11532
- Authors: Shirley Anugrah Hayati, Taehee Jung, Tristan Bodding-Long, Sudipta Kar, Abhinav Sethy, Joo-Kyung Kim, Dongyeop Kang
- Reference count: 28
- Key outcome: CoI-tuned models achieve ROUGE-L scores up to 70.76% vs 24.93% for non-fine-tuned models on 2-instruction tests

## Executive Summary
Chain-of-Instructions (CoI) addresses the limitation of existing instruction-tuned models in handling complex, multi-step tasks by composing single-task instructions into chains where each step's output becomes the next step's input. The approach fine-tunes models on these composed datasets, enabling generation of intermediate results at each step. Experiments demonstrate significant improvements over baselines on compositional instruction tasks, with consistent generalization to longer chains and downstream multilingual summarization tasks.

## Method Summary
The method involves creating CoI datasets by automatically composing existing single-task instructions from the SUPER-NATURALINSTRUCTIONS dataset. An LLM-in-the-loop checks composability by validating input-output format compatibility between instruction pairs. Models are fine-tuned on these composed datasets (CoI2, CoI3) and evaluated on compositional instruction tests with varying chain lengths. The approach enables models to generate intermediate outputs at each subtask step, improving interpretability and compositional reasoning capabilities.

## Key Results
- CoI-tuned models achieve ROUGE-L scores of 70.76% vs 24.93% for non-fine-tuned models on 2-instruction tests
- Models trained on CoI3 generalize to longer chains (σ=4,5) without performance degradation
- CoI-tuned models outperform baselines on downstream multilingual summarization tasks, particularly for longer inputs
- BIG-Bench Hard evaluation shows CoI-tuned models achieve +6.02% ROUGE-L improvement over CoI1-tuned models

## Why This Works (Mechanism)

### Mechanism 1
Chain-of-Instructions (CoI) improves compositional reasoning by enforcing sequential subtask dependencies during fine-tuning. Each instruction step's output becomes the next step's input, mirroring real-world multi-step reasoning patterns. Core assumption: LLMs can learn to maintain intermediate state representations across chained tasks when explicitly trained on such patterns. Break condition: If intermediate outputs are corrupted or if task formats mismatch, the chain fails.

### Mechanism 2
Composability checking via LLM-in-the-loop filters invalid instruction pairs, improving dataset quality. An LLM validates whether two instructions can be chained by checking input-output format compatibility and generating expected outputs. Core assumption: LLM's own reasoning about task compatibility generalizes to real-world instruction chaining. Break condition: If the LLM's composability judgment is noisy or if format heuristics are incomplete, invalid chains enter training data.

### Mechanism 3
Step-by-step intermediate output generation improves interpretability and debugging. Models generate outputs at each subtask step, not just final answers, allowing traceability of reasoning. Core assumption: Explicit intermediate outputs help models align subtask boundaries and preserve semantic consistency. Break condition: If intermediate outputs are too long or noisy, the model may struggle to retain focus on the next subtask.

## Foundational Learning

- **Concept**: Instruction chaining (compositionality in NLP)
  - **Why needed here**: The core innovation relies on chaining multiple instructions where each output feeds the next
  - **Quick check question**: Given instruction A: "Summarize text" and B: "Translate to French", can the output of A be a valid input for B?

- **Concept**: Format compatibility between tasks
  - **Why needed here**: Not all tasks can be composed; classification outputs cannot feed into generation tasks directly
  - **Quick check question**: Can a POS tagging output (list of tags) be a valid input for a summarization task? Why or why not?

- **Concept**: Intermediate state preservation in transformer models
  - **Why needed here**: Models must maintain context across multiple reasoning steps without losing track
  - **Quick check question**: In a 3-step CoI chain, if step 1 fails, how should the model handle steps 2 and 3?

## Architecture Onboarding

- **Component map**: Seed single-task instructions → Instruction composition checker (LLM) → CoI dataset builder → Fine-tuning script → Evaluation harness
- **Critical path**: 1. Seed single-task instructions → 2. Compose via LLM checker → 3. Generate chained outputs → 4. Fine-tune base model → 5. Evaluate on CoI test sets
- **Design tradeoffs**: Using LLM for composability check trades compute cost for higher data quality; shorter instructions (C-CoI) improve generalization but may lose nuance; training on CoI3 helps longer chains but risks overfitting to long-format patterns
- **Failure signatures**: Degraded ROUGE-L on subtask 2/3 indicates chain reasoning collapse; low #valid outputs in downstream tasks signals format mismatch; high variance across runs suggests instability in instruction composition
- **First 3 experiments**: 1. Run ablation: fine-tune with correct vs. random second-step outputs to confirm importance of intermediate correctness; 2. Test generalization: evaluate on CoI4/5 with models trained only on CoI1/2/3; 3. Compare instruction length: train C-CoI vs full CoI and measure BBH performance gain

## Open Questions the Paper Calls Out

### Open Question 1
How does the quality of LLM-generated composability checks affect downstream task performance? The paper states that 75% of LLM-generated composable instructions are valid for CoI2 and 59% for CoI3, with error rates "often found in LLM-generated data." This remains unresolved as the paper only measures validity of generated data, not whether lower quality composability checks would impact model performance differently.

### Open Question 2
Would decomposing complex instructions into subtasks improve model performance more than composing them? The paper focuses on instruction composition but mentions "for future work, we consider looking into instruction decomposition." This remains unresolved as the current work only examines compositionality, leaving open whether decomposition might be more effective for certain task types.

### Open Question 3
How do different instruction lengths affect compositional reasoning in LLMs? The paper notes that "instructions in CoI3 become very long, thereby it becomes harder for the model to generalize" and presents concise-CoI as a variant. This remains unresolved as while the paper shows longer instructions hurt performance, it doesn't systematically explore optimal instruction length or identify breaking points.

## Limitations

- Reliance on LLM-based composability checking introduces potential bias and noise into training data (25% of composed instructions may be invalid)
- Automatic instruction summarization and separation processes are not fully detailed, making reliability assessment difficult
- Evaluation primarily focuses on text generation tasks with limited validation on other instruction types like classification or question answering

## Confidence

**High Confidence**: The core finding that fine-tuning on composed instructions improves performance on multi-step tasks (ROUGE-L scores increasing from 24.93% to 70.76% on 2-instruction tests) is well-supported by experimental data and ablation studies.

**Medium Confidence**: The claim that CoI-tuning generalizes to longer chains (σ=4,5) and downstream tasks is supported but could benefit from more extensive testing across diverse task types and domains.

**Low Confidence**: The assertion that CoI models maintain superior performance across all subtask positions (step 1, 2, and 3) in compositional instructions needs further validation, as the paper only provides aggregate ROUGE-L scores per subtask.

## Next Checks

1. **Dataset Quality Assessment**: Manually inspect a random sample of the composed instructions to verify the 75% validity rate claimed by the LLM checker and identify common failure patterns.

2. **Cross-Domain Generalization**: Evaluate CoI-tuned models on non-text generation tasks (e.g., classification, QA) from BIG-Bench Hard to verify compositional reasoning transfer beyond summarization tasks.

3. **Long Chain Robustness**: Test models trained on CoI2/CoI3 against manually crafted CoI4/CoI5 chains with known correct intermediate outputs to isolate whether performance degradation is due to chain length or instruction quality.