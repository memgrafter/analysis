---
ver: rpa2
title: Cross-lingual Transfer of Reward Models in Multilingual Alignment
arxiv_id: '2410.18027'
source_url: https://arxiv.org/abs/2410.18027
tags:
- reward
- english
- language
- multilingual
- cross-lingual
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This work investigates cross-lingual transfer in reward models\
  \ (RMs) trained across diverse languages, primarily from English. Experiments demonstrate\
  \ that English RMs consistently outperform target language RMs by 3\u20134% on average\
  \ in Multilingual RewardBench, with reasoning tasks showing the most significant\
  \ gains."
---

# Cross-lingual Transfer of Reward Models in Multilingual Alignment

## Quick Facts
- arXiv ID: 2410.18027
- Source URL: https://arxiv.org/abs/2410.18027
- Authors: Jiwoo Hong; Noah Lee; Rodrigo Martínez-Castaño; César Rodríguez; James Thorne
- Reference count: 35
- Primary result: English reward models outperform target language RMs by 3–4% on average in multilingual alignment tasks

## Executive Summary
This work investigates cross-lingual transfer in reward models trained across diverse languages, primarily from English. The authors demonstrate that English reward models consistently outperform target language models by 3–4% on average in Multilingual RewardBench, with reasoning tasks showing the most significant gains. The study reveals that English reward models preserve the representation diversity of multilingual pre-trained models better than other languages, which tend to induce representation collapse. The findings are validated through multilingual alignment experiments showing that English reward models improve instruction-following capabilities in four non-English languages by an average of 9.5% win rate.

## Method Summary
The study fine-tunes multilingual pre-trained language models (MLMs) like Llama-3.2-3B and Qwen2.5-3B as reward models using synthetic preference data from English sources (SafeRLHF, WildGuard, HelpSteer2, Offsetbias, Magpie), translated to Spanish, Italian, Korean, and Chinese. The trained reward models are evaluated on Multilingual RewardBench and analyzed for representation shifts by comparing hidden states with base models. Multilingual alignment is performed using Direct Preference Optimization (DPO), with performance evaluated by Multilingual AlpacaEval.

## Key Results
- English reward models outperform target language RMs by 3–4% on average in Multilingual RewardBench
- Reasoning tasks show the most significant gains from English reward models
- English RMs improve instruction-following capabilities in four non-English languages by an average of 9.5% win rate
- Representation analysis shows English RMs better preserve multilingual representation diversity

## Why This Works (Mechanism)

### Mechanism 1
- Claim: English reward models preserve multilingual representations better than target-language models
- Mechanism: During fine-tuning on English preference data, the English RM maintains the diverse representation space learned by multilingual pre-trained models, while target-language RMs cause representation collapse
- Core assumption: Representation preservation during fine-tuning is crucial for maintaining generalization capabilities across languages
- Evidence anchors: English RMs consistently outperform target language RMs by 3–4% on average in Multilingual RewardBench; English RMs best preserve the representations by staying close to the base instruct model

### Mechanism 2
- Claim: Multilingual pre-trained models inherently encode language-aware representations that should be preserved
- Mechanism: MLMs trained on multilingual corpora develop similar embedding norm distributions across languages, enabling language-aware decoding that preserves fine-grained representations
- Core assumption: Similar token embedding norm distributions across languages indicate that decoder layers can return language-aware hidden states
- Evidence anchors: MLMs have similar token embedding norm distributions across the language; representations of MLMs inherently have a strong understanding of languages

### Mechanism 3
- Claim: Cross-lingual transfer in reward models propagates to downstream multilingual alignment tasks
- Mechanism: English RMs used for preference labeling in alignment training provide better guidance signals that improve instruction-following capabilities across non-English languages
- Core assumption: The quality of preference signals directly impacts the effectiveness of alignment training
- Evidence anchors: Multilingual alignment experiments show that using English RMs improves instruction-following capabilities in four non-English languages by an average of 9.5% win rate

## Foundational Learning

- Concept: Representation preservation during fine-tuning
  - Why needed here: The core finding relies on English RMs preserving multilingual representations better than target-language RMs
  - Quick check question: What metric would you use to measure representation preservation between a fine-tuned model and its base model?

- Concept: Cross-lingual transfer in multilingual pre-trained models
  - Why needed here: The effectiveness of English RMs depends on the underlying MLMs having cross-lingual transfer capabilities
  - Quick check question: How would you determine if a multilingual pre-trained model has learned language-aware representations?

- Concept: Reward modeling and preference optimization
  - Why needed here: Understanding how RMs work and how they're used in alignment training is crucial for implementing the proposed approach
  - Quick check question: What's the difference between classifier-based and generative reward models, and when would you use each?

## Architecture Onboarding

- Component map: Base MLMs (Llama-3.2-3B, Qwen2.5-3B) → English preference dataset → English RM → Multilingual alignment training
- Critical path: Base MLM → English preference fine-tuning → RM training → Alignment data generation → DPO fine-tuning → Performance evaluation
- Design tradeoffs:
  - English-only vs. multilingual preference data: English is more accessible but may miss language-specific nuances
  - Classifier vs. generative RMs: Classifier RMs are more common but generative RMs might capture richer feedback
  - Model size: Smaller models are faster to train but may have limited cross-lingual capacity
- Failure signatures:
  - Low cross-lingual transfer performance despite using English RMs
  - Representation collapse measured by increased proportion of largest singular value
  - Alignment improvements limited to languages similar to English
- First 3 experiments:
  1. Measure representation preservation between English RM and target language RM using singular value analysis
  2. Compare cross-lingual transfer performance of classifier vs. generative RMs on the same multilingual dataset
  3. Test alignment performance using English RMs on languages with varying linguistic distance from English

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What specific properties of multilingual pre-trained language models (MLMs) enable them to develop language-aware representations that benefit cross-lingual transfer in reward models?
- Basis in paper: The paper discusses that MLMs have similar token embedding norm distributions across languages, implying they are trained to encode information with diverse linguality. It also mentions that preservation of these representations is crucial for generalizability

## Limitations

- Synthetic data dependence: The study relies entirely on synthetic preference data generated from English sources and machine-translated to other languages, raising questions about cultural appropriateness and translation quality.
- Dataset composition opacity: The specific composition of the synthetic preference dataset is not fully specified, making it difficult to assess generalizability to other data compositions.
- Evaluation scope constraints: The benchmarks used represent a limited set of tasks and may not capture all aspects of cross-lingual alignment quality.

## Confidence

**High confidence**: The core empirical finding that English RMs outperform target-language RMs by 3-4% on Multilingual RewardBench is well-supported by the experimental results presented.

**Medium confidence**: The mechanism explaining why English RMs perform better (representation preservation preventing collapse) is plausible but not definitively proven, with alternative explanations possible.

**Low confidence**: The generalizability of the 9.5% win rate improvement to other model architectures, alignment approaches, or target languages is uncertain without additional experiments across different settings.

## Next Checks

1. **Real human preference data validation**: Compare performance of English RMs versus target-language RMs using human-annotated preference data rather than synthetic data to verify that results hold for authentic human judgments.

2. **Linguistic distance impact study**: Systematically test alignment performance across languages with varying linguistic distances from English to determine if the advantage of English RMs diminishes for linguistically distant languages.

3. **Alternative explanation elimination**: Conduct controlled experiments varying the diversity and domain distribution of the preference data while keeping the language constant to determine whether English's advantage stems from representation preservation or simply from having more diverse preference data.