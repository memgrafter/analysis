---
ver: rpa2
title: A rationale from frequency perspective for grokking in training neural network
arxiv_id: '2405.17479'
source_url: https://arxiv.org/abs/2405.17479
tags:
- frequency
- training
- data
- test
- grokking
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper provides a frequency-based explanation for the grokking
  phenomenon in neural networks, where training loss decreases faster than test loss
  initially before both improve. The core insight is that networks initially learn
  less salient frequency components present in training data but not dominant in test
  data, due to insufficient sampling causing spurious low-frequency components.
---

# A rationale from frequency perspective for grokking in training neural network

## Quick Facts
- **arXiv ID**: 2405.17479
- **Source URL**: https://arxiv.org/abs/2405.17479
- **Reference count**: 14
- **Primary result**: Frequency-based explanation for grokking phenomenon showing initial overfitting to spurious low-frequency components due to insufficient sampling

## Executive Summary
This paper provides a frequency-based explanation for the grokking phenomenon in neural networks, where training loss decreases faster than test loss initially before both improve. The core insight is that networks initially learn less salient frequency components present in training data but not dominant in test data, due to insufficient sampling causing spurious low-frequency components. With default initialization, networks follow the frequency principle (F-Principle) of learning low to high frequencies, leading to initial overfitting of spurious components and increased test loss. With large initialization, networks learn all frequencies similarly, initially capturing spurious frequencies from insufficient sampling. This explanation is demonstrated empirically across synthetic and real datasets including MNIST.

## Method Summary
The study combines theoretical analysis of frequency domain properties with empirical validation across multiple datasets. The authors analyze how insufficient sampling in training data creates spurious low-frequency components that networks initially learn before capturing the true underlying frequency structure. They examine two initialization regimes: default initialization following frequency principle (low to high frequencies), and large initialization where all frequencies are learned similarly. The methodology includes systematic experiments on synthetic datasets with controlled frequency distributions and real datasets like MNIST, tracking frequency component learning dynamics throughout training.

## Key Results
- Training loss can decrease while test loss increases during early training stages due to initial learning of spurious low-frequency components
- Default initialization leads to F-Principle learning (low to high frequencies), causing initial overfitting to spurious components
- Large initialization causes similar learning rates across all frequencies, initially capturing spurious frequencies from insufficient sampling
- Empirical validation demonstrates the proposed mechanism on synthetic and real datasets including MNIST

## Why This Works (Mechanism)
The mechanism works through the interaction between sampling frequency and network initialization. When training data has insufficient sampling, spurious low-frequency components appear that are not present in the true underlying distribution. Neural networks with default initialization follow the frequency principle, learning low frequencies first. This causes them to initially fit these spurious components present in training data but not in test data, leading to increased test loss despite decreasing training loss. With large initialization, networks learn all frequencies similarly, but still initially capture the spurious low-frequency components created by insufficient sampling.

## Foundational Learning
- **Frequency Principle (F-Principle)**: Neural networks tend to learn low-frequency components before high-frequency components. Why needed: Explains why networks initially fit spurious low-frequency components. Quick check: Verify frequency learning order on simple synthetic data.
- **Insufficient Sampling Effects**: Limited training data samples can create artificial low-frequency components not present in the true distribution. Why needed: Explains the source of spurious components that cause initial overfitting. Quick check: Compare frequency spectra of well-sampled vs. undersampled data.
- **Initialization Scale Impact**: Different initialization scales affect how networks learn different frequency components. Why needed: Explains why both default and large initialization can lead to grokking. Quick check: Vary initialization scales systematically and measure frequency learning dynamics.

## Architecture Onboarding

**Component Map:**
Training Data -> Network Initialization -> Frequency Learning Dynamics -> Training/Test Loss Evolution

**Critical Path:**
Training Data (with insufficient sampling) -> Network Initialization (default or large) -> Initial Frequency Learning (spurious low-frequencies) -> Increased Test Loss / Decreased Training Loss -> Later Learning of True Frequencies -> Improved Test Performance

**Design Tradeoffs:**
- Default initialization follows F-Principle but may initially overfit spurious components
- Large initialization learns all frequencies similarly but still captures spurious components
- Insufficient sampling creates the fundamental problem of spurious frequency components
- Network depth and width may affect frequency learning dynamics

**Failure Signatures:**
- Training loss decreasing while test loss increases during early training stages
- Different behavior between training and test loss curves
- Convergence patterns that show initial degradation before improvement

**3 First Experiments:**
1. Generate synthetic 1D signals with controlled frequency components and test with different sampling rates
2. Train on MNIST with varying initialization scales and track frequency component learning throughout training
3. Create controlled undersampled datasets and compare frequency learning dynamics with well-sampled counterparts

## Open Questions the Paper Calls Out
None

## Limitations
- Theoretical framework relies heavily on assumptions about frequency component dominance in training versus test data
- Connection between insufficient sampling leading to spurious low-frequency components and actual model behavior needs more rigorous mathematical formalization
- Generalizability to more complex real-world scenarios beyond synthetic and MNIST datasets remains uncertain

## Confidence

**High:**
- The empirical observation that training loss can decrease while test loss increases during early training stages

**Medium:**
- The claim that networks initially learn less salient frequency components due to sampling effects

**Low:**
- The specific mechanism linking initialization scale to frequency learning patterns across all neural network architectures

## Next Checks

1. Test the frequency-based explanation on diverse datasets beyond MNIST, including those with known frequency distributions and natural image datasets like CIFAR-10
2. Conduct ablation studies varying initialization scales systematically while measuring frequency component learning dynamics throughout training
3. Develop and validate mathematical bounds on the relationship between sampling frequency and spurious component emergence in training data