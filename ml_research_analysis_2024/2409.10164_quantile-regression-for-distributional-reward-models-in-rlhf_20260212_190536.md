---
ver: rpa2
title: Quantile Regression for Distributional Reward Models in RLHF
arxiv_id: '2409.10164'
source_url: https://arxiv.org/abs/2409.10164
tags:
- reward
- quantile
- distribution
- regression
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the limitations of traditional point-estimate
  reward models in reinforcement learning from human feedback (RLHF), which fail to
  capture the diversity and complexity of human preferences. The authors propose Quantile
  Reward Models (QRM), a novel approach that uses quantile regression to learn a full
  distribution over rewards instead of a single scalar value.
---

# Quantile Regression for Distributional Reward Models in RLHF

## Quick Facts
- arXiv ID: 2409.10164
- Source URL: https://arxiv.org/abs/2409.10164
- Authors: Nicolai Dorka
- Reference count: 17
- Primary result: QRM achieves 91.2 on RewardBench, outperforming point-estimate baselines

## Executive Summary
This paper introduces Quantile Reward Models (QRM), a novel approach for distributional reward modeling in RLHF that captures the full distribution of human preferences rather than point estimates. By using quantile regression to estimate reward distributions for multiple attributes and combining them through a gating network, QRM provides a more nuanced representation of preferences. The authors demonstrate that QRM outperforms traditional Bradley-Terry models on RewardBench and can be leveraged for risk-aware RLHF to produce safer policies while maintaining performance.

## Method Summary
QRM employs a two-stage training approach. First, frozen LLM backbone embeddings are used to train 19 quantile regression layers per attribute (helpfulness, harmlessness, etc.) on 8 attribute datasets, capturing the full reward distribution. Second, a gating network (3-layer MLP with softmax) is trained on 10 preference datasets using Bradley-Terry loss to learn optimal mixing weights for combining attribute distributions into a final reward distribution. The model can then be used for risk-aware RLHF by applying a concave utility function to the reward distribution.

## Key Results
- QRM achieves 91.2 on RewardBench compared to 83.6 for Bradley-Terry and 90.8 for ArmoRM
- Risk-aware RLHF with QRM produces fewer extremely negative responses while maintaining similar overall performance
- The gating network successfully learns to weight attribute distributions based on preference data

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Quantile regression captures the full reward distribution rather than collapsing it into a point estimate.
- Mechanism: By estimating multiple quantiles (e.g., 0.05, 0.10, ..., 0.95), the model learns the conditional distribution of rewards across different levels of satisfaction. This preserves information about uncertainty and multimodal preferences that would otherwise be lost in a single scalar value.
- Core assumption: The true reward distribution can be well-approximated by a finite set of quantiles and that these quantiles can be learned independently without significant crossing issues.
- Evidence anchors:
  - [abstract] "Our method uses quantile regression to estimate a full, potentially multimodal distribution over preferences, providing a more powerful and nuanced representation of preferences."
  - [section 3.3] "Unlike OLS, which provides a single estimate of the central tendency, quantile regression allows to approximately represent a distribution over the response variable, offering more information about the data."

### Mechanism 2
- Claim: The gating network learns to optimally weight attribute distributions to maximize preference likelihood.
- Mechanism: The gating network outputs mixing weights for each attribute's distribution, creating a weighted mixture that represents the final reward distribution. It is trained using the Bradley-Terry loss to ensure that preferred responses have higher expected reward values, effectively learning which attributes matter most for preference decisions.
- Core assumption: The optimal way to combine attribute distributions into a final reward distribution can be learned through preference data without needing to retrain the attribute distributions.
- Evidence anchors:
  - [section 3.4.2] "The gating network is trained using preference data and the Bradley-Terry loss following the approach in (Wang et al., 2024a)."
  - [section 3.4] "The mixed distribution is then computed as the weighted sum of the M quantile values for each attribute, given by: Qmix(τi) = Σ_m gm · Qm(τi)."

### Mechanism 3
- Claim: Risk-aware RLHF can leverage the distributional information to penalize high-variance responses more heavily.
- Mechanism: By applying a concave utility function (e.g., -e^(-λr)) to the reward distribution, responses with significant probability mass on low reward values are penalized more than their expected value would suggest. This encourages policies that avoid producing responses with high variance in quality.
- Core assumption: Penalizing low-reward mass in the distribution leads to safer policies without sacrificing overall performance.
- Evidence anchors:
  - [section 3.5] "This utility function penalizes distributions with substantial probability mass on low reward values. For instance, a bimodal distribution with one peak at a low value and another at a high value will have a lower utility than an unimodal distribution centered around the same expected value."
  - [section 4.3] "Both policies achieve similar final performance on this metric, demonstrating that employing the risk-aware utility does not lead to an overall decrease in performance."

## Foundational Learning

- Concept: Quantile regression
  - Why needed here: Traditional regression methods estimate only the conditional mean, which loses information about the distribution of rewards. Quantile regression allows us to estimate the full distribution of possible rewards for each attribute.
  - Quick check question: What loss function is minimized in quantile regression compared to ordinary least squares regression?

- Concept: Bradley-Terry model for pairwise comparisons
  - Why needed here: The gating network needs to learn which responses are preferred over others using preference data. The Bradley-Terry model provides a probabilistic framework for converting reward estimates into preference probabilities.
  - Quick check question: How does the Bradley-Terry model compute the probability that one response is preferred over another?

- Concept: Risk-aware utility functions in RL
  - Why needed here: The distributional reward model provides more information than just expected reward, enabling the design of utility functions that consider the shape of the distribution, particularly penalizing responses with high probability of being poor quality.
  - Quick check question: How does a concave utility function applied to a reward distribution differ from using just the expected reward?

## Architecture Onboarding

- Component map:
  - LLM backbone (frozen) -> Quantile regression layers (K per attribute) -> Gating network (MLP) -> Distribution mixer -> (Optional) Risk-aware utility

- Critical path:
  1. Input prompt and response → LLM backbone embeddings
  2. Prompt embedding → Gating network → Mixing weights
  3. Prompt-response embedding → Quantile regression layers → Attribute distributions
  4. Attribute distributions × Mixing weights → Final reward distribution
  5. (Optional) Apply risk-aware utility → Final utility value for RL

- Design tradeoffs:
  - Number of quantiles (K): More quantiles capture distribution shape better but increase computational cost and risk of quantile crossing
  - Attribute selection: More attributes capture preferences better but increase model complexity and data requirements
  - Gating network architecture: Deeper networks may capture complex relationships but risk overfitting
  - Risk parameter λ: Higher values increase safety but may reduce creativity/exploration

- Failure signatures:
  - Quantile crossing: Q(0.1) > Q(0.2) or similar violations, indicating unstable quantile regression
  - Uniform gating weights: All attributes weighted equally suggests gating network isn't learning meaningful preferences
  - Extremely peaked distributions: Suggests overfitting to specific training examples
  - High variance in utility estimates: May indicate instability in the risk-aware component

- First 3 experiments:
  1. Train QRM with K=5 quantiles on a single attribute dataset and visualize the learned distribution compared to point estimate baseline
  2. Evaluate the gating network's learned weights across different attribute types to verify it's capturing meaningful preference patterns
  3. Compare RLHF policies trained with risk-aware utility (λ=1) versus risk-neutral utility on both expected reward and left-tail quantiles (τ=0.05, 0.10)

## Open Questions the Paper Calls Out

- **Open Question 1**: Does incorporating raw, conflicting annotations (rather than majority-vote labels) into QRM training improve its performance on capturing diverse human preferences?
  - Basis in paper: [explicit] The authors mention this as an "exciting direction for future research" and note that QRM could account for raw, conflicting annotations for the same prompt-response pair.
  - Why unresolved: The paper only demonstrates QRM's performance with aggregated preference data. The authors hypothesize that incorporating conflicting labels could improve the model's ability to handle diverse preferences but have not tested this.
  - What evidence would resolve it: Experimental results comparing QRM trained on raw conflicting annotations versus aggregated labels, showing improved performance metrics on benchmarks like RewardBench or enhanced ability to capture multimodal preference distributions.

- **Open Question 2**: Can QRM be effectively integrated with distributional RL algorithms that estimate distributions for the value function, and does this integration improve policy performance?
  - Basis in paper: [explicit] The authors suggest this as a promising avenue, noting that "it would be interesting to explore integrating our distributional reward model with distributional RL algorithms."
  - Why unresolved: The paper only demonstrates QRM's application in risk-aware RLHF with a point-estimate value function. The potential benefits of combining distributional reward models with distributional RL remain unexplored.
  - What evidence would resolve it: Experimental results showing improved policy performance (e.g., higher expected rewards, better risk management) when using QRM with distributional RL algorithms compared to traditional RL approaches.

- **Open Question 3**: How does QRM's additional information enhance interpretability and enable more creative ways to steer LLMs during downstream fine-tuning?
  - Basis in paper: [explicit] The authors state that "the extra information provided by our distributional reward model allows for more interpretability as well as more creative ways to steer LLMs during downstream fine-tuning."
  - Why unresolved: While the paper mentions the potential for enhanced interpretability and creative steering, it does not provide concrete examples or experimental evidence of how QRM's distributional information can be leveraged in these ways.
  - What evidence would resolve it: Case studies or experiments demonstrating specific applications where QRM's distributional information improves interpretability (e.g., identifying preference patterns) or enables novel fine-tuning techniques (e.g., targeted attribute optimization based on quantile estimates).

## Limitations
- The two-stage training procedure may introduce optimization challenges and cannot retrain attribute distributions
- Performance-safety trade-off is sensitive to λ parameter selection and requires careful tuning
- Fixed number of quantiles may not optimally capture complex, multimodal reward distributions

## Confidence
**High Confidence:**
- QRM achieves superior performance on RewardBench compared to point-estimate baselines (91.2 vs 83.6 for Bradley-Terry)
- The distributional information can be leveraged for risk-aware RLHF to reduce extremely negative responses
- Quantile regression layers successfully learn meaningful attribute distributions

**Medium Confidence:**
- The gating network learns meaningful mixing weights that reflect attribute importance
- Risk-aware utility does not significantly degrade overall performance while improving safety metrics
- The two-stage training approach is sufficient for capturing complex preference patterns

## Next Checks
1. **Quantile Stability Analysis**: Implement diagnostic tools to monitor quantile crossing during training and evaluate whether the L1 regularization strength (α=0.003) is sufficient to maintain stable, non-crossing quantile estimates across all attributes and datasets.

2. **Gating Network Generalization**: Test the gating network's learned weights on held-out preference datasets and systematically vary the temperature parameter in the Bradley-Terry loss to assess whether the network is overfitting to training preferences versus learning general preference patterns.

3. **Risk Parameter Sensitivity**: Conduct a systematic ablation study varying λ from 0.1 to 10 in the risk-aware utility function, measuring the trade-off between left-tail quantiles (τ=0.05, 0.10) and overall RewardBench performance to identify the optimal balance between safety and capability.