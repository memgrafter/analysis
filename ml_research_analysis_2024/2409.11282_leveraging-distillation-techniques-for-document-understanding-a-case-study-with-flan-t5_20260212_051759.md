---
ver: rpa2
title: 'Leveraging Distillation Techniques for Document Understanding: A Case Study
  with FLAN-T5'
arxiv_id: '2409.11282'
source_url: https://arxiv.org/abs/2409.11282
tags:
- document
- understanding
- language
- learning
- student
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates knowledge distillation from ChatGPT 3.5
  into FLAN-T5 for document understanding tasks, leveraging curriculum learning strategies.
  Documents are converted to textual representations preserving layout, then used
  to generate labels via prompting ChatGPT, which are used to fine-tune FLAN-T5 models
  of three sizes.
---

# Leveraging Distillation Techniques for Document Understanding: A Case Study with FLAN-T5

## Quick Facts
- **arXiv ID**: 2409.11282
- **Source URL**: https://arxiv.org/abs/2409.11282
- **Reference count**: 5
- **Primary result**: Distills ChatGPT 3.5 knowledge into FLAN-T5 for document understanding, achieving promising results across five datasets

## Executive Summary
This paper presents a knowledge distillation approach that transfers document understanding capabilities from ChatGPT 3.5 to FLAN-T5 models. The methodology leverages curriculum learning strategies and converts documents to textual representations that preserve layout information through spacing and newline formatting. The study evaluates three sizes of FLAN-T5 models (small, base, large) across five datasets, finding that while multimodal models outperform, the distilled models achieve competitive results, particularly on TabFact where the large variant even surpasses the teacher model. The approach demonstrates the viability of using pure text representations for document understanding tasks while maintaining computational efficiency.

## Method Summary
The approach converts documents to textual representations preserving layout using LAPDoc SpatialFormat verbalization, which reconstructs original layouts through spacing and newline insertion based on OCR bounding box coordinates. ChatGPT 3.5 generates JSON-formatted labels via task-specific prompts, which are then used to fine-tune FLAN-T5 models through LoRA adapters. Curriculum learning with temperature sampling presents training data in order of increasing difficulty, measured by normalized Levenshtein distance between student predictions and labels. The methodology is evaluated across six datasets (57,202 train samples, 4,329 test samples) using five different evaluation metrics.

## Key Results
- FLAN-T5 models successfully distilled document understanding knowledge from ChatGPT 3.5 across five datasets
- FLAN-T5LARGE achieved 83.45% accuracy on TabFact, surpassing the teacher model's 80.00%
- Larger FLAN-T5 models benefited more from curriculum learning than smaller ones
- Despite multimodal models outperforming, the pure text approach demonstrated computational efficiency benefits

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Spatial layout preservation through textual reconstruction enables layout-aware distillation
- Mechanism: LAPDoc SpatialFormat verbalization converts OCR bounding box coordinates into formatted text with spaces/newlines, preserving spatial relationships in a purely textual form that LLMs can process
- Core assumption: LLMs can learn document understanding from layout-encoded text without requiring multimodal vision components
- Evidence anchors:
  - [abstract] "Documents are converted to textual representations preserving layout"
  - [section 3.1] "LAPDoc SpatialFormat verbalization... aims to reconstruct the documents original layout by formatting the bounding box contents via insertion of spaces and newlines"
  - [corpus] Weak evidence - only mentions related work on layout-aware prompting but not direct evidence of this mechanism working
- Break condition: If layout complexity exceeds what can be represented through spacing/newline formatting, or if LLMs cannot learn spatial reasoning from pure text

### Mechanism 2
- Claim: Curriculum learning improves distillation by presenting data in order of increasing difficulty
- Mechanism: Student predictions on training data are used to sample progressively harder examples via similarity-based temperature sampling, where difficulty is measured by normalized Levenshtein distance between predictions and labels
- Core assumption: Training on simpler examples first helps the student model converge better and generalize more effectively
- Evidence anchors:
  - [abstract] "Our methodology integrates labeling and curriculum-learning mechanisms to facilitate efficient knowledge transfer"
  - [section 3.2] "we utilize curriculum learning and present the training data in order of increasing difficulty to the student"
  - [corpus] Weak evidence - only mentions curriculum learning exists in NLP but not this specific implementation
- Break condition: If similarity measure fails to capture true difficulty, or if temperature schedule doesn't properly progress from easy to hard

### Mechanism 3
- Claim: Knowledge distillation from ChatGPT to FLAN-T5 preserves teacher performance while reducing computational requirements
- Mechanism: ChatGPT generates JSON-formatted answers to layout-encoded prompts, which are used as training targets for FLAN-T5 through supervised fine-tuning with LoRA adapters
- Core assumption: Smaller models can learn to replicate teacher reasoning capabilities when provided with well-formatted training signals
- Evidence anchors:
  - [abstract] "distill document understanding knowledge from the proprietary LLM ChatGPT into FLAN-T5"
  - [section 4.2] "document understanding knowledge was successfully distilled into the student models"
  - [corpus] Moderate evidence - mentions similar approaches in DocKD paper
- Break condition: If teacher signal quality is insufficient, or if LoRA adapters cannot capture necessary knowledge

## Foundational Learning

- Concept: Document layout understanding
  - Why needed here: The approach relies on converting spatial layout information into textual format for LLM processing
  - Quick check question: Can you explain how bounding box coordinates are converted to text formatting?

- Concept: Knowledge distillation principles
  - Why needed here: The core approach involves transferring knowledge from large teacher models to smaller student models
  - Quick check question: What is the difference between knowledge distillation and traditional supervised learning?

- Concept: Curriculum learning strategies
  - Why needed here: The training approach uses difficulty-based data ordering to improve convergence
  - Quick check question: How does presenting simpler examples first benefit model training?

## Architecture Onboarding

- Component map:
  OCR pipeline → SpatialFormat verbalizer → Prompt template → ChatGPT → JSON labels → FLAN-T5 + LoRA → Curriculum sampler

- Critical path: OCR → Layout verbalization → Prompting → Label generation → Student training → Evaluation
  - Any failure in spatial formatting breaks the entire pipeline

- Design tradeoffs:
  - Pure text approach vs multimodal models (computational efficiency vs potential accuracy)
  - Curriculum learning complexity vs simple training
  - LoRA parameter efficiency vs full fine-tuning capability

- Failure signatures:
  - Poor spatial formatting → layout information lost
  - Teacher model bias → student inherits incorrect reasoning
  - Curriculum sampling issues → training stuck on easy/hard examples

- First 3 experiments:
  1. Test LAPDoc SpatialFormat verbalization on sample documents to verify layout preservation
  2. Validate teacher model label quality on small subset before full distillation
  3. Run baseline FLAN-T5 training without curriculum to establish performance floor

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of FLAN-T5XL and FLAN-T5XXL compare to the smaller models in knowledge distillation for document understanding tasks?
- Basis in paper: [explicit] The paper mentions that evaluation of larger variants XL and XXL has been omitted due to time constraints and suggests this as a starting point for future work.
- Why unresolved: The paper only evaluates three sizes of FLAN-T5 models (small, base, and large) due to computational limitations, leaving the performance of larger models unexplored.
- What evidence would resolve it: Experimental results comparing the performance of FLAN-T5XL and FLAN-T5XXL against the smaller models on the same document understanding tasks.

### Open Question 2
- Question: What impact do different document representations have on the effectiveness of knowledge distillation for document understanding?
- Basis in paper: [explicit] The paper suggests investigating other document representations and their utility for knowledge distillation as a point of interest for future work.
- Why unresolved: The study uses LAPDoc SpatialFormat verbalization strategy for document representation, but does not explore alternative representations that might improve distillation outcomes.
- What evidence would resolve it: Comparative experiments using different document representation methods and their corresponding knowledge distillation results.

### Open Question 3
- Question: How does the choice of temperature parameters in curriculum learning affect the performance of different sized FLAN-T5 models?
- Basis in paper: [explicit] The paper explores four different choices of temperature parameters (A, B, C, D) and compares them to a baseline, showing that larger models benefit more from pronounced learning curricula.
- Why unresolved: While the paper evaluates specific temperature parameters, it does not provide a comprehensive analysis of how different parameter choices impact model performance across various tasks and model sizes.
- What evidence would resolve it: Detailed experimental results analyzing the effects of a wider range of temperature parameters on model performance, potentially including ablation studies and sensitivity analyses.

## Limitations

- The spatial layout verbalization through spacing/newline formatting has inherent limitations in representing complex document structures
- Knowledge quality is fundamentally bounded by ChatGPT 3.5's capabilities and potential biases
- Pure text approach cannot capture visual features that multimodal models naturally process

## Confidence

- **High Confidence**: The core distillation methodology and LAPDoc SpatialFormat verbalization approach are well-defined and reproducible
- **Medium Confidence**: The curriculum learning temperature sampling mechanism is conceptually sound but exact parameter values are not fully specified
- **Low Confidence**: The claim that FLAN-T5LARGE surpasses ChatGPT on TabFact requires careful validation

## Next Checks

1. Implement a controlled experiment comparing LAPDoc SpatialFormat verbalization against baseline text-only approaches on a subset of documents to quantify the layout preservation benefits
2. Conduct ablation studies on the curriculum learning variants to determine the optimal temperature schedule and validate that difficulty-based sampling provides measurable improvements over random sampling
3. Perform cross-dataset evaluation to test whether the TabFact performance gains observed with FLAN-T5LARGE generalize to other datasets or represent a dataset-specific phenomenon