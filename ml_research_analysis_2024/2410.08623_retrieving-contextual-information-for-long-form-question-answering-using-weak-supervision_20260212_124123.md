---
ver: rpa2
title: Retrieving Contextual Information for Long-Form Question Answering using Weak
  Supervision
arxiv_id: '2410.08623'
source_url: https://arxiv.org/abs/2410.08623
tags:
- question
- retrieval
- information
- answers
- passages
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work addresses the challenge of retrieving contextual information
  for long-form question answering (LFQA), where existing retrieval systems optimized
  for direct answers often miss relevant context needed for comprehensive responses.
  The authors propose a method to automatically identify silver passages using both
  long-form answers (LFAs) and direct answers (DAs) as weak supervision signals.
---

# Retrieving Contextual Information for Long-Form Question Answering using Weak Supervision

## Quick Facts
- arXiv ID: 2410.08623
- Source URL: https://arxiv.org/abs/2410.08623
- Reference count: 36
- Primary result: Achieved state-of-the-art DR metric of 35.5 on ASQA dataset by retrieving contextual information using weak supervision from long-form and direct answers

## Executive Summary
This paper addresses the challenge of retrieving contextual information for long-form question answering (LFQA), where existing retrieval systems optimized for direct answers often miss relevant context needed for comprehensive responses. The authors propose a method to automatically identify silver passages using both long-form answers (LFAs) and direct answers (DAs) as weak supervision signals. They experiment with different matching techniques (lexical, semantic, LLM perplexity) and introduce a combined SILVER approach that matches against both LFAs and DAs. The method significantly improves retrieval of contextual information, increasing Wikipage recall by 14.7% and groundedness of generated answers by 12.5% on the ASQA dataset.

## Method Summary
The approach uses weak supervision from long-form answers and direct answers to identify relevant contextual passages. First, DPR retrieves top-100 passages for each question. Then, silver passages are identified using three matching techniques: lexical matching (token recall), semantic similarity, and LLM perplexity. These silver passages are used to train BERT-based re-rankers that improve the top-5 retrieved passages before they are passed to an LLM (Vicuna) for long-form answer generation. The SILVER approach combines matching against both long-form answers and direct answers to optimize retrieval for contextual information.

## Key Results
- SILVER re-ranker achieved DR metric of 35.5 on ASQA dataset, state-of-the-art performance
- Wikipage recall improved by 14.7% compared to baseline DPR retrieval
- Groundedness of generated answers increased by 12.5%, reducing hallucination
- Lexical matching (DR 35.0) outperformed more complex semantic similarity (DR 34.1) and LLM perplexity (DR 34.0) approaches
- Retrieval improvements had greater impact than increasing model size from 13B to 33B parameters

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Lexical matching with long-form answers provides sufficient signal for identifying relevant contextual passages
- Mechanism: The method computes token recall between candidate passages and long-form answers, capturing direct textual overlap that indicates contextual relevance
- Core assumption: Relevant contextual information is likely to contain substantial lexical overlap with the long-form answer
- Evidence anchors:
  - [abstract] "Lexical matching shows strong performance. An interesting finding is that simple lexical matching achieves better performance (DR of 35.0) compared to the more complex and computationally expensive variants based on semantic similarity (DR of 34.1) or LLM perplexity (DR of34.0)."
  - [section] "We initially evaluated (i) token recall, (ii) Jaccard similarity between token sets, and (iii) ROUGE-L (Lin, 2004), and found that plain token recall works best."
- Break condition: When contextual information requires rephrasing or paraphrasing that doesn't preserve exact lexical overlap

### Mechanism 2
- Claim: Combining weak supervision from both long-form answers and direct answers optimizes retrieval for contextual information
- Mechanism: The SILVER approach first ensures all direct answers are matched, then fills remaining slots with passages that have highest lexical matching score with the long-form answer
- Core assumption: Direct answers and contextual information can be retrieved jointly without interference, and the long-form answer contains sufficient contextual cues
- Evidence anchors:
  - [abstract] "Our proposed approach matches against a combination of both LFAs and DAs, and we also compare these against matching only with DAs."
  - [section] "SILVER : Matching with LFA & DA. Finally, we consider the combination of matching both the LFA and the set of DAs, for selecting k silver passages."
- Break condition: When long-form answers and direct answers require fundamentally different retrieval strategies

### Mechanism 3
- Claim: Re-ranking based on silver passages improves end-to-end LFQA performance more than scaling model size
- Mechanism: The approach fine-tunes BERT-based re-rankers on silver passages derived from LFAs and DAs, then uses these re-rankers to enhance DPR retrieval results before passing to LLM
- Core assumption: Improving the quality and relevance of retrieved passages has greater impact on final answer quality than increasing LLM parameters
- Evidence anchors:
  - [abstract] "The authors also show that lexical matching performs surprisingly well compared to more complex methods, and that retrieval improvements have a greater impact than increasing model size."
  - [section] "We further investigate the effect of the LLM size, to verify that our improvements still hold for smaller/larger LLMs, using the 7B and 33B versions of Vicuna. Results are shown in Table 4. In general, the DR metric decreases (SILVER : 33.3; DPR : 32.5) using the 7B version. Further, we found that the effect of scaling up the LLM to 33B parameters is negligible compared to enhancements on the retrieval side, observing very similar results as for the 13B version (DR metric for SILVER : 35.6; DPR: 34.3)."
- Break condition: When the LLM's inherent generation capabilities become the primary bottleneck rather than information retrieval quality

## Foundational Learning

- Concept: Weak supervision in information retrieval
  - Why needed here: The approach relies on automatically derived silver passages instead of human-annotated relevant passages, requiring understanding of how to generate and use noisy training signals
  - Quick check question: What are the risks of using automatically generated training data for retrieval models, and how does the SILVER approach mitigate them?

- Concept: Retrieval-augmented generation (RAG) pipeline architecture
  - Why needed here: The method modifies the standard RAG pipeline by adding specialized re-ranking, requiring understanding of how each component (retrieval, re-ranking, generation) interacts
  - Quick check question: How does the addition of re-ranking between DPR retrieval and LLM generation affect the overall system latency and quality?

- Concept: Evaluation metrics for long-form question answering
  - Why needed here: The approach uses specific metrics like Wikipage recall and groundedness that are tailored to LFQA, requiring understanding of what these metrics measure and why they matter
  - Quick check question: Why is Wikipage recall used as a proxy for contextual information recall, and what are its limitations?

## Architecture Onboarding

- Component map: Question → DPR Retriever (top-100) → Silver Passage Identifier (top-k) → BERT Re-ranker (top-5) → Vicuna LLM (long-form answer generation)
- Critical path: The flow from DPR retrieval through silver passage identification to re-ranking and generation is the most critical sequence for system performance
- Design tradeoffs: The approach trades computational complexity (additional re-ranking step) for improved retrieval quality and reduced hallucination
- Failure signatures: Poor performance when silver passages miss critical contextual information, or when lexical matching fails to capture paraphrased relevant content
- First 3 experiments:
  1. Compare SILVER re-ranker performance against DPR alone on ASQA Wikipage recall
  2. Test lexical matching variant against semantic similarity and LLM perplexity variants on the same dataset
  3. Evaluate groundedness improvement by measuring fraction of answer tokens present in retrieved passages before and after applying SILVER re-rankers

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do the proposed SILVER re-rankers perform when applied to non-factoid LFQA datasets with more open-ended, subjective questions?
- Basis in paper: [inferred] The paper focuses on factoid questions with definitive answers, but LFQA also includes open-ended questions requiring more nuanced responses.
- Why unresolved: The experimental evaluation only covers factoid questions from ASQA and CONVMIX datasets, leaving the performance on open-ended LFQA questions unexplored.
- What evidence would resolve it: Empirical evaluation on LFQA datasets with open-ended questions (e.g., ELI5) comparing SILVER re-rankers against baseline methods, measuring both retrieval performance and quality of generated answers.

### Open Question 2
- Question: How does the performance of SILVER re-rankers scale with different document collections beyond Wikipedia, such as web-scale corpora or domain-specific knowledge bases?
- Basis in paper: [inferred] The paper uses Wikipedia as the document collection, but real-world applications may require different or larger document collections.
- Why unresolved: The paper does not explore how the method generalizes to other document collections with different characteristics (size, quality, domain specificity).
- What evidence would resolve it: Comparative experiments evaluating SILVER re-rankers on multiple document collections of varying sizes and domains, measuring retrieval accuracy and answer quality across these collections.

### Open Question 3
- Question: What is the optimal balance between lexical and semantic matching techniques for different types of questions and document collections?
- Basis in paper: [explicit] The paper finds that simple lexical matching performs surprisingly well compared to more complex methods, but does not explore optimal combinations or adaptive approaches.
- Why unresolved: The paper only compares different matching techniques in isolation, without exploring hybrid approaches or determining when each technique is most effective.
- What evidence would resolve it: Systematic experiments varying the weighting between lexical and semantic matching, and testing adaptive approaches that select matching techniques based on question type or document characteristics.

## Limitations
- Lexical matching may miss contextually relevant passages that use different terminology or paraphrasing
- Weak supervision signal quality depends entirely on automatic matching quality, which may introduce noise
- The method hasn't been validated on open-ended or subjective LFQA questions beyond factoid queries

## Confidence

- **High Confidence**: The core finding that lexical matching outperforms more complex semantic methods (DR of 35.0 vs 34.1) - directly supported by experimental results in Table 2 and corroborated by ablation studies
- **Medium Confidence**: The claim that retrieval improvements have greater impact than model size scaling - supported by Vicuna 7B vs 33B comparison, though effect size is small and limited to single dataset
- **Low Confidence**: The mechanism assumption that direct answers and contextual information can be retrieved jointly without interference - paper doesn't provide evidence these signals don't conflict, and combined SILVER approach's superiority isn't conclusively demonstrated

## Next Checks

1. **Generalization Test**: Evaluate the SILVER approach on datasets with significantly different domain distributions (e.g., scientific literature or technical documentation) to verify that lexical matching remains effective when vocabulary and context types differ substantially from ASQA.

2. **Ablation on Weak Supervision Quality**: Systematically vary the quality threshold for silver passage identification (e.g., requiring higher token overlap or semantic similarity) and measure the impact on both retrieval performance and end-to-end QA metrics to understand the sensitivity to weak supervision noise.

3. **Comparison with Alternative Re-ranking**: Replace the BERT re-ranker with a cross-encoder architecture trained on human-annotated relevant passages (where available) to determine whether performance gains stem from weak supervision approach itself or simply from adding a re-ranking stage.