---
ver: rpa2
title: 'SEE-DPO: Self Entropy Enhanced Direct Preference Optimization'
arxiv_id: '2411.04712'
source_url: https://arxiv.org/abs/2411.04712
tags:
- reward
- diffusion
- arxiv
- learning
- image
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses reward hacking and overfitting issues in Direct
  Preference Optimization (DPO)-based methods for diffusion models. The authors introduce
  a self-entropy regularization term into the standard KL-regularized RLHF objective,
  which flattens the reference distribution and encourages broader exploration.
---

# SEE-DPO: Self Entropy Enhanced Direct Preference Optimization

## Quick Facts
- arXiv ID: 2411.04712
- Source URL: https://arxiv.org/abs/2411.04712
- Authors: Shivanshu Shekhar; Shreyas Singh; Tong Zhang
- Reference count: 17
- Key outcome: Self-entropy regularization effectively mitigates reward hacking and improves image diversity and quality across the latent space, achieving state-of-the-art results on multiple image quality metrics.

## Executive Summary
This paper addresses reward hacking and overfitting issues in Direct Preference Optimization (DPO)-based methods for diffusion models by introducing a self-entropy regularization term into the standard KL-regularized RLHF objective. The authors demonstrate that this regularization flattens the reference distribution and encourages broader exploration, effectively mitigating reward hacking while improving image diversity and quality across the latent space. Extensive experiments show that SEE-DPO, SEE-DiffusionDPO, and SEE-SPO outperform their counterparts on multiple image quality metrics including PickScore (+1.8%), HPS (+7.9%), and ImageReward (+206.3% for SEE-SPO), while also achieving superior diversity scores and enhanced visual quality as validated by user studies.

## Method Summary
The authors introduce a self-entropy regularization term into the KL-regularized RLHF objective, which modifies the reference distribution by raising it to a power of (1+γ). This modification flattens the reference distribution, encouraging the model to explore lower-probability regions and reducing overfitting to out-of-distribution samples. The method is applied to three existing DPO-based approaches for diffusion models (D3PO, Diffusion-DPO, and SPO) to create their enhanced versions (SEE-D3PO, SEE-DiffusionDPO, and SEE-SPO). The training involves online iterative preference generation using time-conditional reward models, with hyperparameters including γ (self-entropy regularization weight) and β (KL regularization weight).

## Key Results
- SEE-DPO achieves state-of-the-art performance on image quality metrics: PickScore (+1.8%), HPS (+7.9%), and ImageReward (+206.3% for SEE-SPO)
- All SEE models outperform their respective counterparts across all six diversity metrics (RMSE, PSNR, SSIM, entropy energy scores E1, E2)
- User studies validate enhanced visual quality across four aspects: Prompt Alignment, Visual Appeal, General Preference, and Diversity
- The method successfully mitigates reward hacking and improves image diversity while maintaining or improving image specificity

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Self-entropy regularization flattens the reference distribution, which encourages broader exploration and reduces overfitting.
- Mechanism: Adding the self-entropy term to the KL-regularized RLHF objective modifies the reference distribution such that higher γ values flatten it. This flattening increases the probability mass assigned to lower-probability regions, allowing the model to explore more diverse solutions.
- Core assumption: The reference distribution in the KL divergence term can be modified by the self-entropy regularization without changing the fundamental optimization structure.
- Evidence anchors:
  - [abstract]: "This regularization effectively mitigates reward hacking and improves image diversity and quality across the latent space."
  - [section 4]: "We can see that the only effect of adding our self entropy term is on π_ref, particularly we can simplify Eq. 16 using the usual DPO derivation Rafailov et al. (2024b) to show that adding the self entropy is equivalent to raising π_ref by 1+γ."
  - [corpus]: Weak. Related works focus on entropy control in RLHF but don't explicitly address the flattening of the reference distribution.

### Mechanism 2
- Claim: The self-entropy term mitigates reward hacking by reducing premature overfitting to out-of-distribution samples.
- Mechanism: During prolonged training, diffusion models can shift significantly from the original distribution, leading to artificially high rewards and mode collapse. The self-entropy regularization discourages the model from collapsing to a narrow set of modes by encouraging exploration of the full latent space.
- Core assumption: Reward hacking occurs primarily due to the model's tendency to overfit to high-reward but out-of-distribution samples during prolonged training.
- Evidence anchors:
  - [abstract]: "However, DPO-based methods such as SPO, Diffusion-DPO, and D3PO are highly susceptible to overfitting and reward hacking, especially when the generative model is optimized to fit out-of-distribution during prolonged training."
  - [section 4]: "When combined with the prior state-of-the-art methods D3PO, Diffusion-DPO and SPO we are able to obtain new state-of-the-art results on various image quality metrics."
  - [corpus]: Moderate. Related works like "Entropy Controllable Direct Preference Optimization" discuss entropy control but focus on controllable entropy rather than overfitting mitigation.

### Mechanism 3
- Claim: The self-entropy regularization improves image quality and diversity by enabling the model to explore lower-probability regions of the reference distribution.
- Mechanism: By flattening the reference distribution, the model can generate images from regions that would otherwise have very low probability under the original reference. This leads to more diverse and higher-quality outputs as measured by multiple metrics including PickScore, HPS, and ImageReward.
- Core assumption: The regions of low probability under the original reference distribution contain valid and high-quality image samples that the model should explore.
- Evidence anchors:
  - [abstract]: "Extensive experiments demonstrate that integrating human feedback with self-entropy regularization can significantly boost image diversity and specificity, achieving state-of-the-art results on key image generation metrics."
  - [section 5]: "In terms of image diversity metrics, all our models outperform their respective counterparts across all six metrics."
  - [corpus]: Moderate. Related works like "Boost Your Human Image Generation Model via Direct Preference Optimization" focus on improving image quality but don't specifically address diversity through entropy regularization.

## Foundational Learning

- Concept: Reinforcement Learning from Human Feedback (RLHF)
  - Why needed here: The paper builds on RLHF as the foundation for aligning diffusion models with human preferences, extending it with self-entropy regularization.
  - Quick check question: How does RLHF differ from standard reinforcement learning in terms of reward signal and policy optimization?

- Concept: Direct Preference Optimization (DPO)
  - Why needed here: DPO is the base algorithm that the paper extends with self-entropy regularization, and understanding its mechanics is crucial for implementing SEE-DPO.
  - Quick check question: What is the key mathematical insight that allows DPO to bypass reward modeling and directly optimize the policy?

- Concept: Diffusion models and the denoising process
  - Why needed here: The paper applies the self-entropy regularization to diffusion models, so understanding the forward and reverse diffusion processes is essential.
  - Quick check question: How does the forward diffusion process corrupt data, and how does the reverse process learn to denoise it?

## Architecture Onboarding

- Component map:
  Base diffusion model (e.g., Stable Diffusion v1.5) -> Preference dataset (winning/losing image pairs) -> Self-entropy regularization module -> Training loop with online preference generation -> Evaluation metrics (PickScore, HPS, ImageReward, diversity metrics)

- Critical path:
  1. Load base diffusion model and preference dataset
  2. Initialize self-entropy regularization parameter γ
  3. For each training iteration:
     - Generate images using current model
     - Score images using preference model
     - Update model parameters using SEE-DPO objective
     - Append new preference data to dataset
  4. Evaluate model on held-out prompts

- Design tradeoffs:
  - γ value: Higher γ increases exploration but may reduce convergence speed
  - β value: Controls KL regularization strength, must be balanced with γ
  - Dataset size: Larger datasets may reduce overfitting but increase training time
  - Preference model quality: Better preference models lead to more effective training

- Failure signatures:
  - High variance in training rewards: May indicate insufficient regularization or poor preference model
  - Consistently low diversity metrics: Could suggest γ is too low or preference model is biased
  - Mode collapse: Model generates very similar images, likely γ is too low
  - Poor convergence: May indicate γ is too high, causing excessive exploration

- First 3 experiments:
  1. Implement SEE-DPO with γ=0 (baseline) and compare to original DPO implementation on a small dataset
  2. Vary γ from 0 to 5 and measure impact on diversity metrics and convergence speed
  3. Compare SEE-DPO performance across different diffusion model architectures (e.g., Stable Diffusion vs. custom model)

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the self-entropy regularization term affect the long-term stability and performance of diffusion models across diverse domains beyond text-to-image generation?
- Basis in paper: [explicit] The paper introduces self-entropy regularization to mitigate reward hacking and improve image diversity, but focuses primarily on text-to-image diffusion models.
- Why unresolved: The experiments are limited to text-to-image tasks, and it's unclear whether the regularization's benefits generalize to other domains like video generation, 3D modeling, or non-visual tasks.
- What evidence would resolve it: Testing the method on diverse generative tasks and comparing performance and stability metrics against baselines.

### Open Question 2
- Question: What is the optimal range for the hyperparameter γ that balances exploration and exploitation without causing overfitting or underfitting?
- Basis in paper: [explicit] The paper discusses the role of γ in flattening the reference distribution and enabling exploration, but notes that negative γ values lead to poor performance and that optimal values vary across methods.
- Why unresolved: The paper only provides a few γ values for specific methods, and the relationship between γ, training stability, and performance is not fully explored.
- What evidence would resolve it: Systematic ablation studies varying γ across a wide range and analyzing its impact on training dynamics and final model quality.

### Open Question 3
- Question: How do existing diversity metrics like RMSE and PSNR compare to human judgment in evaluating the true diversity of generated images?
- Basis in paper: [explicit] The paper notes a discrepancy between diversity metrics and user study results, suggesting that current metrics may not align with human perception.
- Why unresolved: The paper relies on standard diversity metrics and user studies but does not propose or validate new metrics that better capture human notions of diversity.
- What evidence would resolve it: Developing and validating new diversity metrics that correlate strongly with human preferences and outperform traditional metrics in user studies.

## Limitations
- The evidence for reward hacking mitigation relies heavily on downstream performance metrics rather than direct measurement of overfitting behavior.
- The online preference generation mechanism, while described, lacks detailed implementation specifications that could affect reproducibility.
- Claims about self-entropy regularization flattening the reference distribution are well-supported by theoretical derivations but lack empirical validation of the distribution flattening effect itself.

## Confidence
- **High Confidence**: Claims about SEE-DPO achieving state-of-the-art performance on multiple image quality metrics (PickScore, HPS, ImageReward) are supported by extensive quantitative comparisons across multiple experiments.
- **Medium Confidence**: Claims about self-entropy regularization improving diversity and reducing overfitting are supported by diversity metric improvements and ablation studies, but the causal mechanism could benefit from more direct analysis.
- **Medium Confidence**: Claims about the effectiveness of online iterative training are supported by performance improvements over offline training, but the impact of specific online preference generation strategies is not fully explored.

## Next Checks
1. **Distribution Analysis Validation**: Visualize and quantitatively measure the reference distribution flattening effect by plotting the probability density functions before and after self-entropy regularization is applied, confirming that γ > 0 actually flattens the distribution as claimed.

2. **Overfitting Behavior Analysis**: Track and visualize the KL divergence between the model distribution and reference distribution during training for different γ values to directly observe how self-entropy regularization affects the model's tendency to drift from the reference distribution.

3. **Ablation on Online Preference Generation**: Conduct controlled experiments comparing SEE-DPO with and without online preference generation (using fixed preference datasets) to isolate the contribution of online data collection versus self-entropy regularization to the observed performance improvements.