---
ver: rpa2
title: 'PLEIADES: Building Temporal Kernels with Orthogonal Polynomials'
arxiv_id: '2405.12179'
source_url: https://arxiv.org/abs/2405.12179
tags:
- temporal
- network
- kernels
- kernel
- memory
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces PLEIADES, a neural network architecture that
  uses orthogonal polynomial basis functions to parameterize temporal convolutional
  kernels for event-based vision tasks. The key innovation is replacing standard temporal
  kernels with weighted sums of Jacobi polynomials, enabling structured temporal representations
  that allow for flexible resampling of input data rates without additional fine-tuning.
---

# PLEIADES: Building Temporal Kernels with Orthogonal Polynomials

## Quick Facts
- arXiv ID: 2405.12179
- Source URL: https://arxiv.org/abs/2405.12179
- Authors: Yan Ru Pei; Olivier Coenen
- Reference count: 21
- One-line primary result: State-of-the-art performance on event-based vision tasks with significantly smaller memory and compute costs

## Executive Summary
PLEIADES introduces a novel neural network architecture for event-based vision tasks that uses orthogonal polynomial basis functions to parameterize temporal convolutional kernels. The key innovation replaces standard temporal kernels with weighted sums of Jacobi polynomials, enabling structured temporal representations that allow for flexible resampling of input data rates without additional fine-tuning. This approach achieves state-of-the-art results across three event-based vision benchmarks while maintaining significantly smaller memory and compute costs compared to baseline approaches.

## Method Summary
PLEIADES uses orthogonal Jacobi polynomials to parameterize temporal kernels in a spatiotemporal convolutional network. The architecture converts event-based data into 4D tensors and applies 1D temporal convolutions with polynomially-parameterized kernels followed by 2D spatial convolutions. The polynomial basis provides a compressed representation of temporal dynamics that enables analytical integration over arbitrary time bins, allowing the same trained network to handle different sampling rates. The method employs Jacobi polynomials with parameters (α, β) = (-0.25, -0.25) and degree up to 4, trained using AdamW optimizer with cosine decay learning rate schedule.

## Key Results
- DVS128 hand gesture recognition: 99.59% accuracy with 192K parameters
- AIS 2024 eye tracking challenge: 99.58% test accuracy with 277K parameters
- Prophesee 1 Megapixel Automotive Detection Dataset: 0.556 mAP with 576K parameters
- Significant improvements over state-of-the-art methods with smaller memory and compute costs

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Orthogonal polynomial basis functions provide a continuous, structured parameterization of temporal kernels that enables efficient resampling of event data rates.
- Mechanism: The weighted sum of Jacobi polynomials creates a continuous kernel function that can be analytically integrated over arbitrary time bins, allowing the same trained network to handle different sampling rates without fine-tuning.
- Core assumption: Event data accumulated into bins remains invariant under resampling if properly rescaled, maintaining the integral contribution of events.
- Evidence anchors:
  - [abstract] "By virtue of using structured temporal kernels and event-based data, we have the freedom to vary the sample rate of the data along with the discretization step-size of the network without additional finetuning."
  - [section] "Under this discretization scheme, it is very easy to resample the continuous temporal kernels (either downsampling or upsampling), to interface with data sampled at arbitrary rates, i.e. arbitrary bin sizes for event-based data."

### Mechanism 2
- Claim: Polynomial parameterization reduces memory and computational costs compared to explicit temporal kernels while maintaining or improving performance.
- Mechanism: The orthogonal polynomial basis provides a compressed representation where the number of trainable parameters (coefficients) is much smaller than the number of time bins, reducing memory requirements while maintaining expressivity through the structured basis.
- Core assumption: The temporal dynamics can be adequately represented by a relatively small number of orthogonal polynomial basis functions.
- Evidence anchors:
  - [abstract] "state-of-the-art results on all three by large margins with significantly smaller memory and compute costs."
  - [section] "PLEIADES performs better with a smaller number of parameters (due to polynomial compression)."

### Mechanism 3
- Claim: The structured temporal kernels provide inductive biases that improve training stability and guide optimization to better minima.
- Mechanism: The Jacobi polynomial basis is associated with Sturm-Liouville differential equations, which injects physical inductive biases into the network, making training more stable and guiding it toward better optima.
- Core assumption: The underlying temporal dynamics have structure that can be captured by the differential equation associated with the Jacobi polynomials.
- Evidence anchors:
  - [section] "since a Jacobi polynomial basis is associated with an underlying Sturm-Louville differential equation, this injects physical inductive biases into our network, making the training more stable and guiding it to a better optimum."

## Foundational Learning

- Concept: Event-based vision and asynchronous data processing
  - Why needed here: PLEIADES is specifically designed to interface with event-based data from event cameras, which produce asynchronous streams of events rather than conventional frame-based data.
  - Quick check question: How does event-based data differ from conventional frame-based video data, and why does this create challenges for traditional neural network architectures?

- Concept: Orthogonal polynomials and function approximation
  - Why needed here: The core innovation uses orthogonal polynomials (Jacobi polynomials) to parameterize temporal kernels, requiring understanding of how orthogonal polynomial bases can approximate continuous functions.
  - Quick check question: Why are orthogonal polynomials particularly suitable for parameterizing continuous functions compared to other basis functions?

- Concept: Temporal convolutional networks and receptive fields
  - Why needed here: PLEIADES builds on temporal convolutional networks but extends them with long temporal kernels, requiring understanding of how temporal convolutions work and how receptive fields affect temporal modeling.
  - Quick check question: What is the trade-off between temporal kernel length and training stability in temporal convolutional networks?

## Architecture Onboarding

- Component map: Event data → binning → temporal convolution with polynomial kernels → spatial convolution → detection/classification head → output predictions
- Critical path: Event data → binning → temporal convolution with polynomial kernels → spatial convolution → detection/classification head → output predictions
- Design tradeoffs:
  - Memory vs. temporal context: Longer temporal kernels capture more context but require more memory for buffering
  - Polynomial degree vs. expressivity: Higher degrees allow more complex temporal patterns but risk overfitting and increased computation
  - Fixed basis vs. learnable filters: Polynomial basis provides structure and compression but may limit flexibility compared to fully learnable filters
- Failure signatures:
  - Degraded accuracy when resampling data rates if polynomial basis cannot capture temporal dynamics
  - Training instability if polynomial degree is too high or if the basis is poorly chosen for the task
  - Memory bottlenecks when using long temporal kernels on high-resolution data
- First 3 experiments:
  1. Implement a single temporal layer with polynomial kernels on DVS128 gesture data and verify that it can handle different bin sizes without retraining
  2. Compare accuracy and parameter count between polynomial kernels and standard temporal kernels on a simple classification task
  3. Test the effect of polynomial degree on accuracy and training stability using DVS128 data with varying latency requirements

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the theoretical justification for why α=β=-0.25 yields superior training dynamics compared to other Jacobi parameter choices?
- Basis in paper: [explicit] The authors note that while all choices of (α, β) yield polynomial bases spanning the same space, α=β=-0.25 balances benefits between Legendre (α=β=0) and Chebyshev (α=β=-0.5) polynomials, but state that "a full mathematical justification... is beyond the scope of this work."
- Why unresolved: The paper provides empirical evidence that this parameter choice performs well but does not offer a rigorous theoretical explanation for why these particular values optimize training dynamics.
- What evidence would resolve it: A theoretical analysis connecting Jacobi polynomial parameters to neural tangent kernel behavior, optimization landscape properties, or mean-field theory predictions that explains why α=β=-0.25 provides optimal inductive biases for stable training.

### Open Question 2
- Question: How would integrating online basis projections (running polynomial coefficients) affect the memory-accuracy trade-off compared to the current finite-window buffering approach?
- Basis in paper: [explicit] The authors discuss this as a limitation in Section 6, noting that "the polynomial structure of our temporal kernels offers a principled workaround" by maintaining compact online basis projections instead of buffering all k feature maps.
- Why unresolved: The paper only mentions this as a future direction without implementing or evaluating it, leaving the practical trade-offs between memory savings and potential accuracy degradation unexplored.
- What evidence would resolve it: Experimental results comparing the current finite-window approach with an implementation using recursive online basis projections, measuring memory usage, latency, and accuracy across different kernel lengths and input resolutions.

### Open Question 3
- Question: What is the impact of polynomial degree selection on model performance in regimes with limited temporal context?
- Basis in paper: [explicit] The authors show in Appendix B.1.2 that "having too high of a polynomial degree actually degraded performance" in low-latency regimes, with a table showing accuracy dropping from 73.2% at degree 4 to 52.5% at degree 10 for 200ms latency.
- Why unresolved: While the paper demonstrates that degree 4 is optimal for their specific setup, it doesn't provide a systematic framework for selecting the optimal degree based on task requirements, available temporal context, or computational constraints.
- What evidence would resolve it: A comprehensive study mapping polynomial degree to performance across different temporal window sizes, latency requirements, and task complexities, potentially leading to a principled method for degree selection based on these factors.

### Open Question 4
- Question: How would activation-sparsity regularization affect energy consumption and latency in event-based vision tasks?
- Basis in paper: [explicit] The authors mention this as a potential avenue for further gains in Section 7, noting that "event streams are naturally sparse" and suggesting that "injecting intermediate loss terms that penalize dense activations could cut energy consumption and latency."
- Why unresolved: The paper only proposes this idea without implementing it or measuring the actual energy and latency benefits versus accuracy trade-offs.
- What evidence would resolve it: Experiments comparing the baseline PLEIADES with variants trained using sparsity-inducing regularization (e.g., L1 penalties on activations, structured sparsity constraints), measuring actual energy consumption on neuromorphic hardware, inference latency, and accuracy degradation.

## Limitations

- Limited theoretical justification for Jacobi polynomial parameter selection (α=β=-0.25)
- Unclear implementation details for optimal tensor contraction ordering
- Potential sensitivity to polynomial degree selection in different temporal context regimes

## Confidence

- High Confidence (Mechanism 1 - Data Rate Resampling): The theoretical foundation for using orthogonal polynomial basis functions to enable continuous temporal kernels that can be analytically integrated over arbitrary time bins is well-established.
- Medium Confidence (Mechanism 2 - Memory Efficiency): While the compression argument is logical and supported by parameter count comparisons, the actual memory and computational savings depend heavily on implementation details.
- Medium Confidence (Mechanism 3 - Training Stability): The claim about improved training stability through physical inductive biases is supported by the mathematical properties of Jacobi polynomials, but empirical evidence is limited.

## Next Checks

1. Systematically vary the timebin sizes across multiple orders of magnitude (e.g., 1 ms, 10 ms, 100 ms) for the DVS128 gesture dataset and measure accuracy degradation.
2. Conduct an ablation study varying the polynomial degree from 2 to 10 on the DVS128 dataset while keeping all other hyperparameters constant.
3. Apply the trained PLEIADES model from DVS128 gesture recognition to a different event-based task (such as N-Cars or N-Caltech101 datasets) without fine-tuning.