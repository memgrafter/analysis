---
ver: rpa2
title: 'No Filter: Cultural and Socioeconomic Diversity in Contrastive Vision-Language
  Models'
arxiv_id: '2405.13777'
source_url: https://arxiv.org/abs/2405.13777
tags:
- data
- cultural
- globe-tl
- diversity
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper studies cultural and socioeconomic diversity in contrastive
  vision-language models (VLMs) like CLIP and SigLIP. The key finding is that filtering
  training data to English-only image-text pairs, a common practice, disproportionately
  harms model performance on culturally diverse data and exacerbates socioeconomic
  disparities.
---

# No Filter: Cultural and Socioeconomic Diversity in Contrastive Vision-Language Models

## Quick Facts
- arXiv ID: 2405.13777
- Source URL: https://arxiv.org/abs/2405.13777
- Reference count: 40
- Primary result: Filtering training data to English-only image-text pairs disproportionately harms model performance on culturally diverse data and exacerbates socioeconomic disparities.

## Executive Summary
This paper investigates cultural and socioeconomic diversity in contrastive vision-language models like CLIP and SigLIP. The key finding is that the common practice of filtering training data to English-only image-text pairs disproportionately harms model performance on culturally diverse data and exacerbates socioeconomic disparities. The authors demonstrate that pretraining on global, multilingual data followed by brief English fine-tuning improves cultural understanding without sacrificing performance on standard Western-centric benchmarks like ImageNet and COCO. The paper introduces few-shot geo-localization as a novel metric for evaluating cultural diversity in VLMs, showing that models trained on global data learn more culturally diverse image features.

## Method Summary
The study uses SigLIP models with Vision Transformer (ViT) for images and Transformer for text, trained on the WebLI dataset with three variants: globe (raw multilingual), en (English-only), and globe-tl (global data with English translation). Models are trained on 10B image-text pairs for ~610k steps. The evaluation uses culturally diverse datasets including Dollar Street, GeoDE, GLDv2, XM3600, MaRVL, plus standard benchmarks ImageNet and COCO. The paper compares zero-shot classification accuracy on culturally diverse datasets, introduces few-shot geo-localization as a novel metric, and evaluates performance on Western-centric benchmarks. The key experimental approach involves pretraining on global data before fine-tuning on English content.

## Key Results
- English-only filtering significantly reduces performance on culturally diverse datasets (Dollar Street, GLDv2, GeoDE, MaRVL) while having minimal impact on Western-centric benchmarks
- Global pretraining followed by brief English fine-tuning improves cultural understanding without sacrificing performance on standard benchmarks
- Few-shot geo-localization shows models trained on global data learn more geographically diverse image features
- Fine-tuning globe-tl on English quickly catches up with English-only models on ImageNet while maintaining superior performance on cultural diversity metrics

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Filtering training data to English-only image-text pairs disproportionately harms model performance on culturally diverse data.
- Mechanism: The model's learned representations are biased toward Western-centric visual patterns and cultural contexts, as the training data lacks global diversity. This leads to poor generalization on non-Western images and household items.
- Core assumption: The model's learned representations directly reflect the distribution of the training data, and removing non-English data reduces the model's exposure to culturally diverse visual concepts.
- Evidence anchors:
  - [abstract] "the common filtering of training data to English image-text pairs disadvantages communities of lower socioeconomic status and negatively impacts cultural understanding."
  - [section 3.1] "Switching to globe-tl data lowers performance for images from North America and Europe but significantly improves performance for other regions such as South America and Africa that are traditionally underrepresented in AI."
  - [corpus] Weak. Related papers discuss cultural evaluation but do not directly anchor the filtering mechanism.
- Break condition: If the model architecture includes explicit mechanisms for domain generalization or adversarial training to mitigate cultural bias, the filtering effect may be reduced.

### Mechanism 2
- Claim: Pretraining on global, unfiltered data before fine-tuning on English content can improve cultural understanding without sacrificing performance on Western-centric benchmarks.
- Mechanism: Global pretraining exposes the model to diverse visual concepts and cultural contexts, creating more robust representations that transfer better across cultures. Brief English fine-tuning then adapts these representations to English without losing the cultural diversity learned during pretraining.
- Core assumption: The model can maintain culturally diverse representations while adapting to English-specific patterns during fine-tuning.
- Evidence anchors:
  - [section 3.2] "globe-tl models achieve competitive results on standard benchmarks while maintaining superior performance on cultural diversity metrics"
  - [section 4] "brief English fine-tuning allows global models to catch up with English-only models on ImageNet while preserving cultural understanding"
- Break condition: If fine-tuning duration is too long or English data dominates the training process, the model may revert to culturally biased representations.

## Foundational Learning
The paper builds on the observation that standard vision-language models suffer from cultural bias due to English-only training data. It extends previous work on cultural evaluation in AI by proposing few-shot geo-localization as a novel metric and demonstrating that multilingual pretraining followed by English fine-tuning can address this bias without performance degradation.

## Architecture Onboarding
The SigLIP architecture uses Vision Transformer (ViT) for image encoding and standard Transformer for text encoding, trained with contrastive loss to align image and text embeddings. The models are evaluated on both culturally diverse datasets and standard benchmarks to assess the impact of training data filtering on cultural understanding.

## Open Questions the Paper Calls Out
- How to balance cultural diversity with task-specific performance requirements
- Whether the proposed approach generalizes to other vision-language tasks beyond classification
- The optimal duration and proportion of English fine-tuning for different model sizes
- How to evaluate cultural bias in more nuanced ways beyond geographic diversity

## Limitations
- The study focuses primarily on classification tasks and may not generalize to other vision-language applications
- The evaluation of cultural diversity relies on specific datasets that may not capture all aspects of cultural variation
- The English fine-tuning approach assumes availability of high-quality English data for adaptation
- The paper does not address potential biases in the global training data itself

## Confidence
Moderate to high. The paper presents clear experimental results showing the negative impact of English-only filtering on cultural understanding, supported by multiple evaluation metrics. The proposed solution of global pretraining followed by English fine-tuning is well-justified and shows consistent improvements across benchmarks.

## Next Checks
- Investigate the impact of training data filtering on other vision-language tasks beyond classification
- Evaluate the proposed approach on additional culturally diverse datasets
- Study the effects of different fine-tuning durations and proportions on cultural understanding
- Explore methods for detecting and mitigating biases in global training data itself