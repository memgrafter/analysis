---
ver: rpa2
title: 'MARVEL: Multidimensional Abstraction and Reasoning through Visual Evaluation
  and Learning'
arxiv_id: '2404.13591'
source_url: https://arxiv.org/abs/2404.13591
tags:
- reasoning
- visual
- mllms
- puzzle
- pattern
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: MARVEL is a new benchmark for testing abstract visual reasoning
  in multimodal large language models (MLLMs). It includes 770 puzzles with 6 reasoning
  patterns, 2 shape types, and 5 task configurations, plus perception questions to
  assess visual understanding.
---

# MARVEL: Multidimensional Abstraction and Reasoning through Visual Evaluation and Learning

## Quick Facts
- **arXiv ID**: 2404.13591
- **Source URL**: https://arxiv.org/abs/2404.13591
- **Reference count**: 25
- **Key outcome**: All tested MLLMs scored near-random on AVR tasks, with a 40% gap versus human performance.

## Executive Summary
MARVEL is a new benchmark designed to comprehensively evaluate abstract visual reasoning (AVR) in multimodal large language models (MLLMs). It includes 770 puzzles with six reasoning patterns, two shape types, and five task configurations, plus perception questions to assess visual understanding. In experiments, all tested MLLMs (including GPT-4V, Gemini, Claude3, and open-source models) scored near-random on the main reasoning task, with a 40% gap versus human performance. Perception questions revealed that models struggle to count or identify visual details, which undermines their reasoning ability. Few-shot prompting with chain-of-thought had little effect, except on GPT-4V for 3D-geometry. The results highlight a significant limitation in current MLLMs' abstract visual reasoning, rooted in weak visual perception.

## Method Summary
MARVEL evaluates MLLMs using a multidimensional benchmark with 770 puzzles organized into six core reasoning patterns, geometric and abstract shapes, and five task configurations. Each puzzle is accompanied by perception questions (testing visual details) and AVR questions (testing abstract reasoning). The evaluation uses zero-shot and few-shot prompting with chain-of-thought demonstrations. Models are tested across nine MLLMs, including closed-source (GPT-4V, Gemini, Claude3) and open-source variants (Qwen-VL, Fuyu, BLIP-2, InstructBLIP, LLaVA-1.5). Human evaluation data provides a performance baseline.

## Key Results
- All tested MLLMs scored near-random on AVR questions, showing a 40% performance gap compared to humans.
- Models struggled with perception questions, with accuracy below 45% on tasks like counting puzzle panels.
- Few-shot prompting with chain-of-thought improved performance only for GPT-4V on 3D-geometry patterns.
- The hierarchical evaluation framework revealed that weak visual perception is a primary bottleneck for AVR performance.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: MARVEL's hierarchical evaluation framework reveals that MLLMs' poor AVR performance is primarily due to weak visual perception rather than reasoning deficits.
- Mechanism: By decomposing each puzzle into perception questions (focusing on visual details) and AVR questions (focusing on abstract reasoning), the framework isolates whether errors stem from misunderstanding the visual input or failing to apply reasoning patterns.
- Core assumption: Visual perception is a necessary foundation for abstract reasoning; without accurate perception, reasoning cannot succeed.
- Evidence anchors:
  - [abstract] "Further analysis of perception questions reveals that MLLMs struggle to comprehend the visual features (near-random performance) and even count the panels in the puzzle (<45%), hindering their ability for abstract reasoning."
  - [section 4] "We enrich MARVEL puzzles with perception questions...designed to test models' perception ability on visual details...to examine if model accuracy is based on perception and reasoning."
- Break condition: If perception questions are answered accurately but AVR questions remain near-random, the mechanism would be invalidated.

### Mechanism 2
- Claim: The multidimensional nature of MARVEL (six patterns, five configurations, geometric and abstract shapes) exposes limitations in MLLMs' ability to generalize across diverse visual reasoning tasks.
- Mechanism: By varying patterns, configurations, and shape types, MARVEL tests whether models can transfer learned reasoning strategies across different contexts, revealing brittleness in their abstract reasoning capabilities.
- Core assumption: Effective abstract reasoning requires generalizable strategies that work across diverse visual contexts.
- Evidence anchors:
  - [abstract] "To evaluate MLLMs' reasoning abilities comprehensively, we introduce MARVEL, a multidimensional AVR benchmark with 770 puzzles composed of six core knowledge patterns, geometric and abstract shapes, and five different task configurations."
  - [section 3.1] "Puzzle panels in MARVEL are organized in the following five task configurations: Sequence, Two-row, Matrix, Group, Reassembling Format."
- Break condition: If models show strong performance across all dimensions, the mechanism would be invalidated.

### Mechanism 3
- Claim: Few-shot prompting with Chain-of-Thought has minimal impact on MLLMs' AVR performance because abstract visual reasoning requires understanding patterns that are not easily captured through demonstrations.
- Mechanism: Demonstrations provide examples of reasoning patterns, but the abstract nature of AVR problems requires the model to understand and apply patterns to novel visual configurations that may differ significantly from demonstrations.
- Core assumption: Abstract visual reasoning requires deep understanding of visual patterns that cannot be easily conveyed through few examples.
- Evidence anchors:
  - [section 6] "Further analysis reveals that the main improvement in GPT-4V's results lies in the 3D-Geometry pattern...However, since most patterns are uniquely implemented on different input shapes and their attributes, the model struggles to learn generalizable patterns from the few-shot demonstrations."
  - [section 5.2] "We explore few-shot prompting with Chain-of-Thought (CoT) (Wei et al., 2022) to guide MLLMs with abstract reasoning patterns."
- Break condition: If few-shot prompting with CoT shows significant improvement across multiple patterns and configurations, the mechanism would be invalidated.

## Foundational Learning

- Concept: Abstract Visual Reasoning (AVR)
  - Why needed here: Understanding AVR is fundamental to grasping why MARVEL is designed as it is and what it tests.
  - Quick check question: What distinguishes AVR from other visual reasoning tasks like visual question answering?

- Concept: Core Knowledge Theory
  - Why needed here: MARVEL's patterns are derived from core knowledge theory, so understanding this theoretical foundation is crucial for understanding the benchmark's design.
  - Quick check question: How does core knowledge theory inform the selection of patterns in MARVEL?

- Concept: Hierarchical Evaluation Framework
  - Why needed here: This framework is central to MARVEL's methodology for diagnosing model capabilities.
  - Quick check question: How does the hierarchical evaluation framework differ from traditional single-question evaluation approaches?

## Architecture Onboarding

- Component map:
  - Data Collection Pipeline: Web scraping → Filtering → Reformatting → Annotation
  - Benchmark Structure: Puzzles (context + choices) → Perception Questions → AVR Questions
  - Evaluation Framework: Zero-shot testing → Few-shot testing with CoT → Performance analysis across dimensions
  - Analysis Tools: Accuracy metrics (instance-based and group-based) → Pattern and configuration breakdowns

- Critical path:
  1. Data collection and filtering to ensure quality puzzles
  2. Hierarchical question annotation (perception + AVR)
  3. Model evaluation across all patterns and configurations
  4. Analysis of perception vs. reasoning performance
  5. Few-shot prompting experiments to test generalization

- Design tradeoffs:
  - Breadth vs. Depth: MARVEL covers many patterns/configurations but may have fewer examples per specific type compared to specialized benchmarks
  - Synthetic vs. Natural: Manual puzzle creation ensures coverage but may lack the naturalness of real-world data
  - Perception vs. Reasoning focus: The hierarchical framework may overemphasize perception at the expense of testing pure reasoning ability

- Failure signatures:
  - Imbalanced output distributions (e.g., models consistently selecting the same choice)
  - Poor performance on perception questions despite good performance on other visual tasks
  - Inability to transfer reasoning patterns from few-shot demonstrations

- First 3 experiments:
  1. Run MARVEL evaluation on a new MLLM model to establish baseline performance
  2. Analyze failure patterns by examining perception vs. AVR question performance
  3. Test few-shot prompting effectiveness with CoT demonstrations across different patterns

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can few-shot prompting be effectively utilized to improve MLLM performance on abstract visual reasoning tasks, particularly for patterns that are not easily generalizable from demonstrations?
- Basis in paper: [explicit] The paper discusses the minimal impact of few-shot Chain-of-Thought (CoT) prompting on most patterns, except for 3D-geometry, suggesting a need for more effective few-shot strategies.
- Why unresolved: The complexity and unique nature of each pattern and input shape combination make it difficult for models to learn generalizable patterns from a few demonstrations.
- What evidence would resolve it: Experiments demonstrating significant performance improvements on a wide range of patterns with few-shot demonstrations, or the development of a new prompting strategy that enhances model understanding of abstract visual reasoning.

### Open Question 2
- Question: What are the specific visual perception limitations of MLLMs that hinder their ability to perform abstract visual reasoning, and how can these limitations be addressed?
- Basis in paper: [explicit] The paper highlights that MLLMs struggle with fine-grained visual feature comprehension, such as understanding the number of lines or the spatial relationship between shapes, which are foundational for abstract reasoning.
- Why unresolved: While the paper identifies perception as a bottleneck, it does not explore the underlying causes or potential solutions to improve visual perception in MLLMs.
- What evidence would resolve it: Studies that pinpoint the exact visual perception failures of MLLMs and propose targeted improvements, such as enhanced visual encoders or training data augmentation focused on visual details.

### Open Question 3
- Question: How does the introduction of text descriptions alongside visual puzzles impact the performance of MLLMs on abstract visual reasoning tasks, and what does this reveal about their reasoning capabilities?
- Basis in paper: [explicit] The paper shows that adding accurate text descriptions of puzzles significantly boosts MLLM performance, indicating that perception is a critical bottleneck in their reasoning process.
- Why unresolved: The study does not explore the long-term implications of relying on text descriptions for reasoning or how this approach affects the development of models' visual understanding capabilities.
- What evidence would resolve it: Research comparing the performance of MLLMs on tasks with and without text descriptions over time, and investigating whether models can learn to reason without relying on textual cues.

## Limitations
- The benchmark's synthetic nature may not fully capture the complexity of real-world visual reasoning tasks.
- The focus on perception may overshadow other potential bottlenecks in abstract reasoning, such as logical inference or pattern generalization.
- The limited number of puzzles per pattern and configuration may reduce statistical power for detecting subtle model differences.

## Confidence
- **High Confidence**: The claim that MLLMs struggle with abstract visual reasoning is well-supported by the experimental results showing near-random performance across all tested models.
- **Medium Confidence**: The attribution of poor AVR performance primarily to visual perception deficits is reasonable but may be oversimplified.
- **Low Confidence**: The effectiveness of few-shot prompting with CoT for 3D-geometry in GPT-4V may be a result of chance or specific dataset characteristics rather than a generalizable pattern.

## Next Checks
1. **Perception-Reaction Isolation Test**: Conduct controlled experiments where perception accuracy is artificially normalized (through ground-truth perception information) to determine if AVR performance improves, directly testing whether perception is the limiting factor.

2. **Cross-Benchmark Generalization Study**: Evaluate MARVEL performance against other AVR benchmarks to determine if the observed limitations are specific to MARVEL's design or represent broader MLLM deficiencies in abstract visual reasoning.

3. **Perception Skill Transfer Analysis**: Test whether improving MLLMs' performance on perception tasks (counting, identifying features) through specialized training leads to measurable improvements in AVR performance, establishing a causal link between the two capabilities.