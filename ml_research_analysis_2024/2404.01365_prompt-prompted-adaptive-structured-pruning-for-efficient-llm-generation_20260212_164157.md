---
ver: rpa2
title: Prompt-prompted Adaptive Structured Pruning for Efficient LLM Generation
arxiv_id: '2404.01365'
source_url: https://arxiv.org/abs/2404.01365
tags:
- arxiv
- griffin
- pruning
- generation
- neurons
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces GRIFFIN, a training-free and calibration-free
  method for efficient LLM generation that exploits naturally occurring structured
  sparsity in feedforward (FF) blocks. The key insight is "flocking" - where relative
  activation magnitudes within a sequence are highly consistent across tokens, revealing
  which neurons are most important.
---

# Prompt-prompted Adaptive Structured Pruning for Efficient LLM Generation

## Quick Facts
- arXiv ID: 2404.01365
- Source URL: https://arxiv.org/abs/2404.01365
- Authors: Harry Dong; Beidi Chen; Yuejie Chi
- Reference count: 17
- Primary result: Achieves 50% FF sparsity with minimal performance loss while improving latency (1.29× and 1.25× speed-ups on Gemma 7B and Llama 2 13B respectively)

## Executive Summary
GRIFFIN introduces a training-free and calibration-free method for efficient LLM generation by exploiting naturally occurring structured sparsity in feedforward (FF) blocks. The key insight is "flocking" - where relative activation magnitudes within a sequence are highly consistent across tokens, revealing which neurons are most important. GRIFFIN uses the prompt to identify expert neurons, then uses only these during generation. This achieves 50% FF sparsity with minimal performance loss while improving latency on various models including Llama 2, Gemma, Mistral, OPT, and ReluLlama across different activation functions.

## Method Summary
GRIFFIN exploits "flocking" - a phenomenon where relative activation magnitudes within a sequence are highly consistent across tokens in trained LLMs. The method uses the prompt phase to compute ℓ2-norms of FF activations across tokens for each neuron, selecting those with largest norms as "expert neurons." During generation, only these expert neurons are activated, reducing FF parameters by ~50% while maintaining performance. The approach works across different architectures and activation functions without modification, achieving significant latency improvements while preserving accuracy on classification and generation tasks.

## Key Results
- Maintains original model performance with 50% FF sparsity across classification and generation tasks
- Achieves 1.29× and 1.25× speed-ups on Gemma 7B and Llama 2 13B respectively on NVIDIA L40
- Works across different models (Llama 2, Gemma, Mistral, OPT, ReluLlama) and activation functions (ReLU, SwiGLU, GEGLU, ReGLU)
- Shows consistent flocking phenomenon across architectures despite differences in design

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Neurons with high relative activation magnitudes within a sequence are consistently important across all tokens.
- Mechanism: During the prompt phase, compute the ℓ2-norm of FF activations across tokens for each neuron. Neurons with the largest norms are selected as "expert neurons" for the entire sequence.
- Core assumption: Relative activation magnitudes are highly consistent within a sequence (flocking) and will remain consistent during generation.
- Evidence anchors:
  - [abstract]: "This is possible due to a critical observation that many trained LLMs naturally produce highly structured FF activation patterns within a sequence, which we call flocking."
  - [section 4.1]: "Since there are distinct vertical streaks, this intriguingly implies that activations that have relatively greater weight are common across all tokens in a sequence."
  - [corpus]: Weak evidence - no direct citations to flocking phenomenon.

### Mechanism 2
- Claim: Using prompt-derived expert neurons for generation maintains performance with reduced computation.
- Mechanism: During generation, only the expert neurons selected from the prompt phase are activated, reducing the number of active parameters by ~50% while maintaining model performance.
- Core assumption: The expert neurons selected from the prompt will remain relevant throughout generation for that specific sequence.
- Evidence anchors:
  - [abstract]: "we show with 50% of the FF parameters, GRIFFIN maintains the original model's performance with little to no degradation on a variety of classification and generation tasks"
  - [section 5.1]: "GRIFFIN maintains its reliability as we extend beyond single inputs."
  - [corpus]: Weak evidence - no direct citations to performance preservation claims.

### Mechanism 3
- Claim: GRIFFIN works across different architectures and activation functions without modification.
- Mechanism: The method relies on relative activation magnitudes rather than absolute values or specific activation functions, making it architecture-agnostic.
- Core assumption: Flocking phenomenon occurs regardless of activation function type (ReLU, SwiGLU, GEGLU, ReGLU).
- Evidence anchors:
  - [abstract]: "This is possible due to a critical observation that many trained LLMs naturally produce highly structured FF activation patterns within a sequence... Llama 2 7B and Gemma 7B use SwiGLU and GEGLU activations, respectively"
  - [section 4.1]: "Notably, Llama 2 7B and Gemma 7B use SwiGLU and GEGLU activations, respectively, along with other major architecture differences."
  - [corpus]: Weak evidence - no direct citations to activation function agnosticism.

## Foundational Learning

- Concept: Feedforward (FF) block operations and structure
  - Why needed here: Understanding how FF blocks work is essential to grasp why pruning specific neurons affects model performance and efficiency.
  - Quick check question: What are the dimensions of W1, Wg, and W2 in a typical FF block with GLU activation, given input dimension D and FF dimension DFF?

- Concept: Vector normalization and relative magnitude comparison
  - Why needed here: The method relies on comparing relative activation magnitudes within sequences, which requires understanding how to normalize and compare vectors.
  - Quick check question: If you have a sequence of activation vectors, how do you compute their relative magnitudes after normalization?

- Concept: Jaccard similarity and set operations
  - Why needed here: The paper uses Jaccard similarity to compare sets of top-k neurons across different sequences, which is key to understanding why static pruning is less effective than adaptive methods.
  - Quick check question: Given two sets of selected neurons A and B, how do you compute their Jaccard similarity?

## Architecture Onboarding

- Component map: Input sequence → FF1 layer → Activation statistics computation → Expert neuron selection → FF2 layer with pruned weights
- Critical path:
  1. Compute FF activations for prompt tokens
  2. Calculate ℓ2-norm statistics across tokens
  3. Select top-k neurons based on statistics
  4. Prune weight matrices to retain only selected neurons
  5. Use pruned matrices for generation phase
- Design tradeoffs:
  - Accuracy vs efficiency: Higher k preserves more accuracy but reduces efficiency gains
  - Prompt length vs generation quality: Longer prompts provide better statistics but increase overhead
  - Batch size vs adaptability: Larger batches reduce adaptability but improve throughput
- Failure signatures:
  - Performance degradation when k is too small
  - Inconsistent results across similar sequences
  - Latency increases due to inefficient expert selection
  - Memory issues when batch sizes are too large
- First 3 experiments:
  1. Validate flocking phenomenon: Run FF block on single sequence, compute relative activation magnitudes, visualize consistency across tokens
  2. Test expert neuron selection: Use different k values, measure performance degradation on simple task
  3. Benchmark latency: Compare full model vs GRIFFIN on generation task with varying prompt lengths and batch sizes

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What causes the phenomenon of "flocking" in feedforward activation patterns across different tokens within a sequence?
- Basis in paper: [explicit] The paper observes that relative activation magnitudes within a sequence are highly consistent across tokens, creating distinct vertical streaks in heatmaps, but does not explain the underlying cause of this phenomenon.
- Why unresolved: While the paper demonstrates flocking exists across various models and activation functions, it only describes the phenomenon without investigating what causes neurons to activate consistently within sequences. The paper mentions this is "bizarre" given architecture differences but doesn't explore potential causes.
- What evidence would resolve it: Systematic ablation studies varying factors like sequence length, token similarity, attention patterns, or positional embeddings could reveal whether flocking is driven by linguistic structure, attention mechanisms, or inherent properties of the feedforward blocks.

### Open Question 2
- Question: How does GRIFFIN's performance scale with sequence length, and what is the optimal prompt length for different generation tasks?
- Basis in paper: [explicit] The paper shows that GRIFFIN's performance improves with longer prompts and shorter generation phases in Figure 5, but doesn't establish concrete guidelines for optimal prompt length.
- Why unresolved: The paper only provides a qualitative observation about the relationship between prompt length and performance without quantitative recommendations or theoretical justification for why this relationship exists.
- What evidence would resolve it: Empirical studies measuring performance degradation across varying prompt-generation length ratios for different tasks, along with analysis of how this relates to the complexity or difficulty of the generation task.

### Open Question 3
- Question: How does GRIFFIN's neuron selection adapt to different domains or types of text, and can the method be made more domain-aware?
- Basis in paper: [inferred] The paper uses WikiText for analysis and demonstrates good performance across various tasks, but doesn't explore whether the neuron selection adapts differently to specialized domains or how domain shifts might affect performance.
- Why unresolved: While the paper shows GRIFFIN works across general tasks, it doesn't investigate whether the same expert neurons are optimal across different domains (e.g., code vs. prose, formal vs. informal text) or how domain adaptation might improve performance.
- What evidence would resolve it: Experiments comparing neuron selection patterns and performance across domain-specific datasets, along with domain-aware variants of GRIFFIN that adjust neuron selection based on detected text characteristics.

## Limitations
- Flocking phenomenon lacks theoretical explanation for why relative activation magnitudes remain consistent across tokens
- 50% sparsity threshold appears effective but without systematic exploration of the accuracy-efficiency tradeoff curve
- Evaluation focuses primarily on English language tasks and mainstream architectures, leaving questions about generalization to other languages, modalities, or specialized domains

## Confidence
- **High Confidence**: The core flocking observation and relative magnitude computation methodology - these are straightforward implementations with clear visual evidence in the activation plots.
- **Medium Confidence**: The performance preservation claims with 50% sparsity - while demonstrated across multiple models and tasks, the results depend on the quality of expert neuron selection which may vary with input characteristics.
- **Low Confidence**: The assertion that the method is "calibration-free" - the paper doesn't thoroughly investigate whether expert neuron selection might need adjustment for different temperature settings or generation strategies.

## Next Checks
1. **Flocking Robustness Test**: Systematically vary input sequence characteristics (length, topic diversity, token distribution) and measure how consistently flocking behavior manifests across different domains and datasets.

2. **Sparsity-Performance Tradeoff Analysis**: Conduct a comprehensive study varying k from 10% to 90% sparsity, measuring both accuracy degradation and latency improvements to identify optimal operating points for different use cases.

3. **Cross-Model Generalization Test**: Apply GRIFFIN to specialized architectures (code models, multilingual models, multimodal models) and evaluate whether the flocking phenomenon persists and whether the same k values remain effective.