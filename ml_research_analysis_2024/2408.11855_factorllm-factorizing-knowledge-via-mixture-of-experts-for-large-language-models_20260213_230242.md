---
ver: rpa2
title: 'FactorLLM: Factorizing Knowledge via Mixture of Experts for Large Language
  Models'
arxiv_id: '2408.11855'
source_url: https://arxiv.org/abs/2408.11855
tags:
- experts
- language
- knowledge
- factorllm
- large
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces FactorLLM, a novel method that factorizes
  dense Feed-Forward Networks (FFNs) in Large Language Models (LLMs) into sparse Mixture-of-Experts
  (MoE) architectures without performance loss. The approach decomposes the FFN weight
  matrix into sub-networks of uniform dimensions, enabling efficient knowledge factorization
  and dynamic expert activation.
---

# FactorLLM: Factorizing Knowledge via Mixture of Experts for Large Language Models

## Quick Facts
- **arXiv ID**: 2408.11855
- **Source URL**: https://arxiv.org/abs/2408.11855
- **Reference count**: 40
- **Primary result**: FactorLLM achieves up to 85% of original model performance while reducing inference FLOPs by over 30% through sparse expert activation.

## Executive Summary
FactorLLM introduces a novel method to decompose dense Feed-Forward Networks (FFNs) in Large Language Models into sparse Mixture-of-Experts (MoE) architectures without performance loss. The approach factorizes the FFN weight matrix into uniformly shaped sub-networks and employs a Prior-Approximate Router (PAR) that leverages teacher model knowledge for efficient expert activation. Extensive experiments demonstrate significant computational efficiency gains while maintaining model accuracy across multiple benchmarks.

## Method Summary
FactorLLM decomposes a pretrained dense LLM's FFN into sparse MoE subnetworks using a permutation matrix that reorders weight elements without modification. The method introduces a Prior-Approximate Router that uses teacher model outputs to guide expert selection through MSE-based pseudo-allocations. During training, the model adapts experts using limited data (0.03-0.04% of original training data) while maintaining performance through teacher-student knowledge distillation.

## Key Results
- Achieves up to 85% of original model performance while obtaining over 30% increase in inference speed
- Requires only 0.03-0.04% of the original model's training data for adaptation
- Outperforms baseline approaches like MoEfication and KnowledgeFactor across multiple benchmarks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Factorizing the dense FFN weight matrix into uniformly shaped subnetworks preserves original model performance.
- Mechanism: The permutation matrix Pδ reorders the weight matrix elements without modifying their values, ensuring the composite output remains identical to the original FFN.
- Core assumption: The FFN weight matrix can be decomposed into N sub-networks of equal dimension without loss of representational capacity.
- Evidence anchors: [abstract] "FactorLLM, a novel approach that decomposes well-trained dense FFNs into sparse sub-networks without requiring any further modifications, while maintaining the same level of performance."; [section] "Eq. (6) shows that the monolithic FFN can be decomposed into N subnetworks while preserving the integrity of the output representation processed by the FFN layer."

### Mechanism 2
- Claim: The Prior-Approximate Router (PAR) accelerates expert activation by leveraging teacher model knowledge.
- Mechanism: PAR computes MSE between teacher FFN outputs and student expert outputs, then uses TopK selection to create pseudo-allocations that guide router learning via cross-entropy loss.
- Core assumption: The teacher FFN outputs provide meaningful similarity signals that correlate with optimal expert selection.
- Evidence anchors: [abstract] "Prior-Approximate (PA) loss term that facilitates the dynamic activation of experts and knowledge adaptation"; [section] "We first compute the Mean Squared Error (MSE) across these features, yielding a set of distances D... Subsequently, we apply the TopK algorithm to extract expert indices I for the smallest dmse"

### Mechanism 3
- Claim: Sparse expert activation reduces computational overhead while maintaining accuracy.
- Mechanism: By activating only K out of N experts per token through router selection, FactorLLM achieves computational efficiency proportional to K/N of the original dense FFN.
- Core assumption: Only a subset of experts contains relevant knowledge for any given input token, making full activation unnecessary.
- Evidence anchors: [abstract] "securing up to 85% model performance while obtaining over a 30% increase in inference speed"; [section] "We factorize the fully pretrained dense LLM into the sparse MoE-LLM together with a randomly initialized injected router, which is designed to facilitate adaptive and efficient computation."

## Foundational Learning

- Concept: Mixture-of-Experts (MoE) architecture
  - Why needed here: FactorLLM builds upon MoE principles by treating factorized FFN subnetworks as experts and using a router for dynamic activation.
  - Quick check question: What is the primary advantage of using a router to select experts in MoE architectures?

- Concept: Matrix permutation and decomposition
  - Why needed here: The FFN factorization relies on reordering weight matrix elements through a permutation matrix without modifying values.
  - Quick check question: How does applying a permutation matrix to a weight matrix enable decomposition into smaller sub-networks?

- Concept: Teacher-student knowledge distillation
  - Why needed here: PAR uses teacher model outputs to guide router learning and expert activation strategies.
  - Quick check question: What role does the teacher model play in accelerating the adaptation of factorized experts?

## Architecture Onboarding

- Component map:
  - Original dense FFN → Factorized experts (N subnetworks of equal dimension)
  - Randomly initialized router → Prior-Approximate Router (PAR) with MSE-based pseudo-allocations
  - Teacher FFN → Provides output features for MSE computation and routing guidance
  - Loss components: Mean Squared Error (MSE), Cross-Entropy (CE) for routing, Final CE loss

- Critical path:
  1. Input token passes through MHA layer
  2. Router computes expert probabilities using PAR mechanism
  3. TopK selection activates K experts
  4. Selected experts process token through factorized FFN subnetworks
  5. Outputs combined and passed to next transformer block

- Design tradeoffs:
  - Expert count (N) vs. computational efficiency: More experts enable finer-grained specialization but increase routing complexity
  - Router complexity vs. adaptation speed: More sophisticated routers may require more training data
  - Factorization uniformity vs. task specificity: Uniform expert dimensions simplify implementation but may not optimize for all tasks

- Failure signatures:
  - Performance degradation despite FLOPs reduction: Indicates router failing to select appropriate experts
  - Router collapse to few experts: Suggests imbalance in expert utilization or inadequate PAR guidance
  - Increased training instability: May result from improper MSE-based routing signals or expert initialization

- First 3 experiments:
  1. Verify FFN factorization preserves output by comparing original FFN vs. N activated experts (K=N)
  2. Test PAR effectiveness by comparing routing performance with and without teacher guidance
  3. Measure computational savings vs. performance trade-off across different K/N ratios

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the Prior-Approximate Router (PAR) mechanism scale when applied to larger, more complex language models beyond TinyLlama and MobileLlama?
- Basis in paper: [explicit] The paper mentions that FactorLLM was tested on TinyLlama and MobileLlama, but does not explore its effectiveness on larger models.
- Why unresolved: The study focuses on smaller models, leaving questions about PAR's performance and scalability on larger architectures unanswered.
- What evidence would resolve it: Testing PAR on larger models like GPT-3 or LLaMA-2, and comparing its efficiency and accuracy gains with those on smaller models.

### Open Question 2
- Question: What is the impact of the number of experts (N) on the model's performance and computational efficiency in different tasks?
- Basis in paper: [inferred] The paper discusses the decomposition of FFNs into subnetworks but does not thoroughly explore how varying the number of experts affects performance across diverse tasks.
- Why unresolved: The experiments primarily focus on a fixed number of experts, and the relationship between expert count and task-specific performance is not fully explored.
- What evidence would resolve it: Conducting experiments with different numbers of experts across various benchmarks to determine optimal configurations for different tasks.

### Open Question 3
- Question: How does FactorLLM handle the potential imbalance in the distribution of inputs among experts over time?
- Basis in paper: [explicit] The paper mentions the introduction of a load balance loss in MoE architectures but does not detail how FactorLLM addresses this issue.
- Why unresolved: The paper does not provide information on how the model ensures balanced expert usage during training or inference.
- What evidence would resolve it: Analyzing the input distribution across experts during training and inference, and implementing strategies to mitigate imbalance if observed.

### Open Question 4
- Question: What are the long-term effects of using FactorLLM on model robustness and generalization across unseen tasks?
- Basis in paper: [inferred] While FactorLLM shows promise in specific benchmarks, the paper does not address its performance on entirely new or evolving tasks.
- Why unresolved: The study does not include evaluations on tasks outside the tested benchmarks, leaving questions about the model's adaptability and robustness unanswered.
- What evidence would resolve it: Testing FactorLLM on a diverse set of new tasks and monitoring its performance over time to assess adaptability and robustness.

## Limitations
- Sparse Routing Generalization: PAR effectiveness across diverse domains beyond tested benchmarks remains uncertain
- Permutation Matrix Sensitivity: Paper lacks analysis of how different permutations affect factorization quality
- Training Data Efficiency Claims: 0.03-0.04% data requirement needs scrutiny for generalizability to larger models

## Confidence

- **High Confidence**: Core FFN factorization mechanism is mathematically sound with clearly described experimental setup
- **Medium Confidence**: PAR design and implementation details are somewhat clear but could benefit from more exposition
- **Low Confidence**: Training data efficiency claims and PAR generalizability to domains beyond natural language understanding lack sufficient evidence

## Next Checks
1. **Permutation Matrix Analysis**: Conduct systematic study varying permutation matrix to determine impact on expert specialization and routing efficiency
2. **Domain Transfer Experiment**: Test FactorLLM on code generation and mathematical reasoning tasks to validate PAR generalizability
3. **Scaling Analysis**: Evaluate FactorLLM on larger models (7B, 13B parameters) to determine if training data efficiency claims hold at scale