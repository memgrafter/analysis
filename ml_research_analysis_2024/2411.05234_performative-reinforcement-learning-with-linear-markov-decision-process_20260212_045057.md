---
ver: rpa2
title: Performative Reinforcement Learning with Linear Markov Decision Process
arxiv_id: '2411.05234'
source_url: https://arxiv.org/abs/2411.05234
tags:
- bracehext
- policy
- following
- then
- bound
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper studies performative reinforcement learning where the
  deployed policy affects both the reward and transition functions of the underlying
  Markov decision process. The main contribution is extending convergence guarantees
  to linear Markov decision processes, where prior tabular results don't apply due
  to the lack of strong convexity in the regularized objective.
---

# Performative Reinforcement Learning with Linear Markov Decision Process

## Quick Facts
- **arXiv ID**: 2411.05234
- **Source URL**: https://arxiv.org/abs/2411.05234
- **Reference count**: 40
- **Primary result**: Extends performative RL convergence guarantees to linear MDPs, overcoming the lack of strong convexity in the regularized objective.

## Executive Summary
This paper studies performative reinforcement learning where the deployed policy affects both the reward and transition functions of the underlying Markov decision process. The main contribution is extending convergence guarantees to linear Markov decision processes, where prior tabular results don't apply due to the lack of strong convexity in the regularized objective. The authors develop a framework that repeatedly optimizes a regularized objective and show convergence to performatively stable policies, even without strong concavity. They also construct an empirical Lagrangian for the finite-sample setting and provide sample complexity bounds under a bounded coverage condition.

## Method Summary
The method consists of two main components: (1) repeated optimization of a regularized objective in the known MDP setting, where a novel recurrence relation based on optimal dual solutions ensures convergence to performatively stable policies; and (2) empirical Lagrangian construction from finite samples combined with saddle point optimization, which converges under a bounded coverage condition. The approach uses a reparametrized primal variable and solves the resulting saddle point problem using a primal-dual algorithm with polynomial sample complexity in the feature dimension.

## Key Results
- Convergence to performatively stable policies in linear MDPs using regularized optimization without requiring strong convexity
- Sample complexity bounds polynomial in feature dimension for the finite-sample setting under bounded coverage condition
- Extension to stochastic Stackelberg games with one or multiple followers
- Bound on the distance between performatively optimal and stable policies, enabling approximation of optimal policies

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Repeatedly optimizing a regularized objective in linear MDPs converges to a performatively stable policy even without strong convexity.
- **Mechanism**: The algorithm constructs a recurrence relation based on a time-varying linear combination of optimal dual solutions, allowing convergence analysis despite the lack of strong concavity in the regularized objective.
- **Core assumption**: The feature matrix Φ has full column rank and satisfies a row-space condition that ensures different occupancy measures produce sufficiently different feature representations.
- **Evidence anchors**:
  - [abstract]: "our analysis leverages a new recurrence relation that uses a specific linear combination of optimal dual solutions"
  - [section]: "In the absence of strong concavity of the objective, we establish a new recurrence relation depending on a time-varying linear combination of the optimal dual solutions"
- **Break condition**: If the feature matrix Φ does not have full column rank, or the row-space condition fails (i.e., κ → 0), the recurrence relation cannot guarantee convergence.

### Mechanism 2
- **Claim**: The empirical Lagrangian constructed from finite samples converges to a performatively stable policy under a bounded coverage condition.
- **Mechanism**: The algorithm reparametrizes the primal variable using a finite-dimensional representation, constructs an empirical Lagrangian, and solves its saddle point. The bounded coverage condition ensures sufficient overlap between the deployed policy and the optimal policy in the updated MDP.
- **Core assumption**: The bounded coverage ratio B > 0 exists, which generalizes the concentrability assumption from offline RL to the performative setting.
- **Evidence anchors**:
  - [abstract]: "under a bounded coverage condition, repeatedly solving a saddle point of this empirical Lagrangian converges to a performatively stable policy"
  - [section]: "Assumption 4 (Bounded Coverage Ratio)... There exists a constant B > 0 such that..."
- **Break condition**: If the coverage ratio B is unbounded or zero, the sample complexity becomes infinite and the algorithm cannot learn the optimal policy.

### Mechanism 3
- **Claim**: The performatively optimal policy can be approximated by tuning the regularization parameter λ based on environment sensitivity.
- **Mechanism**: The algorithm shows that as the sensitivity parameters εθ and εµ approach zero, the solution to the regularized problem approaches both the performatively stable and optimal solutions to the unregularized problem.
- **Core assumption**: The environment sensitivity parameters εθ and εµ are bounded and can be controlled through the choice of regularization parameter λ.
- **Evidence anchors**:
  - [abstract]: "we show that our method also converges to a performatively optimal policy by establishing a bound on the distance between performatively optimal and stable policies"
  - [section]: "We then show how to tune the strength of the regularization to obtain an approximate performatively stable optimal policy based on the sensitivity of the environment"
- **Break condition**: If the environment is highly sensitive (εθ, εµ large), the approximation gap becomes significant regardless of λ choice.

## Foundational Learning

- **Concept**: Linear Markov Decision Processes and feature representation
  - Why needed here: The entire framework relies on representing state-action pairs through feature vectors φ(s,a) and assuming the reward and transition functions are linear in these features
  - Quick check question: Given a state s and action a, can you write out the feature vector φ(s,a) and explain how it relates to the reward r(s,a) = ⟨φ(s,a), θ⟩?

- **Concept**: Lagrangian duality and saddle point optimization
  - Why needed here: The algorithm constructs both true and empirical Lagrangians and solves their saddle points to handle the constraints in the optimization problem
  - Quick check question: Given the primal problem max d d⊤Φθ subject to Bd = ρ + γ·µΦ⊤d, can you write out the Lagrangian and identify the dual variables?

- **Concept**: Performative prediction and distribution shift
  - Why needed here: The setting involves the deployed policy affecting the underlying MDP, which is the core concept of performativity in both prediction and reinforcement learning
  - Quick check question: What is the key difference between performative prediction and standard supervised learning in terms of how the data distribution changes?

## Architecture Onboarding

- **Component map**: Feature extraction → Regularized optimization → Saddle point solution → Policy deployment → Environment update → Repeat

- **Critical path**: Feature extraction → Regularized optimization (or empirical Lagrangian construction) → Saddle point solution → Policy deployment → Environment update → Repeat

- **Design tradeoffs**: Strong regularization ensures convergence but may increase approximation error; weak regularization may not guarantee convergence but provides better approximation to the unregularized problem

- **Failure signatures**: 
  - No convergence: Check if λ is too small or feature matrix rank is insufficient
  - High approximation error: Check if regularization is too strong or environment sensitivity is too high
  - Poor sample efficiency: Check if bounded coverage condition is violated or sample size mt is insufficient

- **First 3 experiments**:
  1. Verify convergence on a small linear MDP with known ground truth policy
  2. Test sensitivity to regularization parameter λ by sweeping different values
  3. Evaluate sample complexity by varying dataset size mt and measuring approximation error

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: How can performative reinforcement learning be extended to nonlinear function approximation beyond linear MDPs?
- **Basis in paper**: [explicit] The paper acknowledges this as an important future direction in the conclusion, noting that extending results to general function approximation is significantly harder due to non-convexity issues
- **Why unresolved**: The paper focuses on linear MDPs specifically because the regularized objective remains convex, but general function approximation introduces non-convex optimization challenges that make stability analysis difficult
- **What evidence would resolve it**: Successful extension of performative RL convergence guarantees to nonlinear architectures like deep neural networks, or formal proofs showing fundamental barriers to such extensions

### Open Question 2
- **Question**: What is the optimal regularization parameter strategy for balancing performative stability and optimality in the finite-sample setting?
- **Basis in paper**: [explicit] The paper shows that regularization strength λ affects both convergence to stable policies and approximation to performatively optimal policies, but the trade-off is complex and depends on sensitivity parameters
- **Why unresolved**: While the paper provides bounds on how λ should scale with problem parameters, determining the optimal λ that minimizes the suboptimality gap in practice requires empirical tuning or adaptive strategies
- **What evidence would resolve it**: Development of principled methods for selecting λ that provably minimize the gap between performative stability and optimality in finite-sample settings

### Open Question 3
- **Question**: How does performative reinforcement learning behave in multi-agent settings with strategic followers beyond the Stackelberg game framework?
- **Basis in paper**: [explicit] The paper studies stochastic Stackelberg games with one or multiple followers but notes that sensitivity parameters grow exponentially with the number of followers, suggesting limitations
- **Why unresolved**: The exponential growth in sensitivity parameters indicates that simple repeated optimization may not scale well to general multi-agent games, but the paper doesn't explore alternative formulations or equilibrium concepts
- **What evidence would resolve it**: Analysis of performative RL convergence in general-sum stochastic games, study of how different follower rationality models affect convergence, or development of algorithms that scale better with the number of agents

## Limitations
- Strong assumptions required: full column rank feature matrix Φ and bounded coverage ratio B
- Requires access to MDP parameters θ_d and μ_d for any occupancy measure d
- Sample complexity depends polynomially on feature dimension D, which could be prohibitive for high-dimensional problems
- Limited to linear MDPs and cannot directly extend to nonlinear function approximation

## Confidence
- **High**: Convergence of regularized optimization in known MDP setting (rigorous recurrence relation analysis)
- **Medium**: Sample complexity bounds (depends on bounded coverage assumption which may be difficult to verify)
- **Low**: Practical applicability to complex real-world problems (strong assumptions and potential computational challenges)

## Next Checks
1. **Empirical verification of bounded coverage**: Implement synthetic experiments to empirically verify whether the bounded coverage ratio B exists for different policy classes and feature representations, and measure how B varies with problem parameters.

2. **Sensitivity analysis of regularization parameter**: Conduct systematic experiments varying the regularization parameter λ across orders of magnitude to empirically validate the theoretical relationship between λ, environment sensitivity (εθ, εμ), and convergence quality.

3. **Scalability assessment**: Test the algorithm on increasingly high-dimensional linear MDPs to empirically measure how the polynomial dependence on feature dimension D affects performance, and identify potential computational bottlenecks in the saddle point optimization.