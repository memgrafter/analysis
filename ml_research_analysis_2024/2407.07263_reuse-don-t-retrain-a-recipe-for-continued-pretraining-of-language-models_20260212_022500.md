---
ver: rpa2
title: 'Reuse, Don''t Retrain: A Recipe for Continued Pretraining of Language Models'
arxiv_id: '2407.07263'
source_url: https://arxiv.org/abs/2407.07263
tags:
- pretraining
- data
- continued
- arxiv
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work studies how to improve an already trained large language
  model through continued pretraining, rather than retraining from scratch. The authors
  propose a recipe consisting of two data distributions and a specific learning rate
  schedule.
---

# Reuse, Don't Retrain: A Recipe for Continued Pretraining of Language Models

## Quick Facts
- arXiv ID: 2407.07263
- Source URL: https://arxiv.org/abs/2407.07263
- Authors: Jupinder Parmar; Sanjev Satheesh; Mostofa Patwary; Mohammad Shoeybi; Bryan Catanzaro
- Reference count: 37
- Primary result: 9% average accuracy improvement on MMLU, HellaSwag, HumanEval, and MGSM benchmarks

## Executive Summary
This work proposes a recipe for continued pretraining of large language models that improves performance without full retraining. The approach uses two distinct data distributions and a specific learning rate schedule to efficiently enhance model capabilities. Applied to a 15B parameter model, the method achieves a 9% average accuracy improvement over baseline continued training.

## Method Summary
The method employs a two-phase continued pretraining approach. The first phase uses a general blend (GB) data distribution that emphasizes high-quality sources and model weaknesses for 250B tokens. The second phase switches to a QA blend (QB) incorporating question-answer data and further upweighting weak areas. The learning rate starts from the pretrained model's minimum (4.5e-5) and decays to 1/100th of the maximum (4.5e-7) with cosine annealing. The distribution switch occurs at 1/5th of the maximum learning rate (9e-6).

## Key Results
- 9% average accuracy improvement over baseline continued training on MMLU, HellaSwag, HumanEval, and MGSM benchmarks
- Effective across continued training scales from 100B to 1T tokens
- Outperforms baseline by 3-4% when continuing from a 1.58T token pretrained model
- Achieves improved performance without catastrophic forgetting

## Why This Works (Mechanism)

### Mechanism 1
Using a two-phase data distribution with GB followed by QB improves learning efficiency without catastrophic forgetting. The GB primes the model without destabilizing it, while the QB reinforces new knowledge and weak domains while preserving learned capabilities. The model can smoothly transition between distributions if the switch occurs at an appropriate learning rate point.

### Mechanism 2
Decaying the learning rate from η_min to η_max/100 with cosine annealing balances magnitude and decay slope. Starting from η_min avoids large initial updates that could destabilize the pretrained model, while decaying to η_min/100 maintains sufficient LR magnitude for the QB phase while ensuring gradual learning.

### Mechanism 3
Switching data distributions at η_max/5 optimizes the balance between learning rate magnitude and decay slope. At this point, the LR is still sufficiently large to enable learning of the new QB distribution but not so large as to cause instability from the data shift.

## Foundational Learning

- **Concept: Catastrophic forgetting**
  - Why needed here: Continued pretraining must avoid overwriting previously learned knowledge while adding new capabilities
  - Quick check question: What happens if the model is trained too aggressively on new data without preserving old knowledge?

- **Concept: Curriculum learning**
  - Why needed here: Starting with a distribution similar to pretraining (GB) before introducing new data (QB) follows a curriculum that stabilizes learning
  - Quick check question: Why is it beneficial to introduce new data sources later in the training run rather than at the beginning?

- **Concept: Learning rate scheduling**
  - Why needed here: Proper LR scheduling (starting from η_min, decaying to η_min/100) ensures stable and efficient learning during continued pretraining
  - Quick check question: What is the risk of starting continued pretraining with a high learning rate?

## Architecture Onboarding

- **Component map**: Pretrained 15B parameter decoder-only transformer -> GB data distribution (250B tokens) -> switch at η_max/5 -> QB data distribution -> cosine annealing LR schedule (η_min → η_min/100) -> AdamW optimizer

- **Critical path**:
  1. Load pretrained model and optimizer state
  2. Initialize GB data distribution
  3. Train until LR reaches η_max/5
  4. Switch to QB distribution
  5. Continue training until LR reaches η_min/100
  6. Evaluate on MMLU, HellaSwag, HumanEval, MGSM

- **Design tradeoffs**:
  - GB must balance similarity to pretraining data (stability) with emphasis on weaknesses (improvement)
  - LR schedule must balance magnitude (learning speed) with decay (stability)
  - Distribution switch point must balance learning rate (magnitude vs. decay)

- **Failure signatures**:
  - Training instability: sudden accuracy drop or NaN loss
  - Catastrophic forgetting: significant degradation on pretraining-like tasks
  - Inefficient learning: slow improvement or plateauing accuracy

- **First 3 experiments**:
  1. Baseline: continued training on pretraining distribution only (for comparison)
  2. GB only: test stability and improvement with general blend
  3. GB→QB switch at η_max/2: test effect of different switch points

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal learning rate schedule for continued pretraining of language models beyond the cosine annealing approach?
- Basis in paper: [explicit] The authors mention that experiments with WSD and varying warmup schedules led to accuracy regressions compared to cosine annealing.
- Why unresolved: The study only compared cosine annealing to WSD and a few warmup variants. There may be other learning rate schedules that could perform better.
- What evidence would resolve it: Experiments comparing a wider range of learning rate schedules on the same model and task would provide evidence.

### Open Question 2
- Question: How does the continued pretraining recipe generalize to models with different architectures (e.g., encoder-decoder, sparse attention) and sizes (e.g., smaller or larger than 15B parameters)?
- Basis in paper: [inferred] The authors state that their findings are likely transferable to most settings and model sizes, but this is not explicitly tested.
- Why unresolved: The study only tested the recipe on a specific model architecture and size. The effectiveness of the recipe for other architectures and sizes is unknown.
- What evidence would resolve it: Experiments applying the recipe to models with different architectures and sizes on the same or similar tasks would provide evidence.

### Open Question 3
- Question: What is the optimal way to dynamically adjust the data distribution during continued pretraining based on the model's performance on specific tasks or domains?
- Basis in paper: [explicit] The authors find that using two data distributions with a switch point at η_max/5 is beneficial. However, the switch point and data distribution weights are fixed in their recipe.
- Why unresolved: The study uses fixed data distributions and a fixed switch point. It does not explore dynamic adjustment of the data distribution based on the model's performance or learning progress.
- What evidence would resolve it: Experiments comparing fixed vs. dynamically adjusted data distributions would provide evidence.

## Limitations
- No ablation studies to isolate contributions of individual recipe components
- Evaluation limited to a single 15B parameter model architecture and four specific benchmarks
- Exact composition of data distributions underspecified (weighting schemes and specific sources)

## Confidence
- **High Confidence**: The general framework of two-phase continued pretraining with data distribution switching is sound and well-supported by curriculum learning principles
- **Medium Confidence**: The specific parameters of the recipe (switching at η_max/5, decaying to η_min/100) are likely effective but may not be optimal
- **Low Confidence**: The claim that this approach generalizes well across all large language model architectures and training scales is not well-supported

## Next Checks
1. Conduct ablation study of recipe components to quantify individual contributions to the 9% improvement
2. Apply the recipe to different model architectures (encoder-decoder, different layer configurations) to verify generalization
3. Analyze data distribution composition through controlled experiments varying weighting schemes and sources to identify critical aspects for improvements