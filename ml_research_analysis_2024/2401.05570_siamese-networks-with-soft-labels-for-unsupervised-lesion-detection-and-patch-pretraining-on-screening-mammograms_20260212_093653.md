---
ver: rpa2
title: Siamese Networks with Soft Labels for Unsupervised Lesion Detection and Patch
  Pretraining on Screening Mammograms
arxiv_id: '2401.05570'
source_url: https://arxiv.org/abs/2401.05570
tags:
- patch
- pair
- siamese
- abnormal
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The study introduces a Siamese network with soft labels for unsupervised
  lesion detection in screening mammograms, leveraging the natural symmetry of human
  body as weak labels. By training the network to distinguish abnormal lesions from
  normal tissues using contralateral mammograms, the approach achieves superior performance
  in mammogram patch classification compared to existing self-supervised learning
  methods.
---

# Siamese Networks with Soft Labels for Unsupervised Lesion Detection and Patch Pretraining on Screening Mammograms

## Quick Facts
- arXiv ID: 2401.05570
- Source URL: https://arxiv.org/abs/2401.05570
- Reference count: 38
- Key outcome: Achieves average AUC scores of 0.927 for abnormal vs. normal patch classification on VinDr-256 dataset using Siamese network with soft labels

## Executive Summary
This study introduces a Siamese network architecture with soft labels for unsupervised lesion detection in screening mammograms. The approach leverages the natural symmetry of human breasts, using contralateral pairs as weak labels where one breast serves as a proxy for normal tissue while the other may contain abnormalities. By training the network to distinguish abnormal from normal tissue without requiring costly manual annotations, the method achieves superior performance compared to existing self-supervised learning approaches for mammogram patch classification.

The key innovation lies in the soft labeling strategy using Gaussian Mixture Models (GMM) to generate probabilistic labels based on embedding distances. This approach addresses the confirmation bias problem common in contrastive learning by using cross-network training with two Siamese networks. The method demonstrates strong performance across multiple datasets (VinDr-256, VinDr-96, OPTIMAM-256, OPTIMAM-96) for various downstream tasks including abnormal versus normal patch classification, BI-RADS classification, and outcome prediction.

## Method Summary
The method employs a Siamese network architecture with two identical ResNet-18 encoders that process paired mammogram patches from contralateral breasts. Normal patches are sampled from one breast while potentially abnormal patches come from the other, creating a natural weak labeling scheme. During training, the network learns to map patches to an embedding space where normal patches cluster together and abnormal patches are separated. A Gaussian Mixture Model is fitted on the training set's pairwise Euclidean distances between embeddings to generate soft labels representing the probability of abnormality. The training process uses cross-loss between two Siamese networks to prevent confirmation bias, where each network's embeddings are used to compute loss for the other network. For evaluation, a linear classifier is trained on frozen embeddings to assess performance on downstream tasks.

## Key Results
- Achieves average AUC of 0.927 for abnormal vs. normal patch classification on VinDr-256 dataset
- Demonstrates AUC of 0.744 for outcome classification (malignant vs. benign) on OPTIMAM-256 dataset
- Outperforms existing self-supervised learning methods on mammogram patch classification tasks

## Why This Works (Mechanism)
The method works by exploiting the natural bilateral symmetry of human breasts as a source of weak supervision. When lesions are present in one breast, the contralateral breast typically remains normal, creating an implicit labeling system without manual annotation. The soft labeling approach using GMM provides probabilistic rather than binary labels, which better captures the uncertainty inherent in medical imaging where abnormality often exists on a spectrum. The cross-network training strategy prevents the networks from reinforcing incorrect associations, a common failure mode in contrastive learning where networks can learn to map different classes to similar embeddings.

## Foundational Learning
- **Siamese Networks**: Neural networks that process pairs of inputs through identical subnetworks to learn similarity metrics. Needed because we want to compare patches from different breasts to identify abnormalities. Quick check: Verify that both subnetworks have identical architectures and share weights.
- **Contrastive Learning**: Self-supervised learning method that pulls similar examples together and pushes dissimilar examples apart in embedding space. Needed to learn meaningful representations without labels. Quick check: Confirm that positive pairs (normal-normal) are closer than negative pairs (normal-abnormal) in embedding space.
- **Gaussian Mixture Models**: Probabilistic models that represent data as a mixture of Gaussian distributions. Needed to generate soft labels based on the distribution of embedding distances. Quick check: Validate that GMM posterior probabilities reflect meaningful uncertainty in abnormality classification.
- **Cross-Network Training**: Training strategy where two networks are trained simultaneously with each network's outputs used to compute loss for the other. Needed to prevent confirmation bias where networks reinforce incorrect associations. Quick check: Verify that both networks converge to similar performance and that swapping their roles doesn't significantly impact results.
- **Linear Evaluation Protocol**: Method where a linear classifier is trained on top of frozen embeddings to evaluate representation quality. Needed to fairly assess the quality of learned features independent of the training procedure. Quick check: Ensure that linear classifier performance correlates with overall task performance.

## Architecture Onboarding

**Component Map**: Input patches -> ResNet-18 encoder -> Embedding space -> GMM soft labels -> Cross-loss -> Two Siamese networks -> Frozen embeddings -> Linear classifier

**Critical Path**: The critical path involves the Siamese network processing paired patches, generating embeddings, computing pairwise distances, deriving soft labels via GMM, and optimizing through cross-loss between the two networks. The quality of embeddings directly determines downstream task performance.

**Design Tradeoffs**: The use of soft labels via GMM versus hard binary labels provides more nuanced supervision but adds complexity and hyperparameter sensitivity. The cross-network training approach prevents confirmation bias but doubles computational requirements. The ResNet-18 encoder balances representational power with computational efficiency, though deeper architectures might capture more complex patterns.

**Failure Signatures**: Training instability manifesting as exploding gradients or NaN losses suggests issues with the cross-loss formulation or learning rate. Poor separation of abnormal and normal patches in embedding space indicates the soft labeling strategy isn't capturing meaningful distinctions. If linear classifier performance is significantly lower than reported, the embeddings may not be discriminative enough or the GMM fitting may be suboptimal.

**3 First Experiments**:
1. Visualize t-SNE/UMAP embeddings of normal and abnormal patches to verify clustering patterns
2. Plot training loss curves for both Siamese networks to check for convergence and stability
3. Evaluate GMM posterior probabilities on a held-out validation set to assess soft label quality

## Open Questions the Paper Calls Out
None explicitly mentioned in the paper.

## Limitations
- The approach assumes contralateral breast is always normal, which may not hold in cases of bilateral abnormalities
- Performance may be sensitive to GMM hyperparameters, though specific parameters are not fully specified
- The method is specialized for mammograms and may not generalize to other medical imaging modalities without adaptation
- Computational overhead of training two Siamese networks simultaneously may limit scalability

## Confidence

**High confidence**: The Siamese network architecture and cross-loss training strategy are well-established and properly implemented

**Medium confidence**: The soft label generation process using GMM is conceptually sound but specific hyperparameters are not fully specified, which could affect reproducibility

**Medium confidence**: The reported performance metrics appear reasonable given the methodology, though the linear evaluation protocol details are somewhat limited

## Next Checks

1. Conduct ablation studies to assess the impact of different GMM hyperparameters (number of components, covariance type) on soft label quality and downstream task performance

2. Evaluate the method on additional medical imaging datasets from different modalities (e.g., chest X-rays, CT scans) to test generalizability beyond mammograms

3. Implement and test a variant of the approach using hard labels (0/1) instead of soft labels to quantify the benefit of the soft label strategy