---
ver: rpa2
title: Cross-lingual paraphrase identification
arxiv_id: '2406.15066'
source_url: https://arxiv.org/abs/2406.15066
tags:
- negative
- embeddings
- dataset
- paraphrase
- similarity
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work proposes a bi-encoder model for multilingual paraphrase
  identification using contrastive learning. The approach leverages LaBSE embeddings
  and modifies Additive Margin Softmax loss with hard-negative mining via mega-batching
  to improve performance.
---

# Cross-lingual paraphrase identification

## Quick Facts
- arXiv ID: 2406.15066
- Source URL: https://arxiv.org/abs/2406.15066
- Authors: Inessa Fedorova; Aleksei Musatow
- Reference count: 27
- This work proposes a bi-encoder model for multilingual paraphrase identification using contrastive learning

## Executive Summary
This paper introduces a bi-encoder model for cross-lingual paraphrase identification that leverages LaBSE embeddings and modifies Additive Margin Softmax loss with hard-negative mining via mega-batching. The approach achieves paraphrase identification accuracy within 7-10% of state-of-the-art cross-encoders while being significantly faster and enabling semantic search through its embeddings. Evaluated on the PAWS-X dataset across 5 languages, the model demonstrates strong performance while maintaining the efficiency advantages of bi-encoder architectures.

## Method Summary
The method uses LaBSE as a base bi-encoder model with its embedding layer frozen, combined with a modified Additive Margin Scale Loss that incorporates hard-negative mining through mega-batching. The training process aggregates multiple mini-batches to increase the pool of negative examples, then selects the most challenging negatives for training. The model is evaluated on the PAWS-X dataset using accuracy metrics and align/uniform losses to assess embedding quality, with results showing competitive performance compared to cross-encoder approaches while maintaining the efficiency benefits of bi-encoders.

## Key Results
- Achieves paraphrase identification accuracy within 7-10% of state-of-the-art cross-encoders
- Enables semantic search through learned embeddings while maintaining fast inference
- Demonstrates effective cross-lingual transfer across 5 languages in PAWS-X dataset

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Additive Margin Scale Loss improves separation between positive and negative pairs in the embedding space.
- Mechanism: The loss function adds a margin term to the cosine similarity between positive pairs, and scales both positive and negative terms to control their relative impact. This pushes positive pairs closer and negative pairs farther apart in the embedding space.
- Core assumption: Increasing the margin between positive and negative pairs improves the discriminative power of the embeddings for semantic similarity tasks.
- Evidence anchors:
  - [abstract] "We modify Additive Margin Softmax Loss and sample hard-negatives to make the model's training process more effective"
  - [section] "Additive Margin Softmax extends the scoring function by the large margin which improves the separation between positive and negative examples"
  - [corpus] Weak evidence - no direct citations to this specific mechanism, but related works on margin-based contrastive learning exist.
- Break condition: If the margin is too large, the model may not learn fine-grained similarities, or if too small, the separation may not be sufficient for downstream tasks.

### Mechanism 2
- Claim: Mega-batching with hard negative mining improves the effectiveness of contrastive learning.
- Mechanism: Instead of using only in-batch negatives, the model aggregates multiple mini-batches (mega-batch) to increase the pool of negative examples. Within this larger pool, it selects the hardest negatives (those most similar to the anchor) to provide stronger training signals.
- Core assumption: Training on harder negative examples leads to more robust and discriminative embeddings.
- Evidence anchors:
  - [section] "Using only in-batch negatives is not the most effective or complex way of model training... To address this, the procedure called 'mega-batching' was proposed"
  - [section] "We decided to make mega-batching more complex and select the most challenging examples for the model"
  - [corpus] Weak evidence - the concept of hard negative mining is mentioned but specific details are not cited in related works.
- Break condition: If hard negatives are too similar to positives, the model may confuse them; if too dissimilar, the training signal may be weak.

### Mechanism 3
- Claim: LaBSE embeddings provide a strong multilingual foundation that can be fine-tuned for paraphrase identification.
- Mechanism: LaBSE is pre-trained on 109 languages using a dual-encoder architecture with contrastive loss, learning language-agnostic sentence embeddings. Fine-tuning on PAWS-X adapts these embeddings to the specific task of paraphrase identification while preserving cross-lingual transfer capabilities.
- Core assumption: Pre-trained multilingual embeddings contain generalizable semantic representations that can be adapted to downstream tasks with relatively small task-specific datasets.
- Evidence anchors:
  - [abstract] "As the base bi-encoder model we decided to use LaBSE"
  - [section] "The LaBSE model is designed to learn language-agnostic sentence embeddings... It supports 109 languages and achieves state-of-the-art performance"
  - [corpus] Moderate evidence - related works cite LaBSE as a strong baseline for cross-lingual tasks, though specific paraphrase identification performance is not well-documented.
- Break condition: If the task distribution in PAWS-X is too different from LaBSE's pre-training data, fine-tuning may not yield significant improvements.

## Foundational Learning

- Concept: Contrastive learning and metric learning
  - Why needed here: The entire approach relies on learning embeddings where semantically similar sentences are close and dissimilar ones are far apart, which is fundamentally a metric learning problem solved through contrastive learning.
  - Quick check question: What is the difference between contrastive loss and classification loss in terms of what they optimize for in embedding space?

- Concept: Bi-encoder vs. cross-encoder architectures
  - Why needed here: The paper explicitly compares bi-encoders (faster, allows semantic search) to cross-encoders (more accurate but slower), and the choice impacts both performance and deployment.
  - Quick check question: Why can bi-encoders enable semantic search while cross-encoders cannot, given the same embedding space?

- Concept: Hard negative mining strategies
  - Why needed here: The paper modifies standard mega-batching to select the most challenging negatives, which is crucial for effective contrastive learning.
  - Quick check question: How does selecting hard negatives differ from random negative sampling in terms of the gradients produced during training?

## Architecture Onboarding

- Component map: PAWS-X dataset -> LaBSE encoder -> Modified loss computation -> Hard negative selection -> Model update -> Evaluation
- Critical path: Data → LaBSE encoder → Modified loss computation → Hard negative selection → Model update → Evaluation
- Design tradeoffs:
  - Speed vs. accuracy: Bi-encoders are ~7-10% less accurate than cross-encoders but enable O(1) inference per sentence pair vs O(n²)
  - Complexity vs. performance: Hard negative mining adds computational overhead but improves embedding quality
  - Language coverage vs. task specificity: Using LaBSE enables zero-shot transfer but may limit task-specific optimization
- Failure signatures:
  - Accuracy plateaus early: May indicate insufficient hard negative mining or margin too small
  - Embeddings not language-agnostic: Could mean pre-training knowledge not being preserved (freezing embeddings layer too aggressively)
  - Training instability: May result from improper scaling parameters in the modified loss
- First 3 experiments:
  1. Baseline test: Train with standard in-batch negatives only, no hard mining, to establish performance floor
  2. Margin sweep: Vary margin parameter (0.1 to 1.0) to find optimal separation between positive and negative pairs
  3. Hard negative threshold: Experiment with different thresholds for selecting hard negatives to balance training difficulty

## Open Questions the Paper Calls Out
None

## Limitations
- The exact implementation details of the modified Additive Margin Scale Loss are not fully specified, including specific ArcFace features incorporated
- The hard-negative mining approach lacks precise selection criteria, whether based on similarity thresholds or top-N selection
- Evaluation is limited to only 5 languages (en, fr, es, de, zh), restricting generalizability to the full 109 languages supported by LaBSE

## Confidence
- **High confidence**: The core mechanism of using LaBSE embeddings with modified contrastive loss for cross-lingual tasks is well-established in the literature
- **Medium confidence**: The specific modifications to Additive Margin Softmax with hard-negative mining are plausible but exact implementation details could significantly impact performance
- **Low confidence**: The claim of "state-of-the-art" performance is relative to the specific comparison set and the 7-10% gap to cross-encoders may be problematic for production applications

## Next Checks
1. **Implementation verification**: Reproduce the modified Additive Margin Scale Loss with the exact parameter settings (margin=0.5, scale=0.5, gamma=1) and verify that the loss behaves as expected on synthetic data with known positive and negative pairs.

2. **Generalization test**: Evaluate the model on languages not included in PAWS-X training (e.g., ja, ko, ar) to assess true cross-lingual transfer capabilities beyond the 5 evaluation languages.

3. **Threshold sensitivity analysis**: Systematically sweep the decision threshold across the full range [0,1] on the development set to find optimal accuracy, rather than using a single threshold value, and report the resulting performance distribution.