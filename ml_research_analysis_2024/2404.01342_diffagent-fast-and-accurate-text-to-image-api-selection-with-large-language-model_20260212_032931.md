---
ver: rpa2
title: 'DiffAgent: Fast and Accurate Text-to-Image API Selection with Large Language
  Model'
arxiv_id: '2404.01342'
source_url: https://arxiv.org/abs/2404.01342
tags:
- diffagent
- human
- apis
- prompts
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: DiffAgent is an LLM agent designed to quickly and accurately select
  appropriate text-to-image (T2I) generation APIs from a vast pool of models and parameters.
  It uses a two-stage training framework (SFTA) that first fine-tunes the LLM on a
  large dataset of user prompts and T2I API pairs, then further aligns it with human
  preferences using a ranking-based approach.
---

# DiffAgent: Fast and Accurate Text-to-Image API Selection with Large Language Model

## Quick Facts
- arXiv ID: 2404.01342
- Source URL: https://arxiv.org/abs/2404.01342
- Reference count: 39
- Primary result: LLM agent with two-stage training framework achieves over 40% improvement for SD 1.5 models and significant gains in human preference metrics across multiple evaluation datasets

## Executive Summary
DiffAgent is an LLM agent designed to quickly and accurately select appropriate text-to-image generation APIs from a vast pool of models and parameters. It addresses the challenge of finding suitable personalized models for specific styles among thousands of options, which typically requires multiple trial-and-error attempts. The method demonstrates substantial performance improvements over baseline T2I models, achieving significant gains in human preference metrics across multiple evaluation datasets.

## Method Summary
DiffAgent uses a two-stage training framework (SFTA) that first fine-tunes the LLM on a large dataset of user prompts and T2I API pairs, then further aligns it with human preferences using a ranking-based approach. The first stage employs supervised fine-tuning (SFT) on DABench to teach the LLM API call patterns. The second stage uses RRHF (Rank Responses with Human Feedback) to refine API selection based on a unified human-preference metric combining CLIP Score, ImageReward, and HPS v2. This approach addresses hallucination errors and improves selection accuracy for specific styles among thousands of available models.

## Key Results
- Achieves over 40% improvement for SD 1.5 models compared to baseline approaches
- Demonstrates significant gains in human preference metrics across COCO Caption and Parti Prompts datasets
- Successfully addresses the challenge of selecting appropriate T2I APIs from a vast pool of models and parameters

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The two-stage training framework (SFTA) enables the LLM agent to first learn API call patterns and then refine them based on human preference scores.
- Mechanism: In the first stage, supervised fine-tuning (SFT) on DABench teaches the LLM to generate syntactically valid API calls from prompts. In the second stage, RRHF fine-tunes the LLM to rank and select API responses that maximize a unified human-preference metric, reducing hallucination errors.
- Core assumption: Human preference scores derived from CLIP Score, ImageReward, and HPS v2 can serve as effective reward signals for LLM alignment.
- Evidence anchors:
  - [abstract] "DiffAgent leverages a novel two-stage training framework, SFTA, enabling it to accurately align T2I API responses with user input in accordance with human preferences."
  - [section] "To address this limitation, we further fine-tune the LLM with the RRHF algorithm by incorporating various image assessment scores aligned with human preference as the reward model function."
  - [corpus] Weak: No direct citations; FMR scores suggest moderate relatedness but no explicit evaluation overlap.
- Break condition: If the reward model fails to correlate with actual human judgments, the RRHF stage would not improve selection accuracy.

### Mechanism 2
- Claim: The unified metric combining CLIP Score, ImageReward, and HPS v2 provides a robust evaluation of API response quality and relevance.
- Mechanism: Each metric captures a different aspect of human preference (text-image alignment, reward-based assessment, and preference prediction). Normalizing and averaging them yields a composite score used both for evaluation and as the reward signal during RRHF.
- Core assumption: These three metrics, despite measuring different properties, can be meaningfully combined without distorting relative rankings.
- Evidence anchors:
  - [section] "We evaluate DiffAgent using a human preference-related unified metric composed of: First, CLIP Score... Second, ImageReward... Third, Human Preference Score v2 (HPS v2)... The final score... is obtained by averaging all image's scores."
  - [section] "The Unified Metric S(t, r) describes the human preference score for an API response r given a prompt t. The Eq. (5) serves a dual purpose, as it can be utilized for both API evaluation and as a reward model function in alignment with human preferences."
  - [corpus] Weak: No direct citations; FMR scores suggest moderate relatedness but no explicit evaluation overlap.
- Break condition: If one metric dominates the average, the unified score may lose sensitivity to other important aspects.

### Mechanism 3
- Claim: Generating multiple API responses and filtering them ensures diversity and validity before evaluation.
- Mechanism: The system uses multinomial sampling to produce a comprehensive response and diverse beam search to generate multiple candidates. It then validates and reconstructs these to produce a final list of API calls, which are executed and scored.
- Core assumption: Diverse sampling strategies combined with validation filters reduce the risk of generating invalid or repetitive API calls.
- Evidence anchors:
  - [section] "For text prompts t, we employ multinomial sampling to get a comprehensive API response and utilize diverse beam search [30] to get m diverse responses from SFT model π."
  - [section] "However, it is difficult to ensure the format and validity of multiple different responses. Therefore, we only take the model information part of responses, then proceed to validate and reconstruct the complete API response information."
  - [corpus] Weak: No direct citations; FMR scores suggest moderate relatedness but no explicit evaluation overlap.
- Break condition: If validation filtering is too strict, it may discard valid diverse options; if too lenient, it may allow invalid calls.

## Foundational Learning

- Concept: Text-to-image diffusion models (e.g., Stable Diffusion)
  - Why needed here: DiffAgent relies on selecting and configuring SD models and LoRA variants to generate images matching user prompts.
  - Quick check question: What are the two main SD architectures used in the Civitai community according to the paper?
  - Answer: SD 1.5 and SD XL 1.0.

- Concept: Large language model fine-tuning (SFT and RLHF variants)
  - Why needed here: The LLM is fine-tuned first with supervised learning on API-call pairs, then aligned with human preferences via a ranking-based RLHF variant.
  - Quick check question: Which two-stage training method is used to align the LLM with human preferences after SFT?
  - Answer: RRHF (Rank Responses to Align Human Feedback).

- Concept: Reward modeling and ranking-based alignment
  - Why needed here: Instead of standard RLHF, the paper uses a ranking loss over multiple API responses scored by a unified metric, allowing fine-grained preference alignment without full reward model likelihood.
  - Quick check question: What is the ranking loss formula used in the RRHF stage?
  - Answer: L_rank = Σ_{s_i < s_j} max(0, p_i - p_j).

## Architecture Onboarding

- Component map: DABench -> DiffAgent-SFT (SFT fine-tuning) -> DiffAgent-RRHF (RRHF fine-tuning) -> Reward model (CLIP Score + ImageReward + HPS v2) -> API generator (multinomial sampling + diverse beam search) -> Validation filter -> API execution

- Critical path: Input prompt → API generation (sampling + search) → API validation → Image generation → Scoring via unified metric → Selection of best API

- Design tradeoffs:
  - Sampling vs. search: Multinomial sampling yields comprehensive but potentially redundant responses; diverse beam search ensures variety but may miss optimal calls
  - Metric normalization: Balancing three different metrics without one dominating requires careful scaling

- Failure signatures:
  - High hallucination error in API generation (model names or parameters invalid)
  - Low diversity in generated API responses (sampling not effective)
  - Reward model scores not correlating with human judgments

- First 3 experiments:
  1. Validate API generation on a small prompt set; measure hallucination rate before/after RRHF
  2. Test unified metric stability by varying random seeds and comparing score variance
  3. Compare DiffAgent-SFT vs DiffAgent-RRHF on a held-out DABench subset to confirm RRHF benefit

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of DiffAgent scale with larger model sizes (e.g., LLaMA-2-13B or LLaMA-2-70B) compared to LLaMA-2-7B?
- Basis in paper: [inferred] The paper only evaluates DiffAgent with LLaMA-2-7B and mentions that larger models could be used.
- Why unresolved: The paper does not provide empirical results for larger model sizes.
- What evidence would resolve it: Training and evaluating DiffAgent with larger LLaMA-2 variants on the same benchmarks (DABench, COCO Caption, Parti Prompts) to compare performance metrics (unified metric scores, hallucination errors, inference time).

### Open Question 2
- Question: How does DiffAgent perform on other types of generative tasks beyond text-to-image, such as text-to-video or text-to-3D?
- Basis in paper: [inferred] The paper focuses on text-to-image generation but the SFTA framework could potentially be applied to other generative domains.
- Why unresolved: The paper only evaluates DiffAgent on text-to-image generation tasks.
- What evidence would resolve it: Applying the DiffAgent methodology to other generative tasks (e.g., text-to-video, text-to-3D) and comparing performance against baseline models in those domains.

### Open Question 3
- Question: How does DiffAgent handle real-time user feedback during the image generation process, rather than just initial prompt-based selection?
- Basis in paper: [explicit] The paper mentions that DiffAgent can free users from tedious attempts and matching, but doesn't discuss iterative refinement.
- Why unresolved: The paper focuses on initial API selection rather than interactive refinement based on user feedback.
- What evidence would resolve it: Implementing a system where DiffAgent can receive user feedback on generated images and adjust subsequent API selections accordingly, then measuring improvements in user satisfaction and image quality over multiple iterations.

## Limitations

- The paper relies on a unified metric combining three evaluation scores without clear validation that these metrics correlate with human preferences across diverse prompt domains
- RRHF implementation details remain underspecified, particularly regarding reward model hyperparameters and sampling strategies during training
- The paper does not provide ablation studies isolating the contribution of each training stage or metric component to overall performance improvements

## Confidence

- **High Confidence**: The two-stage training framework concept (SFTA) and its general effectiveness in improving API selection accuracy, supported by quantitative improvements on evaluation datasets
- **Medium Confidence**: The specific choice of combining CLIP Score, ImageReward, and HPS v2 into a unified metric, as the paper demonstrates improved performance but lacks thorough validation of metric correlations
- **Medium Confidence**: The hallucination reduction mechanism through RRHF fine-tuning, though the extent of improvement depends on reward model quality which is not fully characterized

## Next Checks

1. **Metric Correlation Validation**: Conduct human preference studies comparing the unified metric rankings against actual human judgments across multiple prompt categories to verify that the combined score accurately reflects human preferences

2. **RRHF Hyperparameter Sensitivity**: Systematically vary the reward model temperature, learning rate, and ranking margin parameters in the RRHF stage to determine their impact on API selection accuracy and hallucination rates

3. **Component Ablation Analysis**: Remove each metric (CLIP Score, ImageReward, HPS v2) from the unified score and retrain DiffAgent-RRHF to quantify the individual contribution of each component to overall performance improvements