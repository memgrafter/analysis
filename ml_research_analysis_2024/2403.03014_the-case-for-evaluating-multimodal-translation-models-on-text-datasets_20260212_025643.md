---
ver: rpa2
title: The Case for Evaluating Multimodal Translation Models on Text Datasets
arxiv_id: '2403.03014'
source_url: https://arxiv.org/abs/2403.03014
tags:
- translation
- against
- multi30k
- sets
- test
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper highlights a critical gap in the evaluation of multimodal
  machine translation (MMT) models. Current evaluation practices rely heavily on the
  Multi30k dataset, which contains short image captions rather than complex, real-world
  sentences.
---

# The Case for Evaluating Multimodal Translation Models on Text Datasets

## Quick Facts
- arXiv ID: 2403.03014
- Source URL: https://arxiv.org/abs/2403.03014
- Reference count: 6
- Primary result: MMT models trained on Multi30k perform poorly on WMT test sets, suggesting they don't effectively use visual context

## Executive Summary
This paper identifies a critical gap in multimodal machine translation (MMT) evaluation practices. Current evaluations rely heavily on the Multi30k dataset of short image captions, but these models fail dramatically when tested on complex sentences from news domains. The authors propose a three-part evaluation framework including the CoMMuTE framework for assessing visual information use, WMT news translation test sets for evaluating complex sentences, and Multi30k test sets for baseline MMT performance. Their evaluation of recent MMT models (Gated Fusion, RMMT) reveals that while these models perform well on Multi30k, they achieve BLEU scores as low as 1.3 on WMT test sets compared to text-only models scoring above 36, indicating poor generalization and ineffective use of visual context.

## Method Summary
The study trains MMT models (Gated Fusion and RMMT) exclusively on the Multi30k dataset and evaluates them using a proposed framework. The framework includes calculating BLEU4 scores on Multi30k test sets (Test2016, Test2017) and WMT test sets (newstest2019, newstest2020), plus computing CoMMuTE scores to measure visual information usage. The authors compare performance against text-only models to assess whether multimodal models provide any advantage. The evaluation reveals significant performance drops on WMT datasets, suggesting that current MMT models overfit to short image captions and fail to generalize to real-world translation tasks.

## Key Results
- MMT models trained on Multi30k achieve BLEU scores above 36 on Multi30k test sets but drop to as low as 1.3 on WMT test sets
- Gated Fusion models show identical performance regardless of whether correct or random images are provided, indicating they're not using visual information
- Out of approximately 8200 unique words in newstest2020, 5280 words are not in the Multi30k training set, highlighting vocabulary mismatch issues

## Why This Works (Mechanism)

### Mechanism 1
- Claim: MMT models trained on Multi30k overfit to short image captions and fail to generalize to complex sentences from news domains.
- Mechanism: Multi30k contains image captions averaging 12 words, while WMT news test sets contain sentences averaging 34 words with broader vocabulary and syntactic complexity. Training exclusively on Multi30k leads to overfitting on caption-style sentences and poor adaptation to news-style sentences.
- Core assumption: The domain mismatch between training and evaluation data causes performance degradation when testing on text-only WMT datasets.
- Evidence anchors:
  - [abstract]: "current MMT models are not effectively using visual context and lack generalization to real-world translation tasks"
  - [section]: "we found the drop in performance against the newstest test sets to be very large, making the two models impractical for real-world usage"
  - [corpus]: Weak evidence - related papers focus on Multi30k but don't directly test generalization to WMT datasets
- Break condition: If MMT models are trained on both Multi30k and WMT datasets, the domain mismatch effect would diminish, and performance on WMT would improve.

### Mechanism 2
- Claim: Gated Fusion models learn to ignore visual information when trained on Multi30k, treating images as random noise.
- Mechanism: When visual context doesn't provide disambiguating information in Multi30k captions, the gating mechanism learns to downweight image features, resulting in text-only performance despite multimodal architecture.
- Core assumption: Visual context is not essential for translating unambiguous captions, so models optimize away from using images.
- Evidence anchors:
  - [abstract]: "many MMT models do not make use of the visual context (Caglayan et al., 2019)"
  - [section]: "for the Gated Fusion model, we found performance to be identical no matter which image was associated with the sentence, which indicates that the model is not be using image information to a significant degree"
  - [corpus]: Moderate evidence - Wu et al. (2021) discusses regularization effects and ignoring multimodal information
- Break condition: If CoMMuTE-style evaluation with ambiguous sentences is included in training, the model would be forced to use visual context, preventing this behavior.

### Mechanism 3
- Claim: The BPE preprocessing amplifies overfitting when training data is small and domain-limited.
- Mechanism: BPE creates subword units based on Multi30k training distribution, resulting in vocabulary that poorly represents WMT test sets. This creates compounding effects where tokenization mismatches hurt translation quality.
- Core assumption: BPE optimization for one domain transfers poorly to another domain with different word distributions.
- Evidence anchors:
  - [section]: "overfitting on the very small Multi30k training set via both the training process and the BPE preprocessing (29,000 examples)"
  - [section]: "out of approximately 8200 unique words in newstest2020, 5280 words are not in Multi30k training set"
  - [corpus]: Weak evidence - no direct corpus evidence about BPE effects, but implied by vocabulary mismatch statistics
- Break condition: If models use character-level tokenization or are trained on larger, more diverse datasets, BPE-induced overfitting would be reduced.

## Foundational Learning

- Concept: Domain adaptation in machine translation
  - Why needed here: The paper demonstrates that models trained on one domain (image captions) fail on another (news text), highlighting the importance of understanding how domain differences affect model performance.
  - Quick check question: What is the key difference between Multi30k and WMT test sets that causes the performance drop?

- Concept: Multimodal fusion mechanisms
  - Why needed here: Understanding how visual and textual information are combined (gating, attention, retrieval) is essential for diagnosing why models ignore visual context.
  - Quick check question: How does a gating mechanism decide how much weight to give visual vs. textual information?

- Concept: Evaluation metrics for translation quality
  - Why needed here: BLEU scores are used to quantify the performance drop, and understanding how BLEU works helps interpret the results.
  - Quick check question: What does a BLEU score of 1.3 versus 36 indicate about translation quality?

## Architecture Onboarding

- Component map: Text encoder -> Image encoder -> Fusion module (gating or retrieval) -> Decoder
- Critical path: Data preprocessing → BPE tokenization → model forward pass (text+image or text-only) → decoding → BLEU calculation against reference translations
- Design tradeoffs: Multimodal models have higher computational cost and complexity but may improve disambiguation; text-only models are simpler but miss visual context benefits
- Failure signatures: Identical performance with random vs. correct images (Gated Fusion), BLEU scores below 2 on WMT, high perplexity on CoMMuTE evaluation
- First 3 experiments:
  1. Train Gated Fusion on Multi30k, evaluate on Multi30k, newstest2019, newstest2020 to replicate performance drop.
  2. Train RMMT on Multi30k, compare text-only vs. multimodal input performance on WMT datasets.
  3. Implement CoMMuTE evaluation on both models to measure visual context utilization.

## Open Questions the Paper Calls Out

The paper does not explicitly call out specific open questions. However, based on the content, several implicit open questions emerge from the findings and implications of the study.

## Limitations

- Limited evaluation to only two MMT architectures (Gated Fusion and RMMT) may not represent the full spectrum of multimodal translation approaches
- Performance drop could be partially attributed to domain shift rather than solely to visual information underutilization
- The study doesn't investigate whether visual context in Multi30k is inherently less useful for translation compared to more ambiguous real-world scenarios

## Confidence

**High Confidence:** The empirical finding that MMT models trained only on Multi30k perform significantly worse on WMT test sets is robust. The BLEU score comparison (1.3 vs. 36+) provides clear quantitative evidence of domain-specific overfitting and generalization failure.

**Medium Confidence:** The interpretation that this performance gap indicates MMT models aren't using visual context effectively. While the CoMMuTE evaluation supports this, alternative explanations include the possibility that visual information simply isn't necessary for the types of sentences in Multi30k, or that the models have learned to rely primarily on textual context.

**Low Confidence:** The claim that current MMT models are "impractical for real-world usage" based solely on performance on WMT test sets. This extrapolation assumes that the primary use case for MMT is translating news articles, which may not reflect actual deployment scenarios where visual disambiguation could still provide value.

## Next Checks

1. **Domain Adaptation Study:** Train MMT models on both Multi30k and WMT datasets, then evaluate on held-out WMT test sets to determine whether domain adaptation can close the performance gap observed in this study.

2. **Visual Context Quality Analysis:** Conduct human evaluation studies to assess whether the visual context provided in Multi30k is genuinely useful for translation tasks, or whether the images are too predictable to provide meaningful disambiguation signals.

3. **Alternative Architecture Comparison:** Implement and evaluate additional MMT architectures (e.g., attention-based fusion, retrieval-based methods) on the proposed framework to determine whether the performance issues are architecture-specific or represent broader challenges in multimodal translation.