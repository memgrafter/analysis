---
ver: rpa2
title: 'Verifying Relational Explanations: A Probabilistic Approach'
arxiv_id: '2401.02703'
source_url: https://arxiv.org/abs/2401.02703
tags:
- graph
- explanation
- explanations
- factor
- uncertainty
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a probabilistic approach for verifying relational
  explanations from Graph Neural Networks (GNNs), specifically focusing on GNNExplainer.
  The key challenge addressed is that traditional explanation verification methods
  relying on human subjects are not scalable for complex relational data represented
  as graphs.
---

# Verifying Relational Explanations: A Probabilistic Approach

## Quick Facts
- **arXiv ID**: 2401.02703
- **Source URL**: https://arxiv.org/abs/2401.02703
- **Reference count**: 25
- **Primary result**: Presents a probabilistic approach for verifying relational explanations from GNNs by generating symmetrical counterfactual examples and learning a factor graph model to quantify uncertainty

## Executive Summary
This paper addresses the challenge of verifying relational explanations from Graph Neural Networks (GNNs) by developing a probabilistic approach that overcomes the scalability limitations of human-based verification methods. The key insight is that traditional explanation verification methods relying on human subjects are not scalable for complex relational data represented as graphs. To overcome this, the authors propose generating counterfactual examples through low-rank Boolean matrix factorization to approximate symmetrical structures of the original graph. They then learn a factor graph model from these counterfactual explanations to quantify uncertainty in the original explanation. The approach uses belief propagation for inference over the factor graph and measures uncertainty reduction when injecting the original explanation. Experiments on multiple benchmark datasets demonstrate that their uncertainty estimates are statistically more reliable than those from GNNExplainer, as verified by McNemar's test showing significant differences in prediction when removing high-uncertainty relations.

## Method Summary
The method involves training a GCN on the input relational graph for node classification, then generating explanations for target nodes using GNNExplainer. Low-rank Boolean approximations of the original graph are computed via Boolean Matrix Factorization (BMF) using NIMFA, with rank incrementally increased until approximation error falls below 5% of total edges. For each approximation, a symmetric counterfactual graph is generated and GNNExplainer explanations are obtained. A factor graph is built from these symmetric counterfactual explanations, with weights initialized using GNNExplainer confidence scores and learned via gradient descent. Belief propagation calibrates the factor graph to estimate marginal distributions, then re-calibrates after injecting the original explanation to compute uncertainty reduction scores for each relation. The approach is evaluated using McNemar's test to assess statistical significance of uncertainty estimates compared to GNNExplainer's direct scores.

## Key Results
- Uncertainty estimates from the factor graph model are statistically more reliable than those from GNNExplainer, as verified by McNemar's test
- The approach demonstrates significant differences in prediction when removing high-uncertainty relations from explanations
- Performance validated across multiple benchmark datasets including BAShapes, BACommunity, TreeGrid, TreeCycle, Cora, Citeseer, and WebKB

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Low-rank Boolean matrix factorization creates symmetric counterfactual examples that approximate real-world symmetries, improving uncertainty estimation.
- Mechanism: The approach uses Boolean matrix factorization to approximate the original graph's relation matrix with fewer patterns than its Boolean rank. This creates symmetrical structures that serve as counterfactual examples. These symmetrical structures are hypothesized to be more likely representations of real-world data variations, making the learned distribution over explanations more representative of actual uncertainty.
- Core assumption: Real-world relational data exhibits symmetries that can be captured through low-rank approximations, and these symmetries correspond to more probable counterfactual variations.
- Evidence anchors:
  - [abstract] "We generate these examples as symmetric approximations of the relational structure in the original data."
  - [section] "To learn such a distribution over symmetric counterfactual explanations, we perform a Boolean factorization of the relations specified in the original graph and learn low-rank approximations for them."
  - [corpus] Weak - the corpus doesn't contain specific evidence about the effectiveness of Boolean factorization for symmetry approximation.
- Break condition: If real-world graphs don't exhibit meaningful symmetries, or if the Boolean factorization fails to capture relevant variations, the approach would lose its advantage over random counterfactual generation.

### Mechanism 2
- Claim: Factor graph learning from symmetrical explanations quantifies uncertainty through learned weights that reflect explanation importance.
- Mechanism: The approach constructs a factor graph where each factor represents a logical relationship between entities in the explanation. Weights are initialized using GNNExplainer confidence scores and refined through gradient descent to maximize likelihood over the set of symmetrical counterfactual explanations. Higher weights indicate more reliable relationships.
- Core assumption: The distribution of explanations across symmetrical counterfactuals can be modeled as a log-linear factor graph where weights reflect explanation reliability.
- Evidence anchors:
  - [abstract] "From these explanations, we learn a factor graph model to quantify uncertainty in an explanation."
  - [section] "Let {Ri}Ki=1 be the union of all binary relations specified in the explanations in S. The probability distribution over S is defined as a log-linear model as follows."
  - [corpus] Weak - the corpus doesn't provide specific evidence about factor graph effectiveness for uncertainty quantification.
- Break condition: If the assumption that symmetrical explanations follow a log-linear distribution is incorrect, or if the learning process fails to converge to meaningful weights, the uncertainty estimates would be unreliable.

### Mechanism 3
- Claim: Belief propagation calibration combined with explanation injection measures uncertainty reduction effectively.
- Mechanism: The approach uses belief propagation to calibrate the factor graph and compute marginal probabilities. When an explanation is injected (by adding factors with weights equal to GNNExplainer confidence), the reduction in uncertainty is measured by comparing joint distributions before and after calibration. Larger reductions indicate more reliable explanations.
- Core assumption: The difference in joint probabilities before and after explanation injection accurately reflects the reduction in uncertainty about the explanation's validity.
- Evidence anchors:
  - [abstract] "Our results on several datasets show that our approach can help verify explanations from GNNExplainer by reliably estimating the uncertainty of a relation specified in the explanation."
  - [section] "We estimate probabilities from the factor graph denoted by F that we learn from the symmetric CREs S. Intuitively, a relation Qi is important in E(G, Φ, Y ) if it is important in S."
  - [corpus] Weak - the corpus doesn't provide specific evidence about the effectiveness of this particular uncertainty reduction measurement.
- Break condition: If belief propagation fails to converge properly, or if the relationship between probability differences and uncertainty is not well-calibrated, the uncertainty estimates would be unreliable.

## Foundational Learning

- Concept: Graph Neural Networks (GNNs) and their explanation methods
  - Why needed here: The paper builds on GNNExplainer as the baseline method being improved. Understanding how GNNs work and how GNNExplainer generates subgraph explanations is essential to grasp the contribution.
  - Quick check question: How does GNNExplainer formulate the explanation objective, and what does it optimize for?

- Concept: Factor graphs and belief propagation
  - Why needed here: The core verification mechanism relies on learning a factor graph from counterfactual explanations and using belief propagation for inference. Understanding factor graph structure, message passing, and calibration is crucial.
  - Quick check question: What are the two types of messages in belief propagation for factor graphs, and how are they computed?

- Concept: Boolean matrix factorization and low-rank approximations
  - Why needed here: The approach uses Boolean matrix factorization to generate symmetrical counterfactual examples. Understanding how Boolean operations differ from standard matrix operations and how low-rank approximations create symmetries is important.
  - Quick check question: How does Boolean matrix factorization differ from standard non-negative matrix factorization, and what role does the Boolean rank play?

## Architecture Onboarding

- Component map: Graph → Boolean factorization → Counterfactual generation → Factor graph learning → Belief propagation → Uncertainty measurement → McNemar's test verification
- Critical path: The approach takes a trained GNN and its explanation for a target node, generates counterfactual graphs through Boolean factorization, learns a factor graph from these counterfactuals, calibrates using belief propagation, and measures uncertainty reduction when injecting the original explanation.
- Design tradeoffs:
  - Low-rank approximation rank selection: Higher rank captures more structure but increases computational cost and may reduce symmetry benefits
  - Factor graph complexity: More factors provide finer-grained uncertainty estimates but increase learning and inference time
  - Belief propagation iterations: More iterations improve convergence but increase runtime
- Failure signatures:
  - Low McNemar's test significance across all classes suggests the uncertainty estimates aren't capturing meaningful information
  - Factor graph weight learning failure (weights don't converge or remain uniform) indicates poor counterfactual distribution modeling
  - Belief propagation non-convergence suggests problematic factor graph structure
- First 3 experiments:
  1. Run on BAShapes with a simple GCN, verify that the approach produces higher McNemar's statistics than GNNExplainer for at least one class
  2. Test sensitivity to rank selection by varying the initial rank and observing changes in uncertainty estimates and McNemar's statistics
  3. Evaluate on Cora dataset with a more complex GCN to verify scalability and performance on larger, real-world graphs

## Open Questions the Paper Calls Out

- **Question**: How do the uncertainty estimates from the factor graph model compare to those from GNNExplainer when the GNN model has low accuracy?
- **Basis**: The paper states that for the WebKB dataset, the accuracy of the GCN model was significantly lower (between 50-60%), and it indicates that when the GNN has poor performance, explanations may be harder to verify.
- **Why unresolved**: The paper only presents results for one dataset with low GNN accuracy and does not systematically investigate the relationship between GNN accuracy and the reliability of uncertainty estimates.
- **What evidence would resolve it**: Conduct experiments on multiple datasets with varying GNN accuracy levels to determine if there is a correlation between GNN accuracy and the reliability of uncertainty estimates from the factor graph model compared to GNNExplainer.

- **Question**: Can the uncertainty quantification approach be extended to handle directed graphs?
- **Basis**: The paper assumes that the relationships in the graph are not directed and uses a low-rank Boolean matrix factorization approach to approximate the graph structure. This approach may not directly apply to directed graphs.
- **Why unresolved**: The paper does not discuss the applicability of the approach to directed graphs or propose any modifications to handle them.
- **What evidence would resolve it**: Extend the factor graph model and uncertainty quantification approach to handle directed graphs and evaluate its performance on benchmark datasets with directed graph structures.

- **Question**: How does the choice of the stopping criteria for the low-rank approximation affect the quality of the uncertainty estimates?
- **Basis**: The paper mentions that the rank is initialized to one and increased until the approximation error is below a stopping criteria or the rank plateaus. However, it does not discuss the impact of different stopping criteria on the quality of the uncertainty estimates.
- **Why unresolved**: The paper does not provide a systematic analysis of the impact of different stopping criteria on the uncertainty estimates.
- **What evidence would resolve it**: Conduct experiments with different stopping criteria for the low-rank approximation and compare the resulting uncertainty estimates to determine the optimal stopping criteria.

## Limitations

- The effectiveness of Boolean matrix factorization depends heavily on whether real-world graphs exhibit meaningful symmetries that can be captured through low-rank approximations
- The relationship between probability differences in the factor graph and actual uncertainty reduction is assumed rather than proven theoretically
- The computational complexity of generating multiple counterfactual examples through iterative rank increases may limit scalability to larger graphs

## Confidence

- **High confidence**: The experimental methodology (using McNemar's test to compare uncertainty estimates) is sound and the results showing statistically significant differences between approaches are robust.
- **Medium confidence**: The core mechanism of using symmetrical counterfactuals to improve uncertainty estimation is plausible given the results, but the underlying assumptions about graph symmetries lack strong theoretical or empirical justification.
- **Low confidence**: The specific implementation details of the Boolean factorization stopping criteria and factor graph learning parameters may significantly impact results, as these are underspecified in the paper.

## Next Checks

1. **Symmetry validation study**: Systematically vary the degree of symmetry in synthetic graphs (from fully symmetric to random) and measure how well the approach captures uncertainty compared to baseline methods. This would directly test the core assumption about symmetry's role.

2. **Factor graph sensitivity analysis**: Conduct ablation studies by varying the number of factors, weight initialization strategies, and belief propagation iteration counts to determine which components most affect uncertainty estimation quality.

3. **Cross-architecture generalization**: Apply the verification approach to explanations from other GNN architectures (GAT, GraphSAGE) and different explanation methods (Integrated Gradients, SHAP) to test whether the uncertainty quantification generalizes beyond GNNExplainer.