---
ver: rpa2
title: SPRING Lab IITM's submission to Low Resource Indic Language Translation Shared
  Task
arxiv_id: '2411.00727'
source_url: https://arxiv.org/abs/2411.00727
tags:
- language
- translation
- data
- training
- khasi
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper presents a translation model for four low-resource
  Indic languages: Khasi, Mizo, Manipuri, and Assamese. The authors leveraged existing
  datasets and employed back-translation techniques to expand the training corpus.'
---

# SPRING Lab IITM's submission to Low Resource Indic Language Translation Shared Task

## Quick Facts
- arXiv ID: 2411.00727
- Source URL: https://arxiv.org/abs/2411.00727
- Reference count: 11
- Best BLEU scores: 27.26 (En→As), 20.88 (Mb→En), 12.12 (En→Kh), 18.49 (Mz→En)

## Executive Summary
This paper presents a translation model for four low-resource Indic languages: Khasi, Mizo, Manipuri, and Assamese. The authors leverage existing datasets and employ back-translation techniques to expand the training corpus for languages with limited bilingual data. They fine-tune the pre-trained NLLB 3.3B model using LoRA (Low-Rank Adaptation) for efficient parameter adaptation. For Khasi, which was not supported by the NLLB model, special tokens were introduced and the model was trained on the Khasi corpus. The model achieved improved performance over the baseline, particularly for Khasi, addressing gaps in pre-trained model support for this language.

## Method Summary
The authors collected datasets from WMT, BPCC, PMIndia, and OpenLanguageData, then preprocessed them with punctuation normalization, Unicode normalization, and character filtering. For Mizo and Khasi, they generated back-translated monolingual data using Google Translate. The pre-trained NLLB 3.3B model was fine-tuned using LoRA with learning rate 1e-5, 8 epochs, and rank 128, targeting all linear modules. The training process involved three stages: masked language modeling for target language understanding, English-to-Indic fine-tuning, and Indic-to-English fine-tuning. For Khasi, special tokens were added to the tokenizer. Inference used beam search with 10 beams and repetition penalty of 2.5.

## Key Results
- Achieved best BLEU scores of 27.26 for English-to-Assamese, 20.88 for Manipuri-to-English, 12.12 for English-to-Khasi, and 18.49 for Mizo-to-English
- Demonstrated improved performance over baseline for all language pairs, particularly for Khasi which was not natively supported by NLLB
- Showed that LoRA fine-tuning can effectively adapt large pre-trained models for low-resource languages while maintaining efficiency

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Using LoRA fine-tuning on NLLB 3.3B reduces parameter adaptation while maintaining translation quality
- Mechanism: LoRA approximates weight updates with low-rank matrices, freezing most parameters and only training small adapters
- Core assumption: Low-rank approximation captures the necessary parameter changes for effective fine-tuning
- Evidence anchors:
  - [abstract] "We fine-tune the pre-trained NLLB 3.3B model for Assamese, Mizo, and Manipuri, achieving improved performance over the baseline."
  - [section] "LoRA has been shown to match the performance of traditional fine-tuning methods while reducing the number of trainable parameters by a factor of 50"
  - [corpus] Weak evidence - no specific corpus mentions LoRA effectiveness
- Break condition: If rank is too low to capture language-specific features, translation quality degrades

### Mechanism 2
- Claim: Back-translation expands training data for Mizo and Khasi, improving model performance
- Mechanism: Machine-translated monolingual data creates synthetic parallel pairs, increasing effective training corpus size
- Core assumption: Synthetic parallel data is sufficiently accurate to improve model learning
- Evidence anchors:
  - [abstract] "To address the scarcity of bilingual data, we use back-translation techniques on monolingual datasets for Mizo and Khasi, significantly expanding our training corpus."
  - [section] "The 'Back-Translated' data was initially generated using Google Translate for the first 500k characters from the monolingual WMT task data"
  - [corpus] Moderate evidence - corpus mentions back-translation was used but doesn't evaluate quality
- Break condition: If synthetic translations contain systematic errors, model learns incorrect patterns

### Mechanism 3
- Claim: Masked language modeling pre-training improves target language understanding
- Mechanism: MLM training forces model to learn contextual representations of the target language independently
- Core assumption: Learning language patterns through masked prediction transfers to translation tasks
- Evidence anchors:
  - [section] "The training process was conducted in three stages: first, the model was trained on masked language modelling to enhance its understanding of the target language by leveraging monolingual data."
  - [abstract] "Our training involves masked language modelling, followed by fine-tuning for English-to-Indic and Indic-to-English translations."
  - [corpus] Weak evidence - no corpus data on MLM effectiveness for these languages
- Break condition: If target language has significantly different structure from pre-training languages, MLM may not transfer well

## Foundational Learning

- Concept: Transformer architecture fundamentals
  - Why needed here: Understanding the base NLLB model architecture is crucial for knowing what's being fine-tuned
  - Quick check question: What are the key differences between encoder and decoder layers in a Transformer?

- Concept: Low-rank matrix approximation
  - Why needed here: LoRA's effectiveness depends on understanding how low-rank updates can approximate full parameter changes
  - Quick check question: How does the rank parameter in LoRA affect the expressiveness of the weight updates?

- Concept: Back-translation quality assessment
  - Why needed here: Synthetic data quality directly impacts model performance, requiring understanding of evaluation methods
  - Quick check question: What metrics would you use to assess the quality of back-translated data before using it for training?

## Architecture Onboarding

- Component map:
  NLLB 3.3B base model (pre-trained multilingual) -> LoRA adapters for each language pair -> Tokenizer with special tokens for Khasi -> Data preprocessing pipeline (Moses, Unicode normalization) -> Training scheduler with three-stage approach (MLM → En→Ind → Ind→En)

- Critical path: Data preprocessing → MLM pre-training → LoRA fine-tuning → Inference with beam search
- Design tradeoffs:
  - Larger rank in LoRA gives better performance but slower training
  - More back-translation data improves coverage but may introduce noise
  - More MLM epochs improve language understanding but increase training time
- Failure signatures:
  - Poor BLEU scores with high TER indicate translation quality issues
  - Disproportionate performance drop in one direction suggests training imbalance
  - High variance across runs suggests unstable fine-tuning
- First 3 experiments:
  1. Test baseline NLLB performance on each language pair without fine-tuning
  2. Run LoRA fine-tuning with minimal rank (e.g., 32) to verify setup works
  3. Evaluate impact of different beam search parameters on inference quality

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of the translation model change when using larger datasets or more computational resources?
- Basis in paper: [explicit] The paper mentions that dataset size was constrained due to computational resource limitations, and that this may have hindered the model's ability to generalize.
- Why unresolved: The authors did not have the resources to test the model with larger datasets or more computational power.
- What evidence would resolve it: Testing the model with larger datasets and more computational resources to see if performance improves.

### Open Question 2
- Question: What techniques can be used to improve the quality of back-translated data?
- Basis in paper: [explicit] The paper mentions that the quality of back-translated data negatively influenced the model's performance.
- Why unresolved: The authors did not explore alternative techniques for generating back-translated data.
- What evidence would resolve it: Experimenting with different techniques for generating back-translated data and comparing their impact on model performance.

### Open Question 3
- Question: How can the model be improved to better handle the complexities of Indic language generation?
- Basis in paper: [explicit] The paper mentions a performance gap between translations where English is the target language and those where an Indic language is the target, suggesting the model struggles with Indic language generation.
- Why unresolved: The authors did not explore techniques to improve the model's ability to generate accurate translations in Indic languages.
- What evidence would resolve it: Testing the model with different techniques for handling the complexities of Indic language generation and comparing their impact on performance.

## Limitations
- Limited bilingual datasets for certain language pairs constrain model performance and generalization
- Quality of back-translated data is uncertain and may introduce noise or systematic errors
- Results are validated only on four specific Indic languages, limiting generalizability to other language pairs

## Confidence
- High Confidence: LoRA fine-tuning on NLLB 3.3B reduces parameter adaptation while maintaining translation quality
- High Confidence: Back-translation expands training data for Mizo and Khasi, improving model performance
- Medium Confidence: Masked language modeling pre-training improves target language understanding
- Low Confidence: The special token approach for Khasi is optimal for handling this language

## Next Checks
1. Evaluate back-translation quality using human evaluation or automatic quality assessment metrics to determine if synthetic data introduces noise
2. Conduct ablation study on MLM pre-training by training a model without this stage and comparing performance
3. Test alternative tokenization strategies for Khasi, such as subword tokenization or character-level models, to evaluate if current approach is optimal