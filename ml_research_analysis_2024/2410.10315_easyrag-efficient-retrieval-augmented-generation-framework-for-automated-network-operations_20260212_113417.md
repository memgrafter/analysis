---
ver: rpa2
title: 'EasyRAG: Efficient Retrieval-Augmented Generation Framework for Automated
  Network Operations'
arxiv_id: '2410.10315'
source_url: https://arxiv.org/abs/2410.10315
tags:
- text
- retrieval
- document
- answer
- ranking
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: EasyRAG is a retrieval-augmented generation framework for automated
  network operations. It uses a dual-route sparse retrieval (BM25 and dense) combined
  with an LLM reranker to improve accuracy.
---

# EasyRAG: Efficient Retrieval-Augmented Generation Framework for Automated Network Operations

## Quick Facts
- arXiv ID: 2410.10315
- Source URL: https://arxiv.org/abs/2410.10315
- Reference count: 8
- Primary result: Dual-route sparse retrieval with LLM reranker achieves 84.38% accuracy in preliminary rounds and 96.65% in semifinals

## Executive Summary
EasyRAG is a retrieval-augmented generation framework designed for automated network operations. It combines dual-route sparse retrieval (BM25 and dense) with an LLM reranker to improve accuracy while maintaining computational efficiency. The system achieves high accuracy scores in competition settings and significantly reduces inference latency through various optimization techniques. The framework is lightweight, requiring minimal GPU memory, and supports flexible deployment options.

## Method Summary
EasyRAG processes HTML documents from .zedx files through text segmentation and image extraction, then employs dual-route retrieval using BM25 for keyword matching and dense retrieval for semantic similarity. An LLM reranker (bge-reranker-v2-minicpm-layerwise) refines the initial rankings, followed by context construction using BM25-Extract compression. The GLM4 LLM generates answers with optimization based on top-1 document integration. The system supports deployment via Docker or FastAPI and offers customization options for layer counts and compression rates.

## Key Results
- 84.38% accuracy in preliminary rounds and 96.65% in semifinals
- Inference latency reduced from 26 seconds to under 10 seconds
- Minimal GPU memory usage through layer count customization
- Public availability of code and data

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Dual-route sparse retrieval (BM25 + dense) improves recall coverage
- Mechanism: BM25 handles keyword matching well for short queries while dense retrieval captures semantic similarity for longer or more complex queries. Combining both ensures neither retrieval type misses relevant documents
- Core assumption: Some queries work better with BM25 (keyword-heavy) while others work better with dense retrieval (semantic-heavy)
- Evidence anchors:
  - [abstract] "dual-route sparse retrieval (BM25 and dense) combined with an LLM reranker"
  - [section 1.2.2] "In the sparse retrieval section, we utilized the BM25 algorithm" and "In the dense retrieval section, we employed the gte-Qwen2-7B-instruct model"
  - [corpus] Weak evidence - only general RAG papers, no specific dual-route comparison studies found
- Break condition: When query types are uniformly distributed and neither retrieval method has clear advantage over the other

### Mechanism 2
- Claim: LLM reranker significantly improves precision over initial coarse ranking
- Mechanism: The LLM reranker (bge-reranker-v2-minicpm-layerwise) uses cross-encoder architecture to capture query-document interactions that are impossible with sparse or single-vector dense representations
- Core assumption: The reranker model has been trained on data similar enough to the network operations domain to provide meaningful improvements
- Evidence anchors:
  - [abstract] "combined with an LLM reranker to improve accuracy"
  - [section 1.2.4] "We utilized the bge-reranker-v2-minicpm-layerwise model... This model exhibits advanced ranking performance in both Chinese and English"
  - [section 2.2] "finding that bge-reranker-v2-minicpm-layerwise significantly outperformed BERT-based rerankers, bringing an improvement of more than 3 percentage points"
- Break condition: When the reranker's training distribution diverges significantly from the target domain

### Mechanism 3
- Claim: Answer optimization using top-1 context improves answer completeness
- Mechanism: The LLM is prompted to integrate and supplement answers from all 6 retrieved documents using the top-1 document as a focus anchor, ensuring the most relevant information is properly incorporated
- Core assumption: The top-1 document contains the most critical information needed to answer the question correctly
- Evidence anchors:
  - [abstract] "LLM answer generation and optimization"
  - [section 1.2.7] "we designed an answer integration prompt... allows us to integrate and supplement the answers derived from the 6 text blocks using the top1 text block"
  - [section 2.3] "We discovered that concatenating the top1 text block with the answer could lead to a 2% improvement"
- Break condition: When the top-1 document is actually less relevant than other documents in the top-6

## Foundational Learning

- Concept: Sparse vs Dense Retrieval
  - Why needed here: Understanding the complementary strengths of keyword-based (BM25) and semantic-based (dense) retrieval methods is crucial for implementing the dual-route approach
  - Quick check question: What types of queries would benefit most from BM25 versus dense retrieval?

- Concept: Reranking with Cross-Encoders
  - Why needed here: The LLM reranker uses cross-encoder architecture to evaluate query-document pairs, which is fundamentally different from the bi-encoder approach used in initial retrieval
  - Quick check question: How does cross-encoder reranking differ computationally from the initial bi-encoder retrieval?

- Concept: Context Compression Techniques
  - Why needed here: The BM25-Extract method is used to reduce context length while preserving relevant information, which is critical for efficient LLM inference
  - Quick check question: What are the tradeoffs between different context compression methods (LLMLingua, LongLLMLingua, BM25-Extract)?

## Architecture Onboarding

- Component map: Ingestion Pipeline (zedx → segmentation → image extraction) → Retrieval Pipeline (query rewriting → dual-route retrieval → reranking → ranking fusion) → Generation Pipeline (context construction → answer generation → answer optimization) → Optimization Layer (acceleration + compression)

- Critical path: Query → Query rewriting → Dual-route retrieval → Reranking → Context construction → Answer generation → Answer optimization

- Design tradeoffs:
  - Memory vs Accuracy: Using fewer reranker layers (12 vs 28 vs 40) reduces memory but may impact accuracy
  - Speed vs Completeness: Context compression reduces token usage but may lose relevant information
  - Complexity vs Performance: Dual-route retrieval adds complexity but improves recall coverage

- Failure signatures:
  - Low recall: Check if both BM25 and dense retrieval are returning sufficient documents
  - Poor ranking: Verify reranker is functioning correctly and not just passing through scores
  - Incomplete answers: Check if context compression is removing too much information

- First 3 experiments:
  1. Test single-route retrieval (only BM25, only dense) to establish baseline performance
  2. Test reranker with different layer counts (8, 12, 28, 40) to find optimal accuracy/memory tradeoff
  3. Test different context compression rates (0.5, 0.8) to measure impact on answer quality and inference speed

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the proposed BM25-Extract context compression method be further optimized for different types of documents and queries?
- Basis in paper: [explicit] The paper mentions that BM25-Extract has advantages of no GPU memory usage, faster speed, and higher accuracy compared to other context compression methods.
- Why unresolved: The paper does not provide detailed experiments or analysis on how BM25-Extract performs across different document types and query complexities. It also does not discuss potential limitations or scenarios where BM25-Extract might not be the optimal choice.
- What evidence would resolve it: Conducting extensive experiments comparing BM25-Extract with other compression methods across diverse datasets and query types, along with ablation studies to identify the impact of different parameters on performance.

### Open Question 2
- Question: What are the long-term effects of using query rewriting techniques like HyDE on the overall performance of the RAG system?
- Basis in paper: [inferred] The paper briefly mentions the use of HyDE for query rewriting but does not explore its long-term impact on the system's performance or its effectiveness in different scenarios.
- Why unresolved: The paper does not provide any analysis on how query rewriting techniques might affect the system's performance over time, especially as the underlying data or query patterns change. It also does not discuss potential drawbacks or limitations of using such techniques.
- What evidence would resolve it: Long-term studies tracking the performance of the RAG system with and without query rewriting techniques, along with analysis of how different query rewriting methods impact the system's ability to handle evolving data and query patterns.

### Open Question 3
- Question: How can the dual-route sparse retrieval approach be extended to incorporate other retrieval methods, such as semantic search or graph-based retrieval, to further improve accuracy?
- Basis in paper: [explicit] The paper describes the use of dual-route sparse retrieval (BM25 and dense) combined with an LLM reranker to improve accuracy.
- Why unresolved: The paper does not explore the potential benefits of incorporating additional retrieval methods beyond BM25 and dense retrieval. It also does not discuss how these methods might be integrated into the existing dual-route approach or their potential impact on system performance.
- What evidence would resolve it: Experiments comparing the performance of the dual-route sparse retrieval approach with and without the addition of other retrieval methods, along with analysis of how these methods affect the system's ability to handle different types of queries and documents.

### Open Question 4
- Question: What are the potential trade-offs between model complexity and performance when using different numbers of layers in the LLM Reranker?
- Basis in paper: [explicit] The paper mentions that the LLM Reranker supports customization of the number of inference layers, allowing selection from 8-40 layers based on needs and resource constraints.
- Why unresolved: The paper does not provide a detailed analysis of the trade-offs between model complexity and performance when using different numbers of layers in the LLM Reranker. It also does not discuss how these trade-offs might impact the system's overall efficiency and accuracy.
- What evidence would resolve it: Experiments comparing the performance of the LLM Reranker with different numbers of layers, along with analysis of how these differences affect the system's accuracy, inference time, and resource usage.

## Limitations

- The dual-route retrieval mechanism lacks comparative ablation studies proving the combination outperforms either method alone
- The claimed accuracy improvement between rounds could be attributed to data leakage or prompt engineering refinements rather than architectural improvements
- The LLM reranker's domain-specific performance claims are based on general benchmarks rather than network operations-specific validation

## Confidence

**High Confidence**: The dual-route retrieval architecture is technically sound and the computational efficiency improvements (latency reduction from 26s to under 10s) are verifiable through the described acceleration techniques. The framework's modular design and deployment flexibility are well-documented.

**Medium Confidence**: The 84.38% and 96.65% accuracy figures are reported but lack detailed methodology for how these were measured and whether they account for variations across question types. The effectiveness of the answer optimization prompt design shows reasonable supporting evidence but would benefit from more rigorous ablation testing.

**Low Confidence**: The claim that the reranker "significantly outperformed BERT-based rerankers" by more than 3 percentage points is based on external references rather than direct comparison within the EasyRAG system. The assumption that top-1 document integration improves answer completeness is theoretically reasonable but lacks empirical validation showing when this fails.

## Next Checks

1. **Ablation Study on Retrieval Routes**: Run EasyRAG with only BM25 retrieval, only dense retrieval, and the combined dual-route approach on the same test set to quantify the actual contribution of each method to the final accuracy.

2. **Domain-Specific Reranker Validation**: Evaluate the bge-reranker-v2-minicpm-layerwise model on a held-out subset of network operations documents to measure performance degradation compared to its general benchmark performance, establishing confidence in its domain applicability.

3. **Top-1 Document Reliability Analysis**: Systematically analyze cases where the top-1 document was incorrect or incomplete, measuring how often the answer optimization mechanism fails when the top document assumption is violated, and whether alternative ranking strategies would improve results.