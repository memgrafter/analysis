---
ver: rpa2
title: 'ASETF: A Novel Method for Jailbreak Attack on LLMs through Translate Suffix
  Embeddings'
arxiv_id: '2402.16006'
source_url: https://arxiv.org/abs/2402.16006
tags:
- adversarial
- embedding
- suffixes
- attack
- llms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper introduces ASETF, a framework to generate adversarial\
  \ suffixes that jailbreak large language models (LLMs). The method first optimizes\
  \ continuous adversarial suffix embeddings in the target LLM\u2019s embedding space,\
  \ then translates these embeddings into coherent text using a fine-tuned embedding\
  \ translation model."
---

# ASETF: A Novel Method for Jailbreak Attack on LLMs through Translate Suffix Embeddings

## Quick Facts
- arXiv ID: 2402.16006
- Source URL: https://arxiv.org/abs/2402.16006
- Reference count: 9
- Key outcome: A framework that generates adversarial suffixes through continuous embedding optimization and translation, achieving higher attack success rates with improved fluency and transferability compared to existing methods

## Executive Summary
ASETF introduces a novel framework for generating adversarial suffixes that jailbreak large language models by first optimizing continuous adversarial suffix embeddings in the target LLM's embedding space, then translating these embeddings into coherent text using a fine-tuned embedding translation model. The method addresses the computational inefficiency of traditional discrete token optimization approaches while maintaining or improving attack effectiveness. Experiments on multiple LLMs including Llama2 and Vicuna demonstrate that ASETF achieves higher attack success rates while significantly improving the fluency of generated adversarial suffixes, as measured by perplexity and self-BLEU scores.

## Method Summary
ASETF operates in two main stages: first, it optimizes continuous adversarial suffix embeddings using gradient-based methods in the target LLM's embedding space with cross-entropy and Maximum Mean Discrepancy (MMD) loss; second, it translates these optimized embeddings into coherent text using a fine-tuned embedding translation model (typically GPT-j-6b) trained on a parallel dataset created from Wikipedia. The method leverages the efficiency of continuous optimization while addressing the readability challenge through the translation framework. For transferability, the approach can be trained across multiple target models simultaneously, enabling black-box attacks on models like ChatGPT and Gemini.

## Key Results
- Achieves higher attack success rates compared to existing methods while significantly improving adversarial suffix fluency
- Reduces computational time needed for generating adversarial suffixes compared to gradient-based discrete optimization approaches
- Demonstrates transferability of adversarial suffixes across multiple LLMs, including black-box models
- Improves perplexity scores and self-BLEU scores of generated suffixes, indicating better readability and diversity

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Adversarial suffixes are more effective when optimized in continuous embedding space rather than discrete token space.
- Mechanism: The method optimizes continuous adversarial suffix embeddings in the target LLM's embedding space, which allows gradient-based optimization techniques to be applied. This avoids the computational bottleneck of discrete token optimization which requires over 100,000 LLM calls.
- Core assumption: The continuous embedding space preserves the semantic relationships needed for effective jailbreak attacks while being differentiable for gradient optimization.
- Evidence anchors:
  - [abstract]: "This gradient based discrete optimization attack requires over 100,000 LLM calls"
  - [section 3.1]: "An intuitive approach is to transfer optimization from a discrete token space to a continuous word embedding space"
- Break condition: If the embedding space does not preserve the semantic relationships needed for jailbreak attacks, the continuous optimization would fail to produce effective suffixes.

### Mechanism 2
- Claim: The embedding translation framework can convert continuous adversarial suffix embeddings into coherent text that maintains attack effectiveness.
- Mechanism: A fine-tuned embedding translation model is trained to convert continuous embeddings back into coherent text by learning from a parallel dataset of Wikipedia text with inserted embeddings. This preserves the semantic content while making the adversarial suffix readable.
- Core assumption: The translation model can learn the mapping from continuous embeddings to text without losing the adversarial properties of the suffix.
- Evidence anchors:
  - [abstract]: "transforms continuous adversarial suffix embeddings into coherent and understandable text"
  - [section 3.2.1]: "we use Wikipedia dataset and only use the English corpus within it... the first sentence serves as the context and the second sentence serves as the suffix"
- Break condition: If the translation model cannot preserve the semantic content of the adversarial embeddings, the converted text would lose its effectiveness as a jailbreak prompt.

### Mechanism 3
- Claim: Adversarial suffixes generated by this method can transfer across multiple LLMs, including black-box models.
- Mechanism: By training on multiple target models simultaneously, the embedding translation model learns to generate suffixes that are effective across different embedding spaces, enabling transfer attacks.
- Core assumption: Different LLMs share enough semantic structure in their embedding spaces that suffixes effective on one model will have some effectiveness on others.
- Evidence anchors:
  - [abstract]: "Our approach can be generalized into a broader method for generating transferable adversarial suffixes that can successfully attack multiple LLMs, even black-box LLMs, such as ChatGPT and Gemini"
  - [section 3.2.2]: "For each training sample (c1, c2), we use the following objective to fine-tune the embedded translator across all intended target LLMs"
- Break condition: If the embedding spaces of different LLMs are too dissimilar, the transferability of the generated suffixes would fail.

## Foundational Learning

- Concept: Gradient-based optimization in continuous spaces
  - Why needed here: Traditional adversarial attacks on discrete tokens require exhaustive search through the vocabulary, making them computationally expensive. Continuous optimization allows efficient gradient descent.
  - Quick check question: Why is optimizing in continuous embedding space more efficient than optimizing in discrete token space?

- Concept: Embedding space alignment across different models
  - Why needed here: The method needs to generate suffixes that work across multiple LLMs with potentially different embedding spaces.
  - Quick check question: How does the method ensure that adversarial suffixes optimized for one LLM's embedding space remain effective when translated for another LLM?

- Concept: Self-supervised learning for embedding translation
  - Why needed here: The translation model needs to learn how to convert continuous embeddings back to coherent text without labeled data for this specific task.
  - Quick check question: What training data is used to teach the model how to convert continuous embeddings back to meaningful text?

## Architecture Onboarding

- Component map: Continuous suffix optimization module -> Embedding translation model -> Multi-model training framework -> Evaluation pipeline
- Critical path: 1) Optimize continuous adversarial suffix embeddings using gradient descent with MMD loss 2) Train embedding translation model on Wikipedia corpus with inserted embeddings 3) Convert optimized embeddings to coherent text using the translation model 4) Evaluate attack success rate on target LLMs
- Design tradeoffs: Computational efficiency vs. attack success rate (continuous optimization reduces computation but may sacrifice some effectiveness); Transferability vs. model-specific optimization (training on multiple models enables transfer but may reduce effectiveness on individual models); Readability vs. attack strength (making suffixes coherent reduces detection risk but may limit the attack surface)
- Failure signatures: Low attack success rate despite high computational efficiency; Generated suffixes are grammatically correct but semantically irrelevant to the attack; Transferability fails when attacking models with significantly different architectures; The MMD loss fails to keep optimized embeddings within the target model's embedding distribution
- First 3 experiments: 1) Test continuous optimization on a single instruction with a single target model, comparing computation time and attack success rate to GCG baseline 2) Train the embedding translation model on Wikipedia data and verify it can reconstruct simple sentences from their embeddings 3) Test transfer attack capability by training on two models and attacking a third unseen model, measuring success rate degradation

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the adversarial suffix optimization process be improved to generate more effective and transferable suffixes across diverse LLMs?
- Basis in paper: [inferred] The paper mentions that universal adversarial suffixes optimized for multiple instructions exhibit a lower success rate in attacks compared to independent adversarial suffixes. It also suggests that the capacity for information representation of discrete tokens depends on their length, and an extended length implies a more complex optimization process.
- Why unresolved: The paper does not provide a detailed solution or further exploration into optimizing the adversarial suffix generation process for better transferability and effectiveness across different LLMs.
- What evidence would resolve it: Developing and testing new optimization techniques that focus on enhancing the transferability and effectiveness of adversarial suffixes across various LLMs, along with a comprehensive evaluation of their performance compared to existing methods.

### Open Question 2
- Question: What are the potential defense mechanisms that can be developed to counteract the adversarial suffix attacks described in the paper?
- Basis in paper: [explicit] The paper discusses the limitations of current safety defense methods and highlights the need for more robust defense strategies against adversarial suffix attacks.
- Why unresolved: The paper focuses on developing the attack methodology and does not extensively explore potential defense mechanisms to mitigate these attacks.
- What evidence would resolve it: Proposing and evaluating new defense mechanisms specifically designed to detect and neutralize adversarial suffix attacks, along with empirical results demonstrating their effectiveness against the proposed attack framework.

### Open Question 3
- Question: How can the trade-off between the coherence of generated text and the effectiveness of adversarial attacks be better managed?
- Basis in paper: [inferred] The paper mentions that if the adversarial suffixes generated by the translation model are biased towards semantics related to harmful instructions, the attack is prone to failure. Conversely, if they lean towards maintaining the consistency of embeddings, it can lead to textual incoherence.
- Why unresolved: The paper does not provide a detailed analysis or solution for managing the trade-off between text coherence and attack effectiveness.
- What evidence would resolve it: Conducting experiments to explore different strategies for balancing text coherence and attack effectiveness, along with a comprehensive evaluation of their impact on the success rate of adversarial attacks.

## Limitations
- The transferability claims across black-box models are promising but require broader testing across more diverse architectures and training paradigms
- The effectiveness of ASETF against sophisticated defense mechanisms that go beyond simple prompt filtering is unclear
- The method's effectiveness across a broader range of jailbreak scenarios (role-playing, logical contradictions, etc.) remains to be tested

## Confidence
- High Confidence: The computational efficiency gains from continuous embedding optimization are well-established in optimization literature
- Medium Confidence: The attack success rates reported for specific models (Llama2, Vicuna) are based on the paper's experiments
- Low-Medium Confidence: The transferability claims across black-box models (ChatGPT, Gemini) require broader testing across more diverse architectures

## Next Checks
1. **Cross-Architecture Transferability Test**: Evaluate ASETF-generated suffixes on a model with fundamentally different architecture than those used in training to quantify transferability limits
2. **Defense-Aware Evaluation**: Test ASETF against a multi-layered defense system that includes perplexity filtering, semantic analysis, and context-aware jailbreak detection to assess real-world effectiveness
3. **Instruction Diversity Analysis**: Systematically test ASETF across different categories of jailbreak instructions to identify which instruction types benefit most from the method