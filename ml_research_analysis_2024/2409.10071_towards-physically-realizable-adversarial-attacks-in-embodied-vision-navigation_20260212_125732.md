---
ver: rpa2
title: Towards Physically Realizable Adversarial Attacks in Embodied Vision Navigation
arxiv_id: '2409.10071'
source_url: https://arxiv.org/abs/2409.10071
tags:
- adversarial
- navigation
- patch
- attack
- opacity
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of creating physically realizable
  adversarial attacks on embodied vision navigation systems, which are vulnerable
  to attacks exploiting deep neural networks. The authors propose attaching adversarial
  patches with learnable textures and opacity to objects in navigation scenes.
---

# Towards Physically Realizable Adversarial Attacks in Embodied Vision Navigation

## Quick Facts
- arXiv ID: 2409.10071
- Source URL: https://arxiv.org/abs/2409.10071
- Authors: Meng Chen; Jiawei Tu; Chao Qi; Yonghao Dang; Feng Zhou; Wei Wei; Jianqin Yin
- Reference count: 40
- Primary result: Adversarial patches reduce navigation success rates by 22.39% on HM3D dataset

## Executive Summary
This paper introduces a novel approach to creating physically realizable adversarial attacks on embodied vision navigation systems. The method involves attaching learnable adversarial patches with controllable opacity to objects in navigation scenes, enabling attacks that are both effective and visually natural. Through multi-view optimization and a two-stage opacity refinement process, the patches can deceive navigation agents across varying viewpoints while maintaining inconspicuous appearance. The approach demonstrates significant reductions in navigation success rates while remaining practically implementable in real-world settings.

## Method Summary
The method uses object-aware viewpoint sampling to generate diverse camera positions around target objects, then employs a differentiable renderer to simulate how adversarial patches would appear from these views. A Mask R-CNN model provides feedback for optimization, allowing the patch texture to be iteratively updated using PGD. After texture optimization, a second stage refines the opacity mask to improve visual naturalness while maintaining attack effectiveness. The entire process produces adversarial patches that can be physically printed on transparent material and attached to real objects.

## Key Results
- Achieves 22.39% average reduction in navigation success rates across various targets
- Outperforms baseline methods in both attack effectiveness and visual naturalness
- Demonstrates practical physical realizability through transparent patch printing approach
- Maintains effectiveness across varying viewpoints through multi-view optimization

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Multi-view optimization with object-aware sampling ensures the adversarial patch remains effective across varying viewpoints in navigation scenarios.
- Mechanism: The method samples viewpoints around the target object based on detection model confidence, then optimizes the patch texture using gradients from these diverse views. This ensures the patch degrades detection performance from multiple angles.
- Core assumption: Detection model feedback from multiple views can guide effective patch optimization that generalizes to unseen viewpoints.
- Evidence anchors:
  - [abstract]: "to ensure effectiveness across varying viewpoints, we employ a multi-view optimization strategy based on object-aware sampling"
  - [section]: "Specifically, to ensure effectiveness across varying viewpoints, we employ a multi-view optimization strategy based on object-aware sampling, which optimizes the patch's texture based on feedback from the vision-based perception model used in navigation."
  - [corpus]: Weak evidence - no direct citations of multi-view optimization for embodied navigation in the corpus.

### Mechanism 2
- Claim: Two-stage opacity optimization improves visual naturalness by first optimizing texture for attack effectiveness, then refining opacity for stealth.
- Mechanism: The method first optimizes the RGB texture channels to maximize attack success, then separately optimizes the opacity mask to minimize human perceptibility while maintaining attack effectiveness.
- Core assumption: Texture and opacity optimization have different objectives that can conflict if optimized jointly, requiring staged optimization.
- Evidence anchors:
  - [abstract]: "To make the patch inconspicuous to human observers, we introduce a two-stage opacity optimization mechanism, in which opacity is fine-tuned after texture optimization."
  - [section]: "To improve the naturalness of the patch, we incorporate an opacity optimization mechanism... we employ a two-stage optimization strategy."
  - [corpus]: Weak evidence - the corpus mentions physical adversarial patches but doesn't discuss staged opacity optimization specifically.

### Mechanism 3
- Claim: The adversarial patch's learnable opacity and texture properties make it more physically realizable than methods requiring full object texture modification.
- Mechanism: By attaching a patch with controllable opacity rather than modifying entire object textures, the attack avoids the need for 3D printing or projector-based texture replacement.
- Core assumption: A patch-based approach is simpler to implement physically than full object texture modification while maintaining attack effectiveness.
- Evidence anchors:
  - [abstract]: "we propose a practical attack method for embodied navigation by attaching adversarial patches to objects, where both opacity and textures are learnable."
  - [section]: "Our method achieves an average SR reduction of 22.39% across various targets... Experimental results show that our adversarial patches lead to an average reduction of 22.39% in navigation success rates."
  - [corpus]: Assumption - the paper claims physical realizability but the corpus doesn't provide direct evidence comparing patch vs full texture modification approaches.

## Foundational Learning

- Concept: Multi-view optimization
  - Why needed here: Navigation scenarios involve agents approaching objects from unpredictable angles, requiring attacks to work across diverse viewpoints
  - Quick check question: Why can't a single-view optimized patch work for embodied navigation?

- Concept: Differentiable rendering
  - Why needed here: Enables gradient-based optimization of physical properties (texture, opacity) by simulating how patches appear under different lighting and viewpoints
  - Quick check question: What would happen if we used non-differentiable rendering for patch optimization?

- Concept: Two-stage optimization
  - Why needed here: Separates attack effectiveness (texture) from stealth (opacity) to avoid conflicting gradient directions
  - Quick check question: What might go wrong if texture and opacity were optimized jointly from the start?

## Architecture Onboarding

- Component map: Object-aware sampling module -> Differentiable renderer (Mitsuba 3) -> Mask R-CNN detection model -> Patch representation (RGBA texture) -> PGD optimizer
- Critical path: 1. Initialize patch with Gaussian noise 2. Sample viewpoints using object-aware sampling 3. Render images with differentiable renderer 4. Run through Mask R-CNN 5. Compute detection loss 6. Backpropagate gradients to optimize patch 7. Apply opacity optimization in second stage
- Design tradeoffs:
  - Transparency vs attack effectiveness: Higher opacity improves attack but reduces stealth
  - View sampling density vs computation: More views improve robustness but increase cost
  - Patch size vs coverage: Larger patches affect more views but are more noticeable
- Failure signatures:
  - Low ASR despite optimization: Indicates patch not aligned with model vulnerabilities
  - High human detectability: Opacity optimization failed or transparency too low
  - View-specific failure: Object-aware sampling didn't capture critical viewpoints
- First 3 experiments:
  1. Verify single-view optimization works on a fixed camera position
  2. Test multi-view optimization with 5 viewpoints vs 20 viewpoints
  3. Compare staged vs joint opacity optimization on visual naturalness

## Open Questions the Paper Calls Out

- Question: How do adversarial patches affect navigation agents with different perception models beyond Mask R-CNN, such as transformer-based vision models?
  - Basis in paper: [explicit] The paper states "our navigation experiment setup follows the 2022 Habitat ObjectNav Challenge [40]. For the adversarial attack, we conduct experiments in ObjectNav using the HM3D dataset [41] within the Habitat simulator [42], [43]. We perform white-box optimization using scenes from the validation dataset, with attack targets being several categories listed in Table I." and uses Mask R-CNN
  - Why unresolved: The experiments only evaluate attacks against Mask R-CNN-based agents, leaving uncertainty about effectiveness against other architectures
  - What evidence would resolve it: Systematic evaluation of the same adversarial patches against multiple perception models (transformer-based, feature pyramid networks, etc.) showing success/failure rates across architectures

- Question: What is the relationship between patch opacity levels and attack effectiveness across different lighting conditions and viewing angles?
  - Basis in paper: [explicit] The paper discusses opacity optimization and mentions "we selected an initial opacity of 0.6 to balance attack strength and visibility" but doesn't provide comprehensive analysis
  - Why unresolved: The paper only briefly touches on opacity optimization without detailed analysis of how opacity interacts with environmental factors
  - What evidence would resolve it: Controlled experiments varying opacity across multiple lighting conditions and viewing angles with quantitative metrics for both attack success and visual naturalness

- Question: How do these adversarial patches perform in real-world physical environments versus simulation, and what are the key factors causing performance degradation?
  - Basis in paper: [inferred] The paper discusses physical feasibility and mentions the patch can be "printed on transparent paper with a laser printer and directly affixed to target objects" but all experiments are in simulation
  - Why unresolved: The paper acknowledges the gap between simulation and reality but doesn't conduct physical experiments to validate transfer
  - What evidence would resolve it: Real-world deployment studies comparing attack success rates between simulated and physical environments, identifying specific factors (lighting, camera quality, surface texture) that affect performance

## Limitations
- Physical realizability claim lacks direct empirical validation in real-world settings
- Method's effectiveness relies heavily on the detection model (Mask R-CNN) used for optimization
- Multi-view optimization assumes sampled viewpoints adequately represent all possible agent approaches
- Paper doesn't provide comprehensive human perceptual studies to validate "inconspicuous" claims

## Confidence

**High Confidence**: The claim that adversarial patches can reduce navigation success rates is well-supported by experimental results showing a 22.39% average reduction in SR across various targets. The technical implementation details for multi-view optimization and differentiable rendering are clearly specified.

**Medium Confidence**: The claim that object-aware sampling ensures effectiveness across varying viewpoints is plausible but relies on assumptions about the representativeness of sampled views. The staged opacity optimization for visual naturalness is methodologically sound but lacks comprehensive perceptual validation.

**Low Confidence**: The physical realizability claim lacks direct empirical validation in real-world settings. The assertion that this approach is more practical than full object texture modification is based on theoretical arguments rather than comparative physical experiments.

## Next Checks
1. **Physical Deployment Test**: Print the optimized adversarial patches on transparent material and attach them to real objects in a controlled navigation environment. Compare navigation success rates between baseline, digital simulation, and physical deployment to validate the physical realizability claim.

2. **Cross-Model Generalization**: Apply the optimized patches to a different object detection model (e.g., YOLO or EfficientDet) in the same navigation scenarios. Measure changes in attack effectiveness to assess whether the optimization overfits to Mask R-CNN.

3. **Viewpoint Coverage Analysis**: Systematically vary the approach angles during navigation episodes to identify edge cases where the patch fails. Compare these failure cases against the sampled viewpoints used during optimization to evaluate whether the object-aware sampling adequately captured the critical viewing distributions.