---
ver: rpa2
title: Recording First-person Experiences to Build a New Type of Foundation Model
arxiv_id: '2408.02680'
source_url: https://arxiv.org/abs/2408.02680
tags:
- data
- could
- foundation
- text
- used
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a novel approach to developing more accurate
  foundation models by recording first-person experiences, including environmental
  stimuli and the wearer's emotional and physiological responses. The authors developed
  a recording rig that captures visual, auditory, and biometric data, which is then
  processed to create a rich dataset for training foundation models.
---

# Recording First-person Experiences to Build a New Type of Foundation Model

## Quick Facts
- arXiv ID: 2408.02680
- Source URL: https://arxiv.org/abs/2408.02680
- Authors: Dionis Barcari; David Gamez; Aliya Grig
- Reference count: 17
- Primary result: Presents a novel approach to developing more accurate foundation models by recording first-person experiences, including environmental stimuli and the wearer's emotional and physiological responses

## Executive Summary
This paper introduces a novel approach to developing more accurate foundation models by recording first-person experiences, including environmental stimuli and the wearer's emotional and physiological responses. The authors developed a recording rig that captures visual, auditory, and biometric data, which is then processed to create a rich dataset for training foundation models. Preliminary tests demonstrate the rig's functionality, and the authors suggest that models trained on this data could more accurately replicate human behavior than current approaches. The paper also outlines potential applications, including recommendation systems, personal assistants, and generative adversarial networks.

## Method Summary
The authors propose developing a first-person foundation model (FPFM) that maps environmental stimuli to emotional and physiological states and predicts behavior based on these states. The method involves collecting multimodal data (visual, auditory, and biometric including EEG, GSR, facial expressions) using a wearable recording rig, then training a foundation model from scratch on this data. The rig consists of a Raspberry Pi with camera, microphone, GSR sensor, and EEG headset, connected to a laptop for local processing and cloud services for advanced analysis. The collected data is stored with a blockchain-like hash chain for integrity verification.

## Key Results
- Developed a functional recording rig that captures visual, auditory, and biometric data from wearers
- Created a data processing pipeline that integrates multiple sensor streams into a unified dataset
- Preliminary tests demonstrate the rig's ability to collect and process first-person experiential data
- Proposed applications include recommendation systems, personal assistants, and generative adversarial networks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The recording rig captures multimodal physiological and environmental data that can directly inform the mapping from stimuli to emotional states.
- Mechanism: By integrating EEG, GSR, facial expression, and environmental sensors (camera, microphone), the rig produces temporally aligned data streams. These streams allow a model to learn the causal relationships between sensory inputs and internal states.
- Core assumption: Emotional and physiological responses to stimuli are sufficiently stable and reproducible across individuals to allow general pattern learning from first-person data.
- Evidence anchors:
  - [abstract] "captures what the wearer is seeing and hearing as well as their skin conductance (GSR), facial expression and brain state (14 channel EEG)"
  - [section] "Data recorded by this rig could be used to build a new type of foundation model, a first person-foundation model (FPFM), that maps environmental stimuli to a person's emotional and physiological reactions"
  - [corpus] Weak - corpus papers are unrelated to physiological recording, so no direct evidence is available.
- Break condition: If emotional/physiological responses vary too widely across individuals or are highly context-dependent, a single FPFM may fail to generalize.

### Mechanism 2
- Claim: Combining reinforcement learning, RAG, and prompt engineering with first-person data can produce personality models that more closely approximate actual human behavior than surface-level approximations.
- Mechanism: First-person data provides the internal state context that RL and RAG alone cannot infer. A model trained on this richer signal can simulate the decision-making process more accurately.
- Core assumption: Current personality models lack the emotional/physiological grounding that drives behavior.
- Evidence anchors:
  - [abstract] "These chatbots are not based on people's actual emotional and physiological responses to their environment, so they are, at best, a surface-level approximation"
  - [section] "Foundation models trained on this data could replicate human behaviour much more accurately than the personality models that have been developed so far"
  - [corpus] No direct evidence - corpus papers are unrelated to personality modeling.
- Break condition: If the relationship between internal states and behavior is too indirect or noisy, the added complexity of first-person data may not improve model fidelity.

### Mechanism 3
- Claim: The rig's architecture enables scalable data collection for training large-scale FPFMs, with potential to address predicted data exhaustion.
- Mechanism: By recording ~40 GB/day per user and enabling distributed data gathering (through wearables or cloud backends), the rig provides a sustainable source of training data distinct from Internet scraping.
- Core assumption: First-person experiential data will be more valuable and less prone to depletion than current Internet text/image data.
- Evidence anchors:
  - [abstract] "Data gathering and model training are expensive, so we are currently working on the launch of a start-up that could raise funds for the next stage of the project"
  - [section] "Our hypothesis is that a foundation model that is trained from scratch on the stimuli and emotional and physiological states of a person will replicate human behaviour more effectively than surface-level approximations"
  - [corpus] Weak - corpus papers are unrelated to data exhaustion concerns.
- Break condition: If the cost and complexity of first-person data collection outweigh the benefits, the approach may not be scalable.

## Foundational Learning

- Concept: Multimodal data alignment
  - Why needed here: The rig produces temporally overlapping streams (EEG, GSR, video, audio). Aligning these streams is essential for supervised learning of state-behavior mappings.
  - Quick check question: How would you synchronize a 256 Hz EEG stream with a 30 fps video stream?

- Concept: Foundation model training from scratch
  - Why needed here: Unlike fine-tuning existing models, FPFMs require learning the full input-to-output mapping (stimuli → state → behavior) from raw first-person data.
  - Quick check question: What architectural changes might be needed to handle continuous multimodal sequences instead of discrete text tokens?

- Concept: Privacy and data protection
  - Why needed here: The rig captures personal and potentially sensitive data (faces, voices, internal states). Ensuring compliance and user trust is critical for deployment.
  - Quick check question: What technical measures (e.g., face blurring, hashing) are already in place, and what additional safeguards might be necessary?

## Architecture Onboarding

- Component map:
  Raspberry Pi (data acquisition hub) -> Sensors (camera, microphone, GSR, EEG headset) -> Laptop (local processing, WebSocket bridge to EEG) -> Cloud services (AWS Rekognition, Transcribe, Comprehend) -> Blockchain-like hash chain (data integrity) -> Web interface (configuration, playback)

- Critical path:
  1. Sensor data acquisition (Pi → laptop)
  2. Real-time preprocessing (audio → text, video → labels)
  3. Data storage (JSON files with schemas)
  4. Integrity verification (hash chain)
  5. User access (web interface)

- Design tradeoffs:
  - Local vs. cloud processing: Local reduces latency but limits compute; cloud enables richer analysis but raises privacy concerns.
  - Raw vs. processed data: Raw data preserves fidelity but increases storage; processed data is lighter but may lose nuance.
  - Wearable form factor: Backpack + neck rig is functional but not consumer-friendly; future Humane AI Pin-like designs could improve adoption.

- Failure signatures:
  - Data loss: Missing sensor streams, corrupted JSON, broken hash chain.
  - Latency spikes: Delays in WebSocket EEG transmission or AWS API calls.
  - Privacy breaches: Unintended capture of third-party faces or copyrighted material.

- First 3 experiments:
  1. Verify temporal alignment: Record a controlled stimulus (e.g., audio tone) and check that all sensor timestamps align within 10 ms.
  2. Test hash chain integrity: Tamper with a single byte in a stored file and confirm the blockchain check fails.
  3. Validate preprocessing accuracy: Compare AWS-generated labels/sentiment against ground truth for a small curated dataset.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How effectively can first-person foundation models trained on physiological and emotional data replicate human behavior compared to current personality models using LLMs and prompt engineering?
- Basis in paper: [explicit] The authors claim that FPFMs could replicate human behavior "much more accurately than the personality models that have been developed so far" and that current chatbots are "at best, a surface-level approximation."
- Why unresolved: The paper presents a theoretical framework and preliminary hardware tests but does not provide empirical evidence comparing FPFM performance against existing personality models.
- What evidence would resolve it: Direct comparison studies showing FPFM predictions of human behavior versus ground truth, benchmarked against current personality modeling approaches.

### Open Question 2
- Question: What is the minimum dataset size required to train a first-person foundation model that can accurately model individual personalities?
- Basis in paper: [explicit] The authors provide estimates for GPT-1, GPT-2, and GPT-3 training data requirements, suggesting "a purely text based version of GPT-2 could be created from ~50 days of data from a single individual."
- Why unresolved: These are rough estimates based on scaling assumptions. The actual data requirements may vary significantly depending on model architecture, data quality, and the complexity of human behavior being modeled.
- What evidence would resolve it: Systematic studies varying dataset sizes and measuring model performance on personality prediction tasks, identifying the point of diminishing returns.

### Open Question 3
- Question: How can privacy concerns and copyright issues be effectively addressed when collecting and using first-person data for foundation model training?
- Basis in paper: [explicit] The authors discuss privacy protection through face blurring, but acknowledge this prevents learning emotional reactions to familiar faces, and note concerns about capturing copyrighted content.
- Why unresolved: The paper proposes solutions like consent mechanisms and GPS-based recording restrictions but does not provide tested implementations or evaluate their effectiveness in real-world scenarios.
- What evidence would resolve it: Implementation and evaluation of privacy-preserving data collection methods, along with legal analysis of copyright implications for training data.

## Limitations

- The work is entirely conceptual with no foundation model results presented
- The relationship between physiological responses and behavior prediction is assumed rather than demonstrated
- Privacy implications of collecting intimate biometric data are acknowledged but not addressed in technical detail
- The approach requires extensive human subject testing and regulatory compliance not yet undertaken

## Confidence

- **Low Confidence**: The core claim that first-person experiential data will produce more accurate human behavior models than existing approaches lacks empirical validation. No model results are presented, and the relationship between internal states and behavior prediction remains theoretical.
- **Medium Confidence**: The hardware architecture and data collection pipeline are well-specified and appear technically feasible. The integration of multiple sensors with timestamp synchronization is a reasonable approach, though real-world performance remains untested at scale.
- **Low Confidence**: Claims about scalability and data exhaustion solutions are speculative. The paper acknowledges data gathering is expensive but provides no analysis of cost-benefit tradeoffs or data efficiency compared to existing approaches.

## Next Checks

1. **Temporal Alignment Validation**: Record a controlled stimulus (e.g., audio tone) and verify all sensor streams (EEG at 256 Hz, video at 30 fps, GSR) align within 10 ms. This is critical for supervised learning of state-behavior mappings.

2. **Data Quality Assessment**: Process a small controlled dataset through the full pipeline (Pi → laptop → cloud services → storage) and evaluate: (a) WebSocket EEG transmission latency (<100ms), (b) AWS API call reliability (>95%), and (c) hash chain integrity under simulated tampering.

3. **Privacy Safeguard Implementation**: Test face detection and blurring on video samples containing third-party individuals. Verify the system can automatically detect and anonymize faces not belonging to the wearer before cloud upload, meeting GDPR compliance requirements.