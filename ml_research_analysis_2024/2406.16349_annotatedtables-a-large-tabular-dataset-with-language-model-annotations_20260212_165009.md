---
ver: rpa2
title: 'AnnotatedTables: A Large Tabular Dataset with Language Model Annotations'
arxiv_id: '2406.16349'
source_url: https://arxiv.org/abs/2406.16349
tags:
- data
- select
- tabular
- table
- code
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces AnnotatedTables, a large tabular dataset
  with LLM-generated annotations. The key method idea is to use LLMs to automatically
  annotate tabular data, addressing the scalability bottleneck of traditional human
  annotation.
---

# AnnotatedTables: A Large Tabular Dataset with Language Model Annotations

## Quick Facts
- arXiv ID: 2406.16349
- Source URL: https://arxiv.org/abs/2406.16349
- Authors: Yaojie Hu; Ilias Fountalis; Jin Tian; Nikolaos Vasiloglou
- Reference count: 40
- Primary result: LLM-annotated dataset of 32,119 databases with 405,616 valid SQL programs

## Executive Summary
This paper introduces AnnotatedTables, a large-scale tabular dataset with LLM-generated annotations. The authors address the scalability bottleneck of traditional human annotation by using zero-shot LLM prompts to generate SQL code and identify input-target columns for classification tasks. The dataset contains 32,119 databases with 405,616 valid SQL programs, making it the largest SQL dataset with associated tabular data that supports query execution. The study demonstrates that LLMs can effectively annotate tabular data for downstream tasks like SQL translation and classification model evaluation.

## Method Summary
The authors use LLMs to automatically annotate tabular datasets at scale, focusing on SQL code generation and input-target column identification. They collect 70,000 Kaggle datasets, filter for size and quality, then use zero-shot LLM prompts to generate SQL queries from database schemas and example rows. Valid SQL programs are identified through execution validation. For SQL-to-Rel translation, they employ Incremental Prompt Engineering with execution feedback. For classification tasks, LLMs identify numeric input columns and categorical target columns based on schema descriptions, enabling comparison of TabPFN and AutoGluon performance on 2,720 classification problems.

## Key Results
- LLMs generated 405,616 valid SQL programs across 32,119 databases with 82.25% execution validity
- SQL-to-Rel translation achieved 40% accuracy using Incremental Prompt Engineering with few-shot examples
- TabPFN neural tabular classifier performed on par with AutoGluon baseline on 2,720 classification problems
- Dataset represents the largest SQL dataset with associated tabular data supporting query execution

## Why This Works (Mechanism)

### Mechanism 1
LLMs can replace human annotation for tabular data at scale by leveraging zero-shot synthesis. The LLM is prompted with schema and example rows to generate SQL code, which is then validated through execution. This bypasses the need for human-labeled data. Core assumption: LLMs pre-trained on diverse text and code can generalize to novel tabular structures without explicit training. Evidence anchors: abstract claim of successful large-scale annotation, section 3.1 description of zero-shot SQL annotation prompting. Break condition: LLM generates invalid SQL or hallucinations that cannot be corrected through execution feedback.

### Mechanism 2
Incremental Prompt Engineering (IPE) improves LLM translation accuracy from SQL to Rel. Start with a small set of manual translation examples, generate translations, identify failures, and iteratively add examples that address failure patterns until accuracy converges. Core assumption: Execution feedback can guide effective example selection to teach the LLM the target language syntax and semantics. Evidence anchors: abstract claim of adequate translations with few-shot learning, section 4.2 description of IPE methodology. Break condition: Adding more examples yields diminishing returns or fails to address structural language differences.

### Mechanism 3
LLM-annotated input-target columns enable robust evaluation of TabPFN on diverse real-world data. The LLM identifies numeric input columns and categorical target columns based on schema descriptions, allowing both TabPFN and AutoGluon to be trained and compared. Core assumption: Schema alone is sufficient for the LLM to identify meaningful prediction relationships without examining actual data values. Evidence anchors: abstract claim of LLM-annotated input-target columns for TabPFN evaluation, section 5.2 description of schema-based column identification. Break condition: LLM fails to identify columns that actually support non-trivial classification, resulting in poor model performance.

## Foundational Learning

- Schema representation and data type inference
  - Why needed here: LLMs need structured descriptions of tables to generate meaningful SQL and identify ML-ready columns
  - Quick check question: Can you extract column names and types from a CSV file programmatically?

- SQL execution and validation
  - Why needed here: Generated SQL must be validated for correctness and non-empty results to ensure annotation quality
  - Quick check question: How would you determine if a SQL query returns valid results programmatically?

- Few-shot in-context learning
  - Why needed here: LLMs learn new languages (like Rel) from examples without gradient updates
  - Quick check question: What makes a good few-shot example for teaching a new programming language?

## Architecture Onboarding

- Component map: Data collection -> Schema extraction -> LLM annotation -> Validation -> Translation -> Evaluation
- Critical path: Schema extraction → LLM annotation → Execution validation → Dataset release
- Design tradeoffs:
  - Using first row as example vs. random sampling (simplicity vs. representativeness)
  - Execution timeout duration (completeness vs. cost)
  - Number of few-shot examples (accuracy vs. prompt length limits)
- Failure signatures:
  - High invalid SQL rate → Prompt needs refinement or schema extraction issues
  - Translation accuracy plateaus early → Language gap too wide for few-shot learning
  - Model performance variance → Poor input-target column annotation quality
- First 3 experiments:
  1. Run schema extraction on 10 Kaggle datasets and verify generated descriptions
  2. Generate SQL for one database and validate execution results
  3. Create 5 manual SQL-to-Rel translation examples and test LLM translation accuracy

## Open Questions the Paper Calls Out

### Open Question 1
Can the LLM annotation methodology be extended to other programming languages beyond SQL and Rel? The paper demonstrates successful translation from SQL to Rel, a new language not previously learned by LLMs, suggesting potential for extending the methodology to other languages. This remains unresolved as the paper focuses on SQL and Rel as examples without exploring other programming languages. Experimental results showing successful LLM annotation and translation for other programming languages would resolve this question.

### Open Question 2
How does the quality of LLM-generated annotations compare to human-generated annotations in terms of accuracy and completeness? The paper reports on the quality of LLM-generated SQL annotations with 82.25% validity but does not directly compare to human-generated annotations. A comparative study between LLM-generated and human-generated annotations, measuring accuracy, completeness, and other relevant metrics, would resolve this question.

### Open Question 3
Can the LLM annotation methodology be applied to other types of data beyond tabular data, such as text or image data? The paper focuses on tabular data annotation but mentions that LLMs have been used for data annotation on text domains. Experimental results showing successful LLM annotation for other data types, such as text or image data, using similar techniques would resolve this question.

## Limitations

- Lack of detailed prompt specifications creates uncertainty about reproducibility
- Validation methodology relies heavily on execution feedback, which may miss semantic correctness issues
- TabPFN evaluation uses only AUROC and cross-entropy metrics, omitting potentially important measures

## Confidence

- **High Confidence**: The core methodology of using LLMs for zero-shot SQL generation and basic dataset construction approach
- **Medium Confidence**: The execution validation framework and 40% translation accuracy claim
- **Low Confidence**: The TabPFN evaluation results due to incomplete specifications and limited metrics

## Next Checks

1. Reconstruct the exact LLM prompts used for SQL generation and input-target column identification by systematically testing variations of the described schema format with different datasets to match the reported execution accuracy patterns.

2. Generate a small set of SQL-to-Rel translation examples following the IPE methodology described, then attempt to replicate the 40% accuracy claim by testing on held-out SQL queries to validate the translation approach.

3. Re-run the TabPFN vs AutoGluon comparison using the same 2,720 classification problems but include additional metrics (accuracy, F1-score) and verify the input-target column identification quality by examining model performance variance across different column selections.