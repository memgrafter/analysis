---
ver: rpa2
title: 'Reclaiming the Source of Programmatic Policies: Programmatic versus Latent
  Spaces'
arxiv_id: '2410.12166'
source_url: https://arxiv.org/abs/2410.12166
tags:
- space
- search
- programmatic
- move
- latent
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper challenges the assumption that learned latent spaces
  for programmatic policies are more conducive to search than the original programmatic
  space. The authors compare hill climbing in the original DSL space with CEBS and
  HPRL searching in learned latent spaces across KARELL tasks.
---

# Reclaiming the Source of Programmatic Policies: Programmatic versus Latent Spaces

## Quick Facts
- **arXiv ID**: 2410.12166
- **Source URL**: https://arxiv.org/abs/2410.12166
- **Reference count**: 40
- **Primary result**: Hill climbing in the original DSL space outperforms latent space methods for programmatic policy synthesis

## Executive Summary
This paper challenges the prevailing assumption that learned latent spaces are superior for searching programmatic policies compared to the original domain-specific language (DSL) space. Through systematic experiments on KAREL tasks, the authors demonstrate that hill climbing directly in the programmatic space consistently outperforms cross-entropy methods searching in latent spaces, even when both spaces exhibit similar behavior similarity. The key finding is that the programmatic space is more "conducive to search," meaning it's more likely to find high-performing policies without getting stuck in local optima. This work suggests that the original programmatic space should be used as a baseline when evaluating new latent space approaches for programmatic policy synthesis.

## Method Summary
The paper compares two approaches for synthesizing programmatic policies: direct search in the original DSL space (PROGRAMMATIC SPACE) using hill climbing, versus search in a learned latent space using Cross-Entropy Beam Search (CEBS). The latent space is learned via LEAPS methodology with behavior loss, creating a compressed representation where similar programs are close together. Both methods are evaluated on KARELL tasks in the KAREL environment, with performance measured by episodic return and behavior similarity metrics. The study uses a probabilistic context-free grammar for program generation and evaluates with 16 initial states per task.

## Key Results
- Hill climbing in the programmatic space consistently outperforms CEBS and HPRL searching in latent spaces across KARELL tasks
- Both spaces show similar behavior similarity values, yet the programmatic space yields higher episodic returns
- Algorithms are more likely to get stuck in local maxima when searching in the latent space compared to the programmatic space
- Sampling from the programmatic space is more than 10 times faster than sampling from the latent space

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The programmatic space is more conducive to search because its topology allows hill climbing to avoid local maxima more effectively than in the latent space.
- Mechanism: The programmatic space's reward function combined with the neighborhood function creates a landscape where hill climbing is more likely to find high-return policies without getting stuck.
- Core assumption: The neighborhood function in the programmatic space provides better exploration of promising regions compared to the Gaussian noise-based neighborhoods in the latent space.
- Evidence anchors:
  - [abstract] "We discovered that algorithms are more likely to stop at local maxima when searching in the latent space than when searching in the programmatic space."
  - [section] "We observed that the programmatic space is never worse and is often much superior to the latent space for a wide range of target reward values."
  - [corpus] Weak - corpus doesn't directly address search topology differences.
- Break condition: If the neighborhood function or reward landscape changes significantly, this advantage could disappear.

### Mechanism 2
- Claim: The programmatic space provides comparable behavior locality to the learned latent space without requiring training.
- Mechanism: The DSL's structure naturally creates locality in program behavior, similar to what the latent space tries to learn through behavior loss optimization.
- Core assumption: The probabilistic context-free grammar used to generate programs already captures the locality properties that the latent space attempts to learn.
- Evidence anchors:
  - [abstract] "we show that the programmatic space, induced by the domain-specific language and requiring no training, presents values for the behavior loss similar to those observed in latent spaces presented in previous work."
  - [section] "Although the behavior-similarity metric is one of the objectives of the LATENT SPACE, the PROGRAMMATIC SPACE achieves comparable behavior-similarity values."
  - [corpus] Weak - corpus doesn't directly address behavior locality comparisons.
- Break condition: If the DSL changes or if the behavior loss metric becomes more sophisticated, this equivalence might break.

### Mechanism 3
- Claim: Searching in the programmatic space is more efficient than searching in the latent space in terms of computation time.
- Mechanism: Generating neighbors in the programmatic space is computationally cheaper than sampling from a Gaussian distribution in the latent space.
- Core assumption: The mutation-based neighbor generation in the programmatic space has lower computational overhead than the decoding process in the latent space.
- Evidence anchors:
  - [section] "sampling from the programmatic space is more than 10 times faster than sampling from the latent space."
  - [corpus] Weak - corpus doesn't provide direct evidence about computational efficiency.
- Break condition: If the latent space decoding becomes significantly optimized or if the mutation process becomes more complex, this efficiency advantage could diminish.

## Foundational Learning

- Concept: Domain-specific languages (DSLs) and context-free grammars
  - Why needed here: Understanding how programmatic policies are represented and manipulated is fundamental to grasping the search space comparison
  - Quick check question: Can you explain how a context-free grammar defines the structure of programmatic policies in this work?

- Concept: Variational autoencoders (VAEs) and latent space learning
  - Why needed here: The latent space approach relies on learning a compressed representation of programs using VAEs
  - Quick check question: How does the behavior loss in VAEs ensure that similar programs are close in the latent space?

- Concept: Local search algorithms (hill climbing, CEM, CEBS)
  - Why needed here: These are the primary methods used to search for good policies in both spaces
  - Quick check question: What's the key difference between hill climbing and cross-entropy methods in terms of escaping local optima?

## Architecture Onboarding

- Component map:
  - KAREL environment: Grid world with robot that can move, turn, and manipulate markers
  - DSL: Context-free grammar defining programmatic policies
  - Programmatic space: Direct search over ASTs generated by the DSL
  - Latent space: VAE-learned embedding of programs with behavior locality
  - Search algorithms: Hill climbing, CEM, CEBS for policy synthesis
  - Evaluation metrics: Behavior similarity, convergence rate, episodic return

- Critical path:
  1. Define initial state distribution and reward function for target task
  2. Sample initial program (either from DSL or latent space)
  3. Generate neighbors using neighborhood function
  4. Evaluate neighbors on task
  5. Select best neighbor and repeat until convergence

- Design tradeoffs:
  - Programmatic space: No training required, but potentially larger search space
  - Latent space: Compressed representation, but requires training and may lose important structure
  - Hill climbing: Simple but prone to local optima
  - CEM/CEBS: Better at escaping local optima but more complex and computationally expensive

- Failure signatures:
  - Poor convergence rate indicates unfavorable search space topology
  - High identity rate suggests neighborhoods aren't providing meaningful variation
  - Similar behavior similarity but different performance suggests other factors at play

- First 3 experiments:
  1. Implement hill climbing in programmatic space and compare convergence rate to latent space baseline
  2. Measure behavior similarity and identity rate for both spaces across different neighborhood sizes
  3. Test initialization methods (DSL vs latent sampling) to verify their impact on search performance

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Under what specific conditions (e.g., DSL properties, task complexity, reward structure) does searching in the programmatic space consistently outperform searching in learned latent spaces?
- Basis in paper: [explicit] The authors compare hill climbing in the original DSL space with CEBS and HPRL searching in learned latent spaces across KARELL tasks and find that hill climbing in the programmatic space consistently outperforms methods searching in latent spaces.
- Why unresolved: The paper provides empirical evidence but does not identify the underlying factors that make the programmatic space more conducive to search in certain scenarios.
- What evidence would resolve it: Controlled experiments varying DSL properties (e.g., expressiveness, differentiability), task characteristics (e.g., multi-stage requirements, reward sparsity), and latent space design choices (e.g., compression ratio, locality optimization) to isolate the conditions favoring programmatic space search.

### Open Question 2
- Question: Can the "friendliness" of a search space to local search algorithms be predicted or quantified a priori based on DSL characteristics or latent space properties?
- Basis in paper: [inferred] The authors measure the "friendliness" of the two spaces to local search algorithms by measuring the probability of a hill-climbing search converging to a solution with at least a given target reward value, and find that the programmatic space is more likely to yield policies with higher episodic return.
- Why unresolved: The paper measures friendliness empirically but does not provide a theoretical framework for predicting it based on space characteristics.
- What evidence would resolve it: Development of metrics or analytical tools that can estimate the search space topology's friendliness based on DSL structure, reward function properties, or latent space training objectives, validated through empirical correlation with actual search performance.

### Open Question 3
- Question: What latent space design improvements could bridge the performance gap between searching in programmatic and latent spaces for programmatic policy synthesis?
- Basis in paper: [explicit] The authors suggest that the effectiveness of latent spaces depends on how much the latent space compresses the original space and how conducive to search the space is, and their empirical results suggest that current systems lack either or both of these properties.
- Why unresolved: The paper identifies the need for better latent space design but does not propose specific improvements or evaluate them.
- What evidence would resolve it: Development and empirical evaluation of novel latent space architectures or training objectives that explicitly optimize for search-friendliness and compression, demonstrating improved performance relative to the original programmatic space baseline.

## Limitations
- The results may not generalize to other domains or DSLs with different locality properties
- The comparison assumes both spaces have comparable behavior similarity, which might break under different behavior loss formulations
- The computational efficiency advantage of programmatic space is asserted but not empirically validated with timing measurements

## Confidence
- **High confidence** in the empirical finding that hill climbing in programmatic space outperforms CEBS/HPRL in latent space for KAREL tasks
- **Medium confidence** in the mechanism explanation about search topology and local maxima avoidance
- **Low confidence** in the generalizability of results to other programmatic policy synthesis domains

## Next Checks
1. **Cross-domain validation**: Test the programmatic vs latent space comparison on a different DSL and task domain (e.g., string manipulation or arithmetic expression synthesis) to verify if the programmatic space advantage generalizes beyond KAREL.

2. **Behavior loss sensitivity analysis**: Systematically vary the behavior loss formulation and measure how it affects the equivalence between programmatic and latent space behavior similarity, testing whether the claimed advantage persists under different locality definitions.

3. **Computational efficiency measurement**: Implement precise timing measurements for neighbor generation in both spaces and validate the claimed 10x efficiency advantage, including overhead from VAE decoding and AST mutation operations.