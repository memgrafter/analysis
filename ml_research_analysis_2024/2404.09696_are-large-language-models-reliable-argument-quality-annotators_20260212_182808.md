---
ver: rpa2
title: Are Large Language Models Reliable Argument Quality Annotators?
arxiv_id: '2404.09696'
source_url: https://arxiv.org/abs/2404.09696
tags:
- quality
- human
- argument
- annotations
- expert
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigates the reliability of large language models
  (LLMs) as annotators for argument quality. We compared GPT-3 and PaLM 2 annotations
  with those from human experts and novices using the established argument quality
  taxonomy by Wachsmuth et al.
---

# Are Large Language Models Reliable Argument Quality Annotators?

## Quick Facts
- arXiv ID: 2404.09696
- Source URL: https://arxiv.org/abs/2404.09696
- Reference count: 40
- Key outcome: LLMs, particularly PaLM 2, provide more consistent argument quality evaluations than humans and can improve inter-annotator agreement when added to human annotations

## Executive Summary
This study investigates the reliability of large language models as annotators for argument quality, comparing their performance against human experts and novices. Using the established Wachsmuth et al. taxonomy, we prompted GPT-3 and PaLM 2 to evaluate 320 arguments across 15 quality dimensions. Our results demonstrate that LLMs achieve substantially higher agreement consistency (Krippendorff's α up to 0.99) compared to human annotators (α up to 0.45), while also showing moderate agreement with human expert ratings (α up to 0.71). Notably, incorporating LLM annotations significantly improved agreement between human annotators, suggesting LLMs can serve as valuable tools for automated argument quality assessment.

## Method Summary
The study employed two LLMs (GPT-3.5-turbo-0613 and PaLM 2 text-bison@001) to evaluate arguments from the Dagstuhl-15512-ArgQuality corpus. We generated expert and novice prompt variants for each model, running multiple iterations per argument-quality dimension pair using nucleus sampling with fixed parameters. Human annotations came from three experts and 108 novice students. Inter-annotator agreement was measured using Krippendorff's alpha, comparing consistency within and between human and LLM groups, and analyzing the impact of adding LLM annotations to human annotation sets.

## Key Results
- LLMs achieved near-perfect agreement consistency (Krippendorff's α up to 0.99) compared to human experts (α up to 0.45)
- PaLM 2 showed higher agreement with human expert ratings (α up to 0.71) than GPT-3 across multiple dimensions
- Adding LLM annotations significantly improved agreement between human annotators (p < 0.05)
- Expert prompts aligned better with expert human annotations, while novice prompts aligned with novice annotations

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLMs produce more consistent argument quality ratings than humans
- Mechanism: LLM annotation runs using nucleus sampling generate low-variance outputs for the same prompt, simulating consistent "annotators"
- Core assumption: Multiple LLM runs with the same prompt and parameters approximate independent annotators
- Evidence anchors:
  - [abstract] "LLMs, particularly PaLM 2, provide more consistent evaluations than humans (Krippendorff's α up to 0.99 for models vs. 0.45 for experts)"
  - [section] "LLM agreement between annotation repetitions is substantially higher... PaLM 2 model shows near-perfect agreement for both expert and novice prompts"
- Break condition: If nucleus sampling variance is too low or deterministic, different runs will not simulate independent annotators

### Mechanism 2
- Claim: LLMs can improve inter-annotator agreement when added to human annotations
- Mechanism: LLM ratings serve as a common reference point, reducing subjective variance between human annotators
- Core assumption: LLM ratings are perceived as objective and align well enough with human quality standards
- Evidence anchors:
  - [abstract] "Moreover, we show that using LLMs as additional annotators can significantly improve the agreement between annotators"
  - [section] "adding PaLM 2 annotations can significantly improve the agreement of human experts as well as human novices"
- Break condition: If LLM ratings systematically disagree with human ratings, adding them will decrease overall agreement

### Mechanism 3
- Claim: Prompt type affects LLM agreement with human ratings
- Mechanism: Simplified novice prompts make LLM reasoning more aligned with novice annotator definitions, while expert prompts align with expert definitions
- Core assumption: LLMs can adjust their reasoning based on prompt complexity and definitions
- Evidence anchors:
  - [section] "GPT-3 with expert prompts shows a higher agreement with human expert annotations than with human novice annotations, and a similar trend is observed for GPT-3 with novice prompts and human novices"
  - [section] "annotations generated by PaLM 2 have a similar distribution to that of human annotators"
- Break condition: If LLMs cannot differentiate between prompt types, agreement patterns will not change

## Foundational Learning

- Concept: Krippendorff's alpha for inter-annotator agreement
  - Why needed here: Used to quantify consistency and agreement between human and LLM annotations
  - Quick check question: What does a Krippendorff's alpha of 0.99 indicate about annotation consistency?

- Concept: Nucleus sampling for text generation
  - Why needed here: Controls output diversity in LLM runs, simulating different annotators
  - Quick check question: How does temperature setting in nucleus sampling affect output variance?

- Concept: Argument quality taxonomy
  - Why needed here: Provides the 15-dimensional framework for evaluating arguments
  - Quick check question: What are the three main categories of argument quality dimensions in the Wachsmuth taxonomy?

## Architecture Onboarding

- Component map:
  - Dagstuhl-15512-ArgQuality corpus (320 arguments) -> Human annotators (expert group of 3, novice group of 108) -> LLM prompts (expert and novice variants) -> GPT-3.5-turbo-0613 and PaLM 2 text-bison@001 -> Krippendorff's alpha calculations

- Critical path:
  1. Load arguments and quality dimensions
  2. Generate prompts for each annotator group and LLM
  3. Run LLM inference with fixed parameters (temperature=0.3, p=1.0, k=40, max_tokens=256)
  4. Calculate Krippendorff's alpha within and between groups
  5. Analyze agreement patterns and significance

- Design tradeoffs:
  - Using multiple LLM runs vs. single run: More runs increase confidence but cost more
  - Expert vs. novice prompts: Expert prompts may align better with established standards but are harder for novices
  - PaLM 2 vs. GPT-3: PaLM 2 shows better agreement but may have content policy restrictions

- Failure signatures:
  - Low agreement between LLM runs: Indicates unstable sampling or model issues
  - Systematic disagreement with human ratings: Suggests model misunderstanding of quality dimensions
  - High content policy rejections: PaLM 2 may filter arguments with sensitive content

- First 3 experiments:
  1. Run LLM annotation on a small subset (10 arguments) with both prompt types to verify agreement patterns
  2. Compare LLM agreement with human expert vs. novice subsets to validate prompt alignment
  3. Test impact of adding 1-3 LLM annotations to human annotations to confirm agreement improvement

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do the underlying architectures and training methodologies of GPT-3 and PaLM 2 contribute to their different performances in argument quality annotation tasks?
- Basis in paper: [inferred] The paper notes that PaLM 2 and GPT-3 show different agreement levels with human annotations and that PaLM 2 exhibits near-perfect agreement for both expert and novice prompts, but shows a notable drop when asked to explain its reasoning, whereas GPT-3 shows a slight improvement in agreement when asked to provide an explanation.
- Why unresolved: The paper does not delve into the specific architectural or training differences between the two models that could explain these performance variations.
- What evidence would resolve it: Detailed comparative analysis of the models' architectures and training datasets, along with experiments isolating the impact of these factors on annotation performance.

### Open Question 2
- Question: Can fine-tuning LLMs based on human judgments of argument quality improve their alignment with human annotations?
- Basis in paper: [explicit] The paper mentions the potential of using few-shot prompting or fine-tuning LLMs based on human judgments of argument quality to enhance agreement with human annotations.
- Why unresolved: The paper does not explore the effects of fine-tuning on model performance, focusing instead on zero-shot prompting techniques.
- What evidence would resolve it: Experimental results comparing the performance of fine-tuned models against zero-shot prompting and human annotations.

### Open Question 3
- Question: How generalizable are the results of using LLMs for argument quality assessment to other domains beyond argumentation?
- Basis in paper: [inferred] The paper suggests that the task of argument quality assessment might be a worst-case scenario for LLMs due to its complexity and subjectivity, implying that results could be better in less subjective domains.
- Why unresolved: The paper does not provide empirical evidence or theoretical analysis to support the generalizability of its findings to other domains.
- What evidence would resolve it: Cross-domain studies applying LLMs to assess quality in different types of texts (e.g., technical documents, creative writing) and comparing the results with human annotations.

## Limitations

- The study focused on a single argument corpus (Dagstuhl-15512-ArgQuality) and specific LLMs (GPT-3.5 and PaLM 2), limiting generalizability to other domains or models
- While PaLM 2 showed superior agreement, the study did not explore why GPT-3 performed differently or investigate potential model-specific limitations
- The exact definitions and guidelines used for novice annotations were simplified but not fully specified in the paper, which may affect reproducibility

## Confidence

- **High Confidence**: LLMs produce more consistent ratings than humans (Krippendorff's α up to 0.99 for models vs. 0.45 for experts)
- **Medium Confidence**: LLMs can improve inter-annotator agreement when added to human annotations (statistically significant improvement with p < 0.05)
- **Medium Confidence**: Prompt type affects LLM agreement with human ratings, with expert prompts aligning better with expert annotations

## Next Checks

1. Test LLM annotation reliability across multiple argument datasets beyond Dagstuhl-15512-ArgQuality to assess generalizability
2. Investigate the impact of different nucleus sampling parameters on LLM agreement consistency
3. Compare LLM annotation performance against alternative automated argument quality assessment methods