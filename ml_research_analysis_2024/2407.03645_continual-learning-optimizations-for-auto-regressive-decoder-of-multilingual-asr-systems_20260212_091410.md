---
ver: rpa2
title: Continual Learning Optimizations for Auto-regressive Decoder of Multilingual
  ASR systems
arxiv_id: '2407.03645'
source_url: https://arxiv.org/abs/2407.03645
tags:
- learning
- languages
- arxiv
- decoder
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This work addresses catastrophic forgetting in multilingual ASR
  when adapting a pre-trained Whisper model to new languages. The authors propose
  four decoder-specific optimizations: removing gradient surgery from token embeddings,
  freezing unused embeddings, suppressing output of newly added language ID tokens,
  and reducing learning rate more rapidly during training.'
---

# Continual Learning Optimizations for Auto-regressive Decoder of Multilingual ASR systems

## Quick Facts
- arXiv ID: 2407.03645
- Source URL: https://arxiv.org/abs/2407.03645
- Reference count: 0
- Key outcome: Decoder-specific optimizations reduce average WER on pre-trained languages from 14.2% to 12.4% compared to Experience Replay when adapting Whisper to new languages

## Executive Summary
This work addresses catastrophic forgetting in multilingual automatic speech recognition (ASR) when adapting pre-trained Whisper models to new languages. The authors identify that the auto-regressive decoder's token embeddings are particularly sensitive to gradient modifications during continual learning. They propose four decoder-specific optimizations: removing gradient surgery from token embeddings, freezing unused embeddings, suppressing output of newly added language ID tokens, and reducing learning rate more rapidly during training. Experiments on adapting Whisper to 10 new languages from Common Voice show significant improvements in maintaining performance on pre-trained languages.

## Method Summary
The method adapts a pre-trained Whisper model to new languages while preventing catastrophic forgetting of previously learned languages. The approach uses A-GEM (Average GEM) as the base continual learning method, with four decoder-specific optimizations: (1) removing gradient surgery from token embeddings to preserve their norm and angular information, (2) selectively freezing token embeddings not used by new languages, (3) suppressing output of newly added language ID tokens to prevent error propagation, and (4) using a variant of ReduceLROnPlateau scheduler that reduces learning rate more rapidly. The encoder is frozen during adaptation while the decoder is trained with these optimizations for 2 epochs on Common Voice data.

## Key Results
- Average WER on pre-trained languages reduced from 14.2% to 12.4% compared to Experience Replay baseline
- Method maintains performance on new languages while improving pre-trained language performance
- Four decoder optimizations work synergistically to address different aspects of catastrophic forgetting

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Removing gradient surgery from token embeddings reduces catastrophic forgetting in multilingual ASR
- Mechanism: Gradient surgery disrupts the norm and angular information of token embeddings which are crucial for next-token prediction in autoregressive decoding
- Core assumption: Token embeddings' norm and angular information are critical for maintaining ASR performance and sensitive to gradient modifications
- Evidence anchors: [abstract] and [section] statements about sensitivity of embeddings to gradient modifications; weak corpus evidence

### Mechanism 2
- Claim: Selectively freezing unused token embeddings reduces catastrophic forgetting
- Mechanism: Adapting embeddings for new languages can overwrite semantic information of previously learned languages; freezing unused embeddings preserves this information
- Core assumption: Sub-word token embeddings are shared across languages and their semantic information is critical for maintaining performance on previously learned languages
- Evidence anchors: [section] and [abstract] statements about semantic information overwriting; weak corpus evidence

### Mechanism 3
- Claim: Suppressing output of newly added language ID tokens reduces error propagation
- Mechanism: New language ID tokens may be wrongly transcribed in the middle of ASR output, inducing errors in later transcriptions
- Core assumption: Language ID tokens, when incorrectly transcribed, cause cascading errors in subsequent token predictions
- Evidence anchors: [section] and [abstract] statements about error propagation from ID tokens; weak corpus evidence

## Foundational Learning

- Concept: Catastrophic forgetting in neural networks
  - Why needed here: The paper aims to prevent catastrophic forgetting when adapting multilingual ASR models to new languages
  - Quick check question: What is catastrophic forgetting and why does it occur when fine-tuning neural networks on new data?

- Concept: Auto-regressive decoding in transformer models
  - Why needed here: The paper focuses on optimizing the decoder of an auto-regressive ASR model, which predicts tokens sequentially based on previous tokens
  - Quick check question: How does auto-regressive decoding work in transformer models and why is it particularly challenging for continual learning?

- Concept: Sub-word tokenization and embedding sharing
  - Why needed here: The paper leverages the fact that sub-word units are shared across languages to selectively freeze token embeddings and reduce forgetting
  - Quick check question: What are sub-word tokenization methods and how do they enable embedding sharing across different languages in multilingual models?

## Architecture Onboarding

- Component map: Input speech → Encoder → Token embeddings + positional embeddings → Decoder layers → Output tokens
- Critical path: 1. Input speech → Encoder → Token embeddings + positional embeddings → Decoder layers → Output tokens; 2. For new language adaptation: Token embeddings selectively updated, decoder layers adapted with modified gradient surgery, language ID tokens added and suppressed
- Design tradeoffs: Freezing unused embeddings vs. adapting all embeddings (reduced forgetting vs. potential performance loss); Language ID token suppression vs. natural output (reduced error propagation vs. potential loss of language identification); Rapid LR reduction vs. slower adaptation (reduced forgetting vs. potentially slower convergence)
- Failure signatures: Increased WER on pre-trained languages despite optimization; Degradation in new language performance after applying optimizations; Language ID tokens appearing incorrectly in transcriptions; Slow convergence during adaptation to new languages
- First 3 experiments: 1. Compare A-GEM with and without gradient surgery removal on token embeddings to verify the impact on WER; 2. Test partial embedding update by comparing frozen vs. fully adapted embeddings for new languages; 3. Evaluate language ID token suppression by measuring error propagation with and without token output suppression

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the proposed decoder-layer gradient surgery compare to other gradient-based CL methods specifically designed for auto-regressive decoders?
- Basis in paper: [explicit] The paper mentions that existing CL methods like EWC and MAS do not reduce forgetting as much as A-GEM and ER when applied to MASR with auto-regressive decoders, but it does not compare the proposed decoder-layer gradient surgery to other gradient-based methods specifically designed for auto-regressive decoders.
- Why unresolved: The paper does not provide a direct comparison of the proposed decoder-layer gradient surgery to other gradient-based CL methods designed for auto-regressive decoders.
- What evidence would resolve it: An experimental comparison of the proposed decoder-layer gradient surgery to other gradient-based CL methods specifically designed for auto-regressive decoders on MASR tasks.

### Open Question 2
- Question: What is the optimal balance between freezing unused token embeddings and updating embeddings used by new languages to minimize forgetting while maintaining performance on new languages?
- Basis in paper: [explicit] The paper proposes selectively freezing part of the token embeddings to mitigate CF, but does not explore the optimal balance between freezing unused embeddings and updating embeddings used by new languages.
- Why unresolved: The paper does not provide an experimental analysis of the trade-off between freezing unused embeddings and updating embeddings used by new languages.
- What evidence would resolve it: An experimental study varying the proportion of frozen and updated token embeddings and measuring the impact on forgetting and performance on new languages.

### Open Question 3
- Question: How does the proposed language ID token suppression strategy generalize to languages with different token distributions or to tasks beyond LID?
- Basis in paper: [inferred] The paper proposes suppressing the output of newly added language ID tokens to reduce error propagation, but does not explore the generalizability of this strategy to languages with different token distributions or to tasks beyond LID.
- Why unresolved: The paper does not provide an analysis of the generalizability of the language ID token suppression strategy.
- What evidence would resolve it: An experimental evaluation of the language ID token suppression strategy on languages with different token distributions and on tasks beyond LID.

## Limitations
- Evaluation scope is narrow, testing only 10 pre-trained and 10 new languages from Common Voice
- Sample size insufficient to establish general effectiveness across diverse language families and data distributions
- Improvements over Experience Replay are modest (12.4% vs 14.2% AWER), suggesting incremental rather than fundamental solution

## Confidence
- Medium Confidence: Core mechanism of removing gradient surgery from token embeddings - intuitively plausible but lacks direct empirical validation beyond performance metrics
- Medium Confidence: Selective freezing of unused token embeddings - strong theoretical justification but no ablation studies showing specific contribution
- Medium Confidence: Language ID token suppression mechanism - identifies error propagation but doesn't demonstrate whether suppression might cause other issues or whether alternatives could work better

## Next Checks
1. **Ablation study validation**: Systematically remove each of the four decoder optimizations to quantify their individual contributions to overall performance improvement
2. **Cross-domain robustness testing**: Evaluate the approach on languages with significantly different phonological and morphological structures than those tested (e.g., tonal languages, languages with non-Latin scripts)
3. **Long-term adaptation analysis**: Extend continual learning scenario beyond two phases to include multiple rounds of adaptation with new languages to reveal gradual forgetting patterns