---
ver: rpa2
title: 'Shortened LLaMA: Depth Pruning for Large Language Models with Comparison of
  Retraining Methods'
arxiv_id: '2402.02834'
source_url: https://arxiv.org/abs/2402.02834
tags:
- pruning
- ours
- network
- llm-pruner
- pruned
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Depth pruning of large language models can improve inference speed
  under memory constraints while maintaining competitive zero-shot performance compared
  to width pruning methods. The approach removes entire Transformer blocks identified
  by importance metrics like perplexity or Taylor-based scores, followed by LoRA-based
  retraining.
---

# Shortened LLaMA: Depth Pruning for Large Language Models with Comparison of Retraining Methods

## Quick Facts
- arXiv ID: 2402.02834
- Source URL: https://arxiv.org/abs/2402.02834
- Authors: Bo-Kyeong Kim; Geonmin Kim; Tae-Ho Kim; Thibault Castells; Shinkook Choi; Junho Shin; Hyoung-Kyu Song
- Reference count: 18
- Key outcome: Depth pruning achieves faster inference than width pruning while maintaining zero-shot performance

## Executive Summary
This work introduces a depth pruning methodology for large language models that removes entire Transformer blocks based on importance metrics like perplexity or Taylor-based scores. The pruned models are then retrained using LoRA to restore performance. The approach demonstrates that depth pruning can achieve comparable zero-shot performance to width pruning while offering better inference speed, particularly at small batch sizes where width pruning is ineffective. The method reduces memory usage and enables larger batch sizes without out-of-memory errors.

## Method Summary
The methodology employs importance metrics to identify and remove less critical Transformer blocks from large language models. Two pruning strategies are evaluated: removing blocks with high perplexity scores or those identified by Taylor-based importance metrics. After pruning, models undergo LoRA-based retraining to restore performance. The approach is evaluated against width pruning methods across various model sizes, comparing zero-shot performance, inference speed, and memory usage. The depth pruning method shows particular advantages in latency-sensitive applications and scenarios where memory constraints limit batch sizes.

## Key Results
- Depth pruning achieves comparable zero-shot performance to width pruning on standard benchmarks
- Pruned models demonstrate faster inference speeds than both original and width-pruned models, especially at small batch sizes
- Memory usage is reduced, enabling larger batch sizes without out-of-memory errors

## Why This Works (Mechanism)
Depth pruning works by removing entire Transformer blocks that contribute less to the model's overall performance, as determined by importance metrics. This approach preserves the model's architectural integrity while reducing computational complexity. The subsequent LoRA-based retraining allows the model to adapt to the reduced depth without requiring full fine-tuning, making the process more efficient. The method particularly benefits latency-sensitive applications where width pruning may be less effective.

## Foundational Learning
- Transformer architecture fundamentals - needed to understand how depth pruning affects model structure and why removing entire blocks is viable
- Importance metrics for model components - needed to grasp how perplexity and Taylor-based scores identify pruneable blocks
- LoRA (Low-Rank Adaptation) - needed to understand the efficient retraining approach used after pruning
- Batch size optimization - needed to appreciate how reduced memory usage enables larger batch sizes
- Zero-shot learning evaluation - needed to understand the performance metrics used to compare pruning methods

## Architecture Onboarding
**Component Map:**
LLaMA Base Model -> Importance Metric Calculation -> Block Selection -> Pruning -> LoRA Retraining -> Evaluation

**Critical Path:**
Importance metric computation → Block selection → Pruning → Retraining → Performance evaluation

**Design Tradeoffs:**
Depth vs width pruning: Depth pruning offers better latency improvements but may affect model capacity differently than width pruning. Memory vs performance: Pruning reduces memory usage but requires careful retraining to maintain performance.

**Failure Signatures:**
- Significant performance drop after pruning indicating poor block selection
- Inability to converge during LoRA retraining suggesting too aggressive pruning
- Memory errors persisting after pruning indicating insufficient reduction

**3 First Experiments:**
1. Run importance metrics on a small LLaMA model to identify pruneable blocks
2. Apply depth pruning to a single Transformer block and evaluate performance impact
3. Test LoRA retraining on a minimally pruned model to verify restoration capability

## Open Questions the Paper Calls Out
None

## Limitations
- Importance metrics may not generalize well across all task types, particularly complex reasoning tasks
- Limited evaluation of fine-tuning stability and domain adaptation scenarios
- Trade-offs between model size reduction and practical deployment constraints are not fully explored

## Confidence
- High confidence in depth pruning maintaining competitive zero-shot performance
- Medium confidence in comparative advantages over width pruning
- Medium confidence in generalizability of pruning metrics across model families

## Next Checks
1. Evaluate pruned models on reasoning-intensive tasks and specialized domain benchmarks
2. Test approach across multiple model architectures beyond LLaMA
3. Conduct ablation studies comparing LoRA vs full fine-tuning impact on knowledge retention