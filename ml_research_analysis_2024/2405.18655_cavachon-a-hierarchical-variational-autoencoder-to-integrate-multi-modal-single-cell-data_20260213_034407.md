---
ver: rpa2
title: 'CAVACHON: a hierarchical variational autoencoder to integrate multi-modal
  single-cell data'
arxiv_id: '2405.18655'
source_url: https://arxiv.org/abs/2405.18655
tags:
- data
- modalities
- single-cell
- expression
- cell
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: CAVACHON is a probabilistic learning framework that integrates
  multi-modal single-cell data by explicitly modeling conditional independence relationships
  between modalities as a directed acyclic graph. The method uses a hierarchical variational
  autoencoder to learn shared and modality-specific latent representations, enabling
  tasks such as isolating common and distinct information across modalities, identifying
  modality contributions to differential gene expression, and performing integrated
  unsupervised multi-facet clustering.
---

# CAVACHON: a hierarchical variational autoencoder to integrate multi-modal single-cell data

## Quick Facts
- arXiv ID: 2405.18655
- Source URL: https://arxiv.org/abs/2405.18655
- Reference count: 40
- Method integrates multi-modal single-cell data by modeling conditional independence relationships between modalities as a directed acyclic graph using a hierarchical VAE

## Executive Summary
CAVACHON is a probabilistic learning framework that integrates multi-modal single-cell data by explicitly modeling conditional independence relationships between modalities as a directed acyclic graph. The method uses a hierarchical variational autoencoder to learn shared and modality-specific latent representations, enabling tasks such as isolating common and distinct information across modalities, identifying modality contributions to differential gene expression, and performing integrated unsupervised multi-facet clustering.

## Method Summary
CAVACHON is a generalized hierarchical variational autoencoder that integrates multi-modal single-cell data by modeling conditional independence relationships between modalities as a directed acyclic graph. The model uses modality-specific encoders and hierarchical encoders to map data to latent representations, which are then conditioned on their ancestors in the DAG during the decoding process. This architecture allows CAVACHON to learn shared and modality-specific latent representations, enabling tasks such as isolating common and distinct information across modalities, identifying modality contributions to differential gene expression, and performing integrated unsupervised multi-facet clustering. The model is trained sequentially following the topological order of the DAG, with progressive training to improve the ability to learn disentangled representations across layers.

## Key Results
- On SNARE-Seq mouse cortex data, CAVACHON effectively separated common and distinct chromatin accessibility and gene expression signals, revealing that gene expression differences between astrocytes and oligodendrocytes were not driven by chromatin accessibility.
- On 10k PBMCs data, the model identified that chromatin accessibility states, rather than transcription factor expression, primarily drove differential gene expression between cell types.
- CAVACHON provided a flexible framework for incorporating biological hypotheses into multi-modal single-cell data integration and analysis.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: CAVACHON learns shared and modality-specific latent representations by modeling conditional independence as a DAG, enabling isolation of common and distinct signals.
- Mechanism: The model uses a hierarchical VAE where each modality's latent representation is conditioned on its ancestors in the graph. This enforces that common features are captured by shared latents, while unique features are retained in modality-specific latents.
- Core assumption: The biological dependencies between modalities can be meaningfully represented as a DAG without cycles.
- Evidence anchors:
  - [abstract] "explicitly modeling conditional independence relationships between modalities as a directed acyclic graph"
  - [section 2.1] "if a directed edge exists from modality A to modality B, then the two modalities are conditionally independent given the latent distribution of modality A"
- Break condition: If the true biological dependencies are cyclic or involve feedback loops, the DAG assumption fails.

### Mechanism 2
- Claim: CAVACHON can decompose differential gene expression analysis into modality-specific contributions by creating chimeric molecular profiles.
- Mechanism: By swapping latent representations between cell types for a specific modality, the model generates synthetic cells with mixed molecular profiles. Comparing these to real cells isolates the contribution of that modality to observed expression differences.
- Core assumption: The generative model can accurately reconstruct data from swapped latents, implying the swapped modality is the primary driver.
- Evidence anchors:
  - [section 2.3] "our generative model is capable of creating a chimeric cell with a molecular profile that captures the chromatin accessibility landscape of cell type A and the gene expression profile of cell type B"
  - [section 3.2] "We computed their contribution scores... the majority of the differentially expressed genes are influenced by the states of chromatin accessibility"
- Break condition: If swapped latents do not produce biologically meaningful synthetic profiles, contribution scores are unreliable.

### Mechanism 3
- Claim: CAVACHON enables integrated unsupervised multi-facet clustering by learning modality-specific cluster assignments during training.
- Mechanism: The model includes mixture-of-Gaussians priors on latents and optimizes cluster assignments jointly with reconstruction, so each modality can have its own clustering while still reflecting dependencies.
- Core assumption: Cluster assignments learned during VAE training are stable and meaningful without post-hoc clustering.
- Evidence anchors:
  - [section 2.4] "multi-facet clustering also considers the dependency between modalities, reflecting the heterogeneity across different cells explained by each modality"
  - [section 3.3] "our model can learn clustering assignments conditionally in an unsupervised way for different modalities"
- Break condition: If the number of mixture components is misspecified or the training is unstable, learned clusters may be meaningless.

## Foundational Learning

- Variational Autoencoders
  - Why needed here: CAVACHON is built on hierarchical VAEs to approximate complex posteriors and model the generative process of multi-modal data.
  - Quick check question: What is the evidence lower bound (ELBO) and why is it optimized instead of the true log-likelihood?

- Directed Acyclic Graphs (DAGs) and conditional independence
  - Why needed here: The prior biological relationships are encoded as a DAG, and the model conditions each modality's latent on its ancestors.
  - Quick check question: If modality A points to modality B in the DAG, what does that imply about their statistical relationship?

- Bayesian differential analysis and Monte Carlo sampling
  - Why needed here: CAVACHON uses Bayesian factors to identify differentially expressed genes and decompose contributions across modalities.
  - Quick check question: How is the Bayesian factor K computed between two cell groups for a given modality?

## Architecture Onboarding

- Component map:
  - Modality-specific encoders (f(e)_m) → hierarchical encoders (f(r)_m, f(b)_m) → latents z_m
  - Conditional decoder (f(d)_m) reconstructs x_m from latents and batch info
  - Mix of Gaussians priors on cluster assignments c_m, with VaDE trick for online clustering
  - Tensorflow Dataset pipeline built from AnnData/MuData structures

- Critical path:
  - Data preprocessing → build DAG → topological sort → sequential training (progressive + conditional) → evaluate latent representations and contributions

- Design tradeoffs:
  - DAG assumption vs. flexibility: limits to acyclic dependencies
  - Progressive training vs. computational cost: improves disentanglement but increases epochs
  - Monte Carlo sampling for Bayesian analysis vs. approximation error: simple but potentially noisy

- Failure signatures:
  - Poor reconstruction loss: model underfits or latent dim too low
  - Unstable training: learning rate too high or sequential training order wrong
  - Biased contributions: DAG incorrect or batch effects not properly modeled

- First 3 experiments:
  1. Run CAVACHON on a paired scRNA + scATAC dataset with a simple two-node DAG (RNA ← ATAC) and visualize shared vs. distinct latent spaces.
  2. Create synthetic chimeric profiles by swapping latents between known cell types and verify reconstructed expression differences match expectations.
  3. Apply multi-facet clustering to an unlabeled dataset and check if learned clusters align with known cell type labels.

## Open Questions the Paper Calls Out

- How does the performance of CAVACHON compare to existing multi-modal integration methods like Seurat, Harmony, and GLUE when incorporating prior biological knowledge?
- What is the impact of the choice of data distribution (e.g. Bernoulli, zero-inflated negative binomial) on the performance of CAVACHON for different types of single-cell multi-omics data?
- How does the progressive training strategy used in CAVACHON compare to other training strategies like end-to-end training or greedy layer-wise training in terms of model performance and training stability?

## Limitations
- The DAG assumption may not hold for biological systems with cyclic dependencies or feedback loops
- Performance on datasets with more than two modalities or complex DAG structures is unclear
- The relationship between reconstruction accuracy and biological interpretability is not fully characterized

## Confidence

**High Confidence Claims**:
- The VAE architecture and training procedure are well-established and the implementation details are clearly specified
- The mechanism for isolating common vs. distinct information through conditional independence is mathematically sound
- The chimeric profile generation approach for differential analysis is valid within the model's assumptions

**Medium Confidence Claims**:
- The biological interpretations drawn from the SNARE-Seq and 10k PBMCs analyses
- The effectiveness of the progressive training approach for learning disentangled representations
- The stability and meaningfulness of multi-facet clustering without post-hoc validation

**Low Confidence Claims**:
- Generalization to datasets with different biological contexts or more complex dependency structures
- Performance on datasets with significant technical variation or batch effects
- Applicability to non-pairwise modality combinations

## Next Checks
1. Test CAVACHON on synthetic datasets with known dependency structures, systematically varying the DAG between different configurations (including cycles where possible) to evaluate how sensitive the results are to the graph structure assumption.
2. Generate synthetic datasets where the contribution of specific modalities to differential expression is controlled. Compare CAVACHON's contribution scores against ground truth to validate the accuracy of the Bayesian differential analysis approach.
3. Apply CAVACHON to a dataset with three or more modalities (e.g., RNA + ATAC + methylation) and evaluate whether the method can still effectively separate common and distinct signals while maintaining computational efficiency and interpretability.