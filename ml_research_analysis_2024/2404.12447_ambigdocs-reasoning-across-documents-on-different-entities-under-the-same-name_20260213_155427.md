---
ver: rpa2
title: 'AmbigDocs: Reasoning across Documents on Different Entities under the Same
  Name'
arxiv_id: '2404.12447'
source_url: https://arxiv.org/abs/2404.12447
tags:
- answer
- answers
- question
- entity
- documents
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces AmbigDocs, a new benchmark for evaluating
  language models' ability to reason across documents containing entities with the
  same name. The key idea is to leverage Wikipedia's disambiguation pages to generate
  questions about ambiguous entities and corresponding document-answer pairs.
---

# AmbigDocs: Reasoning across Documents on Different Entities under the Same Name

## Quick Facts
- **arXiv ID**: 2404.12447
- **Source URL**: https://arxiv.org/abs/2404.12447
- **Reference count**: 40
- **Primary result**: Introduces AmbigDocs benchmark showing state-of-the-art models struggle to distinguish entities and generate complete answers for ambiguous entity names

## Executive Summary
This paper introduces AmbigDocs, a new benchmark for evaluating language models' ability to reason across documents containing entities with the same name. The key idea is to leverage Wikipedia's disambiguation pages to generate questions about ambiguous entities and corresponding document-answer pairs. The authors establish an ontology categorizing four types of incomplete answers and develop automatic evaluation metrics. Experiments with state-of-the-art models show they struggle to distinguish entities and generate complete answers, often producing ambiguous or merged information. The paper lays the foundation for future work on multi-document reasoning with ambiguous entities.

## Method Summary
The AmbigDocs benchmark is constructed by leveraging Wikipedia disambiguation pages to identify entities sharing the same surface name. For each ambiguous name, documents about each entity are collected and questions are generated that can be answered differently by each entity. The dataset includes 36,098 examples with questions and corresponding sets of gold document-answer pairs. The evaluation uses retrieval-augmented generation with both gold documents and retrieved documents, employing open-source models (Llama2, Mistral) and commercial APIs (GPT-3.5, GPT-4). Models are evaluated using standard QA metrics plus a new ontology categorizing five types of answers (Complete, Partial, Ambiguous, Merged, No Answer) with automatic classification heuristics.

## Key Results
- Models generate ambiguous or merged answers instead of complete answers when reasoning about ambiguous entities
- Even with few-shot learning using complete answers as examples, performance remains poor
- Retrieved documents often distract models from gold documents, leading to incorrect answers
- Automatic categorization heuristic achieves Cohen's Kappa of 0.83 agreement with manual annotation

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Leveraging Wikipedia disambiguation pages creates a reliable source of entity pairs that share the same surface name but represent distinct real-world entities.
- Mechanism: The disambiguation pages explicitly list entities that share a name, providing both the surface name and the distinct entity names in one place. This eliminates the need for manual annotation of which entities share a name.
- Core assumption: Wikipedia's disambiguation pages are comprehensive and accurate enough to cover a meaningful set of ambiguous names across domains.
- Evidence anchors:
  - [abstract] "By leveraging Wikipedia’s disambiguation pages, we identify a set of documents, belonging to different entities who share an ambiguous name."
  - [section] "We leverage Wikipedia disambiguation pages, which provide multiple entities that can be referred to by the same surface name."
- Break condition: If Wikipedia's disambiguation pages are incomplete or contain errors, the dataset would miss important ambiguous entity cases or include incorrect pairings.

### Mechanism 2
- Claim: Generating questions that focus on specific attributes (like location, date, profession) ensures that each disambiguated entity has a distinct answer, making the ambiguity task meaningful.
- Mechanism: By crafting questions that target entity-specific attributes, the system guarantees that each entity in the disambiguation set will have a different answer, creating genuine ambiguity that requires entity disambiguation.
- Core assumption: There exists at least one attribute that differs between entities sharing the same name, and that attribute can be formulated into a natural question.
- Evidence anchors:
  - [abstract] "From these documents, we generate questions containing an ambiguous name and their corresponding sets of answers."
  - [section] "we proceed to generate the question qsn that can be answered differently by each document."
- Break condition: If all entities sharing a name have identical attributes, no meaningful question can be generated, or if the question generation produces questions that don't naturally map to the entities' distinguishing features.

### Mechanism 3
- Claim: The automatic categorization heuristic using token-level recall provides a practical way to evaluate whether models correctly distinguish entities and provide complete answers.
- Mechanism: By checking if both the answer and disambiguated entity name appear in the generated response (with token overlap thresholds), the system can automatically classify responses into complete, partial, ambiguous, merged, or no answer categories.
- Core assumption: Token overlap is a reliable proxy for whether an entity and its answer are correctly identified in the response.
- Evidence anchors:
  - [abstract] "We establish an ontology categorizing four types of incomplete answers and automatic evaluation metrics to identify such categories."
  - [section] "We develop heuristics to automatically classify the generated answer y into one of the categories proposed in Section 2."
  - [section] "The Cohen’s Kappa statistic between the two label sets is 0.83, showing strong agreement, though slightly lower than the human agreement of 0.85."
- Break condition: If token-level matching fails to capture semantic equivalence (e.g., synonyms, paraphrases) or if the thresholds are poorly calibrated, the categorization will misclassify responses.

## Foundational Learning

- Concept: Entity disambiguation and coreference resolution
  - Why needed here: The task fundamentally requires determining which entity a surface name refers to in context, which is a coreference resolution problem across multiple documents.
  - Quick check question: Given "Michael Jordan" in a document about basketball and another about economics, can you identify which specific Michael Jordan each document refers to based on context clues?

- Concept: Multi-document reasoning and information aggregation
  - Why needed here: The model must process multiple documents containing different entities with the same name and synthesize a complete answer that distinguishes between them.
  - Quick check question: If you have documents about two different people named "John Smith" - one a doctor and one an actor - how would you construct a single answer that correctly attributes each person's profession?

- Concept: Natural Language Inference and entailment checking
  - Why needed here: The dataset generation uses NLI to verify that generated answers are supported by the documents, ensuring answer quality and preventing hallucination.
  - Quick check question: Given a document stating "The conference was held in Austin in 2024" and a question "Where was the conference held?", does the answer "Austin" follow logically from the document?

## Architecture Onboarding

- Component map: Wikipedia disambiguation page scraper -> document pair selector -> question/answer generator -> answer set expander -> automatic categorization
- Critical path: Document pair selection -> question generation -> answer generation -> automatic categorization -> metric computation
- Design tradeoffs: The system trades manual annotation effort for synthetic data generation, which may introduce noise but enables large-scale evaluation.
- Failure signatures: High rates of "no answer" or "merged answer" categories indicate models struggle with entity disambiguation; low K-precision scores suggest hallucination; poor agreement between automatic and manual categorization indicates metric issues.
- First 3 experiments:
  1. Test automatic categorization on a small manually annotated sample to verify metric reliability (Cohen's Kappa calculation)
  2. Compare model performance with and without in-context examples to measure few-shot learning effectiveness
  3. Evaluate the impact of adding retrieved documents versus only using gold documents on model performance

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can we improve the automatic categorization of model outputs into the five answer categories (Complete, Partial, No Answer, Ambiguous, Merged)?
- Basis in paper: [inferred]
- Why unresolved: The paper reports that while the automatic categorization heuristic achieves high agreement with manual annotation (Cohen's Kappa of 0.83), there are still some misclassifications, particularly of Ambiguous answers being misclassified as Partial answers due to limitations of lexical-based heuristics. The paper notes that this is due to issues like plural forms and spelling variations, but doesn't explore potential solutions.
- What evidence would resolve it: A follow-up study that explores and evaluates different approaches to improve the automatic categorization, such as using more sophisticated natural language processing techniques or expanding the heuristics to handle more edge cases.

### Open Question 2
- Question: How do the results change when using a different set of in-context examples for few-shot learning?
- Basis in paper: [explicit]
- Why unresolved: The paper uses two in-context examples with complete answers for the few-shot learning experiment. It's unclear if these specific examples were chosen because they were particularly effective, or if the results would be similar with different examples.
- What evidence would resolve it: A study that tests the few-shot learning results with different sets of in-context examples, varying in content and quality, to determine the robustness of the approach and identify the characteristics of effective examples.

### Open Question 3
- Question: How does the performance of the models change when dealing with more than two disambiguated entities?
- Basis in paper: [inferred]
- Why unresolved: The paper focuses on questions with a maximum of two answers during the initial generation phase, although it expands to include more answers in a later step. The paper doesn't explore how the models perform when dealing with a larger number of disambiguated entities, which could be a more challenging scenario.
- What evidence would resolve it: A study that generates questions with a larger number of disambiguated entities and evaluates the performance of the models in this more complex setting.

## Limitations
- The approach relies heavily on Wikipedia's disambiguation pages, which may have coverage bias across domains and languages
- Automatic categorization heuristic using token-level recall may struggle with synonyms or paraphrased answers that are semantically correct but lexically different
- The dataset generation process involves multiple heuristics (question generation, answer expansion, NLI filtering) that could introduce cascading errors

## Confidence
**High Confidence**: The core methodology of using Wikipedia disambiguation pages to create entity pairs with shared surface names is well-established and the evaluation metrics (Answer Recall, Entity Recall, Entity-Answer Recall, Disambig-F1) are standard in QA evaluation.

**Medium Confidence**: The automatic categorization heuristic shows strong agreement with human annotations but relies on token-level matching which may not capture semantic equivalence. The claim that models struggle with entity disambiguation is supported by the experimental results but could be influenced by the specific prompts and few-shot examples used.

**Low Confidence**: The dataset generation process involves multiple steps (question generation, answer expansion, NLI filtering) where errors could propagate, but the paper doesn't provide extensive validation of each step's reliability. The generalizability of findings to other domains or languages remains unclear.

## Next Checks
1. **Categorization Heuristic Validation**: Manually annotate a random sample of 100 model outputs and compare against the automatic categorization results to verify the accuracy of the token-level recall thresholds and identify any systematic misclassifications.

2. **Robustness Across Domains**: Generate a subset of AmbigDocs examples from non-Wikipedia sources (e.g., news articles, scientific papers) to test whether the observed model performance generalizes beyond Wikipedia-derived data.

3. **Retrieval vs. Gold Document Comparison**: Conduct a controlled experiment where models must retrieve documents from a larger corpus rather than using gold documents, to assess whether the observed entity disambiguation challenges persist in more realistic retrieval scenarios.