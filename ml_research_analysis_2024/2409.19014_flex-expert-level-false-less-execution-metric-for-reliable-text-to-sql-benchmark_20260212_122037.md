---
ver: rpa2
title: 'FLEX: Expert-level False-Less EXecution Metric for Reliable Text-to-SQL Benchmark'
arxiv_id: '2409.19014'
source_url: https://arxiv.org/abs/2409.19014
tags:
- query
- ground
- truth
- schools
- prediction
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of unreliable evaluation in text-to-SQL
  benchmarks, where the common Execution Accuracy (EX) metric suffers from false positives
  and negatives due to coincidental database states or annotation issues. To resolve
  this, the authors introduce FLEX (False-Less EXecution), an LLM-based evaluation
  method that emulates human expert reasoning by analyzing comprehensive context and
  employing sophisticated evaluation criteria.
---

# FLEX: Expert-level False-Less EXecution Metric for Reliable Text-to-SQL Benchmark

## Quick Facts
- arXiv ID: 2409.19014
- Source URL: https://arxiv.org/abs/2409.19014
- Reference count: 40
- Key outcome: FLEX significantly improves agreement with human experts (from 62 to 87.04 in Cohen's kappa) compared to Execution Accuracy (EX) metric for text-to-SQL evaluation

## Executive Summary
This paper addresses the problem of unreliable evaluation in text-to-SQL benchmarks, where the common Execution Accuracy (EX) metric suffers from false positives and negatives due to coincidental database states or annotation issues. To resolve this, the authors introduce FLEX (False-Less EXecution), an LLM-based evaluation method that emulates human expert reasoning by analyzing comprehensive context and employing sophisticated evaluation criteria. FLEX significantly improves agreement with human experts (from 62 to 87.04 in Cohen's kappa) compared to EX. Re-evaluation of 50 models on Spider and BIRD benchmarks reveals that EX underestimates model performance by over 2.6 points on average, substantially affecting rankings. The study also identifies annotation quality as the primary cause of underestimation and highlights that model performance on challenging questions tends to be overestimated.

## Method Summary
FLEX (False-Less EXecution) is an LLM-based evaluation method that addresses the limitations of Execution Accuracy (EX) metric in text-to-SQL benchmarks. It uses comprehensive context analysis and sophisticated evaluation criteria to reduce false positives and negatives. FLEX builds context based on execution results - using TEQ criteria when results match and TN EQ when they differ. The method significantly improves agreement with human experts and reveals that EX metric underestimates model performance by 2.6 points on average.

## Key Results
- FLEX improves agreement with human experts from 62 to 87.04 in Cohen's kappa compared to EX
- EX metric underestimates model performance by over 2.6 points on average across 50 models
- Annotation quality is identified as the primary cause of underestimation in EX metric

## Why This Works (Mechanism)

### Mechanism 1
- Claim: FLEX reduces false positives by analyzing semantic differences between generated and ground truth queries even when execution results match.
- Mechanism: When Rgen(x) = Rgt(x), FLEX uses comprehensive context (question, schema, external knowledge) and evaluation criteria TEQ to detect logical discrepancies that might not affect current execution but could lead to incorrect results if database state changes.
- Core assumption: LLMs can effectively analyze query structure and logic to detect semantic differences beyond syntactic matching.
- Evidence anchors: [abstract]: "FLEX (False-Less EXecution), a novel approach leveraging LLMs to emulate expert-level evaluation of SQL queries"; [section 5.2]: "CEQ does not contain Rgt(x) and Rgen(x). LLM occasionally judges false positives as correct due to the equivalent execution results"

### Mechanism 2
- Claim: FLEX reduces false negatives by considering acceptable variations in output structure and value representation.
- Mechanism: When Rgen(x) â‰  Rgt(x), FLEX uses criteria TN EQ to evaluate whether structural differences (column order, extra columns) or value representation differences (formatting, percentages vs decimals) are acceptable given question phrasing.
- Core assumption: LLMs can understand question intent and determine when structural variations still correctly answer the question.
- Evidence anchors: [abstract]: "Our metric improves agreement with human experts (from 62 to 87.04 in Cohen's kappa) with comprehensive context and sophisticated criteria"; [section 5.2]: "CEQ contains Rgt(x), Rgen(x), and TN EQ which designed to evaluate semantic correctness between the question and Qgen(x)"

### Mechanism 3
- Claim: FLEX improves evaluation agreement with human experts by using detailed evaluation criteria instead of ambiguous rubrics.
- Mechanism: FLEX provides specific binary criteria for evaluating queries (schema alignment, correct filtering, handling nullable columns, etc.) rather than using vague scoring rubrics.
- Core assumption: Binary criteria with clear guidelines are more effective than numerical scoring for complex evaluation tasks.
- Evidence anchors: [section 4.3]: "Prometheus-2 employs a 1-5 rubric scoring system, but designing optimal rubrics becomes heuristic and ambiguous"; [section 5.1]: "CF LEX, including the question x, generated query Qgen(x), ground truth query Qgt(x), execution results Rgt(x) and Rgen(x), schema S, external knowledge K and two criteria TEQ and TN EQ"

## Foundational Learning

- Concept: Semantic equivalence in SQL queries
  - Why needed here: FLEX needs to understand when two different SQL queries produce the same results for the same question
  - Quick check question: Can you explain why "SELECT fname, lname FROM student ORDER BY score DESC LIMIT 1" and "SELECT fname, lname FROM student WHERE score == (SELECT MAX(score) FROM student)" might be semantically equivalent?

- Concept: False positives and false negatives in evaluation metrics
  - Why needed here: Understanding these concepts is crucial for grasping why EX metric fails and how FLEX addresses these issues
  - Quick check question: In the context of text-to-SQL evaluation, what's the difference between a false positive and a false negative?

- Concept: LLM-based evaluation systems
  - Why needed here: FLEX is built on using LLMs as evaluators, so understanding how these systems work is essential
  - Quick check question: What are the main advantages and disadvantages of using LLMs for automated evaluation compared to traditional metrics?

## Architecture Onboarding

- Component map: FLEX core (LLM judge) -> Context builder (constructs CF LEX) -> Evaluation criteria (TEQ/TN EQ) -> Human evaluation system -> Leaderboard re-evaluation pipeline

- Critical path:
  1. Receive generated query and ground truth
  2. Execute both queries to get results
  3. Build appropriate context (CEQ or CN EQ) based on execution results
  4. LLM evaluates query using criteria
  5. Categorize errors if query is incorrect
  6. Aggregate results for overall score

- Design tradeoffs:
  - Accuracy vs. efficiency: FLEX is more accurate but slower than EX
  - LLM dependency vs. reproducibility: Proprietary LLMs offer better performance but pose reproducibility risks
  - Comprehensive context vs. prompt length: More context improves evaluation but increases costs

- Failure signatures:
  - Low agreement with human experts: Indicates issues with criteria or context
  - High false positive rate: May indicate LLM struggles with detecting semantic differences
  - High false negative rate: May indicate LLM is too strict on structural variations

- First 3 experiments:
  1. Compare FLEX agreement with human experts on a small sample (100 queries) vs. EX
  2. Ablation study: Remove each component of CF LEX to measure impact on performance
  3. Error analysis: Categorize errors in top models to identify systematic issues

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does FLEX's performance vary across different SQL complexity levels, and what specific features of complex queries does it struggle with most?
- Basis in paper: [inferred] The paper mentions that FLEX detects overestimation in BIRD's challenging questions but doesn't provide detailed analysis of performance across complexity levels
- Why unresolved: The paper provides aggregate performance metrics but lacks granular analysis of FLEX's performance on different SQL complexity tiers (simple, moderate, complex)
- What evidence would resolve it: A detailed breakdown of FLEX's agreement scores across predefined SQL complexity categories, showing where it performs well and where it struggles

### Open Question 2
- Question: What is the optimal balance between context information and evaluation efficiency in FLEX, and how does increasing context affect both accuracy and computational cost?
- Basis in paper: [explicit] The paper discusses the importance of comprehensive context but doesn't quantify the trade-off between context richness and evaluation efficiency
- Why unresolved: While FLEX demonstrates high accuracy with comprehensive context, the paper doesn't explore the marginal benefit of additional context versus the increased computational cost
- What evidence would resolve it: Systematic experiments varying the amount and type of context information, measuring both evaluation accuracy and processing time/cost

### Open Question 3
- Question: How well does FLEX generalize to real-world enterprise databases with different schemas, data distributions, and query patterns compared to benchmark datasets?
- Basis in paper: [explicit] The paper acknowledges this as a limitation, stating FLEX was only tested on Spider and BIRD benchmarks
- Why unresolved: The evaluation is limited to controlled benchmark datasets, leaving uncertainty about FLEX's effectiveness in real-world scenarios with more complex and varied database structures
- What evidence would resolve it: Empirical testing of FLEX on multiple enterprise database systems with varying schemas, data volumes, and query patterns, comparing results to human expert evaluation

### Open Question 4
- Question: What is the impact of different LLM model sizes and architectures on FLEX's performance, and are there specific architectural features that correlate with better SQL evaluation capabilities?
- Basis in paper: [inferred] The paper compares various LLMs but doesn't analyze how model size, architecture, or training objectives affect evaluation quality
- Why unresolved: While the paper shows different LLMs achieve varying performance levels, it doesn't identify which model characteristics are most important for SQL evaluation tasks
- What evidence would resolve it: Correlation analysis between LLM architectural features (parameter count, training data, instruction-tuning quality) and FLEX performance metrics across multiple model families

## Limitations
- FLEX evaluation depends heavily on proprietary LLM performance, with exact prompts and criteria not fully disclosed
- Study focuses on Spider and BIRD benchmarks only, limiting generalizability to other datasets or domains
- Analysis of model performance changes is based on a specific set of 50 models, which may not represent the broader landscape

## Confidence

**High Confidence**: The improvement in agreement with human experts (from 62 to 87.04 Cohen's kappa) is well-supported by the methodology and experimental design.

**Medium Confidence**: The claim that EX underestimates model performance by 2.6 points on average is supported by the data but could vary with different model sets or benchmarks.

**Low Confidence**: The finding that challenging questions tend to be overestimated lacks sufficient statistical validation.

## Next Checks
1. Cross-benchmark validation: Test FLEX on additional text-to-SQL benchmarks (e.g., Sparc, CoSQL) to verify if the 87.04 kappa agreement holds across different datasets and domains.

2. Error category analysis reproducibility: Independently categorize errors in a subset of model outputs using FLEX's error categories to verify the claimed systematic patterns.

3. LLM dependency assessment: Implement FLEX with multiple open-source LLMs to quantify the performance variance and establish minimum model capability required for reliable evaluation.