---
ver: rpa2
title: Personalized Reinforcement Learning with a Budget of Policies
arxiv_id: '2401.06514'
source_url: https://arxiv.org/abs/2401.06514
tags:
- policy
- algorithms
- policies
- learning
- agents
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces the r-MDP framework to address the challenge
  of personalizing policies under regulatory constraints that limit the number of
  policies that can be deployed. In an r-MDP, agents are matched to one of k representatives,
  each managing a policy.
---

# Personalized Reinforcement Learning with a Budget of Policies

## Quick Facts
- arXiv ID: 2401.06514
- Source URL: https://arxiv.org/abs/2401.06514
- Authors: Dmitry Ivanov; Omer Ben-Porat
- Reference count: 26
- Key outcome: r-MDP framework with EM-like and end-to-end algorithms achieves meaningful personalization under policy budget constraints

## Executive Summary
This paper addresses the challenge of personalizing policies for large populations of agents with diverse preferences under regulatory constraints that limit the number of deployable policies. The authors introduce the r-MDP framework where agents are matched to one of k representative policies, and propose two deep RL algorithms to optimize both the assignment of agents to representatives and the policies themselves. The work provides theoretical guarantees of monotonic improvement and convergence to local maxima, while empirical results demonstrate significant performance gains over baseline approaches in both Resource Gathering and MuJoCo environments.

## Method Summary
The authors propose two algorithms for solving r-MDPs: an EM-like algorithm that alternates between E-steps (greedy assignment of agents to representatives) and M-steps (policy optimization for each representative), and an end-to-end algorithm that parameterizes assignment probabilities using softmax over learned logits and updates them through gradient descent alongside policies. Both algorithms are based on PPO and employ parameter sharing between actors and critics across representatives to improve sample efficiency. The EM-like algorithm provides theoretical convergence guarantees, while the end-to-end approach enables more flexible optimization of assignments.

## Key Results
- EM-like algorithm achieves monotonic improvement in social welfare and converges to local maxima
- End-to-end algorithm learns soft assignments that approximate hard assignments while enabling gradient-based optimization
- Both algorithms outperform K-means clustering baseline and achieve meaningful personalization with limited policy budgets
- Parameter sharing improves sample efficiency without compromising ability to learn diverse policies

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The EM-like algorithm converges to a local maximum of social welfare through alternating optimization of assignments and policies.
- Mechanism: The algorithm alternates between E-steps (greedy assignment of agents to best-performing representatives) and M-steps (policy optimization for each representative). Since each step monotonically improves or maintains social welfare, and the number of possible assignments is finite, the algorithm must converge.
- Core assumption: Each M-step can be solved to convergence using an RL algorithm with global convergence guarantees, and the E-step greedily optimizes assignments given current policies.
- Evidence anchors:
  - [abstract] "Theoretical guarantees of monotonic improvement and convergence to a local maximum are provided."
  - [section] "By Lemma 1, we can use any RL algorithm that has convergence guarantees to perform the M-step for each j."
  - [corpus] Weak evidence - no direct mention of EM convergence in related papers.
- Break condition: If the M-step cannot be solved to convergence or if the E-step does not greedily optimize assignments, convergence is not guaranteed.

### Mechanism 2
- Claim: The end-to-end algorithm learns soft assignments that approximate the hard assignments of the EM algorithm while enabling gradient-based optimization.
- Mechanism: The algorithm parameterizes assignment probabilities α_i(j) using a softmax over learned logits. The loss function for these logits is the same as for the policies, allowing them to be updated in the same backward pass. This creates a differentiable approximation of the E-step that can be optimized end-to-end.
- Core assumption: The policy loss is differentiable with respect to the assignment probabilities, and gradient descent can find good assignments that balance exploration and exploitation.
- Evidence anchors:
  - [abstract] "We propose our second algorithm employing gradient descent for end-to-end training."
  - [section] "The intuition behind this update rule is that the probability α_i^ψ(j) only increases for the best-performing representative, i.e., such j that maximizes the advantage Ã_ij(s, a) averaged over the mini-batch."
  - [corpus] Weak evidence - no direct mention of end-to-end learning for assignments in related papers.
- Break condition: If the softmax parameterization cannot capture the optimal assignment structure, or if gradient descent gets stuck in poor local optima.

### Mechanism 3
- Claim: Parameter sharing between actors and critics improves sample efficiency without compromising the ability to learn diverse policies.
- Mechanism: The intermediate layers of actor and critic networks are shared across all representatives, while the output layers are separate. This allows experiences from all representatives to be used to train all networks, increasing the effective sample size.
- Core assumption: The shared features are useful for all representatives, and the separate output layers can still learn distinct policies.
- Evidence anchors:
  - [section] "To improve training efficiency, we share the parameters of intermediate layers between actors θ_j, as well as critics ϕ_j."
  - [corpus] Weak evidence - no direct mention of parameter sharing in related papers, though it is a common technique in multi-task learning.
- Break condition: If the shared features are not useful for all representatives, or if the separate output layers cannot learn sufficiently diverse policies.

## Foundational Learning

- Concept: Markov Decision Processes (MDPs)
  - Why needed here: The r-MDP framework is an extension of MDPs that incorporates multiple agents with different reward functions and a limited budget of policies.
  - Quick check question: What is the Bellman equation for the value function in an MDP?

- Concept: Reinforcement Learning (RL)
  - Why needed here: The algorithms for solving r-MDPs are based on RL techniques, specifically actor-critic methods like PPO.
  - Quick check question: What is the difference between on-policy and off-policy RL algorithms?

- Concept: Clustering
  - Why needed here: The EM-like algorithm is inspired by K-means clustering, and the baseline algorithm uses K-means clustering on agent trajectories.
  - Quick check question: What is the objective function minimized by the K-means algorithm?

## Architecture Onboarding

- Component map: Simulator -> Generate experiences for representatives -> Actor-critic networks (one per representative) -> Assignment probabilities table -> Training loop (alternates policy updates and assignment updates)
- Critical path: The training loop, which involves generating experiences, updating policies, and updating assignments. The frequency of assignment updates is a key design decision.
- Design tradeoffs: The main tradeoff is between the frequency of assignment updates (E-steps) and the magnitude of policy updates (M-steps). More frequent assignment updates allow the policies to adapt to changing assignments, but may also introduce instability.
- Failure signatures: If the assignments converge to a poor solution, it may be due to insufficient exploration during policy learning. If the policies fail to learn, it may be due to vanishing gradients or poor initialization.
- First 3 experiments:
  1. Run the EM-like algorithm with a single representative (k=1) and verify that it converges to the optimal policy for the average reward function.
  2. Run the EM-like algorithm with two representatives (k=2) and a small number of agents (n=10) and visualize the learned assignments and policies.
  3. Run the end-to-end algorithm with two representatives and compare the learned assignments and policies to those of the EM-like algorithm.

## Open Questions the Paper Calls Out

- What is the precise definition of "utilitarian social welfare" in this context? Is it a simple sum of all individual utilities, or does it incorporate any weighting or normalization schemes?
- How does the r-MDP framework handle cases where the number of agents (n) is significantly larger than the policy budget (k)? Are there any limitations or trade-offs that arise in such scenarios?
- How does the r-MDP framework handle dynamic environments where agent preferences or the reward functions may change over time?

## Limitations
- Theoretical convergence guarantees rely on idealized assumptions about M-step optimization and E-step optimality
- Empirical validation limited to two environments with synthetic reward variations
- Comparison to baselines doesn't establish optimality given budget constraint

## Confidence
- **High**: The mathematical framework for r-MDPs is sound and the connection to existing RL methods is clear
- **Medium**: The empirical results show meaningful improvements over baselines in the tested environments
- **Low**: The generalizability to real-world personalization problems with complex preference structures remains unproven

## Next Checks
1. Apply the algorithms to a real personalization problem (e.g., recommendation systems or adaptive tutoring) where ground truth agent preferences can be measured, to validate performance beyond synthetic reward functions.

2. Systematically vary the policy budget k and measure the marginal benefit of additional policies to identify the point of diminishing returns and optimal resource allocation.

3. Test the algorithms with multiple random seeds and different initializations of representatives to assess sensitivity to starting conditions and verify that convergence to similar social welfare levels is consistent.