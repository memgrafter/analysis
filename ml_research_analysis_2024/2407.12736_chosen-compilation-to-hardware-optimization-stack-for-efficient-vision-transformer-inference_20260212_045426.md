---
ver: rpa2
title: 'CHOSEN: Compilation to Hardware Optimization Stack for Efficient Vision Transformer
  Inference'
arxiv_id: '2407.12736'
source_url: https://arxiv.org/abs/2407.12736
tags:
- fpga
- hardware
- memory
- optimal
- chosen
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces CHOSEN, a software-hardware co-design framework
  for deploying Vision Transformers (ViTs) on FPGAs. The primary challenge addressed
  is the high computational and memory demands of ViTs due to non-linear calculations
  and self-attention mechanisms.
---

# CHOSEN: Compilation to Hardware Optimization Stack for Efficient Vision Transformer Inference

## Quick Facts
- arXiv ID: 2407.12736
- Source URL: https://arxiv.org/abs/2407.12736
- Authors: Mohammad Erfan Sadeghi; Arash Fayyazi; Suhas Somashekar; Armin Abdollahi; Massoud Pedram
- Reference count: 17
- Primary result: CHOSEN achieves 1.5x and 1.42x throughput improvement for DeiT-S and DeiT-B models respectively compared to state-of-the-art ViT accelerators

## Executive Summary
This paper introduces CHOSEN, a software-hardware co-design framework for deploying Vision Transformers (ViTs) on FPGAs. The framework addresses the high computational and memory demands of ViTs through a multi-kernel design that maximizes DDR memory bandwidth, hardware-friendly approximations of non-linear functions, and an efficient compiler with heuristic-based design space exploration. CHOSEN significantly reduces the number of evaluations required for design space exploration while achieving near-optimal configurations. The framework demonstrates improved resource utilization with fewer LUTs and FFs while achieving higher DSP usage for matrix multiplications.

## Method Summary
CHOSEN employs a multi-kernel design approach to maximize DDR memory bandwidth for ViT inference on FPGAs. The framework converts high-level ViT models into computational graphs, schedules execution, and optimizes nodes through a heuristic-based design space exploration algorithm. The compiler transforms models into hardware implementations while using hardware-friendly approximations for non-linear functions to reduce computational complexity. The design space exploration is accelerated through a heuristic algorithm that achieves near-optimal configurations with significantly fewer computations compared to exhaustive search methods.

## Key Results
- 1.5x throughput improvement for DeiT-S models compared to state-of-the-art ViT accelerators
- 1.42x throughput improvement for DeiT-B models compared to state-of-the-art ViT accelerators
- Improved resource utilization with fewer LUTs and FFs while achieving higher DSP usage for matrix multiplications

## Why This Works (Mechanism)
The framework's effectiveness stems from three key mechanisms: (1) multi-kernel design that maximizes DDR memory bandwidth utilization, (2) hardware-friendly approximations of non-linear functions that reduce computational complexity while maintaining accuracy, and (3) an efficient compiler with heuristic-based design space exploration that significantly reduces optimization time while achieving near-optimal configurations.

## Foundational Learning
- **Vision Transformer Architecture**: Understanding of self-attention mechanisms and their computational requirements. Why needed: Critical for identifying bottlenecks in ViT inference. Quick check: Can explain the difference between ViT and CNN architectures.
- **FPGA Memory Hierarchy**: Knowledge of DDR memory bandwidth limitations and optimization techniques. Why needed: Essential for designing efficient multi-kernel architectures. Quick check: Can describe the impact of memory bandwidth on computational throughput.
- **Hardware-Software Co-design**: Understanding of how software optimizations translate to hardware implementations. Why needed: Key for developing effective compiler optimizations. Quick check: Can explain the trade-offs between software abstraction and hardware efficiency.

## Architecture Onboarding
- **Component Map**: High-level ViT model -> Computational Graph Conversion -> Node Scheduling -> Hardware Implementation -> FPGA Deployment
- **Critical Path**: Computational graph conversion and node scheduling are the most critical components for achieving optimal performance
- **Design Tradeoffs**: Accuracy vs. hardware efficiency trade-off in non-linear function approximations; exploration time vs. solution quality in design space exploration
- **Failure Signatures**: Suboptimal memory bandwidth utilization; excessive LUT/FF usage; degraded model accuracy due to aggressive approximations
- **First Experiments**: (1) Benchmark multi-kernel design against single-kernel implementation for memory bandwidth utilization, (2) Compare accuracy of hardware-friendly approximations against exact implementations, (3) Evaluate design space exploration time vs. exhaustive search for different model sizes

## Open Questions the Paper Calls Out
None

## Limitations
- Evaluation focuses primarily on DeiT models with limited testing across diverse ViT architectures
- Performance claims are based on FPGA implementations, which may not directly translate to other hardware platforms
- Hardware-friendly approximations of non-linear functions could potentially impact model accuracy, though this trade-off is not thoroughly quantified

## Confidence
- High confidence in: Technical implementation of multi-kernel design and its effectiveness in maximizing DDR memory bandwidth; comparative throughput improvements (1.5x for DeiT-S and 1.42x for DeiT-B) are well-supported by experimental data
- Medium confidence in: Generalization of framework to different ViT architectures beyond DeiT models; scalability claims for larger models based on limited experimental validation
- Low confidence in: Long-term robustness of heuristic-based design space exploration algorithm across diverse hardware configurations and model types; impact of hardware-friendly approximations on model accuracy

## Next Checks
1. Test the framework's performance across a broader range of ViT architectures, including Swin, PVT, and other emerging transformer models, to validate generalization claims
2. Conduct a comprehensive analysis of the accuracy trade-offs introduced by hardware-friendly approximations of non-linear functions, comparing results with exact implementations
3. Evaluate the framework's performance on different FPGA platforms and configurations to assess scalability and portability claims