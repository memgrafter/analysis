---
ver: rpa2
title: 'Rolling the DICE on Idiomaticity: How LLMs Fail to Grasp Context'
arxiv_id: '2410.16069'
source_url: https://arxiv.org/abs/2410.16069
tags:
- literal
- figurative
- frequency
- performance
- expression
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of evaluating whether large
  language models (LLMs) can use context to accurately disambiguate idiomatic expressions.
  The authors introduce a novel contrastive dataset, DICE, which contains potentially
  idiomatic expressions balanced for literal and figurative senses, without syntactic
  changes.
---

# Rolling the DICE on Idiomaticity: How LLMs Fail to Grasp Context

## Quick Facts
- arXiv ID: 2410.16069
- Source URL: https://arxiv.org/abs/2410.16069
- Authors: Maggie Mi; Aline Villavicencio; Nafise Sadat Moosavi
- Reference count: 40
- Primary result: Models struggle to use context effectively for idiomaticity detection, with only a few achieving above 40% accuracy under strict consistency

## Executive Summary
This paper introduces DICE, a novel contrastive dataset for evaluating whether large language models can accurately disambiguate idiomatic expressions using context. The dataset contains potentially idiomatic expressions balanced for literal and figurative senses without syntactic changes, preventing models from relying on surface-level differences. The study evaluates 13 models including GPT and Llama variants on idiomaticity detection tasks under various settings. Results show that models struggle significantly with contextual understanding, particularly under strict consistency evaluation, and that expression frequency creates a trade-off between literal and figurative detection performance.

## Method Summary
The study introduces DICE, a novel contrastive dataset containing 2,066 sentences with 402 unique potentially idiomatic expressions (103 noun compounds + 299 phrasal expressions). The dataset is carefully constructed to have identical expression forms across literal and figurative contexts, eliminating syntactic modifications as cues. The study evaluates 13 models (GPT-4o, GPT-3.5-Turbo, FLAN-T5 models, Llama 2, Llama 3) using zero-shot and few-shot prompting on DICE dataset. Performance is measured using accuracy metrics (figurative accuracy, literal accuracy), lenient consistency, and strict consistency.

## Key Results
- Models struggle to use context effectively for idiomaticity detection, particularly under strict consistency where only a few models achieve above 40% accuracy
- Expression frequency creates a trade-off: higher frequency aids literal detection but hinders figurative interpretation
- Sentence likelihood correlates positively with model performance, suggesting training exposure influences results
- The controlled form of DICE expressions forces models to depend on contextual understanding rather than surface-level patterns

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Models rely on surface-level differences rather than contextual understanding to disambiguate idiomaticity.
- **Mechanism**: The dataset contains potentially idiomatic expressions with identical forms across literal and figurative contexts, eliminating syntactic or semantic modifications that would otherwise serve as cues. This forces models to depend on contextual understanding rather than surface-level patterns.
- **Core assumption**: Models cannot rely on memorized idioms when the expression form is held constant across contexts.
- **Evidence anchors**:
  - [abstract]: "This dataset prevents models from relying on surface-level differences and forces them to depend on contextual understanding."
  - [section]: "We propose a novel evaluation set (DICE) where we strictly control the form of idiomatic expressions. This eliminates the possibility that models rely on grammatical variations for idiomaticity disambiguation."
  - [corpus]: Weak - corpus evidence is limited to the claim about existing datasets allowing syntactic modifications.

### Mechanism 2
- **Claim**: Expression frequency influences model performance differently for literal versus figurative detection.
- **Mechanism**: High-frequency idioms are more likely to be correctly identified in literal contexts due to their dominant literal meaning in pretraining data, but this same frequency hinders figurative interpretation. Lower frequency expressions show more balanced performance across both senses.
- **Core assumption**: Pretraining data contains more literal than figurative uses of high-frequency idioms.
- **Evidence anchors**:
  - [abstract]: "We find frequency is not a 'free lunch' and that, whilst highly frequent idioms maybe be more likely to be disambiguated correctly, there is a trade-off between performance on the two idiomaticity settings (literal/figurative)."
  - [section]: "We observe that, as the frequency of an expression increases, the models tend to perform better at identifying its literal occurrences, but their accuracy in recognizing idiomatic uses declines."
  - [corpus]: Moderate - corpus evidence shows frequency distributions but doesn't directly confirm the trade-off mechanism.

### Mechanism 3
- **Claim**: Sentence likelihood correlates positively with model performance on idiomaticity detection.
- **Mechanism**: Models perform better on sentences with higher likelihood scores because these sentences are encountered more frequently during training, leading to better representation and recognition patterns.
- **Core assumption**: Training exposure directly influences model performance on specific sentence patterns.
- **Evidence anchors**:
  - [abstract]: "We observe a positive correlation between a model's likelihood score and its performance on DICE, suggesting training exposure influences results."
  - [section]: "In both settings, the model performs better on sentences on which it has a higher likelihood."
  - [corpus]: Weak - corpus evidence is limited to the claim about likelihood scores without direct supporting data.

## Foundational Learning

- **Concept**: Contrastive evaluation
  - Why needed here: This approach isolates contextual understanding by controlling for form variations, allowing researchers to test whether models truly understand context versus relying on surface patterns.
  - Quick check question: If you wanted to test whether a model understands sarcasm versus literal statements, how would you design a contrastive evaluation?

- **Concept**: Idiomatic vs compositional expressions
  - Why needed here: Understanding the distinction between expressions whose meaning is compositionally derived versus those requiring contextual interpretation is crucial for evaluating model performance on figurative language.
  - Quick check question: Is "hot dog" idiomatic or compositional? Why?

- **Concept**: Frequency-based language modeling
  - Why needed here: Models make predictions based on learned likelihoods from training data, and understanding how frequency influences these predictions is essential for interpreting performance patterns.
  - Quick check question: Why might a model perform better on "spill the tea" versus "spill the coffee" even if both are equally idiomatic?

## Architecture Onboarding

- **Component map**: Dataset curation -> Expert annotation -> Model evaluation -> Frequency/likelihood analysis -> Results interpretation
- **Critical path**: The dataset curation and expert annotation phase is most critical, as errors here propagate through all subsequent evaluations.
- **Design tradeoffs**: Controlled form vs. natural variation - maintaining identical expression forms enables clean testing but may reduce dataset size and naturalness.
- **Failure signatures**: Low consistency scores across contexts indicate surface-level pattern reliance rather than contextual understanding.
- **First 3 experiments**:
  1. Evaluate a small subset of expressions with varying frequency levels to confirm the frequency-performance relationship.
  2. Test model performance on sentences with identical likelihood scores but different idiomaticity contexts to isolate likelihood effects.
  3. Compare performance on expressions with clear contextual cues versus ambiguous contexts to measure contextual sensitivity.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do language models handle idiomatic expressions that are not compositional or partially compositional, such as "skin tone" or "noble gas," compared to non-compositional idioms?
- Basis in paper: [explicit] The paper explicitly mentions that the dataset excludes compositional and partially compositional compounds, focusing only on non-compositional idioms to ensure the challenge lies in interpreting figurative language based on context.
- Why unresolved: The paper does not explore how models perform on compositional or partially compositional idioms, leaving a gap in understanding the full range of idiomatic expressions models can handle.
- What evidence would resolve it: Conduct experiments evaluating model performance on compositional and partially compositional idioms to compare with non-compositional idioms, analyzing differences in context comprehension and accuracy.

### Open Question 2
- Question: How does the frequency of encountering an idiom during pretraining affect a model's ability to interpret its figurative meaning in new contexts?
- Basis in paper: [explicit] The paper discusses the impact of expression frequency on model performance, noting that higher frequency aids literal detection but hinders figurative interpretation, suggesting a trade-off between performance on the two idiomaticity settings.
- Why unresolved: While the paper identifies a correlation between frequency and performance, it does not fully explore the underlying mechanisms or provide a comprehensive explanation for why higher frequency affects figurative interpretation negatively.
- What evidence would resolve it: Perform a detailed analysis of pretraining data to quantify the exposure of idioms at different frequencies and correlate this with model performance on idiomaticity tasks, potentially using ablation studies to isolate the effects of frequency.

### Open Question 3
- Question: How does the likelihood of a sentence, as determined by a model's probability estimates, influence its ability to correctly identify idiomatic expressions?
- Basis in paper: [explicit] The paper finds a positive correlation between a model's likelihood score and its performance on idiomaticity detection, suggesting that training exposure influences results and emphasizing the importance of context comprehension in LLMs.
- Why unresolved: The paper does not fully explore the causal relationship between sentence likelihood and idiomaticity detection accuracy, nor does it investigate whether this correlation holds across different types of idioms or contexts.
- What evidence would resolve it: Conduct experiments varying sentence likelihood and idiomaticity complexity, measuring changes in detection accuracy to determine if likelihood is a reliable predictor of performance across diverse idiomatic expressions.

## Limitations

- The controlled nature of DICE may not fully capture the complexity and variability of real-world idiomatic usage
- The study is constrained to English idioms, raising questions about cross-linguistic applicability
- The frequency-performance relationship analysis cannot definitively establish causation
- The positive correlation between sentence likelihood and performance may be influenced by confounding factors

## Confidence

**Medium Confidence**: The claim that models struggle with contextual understanding of idiomatic expressions, particularly under strict consistency evaluation. This is supported by systematic evaluation across 13 diverse models but may be influenced by the specific dataset construction and evaluation methodology.

**Medium Confidence**: The frequency-performance trade-off mechanism, where high-frequency idioms show better literal detection but worse figurative interpretation. The pattern is consistently observed but the underlying causal mechanisms remain speculative.

**Medium Confidence**: The correlation between sentence likelihood and model performance on idiomaticity tasks. While the relationship is statistically robust, the directionality and practical significance require further investigation.

## Next Checks

1. **Cross-linguistic validation**: Evaluate the same models on idiomatic expressions from languages with different structural properties (e.g., Mandarin chengyu, Spanish refranes) to test whether the observed limitations are universal or language-specific.

2. **Natural variation testing**: Create a parallel evaluation set with naturally occurring idiom variations (syntactic modifications, different tenses, etc.) to determine if the strict consistency results generalize to more realistic usage patterns.

3. **Fine-tuning impact assessment**: Fine-tune selected models on idiom-rich corpora and re-evaluate on DICE to determine whether targeted training can improve contextual understanding or whether the limitations are fundamental to current LLM architectures.