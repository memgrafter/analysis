---
ver: rpa2
title: '2DSig-Detect: a semi-supervised framework for anomaly detection on image data
  using 2D-signatures'
arxiv_id: '2409.04982'
source_url: https://arxiv.org/abs/2409.04982
tags:
- images
- data
- dsig-norm
- image
- d-signature
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces 2DSig-Detect, a novel anomaly detection method
  for image data using 2D-signatures and learned representations. The method addresses
  security challenges in machine learning models by detecting adversarial attacks
  during both training (backdoor) and test (evasion) phases.
---

# 2DSig-Detect: a semi-supervised framework for anomaly detection on image data using 2D-signatures

## Quick Facts
- arXiv ID: 2409.04982
- Source URL: https://arxiv.org/abs/2409.04982
- Reference count: 40
- Primary result: Novel 2D-signature-based framework outperforms state-of-the-art methods for detecting adversarial attacks with 100x lower computational cost

## Executive Summary
2DSig-Detect introduces a semi-supervised anomaly detection framework specifically designed for image data that addresses critical security challenges in machine learning models. The method leverages 2D-signatures and learned representations from early neural network layers to detect both backdoor attacks during training and evasion attacks during inference. By embedding 2D-signature features into anomaly detection metrics like covariance norm, the framework achieves superior performance compared to traditional approaches like GMM and spectral signatures while maintaining significantly lower computational overhead.

## Method Summary
2DSig-Detect operates by extracting 2D-signature features from learned representations of images, then applying anomaly detection metrics to identify adversarial manipulations. The framework uses early neural network layers that capture distributional differences between benign and poisoned images while minimizing inter-class variations. Two variants exist: 2DSig-Norm (covariance norm) offering 10x speedup with similar accuracy, and 2DSig-Conf (conformance score). The method processes images through learned representations, computes 2D-signature transforms capturing cross-channel and self-channel interactions, and applies distance metrics to score anomalies relative to the benign distribution.

## Key Results
- Achieves AUCs above 0.85 for evasion attacks with 1/100th the computation time of baseline methods
- Detects poisoned training images with TPR above 0.92 for backdoor attack scenarios
- Outperforms GMM and spectral signature approaches in detecting adversarial manipulations across multiple attack types

## Why This Works (Mechanism)

### Mechanism 1
- Claim: 2D-signature features capture cross-channel and self-channel interactions distinguishing adversarial perturbations from normal variations
- Mechanism: The 2D-signature transform computes iterated integrals over image area, capturing both linear and nonlinear pixel value effects in a compact representation
- Core assumption: Adversarial perturbations create distinct distributional patterns in 2D-signature feature space detectable even when imperceptible in pixel space
- Evidence anchors: [abstract] "2DSig-Detect uses a 2D-signature-embedded semi-supervised framework rooted in rough path theory"
- Break condition: If adversarial attacks mimic normal image structure in 2D-signature space or perturbation magnitude falls below noise threshold

### Mechanism 2
- Claim: Learned representations from early neural network layers emphasize distributional differences between benign and poisoned images
- Mechanism: Early layers capture low-level features like edges and textures where adversarial perturbations manifest as anomalies
- Core assumption: Learned representation transformation amplifies adversarial modification signals while preserving discriminative power for anomaly detection
- Evidence anchors: [section] "For a well-trained classification model, it's expected that the later layers are adept at distinguishing different image clusters"
- Break condition: If adversarial attacks specifically target and normalize early-layer representations

### Mechanism 3
- Claim: Covariance norm distance metric provides efficient anomaly scoring with theoretical bounds on false positive and false negative rates
- Mechanism: Covariance norm measures Mahalanobis-equivalent distance between test instances and benign distribution with explicit bounds on Type-I and Type-II errors
- Core assumption: Benign and adversarial distributions have sufficiently separated means and covariances
- Evidence anchors: [section] "Proposition 2.4 (Bounds for Type-I and Type-II errors of cov-norm framework)"
- Break condition: If adversarial distribution mean approaches benign distribution mean, error bounds degrade

## Foundational Learning

- Concept: 2D-signature transform and rough path theory
  - Why needed here: Understanding iterated integrals capturing multi-dimensional path information is crucial for grasping why 2DSig-Detect works
  - Quick check question: What is the difference between cross-channel and self-channel terms in 2D-signature computation, and why does this matter for grayscale vs color images?

- Concept: Mahalanobis distance and covariance norm
  - Why needed here: Anomaly scoring mechanism relies on measuring distances in feature space relative to benign distribution's covariance structure
  - Quick check question: How does covariance norm differ from standard Euclidean distance, and why is this advantageous for anomaly detection?

- Concept: Semi-supervised learning framework
  - Why needed here: Algorithm operates with minimal labeled data (only benign examples) and must generalize to detect unknown attack types
  - Quick check question: What are key differences between supervised, semi-supervised, and unsupervised anomaly detection approaches in adversarial settings?

## Architecture Onboarding

- Component map: Image → Learned representation → 2D-signature → Distance metric → Threshold comparison → Anomaly decision

- Critical path: Image → Learned representation → 2D-signature → Distance metric → Threshold comparison → Anomaly decision

- Design tradeoffs:
  - Covariance norm vs. conformance score: 10x speedup for covariance norm at similar accuracy
  - Level 1 vs. Level 2 2D-signature: Level 1 captures sufficient information with lower computational cost
  - Learned representation layer choice: Earlier layers better for anomaly detection but may miss some attack patterns

- Failure signatures:
  - High false negative rate: Adversarial attacks too subtle to be captured in 2D-signature space
  - High false positive rate: Benign distribution too heterogeneous or threshold too sensitive
  - Performance degradation with resolution: 2D-signature may not scale effectively with very high-resolution images
  - Slow inference: Computational bottleneck in 2D-signature calculation or distance metric computation

- First 3 experiments:
  1. Baseline test: Apply 2DSig-Detect to clean CIFAR-10 test set - should achieve ~50% AUC (random guessing baseline)
  2. Evasion attack detection: Generate IFGSM adversarial examples and measure AUC improvement over naive approaches
  3. Backdoor attack detection: Inject backdoor patterns into training data and evaluate TPR at different contamination levels

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does performance of 2DSig-Detect vary with different levels of the 2D-signature transform?
- Basis in paper: [explicit] Paper mentions performance does not necessarily increase with 2D-signature level; Level-2 features improved evasion attack performance compared to Level-1
- Why unresolved: Paper lacks comprehensive analysis across different levels of 2D-signature transform
- What evidence would resolve it: Systematic evaluation of 2DSig-Detect's performance using different 2D-signature levels across various datasets and attack scenarios

### Open Question 2
- Question: How does performance of 2DSig-Detect compare to other state-of-the-art anomaly detection techniques for image data?
- Basis in paper: [inferred] Paper shows superiority over GMM and naive-norm but lacks comprehensive comparison with other established anomaly detection techniques
- Why unresolved: Paper focuses on comparing with GMM, naive-norm, and 2DSig-Conf only
- What evidence would resolve it: Thorough benchmarking study comparing 2DSig-Detect with other state-of-the-art anomaly detection techniques for image data

### Open Question 3
- Question: How does choice of learned representation affect performance of 2DSig-Detect in detecting different types of adversarial attacks?
- Basis in paper: [explicit] Paper discusses importance of choosing appropriate learned representation that emphasizes distributional differences
- Why unresolved: While demonstrating importance of learned representation choice, paper lacks comprehensive analysis across various attack types and datasets
- What evidence would resolve it: Systematic evaluation of 2DSig-Detect's performance using different learned representations across different attack types and datasets

## Limitations
- Performance claims rely heavily on specific hyperparameter choices and attack configurations not fully detailed in paper
- 2D-signature transform implementation details, particularly for level 2 features, remain unclear and could impact reproducibility
- Generalizability of results to other image datasets beyond CUReT and CIFAR-10 has not been established

## Confidence
- **High confidence**: Core mechanism of using 2D-signatures for anomaly detection and theoretical error bounds for covariance norm are well-supported
- **Medium confidence**: Superiority claims over baseline methods supported by experimental results but limited to specific attack types and datasets
- **Low confidence**: Computational efficiency claims (1/100th computation time) need independent verification across different hardware configurations

## Next Checks
1. **Cross-dataset generalization test**: Apply 2DSig-Detect to additional image datasets (e.g., ImageNet subsets) with varying resolution and complexity to assess robustness across different data distributions

2. **Attack adaptation study**: Evaluate 2DSig-Detect's performance against adaptive attacks specifically designed to evade 2D-signature detection by optimizing perturbations in the signature feature space

3. **Parameter sensitivity analysis**: Systematically vary key hyperparameters including signature level, learned representation layer selection, and threshold percentile to understand their impact on detection performance and identify optimal configurations