---
ver: rpa2
title: Deep linear networks for regression are implicitly regularized towards flat
  minima
arxiv_id: '2405.13456'
source_url: https://arxiv.org/abs/2405.13456
tags:
- wprod
- networks
- initialization
- deep
- bound
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper analyzes sharpness in deep linear networks for overdetermined
  univariate regression, showing that while minimizers can have arbitrarily large
  sharpness, there is a linear lower bound on sharpness that grows with depth. The
  authors prove that gradient flow implicitly regularizes towards flat minima by finding
  minimizers whose sharpness is within a constant factor (depending on data covariance
  condition number) of the theoretical lower bound.
---

# Deep linear networks for regression are implicitly regularized towards flat minima

## Quick Facts
- arXiv ID: 2405.13456
- Source URL: https://arxiv.org/abs/2405.13456
- Authors: Pierre Marion; Lénaïc Chizat
- Reference count: 40
- Key outcome: Deep linear networks for overdetermined univariate regression are implicitly regularized towards flat minima, with sharpness growing linearly with depth but gradient flow finding minimizers close to the theoretical lower bound.

## Executive Summary
This paper analyzes sharpness in deep linear networks for overdetermined univariate regression, showing that while minimizers can have arbitrarily large sharpness, there is a linear lower bound on sharpness that grows with depth. The authors prove that gradient flow implicitly regularizes towards flat minima by finding minimizers whose sharpness is within a constant factor (depending on data covariance condition number) of the theoretical lower bound. Two initialization schemes are studied: small-scale initialization leads to rank-one aligned weight matrices, while residual initialization with Gaussian entries converges globally. The results are illustrated through experiments showing successful training for learning rates below a critical threshold determined by the sharpness lower bound, with sharpness after training matching theoretical predictions.

## Method Summary
The paper studies deep linear networks for overdetermined univariate regression, where a deep linear network with weight matrices W₁,...,W_L and fixed projection vector w minimizes the empirical risk. Two initialization schemes are analyzed: small-scale initialization with Gaussian entries scaled by ε, and residual initialization with weight matrices of the form I + scaled Gaussian noise. The analysis uses the Polyak-Łojasiewicz (PL) condition to prove convergence of gradient flow and to bound the sharpness of the resulting minimizers. The sharpness is computed via the largest eigenvalue of the Hessian, and the implicit regularization mechanism is analyzed through the structure of the weight matrices at convergence.

## Key Results
- Minimizers can have arbitrarily large sharpness, but not arbitrarily small - there is a linear lower bound on sharpness that grows with depth
- Gradient flow with small-scale initialization implicitly regularizes towards flat minima by finding minimizers whose sharpness is within a constant factor of the theoretical lower bound
- Residual initialization with Gaussian entries enables global convergence of gradient flow without requiring overparameterization
- Experiments validate the theoretical predictions, showing that training succeeds for learning rates below a critical threshold determined by the sharpness lower bound

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Gradient flow with small-scale initialization implicitly regularizes towards flat minima by finding minimizers whose sharpness is close to the theoretical lower bound.
- **Mechanism:** Small-scale initialization leads to rank-one aligned weight matrices with equal norms across layers, minimizing sharpness. The gradient flow dynamics are constrained by the Polyak-Łojasiewicz condition, forcing convergence to these low-sharpness configurations.
- **Core assumption:** The initialization satisfies Assumption (A2) - small scale with non-zero gradient - and Assumption (A3) - scaled down as depth increases so that ε = O(1/L²).
- **Evidence anchors:**
  - [abstract] "we bound the sharpness of the minimizer found by gradient flow: the ratio between the sharpness after training and Smin is less than a constant depending mainly on the condition number of the empirical covariance matrix"
  - [section 4] "we show that the learned weight matrices are close to being rank-one, in the sense that all their singular values but the largest one are small"
  - [corpus] Weak evidence - related papers focus on sharpness-aware optimization but don't directly address this specific mechanism
- **Break condition:** If the initialization scale is not sufficiently small (violating Assumption A3), the PL condition may not hold and the implicit regularization fails.

### Mechanism 2
- **Claim:** Residual initialization enables global convergence of gradient flow for standard Gaussian initialization without requiring overparameterization.
- **Mechanism:** The residual structure (I + scaled Gaussian noise) stabilizes training by preventing vanishing/exploding gradients. High-probability bounds on singular values of the product of weight matrices ensure the PL condition holds throughout training.
- **Core assumption:** The loss at initialization is not too large (satisfying the condition in Theorem 4) and the width d is sufficiently large relative to depth L.
- **Evidence anchors:**
  - [abstract] "convergence of the gradient flow for a Gaussian initialization of the residual network is proven"
  - [section 5] "we show global convergence of the empirical risk...it is the first time that convergence is proven for a standard Gaussian initialization of the residual network outside the large width regime"
  - [corpus] Weak evidence - related papers focus on sharpness-aware optimization but don't directly address this specific mechanism
- **Break condition:** If the initialization loss exceeds the threshold in Theorem 4 or if width d is too small relative to depth L, convergence may fail.

### Mechanism 3
- **Claim:** Sharpness of any minimizer grows linearly with depth, creating a fundamental constraint on learning rate selection.
- **Mechanism:** The Hessian's largest eigenvalue (sharpness) is lower bounded by 2Lλ∥w*∥² - 2/L, which scales linearly with depth. This bound arises from considering rank-one perturbations in parameter space.
- **Core assumption:** The data covariance matrix is full rank (Assumption A1) and the regression problem is overdetermined.
- **Evidence anchors:**
  - [abstract] "Minimizers can have arbitrarily large sharpness, but not an arbitrarily small one. Indeed, we show a lower bound on the sharpness of minimizers, which grows linearly with depth"
  - [section 3] "we show a lower bound on the sharpness of minimizers, which grows linearly with depth...the sharpness of any minimizer grows linearly with depth"
  - [corpus] Weak evidence - related papers focus on sharpness-aware optimization but don't directly address this specific mechanism
- **Break condition:** If the depth L is small or the data covariance has very small eigenvalues, the sharpness bound may not be as restrictive.

## Foundational Learning

- **Concept:** Sharpness (largest eigenvalue of Hessian) and its relationship to optimization stability
  - Why needed here: The paper's central question is how sharpness constrains learning rate selection and what minimizers gradient flow finds
  - Quick check question: Why does the paper claim that sharpness S(W) ≥ 2/η is a stability constraint, and what happens when this bound is violated?

- **Concept:** Polyak-Łojasiewicz (PL) condition and its implications for non-convex optimization
  - Why needed here: The PL condition is the key tool used to prove convergence of gradient flow in both initialization settings
  - Quick check question: How does the PL condition (∥∇f(x)∥² ≥ µ(f(x) - fmin)) ensure exponential convergence to the minimum, and why is this sufficient for proving convergence to minimizers rather than just critical points?

- **Concept:** Implicit regularization and its distinction from explicit regularization
  - Why needed here: The paper's main contribution is showing that gradient flow implicitly regularizes towards flat minima without any explicit sharpness penalty
  - Quick check question: What is the difference between implicit regularization (as studied here) and explicit regularization (like weight decay), and why is the former particularly interesting in deep learning?

## Architecture Onboarding

- **Component map:**
  - Data layer: Design matrix X ∈ Rⁿˣᵈ and target y ∈ Rⁿ
  - Model layer: Deep linear network with weight matrices W₁,...,W_L and fixed projection vector w
  - Optimization layer: Gradient flow dynamics (continuous-time limit of gradient descent)
  - Analysis layer: Sharpness computation via Hessian eigenvalues and PL condition verification

- **Critical path:**
  1. Initialize weights according to chosen scheme (small-scale or residual)
  2. Verify initialization assumptions (A2 for small-scale, loss bound for residual)
  3. Run gradient flow dynamics and track risk and sharpness evolution
  4. At convergence, compute sharpness and compare to theoretical bounds
  5. Analyze weight matrix structure to understand implicit regularization mechanism

- **Design tradeoffs:**
  - Small-scale vs residual initialization: Small-scale gives cleaner rank-one structure but requires careful scaling; residual initialization is more practical but requires larger width
  - Overparameterization level: The paper uses moderate overparameterization (n > d) which is realistic but requires careful analysis
  - Continuous vs discrete time: Continuous gradient flow enables cleaner analysis but requires careful connection to practical gradient descent

- **Failure signatures:**
  - Training fails to converge: Likely indicates violation of initialization assumptions or learning rate too large relative to sharpness bound
  - Sharpness after training much larger than Smin: May indicate initialization scale too large or insufficient depth
  - Weight matrices not rank-one aligned: Suggests small-scale initialization not properly scaled

- **First 3 experiments:**
  1. Reproduce Figure 1a: Train deep linear network with varying learning rates and initialization scales, plot distance to optimal regressor vs learning rate
  2. Verify Theorem 2: For fixed minimizer, compute sharpness via power iteration and compare to lower bound 2Lλ∥w*∥² - 2/L
  3. Test residual initialization: Implement residual initialization as in (2), verify convergence and check if weight structure matches Theorem 4's prediction

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the implicit regularization towards flat minima persist when using non-vanishing learning rates in deep linear networks for regression?
- Basis in paper: [explicit] The authors show that for small-scale initialization, the sharpness of the minimizer found by gradient flow is close to the theoretical lower bound S_min, and they connect this to gradient descent with non-vanishing learning rate in experiments.
- Why unresolved: The paper focuses on gradient flow (vanishing learning rate) as a theoretical limit, and while experiments suggest a connection to practical learning rates, a rigorous analysis of non-vanishing learning rates is not provided.
- What evidence would resolve it: Theoretical analysis or empirical evidence showing that gradient descent with practical learning rates consistently finds minimizers with sharpness close to S_min, or proving that this behavior breaks down for certain learning rates.

### Open Question 2
- Question: How does the implicit regularization towards flat minima generalize to deep non-linear networks for regression tasks?
- Basis in paper: [inferred] The authors study deep linear networks and show implicit regularization towards flat minima, which is related to better generalization. They mention that this phenomenon is different from the well-studied implicit regularization caused by SGD stochasticity.
- Why unresolved: The paper focuses specifically on linear networks, and extending the analysis to non-linear networks would require addressing the additional complexity introduced by non-linearities.
- What evidence would resolve it: Analysis of the sharpness of minimizers found by gradient flow in deep non-linear networks, showing whether the implicit regularization towards flat minima persists, or empirical evidence demonstrating the impact on generalization.

### Open Question 3
- Question: What is the impact of different initialization schemes on the sharpness of minimizers found by gradient flow in deep linear networks for regression?
- Basis in paper: [explicit] The authors study two initialization schemes: small-scale initialization and residual initialization, and show that the sharpness of the trained network is close to the theoretical lower bound in both cases.
- Why unresolved: While the authors provide analysis for two specific initialization schemes, it is unclear how the sharpness of minimizers depends on other possible initialization schemes.
- What evidence would resolve it: Analysis of the sharpness of minimizers found by gradient flow for a range of different initialization schemes, identifying common patterns or differences in the implicit regularization behavior.

## Limitations
- The analysis is limited to deep linear networks for overdetermined univariate regression, which may not capture the complexity of practical deep learning tasks
- The results depend on specific initialization scaling assumptions that may be difficult to satisfy in practice
- The paper doesn't address computational aspects of sharpness-aware optimization or the impact of finite learning rates on the implicit regularization mechanism

## Confidence
- Theoretical claims about implicit regularization towards flat minima: **High confidence** due to rigorous proofs based on the PL condition and explicit bounds
- Experimental validation of practical implications: **Medium confidence** though the synthetic data setting limits generalizability
- Analysis of residual initialization: **Medium confidence** given the technical complexity of the proof and dependence on width scaling conditions

## Next Checks
1. Test the sharpness lower bound predictions on real-world regression datasets to verify if the linear depth scaling holds beyond synthetic Gaussian data.
2. Experiment with nonlinear deep networks to determine if similar implicit regularization towards flat minima occurs, and if depth still linearly affects sharpness bounds.
3. Implement practical sharpness-aware optimization methods based on the theoretical insights and evaluate their performance compared to standard training procedures.