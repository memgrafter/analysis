---
ver: rpa2
title: Towards Quantitative Evaluation of Explainable AI Methods for Deepfake Detection
arxiv_id: '2404.18649'
source_url: https://arxiv.org/abs/2404.18649
tags:
- deepfake
- image
- detection
- explanation
- methods
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a new framework for evaluating the performance
  of explanation methods for deepfake detection. The framework assesses the ability
  of an explanation method to identify the most influential regions of a fake image
  by measuring the reduction in deepfake detection accuracy after adversarial attacks
  are applied to these regions.
---

# Towards Quantitative Evaluation of Explainable AI Methods for Deepfake Detection

## Quick Facts
- arXiv ID: 2404.18649
- Source URL: https://arxiv.org/abs/2404.18649
- Reference count: 40
- Key outcome: LIME explanation method achieves the largest decrease in deepfake detection accuracy when adversarial attacks target top-scoring image segments, demonstrating superior ability to identify manipulated regions.

## Executive Summary
This paper introduces a novel framework for evaluating explanation methods for deepfake detection by measuring their ability to identify the most influential regions in fake images. The framework applies adversarial attacks to the top-scoring segments identified by each explanation method and measures the reduction in detection accuracy. Applied to five explanation methods (Grad-CAM++, RISE, SHAP, LIME, and SOBOL) on an EfficientNet deepfake detector trained on FaceForensics++, the study finds that LIME consistently outperforms other methods in identifying manipulated regions across all deepfake types.

## Method Summary
The framework evaluates explanation methods by generating adversarial examples from fake images and measuring the resulting drop in detection accuracy. Images are first processed through a deepfake detector (EfficientNet), then explained using one of five methods. The image is segmented using SLIC, and adversarial noise is applied to the top-k segments identified by the explanation. The Natural Evolution Strategies (NES) algorithm is used to generate adversarial perturbations that flip the detector's prediction. The framework is tested on the FaceForensics++ dataset with four types of deepfakes, comparing the effectiveness of explanation methods based on their ability to identify regions that, when modified, cause significant accuracy drops.

## Key Results
- LIME consistently achieves the largest decrease in detection accuracy across all deepfake types when adversarial attacks target top-1, top-2, or top-3 image segments
- LIME's effectiveness is particularly pronounced for DeepFakes and Face2Face manipulations
- Qualitative analysis confirms LIME highlights manipulated areas more effectively than other methods
- The framework successfully distinguishes between explanation methods' abilities to identify influential regions

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The adversarial attack framework identifies the most influential image regions by progressively modifying top-scoring segments until the detector's prediction changes.
- Mechanism: The framework segments the image using SLIC, overlays the explanation heatmap to compute segment importance scores, then applies NES-based adversarial noise to the top-k segments. If the detector's output changes, the method correctly identified influential regions.
- Core assumption: The detector's prediction is primarily determined by a small number of image regions, and modifying these regions will change the prediction.
- Evidence anchors:
  - [abstract] "This framework assesses the ability of an explanation method to spot the regions of a fake image with the biggest influence on the decision of the deepfake detector, by examining the extent to which these regions can be modified through a set of adversarial attacks, in order to flip the detector's prediction or reduce its initial prediction"
  - [section] "We assess the performance of this method by examining the extent to which the indicated regions in the visual explanation as the most important ones, can be used to flip the deepfake detector's decision"
  - [corpus] Weak evidence - no directly comparable studies in corpus, though related work on adversarial attacks for explanation exists
- Break condition: If the detector's prediction does not change despite modifications to top-scoring regions, or if modifications to non-top regions cause prediction changes.

### Mechanism 2
- Claim: LIME produces more effective explanations because it generates local linear approximations around perturbed samples.
- Mechanism: LIME perturbs the image by randomly masking segments, gets predictions for each perturbation, then fits a linear model to explain the relationship between masked regions and predictions. This creates explanations that are locally faithful and interpretable.
- Core assumption: A locally linear approximation can adequately capture the relationship between image regions and detector predictions in the vicinity of the input image.
- Evidence anchors:
  - [abstract] "LIME consistently achieves the largest decrease in detection accuracy across all types of deepfakes when adversarial attacks target the top-1, top-2, or top-3 scoring image segments"
  - [section] "Using a linear model (e.g., a linear regressor), LIME fits the binary masks of each perturbation to the corresponding scores and constructs the visual explanation by examining the coefficients/weights that emerge from this simpler model"
  - [corpus] No corpus evidence directly comparing LIME's effectiveness for deepfake detection explanations
- Break condition: If the linear approximation fails to capture non-linear relationships between regions and predictions, or if perturbations don't adequately sample the local neighborhood.

### Mechanism 3
- Claim: The framework's evaluation approach is more meaningful than alternatives because it evaluates explanations for fake images rather than real images.
- Mechanism: By generating adversarial examples from fake images and measuring detection accuracy drops, the framework directly assesses whether explanations identify manipulated regions. This is more relevant to the user's need to understand detection decisions.
- Core assumption: Users are primarily interested in understanding why a detector identifies an image as fake, not why it identifies an image as real.
- Evidence anchors:
  - [abstract] "we argue that the opposite use-case of explanations - i.e., when the model detects a deepfake - is both more meaningful and useful to the user"
  - [section] "We argue that the provision of an explanation after detecting a fake image is more meaningful for the user, as it can give clues about regions of the image (the highlighted ones by the visual explanation) that were found to be manipulated"
  - [corpus] Weak evidence - related work exists but doesn't directly address this user-centered evaluation approach
- Break condition: If users need explanations for both fake and real images, or if understanding why real images are classified as real is equally important.

## Foundational Learning

- Concept: Adversarial attack methodology (NES/Natural Evolution Strategies)
  - Why needed here: The framework relies on iterative adversarial noise generation to test explanation quality. Understanding NES is critical for implementing and debugging the attack process.
  - Quick check question: How does NES differ from gradient-based adversarial attacks, and why might it be preferred for explanation evaluation?

- Concept: Superpixel segmentation (SLIC algorithm)
  - Why needed here: The framework segments images into superpixels to evaluate segment-level importance. Understanding SLIC parameters and trade-offs is important for implementation.
  - Quick check question: What SLIC parameters affect segment quality, and how might different parameters impact explanation evaluation?

- Concept: Explanation method fundamentals (LIME, SHAP, Grad-CAM++)
  - Why needed here: Different explanation methods have different assumptions and mechanisms. Understanding these helps interpret results and choose appropriate methods.
  - Quick check question: What are the key differences between perturbation-based (LIME, RISE) and attribution-based (SHAP, Grad-CAM++) explanation methods?

## Architecture Onboarding

- Component map: Input image → Face detection (RetinaFace) → Deepfake detector (EfficientNet) → Explanation method → Segmentation (SLIC) → Adversarial attack (NES) → Evaluation metrics (accuracy drop, sufficiency scores)
- Critical path: Image → Detector → Explanation → Attack → Evaluation
- Design tradeoffs: The framework trades computational cost (many perturbations and attacks) for evaluation quality. Using EfficientNet balances performance with efficiency.
- Failure signatures: Low accuracy drops across all methods suggest either detector is not sensitive to perturbations or explanations are uniformly poor. High variance in results may indicate instability in attack process.
- First 3 experiments:
  1. Run framework on a small subset of images with LIME explanation to verify basic functionality and establish baseline results
  2. Compare framework results using different numbers of NES samples (n parameter) to find sweet spot between accuracy and speed
  3. Test framework on images from different manipulation types (DF, F2F, FS, NT) to understand type-specific performance patterns

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do the findings from this study on EfficientNet-based deepfake detection models generalize to other deepfake detection architectures?
- Basis in paper: [explicit] The authors note that EfficientNet has been shown to outperform alternative CNN architectures on various deepfake datasets and achieves similar performance to other vanilla CNNs while requiring fewer parameters. However, they acknowledge that there is no evidence that results for one deepfake detector can be generalized to others.
- Why unresolved: This study only evaluates explanation methods on a single deepfake detection model (EfficientNet). The effectiveness of explanation methods may vary depending on the architecture and features used by different deepfake detectors.
- What evidence would resolve it: Replicating this study with different deepfake detection architectures (e.g., XceptionNet, MesoNet, Vision Transformers) and comparing the effectiveness of explanation methods across models.

### Open Question 2
- Question: How well do the proposed evaluation framework and findings generalize to real-world deepfake datasets beyond FaceForensics++?
- Basis in paper: [inferred] The study uses the FaceForensics++ dataset, which contains controlled, high-quality deepfakes. The authors do not discuss the applicability of their findings to more challenging, real-world scenarios with diverse deepfake types, quality levels, and distribution shifts.
- Why unresolved: Real-world deepfakes may exhibit different characteristics, manipulation techniques, and quality levels compared to the controlled dataset used in this study. The effectiveness of explanation methods may vary depending on the dataset and the nature of the deepfakes.
- What evidence would resolve it: Evaluating the proposed framework and explanation methods on diverse, real-world deepfake datasets (e.g., DFDC, Celeb-DF, WildDeepfake) and comparing the results to those obtained on FaceForensics++.

### Open Question 3
- Question: How do the proposed evaluation framework and explanation methods perform when applied to deepfake videos instead of images?
- Basis in paper: [explicit] The study focuses on deepfake images sampled from videos. The authors do not discuss the applicability of their findings to full video sequences or the challenges specific to video-based deepfake detection and explanation.
- Why unresolved: Deepfake videos introduce additional complexities, such as temporal inconsistencies, artifacts, and manipulations that may span across frames. The effectiveness of explanation methods and the proposed evaluation framework may differ when applied to videos.
- What evidence would resolve it: Extending the study to deepfake videos by incorporating temporal information and analyzing the performance of explanation methods on video sequences. This could involve using video-specific deepfake detection models and adapting the evaluation framework to account for temporal aspects.

## Limitations

- The framework assumes that adversarial attack success directly indicates explanation quality, which may not always hold true
- The study only evaluates five explanation methods on a single detector architecture, limiting generalizability
- The choice of NES parameters (max iterations, learning rate, etc.) significantly impacts results but was not extensively tuned

## Confidence

- High confidence: LIME's superior performance in the specific experimental setup with the tested EfficientNet model and FaceForensics++ dataset
- Medium confidence: The general framework's validity for explanation evaluation across different detectors and datasets
- Low confidence: The framework's applicability to real-world deployment scenarios with varying image qualities and deepfake generation techniques

## Next Checks

1. **Parameter sensitivity analysis**: Test how different NES parameters (learning rate, max iterations, sample count) affect the framework's ability to distinguish between explanation methods. Vary each parameter systematically while keeping others constant to identify which have the most impact on results.

2. **Cross-detector validation**: Apply the framework to at least two additional deepfake detection architectures (e.g., ResNet, Xception) to verify whether LIME consistently outperforms other methods across different model architectures and to test the framework's generalizability.

3. **Real-world robustness testing**: Evaluate the framework's performance on images with varying qualities, compression levels, and deepfake generation methods not present in FaceForensics++. This includes testing on images with different resolutions, noise levels, and compression artifacts to assess real-world applicability.