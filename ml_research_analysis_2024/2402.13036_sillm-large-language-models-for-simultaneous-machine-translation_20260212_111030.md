---
ver: rpa2
title: 'SiLLM: Large Language Models for Simultaneous Machine Translation'
arxiv_id: '2402.13036'
source_url: https://arxiv.org/abs/2402.13036
tags:
- simt
- translation
- policy
- source
- agent
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: SiLLM proposes a framework for Simultaneous Machine Translation
  (SiMT) by leveraging a conventional SiMT model for policy-decision and a Large Language
  Model (LLM) for translation, addressing the complexity of handling both tasks with
  a single model. To align token-level policies from conventional models with the
  word-level processing of LLMs, SiLLM introduces a word-level policy with boundary
  restrictions to prevent outlier policies.
---

# SiLLM: Large Language Models for Simultaneous Machine Translation

## Quick Facts
- arXiv ID: 2402.13036
- Source URL: https://arxiv.org/abs/2402.13036
- Authors: Shoutao Guo; Shaolei Zhang; Zhengrui Ma; Min Zhang; Yang Feng
- Reference count: 26
- Primary result: SiLLM achieves state-of-the-art performance in Simultaneous Machine Translation by decoupling policy-decision and translation tasks between a conventional SiMT model and an LLM.

## Executive Summary
SiLLM addresses the challenge of Simultaneous Machine Translation (SiMT) by proposing a two-agent framework that separates policy-decision and translation tasks. The approach uses a conventional SiMT model (HMT) to make read/generate decisions while an LLM (Llama2-7B-chat) handles translation. This decoupling overcomes the complexity of having a single model handle both tasks simultaneously. With minimal fine-tuning data (100k sentence pairs), SiLLM outperforms previous SiMT methods on German→English and English→German translation tasks while reducing hallucination and maintaining practical translation speeds of 9 words per second.

## Method Summary
SiLLM implements a two-agent framework where a conventional SiMT model serves as the policy-decision agent and an LLM serves as the translation agent. The policy-decision agent (HMT) determines when to read or generate based on token-level policies, which are then transformed into word-level policies with boundary restrictions to prevent outlier decisions. The LLM is fine-tuned using LoRA on a small amount of full-sentence parallel corpus to enable partial-source translation capability. The agents communicate through a shared memory buffer, with the translation agent only activated when the policy-decision agent instructs it to generate.

## Key Results
- SiLLM achieves state-of-the-art performance on WMT15 German→English and MuST-C English→German translation tasks
- Outperforms previous SiMT methods while reducing translation hallucinations
- Maintains practical translation speed of 9 words per second with minimal fine-tuning data (100k samples)
- Successfully handles the vocabulary mismatch problem between token-level policies and LLM processing

## Why This Works (Mechanism)

### Mechanism 1
- Claim: SiLLM decouples SiMT into policy-decision and translation sub-tasks, assigning them to separate agents to avoid overloading a single model.
- Mechanism: A conventional SiMT model (e.g., HMT) serves as the policy-decision agent, determining when to read or generate. An LLM (e.g., Llama2-7B-chat) serves as the translation agent, generating translations when instructed. The two agents collaborate via a shared memory buffer.
- Core assumption: The complexity of SiMT exceeds the capacity of a single model to handle both policy-decision and translation simultaneously.
- Evidence anchors:
  - [abstract] "there is a need to decouple the SiMT task into policy-decision and translation sub-tasks"
  - [section 2] "This exceeds the capability of a single model"
  - [corpus] Weak evidence: only one related paper mentions agent-assisted SiMT, but details differ.
- Break condition: If the policy-decision agent makes poor decisions frequently, the translation agent will generate low-quality output regardless of its strength.

### Mechanism 2
- Claim: Word-level policy derived from token-level policy solves the vocabulary mismatch problem when applying SiMT policies to LLM.
- Mechanism: Token-level policies from conventional SiMT models are transformed into word-level policies. Boundary restrictions (min/max source words per target word) prevent outlier policies that cause excessive latency or insufficient context.
- Core assumption: Token-level policies are incompatible with LLM vocabularies, and outlier policies degrade performance.
- Evidence anchors:
  - [abstract] "To facilitate the application of token-level policies... we propose a word-level policy adapted for LLM"
  - [section 3.2] "applying them directly to LLM poses a vocabulary mismatch problem"
  - [corpus] Weak evidence: no direct corpus support for word-level vs token-level policy transformation.
- Break condition: If boundary hyperparameters B and T are poorly chosen, either latency will be too high or translation quality will suffer.

### Mechanism 3
- Claim: Supervised Fine-Tuning (SFT) on full-sentence parallel corpus enhances LLM translation capability for partial source inputs.
- Mechanism: Despite not being trained on SiMT-specific data, SFT on full-sentence data enables the LLM to generate high-quality translations from partial source sentences during inference.
- Core assumption: LLM can generalize from full-sentence translation to partial-source translation without explicit SiMT training data.
- Evidence anchors:
  - [abstract] "with a small amount of data for fine-tuning LLM, SiLLM attains state-of-the-art performance"
  - [section 3.3] "we fine-tune the LLM using a limited amount of full-sentence parallel corpus"
  - [corpus] Weak evidence: no corpus evidence directly supports this generalization claim.
- Break condition: If the SFT data is too small or unrepresentative, the LLM may fail to generalize to partial-source translation scenarios.

## Foundational Learning

- Concept: Transformer architecture and its components (encoder, decoder, attention mechanisms)
  - Why needed here: Understanding the baseline SiMT methods that SiLLM builds upon and how the policy-decision agent operates
  - Quick check question: What is the key difference between encoder-decoder transformers and decoder-only transformers in terms of information flow?

- Concept: Simultaneous Machine Translation (SiMT) concepts: policy, latency metrics (Average Lagging), and trade-offs between latency and translation quality
  - Why needed here: To understand the problem SiLLM solves and how its performance is measured
  - Quick check question: How does Average Lagging (AL) quantify latency in SiMT, and what does lower AL mean for user experience?

- Concept: Large Language Models (LLMs): architecture, training objectives, and limitations in streaming scenarios
  - Why needed here: To understand why LLMs alone cannot perform SiMT and how SiLLM adapts them
  - Quick check question: Why can't standard LLMs handle streaming input directly, and what architectural change would be needed?

## Architecture Onboarding

- Component map:
  Memory buffer -> Policy-decision agent (HMT) -> Word-level policy converter -> Translation agent (LLM) -> Memory buffer

- Critical path: Policy-decision agent reads from memory → decides action → if generate, activates translation agent → translation agent reads from memory → generates next word → updates memory → cycle repeats

- Design tradeoffs:
  - Using LLM for translation provides strong generation capability but introduces inference speed overhead
  - Word-level policy conversion adds complexity but solves vocabulary mismatch
  - SFT on full sentences works empirically but lacks theoretical justification
  - Memory buffer design impacts both latency and implementation complexity

- Failure signatures:
  - High latency with poor translation quality: likely policy-decision agent making overly conservative decisions
  - Translation hallucinations: insufficient source context or poor policy decisions
  - System deadlock: memory buffer management errors or agent communication failures
  - Degradation with longer sentences: memory buffer overflow or policy granularity issues

- First 3 experiments:
  1. Test basic agent collaboration: Run SiLLM with a simple fixed policy (Wait-k) as policy-decision agent and un-fine-tuned LLM, verify memory buffer updates correctly
  2. Validate word-level policy conversion: Compare token-level vs word-level policy outputs on sample sentences, verify boundary restrictions work
  3. Measure SFT impact: Fine-tune LLM on small parallel corpus, compare translation quality on partial vs full source inputs

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of SiLLM vary when using different types of LLM as the translation agent (e.g., different sizes, architectures, or pre-training objectives)?
- Basis in paper: [inferred] The paper uses Llama2-7B-chat as the translation agent but mentions that "any LLM can be employed as the translation agent" without restraints on the architecture.
- Why unresolved: The paper only experiments with one specific LLM model (Llama2-7B-chat) and does not explore the impact of varying the translation agent.
- What evidence would resolve it: Experiments comparing the performance of SiLLM using different LLM models (varying in size, architecture, and pre-training objectives) as the translation agent.

### Open Question 2
- Question: What is the impact of the amount and quality of fine-tuning data on the performance of SiLLM, particularly when using SiMT data versus full-sentence parallel data?
- Basis in paper: [explicit] The paper mentions that "with a small amount of data for fine-tuning LLM, SiLLM attains state-of-the-art performance" and compares using full-sentence data versus SiMT data for fine-tuning.
- Why unresolved: The paper only explores a limited amount of data (100k samples) and does not investigate the impact of varying the amount and quality of fine-tuning data.
- What evidence would resolve it: Experiments comparing the performance of SiLLM using different amounts and qualities of fine-tuning data, including both SiMT data and full-sentence parallel data.

### Open Question 3
- Question: How does the performance of SiLLM change when using different policy-decision agents, such as other Transformer-based SiMT models or non-Transformer models?
- Basis in paper: [explicit] The paper mentions that "any conventional SiMT model can serve as the policy-decision agent" and experiments with HMT and Wait-k as policy-decision agents.
- Why unresolved: The paper only experiments with two specific policy-decision agents (HMT and Wait-k) and does not explore the impact of using different types of policy-decision agents.
- What evidence would resolve it: Experiments comparing the performance of SiLLM using different policy-decision agents, including other Transformer-based SiMT models and non-Transformer models.

## Limitations

- The word-level policy transformation mechanism lacks detailed implementation specifications and theoretical justification for boundary hyperparameter selection
- The claim that SFT on full-sentence data enables effective partial-source translation is empirically supported but lacks theoretical justification
- The paper does not address computational overhead comparisons between SiLLM and conventional SiMT methods

## Confidence

- **High confidence**: The fundamental architectural claim that decoupling policy-decision and translation into separate agents enables better performance. This is supported by clear experimental results showing SiLLM outperforming state-of-the-art methods.
- **Medium confidence**: The word-level policy transformation mechanism. While the concept is sound and the boundary restriction approach is reasonable, the lack of detailed implementation specifics and corpus evidence weakens confidence.
- **Medium confidence**: The SFT generalization claim. The empirical results support the approach, but the theoretical justification for why full-sentence fine-tuning enables partial-source translation is weak.

## Next Checks

1. **Policy granularity ablation study**: Systematically vary the boundary hyperparameters B and T in the word-level policy transformation to identify optimal settings and validate the claim that these prevent outlier policies. Measure both translation quality and latency across the hyperparameter space.

2. **Zero-shot partial-source translation test**: Evaluate the fine-tuned LLM's ability to generate translations from partial source inputs without any SiMT-specific fine-tuning data. Compare performance against models trained with even small amounts of SiMT data to quantify the generalization claim.

3. **Agent failure mode analysis**: Deliberately introduce errors in the policy-decision agent (e.g., force incorrect read/generate decisions) and measure the impact on translation quality. This would validate the claim that poor policy decisions degrade translation regardless of translation agent strength.