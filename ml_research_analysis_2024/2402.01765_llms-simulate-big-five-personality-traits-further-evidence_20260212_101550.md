---
ver: rpa2
title: 'LLMs Simulate Big Five Personality Traits: Further Evidence'
arxiv_id: '2402.01765'
source_url: https://arxiv.org/abs/2402.01765
tags:
- personality
- traits
- llms
- arxiv
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigates whether large language models (LLMs) simulate
  human-like personality traits by applying the Big Five model (FFM) to ChatGPT4,
  Llama2, and Mixtral. Using the IPIP-NEO-120 questionnaire with slight prompt variations
  and different temperature settings, the models were evaluated on the five dimensions
  of personality.
---

# LLMs Simulate Big Five Personality Traits: Further Evidence

## Quick Facts
- **arXiv ID**: 2402.01765
- **Source URL**: https://arxiv.org/abs/2402.01765
- **Reference count**: 8
- **Primary result**: Different LLMs exhibit distinct Big Five personality profiles, with GPT4 showing highest extraversion, Llama2 neutral, and Mixtral displaying higher openness, agreeableness, and conscientiousness.

## Executive Summary
This study investigates whether large language models simulate human-like personality traits by applying the Big Five model to GPT4, Llama2, and Mixtral. Using the IPIP-NEO-120 questionnaire with prompt variations and temperature settings, the models were evaluated on five personality dimensions. Results show distinct personality profiles across models, with GPT4 most responsive to temperature variation. The findings suggest LLMs may be perceived as having personality traits, with implications for personalized human-computer interaction and fine-tuning strategies.

## Method Summary
The study administered the IPIP-NEO-120 questionnaire to three LLMs (GPT4, Llama2, Mixtral) using six prompt variations (two headers Ã— three temperature settings). Each model completed the questionnaire five times per treatment, with responses scored on a 1-5 scale for Big Five traits. Mean scores and standard deviations were calculated across repetitions to assess personality profiles and stability under different conditions.

## Key Results
- GPT4 scored highest on extraversion across all temperature settings
- Llama2 showed the most neutral personality profile among the three models
- Mixtral displayed higher openness, agreeableness, and conscientiousness compared to other models
- Minor prompting changes affected all models' personality profiles
- GPT4 demonstrated notable responsiveness to temperature variation

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLMs simulate personality traits by producing output distributions that align with statistical patterns associated with human-generated text exhibiting those traits.
- Mechanism: The LLM's internal probability distributions over tokens are influenced by fine-tuning and prompting in ways that shift the likelihood of generating certain linguistic patterns, which are associated with Big Five traits.
- Core assumption: The LLM's learned representations encode statistical regularities from human text that correlate with personality dimensions, even without explicit personality modeling.
- Evidence anchors:
  - [abstract] The study shows that different models (GPT4, Llama2, Mixtral) exhibit distinct personality profiles, indicating that their output distributions vary in ways consistent with human personality traits.
  - [section] "This type of anthropomorphism lies at the very heart of the intended uses of generative language technologies."

### Mechanism 2
- Claim: Prompt variations and temperature settings modulate the LLM's output to emphasize or de-emphasize specific personality traits.
- Mechanism: Changing the prompt or temperature alters the sampling process, which affects the probability distribution of generated text and thus the simulated personality expression.
- Core assumption: The LLM's response to prompt variations and temperature changes is stable enough to consistently shift the personality profile.
- Evidence anchors:
  - [abstract] "GPT4 is notably responsive to temperature variation, while minor prompting changes affect all models."
  - [section] "The second prompt variation that contains the sentence 'Answer as if you were a person' was included during our initial experiments since it allowed us to elicit answers to questionnaire items that would otherwise be caught by the restriction mechanisms of some LLMs."

### Mechanism 3
- Claim: The IPIP-NEO-120 questionnaire, when administered to LLMs, elicits responses that can be mapped to Big Five personality trait scores.
- Mechanism: The LLM interprets the questionnaire items as instructions to generate text that reflects a personality profile, and the responses are scored according to the standard Big Five framework.
- Core assumption: The LLM can interpret and respond to the questionnaire items in a way that is meaningful for personality assessment.
- Evidence anchors:
  - [section] "The prompts consisted of one of the following two headers, followed by the respective descriptive statement from the IPIP-NEO-120 questionnaire."
  - [section] "We report results in the form of scores between 1 and 5, as is common for Big5."

## Foundational Learning

- **Concept: Statistical language modeling**
  - Why needed here: Understanding how LLMs generate text based on probability distributions is crucial for grasping how they can simulate personality traits.
  - Quick check question: How does an LLM's next-token prediction relate to the generation of text exhibiting specific personality traits?

- **Concept: Anthropomorphism in AI**
  - Why needed here: Recognizing that attributing personality traits to LLMs is a form of anthropomorphism helps in interpreting the results and their implications.
  - Quick check question: Why is it important to explicitly state that LLMs do not possess agency when discussing their personality profiles?

- **Concept: Big Five personality model**
  - Why needed here: Familiarity with the Big Five model is essential for understanding the framework used to assess the LLM's personality simulation.
  - Quick check question: What are the five fundamental personality traits in the Big Five model, and how are they typically measured?

## Architecture Onboarding

- **Component map**: LLM -> Prompting module -> Scoring module -> Evaluation module
- **Critical path**:
  1. Construct prompts based on the IPIP-NEO-120 questionnaire.
  2. Send prompts to the LLM and collect responses.
  3. Score the responses according to the Big Five framework.
  4. Analyze the results for stability and consistency across different treatments.

- **Design tradeoffs**:
  - Prompt variation vs. stability: More varied prompts may elicit richer responses but could reduce stability.
  - Temperature settings vs. consistency: Higher temperatures may generate more diverse responses but could lead to less consistent personality profiles.

- **Failure signatures**:
  - Inconsistent responses across repeated trials with the same prompt and temperature.
  - LLM refusing to answer certain questionnaire items.
  - Scores that do not align with expected personality trait distributions.

- **First 3 experiments**:
  1. Test the LLM's response to a standard Big Five questionnaire with no prompt variation or temperature adjustment.
  2. Introduce a minor prompt variation (e.g., "Answer as if you were a person") and observe the effect on the LLM's responses.
  3. Vary the temperature settings and analyze the impact on the stability of the simulated personality profiles.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: To what extent do temperature parameters systematically affect the simulation of personality traits in LLMs?
- Basis in paper: [explicit] The paper notes that GPT4 is the only model responsive to temperature variation, but results for various temperatures do not allow conclusive statements about the effect of temperature parameters on simulated personality traits.
- Why unresolved: The study's temperature parameter choices were based on documentation recommendations, and the variations observed were not conclusive enough to establish a systematic effect.
- What evidence would resolve it: A systematic study varying temperature parameters across a wider range and analyzing the resulting personality trait simulations could provide more definitive evidence.

### Open Question 2
- Question: How generalizable are the observed personality profiles of LLMs across different domains and tasks?
- Basis in paper: [explicit] The paper acknowledges that the versatility of LLMs makes it difficult to estimate the extent to which the observations could be generalized.
- Why unresolved: The study is a case-study, and the observed personality profiles may not be consistent across different domains and tasks due to the inherent versatility of LLMs.
- What evidence would resolve it: Conducting experiments across diverse domains and tasks, and comparing the resulting personality profiles, would help assess the generalizability of the observed profiles.

### Open Question 3
- Question: How can fine-tuning and prompt design be optimized to personalize LLM outputs while maintaining the stability and consistency of simulated personality traits?
- Basis in paper: [explicit] The paper suggests that future work should explore how fine-tuning and prompt design may be used to optimize LLM outputs for personalized user engagement, considering the appropriateness, stability, and consistency of simulated personality traits.
- Why unresolved: The study does not provide insights into the optimization of fine-tuning and prompt design for personalized LLM outputs, nor does it address the stability and consistency of simulated personality traits.
- What evidence would resolve it: Research focusing on fine-tuning strategies and prompt design techniques that maintain stable and consistent personality trait simulations while optimizing for personalized user engagement would be valuable.

## Limitations
- The study relies on self-report-style questionnaires administered to non-conscious systems, which may capture response patterns rather than authentic personality-like states.
- The stability of personality profiles across longer interactions and varied contexts has not been established.
- The scoring methodology assumes that LLM responses can be meaningfully mapped to human personality trait scales without accounting for potential systematic differences in how models process questionnaire items.

## Confidence
- **High Confidence**: The finding that different LLM architectures produce distinct personality profiles under standardized assessment conditions is well-supported by the experimental design and results.
- **Medium Confidence**: The responsiveness of GPT4 to temperature variation and the general effect of prompt variations on all models are moderately supported, though the specific magnitude of these effects and their practical significance require further investigation.
- **Low Confidence**: The interpretation that these personality profiles represent genuine simulation of human-like personality traits rather than response pattern artifacts remains speculative and requires additional validation.

## Next Checks
1. **Cross-context stability test**: Administer the same personality assessment across multiple interaction sessions and varied conversational contexts to determine whether the observed personality profiles remain consistent over time and usage patterns.

2. **Human comparison validation**: Compare LLM personality profiles against matched human populations using the same assessment instrument to establish whether the observed patterns genuinely reflect human-like personality distributions or represent model-specific artifacts.

3. **Fine-tuning sensitivity analysis**: Test whether targeted fine-tuning on personality-specific datasets can systematically modify the observed personality profiles, helping to distinguish between inherent architectural properties and learned response patterns.