---
ver: rpa2
title: 'FADA: Fast Diffusion Avatar Synthesis with Mixed-Supervised Multi-CFG Distillation'
arxiv_id: '2412.16915'
source_url: https://arxiv.org/abs/2412.16915
tags:
- distillation
- arxiv
- diffusion
- teacher
- talking
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the slow inference speed of diffusion-based
  audio-driven talking avatar methods by proposing FADA, a fast diffusion avatar synthesis
  framework with mixed-supervised multi-CFG distillation. The method introduces a
  mixed-supervised loss to leverage data of varying quality for enhanced model robustness
  and a multi-CFG distillation with learnable tokens to reduce inference runs while
  maintaining audio-visual correlation.
---

# FADA: Fast Diffusion Avatar Synthesis with Mixed-Supervised Multi-CFG Distillation

## Quick Facts
- arXiv ID: 2412.16915
- Source URL: https://arxiv.org/abs/2412.16915
- Reference count: 40
- Achieves up to 12.5× speedup while maintaining generation quality

## Executive Summary
FADA addresses the slow inference speed of diffusion-based audio-driven talking avatar methods by proposing a fast diffusion avatar synthesis framework with mixed-supervised multi-CFG distillation. The method introduces a mixed-supervised loss to leverage data of varying quality for enhanced model robustness and a multi-CFG distillation with learnable tokens to reduce inference runs while maintaining audio-visual correlation. Experiments across multiple datasets show that FADA achieves up to a 12.5× speedup while generating vivid portrait videos comparable to recent diffusion model-based methods.

## Method Summary
FADA combines PeRFlow optimization with mixed-supervised distillation and multi-CFG distillation with learnable tokens. The teacher model is trained on high-quality data using a dual-Unet architecture based on Stable Diffusion 1.5. The student model is then trained using a mixed-supervised loss that combines teacher-supervised and ground-truth losses with adaptive weighting based on data quality. Multi-CFG distillation replaces three separate CFG inferences with learnable tokens that capture audio-visual relationships, while PeRFlow rectification enables fewer inference steps.

## Key Results
- Achieves up to 12.5× speedup (reducing NFE-D from 75 to 6)
- Maintains competitive performance in IQA, FVD, and Sync-D metrics
- Demonstrates effectiveness across multiple datasets including CelebV-HQ, HDTF, and Loopy openset

## Why This Works (Mechanism)

### Mechanism 1
Mixed-supervised loss leverages lower-quality data to improve model robustness without degrading teacher model performance. The student model is trained on a larger dataset that includes high-quality and moderate-quality data, with an adaptive weight that adjusts based on the ratio between ground-truth and teacher-supervised losses.

### Mechanism 2
Multi-CFG distillation with learnable tokens reduces inference runs while maintaining audio-visual correlation. Instead of running three separate CFG inferences per denoising step, learnable tokens are injected into the network to mimic the weighted calculations of multi-CFG, learning the relationships between multiple conditions during training.

### Mechanism 3
PeRFlow distillation enables efficient inference by rectifying ODE flow to piecewise straight lines. The training creates piecewise straight time windows and uses linear interpolation to derive input noisy latents, reducing the number of timesteps needed at inference.

## Foundational Learning

- **Diffusion models and denoising process**: Understanding how diffusion models work is essential for grasping why distillation can accelerate inference
  - Quick check: How does a diffusion model generate images from noise through iterative denoising steps?

- **Classifier-Free Guidance (CFG) and multi-CFG**: Multi-CFG is used to balance audio and image conditions, and understanding its mechanism is crucial for the distillation approach
  - Quick check: What is the difference between single CFG and multi-CFG inference in terms of how they combine conditional predictions?

- **Audio-driven talking avatar synthesis**: The specific task involves synchronizing lip movements and expressions with audio, which is more complex than standard image generation
  - Quick check: Why is audio considered a weaker condition compared to visual references in talking avatar synthesis?

## Architecture Onboarding

- **Component map**: Reference image → Reference net → Denoising net (with CFG tokens) → VAE → Output video
- **Critical path**: Reference image → Reference net → Denoising net (with CFG tokens) → VAE → Output video
- **Design tradeoffs**: Speed vs quality (fewer inference steps reduce speed but may impact quality), data quality vs robustness (using moderate-quality data improves robustness but requires careful loss balancing), complexity vs performance (learnable tokens add complexity but enable significant speedup)
- **Failure signatures**: Audio-visual desynchronization (multi-CFG distillation not working properly), artifacts in generated videos (learnable tokens failing to capture condition relationships), poor identity preservation (reference processing not working correctly)
- **First 3 experiments**:
  1. Test basic PeRFlow distillation without mixed supervision or CFG distillation to establish baseline
  2. Add mixed-supervised loss with fixed weights to evaluate data quality utilization
  3. Implement learnable token-based CFG distillation to measure speedup impact on quality

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of FADA vary when trained on datasets with different degrees of audio-visual synchronization quality?
- Basis in paper: The paper discusses using high-quality data for teacher training and moderate-quality data for student distillation, with adaptive loss weighting based on the ratio of ground-truth to teacher-supervised loss.
- Why unresolved: The paper does not provide experiments isolating the impact of varying audio-visual synchronization quality in the training data on the final model performance.

### Open Question 2
- Question: What is the impact of different noise schedules on the performance and speed of FADA during inference?
- Basis in paper: The paper mentions using a noise schedule in the PeRFlow framework for piecewise rectified flow, but does not explore different noise schedules.
- Why unresolved: The paper does not investigate how alternative noise schedules might affect the trade-off between inference speed and generation quality in FADA.

### Open Question 3
- Question: How does FADA's performance scale with the number of denoising steps during inference, and is there an optimal balance between speed and quality?
- Basis in paper: The paper demonstrates FADA achieving up to 12.5× speedup with 6-step inference compared to the teacher model's 75 NFE-D, but does not explore the performance-quality trade-off across a wider range of step counts.
- Why unresolved: The paper focuses on demonstrating the effectiveness of FADA with a fixed 6-step inference, without investigating how performance changes with different numbers of steps.

## Limitations

- Evaluation lacks comparisons with other acceleration techniques like consistency models or alternative distillation methods
- Mixed-supervised loss mechanism relies heavily on teacher model reliability across varying data qualities without extensive ablations
- Learnable CFG tokens' effectiveness is primarily theoretical with limited empirical validation of learned representations

## Confidence

**High confidence**: The core technical contribution of combining PeRFlow with multi-CFG distillation is well-defined and the reported speedup numbers are plausible given the reduction from 75 to 6 NFE-D.

**Medium confidence**: The effectiveness of the adaptive weighting strategy depends on proper calibration of Rp and Rd thresholds, which are not specified. The claim about maintaining "vivid portrait videos comparable to recent undistilled diffusion models" is supported by the reported metrics but could benefit from more extensive qualitative comparisons.

**Low confidence**: The paper's assertion that the learnable tokens effectively replace three separate CFG inferences is primarily theoretical, with limited empirical validation of the tokens' learned representations.

## Next Checks

1. **Ablation study on adaptive loss weighting**: Run experiments with fixed vs adaptive weights across different data quality distributions to quantify the benefit of the mixed-supervised approach and identify optimal threshold values for Rp and Rd.

2. **CFG token interpretability analysis**: Visualize and analyze the learned values of γb, γr, γa across different input conditions to verify they capture meaningful relationships between audio, reference, and background conditions, and test model performance when these tokens are randomly initialized.

3. **Cross-dataset robustness evaluation**: Test the distilled model on out-of-distribution data with quality levels not seen during training to assess whether the mixed-supervised training truly improves robustness or if performance degrades when teacher guidance becomes unreliable.