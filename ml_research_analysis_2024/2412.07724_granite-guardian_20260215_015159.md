---
ver: rpa2
title: Granite Guardian
arxiv_id: '2412.07724'
source_url: https://arxiv.org/abs/2412.07724
tags:
- granite
- prompt
- guardian
- risk
- harmful
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Granite Guardian, a family of risk detection
  models (2B and 8B variants) designed to safeguard large language models (LLMs) by
  detecting a broad spectrum of risks including harmful content, social bias, profanity,
  violence, sexual content, unethical behavior, jailbreaks, and RAG-specific hallucination
  risks such as context relevance, groundedness, and answer relevance. Trained on
  a unique dataset combining human-annotated data from diverse contributors and synthetic
  data, Granite Guardian achieves state-of-the-art performance with AUC scores of
  0.871 for harmful content detection and 0.854 for RAG hallucination detection, outperforming
  existing models like Llama Guard and ShieldGemma on both aggregated and dataset-specific
  benchmarks.
---

# Granite Guardian

## Quick Facts
- arXiv ID: 2412.07724
- Source URL: https://arxiv.org/abs/2412.07724
- Authors: Inkit Padhi, Manish Nagireddy, Giandomenico Cornacchia, Subhajit Chaudhury, Tejaswini Pedapati, Pierre Dognin, Keerthiram Murugesan, Erik Miehling, Martín Santillán Cooper, Kieran Fraser, Giulio Zizzo, Muhammad Zaid Hameed, Mark Purcell, Michael Desmond, Qian Pan, Zahra Ashktorab, Inge Vejsbjerg, Elizabeth M. Daly, Michael Hind, Werner Geyer, Ambrish Rawat, Kush R. Varshney, Prasanna Sattigeri
- Reference count: 20
- Primary result: Introduces Granite Guardian, a family of risk detection models (2B and 8B variants) achieving state-of-the-art performance with AUC scores of 0.871 for harmful content detection and 0.854 for RAG hallucination detection

## Executive Summary
This paper introduces Granite Guardian, a family of risk detection models designed to safeguard large language models by detecting a broad spectrum of risks including harmful content, social bias, profanity, violence, sexual content, unethical behavior, jailbreaks, and RAG-specific hallucination risks. The models are trained on a unique dataset combining human-annotated data from diverse contributors and synthetic data, achieving state-of-the-art performance on both aggregated and dataset-specific benchmarks. Granite Guardian is released as open-source to promote responsible AI development.

## Method Summary
Granite Guardian is developed through supervised fine-tuning of Granite 3.0 language models using a specialized safety instruction template. The training combines human-annotated data from diverse sources with synthetic data generation covering adversarial attacks, jailbreaks, and RAG-specific risks. The models use token-based probability aggregation for risk detection, computing scores by summing log-likelihoods of top-k tokens containing "Yes" or "No" variations. Evaluation is performed across multiple benchmarks including AegisSafetyTest, ToxicChat, and BeaverTails, measuring performance using AUC, AUPRC, F1 scores, and other classification metrics.

## Key Results
- Achieves AUC score of 0.871 for harmful content detection and 0.854 for RAG hallucination detection
- Outperforms existing models like Llama Guard and ShieldGemma on both aggregated and dataset-specific benchmarks
- Available in two variants (2B and 8B) to balance performance and resource requirements
- Supports custom risk definitions while maintaining strong baseline performance across standard risk categories

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: The dual-training on both human-annotated and synthetic data broadens the model's coverage of real-world risks.
- **Mechanism**: Synthetic data fills gaps in adversarial scenarios (e.g., jailbreaks) and rare RAG-specific hallucinations, while human annotations ensure nuanced understanding of social bias, profanity, and harmful intent.
- **Core assumption**: Human and synthetic data are sufficiently diverse and balanced to prevent overfitting or bias amplification.
- **Evidence anchors**:
  - [abstract]: "Trained on a unique dataset combining human-annotations from diverse sources and synthetic data..."
  - [section 3.1]: Detailed annotation process with diverse demographics and quality controls.
- **Break condition**: If synthetic data is not properly curated, it could introduce bias or irrelevant patterns that harm generalization.

### Mechanism 2
- **Claim**: Using a unified safety instruction template with control tokens allows flexible risk detection across multiple categories.
- **Mechanism**: The template standardizes input format for both prompts and responses, enabling the model to parse and detect risks like harm, bias, or RAG-specific issues without retraining per category.
- **Core assumption**: The control tokens are consistently parsed by the model and the template captures all relevant context.
- **Evidence anchors**:
  - [section 4.1]: "Utilizing the safety instruction template... we transformed each sample... tailoring it to the specific risk category."
- **Break condition**: If the template parsing fails or tokens are misinterpreted, risk detection accuracy will drop.

### Mechanism 3
- **Claim**: Improved probability estimation via token aggregation improves detection confidence.
- **Mechanism**: Instead of relying on a single token's probability, Granite Guardian sums log-likelihoods of top-k tokens containing "Yes" or "No" to better estimate class confidence.
- **Core assumption**: Log-likelihood aggregation across lexical variations of "Yes" and "No" yields more robust probability estimates.
- **Evidence anchors**:
  - [section 4.3]: "We compute the detection scores for the positive (unsafe) and negative (safe) classes as, score unsafe = ∑u∈U|k exp(LL(u)), and score safe = ∑s∈S|k exp(LL(s))"
- **Break condition**: If the top-k tokens do not capture meaningful variations, the aggregation may be noisy or misleading.

## Foundational Learning

- **Concept**: Risk taxonomy and categorization
  - Why needed here: Granite Guardian operates over a broad set of risk categories (harm, bias, jailbreak, RAG-specific). Understanding the taxonomy ensures correct template usage and risk definition mapping.
  - Quick check question: What are the three main dimensions of risk that Granite Guardian detects in RAG use cases?
    - Answer: Context relevance, groundedness, and answer relevance.

- **Concept**: Supervised fine-tuning (SFT) and safety instruction templates
  - Why needed here: The model is built by fine-tuning Granite 3.0 using a safety instruction template. Engineers must understand how to prepare data and structure prompts for training.
  - Quick check question: In the safety instruction template, what are the three key components that guide the model's behavior?
    - Answer: The agent's role definition, the content to be evaluated (tagged), and the risk definition in control tokens.

- **Concept**: Probability estimation in classification tasks
  - Why needed here: Granite Guardian computes detection scores by aggregating token probabilities. Knowing this helps in interpreting model outputs and setting thresholds.
  - Quick check question: How does Granite Guardian compute the probability of risk differently from typical single-token probability methods?
    - Answer: It sums the exponentiated log-likelihoods of all top-k tokens containing "Yes" or "No" before applying softmax.

## Architecture Onboarding

- **Component map**:
  Input Layer (safety instruction template) -> Granite Guardian model (2B or 8B) -> Output Layer ("Yes"/"No" label + probability)

- **Critical path**:
  1. Receive input (prompt/response + context if RAG)
  2. Format into safety instruction template
  3. Pass through Granite Guardian model
  4. Compute token-based probability scores
  5. Output label and risk probability

- **Design tradeoffs**:
  - Larger model (8B) offers better detection performance but higher latency/cost
  - Smaller model (2B) is faster and cheaper but slightly less accurate
  - Unified template approach enables multi-risk detection but may sacrifice per-risk specialization

- **Failure signatures**:
  - Low recall on specific risk categories → template parsing or training data imbalance
  - High false positives → threshold too low or model overconfident on benign content
  - Inconsistent outputs → tokenization or probability aggregation issues

- **First 3 experiments**:
  1. **Template parsing test**: Input a known safe and unsafe example through the safety template and verify correct label and probability output.
  2. **Threshold tuning test**: Sweep probability thresholds and plot recall vs false positive rate to find optimal deployment threshold.
  3. **RAG hallucination detection test**: Use a small RAG dataset to verify detection of context relevance, groundedness, and answer relevance risks.

## Open Questions the Paper Calls Out

- **Open Question 1**: How does Granite Guardian's performance generalize to languages other than English?
  - Basis in paper: [inferred] from the statement "Trained and tested on English data" and lack of multilingual evaluation
  - Why unresolved: The paper only evaluates on English datasets and acknowledges the models are trained/tested on English data, but doesn't explore multilingual capabilities
  - What evidence would resolve it: Evaluation results on multilingual datasets across different languages would demonstrate generalization capability

- **Open Question 2**: What is the optimal threshold for risk probability that balances safety and utility across different application contexts?
  - Basis in paper: [explicit] from the statement "Users can further tailor Granite Guardian to specific operational needs by defining thresholds over the probability of risk"
  - Why unresolved: The paper provides general guidelines but doesn't specify optimal threshold values for different use cases or how to determine them
  - What evidence would resolve it: Empirical studies showing performance trade-offs at different threshold values across various deployment scenarios

- **Open Question 3**: How does Granite Guardian perform on emerging or novel types of risks not included in the training taxonomy?
  - Basis in paper: [explicit] from the statement "Custom risk definitions are also supported but require testing"
  - Why unresolved: While the paper mentions custom risk definitions are possible, it doesn't evaluate how well the models adapt to new, unseen risk categories
  - What evidence would resolve it: Testing results on novel risk categories outside the training taxonomy would show adaptation capabilities

- **Open Question 4**: What is the computational overhead of integrating Granite Guardian as a safeguard compared to using the base LLM alone?
  - Basis in paper: [inferred] from the discussion of model variants (2B vs 8B) and the mention of resource-constrained scenarios
  - Why unresolved: The paper discusses model sizes and variants but doesn't provide specific performance metrics or latency comparisons
  - What evidence would resolve it: Quantitative measurements of inference time, memory usage, and throughput for different model sizes compared to baseline LLM performance

## Limitations

- Synthetic data generation process lacks detailed implementation specifics, creating uncertainty about reproducibility of the key dual-training approach
- Performance claims are primarily validated against existing models rather than through extensive real-world deployment testing
- Unified template approach may compromise per-risk specialization despite enabling flexible multi-risk detection

## Confidence

- Synthetic Data Quality: Medium - Key component but lacks detailed validation and implementation specifics
- Template Parsing Robustness: Medium - Relies on consistent control token interpretation with limited error analysis
- Benchmark Performance: High - Strong results supported by comprehensive evaluation across multiple datasets

## Next Checks

1. **Synthetic Data Quality Validation**: Generate a small set of synthetic adversarial examples and manually verify their quality and diversity to ensure they represent realistic attack scenarios rather than artificial patterns.

2. **Template Parsing Robustness Test**: Create variations of control tokens and test whether the model consistently parses them across different risk categories, identifying any parsing failures or inconsistencies.

3. **Real-world Deployment Pilot**: Deploy the model on a small-scale production dataset to evaluate performance drift and identify any failure modes not captured in benchmark testing, particularly for rare risk categories.