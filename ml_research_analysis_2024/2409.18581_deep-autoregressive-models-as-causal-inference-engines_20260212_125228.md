---
ver: rpa2
title: Deep Autoregressive Models as Causal Inference Engines
arxiv_id: '2409.18581'
source_url: https://arxiv.org/abs/2409.18581
tags:
- causal
- data
- outcome
- potential
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a causal inference framework that leverages
  autoregressive (AR) language models through a process called sequencification. The
  method transforms observational data into sequential token representations based
  on a known causal DAG, allowing a single AR model to compute multiple interventional
  distributions efficiently.
---

# Deep Autoregressive Models as Causal Inference Engines

## Quick Facts
- arXiv ID: 2409.18581
- Source URL: https://arxiv.org/abs/2409.18581
- Authors: Daniel Jiwoong Im; Kevin Zhang; Nakul Verma; Kyunghyun Cho
- Reference count: 19
- Primary result: Autoregressive language models can efficiently compute causal effects through sequencification of observational data

## Executive Summary
This paper introduces a novel framework for causal inference using autoregressive language models through a process called sequencification. The method transforms observational data into sequential token representations based on a known causal DAG, allowing a single AR model to compute multiple interventional distributions efficiently. The approach addresses limitations of existing CI methods by handling high-dimensional confounders and variable-length action sequences, demonstrated through experiments on maze navigation, chess endgames, and the PeerRead dataset.

## Method Summary
The method sequencifies observational data by converting each variable's value into a unique token sequence with special start tokens identifying variable positions. An autoregressive transformer model is then trained on these sequencified representations to learn conditional probability distributions implied by the causal DAG. Interventional distributions are computed through Monte Carlo sampling from the trained model, which can leverage pre-trained language models for text-based causal inference tasks.

## Key Results
- Successfully navigates mazes with mean squared error of potential outcome predictions
- Accurately estimates chess endgame move effects through partial interventions
- Evaluates keyword impact on paper acceptance rates with improved Average Treatment Effect on the Treated (ATT) relative error

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Sequencification transforms observational data into sequential token representations based on a known causal DAG, enabling a single AR model to compute multiple interventional distributions efficiently.
- Mechanism: The sequencification process converts each variable's value into a unique token sequence with a special start token (⟨starti⟩) to identify the variable's position. This linearization preserves the causal ordering information from the DAG while allowing the AR model to learn conditional probability distributions in a unified way.
- Core assumption: The underlying causal DAG is known exactly and all variables are observed (no latent confounders).

### Mechanism 2
- Claim: The AR model can efficiently compute interventional distributions through Monte Carlo sampling by leveraging the learned conditional distributions from sequencified data.
- Mechanism: Once trained on sequencified data, the AR model learns pθ(X) and pθ(Y|A,X) distributions. Interventional distributions like p(Y|do(A=a)) can be approximated by sampling X from pθ(X) and computing pθ(Y|A=a,X) for each sample, then averaging results.
- Core assumption: The AR model accurately learns the conditional distributions implied by the causal DAG.

### Mechanism 3
- Claim: Pre-trained language models can be leveraged for causal inference in text-based settings by fine-tuning on sequencified representations.
- Mechanism: Pre-trained LMs like BERT and GPT have learned rich representations of text data. By fine-tuning these models on sequencified causal data, they can leverage their understanding of language structure while learning causal relationships, improving performance on text-based CI tasks.
- Core assumption: The pre-trained LM has learned representations that capture relevant information for the causal inference task.

## Foundational Learning

- Concept: Directed Acyclic Graphs (DAGs) and causal ordering
  - Why needed here: The sequencification process relies on knowing the topological ordering of variables in the causal DAG to construct meaningful token sequences.
  - Quick check question: If variable A causes variable B in a DAG, which variable must appear first in the sequencified representation?

- Concept: Autoregressive modeling and next-token prediction
  - Why needed here: The AR model learns to predict each token given previous tokens in the sequence, which corresponds to learning conditional probability distributions in the causal DAG.
  - Quick check question: How does the AR model compute p(X1,X2,X3) when trained on the sequence [⟨start1⟩, x1, ⟨start2⟩, x2, ⟨start3⟩, x3]?

- Concept: Monte Carlo estimation and sampling
  - Why needed here: Interventional distributions are estimated by sampling from learned distributions and averaging results, which is computationally efficient compared to exact computation.
  - Quick check question: Why can we approximate p(Y|do(A=a)) by sampling X and computing p(Y|A=a,X) for each sample?

## Architecture Onboarding

- Component map: Sequencification module -> AR language model -> Inference engine -> Monte Carlo sampler

- Critical path:
  1. Define causal DAG and variable types
  2. Implement sequencification function for each variable type
  3. Generate sequencified training data
  4. Train AR model on sequencified data
  5. Implement inference functions for desired causal queries
  6. Validate on synthetic/test data

- Design tradeoffs:
  - Tokenization granularity: Fine-grained tokens capture more detail but increase sequence length and computational cost
  - Model capacity: Larger models handle more complex relationships but require more data and computation
  - Sampling strategy: More samples improve accuracy but increase computation time

- Failure signatures:
  - Poor performance on interventional queries: Indicates sequencification or AR model training issues
  - High variance in Monte Carlo estimates: Suggests insufficient samples or model uncertainty
  - Mode collapse in generated samples: Indicates AR model overfitting or training instability

- First 3 experiments:
  1. Verify sequencification preserves causal relationships on simple DAG with known ground truth
  2. Test AR model on synthetic maze data with complete interventions (Section 6.1.1)
  3. Evaluate Monte Carlo sampling accuracy on chess endgame data with partial interventions (Section 6.1.2)

## Open Questions the Paper Calls Out

The paper does not explicitly call out open questions, but identifies several limitations and assumptions that warrant further investigation.

## Limitations
- Requires complete knowledge of the underlying causal DAG, which may not be available in many real-world scenarios
- Assumes all variables are observed, limiting utility when latent confounders are present
- Computational complexity increases with the number of variables and their possible values, potentially limiting scalability

## Confidence
- High confidence: The core sequencification mechanism and its ability to convert causal DAGs into sequential token representations is well-supported by theoretical foundations and experimental validation.
- Medium confidence: The Monte Carlo sampling approach for estimating interventional distributions is sound, but performance depends heavily on the quality of the trained AR model and may degrade with high-dimensional confounders.
- Medium confidence: Pre-trained language model fine-tuning shows promise for text-based causal inference, though results may be domain-dependent and require careful hyperparameter tuning.

## Next Checks
1. **Robustness to DAG misspecification**: Systematically evaluate how errors in the assumed causal structure affect intervention estimation accuracy across all three experimental domains.

2. **Latent confounder handling**: Extend the framework to incorporate instrumental variables or proxy methods for scenarios where not all confounding variables are observed.

3. **Scalability testing**: Benchmark the method's performance and computational requirements on synthetic datasets with increasing numbers of variables and variable cardinalities to identify practical limits.