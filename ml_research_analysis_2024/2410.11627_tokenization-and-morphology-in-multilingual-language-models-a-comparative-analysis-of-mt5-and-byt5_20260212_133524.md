---
ver: rpa2
title: 'Tokenization and Morphology in Multilingual Language Models: A Comparative
  Analysis of mT5 and ByT5'
arxiv_id: '2410.11627'
source_url: https://arxiv.org/abs/2410.11627
tags:
- language
- languages
- byt5
- morphological
- probing
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper compares morphological knowledge in two multilingual
  models (mT5 and ByT5) that differ only in tokenization strategy: subword vs character-level.
  Both models are evaluated on 17 languages across 4 morphological tasks.'
---

# Tokenization and Morphology in Multilingual Language Models: A Comparative Analysis of mT5 and ByT5

## Quick Facts
- arXiv ID: 2410.11627
- Source URL: https://arxiv.org/abs/2410.11627
- Reference count: 40
- Primary result: Character-level ByT5 captures morphological knowledge comparably to subword mT5, with both showing middle-to-late layer encoding

## Executive Summary
This paper compares morphological knowledge encoding in mT5 and ByT5, two multilingual models differing only in tokenization strategy (subword vs character-level). Both models are evaluated on 17 languages across 4 morphological tasks. Results show comparable performance between models, with morphological information best encoded in middle-to-late layers rather than early layers. The study finds that number is the easiest morphological feature to learn while case is the hardest, and languages with higher morphological irregularity benefit more from larger training data proportions.

## Method Summary
The study uses a controlled comparison of mT5-base and ByT5-base, both T5-based models trained on the same data but with different tokenization strategies. Researchers extract contextualized embeddings at each layer for target words from a multilingual morphological probing dataset covering 17 languages and 43 tasks. MLP classifiers are trained on these embeddings to predict morphological features (number, tense, case, gender). Performance is measured via classification accuracy, and statistical mixed-effects logistic regression models analyze the effects of training data proportion and morphological complexity factors on accuracy.

## Key Results
- Both mT5 and ByT5 capture morphological knowledge comparably, with ByT5 requiring more layers for processing
- Morphological information is encoded in middle-to-late layers, contradicting earlier findings of early-layer encoding
- Number is the easiest morphological feature to learn; case is the hardest
- Languages with higher morphological irregularity benefit more from larger training data proportions

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Character-level tokenization (ByT5) can capture morphological knowledge on par with subword tokenization (mT5) despite needing more layers for processing.
- Mechanism: By operating at the character/byte level, ByT5 avoids segmentation artifacts that can obscure morphological boundaries. The model must reconstruct word-level structure internally, which encourages robust morphological representation even if distributed across more layers.
- Core assumption: Character-level models can recover word-level morphological patterns given sufficient depth and training data.
- Evidence anchors:
  - [abstract] "character-level ByT5 yields similar performance as mT5 when allowing more training to recover word-level structures"
  - [section] "ByT5 needs more layers for processing the input to yield results that are comparable with mT5"
  - [corpus] Weak - corpus lacks specific ByT5 vs mT5 morphology studies
- Break condition: If character-level models cannot reconstruct word-level structures efficiently, performance gaps will persist regardless of depth.

### Mechanism 2
- Claim: Morphological information is best encoded in middle to late layers of multilingual language models, not in early layers.
- Mechanism: Early layers handle basic tokenization and surface features, while middle-to-late layers integrate contextual and morphological information through deeper processing. This contradicts earlier findings that morphology is a low-level feature encoded in early layers.
- Core assumption: Deeper processing is required to integrate morphological knowledge with contextual information in multilingual settings.
- Evidence anchors:
  - [abstract] "morphological information is encoded in the middle and late layers"
  - [section] "best performance in the middle to late layers" and "This finding

## Foundational Learning

### Tokenization Strategies
- Why needed: Different tokenization approaches affect how morphological boundaries are preserved and processed
- Quick check: Verify that subword tokenization splits words into meaningful units while character-level treats each character independently

### Probing Methodology
- Why needed: Requires extracting embeddings at each layer and training classifiers to measure linguistic knowledge content
- Quick check: Confirm MLP classifiers are trained on pooled embeddings and evaluated on separate test sets

### Morphological Complexity Metrics
- Why needed: Captures language-specific challenges in learning morphological systems
- Quick check: Verify irregularity scores and type-token ratios are computed correctly for each language

## Architecture Onboarding

### Component Map
fastText embeddings -> MLP classifier -> morphological feature prediction
                   ↓
mT5/ByT5 embeddings (layer-wise) -> MLP classifier -> morphological feature prediction

### Critical Path
Embedding extraction → Layer-wise pooling → MLP classifier training → Accuracy evaluation

### Design Tradeoffs
- Tokenization: Subword (mT5) vs Character-level (ByT5) - affects boundary preservation vs processing efficiency
- Probing: MLP classifiers vs alternative methods - affects measurement sensitivity vs complexity
- Layer selection: Single layer vs layer-wise analysis - affects localization of knowledge vs computational cost

### Failure Signatures
- Accuracy below 80% suggests issues with embedding extraction or classifier training
- Uniform accuracy across layers indicates probing methodology problems
- Model-specific performance gaps suggest tokenization strategy limitations

### First Experiments
1. Extract embeddings at each layer for a small subset of tasks and verify layer-wise patterns
2. Train MLP classifiers on fastText embeddings as control baseline
3. Compare probing accuracy between models on a single language/task pair

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do character-level models like ByT5 handle rare and morphologically complex words compared to subword models like mT5 across different language families?
- Basis in paper: [explicit] The paper mentions that ByT5 is "better in handling rare and similar words" and performs better in several aspects including "low-resource languages," but does not provide detailed analysis of rare word handling across different language families.
- Why unresolved: The paper only briefly mentions these advantages without detailed quantitative analysis or cross-linguistic comparison of rare word handling.
- What evidence would resolve it: Detailed error analysis and performance metrics specifically focusing on rare and morphologically complex words across different language families for both models.

### Open Question 2
- Question: Does the interaction between morphological irregularity and training data size hold for languages beyond the 17 studied, particularly for languages with very high irregularity scores?
- Basis in paper: [explicit] The paper states that "languages with more irregularities benefit more from having a higher share of the pre-training data" but only tests this on 17 languages.
- Why unresolved: The study's sample size of 17 languages, while diverse, may not capture the full spectrum of morphological irregularity found in all languages.
- What evidence would resolve it: Probing experiments on a larger and more diverse set of languages, particularly those with extreme irregularity scores.

### Open Question 3
- Question: What specific architectural changes could improve ByT5's early layer morphological learning to match mT5's performance without increasing model size?
- Basis in paper: [inferred] The paper notes that ByT5 shows "greater improvement after the embedding layer than mT5" and needs "more layers to capture morphological patterns," suggesting architectural inefficiency.
- Why unresolved: The paper identifies the performance gap but does not explore architectural modifications to address it.
- What evidence would resolve it: Experimental results comparing modified ByT5 architectures with different layer configurations or attention mechanisms to mT5 baseline.

## Limitations
- Comparison limited to mT5 and ByT5 specifically, constraining generalizability to other tokenization strategies
- Probing methodology relies on MLP classifiers that may not capture all aspects of morphological knowledge
- Analysis focuses on four morphological features across 17 languages, potentially missing language-specific phenomena

## Confidence

- **High**: The finding that morphological information is encoded in middle-to-late layers rather than early layers - supported by clear layer-wise accuracy patterns across multiple languages and tasks
- **Medium**: The claim that ByT5 matches mT5 performance given sufficient training data - statistically supported but mechanism remains speculative
- **Medium**: The observation that morphological irregularity affects learning and requires more data - supported by correlation analyses but confounded by language family factors

## Next Checks

1. **Cross-model validation**: Test the same tokenization comparison on non-T5 architectures (e.g., multilingual BERT, XLM-R) to verify whether findings generalize beyond the T5 family
2. **Fine-grained probing**: Implement gradient-based or attention-based probing methods alongside MLP classifiers to validate that morphological information patterns are not artifacts of the classifier architecture
3. **Temporal analysis**: Track morphological learning curves during training to determine whether irregular languages indeed require more data as suggested, or whether this reflects other training dynamics