---
ver: rpa2
title: LLM Questionnaire Completion for Automatic Psychiatric Assessment
arxiv_id: '2406.06636'
source_url: https://arxiv.org/abs/2406.06636
tags:
- your
- much
- have
- depression
- questionnaires
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a novel approach to psychiatric assessment
  using Large Language Models (LLMs) to convert unstructured psychological interviews
  into structured questionnaires. The method, called LMIQ, employs an LLM to impersonate
  interviewees and complete questionnaires spanning various psychiatric and personality
  domains.
---

# LLM Questionnaire Completion for Automatic Psychiatric Assessment

## Quick Facts
- arXiv ID: 2406.06636
- Source URL: https://arxiv.org/abs/2406.06636
- Reference count: 33
- Method achieves improved diagnostic accuracy for depression and PTSD prediction from unstructured interviews

## Executive Summary
This paper introduces LMIQ, a novel approach for automatic psychiatric assessment that leverages Large Language Models to convert unstructured psychological interviews into structured questionnaire responses. The method involves having an LLM impersonate interviewees to complete standardized psychiatric questionnaires, which are then used as features for predicting depression (PHQ-8) and PTSD (PCL-C) severity scores. The approach demonstrates superior performance compared to multiple baseline methods including direct embedding techniques and TF-IDF vectorization, achieving lower Mean Squared Errors in predicting psychiatric severity scores.

## Method Summary
LMIQ employs an LLM to process unstructured interview transcripts and complete standardized psychiatric questionnaires as if it were the interviewee. The LLM impersonates the interviewee based on their interview content, generating responses to domain-specific questionnaires spanning various psychiatric and personality assessments. These generated responses are then used as structured features to train a Random Forest regressor for predicting standardized psychiatric measures including PHQ-8 depression scores and PCL-C PTSD scores. The method processes interviews by first summarizing them, then using the summary to generate questionnaire completions across multiple psychiatric domains.

## Key Results
- LMIQ achieves MSE of 20.42 for depression prediction on test set
- LMIQ achieves MSE of 192.93 for PTSD prediction on test set
- Outperforms baselines including direct embedding methods, TF-IDF, and naive guessing approaches

## Why This Works (Mechanism)
The approach works by bridging the gap between unstructured interview data and structured psychiatric assessments. By having an LLM impersonate interviewees and complete standardized questionnaires, it converts qualitative interview content into quantitative features that can be used for prediction. The LLM's ability to process and synthesize interview information allows it to generate contextually appropriate responses across multiple psychiatric domains, creating a rich feature set that captures various aspects of mental health status.

## Foundational Learning
- **LLM-based questionnaire completion**: Why needed - to transform unstructured interviews into structured data; Quick check - verify generated responses align with interview content
- **Random Forest regression**: Why needed - to predict continuous psychiatric severity scores from structured features; Quick check - validate feature importance aligns with psychiatric symptoms
- **Psychiatric questionnaire domains**: Why needed - to capture comprehensive mental health assessment; Quick check - ensure coverage of relevant symptom domains

## Architecture Onboarding

**Component Map**: Interview transcript -> LLM summarization -> LLM questionnaire completion -> Structured features -> Random Forest regression

**Critical Path**: The most critical components are the LLM's ability to accurately impersonate interviewees and generate contextually appropriate questionnaire responses. The quality of these generated responses directly impacts the predictive performance of the downstream Random Forest model.

**Design Tradeoffs**: The method trades off direct interpretation of interview content for structured questionnaire responses. While this introduces an additional processing step, it enables the use of standardized psychiatric assessment frameworks and potentially improves prediction accuracy.

**Failure Signatures**: Poor LLM impersonations would result in questionnaire responses that don't align with actual interview content. This would manifest as low correlation between generated responses and actual symptom severity scores, or poor performance on test data.

**First Experiments**:
1. Validate that LLM-generated questionnaire responses correlate with actual interview content
2. Test different LLM models and prompting strategies for questionnaire completion
3. Compare feature importance distributions with known psychiatric symptom hierarchies

## Open Questions the Paper Calls Out
None specified in the provided content.

## Limitations
- Evaluation limited to single dataset (PsyProfile), limiting generalizability
- Assumes LLM-generated responses accurately represent genuine patient experiences
- Substantial MSE values indicate room for improvement in prediction accuracy

## Confidence

**High confidence**: The core methodology of using LLMs to convert unstructured interviews into structured questionnaire responses is technically sound and well-implemented.

**Medium confidence**: The improved performance over baseline methods is well-demonstrated, though the clinical significance of these improvements requires further validation.

**Medium confidence**: The interpretability findings align with expected symptom-question relationships, but the depth of this alignment needs more thorough examination.

## Next Checks
1. Cross-dataset validation: Test LMIQ on multiple independent psychiatric interview datasets to assess generalizability across different clinical contexts and populations.

2. Human expert comparison: Conduct a comparative study where psychiatric experts evaluate both LLM-generated questionnaire completions and actual patient responses to assess the authenticity and clinical utility of the generated data.

3. Clinical outcome correlation: Evaluate whether LMIQ's predictions correlate with actual clinical outcomes and treatment responses, rather than just standardized questionnaire scores.