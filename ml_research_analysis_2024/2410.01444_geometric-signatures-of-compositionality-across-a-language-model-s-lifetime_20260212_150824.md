---
ver: rpa2
title: Geometric Signatures of Compositionality Across a Language Model's Lifetime
arxiv_id: '2410.01444'
source_url: https://arxiv.org/abs/2410.01444
tags:
- complexity
- linear
- figure
- shuffled
- length
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigates whether language models (LMs) encode the
  inherent simplicity of language, enabled by compositionality, in their internal
  representations. The authors take a geometric approach, relating the degree of compositionality
  in a dataset to the intrinsic dimension (Id) of its representations under an LM,
  a measure of feature complexity.
---

# Geometric Signatures of Compositionality Across a Language Model's Lifetime

## Quick Facts
- arXiv ID: 2410.01444
- Source URL: https://arxiv.org/abs/2410.01444
- Reference count: 40
- This study finds that nonlinear intrinsic dimension (Id) tracks semantic compositionality while linear dimensionality (d) tracks superficial input complexity in language model representations.

## Executive Summary
This paper investigates whether language models encode the inherent simplicity of language enabled by compositionality in their internal representations. The authors take a geometric approach, relating the degree of compositionality in a dataset to the intrinsic dimension (Id) of its representations under an LM, a measure of feature complexity. They design controlled datasets with tunable compositionality and analyze representations across model sizes and training stages. The key finding is that nonlinear Id reflects meaningful compositional complexity, while linear dimensionality encodes superficial input complexity. This relationship arises over training, with Id encoding superficial complexity as an inductive bias of the LM's architecture, but meaningful compositional complexity by the end of training.

## Method Summary
The study uses controlled synthetic datasets generated from a grammar with tunable compositionality (varying coupling factor k), and naturalistic datasets like The Pile. Pre-trained causal language models (Pythia, Llama, Mistral) are used to extract last-token representations across layers. The authors compute TwoNN-based intrinsic dimension (Id) and PCA-based linear dimensionality (d) for each layer, dataset, and model. Kolmogorov complexity is estimated via gzip compression to quantify dataset compositionality. The analysis tracks how these metrics evolve over model size and training stages, with particular focus on the relationship between representational geometry and linguistic competence.

## Key Results
- Nonlinear intrinsic dimension (Id) tracks semantic compositionality while linear dimensionality (d) tracks superficial input complexity
- Shuffling destroys phrase-level semantics and collapses Id but increases d, demonstrating the form-meaning dichotomy
- Over training, LMs shift from encoding superficial complexity as an inductive bias to encoding compositional meaning through learned features
- Id and d scale differently with model size, with Id showing less growth than d

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Nonlinear intrinsic dimension (Id) tracks semantic compositionality while linear dimensionality (d) tracks superficial input complexity.
- Mechanism: Id captures the number of latent degrees of freedom needed to describe the data manifold, encoding the true compositional structure. d measures the span of the linear subspace covering the data, reflecting surface-level variability in wordforms and unigram frequencies.
- Core assumption: The LM's representation space can be decomposed into a low-dimensional nonlinear manifold (semantic) embedded in a higher-dimensional linear subspace (surface).
- Evidence anchors: [abstract] "nonlinear Id and linear dimensionality scale differently with model size"; [section 4.3] "shuffling destroys phrase-level semantics and Id, also attested for naturalistic sentences"; [corpus] Weak empirical support for this dichotomy.

### Mechanism 2
- Claim: Over training, LMs shift from encoding superficial complexity as an inductive bias to encoding compositional meaning through learned features.
- Mechanism: Initially, the architecture biases representations to preserve input complexity (high d). As training proceeds, the model learns to extract compositional semantics, collapsing Id in shuffled inputs while maintaining high d for both coherent and shuffled.
- Core assumption: The training objective (next-token prediction) rewards extraction of latent compositional structure over mere surface preservation.
- Evidence anchors: [section 4.3] "The relationship between dimensionality and superficial complexity... differs at the start and end"; [section 4.2] "Id decreases sharply before checkpoint 103 and then redistributes"; [corpus] Weak empirical support for training-phase shift.

### Mechanism 3
- Claim: Dataset compositionality, quantified via Kolmogorov complexity (KC), is reflected in representational dimensionality, but in opposite directions for Id and d.
- Mechanism: Lower k (more compositional) → higher KC → higher d (surface complexity encoded). Lower k → fewer degrees of freedom → lower Id (semantic complexity encoded). Shuffling increases KC but destroys semantics, so d increases while Id collapses.
- Core assumption: KC is a valid proxy for superficial complexity; gzip compression approximates KC.
- Evidence anchors: [section 4.3] "We quantify compositionality of a dataset by its Kolmogorov complexity"; [section 5] "Table 1 shows Spearman correlations ρ between KC and dimensionality"; [corpus] Weak empirical support for KC as compositional measure.

## Foundational Learning

- Concept: Manifold hypothesis and intrinsic dimension.
  - Why needed here: The paper's core claim is that LMs represent language on low-dimensional nonlinear manifolds whose Id encodes compositional structure.
  - Quick check question: What is the difference between intrinsic dimension and ambient (embedding) dimension?

- Concept: Kolmogorov complexity and its approximation via gzip.
  - Why needed here: The study uses KC to quantify dataset compositionality; understanding how gzip approximates KC is crucial for interpreting the correlation results.
  - Quick check question: Why does shuffling increase KC but decrease Id?

- Concept: Linear vs. nonlinear dimensionality estimation.
  - Why needed here: The paper contrasts PCA (linear) and TwoNN/MLE (nonlinear) to show they encode different aspects of data complexity.
  - Quick check question: What is the mathematical relationship between Id, d, and D (embedding dimension)?

## Architecture Onboarding

- Component map: Controlled grammar generator -> Pre-trained LMs (Pythia, Llama, Mistral) -> Dimensionality estimators (TwoNN, PCA) -> KC estimator (gzip) -> Benchmark tasks

- Critical path: 1) Generate controlled datasets with varying k 2) Extract last-token representations from each LM layer 3) Compute TwoNN Id and PCA d for each layer/dataset 4) Compute gzip-based KC for each dataset 5) Correlate Id and d to KC and task performance over layers and training checkpoints

- Design tradeoffs: Using synthetic grammar vs. natural corpora offers precise control over compositionality but may miss real-world linguistic patterns; TwoNN vs. other Id estimators is robust to local uniformity assumptions but may be less accurate for very high ambient dimensions; PCA variance threshold (99%) ensures full subspace coverage but may include noise.

- Failure signatures: Id ≈ d ≈ D suggests no low-dimensional manifold; no correlation between Id and task performance suggests Id does not track semantics; shuffling does not change Id suggests Id does not encode semantics.

- First 3 experiments: 1) Generate controlled grammar datasets with k ∈ {1,2,3,4}, extract representations from a pre-trained LM, compute Id and d per layer, verify Id < d < D 2) Shuffle datasets, recompute Id and d, verify Id collapses but d increases 3) Correlate layerwise Id and d to gzip-based KC, verify d correlates strongly, Id does not

## Open Questions the Paper Calls Out

- Question: Does the observed contrast between linear and nonlinear feature complexity extend to larger language models (beyond 8B parameters) and more complex linguistic structures (e.g., recursive embeddings)?
- Question: What specific learned semantic features cause shuffling feature collapse in nonlinear Id, and how do they differ from features captured by linear dimensionality?
- Question: How does the relationship between dataset compositionality and feature complexity evolve during fine-tuning on downstream tasks, and does it depend on the compositional nature of the target task?

## Limitations

- The study's controlled experiments use synthetic grammars with fixed syntactic structures, limiting generalizability to naturalistic language
- The assumption that gzip-based Kolmogorov complexity accurately proxies superficial complexity is reasonable but not rigorously validated for the specific controlled datasets
- The phase transition in training is presented as evidence of emerging compositional competence, but the causal relationship between Id changes and linguistic performance gains remains correlational

## Confidence

- High Confidence: The empirical observation that Id and d scale differently with model size and encode different aspects of data complexity
- Medium Confidence: The claim that the relationship between d and superficial complexity is an architectural inductive bias that persists throughout training, while Id's relationship with compositional complexity emerges during training
- Low Confidence: The assertion that the phase transition in Id at checkpoint 103 directly causes improvements in linguistic competence

## Next Checks

1. **Cross-linguistic validation**: Test whether the Id vs. d dichotomy holds across multiple natural languages with different morphological and syntactic properties.

2. **Controlled ablation of compositional structure**: Design datasets where compositional structure is manipulated independently of surface-level features (e.g., synonym substitution that preserves meaning but changes surface statistics).

3. **Intervention experiment on training**: Implement targeted training interventions that explicitly encourage compositional generalization (e.g., meta-learning objectives or compositional data augmentation).