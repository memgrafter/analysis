---
ver: rpa2
title: 'Repurformer: Transformers for Repurposing-Aware Molecule Generation'
arxiv_id: '2407.11439'
source_url: https://arxiv.org/abs/2407.11439
tags:
- compounds
- repurformer
- generated
- compound
- generation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The study introduces Repurformer, a novel method to tackle the
  sample bias problem in de novo molecule generation by leveraging multi-hop relationships
  among proteins and compounds. The core idea involves integrating bi-directional
  pretraining with Fast Fourier Transform (FFT) and low-pass filtering (LPF) to capture
  complex interactions and generate diverse molecules.
---

# Repurformer: Transformers for Repurposing-Aware Molecule Generation

## Quick Facts
- **arXiv ID**: 2407.11439
- **Source URL**: https://arxiv.org/abs/2407.11439
- **Reference count**: 24
- **Primary result**: Repurformer with α=4 achieved BLEU score of 0.662 and QED score of 0.598

## Executive Summary
This paper introduces Repurformer, a novel method for de novo molecule generation that addresses sample bias by leveraging multi-hop relationships among proteins and compounds. The core innovation involves integrating bi-directional pretraining with Fast Fourier Transform (FFT) and low-pass filtering (LPF) to capture complex interactions in the protein-compound network. The model successfully generates diverse molecules that serve as substitutes for anchor compounds while maintaining structural similarity to positive compounds. Experimental results on the BindingDB dataset demonstrate that Repurformer can create drug-like molecules with desired properties by focusing on low-frequency components that represent longer propagation through multi-hop relationships.

## Method Summary
Repurformer uses a bi-directional pretraining approach with two identical encoder-decoder Transformer structures trained in opposite directions (protein-to-compound and compound-to-protein). After pretraining, the protein and compound encoders are frozen, and their latent representations are summed and transformed to the frequency domain using FFT. Low-pass filtering with cutoff parameter α is applied to isolate low-frequency components representing multi-hop relationships, followed by inverse FFT transformation. A compound decoder then generates SMILES strings from the filtered latent vectors. The model is trained on the BindingDB dataset with 60,719 protein-compound pairs, using reconstruction accuracy of positive compounds as the objective function.

## Key Results
- Repurformer with α=4 achieved BLEU score of 0.662 and QED score of 0.598
- Generated compounds show high structural similarity to positive compounds while increasing diversity from anchor compounds
- The model successfully creates drug-like molecules that satisfy physicochemical property constraints (molecular weights and LogP)
- Performance demonstrates effective leverage of multi-hop relationships for repurposing-aware generation

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Bi-directional pretraining of protein and compound encoders captures both protein-to-compound and compound-to-protein relationships, enabling the model to leverage multi-hop repurposing flows.
- Mechanism: The model uses two identical encoder-decoder structures trained in opposite directions (protein-to-compound and compound-to-protein). This allows both encoders to extract latent representations that encompass edges where proteins and compounds can serve as either head or tail of an interaction, enabling the model to capture indirect multi-hop relationships.
- Core assumption: The latent representations learned from both directions contain complementary information about the protein-compound interaction network that can be combined to represent multi-hop relationships.
- Evidence anchors:
  - [abstract] "Our model, Repurformer, integrates bi-directional pretraining with Fast Fourier Transform (FFT) and low-pass filtering (LPF) to capture complex interactions and generate diverse molecules."
  - [section] "Specifically, we built two Transformers with identical encoder-decoder structures but opposite training directions: one was trained in the protein-to-compound direction, and the other in the compound-to-protein direction"
  - [corpus] Weak evidence. The corpus neighbors don't discuss bi-directional pretraining specifically, though some mention related concepts like "Peptide2Mol: A Diffusion Model for Generating Small Molecules as Peptide Mimics" which involves bidirectional considerations in a different context.
- Break condition: If the latent representations from the two directions are not complementary or if the combination method (simple addition) doesn't effectively merge the information.

### Mechanism 2
- Claim: Low-pass filtering in the frequency domain isolates low-frequency components that represent longer propagation through multi-hop relations, enabling generation of structurally diverse compounds.
- Mechanism: After applying FFT to the latent space, low-pass filtering removes high-frequency components (representing local, single-hop interactions) while preserving low-frequency components (representing global, multi-hop interactions). This allows the model to focus on the longer propagation paths that enable repurposing.
- Core assumption: Low-frequency components in the latent space correspond to multi-hop propagation paths, while high-frequency components correspond to single-hop relationships.
- Evidence anchors:
  - [abstract] "This approach allows the model to distinguish the different scales of interactions. By focusing on low-frequency components, which correspond to the longer propagation through the multi-hop protein-compound interaction network, Repurformer generates as diverse compounds as possible with desired properties."
  - [section] "In our setting, a scale can be understood as the number of hops. Specifically, the lower frequency implies a longer propagation through multi-hop relations while the higher one implies a shorter propagation within a single-hop relation."
  - [corpus] Missing evidence. The corpus doesn't contain direct discussion of low-pass filtering in molecular generation contexts.
- Break condition: If the frequency-scale relationship doesn't hold in the latent space, or if important information is lost when high-frequency components are removed.

### Mechanism 3
- Claim: The combination of FFT and IFFT with LPF enables effective signal processing in the latent space without losing the ability to decode back to molecular structures.
- Mechanism: The model transforms latent vectors to frequency domain using FFT, applies LPF to isolate relevant frequency components, then transforms back using IFFT before passing to the decoder. This preserves the ability to generate valid SMILES strings while focusing on the desired multi-hop relationships.
- Core assumption: The latent space transformation via FFT/IFFT is invertible and preserves enough information for the decoder to generate valid molecular structures.
- Evidence anchors:
  - [section] "Lastly, we transformed HLPF back to the features of an original domain using the inverse FFT (IFFT), before passing it to the compound decoder"
  - [section] "The structure of the Repurformer is essentially identical to that of pretrained transformers. It consists of encoder and decoder networks, each linearly stacked with 4 layers of 256 dimensions"
  - [corpus] Weak evidence. The corpus neighbors mention related signal processing concepts but not specifically FFT/IFFT in latent spaces for molecular generation.
- Break condition: If the IFFT transformation doesn't adequately preserve information needed for molecular generation, leading to invalid or nonsensical outputs.

## Foundational Learning

- Concept: Fourier Transform and frequency domain analysis
  - Why needed here: Understanding how FFT decomposes signals into frequency components is crucial for grasping why low-pass filtering can isolate multi-hop relationships in the latent space.
  - Quick check question: What is the difference between low-frequency and high-frequency components in signal processing, and how does this relate to local vs. global structure in data?

- Concept: Transformer architecture and attention mechanisms
  - Why needed here: The bi-directional pretraining uses Transformer encoders, and understanding self-attention and cross-attention is essential for understanding how the model captures relationships between proteins and compounds.
  - Quick check question: How does the multi-head attention mechanism in Transformers allow for capturing different types of relationships in the input data?

- Concept: Molecular representation and SMILES strings
  - Why needed here: The model generates compounds as SMILES strings, so understanding this molecular representation format is essential for interpreting results and understanding the generation task.
  - Quick check question: What are the advantages and limitations of using SMILES strings as a molecular representation for generative models?

## Architecture Onboarding

- Component map: Protein input -> Protein encoder -> Latent vector -> FFT -> LPF -> IFFT -> Compound decoder -> Generated SMILES; Compound input -> Compound encoder -> Latent vector -> FFT -> LPF -> IFFT -> Compound decoder -> Generated SMILES

- Critical path: Protein/Compound input → Encoder(s) → Latent vector sum → FFT → LPF → IFFT → Decoder → Generated SMILES

- Design tradeoffs:
  - α parameter: Higher values preserve more frequency components (more diversity, less validity) while lower values preserve fewer (more validity, less diversity)
  - Bi-directional vs. uni-directional pretraining: Bi-directional captures more complete relationship information but requires more training resources
  - Frequency vs. spatial domain processing: Frequency domain allows scale-based filtering but requires invertible transformations

- Failure signatures:
  - Invalid SMILES strings: Likely indicates information loss during FFT/IFFT transformation or inappropriate α value
  - Mode collapse (low diversity): May indicate α is too low, removing too much information
  - Poor binding affinity: May indicate LPF is removing important structural features needed for protein binding

- First 3 experiments:
  1. Test the effect of different α values on validity vs. uniqueness tradeoff by generating compounds and measuring the ratio of valid SMILES strings and the diversity of generated structures.
  2. Compare bi-directional pretraining vs. uni-directional pretraining by training two versions and measuring their ability to generate diverse compounds for the same protein target.
  3. Evaluate the impact of FFT/IFFT transformation by comparing a version with and without these steps, measuring both reconstruction accuracy and generation diversity.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal cutoff parameter α for the low-pass filter to balance validity and uniqueness in Repurformer?
- Basis in paper: [explicit] The paper demonstrates a negative relationship between validity and uniqueness as α increases, indicating a trade-off.
- Why unresolved: The paper does not provide a definitive optimal value for α, suggesting that further experimentation is needed to find the best balance.
- What evidence would resolve it: Conducting additional experiments with varying α values and analyzing their impact on both validity and uniqueness could help identify the optimal cutoff parameter.

### Open Question 2
- Question: How does Repurformer perform on real-world drug repurposing cases compared to synthetic data?
- Basis in paper: [inferred] The paper mentions the need to validate Repurformer's applicability in real-world scenarios but does not provide specific case studies or results.
- Why unresolved: The paper focuses on synthetic data from BindingDB and lacks real-world application examples.
- What evidence would resolve it: Testing Repurformer on actual drug repurposing cases and comparing its performance to existing methods would provide insights into its practical utility.

### Open Question 3
- Question: How does the introduction of advanced models like diffusion or graph neural networks impact Repurformer's performance?
- Basis in paper: [explicit] The paper suggests that incorporating advanced models like diffusion or graph neural networks could further improve Repurformer's ability to capture multi-hop protein-compound interactions.
- Why unresolved: The paper does not explore these advanced models, leaving their potential impact on performance untested.
- What evidence would resolve it: Implementing Repurformer with these advanced models and evaluating their impact on performance metrics would clarify their effectiveness.

## Limitations

- The fundamental assumption that low-frequency components correspond to multi-hop propagation paths needs more rigorous validation
- The choice of α parameter and its impact on the validity-diversity tradeoff is not fully explored
- Limited empirical validation of the frequency-scale relationship in protein-compound interaction networks

## Confidence

- Mechanism 1 (Bi-directional pretraining captures multi-hop relationships): Medium confidence
- Mechanism 2 (LPF isolates multi-hop relationships): Low confidence
- Mechanism 3 (FFT/IFFT preserves generation capability): Medium confidence

## Next Checks

1. **Frequency-scale relationship validation**: Systematically vary α values and measure how well different frequency components correlate with single-hop vs. multi-hop relationships in the protein-compound interaction network, using network analysis metrics to validate the assumption.

2. **Information preservation analysis**: Compare generated molecules from the full Repurformer pipeline against a baseline that skips FFT/IFFT steps, measuring not just validity rates but also structural similarity to training compounds and binding affinity predictions.

3. **Encoder complementarity test**: Train separate models using only protein-to-compound or only compound-to-protein pretraining, then compare their performance against the bi-directional model to quantify the actual benefit of capturing both directions.