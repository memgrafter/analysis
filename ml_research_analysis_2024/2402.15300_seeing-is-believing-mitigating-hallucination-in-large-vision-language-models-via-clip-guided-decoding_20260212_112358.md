---
ver: rpa2
title: 'Seeing is Believing: Mitigating Hallucination in Large Vision-Language Models
  via CLIP-Guided Decoding'
arxiv_id: '2402.15300'
source_url: https://arxiv.org/abs/2402.15300
tags:
- hallucination
- sentence
- arxiv
- image
- coco
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses object hallucination in large vision-language
  models (LVLMs), where generated text contains non-existent objects. The authors
  find that CLIP similarity to the image is a stronger indicator of hallucination
  than token likelihoods, particularly in later sentences.
---

# Seeing is Believing: Mitigating Hallucination in Large Vision-Language Models via CLIP-Guided Decoding

## Quick Facts
- arXiv ID: 2402.15300
- Source URL: https://arxiv.org/abs/2402.15300
- Authors: Ailin Deng; Zhirui Chen; Bryan Hooi
- Reference count: 40
- Key outcome: CLIP-Guided Decoding (CGD) effectively mitigates object hallucination in LVLMs while preserving text generation quality, outperforming baseline methods on CHAIR metrics

## Executive Summary
This paper addresses the problem of object hallucination in large vision-language models (LVLMs), where generated text contains non-existent objects not present in the input image. The authors propose CLIP-Guided Decoding (CGD), a training-free approach that uses CLIP to guide the decoding process by enhancing visual grounding of generated text with the image. Experiments show CGD effectively mitigates object hallucination across multiple LVLM families while preserving text generation quality, outperforming baselines like greedy decoding, nucleus sampling, and other hallucination mitigation methods.

## Method Summary
The method, CLIP-Guided Decoding (CGD), works by generating multiple candidate sentences using the LVLM, computing CLIP scores between each candidate and the image, and combining the LVLM likelihood with CLIP scores to select the best candidate. The algorithm operates at the sentence level rather than token level, allowing full-sentence CLIPScore computation. The final score for each candidate combines the LVLM's log-likelihood with the CLIP score weighted by a hyperparameter α. This approach leverages CLIP's strong visual grounding capabilities to guide the LVLM towards more accurate, hallucination-free generation.

## Key Results
- CGD outperforms baseline decoding methods (greedy, nucleus sampling, TopK sampling) on hallucination evaluation metrics (CHAIRi and CHAIRs)
- CGD maintains generation quality as measured by BLEU, METEOR, ROUGE-L, CIDEr, SPICE, and CLIPScore
- CLIP similarity to the image is a stronger indicator of hallucination than token likelihoods, particularly in later sentences
- CGD improves hallucination mitigation even when reusing the same CLIP models already used in the LVLMs

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** CLIPScore is a stronger and more robust indicator of hallucination than token likelihoods, especially in later sentences.
- **Mechanism:** CLIPScore directly measures the alignment between generated text and the image by comparing image and text embeddings from a vision-language model. This external comparison bypasses the internal biases and overconfidence present in the LVLM's token likelihood distributions.
- **Core assumption:** The CLIP model's embedding space captures meaningful semantic similarity between visual content and descriptive text, allowing it to detect mismatches indicative of hallucination.
- **Evidence anchors:**
  - [abstract] "finding that CLIP similarity to the image acts as a stronger and more robust indicator of hallucination compared to token likelihoods"
  - [section] "we define two metrics: hallucination ratio R(·) and first-time hallucination ratio Rfirst(·)...we use AUROC metric...As depicted in Figure 3, the performance of sentence likelihood diminishes in later sentences and exhibits generally weak predictive capabilities in hallucination detection."
  - [corpus] Weak evidence. Most related papers focus on hallucination detection but don't directly compare CLIPScore vs token likelihoods as indicators.

### Mechanism 2
- **Claim:** Later sentences in LVLM-generated captions are more prone to hallucination.
- **Mechanism:** As generation proceeds, LVLMs may pay diminishing attention to the visual input and increasingly rely on language priors, leading to hallucination. This bias is not solely due to error propagation but is inherent in the generation process.
- **Core assumption:** LVLMs have a tendency to shift focus from visual grounding to language modeling as they generate longer sequences.
- **Evidence anchors:**
  - [section] "Figure 2a shows that the sentences generated in the later part are more prone to hallucination, with surprisingly consistent increasing pattern across multiple LVLMs...we focus on first-time hallucination ratios Rfirst(·)...the bias remains evident...This suggests that positional bias is not exclusively a result of error propagation."
  - [corpus] No direct evidence found in corpus. This appears to be an original observation from the paper.

### Mechanism 3
- **Claim:** CLIP-guided decoding improves hallucination mitigation even when reusing the CLIP models already used in the LVLMs.
- **Mechanism:** During decoding, CLIP scores are used to guide sentence selection towards responses that are better visually grounded, even if the CLIP model is the same one used in the LVLM. This suggests the fine-tuning process in LVLMs may compromise the original vision capabilities of the CLIP encoder.
- **Core assumption:** The fine-tuning process for LVLM alignment can degrade the original visual understanding capabilities of the CLIP vision encoder.
- **Evidence anchors:**
  - [abstract] "Interestingly, we observe improvement even when reusing the CLIP models utilized in the LVLMs, indicating the potential overfitting during the fine-tuning phrase in existing LVLMs."
  - [section] "Table 4 shows that our method still surpasses the baseline method even when using this vision encoder. This finding suggests that the existing fine-tuning processes for vision-language alignment in LVLMs might, to some extent, compromise the original vision capabilities."
  - [corpus] Weak evidence. While some related work explores vision-language alignment, there's no direct evidence in the corpus about fine-tuning degrading CLIP's vision capabilities.

## Foundational Learning

- **Concept:** Vision-Language Models (VLMs) and their training objectives
  - **Why needed here:** Understanding how VLMs like CLIP are trained on image-text pairs is crucial to grasp why their embeddings can measure caption-image alignment and detect hallucinations.
  - **Quick check question:** What is the primary training objective for models like CLIP, and how does it differ from the training objectives of LVLMs?

- **Concept:** Likelihood-based decoding strategies (Greedy, Nucleus, Top-k sampling)
  - **Why needed here:** The paper compares CGD to these common decoding strategies, so understanding how they work and their limitations is important.
  - **Quick check question:** How does nucleus sampling differ from top-k sampling, and what is the purpose of the "temperature" parameter in these methods?

- **Concept:** Automatic evaluation metrics for image captioning (BLEU, METEOR, ROUGE-L, CIDEr, SPICE, CLIPScore)
  - **Why needed here:** The paper uses these metrics to evaluate the quality of generated captions, so understanding what each metric measures is important for interpreting the results.
  - **Quick check question:** Which metric specifically measures the semantic propositional content of a caption, and how does it differ from n-gram based metrics like BLEU?

## Architecture Onboarding

- **Component map:** Image and text prompt -> LVLM generates candidate sentences -> CLIP model computes CLIPScore for each candidate -> CGD algorithm combines LVLM likelihood and CLIPScore -> Hallucination-mitigated caption

- **Critical path:**
  1. Generate multiple candidate next sentences using the LVLM
  2. Compute CLIPScore for each candidate sentence
  3. Combine LVLM likelihood and CLIPScore to get a final score
  4. Select the top-scoring candidate sentences
  5. Repeat until end-of-sequence

- **Design tradeoffs:**
  - Using CLIPScore adds computational overhead but improves hallucination detection
  - Combining LVLM likelihood and CLIPScore allows balancing between generation quality and visual grounding
  - Decoding at the sentence level (vs token level) enables the use of full-sentence CLIPScore but may be less granular

- **Failure signatures:**
  - If CLIPScore doesn't correlate well with human judgment of caption quality, CGD may select poor captions
  - If the LVLM likelihood component dominates the score, CGD may not effectively mitigate hallucination
  - If the candidate generation or selection process is flawed, CGD may not explore the space of good captions effectively

- **First 3 experiments:**
  1. Run CGD with α=0 (only LVLM likelihood) and compare to baseline decoding methods to verify the importance of CLIP guidance
  2. Run CGD with α=1 (only CLIPScore) and compare to baseline decoding methods to verify the importance of LVLM likelihood
  3. Run CGD with different values of N (max candidates) and M (sampling times) to find the best tradeoff between performance and efficiency

## Open Questions the Paper Calls Out
None explicitly called out in the provided text.

## Limitations
- CGD may underperform in tasks requiring complex relational understanding or spatial awareness
- The method is sensitive to the choice of CLIP guiding model
- Computational overhead is added due to CLIP score computation for multiple candidates

## Confidence

- **High Confidence:** The paper's main claims about the effectiveness of CGD in reducing hallucination on standard image captioning datasets (COCO, NoCaps) are well-supported by experimental results.
- **Medium Confidence:** The claim that LVLMs exhibit positional bias towards hallucination in later sentences is supported by the experimental results, but the underlying reasons for this bias are not fully explored.
- **Low Confidence:** The paper's claims about the generality of CGD across different LVLMs and tasks are not fully substantiated.

## Next Checks

1. **Ablation Study on Hyperparameters:** Conduct a more extensive ablation study on the hyperparameters N, M, and α to determine their impact on performance and identify optimal settings for different LVLMs and datasets.

2. **Runtime Analysis:** Measure and report the runtime overhead introduced by CGD compared to baseline decoding methods. Analyze the tradeoff between hallucination mitigation and computational efficiency.

3. **Generalization to Other Tasks and Domains:** Evaluate the effectiveness of CGD on a wider range of tasks beyond image captioning, such as video captioning, visual question answering, and cross-modal retrieval. Test the method on diverse datasets and domains to assess its generalizability.