---
ver: rpa2
title: Large Pre-Training Datasets Don't Always Guarantee Robustness after Fine-Tuning
arxiv_id: '2410.21582'
source_url: https://arxiv.org/abs/2410.21582
tags:
- objnet
- in-a
- in-r
- in-v2
- pretrained
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates how fine-tuning affects the robustness
  of pretrained models, revealing that models pretrained on larger, more diverse datasets
  experience greater robustness degradation when fine-tuned on smaller datasets. The
  authors introduce the ImageNet-RIB benchmark to systematically evaluate robustness
  preservation across multiple downstream and evaluation tasks.
---

# Large Pre-Training Datasets Don't Always Guarantee Robustness after Fine-Tuning

## Quick Facts
- arXiv ID: 2410.21582
- Source URL: https://arxiv.org/abs/2410.21582
- Reference count: 40
- Key outcome: Larger pretraining datasets can lead to greater robustness degradation when fine-tuned on smaller datasets, challenging the assumption that bigger pretraining always yields better downstream performance.

## Executive Summary
This paper challenges the conventional wisdom that larger pretraining datasets automatically lead to better downstream performance. Through systematic experiments using the newly introduced ImageNet-RIB benchmark, the authors demonstrate that models pretrained on large-scale datasets (like LAION-2B) experience significant catastrophic forgetting and loss of out-of-distribution (OOD) generalization when fine-tuned on small datasets. Surprisingly, these large-pretrained models can end up with worse robustness than models pretrained on smaller datasets. The study reveals that fine-tuning dataset size is the primary determinant of robustness collapse, and while continual learning methods help, they cannot fully compensate for the forgetting problem in large-pretrained models.

## Method Summary
The study introduces the ImageNet-RIB benchmark, which uses 7 OOD datasets derived from ImageNet to evaluate robustness preservation during fine-tuning. The authors fine-tune various pretrained models (ViT and ResNet architectures) on one OOD dataset and evaluate on the remaining 6, using Robustness Improvement (RI) as the primary metric. They compare vanilla fine-tuning with continual learning methods (EWC, LwF), robust fine-tuning methods (LP-FT, WiSE-FT), and combinations (model soup). Models are fine-tuned for 10 epochs with SGD (lr=0.001, momentum=0.9, batch size 64), and CKA analysis is used to examine representational shifts between pretrained and fine-tuned models.

## Key Results
- Models pretrained on large datasets (LAION-2B) show greater robustness degradation after fine-tuning on small datasets compared to models pretrained on smaller datasets
- Fine-tuning dataset size is the primary determinant of robustness collapse in large-pretrained models
- Larger pretraining datasets lead to more severe representational shifts during fine-tuning, particularly in later transformer layers
- Regularization-based continual learning methods (EWC, LwF) and model soup combinations improve robustness preservation but cannot fully compensate for forgetting

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Larger pretraining datasets provide higher initial robustness but more room for catastrophic forgetting when fine-tuned on small datasets.
- Mechanism: Models pretrained on larger, more diverse datasets develop higher baseline OOD generalization performance. When fine-tuned on small datasets, they experience larger absolute robustness losses due to the greater capacity for forgetting.
- Core assumption: The initial robustness level of the pretrained model correlates with the magnitude of robustness degradation during fine-tuning.
- Evidence anchors:
  - [abstract]: "models pretrained on large datasets exhibited strong catastrophic forgetting and loss of OOD generalization"
  - [section]: "models pretrained on larger and more diverse datasets can have worse robustness after fine-tuning than models pretrained on smaller datasets"
  - [corpus]: Weak - no direct corpus evidence for this specific mechanism

### Mechanism 2
- Claim: Larger pretraining datasets lead to more severe representational shifts during fine-tuning, particularly in later transformer layers.
- Mechanism: Centered Kernel Alignment (CKA) analysis shows that models pretrained on LAION-2B and OpenAI datasets exhibit larger representational changes during fine-tuning, especially in the sixth transformer's mlp.fc2 layer, contributing to robustness degradation.
- Core assumption: Representational similarity between pretrained and fine-tuned models correlates with OOD robustness preservation.
- Evidence anchors:
  - [section]: "models pretrained on LAION-2B and OpenAI's internal dataset show especially large discrepancies...particularly in earlier layers"
  - [section]: "models pretrained on LAION and OpenAI datasets generally exhibit lower CKA values"
  - [corpus]: Missing - no corpus evidence for this specific mechanism

### Mechanism 3
- Claim: Fine-tuning dataset size is the primary determinant of robustness collapse in large-pretrained models.
- Mechanism: Experiments show that CLIP models pretrained on large datasets (LAION-2B, OpenAI) suffer severe OOD robustness degradation when fine-tuned on small subsets of ImageNet-1K, while models pretrained on smaller datasets (LAION-100M, CC-12M) maintain robustness.
- Core assumption: Insufficient fine-tuning data exacerbates catastrophic forgetting in models pretrained on large datasets.
- Evidence anchors:
  - [section]: "Fine-Tuning Dataset Size is a Major Determinant" and "both ImageNet-1K validation accuracy and OOD performance degrade significantly when fine-tuned on smaller subsets"
  - [section]: "Fine-tuning on small dataset leads to severe accuracy degradation both in- and out-of distribution"
  - [corpus]: Missing - no corpus evidence for this specific mechanism

## Foundational Learning

- Concept: Catastrophic forgetting in continual learning
  - Why needed here: The paper shows that fine-tuning causes catastrophic forgetting, where models lose previously learned knowledge, particularly affecting OOD robustness.
  - Quick check question: What is the primary difference between catastrophic forgetting and gradual forgetting in neural networks?

- Concept: Out-of-distribution (OOD) generalization
  - Why needed here: The paper's core focus is on how fine-tuning affects a model's ability to generalize to data distributions different from both pretraining and fine-tuning data.
  - Quick check question: How does OOD performance differ from in-distribution performance after fine-tuning?

- Concept: Centered Kernel Alignment (CKA) for representation analysis
  - Why needed here: The paper uses CKA to quantify representational shifts between pretrained and fine-tuned models, showing that larger pretraining datasets lead to more significant changes.
  - Quick check question: What does a low CKA score between two model representations indicate about their similarity?

## Architecture Onboarding

- Component map:
  Pretrained models (ViT, ResNet) -> Fine-tuning pipeline -> Robustness evaluation (ImageNet-RIB) -> Continual learning methods (EWC, LwF) -> Model soup integration -> CKA analysis

- Critical path:
  1. Load pretrained model
  2. Fine-tune on downstream dataset
  3. Evaluate robustness on OOD datasets
  4. Compute robustness improvement scores
  5. Analyze representational shifts with CKA

- Design tradeoffs:
  - Model size vs. robustness: Larger models show more severe robustness degradation
  - Fine-tuning dataset size vs. robustness: Smaller datasets cause more forgetting
  - Continual learning methods vs. performance: Some methods preserve robustness but may reduce downstream task performance

- Failure signatures:
  - Large drop in OOD accuracy after fine-tuning on small datasets
  - Low CKA scores between pretrained and fine-tuned models
  - Significant representational shifts in later transformer layers
  - Negative robustness improvement scores

- First 3 experiments:
  1. Fine-tune a LAION-2B pretrained model on a small subset of ImageNet-1K and measure OOD robustness degradation
  2. Compare CKA scores between pretrained and fine-tuned models across different transformer layers
  3. Apply EWC regularization during fine-tuning and measure its effect on OOD robustness preservation

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the catastrophic forgetting observed in large-scale pretrained models (e.g., LAION-2B) primarily stem from the dataset size, the pretraining methodology (e.g., CLIP), or a combination of both?
- Basis in paper: [explicit] The paper compares CLIP models pretrained on different scales (LAION-100M vs. LAION-2B) and different architectures (ViT vs. ResNet), finding that only models pretrained on large-scale datasets exhibit severe robustness degradation after fine-tuning on small datasets.
- Why unresolved: The analysis isolates dataset size as a key factor but does not fully disentangle the effects of pretraining methodology versus scale.
- What evidence would resolve it: Systematic experiments ablating pretraining scale while keeping the CLIP methodology fixed, and vice versa, would clarify the relative contributions of each factor.

### Open Question 2
- Question: Is the observed catastrophic forgetting in large-scale pretrained models due to overfitting to the pretraining data, or is it a consequence of the models' learned representations being overly specialized?
- Basis in paper: [explicit] The authors rule out overfitting as the primary driver, noting that LAION-2B models learn more slowly than ImageNet-21K models but still suffer greater robustness degradation.
- Why unresolved: While overfitting is discounted, the paper does not definitively establish whether the representations themselves are overly specialized to the pretraining distribution or whether other factors contribute.
- What evidence would resolve it: Comparative analysis of representation similarity (e.g., CKA scores) between pretrained and fine-tuned models across different scales, combined with probing tasks to assess the generality of learned features.

### Open Question 3
- Question: Can continual learning methods or model soup sufficiently mitigate catastrophic forgetting in large-scale pretrained models, or are more radical architectural changes required?
- Basis in paper: [explicit] The authors demonstrate that regularization-based continual learning methods and model soup improve robustness, but models pretrained on large-scale datasets still exhibit significant degradation compared to those pretrained on smaller datasets.
- Why unresolved: The study shows that these methods help but do not fully close the performance gap.
- What evidence would resolve it: Direct comparison of continual learning methods with architectural interventions (e.g., progressive networks, elastic weight consolidation) on large-scale pretrained models.

## Limitations

- The study primarily focuses on CLIP-based models and ImageNet-related datasets, limiting generalizability to other architectures and domains
- The ImageNet-RIB benchmark, while comprehensive for vision tasks, represents only a subset of possible OOD scenarios
- The paper doesn't extensively explore the interaction between model architecture (ViT vs. ResNet) and pretraining dataset size effects on robustness degradation

## Confidence

**High Confidence**: The empirical observation that fine-tuning generally reduces OOD robustness across all pretrained models, and that fine-tuning dataset size is a major determinant of robustness collapse.

**Medium Confidence**: The claim that larger pretraining datasets lead to more severe robustness degradation is supported by the data but may be influenced by the specific datasets and models studied.

**Low Confidence**: The generalizability of findings to non-CLIP models, non-vision domains, and real-world scenarios where fine-tuning datasets may be larger or more representative of target distributions.

## Next Checks

1. **Architecture Generalization Test**: Replicate the robustness degradation experiments using non-CLIP models (e.g., BERT, GPT variants) fine-tuned on NLP tasks to determine if the phenomenon extends beyond vision transformers.

2. **Dataset Size Interaction Analysis**: Systematically vary both pretraining and fine-tuning dataset sizes to map the full parameter space of their interaction effects on robustness, particularly testing whether the proposed relationship holds at different scales.

3. **Long-term Adaptation Study**: Implement a curriculum learning approach where models are gradually exposed to larger fine-tuning datasets over time, measuring whether this mitigates the catastrophic forgetting observed with immediate fine-tuning on small datasets.