---
ver: rpa2
title: 'GleanVec: Accelerating vector search with minimalist nonlinear dimensionality
  reduction'
arxiv_id: '2410.22347'
source_url: https://arxiv.org/abs/2410.22347
tags:
- search
- vectors
- vector
- dimensionality
- database
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This work introduces two new methods for dimensionality reduction
  in high-dimensional vector search: LeanVec-Sphering and Generalized LeanVec (GleanVec).
  LeanVec-Sphering is a linear method that outperforms existing techniques, trains
  faster, and requires no hyperparameters.'
---

# GleanVec: Accelerating vector search with minimalist nonlinear dimensionality reduction

## Quick Facts
- arXiv ID: 2410.22347
- Source URL: https://arxiv.org/abs/2410.22347
- Authors: Mariano Tepper; Ishwar Singh Bhati; Cecilia Aguerrebere; Ted Willke
- Reference count: 40
- Introduces LeanVec-Sphering and Generalized LeanVec (GleanVec) for dimensionality reduction in high-dimensional vector search

## Executive Summary
This paper addresses the challenge of accelerating high-dimensional vector search while maintaining accuracy, particularly for out-of-distribution (OOD) queries where query and database vectors have different statistical distributions. The authors introduce two methods: LeanVec-Sphering, a linear dimensionality reduction technique that outperforms existing methods and requires no hyperparameters, and GleanVec, a nonlinear method using piecewise linear transformations to further improve accuracy. Both methods significantly improve search performance by reducing vector dimensionality while preserving inner products, with particular benefits in OOD settings.

## Method Summary
The paper introduces two complementary methods for accelerating vector search through dimensionality reduction. LeanVec-Sphering uses singular value decomposition (SVD) to project high-dimensional vectors to lower dimensions while preserving inner products, trained in a query-aware fashion by optimizing over both database and query vectors. GleanVec extends this with a piecewise linear approach, first clustering the database vectors using spherical k-means, then applying local linear dimensionality reduction within each cluster. Both methods maintain the maximum inner product search (MIPS) objective while reducing memory bandwidth requirements by a factor of D/d, where D is the original dimension and d is the target dimension.

## Key Results
- LeanVec-Sphering outperforms existing linear dimensionality reduction methods while training faster and requiring no hyperparameters
- GleanVec achieves superior accuracy to LeanVec-Sphering by using piecewise linear transformations, particularly in OOD settings
- Both methods significantly reduce memory bandwidth requirements, accelerating search performance by factors of D/d
- Experiments on datasets with 1M-13M vectors and dimensions 200-960 demonstrate consistent improvements across various scenarios

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Dimensionality reduction improves vector search speed by reducing memory bandwidth usage
- Mechanism: The proposed methods reduce vector dimensionality from D to d, decreasing the number of entries fetched from memory during similarity computations. This directly reduces memory bandwidth requirements and computational complexity by a factor of D/d.
- Core assumption: Memory latency is the primary bottleneck in high-dimensional graph-based vector search, not computational complexity of the similarity metric itself.
- Evidence anchors:
  - [abstract]: "The predominant example are graph indices [e.g., 6, 26, 36, 51] that stand out with their high accuracy and performance for moderate dimensionalities (D≈128) [2] but suffer when dealing with the higher dimensionalities typically produced by deep learning models (D≈512, 768 and upwards) [61]."
  - [section 2]: "The root cause of this performance degradation is that graph search is bottlenecked by the memory latency of the system, as we are fetching database vectors from memory in a random-like access pattern."
  - [corpus]: Weak - the corpus doesn't directly address memory bandwidth bottlenecks in graph-based search.
- Break Condition: If the similarity computation becomes the bottleneck rather than memory access, or if cache hierarchy changes make random access patterns less costly.

### Mechanism 2
- Claim: Query-aware dimensionality reduction preserves inner products better than query-agnostic methods
- Mechanism: The methods optimize the inner product preservation directly by considering both database vectors X and query vectors Q during training, rather than just minimizing reconstruction error. This ensures the reduced-dimensional representations maintain the maximum inner product search accuracy.
- Core assumption: The statistical distributions of database and query vectors differ significantly in practical applications, making query-agnostic methods suboptimal.
- Evidence anchors:
  - [abstract]: "Here, it is challenging to achieve high accuracy as the queries often have different statistical distributions than the database vectors."
  - [section 2]: "Thus, we seek to optimize the inner-products directly in a query-aware fashion as in Equation (1). Not considering the queries and following the more traditional approach of minimizing the reconstruction error... might lead to greater inaccuracies in the inner-product computation, as depicted in Figure 1."
  - [corpus]: Weak - the corpus doesn't provide direct evidence about query-aware vs query-agnostic dimensionality reduction.
- Break Condition: If database and query vectors have similar distributions, making the query-aware optimization unnecessary.

### Mechanism 3
- Claim: Piecewise linear transformations enable nonlinear dimensionality reduction with minimal computational overhead
- Mechanism: GleanVec partitions the database vectors into clusters using spherical k-means, then applies linear dimensionality reduction within each cluster. This creates a piecewise linear approximation that captures nonlinear relationships while maintaining computational efficiency.
- Core assumption: The vector space can be effectively partitioned into regions where linear dimensionality reduction is sufficient.
- Evidence anchors:
  - [abstract]: "The nonlinear Generalized LeanVec (GleanVec) uses a piecewise linear scheme to further improve the search accuracy while remaining computationally nimble."
  - [section 4]: "From these considerations, the core idea behind GleanVec is to decouple the problem in two stages: (1) data partitioning, and (2) locally-linear dimensionality reduction applied in a query-aware fashion."
  - [corpus]: Weak - the corpus doesn't discuss piecewise linear dimensionality reduction approaches.
- Break Condition: If the vector space cannot be effectively partitioned, or if the partitioning creates too many small clusters making the approach inefficient.

## Foundational Learning

- Concept: Maximum Inner Product Search (MIPS)
  - Why needed here: The paper uses MIPS as the primary similarity metric, which is common for deep learning embeddings but requires specific handling compared to cosine similarity or Euclidean distance.
  - Quick check question: How can you convert cosine similarity to maximum inner product search?
  - Answer: By normalizing the vectors, since cosine similarity is equivalent to inner product of normalized vectors.

- Concept: Graph-based vector search indices
  - Why needed here: The paper builds on graph-based indices as the baseline, which are state-of-the-art for moderate dimensions but degrade in high dimensions.
  - Quick check question: What is the fundamental trade-off in graph-based vector search?
  - Answer: Graph indices provide sublinear search time at the cost of building time and memory usage.

- Concept: Singular Value Decomposition (SVD) and Stiefel manifold
  - Why needed here: LeanVec-Sphering uses SVD for efficient linear dimensionality reduction, and the optimization is constrained to the Stiefel manifold of orthonormal matrices.
  - Quick check question: What property must a matrix have to belong to the Stiefel manifold St(D,d)?
  - Answer: The matrix must have orthonormal rows, i.e., UU⊤ = I.

## Architecture Onboarding

- Component map: Data preprocessing -> Learning module -> Search module -> Query processing -> Results
- Critical path: Data → Learning → Search → Results
  - Data preprocessing feeds into learning
  - Learning produces projection matrices/clusters
  - Search uses these for accelerated similarity computation
  - Re-ranking corrects for approximation errors

- Design tradeoffs:
  - Target dimensionality d vs accuracy: Lower d reduces memory and computation but may hurt accuracy
  - Number of clusters C vs efficiency: More clusters can capture more structure but increase search overhead
  - Lazy vs eager inner product computation: Lazy is more computationally expensive but may reduce memory usage

- Failure signatures:
  - Accuracy degradation: Check if target dimensionality is too low or clustering is poor
  - Performance regression: Verify memory bandwidth reduction is actually being realized
  - Training instability: Ensure proper normalization and scaling of input vectors

- First 3 experiments:
  1. Benchmark LeanVec-Sphering vs SVD on an ID dataset to verify it doesn't degrade performance in the simple case
  2. Test GleanVec with varying numbers of clusters (C=16, 32, 48) on an OOD dataset to find optimal tradeoff
  3. Measure memory bandwidth usage with and without dimensionality reduction to quantify the theoretical gains

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the computational efficiency of GleanVec scale with the number of clusters C when using the eager inner product algorithm (Algorithm 4)?
- Basis in paper: [explicit] The paper discusses the trade-off between using more clusters (C = 48 vs C = 16) for improved accuracy versus potential cache contention issues. However, it does not provide empirical data on how computational efficiency scales with C.
- Why unresolved: The paper mentions that further experiments and benchmarks are needed to determine reasonable default values for the number of clusters C, indicating that this aspect has not been fully explored.
- What evidence would resolve it: Conducting experiments to measure the computational efficiency (e.g., queries per second) of GleanVec with varying numbers of clusters C, and analyzing the relationship between C and performance, would provide the necessary evidence.

### Open Question 2
- Question: Can GleanVec be effectively learned in an end-to-end fashion, as opposed to the stage-wise training approach described in Section 4.1?
- Basis in paper: [explicit] The paper mentions that GleanVec could be learned in an end-to-end fashion, similar to VQ-VAE, using appropriate techniques to propagate the gradient through the bottleneck. However, this alternative is left for future work.
- Why unresolved: The paper explicitly states that the exploration of this end-to-end learning technique is left for future work, indicating that it has not been investigated yet.
- What evidence would resolve it: Implementing and evaluating an end-to-end learning approach for GleanVec, and comparing its performance and efficiency to the stage-wise training approach, would provide the necessary evidence.

### Open Question 3
- Question: How does the performance of LeanVec-Sphering compare to LeanVec-ES+FW in terms of both loss minimization and search accuracy, particularly in the out-of-distribution (OOD) setting?
- Basis in paper: [explicit] The paper states that LeanVec-Sphering outperforms other linear methods, including LeanVec-ES+FW, in terms of both loss minimization and search accuracy in the OOD setting. However, it does not provide a direct comparison between LeanVec-Sphering and LeanVec-ES+FW in terms of both metrics.
- Why unresolved: While the paper demonstrates the superiority of LeanVec-Sphering over other linear methods, it does not provide a detailed comparison specifically between LeanVec-Sphering and LeanVec-ES+FW in terms of both loss minimization and search accuracy.
- What evidence would resolve it: Conducting experiments to directly compare the performance of LeanVec-Sphering and LeanVec-ES+FW in terms of both loss minimization and search accuracy, particularly in the OOD setting, would provide the necessary evidence.

## Limitations

- The paper lacks detailed implementation specifications for the lazy and eager inner product computation methods in GleanVec, which are critical for faithful reproduction
- The claim that memory bandwidth is the primary bottleneck in high-dimensional graph search is asserted but not empirically validated through bandwidth measurements
- Limited related work on query-aware dimensionality reduction suggests this may be a novel approach with unexplored limitations

## Confidence

- **High Confidence**: LeanVec-Sphering outperforms existing linear methods and trains faster (supported by quantitative comparisons in Table 1)
- **Medium Confidence**: Query-aware optimization provides significant benefits over query-agnostic methods (supported by accuracy improvements but limited ablation studies)
- **Low Confidence**: Memory bandwidth reduction is the primary mechanism for performance gains (assertion made but not directly measured)

## Next Checks

1. Implement bandwidth monitoring during search to empirically verify that dimensionality reduction reduces memory bandwidth usage by the claimed factor of D/d
2. Conduct ablation studies comparing query-aware vs query-agnostic dimensionality reduction on datasets where database and query distributions are known to differ
3. Test GleanVec with varying numbers of clusters (C=16, 32, 48) and measure the tradeoff between accuracy improvement and computational overhead to validate the piecewise linear approach