---
ver: rpa2
title: Pitfalls and Outlooks in Using COMET
arxiv_id: '2408.15366'
source_url: https://arxiv.org/abs/2408.15366
tags:
- comet
- translation
- metrics
- scores
- machine
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'COMET, a popular learned MT evaluation metric, exhibits several
  pitfalls: software version differences cause inconsistent scores; model quantization
  and batch size choices affect accuracy; empty or mismatched-language hypotheses
  receive unexpectedly high scores; training data biases lead to non-comparable scores
  across languages and domains; multi-reference evaluation lacks a robust approach;
  translationese in references and hypotheses impacts scores; and model reporting
  is often incomplete in literature. The authors release sacreCOMET to improve reproducibility
  and recommend using the latest software, proper score handling, and awareness of
  these biases when interpreting COMET scores.'
---

# Pitfalls and Outlooks in Using COMET

## Quick Facts
- arXiv ID: 2408.15366
- Source URL: https://arxiv.org/abs/2408.15366
- Authors: Vilém Zouhar, Pinzhen Chen, Tsz Kin Lam, Nikita Moghe, Barry Haddow
- Reference count: 22
- Primary result: COMET scores are inconsistent across software versions, sensitive to quantization, vulnerable to empty/mismatched-language inputs, biased by training data, lack robust multi-reference support, and suffer from incomplete reporting.

## Executive Summary
COMET, a learned metric for machine translation evaluation, suffers from multiple pitfalls that undermine its reliability and comparability across research papers. These issues include version-dependent scoring due to software and numerical precision changes, vulnerability to malformed inputs like empty or wrong-language hypotheses, biases from training data distribution affecting cross-language and cross-domain comparisons, lack of robust multi-reference handling, sensitivity to translationese, and poor model reporting practices. The authors introduce sacreCOMET to improve reproducibility and provide recommendations for mitigating these pitfalls.

## Method Summary
The study evaluates COMET across multiple dimensions: software version sensitivity, quantization effects, handling of empty and mismatched-language hypotheses, domain and language biases, multi-reference strategies, translationese impact, and model reporting practices. Experiments use WMT shared task data for English↔German and English↔Chinese directions, comparing COMET scores with human direct assessments. The authors test different Python and unbabel-comet versions, compute precisions (FP32, FP16, QINT8), and evaluate three multi-reference aggregation methods (max, avg, agg). They also examine score distributions across domains and translation directions, and assess the impact of translationese in references and hypotheses.

## Key Results
- COMET scores vary significantly across different software versions and numerical precisions due to implementation changes and quantization effects.
- Empty or mismatched-language hypotheses can receive unexpectedly high scores because COMET lacks explicit input validation.
- Training data biases cause COMET scores to be non-comparable across translation directions and domains, with significant score distribution shifts.
- Multi-reference evaluation lacks a robust approach; current ad-hoc methods (max, avg, agg) yield inconsistent results depending on evaluation metrics.
- Translationese in references changes absolute COMET scores but preserves system rankings, while translationese in hypotheses affects both scores and rankings differently across COMET variants.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: COMET scores are sensitive to software versions and numerical precision settings, causing inconsistent evaluation results.
- Mechanism: COMET relies on underlying Python and library versions for tokenization, smoothing, and computation. Different versions can change internal processing, and model quantization (FP16, QINT8) alters weight precision, leading to score shifts.
- Core assumption: The COMET metric implementation is deterministic but version-dependent; quantization changes numerical behavior.
- Evidence anchors:
  - [abstract] "software version differences cause inconsistent scores; model quantization and batch size choices affect accuracy"
  - [section] "neglecting this would lead to unexpected scores because the same COMET checkpoint can produce vastly different scores with previous COMET framework versions"
  - [corpus] Corpus signals: weak - related papers discuss COMET but not specific version pitfalls.
- Break condition: When the COMET model is retrained from scratch or uses fixed deterministic computation unaffected by software changes.

### Mechanism 2
- Claim: COMET is vulnerable to data biases such as empty or mismatched-language hypotheses, and translationese, leading to unreliable scores.
- Mechanism: As a neural model trained on human-rated MT outputs, COMET learns to regress from source, hypothesis, reference to human scores. It lacks explicit handling for malformed inputs (empty strings, wrong language) or domain/translationese shifts, so such cases receive scores inconsistent with human expectations.
- Core assumption: Training data does not include or properly represent empty or wrong-language hypotheses, nor translationese variation.
- Evidence anchors:
  - [abstract] "empty or mismatched-language hypotheses receive unexpectedly high scores; training data biases lead to non-comparable scores across languages and domains"
  - [section] "neural metrics provide no such guarantee... assigns a positive instance-level score even if the hypothesis is an empty string"
  - [corpus] Corpus signals: weak - related papers do not address these specific pitfalls.
- Break condition: When training data is augmented with adversarial or malformed examples and model is retrained to handle them.

### Mechanism 3
- Claim: COMET lacks robust multi-reference support and transparent model reporting, limiting reproducibility and fair comparison.
- Mechanism: COMET architecture accepts only one reference; multi-reference strategies (max, avg, agg) are ad-hoc and yield inconsistent results. Model versioning and citations are often omitted, preventing exact reproduction of results.
- Core assumption: Adding references improves metric robustness but current implementation methods are not validated; model metadata is essential for reproducibility.
- Evidence anchors:
  - [abstract] "multi-reference evaluation lacks a robust approach; model reporting is often incomplete in literature"
  - [section] "there is no single method that can consistently take advantage of the inclusion of multiple references"
  - [corpus] Corpus signals: weak - related papers do not discuss multi-reference or reporting issues.
- Break condition: When a unified multi-reference architecture is developed and reporting standards are enforced.

## Foundational Learning

- Concept: Neural metric regression and its reliance on training data distribution.
  - Why needed here: COMET is a regression model trained on human DA scores; its behavior is shaped by the data it sees. Understanding this is key to diagnosing score inconsistencies.
  - Quick check question: Why might COMET assign a high score to an empty hypothesis?

- Concept: Software versioning and numerical precision in ML inference.
  - Why needed here: Different Python/COMET versions and quantization settings alter internal computations, producing different metric scores on the same input.
  - Quick check question: What effect does switching from FP32 to FP16 have on COMET scores?

- Concept: Language identification and reference handling in MT evaluation.
  - Why needed here: COMET lacks explicit language checking; mismatched-language hypotheses can receive misleadingly high scores. Proper preprocessing is required.
  - Quick check question: How can you detect if a hypothesis is in the wrong language before scoring?

## Architecture Onboarding

- Component map: Load COMET checkpoint → preprocess inputs → run model inference → obtain score → aggregate (if multiple references) → report
- Critical path: Load COMET checkpoint → preprocess inputs → run model inference → obtain score → aggregate (if multiple references) → report
- Design tradeoffs: Single-reference simplicity vs. multi-reference robustness; high-precision inference vs. speed; transparency in reporting vs. ease of use
- Failure signatures: Inconsistent scores across versions, unexpected high scores for empty/wrong-language inputs, unstable multi-reference results, missing model info in papers
- First 3 experiments:
  1. Compare COMET scores on same inputs using different Python/unbabel-comet versions
  2. Test COMET on empty and mismatched-language hypotheses to confirm score anomalies
  3. Evaluate multi-reference strategies (max, avg, agg) on WMT test sets and measure system ranking stability

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal way to incorporate multiple references into COMET evaluations to consistently improve accuracy?
- Basis in paper: [explicit] The paper identifies that COMET lacks a defined way to handle multiple references and tests three strategies (max, avg, agg) with inconsistent results.
- Why unresolved: Different methods yield varying accuracies depending on the evaluation metric and reference quality used, and there's no clear winner.
- What evidence would resolve it: Systematic experiments across diverse datasets and evaluation metrics, potentially exploring architectural modifications to COMET to natively handle multiple references.

### Open Question 2
- Question: How can we develop COMET or similar metrics that are robust to translationese in both references and hypotheses?
- Basis in paper: [explicit] The paper shows COMET scores are sensitive to translationese in references (absolute scores change but rankings stay stable) and hypotheses (COMETkiwiDA is more sensitive than COMETDA).
- Why unresolved: The impact of translationese on metric reliability is complex and varies between metric variants; current models are trained on translationese, creating bias.
- What evidence would resolve it: Experiments with metrics trained on less translationese data, or methods to detect and normalize for translationese, evaluated on both absolute scores and ranking stability.

### Open Question 3
- Question: How can we ensure COMET scores are comparable across different translation directions and domains?
- Basis in paper: [explicit] The paper demonstrates that COMET scores are biased by training data distribution, leading to non-comparable scores across translation directions and domains.
- Why unresolved: The statistical learning nature of COMET inherently captures these biases, and simple normalization methods like z-scoring have limitations.
- What evidence would resolve it: Development and evaluation of methods to calibrate or normalize COMET scores across directions/domains, or architectures that explicitly learn direction/domain-agnostic representations.

## Limitations

- Many claimed pitfalls rely on theoretical reasoning rather than comprehensive experimental validation across diverse datasets.
- The translationese impact study lacks rigorous validation of what constitutes translationese in reference data.
- The proposed sacreCOMET package addresses reproducibility but its real-world effectiveness remains untested beyond internal validation.

## Confidence

**High Confidence**: Software version sensitivity and quantization effects - Well-documented ML phenomena with straightforward experimental verification.

**Medium Confidence**: Empty and mismatched-language hypothesis scoring - Theoretical mechanism is sound but practical impact frequency is uncertain.

**Low Confidence**: Multi-reference strategy limitations and translationese impact - Based on limited experiments without systematic comparisons or rigorous validation.

## Next Checks

1. Independently verify that switching Python versions (3.8 vs 3.9 vs 3.10) or unbabel-comet package versions produces statistically significant score differences on the same MT outputs across multiple translation directions.

2. Systematically evaluate all three reference aggregation methods (max, avg, agg) on WMT test sets for at least 10 language pairs, measuring not just score differences but also system ranking stability using Kendall's tau-c correlation.

3. Create a controlled experiment where human evaluators rate the same MT outputs with original vs. paraphrased (translationese) references, then compare these human judgments with COMET scores to determine if score shifts align with human perception changes.