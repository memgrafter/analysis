---
ver: rpa2
title: Optimal and efficient text counterfactuals using Graph Neural Networks
arxiv_id: '2408.01969'
source_url: https://arxiv.org/abs/2408.01969
tags:
- edge
- graph
- which
- each
- edits
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work introduces a graph-based framework for generating optimal
  and efficient text counterfactuals using Graph Neural Networks (GNNs). The method
  frames word-level substitutions as a combinatorial optimization problem, solving
  it via rectangular linear assignment on bipartite graphs.
---

# Optimal and efficient text counterfactuals using Graph Neural Networks

## Quick Facts
- arXiv ID: 2408.01969
- Source URL: https://arxiv.org/abs/2408.01969
- Authors: Dimitris Lymperopoulos; Maria Lymperaiou; Giorgos Filandrianos; Giorgos Stamou
- Reference count: 25
- One-line primary result: Achieves state-of-the-art performance across fluency, minimality, and closeness metrics while being 20-97x faster than existing methods

## Executive Summary
This work introduces a graph-based framework for generating optimal and efficient text counterfactuals using Graph Neural Networks (GNNs). The method frames word-level substitutions as a rectangular linear assignment problem (RLAP) on bipartite graphs, solving it via graph theory algorithms and GNN approximation. Evaluated on IMDB and Newsgroups datasets, the approach achieves state-of-the-art performance across multiple metrics while significantly speeding up inference compared to deterministic algorithms.

## Method Summary
The framework constructs bipartite graphs with source words as one node set and potential target substitutions as the other, connected by edges weighted by semantic distance (WordNet path similarity or embedding cosine similarity). A GNN approximates the optimal RLAP solution, followed by beam search to select final substitutions. The method supports both POS-specific and POS-agnostic substitutions, with edge filtering to preserve syntactic structure. Training uses synthetic data with Hungarian algorithm solutions as targets, optimized via Balanced Cross Entropy loss for 20 epochs.

## Key Results
- Achieves state-of-the-art performance across fluency, minimality, and closeness metrics
- Provides 20-97x speedup compared to deterministic algorithms
- Generalizes to both POS-specific and POS-agnostic substitutions using WordNet or embedding-based similarities
- Maintains comparable quality to optimal solutions while significantly improving runtime efficiency

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Framing word-level counterfactual interventions as a rectangular linear assignment problem (RLAP) enables optimal and controllable substitutions.
- Mechanism: By modeling the problem as finding a minimum-weight matching between source and target words in a bipartite graph, we can leverage graph theory algorithms to ensure each source word is substituted optimally while preserving control over which words are changed.
- Core assumption: The semantic distance between words can be accurately captured as edge weights in a bipartite graph, and the optimal assignment will produce meaningful counterfactuals.
- Evidence anchors:
  - [abstract] "This work introduces a graph-based framework for generating optimal and efficient text counterfactuals using Graph Neural Networks (GNNs). The method frames word-level substitutions as a combinatorial optimization problem, solving it via rectangular linear assignment on bipartite graphs."
  - [section 3] "Under these requirements, we formulate the following constraint optimization problem: min X we, subject to s ̸= t if ∃es→t (1)"
  - [corpus] Weak - corpus neighbors focus on graph counterfactual explanations but don't directly address the RLAP formulation.

### Mechanism 2
- Claim: Using a Graph Neural Network to approximate the RLAP solution significantly speeds up inference compared to deterministic algorithms.
- Mechanism: A trained GNN can quickly approximate the optimal matching found by deterministic algorithms like the Hungarian method, achieving 20-97x speedup while maintaining comparable quality of substitutions.
- Core assumption: The GNN can learn to approximate the optimal RLAP solution from synthetic training data, and this approximation is good enough for practical counterfactual generation.
- Evidence anchors:
  - [abstract] "A GNN approximates the optimal matching, significantly speeding up inference compared to deterministic algorithms."
  - [section 4.2] "As previously discussed (Section 3), traditional deterministic approaches achieve this in O(mn log n). While these methods provide the optimal solution, they lack speed as the dataset size, and therefore graph size grows larger."
  - [corpus] Weak - corpus neighbors discuss GNN counterfactual explanations but don't specifically address using GNNs to speed up RLAP solutions.

### Mechanism 3
- Claim: Combining POS-specific and POS-agnostic substitution strategies with edge filtering preserves syntactic structure while enabling flexible counterfactual generation.
- Mechanism: By either restricting substitutions to same-POS words or using edge filtering to penalize different-POS substitutions, the framework maintains grammatical correctness while allowing for diverse semantic changes.
- Core assumption: Preserving POS during substitutions is crucial for maintaining fluency, and the edge filtering mechanism can effectively enforce this constraint.
- Evidence anchors:
  - [section 4.1] "In order to preserve syntax in the POS-agnostic case, we force substitutions between same-POS words exclusively: thus, we experiment with an edge filtering mechanism, which sets a predefined large weight to edges, ~10 times bigger than the normal edge weights as instructed from WordNet path similarity or cosine similarity of embeddings."
  - [section 5.2] "By examining the results with and without the use of edge filtering we observe that they are quite similar. This leads us to assume that such a mechanism is redundant and its functionality is covered by the GNN solution to our graph assignment problem."
  - [corpus] Weak - corpus neighbors discuss graph counterfactual explanations but don't specifically address POS-preserving substitution strategies.

## Foundational Learning

- Concept: Graph Neural Networks
  - Why needed here: GNNs are used to approximate the optimal solution to the RLAP problem, significantly speeding up counterfactual generation compared to deterministic algorithms.
  - Quick check question: How does a GNN differ from traditional graph algorithms, and why is it suitable for approximating RLAP solutions?

- Concept: Combinatorial Optimization
  - Why needed here: The problem of finding optimal word substitutions is framed as a combinatorial optimization problem, specifically the rectangular linear assignment problem, which can be solved using graph theory algorithms.
  - Quick check question: What is the rectangular linear assignment problem, and how does it relate to finding optimal word substitutions in text counterfactuals?

- Concept: Part-of-Speech Tagging
  - Why needed here: POS tagging is used to extract words from text based on their grammatical role, enabling both POS-specific and POS-agnostic substitution strategies while preserving syntactic structure.
  - Quick check question: How does POS tagging help in maintaining grammatical correctness during word substitutions, and what are the trade-offs between POS-specific and POS-agnostic approaches?

## Architecture Onboarding

- Component map: Text input -> Word extraction and POS tagging -> Bipartite graph construction -> GNN inference -> Beam search for final selection -> Output counterfactual text
- Critical path: The critical path for counterfactual generation is: Text input → Word extraction and POS tagging → Bipartite graph construction → GNN inference → Beam search for final selection → Output counterfactual text.
- Design tradeoffs: The framework trades off between explainability and speed by using GNNs instead of deterministic algorithms, and between optimality and efficiency by using approximate solutions. It also balances between preserving syntactic structure and enabling flexible substitutions through POS-specific and POS-agnostic strategies.
- Failure signatures: Common failure modes include: GNN failing to approximate the optimal solution (resulting in poor quality counterfactuals), edge weights not accurately capturing semantic distance (leading to semantically meaningless substitutions), and POS filtering being too strict or lenient (causing grammatically incorrect counterfactuals or limiting substitution diversity).
- First 3 experiments:
  1. Implement the deterministic RLAP solver using the Hungarian algorithm and compare its runtime and solution quality with the GNN-based approach on a small text dataset.
  2. Experiment with different edge weight computation methods (WordNet path similarity vs. embedding cosine similarity) and evaluate their impact on the quality of counterfactuals.
  3. Test the effect of POS filtering on counterfactual generation by comparing the results with and without the edge filtering mechanism on a diverse set of text samples.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the choice of embedding model (AnglE, GIST, Jina, MUG) affect the semantic quality and diversity of counterfactual edits beyond the metrics reported?
- Basis in paper: [explicit] The paper compares four embedding models (AnglE, GIST, Jina, MUG) for edge weight computation in the bipartite graph and reports their performance across metrics.
- Why unresolved: The paper focuses on quantitative metrics (fluency, closeness, flip-rate, minimality) but doesn't explore qualitative aspects like semantic diversity or the interpretability of substitutions.
- What evidence would resolve it: A detailed qualitative analysis comparing the types of substitutions each embedding model produces, including examples of edits that are semantically diverse or contextually appropriate.

### Open Question 2
- Question: Can the GNN-based RLAP solver be further optimized to achieve closer approximations to the deterministic optimal solution while maintaining or improving runtime efficiency?
- Basis in paper: [explicit] The paper uses a GNN as a faster alternative to deterministic graph matching algorithms for solving RLAP, noting a trade-off between optimality and execution speed.
- Why unresolved: The paper demonstrates that the GNN achieves good results but doesn't explore techniques to improve its accuracy or efficiency further, such as different architectures, training strategies, or loss functions.
- What evidence would resolve it: Experiments comparing different GNN architectures, training regimes, or loss functions on the RLAP task, showing improvements in approximation accuracy or runtime.

### Open Question 3
- Question: How does the performance of the framework generalize to more complex NLP tasks beyond binary sentiment and topic classification, such as multi-class classification or natural language inference?
- Basis in paper: [inferred] The paper evaluates the framework on binary sentiment classification (IMDB) and topic classification (Newsgroups), suggesting potential applicability to other tasks.
- Why unresolved: The experiments are limited to two specific tasks, and it's unclear how well the framework would perform on tasks with different characteristics, such as more classes or different types of semantic relationships.
- What evidence would resolve it: Experiments applying the framework to a diverse set of NLP tasks, including multi-class classification, natural language inference, and named entity recognition, with performance comparisons to existing methods.

## Limitations
- The framework's reliance on synthetic training data for the GNN raises questions about generalization to out-of-distribution text
- The claimed 20-97x speedup depends heavily on implementation details and hardware configurations not fully specified
- The assertion that POS-agnostic substitutions with edge filtering are "redundant" may not hold for more complex linguistic phenomena

## Confidence
- **High Confidence**: The RLAP formulation as a bipartite graph matching problem and the framework's ability to achieve state-of-the-art performance on established metrics (fluency, minimality, closeness) on IMDB and Newsgroups datasets.
- **Medium Confidence**: The claimed 20-97x speedup through GNN approximation, as this depends heavily on implementation details and hardware configurations not fully specified in the paper.
- **Low Confidence**: The assertion that POS-agnostic substitutions with edge filtering are "redundant," as this conclusion is based primarily on limited experimental results and may not hold for more complex linguistic phenomena.

## Next Checks
1. **Generalization Test**: Evaluate the framework on out-of-domain text (e.g., medical or legal documents) to assess whether GNN approximations maintain quality when training distribution differs significantly from test data.
2. **Ablation Study**: Systematically remove the GNN component and compare runtime and performance with pure deterministic RLAP solvers across varying text lengths to validate the claimed speedup.
3. **Cross-Lingual Evaluation**: Test the framework on non-English text datasets to determine whether the RLAP + GNN approach generalizes across languages with different syntactic structures and morphological complexities.