---
ver: rpa2
title: 'Multilingual Sentence-T5: Scalable Sentence Encoders for Multilingual Applications'
arxiv_id: '2403.17528'
source_url: https://arxiv.org/abs/2403.17528
tags:
- sentence
- language
- multilingual
- languages
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents Multilingual Sentence T5 (m-ST5), a scalable
  sentence embedding model that extends Sentence T5 to multilingual scenarios using
  the mT5 architecture and low-rank adaptation (LoRA) techniques. The proposed method
  achieves state-of-the-art performance on multilingual sentence retrieval and semantic
  textual similarity tasks, outperforming prior NLI-based approaches like mSimCSE.
---

# Multilingual Sentence-T5: Scalable Sentence Encoders for Multilingual Applications

## Quick Facts
- arXiv ID: 2403.17528
- Source URL: https://arxiv.org/abs/2403.17528
- Authors: Chihiro Yano; Akihiko Fukuchi; Shoko Fukasawa; Hideyuki Tachibana; Yotaro Watanabe
- Reference count: 0
- Key outcome: Achieves state-of-the-art performance on multilingual sentence retrieval and semantic textual similarity tasks, with larger models showing improved performance especially for low-resource languages

## Executive Summary
This paper presents Multilingual Sentence T5 (m-ST5), a scalable sentence embedding model that extends Sentence T5 to multilingual scenarios using the mT5 architecture and low-rank adaptation (LoRA) techniques. The proposed method achieves state-of-the-art performance on multilingual sentence retrieval and semantic textual similarity tasks, outperforming prior NLI-based approaches like mSimCSE. The model demonstrates improved performance with larger sizes, particularly benefiting low-resource languages. Notably, even monolingual fine-tuning (using English NLI data) can achieve competitive results across multiple languages, suggesting that large-scale multilingual pre-training can effectively transfer to low-resource languages without requiring target language data during fine-tuning.

## Method Summary
The method extends Sentence T5 to multilingual scenarios by fine-tuning the mT5 encoder using contrastive learning with natural language inference (NLI) data. The model employs low-rank adaptation (LoRA) to enable efficient scaling to billions of parameters while preserving fine-tuning performance. The training process involves minimizing distances between positive pairs (entailment) and maximizing distances between negative pairs (contradiction) using triplets constructed from premise-hypothesis pairs in different languages. The model is evaluated on Tatoeba, BUCC, and XSTS datasets using cosine similarity as the distance metric.

## Key Results
- Achieves state-of-the-art performance on multilingual sentence retrieval and STS tasks
- Larger models (5.7B parameters) show significant performance improvements, especially for low-resource and distant languages
- Monolingual fine-tuning on English NLI data can achieve competitive multilingual results without target language data
- LoRA with rank=8 enables efficient scaling while maintaining performance across different model sizes

## Why This Works (Mechanism)

### Mechanism 1
LoRA enables efficient scaling of multilingual sentence embeddings to billions of parameters while preserving fine-tuning performance by applying low-rank decomposition to the weight matrices of the encoder layers, replacing full matrix updates with small trainable low-rank matrices.

### Mechanism 2
Cross-lingual NLI data enables multilingual alignment without parallel corpora by learning semantic similarity across languages through contrastive fine-tuning that minimizes distance between semantically related sentence pairs (entailment) regardless of language while maximizing distance between unrelated pairs (contradiction).

### Mechanism 3
Scaling up model size improves performance, especially for low-resource and distant languages, following scaling laws observed in other language models by providing more parameters to capture fine-grained linguistic patterns and semantic nuances.

## Foundational Learning

- Concept: Contrastive learning with NLI data
  - Why needed here: Contrastive learning is the core training methodology that enables the model to learn meaningful semantic representations by distinguishing between related and unrelated sentence pairs
  - Quick check question: How does contrastive learning with NLI triplets differ from standard supervised classification?

- Concept: Cross-lingual transferability
  - Why needed here: Understanding how knowledge transfers across languages is crucial for multilingual models, especially when fine-tuning on one language and evaluating on others
  - Quick check question: What factors influence the degree of cross-lingual transfer in multilingual models?

- Concept: Low-rank adaptation (LoRA)
  - Why needed here: LoRA is the key technique that makes it computationally feasible to fine-tune extremely large models by decomposing weight updates into low-rank matrices
  - Quick check question: How does LoRA compare to full fine-tuning in terms of parameter efficiency and performance?

## Architecture Onboarding

- Component map: Data → Encoder → LoRA adaptation → Sentence embedding → Contrastive loss
- Critical path: Data → Encoder → LoRA adaptation → Sentence embedding → Contrastive loss
- Design tradeoffs:
  - LoRA rank vs. performance: Higher rank provides more capacity but increases computational cost
  - Monolingual vs. multilingual fine-tuning: Monolingual may work better for STS tasks while multilingual better for retrieval
  - Batch size: Limited by GPU memory but affects training stability
- Failure signatures:
  - Degraded performance on low-resource languages: May indicate insufficient LoRA capacity
  - Unstable training: Could be caused by improper learning rate or batch size
  - Poor cross-lingual retrieval: Might indicate inadequate cross-lingual alignment in training data
- First 3 experiments:
  1. Compare LoRA rank 4 vs rank 8 on a small validation set to find optimal rank
  2. Test monolingual vs multilingual fine-tuning on STS-B to understand task-specific differences
  3. Evaluate scaling effect by comparing different model sizes (e.g., mT5-large vs mT5-xxl) on low-resource languages

## Open Questions the Paper Calls Out

### Open Question 1
How does the performance of m-ST5 compare when using different rank values for the LoRA adaptation matrix beyond r=4, 8, and 16? The paper mentions that ranks r=4, 8, and 16 were tried but did not observe significant differences in performance, though details are shown in Table 5.

### Open Question 2
What is the impact of using different pre-training data sizes and qualities on the performance of m-ST5 for low-resource languages? The paper mentions that low-resource languages benefit more from parameter increases, but it does not explore how different pre-training data sizes or qualities affect this benefit.

### Open Question 3
How does the performance of m-ST5 vary across different language families and linguistic distances from English? The paper notes that languages far from English (e.g., ar and tr) benefit more from parameter increases, but it does not provide a comprehensive analysis across different language families.

## Limitations
- Evaluation only covers 36 languages, leaving uncertainty about performance on truly low-resource languages not represented in test sets
- LoRA rank of 8 is chosen without systematic ablation studies to determine if this is optimal across different model sizes and tasks
- Limited data points for scaling law analysis (only three model sizes tested)

## Confidence
- High Confidence: Core claim of achieving state-of-the-art performance on multilingual sentence retrieval and STS tasks
- Medium Confidence: Claim that monolingual fine-tuning can achieve competitive multilingual results
- Low Confidence: Assertion that scaling laws will hold for even larger models or that benefits are uniform across all language families

## Next Checks
1. Evaluate m-ST5 on a diverse set of truly low-resource languages to validate cross-lingual transfer claims beyond the 36 languages tested
2. Systematically vary the LoRA rank parameter across different model sizes to determine optimal configuration for each scale
3. Monitor and evaluate model performance on cross-lingual tasks at multiple points during fine-tuning to assess catastrophic forgetting of multilingual capabilities