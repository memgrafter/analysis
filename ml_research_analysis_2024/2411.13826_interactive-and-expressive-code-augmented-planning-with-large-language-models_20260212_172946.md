---
ver: rpa2
title: Interactive and Expressive Code-Augmented Planning with Large Language Models
arxiv_id: '2411.13826'
source_url: https://arxiv.org/abs/2411.13826
tags:
- description
- product
- code
- page
- repl
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces REPL-Plan, a novel approach for planning
  with large language models (LLMs) that combines code-expressivity with dynamic adaptation.
  REPL-Plan allows LLMs to interact with Read-Eval-Print Loops (REPLs) that can recursively
  spawn child REPLs, enabling top-down task decomposition and error correction.
---

# Interactive and Expressive Code-Augmented Planning with Large Language Models

## Quick Facts
- arXiv ID: 2411.13826
- Source URL: https://arxiv.org/abs/2411.13826
- Authors: Anthony Z. Liu; Xinhe Wang; Jacob Sansom; Yao Fu; Jongwook Choi; Sungryull Sohn; Jaekyeom Kim; Honglak Lee
- Reference count: 40
- Primary result: REPL-Plan achieves 97.0% success rate on ALFWorld and 52% on WebShop k=10 setting, outperforming baseline methods

## Executive Summary
This paper introduces REPL-Plan, a novel approach for planning with large language models (LLMs) that combines code-expressivity with dynamic adaptation. REPL-Plan allows LLMs to interact with Read-Eval-Print Loops (REPLs) that can recursively spawn child REPLs, enabling top-down task decomposition and error correction. The approach addresses key limitations of code-based planning, including handling "fuzzy" sub-problems, bottom-up code writing, and coding bugs. Experimental results show that REPL-Plan achieves strong performance across various planning domains, including ALFWorld (97.0% success rate), WebShop (52% success rate in k=10 setting), and real-world web tasks, outperforming baseline methods that lack full code-expressivity or dynamic adaptation.

## Method Summary
REPL-Plan uses LLM-REPLs to execute code with recursive spawning, allowing LLMs to write code line-by-line while interacting with planning environments. The framework combines code-expressivity with dynamic adaptation by enabling the LLM to handle "fuzzy" sub-problems and correct coding errors in real-time. Demonstrations are stored in a global REPL pool for in-context learning, where spawned REPLs reuse code/output history from previous tasks. The approach addresses limitations of purely code-based methods by enabling top-down task decomposition and flexible adaptation to errors or ambiguous situations.

## Key Results
- REPL-Plan achieves 97.0% success rate on ALFWorld household tasks, significantly outperforming baselines
- On WebShop, REPL-Plan reaches 52% success rate in k=10 setting, demonstrating strong e-commerce navigation capabilities
- In real-world web tasks, REPL-Plan achieves 39.6% of expert score, showing practical applicability to complex web environments

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** REPL-Plan enables top-down task decomposition by recursively spawning child REPLs for subtasks, allowing LLMs to handle complex planning problems dynamically.
- **Mechanism:** The REPL-Plan framework allows an LLM to interact with LLM-REPLs, which can recursively spawn child REPLs. This recursive spawning enables the model to break down complex tasks into smaller, manageable subtasks. The LLM can dynamically adapt its plan by correcting errors and handling ambiguous situations in real-time.
- **Core assumption:** The LLM can effectively decompose tasks into subtasks and generate appropriate code for each subtask.
- **Evidence anchors:**
  - [abstract] "REPL-Plan allows LLMs to interact with Read-Eval-Print Loops (REPLs) that can recursively spawn child REPLs, enabling top-down task decomposition and error correction."
  - [section] "REPL-Plan is an approach for solving complex planning problems by using LLMs to interact with LLM-REPLs, where the LLM writes code line-by-line, writing and calling functions to interact with the planning environment."
  - [corpus] "Thought-Augmented Planning for LLM-Powered Interactive Recommender Agent" suggests similar recursive decomposition in planning tasks.
- **Break condition:** The LLM fails to correctly decompose tasks or generate appropriate code for subtasks, leading to incorrect or incomplete plans.

### Mechanism 2
- **Claim:** REPL-Plan combines code-expressivity with dynamic adaptation, allowing LLMs to handle "fuzzy" sub-problems and coding bugs.
- **Mechanism:** REPL-Plan provides a fully code-expressive environment where LLMs can write and execute code line-by-line. This allows the model to handle ambiguous or unstructured data ("fuzzy" sub-problems) and correct coding errors in real-time. The dynamic nature of REPL-Plan enables the LLM to adapt its plan based on the results of code execution.
- **Core assumption:** LLMs can effectively write and debug code in a REPL environment.
- **Evidence anchors:**
  - [abstract] "REPL-Plan is an LLM planning approach that is fully code-expressive (it can utilize all the benefits of code) while also being dynamic (it can flexibly adapt from errors and use the LLM for fuzzy situations)."
  - [section] "Using a REPL allows a programmer to write code step-by-step, see the results at each step, and correct their mistakes while interacting with the REPL."
  - [corpus] "PGPO: Enhancing Agent Reasoning via Pseudocode-style Planning Guided Preference Optimization" suggests that pseudocode-style planning can improve reasoning in agents.
- **Break condition:** The LLM struggles to write correct code or effectively debug errors, leading to incorrect or incomplete plans.

### Mechanism 3
- **Claim:** REPL-Plan leverages k-shot learning by maintaining a global REPL pool, allowing the LLM to perform in-context learning from previous tasks and demonstrations.
- **Mechanism:** REPL-Plan keeps a global REPL pool, which is a set of spawned REPLs from previous task executions and demonstrations. When a REPL is called or spawned, it re-uses the code/output history, allowing the LLM to learn from previous experiences and generate consistent and correct code.
- **Core assumption:** The LLM can effectively learn from demonstrations and apply that knowledge to new tasks.
- **Evidence anchors:**
  - [abstract] "To use REPL-Plan in multiple planning tasks, we keep a global REPL pool, a set of spawned REPLs from previous task executions and demonstrations."
  - [section] "In this way, an LLM can perform in-context-learning from previous tasks and demonstrations to output correct and consistent code."
  - [corpus] "Sub-goal Distillation: A Method to Improve Small Language Agents" suggests that learning from sub-goals can improve agent performance.
- **Break condition:** The LLM fails to learn from demonstrations or apply that knowledge effectively to new tasks, leading to incorrect or incomplete plans.

## Foundational Learning

- **Concept:** Large Language Models (LLMs) and their limitations in planning tasks.
  - **Why needed here:** Understanding the strengths and weaknesses of LLMs is crucial for appreciating the need for REPL-Plan and its approach to improving planning performance.
  - **Quick check question:** What are some common limitations of LLMs in planning tasks, and how do they impact the effectiveness of traditional planning approaches?

- **Concept:** Code-augmented planning and its limitations.
  - **Why needed here:** REPL-Plan builds upon the concept of code-augmented planning, so understanding its benefits and limitations is essential for appreciating the novel contributions of REPL-Plan.
  - **Quick check question:** What are some common limitations of purely code-based approaches in planning tasks, and how do they impact the effectiveness of the planning process?

- **Concept:** Read-Eval-Print Loops (REPLs) and their application in programming.
  - **Why needed here:** REPL-Plan is based on the concept of REPLs, so understanding their functionality and benefits is crucial for appreciating the novel approach of REPL-Plan.
  - **Quick check question:** How do REPLs facilitate interactive programming, and what are some benefits of using REPLs for task solving and debugging?

## Architecture Onboarding

- **Component map:** LLM -> LLM-REPL -> Environment (act/get_obs) with Global REPL pool for demonstrations
- **Critical path:**
  1. The LLM receives a task description and initializes the main LLM-REPL.
  2. The LLM writes code in the main LLM-REPL, interacting with the environment through the act function.
  3. If the LLM encounters an undefined function, it spawns a child LLM-REPL to handle the subtask.
  4. The LLM passes context between parent-child REPLs using the get_args and answer functions.
  5. The LLM iteratively executes code and adapts its plan based on the results of code execution.
  6. The LLM completes the task by generating a sequence of actions that accomplishes the given task description.
- **Design tradeoffs:** REPL-Plan prioritizes code-expressivity and dynamic adaptation over efficiency, as the iterative nature of REPLs can be slower than traditional planning approaches. REPL-Plan relies on the LLM's ability to write and debug code, which may be a limitation for some LLMs or tasks. REPL-Plan requires a large number of demonstrations for effective in-context learning, which may not always be available.
- **Failure signatures:** The LLM fails to correctly decompose tasks or generate appropriate code for subtasks. The LLM struggles to write correct code or effectively debug errors. The LLM fails to learn from demonstrations or apply that knowledge effectively to new tasks. The LLM enters an infinite loop or generates an excessive number of actions.
- **First 3 experiments:**
  1. Test REPL-Plan on a simple planning task with a clear decomposition and minimal ambiguity to verify the basic functionality of the framework.
  2. Introduce errors or ambiguities into the task description or environment to test the dynamic adaptation capabilities of REPL-Plan.
  3. Vary the number of demonstrations provided to the LLM to assess the impact of in-context learning on the performance of REPL-Plan.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does REPL-Plan perform when scaling to larger, more complex real-world web environments with thousands of tokens per page?
- Basis in paper: [explicit] The paper mentions that real-world web tasks involve 4k-20k tokens per page and shows REPL-Plan achieves 39.6% of expert score on complex tasks, but doesn't explore scaling limits.
- Why unresolved: The experiments only tested a limited set of websites and task types. The paper doesn't examine performance degradation as observation length increases or when dealing with highly dynamic web content.
- What evidence would resolve it: Testing REPL-Plan on larger e-commerce sites like Amazon with diverse product categories, measuring success rate as observation length increases, and comparing against baselines in these settings.

### Open Question 2
- Question: What is the optimal trade-off between code-expressivity and dynamic adaptation in REPL-Plan for different types of planning tasks?
- Basis in paper: [inferred] The paper shows REPL-Plan combines both features but doesn't systematically explore how varying levels of each affect performance across different environments.
- Why unresolved: The experiments use a fixed REPL-Plan configuration. The paper doesn't investigate whether simpler planning tasks benefit from less code-expressivity or whether certain domains require more dynamic adaptation.
- What evidence would resolve it: Ablation studies testing versions with reduced code-expressivity or less dynamic adaptation, comparing performance across different planning domains (ALFWorld, WebShop, real-world tasks).

### Open Question 3
- Question: How does REPL-Plan's performance compare to fine-tuned models specifically trained to understand and generate REPL-compatible code?
- Basis in paper: [explicit] The paper mentions that zero-shot subtask-REPL inference is difficult and suggests fine-tuning as future work, but doesn't compare against fine-tuned alternatives.
- Why unresolved: The paper relies on in-context learning with demonstrations. It doesn't explore whether models fine-tuned on REPL-specific code generation could achieve better performance, especially in zero-shot scenarios.
- What evidence would resolve it: Training a model on REPL-compatible code generation tasks and comparing its performance to REPL-Plan's in-context learning approach across various planning benchmarks.

### Open Question 4
- Question: What mechanisms could make REPL-Plan more efficient by automatically identifying when subtasks can be solved independently with pure code?
- Basis in paper: [explicit] The paper mentions efficiency limitations and suggests this as future work in the limitations section.
- Why unresolved: The current implementation requires LLM intervention for all subtasks. The paper doesn't explore techniques for detecting when code can be executed independently or how to implement such automation.
- What evidence would resolve it: Developing and testing heuristics for identifying code-independent subtasks, measuring execution time improvements, and comparing success rates between automated and LLM-assisted subtask execution.

## Limitations

- REPL-Plan relies heavily on LLM coding capabilities, which may vary significantly across different model families and could limit its effectiveness.
- The recursive REPL spawning mechanism could lead to exponential growth in computational resources for complex tasks, creating efficiency concerns.
- The approach requires a substantial number of demonstrations for effective in-context learning, which may not always be available in real-world scenarios.

## Confidence

- **High confidence:** The core mechanism of recursive REPL spawning and its effectiveness for top-down task decomposition is well-supported by experimental results.
- **Medium confidence:** The code-expressivity benefits depend heavily on the underlying LLM's coding capabilities, which vary across models and tasks.
- **Medium confidence:** The k-shot learning claims are supported but the paper doesn't fully explore the impact of demonstration quality versus quantity on performance.

## Next Checks

1. **Model Agnosticism Test:** Implement REPL-Plan using multiple LLM architectures (e.g., GPT-4, Claude, Llama) to assess whether the approach's effectiveness depends on specific model capabilities.

2. **Resource Scaling Analysis:** Systematically measure computational overhead as task complexity increases, particularly focusing on the growth in REPL instances and memory usage.

3. **Generalization Evaluation:** Test REPL-Plan on a diverse set of planning domains beyond the evaluated ones, including domains with different action spaces, observation types, and task structures.