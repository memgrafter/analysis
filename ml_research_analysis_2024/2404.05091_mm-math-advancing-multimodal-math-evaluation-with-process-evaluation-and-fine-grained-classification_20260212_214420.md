---
ver: rpa2
title: 'MM-MATH: Advancing Multimodal Math Evaluation with Process Evaluation and
  Fine-grained Classification'
arxiv_id: '2404.05091'
source_url: https://arxiv.org/abs/2404.05091
tags:
- reasoning
- error
- evaluation
- lmms
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces MM-MATH, a challenging benchmark for evaluating
  multimodal math reasoning in large multimodal models (LMMs). MM-MATH includes 5,929
  open-ended middle school math problems with visual contexts, classified by difficulty,
  grade level, and knowledge points.
---

# MM-MATH: Advancing Multimodal Math Evaluation with Process Evaluation and Fine-grained Classification

## Quick Facts
- arXiv ID: 2404.05091
- Source URL: https://arxiv.org/abs/2404.05091
- Authors: Kai Sun; Yushi Bai; Ji Qi; Lei Hou; Juanzi Li
- Reference count: 18
- Primary result: Introduces MM-MATH benchmark with 5,929 multimodal math problems, achieving 31% accuracy on best models versus 82% for humans

## Executive Summary
MM-MATH is a comprehensive benchmark designed to evaluate multimodal mathematical reasoning in large multimodal models (LMMs). Unlike existing benchmarks that focus solely on final answer accuracy, MM-MATH incorporates both outcome and process evaluations, using LMM-as-a-judge to analyze solution steps and categorize errors into four types. The benchmark includes fine-grained classification across difficulty, grade level, and knowledge points, revealing that diagram misinterpretation accounts for more than half of all errors, highlighting fundamental limitations in current LMM visual processing capabilities.

## Method Summary
The MM-MATH benchmark comprises 5,929 open-ended middle school math problems with visual contexts, transformed from original MathML data into LaTeX format. The evaluation pipeline consists of two components: outcome evaluation compares final answers to ground truth using SymPy for expression simplification, while process evaluation employs GPT-4V to analyze solution steps and categorize the first error into one of four types (diagram misinterpretation, reasoning error, calculation error, or textual condition misunderstanding). The benchmark features fine-grained classification along three dimensions: difficulty, grade level, and knowledge points, enabling detailed analysis of model performance across different mathematical domains.

## Key Results
- Best-performing model achieves only 31% accuracy on MM-MATH compared to 82% for humans
- Diagram misinterpretation accounts for more than half of total error cases
- Significant performance gap between text-only and multimodal input indicates current LMMs primarily rely on linguistic knowledge
- Fine-grained classification reveals varying performance across difficulty levels, grade levels, and knowledge points

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Fine-grained classification across difficulty, grade level, and knowledge points enables more precise evaluation of multimodal math reasoning capabilities.
- **Mechanism**: By stratifying problems along multiple dimensions, the benchmark can identify specific strengths and weaknesses of LMMs in different mathematical domains and complexity levels.
- **Core assumption**: LMMs exhibit varying performance across different mathematical topics and difficulty levels, which can be reliably measured through stratified evaluation.
- **Evidence anchors**:
  - [abstract]: "MM-MATH incorporates both outcome and process evaluations, using LMM-as-a-judge to analyze solution steps and categorize errors into four types"
  - [section]: "MM-MATH includes fine-grained classification, where the problems are classified along three dimensions: difficulty, grade level, and knowledge points"
  - [corpus]: Weak - related papers focus on general multimodal reasoning but don't specifically address the impact of fine-grained classification on evaluation precision
- **Break condition**: If LMMs show uniform performance across all dimensions, the fine-grained classification would provide limited additional insight beyond binary evaluation.

### Mechanism 2
- **Claim**: Process evaluation through LMM-as-a-judge reveals specific error types that binary outcome evaluation misses.
- **Mechanism**: By analyzing step-by-step solutions and comparing them to ground truth, the system can categorize errors into diagram misinterpretation, reasoning error, calculation error, and textual condition misunderstanding.
- **Core assumption**: The first error in a solution process is the primary cause of incorrect final answers, and LMMs can reliably identify and categorize these errors.
- **Evidence anchors**:
  - [abstract]: "Process evaluation employs LMM-as-a-judge to automatically analyze solution steps, identifying and categorizing errors into specific error types"
  - [section]: "Our process evaluation reveals that diagram misinterpretation is the most common error, accounting for more than half of the total error cases"
  - [corpus]: Weak - related papers mention process evaluation but don't provide detailed analysis of error categorization effectiveness
- **Break condition**: If LMMs cannot reliably identify the first error or if error categorization becomes inconsistent across different problem types.

### Mechanism 3
- **Claim**: The significant performance gap between text-only and multimodal input indicates that current LMMs primarily rely on linguistic knowledge rather than visual information.
- **Mechanism**: By comparing model performance with text-only versus text-plus-image inputs, the benchmark reveals the extent to which LMMs utilize visual context in mathematical reasoning.
- **Core assumption**: A substantial difference in accuracy between text-only and multimodal conditions would indicate effective use of visual information.
- **Evidence anchors**:
  - [abstract]: "The best-performing model achieves only 31% accuracy on MM-MATH, compared to 82% for humans"
  - [section]: "we find that current LMMs' multimodal reasoning remains primarily text-based, lacking effective utilization of graphical information"
  - [corpus]: Weak - related papers don't specifically address the text-only vs. multimodal performance comparison
- **Break condition**: If future LMMs show minimal performance difference between text-only and multimodal conditions, indicating they've learned to effectively integrate visual information.

## Foundational Learning

- **Concept**: Multimodal reasoning in mathematical problem-solving
  - Why needed here: Understanding how models process both textual and visual information is crucial for evaluating their mathematical reasoning capabilities
  - Quick check question: What are the four types of errors identified in the process evaluation, and how do they relate to multimodal reasoning?

- **Concept**: Fine-grained classification systems
  - Why needed here: The ability to categorize problems by difficulty, grade level, and knowledge points is essential for creating a comprehensive evaluation benchmark
  - Quick check question: How does the knowledge point taxonomy in MM-MATH differ from traditional educational taxonomies?

- **Concept**: LMM-as-a-judge methodology
  - Why needed here: This approach enables automated process evaluation by having LMMs analyze their own solution steps and identify errors
  - Quick check question: What are the limitations of using LMM-as-a-judge for process evaluation, as identified in the paper?

## Architecture Onboarding

- **Component map**: Dataset (5,929 problems with fine-grained annotations) -> Format transformation (MathML to LaTeX) -> Fine-grained classification (difficulty, grade level, knowledge points) -> Evaluation setup (outcome and process evaluation) -> Error categorization (four error types)

- **Critical path**: Data collection → Format transformation → Fine-grained classification → Evaluation setup → Outcome evaluation → Process evaluation → Error analysis

- **Design tradeoffs**: The paper chose open-ended format over multiple choice to facilitate step-by-step solution analysis, but this may increase evaluation complexity and processing time.

- **Failure signatures**: 
  - Uniform performance across all difficulty levels suggests the classification system may not be effective
  - High rates of diagram misinterpretation errors indicate visual processing limitations
  - Minimal difference between text-only and multimodal performance suggests ineffective visual information utilization

- **First 3 experiments**:
  1. Run a baseline evaluation with text-only input to establish the model's text-based reasoning capabilities
  2. Conduct a multimodal evaluation to measure the impact of visual information on performance
  3. Perform process evaluation on a subset of problems to validate the error categorization system and identify the most common error types

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What specific architectural modifications to multimodal models would most effectively improve their ability to interpret geometric diagrams in math problems?
- Basis in paper: Explicit - The paper identifies "diagram misinterpretation" as the most common error type, accounting for more than half of total errors.
- Why unresolved: The paper demonstrates that diagram misinterpretation is the primary weakness but doesn't explore what architectural changes would address this limitation.
- What evidence would resolve it: Comparative studies testing models with different visual encoding architectures (CNN-based, transformer-based, hybrid) on MM-MATH with ablation analysis of visual components.

### Open Question 2
- Question: How does the performance gap between text-only and multimodal approaches vary across different types of mathematical problems (geometry, algebra, functions)?
- Basis in paper: Inferred - The paper notes that multimodal models show only minimal improvement when given images versus text alone, but doesn't break this down by mathematical domain.
- Why unresolved: The aggregate analysis doesn't reveal whether certain mathematical domains benefit more from visual information than others.
- What evidence would resolve it: Detailed breakdown of text-only vs. multimodal performance across the three knowledge point categories (Properties of Shapes, Shape Transformation, Function) in the MM-MATH benchmark.

### Open Question 3
- Question: What is the relationship between model size and error type distribution in multimodal mathematical reasoning?
- Basis in paper: Explicit - The paper observes that larger models show different error patterns, with smaller models like InternVL-4B-Chat-1.5 having fewer diagram misinterpretation errors but more reasoning errors.
- Why unresolved: The analysis is limited to a few models; the paper doesn't establish whether there's a systematic relationship between scale and specific error types.
- What evidence would resolve it: Systematic scaling experiments across multiple model sizes measuring error type distribution, potentially revealing optimal model sizes for different aspects of mathematical reasoning.

## Limitations

- The reliance on LMM-as-a-judge for error categorization introduces potential reliability concerns with approximately 9% error rate in identifying the first error
- The significant performance gap between human (82% accuracy) and best-performing model (31% accuracy) suggests the benchmark may be too challenging for current LMMs
- The error analysis shows diagram misinterpretation accounts for over half of all errors, indicating fundamental limitations in current visual processing capabilities

## Confidence

**High Confidence:**
- The benchmark successfully captures significant challenges for current LMMs in multimodal mathematical reasoning
- The fine-grained classification system effectively stratifies problems by difficulty, grade level, and knowledge points
- The four-category error classification (diagram misinterpretation, reasoning error, calculation error, textual condition misunderstanding) provides meaningful insights into failure modes

**Medium Confidence:**
- The LMM-as-a-judge methodology reliably identifies and categorizes errors in solution processes
- The performance gap between text-only and multimodal inputs indicates current LMMs primarily rely on linguistic knowledge
- The benchmark represents a significant advancement over existing multimodal math evaluation approaches

**Low Confidence:**
- The specific numerical accuracy differences between models are precise indicators of relative capabilities
- The benchmark will remain relevant and appropriately challenging as LMMs continue to improve
- The error categorization system will maintain consistency across future, more capable models

## Next Checks

1. **Inter-rater Reliability Validation**: Conduct human expert evaluation of a subset of process evaluations to measure agreement with GPT-4V's error categorization, establishing the reliability of the LMM-as-a-judge approach.

2. **Cross-dataset Transferability Test**: Evaluate whether models that perform well on MM-MATH also demonstrate improved performance on other multimodal math benchmarks, validating the benchmark's effectiveness in driving generalizable capability improvements.

3. **Progressive Difficulty Calibration**: Analyze model performance across the difficulty spectrum to ensure the classification system appropriately distinguishes between problem complexities, adjusting classifications if models show uniform performance across levels.