---
ver: rpa2
title: Disentangling Questions from Query Generation for Task-Adaptive Retrieval
arxiv_id: '2409.16570'
source_url: https://arxiv.org/abs/2409.16570
tags:
- query
- queries
- retrieval
- learning
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of adapting information retrieval
  systems to unseen tasks by generating task-adaptive synthetic queries from domain-specific
  documents. The authors propose EGG (Efficient Generalized Generator), which conceptualizes
  query generation as a "compilation" of high-level intent into task-adaptive queries
  using meta-prompts and retriever feedback.
---

# Disentangling Questions from Query Generation for Task-Adaptive Retrieval

## Quick Facts
- arXiv ID: 2409.16570
- Source URL: https://arxiv.org/abs/2409.16570
- Reference count: 15
- Key outcome: EGG achieves state-of-the-art performance on BeIR benchmarks with a query generator 47x smaller than previous approaches

## Executive Summary
This paper addresses the challenge of adapting information retrieval systems to unseen tasks by generating task-adaptive synthetic queries from domain-specific documents. The authors propose EGG (Efficient Generalized Generator), which uses meta-prompts and retriever feedback to compile high-level intent into task-adaptive queries. EGG employs two model variants: EGG-FLAN for small LMs using meta-prompt instructions, and EGG-LLAMA for larger LMs using prototype queries and in-context learning. The method significantly outperforms zero-shot and few-shot baselines across four BeIR benchmark tasks, demonstrating that explicitly instructing the LM with search intent is key to generating effective task-adaptive queries.

## Method Summary
EGG tackles task-adaptive retrieval through a two-stage approach. First, it generates synthetic queries from unlabeled documents using either meta-prompt instructions (EGG-FLAN) or prototype queries with in-context learning (EGG-LLAMA). The meta-prompt approach explicitly encodes task-specific intent attributes into query generation, while the prototype approach generates relevant demonstrations from the same documents. Second, these synthetic query-document pairs are used to train retrievers (DPR or GPL frameworks) that can generalize to the target tasks. The method disentangles query generation from question-answering, enabling better generalization to diverse search intents beyond traditional QA formats.

## Key Results
- EGG-FLAN with GPL training achieved 79.4 nDCG@10 on Fever, 58.7 on Arguana, 16.9 on Scidocs, and 40.0 on DBPedia
- Outperformed both zero-shot and few-shot baselines across all four BeIR benchmark tasks
- Query generator is 47 times smaller than previous state-of-the-art approaches
- Meta-prompt instruction proved more effective than few-shot examples for small LMs

## Why This Works (Mechanism)

### Mechanism 1
- Claim: EGG's meta-prompt approach enables small LMs to generate task-adaptive queries without expensive few-shot examples
- Mechanism: The meta-prompt instruction explicitly encodes task-specific intent attributes into the query generation process, replacing the need for in-context learning examples
- Core assumption: The LM can generalize task adaptation from a well-crafted instruction about search intent rather than needing to see examples
- Evidence anchors: [abstract] "instructing the LM with explicit search intent is a key aspect of modeling an effective query generator"

### Mechanism 2
- Claim: EGG-L LAMA's prototype query + in-context learning pipeline improves query relevance by creating task-relevant demonstrations
- Mechanism: Instead of using generic few-shot examples, EGG-L LAMA generates prototype queries from the same documents using meta-prompts, then uses these as in-context examples to guide generation of final task-adaptive queries
- Core assumption: Prototype queries generated with task-specific meta-prompts will be more relevant than random few-shot examples from other domains
- Evidence anchors: [section 2.3] "we enable in-context learning with prototype queries generated by meta-prompts to benefit from relevant documents"

### Mechanism 3
- Claim: EGG's disentanglement of query from question enables better generalization to non-QA search intents
- Mechanism: By explicitly defining eq as task-adaptive rather than assuming question format, EGG can generate queries that match diverse search intents like navigational or transactional
- Core assumption: Different search tasks require fundamentally different query formulations, not just questions about documents
- Evidence anchors: [abstract] "we challenge a trend equating query and question, and instead conceptualize query generation task as a 'compilation' of high-level intent into task-adaptive query"

## Foundational Learning

- Concept: Dense retrieval and latent space alignment
  - Why needed here: The entire approach depends on training retrievers using synthetic query-document pairs
  - Quick check question: How does the retriever training process use synthetic query-document pairs to align latent spaces?

- Concept: Zero-shot vs few-shot learning paradigms
  - Why needed here: EGG operates in the zero-shot regime but incorporates elements of few-shot learning through meta-prompts and prototype queries
  - Quick check question: What distinguishes zero-shot query generation from few-shot approaches in this context?

- Concept: Meta-prompting and instruction following in LLMs
  - Why needed here: EGG-FLAN relies on meta-prompts to instruct the LM about task-specific search intent without examples
  - Quick check question: How does a meta-prompt differ from a standard prompt in terms of task specification?

## Architecture Onboarding

- Component map: Document → Meta-prompt generation → Query generation → Synthetic dataset → Retriever training → Retrieval inference
- Critical path: Meta-prompt → Query generation → Retriever training
- Design tradeoffs: 
  - EGG-FLAN: Smaller, faster, no in-context learning but relies heavily on meta-prompt quality
  - EGG-L LAMA: Larger, slower, uses prototype queries + in-context learning but more computationally expensive
- Failure signatures: Poor query diversity, misalignment between generated queries and task intent, suboptimal retriever performance
- First 3 experiments:
  1. Generate 8 queries per document using EGG-FLAN meta-prompt on a small subset of documents
  2. Generate prototype queries and test in-context learning pipeline with EGG-L LAMA
  3. Train retrievers with both DPR and GPL methods on synthetic datasets and evaluate on target tasks

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the EGG approach generalize to datasets with mixed or ambiguous search intents, such as DBPedia and NFCorpus?
- Basis in paper: Explicit - The paper mentions these datasets as having mixed search intents in the Limitations section.
- Why unresolved: The authors only evaluated EGG on datasets with single, well-defined intents and did not explore its performance on datasets with ambiguous or mixed intents.
- What evidence would resolve it: Experimental results showing EGG's performance on datasets with mixed intents, compared to baseline methods and with different approaches to handling multiple intent candidates.

### Open Question 2
- Question: How does EGG's performance scale with different model sizes of FLAN-T5 and Llama2?
- Basis in paper: Inferred - The authors mention they focused on "commonly affordable sizes of LMs" and reserve exploration of other variants for future work.
- Why unresolved: The paper only tested EGG with FLAN-T5-XL (3B) and Llama2 (7B), leaving open questions about how larger or smaller models would perform.
- What evidence would resolve it: Experimental results comparing EGG's performance across multiple model sizes, showing performance curves and identifying optimal model sizes for different tasks.

### Open Question 3
- Question: How would incorporating a reranker instead of a retriever affect EGG's performance?
- Basis in paper: Explicit - The Limitations section mentions that using a reranker has demonstrated high performance in other work.
- Why unresolved: The authors only evaluated EGG with retrievers and acknowledged that rerankers might further enhance performance but did not test this.
- What evidence would resolve it: Experimental results comparing EGG with retriever-based and reranker-based approaches, including computational cost analysis and performance differences across tasks.

## Limitations

- The meta-prompt approach may struggle with complex or nuanced search tasks where intent is difficult to articulate
- Prototype query quality could vary significantly across different domains, affecting EGG-L LAMA's performance
- Experimental results are limited to four BeIR benchmark tasks, which may not generalize to all retrieval scenarios
- Computational cost comparison is based on LM size rather than actual training/fine-tuning resources used

## Confidence

- High confidence: EGG achieves state-of-the-art performance on BeIR benchmarks (nDCG@10 scores of 79.4 on Fever, 58.7 on Arguana, 16.9 on Scidocs, 40.0 on DBPedia)
- Medium confidence: The meta-prompt instruction approach effectively replaces few-shot examples for small LMs
- Medium confidence: Prototype queries + in-context learning improves relevance compared to random few-shot examples
- Low confidence: The query-question disentanglement concept will generalize to all non-QA search tasks

## Next Checks

1. **Meta-prompt robustness test**: Evaluate EGG-FLAN's performance across a broader range of task types with varying complexity of search intent descriptions to determine the limits of meta-prompt effectiveness

2. **Prototype quality analysis**: Conduct a human evaluation of prototype queries generated by meta-prompts to assess their relevance and representativeness across different domains

3. **Computational resource audit**: Measure actual training time, memory usage, and energy consumption for both EGG variants compared to previous approaches to validate the claimed 47x size reduction advantage