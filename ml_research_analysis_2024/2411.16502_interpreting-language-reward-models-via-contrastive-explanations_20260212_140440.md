---
ver: rpa2
title: Interpreting Language Reward Models via Contrastive Explanations
arxiv_id: '2411.16502'
source_url: https://arxiv.org/abs/2411.16502
tags:
- response
- responses
- explanations
- perturbed
- attributes
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work introduces a method for explaining language reward models
  (RMs) by generating contrastive explanations in the form of counterfactual and semifactual
  comparisons. The approach perturbs responses along high-level evaluation attributes
  like helpfulness and harmlessness, then categorizes the resulting comparisons to
  characterize local RM behavior.
---

# Interpreting Language Reward Models via Contrastive Explanations

## Quick Facts
- arXiv ID: 2411.16502
- Source URL: https://arxiv.org/abs/2411.16502
- Reference count: 21
- Method generates contrastive explanations for language reward models through counterfactual and semifactual comparisons

## Executive Summary
This paper introduces a method for interpreting language reward models by generating contrastive explanations through systematic perturbations of responses along high-level evaluation attributes. The approach characterizes local RM behavior by creating counterfactual (preference-flipping) and semifactual (preference-maintaining) comparisons, then aggregates these to reveal global sensitivity patterns. Quantitative experiments demonstrate the method achieves 68-90% counterfactual coverage while maintaining semantic similarity to original responses, and qualitative analysis uncovers both expected behaviors (like harmlessness sensitivity in models trained on safe datasets) and problematic patterns (such as preferences for disrespectful content).

## Method Summary
The method perturbs responses along 15 evaluation attributes using a two-step LLM prompting approach to generate counterfactual and semifactual examples that characterize local RM behavior. For each binary comparison (chosen vs rejected response), the method creates 15 perturbed versions of each response, categorizes them based on whether they flip the RM's preference, and aggregates these results to identify global sensitivity patterns. The approach uses a black-box RM interface, making it applicable to any reward model, and includes a workflow for automatically identifying representative examples that illustrate the RM's sensitivities.

## Key Results
- Achieves high counterfactual coverage (68-90%) across three datasets while maintaining semantic similarity to original responses
- Reveals global sensitivity patterns showing which evaluation attributes most influence RM decisions
- Identifies undesirable RM behaviors including preferences for disrespectful or harmful content in certain models
- Demonstrates that models trained on harmless datasets show higher sensitivity to harmlessness attributes

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Perturbing responses along high-level evaluation attributes generates both counterfactual and semifactual examples that characterize local RM behavior
- Mechanism: The method systematically modifies responses in directions that oppose the RM's original evaluation (making chosen responses worse and rejected responses better) while maintaining semantic similarity, creating a diverse set of comparisons that straddle the model's decision boundary
- Core assumption: The RM's local behavior can be effectively characterized by observing how it responds to perturbations along human-interpretable evaluation dimensions
- Evidence anchors:
  - [abstract] "generate a diverse set of new comparisons similar to the original one to characterise the RM's local behaviour"
  - [section 2.3] "perturbations computed by our method also have lower (better) distances to the original inputs than the baselines"
  - [corpus] Weak - corpus focuses on different RM approaches, not contrastive explanation mechanisms
- Break condition: If the RM exhibits highly non-linear behavior in its reward function, the perturbations may not effectively capture the true decision boundary

### Mechanism 2
- Claim: Global sensitivity patterns emerge from aggregating local contrastive explanations across many comparisons
- Mechanism: By counting how frequently perturbing along each attribute causes the RM to flip its preference (counterfactual) versus maintain it (semifactual), the method quantifies which attributes the RM is most sensitive to globally
- Core assumption: Local contrastive explanations can be meaningfully aggregated to reveal global behavioral patterns of the RM
- Evidence anchors:
  - [abstract] "aggregate CFs and SFs over multiple comparisons, as similarly done in other settings such as global recourse"
  - [section 4.1] "aggregating these labels over many test comparisons gives the RM's global sensitivity to each evaluation attribute"
  - [corpus] Moderate - some papers mention benchmarking but not the aggregation methodology described here
- Break condition: If the RM's behavior is highly context-dependent, aggregating across diverse prompts may obscure important local variations

### Mechanism 3
- Claim: Representative examples can be automatically identified by matching local perturbation rankings to global sensitivity patterns
- Mechanism: For each comparison, the method ranks attributes by how much they change the RM's reward, then finds examples where this local ranking best matches the global sensitivity ranking, providing illustrative cases of typical RM behavior
- Core assumption: The ranking of attribute importance at the local level should correlate with global sensitivity patterns
- Evidence anchors:
  - [abstract] "present a workflow for automatically finding examples that illustrate these sensitivities"
  - [section 4.2] "We calculate the ranking similarity (Kendall's τ correlation coefficient) between the global and local rankings"
  - [corpus] Missing - corpus does not contain evidence for this specific matching mechanism
- Break condition: If the RM exhibits inconsistent behavior across different contexts, the correlation between local and global rankings may be weak

## Foundational Learning

- Concept: Binary response comparison as classification task
  - Why needed here: This framing enables the application of contrastive explanation techniques that were originally developed for classification problems
  - Quick check question: How does treating "response A is better than response B" as a binary classification enable the use of counterfactual explanations?

- Concept: Counterfactual vs semifactual explanations
  - Why needed here: Understanding this distinction is crucial for categorizing perturbations and interpreting what they reveal about the RM's decision-making
  - Quick check question: What is the key difference between a counterfactual and semifactual explanation in the context of RM preference predictions?

- Concept: High-level evaluation attributes for text perturbation
  - Why needed here: These attributes provide a structured way to generate diverse, meaningful perturbations that are interpretable to humans
  - Quick check question: Why is it important that perturbations are generated along specific evaluation attributes rather than random text changes?

## Architecture Onboarding

- Component map: Prompt → Perturbation generation → RM evaluation → Categorization → Analysis
- Critical path: Prompt → Perturbation generation → RM evaluation → Categorization → Analysis
- Design tradeoffs:
  - Perturbation diversity vs semantic similarity: More diverse perturbations provide better coverage but may be less similar to original responses
  - Number of attributes vs computational cost: More attributes provide richer analysis but increase computation
  - LLM choice vs perturbation quality: Different LLMs may generate perturbations of varying quality
- Failure signatures:
  - Low CF coverage indicates the method struggles to find perturbations that flip preferences
  - High semantic distance suggests perturbations are too different from original responses
  - Inconsistent local-global ranking correlation suggests context-dependent RM behavior
- First 3 experiments:
  1. Validate perturbation quality by comparing CF/SF coverage and semantic distances against baselines
  2. Test global sensitivity extraction by comparing known behaviors (e.g., v2 trained on harmless dataset should be sensitive to harmlessness)
  3. Verify representative example identification by checking correlation between local rankings and global sensitivity patterns

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the choice of evaluation attributes affect the quality and coverage of counterfactual and semifactual explanations?
- Basis in paper: [explicit] The paper discusses a list of 15 evaluation attributes (avoid-to-answer, appropriateness, assertiveness, clarity, coherence, complexity, correctness, engagement, harmlessness, helpfulness, informativeness, neutrality, relevance, sensitivity, verbosity) used for generating perturbations.
- Why unresolved: The paper does not systematically investigate how different attribute choices or attribute combinations impact explanation quality, coverage, or the types of insights gained.
- What evidence would resolve it: Systematic ablation studies comparing explanation quality across different attribute sets, or empirical analysis showing which attributes yield the most informative counterfactuals/semifactuals for different RM behaviors.

### Open Question 2
- Question: To what extent can contrastive explanations generalize across different reward models and datasets?
- Basis in paper: [inferred] The paper tests three RMs on three datasets and shows similar global sensitivity patterns across models, but does not explicitly test generalization capabilities.
- Why unresolved: The paper demonstrates that similar patterns exist across RMs but does not test whether explanations generated for one RM/dataset can be applied to explain other RMs/datasets effectively.
- What evidence would resolve it: Cross-RM validation experiments where explanations generated for one RM are tested on others, or transfer learning approaches where explanations from one dataset improve explanation quality on new datasets.

### Open Question 3
- Question: Can the contrastive explanation framework be extended to handle multi-turn conversations rather than single-turn conversations?
- Basis in paper: [explicit] The paper states "We focus on explaining single-turn conversations in our experiments" and does not address multi-turn scenarios.
- Why unresolved: The paper explicitly limits itself to single-turn conversations and does not explore how the framework would need to be adapted for sequential, context-dependent conversations.
- What evidence would resolve it: Experiments demonstrating the framework's effectiveness on multi-turn dialogue datasets, or theoretical analysis of how conversational context affects the perturbation and explanation generation process.

## Limitations

- The method's effectiveness depends on the quality of LLM-generated perturbations, which are not fully specified
- Global sensitivity aggregation may obscure context-dependent behaviors when RM preferences vary across different prompts
- The framework is limited to single-turn conversations and does not address multi-turn dialogue scenarios

## Confidence

- High confidence: The perturbation method successfully generates diverse contrastive examples (validated by quantitative coverage metrics)
- Medium confidence: Global sensitivity patterns are meaningfully extracted from local perturbations (methodology is sound but context-dependency is a concern)
- Low confidence: Representative example identification reliably matches local to global patterns (correlation mechanism is described but not extensively validated)

## Next Checks

1. Conduct ablation studies removing different subsets of evaluation attributes to determine which are most critical for generating valid CFs/SFs
2. Test the method on RMs with known adversarial behaviors to verify it can detect these cases through contrastive explanations
3. Compare the global sensitivity patterns extracted by this method against human annotations of RM behavior to validate the aggregation approach