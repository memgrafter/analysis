---
ver: rpa2
title: Rethinking Targeted Adversarial Attacks For Neural Machine Translation
arxiv_id: '2407.05319'
source_url: https://arxiv.org/abs/2407.05319
tags:
- targeted
- adversarial
- attack
- translation
- setting
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper identifies a critical issue in existing NMT targeted
  adversarial attack settings, where their attacking results are largely overestimated
  due to invalid adversarial examples that break the label-preserving assumption.
  To address this, the authors propose a new setting that only allows perturbations
  to non-targeted word tokens and requires crafted adversarial examples to be fluent
  and meaningful.
---

# Rethinking Targeted Adversarial Attacks For Neural Machine Translation

## Quick Facts
- arXiv ID: 2407.05319
- Source URL: https://arxiv.org/abs/2407.05319
- Authors: Junjie Wu; Lemao Liu; Wei Bi; Dit-Yan Yeung
- Reference count: 0
- Primary result: Introduces TWGA method achieving 82.82% and 72.91% attack success rates on LSTM and Transformer models

## Executive Summary
This paper identifies a critical flaw in existing neural machine translation (NMT) targeted adversarial attack settings where attack results are significantly overestimated due to invalid adversarial examples that violate the label-preserving assumption. The authors propose a new attack setting that restricts perturbations to non-targeted word tokens and requires adversarial examples to remain fluent and meaningful. They introduce TWGA, a white-box attack method that uses an optimized adversarial distribution to generate high-quality adversarial examples under these stricter constraints.

## Method Summary
The paper addresses the problem of invalid adversarial examples in NMT targeted attacks by proposing a new attack setting where perturbations are only allowed on non-targeted tokens, and crafted examples must maintain fluency and semantic meaning. The TWGA method employs an optimized adversarial distribution strategy that iteratively refines perturbations while ensuring the generated examples remain both effective and natural-sounding. This approach contrasts with previous methods that often produced adversarial examples breaking the label-preserving assumption, leading to overestimated attack success rates.

## Key Results
- TWGA achieves 82.82% attack success rate on LSTM models and 72.91% on Transformer models
- The method significantly outperforms existing attack approaches under the new restricted setting
- First large-scale analysis of NMT targeted adversarial attacks provides new insights into attack effectiveness and limitations

## Why This Works (Mechanism)
The paper's approach works by fundamentally restructuring the attack problem space. By restricting perturbations to non-targeted tokens and enforcing fluency requirements, TWGA operates within a more realistic and constrained optimization space. The optimized adversarial distribution allows for more effective gradient-based perturbations while maintaining the semantic integrity of the translation. This targeted constraint actually improves attack effectiveness by preventing the generation of obviously invalid or nonsensical examples that would be easily detected or fail to transfer properly.

## Foundational Learning

1. **Label-preserving assumption**: Why needed - Ensures adversarial examples remain semantically equivalent to original inputs. Quick check - Verify that small perturbations don't change the intended meaning of translations.

2. **White-box attack methodology**: Why needed - Provides full access to model gradients for optimal perturbation generation. Quick check - Confirm that the attack can compute and utilize model-specific gradients effectively.

3. **Adversarial distribution optimization**: Why needed - Enables efficient search through perturbation space while maintaining quality constraints. Quick check - Validate that the distribution converges to effective perturbations within reasonable iterations.

## Architecture Onboarding

**Component Map**: Input text -> Non-targeted token identification -> Optimized adversarial distribution -> Perturbation application -> Fluent adversarial example generation -> NMT model evaluation

**Critical Path**: The core workflow involves identifying non-targeted tokens, computing optimized perturbations through the adversarial distribution, applying these perturbations while maintaining fluency constraints, and evaluating against the NMT model to achieve the targeted attack goal.

**Design Tradeoffs**: The restricted perturbation setting limits the attack space but ensures higher quality adversarial examples. This tradeoff between attack power and example validity represents a fundamental shift from previous approaches that prioritized attack success over example quality.

**Failure Signatures**: Attack failures occur when: 1) No valid perturbations can be found within the constrained space, 2) Generated examples fail fluency requirements despite successful gradient optimization, or 3) The NMT model's robustness prevents successful targeted attacks even with optimal perturbations.

**3 First Experiments**:
1. Test TWGA on simple sentence pairs with clear non-targeted tokens to verify basic functionality
2. Compare attack success rates on LSTM vs Transformer models with identical input pairs
3. Evaluate the impact of different fluency constraints on attack effectiveness and example quality

## Open Questions the Paper Calls Out
None specified in the source material.

## Limitations
- The foundational claim about overestimation lacks quantitative evidence of prevalence in existing literature
- Performance comparison is limited to the new restricted setting rather than original unrestricted settings
- Results are based on only two translation datasets, limiting generalizability across language pairs

## Confidence
**High confidence**: The theoretical identification of label-preserving assumption violations as a problem; technical soundness of the restricted perturbation approach; demonstration of adversarial example generation under new constraints.

**Medium confidence**: Quantitative claims about overestimation prevalence without supporting data; comparative advantage under original attack settings; generalizability across different NMT architectures.

**Low confidence**: Practical implications without prevalence data; robustness against potential defenses.

## Next Checks
1. Conduct empirical audit of existing NMT targeted attack literature to quantify prevalence of label-preserving violations
2. Evaluate TWGA under original unrestricted attack settings used by prior work to compare performance under identical constraints
3. Test TWGA's transferability across additional language pairs and NMT architectures beyond the two tested datasets