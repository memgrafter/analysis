---
ver: rpa2
title: 'Multimodal Approaches to Fair Image Classification: An Ethical Perspective'
arxiv_id: '2412.12165'
source_url: https://arxiv.org/abs/2412.12165
tags:
- image
- classification
- images
- these
- photo
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This thesis addresses harmful biases in image classification systems
  by developing two techniques: MuSE (Multimodal Synthetic Embeddings) and D3G (Diverse
  Demographic Data Generation). MuSE improves accuracy and generalizability of multimodal
  models by generating synthetic images and combining them with text embeddings at
  inference time, achieving up to 3% accuracy gains across diverse datasets like Flowers
  102 and FGVC Aircraft.'
---

# Multimodal Approaches to Fair Image Classification: An Ethical Perspective

## Quick Facts
- arXiv ID: 2412.12165
- Source URL: https://arxiv.org/abs/2412.12165
- Reference count: 0
- Primary result: Training-free multimodal methods achieve 3-7% accuracy gains and reduce demographic bias in image classification

## Executive Summary
This thesis presents two training-free, zero-shot methods for improving fairness and accuracy in image classification: MuSE (Multimodal Synthetic Embeddings) and D3G (Diverse Demographic Data Generation). MuSE generates synthetic images and combines them with text embeddings at inference time, achieving up to 3% accuracy gains across diverse datasets. D3G reduces demographic bias by generating diverse demographic images during classification, improving accuracy by 4-7% on race classification tasks while maintaining high performance on other demographics. Both methods leverage the strengths of multimodal models to address harmful biases present in training data.

## Method Summary
MuSE creates weighted combinations of text and generated image embeddings to improve classification accuracy, while D3G generates diverse demographic images to reduce bias in multimodal models. Both methods use CLIP for text-image retrieval and Stable Diffusion XL for image generation, operating entirely in a zero-shot setting without requiring additional training. The methods generate synthetic data to fill gaps in cross-modal representations, with MuSE optimizing weights for classification accuracy and D3G focusing on demographic diversity through weighted averaging of generated images.

## Key Results
- MuSE achieves up to 3% accuracy gains across Flowers 102, DTD, RESISC45, and FGVC Aircraft datasets
- D3G improves race classification accuracy by 4-7% while maintaining performance on other demographics
- Both methods demonstrate training-free, zero-shot capability with minimal computational overhead

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Multimodal models can reduce harmful bias by leveraging diverse demographic data at inference time
- Mechanism: D3G generates images representing diverse demographics for each class and combines them with text embeddings through weighted averaging. This approach shifts the embedding space to better represent underrepresented groups, improving classification accuracy for biased demographics
- Core assumption: The image generation model can produce realistic, stereotype-free images of diverse demographics
- Evidence anchors:
  - [abstract] "D3G reduces demographic bias by generating diverse demographic images during classification, improving accuracy by 4-7% on race classification tasks"
  - [section 3.3.3] "By simply implementing D3G, we are able to push the accuracies up to by 4-7%"
  - [corpus] Weak evidence - related papers focus on fairness evaluation but don't provide direct evidence for D3G's mechanism
- Break condition: If generated images contain stereotypes or fail to represent true demographic diversity, the method may accentuate rather than reduce bias

### Mechanism 2
- Claim: Weighted combination of text and generated image embeddings can improve classification accuracy
- Mechanism: MuSE creates a weighted sum of text embeddings and generated image embeddings, where the weight is optimized to maximize classification accuracy. This allows the model to leverage the strengths of both modalities
- Core assumption: The multimodal model can distinguish between similar classes when provided with both text and image embeddings
- Evidence anchors:
  - [abstract] "MuSE improves accuracy and generalizability of multimodal models by generating synthetic images and combining them with text embeddings"
  - [section 2.5.3] "MuSE improves performance across all datasets; however, another important feature... two of the evaluations with MuSE Standard where w = 0.1, ended up being the optimal configuration"
  - [corpus] Moderate evidence - papers on model ensembling support the general approach of combining predictions
- Break condition: If the multimodal model cannot distinguish between similar classes, the weighted sum will not improve accuracy

### Mechanism 3
- Claim: Rich cross-modal representations are essential for fair and accurate classification
- Mechanism: Both MuSE and D3G rely on the principle that multimodal models need diverse, high-quality training data with rich cross-modal representations. When these representations are lacking for certain demographics or classes, the methods generate synthetic data to fill the gap
- Core assumption: The multimodal model's performance is limited by the diversity and quality of its training data
- Evidence anchors:
  - [abstract] "Machine Learning systems that depend on a single data modality... can exaggerate hidden biases present in the training data"
  - [section 2.5.5] "CLIP baseline received 0% accuracy on classifying this particular flower... CLIP had very little cross modal representations of the Bishop of Llandaff"
  - [corpus] Weak evidence - related papers discuss data balancing but don't directly address cross-modal representation quality
- Break condition: If the multimodal model already has rich cross-modal representations for all classes, the synthetic data generation may not provide additional benefit

## Foundational Learning

- Concept: Cross-modal representation learning
  - Why needed here: Understanding how models learn associations between visual and textual data is crucial for grasping why MuSE and D3G work
  - Quick check question: What is the key difference between unimodal and multimodal models in terms of data representation?

- Concept: Bias in machine learning
  - Why needed here: The paper addresses demographic bias, so understanding different types of bias and their sources is essential
  - Quick check question: How does class imbalance in training data lead to biased predictions?

- Concept: Zero-shot learning
  - Why needed here: Both methods operate in a zero-shot setting, meaning they don't require additional training
  - Quick check question: What distinguishes zero-shot learning from few-shot or traditional supervised learning?

## Architecture Onboarding

- Component map:
  CLIP ViT-L/14 -> Stable Diffusion XL -> CuPL -> Weighted sum module -> Classification module

- Critical path:
  1. Generate descriptive prompts for each class using CuPL
  2. Create synthetic images using Stable Diffusion XL
  3. Extract embeddings from both text and images
  4. Find optimal weight for combining embeddings
  5. Classify query images using weighted embeddings

- Design tradeoffs:
  - Speed vs. accuracy: Generating synthetic images adds overhead but improves accuracy
  - Bias vs. representation: Generated images may contain stereotypes but improve demographic coverage
  - Complexity vs. generalizability: More complex prompts improve fine-grained classification but may not generalize well

- Failure signatures:
  - No accuracy improvement: Generated images don't match dataset distribution
  - Decreased accuracy: Generated images contain harmful stereotypes
  - High weight on text: Generated images are low quality or irrelevant
  - Low weight on text: Text embeddings are already highly informative

- First 3 experiments:
  1. Test MuSE on Flowers 102 with "A photo of a <classname>" text to establish baseline improvement
  2. Test D3G on IdenProf race classification to measure demographic bias reduction
  3. Compare MuSE performance with different numbers of generated images (1 vs. 5) to find optimal configuration

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the weighting strategy based on demographic proportions in the training data actually improve fairness metrics compared to uniform weighting?
- Basis in paper: [inferred] The paper mentions in Section 4.1 that they aim to explore creating a weighted sum of embeddings informed by demographics of the training data, noting that "if CLIP tends to favor one demographic, then we will down weight those images, and vice-versa if CLIP rarely selects another demographic."
- Why unresolved: The authors explicitly state this as future work and have not yet tested this weighted approach versus uniform averaging of generated image embeddings.
- What evidence would resolve it: Running experiments comparing standard D3G (uniform weighting) versus demographic-proportion-weighted D3G on fairness metrics like demographic parity or equal opportunity across multiple datasets.

### Open Question 2
- Question: Can modifying the query image in-place to include diverse demographic features achieve similar accuracy improvements as generating separate diverse images?
- Basis in paper: [explicit] Section 4.1 states "we also aim to explore methods of... modifying the demographics of the query image in-place" as a potential way to reduce stereotype reinforcement from generated images.
- Why unresolved: This represents a future direction mentioned by the authors that has not been implemented or tested yet.
- What evidence would resolve it: Implementing an image modification pipeline that adjusts demographic features of the input image and comparing classification accuracy and bias metrics against the current D3G approach.

### Open Question 3
- Question: How does D3G performance vary when tested on datasets with different class imbalance structures and demographic distributions?
- Basis in paper: [inferred] The authors note in Section 4.1 they want to "expand our evaluation suite to multiple datasets" and mention testing on FairFace or Labelled Faces in the Wild, suggesting current results are limited to IdenProf.
- Why unresolved: All D3G results presented are on a single dataset (IdenProf), so generalizability to other data distributions is unknown.
- What evidence would resolve it: Running D3G experiments on multiple diverse datasets with varying class imbalances and demographic distributions, measuring both accuracy and fairness metrics across each dataset.

## Limitations

- Generated images may contain stereotypes that could accentuate rather than reduce bias if the image generation model is not carefully controlled
- Optimal weighting strategies for combining text and image embeddings are empirically determined rather than theoretically derived
- Methods show variable effectiveness across different datasets and may not generalize equally well to all domains

## Confidence

**High Confidence:**
- The core claim that multimodal approaches can improve classification accuracy is well-supported by experimental results showing 3-7% gains across multiple datasets
- The mechanism of combining text and image embeddings through weighted sums is technically sound and aligns with established multimodal learning principles
- The zero-shot, training-free nature of both methods is clearly demonstrated and reproducible

**Medium Confidence:**
- The effectiveness of D3G in reducing demographic bias is supported by accuracy improvements on IdenProf but lacks extensive validation across diverse demographic categories and real-world datasets
- The assumption that generated images will be stereotype-free is acknowledged as a limitation but not rigorously tested or quantified
- The generalizability of the methods beyond the tested datasets remains uncertain without broader validation

**Low Confidence:**
- The claim that cross-modal representation quality is the primary limiting factor for multimodal model performance is not thoroughly validated against alternative explanations such as model architecture limitations
- The long-term implications of using synthetic data for fairness improvement are not explored, particularly regarding potential data drift or model staleness

## Next Checks

1. **Stereotype Audit of Generated Images:**
   Conduct a systematic audit of images generated by Stable Diffusion XL for demographic attributes (race, gender, age, profession) to quantify the presence of stereotypical representations. This would validate the assumption that generated images are stereotype-free and identify specific demographic categories where bias may be introduced.

2. **Cross-Dataset Generalization Test:**
   Evaluate MuSE and D3G on a more diverse set of real-world datasets with known demographic biases (e.g., medical imaging datasets, social media image datasets) to assess whether the 3-7% accuracy improvements generalize beyond the controlled experimental conditions of the thesis.

3. **Fairness Metric Expansion:**
   Implement and report additional fairness metrics beyond top-1 accuracy, including demographic parity, equal opportunity difference, and individual fairness scores. This would provide a more comprehensive evaluation of whether the methods truly reduce harmful bias rather than just improving overall accuracy.