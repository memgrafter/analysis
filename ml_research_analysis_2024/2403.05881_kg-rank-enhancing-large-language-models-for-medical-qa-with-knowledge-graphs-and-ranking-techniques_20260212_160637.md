---
ver: rpa2
title: 'KG-Rank: Enhancing Large Language Models for Medical QA with Knowledge Graphs
  and Ranking Techniques'
arxiv_id: '2403.05881'
source_url: https://arxiv.org/abs/2403.05881
tags:
- medical
- kg-rank
- ranking
- question
- knowledge
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces KG-Rank, a framework that integrates medical
  knowledge graphs with ranking techniques to improve the factual consistency of large
  language models (LLMs) in medical question answering. The method extracts medical
  entities from questions, retrieves relevant triples from the UMLS knowledge graph,
  and applies ranking and re-ranking techniques to refine the information provided
  to the LLM.
---

# KG-Rank: Enhancing Large Language Models for Medical QA with Knowledge Graphs and Ranking Techniques

## Quick Facts
- arXiv ID: 2403.05881
- Source URL: https://arxiv.org/abs/2403.05881
- Reference count: 25
- Key outcome: KG-Rank improves ROUGE-L score by over 18% on four medical datasets and 14% in open-domain applications.

## Executive Summary
KG-Rank is a framework that integrates medical knowledge graphs with ranking techniques to enhance the factual consistency of large language models (LLMs) in medical question answering. By extracting medical entities from questions, retrieving relevant triples from the UMLS knowledge graph, and applying ranking and re-ranking techniques, KG-Rank significantly improves the relevance and factuality of LLM-generated answers. The framework also demonstrates effectiveness in open-domain applications by adapting to general knowledge graphs like Wikipedia via DBpedia.

## Method Summary
KG-Rank is a framework that integrates medical knowledge graphs with ranking techniques to improve the factual consistency of large language models (LLMs) in medical question answering. The method extracts medical entities from questions, retrieves relevant triples from the UMLS knowledge graph, and applies ranking and re-ranking techniques to refine the information provided to the LLM. KG-Rank demonstrates significant improvements in ROUGE-L score (over 18% on four medical datasets) and BERTScore, indicating enhanced factuality and relevance of generated answers. The framework is also effective in open-domain applications, achieving a 14% improvement in ROUGE-L score.

## Key Results
- KG-Rank achieves over 18% improvement in ROUGE-L score on four medical QA datasets.
- The framework also realizes a 14% improvement in ROUGE-L score in open-domain applications.
- BERTScore and other metrics confirm enhanced factuality and relevance of generated answers.

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Medical knowledge graphs improve factual consistency in LLM-generated answers by grounding responses in validated medical triples.
- **Mechanism:** KG-Rank extracts medical entities from questions, retrieves one-hop relations from UMLS, and feeds ranked triples to LLMs for context-aware generation.
- **Core assumption:** Medical triples from UMLS accurately reflect current clinical knowledge and are relevant to the question context.
- **Evidence anchors:**
  - [abstract] "KG-Rank achieves an improvement of over 18% in ROUGE-L score" on medical QA datasets.
  - [section 2.3] "We then propose ranking methods... to facilitate the extraction of the most relevant."
- **Break condition:** If retrieved triples are outdated, incorrect, or irrelevant, factual consistency degrades despite ranking.

### Mechanism 2
- **Claim:** Ranking and re-ranking of knowledge graph triples enhances the relevance and diversity of information provided to the LLM.
- **Mechanism:** Three ranking strategies (similarity, answer expansion, MMR) prioritize clinically relevant triples; a cross-encoder re-ranker further filters for precision.
- **Core assumption:** Semantic similarity and MMR-based diversity can effectively prioritize clinically useful triples over redundant or off-topic ones.
- **Evidence anchors:**
  - [section 2.3] "We then employ a medical cross-encoder model, MedCPT... to re-rank them, ensuring that the most relevant triples are chosen."
  - [abstract] "KG-Rank innovatively applies multiple ranking techniques to refine the ordering of these triples."
- **Break condition:** If ranking scores poorly reflect true relevance, the LLM may still generate off-topic or contradictory answers.

### Mechanism 3
- **Claim:** Domain adaptation (medical vs. open-domain) preserves KG-Rank’s effectiveness across knowledge domains.
- **Mechanism:** Replacing UMLS with Wikipedia/DBpedia and adapting prompts enables similar gains in non-medical QA tasks.
- **Core assumption:** General knowledge graphs (e.g., Wikipedia) provide sufficient context for LLM reasoning in open domains.
- **Evidence anchors:**
  - [abstract] "we extend KG-Rank to open domains... where it realizes a 14% improvement in ROUGE-L score."
  - [section 4] "we extend it to the open domain by replacing UMLS with Wikipedia through the DBpedia API."
- **Break condition:** If the knowledge graph lacks depth in a target domain, ranking and re-ranking cannot recover missing context.

## Foundational Learning

- **Concept:** Entity extraction and mapping from text to knowledge graph nodes
  - **Why needed here:** KG-Rank must translate natural language medical questions into formal medical entities to query UMLS effectively.
  - **Quick check question:** Given "A 56 year old male with atrial fibrillation," what UMLS entities should KG-Rank extract?
- **Concept:** Ranking functions (similarity scoring, MMR, answer expansion)
  - **Why needed here:** Multiple ranking strategies are used to filter, diversify, and prioritize triples before LLM inference.
  - **Quick check question:** How does MMR differ from simple similarity ranking in selecting knowledge graph triples?
- **Concept:** Cross-encoder re-ranking models for domain-specific precision
  - **Why needed here:** A general re-ranker (e.g., Cohere) underperforms a medical cross-encoder (MedCPT), showing the importance of domain alignment.
  - **Quick check question:** Why does MedCPT outperform Cohere in re-ranking medical triples?

## Architecture Onboarding

- **Component map:** Medical NER → Entity mapping → UMLS triple retrieval → Ranking (Sim/AE/MMR) → Re-ranking (MedCPT) → LLM prompt + answer generation
- **Critical path:** Entity extraction → Triple retrieval → Ranking → Re-ranking → LLM generation
- **Design tradeoffs:**
  - Speed vs. precision: ranking and re-ranking add latency but improve answer quality.
  - Knowledge graph choice: UMLS is rich but domain-locked; Wikipedia/DBpedia are broader but less precise.
  - Model size: Using GPT-4 vs. LLaMa2 affects performance and cost.
- **Failure signatures:**
  - Poor entity extraction → No triples retrieved → No improvement over baseline.
  - Low-quality ranking → Irrelevant triples → Factually incorrect answers.
  - Weak re-ranker → Noisy triples → Degraded answer coherence.
- **First 3 experiments:**
  1. Test entity extraction accuracy on a held-out set of medical questions.
  2. Compare the three ranking methods on a subset of triples for relevance ranking.
  3. Evaluate MedCPT vs. Cohere re-rankers on a fixed set of ranked triples for factual accuracy gains.

## Open Questions the Paper Calls Out
None explicitly called out in the paper.

## Limitations
- The study does not provide a detailed error analysis of when KG-Rank fails, such as cases where UMLS triples are incomplete or contradictory, which could mislead the LLM despite ranking.
- The effectiveness of the ranking strategies (Similarity, Answer Expansion, MMR) is not individually validated, making it unclear which contributes most to the performance gains.
- While open-domain adaptation shows promise, the 14% ROUGE-L improvement is reported without comparison to domain-specific knowledge graphs that might be more appropriate for certain open-domain tasks.

## Confidence

**High:** The reported ROUGE-L and BERTScore improvements on medical datasets are supported by clear quantitative results and direct comparisons to baseline models.

**Medium:** The claim of cross-domain applicability is plausible but relies on substituting UMLS with DBpedia, which may not always provide equivalent depth or accuracy.

**Low:** The lack of ablation studies on ranking components and the absence of qualitative analysis of failure cases reduce confidence in understanding the full scope of KG-Rank's limitations.

## Next Checks

1. Conduct an ablation study to isolate the contribution of each ranking method (Similarity, Answer Expansion, MMR) to the overall performance improvement.
2. Perform a qualitative error analysis on KG-Rank outputs to identify patterns of failure, such as outdated or contradictory triples from UMLS, and propose mitigation strategies.
3. Test KG-Rank on open-domain datasets with domain-specific knowledge graphs (e.g., scientific literature for technical QA) to evaluate whether the DBpedia substitution consistently yields similar gains.