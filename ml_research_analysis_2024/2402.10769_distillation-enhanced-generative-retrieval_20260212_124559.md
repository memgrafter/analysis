---
ver: rpa2
title: Distillation Enhanced Generative Retrieval
arxiv_id: '2402.10769'
source_url: https://arxiv.org/abs/2402.10769
tags:
- retrieval
- generative
- teacher
- passages
- distillation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of incomplete relevance judgments
  in generative retrieval, where only a few passages are judged for each query, and
  the judgments are typically binary. The authors propose a distillation framework,
  DGR, to enhance generative retrieval by leveraging sophisticated ranking models
  as teachers.
---

# Distillation Enhanced Generative Retrieval

## Quick Facts
- arXiv ID: 2402.10769
- Source URL: https://arxiv.org/abs/2402.10769
- Reference count: 15
- Primary result: DGR achieves state-of-the-art performance among generative retrieval methods with a 3.78% relative improvement in hits@5 on NQ.

## Executive Summary
This paper addresses the challenge of incomplete relevance judgments in generative retrieval, where only a few passages are judged for each query, and the judgments are typically binary. The authors propose a distillation framework, DGR, to enhance generative retrieval by leveraging sophisticated ranking models as teachers. DGR uses the rank order of passages provided by the teacher model as knowledge and employs a custom-designed distilled RankNet loss to optimize the generative retrieval model. Experiments on four datasets demonstrate that DGR achieves state-of-the-art performance among generative retrieval methods, with a 3.78% relative improvement in hits@5 on NQ compared to the previous best method. DGR also exhibits exceptional robustness and generalizability with various teacher models and distillation losses.

## Method Summary
DGR enhances generative retrieval by incorporating knowledge distillation from sophisticated ranking models (teachers) to provide fine-grained relevance signals beyond binary labels. The framework uses the rank order of passages provided by the teacher model as supervision, employing a custom distilled RankNet loss that incorporates margin-based pairwise ranking with list-wise optimization. During training, the generative retrieval model retrieves passages, which are then re-ranked by the teacher model to create a rank list. The distilled RankNet loss ensures the student model's ranking matches the teacher's rank order with incremental margins based on relative positions. Inference remains unchanged from base generative retrieval, maintaining efficiency while improving retrieval performance.

## Key Results
- DGR achieves state-of-the-art performance among generative retrieval methods with a 3.78% relative improvement in hits@5 on NQ.
- The framework demonstrates exceptional robustness and generalizability across various teacher models and distillation losses.
- DGR effectively addresses the challenge of incomplete relevance judgments by leveraging fine-grained rank information from teacher models.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: DGR improves generative retrieval by using teacher models to provide fine-grained relevance signals through ranked passage lists.
- Mechanism: The teacher model (e.g., cross-encoder) re-ranks the passages retrieved by the student model, creating a rank list that reflects varying relevance degrees. This rank list is then used as supervision for the student model via the distilled RankNet loss, which incorporates pairwise ranking with incremental margins based on teacher rank positions.
- Core assumption: The rank order from the teacher model captures more nuanced relevance information than binary labels, and the generative retrieval model can effectively learn from these soft labels.
- Evidence anchors:
  - [abstract]: "DGR utilizes sophisticated ranking models, such as the cross-encoder, in a teacher role to supply a passage rank list, which captures the varying relevance degrees of passages instead of binary hard labels"
  - [section]: "We propose using the rank order of the passages provided by the teacher model as the distilled knowledge. ... The rank order given by the teacher model contains fine-grained supervisor signals, which reflect the varying relevance degrees rather than simple binary labels."
  - [corpus]: Weak evidence; no direct citation, but the claim aligns with the paper's experimental results showing improved performance over binary-label baselines.
- Break condition: If the teacher model's rank list does not accurately reflect true relevance (e.g., due to poor teacher quality), the distillation signal becomes noisy and degrades performance.

### Mechanism 2
- Claim: The distilled RankNet loss effectively incorporates teacher rank knowledge by introducing margin-based pairwise ranking that respects the teacher's ranking order.
- Mechanism: For each pair of passages in the teacher's rank list, the loss ensures the student's score difference matches the relative ranking, with margins increasing for passages further apart in the teacher's list. This encourages the student to learn a ranking function that mimics the teacher's nuanced judgments.
- Core assumption: The margin-based pairwise loss with list-wise optimization can effectively transfer the teacher's ranking knowledge to the generative retrieval model.
- Evidence anchors:
  - [section]: "We propose the distilled RankNet loss to better suit the DGR framework, as it can effectively leverage the rank order provided by the teacher model... Our distilled RankNet loss guarantees list-wise optimization under the guidance of the teacher model."
  - [abstract]: "DGR employs a specially designed distilled RankNet loss to optimize the generative retrieval model, considering the passage rank order provided by the teacher model as labels."
  - [corpus]: Weak evidence; no direct citation, but the claim is supported by ablation studies in Table 4 showing distilled RankNet outperforms other distillation losses.
- Break condition: If the margin values (mbase, mgap) are poorly tuned, the loss may either be too weak to learn meaningful ranking or too strong causing overfitting to teacher's noisy rankings.

### Mechanism 3
- Claim: DGR maintains inference efficiency by only adding a distillation step during training, without altering the generative retrieval model's architecture or inference procedure.
- Mechanism: The distillation framework trains the generative retrieval model to better rank passages by leveraging teacher knowledge, but the inference remains identical to the base generative retrieval: generating identifier strings and mapping them to passages via a heuristic function.
- Core assumption: The improvements in ranking during training translate to better retrieval performance at inference without requiring additional computational overhead.
- Evidence anchors:
  - [abstract]: "This framework only requires an additional distillation step to enhance current generative retrieval systems and does not add any burden to the inference stage."
  - [section]: "During inference, we use the trained model to retrieve passages as in the typical generative retrieval. Therefore, the DGR framework only requires an additional distillation step and does not add any burden to the inference stage."
  - [corpus]: Weak evidence; no direct citation, but the claim is consistent with the paper's experimental setup where inference speed is comparable to the base model.
- Break condition: If the distillation process significantly alters the model's behavior, it may require architectural changes at inference, violating the efficiency claim.

## Foundational Learning

- Concept: Knowledge distillation in neural networks
  - Why needed here: Understanding how teacher models transfer knowledge to student models via loss functions is crucial for grasping how DGR leverages cross-encoders to improve generative retrieval.
  - Quick check question: What is the difference between knowledge distillation and supervised learning with labeled data?

- Concept: Ranking loss functions (pairwise vs. list-wise)
  - Why needed here: DGR uses a custom distilled RankNet loss that combines pairwise ranking with list-wise optimization, so understanding these concepts is essential for interpreting the method's effectiveness.
  - Quick check question: How does RankNet differ from list-wise ranking losses like ListNet or ListMLE?

- Concept: Generative retrieval vs. dense retrieval paradigms
  - Why needed here: DGR bridges these paradigms by using generative models (which generate identifiers) and distilling knowledge from rank models (common in dense retrieval), so understanding their differences is key to appreciating the innovation.
  - Quick check question: What are the main architectural differences between generative retrieval and dense retrieval?

## Architecture Onboarding

- Component map:
  - Student model: Autoregressive generative retrieval (e.g., MINDER) that generates passage identifiers from queries.
  - Teacher model: Sophisticated rank model (e.g., cross-encoder like SimLM) that re-ranks passages based on query-passage interaction.
  - Distillation loss: Custom distilled RankNet loss that incorporates teacher rank list with margin-based pairwise ranking.
  - Training pipeline: Standard generative retrieval training + distillation step using teacher re-ranked passages.
  - Inference pipeline: Unchanged from base generative retrieval (beam search + heuristic mapping).

- Critical path:
  1. Retrieve initial passages using student model.
  2. Rerank retrieved passages using teacher model.
  3. Compute distilled RankNet loss based on teacher rank list.
  4. Update student model parameters via combined generative + distillation loss.

- Design tradeoffs:
  - Using cross-encoders as teachers provides better ranking but is computationally expensive during distillation.
  - Sampling only top M passages for reranking balances GPU memory constraints with distillation quality.
  - Margin values in distilled RankNet require tuning; too large margins may overfit to teacher's rankings.

- Failure signatures:
  - Distillation loss dominates generative loss → model focuses on ranking at expense of identifier generation quality.
  - Teacher model ranks irrelevant passages highly → student learns incorrect relevance signals.
  - Insufficient GPU memory → cannot use large enough teacher models or rerank enough passages.

- First 3 experiments:
  1. Run base generative retrieval (e.g., MINDER) on a small dataset to establish baseline performance.
  2. Implement distillation with a simple teacher (e.g., BM25) and distilled RankNet loss to verify the framework works.
  3. Scale up to cross-encoder teacher and tune margin parameters (mbase, mgap) on development set.

## Open Questions the Paper Calls Out
- Question: How does the performance of DGR scale with larger teacher models, and what is the impact of increasing the length of the teacher's rank list (Rtea)?
  - Basis in paper: [explicit] The paper discusses the use of different teacher models and mentions that the performance could potentially be enhanced with a longer Rtea, but due to GPU memory limitations, they were only able to set it to 8.
  - Why unresolved: The paper was limited by GPU memory constraints and could not test the performance of DGR with longer Rtea or larger teacher models.
  - What evidence would resolve it: Conducting experiments with larger teacher models and longer Rtea would provide evidence on how the performance of DGR scales and the impact of these factors.

- Question: Can generative retrieval models serve as teacher models for outdated generative retrieval models, and how would this affect the performance of the student model?
  - Basis in paper: [inferred] The paper mentions that advanced generative retrieval models could potentially serve as teacher models, but it does not explore this possibility.
  - Why unresolved: The paper focuses on using cross-encoder rankers and dense retrievers as teacher models, but does not investigate the use of generative retrieval models as teachers.
  - What evidence would resolve it: Conducting experiments with generative retrieval models as teacher models and evaluating the performance of the student model would provide evidence on the effectiveness of this approach.

- Question: What are the potential benefits and drawbacks of using progressive training methods in DGR, and how would they affect the performance of the generative retrieval model?
  - Basis in paper: [inferred] The paper mentions that further studies, including progressive training methods, are worth exploring, but it does not investigate this aspect.
  - Why unresolved: The paper focuses on the basic implementation of DGR and does not explore the potential benefits and drawbacks of using progressive training methods.
  - What evidence would resolve it: Conducting experiments with progressive training methods in DGR and evaluating the performance of the generative retrieval model would provide evidence on the effectiveness of this approach.

## Limitations
- The distilled RankNet loss implementation details are underspecified, particularly the margin calculation mechanism (mbase, mgap) and how they interact with the ranking optimization.
- The teacher model's quality is critical but not extensively validated across different domains; poor teacher rankings could propagate errors to the student model.
- The claim of "exceptional robustness and generalizability" is based on experiments with only a few teacher models (BM25, SimLM, etc.), leaving questions about performance with other sophisticated rankers.

## Confidence

**High confidence** in the core hypothesis that knowledge distillation can enhance generative retrieval by providing fine-grained relevance signals beyond binary labels.

**Medium confidence** in the distilled RankNet loss design, as the margin-based pairwise ranking mechanism is theoretically sound but implementation details are sparse.

**Medium confidence** in the efficiency claim, as the paper states no inference burden but doesn't provide quantitative comparisons of training vs. inference costs.

## Next Checks

1. Implement the distilled RankNet loss with different margin configurations (mbase, mgap) and evaluate how sensitive the performance is to these hyperparameters across all four datasets.

2. Test DGR with multiple teacher models beyond the ones evaluated (e.g., monoBERT, ELECTRA) to assess generalizability claims and identify failure modes when teacher quality varies.

3. Conduct an ablation study comparing DGR against standard generative retrieval with binary label supervision on the same datasets to quantify the specific benefits of rank-based distillation versus traditional approaches.