---
ver: rpa2
title: 'QCQA: Quality and Capacity-aware grouped Query Attention'
arxiv_id: '2406.10247'
source_url: https://arxiv.org/abs/2406.10247
tags:
- kv-cache
- accuracy
- qcqa
- heads
- groups
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes Quality and Capacity-Aware Grouped Query Attention
  (QCQA) to optimize KV-cache size in large language models (LLMs) while minimizing
  accuracy loss. QCQA uses an evolutionary algorithm to find optimal query head groupings
  within and across layers, considering both KV-cache size and accuracy.
---

# QCQA: Quality and Capacity-aware grouped Query Attention

## Quick Facts
- arXiv ID: 2406.10247
- Source URL: https://arxiv.org/abs/2406.10247
- Reference count: 40
- Key outcome: QCQA optimizes KV-cache size in LLMs while minimizing accuracy loss using evolutionary algorithms with weight-sharing error (WSE) as proxy metric.

## Executive Summary
This paper introduces Quality and Capacity-Aware Grouped Query Attention (QCQA), a method to optimize KV-cache size in large language models while minimizing accuracy loss. Unlike existing approaches like Multi-Query Attention (MQA) and Grouped Query Attention (GQA) that use fixed group sizes, QCQA employs an evolutionary algorithm to find optimal arbitrary-sized query head groupings within and across layers. The method uses a computationally inexpensive fitness function called weight-sharing error (WSE) to estimate accuracy loss during optimization. Experiments on Llama2 models demonstrate that QCQA achieves up to 20% higher accuracy than GQA with similar KV-cache size requirements, and requires 40% less KV-cache size to attain similar accuracy.

## Method Summary
QCQA optimizes KV-cache size in LLMs by finding optimal query head groupings using an evolutionary algorithm (NSGA-II) with weight-sharing error (WSE) as the fitness function. The method operates in two stages: first, it forms arbitrary-sized groups of query heads within each layer; second, it selects which layers to apply grouping to while leaving others as multi-head attention (MHA). The evolutionary algorithm explores the vast search space of possible groupings, evaluating candidates using WSE as a computationally inexpensive proxy for accuracy drop. The optimized model can then be optionally fine-tuned to further improve accuracy while maintaining KV-cache efficiency.

## Key Results
- QCQA achieves up to 20% higher accuracy than GQA with similar KV-cache size requirements
- QCQA requires 40% less KV-cache size to attain similar accuracy compared to GQA
- After fine-tuning, QCQA provides 10.55% higher accuracy than GQA for similar KV-cache size
- The method demonstrates strong correlation between WSE and actual accuracy loss

## Why This Works (Mechanism)

### Mechanism 1
- Claim: QCQA uses evolutionary algorithm with weight-sharing error (WSE) to optimize KV-cache size while minimizing accuracy loss.
- Mechanism: The evolutionary algorithm explores combinations of query head groupings, evaluating candidates using WSE as a computationally inexpensive proxy for accuracy drop.
- Core assumption: WSE reliably predicts LLM accuracy drop due to grouping of query heads.
- Evidence anchors:
  - [abstract] "QCQA employs a computationally inexpensive fitness function called weight-sharing error (WSE) to estimate accuracy loss."
  - [section 3.1] "Our experiments (section 4) show a strong relationship between accuracy drop and WSE."
  - [corpus] Weak evidence - no direct comparison of WSE to accuracy drop found in related papers.
- Break condition: If WSE does not reliably correlate with actual accuracy drop, the evolutionary algorithm will optimize for the wrong objective.

### Mechanism 2
- Claim: QCQA allows arbitrary-sized groups of query heads, unlike MQA and GQA which use fixed group sizes.
- Mechanism: By allowing flexible group sizes, QCQA can place similar query heads in the same group and dissimilar heads in different groups, minimizing accuracy loss.
- Core assumption: Query heads within a group should have similar distributions to minimize accuracy loss when sharing key and value heads.
- Evidence anchors:
  - [abstract] "QCQA allows for arbitrary-sized groups of query heads and employs a computationally inexpensive fitness function called weight-sharing error (WSE) to estimate accuracy loss."
  - [section 1] "The constraint of equally sized groups of consecutive heads could force two heads with highly distinct distribution into the same group."
  - [corpus] No direct evidence found in related papers, but this is a novel contribution of QCQA.
- Break condition: If the evolutionary algorithm cannot find good groupings within the search space, arbitrary group sizes may not provide benefits.

### Mechanism 3
- Claim: QCQA selects which layers to apply grouping to, while leaving other layers as multi-head attention (MHA).
- Mechanism: By selectively applying grouping only to layers where it provides the best tradeoff between KV-cache reduction and accuracy loss, QCQA achieves better overall performance than applying grouping to all layers.
- Core assumption: Not all layers benefit equally from grouping - some layers can be grouped with minimal accuracy loss while others should remain as MHA.
- Evidence anchors:
  - [section 3.3] "The second stage evaluates the impact of applying grouping to a layer on the LLM accuracy. Layers with a high impact on LLM accuracy are retained in their original MHA implementation otherwise the query heads are grouped to minimize KV-cache."
  - [section 4.1] "At 0.5 KV-cache size, not fine-tuned QCQA-AC model achieves the same accuracy as that of GQA fine-tuned for 3 epochs."
  - [corpus] No direct evidence found in related papers, but this is a novel contribution of QCQA.
- Break condition: If the layer selection algorithm cannot accurately predict which layers benefit from grouping, the selective approach may perform worse than uniform grouping.

## Foundational Learning

- Concept: Evolutionary algorithms (NSGA-II)
  - Why needed here: The search space for optimal query head groupings is enormous (Stirling numbers of the second kind), making exhaustive search infeasible. NSGA-II efficiently explores this space to find Pareto-optimal solutions.
  - Quick check question: What is the primary advantage of using NSGA-II over random search or grid search for this problem?

- Concept: Multi-head attention (MHA) and grouped query attention (GQA)
  - Why needed here: Understanding the difference between MHA (one key/value head per query head) and GQA (multiple query heads share one key/value head) is crucial for understanding the problem QCQA solves.
  - Quick check question: How does GQA reduce KV-cache size compared to MHA, and what is the tradeoff?

- Concept: Weight-sharing error (WSE) as a proxy metric
  - Why needed here: WSE provides a computationally inexpensive way to estimate accuracy loss from grouping query heads, enabling efficient exploration of the search space.
  - Quick check question: Why is WSE more efficient to compute than actual LLM accuracy, and how does it relate to accuracy loss?

## Architecture Onboarding

- Component map: NSGA-II evolutionary algorithm -> WSE evaluation -> Layer selection -> QCQA model derivation -> Fine-tuning (optional) -> Accuracy evaluation

- Critical path: NSGA-II → WSE evaluation → Layer selection → QCQA model derivation → Fine-tuning (optional) → Accuracy evaluation

- Design tradeoffs:
  - Flexibility vs. simplicity: QCQA-AC allows arbitrary group sizes for better optimization but adds complexity compared to QCQA-EC
  - Accuracy vs. KV-cache: The core tradeoff QCQA optimizes for
  - Computational cost vs. accuracy: Using WSE instead of actual accuracy for candidate evaluation

- Failure signatures:
  - WSE not correlating with actual accuracy drop
  - Evolutionary algorithm getting stuck in local optima
  - Layer selection algorithm choosing wrong layers to group
  - Insufficient fine-tuning leading to accuracy loss

- First 3 experiments:
  1. Verify WSE correlation with accuracy: Run QCQA with different groupings and compare WSE values to actual accuracy drops
  2. Compare QCQA-AC vs QCQA-EC: Run both variants with the same settings and compare accuracy vs KV-cache tradeoffs
  3. Layer selection ablation: Run QCQA with all layers grouped vs selective grouping and measure the difference in accuracy and KV-cache size

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the weight-sharing error (WSE) formulation scale to models with significantly different attention head sizes or dimensions compared to Llama2 and OPT models?
- Basis in paper: [explicit] The paper demonstrates WSE's effectiveness on Llama2 7B, Llama2 13B, and OPT models, but doesn't explore extreme variations in head sizes or dimensions.
- Why unresolved: The paper doesn't provide evidence for WSE's reliability across a wide range of model architectures with varying head dimensions.
- What evidence would resolve it: Experiments showing WSE correlation with accuracy loss across models with diverse head sizes, such as very large (e.g., 128+ heads) or very small (e.g., 4-8 heads) configurations, and different key/value dimensions.

### Open Question 2
- Question: What is the impact of QCQA on inference latency compared to GQA and MHA, considering the computational overhead of the evolutionary algorithm during optimization?
- Basis in paper: [inferred] The paper focuses on KV-cache optimization and accuracy, but doesn't discuss inference latency differences between QCQA, GQA, and MHA.
- Why unresolved: The paper doesn't provide latency measurements or comparisons, which are crucial for real-world deployment considerations.
- What evidence would resolve it: Benchmarking results showing end-to-end inference latency for QCQA, GQA, and MHA models across various sequence lengths and batch sizes, including the time taken for the evolutionary algorithm optimization.

### Open Question 3
- Question: How does the performance of QCQA change when applied to encoder-decoder architectures or models with multiple attention mechanisms (e.g., cross-attention)?
- Basis in paper: [inferred] The paper explicitly states that QCQA is currently limited to autoregressive inference of LLMs, implying potential limitations for other architectures.
- Why unresolved: The paper doesn't explore QCQA's applicability beyond standard autoregressive decoder-only models.
- What evidence would resolve it: Experiments demonstrating QCQA's effectiveness on encoder-decoder models like T5 or BART, and analysis of how cross-attention layers impact the optimization process and results.

## Limitations

- The method's reliance on WSE as a proxy for accuracy loss requires more rigorous validation across diverse model architectures
- Results are primarily demonstrated on Llama2 models, limiting generalizability to other architectures
- The computational overhead of the evolutionary algorithm during optimization is not fully characterized

## Confidence

- **High confidence**: The core mechanism of using evolutionary algorithms for query head grouping is technically sound and well-explained
- **Medium confidence**: The empirical results showing QCQA outperforming GQA are convincing but based on a limited set of benchmarks
- **Low confidence**: The WSE metric's reliability as a proxy for accuracy loss needs more rigorous validation

## Next Checks

1. **WSE correlation validation**: Conduct a systematic study measuring the correlation coefficient between WSE values and actual accuracy drops across different grouping configurations and model scales.

2. **Cross-architecture generalization**: Test QCQA on non-Llama2 architectures (e.g., OPT, BLOOM) to verify if the observed benefits transfer across different model families.

3. **Real-time inference impact**: Measure the actual latency and memory usage during generation to confirm that the theoretical KV-cache savings translate to practical performance gains in deployed systems.