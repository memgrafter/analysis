---
ver: rpa2
title: Functional Bilevel Optimization for Machine Learning
arxiv_id: '2403.20233'
source_url: https://arxiv.org/abs/2403.20233
tags:
- learning
- bilevel
- function
- optimization
- functional
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a functional perspective on bilevel optimization
  problems in machine learning, where the inner objective is minimized over a function
  space rather than parameters. The authors propose a method called Functional Implicit
  Differentiation (FuncID) that exploits the strong convexity of the inner objective
  with respect to the output of the prediction function, allowing the use of over-parameterized
  neural networks.
---

# Functional Bilevel Optimization for Machine Learning

## Quick Facts
- arXiv ID: 2403.20233
- Source URL: https://arxiv.org/abs/2403.20233
- Reference count: 40
- Key outcome: Introduces Functional Implicit Differentiation (FuncID) for bilevel optimization, showing improved performance on two-stage least squares regression and model-based reinforcement learning compared to existing methods.

## Executive Summary
This paper introduces a functional perspective on bilevel optimization problems in machine learning, where the inner objective is minimized over a function space rather than parameters. The authors propose a method called Functional Implicit Differentiation (FuncID) that exploits the strong convexity of the inner objective with respect to the output of the prediction function, allowing the use of over-parameterized neural networks. FuncID computes the total gradient by solving a functional linear system, which is more stable than the parametric approach. The method is applied to two problems: two-stage least squares regression and model-based reinforcement learning, showing improved performance compared to existing methods like AID and ITD. The key advantage of FuncID is its ability to handle over-parameterized models and its reduced computational cost compared to AID.

## Method Summary
The FuncID method reformulates bilevel optimization problems in a functional space, where the inner objective is minimized over a Hilbert space of functions rather than model parameters. This allows using over-parameterized neural networks as the inner prediction function while maintaining well-defined solutions. The method computes the total gradient by solving a functional linear system involving the Hessian of the inner objective with respect to the output of the prediction function. This approach is more stable than traditional parametric methods like AID, as it avoids ill-posed linear systems when the inner objective is non-convex in the model parameters. The method is applied to two-stage least squares regression and model-based reinforcement learning, showing improved performance compared to existing methods.

## Key Results
- FuncID reduces computational complexity by avoiding Hessian-vector products with respect to model parameters
- FuncID is more stable than AID because it only requires second-order information with respect to the output of h, not its parameters
- FuncID can leverage over-parameterized neural networks for function approximation while maintaining well-defined solutions

## Why This Works (Mechanism)

### Mechanism 1
- Claim: FuncID reduces computational complexity by avoiding Hessian-vector products with respect to model parameters.
- Mechanism: In FuncID, the Hessian is computed with respect to the output of the prediction function h(x) rather than its parameters. Since the output dimension dv is typically much smaller than the parameter dimension pin, this reduces both memory and time complexity.
- Core assumption: The output dimension dv of the prediction function is significantly smaller than the number of parameters pin in the model.
- Evidence anchors:
  - [abstract]: "The key advantage of FuncID is its ability to handle over-parameterized models and its reduced computational cost compared to AID."
  - [section 4.4]: "Unlike AID, Algorithm 1 does not require differentiating through the parameters of the prediction model when estimating the total gradient ∇F(ω). This property results in an improved cost in time and memory in most practical cases as shown in Table 1 and Figure 1."
  - [corpus]: Weak - No direct comparison of computational complexity between methods.
- Break condition: If the output dimension dv becomes comparable to or larger than the parameter dimension pin, the computational advantage disappears.

### Mechanism 2
- Claim: FuncID is more stable than AID because it only requires second-order information with respect to the output of h, not its parameters.
- Mechanism: AID methods require solving a quadratic problem defined by the parametric Hessian matrix, which can be ill-posed when the inner objective is non-convex in the model parameters. FuncID only requires solving a quadratic problem in the Hilbert space H, which is always guaranteed to have a solution due to the strong convexity of the inner objective in the output of h.
- Core assumption: The inner objective is strongly convex with respect to the output of the prediction function h(x).
- Evidence anchors:
  - [abstract]: "The method is applied to two problems: two-stage least squares regression and model-based reinforcement learning, showing improved performance compared to existing methods like AID and ITD."
  - [section 3.1]: "AID approximates the solution of a finite dimensional linear system, which involves second order derivatives of the inner objective with respect to the parameters of the model approximating the prediction function h. Such a linear system might be ill-posed when the inner objective is non-convex in the model parameters, thus resulting in instabilities."
  - [corpus]: Weak - No explicit discussion of stability differences between methods.
- Break condition: If the inner objective is not strongly convex with respect to the output of h, the stability advantage may be lost.

### Mechanism 3
- Claim: FuncID can leverage over-parameterized neural networks for function approximation while maintaining well-defined solutions.
- Mechanism: The functional point of view does not rely on the strong convexity of the inner objective with respect to the model parameters, unlike classical bilevel formulations. This allows using over-parameterized neural networks as the inner prediction function without requiring further optimization of their parameters for the outer-level objective.
- Core assumption: The inner objective is strongly convex with respect to the output of the prediction function h(x).
- Evidence anchors:
  - [abstract]: "The functional point of view does not rely on this assumption and notably allows using over-parameterized neural networks as the inner prediction function."
  - [section 3]: "These observations enable us to view the machine learning model, typically a neural network, as a function approximation tool within a larger functional space where the bilevel formulation is well defined without the need for strong convexity with respect to model parameters."
  - [corpus]: Weak - No explicit discussion of using over-parameterized networks in other bilevel optimization methods.
- Break condition: If the inner objective is not strongly convex with respect to the output of h, the ability to use over-parameterized networks may be compromised.

## Foundational Learning

- Concept: Implicit Function Theorem (IFT)
  - Why needed here: The IFT is used to derive the Jacobian of the inner-level solution with respect to the outer variable, which is crucial for computing the total gradient in bilevel optimization.
  - Quick check question: What are the conditions required for the IFT to hold in the context of bilevel optimization?

- Concept: Strong Convexity
  - Why needed here: Strong convexity of the inner objective with respect to the output of the prediction function ensures the existence and uniqueness of the solution, as well as the stability of the functional implicit differentiation method.
  - Quick check question: How does strong convexity of the inner objective with respect to the output of h differ from strong convexity with respect to the model parameters?

- Concept: Hilbert Spaces
  - Why needed here: The functional bilevel optimization problem is formulated in a Hilbert space of functions, which allows for a more general and flexible treatment of the problem compared to the parametric setting.
  - Quick check question: What are the key properties of Hilbert spaces that make them suitable for functional bilevel optimization?

## Architecture Onboarding

- Component map: Outer parameter ω -> Inner-level optimization -> Adjoint optimization -> Total gradient estimation -> Function approximation
- Critical path:
  1. Initialize outer parameter ω and inner/adjoint model parameters θ and ξ
  2. Optimize the inner model to approximate the prediction function hω
  3. Optimize the adjoint model to approximate the adjoint function aω
  4. Estimate the total gradient using the approximated prediction and adjoint functions
  5. Update the outer parameter ω using the estimated gradient
- Design tradeoffs:
  - Memory vs. accuracy: Using smaller models for function approximation reduces memory usage but may sacrifice accuracy
  - Computational cost vs. stability: AID is computationally cheaper but may be unstable for non-convex inner objectives, while FuncID is more stable but computationally more expensive
  - Expressiveness vs. tractability: More expressive models can better approximate the optimal functions but may be harder to optimize
- Failure signatures:
  - Divergence or oscillation in the outer optimization loop
  - Poor approximation of the prediction or adjoint functions
  - Instability or numerical issues in the adjoint optimization
- First 3 experiments:
  1. Implement FuncID on a simple synthetic bilevel optimization problem with a known solution to verify correctness
  2. Compare the performance of FuncID and AID on a benchmark instrumental variable regression problem
  3. Test the scalability of FuncID on a large-scale model-based reinforcement learning problem

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of FuncID scale with increasing data dimensionality and complexity of the prediction function?
- Basis in paper: [explicit] The paper discusses scalability in terms of computational cost but doesn't explore performance trends with increasing data complexity.
- Why unresolved: The experiments focus on specific datasets and model architectures, leaving scalability questions open.
- What evidence would resolve it: Experiments varying data dimensionality and model complexity while tracking performance metrics.

### Open Question 2
- Question: What is the theoretical relationship between the strong convexity assumption on the inner objective with respect to the prediction function output and the convergence guarantees of FuncID?
- Basis in paper: [explicit] The paper relies on strong convexity for theoretical guarantees but doesn't fully explore the implications of relaxing this assumption.
- Why unresolved: The theoretical analysis focuses on the well-defined case and doesn't explore the impact of potential violations of the strong convexity assumption.
- What evidence would resolve it: Theoretical analysis or empirical studies examining convergence behavior when the strong convexity assumption is weakened or violated.

### Open Question 3
- Question: How does the choice of regularization for the inner and adjoint models affect the generalization performance of FuncID?
- Basis in paper: [explicit] The paper mentions the use of regularization but doesn't provide a systematic study of its impact on performance.
- Why unresolved: The experiments use specific regularization choices without exploring the sensitivity to different regularization strategies.
- What evidence would resolve it: Experiments comparing different regularization techniques and their effects on generalization error.

## Limitations
- The method requires the inner objective to be strongly convex with respect to the output of the prediction function, which may not always hold in practice.
- The paper does not provide a detailed comparison of the computational complexity of FuncID and AID in terms of Big-O notation.
- The stability improvements of FuncID over AID are demonstrated empirically but not theoretically justified.

## Confidence
- **High**: The core mathematical formulation of FuncID is sound, as it builds on well-established concepts from the implicit function theorem and functional analysis.
- **Medium**: The empirical improvements shown in the two applications are promising, but the sample size of experiments is limited (only two domains tested).
- **Low**: The paper lacks a comprehensive theoretical analysis of the convergence properties of FuncID, particularly in the over-parameterized regime.

## Next Checks
1. Conduct a rigorous theoretical analysis of the convergence properties of FuncID, particularly in the over-parameterized regime where the inner objective may not be strongly convex with respect to the model parameters.
2. Provide a detailed comparison of the computational complexity of FuncID and AID in terms of Big-O notation, considering different problem sizes and architectures.
3. Investigate the behavior of FuncID when the inner objective is not strongly convex with respect to the output of the prediction function, and explore potential modifications or regularization techniques to handle such cases.