---
ver: rpa2
title: 'Greenback Bears and Fiscal Hawks: Finance is a Jungle and Text Embeddings
  Must Adapt'
arxiv_id: '2411.07142'
source_url: https://arxiv.org/abs/2411.07142
tags:
- text
- embeddings
- query
- queries
- passage
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of adapting text embeddings
  for financial document retrieval by introducing BAM embeddings, a domain-specific
  model finetuned on a large dataset of 14.3M query-passage pairs constructed from
  2.8M financial documents. The core method involves synthetically generating queries
  using few-shot prompting and finetuning the Multilingual-E5 model with contrastive
  learning and hard negative mining.
---

# Greenback Bears and Fiscal Hawks: Finance is a Jungle and Text Embeddings Must Adapt

## Quick Facts
- **arXiv ID**: 2411.07142
- **Source URL**: https://arxiv.org/abs/2411.07142
- **Reference count**: 25
- **Primary result**: BAM embeddings achieve 62.8% Recall@1 on financial document retrieval, outperforming general-purpose embeddings (39.2%)

## Executive Summary
This paper addresses the challenge of adapting text embeddings for financial document retrieval by introducing BAM embeddings, a domain-specific model finetuned on a large dataset of 14.3M query-passage pairs constructed from 2.8M financial documents. The core method involves synthetically generating queries using few-shot prompting and finetuning the Multilingual-E5 model with contrastive learning and hard negative mining. The primary results show that BAM embeddings achieve 62.8% Recall@1 on a held-out test set, significantly outperforming general-purpose embeddings (e.g., OpenAI's 39.2%) and demonstrating improved sensitivity to finance-specific elements like company names and financial metrics. When deployed in a production retrieval system, BAM embeddings outperform traditional BM25 search, especially for longer and more detailed queries. Additionally, using BAM embeddings in a financial question-answering benchmark increases accuracy by 8%, validating their ability to generalize to out-of-domain tasks.

## Method Summary
The method constructs a large-scale training corpus by parsing 2.8M financial documents into passages, synthetically generating queries using few-shot prompting with 231 human-written examples, and finetuning Multilingual-E5 with contrastive learning. The training uses InfoNCE loss with in-batch negatives and hard negative mining, where hard negatives are mined from early checkpoints. The process generates 15.2M query-passage pairs (14.3M training, 444K validation, 447K test) after filtering invalid/skipped and duplicate queries. The finetuning uses learning rates of 3e-5 (small), 2e-5 (base), 1e-5 (large) for 3 epochs with batch size 512.

## Key Results
- BAM embeddings achieve 62.8% Recall@1 on held-out test set, significantly outperforming general-purpose embeddings (39.2%)
- BAM embeddings outperform BM25 search in production retrieval system, especially for longer and more detailed queries
- Using BAM embeddings in financial question-answering benchmark increases accuracy by 8%

## Why This Works (Mechanism)
The method works by addressing the domain-specific vocabulary and semantic structures in financial documents through contrastive learning on synthetically generated queries. The synthetic query generation captures diverse financial concepts while the hard negative mining ensures the model learns fine-grained distinctions between relevant and irrelevant passages. The prepending of document context (company, ticker, date) provides crucial metadata that enhances retrieval precision for finance-specific entities.

## Foundational Learning
- **Contrastive Learning**: Why needed: To learn embeddings where relevant query-passage pairs are close in embedding space while irrelevant pairs are distant. Quick check: Verify loss decreases and Recall@1 improves during training.
- **Hard Negative Mining**: Why needed: To focus learning on difficult negative examples that are semantically similar but irrelevant, improving fine-grained distinctions. Quick check: Compare Recall@1 with/without hard negatives to confirm improvement.
- **Synthetic Query Generation**: Why needed: To create large-scale training data with diverse financial queries when real user query logs are limited. Quick check: Manually inspect random query-passage pairs for realism and diversity.

## Architecture Onboarding
**Component Map**: PDF documents -> Text passages (with context) -> Synthetic queries -> Query-passage pairs -> Contrastive training (InfoNCE + hard negatives) -> BAM embeddings -> Retrieval system
**Critical Path**: Document parsing → Query generation → Contrastive training → Hard negative mining → Evaluation/Deployment
**Design Tradeoffs**: Synthetic queries enable large-scale training but may lack real-world query diversity; hard negatives improve precision but require additional computation; prepending context improves finance-specific retrieval but adds preprocessing complexity.
**Failure Signatures**: Poor query-passage relevance indicates synthetic query generation issues; suboptimal embeddings without hard negatives suggest need for harder training examples; performance degradation on shorter queries indicates context-dependency limitations.
**First Experiments**: 1) Generate and inspect 100 random synthetic query-passage pairs for quality, 2) Train with and without hard negatives to measure impact on Recall@1, 3) Compare BAM embeddings against BM25 on diverse query lengths.

## Open Questions the Paper Calls Out
None

## Limitations
- Synthetic query generation may not fully capture real user query diversity and nuance
- Evaluation focuses primarily on retrieval metrics without extensive analysis of embedding quality across financial subdomains
- Production deployment results lack detailed error analysis or comparison against alternative retrieval architectures

## Confidence
**High confidence**: The core technical methodology (contrastive learning with hard negatives on synthetic queries) is sound and well-documented. The quantitative improvements over general-purpose embeddings are substantial and reproducible given access to the same training corpus and infrastructure.

**Medium confidence**: Claims about improved sensitivity to finance-specific elements (company names, financial metrics) and the 8% accuracy gain on FinanceBench are supported by results but lack detailed qualitative analysis of failure cases or robustness testing across different financial document types.

**Low confidence**: The assertion that BAM embeddings "significantly outperform" BM25 search is qualified by the observation that performance degrades for shorter queries, suggesting the advantage may be context-dependent rather than universally superior.

## Next Checks
1. **Query Diversity Analysis**: Conduct systematic analysis of synthetic query diversity and realism by comparing query distributions against actual user query logs, identifying potential gaps in coverage.
2. **Robustness Testing**: Evaluate BAM embeddings on out-of-distribution financial documents (e.g., emerging market disclosures, non-SEC filings) to assess domain generalization beyond the training corpus.
3. **Embedding Quality Diagnostics**: Perform qualitative analysis of top retrieved passages for both successful and failed queries, examining whether improvements stem from better semantic matching or overfitting to training patterns.