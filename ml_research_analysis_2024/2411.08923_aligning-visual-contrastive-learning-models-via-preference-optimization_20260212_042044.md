---
ver: rpa2
title: Aligning Visual Contrastive learning models via Preference Optimization
arxiv_id: '2411.08923'
source_url: https://arxiv.org/abs/2411.08923
tags:
- typographic
- arxiv
- dataset
- preference
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper extends preference optimization (PO) to contrastive\
  \ learning models, focusing on enhancing robustness against typographic attacks\
  \ and mitigating gender bias. The authors propose using preference optimization\
  \ methods\u2014DPO, IPO, and KTO\u2014to align model behavior with desired preferences,\
  \ while retaining pre-trained knowledge via regularization."
---

# Aligning Visual Contrastive learning models via Preference Optimization

## Quick Facts
- arXiv ID: 2411.08923
- Source URL: https://arxiv.org/abs/2411.08923
- Reference count: 40
- Primary result: Preference optimization improves CLIP robustness against typographic attacks by up to 19.63% while mitigating gender bias

## Executive Summary
This paper extends preference optimization (PO) methods to contrastive learning models, specifically focusing on enhancing CLIP's robustness against typographic attacks and mitigating gender bias. The authors propose using DPO, IPO, and KTO to align model behavior with desired preferences while retaining pre-trained knowledge through KL regularization. Experiments demonstrate that PO-trained models outperform standard contrastive learning techniques, achieving significant improvements in adversarial robustness while maintaining accuracy on clean data and downstream tasks.

## Method Summary
The approach applies preference optimization to contrastive learning by treating it as a one-step Markov Decision Process, where the policy selects embeddings based on pairwise preference data. A frozen text encoder ensures linguistic knowledge preservation, while a trainable image encoder and learnable linear transformation layer (parameterized as W = UΣV^T) enable adaptation. The method combines PO loss functions with KL regularization to maintain proximity to the reference model, using Binary Model Averaging (BMA) for better generalization.

## Key Results
- 19.63% improvement in typographic attack robustness over baseline models
- Effective gender bias mitigation demonstrated on VL-Bias dataset
- Fine-grained control over task trade-offs (e.g., OCR vs object detection) via linear transformation parameters
- KTO shows greater sample efficiency compared to DPO and IPO

## Why This Works (Mechanism)

### Mechanism 1
Preference optimization methods improve contrastive learning robustness by aligning model embeddings with human-preferred outputs while preserving pre-trained knowledge. The preference optimization loss adjusts image embeddings to increase similarity to preferred text embeddings and decrease similarity to dispreferred ones, while KL regularization ensures proximity to the reference model.

### Mechanism 2
Linear transformations with SVD decomposition provide fine-grained control over model behavior by selectively strengthening or weakening embedding directions. The learnable matrix W = UΣV^T transforms embeddings, where singular values σ_i control amplification (σ_i > 1) or attenuation (σ_i < 1) of corresponding directions.

### Mechanism 3
KTO's binary preference signal improves sample efficiency compared to DPO/IPO by effectively doubling training samples. Each preference triplet (x, y_w, y_l) is converted into two samples: (x, y_desired) and (x, y_undesired), allowing the model to learn from both positive and negative examples without detailed pairwise comparisons.

## Foundational Learning

- Concept: Contrastive learning and CLIP architecture
  - Why needed here: The paper extends preference optimization to contrastive learning models like CLIP, requiring understanding of how these models learn semantic similarities through embedding alignment.
  - Quick check question: How does CLIP compute similarity between image and text embeddings, and what role does the temperature parameter play?

- Concept: Preference optimization methods (DPO, IPO, KTO)
  - Why needed here: The paper applies these methods to non-generative models, requiring understanding of their mathematical formulations and differences.
  - Quick check question: What is the key difference between DPO and IPO in terms of their loss functions and regularization approaches?

- Concept: Reinforcement learning and MDP formulation
  - Why needed here: The paper frames contrastive learning as a one-step MDP to apply preference optimization, requiring understanding of state-action spaces and reward functions.
  - Quick check question: How does the MDP formulation for contrastive learning differ from traditional RL problems in terms of state transitions and discount factors?

## Architecture Onboarding

- Component map:
  Pre-trained CLIP encoders (frozen text, trainable image) -> Learnable linear transformation layer (W = UΣV^T) -> Preference dataset (x, y_w, y_l) pairs -> Regularization dataset (clean samples) -> Loss function combining PO objective and KL regularization

- Critical path:
  1. Initialize with pre-trained CLIP and frozen text encoder
  2. Process preference data through encoders to get embeddings
  3. Compute PO loss (DPO/IPO/KTO) based on embedding differences
  4. Add KL regularization to maintain proximity to reference model
  5. Backpropagate through image encoder and linear layer
  6. Apply BMA for better generalization

- Design tradeoffs:
  - Frozen text encoder vs. trainable: Preserves linguistic knowledge but limits adaptation
  - Regularization strength λ: Balances robustness vs. clean data performance
  - β parameter: Controls policy deviation - higher values risk overfitting
  - Linear layer vs. full fine-tuning: Better interpretability vs. potentially higher performance

- Failure signatures:
  - KL divergence increasing during training → regularization too weak
  - Clean data accuracy dropping significantly → over-regularization or preference overfitting
  - Singular values deviating from 1.0 → excessive transformation
  - Performance similar to baseline → insufficient preference data or wrong β/λ settings

- First 3 experiments:
  1. Typographic attack robustness on ImageNet-100 with varying β and λ parameters
  2. Gender bias mitigation using VL-Bias dataset with linear transformation scaling
  3. Ablation study comparing DPO, IPO, and KTO on FOOD101 dataset for sample efficiency analysis

## Open Questions the Paper Calls Out

### Open Question 1
How does the effectiveness of preference optimization methods (DPO, IPO, KTO) vary across different types of biases (e.g., gender, racial, age) in contrastive learning models?

### Open Question 2
What is the impact of preference optimization on the robustness of contrastive learning models against non-typographic adversarial attacks (e.g., PGD, FGSM)?

### Open Question 3
How does the choice of regularization dataset size and composition affect the retention of pre-trained knowledge and task-specific performance?

## Limitations

- Frozen text encoder assumption may constrain full potential of preference alignment
- Reliance on preference datasets introduces potential bias based on human annotator preferences
- Linear transformation approach may not capture complex nonlinear relationships needed for certain alignment tasks

## Confidence

- High Confidence: Typographic attack robustness improvements (19.63% over baseline)
- Medium Confidence: Gender bias mitigation results across different bias types
- Low Confidence: Linear transformation effectiveness across diverse downstream tasks

## Next Checks

1. Validate typographic attack resistance on diverse datasets (medical imaging, satellite imagery) beyond ImageNet-100
2. Test gender bias mitigation methods on additional bias dimensions (racial, age-related) using FairFace or similar datasets
3. Evaluate performance and computational efficiency when applying the method to larger CLIP variants (e.g., ViT-L/14)