---
ver: rpa2
title: Targeted Visualization of the Backbone of Encoder LLMs
arxiv_id: '2403.18872'
source_url: https://arxiv.org/abs/2403.18872
tags:
- deepview
- data
- classification
- bert
- space
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper applies DeepView, a discriminative dimensionality reduction
  technique, to visualize the embedding space and decision boundaries of BERT-based
  encoder models for NLP classification tasks. The method modifies DeepView by using
  BERT embeddings as input, cosine distance for regularization, and varying the balance
  between discriminative and unsupervised metrics.
---

# Targeted Visualization of the Backbone of Encoder LLMs

## Quick Facts
- arXiv ID: 2403.18872
- Source URL: https://arxiv.org/abs/2403.18872
- Reference count: 24
- Key result: DeepView with λ=0.8 reduces QkN N error from 31% to 3% for pre-trained models and maintains ~1% error for fine-tuned models

## Executive Summary
This paper applies DeepView, a discriminative dimensionality reduction technique, to visualize the embedding space and decision boundaries of BERT-based encoder models for NLP classification tasks. The method modifies DeepView by using BERT embeddings as input, cosine distance for regularization, and varying the balance between discriminative and unsupervised metrics. Experiments across multiple GLUE tasks show that discriminative distances are crucial for meaningful visualizations in pre-trained models, while fine-tuned models inherently incorporate task-relevant structure. The technique successfully identifies adversarially perturbed samples, reveals task synergies in multi-task models, and enables quantitative comparison of model embedding spaces.

## Method Summary
The authors apply DeepView to BERT-based NLP classifiers with modifications: using BERT CLS token embeddings as input, cosine distance for regularization, and varying the balance between discriminative and unsupervised metrics (λ). They train classification models with pre-trained, fine-tuned, and multi-task settings on the GLUE dataset. The fidelity of the DeepView DR projection is evaluated using QkN N (leave-one-out error of a k-nearest neighbor classifier with k = 5 using the model's predicted labels) and Qdata (agreement between the prediction labels of data points and the background of the scatter plot). Additionally, they assess the similarity of classification strategies between models using QN N and LCMC scores.

## Key Results
- Discriminative distances are crucial for meaningful visualizations in pre-trained models, with DeepView (λ=0.8) reducing QkN N error from 31% to 3%
- Fine-tuned models inherently incorporate task-relevant structure, maintaining ~1% QkN N error even with minimal discriminative emphasis
- DeepView successfully identifies adversarially perturbed samples through visualization of high-uncertainty regions
- The technique reveals task synergies in multi-task models, showing RTE and MNLI exhibit closer alignment than other task pairs

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Discriminative distance metrics enable DeepView to project embedding spaces in ways that preserve class boundaries, which is critical for interpretable visualizations of pre-trained models.
- Mechanism: The distance metric used by DeepView blends a discriminative component (Jensen-Shannon divergence over classifier outputs) with an unsupervised component (cosine or Euclidean). This combination encourages embeddings to be arranged such that transitions between classes are preserved in the 2D projection, while also maintaining local neighborhood structure.
- Core assumption: The embedding space of a pre-trained encoder contains discriminative information relevant to downstream tasks, even if not explicitly trained for them.
- Evidence anchors:
  - [abstract]: "discriminative distances are crucial for meaningful visualizations in pre-trained models"
  - [section]: "For the pre-trained models, the error changes heavily as λ decreases from 1" (Section 5.1)
  - [corpus]: Weak; no direct neighbor discusses discriminative metrics in embedding visualization.
- Break condition: If the embedding space is too noisy or task-irrelevant, the discriminative component will fail to align meaningful class boundaries, resulting in random-looking projections.

### Mechanism 2
- Claim: Fine-tuned models inherently adjust their embedding spaces to reflect task-relevant structure, reducing the need for a strong discriminative component in DeepView.
- Mechanism: During fine-tuning, the encoder weights are updated so that the CLS token embeddings directly encode task-specific decision boundaries. Thus, even a purely unsupervised metric (λ = 0) in DeepView yields a projection that preserves meaningful class structure.
- Core assumption: Fine-tuning on a downstream task sufficiently restructures the embedding space to encode the decision boundary.
- Evidence anchors:
  - [abstract]: "fine-tuned models inherently incorporate task-relevant structure"
  - [section]: "For the two fine-tuned cases, the error values change very little" (Section 5.1)
  - [corpus]: Weak; no neighbor explicitly discusses fine-tuning effects on embedding space structure.
- Break condition: If fine-tuning is shallow or the task is too complex, the embedding space may not fully adapt, and discriminative metrics would again become necessary.

### Mechanism 3
- Claim: DeepView can reveal adversarial samples by identifying regions of high classification uncertainty in the 2D projection.
- Mechanism: Adversarial perturbations often push samples into regions of the embedding space where the classifier is uncertain. In DeepView, these uncertain regions appear as areas with low color saturation or mixed background colors, making them visually identifiable. By focusing on these uncertain regions, one can isolate suspicious samples.
- Core assumption: Adversarial examples create local regions of high uncertainty in the classifier's decision function.
- Evidence anchors:
  - [abstract]: "successfully identifies adversarially perturbed samples"
  - [section]: "Upon closer inspection, we identify particularly difficult areas... The white coloration... indicate difficulty for the model in determining sentiment" (Section 5.2)
  - [corpus]: Weak; no neighbor discusses adversarial detection via visualization.
- Break condition: If adversarial examples are designed to mimic normal samples closely, they may fall into low-uncertainty regions and evade detection.

## Foundational Learning

- Concept: Jensen-Shannon divergence as a distance metric for classifier outputs
  - Why needed here: It provides a symmetric, bounded measure of difference between probability distributions, which is essential for quantifying how classifier predictions change across the embedding space.
  - Quick check question: What is the range of Jensen-Shannon divergence values?

- Concept: Role of the CLS token in BERT embeddings
  - Why needed here: DeepView uses the CLS token embedding as the input representation for each sequence, so understanding its role is key to interpreting the visualizations.
  - Quick check question: How does the CLS token embedding differ from average pooling of all token embeddings in BERT?

- Concept: Trade-off between discriminative and unsupervised metrics (λ parameter)
  - Why needed here: The λ parameter controls how much emphasis is placed on preserving class boundaries versus local structure, directly affecting the interpretability of the visualization.
  - Quick check question: What happens to the visualization when λ = 0 versus λ = 1?

## Architecture Onboarding

- Component map:
  Input: BERT CLS token embeddings (768-dim) -> DeepView with UMAP + discriminative distance metric -> 2D scatter plot with background decision function -> Supporting: kNN classifier for quantitative evaluation (QkN N, Qdata)

- Critical path:
  1. Compute embeddings for validation samples
  2. Apply DeepView with chosen λ
  3. Evaluate fidelity using QkN N and Qdata
  4. Visualize and inspect for patterns (e.g., adversarial samples, task synergies)

- Design tradeoffs:
  - High λ: Better class boundary preservation but may lose local neighborhood structure
  - Low λ: Better local structure but may obscure class boundaries in pre-trained models
  - Tradeoff between computational cost (sampling grid size) and visualization resolution

- Failure signatures:
  - Random-looking scatter plots with no discernible class structure (λ too low for pre-trained models)
  - Overly clustered points with no variation (λ too high, losing intra-class variation)
  - High QkN N error indicating poor alignment between projection and classifier decision function

- First 3 experiments:
  1. Run DeepView on a pre-trained BERT model with λ = 1, 0.8, 0.6; compare QkN N errors to identify optimal λ.
  2. Apply DeepView to a fine-tuned BERT model with λ = 1; verify that class structure is preserved without discriminative emphasis.
  3. Use DeepView on a multi-task BERT model; inspect for dataset separation and potential synergies (e.g., RTE vs MNLI).

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the choice of embedding space (e.g., using CLS token vs. average pooling vs. sentence embeddings) affect the quality and interpretability of DeepView visualizations for BERT-based models?
- Basis in paper: [explicit] The paper uses BERT CLS token embeddings as input to DeepView, but doesn't explore alternative embedding strategies.
- Why unresolved: The paper focuses on demonstrating DeepView's utility rather than systematically comparing embedding methods. Different embedding choices could significantly impact visualization quality.
- What evidence would resolve it: Comparative experiments showing DeepView results using CLS tokens, mean pooling, and other embedding strategies across the same tasks and models.

### Open Question 2
- Question: Can DeepView effectively identify adversarial examples in larger, more diverse datasets beyond the SST2 demonstration?
- Basis in paper: [explicit] The paper demonstrates adversarial detection on SST2 with 5000 points but doesn't explore scalability or effectiveness on other datasets.
- Why unresolved: The SST2 demonstration is limited in scope. The method's effectiveness on larger datasets, different attack types, or more complex tasks remains unknown.
- What evidence would resolve it: Systematic evaluation of DeepView's adversarial detection capabilities across all GLUE tasks, varying dataset sizes, and different attack methodologies.

### Open Question 3
- Question: How does DeepView compare quantitatively to other dimensionality reduction techniques specifically designed for NLP interpretability (like UMAP, t-SNE, or projection pursuit)?
- Basis in paper: [inferred] The paper mentions that DR methods in NLP "often boil down to a simple application of nonlinear dimensionality reduction" but doesn't directly compare DeepView to these alternatives.
- Why unresolved: While the paper demonstrates DeepView's unique features, it doesn't establish whether it provides superior or complementary insights compared to established DR methods.
- What evidence would resolve it: Controlled experiments comparing DeepView visualizations and quantitative metrics against standard DR techniques on the same tasks and models.

## Limitations

- The analysis relies on interpreting a single abstract and sparse section text, with confidence rated as Low due to the absence of experimental results, ablation studies, and direct methodological descriptions.
- Lack of explicit evaluation metrics beyond QkN N and Qdata, unclear sample size or dataset specifics for validation, and no information on computational resources or scalability of DeepView.
- The corpus signals reveal no closely related work, suggesting this is a novel application of DeepView to encoder LLMs, but also limiting comparative validation.

## Confidence

- High: The core claim that DeepView with discriminative distance metrics preserves class boundaries in pre-trained models is well-supported by the abstract and section text, particularly the QkN N error reduction from 31% to 3% for pre-trained models.
- Medium: The assertion that fine-tuned models inherently incorporate task-relevant structure is reasonable but relies on indirect evidence (minimal QkN N error change) and assumes fine-tuning sufficiently restructures the embedding space.
- Low: The claim that DeepView successfully identifies adversarial samples is based on a single qualitative observation (white coloration in uncertain regions) without quantitative validation or discussion of attack methods.

## Next Checks

1. Replicate QkN N Error Analysis: Re-run DeepView with λ = 1, 0.8, 0.6 on a pre-trained BERT model using SST2 or another GLUE task. Compute QkN N errors and verify the reduction from 31% to 3% as claimed. Include fine-tuned models to confirm minimal error change (~1%).

2. Adversarial Sample Detection Experiment: Generate adversarial examples (e.g., using FGSM or PGD) for a fine-tuned BERT model on SST2. Apply DeepView with λ = 0.8 and visually inspect for high-uncertainty regions. Quantify detection accuracy by comparing identified uncertain regions to known adversarial samples.

3. Multi-Task Synergy Analysis: Train a multi-task BERT model on RTE and MNLI tasks. Apply DeepView and use clustering or QN N/LCMC scores to quantify dataset separation and task synergies. Validate whether RTE and MNLI show closer alignment compared to other task pairs.