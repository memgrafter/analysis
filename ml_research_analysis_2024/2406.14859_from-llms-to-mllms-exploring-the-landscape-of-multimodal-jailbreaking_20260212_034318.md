---
ver: rpa2
title: 'From LLMs to MLLMs: Exploring the Landscape of Multimodal Jailbreaking'
arxiv_id: '2406.14859'
source_url: https://arxiv.org/abs/2406.14859
tags:
- arxiv
- jailbreak
- preprint
- safety
- multimodal
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper surveys the landscape of jailbreaking research for both
  Large Language Models (LLMs) and Multimodal Large Language Models (MLLMs). It highlights
  recent advancements in evaluation benchmarks, attack techniques, and defense strategies.
---

# From LLMs to MLLMs: Exploring the Landscape of Multimodal Jailbreaking

## Quick Facts
- arXiv ID: 2406.14859
- Source URL: https://arxiv.org/abs/2406.14859
- Authors: Siyuan Wang; Zhuohan Long; Zhihao Fan; Zhongyu Wei
- Reference count: 29
- Primary result: Comprehensive survey of jailbreaking research for both LLMs and MLLMs, identifying underexplored areas in multimodal jailbreaking and proposing future research directions.

## Executive Summary
This paper provides a comprehensive survey of jailbreaking research, contrasting the well-explored domain of Large Language Model (LLM) jailbreaking with the nascent field of Multimodal Large Language Model (MLLM) jailbreaking. It systematically reviews existing evaluation benchmarks, attack methodologies, and defense strategies for both unimodal and multimodal contexts. The study highlights that while significant progress has been made in understanding and mitigating text-based jailbreak attacks on LLMs, multimodal jailbreaking remains largely underexplored, with limited benchmarks and detection methods tailored to image-text interactions. The authors outline key limitations in current research and propose actionable directions for advancing the robustness and security of MLLMs.

## Method Summary
The paper employs a systematic literature review approach, synthesizing findings from existing research on jailbreaking techniques for both LLMs and MLLMs. It categorizes attack methods, defense mechanisms, and evaluation benchmarks, identifying gaps and trends in the literature. The analysis is grounded in the current state of multimodal model development and threat landscapes, with an emphasis on the unique challenges posed by the integration of visual and textual modalities.

## Key Results
- Unimodal jailbreaking is well-studied, with numerous benchmarks and defense strategies established for LLMs.
- Multimodal jailbreaking remains underexplored, with limited benchmarks and detection methods for MLLMs.
- The paper identifies critical research gaps, including the need for diverse multimodal tasks, improved image-based detection, and adaptive defense strategies.

## Why This Works (Mechanism)
The survey works by systematically categorizing and comparing the state of jailbreaking research across unimodal and multimodal domains. It leverages the maturity of LLM jailbreaking literature to highlight the relative immaturity of MLLM defenses, providing a clear roadmap for future research. The mechanism of identifying gaps and proposing targeted research directions is effective because it builds on well-established findings in LLMs while acknowledging the unique challenges of multimodal inputs.

## Foundational Learning

1. **Jailbreaking (why needed)**: Understanding how attackers bypass safety constraints in LLMs and MLLMs to generate harmful or unintended outputs.
   - *Quick check*: Can you describe a common jailbreak prompt or technique?

2. **Multimodal Inputs (why needed)**: Recognizing how the integration of text and images in MLLMs creates new attack vectors not present in unimodal models.
   - *Quick check*: How might an image be used to bypass text-based safety filters?

3. **Evaluation Benchmarks (why needed)**: Assessing the effectiveness of jailbreak attacks and defenses using standardized metrics and datasets.
   - *Quick check*: What are the key differences between LLM and MLLM jailbreak benchmarks?

4. **Defense Strategies (why needed)**: Learning how to detect and mitigate jailbreak attempts through input filtering, adversarial training, and other techniques.
   - *Quick check*: What are the main limitations of current MLLM defense mechanisms?

## Architecture Onboarding

**Component Map**: Text input -> Safety filter -> LLM/MLLM -> Output
**Critical Path**: Attack generation -> Model inference -> Safety violation detection
**Design Tradeoffs**: Balancing model performance and safety, handling multimodal inputs, and adapting defenses to evolving attack techniques.
**Failure Signatures**: Safety filters bypassed, harmful outputs generated, inconsistent behavior across modalities.
**First Experiments**:
1. Test a standard LLM jailbreak attack on a unimodal model.
2. Attempt a multimodal jailbreak using both text and image inputs on an MLLM.
3. Evaluate the effectiveness of a basic image-based detection method for multimodal jailbreak attempts.

## Open Questions the Paper Calls Out
The paper identifies several open questions, including:
- How can benchmarks for MLLM jailbreaking be made more diverse and complex?
- What are the most effective image-based detection methods for multimodal jailbreak attempts?
- How can defense strategies be adapted to address evolving and sophisticated attack techniques in the multimodal domain?

## Limitations
- The field of MLLM jailbreaking is underexplored, leading to a reliance on a limited set of studies.
- Lack of standardized benchmarks and evaluation protocols for MLLMs introduces uncertainty about generalizability.
- The rapid pace of development in this domain may render some discussed methods outdated.

## Confidence
- **Medium**: While the identification of research gaps is well-supported, the absence of empirical validation and systematic comparisons limits confidence in proposed directions.

## Next Checks
1. Design and implement a standardized, diverse benchmark for evaluating MLLM jailbreaking that includes both text and image-based attack scenarios.
2. Conduct head-to-head experiments comparing the robustness of leading MLLMs against a battery of established and novel jailbreak attacks.
3. Test the effectiveness of existing defense mechanisms (e.g., input filtering, adversarial training) on multimodal jailbreak attempts, and assess their adaptability to evolving attack strategies.