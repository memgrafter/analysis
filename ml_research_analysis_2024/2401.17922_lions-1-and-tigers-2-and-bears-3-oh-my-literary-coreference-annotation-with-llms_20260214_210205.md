---
ver: rpa2
title: '[Lions: 1] and [Tigers: 2] and [Bears: 3], Oh My! Literary Coreference Annotation
  with LLMs'
arxiv_id: '2401.17922'
source_url: https://arxiv.org/abs/2401.17922
tags:
- coreference
- annotation
- language
- pages
- computational
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The authors demonstrate that fine-tuning generative large language
  models for coreference annotation yields high performance on literary text. Using
  a T5-3B model, they achieve 91.03% entity recognition F1 and 80.16% coreference
  F1, with 70.72% exact string match to human-annotated outputs.
---

# [Lions: 1] and [Tigers: 2] and [Bears: 3], Oh My! Literary Coreference Annotation with LLMs

## Quick Facts
- arXiv ID: 2401.17922
- Source URL: https://arxiv.org/abs/2401.17922
- Reference count: 14
- Fine-tuned T5-3B achieves 91.03% entity recognition F1 and 80.16% coreference F1 on literary text

## Executive Summary
This paper demonstrates that fine-tuning generative large language models for coreference annotation yields high performance on literary text. Using a T5-3B model, the authors achieve 91.03% entity recognition F1 and 80.16% coreference F1, with 70.72% exact string match to human-annotated outputs. The approach requires no custom software and produces inline markdown-style annotations, making it accessible and adaptable. Results suggest seq2seq models are effective for structured linguistic annotation in domains with nuanced language.

## Method Summary
The authors fine-tune various T5, mT5, and Pythia models on the LitBank corpus of 92 novels, converting sentences into input-output pairs where outputs contain markdown-style coreference annotations. Models are trained on 3,480 sentences with validation on 174 sentences and tested on 4,560 sentences. Training uses learning rate 2e-5, weight decay 0.01, 10 epochs, and gradient checkpointing. Performance is evaluated using entity recognition F1, coreference F1 (MUC, B3, CEAFm, CEAFe, BLANC, LEA, CoNLL average), average Levenshtein distance, and exact string match percentage.

## Key Results
- T5-3B achieves highest performance with 91.03% entity recognition F1 and 80.16% coreference F1
- Exact string match between model output and human annotations reaches 70.72%
- Larger models (T5-3B) significantly outperform smaller variants on coreference annotation tasks
- Inline markdown-style annotation generation works effectively without custom architectures

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Seq2seq LLMs can directly generate inline markdown-style coreference annotations without custom architectures
- Mechanism: The model learns to copy input text while inserting bracketed entity mentions with cluster indices, leveraging its pretraining on large text corpora to understand both entity boundaries and coreference relationships
- Core assumption: The model's pretraining provides sufficient linguistic knowledge to recognize entities and their references, and the fine-tuning data is representative enough to learn the annotation format
- Evidence anchors:
  - [abstract] "New language-model-based seq2seq systems present the opportunity to solve both these problems by learning to directly generate a copy of an input sentence with markdown-like annotations"
  - [section] "Generative large language models (LLMs) have recently demonstrated a capacity to solve both problems (Bohnet et al., 2023; Zhang et al., 2023)"
  - [corpus] Weak - the paper doesn't provide corpus-level evidence about pretraining data composition
- Break condition: The model fails to generate valid markdown output, produces excessive hallucinations, or cannot learn the mapping between entities and cluster indices

### Mechanism 2
- Claim: Fine-tuning on domain-specific data (literary text) improves coreference annotation performance compared to general models
- Mechanism: The fine-tuning process adapts the model's general linguistic knowledge to the specific patterns, vocabulary, and coreference conventions found in literary texts, allowing it to better handle subtle inferences and varied language
- Core assumption: Literary texts have unique characteristics that make them challenging for general NLP models, and these characteristics are learnable through fine-tuning
- Evidence anchors:
  - [abstract] "Literary text involves subtle inferences and highly varied language"
  - [section] "Zhang et al. (2023) achieve high performance on the LitBank corpus when data from the corpus is included in the fine-tuning dataset; we seek to further explore the capabilities of a model adapted specifically for literary coreference"
  - [corpus] Weak - the paper doesn't analyze the specific linguistic features of the LitBank corpus in detail
- Break condition: The model overfits to the training data, fails to generalize to unseen literary styles, or performs worse than general models on test data

### Mechanism 3
- Claim: Larger T5 models achieve better coreference annotation performance due to their increased capacity to learn complex patterns
- Mechanism: The additional parameters in larger models allow them to better capture the nuanced relationships between entities and their references, handle more complex entity structures (nested entities, multi-word entities), and reduce formatting errors
- Core assumption: The performance improvement from larger models is due to increased model capacity rather than other factors like different pretraining data or architecture differences
- Evidence anchors:
  - [section] "Larger models do better. The smaller T5 models, particularly t5-small, struggle to accurately match brackets and parentheses"
  - [section] "Of the models tested, fine-tuned t5-3b achieves the highest performance"
  - [corpus] Weak - the paper doesn't provide evidence about how model size affects performance on specific types of literary text
- Break condition: The largest model doesn't significantly outperform smaller models, or the performance gain doesn't justify the increased computational cost

## Foundational Learning

- Concept: Markdown-style text formatting
  - Why needed here: The model needs to understand and generate the specific bracket-based annotation format used for coreference
  - Quick check question: Can you write a sentence with inline annotations for entities "Alice" and "Bob" who are coreferent, using the format [Alice: 1] met [Bob: 1]?

- Concept: Coreference resolution fundamentals
  - Why needed here: Understanding what coreference is and why it's important for tasks like entity recognition and relationship extraction
  - Quick check question: In the sentence "John said he would come", what does "he" refer to and why?

- Concept: Seq2seq model architecture
  - Why needed here: Understanding how encoder-decoder models work and why they're suitable for generating structured text outputs
  - Quick check question: What's the difference between encoder-only, decoder-only, and encoder-decoder model architectures, and when would you use each?

## Architecture Onboarding

- Component map: Input text -> Encoder (T5) -> Decoder (T5) -> Output text with annotations
- Critical path: Text input -> Tokenization -> Encoding -> Decoding -> Annotation generation -> Post-processing -> Evaluation
- Design tradeoffs: T5 vs. other model families (mT5 for multilingual, Pythia for decoder-only), model size vs. performance vs. computational cost, inline annotation vs. separate coreference steps
- Failure signatures: Inability to reproduce input text, excessive hallucinations, bracket matching errors, nested entity annotation failures, poor generalization to new literary styles
- First 3 experiments:
  1. Fine-tune a small T5 model on a subset of LitBank data and evaluate on held-out sentences to establish baseline performance
  2. Compare different model sizes (small, base, large) on the same task to determine the optimal size-performance tradeoff
  3. Test the fine-tuned model on literary texts outside the LitBank corpus to evaluate generalization capabilities

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How would fine-tuned models perform on coreference annotation for longer contexts beyond individual sentences?
- Basis in paper: [explicit] The authors propose to pre-pend all previously identified entities to each successive input for future work on longer contexts.
- Why unresolved: This approach has not yet been tested or evaluated.
- What evidence would resolve it: Empirical evaluation of model performance on longer text spans (paragraphs, chapters) with entity history tracking.

### Open Question 2
- Question: Can the seq2seq models be adapted to perform more complex annotations like identifying emotional states or power dynamics between characters?
- Basis in paper: [explicit] The authors suggest this is a promising direction given the high performance on coreference, stating "we believe that the high performance of the large, encoder-decoder models like t5-3b suggests that these models may be capable of performing more complex annotations, such as identifying emotional states or power dynamics between characters."
- Why unresolved: This has not been implemented or tested.
- What evidence would resolve it: Training and evaluating models on datasets with emotional state or power dynamic annotations, comparing performance to baseline methods.

### Open Question 3
- Question: How would multilingual coreference annotation performance improve with additional training data across different languages?
- Basis in paper: [explicit] The authors note that "This performance may be boosted using additional training data, thus making it a viable option for further exploration into multi-lingual coreference annotation" when discussing mT5's performance.
- Why unresolved: The potential improvement has not been quantified or tested with expanded multilingual datasets.
- What evidence would resolve it: Training mT5 on larger multilingual coreference datasets and measuring performance gains across different language pairs.

## Limitations

- Evaluation relies entirely on a single literary corpus (LitBank) with a specific annotation scheme, limiting generalizability to other literary styles or annotation conventions
- The model's performance on nested entities and complex coreference structures remains unclear due to evaluation metrics that may not fully capture these cases
- The approach requires substantial computational resources for fine-tuning large T5 models, potentially limiting accessibility

## Confidence

**High confidence**: The claim that seq2seq LLMs can directly generate inline markdown-style coreference annotations is well-supported by the demonstrated 70.72% exact string match and successful generation of valid annotated outputs across multiple model sizes.

**Medium confidence**: The superiority of fine-tuned models over existing systems is supported by benchmark results, but the comparison lacks direct head-to-head testing against all major rule-based and neural systems on the same data splits.

**Medium confidence**: The claim that larger T5 models achieve better performance is supported by the empirical results showing t5-3b outperforming smaller variants, though the paper doesn't establish causation or rule out pretraining data differences.

## Next Checks

1. **Cross-corpus generalization test**: Evaluate the fine-tuned t5-3b model on at least two other literary corpora (e.g., LitBank-French and KoCoNovel) to assess performance on different annotation schemes and literary traditions.

2. **Nested entity stress test**: Create a targeted evaluation set with complex nested entity structures (e.g., "John's brother [the professor: 1] gave [him: 1] the book") and measure whether the model maintains high accuracy on these challenging cases.

3. **Resource-efficiency analysis**: Compare the performance-cost tradeoff by evaluating whether t5-base or t5-large achieves 90%+ of t5-3b's performance with 50% or less computational cost, establishing practical deployment guidelines.