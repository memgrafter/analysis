---
ver: rpa2
title: 'Recurrent neural networks: vanishing and exploding gradients are not the end
  of the story'
arxiv_id: '2405.21064'
source_url: https://arxiv.org/abs/2405.21064
tags:
- learning
- recurrent
- networks
- figure
- loss
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper analyzes a previously unexplored challenge in recurrent
  neural networks (RNNs): as RNNs encode longer-term memories, changes in their parameters
  cause increasingly large output variations, making gradient-based learning highly
  sensitive. This occurs even when dynamics remain stable, beyond the well-known vanishing
  and exploding gradients problem.'
---

# Recurrent neural networks: vanishing and exploding gradients are not the end of the story

## Quick Facts
- **arXiv ID**: 2405.21064
- **Source URL**: https://arxiv.org/abs/2405.21064
- **Reference count**: 40
- **Primary result**: RNNs suffer from parameter sensitivity that grows with memory length, beyond the well-known vanishing/exploding gradients problem.

## Executive Summary
This paper identifies a fundamental optimization challenge in recurrent neural networks that occurs as networks encode longer-term memories. Beyond the well-known vanishing and exploding gradients problem, the authors demonstrate that as RNNs retain information over longer time horizons, small changes in parameters cause increasingly large output variations. This sensitivity arises because each parameter update affects all recurrent steps in the sequence, causing errors to accumulate over time. The paper shows that this effect is particularly severe for networks with diagonal recurrence matrices and provides theoretical analysis supported by empirical validation on both synthetic and real data.

## Method Summary
The authors analyze RNN optimization challenges through theoretical analysis of signal propagation in linear RNNs, combined with empirical validation on synthetic and real data. They focus on linear RNNs with diagonal recurrence matrices, examining how second moments of hidden states and gradients evolve with memory length. The study uses synthetic data with varying autocorrelation properties and real data (BERT embeddings from Wikipedia). Models include linear RNNs, complex diagonal RNNs, LRUs, GRUs, and S4 architectures, trained with Adam optimizer and various initialization schemes. The theoretical framework assumes wide-sense stationarity for tractable analysis.

## Key Results
- RNNs experience a "curse of memory" where parameter sensitivity increases with memory length, even when dynamics remain stable
- Diagonal recurrence with careful parametrization (input normalization and eigenvalue reparametrization) can mitigate this sensitivity issue
- Adaptive optimizers like Adam can compensate for this sensitivity when recurrence is close to diagonal by dampening exploding directions
- The Hessian's structure becomes increasingly diagonal as memory length increases, concentrating sensitivity on few coordinates

## Why This Works (Mechanism)

### Mechanism 1
- Claim: RNNs suffer from an additional sensitivity problem beyond vanishing/exploding gradients: as the memory length increases, small parameter changes cause increasingly large output variations.
- Mechanism: Each parameter update affects every recurrent step in the sequence, so errors accumulate over time. As the network keeps information longer (λ → 1), hidden states become more sensitive to parameter perturbations.
- Core assumption: Linear diagonal recurrence ht+1 = λ ⊙ ht + xt+1 with infinite horizon and wide-sense stationary inputs.
- Evidence anchors:
  - [abstract]: "as the memory of a network increases, changes in its parameters result in increasingly large output variations, making gradient-based learning highly sensitive"
  - [section 2.1]: "Modifying the parameters θ will not only influence one update, but all of them. As the memory of the network gets longer, the hidden states keep a trace of the effects of more updates"
  - [corpus]: Weak evidence - related papers mention vanishing/exploding gradients but not this specific sensitivity mechanism
- Break condition: When the spectral radius of the recurrent Jacobian is bounded away from 1, or when recurrence is not diagonal (sensitivity distributes across matrix entries).

### Mechanism 2
- Claim: Diagonal recurrence with careful parametrization (input normalization and eigenvalue reparametrization) mitigates the curse of memory.
- Mechanism: Input normalization γ(λ) scales inputs to keep hidden state variance constant. Reparametrization λ(ω) makes the parameter space more granular near λ=1, preventing gradient explosion.
- Core assumption: γ(λ)2E[h²t] = Θ(1) and γ(λ)2λ′(ω)2E[(dλht)²] = 1 at optimality.
- Evidence anchors:
  - [abstract]: "RNN architectures with element-wise recurrence and careful parametrizations...can mitigate this issue"
  - [section 3.1]: "We aim to keep E[h²t], E[(dλht)²] and E[(dxt ht)²] independent of λ"
  - [corpus]: Weak evidence - related papers discuss initialization and normalization but not this specific dual normalization/parametrization approach
- Break condition: When input distribution changes dramatically, or when complex eigenvalue angles become problematic (angle sensitivity explodes as |λ| → 1).

### Mechanism 3
- Claim: Adaptive optimizers (Adam) can compensate for the curse of memory when recurrence is close to diagonal because sensitivity concentrates on few coordinates.
- Mechanism: In diagonal networks, the Hessian's top eigenvectors align with parameter axes, so Adam's coordinate-wise adaptive learning rates can selectively dampen exploding directions.
- Core assumption: The Hessian at optimality has large entries concentrated on few coordinates when recurrence is diagonal.
- Evidence anchors:
  - [section 4.3]: "For the complex diagonal one...the top eigenvectors are concentrated on a few coordinates...This structure makes it possible for Adam to efficiently deal with the extra sensitivity"
  - [section 5]: "Adam uses small learning rates to compensate for the sensitivity...for the complex diagonal one...it can use larger ones without hindering training stability"
  - [corpus]: No direct evidence - related papers don't discuss Hessian structure or adaptive learning rates in this context
- Break condition: When eigenvalues are not well-concentrated (many eigenvalues near 1 with varying angles), or when optimizer is not adaptive (SGD fails).

## Foundational Learning

- Concept: Wide-sense stationarity (WSS)
  - Why needed here: Enables tractable analysis of signal propagation by assuming input statistics don't change over time
  - Quick check question: What property must a random process have for RX(∆) = E[Xt+∆Xt] to be independent of t?

- Concept: Complex diagonalization
  - Why needed here: Allows decomposition of non-diagonal recurrence into independent modes for analysis
  - Quick check question: If A is diagonalizable as A = P diag(λ)P⁻¹, what is the form of ht in terms of hdiag_t?

- Concept: Wirtinger derivatives
  - Why needed here: Provides framework for differentiating complex-valued functions when computing gradients/Hessian
  - Quick check question: For real-valued loss L(z), what is the relationship between ∂L/∂z and ∂L/∂z̄?

## Architecture Onboarding

- Component map: Input normalization (γ) → Eigenvalue parametrization (λ(ω)) → Diagonal recurrence → Adam compensation
- Critical path: γ(λ) → λ(ω) → diagonal recurrence → Adam compensation
- Design tradeoffs:
  - Diagonal vs full connectivity: diagonal enables adaptive optimization but reduces expressivity
  - Input normalization: stabilizes training but requires knowledge of input distribution
  - Complex parametrization: enables richer dynamics but introduces angle sensitivity problems
- Failure signatures:
  - Loss landscape becomes extremely sharp in angle direction as |λ| → 1
  - Adam learning rates become very small for certain parameters
  - Training stalls when eigenvalues concentrate near unit circle
- First 3 experiments:
  1. Compare training curves of diagonal vs full RNN on long-sequence synthetic task
  2. Test effect of input normalization on gradient explosion with varying λ values
  3. Measure Hessian diagonalizability for different eigenvalue distributions

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal parametrization for complex eigenvalues in recurrent networks, particularly the angle component?
- Basis in paper: [explicit] The paper demonstrates that while reparametrizing the magnitude of complex eigenvalues (using exponential or tanh functions) helps mitigate exploding gradients, the angle component remains challenging. The authors show that the optimal parametrization of the angle depends on the magnitude of the eigenvalue, making learning difficult.
- Why unresolved: The paper acknowledges that the ideal parametrization for the angle of complex eigenvalues is input-distribution dependent and complex to derive. They provide theoretical insights but note that practical solutions remain elusive, particularly for angles away from optimality.
- What evidence would resolve it: Empirical studies comparing different parametrization schemes (e.g., polar, exponential, or learned parametrizations) on real-world tasks, measuring both training stability and final performance, would help determine optimal strategies.

### Open Question 2
- Question: How does the concentration of eigenvalues affect the loss landscape and optimization in recurrent networks?
- Basis in paper: [explicit] The authors show that when eigenvalues are concentrated (small θ0), the Hessian becomes less diagonal, making optimization more difficult. This effect is particularly pronounced for complex-valued diagonal RNNs compared to LRUs.
- Why unresolved: While the paper provides theoretical analysis and empirical evidence of this phenomenon, the exact relationship between eigenvalue concentration and optimization difficulty across different architectures and tasks remains unclear.
- What evidence would resolve it: Systematic experiments varying eigenvalue distributions across architectures (fully connected, block diagonal, diagonal) and tasks, measuring Hessian structure, training dynamics, and final performance, would clarify these relationships.

### Open Question 3
- Question: How well does the diagonal linear approximation apply to modern gated RNN architectures like GRUs and LSTMs?
- Basis in paper: [explicit] The authors show that GRUs at initialization behave similarly to diagonal linear networks, particularly for slowly decaying neurons. However, they acknowledge that the theory doesn't perfectly capture all aspects of these architectures.
- Why unresolved: The paper provides initial evidence of the approximation's validity but doesn't explore its limitations or how well it holds during training. The impact of different initialization schemes and architectural variations remains unexplored.
- What evidence would resolve it: Detailed analysis of signal propagation throughout training for various gated architectures, comparing theoretical predictions with empirical measurements, would reveal the approximation's accuracy and limitations.

## Limitations
- Theoretical analysis assumes linear RNNs with i.i.d. or wide-sense stationary inputs, which may not capture nonlinear networks on complex real-world data
- Empirical validation uses relatively small-scale experiments that may not generalize to larger models or longer sequences
- The study focuses on linear RNNs, limiting applicability to modern gated architectures like LSTMs and GRUs

## Confidence
- **High Confidence**: The theoretical derivation of the curse of memory and its manifestation in linear RNNs is rigorous and well-supported. The mechanism of parameter sensitivity accumulation over time is clearly demonstrated.
- **Medium Confidence**: The mitigation strategies (input normalization, eigenvalue parametrization, diagonal recurrence) are theoretically justified but their effectiveness in nonlinear networks requires further validation.
- **Low Confidence**: The claim that adaptive optimizers like Adam can effectively compensate for the curse of memory in diagonal networks is primarily based on observations from linear RNNs and needs more extensive empirical support.

## Next Checks
1. **Scale-up validation**: Test the proposed mitigation strategies (diagonal recurrence with input normalization and eigenvalue parametrization) on large-scale nonlinear RNN architectures (e.g., LSTMs, GRUs) trained on long sequences from diverse domains (language modeling, time series forecasting).

2. **Optimizer ablation**: Systematically compare training stability and performance of diagonal vs. full RNNs using different optimizers (SGD, Adam, and newer adaptive methods) on tasks with varying memory requirements to isolate the contribution of adaptive learning rates.

3. **Nonlinear generalization**: Extend the theoretical analysis to nonlinear RNNs by deriving bounds on parameter sensitivity for common activation functions and comparing them with the linear case to assess the robustness of the curse of memory phenomenon.