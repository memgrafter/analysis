---
ver: rpa2
title: Depth Separation in Norm-Bounded Infinite-Width Neural Networks
arxiv_id: '2402.08808'
source_url: https://arxiv.org/abs/2402.08808
tags:
- networks
- lemma
- learning
- depth
- network
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper studies depth separation in infinite-width neural networks
  where complexity is controlled by the overall squared $\ell2$-norm of the weights.
  Previous depth separation results focused on separation in terms of width, but those
  do not provide insight into whether depth determines if it is possible to learn
  a network that generalizes well even when the network width is unbounded.
---

# Depth Separation in Norm-Bounded Infinite-Width Neural Networks

## Quick Facts
- arXiv ID: 2402.08808
- Source URL: https://arxiv.org/abs/2402.08808
- Reference count: 40
- Primary result: Depth-3 ReLU networks can learn certain functions with polynomial sample complexity that require exponential sample complexity for depth-2 ReLU networks under norm-based regularization.

## Executive Summary
This paper establishes depth separation in infinite-width neural networks where complexity is controlled by the overall squared $\ell_2$-norm of weights rather than width. Unlike previous depth separation results that focused on width, this work demonstrates that depth fundamentally determines learnability even when width is unbounded. The key finding is that certain functions can be learned with polynomial sample complexity using depth-3 ReLU networks but require exponential sample complexity with depth-2 ReLU networks under the same norm-based regularization. The paper also shows that no reverse depth separation exists - any function learnable with polynomial samples by depth-2 networks can also be learned with polynomial samples by depth-3 networks.

## Method Summary
The paper studies depth separation through sample complexity by comparing the minimum number of samples required for norm-controlled ReLU networks of different depths to learn target functions. It defines representation cost $R_L(f)$ as the minimum squared L2 norm needed to represent function $f$ as an L-layer network. The analysis uses Pareto-optimal learning rules that select networks balancing empirical loss and representation cost. For the forward direction, it constructs oscillatory target functions with exponential $R_2$ cost but polynomial $R_3$ cost, then proves depth-2 learners require exponential samples while depth-3 learners succeed with polynomial samples. For the reverse direction, it shows that functions with small $R_2$ cost also have small $R_3$ cost (specifically $R_3 \leq 4d/3 + 4R_2/3$), ensuring depth-3 networks can always match depth-2 performance.

## Key Results
- There exists a family of functions requiring exponential (in $d$) sample complexity for depth-2 ReLU networks but only polynomial sample complexity for depth-3 ReLU networks under norm-based regularization.
- Any function learnable with polynomial sample complexity by depth-2 ReLU networks can also be learned with polynomial sample complexity by depth-3 ReLU networks.
- The sample complexity separation is proven through Rademacher complexity bounds and approximation analysis, showing depth-3 networks can achieve good generalization with far fewer samples than depth-2 networks for certain function families.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Depth-3 networks can approximate certain functions with polynomial sample complexity, while depth-2 networks require exponential sample complexity.
- Mechanism: The target functions are designed to have high $R_2$ representation cost (exponential in $d$) but low $R_3$ representation cost (polynomial in $d$). When using depth-2 networks with norm-based regularization, the interpolant has prohibitively high representation cost unless exponentially many samples are available. Depth-3 networks can find interpolants with low representation cost using only polynomially many samples.
- Core assumption: The function family has the property that approximating it in L2 norm with depth-2 networks requires exponential $R_2$ cost, while depth-3 networks can approximate it with polynomial $R_3$ cost.
- Evidence anchors:
  - [abstract] "there are functions that are learnable with sample complexity polynomial in the input dimension by norm-controlled depth-3 ReLU networks, yet are not learnable with sub-exponential sample complexity by norm-controlled depth-2 ReLU networks"
  - [section] "By fixing a function $f \in N_L$ and optimizing over its parametrizations $f = f_\phi$ as an L-layer network, we see that the above parameter space minimization problem is equivalent to the function space minimization problem"
- Break condition: If the function family doesn't have the required $R_2/R_3$ cost properties, or if the sample complexity bounds are not tight enough to demonstrate the separation

### Mechanism 2
- Claim: Functions with small $R_2$ cost also have small $R_3$ cost, ensuring no reverse depth separation.
- Mechanism: Adding an identity layer to a depth-2 network converts it to a depth-3 network with only a small increase in representation cost. Specifically, $R_3 \leq 4d/3 + 4R_2/3$ for all functions, meaning any function learnable with polynomial samples using depth-2 networks can also be learned with polynomial samples using depth-3 networks.
- Core assumption: The relationship $R_3 \leq 4d/3 + 4R_2/3$ holds for all functions, providing a bridge between the two depths.
- Evidence anchors:
  - [section] "Given $f \in N_2,\omega$, we have $f \in N_3,\max(\omega,4d)$ and $R_3(f; \max(\omega, 4d)) \leq 4d/3 + 4R_2(f; \omega)/3$"
- Break condition: If the bound on how $R_3$ relates to $R_2$ is too loose, or if there exist functions with polynomial $R_2$ cost but exponential $R_3$ cost

### Mechanism 3
- Claim: The learning rules $A_{L}^{\theta,\alpha}$ and $A_L^*$ effectively separate the Pareto frontier of (LS, RL) tradeoffs, allowing comparison between depth-2 and depth-3 performance.
- Mechanism: $A_L^*$ represents the best possible depth-L learning by choosing the Pareto optimal point minimizing population error, while $A_{L}^{\theta,\alpha}$ represents a practical depth-L learner with a threshold on empirical loss. The separation is shown by demonstrating $A_2^*$ fails to learn with polynomial samples while $A_{3}^{\theta,\alpha}$ succeeds, but $A_2^*$ succeeding implies $A_{3}^{\theta,\alpha}$ also succeeds.
- Core assumption: The Pareto frontier characterization correctly captures the inductive bias of norm-based learning, and the Rademacher complexity bounds provide tight enough estimation error guarantees.
- Evidence anchors:
  - [section] "Given $\theta \geq 0$, define $A_L^\theta$ to be a learning rule which, given training samples $S$, selects an L-layer network such that $L_S(A_L^\theta(S)) \leq \theta$ and $R_L(A_L^\theta(S)) = \inf_{f \in N_L} L_S(f) \leq \theta R_L(f)$"
  - [section] "By Lemma A.19 and the union bound, with probability at least $1 - \delta$ we have that $|L_S(A_{3}^{\theta,\alpha,\omega}(S)) - L_D(A_{3}^{\theta,\alpha,\omega}(S))| = O(R_3(A_{3}^{\theta,\alpha,\omega}(S); \omega)^{3/2} \log(1/\delta)|S|^{-1/2})$"
- Break condition: If the Rademacher complexity bounds are too loose, or if the practical learning rule $A_{L}^{\theta,\alpha}$ doesn't effectively explore the Pareto frontier

## Foundational Learning

- Concept: Rademacher complexity and its role in generalization bounds
  - Why needed here: The proof of depth separation relies on Rademacher complexity bounds to establish that depth-3 networks can generalize with polynomial samples while depth-2 cannot
  - Quick check question: How does the Rademacher complexity of the set of depth-3 networks with bounded $R_3$ cost scale with the sample size and representation cost?

- Concept: Representation cost $R_L(f)$ as a complexity measure for infinite-width networks
  - Why needed here: The paper studies depth separation in terms of representation cost rather than width, requiring understanding of how $R_L(f)$ captures the complexity of functions in infinite-width networks
  - Quick check question: How does the representation cost $R_L(f)$ relate to the weight decay regularization parameter $\lambda$ in the regularized risk minimization framework?

- Concept: Pareto optimality in multi-objective optimization for learning
  - Why needed here: The learning rules are defined as selecting Pareto optimal points from the tradeoff between empirical loss and representation cost, which is central to the depth separation argument
  - Quick check question: What conditions must a function satisfy to be Pareto optimal for the bicriterion minimization problem involving empirical loss and representation cost?

## Architecture Onboarding

- Component map:
  - Function class $N_L$: Depth-L neural networks with unbounded width
  - Representation cost $R_L(f)$: Minimum squared L2 norm of weights needed to represent $f$
  - Learning rules: $A_L^\theta$ (threshold-based), $A_{L}^{\theta,\alpha}$ (relaxed), $A_L^*$ (idealized)
  - Sample complexity: Number of samples needed to achieve $\epsilon$ generalization error
  - Target functions: Family $f_d(x) = \psi_{3d}(\sqrt{d}\langle x^{(1)}, x^{(2)}\rangle)$ with sawtooth activation

- Critical path:
  1. Construct target function family with exponential $R_2$ cost but polynomial $R_3$ cost
  2. Show depth-2 learner $A_2^*$ requires exponential samples to learn (interpolant has high $R_2$ cost)
  3. Show depth-3 learner $A_{3}^{\theta,\alpha}$ can learn with polynomial samples (good approximation exists with low $R_3$ cost)
  4. Prove no reverse separation (functions with low $R_2$ cost also have low $R_3$ cost)

- Design tradeoffs:
  - Using depth-3 instead of deeper networks: Simpler analysis but may miss separation at even deeper levels
  - Focusing on $R_2$ vs $R_3$ costs: Captures norm-based complexity but may not reflect other architectural differences
  - Polynomial vs exponential sample complexity separation: Strong result but may be too coarse for practical insights

- Failure signatures:
  - If Rademacher complexity bounds are too loose, the depth separation may disappear
  - If the function family doesn't have the required $R_2/R_3$ cost properties, the mechanism breaks
  - If the learning rules don't effectively explore the Pareto frontier, the comparison becomes invalid

- First 3 experiments:
  1. Implement the target function family $f_d(x) = \psi_{3d}(\sqrt{d}\langle x^{(1)}, x^{(2)}\rangle)$ and verify its properties (Lipschitz constant, oscillation rate)
  2. Compute $R_2$ and $R_3$ costs for simple functions (e.g., linear, quadratic) to understand the relationship between depths
  3. Simulate the learning rules $A_2^*$ and $A_{3}^{\theta,\alpha}$ on synthetic data from $f_d$ to observe the sample complexity difference empirically

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does depth separation in sample complexity persist when the target functions have bounded Lipschitz constants?
- Basis in paper: [inferred] The paper acknowledges that their depth separation results rely on highly oscillatory functions with unbounded Lipschitz constants, and references Safran et al. (2019) discussing evidence that current techniques cannot establish depth separation with bounded Lipschitz constants.
- Why unresolved: The paper uses a sawtooth function construction that becomes increasingly oscillatory as dimension grows, making the Lipschitz constant unbounded. While this provides a clean separation result, such highly oscillatory functions may not be representative of practical scenarios where functions tend to be smoother.
- What evidence would resolve it: A proof or counterexample showing whether there exists a family of functions with bounded Lipschitz constants that demonstrates depth separation in sample complexity between depth-2 and depth-3 networks.

### Open Question 2
- Question: Can tighter bounds be established for the sample complexity required for depth-3 networks to achieve learning?
- Basis in paper: [explicit] The paper states "The relatively restrictive assumptions on the distribution of x in Theorem 5.2 can be relaxed" and "We conjecture that smaller sample complexities for depth-3 learning are possible in both results."
- Why unresolved: The current analysis yields polynomial sample complexity bounds for depth-3 networks, but with high polynomial degrees (particularly involving terms like $d^{15}$). The authors suggest these bounds are loose and that smaller sample complexities might be achievable with alternative constructions.
- What evidence would resolve it: Explicit construction of target functions or distributions that can be learned by depth-3 networks with improved (lower-degree) polynomial sample complexity bounds, or improved analytical techniques that yield tighter bounds on the Rademacher complexity or generalization error for depth-3 networks.

### Open Question 3
- Question: How do optimization dynamics affect depth separation beyond the Pareto optimal solutions considered in this work?
- Basis in paper: [explicit] The conclusion section states "A major open question is how optimization dynamics affect depth separation" and notes that the work "focuses on the output of learning rules seeking (approximately) Pareto optimal solutions, but neglects optimization dynamics."
- Why unresolved: The paper's analysis assumes access to learning rules that can find (approximately) Pareto optimal solutions, which may not reflect practical optimization procedures like gradient descent. The implicit bias of different optimization algorithms and their interaction with depth could lead to different depth separation phenomena.
- What evidence would resolve it: Empirical or theoretical analysis comparing the performance of depth-2 versus depth-3 networks when trained with practical optimization algorithms (e.g., SGD with different learning rates and architectures), or analytical characterization of the implicit bias induced by optimization dynamics in infinite-width networks of different depths.

## Limitations

- The depth separation relies on highly oscillatory target functions with unbounded Lipschitz constants, which may not represent practical function classes
- The analysis assumes idealized learning rules that can find Pareto-optimal solutions, not reflecting practical optimization procedures
- The Rademacher complexity bounds contain unspecified constants that could affect the tightness of the sample complexity separation

## Confidence

- Forward direction (depth-2 requires exponential, depth-3 requires polynomial): Medium
- Reverse direction (no reverse separation): High

## Next Checks

1. Compute explicit constants for the Rademacher complexity bound $O(M^3 \sqrt{\log(1/\delta)/|S|})$ on synthetic data to verify they don't weaken the depth separation
2. Test the target function construction $f_d(x) = \psi_{3d}(\sqrt{d}\langle x^{(1)}, x^{(2)}\rangle)$ with varying $d$ values to empirically observe the $R_2$ vs $R_3$ cost separation
3. Verify the approximation bounds for depth-2 vs depth-3 networks on the sawtooth activation function to ensure the exponential vs polynomial cost gap