---
ver: rpa2
title: Investigating Decoder-only Large Language Models for Speech-to-text Translation
arxiv_id: '2407.03169'
source_url: https://arxiv.org/abs/2407.03169
tags:
- speech
- language
- translation
- text
- llms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates using decoder-only LLMs for speech-to-text
  translation (S2TT). The authors propose a decoder-only architecture that directly
  consumes encoded speech representations and generates text translations, without
  discretizing speech tokens as in prior work.
---

# Investigating Decoder-only Large Language Models for Speech-to-text Translation

## Quick Facts
- arXiv ID: 2407.03169
- Source URL: https://arxiv.org/abs/2407.03169
- Reference count: 0
- Achieves 37.1 BLEU on CoVoST 2 X-En directions and 23.4 COMET on FLEURS

## Executive Summary
This paper investigates the use of decoder-only large language models for speech-to-text translation (S2TT), proposing an architecture that directly consumes encoded speech representations to generate text translations without speech token discretization. The authors compare parameter-efficient fine-tuning methods including LNA, LoRA, and freezing components, achieving state-of-the-art performance on CoVoST 2 and FLEURS benchmarks among models trained without proprietary data. The proposed decoder-only approach eliminates the need for speech tokenization while maintaining competitive translation quality.

## Method Summary
The authors propose a decoder-only LLM architecture for S2TT that directly processes encoded speech representations from a frozen speech encoder. The model is fine-tuned using parameter-efficient methods, with LNA (Lightweight and Configurable Adaptation) showing superior performance compared to LoRA and freezing approaches. The architecture maintains the speech encoder frozen while adapting the decoder weights through various fine-tuning strategies, enabling efficient adaptation to speech-to-text translation tasks without full model retraining.

## Key Results
- Achieves 37.1 BLEU on CoVoST 2 X-En translation directions
- Achieves 23.4 COMET score on FLEURS benchmark
- LNA fine-tuning outperforms LoRA and freezing approaches

## Why This Works (Mechanism)
The decoder-only approach works by leveraging the strong language modeling capabilities of LLMs while eliminating the complexity of speech tokenization. By directly consuming continuous speech embeddings from a frozen encoder, the model can focus on learning the translation mapping without being constrained by discrete speech token boundaries. This approach allows the decoder to maintain its pre-trained linguistic knowledge while adapting to the specific characteristics of speech-to-text translation.

## Foundational Learning
- **Speech Encoding**: Converting raw audio to continuous embeddings; needed to represent acoustic information in a format LLMs can process; quick check: verify encoder output dimensionality matches LLM input requirements
- **Parameter-efficient Fine-tuning**: Methods like LNA and LoRA that adapt large models with fewer parameters; needed to make LLM adaptation computationally feasible; quick check: confirm parameter count reduction vs full fine-tuning
- **BLEU Score**: Standard metric for translation quality; needed to compare against existing S2TT systems; quick check: ensure multi-reference scoring when available
- **COMET Score**: Reference-free translation evaluation metric; needed for comprehensive quality assessment; quick check: verify model compatibility with evaluation framework
- **Decoder-only Architecture**: LLM design without encoder component; needed to simplify the S2TT pipeline; quick check: confirm decoder can handle continuous speech embeddings

## Architecture Onboarding
**Component Map**: Speech Encoder -> LLM Decoder -> Text Output

**Critical Path**: The core translation pipeline flows from frozen speech encoder features through the adapted LLM decoder to generate translated text. The critical optimization decision involves choosing between LNA, LoRA, or freezing strategies for decoder adaptation.

**Design Tradeoffs**: The main tradeoff involves balancing translation quality against computational efficiency. Using a frozen speech encoder reduces training complexity but may limit adaptation potential. The choice of fine-tuning method (LNA vs LoRA) affects both performance and parameter count.

**Failure Signatures**: Performance degradation when freezing the speech encoder indicates the model's reliance on fine-tuning both components. Suboptimal fine-tuning methods may lead to catastrophic forgetting of pre-trained LLM capabilities while failing to learn effective speech-to-text mappings.

**3 First Experiments**:
1. Compare LNA vs LoRA fine-tuning on a small validation set to establish baseline performance differences
2. Test freezing different proportions of the speech encoder to quantify adaptation requirements
3. Evaluate translation quality on held-out speech samples with known transcriptions to verify encoder effectiveness

## Open Questions the Paper Calls Out
None

## Limitations
- Experimental scope limited to CoVoST 2 and FLEURS benchmarks only
- Statistical significance of LNA vs LoRA comparisons not reported
- Performance degradation from freezing speech encoder lacks quantitative detail
- SOTA claims exclude proprietary models without clear definition of "proprietary data"

## Confidence
- BLEU/COMET results on CoVoST 2 and FLEURS: High
- LNA vs LoRA performance comparison: Medium
- Generalizability to other S2TT scenarios: Low

## Next Checks
1. Test the proposed decoder-only architecture on at least two additional speech-to-text translation benchmarks (e.g., MUST-C, MuST-SHE) to assess domain generalization.
2. Conduct ablation studies with statistical significance testing across multiple random seeds to verify the LNA vs LoRA performance difference.
3. Evaluate model performance degradation when progressively freezing different components (encoder, decoder layers) with quantitative metrics to better understand the "hurts performance" observation.