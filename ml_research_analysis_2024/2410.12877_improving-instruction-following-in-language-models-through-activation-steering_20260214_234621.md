---
ver: rpa2
title: Improving Instruction-Following in Language Models through Activation Steering
arxiv_id: '2410.12877'
source_url: https://arxiv.org/abs/2410.12877
tags:
- steering
- instructions
- language
- https
- instruction
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a method for improving instruction-following
  in large language models through activation steering. The approach computes vector
  representations as differences in model activations between inputs with and without
  instructions, then applies these vectors during inference to guide model behavior.
---

# Improving Instruction-Following in Language Models through Activation Steering

## Quick Facts
- arXiv ID: 2410.12877
- Source URL: https://arxiv.org/abs/2410.12877
- Reference count: 40
- Key outcome: Activation steering improves instruction-following from ~10% to ~30% accuracy for format instructions without explicit text instructions, and works across multiple model families

## Executive Summary
This paper introduces activation steering as a method to improve instruction-following in language models without modifying model weights. The approach computes vector representations as differences in model activations between inputs with and without instructions, then applies these vectors during inference to guide model behavior. The method successfully improves instruction adherence for fine-grained instructions like output format, length constraints, and word inclusion/exclusion across four different models (Phi-3, Gemma 2, Mistral).

## Method Summary
The method computes steering vectors via a difference-in-means approach, calculating the difference in residual stream activations at the last input token between inputs with and without instructions. These vectors are then applied during inference at a selected layer, with the steering weight dynamically computed to map the model's activations to their mean value when the instruction is present. The approach is evaluated on the IFEval dataset across multiple models and instruction types, with both individual instruction steering and multi-instruction steering capabilities demonstrated.

## Key Results
- Activation steering improves format instruction adherence from ~10% to ~30% accuracy without explicit text instructions
- The method enhances performance even when instructions are present in the input
- Cross-model transferability demonstrated: vectors from instruction-tuned models improve base models' instruction-following capabilities
- Multi-instruction steering successfully combines multiple instruction vectors for compound behaviors

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Linear representations of instructions exist in the residual stream of language models.
- **Mechanism:** The model's residual stream activations at the last input token capture instruction-relevant features that can be isolated by computing the difference between inputs with and without instructions (difference-in-means approach).
- **Core assumption:** Instructions are represented linearly in activation space, meaning their representation can be captured as a difference vector.
- **Evidence anchors:**
  - [abstract] "These vectors are computed as the difference in activations between inputs with and without instructions"
  - [section] "High similarity in set (2) would suggest that the model captures a shared feature (the instruction)"
  - [corpus] Weak - the corpus papers focus on sparse autoencoders and sparse spaces rather than linear representations
- **Break condition:** If instructions are not linearly represented in activation space, the difference-in-means approach would fail to isolate meaningful instruction vectors.

### Mechanism 2
- **Claim:** Steering vectors can be dynamically scaled to achieve the desired instruction-following behavior.
- **Mechanism:** The steering weight c is computed to map the model's residual stream activations to the mean value on inputs containing the instruction, using the formula c = z̄ - x'_l u_l where z̄ is the mean activation when instruction is present.
- **Core assumption:** There exists a scaling relationship between the steering vector and the desired change in activation space that can be computed from the training data.
- **Evidence anchors:**
  - [abstract] "The steering vector cul is then added to the corresponding residual stream layer"
  - [section] "we use a systematic scaling approach where the value of c is selected to ensure that the residual stream activations are mapped to their mean value"
  - [corpus] Moderate - papers like "SAIF" and "Sparse Activation Editing" suggest sparse representations rather than continuous scaling
- **Break condition:** If the relationship between steering vector magnitude and behavioral change is non-linear or discontinuous, the dynamic scaling approach would fail.

### Mechanism 3
- **Claim:** Cross-model steering is possible because instruction representations are transferable between models.
- **Mechanism:** Steering vectors computed on instruction-tuned models can be applied to base models to improve instruction-following, suggesting that the underlying representations are similar across model variants.
- **Core assumption:** Instruction-tuned and base models share similar internal representations for instructions, likely because instruction tuning doesn't significantly alter the model's weights.
- **Evidence anchors:**
  - [abstract] "Finally, we demonstrate that steering vectors computed on instruction-tuned models can transfer to improve base models"
  - [section] "We observe that subtracting the inclusion vectors significantly reduces the frequency of the undesired keywords"
  - [corpus] Moderate - papers like "SAIF" and "Sparse Activation Editing" suggest sparse representations rather than continuous scaling
- **Break condition:** If instruction-tuned models develop fundamentally different internal representations compared to base models, cross-model steering would fail.

## Foundational Learning

- **Concept:** Residual stream and activation space
  - Why needed here: The method operates by directly modifying residual stream activations during inference, so understanding this architecture is fundamental
  - Quick check question: What is the dimensionality of the residual stream in typical transformer models, and at which layer does activation steering typically occur?

- **Concept:** Difference-in-means computation
  - Why needed here: The core methodology for computing steering vectors relies on comparing activations between instruction-present and instruction-absent inputs
  - Quick check question: How does the difference-in-means approach isolate instruction-specific features from the activation space?

- **Concept:** Cosine similarity for representation analysis
  - Why needed here: The paper uses cosine similarity to verify that the model captures instruction-relevant features in its activations
  - Quick check question: What does high cosine similarity between different inputs with the same instruction indicate about the model's internal representation?

## Architecture Onboarding

- **Component map:** Input processing → Residual stream computation → Steering vector application → Output generation. The key components are the residual stream at the last input token, the steering vector computation layer, and the layer where steering is applied during inference.

- **Critical path:** The most critical path is computing the steering vector (difference-in-means), selecting the appropriate layer and weight, and applying the vector during inference. Any failure in these steps will prevent the method from working.

- **Design tradeoffs:** The method trades computational efficiency (dynamic weight computation) for precision in instruction following. It also assumes linear representations, which may not capture all instruction types.

- **Failure signatures:** Poor instruction following despite steering, quality score drops indicating the model is over-steered, or steering vectors that don't improve performance even with optimal layer selection.

- **First 3 experiments:**
  1. Verify residual stream similarity between inputs with same instruction vs different instructions using cosine similarity
  2. Compute steering vectors for a simple instruction (like "JSON format") and apply them to confirm improvement
  3. Test cross-model steering by applying vectors from an instruction-tuned model to its base counterpart

## Open Questions the Paper Calls Out

- **Question:** How do instruction representations vary across different model architectures and sizes?
  - Basis in paper: Inferred from the cross-model steering experiments in Section 7, which showed that vectors computed on instruction-tuned models can improve base models' behavior, but with varying effectiveness across different base model sizes (Gemma 2B vs 9B).
  - Why unresolved: The paper only tested cross-model steering between base and instruction-tuned versions of the same model family. It's unclear how well instruction representations transfer between completely different architectures or model families.
  - What evidence would resolve it: Experiments testing cross-model steering between different model families (e.g., using Phi-3 instruction-tuned vectors to steer Mistral base models, or vice versa).

- **Question:** What is the relationship between the linear representation hypothesis and the effectiveness of activation steering for instruction-following?
  - Basis in paper: The paper relies on the linear representation hypothesis for computing steering vectors, but recent work suggests not all features may be linearly encoded. This is mentioned in the Related Work section discussing the linearity assumption.
  - Why unresolved: The paper demonstrates that linear steering vectors can improve instruction-following, but doesn't investigate whether this effectiveness is limited to linearly encoded features or extends to non-linear representations as well.
  - What evidence would resolve it: Experiments comparing the effectiveness of linear steering vectors versus non-linear steering methods (e.g., sparse autoencoders or other non-linear feature extraction techniques) for improving instruction-following.

- **Question:** How does the quality of steering vectors vary with the diversity and complexity of base queries used for their computation?
  - Basis in paper: Inferred from the methodology section describing how steering vectors are computed by averaging differences between inputs with and without instructions across multiple base queries.
  - Why unresolved: The paper uses a fixed number of examples (20 for word-specific instructions, 50 for length instructions) but doesn't investigate how the quality of steering vectors scales with more diverse or complex base queries, or whether there's a point of diminishing returns.
  - What evidence would resolve it: Systematic experiments varying the number and diversity of base queries used to compute steering vectors, measuring the impact on instruction-following accuracy and quality score.

## Limitations
- The method assumes linear representations of instructions in activation space, which may not capture all instruction types
- Cross-model transferability has only been demonstrated between instruction-tuned and base versions of the same model family
- The approach may struggle with compound instructions that combine multiple constraints

## Confidence
- **High confidence:** The core methodology of computing steering vectors via difference-in-means and applying them during inference is technically sound and well-validated through multiple experiments
- **Medium confidence:** The effectiveness claims for individual instruction types (format, length, word constraints) are supported by experiments, though the sample sizes and diversity of instructions tested could be expanded
- **Low confidence:** The scalability and generalization claims to more complex instruction-following scenarios and vastly different model architectures need further validation

## Next Checks
1. **Representation Analysis:** Conduct ablation studies to verify that instruction representations remain linear across different instruction types and model scales, testing the fundamental assumption of the difference-in-means approach

2. **Cross-Architecture Transfer:** Test steering vector transferability beyond instruction-tuned vs base model pairs by applying vectors from one model family (e.g., Phi-3) to another (e.g., Llama), assessing the limits of cross-model generalization

3. **Complex Instruction Handling:** Evaluate the method's effectiveness on compound instructions that combine multiple constraints (e.g., "write a 100-word JSON response without using the word 'data'"), testing the method's ability to handle more realistic instruction-following scenarios