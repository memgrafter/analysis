---
ver: rpa2
title: 'Transformers are SSMs: Generalized Models and Efficient Algorithms Through
  Structured State Space Duality'
arxiv_id: '2405.21060'
source_url: https://arxiv.org/abs/2405.21060
tags:
- attention
- matrix
- state
- structured
- sequence
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper establishes a theoretical framework called structured
  state space duality (SSD) that connects state space models (SSMs) like Mamba with
  variants of attention through structured semiseparable matrices. The authors show
  that SSMs and linear attention variants are dual representations of the same underlying
  mathematical structure, where SSMs have both linear (recurrent) and quadratic (attention-like)
  forms.
---

# Transformers are SSMs: Generalized Models and Efficient Algorithms Through Structured State Space Duality

## Quick Facts
- arXiv ID: 2405.21060
- Source URL: https://arxiv.org/abs/2405.21060
- Authors: Tri Dao; Albert Gu
- Reference count: 40
- Primary result: Mamba-2 architecture is 2-8x faster than Mamba while matching Transformer performance on language tasks

## Executive Summary
This paper establishes a theoretical framework called structured state space duality (SSD) that unifies state space models (SSMs) like Mamba with attention-based Transformers through the lens of structured semiseparable matrices. The authors demonstrate that SSMs and linear attention variants are dual representations of the same underlying mathematical structure, enabling a new architecture called Mamba-2 that leverages both modes efficiently. Mamba-2 achieves 2-8x speedup over Mamba while maintaining competitive performance with Transformers on language modeling tasks across various model sizes.

## Method Summary
The paper introduces structured state space duality (SSD), a theoretical framework connecting SSMs and attention through structured semiseparable matrices. The key insight is that SSMs possess both linear (recurrent) and quadratic (attention-like) forms, which can be computed efficiently using structured matrix algorithms. Mamba-2 is designed to exploit this duality by maintaining both representations and switching between them based on computational efficiency. The architecture uses diagonal state spaces with input-dependent parameters, enabling selective token processing while supporting large state sizes without slowdown.

## Key Results
- Mamba-2 achieves 2-8x speedup over Mamba across different hardware configurations
- Maintains competitive perplexity with Transformers on pretraining tasks
- Matches or outperforms Mamba and Transformer baselines on downstream tasks
- Supports state sizes up to 16k without performance degradation
- Demonstrates consistent improvements across model sizes from 130M to 2.7B parameters

## Why This Works (Mechanism)
The paper's mechanism centers on the duality between state space models and structured semiseparable matrices. State space models can be expressed as convolutions with exponential filters, which correspond to semiseparable matrices when viewed as linear operators. This structure allows for efficient algorithms that exploit both the recurrence structure and the low-rank properties of the attention matrix. By maintaining both representations simultaneously, Mamba-2 can dynamically choose the most efficient computation path based on context and hardware characteristics.

## Foundational Learning
**Structured Semiseparable Matrices**: Matrices where each off-diagonal block has low rank. Why needed: Forms the mathematical foundation linking SSMs and attention. Quick check: Verify that matrix-vector products can be computed in O(n) rather than O(n²) time.

**State Space Models**: Continuous-time dynamical systems discretized for sequence processing. Why needed: Provides the temporal modeling capability. Quick check: Confirm state update equations correctly implement the desired dynamics.

**Duality Theory**: Mathematical principle showing two seemingly different representations are equivalent. Why needed: Enables switching between computational modes. Quick check: Prove both representations produce identical outputs for test sequences.

**Selective State Spaces**: Input-dependent parameterization of state matrices. Why needed: Allows context-aware processing. Quick check: Verify selection mechanism responds appropriately to input patterns.

## Architecture Onboarding

Component map: Input → Selective State Space → Dual Representation → Output
Critical path: Input features → State update (recurrent mode) OR Kernel computation (attention mode) → Output projection

Design tradeoffs:
- State size vs. memory efficiency: Larger states improve modeling but increase memory pressure
- Selective parameterization vs. computational overhead: More selective parameters improve accuracy but add complexity
- Dual-mode operation vs. implementation complexity: Supporting both modes increases flexibility but requires careful scheduling

Failure signatures:
- Performance degradation when state size exceeds hardware memory limits
- Instability in recurrent mode with poorly conditioned state matrices
- Accuracy loss when selective mechanism fails to capture important patterns

First experiments:
1. Verify dual-mode computation produces identical outputs for simple sequences
2. Benchmark speed improvement on short sequences with varying state sizes
3. Test selective state space response to controlled input patterns

## Open Questions the Paper Calls Out
None

## Limitations
- Proof relies on specific assumptions about semiseparable matrix structure that may not generalize
- Performance comparisons primarily focus on autoregressive language modeling
- Limited evaluation on non-language domains like vision or multimodal tasks

## Confidence
**Core theoretical framework (High)**: The structured semiseparable matrix connection appears mathematically sound and well-demonstrated through both analysis and empirical validation.

**Practical efficiency claims (Medium)**: While Mamba-2 shows consistent speedups, the 2-8x improvement range varies significantly across hardware configurations and sequence lengths, suggesting implementation-dependent factors.

**Downstream task performance (Medium)**: The paper shows competitive results on standard benchmarks, but the evaluation set is somewhat limited in scope and doesn't explore challenging long-range dependency tasks where SSMs typically excel.

## Next Checks
1. Test Mamba-2 on non-language domains (vision, speech, multimodal) to verify architectural generalization
2. Conduct ablation studies isolating the contribution of each structured semiseparable component to the overall performance
3. Benchmark memory efficiency and training stability at scale (10B+ parameters) to assess practical deployment limits