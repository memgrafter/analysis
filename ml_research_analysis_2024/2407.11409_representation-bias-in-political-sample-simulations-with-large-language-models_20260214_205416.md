---
ver: rpa2
title: Representation Bias in Political Sample Simulations with Large Language Models
arxiv_id: '2407.11409'
source_url: https://arxiv.org/abs/2407.11409
tags:
- simulation
- political
- public
- bias
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigates representation bias in large language model
  (LLM) simulations of political samples, focusing on vote choice and public opinion
  across different countries, demographic groups, and political regimes. Using GPT-3.5-Turbo
  and datasets from the US, Germany, and China, the research evaluates simulation
  accuracy through agreement scores between LLM-generated and actual human responses.
---

# Representation Bias in Political Sample Simulations with Large Language Models

## Quick Facts
- arXiv ID: 2407.11409
- Source URL: https://arxiv.org/abs/2407.11409
- Authors: Weihong Qi; Hanjia Lyu; Jiebo Luo
- Reference count: 32
- This study investigates representation bias in LLM simulations of political samples across different countries, demographics, and political regimes

## Executive Summary
This study investigates representation bias in large language model (LLM) simulations of political samples, focusing on vote choice and public opinion across different countries, demographic groups, and political regimes. Using GPT-3.5-Turbo and datasets from the US, Germany, and China, the research evaluates simulation accuracy through agreement scores between LLM-generated and actual human responses. Results show better performance in vote choice simulations than public opinion, with higher accuracy for English-speaking countries, bipartisan systems, and democratic regimes. Simulations are less accurate for younger age groups and perform poorly in non-English, multi-party, and authoritarian contexts. These findings highlight the need to diversify training data, improve LLM capabilities for complex political behaviors, and address age-related representation gaps.

## Method Summary
The study employs GPT-3.5-Turbo to simulate political samples using real-world datasets from the US, Germany, and China. Researchers conducted controlled experiments comparing LLM-generated responses against actual human survey data across multiple dimensions including vote choice, public opinion, demographic groups, and political regimes. The evaluation methodology uses agreement scores to measure accuracy, with systematic analysis of performance variations across different country contexts, age groups, and political systems. The experimental design includes controlled variations in prompt engineering and parameter settings to isolate factors affecting simulation accuracy.

## Key Results
- Vote choice simulations demonstrate higher accuracy than public opinion simulations across all tested countries
- English-speaking countries and democratic regimes show significantly better LLM performance than non-English and authoritarian contexts
- Age-related representation gaps exist, with younger demographic groups showing lower simulation accuracy

## Why This Works (Mechanism)
The study's findings are driven by the alignment between LLM training data and the political contexts being simulated. Models trained predominantly on English-language data and democratic political discourse naturally perform better when simulating similar contexts. The binary nature of vote choice provides clearer decision boundaries for LLMs compared to the nuanced spectrum of public opinion, resulting in higher accuracy for the former. The observed age-related gaps likely stem from underrepresentation of younger voices in historical training data, while the country-specific variations reflect the extent to which different political systems and languages are represented in the training corpus.

## Foundational Learning
- **LLM political simulation**: Understanding how language models can generate synthetic political data for research and prediction - why needed: Forms the basis for using LLMs as tools in political science; quick check: Verify model can generate coherent political responses matching prompt specifications
- **Representation bias**: Recognizing systematic errors in how LLMs represent different demographic and political groups - why needed: Identifies limitations in model generalizability; quick check: Compare model outputs across different demographic prompts for consistency
- **Cross-cultural political behavior**: Understanding how political attitudes and behaviors vary across different countries and systems - why needed: Essential for evaluating model performance in diverse contexts; quick check: Ensure training data includes sufficient representation from target countries
- **Demographic modeling**: Ability to simulate responses across different age groups, genders, and socioeconomic backgrounds - why needed: Critical for creating representative political samples; quick check: Validate age-specific response patterns match known demographic trends
- **Agreement score metrics**: Using statistical measures to compare model outputs against ground truth data - why needed: Provides objective evaluation of simulation accuracy; quick check: Calculate inter-rater reliability on human-coded responses
- **Political regime classification**: Categorizing countries by governance type (democratic, authoritarian, etc.) - why needed: Enables analysis of how regime type affects simulation performance; quick check: Cross-validate regime classifications with multiple sources

## Architecture Onboarding
**Component Map**: GPT-3.5-Turbo -> Prompt Engineering -> Simulation Generation -> Agreement Score Calculation -> Performance Analysis

**Critical Path**: The core workflow involves prompt design feeding into the LLM, which generates simulated political responses that are then compared against ground truth data using agreement scores. Performance analysis aggregates these scores across different experimental conditions.

**Design Tradeoffs**: The study prioritizes using a single, well-established LLM (GPT-3.5-Turbo) for consistency, but this limits generalizability to other model architectures. The focus on agreement scores provides quantitative rigor but may miss qualitative aspects of bias. Country selection balances diversity with data availability, potentially constraining broader applicability.

**Failure Signatures**: Poor performance in non-English contexts suggests language-specific training data limitations. Lower accuracy for younger demographics indicates age-related representation gaps in training corpora. Multi-party system challenges reveal difficulties with complex political landscapes. Authoritarian regime underperformance may reflect limited exposure to such political discourse during training.

**First Experiments**: 1) Test prompt variations to optimize simulation accuracy across different political contexts; 2) Conduct ablation studies removing demographic modifiers to isolate their impact; 3) Compare simulation outputs with human-generated political discourse to identify systematic biases.

## Open Questions the Paper Calls Out
None

## Limitations
- Study focuses on only three countries (US, Germany, China) with specific demographic and political contexts, limiting generalizability
- Evaluation relies solely on agreement scores between LLM outputs and actual human responses without deeper analysis of why certain biases occur
- Uses GPT-3.5-Turbo as the sole LLM model, preventing assessment of whether biases are consistent across different architectures

## Confidence
- **High confidence**: Vote choice simulations perform better than public opinion simulations (well-supported by empirical results)
- **Medium confidence**: Better performance in English-speaking countries and democratic regimes (may be influenced by specific countries chosen)
- **Medium confidence**: Age-related representation gaps exist (based on available data but underlying causes not explored)

## Next Checks
1. Replicate the study using multiple LLM models (including open-source alternatives) to determine if representation biases are model-specific or universal across architectures
2. Conduct qualitative analysis of LLM outputs to identify specific patterns in how political biases manifest beyond simple agreement scores, including examination of reasoning patterns and response framing
3. Expand the demographic scope to include additional countries with different political systems (e.g., multi-party democracies, hybrid regimes) and age distributions to test the generalizability of the findings