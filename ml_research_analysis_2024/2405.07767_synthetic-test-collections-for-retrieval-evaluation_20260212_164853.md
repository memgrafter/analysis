---
ver: rpa2
title: Synthetic Test Collections for Retrieval Evaluation
arxiv_id: '2405.07767'
source_url: https://arxiv.org/abs/2405.07767
tags:
- synthetic
- queries
- test
- collections
- judgments
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper explores the feasibility of constructing fully synthetic
  test collections using Large Language Models (LLMs) for information retrieval evaluation.
  The authors generate synthetic queries from passages using T5 and GPT-4, filter
  low-quality queries, and produce synthetic relevance judgments via GPT-4.
---

# Synthetic Test Collections for Retrieval Evaluation

## Quick Facts
- arXiv ID: 2405.07767
- Source URL: https://arxiv.org/abs/2405.07767
- Reference count: 22
- Primary result: Synthetic test collections show strong correlation (Kendall's œÑ = 0.8568) with real collections in system rankings

## Executive Summary
This paper explores the feasibility of constructing fully synthetic test collections for information retrieval evaluation using Large Language Models (LLMs). The authors generate synthetic queries from passages using T5 and GPT-4, filter them through expert assessors, and produce synthetic relevance judgments via GPT-4. They compare system rankings on synthetic versus real test collections and find strong correlation in system ordering. Their analysis shows little systematic bias toward LLM-based systems, though synthetic test collections tend to overestimate performance. The results suggest that synthetic test collections can reliably substitute for real ones in retrieval evaluation.

## Method Summary
The method involves generating synthetic queries from a corpus of 1000 passages from MS MARCO v2 using both T5 and GPT-4, followed by expert filtering to remove low-quality queries. Synthetic relevance judgments are then generated using GPT-4 with a prompt template from prior work. The synthetic test collection is evaluated by comparing system rankings with those from real test collections using Kendall's tau correlation. The authors also analyze potential biases by categorizing runs based on the LLM used and comparing performance across real and synthetic collections.

## Key Results
- Strong correlation (Kendall's œÑ = 0.8568) between system rankings on synthetic versus real test collections
- Fair agreement between synthetic and human relevance judgments (Cohen's Œ∫ = 0.24-0.26)
- Little systematic bias toward LLM-based systems in synthetic test collections
- Synthetic test collections tend to overestimate performance across all system types

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Synthetic queries generated by LLMs preserve system ranking similarity to real queries.
- Mechanism: LLMs generate queries from passage content, maintaining topical coherence. These queries are filtered and validated by expert assessors before inclusion in the test collection.
- Core assumption: The query generation process (T5 or GPT-4) captures the intent and complexity of real user queries, and expert filtering removes low-quality synthetic queries.
- Evidence anchors:
  - [abstract] "strong correlation (Kendall's ùúè = 0.8568) in system ordering"
  - [section] "synthetic queries and real queries show similar patterns in terms of evaluation results and system ranking, with a system ordering agreement of Kendall's ùúè = 0.8151"
  - [corpus] Weak: no corpus evidence provided about query generation quality.

### Mechanism 2
- Claim: Synthetic relevance judgments from GPT-4 approximate human judgments well enough to maintain evaluation validity.
- Mechanism: GPT-4 is prompted using a template proven to generate high-quality relevance judgments. The model labels documents with the same four-grade scale used by human assessors.
- Core assumption: GPT-4's understanding of relevance aligns sufficiently with human assessors, and the prompt template guides consistent judgment generation.
- Evidence anchors:
  - [abstract] "strong correlation (Kendall's ùúè = 0.8568) in system ordering"
  - [section] "we observe a fair level of agreement between synthetic judgements generated using GPT-4 and manual judgments: The Cohen's ùúÖ on real queries is 0.24 and on synthetic queries is 0.26"
  - [corpus] Weak: no corpus evidence provided about judgment quality.

### Mechanism 3
- Claim: Synthetic test collections do not systematically bias toward LLM-based systems.
- Mechanism: By categorizing runs by the LLM used (GPT, T5, GPT+T5, others) and comparing performance across real vs. synthetic collections, the authors check for systematic over- or underestimation.
- Core assumption: The synthetic queries and judgments are sufficiently neutral to avoid favoring systems built with the same LLM.
- Evidence anchors:
  - [abstract] "little systematic bias toward LLM-based systems"
  - [section] "synthetic test collections we have generated using a particular LLM do exhibit little to no bias towards systems based on the same LLM"
  - [corpus] Weak: no corpus evidence provided about bias mitigation.

## Foundational Learning

- Concept: Cranfield paradigm for test collection construction
  - Why needed here: The paper relies on the Cranfield paradigm (queries + relevance judgments) as the foundation for evaluating IR systems.
  - Quick check question: What are the two essential components of a Cranfield-style test collection?

- Concept: Kendall's tau (ùúè) as a ranking correlation metric
  - Why needed here: The paper uses Kendall's tau to measure the agreement between system rankings on real vs. synthetic test collections.
  - Quick check question: What does a Kendall's tau value of 0.8568 indicate about the similarity of system rankings?

- Concept: Cohen's kappa (ùúÖ) for inter-rater agreement
  - Why needed here: The paper uses Cohen's kappa to measure agreement between human and LLM-generated relevance judgments.
  - Quick check question: What does a Cohen's kappa value of 0.24-0.26 indicate about the agreement between human and synthetic judgments?

## Architecture Onboarding

- Component map: Passage corpus ‚Üí Query generation (T5/GPT-4) ‚Üí Query filtering (expert) ‚Üí Document pooling ‚Üí Judgment generation (GPT-4) ‚Üí Evaluation
- Critical path: Passage selection ‚Üí Query generation ‚Üí Expert filtering ‚Üí Document pooling ‚Üí Judgment generation ‚Üí System evaluation
- Design tradeoffs:
  - Using GPT-4 vs. T5 for query generation: GPT-4 produces longer, potentially more complex queries; T5 is faster and cheaper but may be less expressive.
  - Human vs. LLM judgments: Human judgments are more accurate but costly; LLM judgments are cheaper but may have systematic biases.
- Failure signatures:
  - Low Kendall's tau between real and synthetic rankings ‚Üí query generation or filtering issues
  - Low Cohen's kappa between human and LLM judgments ‚Üí judgment generation issues
  - Systematic bias toward certain system types ‚Üí LLM generation bias
- First 3 experiments:
  1. Generate queries using both T5 and GPT-4 from the same passage set; compare query quality and system ranking correlation.
  2. Generate synthetic judgments for real queries using GPT-4; compare Cohen's kappa with human judgments.
  3. Construct a fully synthetic test collection; evaluate system rankings and check for bias toward LLM-based systems.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does synthetic test collection performance vary across different domains and document types (e.g., scientific literature, legal documents, social media)?
- Basis in paper: [inferred] The paper only tested on passage retrieval using MS MARCO passages, suggesting domain-specific variations may exist.
- Why unresolved: The study is limited to one domain (general web passages), and no cross-domain experiments were conducted to verify generalizability.
- What evidence would resolve it: Systematic experiments constructing synthetic test collections across diverse document collections and evaluating retrieval performance differences.

### Open Question 2
- Question: What specific aspects of LLM-generated queries lead to overestimation of system performance compared to real queries?
- Basis in paper: [explicit] The authors note that synthetic test collections tend to overestimate performance across all system types, but do not investigate the underlying reasons.
- Why unresolved: The paper identifies the phenomenon but doesn't analyze query characteristics that contribute to this performance inflation.
- What evidence would resolve it: Detailed analysis comparing query complexity, ambiguity, and information need representation between synthetic and real queries, correlated with system performance patterns.

### Open Question 3
- Question: Can synthetic test collections maintain reliability as LLM capabilities evolve and models become more sophisticated?
- Basis in paper: [inferred] The study uses specific LLM versions (GPT-4, T5), implying potential variability as models improve or change over time.
- Why unresolved: The experiments are based on current LLM capabilities without considering how future model improvements might affect synthetic judgment quality and potential biases.
- What evidence would resolve it: Longitudinal studies tracking synthetic test collection performance and bias patterns across multiple LLM generations and capability levels.

## Limitations
- Limited to 1000 passages from MS MARCO v2 corpus, potentially not representative of broader information needs
- Only tested on one existing test collection (MS MARCO), limiting generalizability
- Focuses primarily on NDCG@10 metric, potentially missing nuances captured by other IR metrics

## Confidence
- High confidence: Strong correlation between synthetic and real test collections in system ranking (Kendall's œÑ = 0.8568)
- Medium confidence: Little systematic bias toward LLM-based systems, as this requires more extensive testing across diverse system architectures
- Medium confidence: Synthetic collections tend to overestimate performance, based on limited comparison points

## Next Checks
1. **Domain generalization test**: Construct synthetic test collections from different domains (e.g., scientific literature, legal documents, medical texts) and measure correlation with corresponding real collections to validate cross-domain applicability.

2. **Longitudinal stability assessment**: Generate synthetic test collections at different time intervals and evaluate whether system rankings remain consistent as both LLMs and IR systems evolve, checking for temporal drift in evaluation validity.

3. **Multi-metric correlation analysis**: Extend the evaluation to include additional IR metrics beyond NDCG@10 (such as MRR, MAP, and recall) to determine if the strong correlation observed holds across different evaluation perspectives and use cases.