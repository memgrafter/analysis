---
ver: rpa2
title: 'From Pixels to Tokens: Revisiting Object Hallucinations in Large Vision-Language
  Models'
arxiv_id: '2410.06795'
source_url: https://arxiv.org/abs/2410.06795
tags:
- object
- visual
- lvlms
- hallucinations
- information
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates object hallucinations in large vision-language
  models (LVLMs), attributing the problem to inadequate cross-modal alignment rather
  than visual encoding deficiencies. The authors propose PATCH, a parameter-efficient
  fine-tuning method that uses trainable virtual tokens to better integrate object
  detection information into the LVLM's input, thereby improving alignment between
  visual features and textual semantics.
---

# From Pixels to Tokens: Revisiting Object Hallucinations in Large Vision-Language Models

## Quick Facts
- arXiv ID: 2410.06795
- Source URL: https://arxiv.org/abs/2410.06795
- Reference count: 40
- Primary result: PATCH reduces object hallucinations by 5-31% accuracy improvement across multiple LVLM models

## Executive Summary
This paper investigates object hallucinations in large vision-language models (LVLMs), attributing the problem to inadequate cross-modal alignment rather than visual encoding deficiencies. The authors propose PATCH, a parameter-efficient fine-tuning method that uses trainable virtual tokens to better integrate object detection information into the LVLM's input. Experimental results show PATCH significantly outperforms existing methods, improving hallucination detection accuracy from 85.17% to 90.20% for LLaVA-v1.5 and from 57.67% to 88.13% for MiniGPT-4 on the POPE dataset.

## Method Summary
PATCH addresses object hallucinations by introducing trainable virtual tokens between encoded image features and augmented text prompts. The method uses a frozen Cascade Mask R-CNN for object detection to obtain categories and bounding boxes, which are then formatted as text prompts. These prompts, along with virtual tokens, are inserted between the visual encoder output and the LLM input. During fine-tuning, only the virtual token parameters (0.08M) are updated while all other LVLM parameters remain frozen. The virtual tokens learn to optimally align visual features with the augmented textual information containing object detection results.

## Key Results
- PATCH improves POPE accuracy from 85.17% to 90.20% for LLaVA-v1.5
- PATCH improves POPE accuracy from 57.67% to 88.13% for MiniGPT-4
- PATCH improves POPE accuracy from 83.33% to 90.03% for MiniGPT-v2
- PATCH shows strong performance on misleading questions in the PhD dataset
- Optimal virtual token count is 20 for the POPE dataset

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Cross-modal alignment, not visual encoding, is the primary cause of object hallucinations.
- **Mechanism**: The visual encoder accurately extracts features, but the visual projection layer fails to properly decouple and align these features with the LLM's semantic space, causing hallucinations.
- **Core assumption**: If object detection is accurate but LVLM inference is incorrect, the fault lies in the modal alignment module rather than the visual encoder.
- **Evidence anchors**:
  - [abstract] "Our analysis reveals that the primary source of object hallucinations lies in insufficient cross-modal alignment at the modal alignment module, rather than limitations in the encoding capacity of the visual encoder."
  - [section 3.2] "74.58% of these cases arise when object detection is accurate while the model's inference is incorrect, indicating this as the predominant failure mode."
  - [corpus] Weak - related work focuses on attention calibration and decoding strategies, but this paper's specific attribution to cross-modal alignment is unique.
- **Break condition**: If the visual encoder fails to capture critical image details, or if object detection is inaccurate, this mechanism would break down.

### Mechanism 2
- **Claim**: Incorporating object-related information directly into prompts improves LVLM performance on hallucination tasks.
- **Mechanism**: Adding object categories and bounding boxes to the input prompt helps the LLM better understand the spatial and semantic relationships between objects and the question.
- **Core assumption**: The LLM can effectively utilize explicit object detection information to improve its understanding of image content.
- **Evidence anchors**:
  - [section 3.2] "incorporating accurate detection information significantly improves the model's performance on questions related to object existence, thus enhancing its image interpretation capability and effectively reducing object hallucinations."
  - [section 5.4] "Both Hard Prompt and PATCH improve the backbone performance, confirming that incorporating object-related information aids LVLMs in better interpreting visual features."
  - [corpus] Weak - related work mentions object detection integration but doesn't specifically validate prompt-based approaches.
- **Break condition**: If object detection information is inaccurate or irrelevant to the question, this mechanism would degrade performance.

### Mechanism 3
- **Claim**: Trainable virtual tokens can effectively bridge the gap between encoded image features and augmented text prompts.
- **Mechanism**: Virtual tokens are inserted between image features and object-related information, allowing the model to learn optimal representations that align visual and textual features in the semantic space.
- **Core assumption**: The LLM can learn to effectively utilize virtual tokens to selectively extract and process relevant object-related information.
- **Evidence anchors**:
  - [abstract] "PATCH introduces trainable and pluggable virtual tokens between image features and enhanced prompt texts, bridging the gap between the encoded image features and input-augmented texts with minimal parameter tuning."
  - [section 4.2] "These token embeddings are optimized during training, with parameters δ ∈ Rⁿ×ᵈ, where d is the embedding dimension of the LVLM."
  - [corpus] Weak - related work mentions virtual tokens but doesn't specifically validate their use for hallucination mitigation.
- **Break condition**: If the number of virtual tokens is excessive or insufficient, or if their initialization is poor, this mechanism would fail.

## Foundational Learning

- **Concept**: Cross-modal alignment
  - Why needed here: Understanding how visual and textual features are integrated is crucial for diagnosing and fixing hallucination issues
  - Quick check question: What is the difference between feature extraction and feature alignment in multi-modal models?

- **Concept**: Object detection and bounding boxes
  - Why needed here: Object detection provides the spatial and categorical information that helps LVLMs understand image content
  - Quick check question: How do bounding boxes help a model understand the spatial relationships between objects in an image?

- **Concept**: Parameter-efficient fine-tuning
  - Why needed here: PATCH only updates a small fraction of model parameters, making it computationally efficient while still improving performance
  - Quick check question: What is the advantage of updating only virtual token parameters versus fine-tuning the entire model?

## Architecture Onboarding

- **Component map**:
  Visual Encoder (ViT) → Visual Projection Layer → LLM
  Cascade Mask R-CNN (frozen) → Object Detection Results
  PATCH: Virtual Tokens inserted between Visual Encoder output and Object Detection Results

- **Critical path**: Image → Visual Encoder → Virtual Tokens → Object Detection → LLM → Answer

- **Design tradeoffs**:
  - Parameter efficiency vs. performance: PATCH updates only 0.08M parameters vs. full model fine-tuning
  - Object detection accuracy vs. hallucination mitigation: Depends on quality of detection results
  - Virtual token quantity: Too few may be insufficient, too many may introduce noise

- **Failure signatures**:
  - Performance degradation when object detection is inaccurate
  - Overfitting to specific detection formats or object categories
  - Virtual tokens failing to learn meaningful representations

- **First 3 experiments**:
  1. Run baseline LVLM without any modifications to establish baseline hallucination rate
  2. Add object detection results directly to prompts (Hard Prompt) to verify the importance of object information
  3. Implement PATCH with varying numbers of virtual tokens to find optimal configuration

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the choice of object detection model affect PATCH's performance on object hallucination mitigation?
- Basis in paper: [explicit] The authors mention using Cascade Mask R-CNN as the detection head and note that "with the continuous development of object detection models, we believe that there will be potential to further enhance the robustness and effectiveness of our method."
- Why unresolved: The paper does not experimentally compare different detection models (e.g., DETR, YOLO, DINO) or analyze how detection accuracy impacts hallucination mitigation performance.
- What evidence would resolve it: Systematic comparison of PATCH performance using different state-of-the-art detection models on the same hallucination evaluation datasets, with correlation analysis between detection accuracy and hallucination reduction.

### Open Question 2
- Question: What is the optimal quantity and positioning of virtual tokens for different types of hallucination tasks?
- Basis in paper: [explicit] The authors conduct ablation studies on token quantities (showing optimal performance at 20 tokens for POPE) and token positioning (comparing PATCH vs PATCHw/ Late), but these studies are limited to specific datasets and tasks.
- Why unresolved: The experiments only explore a narrow range of token configurations on limited datasets. Different hallucination types (object vs attribute vs positional) may benefit from different virtual token strategies.
- What evidence would resolve it: Comprehensive grid search experiments varying token quantities (1-100) and positioning across multiple hallucination task types, with performance analysis to identify optimal configurations for each task category.

### Open Question 3
- Question: How does PATCH generalize to more complex hallucination scenarios beyond binary object presence questions?
- Basis in paper: [inferred] The authors evaluate on POPE (binary classification) and PhD (multi-task with misleading contexts), but both focus on object-related hallucinations. The paper mentions "diverse downstream tasks" but doesn't test complex reasoning or generation tasks where hallucinations could manifest differently.
- Why unresolved: The evaluation focuses on relatively constrained tasks. Real-world applications often involve open-ended generation, complex reasoning chains, or multi-step visual question answering where hallucinations could emerge in more subtle ways.
- What evidence would resolve it: Testing PATCH on open-ended image captioning, complex visual reasoning tasks (like A-OKVQA or VQAv2), and multimodal generation tasks to assess its effectiveness in preventing various hallucination forms in unconstrained scenarios.

## Limitations

- The analysis relies on controlled hallucination datasets that may not capture real-world complexity
- PATCH's performance depends on the quality of pre-trained object detection models
- The study focuses on hallucination detection rather than overall generation quality improvements

## Confidence

**High Confidence**: The attribution of object hallucinations to cross-modal alignment issues rather than visual encoding deficiencies. This is supported by thorough analysis showing that 74.58% of hallucination cases occur when object detection is accurate but the model's inference is incorrect, indicating the failure point is in the alignment module rather than the visual encoder.

**Medium Confidence**: The effectiveness of virtual tokens as a solution mechanism. While experimental results show consistent improvements across multiple models and datasets, the specific contribution of virtual tokens versus simply providing object detection information remains somewhat unclear, as the Hard Prompt baseline also shows improvements.

**Low Confidence**: The generalizability of PATCH to completely unseen domains and tasks. The paper demonstrates strong performance on held-out test sets from the same distribution as training data, but doesn't extensively validate performance on truly out-of-distribution scenarios or different types of hallucination beyond object existence questions.

## Next Checks

1. **Cross-Domain Generalization Test**: Evaluate PATCH performance on diverse, real-world image datasets (e.g., COCO, Flickr30k) to assess whether the method maintains effectiveness outside controlled hallucination datasets.

2. **Ablation Study on Object Detection Quality**: Systematically vary the quality of object detection results by using different detection models or injecting noise, to quantify how sensitive PATCH's performance is to detection accuracy.

3. **Generation Quality Assessment**: Beyond hallucination detection accuracy, evaluate the impact of PATCH on overall generation quality metrics such as fluency, coherence, and relevance using human evaluation or automated metrics like ROUGE or BERTScore.