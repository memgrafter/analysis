---
ver: rpa2
title: 'Enabling Low-Resource Language Retrieval: Establishing Baselines for Urdu
  MS MARCO'
arxiv_id: '2412.12997'
source_url: https://arxiv.org/abs/2412.12997
tags:
- urdu
- dataset
- languages
- retrieval
- low-resource
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of low-resource language retrieval
  by introducing the first large-scale Urdu IR dataset created through machine translation
  of MS MARCO. The authors establish baseline performance using BM25 and zero-shot
  mMARCO, then fine-tune the model on the Urdu dataset.
---

# Enabling Low-Resource Language Retrieval: Establishing Baselines for Urdu MS MARCO

## Quick Facts
- **arXiv ID**: 2412.12997
- **Source URL**: https://arxiv.org/abs/2412.12997
- **Reference count**: 13
- **Primary result**: Fine-tuned Urdu-mT5-mMARCO model achieves MRR@10 of 0.247 and Recall@10 of 0.439 on machine-translated Urdu MS MARCO dataset

## Executive Summary
This paper addresses the challenge of low-resource language retrieval by introducing the first large-scale Urdu IR dataset created through machine translation of MS MARCO. The authors establish baseline performance using BM25 and zero-shot mMARCO, then fine-tune the model on the Urdu dataset. The fine-tuned Urdu-mT5-mMARCO model achieves an MRR@10 of 0.247 and Recall@10 of 0.439, representing significant improvements over zero-shot results. This work demonstrates that machine translation combined with fine-tuning can effectively enable IR for low-resource languages like Urdu, providing valuable insights and laying groundwork for future multilingual IR research in South Asian languages.

## Method Summary
The authors created a large-scale Urdu IR dataset by translating the MS MARCO dataset using IndicTrans2, a multilingual translation model trained on South Asian languages. The translation process involved tokenization, translation using a distilled 200M parameter model, and post-processing to reconstruct text from subword tokens. The resulting dataset contains approximately 39 million triples. They established baselines using BM25 for initial retrieval and zero-shot mMARCO re-ranking, then fine-tuned the mT5 model on the Urdu-translated data to create Urdu-mT5-mMARCO. The fine-tuning process ran for approximately 75 hours on an A100 GPU, using the mT5-XXL architecture with a sequence length of 512 tokens.

## Key Results
- Fine-tuned Urdu-mT5-mMARCO model achieves MRR@10 of 0.247 and Recall@10 of 0.439
- Zero-shot mMARCO performance is significantly lower than fine-tuned model
- BM25 provides reasonable initial retrieval with top 1000 passages per query
- Machine translation enables large-scale dataset creation for low-resource language IR

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Machine translation of high-resource datasets can provide usable training data for low-resource IR tasks.
- Mechanism: Translating a large-scale English dataset (MS MARCO) into Urdu using IndicTrans2 creates a scalable corpus that captures query-passage relevance patterns in the target language, enabling supervised fine-tuning of multilingual models.
- Core assumption: The translation preserves enough semantic alignment between queries and passages to support effective model training, even with occasional errors.
- Evidence anchors:
  - [abstract] "This paper introduces the first large-scale Urdu IR dataset, created by translating the MS MARCO dataset through machine translation."
  - [section] "Overall the translation took around 120 hours to translate the dataset on a single V100 GPU with 32GB Vram."
  - [corpus] Weak - no direct evaluation of translation quality metrics in neighbors.
- Break condition: If translation introduces systematic semantic drift or context loss, the model cannot learn meaningful relevance patterns.

### Mechanism 2
- Claim: Fine-tuning multilingual models on low-resource data improves retrieval performance over zero-shot approaches.
- Mechanism: The mMARCO model, originally trained on MS MARCO but not on Urdu, learns Urdu-specific relevance patterns when fine-tuned on the translated dataset, adapting its semantic understanding to the target language's characteristics.
- Core assumption: Multilingual pre-training provides sufficient cross-lingual transfer to benefit from Urdu-specific fine-tuning.
- Evidence anchors:
  - [abstract] "Our findings demonstrate that the fine-tuned model (Urdu-mT5-mMARCO) achieves a Mean Reciprocal Rank (MRR@10) of 0.247 and a Recall@10 of 0.439, representing significant improvements over zero-shot results."
  - [section] "Our fine-tuned mMARCO model, trained on the Urdu-translated dataset, provided further improvements when used as a re-ranker over BM25 outputs."
  - [corpus] Weak - no direct comparison of multilingual transfer mechanisms in neighbors.
- Break condition: If the pre-training corpus lacks sufficient linguistic overlap with Urdu, fine-tuning gains will be minimal.

### Mechanism 3
- Claim: Using BM25 for initial retrieval followed by neural re-ranking creates an effective two-stage pipeline for low-resource IR.
- Mechanism: BM25 provides a broad candidate set (top 1000 passages) that is computationally efficient, while the mT5-based re-ranker refines this set by learning complex relevance patterns from the translated data.
- Core assumption: The combination of traditional and neural methods compensates for the weaknesses of each approach individually in low-resource settings.
- Evidence anchors:
  - [section] "We employed BM25, a well-established non-neural retrieval method [9], as our initial retrieval model to retrieve the top1,000 passages per query."
  - [section] "Applying the zero-shot mMARCO re-ranker to the BM25 outputs improved performance, despite the model not being trained on Urdu."
  - [corpus] Weak - no direct evidence of two-stage pipeline effectiveness in neighbors.
- Break condition: If BM25 retrieval quality is too poor, the re-ranker cannot recover sufficient relevant passages regardless of its quality.

## Foundational Learning

- Concept: Machine translation evaluation metrics (chrF++, BLEU)
  - Why needed here: Understanding translation quality is crucial for assessing the reliability of the Urdu dataset and explaining performance limitations.
  - Quick check question: What chrF++ score did IndicTrans2 achieve for English-to-Urdu translation according to the paper?

- Concept: Multilingual model pre-training and transfer learning
  - Why needed here: The mMARCO model's ability to generalize to Urdu despite not being trained on it depends on its multilingual pre-training strategy and cross-lingual transfer capabilities.
  - Quick check question: Which model architecture serves as the foundation for mMARCO?

- Concept: Information retrieval evaluation metrics (MRR, Recall@K)
  - Why needed here: These metrics are used to quantify retrieval performance improvements and compare different approaches systematically.
  - Quick check question: What does Recall@10 measure in the context of this paper?

## Architecture Onboarding

- Component map: IndicTrans2 translation pipeline (tokenization → translation → post-processing) → BM25 retriever (candidate generation) → mT5-mMARCO fine-tuned model (re-ranking) → Evaluation framework (MRR@10, Recall@10 computation)

- Critical path: Translation → BM25 retrieval → mT5 re-ranking → Evaluation

- Design tradeoffs:
  - Translation speed vs quality (used distilled 200M model for efficiency)
  - BM25 recall vs precision (retrieved top 1000 to ensure coverage)
  - Fine-tuning duration vs performance (75 hours on A100 for best results)

- Failure signatures:
  - Low BM25 performance indicates poor keyword matching for Urdu morphology
  - Minimal improvement from fine-tuning suggests translation quality issues
  - High variance across queries may indicate inconsistent translation quality

- First 3 experiments:
  1. Evaluate BM25 on small Urdu test set to establish baseline keyword matching quality
  2. Test zero-shot mMARCO on translated validation set to measure cross-lingual transfer
  3. Fine-tune mMARCO on subset of translated data to assess training dynamics before full training

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the quality of machine-translated datasets impact the performance of fine-tuned IR models for low-resource languages compared to human-translated datasets?
- Basis in paper: [explicit] The paper acknowledges that machine translations may not match human annotation precision and that translation errors can misalign query-passage semantics
- Why unresolved: The study only used machine-translated data and did not compare against human-translated data to quantify the impact of translation quality
- What evidence would resolve it: A controlled experiment comparing IR model performance on machine-translated versus human-translated Urdu datasets would quantify the performance gap and determine if the benefits of fine-tuning outweigh translation quality limitations

### Open Question 2
- Question: What is the minimum amount of parallel text data needed to achieve effective fine-tuning for low-resource language IR models?
- Basis in paper: [inferred] The paper used the full MS MARCO dataset (39 million triples) but didn't explore whether smaller subsets would suffice for effective Urdu model adaptation
- Why unresolved: The study didn't conduct experiments with reduced training data sizes to identify the data efficiency threshold
- What evidence would resolve it: Systematic experiments training models on progressively smaller subsets of the Urdu-translated data while measuring performance would establish minimum effective dataset size

### Open Question 3
- Question: How transferable is the Urdu IR model fine-tuning approach to other morphologically complex South Asian languages?
- Basis in paper: [explicit] The authors state their methods can benefit other South Asian languages and provide a foundation for future research in similar languages
- Why unresolved: The study only evaluated Urdu and didn't test the approach on other languages to confirm transferability
- What evidence would resolve it: Applying the same translation and fine-tuning methodology to other South Asian languages like Hindi, Bengali, or Tamil and comparing performance across languages would validate cross-linguistic applicability

### Open Question 4
- Question: What specific aspects of Urdu morphology and script most significantly impact IR performance and could benefit from targeted preprocessing?
- Basis in paper: [explicit] The paper mentions Urdu's "rich morphology" and "complex tokenization" as challenges, but doesn't analyze which morphological features most affect retrieval
- Why unresolved: The study used standard tokenization and preprocessing without analyzing which linguistic features contribute most to performance gaps
- What evidence would resolve it: Detailed error analysis of retrieval failures combined with linguistic feature ablation studies would identify which morphological characteristics most impact IR effectiveness and guide targeted preprocessing development

## Limitations
- Translation quality assessment relies solely on post-editing evaluation rather than automated metrics, making it difficult to quantify translation reliability
- Performance metrics remain substantially below high-resource language baselines, suggesting limitations in the translation quality or model adaptation capacity
- The approach depends heavily on machine translation quality, which may introduce semantic drift that cannot be detected without human evaluation

## Confidence
- **High confidence**: The methodology for creating a machine-translated dataset and fine-tuning approach is technically sound and reproducible
- **Medium confidence**: The reported performance improvements are valid but the absolute performance levels suggest limitations in the translation quality or model adaptation capacity
- **Low confidence**: Claims about the scalability of this approach to other low-resource languages are speculative without validation on additional language pairs

## Next Checks
1. Conduct automated translation quality evaluation using chrF++ and BLEU scores on a held-out validation set to quantify translation reliability and identify systematic errors
2. Perform ablation studies comparing fine-tuning with different proportions of translated data to determine the minimum dataset size required for effective performance
3. Evaluate the approach on a small human-translated subset of queries to establish an upper bound on achievable performance and quantify the impact of translation quality on retrieval results