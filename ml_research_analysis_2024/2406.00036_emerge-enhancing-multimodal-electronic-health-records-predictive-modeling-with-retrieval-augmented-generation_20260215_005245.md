---
ver: rpa2
title: 'EMERGE: Enhancing Multimodal Electronic Health Records Predictive Modeling
  with Retrieval-Augmented Generation'
arxiv_id: '2406.00036'
source_url: https://arxiv.org/abs/2406.00036
tags:
- data
- knowledge
- clinical
- multimodal
- emerge
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: EMERGE enhances multimodal EHR predictive modeling by integrating
  clinical notes, time-series data, and external knowledge graphs using Retrieval-Augmented
  Generation. It extracts entities from EHR data via LLM prompting and aligns them
  with the PrimeKG knowledge graph, ensuring consistency through cosine similarity-based
  filtering.
---

# EMERGE: Enhancing Multimodal Electronic Health Records Predictive Modeling with Retrieval-Augmented Generation

## Quick Facts
- arXiv ID: 2406.00036
- Source URL: https://arxiv.org/abs/2406.00036
- Reference count: 40
- EMERGE achieves AUROC scores of 86.25% and 89.50% on mortality and readmission tasks respectively, with robustness to data sparsity

## Executive Summary
EMERGE is a novel framework that enhances multimodal EHR predictive modeling by integrating clinical notes, time-series data, and external knowledge graphs using Retrieval-Augmented Generation (RAG). The framework extracts entities from heterogeneous EHR data using LLM prompting and aligns them with the PrimeKG knowledge graph through cosine similarity-based filtering. EMERGE generates task-relevant patient health status summaries using long-context LLMs and fuses these with other modalities via an adaptive cross-attention network. Extensive experiments on MIMIC-III and MIMIC-IV datasets demonstrate superior performance over baseline models while maintaining robustness to data sparsity.

## Method Summary
EMERGE processes multimodal EHR data by first extracting entities from time-series data (using z-score filtering for abnormal features) and clinical notes (via LLM prompting). These entities are aligned with the PrimeKG knowledge graph using cosine similarity matching. The framework then generates task-relevant patient summaries through LLM distillation, incorporating entity definitions and descriptions. Finally, an adaptive multimodal fusion network with cross-attention layers integrates the representations of time-series data, clinical notes, and generated summaries for clinical predictions. The model is trained end-to-end using standard loss functions with early stopping on MIMIC datasets.

## Key Results
- Achieves AUROC scores of 86.25% (mortality) and 89.50% (readmission) on MIMIC-III/MIMIC-IV datasets
- Demonstrates robust performance with only 1% of training samples
- Outperforms baseline models across multiple clinical prediction tasks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: EMERGE improves EHR predictive modeling by retrieving task-relevant medical entities and generating concise patient summaries via RAG.
- Mechanism: LLMs extract disease entities from structured time-series data (using z-score filtering) and unstructured clinical notes, which are matched to PrimeKG using cosine similarity-based filtering. Retrieved entities and definitions generate task-relevant summaries.
- Core assumption: LLMs can reliably extract clinically meaningful entities from heterogeneous EHR data, and PrimeKG contains relevant, high-quality medical knowledge.
- Evidence anchors:
  - [abstract] "We extract entities from both time-series data and clinical notes by prompting Large Language Models (LLMs) and align them with professional PrimeKG"
  - [section] "Retrieval process for time-series data...Features over a specified threshold ϵ (such as 3-σ deviation) are identified as abnormal"
- Break condition: If LLM hallucinates entities or cosine similarity threshold is too low, retrieved knowledge may be inaccurate.

### Mechanism 2
- Claim: EMERGE's cross-attention-based fusion network effectively integrates multimodal EHR data for improved clinical predictions.
- Mechanism: Cross-attention layers combine representations of time-series data, clinical notes, and RAG-generated summaries, allowing dynamic focus on relevant information from each modality.
- Core assumption: Cross-attention can effectively learn interactions between different modalities and that summaries contain useful, task-relevant information.
- Evidence anchors:
  - [abstract] "Finally, we fuse the summary with other modalities using an adaptive multimodal fusion network with cross-attention"
- Break condition: If cross-attention weights are not learned effectively or summaries are not informative.

### Mechanism 3
- Claim: EMERGE demonstrates robustness to data sparsity, maintaining strong performance even with limited training samples.
- Mechanism: External knowledge from PrimeKG and task-relevant summaries provide additional context that compensates for lack of training data.
- Core assumption: External knowledge and generated summaries provide sufficient additional information to overcome limitations of small training sets.
- Evidence anchors:
  - [abstract] "Comprehensive ablation studies and analysis highlight the efficacy of each designed module and robustness to data sparsity"
- Break condition: If external knowledge is not sufficiently relevant or summaries are not task-specific.

## Foundational Learning

- Concept: Retrieval-Augmented Generation (RAG)
  - Why needed here: RAG retrieves relevant medical knowledge from PrimeKG and generates task-specific summaries to enhance EHR predictive models.
  - Quick check question: How does RAG differ from traditional knowledge graph embedding approaches in EHR predictive modeling?

- Concept: Cross-attention mechanism
  - Why needed here: Cross-attention fuses different modalities by allowing each to attend to most relevant information from others.
  - Quick check question: What is the key difference between cross-attention and self-attention in multimodal fusion?

- Concept: Cosine similarity for entity matching
  - Why needed here: Cosine similarity matches entities extracted from EHR data to PrimeKG nodes, ensuring consistency and filtering LLM hallucinations.
  - Quick check question: Why is cosine similarity suitable for matching entity embeddings in knowledge graph alignment?

## Architecture Onboarding

- Component map: Entity extraction (LLM + z-score) -> KG matching (cosine similarity) -> Summary generation (LLM) -> Cross-attention fusion -> Prediction
- Critical path: Entity extraction → KG matching → Summary generation → Fusion → Prediction
- Design tradeoffs:
  - LLM for entity extraction vs. rule-based NER: LLMs are more flexible but may hallucinate; rules are more precise but less adaptable
  - Cosine similarity threshold: Higher threshold reduces false positives but may miss relevant entities; lower threshold increases recall but may introduce noise
  - Cross-attention fusion vs. simpler methods: Cross-attention is more expressive but computationally more expensive
- Failure signatures:
  - Low performance despite high-quality data: Likely issues with entity extraction or KG matching
  - Unstable training or overfitting: Possible issues with fusion network or insufficient regularization
  - Poor generalization: Summaries may not be task-relevant or fusion network not learning robust representations
- First 3 experiments:
  1. Evaluate entity extraction quality: Manually check sample of extracted entities from both time-series and clinical notes
  2. Tune KG matching threshold: Vary cosine similarity threshold and observe impact on model performance
  3. Ablation study on fusion methods: Compare cross-attention fusion with simpler methods (concat, add)

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does EMERGE's performance scale with increasingly larger external knowledge graphs (KGs)?
- Basis in paper: [inferred] The paper mentions using PrimeKG with 17,080 diseases and 4,050,249 relationships, but does not explore performance with varying KG sizes.
- Why unresolved: The paper only evaluates the framework with one specific KG and does not explore how performance changes with KG size or density.
- What evidence would resolve it: Experiments comparing EMERGE's performance using different KGs of varying sizes and densities.

### Open Question 2
- Question: What is the impact of different LLM prompting strategies on the quality and relevance of generated patient health status summaries?
- Basis in paper: [explicit] The paper discusses using task-relevant prompting strategies and provides a prompt template for summary generation.
- Why unresolved: While a specific prompting strategy is presented, the paper does not explore how variations in the prompt affect summary quality and impact on predictive tasks.
- What evidence would resolve it: Comparative experiments using different prompting strategies, evaluating generated summaries' quality and correlation with improved predictive performance.

### Open Question 3
- Question: How does EMERGE handle and mitigate potential biases present in the external knowledge graph and the LLM?
- Basis in paper: [inferred] The paper mentions using PrimeKG and discusses the hallucination issue associated with LLMs, emphasizing consistency guarantees.
- Why unresolved: The paper does not explicitly address how EMERGE handles potential biases in the KG (e.g., representation of certain diseases) or LLM (e.g., cultural or gender biases).
- What evidence would resolve it: Analysis of EMERGE's performance across different patient demographics and disease types, demonstrating fairness and robustness to potential biases.

## Limitations
- Reliance on LLM-generated entities and summaries may introduce hallucinations despite cosine similarity filtering
- Evaluation limited to two specific tasks (mortality and readmission prediction) using only MIMIC datasets
- Specific implementation details of adaptive multimodal fusion network and exact LLM prompt templates not fully specified

## Confidence

- **High Confidence**: Core RAG methodology and cross-attention fusion mechanism are well-established approaches in multimodal learning. Experimental design using standard MIMIC datasets provides solid foundation.

- **Medium Confidence**: Claimed robustness to data sparsity (maintaining performance with 1% training samples) needs independent verification and may not generalize.

- **Medium Confidence**: Entity extraction quality and KG matching effectiveness are difficult to assess without access to specific LLM prompts and PrimeKG content.

## Next Checks

1. **Entity Quality Audit**: Manually evaluate a stratified sample of 100+ extracted entities from both time-series and clinical notes to quantify hallucination rates and clinical relevance scores.

2. **Knowledge Graph Coverage Analysis**: Measure the entity alignment success rate (percentage of extracted entities that find matches in PrimeKG) and analyze distribution of match scores to identify potential coverage gaps.

3. **Data Efficiency Benchmarking**: Systematically evaluate model performance across multiple data fraction levels (1%, 5%, 10%, 25%, 50%, 100%) on both MIMIC-III and MIMIC-IV to verify claimed robustness to data sparsity holds across different data splits and dataset versions.