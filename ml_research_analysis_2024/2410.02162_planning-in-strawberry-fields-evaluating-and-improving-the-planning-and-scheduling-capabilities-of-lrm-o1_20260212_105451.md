---
ver: rpa2
title: 'Planning in Strawberry Fields: Evaluating and Improving the Planning and Scheduling
  Capabilities of LRM o1'
arxiv_id: '2410.02162'
source_url: https://arxiv.org/abs/2410.02162
tags:
- vertex
- days
- chattanooga
- knoxville
- nashville
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper evaluates the planning and scheduling capabilities\
  \ of OpenAI\u2019s o1 (Strawberry) Large Reasoning Models (LRMs) and compares them\
  \ to previous Large Language Models (LLMs) on established benchmarks. The authors\
  \ find that o1 models show significant improvements over LLMs on planning tasks,\
  \ especially in solving obfuscated \"Mystery Blocksworld\" problems, but still struggle\
  \ with longer plans and recognizing unsolvable instances."
---

# Planning in Strawberry Fields: Evaluating and Improving the Planning and Scheduling Capabilities of LRM o1

## Quick Facts
- **arXiv ID**: 2410.02162
- **Source URL**: https://arxiv.org/abs/2410.02162
- **Reference count**: 40
- **Key outcome**: This paper evaluates the planning and scheduling capabilities of OpenAI's o1 (Strawberry) Large Reasoning Models (LRMs) and compares them to previous Large Language Models (LLMs) on established benchmarks.

## Executive Summary
This paper evaluates the planning and scheduling capabilities of OpenAI's o1 models and compares them to previous LLMs on established benchmarks. The authors find that o1 models show significant improvements over LLMs on planning tasks, especially in solving obfuscated "Mystery Blocksworld" problems, but still struggle with longer plans and recognizing unsolvable instances. On scheduling tasks, o1 models perform well on some domains (e.g., graph coloring, calendar scheduling) but poorly on others (e.g., trip planning, meeting planning). The paper highlights the steep inference cost of LRMs and demonstrates that combining o1 models with external verifiers in an LRM-Modulo framework can improve performance and provide correctness guarantees, though at additional cost. The study underscores the need for comprehensive evaluations that account for accuracy, efficiency, cost, and guarantees when assessing planning and scheduling systems.

## Method Summary
The authors evaluate o1-preview and o1-mini models on established planning and scheduling benchmarks, including PlanBench (Blocksworld, Mystery Blocksworld, Logistics, Sokoban) and various scheduling domains (graph coloring, calendar scheduling, trip planning). They extend these benchmarks to harder instances and implement an LRM-Modulo framework that combines o1 models with external verifiers to provide correctness guarantees. The evaluation measures accuracy, inference cost, and time across different problem domains and difficulty levels, comparing performance against previous LLM approaches.

## Key Results
- o1 models show significant improvements over LLMs on planning tasks, particularly solving obfuscated "Mystery Blocksworld" problems
- o1 models struggle with longer plans (>10 steps) and recognizing unsolvable instances
- LRM-Modulo framework with external verifiers improves performance and provides correctness guarantees at additional cost

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LRMs like o1-preview achieve significant performance improvements over LLMs on planning and scheduling tasks by using reinforcement learning to learn effective Chain-of-Thought (CoT) generation strategies.
- Mechanism: The model generates candidate CoTs conditioned on the problem prompt, evaluates them using massive synthetic training data with known correct answers, and learns q-values for different CoT moves. At inference time, it may further refine these q-values through rollout-like processes before selecting a final CoT.
- Core assumption: The success signal can be reliably obtained from synthetic training data with correct answers, and the CoT moves are sufficiently expressive to capture planning reasoning.
- Evidence anchors:
  - [abstract] "o1 seems to have been trained to be an approximate reasoner, capable of scaling the amount of compute it uses depending on the query"
  - [section 3] "o1 combines an underlying LLM... into an RL-trained system that steers the creation, curation, and final selection of private Chain-of-Thought reasoning traces"
  - [corpus] "Found 25 related papers... Average neighbor FMR=0.496" - moderate evidence from related literature on LLM planning capabilities
- Break condition: If the synthetic training data is not representative of real planning problems, or if the CoT space is too sparse for effective RL learning, performance will degrade significantly.

### Mechanism 2
- Claim: The LRM-Modulo framework improves LRM performance by combining fallible generators with sound external verifiers, providing correctness guarantees.
- Mechanism: The LRM generates candidate solutions which are checked by an external verifier. If the solution passes verification, it is output; otherwise, the LRM is prompted again with feedback from the verifier to generate a new candidate.
- Core assumption: A sound verifier exists for the domain and can efficiently check candidate solutions.
- Evidence anchors:
  - [abstract] "combining o1 models with external verifiers–in a so-called LRM-Modulo system–guarantees the correctness of the combined system's output while further improving performance"
  - [section 4] "Prior to the release of these models, the best way to coax planning capabilities out of LLMs has been to pair them with sound external verifier in generate-test frameworks"
  - [corpus] "Found 25 related papers... Top related titles: LLMs Still Can't Plan; Can LRMs? A Preliminary Evaluation of OpenAI's o1 on PlanBench" - direct evidence from companion work
- Break condition: If the verifier is incomplete or cannot efficiently check solutions, the system will either output incorrect answers or fail to terminate.

### Mechanism 3
- Claim: LRMs show improved performance on scheduling tasks compared to LLMs because scheduling problems are NP-Hard rather than PSPACE-complete like classical planning, making them more amenable to approximate reasoning.
- Mechanism: The LRM's ability to generate and evaluate CoTs allows it to effectively search the solution space for scheduling problems, which have lower computational complexity than planning problems.
- Core assumption: The LRM's reasoning capabilities are sufficient to handle the complexity of NP-Hard scheduling problems but not the higher complexity of PSPACE-complete planning problems.
- Evidence anchors:
  - [abstract] "On scheduling tasks, o1 models perform well on some domains (e.g., graph coloring, calendar scheduling) but poorly on others (e.g., trip planning, meeting planning)"
  - [section 2.2] "Scheduling problems are only NP-Hard [5], and mainly revolve around resource allocation. These problems are equivalent to constraint satisfaction problems, and thus easier than the planning problems we describe above"
  - [corpus] Weak evidence - corpus focuses on planning literature, not scheduling
- Break condition: If scheduling problems in practice require solving instances at the higher complexity boundary, or if the LRM's reasoning is not well-suited to the constraint satisfaction structure, performance will degrade.

## Foundational Learning

- Concept: Classical planning vs scheduling problem complexity
  - Why needed here: Understanding why LRMs perform better on scheduling than planning tasks requires knowledge of the computational complexity differences between these problem classes.
  - Quick check question: What is the computational complexity class of STRIPS planning problems versus scheduling problems?

- Concept: Chain-of-Thought reasoning and its role in LRMs
  - Why needed here: The paper's explanation of how o1 works relies on understanding CoT as a reasoning mechanism and how RL can be applied to improve it.
  - Quick check question: How does Chain-of-Thought prompting differ from standard prompt engineering in LLMs?

- Concept: Generate-test frameworks and soundness
  - Why needed here: The LRM-Modulo approach depends on understanding how external verifiers can provide guarantees for otherwise fallible generators.
  - Quick check question: What does it mean for a verifier to be "sound" in the context of planning and scheduling?

## Architecture Onboarding

- Component map: Problem → LRM CoT generation → Solution candidate → Verifier check → Output (if verified) OR backprompt → repeat
- Critical path: Problem → LRM CoT generation → Solution candidate → Verifier check → Output (if verified) OR backprompt → repeat
- Design tradeoffs:
  - Cost vs. accuracy: LRMs are significantly more expensive than LLMs but provide better performance
  - Speed vs. guarantees: LRM-Modulo provides guarantees but at the cost of multiple inference cycles
  - Black box vs. interpretability: LRMs remain opaque despite better performance
- Failure signatures:
  - High inference cost with poor performance: LRM not effective for the domain
  - LRM-Modulo stuck in loops: Verifier too strict or LRM unable to generate valid candidates
  - Performance degradation on harder problems: LRM reasoning capacity limits reached
- First 3 experiments:
  1. Run o1-preview on simple Blocksworld problems (3-5 blocks) to verify baseline performance
  2. Test LRM-Modulo with o1-mini on graph coloring problems to validate the framework
  3. Compare cost per correct answer between o1 models and LLM-Modulo approaches on calendar scheduling tasks

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What specific architectural modifications or training techniques distinguish Large Reasoning Models (LRMs) like o1 from traditional Large Language Models (LLMs), and how do these changes contribute to improved planning capabilities?
- Basis in paper: Explicit - The paper discusses that o1 is a new class of model, designed to combine LLM language capabilities with approximate reasoning, but exact details are sparse and internal traces remain inaccessible.
- Why unresolved: OpenAI keeps the architecture under wraps and hides reasoning traces, making it impossible to verify the speculated internal operations or determine the precise mechanisms enabling LRM's reasoning abilities.
- What evidence would resolve it: Detailed technical documentation or research papers from OpenAI revealing the specific architectural components, training methodologies, and inference procedures used in o1, along with access to internal reasoning traces for analysis.

### Open Question 2
- Question: How does the performance of LRMs like o1 degrade with increasing problem complexity, and what are the specific limitations that prevent robust planning in larger or more complex domains?
- Basis in paper: Explicit - The paper shows that o1's performance degrades on harder Blocksworld problems requiring more than 10 steps and struggles with recognizing unsolvable instances, indicating limitations in robust planning.
- Why unresolved: While the paper provides initial observations on performance degradation, a comprehensive analysis of how LRMs handle varying problem sizes and complexities across different domains is lacking, and the reasons for these limitations remain unclear.
- What evidence would resolve it: Extensive benchmarking of LRMs on a wide range of planning problems with varying complexity, including analysis of failure modes and identification of specific bottlenecks or limitations in the reasoning process.

### Open Question 3
- Question: Can the LLM-Modulo framework be effectively adapted to LRMs like o1 to provide correctness guarantees while maintaining cost-effectiveness and improving performance on planning tasks?
- Basis in paper: Explicit - The paper demonstrates that combining o1 models with external verifiers in an LRM-Modulo system can improve performance and provide correctness guarantees, but questions remain about cost-effectiveness.
- Why unresolved: The paper shows initial success with LRM-Modulo but does not provide a comprehensive analysis of the trade-offs between performance improvement, cost, and the effectiveness of this approach across various planning domains.
- What evidence would resolve it: Systematic evaluation of LRM-Modulo systems across multiple planning domains, comparing performance, cost, and guarantees against direct LRM usage and other approaches like classical planners, along with analysis of the effectiveness of backprompting and feedback mechanisms.

## Limitations

- o1 models show inconsistent performance across different planning and scheduling domains, with strong results on some tasks but poor performance on others
- The steep inference costs of o1 models (10-20× higher than LLMs) may limit their practical deployment for many applications
- LRM-Modulo framework, while providing correctness guarantees, adds additional complexity and cost that may not be justified in all use cases

## Confidence

Medium confidence in the paper's core findings due to several factors:
- The benchmark results clearly demonstrate o1's superiority over traditional LLMs on certain planning tasks
- The inconsistent performance across domains suggests improvements are not universal
- The analysis of cost-effectiveness and practical deployment limitations adds important context to the performance claims

## Next Checks

1. Test o1 models on extended benchmark suites with varying problem sizes to establish clear scaling limits and identify the point at which performance degrades significantly.
2. Implement cost-benefit analysis comparing LRM-Modulo systems against traditional LLM-Modulo approaches across multiple application domains to determine when the additional expense is justified.
3. Conduct ablation studies on the Chain-of-Thought reasoning patterns to identify which specific reasoning strategies contribute most to performance improvements on different task types.