---
ver: rpa2
title: 'Better Representations via Adversarial Training in Pre-Training: A Theoretical
  Perspective'
arxiv_id: '2401.15248'
source_url: https://arxiv.org/abs/2401.15248
tags:
- adversarial
- training
- features
- learning
- clean
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper investigates the inheritance of adversarial robustness\
  \ from pre-trained models to downstream tasks using a theoretical framework based\
  \ on sparse coding models and two-layer neural networks. The authors demonstrate\
  \ that adversarial training promotes feature purification\u2014each hidden node\
  \ learning one or a few features\u2014while clean training does not."
---

# Better Representations via Adversarial Training in Pre-Training: A Theoretical Perspective

## Quick Facts
- arXiv ID: 2401.15248
- Source URL: https://arxiv.org/abs/2401.15248
- Reference count: 40
- Primary result: Adversarial pre-training promotes feature purification, enabling downstream clean training to achieve robust performance

## Executive Summary
This paper investigates how adversarial training in pre-training phases affects the robustness of downstream tasks. The authors develop a theoretical framework based on sparse coding models and two-layer neural networks to analyze feature purification—where hidden nodes learn single features rather than mixtures. They demonstrate that adversarial training achieves this purification effect, which enables downstream models trained with clean training to maintain good adversarial robustness. The theory extends to both supervised and contrastive learning settings, with contrastive learning showing the unique property that adversarial attacks primarily affect dissimilar data pairs. Real-data experiments on CIFAR-10 validate the theoretical findings, showing improved robustness against PGD attacks when using adversarial pre-training compared to clean pre-training.

## Method Summary
The method involves pre-training a two-layer neural network on sparse coding data using either clean or adversarial training, followed by downstream training on a new last layer. For supervised learning, adversarial training uses fast gradient method (FGM) attacks with l∞ norm, while contrastive learning uses similar attacks but with pairs of data (Y=1 for similar pairs, Y=-1 for dissimilar pairs). The key innovation is analyzing how adversarial training promotes feature purification—where each hidden node learns one or a few features rather than a mixture. Downstream tasks then use clean training on the purified pre-trained model, and robustness is evaluated using PGD attacks under l∞ norm with ε = 8/255.

## Key Results
- Adversarial training promotes feature purification where each hidden node learns one or a few features, while clean training does not
- Contrastive learning achieves purification differently than supervised learning, primarily through attacks on dissimilar data pairs
- Downstream tasks trained with clean training achieve good adversarial robustness when pre-trained with adversarial training
- Real-data experiments on CIFAR-10 show improved robustness against PGD attacks with adversarial pre-training versus clean pre-training

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Adversarial training promotes feature purification in pre-trained models, where each hidden node learns only one or a few features rather than a mixture.
- Mechanism: The adversarial loss design encourages the model to learn sparse representations where hidden nodes become specialized to individual features, reducing the impact of adversarial attacks.
- Core assumption: The data follows a sparse coding model where underlying features X are mixed into observable Z with noise, and Y depends directly on X.
- Evidence anchors:
  - [abstract] "adversarial training promotes feature purification—each hidden node learning one or a few features"
  - [section 4.2] "With purified features, adding an attack does not impact the inactive features"
  - [corpus] Weak - related papers don't directly address feature purification mechanisms
- Break condition: If the data generation model deviates significantly from the sparse coding assumption, or if the activation function cannot effectively screen out noise.

### Mechanism 2
- Claim: Contrastive learning achieves feature purification differently than supervised learning, primarily through attacks on dissimilar data pairs.
- Mechanism: In contrastive learning, adversarial attacks on dissimilar pairs (different true features) affect the loss more than attacks on similar pairs, driving the network to purify features in the dissimilar case.
- Core assumption: The contrastive learning setup uses pairs of data where Y=1 indicates similar pairs (same true features) and Y=-1 indicates dissimilar pairs (different true features).
- Evidence anchors:
  - [section 5.3] "adversarial training affects more on the loss of dissimilar pairs" and simulation showing this effect
  - [section 5.2] Theoretical analysis showing optimal contrastive solutions under linear network constraints
  - [corpus] Weak - related papers focus on contrastive learning theory but not the purification mechanism via dissimilar pairs
- Break condition: If the contrastive learning objective or data pairing strategy changes significantly from the assumed setup.

### Mechanism 3
- Claim: When pre-trained models achieve feature purification, downstream tasks trained with clean training can achieve good adversarial robustness through inheritance.
- Mechanism: Purified hidden nodes in the pre-trained model mean that clean training of downstream tasks can still maintain robustness because the critical features are already isolated and protected from attack.
- Core assumption: The pre-trained model has achieved sufficient feature purification (most hidden nodes learn single features) and the downstream task uses the same or similar data generation model.
- Evidence anchors:
  - [abstract] "downstream tasks trained with clean training can achieve good adversarial robustness" when pre-trained model is purified
  - [section 6] "When the pre-trained model perfectly purifies the hidden nodes, we can achieve good model robustness when the downstream tasks are trained using clean training"
  - [section 7.1] Real-data experiments showing improved robustness with adversarial pre-training
- Break condition: If the downstream data distribution differs significantly from pre-training data, or if the purification level is insufficient.

## Foundational Learning

- Concept: Sparse coding models
  - Why needed here: The theoretical framework relies on data being generated from sparse underlying features X that are mixed into observable Z with noise
  - Quick check question: If you have d features and only k of them are active in any given sample, what's the sparsity ratio?

- Concept: Adversarial training and fast gradient attack (FGM)
  - Why needed here: The paper analyzes how adversarial training affects feature learning and robustness through gradient-based attacks
  - Quick check question: If you have a loss function l and model f, what is the formula for the L2 FGM attack?

- Concept: Contrastive learning objectives and positive/negative pairs
  - Why needed here: The paper extends analysis to self-supervised contrastive learning and how purification occurs differently there
  - Quick check question: In contrastive learning, what do Y=1 and Y=-1 represent in terms of data pairs?

## Architecture Onboarding

- Component map: Two-layer neural network with H hidden nodes, each using activation function σ(v,e) = v1{|v|≥e}, input layer, output layer (supervised) or multiple outputs (contrastive)
- Critical path: Pre-training (adversarial vs clean) → Feature purification assessment → Downstream training (clean) → Robustness evaluation
- Design tradeoffs: Purified features improve robustness but may slightly reduce standard accuracy; contrastive learning purifies differently than supervised learning
- Failure signatures: Downstream model shows poor robustness despite adversarial pre-training; hidden nodes don't show sparse activation patterns; contrastive loss doesn't differentiate between similar and dissimilar pair attacks
- First 3 experiments:
  1. Train two-layer network with clean training on synthetic sparse data, measure feature mixture in hidden nodes
  2. Repeat with adversarial training, verify hidden nodes now learn single features
  3. Apply pre-trained model to downstream task with clean training, test adversarial robustness inheritance

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the precise mathematical relationship between feature purification in pre-trained models and robustness inheritance in downstream tasks under different attack magnitudes (ϵ)?
- Basis in paper: Explicit. The paper proves that when hidden nodes are purified through adversarial pre-training, clean training suffices for downstream robustness, but the exact quantitative relationship between attack strength and robustness inheritance remains unclear.
- Why unresolved: The paper establishes theoretical bounds showing robustness inheritance occurs, but doesn't provide precise numerical relationships between attack magnitude and the degree of robustness transfer.
- What evidence would resolve it: Experimental studies varying attack strengths systematically across different pre-training scenarios, combined with theoretical analysis deriving exact functional relationships between ϵ and robustness inheritance metrics.

### Open Question 2
- Question: How do different activation functions beyond ReLU and the specific gate function affect feature purification and robustness inheritance?
- Basis in paper: Inferred. The paper primarily analyzes ReLU and a specific gate function (σ(v,e) = v1{|v|≥e}), noting these can screen out noise, but acknowledges other activation functions might work similarly.
- Why unresolved: The theoretical framework focuses on these specific activation functions, leaving open how other common activation functions like tanh, sigmoid, or Swish would perform in terms of purification and robustness inheritance.
- What evidence would resolve it: Comparative theoretical analysis of various activation functions' ability to achieve feature purification under adversarial training, validated through experiments measuring purification levels and downstream robustness.

### Open Question 3
- Question: Does feature purification occur in contrastive learning when attacking similar data pairs, contrary to the paper's observation that only dissimilar pairs are affected?
- Basis in paper: Explicit. The paper observes that adversarial attacks primarily affect dissimilar pairs in contrastive learning, with similar pairs showing resistance to attacks.
- Why unresolved: While the paper provides theoretical justification and simulations supporting this asymmetry, it doesn't explore whether purification can be achieved by attacking similar pairs or what conditions would enable this.
- What evidence would resolve it: Theoretical extension of the contrastive learning framework to include attacks on similar pairs, combined with experiments testing whether such attacks can induce purification effects in those pairs.

## Limitations
- Theoretical framework relies on sparse coding data generation model which may not capture all real-world data distributions
- Two-layer neural network architecture provides tractable analysis but may not fully represent modern deep networks
- Empirical validation is limited to CIFAR-10 and specific attack settings (PGD with l∞ norm, ε = 8/255)

## Confidence
- Feature purification mechanism (Mechanism 1): High confidence based on theoretical proofs and synthetic data experiments
- Contrastive learning purification (Mechanism 2): Medium confidence - theoretical analysis is sound but synthetic data results show mixed effects
- Robustness inheritance phenomenon (Mechanism 3): Medium confidence - real-data experiments support findings but limited to specific datasets and models

## Next Checks
1. Test the robustness inheritance effect across different datasets (beyond CIFAR-10) and network architectures to assess generalizability
2. Conduct ablation studies varying the degree of feature purification to quantify the relationship between purification level and robustness inheritance
3. Evaluate performance against multiple attack types beyond PGD to verify the robustness claims under diverse threat models