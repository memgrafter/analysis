---
ver: rpa2
title: 'Let''s Think Dot by Dot: Hidden Computation in Transformer Language Models'
arxiv_id: '2404.15758'
source_url: https://arxiv.org/abs/2404.15758
tags:
- tokens
- filler
- performance
- token
- input
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates whether transformers can use meaningless
  filler tokens (e.g., repeated dots) to perform computations that are not reflected
  in the observable chain-of-thought tokens. The authors construct two synthetic tasks,
  3SUM and 2SUM-Transform, where transformers fail to solve the task without filler
  tokens but achieve high accuracy (100% for 3SUM and 94% for 2SUM-Transform) when
  provided filler tokens.
---

# Let's Think Dot by Dot: Hidden Computation in Transformer Language Models

## Quick Facts
- arXiv ID: 2404.15758
- Source URL: https://arxiv.org/abs/2404.15758
- Reference count: 13
- Transformers can use meaningless filler tokens to perform hidden computations not reflected in observable chain-of-thought tokens

## Executive Summary
This paper investigates whether transformers can leverage meaningless filler tokens (like repeated dots) to perform computations that are not visible in the observable reasoning tokens. Through experiments on synthetic tasks (3SUM and 2SUM-Transform), the authors demonstrate that transformers fail to solve these problems without filler tokens but achieve high accuracy when provided with filler tokens. The results show that additional tokens can provide computational benefits independent of their semantic content, raising concerns about unauditable, hidden computations in large language models. The paper also provides a theoretical characterization of when filler tokens are useful in terms of quantifier depth in first-order logic.

## Method Summary
The authors construct two synthetic tasks: 3SUM (determining if any three numbers sum to zero) and 2SUM-Transform (a permutation-based variant). They use a 34M-parameter 4-layer Llama transformer with 384 hidden dimension and 6 attention heads. The model is trained on 10 million samples and tested on 2,000 samples using Adam optimizer with learning rate 1e-4, weight decay 0.01, and gradient clipping. Experiments compare performance with chain-of-thought tokens, filler tokens, and immediate answers across varying input lengths and dimensions.

## Key Results
- Transformers achieve 100% accuracy on length-12 3SUM with filler tokens vs 66% without
- 2SUM-Transform accuracy improves from 78.7% to 93.6% with filler tokens
- Performance gaps increase with input complexity (longer sequences and higher dimensions)
- Learning to use filler tokens requires specific dense supervision, not standard chain-of-thought data

## Why This Works (Mechanism)

### Mechanism 1
- Filler tokens provide additional computational capacity that allows transformers to solve problems beyond their single-forward-pass expressivity limit
- Filler tokens enable parallel computation of intermediate results that can be attended to by the final prediction token
- Core assumption: Computational benefit is independent of semantic content; any tokens can serve as filler tokens
- Evidence: "Our results show that additional tokens can provide computational benefits independent of token choice"

### Mechanism 2
- Learning to use filler tokens is difficult because it requires discovering parallelizable algorithmic solutions
- Model must learn to map filler tokens to intermediate computations in a parallelizable way
- Core assumption: Instance-adaptive chains of thought don't transfer to filler token usage because they require serial computation
- Evidence: "We find that learning to use filler tokens is difficult and requires specific, dense supervision"

### Mechanism 3
- Computational benefit of filler tokens scales with problem complexity
- Longer or higher-dimensional inputs show larger performance gaps between filler and no-filler settings
- Core assumption: Problem complexity correlates with expressivity gap that filler tokens can bridge
- Evidence: "Figure 2 shows that, as we scale the length of inputs up to length 12, we find increasing performance gaps"

## Foundational Learning

- Concept: Quantifier depth in first-order logic
  - Why needed here: Characterizes when filler tokens are useful in terms of the quantifier depth of a first-order formula
  - Quick check question: What is the maximum quantifier depth that can be expressed by a transformer without filler tokens?

- Concept: Circuit complexity classes (TC0)
  - Why needed here: Places transformers without additional reasoning tokens in TC0 to understand limitations
  - Quick check question: What does it mean for a problem to be in TC0, and why can't transformers solve problems outside this class without additional reasoning tokens?

- Concept: Parallel vs serial computation
  - Why needed here: Distinguishes between parallelizable computations (compatible with filler tokens) and instance-adaptive, serial computations
  - Quick check question: Why can't instance-adaptive chains of thought be parallelized across filler tokens?

## Architecture Onboarding

- Component map: Input layer -> Encoder layers -> Positional encodings -> Output layer
- Critical path: Input embedding -> Multi-head attention -> Feed-forward network -> Final attention layer -> Loss computation
- Design tradeoffs: One-hot vs embedding vectors, number of filler tokens vs computational cost, mixed vs pure training data, parallelizable vs instance-adaptive supervision
- Failure signatures: Model fails to converge with instance-adaptive chain-of-thought data, no improvement on single-forward-pass solvable problems, training instability with certain hyperparameters
- First 3 experiments:
  1. Train on length-6 3SUM: Should achieve 100% accuracy with and without filler tokens
  2. Train on length-12 3SUM: Should show performance gap (66% without, 100% with filler)
  3. Train on 2SUM-Transform with and without filler tokens: Should show moderate improvement (78.7% to 93.6%)

## Open Questions the Paper Calls Out

1. Do large language models with hundreds of layers require prohibitively many filler tokens to realize improved performance over the no-filler baseline?

2. To what extent do token-parallelizable, TC0 algorithmic problems arise in the natural language context?

3. To what extent does natural-language text provide adequate supervision for filler-token computation, providing parallelizable supervision rather than non-parallelizable, instance-adaptive chains of thought?

## Limitations

- Expressivity gap quantification remains unclear - theoretical bounds don't precisely measure practical impact
- Training data sensitivity - exact nature of "dense supervision" and generalization to other problem types unclear
- Positional encoding dependence - how different schemes affect filler token usage needs more exploration

## Confidence

- High Confidence: Empirical demonstration of performance improvements with filler tokens on 3SUM and 2SUM-Transform
- Medium Confidence: Theoretical characterization in terms of quantifier depth and circuit complexity
- Low Confidence: Claim that filler tokens provide benefits independent of token choice across all problem domains

## Next Checks

1. Evaluate filler token approach on diverse problems beyond 3SUM and 2SUM-Transform, including natural language tasks

2. Systematically vary positional encoding schemes and measure effects on filler token utilization

3. Design training protocols that explicitly teach mapping between chain-of-thought steps and parallel filler token computations