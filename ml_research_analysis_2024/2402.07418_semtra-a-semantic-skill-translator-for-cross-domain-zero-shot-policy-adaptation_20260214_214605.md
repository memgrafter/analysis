---
ver: rpa2
title: 'SemTra: A Semantic Skill Translator for Cross-Domain Zero-Shot Policy Adaptation'
arxiv_id: '2402.07418'
source_url: https://arxiv.org/abs/2402.07418
tags:
- skill
- task
- semantic
- domain
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This work presents SemTra, a framework that achieves zero-shot\
  \ policy adaptation across domains using semantic skills derived from multi-modal\
  \ task prompts. The two-phase hierarchical adaptation\u2014task-level semantic skill\
  \ generation via PLM prompting, followed by skill-level domain context optimization\
  \ through parametric instantiation\u2014enables robust adaptation to unseen long-horizon\
  \ tasks."
---

# SemTra: A Semantic Skill Translator for Cross-Domain Zero-Shot Policy Adaptation

## Quick Facts
- arXiv ID: 2402.07418
- Source URL: https://arxiv.org/abs/2402.07418
- Reference count: 40
- Outperforms baselines by 36.34–66.24% in N-rate for zero-shot cross-domain policy adaptation

## Executive Summary
SemTra presents a framework for zero-shot policy adaptation across domains using semantic skills derived from multi-modal task prompts. The approach employs a two-phase hierarchical adaptation: task-level semantic skill generation via PLM prompting, followed by skill-level domain context optimization through parametric instantiation. This disentangles domain-invariant semantics from domain-specific execution parameters, enabling robust adaptation to unseen long-horizon tasks. Evaluated on Franka Kitchen, Meta-World, RLBench, and CARLA, SemTra demonstrates strong cross-domain performance comparable to single-domain baselines.

## Method Summary
The framework processes multi-modal task prompts (video, sensor data, text) through specialized encoders that convert them into skill-level language instructions. A PLM then translates these into semantic skill sequences, which are parameterized for domain contexts and executed through a behavior decoder. The two-level hierarchy separates task adaptation (semantic skill generation) from skill adaptation (domain context optimization), enabling zero-shot transfer across domains.

## Key Results
- Outperforms baselines by 36.34–66.24% in N-rate across multiple benchmarks
- Achieves cross-domain performance comparable to single-domain baselines
- Fine-tuning video skill encoders improves adaptation when demonstrations are annotated
- Demonstrates practical applicability in cognitive robotics and autonomous driving

## Why This Works (Mechanism)

### Mechanism 1
The hierarchical two-phase adaptation (task-level semantic skill generation, then skill-level domain context optimization) enables zero-shot cross-domain policy adaptation by separating domain-invariant semantics from domain-specific execution parameters. PLM prompting transforms multi-modal snippets into semantic skills, while parametric instantiation conditions skill execution on domain context.

### Mechanism 2
Multi-modal skill encoders (vision-language models for video, classifiers for sensor, identity for text) allow robust extraction of semantic skills from diverse input formats. Different modalities are processed by specialized encoders that map them to skill-level language tokens, creating a unified semantic representation from heterogeneous inputs.

### Mechanism 3
Online context encoding via contrastive learning captures dynamic environmental shifts at runtime, enabling adaptation to non-stationary conditions not present in the task prompt. The online context encoder infers hidden context from recent state-action history, allowing real-time adjustment to environmental changes.

## Foundational Learning

- **Pretrained Language Models (PLMs)**: PLMs provide logical reasoning capability to translate multi-modal skill descriptions into coherent semantic skill sequences and plan skill execution order. *Quick check*: Can the PLM generate a correct skill sequence from "Open the microwave, then turn on the light" when given corresponding semantic skill embeddings?

- **Contrastive Learning for Video-to-Text Alignment**: Contrastive learning aligns video demonstrations with semantic descriptions, enabling video skill encoders to retrieve correct semantic skills from video snippets. *Quick check*: Given a video of opening a microwave, does the video skill encoder retrieve the correct semantic skill embedding matching "open microwave"?

- **Parametric Skill Instantiation and Disentanglement**: Domain contexts (speed, wind, embodiment) must be encoded as parameters that modify skill execution without changing the semantic skill itself, enabling zero-shot adaptation. *Quick check*: Does the framework correctly instantiate "move to destination" with different speed parameters for fast vs. slow contexts while preserving underlying skill semantics?

## Architecture Onboarding

- **Component map**: Multi-modal task prompt → Multi-modal skill encoders → Skill-level language instruction → Semantic skill decoder → Semantic skill sequence → Context encoder → Executable skill sequence → Behavior decoder → Action output → Environment interaction

- **Critical path**: Task prompt → Multi-modal encoding → PLM semantic translation → Skill sequence generation → Context parameterization → Action generation → Environment execution

- **Design tradeoffs**: PLM size vs. fine-tuning feasibility (larger PLMs give better reasoning but are harder to fine-tune); annotation requirements (full semantic annotation is ideal but expensive); context encoding granularity (too coarse loses adaptation capability, too fine-grained makes learning harder)

- **Failure signatures**: Semantic skill sequence generation fails (incorrect skill ordering or missing skills); context parameterization fails (wrong domain parameters); online context encoding fails (cannot adapt to sudden environmental changes); multi-modal encoding fails (wrong semantic skills extracted)

- **First 3 experiments**: 1) Test semantic skill sequence generation with known task prompts; 2) Test context parameterization on simple domain contexts like speed; 3) Run complete framework on simple Franka Kitchen task with known domain context

## Open Questions the Paper Calls Out

### Open Question 1
How does the performance of SemTra change when using different types of PLMs for the semantic skill sequence generator? The paper mentions that several PLMs (GPT2, GPT2-large, Bloom) were tested but only one type was used in final experiments.

### Open Question 2
How does the performance of SemTra change when using different types of video skill encoders? The paper mentions using a video skill encoder but does not explore different types of video skill encoders.

### Open Question 3
How does the performance of SemTra change when using different types of context encoders? The paper mentions using a context encoder but does not explore different types of context encoders.

## Limitations
- Multi-modal skill encoder generalization robustness to noisy or incomplete input data remains unclear
- Online context encoding effectiveness lacks thorough quantitative evaluation in dynamic environments
- PLM reasoning limitations may struggle with complex temporal dependencies or rare skill combinations

## Confidence
- **High Confidence**: Hierarchical two-phase adaptation mechanism (task-level semantic skill generation, skill-level domain context optimization)
- **Medium Confidence**: Multi-modal skill encoding approach supported by ablation studies but needs validation on diverse real-world inputs
- **Low Confidence**: Online context encoding for non-stationary adaptation proposed but lacks comprehensive experimental validation

## Next Checks
1. **Stress Test Multi-modal Encoders**: Evaluate performance when individual modalities are degraded (low-quality video, noisy sensor data) to assess robustness of semantic skill extraction
2. **Quantify Online Context Adaptation**: Design experiments testing online context encoder's ability to handle sudden environmental changes and measure performance improvement over static encoding
3. **Evaluate PLM Reasoning Limits**: Systematically test framework on increasingly complex task prompts with longer skill sequences to identify breaking point of PLM's semantic reasoning capabilities