---
ver: rpa2
title: Naturally Occurring Feedback is Common, Extractable and Useful
arxiv_id: '2407.10944'
source_url: https://arxiv.org/abs/2407.10944
tags:
- feedback
- user
- response
- arxiv
- human
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper presents a method for extracting naturally occurring
  feedback from human-AI conversations to improve language model alignment. The authors
  identify and classify five types of feedback patterns in conversations, showing
  that about 30% of recent conversations contain explicit feedback.
---

# Naturally Occurring Feedback is Common, Extractable and Useful

## Quick Facts
- arXiv ID: 2407.10944
- Source URL: https://arxiv.org/abs/2407.10944
- Authors: Shachar Don-Yehiya; Leshem Choshen; Omri Abend
- Reference count: 17
- Models trained on extracted feedback win 77-79% of human comparison tests

## Executive Summary
This paper presents a method for extracting naturally occurring feedback from human-AI conversations to improve language model alignment. The authors identify and classify five types of feedback patterns in conversations, showing that about 30% of recent conversations contain explicit feedback. They develop an automated extraction method using large language models, which successfully identifies hundreds of thousands of feedback instances from over 1 million conversations. The extracted feedback demonstrates clear value: models trained on this data outperform baseline models in human evaluations and other assessment methods.

## Method Summary
The method involves defining a taxonomy of five feedback categories (repeat/rephrase, make aware with correction, make aware without correction, ask for clarification, and positive feedback), then using a large language model (Mixtral-8x7B-Instruct) to automatically extract these feedback instances from conversations in the LMSYS-Chat-1M dataset. The extracted feedback is then used for model training through finetuning and preference learning (KTO), with the trained models evaluated against baseline pretrained models using human judgment, open reward models, and GPT-4 as judge.

## Key Results
- Automated extraction identified hundreds of thousands of feedback instances from over 1 million conversations
- Models trained on extracted feedback won 77-79% of human comparison tests against baseline models
- The extraction method achieved 0.43 precision and 0.58 recall against manual annotations

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Extracting naturally occurring feedback from human-AI conversations creates scalable training data that improves model alignment without explicit annotation
- Mechanism: When users interact with language models, they naturally provide feedback through explicit statements (positive or negative) and implicit signals (rephrasing, asking for clarification). This feedback can be automatically extracted using large language models and used as training signals.
- Core assumption: User feedback in conversations contains meaningful alignment signals that generalize to improve model behavior across different contexts
- Evidence anchors:
  - [abstract]: "We propose a method for automatically extracting this feedback, and apply it to over 1M conversations to obtain hundreds of thousands of feedback samples. The extracted feedback shows promise: training with it improves over baseline models and enhances model alignment to human preferences."
  - [section 3.1]: "We define the following categories, split into four negative feedback categories and one positive: 1. Repeat or Rephrase (rephrase): The user repeats or rephrases their last response, explaining again what they wants."
  - [corpus]: Found 25 related papers (using 8). Average neighbor FMR=0.441, average citations=0.0. Top related titles include "Trust No Bot: Discovering Personal Disclosures in Human-LLM Conversations in the Wild" suggesting related work on human-AI interaction patterns.
- Break condition: If extracted feedback contains too much noise or irrelevant signals, the training process may not converge or could reinforce unwanted behaviors

### Mechanism 2
- Claim: Feedback extraction works better on newer conversation datasets because user expectations and interaction patterns have evolved
- Mechanism: As language models improve over time, users interact with them more naturally and provide more explicit feedback. This creates a positive feedback loop where better models generate more useful feedback data.
- Core assumption: User behavior and expectations evolve in parallel with model capabilities, leading to more frequent and clearer feedback signals in recent conversations
- Evidence anchors:
  - [section 3.3]: "Comparing to older datasets, we find that naturally occurring feedback is more prevalent in recent conversation datasets, possibly due to users raising their expectations and being able to conduct a more 'human-like' conversation with the model."
  - [abstract]: "Comparing to older datasets, we find that naturally occurring feedback is more prevalent in recent conversation datasets, suggesting that more than ever, naturally occurring feedback can serve as a valuable resource for feedback data."
  - [corpus]: Limited direct evidence; the corpus shows related work on human-AI conversations but doesn't specifically address temporal trends in feedback prevalence
- Break condition: If user expectations plateau or if users become desensitized to model capabilities, feedback frequency may stop increasing despite model improvements

### Mechanism 3
- Claim: Using extracted feedback for preference training improves model alignment more effectively than random chat data
- Mechanism: The extracted feedback contains both positive and negative examples that are specifically tied to user satisfaction, making them more informative than generic conversation data for learning alignment preferences.
- Core assumption: Feedback explicitly tied to user satisfaction provides more targeted learning signals than general conversation data
- Evidence anchors:
  - [section 5.4]: "To balance this, we use only the 'Make Aware with Correction' and 'Make Aware without Correction' categories, and on top of that we down-sample. We chose these categories as we assume their 'negative' signal is the strongest."
  - [section 5.3]: "As an additional baseline, we replace our extracted positive examples with a random sample of chat examples from the LMSYS-Chat-1M dataset of the same size... We finetune the 7B model on them, and evaluate their performance. Eurus-RM-7b reports 64% wins, FsfairX-LLaMA3-RM-v0.1 reports 68% wins, and GPT-4 reports 75% wins, all outperformed by the model we trained on the extracted positive examples."
  - [corpus]: No direct evidence; corpus neighbors don't specifically address preference training comparisons
- Break condition: If the feedback extraction process introduces significant bias or if the negative examples are not truly representative of user preferences, the training may learn incorrect alignment signals

## Foundational Learning

- Concept: Feedback taxonomy and categorization
  - Why needed here: The extraction process relies on categorizing user responses into specific feedback types to identify meaningful signals from conversations
  - Quick check question: Can you list all five feedback categories defined in the paper and explain how each type provides different alignment signals?

- Concept: Preference learning and reinforcement learning from human feedback (RLHF)
  - Why needed here: The extracted feedback is used to train models using techniques like KTO (which improves over DPO), requiring understanding of how preference learning works
  - Quick check question: What is the key difference between KTO and DPO, and why was KTO chosen for this work?

- Concept: Evaluation methodology for alignment tasks
  - Why needed here: The paper uses multiple evaluation methods (human evaluation, open models as judges, GPT-4 as judge) to validate improvements, requiring understanding of evaluation design
  - Quick check question: Why did the authors use multiple evaluation methods, and what potential biases did they try to address with this approach?

## Architecture Onboarding

- Component map: LMSYS-Chat-1M dataset -> Feedback extraction pipeline (Mixtral-8x7B-Instruct) -> Natural Feedback Dataset (~170k samples) -> Model training (finetuning + preference training) -> Evaluation (human + open models + GPT-4)

- Critical path: Conversation -> Feedback extraction -> Dataset creation -> Model training -> Evaluation

- Design tradeoffs:
  - Using larger models for extraction vs. smaller, faster models
  - Precision vs. recall in feedback extraction (0.43 precision, 0.58 recall)
  - Number of feedback categories vs. extraction accuracy
  - Real-time extraction vs. batch processing of conversations

- Failure signatures:
  - Low precision in extraction (many false positives)
  - Models not improving over baselines in evaluation
  - Feedback distribution skewed heavily toward one category
  - Extraction pipeline failing on certain conversation formats

- First 3 experiments:
  1. Run extraction on a small sample (100 conversations) and manually verify feedback accuracy
  2. Train a small model (1.4B) on extracted positive examples only and compare to baseline
  3. Test different taxonomy configurations (e.g., binary positive/negative vs. five categories) to find optimal extraction performance

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the precision and recall of the automatic feedback extraction method be further improved?
- Basis in paper: [inferred] The paper mentions that the automatic extraction method can be improved with better models, prompts, or more sophisticated extraction algorithms.
- Why unresolved: The authors only experimented with a few variations of the extraction method and did not exhaustively explore all possible improvements.
- What evidence would resolve it: Conducting a systematic study of different model architectures, prompt engineering techniques, and post-processing methods to optimize the precision-recall tradeoff.

### Open Question 2
- Question: How does the performance of models trained on naturally occurring feedback compare to models trained on expert-annotated feedback?
- Basis in paper: [explicit] The authors mention that collecting human feedback is costly and not scalable, and propose using naturally occurring feedback as an alternative.
- Why unresolved: The paper does not directly compare the performance of models trained on naturally occurring feedback to those trained on expert-annotated feedback.
- What evidence would resolve it: Training and evaluating models on both naturally occurring feedback and expert-annotated feedback, and comparing their performance on a held-out test set.

### Open Question 3
- Question: How can the feedback extraction method be adapted to handle conversations in languages other than English?
- Basis in paper: [explicit] The authors mention that they filtered out non-English conversations during evaluation due to language barriers.
- Why unresolved: The paper does not explore the feasibility or performance of the feedback extraction method on non-English conversations.
- What evidence would resolve it: Adapting the feedback extraction method to handle non-English conversations and evaluating its performance on multilingual datasets.

### Open Question 4
- Question: How does the performance of models trained on naturally occurring feedback vary across different domains or tasks?
- Basis in paper: [inferred] The authors use a general-purpose dataset (LMSYS-Chat-1M) and do not explore the domain-specific performance of the trained models.
- Why unresolved: The paper does not investigate how the effectiveness of naturally occurring feedback varies across different domains or tasks.
- What evidence would resolve it: Evaluating the performance of models trained on naturally occurring feedback across a diverse set of domains or tasks, and analyzing the factors that contribute to domain-specific variations in performance.

### Open Question 5
- Question: How can the feedback extraction method be extended to handle implicit feedback signals in conversations?
- Basis in paper: [explicit] The authors focus on explicit feedback cues and mention that implicit feedback forms are more diverse and complex.
- Why unresolved: The paper does not explore methods for extracting and leveraging implicit feedback signals in conversations.
- What evidence would resolve it: Developing and evaluating methods for identifying and extracting implicit feedback signals from conversations, and assessing the impact of incorporating such signals on model performance.

## Limitations
- The extraction pipeline achieves only moderate precision (0.43) and recall (0.58), indicating significant room for improvement
- Results are based primarily on the LMSYS-Chat-1M dataset, which may not generalize to all conversation domains
- The study focuses on English conversations, and feedback patterns may differ across languages and cultural contexts

## Confidence

High confidence in the feedback extraction methodology and its feasibility - the automated extraction process successfully identifies hundreds of thousands of feedback instances with reasonable accuracy. Medium confidence in the alignment improvement claims - while the human evaluation results show consistent improvement, the preference training results show more mixed outcomes with only moderate wins over baselines. Low confidence in the generalizability across different conversation types and languages, as the study is limited to a specific English-language dataset.

## Next Checks

1. **Cross-dataset validation**: Test the extraction pipeline on diverse conversation datasets (customer service, technical support, educational tutoring) to verify that feedback patterns are consistent across domains and not specific to the LMSYS dataset characteristics.

2. **Ablation study on feedback categories**: Systematically remove each feedback category from training to determine which types contribute most to alignment improvements, helping optimize the extraction process and identify potentially noisy categories.

3. **Long-term stability evaluation**: Monitor model performance over extended periods and across multiple conversation turns to ensure that training on extracted feedback doesn't introduce instability or degradation in response quality for complex, multi-turn interactions.