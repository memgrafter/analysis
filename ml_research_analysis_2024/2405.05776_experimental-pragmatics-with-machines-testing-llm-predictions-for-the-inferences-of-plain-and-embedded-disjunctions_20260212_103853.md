---
ver: rpa2
title: 'Experimental Pragmatics with Machines: Testing LLM Predictions for the Inferences
  of Plain and Embedded Disjunctions'
arxiv_id: '2405.05776'
source_url: https://arxiv.org/abs/2405.05776
tags:
- inferences
- human
- ball
- llms
- were
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "The paper evaluates how well large language models (LLMs) predict\
  \ linguistic inferences for plain and embedded disjunctions, comparing their predictions\
  \ to human data from three recent studies. The authors use the mystery box experimental\
  \ paradigm to test LLM predictions for three types of inferences\u2014FREE CHOICE,\
  \ IGNORANCE, and DISTRIBUTIVE\u2014against scalar implicatures."
---

# Experimental Pragmatics with Machines: Testing LLM Predictions for the Inferences of Plain and Embedded Disjunctions

## Quick Facts
- arXiv ID: 2405.05776
- Source URL: https://arxiv.org/abs/2405.05776
- Reference count: 11
- Key outcome: GPT-3.5 and Llama-2-70b best align with human data for disjunction inferences, with GPT-3.5 achieving highest adjusted R² (0.68) across conditions

## Executive Summary
This paper evaluates how well large language models predict linguistic inferences for plain and embedded disjunctions compared to human data from three recent studies. Using the mystery box experimental paradigm, the authors test LLM predictions for three inference types—FREE CHOICE, IGNORANCE, and DISTRIBUTIVE—against scalar implicatures. Five LLMs are evaluated using control accuracy and acceptance rate predictions, with GPT-3.5 showing the strongest alignment with human data. Results reveal that while LLMs can partially replicate human inference patterns, they exhibit notable inconsistencies across inference types and contexts, particularly in modal contexts and for FREE CHOICE inferences.

## Method Summary
The study converts human experimental vignettes into text-based prompts featuring four boxes (three visible with colored balls, one mystery box with question mark) and trigger sentences about mystery box contents. Five LLMs (GPT-3.5, Llama-2-70b, Mistral-Instruct, Mixtral, and Mixtral-Instruct) are evaluated using few-shot prompting with control trials from human training phases. Control accuracy measures correct responses on baseline trials, while acceptance rates measure "good" responses on target trials. Performance is compared to human data using R² and adjusted R² metrics to assess fit between model predictions and human inference patterns.

## Key Results
- GPT-3.5 achieved the highest adjusted R² (0.68) across all conditions, with Llama-2-70b also showing strong alignment with human data
- LLMs demonstrated differential performance across inference types, with notable discrepancies in modal contexts and for FREE CHOICE inferences
- Performance varies across triggers, with LLMs less consistent for FREE CHOICE and DISTRIBUTIVE conditions compared to other inference types
- Control accuracy provides clear baselines, with some models showing strong task understanding while others exhibit biases toward particular response options

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLMs can partially replicate human inference patterns for disjunction inferences when tested with the mystery box paradigm
- Mechanism: LLMs learn statistical associations between linguistic forms (e.g., "or", modal verbs) and pragmatic interpretations through training on large text corpora
- Core assumption: Statistical patterns captured in LLM training data sufficiently represent linguistic phenomena including contextual dependencies
- Evidence anchors:
  - [abstract]: "The results of our best performing models mostly align with those of humans, both in the large differences we find between those inferences and implicatures, as well as in fine-grained distinctions among different aspects of those inferences"
  - [section]: "GPT-3.5 and Llama-2-70b best align with human data, with GPT-3.5 achieving the highest adjusted R² (0.68) across all conditions"
  - [corpus]: Weak evidence - only 5/25 corpus neighbors are directly related to implicature or pragmatics

### Mechanism 2
- Claim: LLMs show differential performance across inference types and contexts, revealing limitations in fully capturing human pragmatic reasoning
- Mechanism: Architecture and training result in varying sensitivity to different linguistic phenomena
- Core assumption: Performance differences reflect genuine limitations rather than experimental artifacts
- Evidence anchors:
  - [abstract]: "However, performance varies across inference types and contexts, with notable discrepancies in modal contexts and for FREE CHOICE inferences"
  - [section]: "LLMs were less consistent across triggers with respect to human data in the FC and DI conditions, than in the other two conditions"
  - [corpus]: Weak evidence - corpus neighbors don't address performance differences across inference types

### Mechanism 3
- Claim: Control trials and few-shot prompting provide baseline for assessing model performance and generalization
- Mechanism: Control conditions isolate model's ability to make pragmatic inferences from general language understanding
- Core assumption: Control conditions and few-shot examples are representative and unbiased
- Evidence anchors:
  - [abstract]: "Control contexts served to provide clear baselines for acceptance and rejection of the target sentences under investigation"
  - [section]: "Human subjects completed training trials where they learned the experimental task by seeing control conditions and receiving feedback about correctness of their responses"
  - [corpus]: Weak evidence - corpus neighbors don't discuss control trials or few-shot prompting

## Foundational Learning

- Concept: Scalar implicatures and their distinction from other types of pragmatic inferences
  - Why needed here: Understanding theoretical framework is crucial for interpreting results and implications for linguistic theories
  - Quick check question: What is the key difference between scalar implicatures and free choice inferences, and how do they arise from the same linguistic triggers?

- Concept: Experimental pragmatics methodology, including the mystery box paradigm and its application to testing linguistic inferences
  - Why needed here: Familiarity with experimental design is essential for understanding how LLM predictions are evaluated
  - Quick check question: How does the mystery box paradigm control for context and allow researchers to test specific inferences independently?

- Concept: Large language model evaluation techniques, including few-shot prompting and control condition analysis
  - Why needed here: Understanding evaluation methodology is crucial for assessing validity and limitations of LLM predictions
  - Quick check question: How do control conditions and few-shot prompting contribute to more accurate evaluation of LLM performance on pragmatic inference tasks?

## Architecture Onboarding

- Component map: Experimental materials (converted vignettes) -> LLM models (5 total) -> Evaluation metrics (control accuracy, acceptance rates, R²) -> Human data comparison
- Critical path: Convert human study materials to LLM prompts → Run prompts through selected LLM models → Compute control accuracy and acceptance rates → Compare model predictions to human data using R² metrics
- Design tradeoffs: Few-shot prompting with control conditions provides accurate evaluation but may introduce bias; selecting only top-performing models may overlook insights from lower-performing models
- Failure signatures: Inconsistent performance across inference types and contexts; lower performance in modal contexts; inability to distinguish between different types of inferences
- First 3 experiments:
  1. Run all LLM models on control conditions to establish baseline accuracy and identify top-performing models
  2. Evaluate top-performing models on critical trials for each inference type and compare acceptance rates to human data
  3. Analyze performance differences across inference types and contexts, focusing on modal contexts and FREE CHOICE inferences

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Do inconsistencies across different LLM models in predicting pragmatic inferences reflect fundamental limitations in their training data or architectures, or are they artifacts of experimental design and prompting strategies?
- Basis in paper: [explicit] The paper notes that different LLMs exhibit noticeable inconsistencies across conditions in non-human-like ways, and suggests that future experiments should test the robustness of predictions under different prompting strategies
- Why unresolved: Current study uses specific prompting approach and limited number of models
- What evidence would resolve it: Systematic comparison of LLM performance across multiple prompting strategies, architectures, and training datasets

### Open Question 2
- Question: How do predictions of pragmatic inferences by LLMs scale with model capacity and training data size, and how does this scaling compare to human acquisition trajectories?
- Basis in paper: [explicit] The paper suggests that performance of LLMs of different capacity should be compared to human acquisition trajectories, and notes that model size appears to be a predictor of fit to human data
- Why unresolved: Current study only tests limited range of model sizes
- What evidence would resolve it: Systematic testing of LLMs across wider range of model sizes with direct comparison to developmental studies

### Open Question 3
- Question: What specific linguistic features or patterns in training data enable certain LLMs to predict pragmatic inferences more accurately than others, and can these features be systematically identified and incorporated into future model development?
- Basis in paper: [explicit] The paper notes that different LLMs exhibit varying performance on pragmatic inference tasks
- Why unresolved: Current study doesn't analyze specific linguistic features contributing to performance differences
- What evidence would resolve it: Detailed analysis of training data and linguistic features associated with high-performing LLMs on pragmatic inference tasks

## Limitations

- Limited generalizability beyond mystery box paradigm and specific inference types tested
- Potential confounding from few-shot prompting methodology affecting LLM responses
- Five LLM models represent a small sample of available models, potentially missing relevant performance patterns
- Confidence in mechanistic explanations is constrained by weak external validation from related literature

## Confidence

- **High confidence**: Core finding that LLMs can partially replicate human inference patterns, supported by multiple R² metrics showing good alignment with human data
- **Medium confidence**: Claims about differential performance across inference types and contexts, limited by small sample size and potential methodological artifacts
- **Low confidence**: Specific mechanistic explanations for performance differences, as corpus evidence provides minimal external validation

## Next Checks

1. **Replication with alternative paradigms**: Test LLM predictions using different experimental paradigms (e.g., truth-value judgment tasks) to verify that mystery box-specific effects aren't driving the results
2. **Control condition ablation study**: Systematically remove or vary few-shot examples and control trials to isolate their impact on LLM inference predictions
3. **Broader model comparison**: Evaluate performance across 10+ additional LLM architectures spanning different training regimes to establish whether observed patterns generalize beyond current sample