---
ver: rpa2
title: Harnessing Causality in Reinforcement Learning With Bagged Decision Times
arxiv_id: '2410.14659'
source_url: https://arxiv.org/abs/2410.14659
tags:
- decision
- times
- state
- policy
- time
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper develops a reinforcement learning (RL) algorithm for
  optimizing policies with bagged decision times, where multiple sequential actions
  within a fixed time window jointly affect a delayed reward. Leveraging a causal
  directed acyclic graph (DAG), the authors construct states as a dynamical Bayesian
  sufficient statistic to achieve Markovian transitions within and across bags, and
  model the problem as a periodic Markov decision process (MDP) with time-varying
  dynamics.
---

# Harnessing Causality in Reinforcement Learning With Bagged Decision Times

## Quick Facts
- arXiv ID: 2410.14659
- Source URL: https://arxiv.org/abs/2410.14659
- Authors: Daiqi Gao; Hsin-Yu Lai; Predrag Klasnja; Susan A. Murphy
- Reference count: 40
- Primary result: An RL algorithm that leverages causal DAGs to construct sufficient states for bagged decision times, achieving superior performance over baselines in HeartSteps mHealth trial data

## Executive Summary
This paper develops a reinforcement learning algorithm for optimizing policies with bagged decision times, where multiple sequential actions within a fixed time window jointly affect a delayed reward. By leveraging a causal directed acyclic graph (DAG), the authors construct states as a dynamical Bayesian sufficient statistic to achieve Markovian transitions within and across bags, and model the problem as a periodic Markov decision process (MDP) with time-varying dynamics. The proposed Bagged RLSVI (BRLSVI) algorithm is shown to outperform baseline methods, particularly when negative action effects are strong, and demonstrates robustness to misspecified DAG assumptions.

## Method Summary
The method constructs states as dynamical Bayesian sufficient statistics based on a causal DAG, transforming a non-Markovian process into a Markovian one. The problem is formulated as a K-periodic MDP allowing non-stationarity within periods. The algorithm generalizes Bellman optimality equations for stationary MDPs to handle the periodic structure and implements an online RL approach using Bayesian linear regression. The state construction ensures sufficiency while maintaining Markovian properties, and theoretical analysis shows the constructed state achieves maximal optimal value among all possible state constructions.

## Key Results
- BRLSVI outperforms baseline methods (Stationary RLSVI, RLSVI with horizon K, Random policy, Thompson sampling) in HeartSteps testbed variants
- Performance advantage is particularly pronounced when negative action effects are strong
- The algorithm demonstrates robustness to misspecified DAG assumptions
- The constructed state achieves maximal optimal value function among all state constructions for periodic MDPs

## Why This Works (Mechanism)

### Mechanism 1
The causal DAG allows construction of a dynamical Bayesian sufficient statistic (D-BaSS) that transforms a non-Markovian process into a Markovian one within and across bags. By identifying d-separators in the DAG, the algorithm constructs states that block all paths from history to future rewards except through the state itself, ensuring sufficiency and Markovian transitions. Core assumption: The DAG is correctly specified and contains sufficient causal information to identify appropriate d-separators.

### Mechanism 2
Reformulating the problem as a periodic MDP allows generalization of stationary MDP algorithms to handle time-varying dynamics within bags. By treating each bag as a period with K decision times, the algorithm can apply Bellman optimality equations adapted for periodic MDPs, handling non-stationary transitions and rewards within each period. Core assumption: The state transitions and rewards are stationary across bags but can vary within each bag.

### Mechanism 3
The minimal D-BaSS achieves the maximal optimal value function among all possible state constructions. The minimal D-BaSS contains only the necessary variables to ensure sufficiency, reducing dimensionality while preserving the optimal value function. This improves sample efficiency compared to larger state representations. Core assumption: The minimal D-BaSS is correctly identified and is indeed minimal.

## Foundational Learning

- Concept: Causal DAG and d-separation
  - Why needed here: Understanding d-separation is crucial for identifying which variables block paths between history and future rewards, enabling construction of sufficient states.
  - Quick check question: Can you explain why conditioning on eBd d-separates Bd+1 from B0:d in the context of the DAG?

- Concept: Markov decision processes and Bellman equations
  - Why needed here: The periodic MDP formulation relies on Bellman optimality equations, so understanding MDP fundamentals is essential.
  - Quick check question: How does the Bellman equation change when moving from stationary to periodic MDPs?

- Concept: State construction and sufficiency
  - Why needed here: The algorithm's core innovation is constructing states that are sufficient statistics for future rewards while maintaining Markovian properties.
  - Quick check question: What is the difference between a regular sufficient statistic and a dynamical Bayesian sufficient statistic in this context?

## Architecture Onboarding

- Component map: DAG specification -> State construction module -> Periodic MDP formulation module -> Bellman equation generalization module -> Online RL algorithm
- Critical path: DAG → State Construction → MDP Formulation → Bellman Generalization → Learning Algorithm
- Design tradeoffs: State dimensionality vs. sample efficiency; DAG accuracy vs. robustness; Period length K vs. computational complexity
- Failure signatures: Non-convergence of learning algorithm; poor performance despite correct DAG; high variance in learned policies
- First 3 experiments:
  1. Verify Markov property: Test that the constructed state transitions are Markovian by checking if future rewards are independent of past given current state.
  2. Validate state sufficiency: Confirm that the constructed state is a sufficient statistic by checking if it blocks all paths from history to future rewards.
  3. Compare state constructions: Test different state representations (e.g., with/without mediators) to empirically verify Theorem 4.4's value function guarantees.

## Open Questions the Paper Calls Out

### Open Question 1
Can the Bellman optimality equations for periodic MDPs with time-varying discount factors be further generalized to handle arbitrary reward distributions beyond bounded or sub-Gaussian assumptions? The authors mention extending Bellman optimality equations to accommodate time-varying discount factors but focus on theoretical analysis rather than algorithmic implementation.

### Open Question 2
How does the performance of the proposed algorithm scale with the number of decision times K within each bag, particularly when K becomes large? The current implementation pools data across decision times but does not systematically study the effect of increasing K on sample complexity or computational efficiency.

### Open Question 3
What are the implications of relaxing the assumption that the same set of variables is defined across all decision times when extending to Markovian bag lengths? The authors discuss extending to Markovian bag lengths using absorbing states but note this requires consistent variable definitions across decision times.

### Open Question 4
Can the model-free nature of the proposed algorithm be leveraged to provide uncertainty estimates or confidence intervals for the intervention assignment probabilities? The authors note that obtaining intervention assignment probabilities is crucial for post-study analysis but remains an open question for BRLSVI.

## Limitations

- Performance sensitivity to DAG specification accuracy, though the paper claims robustness to misspecification
- Evaluation limited to a single dataset (HeartSteps V2), raising questions about generalizability to other domains
- The testbed construction involves assumptions about variable relationships that may not perfectly reflect real-world dynamics

## Confidence

**High Confidence**: The core theoretical results regarding state construction achieving maximal optimal value (Theorem 4.4) are well-supported by mathematical proofs. The generalization of Bellman equations to periodic MDPs is also well-established.

**Medium Confidence**: The empirical evaluation demonstrates strong performance in the HeartSteps testbed, particularly when negative action effects are present. However, the testbed is a simulation based on historical data with assumed DAG structures, which may not capture all real-world complexities.

**Low Confidence**: The robustness claims to DAG misspecification are supported by limited experiments. The paper mentions testing with removed mediators but doesn't comprehensively explore the impact of various DAG errors on algorithm performance.

## Next Checks

1. **DAG Sensitivity Analysis**: Systematically vary the DAG structure by removing or adding edges and measure the impact on learned policy performance and state sufficiency. This would quantify the algorithm's actual robustness to specification errors.

2. **Cross-Domain Testing**: Apply the algorithm to a different domain with bagged decision times (e.g., education technology, energy management) to assess generalizability beyond the mHealth context.

3. **Theoretical Bounds on Misspecification**: Develop bounds on how DAG errors affect the gap between the learned policy's value and the optimal policy's value, providing theoretical guarantees for the robustness claims.