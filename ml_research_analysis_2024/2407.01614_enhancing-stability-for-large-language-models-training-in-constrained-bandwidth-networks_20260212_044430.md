---
ver: rpa2
title: Enhancing Stability for Large Language Models Training in Constrained Bandwidth
  Networks
arxiv_id: '2407.01614'
source_url: https://arxiv.org/abs/2407.01614
tags:
- training
- zero
- communication
- allgather
- algorithm
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work addresses instability issues in training large language
  models (LLMs) using ZeRO++'s hierarchical partitioning (hpZ) scheme on low-bandwidth
  networks. The instability stems from a race condition between asynchronous parameter
  partitioning and collective communication operations.
---

# Enhancing Stability for Large Language Models Training in Constrained Bandwidth Networks

## Quick Facts
- arXiv ID: 2407.01614
- Source URL: https://arxiv.org/abs/2407.01614
- Authors: Yun Dai; Tejas Dharamsi; Byron Hsu; Tao Song; Hamed Firooz
- Reference count: 9
- Primary result: Simple CUDA synchronization fix restores reliable convergence for ZeRO++ hpZ training on low-bandwidth networks

## Executive Summary
This paper addresses instability issues in training large language models (LLMs) using ZeRO++'s hierarchical partitioning (hpZ) scheme on low-bandwidth networks. The instability stems from a race condition between asynchronous parameter partitioning and collective communication operations. The authors propose a simple yet effective fix: inserting explicit CUDA synchronization points to ensure parameter partitioning completes before collective communication begins. This modification restores reliable convergence for training massive models like Falcon (40B) and Llama-2 (70B) while maintaining high throughput—achieving up to 98% improvement compared to non-hpZ configurations and showing no degradation in convergence quality.

## Method Summary
The authors identify a race condition in ZeRO++'s hierarchical partitioning scheme where asynchronous device-to-device memory copies may not complete before AllGather operations begin, causing corrupted parameters and training instability. They implement explicit CUDA synchronization points between the D2D memcpy operations and AllGather calls in the backward pass. This ensures the secondary parameter copy is fully initialized before collective communication begins. The modification is tested on 8xA100 GPU nodes with constrained 1×12.5Gbps Ethernet bandwidth, training Falcon-40B and Llama-2 models (7B, 13B, 70B) on the MMLU dataset.

## Key Results
- Inserting CUDA synchronization points between D2D memcpy and AllGather eliminates training instability (NaN values and loss divergence)
- The modified hpZ scheme achieves up to 98% improvement in throughput compared to non-hpZ configurations
- Convergence quality is preserved—validation loss convergence per optimization step remains unchanged
- Stable training is demonstrated for multi-billion parameter models including Falcon-40B and Llama-2-70B

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Explicit CUDA synchronization between D2D memcpy and AllGather eliminates the race condition causing training instability.
- Mechanism: The asynchronous nature of device-to-device memory copy operations means the secondary copy tensor may not be fully initialized when the AllGather kernel launches. Adding a synchronization point ensures the copy completes before aggregation begins, preventing corrupted parameter values from being communicated.
- Core assumption: The race condition between MemcpyD2D and AllGather is the primary cause of the observed NaN values and training divergence.
- Evidence anchors:
  - [abstract] "We then propose a modification to the partitioning algorithm that addresses these convergence challenges while maintaining competitive training efficiency."
  - [section] "To avoid such a race condition, we add a CUDA synchronization operator between D2D Memcpy and AllGather."
  - [corpus] Weak evidence - corpus papers focus on federated learning and bandwidth optimization rather than CUDA-level synchronization issues.
- Break condition: If the synchronization introduces unacceptable latency that negates the throughput gains from hpZ, or if the race condition manifests differently than assumed.

### Mechanism 2
- Claim: The hpZ scheme's secondary copy replication within nodes eliminates inter-node communication overhead during the backward pass.
- Mechanism: By partitioning weights into a secondary copy replicated on each node after the forward pass, AllGather operations during the backward pass only need to communicate within nodes rather than across the entire cluster. This leverages the higher intra-node bandwidth to reduce overall communication costs.
- Core assumption: Intra-node communication bandwidth is sufficiently higher than inter-node bandwidth to justify the memory overhead of the secondary copy.
- Evidence anchors:
  - [abstract] "Empirical evaluation on training the multi-billion parameters Falcon Models and Llama-2 models demonstrates the updated algorithm's ability to achieve reliable convergence on these massive models, where stock ZeRO++ hpZ fails to converge."
  - [section] "AllGather in the backward pass thus operates on the secondary copy and will only involve intra-node communication, which has multiple factors higher bandwidth than inter-node."
  - [corpus] Weak evidence - corpus papers discuss bandwidth optimization but not specifically the ZeRO++ hpZ scheme or its trade-offs.
- Break condition: If the memory overhead of maintaining the secondary copy becomes prohibitive for even larger models, or if the intra-node vs inter-node bandwidth ratio changes significantly.

### Mechanism 3
- Claim: The quantization schemes (qgZ and qwZ) further reduce communication volume by compressing gradients and weights before collective operations.
- Mechanism: By quantizing gradients before ReduceScatter and weights before AllGather, the amount of data that needs to be communicated is significantly reduced. This complements the hpZ scheme by minimizing the communication volume for both inter-node and intra-node operations.
- Core assumption: The performance loss from quantization is negligible compared to the gains from reduced communication volume.
- Evidence anchors:
  - [abstract] "ZeRO++ algorithm (Wang et al., 2023) introduced a set of efficient parallelization strategy leveraging Quantized Weight Communication for ZeRO (qwZ), Quantized Gradient Communication for ZeRO (qgZ), Hierarchical Weight Partition for ZeRO (hpZ) hierarchical partitioning to minimize cross device communication volume."
  - [section] "The qgZ scheme quantizes gradients before ReduceScatter operations, and the qwZ scheme quantizes weights before AllGather operations, both of which further optimize communication efficiency."
  - [corpus] Weak evidence - corpus papers discuss bandwidth optimization but not specifically the ZeRO++ quantization schemes or their trade-offs.
- Break condition: If the quantization introduces numerical instability or if the performance loss from decompression outweighs the communication gains.

## Foundational Learning

- Concept: CUDA memory operations and synchronization primitives
  - Why needed here: Understanding how asynchronous device-to-device memory copies work and how synchronization points can be inserted is crucial for grasping why the race condition occurs and how the fix addresses it.
  - Quick check question: What is the difference between a synchronous and asynchronous CUDA memory copy operation, and how does this relate to the race condition in ZeRO++ hpZ?

- Concept: Collective communication patterns in distributed training
  - Why needed here: The paper relies heavily on AllGather and ReduceScatter operations. Understanding how these work, their bandwidth requirements, and their role in data-parallel training is essential for understanding the communication bottlenecks the hpZ scheme addresses.
  - Quick check question: How do AllGather and ReduceScatter operations differ in terms of communication patterns and bandwidth requirements, and why does this matter for distributed training on constrained networks?

- Concept: Memory partitioning strategies for large models
  - Why needed here: The ZeRO++ hpZ scheme is a specific type of memory partitioning strategy. Understanding the general principles of how model parameters, gradients, and optimizer states can be partitioned across devices is important for appreciating the innovation and trade-offs of this approach.
  - Quick check question: What are the key differences between data parallelism, model parallelism, and ZeRO-style partitioning, and how do these strategies impact memory usage and communication patterns?

## Architecture Onboarding

- Component map: Forward pass with parameter sharding -> Backward pass with secondary copy AllGather and gradient ReduceScatter -> Optimizer step with gradient aggregation -> CUDA synchronization points
- Critical path: Forward pass with parameter sharding -> Backward pass with secondary copy AllGather and gradient ReduceScatter -> Optimizer step with gradient aggregation
- Design tradeoffs: Memory vs. communication: The secondary copy in hpZ increases memory usage but reduces communication volume. Precision vs. bandwidth: Quantization reduces bandwidth requirements but may introduce numerical errors. Synchronization vs. throughput: Adding CUDA sync points prevents race conditions but may introduce latency.
- Failure signatures: NaN values in parameter tensors during training, loss divergence or failure to decrease over training steps, inconsistent training results across different runs with the same hyperparameters
- First 3 experiments: 1) Reproduce the instability with stock ZeRO++ hpZ on a smaller model (e.g., Llama-2-7B) to verify the race condition. 2) Implement the CUDA synchronization fix and verify that it resolves the instability while measuring the impact on throughput. 3) Compare the performance of the modified hpZ with and without quantization (qgZ/qwZ) to understand the trade-offs between memory usage, communication volume, and training speed.

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions, but based on the evaluation scope, several questions remain unexplored regarding the generalizability and theoretical foundations of the approach.

## Limitations
- The synchronization fix implementation details are not fully specified, making independent verification challenging
- Performance claims are based on synthetic bandwidth constraints rather than real-world network measurements
- The evaluation is limited to transformer-based models, leaving generalization to other architectures unproven

## Confidence
- **High confidence**: The core mechanism identifying the race condition between asynchronous D2D memcpy and AllGather operations - this is a well-understood issue in CUDA programming and the symptoms (NaN values and training divergence) are consistent with corrupted memory access patterns.
- **Medium confidence**: The throughput improvement claims (up to 98%) - while the mechanism is sound, the exact performance gains depend heavily on implementation details and network conditions that are not fully specified.
- **Medium confidence**: The convergence quality preservation - the paper demonstrates successful training of large models but doesn't provide extensive ablation studies or comparisons with alternative synchronization strategies.

## Next Checks
1. **Implementation verification**: Create a minimal reproduction of the race condition using the standard DeepSpeed ZeRO++ implementation on 8xA100 GPUs, then systematically insert CUDA synchronization points at different locations to identify the exact placement that resolves the instability while preserving throughput.

2. **Network condition validation**: Test the modified hpZ implementation across a spectrum of network conditions (from 1Gbps to 100Gbps) to determine the bandwidth threshold below which the synchronization becomes necessary, and measure the performance impact at different network speeds.

3. **Memory overhead characterization**: Quantify the memory overhead of the secondary copy mechanism in hpZ across different model sizes (7B, 13B, 30B, 65B, 70B) and evaluate at what model scale the memory costs begin to outweigh the communication benefits, particularly when combined with quantization schemes.