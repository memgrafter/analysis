---
ver: rpa2
title: Can Language Models Evaluate Human Written Text? Case Study on Korean Student
  Writing for Education
arxiv_id: '2407.17022'
source_url: https://arxiv.org/abs/2407.17022
tags:
- writing
- text
- arxiv
- evaluation
- students
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper investigates whether large language models (LLMs) can
  reliably evaluate human-written text for educational purposes. The authors collected
  100 texts from 32 Korean students across 15 types of writing and used GPT-4-Turbo
  to evaluate them on five criteria: grammaticality, fluency, coherence, consistency,
  and relevance.'
---

# Can Language Models Evaluate Human Written Text? Case Study on Korean Student Writing for Education

## Quick Facts
- arXiv ID: 2407.17022
- Source URL: https://arxiv.org/abs/2407.17022
- Reference count: 24
- Primary result: LLM evaluators can reliably assess grammaticality and fluency in student writing (87% and 93% agreement with human judgments)

## Executive Summary
This study investigates whether large language models can reliably evaluate human-written text for educational purposes, focusing on Korean student writing across 15 different types. Using GPT-4-Turbo to evaluate 100 texts from 32 students on five criteria, the authors found that LLM evaluators perform well on objective criteria like grammaticality and fluency but struggle with more subjective aspects and writing types. The study demonstrates that LLM evaluators can discriminate between writings from younger and older students, suggesting potential educational applications. The dataset and feedback are publicly released for future research.

## Method Summary
The researchers collected 100 texts from 32 Korean students (ages 11-19) across 15 writing types, including essays, reports, scripts, and diaries. GPT-4-Turbo was used to evaluate these texts based on five criteria: grammaticality, fluency, coherence, consistency, and relevance. Students were then asked to verify the LLM judgments, categorizing them as reasonable, overly critical, or overly optimistic. The study analyzed agreement rates between LLM and human judgments across different writing types and student age groups.

## Key Results
- LLM evaluators achieved 87% agreement with human judgments on grammaticality and 93% on fluency
- Performance was higher for objective writing types (process essays, descriptive essays, scientific reports) than subjective ones (diaries, self-introductions)
- LLM evaluators could discriminate between writings from younger (11-13) and older (17-19) students, with older students receiving higher scores

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLM evaluators can reliably assess grammaticality and fluency in student writing.
- Mechanism: LLMs detect structural patterns and linguistic anomalies in text, matching human judgment when criteria are objective.
- Core assumption: Grammaticality and fluency are more objective criteria than coherence or consistency.
- Evidence anchors:
  - [abstract] "Our analyses indicate that LLM evaluators can reliably assess grammaticality and fluency (87% and 93% agreement with human judgments)"
  - [section] "the participants we tested are all Korean students who do not use English as their first language. Also, for more objective evaluation criteria such as grammaticality, the validity was high (87%)."
  - [corpus] Weak - no direct corpus evidence for grammaticality/fluency patterns.
- Break condition: When text contains idiomatic or culturally-specific expressions that deviate from standard grammatical patterns.

### Mechanism 2
- Claim: LLM evaluators perform better on objective writing types than subjective ones.
- Mechanism: Objective writing types (e.g., scientific reports) have clearer structure and content requirements, making evaluation more deterministic.
- Core assumption: Subjective writing types (e.g., diaries, self-introductions) have variable structure and personal expression that are harder to evaluate consistently.
- Evidence anchors:
  - [abstract] "they struggle with other criteria and more subjective types of writing"
  - [section] "The validity ratio tends to be higher for texts with more objective characteristics, including process essays, descriptive essays, and scientific reports, while it is lower for self-introduction essays, argumentative essays, and diaries."
  - [corpus] Weak - no direct corpus evidence for subjective vs objective writing patterns.
- Break condition: When subjective writing contains highly structured elements (e.g., diary with specific formatting requirements).

### Mechanism 3
- Claim: LLM evaluators can discriminate between writings from younger and older students.
- Mechanism: LLMs detect linguistic maturity markers (vocabulary complexity, sentence structure, coherence) that correlate with age/education level.
- Core assumption: Older students produce more linguistically sophisticated writing than younger students.
- Evidence anchors:
  - [abstract] "The study also found that LLM evaluators can discriminate between writings from younger and older students, suggesting potential for educational applications."
  - [section] "when comparing the average scores between texts written by 11-13-year-olds and those by 17-19-year-olds, we found that the latter tended to receive higher scores."
  - [corpus] Weak - no direct corpus evidence for age-related linguistic patterns.
- Break condition: When younger students produce unusually sophisticated writing or older students produce simpler text.

## Foundational Learning

- Concept: Human evaluation reliability
  - Why needed here: Understanding how human judgment serves as ground truth for LLM evaluation
  - Quick check question: If 87% of human evaluators agree with LLM on grammaticality, what does this tell us about the reliability of both evaluation methods?

- Concept: Objective vs subjective criteria in writing assessment
  - Why needed here: Different evaluation criteria have varying degrees of subjectivity, affecting LLM performance
  - Quick check question: Why might coherence be harder for LLMs to evaluate than grammaticality?

- Concept: Corpus construction and annotation
  - Why needed here: Understanding how the dataset was collected and labeled affects interpretation of results
  - Quick check question: What potential biases might exist in a corpus collected from 32 Korean students across different age groups?

## Architecture Onboarding

- Component map:
  Data collection -> Text categorization -> GPT-4-Turbo evaluation -> Human validation -> Analysis

- Critical path:
  1. Collect student writing samples
  2. Process and categorize texts
  3. Run LLM evaluation with Prometheus pipeline
  4. Collect human validation feedback
  5. Analyze results and identify patterns

- Design tradeoffs:
  - Using GPT-4-Turbo provides state-of-the-art evaluation but increases costs
  - Limited sample size (100 texts) provides preliminary insights but may not be statistically robust
  - Korean language focus limits generalizability to other languages

- Failure signatures:
  - Low validity ratios for certain writing types indicate LLM limitations
  - Inconsistent scoring across similar texts suggests evaluation instability
  - High disagreement with human judgments on subjective criteria

- First 3 experiments:
  1. Test LLM evaluation on a subset of objective writing types to establish baseline performance
  2. Compare LLM judgments with human evaluators on the same texts to measure agreement
  3. Analyze scoring patterns for different age groups to verify discrimination capability

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of LLM-as-a-Judge vary when evaluating writing from different age groups on the same topic?
- Basis in paper: [explicit] The paper found that GPT-4 gave higher scores to writings from older students (17-19) compared to younger students (11-13) across all criteria, but notes this wasn't a controlled comparison since writings weren't on the same topic.
- Why unresolved: The study didn't compare writings on identical topics across age groups, so it's unclear if the scoring differences are due to actual quality differences or topic selection biases.
- What evidence would resolve it: A controlled study where students of different ages write on the same topics, with both LLM and human evaluations of the same texts.

### Open Question 2
- Question: Can LLM-as-a-Judge effectively evaluate highly subjective or creative writing genres beyond the ones tested?
- Basis in paper: [explicit] The paper found that LLM-as-a-Judge struggled with subjective writing types like diaries and self-introduction essays, but only tested 15 specific writing types.
- Why unresolved: The study only examined 15 writing types, and there are many other creative or subjective genres that weren't tested.
- What evidence would resolve it: Testing LLM-as-a-Judge on a broader range of creative writing genres (poetry, stream of consciousness, experimental prose, etc.) with both LLM and human evaluations.

### Open Question 3
- Question: How would incorporating student feedback into the evaluation process affect the reliability and usefulness of LLM-as-a-Judge for educational purposes?
- Basis in paper: [explicit] The paper asked students to verify if the LLM's judgments were reasonable, overly critical, or overly optimistic, but didn't explore how this feedback could be used to improve the evaluation process.
- Why unresolved: The study collected student feedback but didn't implement a system for incorporating it into the evaluation process.
- What evidence would resolve it: A study where student feedback is used to iteratively improve the LLM's evaluation criteria and prompts, followed by comparison of pre- and post-feedback evaluation reliability.

## Limitations
- Sample size of 100 texts from 32 students limits statistical generalizability
- Korean language focus restricts broader applicability to other languages
- Human validation relied on students verifying their own work, potentially introducing bias

## Confidence

- High confidence: LLM performance on grammaticality and fluency assessment (87% and 93% agreement rates are well-supported)
- Medium confidence: Age discrimination capability (observed patterns need larger sample validation)
- Medium confidence: Subjective vs objective writing type performance (sample size limitations affect robustness)

## Next Checks

1. Replicate the study with a larger, more diverse corpus (minimum 500 texts across multiple language backgrounds) to test generalizability of the observed patterns.
2. Conduct blind human validation where independent evaluators assess both student texts and LLM judgments without knowing the source, to eliminate self-verification bias.
3. Test the evaluation pipeline across different LLM architectures (not just GPT-4-Turbo) to determine if performance patterns hold across model families and parameter scales.