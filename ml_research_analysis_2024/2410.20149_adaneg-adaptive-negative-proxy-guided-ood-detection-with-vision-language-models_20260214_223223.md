---
ver: rpa2
title: 'AdaNeg: Adaptive Negative Proxy Guided OOD Detection with Vision-Language
  Models'
arxiv_id: '2410.20149'
source_url: https://arxiv.org/abs/2410.20149
tags:
- detection
- proxies
- negative
- test
- samples
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proposes AdaNeg, an adaptive negative proxy method for
  out-of-distribution (OOD) detection using vision-language models (VLMs). The key
  problem is that static negative labels used in prior work (e.g., NegLabel) can misalign
  with actual OOD distributions, reducing detection performance.
---

# AdaNeg: Adaptive Negative Proxy Guided OOD Detection with Vision-Language Models

## Quick Facts
- **arXiv ID**: 2410.20149
- **Source URL**: https://arxiv.org/abs/2410.20149
- **Reference count**: 40
- **Key result**: Achieves 2.45% increase in AUROC and 6.48% reduction in FPR95 on ImageNet OOD detection

## Executive Summary
AdaNeg addresses a fundamental limitation in vision-language model (VLM) based out-of-distribution (OOD) detection: static negative labels fail to align with actual OOD distributions. The method introduces task-adaptive and sample-adaptive negative proxies that dynamically capture OOD characteristics during testing. By caching discriminative OOD features in a memory bank and using attention mechanisms, AdaNeg generates proxies that better represent the true OOD distribution compared to static approaches.

## Method Summary
AdaNeg is a training-free, annotation-free approach that improves VLM-based OOD detection by replacing static negative labels with adaptive proxies. The method builds a feature memory bank during testing, caching discriminative OOD features from test images using an adaptive threshold. Task-adaptive proxies are computed by averaging features per category, while sample-adaptive proxies use attention mechanisms to capture sample-level nuances. The final OOD score combines these adaptive proxies with static negative labels, integrating both textual and visual knowledge for improved detection performance.

## Key Results
- Achieves 2.45% increase in AUROC compared to baseline NegLabel method
- Reduces FPR95 by 6.48% on ImageNet OOD detection tasks
- Maintains fast testing speed while being training-free and annotation-free

## Why This Works (Mechanism)
The method works by addressing the misalignment between static negative labels and actual OOD distributions. Static labels (like NegLabel) use a fixed set of negative samples that may not represent the true OOD characteristics encountered during testing. AdaNeg's adaptive proxies dynamically capture OOD features as they appear in test data, creating more representative negative samples. The combination of task-adaptive (category-level) and sample-adaptive (individual-level) proxies ensures both broad coverage and fine-grained discrimination, while the memory bank mechanism enables efficient feature caching and retrieval during inference.

## Foundational Learning

**VLM-based OOD Detection**: Uses vision-language models (like CLIP) for OOD detection by leveraging semantic understanding. *Why needed*: Traditional image-only models lack semantic context for distinguishing subtle OOD cases. *Quick check*: Verify the pre-trained CLIP model can generate meaningful text-image embeddings.

**Negative Proxy Generation**: Creates representative negative samples dynamically rather than using fixed static labels. *Why needed*: Static labels often misalign with actual OOD distributions encountered during testing. *Quick check*: Ensure cached features in memory bank show clear separation from in-distribution samples.

**Feature Memory Bank**: Caches discriminative OOD features during testing for proxy generation. *Why needed*: Enables dynamic adaptation to test-time OOD characteristics without retraining. *Quick check*: Monitor memory bank size and feature quality during initial test batches.

## Architecture Onboarding

**Component Map**: CLIP model → Feature extraction → Memory bank → Task-adaptive proxies → Sample-adaptive proxies → OOD scoring

**Critical Path**: Feature extraction → Memory bank caching → Proxy generation → OOD score computation

**Design Tradeoffs**: Training-free approach sacrifices potential performance gains from fine-tuning but gains deployment flexibility. Dynamic proxy generation adds computational overhead but improves detection accuracy.

**Failure Signatures**: 
- Memory bank fills with non-discriminative features → poor proxy quality
- Adaptive threshold too conservative → insufficient OOD samples cached
- Proxy generation fails to capture semantic nuances → reduced detection performance

**First Experiments**:
1. Test basic feature extraction and memory bank initialization with sample test images
2. Verify task-adaptive proxy generation by computing category-level feature averages
3. Validate sample-adaptive proxy computation using attention mechanism on cached features

## Open Questions the Paper Calls Out
None identified in the provided content.

## Limitations
- Evaluation relies on synthetic negative sampling strategies with untested real-world generalization
- Fixed hyper-parameter values (γ=0.5, g=0.5) without sensitivity analysis
- Memory bank assumes cached OOD features remain representative across varying test distributions

## Confidence

**High Confidence**: Architectural framework combining static and adaptive negative proxies is technically sound and well-documented. Improvement over NegLabel baseline is statistically significant with proper experimental controls.

**Medium Confidence**: Performance gains (2.45% AUROC increase, 6.48% FPR95 reduction) are methodologically valid but may have limited generalizability beyond tested datasets and CLIP model variants.

**Low Confidence**: Claims about training-free and annotation-free deployment are accurate for proposed method but don't account for potential threshold calibration needs on new target domains.

## Next Checks

1. **Cross-model Validation**: Test AdaNeg with alternative VLM architectures (e.g., ALIGN, BLIP) to verify performance claims aren't CLIP-specific

2. **Domain Adaptation Analysis**: Evaluate OOD detection performance when ID and OOD distributions have semantic overlap to stress-test adaptive proxy mechanism

3. **Memory Bank Sensitivity Study**: Systematically vary γ, g, and L parameters to quantify their impact on detection performance and identify optimal configurations for different dataset characteristics