---
ver: rpa2
title: 'WisdoM: Improving Multimodal Sentiment Analysis by Fusing Contextual World
  Knowledge'
arxiv_id: '2401.06659'
source_url: https://arxiv.org/abs/2401.06659
tags:
- context
- sentiment
- image
- wisdom
- multimodal
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of incorporating contextual
  world knowledge into multimodal sentiment analysis (MSA) to improve performance
  beyond relying solely on superficial information from images and text pairs. The
  authors propose a plug-in framework called WisdoM that leverages large vision-language
  models (LVLMs) to generate contextual world knowledge, which is then fused with
  MSA predictions using a training-free mechanism to reduce noise.
---

# WisdoM: Improving Multimodal Sentiment Analysis by Fusing Contextual World Knowledge

## Quick Facts
- **arXiv ID:** 2401.06659
- **Source URL:** https://arxiv.org/abs/2401.06659
- **Authors:** Wenbin Wang; Liang Ding; Li Shen; Yong Luo; Han Hu; Dacheng Tao
- **Reference count:** 6
- **Primary Result:** Proposed WisdoM framework achieves up to +5.8% F1 score gains on MSA benchmarks using LVLM-generated contextual knowledge

## Executive Summary
This paper addresses the challenge of incorporating contextual world knowledge into multimodal sentiment analysis to improve performance beyond relying solely on superficial information from images and text pairs. The authors propose WisdoM, a plug-in framework that leverages large vision-language models to generate contextual world knowledge, which is then selectively fused with MSA predictions using a training-free mechanism to reduce noise. The approach demonstrates consistent improvements across aspect-level and sentence-level MSA tasks on multiple benchmarks, achieving up to +5.8% F1 score gains over state-of-the-art methods while remaining model-agnostic and scalable across different model sizes and data volumes.

## Method Summary
WisdoM follows a three-stage framework to incorporate contextual world knowledge into multimodal sentiment analysis. First, ChatGPT generates diverse prompt templates covering historical, cultural, political, and other perspectives. Second, LVLMs use these templates with input text-image pairs to generate relevant contextual knowledge. Third, a training-free contextual fusion mechanism selectively combines original MSA predictions with context-augmented predictions only for low-confidence "hard samples" identified through a confidence difference metric. The framework uses convex interpolation weighted by β to balance original predictions and contextual information, avoiding overfitting while adapting to arbitrary model sizes.

## Key Results
- WisdoM achieves up to +5.8% F1 score improvement over state-of-the-art MSA methods across multiple benchmarks
- The framework demonstrates consistent performance gains on both aspect-level and sentence-level MSA tasks
- Model-agnostic approach shows effectiveness across different LVLM architectures (LLaVA-v1.5, MMICL, mPLUG-Owl2, AoM, ALMT)
- Selective fusion of contextual knowledge for hard samples reduces noise while improving sentiment understanding

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Contextual world knowledge helps resolve ambiguous sentiment cases that pure multimodal models struggle with.
- **Mechanism:** LVLMs generate background information (historical events, cultural context, etc.) that bridges the gap between superficial image/text features and deeper semantic understanding. This context is selectively fused only for low-confidence predictions to avoid noise.
- **Core assumption:** The generated context is relevant and noise can be effectively filtered out by the confidence-based fusion mechanism.
- **Evidence anchors:**
  - [abstract] "WisdoM utilizes LVLMs to comprehensively analyze both images and corresponding sentences, simultaneously generating pertinent context."
  - [section 4.3] "we first determine hard sample and then fuse the Pi and ˆPi in hard sample."
  - [corpus] Weak - related papers focus on sarcasm, causal reasoning, and modality attention, but none directly validate LVLM-generated context for MSA.

### Mechanism 2
- **Claim:** Training-free contextual fusion avoids overfitting and adapts to arbitrary model sizes.
- **Mechanism:** Uses a convex combination of original prediction Pi and context-augmented prediction ˆPi, weighted by β. Only applied to "hard samples" identified by confidence δi.
- **Core assumption:** Hard samples can be identified reliably using confidence difference, and simple interpolation suffices for fusion.
- **Evidence anchors:**
  - [section 4.3] "we take the convex combinations of Pi and ˆPi to obtain the final prediction."
  - [section 5.2] "we found that β values within the range of [0.4, 0.5] demonstrate strong performance for both models."
  - [corpus] Weak - no corpus evidence on training-free fusion effectiveness for MSA.

### Mechanism 3
- **Claim:** Prompt templates guide LVLMs to produce task-relevant world knowledge.
- **Mechanism:** ChatGPT generates diverse prompt templates covering historical, cultural, political, etc., contexts. These templates are filled with input sentence and fed to LVLMs with the image to generate focused context.
- **Core assumption:** ChatGPT can generate high-quality prompt templates that elicit useful context from LVLMs.
- **Evidence anchors:**
  - [section 4.1] "we ask ChatGPT to provide the appropriate prompt templates. The prompt templates provided by ChatGPT consider world knowledge of different perspectives."
  - [section 5.6] "Fig. 7 shows noticeable differences in different types of world knowledge across various datasets. For example, although artistic obtain the best result on twitter2015, the result obviously worse than baseline (AoM) on twitter2017. Nevertheless, history consistently yields superior results on both datasets."
  - [corpus] Weak - no direct corpus evidence on prompt template effectiveness for MSA.

## Foundational Learning

- **Concept:** Multimodal Sentiment Analysis (MSA)
  - Why needed here: Core task being improved; understanding how text+image fusion works is prerequisite to seeing why context helps.
  - Quick check question: What are the two main granularities of MSA discussed, and how do they differ in input/output?

- **Concept:** Large Vision-Language Models (LVLMs)
  - Why needed here: Source of contextual knowledge; knowing how they process image+text and generate text is key to understanding WisdoM's mechanism.
  - Quick check question: How does WisdoM integrate the image into the LVLM prompt, and why is this necessary?

- **Concept:** Confidence-based sample selection
  - Why needed here: Determines when to apply context fusion; understanding δi calculation and α threshold is essential for reproducing the method.
  - Quick check question: What is the formula for δi, and what does a low δi signify about the model's certainty?

## Architecture Onboarding

- **Component map:** Input (image, sentence) → MSA model → Confidence δi → (optional) Context generation → Fusion → Output
- **Critical path:** Input → MSA model → Confidence → (optional) Context generation → Fusion → Output
- **Design tradeoffs:**
  - Training-free fusion vs. fine-tuning with context: Simpler, more general, but may miss model-specific nuances.
  - Selective fusion (hard samples only) vs. full fusion: Reduces noise but risks missing subtle context benefits in high-confidence cases.
  - Using ChatGPT for prompts vs. hand-crafted: More diverse but less controllable.
- **Failure signatures:**
  - Performance drop when using context: Likely noisy context or poor α/β tuning.
  - No improvement on easy samples: Context fusion correctly bypassed, but maybe α too high.
  - Model-specific degradation: LVLM context not aligned with MSA model's feature space.
- **First 3 experiments:**
  1. Run baseline MSA model, record confidence δi distribution to understand "hard sample" prevalence.
  2. Generate context using LVLM with a few sample prompts; manually check relevance to MSA task.
  3. Test simple fusion (average of Pi and ˆPi) on hard samples only; compare F1 to baseline.

## Open Questions the Paper Calls Out

The paper identifies several open questions regarding the extension of contextual fusion mechanisms to additional modalities beyond text and image in MSA. Specifically, it acknowledges limitations regarding other modalities like speech and suggests the framework could be extended but doesn't explore this possibility. The authors also highlight the need for systematic experiments varying the interpolation coefficient β across multiple model sizes, task types, and datasets to find task-specific optimal values and determine if there are generalizable patterns.

## Limitations

- The quality and relevance of LVLM-generated contextual knowledge remains uncertain without access to specific prompt templates or example contexts.
- The framework's effectiveness depends heavily on proper parameter tuning (α and β thresholds) which may require dataset-specific optimization.
- The method is limited to text-image pairs and does not address other potential modalities like speech or video that could be relevant for MSA.

## Confidence

- **High Confidence:** The three-stage framework architecture and overall experimental methodology (using standard MSA datasets and metrics) are well-defined and reproducible.
- **Medium Confidence:** The training-free fusion mechanism and confidence-based selection are theoretically sound, but their effectiveness depends heavily on proper parameter tuning (α and β thresholds) which may require dataset-specific optimization.
- **Low Confidence:** The effectiveness of ChatGPT-generated prompts for eliciting relevant world knowledge from LVLMs in the MSA context is not empirically validated, and the specific prompt templates are not provided.

## Next Checks

1. **Prompt Quality Validation:** Generate sample contexts using the described prompt categories on a small dataset subset and manually evaluate their relevance and usefulness for sentiment determination before full-scale implementation.

2. **Confidence Threshold Sensitivity:** Systematically vary the α threshold (0.1 to 0.5) and analyze how the proportion of hard samples affects overall performance to find optimal trade-offs between noise reduction and context utilization.

3. **Context Ablation Study:** Run experiments with and without the context generation stage while keeping all other components identical to isolate the contribution of world knowledge to performance improvements.