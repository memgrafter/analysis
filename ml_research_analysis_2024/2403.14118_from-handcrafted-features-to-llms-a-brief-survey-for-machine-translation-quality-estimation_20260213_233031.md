---
ver: rpa2
title: 'From Handcrafted Features to LLMs: A Brief Survey for Machine Translation
  Quality Estimation'
arxiv_id: '2403.14118'
source_url: https://arxiv.org/abs/2403.14118
tags:
- quality
- translation
- methods
- estimation
- llms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper provides a comprehensive survey of Machine Translation
  Quality Estimation (MTQE), a task that assesses translation quality without reference
  translations. The authors review datasets, annotation methods, shared tasks, and
  methodologies, categorizing methods into handcrafted features, deep learning, and
  Large Language Models (LLMs).
---

# From Handcrafted Features to LLMs: A Brief Survey for Machine Translation Quality Estimation

## Quick Facts
- arXiv ID: 2403.14118
- Source URL: https://arxiv.org/abs/2403.14118
- Authors: Haofei Zhao; Yilun Liu; Shimin Tao; Weibin Meng; Yimeng Chen; Xiang Geng; Chang Su; Min Zhang; Hao Yang
- Reference count: 40
- Primary result: Comprehensive survey of Machine Translation Quality Estimation methods spanning handcrafted features, deep learning, and Large Language Models

## Executive Summary
This survey provides a comprehensive overview of Machine Translation Quality Estimation (MTQE), examining the evolution from traditional handcrafted feature-based approaches to modern Large Language Model (LLM) methods. The authors systematically categorize existing methodologies while identifying key challenges including data scarcity, computational resource demands, and interpretability issues. The survey emphasizes the growing role of LLMs in QE, discussing approaches like direct prediction, generative probabilities, and pseudo-data generation, while also highlighting persistent challenges in low-resource languages and domain adaptation.

## Method Summary
The survey examines three main categories of MTQE methods: handcrafted features, deep learning approaches, and Large Language Models. For handcrafted features, it reviews traditional methods like QuEst++ that extract linguistic and statistical features. Deep learning approaches include classic neural networks and pre-trained language models such as XLM-R. LLM-based methods leverage models like GPT for direct quality prediction, error analysis, and pseudo-data generation. The survey synthesizes methodologies from 40+ references, analyzing their architectures, training procedures, and evaluation metrics across word-level, sentence-level, and document-level QE tasks.

## Key Results
- LLMs can serve as universal evaluators through direct prediction and generative probabilities, though performance still lags behind pre-trained LM-based methods
- Data scarcity remains a critical challenge, particularly for low-resource languages, limiting both traditional and LLM-based QE approaches
- Current methods lack sufficient interpretability, with deep learning and LLM approaches offering limited transparency for professional translation workflows

## Why This Works (Mechanism)

### Mechanism 1
LLMs can serve as universal evaluators for translation quality estimation by leveraging their knowledge base and generation capabilities. LLMs directly predict quality scores, error levels, or fluency assessments based on content generated from translation prompts, bypassing the need for reference translations. Core assumption: LLMs possess sufficient linguistic knowledge and reasoning ability to evaluate translation quality without task-specific fine-tuning. Evidence anchors: GEMBA, a GPT-based translation quality assessment metric that uses single-step prompting and can be applied to scenarios with reference translations as well as to QE. Break condition: If LLM predictions consistently underperform compared to traditional QE methods across diverse language pairs and domains.

### Mechanism 2
LLMs can generate pseudo-labeled data to augment scarce QE training datasets. LLMs use their knowledge to corrupt reference sentences, generate fluent translations, and label these as training examples for QE models, addressing data scarcity. Core assumption: Pseudo-data generated by LLMs maintains sufficient quality and diversity to improve QE model performance. Evidence anchors: Huang et al. leveraged LLMs to corrupt reference sentences. They then generated a fluent sentence from the corrupted one and used this as a noisy negative view. Break condition: If models trained on pseudo-data show significant performance degradation when evaluated on real-world translation quality.

### Mechanism 3
LLMs can enhance interpretability in QE by providing detailed error analysis and explanations. LLMs predict error severity, count errors, and explain translation issues, creating MQM-like assessments that improve transparency in QE systems. Core assumption: LLMs can accurately identify and explain translation errors at a level comparable to human annotators. Evidence anchors: To enhance the performance of LLMs in quality assessment, Lu et al. introduced Error Analysis Prompting (EAPrompt), a new prompting method that combines Chain-of-Thought (CoT) with EA. Break condition: If LLM explanations contain factual errors or fail to identify critical translation issues that human annotators would catch.

## Foundational Learning

- Concept: Translation Quality Estimation (QE)
  - Why needed here: QE is the core task being surveyed, requiring understanding of what it is and how it differs from traditional MT evaluation.
  - Quick check question: What distinguishes QE from traditional MT evaluation methods like BLEU or METEOR?

- Concept: Machine Translation Quality Metrics
  - Why needed here: Understanding HTER, DA, and MQM is essential for grasping how QE systems are evaluated and trained.
  - Quick check question: How does MQM differ from HTER in terms of granularity and evaluation approach?

- Concept: Large Language Models (LLMs) in NLP
  - Why needed here: The paper focuses on LLM-based QE methods, requiring understanding of LLM capabilities and limitations.
  - Quick check question: What are the key advantages of using LLMs for QE compared to traditional deep learning approaches?

## Architecture Onboarding

- Component map: Source text and machine-translated text → Feature extraction (handcrafted or learned) → Model training (regression/classification for quality scores) → Quality prediction → Evaluation (correlation metrics)
- Critical path: Data → Feature extraction → Model training → Quality prediction → Evaluation. For LLM-based methods, prompt design becomes critical.
- Design tradeoffs: Traditional methods offer interpretability and efficiency but may lack accuracy. LLM methods provide strong performance but require significant computational resources and may lack transparency.
- Failure signatures: Poor correlation with human judgments, inability to handle low-resource languages, high computational costs, and overfitting to specific domains.
- First 3 experiments:
  1. Implement a basic handcrafted feature-based QE system using QuEst++ and evaluate on a standard dataset.
  2. Develop a simple LLM-based QE approach using direct prompting on GPT-3.5 and compare performance to traditional methods.
  3. Create a hybrid system combining LLM-generated pseudo-data with traditional QE training to test data augmentation effectiveness.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can LLMs be effectively integrated into existing QE frameworks to improve both accuracy and interpretability?
- Basis in paper: The paper discusses various LLM-based approaches for QE, including direct prediction, generative probabilities, and pseudo-data generation, but notes that their performance has not yet surpassed that of pre-trained LM-based methods.
- Why unresolved: The paper highlights the potential of LLMs but also acknowledges that their performance in QE is still developing and needs further research to achieve state-of-the-art results.
- What evidence would resolve it: Comparative studies demonstrating the effectiveness of LLM-integrated QE frameworks against current SOTA methods, along with evaluations of interpretability improvements.

### Open Question 2
- Question: What are the most effective strategies for addressing data scarcity in low-resource language pairs for QE?
- Basis in paper: The paper identifies data scarcity as a major challenge, particularly for low-resource languages, and notes the high costs associated with acquiring sufficient annotated data.
- Why unresolved: The paper does not provide specific solutions for overcoming data scarcity in low-resource languages, leaving this as an open area for research.
- What evidence would resolve it: Research demonstrating successful data augmentation techniques or alternative data sources that improve QE performance in low-resource language pairs.

### Open Question 3
- Question: How can standardized evaluation metrics be developed to better compare and integrate QE model performance across different tasks and languages?
- Basis in paper: The paper discusses the lack of standardized evaluation metrics as a challenge, noting that the subjectivity of QE tasks and varying preferences for translation quality make it difficult to compare models.
- Why unresolved: The paper highlights the need for standardized metrics but does not propose specific solutions, indicating this as an area requiring further investigation.
- What evidence would resolve it: Development and validation of standardized evaluation metrics that can consistently assess QE performance across diverse tasks and languages.

## Limitations

- Data scarcity remains a fundamental constraint, particularly for low-resource languages where limited annotated data hampers both traditional and LLM-based approaches
- Computational resource requirements for LLM-based methods present significant barriers to widespread adoption, demanding substantial GPU memory and processing time
- Insufficient interpretability of QE models, especially those based on deep learning and LLMs, limits their practical deployment in professional translation workflows

## Confidence

**High Confidence**: The survey's categorization of MTQE methods into handcrafted features, deep learning, and LLM-based approaches is well-supported by the literature and represents a clear taxonomy of the field. The identification of data scarcity and computational resource requirements as key challenges is consistently observed across multiple studies.

**Medium Confidence**: The claim about LLMs serving as universal evaluators shows promise but requires more empirical validation across diverse language pairs and domains. While the mechanisms described are theoretically sound, their relative effectiveness compared to traditional methods needs more systematic benchmarking.

**Low Confidence**: The survey's discussion of interpretability enhancement through LLMs, while conceptually appealing, lacks concrete empirical evidence demonstrating that LLM-generated explanations consistently match or exceed human-level error analysis quality.

## Next Checks

1. **Cross-lingual Generalization Test**: Implement and evaluate LLM-based QE methods across multiple language pairs (including low-resource languages) to verify claims about universal applicability and identify language-specific failure patterns.

2. **Computational Efficiency Benchmark**: Systematically measure the resource requirements (GPU memory, inference time) of LLM-based QE methods versus traditional approaches across different model sizes to quantify the practical deployment trade-offs.

3. **Interpretability Validation**: Conduct a human evaluation study comparing LLM-generated error explanations against professional translator annotations to assess whether LLM-based interpretability genuinely enhances practical utility.