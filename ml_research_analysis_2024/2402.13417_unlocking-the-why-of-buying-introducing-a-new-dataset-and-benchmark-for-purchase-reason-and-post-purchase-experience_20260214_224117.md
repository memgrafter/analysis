---
ver: rpa2
title: 'Unlocking the `Why'' of Buying: Introducing a New Dataset and Benchmark for
  Purchase Reason and Post-Purchase Experience'
arxiv_id: '2402.13417'
source_url: https://arxiv.org/abs/2402.13417
tags:
- purchase
- user
- reason
- product
- reviews
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a new task of purchase reason explanation
  for recommendation systems, addressing the limitation of existing approaches that
  focus on post-purchase sentiment rather than the motivations behind initial purchase
  decisions. The authors propose an LLM-based approach to generate a high-quality
  dataset containing both purchase reasons and post-purchase experiences from user
  reviews, explicitly distinguishing between pre- and post-purchase information.
---

# Unlocking the `Why' of Buying: Introducing a New Dataset and Benchmark for Purchase Reason and Post-Purchase Experience

## Quick Facts
- arXiv ID: 2402.13417
- Source URL: https://arxiv.org/abs/2402.13417
- Reference count: 26
- This paper introduces a new task of purchase reason explanation for recommendation systems, addressing the limitation of existing approaches that focus on post-purchase sentiment rather than the motivations behind initial purchase decisions.

## Executive Summary
This paper introduces a novel approach to purchase reason explanation for recommendation systems, addressing a critical gap in current methodologies. Traditional recommendation systems primarily focus on post-purchase sentiment analysis, neglecting the motivations behind initial purchase decisions. The authors propose an LLM-based approach to generate a high-quality dataset containing both purchase reasons and post-purchase experiences from user reviews, explicitly distinguishing between pre- and post-purchase information. The resulting dataset achieves 96.4% coverage of purchase reasons with only 0.49% hallucination rate, and 88.2% coverage of post-purchase experiences with 0.21% hallucination rate. Benchmarking results show that the dataset enables effective purchase reason and post-purchase experience generation, with the best performance achieved using raw past user reviews and item metadata.

## Method Summary
The authors propose an LLM-based approach to generate a high-quality dataset containing both purchase reasons and post-purchase experiences from user reviews. Their method explicitly distinguishes between pre- and post-purchase information by analyzing user reviews to extract motivations behind initial purchase decisions and experiences after the purchase. The approach involves fine-tuning a model on the generated dataset and evaluating it using both automated LLM-driven metrics and small-scale human annotation. The methodology focuses on creating a benchmark that can effectively generate personalized purchase reason explanations and post-purchase experience summaries for recommendation systems.

## Key Results
- Dataset achieves 96.4% coverage of purchase reasons with only 0.49% hallucination rate
- Dataset achieves 88.2% coverage of post-purchase experiences with 0.21% hallucination rate
- Best performance achieved using raw past user reviews and item metadata

## Why This Works (Mechanism)
The approach works by leveraging large language models to extract and categorize user review content into distinct pre-purchase motivations and post-purchase experiences. By explicitly separating these two types of information, the system can provide more accurate and contextually relevant explanations for both why a user might consider purchasing an item and what they experienced after the purchase. The use of LLM-driven generation allows for capturing nuanced user sentiments and motivations that traditional rule-based or keyword extraction methods might miss.

## Foundational Learning
- **Purchase reason extraction**: Understanding user motivations requires analyzing language patterns that indicate decision-making factors. This is needed because traditional sentiment analysis conflates pre- and post-purchase information. Quick check: Verify that extracted reasons align with actual purchase behavior patterns.
- **Post-purchase experience classification**: Distinguishing between expectations and actual experiences is crucial for accurate feedback. This is needed because users often have different experiences than anticipated. Quick check: Validate that classified experiences match chronological review progression.
- **LLM hallucination detection**: Identifying when generated content deviates from source material is essential for dataset quality. This is needed because LLMs can create plausible but inaccurate content. Quick check: Compare generated explanations against original review content.
- **Coverage metrics calculation**: Measuring the completeness of extracted information ensures dataset utility. This is needed because partial or biased datasets limit recommendation effectiveness. Quick check: Calculate coverage across diverse product categories.

## Architecture Onboarding

Component map:
User Reviews -> LLM Extraction -> Purchase Reason Classification -> Post-Purchase Experience Classification -> Dataset Generation -> Fine-tuning -> Evaluation

Critical path:
The critical path flows from user reviews through LLM extraction to dataset generation, as the quality of extracted information directly impacts the effectiveness of subsequent fine-tuning and evaluation. Any bottleneck or error in the extraction phase will propagate through the entire pipeline.

Design tradeoffs:
The authors chose to use LLM-based extraction over traditional rule-based methods to capture more nuanced and context-dependent information from user reviews. This tradeoff favors accuracy and comprehensiveness over computational efficiency and interpretability. The decision to explicitly separate pre- and post-purchase information adds complexity but enables more targeted and relevant explanations for recommendation systems.

Failure signatures:
- Low coverage rates indicate insufficient training data or ineffective prompt engineering
- High hallucination rates suggest the LLM is generating content not supported by source reviews
- Poor correlation between extracted reasons and actual purchase behavior indicates misalignment between extraction criteria and user decision-making processes
- Inability to generalize across product categories suggests overfitting to specific review patterns

First experiments:
1. Test extraction accuracy on a small, manually labeled subset of reviews to establish baseline performance
2. Evaluate coverage and hallucination rates across different product categories to identify potential biases
3. Compare LLM-generated explanations against human-annotated reasons to validate extraction quality

## Open Questions the Paper Calls Out
None

## Limitations
- Primary uncertainty lies in the scalability of the LLM-based approach for dataset generation, as results are based on a limited dataset from one e-commerce platform
- Automated evaluation methodology may not fully capture nuanced quality of purchase reason explanations that human users find meaningful
- Effectiveness of generated dataset in real-world recommendation systems remains to be demonstrated through actual deployment and A/B testing

## Confidence
- High confidence in the dataset quality metrics (coverage and hallucination rates)
- Medium confidence in the generalizability of results to other e-commerce domains
- Medium confidence in the effectiveness of the proposed recommendation approach

## Next Checks
1. Conduct cross-platform validation by applying the same methodology to datasets from multiple e-commerce platforms to verify generalizability
2. Perform comprehensive human evaluation studies with actual users to assess the practical utility and perceived quality of generated purchase reasons
3. Implement a real-world A/B test in an operational recommendation system to measure the impact of purchase reason explanations on user engagement and conversion rates