---
ver: rpa2
title: 'A Survey on Group Fairness in Federated Learning: Challenges, Taxonomy of
  Solutions and Directions for Future Research'
arxiv_id: '2410.03855'
source_url: https://arxiv.org/abs/2410.03855
tags:
- fairness
- data
- learning
- group
- clients
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This survey provides the first comprehensive overview of group
  fairness in federated learning (FL), analyzing 47 research works that address this
  critical issue. The paper identifies key challenges in achieving group fairness
  in FL, including data heterogeneity, restricted information access, aggregation
  algorithms, limited client participation, and resource constraints.
---

# A Survey on Group Fairness in Federated Learning: Challenges, Taxonomy of Solutions and Directions for Future Research

## Quick Facts
- arXiv ID: 2410.03855
- Source URL: https://arxiv.org/abs/2410.03855
- Reference count: 40
- This survey provides the first comprehensive overview of group fairness in federated learning, analyzing 47 research works that address this critical issue.

## Executive Summary
This survey presents the first comprehensive analysis of group fairness in federated learning (FL), examining 47 research works that address fairness challenges in distributed machine learning systems. The paper identifies five key challenges: data heterogeneity, restricted information access, aggregation algorithms, limited client participation, and resource constraints. A novel taxonomy categorizes approaches based on data partitioning, location, strategies, concerns, sensitive attributes, and datasets. The survey reveals that most research focuses on horizontal FL with binary sensitive attributes using traditional datasets like Adult and COMPAS. Future directions include exploring vertical FL, federated transfer learning, intersectionality in sensitive attributes, and developing frameworks for standardized evaluation.

## Method Summary
The survey methodology involved collecting and analyzing 47 research works on group fairness in federated learning using Google Scholar with specific query terms. A taxonomy-based analysis framework was developed to categorize approaches based on data partitioning (horizontal, vertical, transfer learning), location (local, global, hybrid), strategies (aggregation, reweighting, adversarial learning, client participation, personalization, clustering, thresholding, constrained optimization), concerns (non-IID data, privacy, robustness, concept drift), sensitive attributes (single/multiple, binary/multivalued), and datasets/applications. The analysis identified research gaps and proposed future directions for the field.

## Key Results
- Five key challenges in achieving group fairness in FL: data heterogeneity, restricted information access, aggregation algorithms, limited client participation, and resource constraints
- Most research focuses on horizontal FL with binary sensitive attributes, using traditional datasets like Adult and COMPAS
- A novel taxonomy categorizing approaches based on multiple dimensions is introduced
- Future research directions include exploring vertical FL, intersectionality in sensitive attributes, and standardized evaluation frameworks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: FairFed achieves global fairness by dynamically adjusting client weights based on local fairness deviation from the global average.
- Mechanism: Clients evaluate fairness of the global model on their local data each round. The server computes a weight for each client as a function of the mismatch between global and local fairness metrics, favoring clients whose local fairness measures align with the global measure.
- Core assumption: Local fairness deviations provide a reliable signal for global fairness improvement.
- Evidence anchors:
  - [section]: "The clients evaluate the fairness of the global model on their local datasets in each round and collectively collaborate with the server to adjust its model weights. The weights are a function of the mismatch between the global fairness measurement (on the full dataset) and the local fairness measurement at each client, favoring clients whose local measures match the global measure."
  - [corpus]: No direct evidence found in corpus, indicating this is a unique contribution of the paper.
- Break condition: If clients' local fairness metrics are systematically biased or if clients with poor fairness metrics are over-represented in the federation.

### Mechanism 2
- Claim: Hybrid approaches combining local and global fairness strategies are most effective in federated learning.
- Mechanism: Local methods allow clients to tailor fairness strategies to their specific data, while global methods ensure overall fairness across the federation. Hybrid solutions leverage the strengths of both approaches.
- Core assumption: Local data distributions vary significantly across clients, requiring both local adaptation and global coordination.
- Evidence anchors:
  - [section]: "The majority of current solutions involve a hybrid solution, where clients and the central server collaborate to achieve fairness. These hybrid approaches aim to leverage the strengths of both local and global strategies while mitigating their respective weaknesses."
  - [corpus]: Weak evidence - only mentions hybrid approaches exist but doesn't provide comparative effectiveness data.
- Break condition: If communication overhead becomes prohibitive or if privacy concerns prevent sharing necessary information between clients and server.

### Mechanism 3
- Claim: Constrained optimization formulations can ensure group fairness by limiting fairness violations to predefined thresholds.
- Mechanism: The optimization problem is formulated with fairness constraints that bound the disparity in performance across groups. This can be achieved through Lagrangian multipliers, differential multipliers, or minimax formulations.
- Core assumption: Fairness constraints can be effectively incorporated into the optimization process without severely compromising model performance.
- Evidence anchors:
  - [section]: "One approach to achieving global fairness in FL is to formulate a constrained optimization problem where each client seeks to optimize their local model while ensuring that fairness-related disparities do not exceed a predefined threshold."
  - [corpus]: No direct evidence found in corpus, suggesting this is a novel contribution or common approach not captured in the surveyed papers.
- Break condition: If the optimization becomes intractable or if the fairness constraints are too restrictive, leading to poor model performance.

## Foundational Learning

- Concept: Group fairness in machine learning
  - Why needed here: Understanding group fairness is essential for designing fair federated learning systems that ensure equitable outcomes across different demographic groups.
  - Quick check question: What is the difference between group fairness and individual fairness in machine learning?

- Concept: Federated Learning (FL)
  - Why needed here: FL is the underlying framework for distributed model training, and understanding its challenges is crucial for developing fair FL algorithms.
  - Quick check question: How does the non-IID nature of data in FL affect the implementation of fairness-aware algorithms?

- Concept: Sensitive attributes and intersectionality
  - Why needed here: Identifying and handling sensitive attributes, including their intersections, is critical for ensuring comprehensive fairness in FL systems.
  - Quick check question: Why is it important to consider intersectionality when addressing group fairness in machine learning?

## Architecture Onboarding

- Component map:
  - Clients -> Local training and fairness evaluation -> Send updates to server
  - Server -> Aggregates weighted model updates -> Broadcasts updated global model
  - Validation/proxy dataset -> Used for fairness evaluation and client selection
  - Privacy mechanisms -> Differential privacy, secure multiparty computation

- Critical path:
  1. Clients download global model
  2. Clients train locally on their data
  3. Clients evaluate fairness of global model on local data
  4. Clients send model updates and fairness statistics to server
  5. Server computes global fairness metrics and adjusts client weights
  6. Server aggregates weighted model updates
  7. Server broadcasts updated global model

- Design tradeoffs:
  - Privacy vs. Fairness: More information sharing improves fairness but reduces privacy
  - Communication efficiency vs. Fairness accuracy: More frequent communication enables better fairness monitoring but increases overhead
  - Global vs. Local fairness: Balancing overall fairness with local fairness requirements

- Failure signatures:
  - Poor convergence: May indicate excessive fairness constraints or ineffective weight adjustment
  - Bias in model outcomes: Could suggest inadequate client representation or ineffective fairness metrics
  - Privacy breaches: Might result from insufficient privacy mechanisms when sharing fairness information

- First 3 experiments:
  1. Implement FairFed with synthetic non-IID data to test weight adjustment mechanism
  2. Compare FairFed performance against FedAvg baseline on Adult dataset using statistical parity metric
  3. Evaluate FairFed under concept drift conditions using synthetic data with group-specific drift

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What are effective strategies for addressing intersectionality in federated learning systems?
- Basis in paper: [explicit] The paper notes that no research has addressed intersectionality in the context of FL, despite its importance in understanding combined effects of multiple sensitive attributes.
- Why unresolved: The distributed nature of FL creates unique challenges for handling intersectional groups that are under-represented in local datasets but more prominent when data is aggregated.
- What evidence would resolve it: Empirical studies demonstrating successful methods for ensuring fairness across intersectional groups in FL settings, with measurable improvements in outcomes for these groups.

### Open Question 2
- Question: How can federated learning systems be designed to handle group-specific concept drift while maintaining fairness?
- Basis in paper: [explicit] The paper introduces group-specific distributed concept drift as an open challenge and notes that existing solutions like FairFedDrift are computationally expensive.
- Why unresolved: Different clients experience distinct group-specific concept drifts over time, requiring adaptive algorithms that can monitor and adjust for these changes while preserving fairness.
- What evidence would resolve it: Development and validation of efficient, scalable algorithms that can detect and adapt to group-specific concept drift in real-time while maintaining fairness metrics.

### Open Question 3
- Question: What are effective methods for creating representative validation datasets in global fairness solutions for federated learning?
- Basis in paper: [explicit] The paper notes that existing global solutions lack efficient methods for creating validation sets needed for aggregation algorithms.
- Why unresolved: Global aggregation strategies require validation data to evaluate fairness, but obtaining representative data without compromising privacy remains challenging in FL settings.
- What evidence would resolve it: Demonstrated techniques that incentivize client participation in validation dataset creation while ensuring privacy preservation and representativeness.

## Limitations
- The analysis is based on 47 research works identified through specific search criteria, which may have excluded relevant papers using different terminology
- The taxonomy presented, while novel, may not capture all emerging approaches as the field evolves rapidly
- Direct comparative analyses of solution effectiveness are limited in the surveyed literature

## Confidence
- **High confidence**: The identification of key challenges in achieving fairness in FL is well-supported by the surveyed literature
- **Medium confidence**: The categorization of solutions and their effectiveness is based on reported results in the surveyed papers, but direct comparative analyses are limited
- **Medium confidence**: The future research directions are inferred from current gaps but may not fully capture emerging trends

## Next Checks
1. **Reproducibility Check**: Implement and evaluate at least three different fairness approaches (e.g., aggregation-based, adversarial learning, and constrained optimization) on a standard FL benchmark to validate the comparative effectiveness claims.

2. **Intersectionality Analysis**: Design an experiment using datasets with multiple sensitive attributes to empirically demonstrate the importance of considering intersectionality in group fairness, addressing the identified research gap.

3. **Vertical FL Fairness**: Develop a prototype framework for achieving group fairness in vertical federated learning and evaluate its performance against horizontal FL approaches on a real-world dataset like Criteo or a medical dataset with multiple institutions.