---
ver: rpa2
title: pFedGame -- Decentralized Federated Learning using Game Theory in Dynamic Topology
arxiv_id: '2410.04058'
source_url: https://arxiv.org/abs/2410.04058
tags:
- learning
- data
- federated
- aggregation
- game
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces pFedGame, a novel game-theoretic approach
  to decentralized federated learning that operates without a central aggregation
  server and adapts to dynamic network topologies. The method consists of two sequential
  steps in each federated learning round: peer selection based on model accuracy thresholds
  and a two-player constant-sum cooperative game to optimize model aggregation weights.'
---

# pFedGame -- Decentralized Federated Learning using Game Theory in Dynamic Topology

## Quick Facts
- arXiv ID: 2410.04058
- Source URL: https://arxiv.org/abs/2410.04058
- Authors: Monik Raj Behera; Suchetana Chakraborty
- Reference count: 11
- Primary result: pFedGame achieves accuracy higher than 70% for heterogeneous data, performing comparably to state-of-the-art methods

## Executive Summary
This paper introduces pFedGame, a novel game-theoretic approach to decentralized federated learning that operates without a central aggregation server and adapts to dynamic network topologies. The method consists of two sequential steps in each federated learning round: peer selection based on model accuracy thresholds and a two-player constant-sum cooperative game to optimize model aggregation weights. Experiments on Fashion-MNIST, CIFAR-10, and CIFAR-100 datasets demonstrate that pFedGame achieves accuracy higher than 70% for heterogeneous data, performing comparably to state-of-the-art methods under extreme and severe heterogeneity scenarios. The approach is particularly effective for dynamic networks where node relationships change over time, addressing key challenges in conventional federated learning including central server bottlenecks, data bias, poor convergence, and model poisoning vulnerabilities.

## Method Summary
pFedGame operates through a decentralized federated learning framework where each node independently selects peers based on accuracy thresholds and performs model aggregation through a two-player constant-sum cooperative game. The algorithm uses neural network architectures (multi-layer for Fashion-MNIST, convolutional for CIFAR datasets) and tests under extreme (5 participants), severe (10 participants), and homogeneous (10 participants) heterogeneity settings. Each node evaluates potential peers by testing their models on local data, then plays a cooperative game to determine optimal aggregation weights, with the process repeated across federated learning rounds. The system is implemented using TensorFlow 2.2 on GPU hardware with Python 3.10.6.

## Key Results
- Achieves accuracy higher than 70% for heterogeneous data distributions
- Performs comparably to state-of-the-art federated learning methods under extreme and severe heterogeneity scenarios
- Successfully operates in dynamic network topologies without central aggregation servers

## Why This Works (Mechanism)

### Mechanism 1
- Claim: pFedGame achieves convergence by iteratively adjusting peer weights through a two-player constant-sum cooperative game.
- Mechanism: In each federated learning round, pFedGame plays a cooperative game between two players: the local model (M(x)) and the aggregated model from selected peers (M(α)). The game adjusts weights ψ(x) and ψ(α) such that their sum remains constant (ψ(x) + ψ(α) = 1). Through multiple game rounds (similar to epochs), the algorithm increases the weight of the model that performs better on the local data, moving toward a saddle point that optimizes the aggregated model's performance.
- Core assumption: The performance difference between the two models on local data is a reliable indicator of which model should contribute more to the aggregation.
- Evidence anchors:
  - [abstract]: "it executes a two-player constant sum cooperative game to reach convergence by applying an optimal federated learning aggregation strategy"
  - [section]: "Equation 5 shows the relation between utilities ψ(x) and ψ(α), for two players M(x) and M(α) respectively"
- Break condition: The mechanism breaks if the performance difference threshold β is set too high or too low, preventing meaningful weight adjustments.

### Mechanism 2
- Claim: Peer selection based on accuracy thresholds improves aggregation efficiency and relevance.
- Mechanism: Before each federated learning round, pFedGame filters potential peers by evaluating whether their models achieve accuracy above a threshold θ when tested on the local data (H(M(c), D(x)) ≥ θ). This ensures that only peers whose models are likely to contribute meaningfully to the local model's performance are selected for collaboration.
- Core assumption: A model's accuracy on another node's data distribution is a good proxy for its potential contribution to that node's federated learning aggregation.
- Evidence anchors:
  - [abstract]: "First, it selects suitable peers for collaboration in federated learning"
  - [section]: "Peer selection restricts the number of nodes (peers), with whom a given node x needs to collaborate during an arbitrary FL round"
- Break condition: The mechanism breaks if the accuracy threshold θ is set too high, resulting in too few peers for collaboration, or too low, including irrelevant peers.

### Mechanism 3
- Claim: Decentralized aggregation without a central server improves resilience to model poisoning and trust issues.
- Mechanism: By eliminating the central aggregation server and performing all aggregation locally through peer-to-peer communication, pFedGame reduces the attack surface for model poisoning attacks and removes the need for trust in a central authority. Each node independently determines its aggregation strategy based on its local evaluation of peer contributions.
- Core assumption: Decentralized aggregation is inherently more secure against certain types of attacks than centralized approaches.
- Evidence anchors:
  - [abstract]: "Conventional federated learning frameworks... exposure to model poisoning attacks"
  - [section]: "decentralized FL also mitigates the risk of overgeneralization and poor convergence on extremely heterogeneous data"
- Break condition: The mechanism breaks if peer-to-peer communication becomes unreliable or if malicious peers can consistently manipulate their model updates to influence aggregation.

## Foundational Learning

- Concept: Game Theory and Cooperative Games
  - Why needed here: The core innovation of pFedGame is framing the model aggregation problem as a two-player constant-sum cooperative game to find optimal weights for combining models.
  - Quick check question: What is the difference between a constant-sum game and a general-sum game, and why is the constant-sum assumption appropriate for this federated learning aggregation problem?

- Concept: Federated Learning Fundamentals
  - Why needed here: Understanding how traditional federated learning works (with central aggregation servers and FedAvg algorithms) is essential to appreciate why pFedGame's decentralized approach is innovative.
  - Quick check question: How does the FedAvg algorithm typically aggregate models from different clients, and what are its limitations in heterogeneous data scenarios?

- Concept: Graph Theory and Dynamic Networks
  - Why needed here: The system models participants as nodes in a graph with dynamic edges, and the peer selection algorithm depends on understanding network topology and relationships between nodes.
  - Quick check question: In the context of federated learning, what does it mean for a graph to be "dynamic," and how does this affect the peer selection process?

## Architecture Onboarding

- Component map: Graph G(V,E) -> Peer Selection Module -> pFedGame Engine -> Model Evaluation Function H(m,d) -> Communication Layer
- Critical path: For each participant in each round: evaluate peer models → select suitable peers → initialize game weights → run pFedGame iterations → compute final aggregated model
- Design tradeoffs:
  - Accuracy vs. computation: Higher accuracy thresholds in peer selection reduce computation but may limit available peers
  - Game rounds vs. convergence: More game rounds improve convergence but increase computational cost
  - Dynamic vs. static topology: Dynamic topology adapts to changing conditions but adds complexity
- Failure signatures:
  - Poor accuracy despite many peers: Likely indicates threshold θ is too low or game parameters (δ, r) are suboptimal
  - Very few peers selected: Threshold θ may be too high or network connectivity poor
  - Slow convergence: Game parameters may need adjustment or network topology may be problematic
- First 3 experiments:
  1. Run pFedGame with only one peer (simplest case) to verify basic game mechanics work
  2. Test peer selection with varying threshold θ values on a small network to find optimal threshold
  3. Compare convergence speed with different game round counts (r) and weight change rates (δ)

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the computational overhead of peer selection and pFedGame compare to centralized approaches when scaling to hundreds of nodes?
- Basis in paper: [explicit] The paper states peer selection restricts collaboration to relevant nodes to decrease computation, but doesn't quantify this overhead or compare it to centralized approaches
- Why unresolved: The paper mentions complexity analysis (O(|P|) for peer selection and O(|C(x)|) + O(r) for pFedGame) but doesn't provide empirical measurements of actual computation time or energy consumption
- What evidence would resolve it: Detailed benchmarking showing CPU/GPU usage, memory consumption, and execution time for pFedGame versus centralized approaches with varying numbers of nodes and different network conditions

### Open Question 2
- Question: How does pFedGame's performance degrade under different types of Byzantine attacks compared to existing Byzantine-robust federated learning methods?
- Basis in paper: [explicit] The paper mentions that decentralized FL is generally resilient to model poisoning attacks, but doesn't specifically test or compare against Byzantine attacks
- Why unresolved: While the paper mentions resilience to model poisoning attacks in the related works section, it doesn't empirically evaluate pFedGame's robustness against various Byzantine attack scenarios (e.g., sign-flipping, label-flipping, or data poisoning)
- What evidence would resolve it: Experimental results showing pFedGame's accuracy and convergence under various Byzantine attack scenarios, compared to established Byzantine-robust methods like Krum, Median, or Trimmed Mean

### Open Question 3
- Question: What is the optimal strategy for determining the game parameters r (number of rounds) and δ (learning rate) for different network topologies and data distributions?
- Basis in paper: [explicit] The paper states r and δ were set to 10 and 0.1 respectively based on empirical observation, but doesn't provide a systematic approach for determining these parameters
- Why unresolved: The parameter selection appears to be ad-hoc rather than derived from theoretical analysis or automated tuning, and the paper doesn't explore how these parameters affect convergence across different scenarios
- What evidence would resolve it: A comprehensive sensitivity analysis showing how different r and δ values affect convergence speed and final accuracy across various network topologies, data distributions, and heterogeneity levels, ideally with an automated parameter selection mechanism

## Limitations

- The paper does not specify exact neural network architectures for CIFAR datasets, which could significantly impact performance results
- Game parameters (δ, r, β) are mentioned but optimal values are not provided for different heterogeneity scenarios
- The dynamic topology adaptation mechanism is described conceptually but lacks algorithmic details for implementation

## Confidence

- High confidence: The peer selection mechanism based on accuracy thresholds (Mechanism 2) is well-defined and directly implementable
- Medium confidence: The two-player game aggregation mechanism (Mechanism 1) has clear theoretical grounding but implementation details are incomplete
- Low confidence: The security claims regarding decentralized aggregation (Mechanism 3) are stated but not empirically validated against specific attack scenarios

## Next Checks

1. Implement a simplified version of pFedGame with fixed game parameters and test convergence behavior on a small network with synthetic data distributions
2. Conduct ablation studies comparing peer selection with different accuracy thresholds (θ) to determine the optimal balance between peer availability and aggregation quality
3. Test the system's robustness by introducing simulated malicious nodes and measuring the impact on model performance and convergence rates