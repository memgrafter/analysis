---
ver: rpa2
title: Easy Problems That LLMs Get Wrong
arxiv_id: '2405.19616'
source_url: https://arxiv.org/abs/2405.19616
tags:
- each
- score
- goat
- other
- farmer
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: A new linguistic benchmark of 30 questions was developed to test
  LLMs' reasoning and comprehension on tasks that are easy for humans. Six models
  were evaluated across domains like logical reasoning, spatial intelligence, and
  linguistic understanding.
---

# Easy Problems That LLMs Get Wrong

## Quick Facts
- arXiv ID: 2405.19616
- Source URL: https://arxiv.org/abs/2405.19616
- Authors: Sean Williams; James Huckle
- Reference count: 40
- Primary result: Best LLM scored only 38% on benchmark of 30 questions that are easy for humans

## Executive Summary
A new linguistic benchmark of 30 questions was developed to test LLMs' reasoning and comprehension on tasks that are easy for humans. Six models were evaluated across domains like logical reasoning, spatial intelligence, and linguistic understanding. On average, the best model scored only 38%, with the open-source models scoring as low as 20%. Performance improved by 40% when models were allowed to ask clarifying questions first. The results show that even top LLMs struggle with tasks requiring commonsense reasoning and that prompt engineering can partially mitigate these failures.

## Method Summary
The study developed a linguistic benchmark of 30 questions across multiple domains where LLMs have known limitations. Six models (GPT-4 Turbo, Claude 3 Opus, Gemini 1.5 Pro, Mistral Large, Llama 3 70B, Mistral 8x22B) were evaluated using a predefined scoring rubric from April 14-28, 2024. Manual scoring by authors was the primary evaluation method, with automated scoring using GPT-4 Turbo as supplementary material. Performance was measured as average score across questions with 95% confidence intervals, comparing results when models were allowed to ask clarifying questions first.

## Key Results
- Average score across 30 questions: 38% for best model, 20% for open-source models
- Performance improved by 40% when models were allowed to ask clarifying questions first
- All models exhibited non-deterministic behavior even with temperature set to zero
- Models frequently defaulted to solutions for original versions of logic puzzles rather than addressing modified benchmark questions

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLMs struggle with tasks requiring novel reasoning because they overfit to patterns in their training data.
- Mechanism: LLMs memorize and apply solutions to similar problems encountered during training, rather than truly understanding underlying principles.
- Core assumption: Training data contains high frequency of well-known logic puzzles and their solutions.
- Evidence anchors: Models defaulted to original puzzle solutions instead of addressing modified benchmark questions; related research on LLM benchmarking and limitations.
- Break condition: If training data has minimal exposure to specific logic puzzle types or benchmark questions are sufficiently novel.

### Mechanism 2
- Claim: LLMs lack robust commonsense reasoning due to absence of embodied experience.
- Mechanism: Without direct physical world interaction, LLMs cannot develop intuitive understanding of spatial relationships, object permanence, and cause-and-effect.
- Core assumption: Embodied experience is crucial for developing common sense reasoning.
- Evidence anchors: Problems involving visual-spatial skills caused difficulties; philosopher Hubert Dreyfus highlighted embodied experience as crucial for common sense.
- Break condition: If future LLMs develop enhanced capabilities to simulate or learn from embodied experiences.

### Mechanism 3
- Claim: LLMs exhibit non-deterministic behavior even with temperature set to zero.
- Mechanism: Minor variations in output token logits from Sparse Mixture of Experts architectures, GPU non-determinism, or caching mechanisms lead to inconsistent responses.
- Core assumption: Non-deterministic behavior stems from factors beyond simple temperature variations.
- Evidence anchors: Documented minor alterations in output token logits; variations too large to be explained solely by existing hypotheses; potential causes include temperature-agnostic caching and undisclosed model versioning.
- Break condition: If LLM providers implement solutions ensuring output determinism through disabled caching or version control.

## Foundational Learning

- Concept: Probability and statistics
  - Why needed here: Understanding probability-based nature of LLM predictions and implications of non-deterministic behavior.
  - Quick check question: If an LLM has a 70% chance of producing correct answer for a given question, what is probability it produces correct answer at least once in three independent attempts?

- Concept: Logical reasoning and problem-solving
  - Why needed here: Analyzing LLM failure modes and understanding why it struggles with novel reasoning or commonsense knowledge tasks.
  - Quick check question: Given the "wolf, goat, and cabbage" river crossing puzzle, explain logical steps required to solve it and why an LLM might struggle.

- Concept: Machine learning and model training
  - Why needed here: Understanding underlying mechanisms of LLM training including overfitting and how it impacts performance on novel tasks.
  - Quick check question: Explain concept of overfitting in machine learning and how it might lead LLM to provide incorrect answers to modified versions of well-known logic puzzles.

## Architecture Onboarding

- Component map: Data collection -> Model API querying -> Response evaluation -> Result aggregation and analysis
- Critical path:
  1. Collect and curate benchmark questions
  2. Query LLM APIs with questions
  3. Manually score responses using rubric
  4. Aggregate and analyze results
  5. Identify failure modes and generate insights
- Design tradeoffs:
  - Manual vs. automated scoring: Manual provides nuanced evaluation but is time-consuming; automated is faster but may lack accuracy
  - Question selection: Balancing novel, challenging questions with representative real-world tasks
- Failure signatures:
  - Overfitting: Provides answers corresponding to original puzzle versions rather than modified benchmark questions
  - Lack of commonsense reasoning: Struggles with intuitive understanding of spatial relationships or cause-and-effect
  - Non-deterministic behavior: Produces inconsistent responses even with temperature=0
- First 3 experiments:
  1. Run benchmark on diverse set of LLM models to identify common failure modes and compare performance
  2. Introduce clarifying questions to LLM before benchmark questions and measure impact on performance
  3. Modify benchmark questions to be more/less similar to well-known logic puzzles and observe performance changes

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does performance of smaller or quantized LLM models compare to results presented in this study?
- Basis in paper: The paper suggests testing on a sample of smaller LLMs to see if performance correlates with model size.
- Why unresolved: Study focused on specific set of well-known models, leaving performance of smaller or quantized models unexplored.
- What evidence would resolve it: Conducting same benchmark tests on range of smaller or quantized models and comparing performance to results presented.

### Open Question 2
- Question: How would results change if temperature parameter was set above zero for all models?
- Basis in paper: The paper mentions running inference multiple times with temperature set above zero and generating aggregate statistics could be an improvement.
- Why unresolved: Study used temperature of zero to ensure deterministic outputs, which may not reflect models' performance in typical usage scenarios.
- What evidence would resolve it: Running benchmark tests with various temperature settings above zero and comparing results to those obtained with temperature of zero.

### Open Question 3
- Question: Can fine-tuning models with training dataset of perturbed variations of well-known logic-type problems found in training corpora decrease overfitting variance?
- Basis in paper: The paper suggests fine-tuning models with training dataset of perturbed variations of well-known logic-type problems found in training corpora to see if this decreases overfitting variance.
- Why unresolved: Study did not explore potential benefits of fine-tuning models with perturbed variations of common logic problems.
- What evidence would resolve it: Fine-tuning models using dataset of perturbed variations of well-known logic-type problems and comparing performance on benchmark tests to original models.

## Limitations

- Benchmark questions are not fully disclosed, making exact replication challenging
- All models exhibited non-deterministic behavior even with temperature=0, complicating reproducibility
- Primary evaluation method relies on manual scoring by authors, introducing potential subjectivity

## Confidence

- High Confidence: General finding that top LLMs struggle with commonsense reasoning tasks easy for humans (supported by low average scores)
- Medium Confidence: Claim that prompt engineering (clarifying questions) improves performance by 40% (supported but affected by non-deterministic behavior)
- Low Confidence: Exact causes of non-deterministic behavior and whether performance variations stem primarily from this factor versus genuine reasoning limitations

## Next Checks

1. Run same questions against each model 10+ times at temperature=0 to quantify variance in outputs and determine if performance differences are statistically significant after accounting for non-deterministic behavior.

2. Have human subjects (non-ML experts) solve benchmark questions to verify they are indeed "easy for humans" and establish gold standard baseline for comparison with LLM performance.

3. Test same models on established benchmarks like HellaSwag, PIQA, and IOLBENCH to determine if observed limitations are specific to this benchmark or reflect broader weaknesses in commonsense reasoning across multiple evaluation frameworks.