---
ver: rpa2
title: Reverse Training to Nurse the Reversal Curse
arxiv_id: '2403.13799'
source_url: https://arxiv.org/abs/2403.13799
tags:
- reverse
- training
- reversal
- standard
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The reversal curse occurs when LLMs trained on "A has feature B"
  fail to generalize to "B is a feature of A", due to standard left-to-right training.
  This work proposes reverse training, where all words are used twice by reversing
  training strings while preserving substrings like entities.
---

# Reverse Training to Nurse the Reversal Curse

## Quick Facts
- arXiv ID: 2403.13799
- Source URL: https://arxiv.org/abs/2403.13799
- Reference count: 21
- Primary result: Reverse training significantly improves LLM performance on reversal tasks while maintaining or improving standard task performance

## Executive Summary
This paper addresses the "reversal curse" in LLMs where models trained on "A has feature B" fail to generalize to "B is a feature of A". The proposed solution, reverse training, exposes models to bidirectional token sequences by reversing training strings while preserving entities. The approach doubles available training tokens and shows significant improvements: from 0.9% to 3.6% accuracy on parent-to-celebrity tasks with 1.4B models, and from 0.0% to 89.7% on fictitious facts with 7B models. The method maintains performance on standard benchmarks while substantially improving reversal task performance.

## Method Summary
Reverse training constructs reversed samples by reversing training strings while preserving chosen substrings like entities, effectively doubling training data. The LLM is trained on both original and reversed samples using four reversal types: token reversal, word reversal, entity-preserving reversal (REVERSEentity), and random segment reversal (REVERSErand). The approach maintains entity word order during reversal to preserve semantic coherence. Models are evaluated on standard benchmarks and reversal tasks including celebrity biography fields and fictitious fact retrieval.

## Key Results
- Data-matched reverse-trained models outperform standard models on standard tasks
- Compute-matched reverse-trained models show dramatic improvements on reversal tasks
- REVERSEentity achieves perfect accuracy on 5-word entities in symbolic tasks
- Fictitious celebrity parent-to-celebrity accuracy improves from 0.0% to 89.7% with 7B models

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Reverse training mitigates the reversal curse by exposing the model to bidirectional token sequences
- Core assumption: The model can transfer knowledge from reversed sequences to forward generation
- Evidence: Abstract mentions training in both forward and reverse directions; section notes reversed language has similar perplexity difficulty
- Break condition: If the model cannot transfer knowledge from reversed to forward sequences

### Mechanism 2
- Claim: Preserving entity ordering maintains meaningful associations
- Core assumption: Entities are atomic semantic units whose structure shouldn't be disrupted
- Evidence: Abstract specifies preserving substrings like entities; REVERSEentity shows perfect accuracy on 5-word entities
- Break condition: If entities aren't properly detected or preserving them doesn't maintain semantic coherence

### Mechanism 3
- Claim: Random segment reversal provides flexible middle ground
- Core assumption: Model can handle reversed segments of varying sizes while maintaining reversal ability
- Evidence: REVERSErand works when maximum segment length k is long enough; fails when k is smaller than entity name length
- Break condition: If segment size is too small relative to entity length

## Foundational Learning

- Concept: Zipf's law and training data coverage
  - Why needed: Explains why standard training on internet-scale data still fails to cover all fact directions
  - Quick check: If a fact appears 100 times in one direction and once in reverse, what fraction of the time does standard training see the reverse?

- Concept: Autoregressive language modeling and left-to-right prediction
  - Why needed: Understanding why standard training cannot learn bidirectional associations
  - Quick check: In standard autoregressive training, what probability distribution is the model trying to learn?

- Concept: Subword tokenization (BPE) and reversal implications
  - Why needed: Token reversal vs word reversal vs entity-preserving reversal require understanding tokenization effects
  - Quick check: If a word is split into multiple subword tokens, how does token reversal affect the word's internal structure?

## Architecture Onboarding

- Component map: Training data preprocessing -> Reversal transformation module -> LLM training loop -> Evaluation pipeline
- Critical path: Data -> Reverse transformation -> Training -> Evaluation (forward/reverse tasks)
- Design tradeoffs: Entity detection accuracy vs computational cost vs reversal effectiveness
- Failure signatures: No improvement on reverse tasks despite successful forward performance; training perplexity remains high for reversed sequences
- First 3 experiments:
  1. Implement token reversal and test on synthetic symmetric fact dataset
  2. Add entity detection and entity-preserving reversal, compare performance
  3. Test random segment reversal with varying segment sizes on same dataset

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does reverse training scale with model size beyond 1.4B and 7B models?
- Basis: Paper only tests relatively small models
- Why unresolved: Benefits at larger scales unknown
- What evidence: Experiments on 10B+ parameter models

### Open Question 2
- Question: What is the optimal segmentation granularity for random segment reversal?
- Basis: Paper tests various k values (2,3,5,10,25,50) with task-dependent performance
- Why unresolved: No general theory for selecting k
- What evidence: Theoretical framework mapping task characteristics to optimal parameters

### Open Question 3
- Question: Does reverse training improve cross-lingual generalization for reversed facts?
- Basis: Paper focuses on English-only experiments
- Why unresolved: Cross-lingual benefits untested
- What evidence: Experiments on bilingual/multilingual datasets

### Open Question 4
- Question: What is the relationship between reverse training and model interpretability for knowledge retrieval?
- Basis: Paper notes joint storage of knowledge across attributes but doesn't investigate internal changes
- Why unresolved: Effects on knowledge representations unexplored
- What evidence: Probing experiments on attention patterns and activation spaces

## Limitations
- Theoretical justification for why reverse training works is minimal and relies on intuition
- Corpus relevance signals moderate (0.486 average neighbor FMR) with no citations, suggesting limited connection to broader literature
- Optimal segmentation granularity appears task-dependent with no systematic selection method provided

## Confidence
- High Confidence: Experimental methodology is sound with controlled data-matched and compute-matched comparisons
- Medium Confidence: Bidirectional learning mechanism is plausible but not rigorously proven
- Low Confidence: Entity-preserving reversal superiority lacks strong empirical support beyond specific cases

## Next Checks
1. Ablation study on entity detection quality: Systematically vary entity detection accuracy and measure reversal performance to test necessity of entity-preserving mechanism
2. Theoretical analysis of training dynamics: Use reversal curse framework to analyze how reverse training changes loss landscape and gradient flow
3. Cross-dataset generalization test: Evaluate reverse-trained models on reversal tasks from completely different domains to test general bidirectional reasoning capabilities