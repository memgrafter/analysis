---
ver: rpa2
title: Refining Answer Distributions for Improved Large Language Model Reasoning
arxiv_id: '2412.13292'
source_url: https://arxiv.org/abs/2412.13292
tags:
- answer
- reasoning
- answers
- llms
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Refined Answer Distributions (RAD), a novel
  iterative method to improve the reasoning accuracy of large language models. Unlike
  existing techniques that refine single answers, RAD maintains and updates a distribution
  over possible answers, using marginalization to assign higher weight to promising
  candidates across iterations.
---

# Refining Answer Distributions for Improved Large Language Model Reasoning

## Quick Facts
- arXiv ID: 2412.13292
- Source URL: https://arxiv.org/abs/2412.13292
- Reference count: 26
- One-line primary result: RAD improves LLM reasoning accuracy by 2.8%-10.6% across multiple models and datasets

## Executive Summary
This paper introduces Refined Answer Distributions (RAD), an iterative method that improves reasoning accuracy by maintaining and refining distributions over possible answers rather than single answers. Unlike Self-Consistency which samples independent chains-of-thought, RAD iteratively refines answer probabilities through marginalization, assigning higher weight to promising candidates. The method starts with an initial answer distribution (from Chain-of-Thought or Progressive Hint Prompting) and updates it by conditioning on previously sampled answers. Experiments on six arithmetic benchmarks and two BIG-Bench Hard tasks using GPT-3.5 Turbo, GPT-4 Turbo, GPT-4o-mini, and Llama models show consistent accuracy improvements over state-of-the-art baselines.

## Method Summary
RAD is an iterative Monte Carlo approximation technique for refining answer distributions in LLM reasoning. Starting with an initial distribution p1(˜y|x) from CoT+SC or PHP+SC, the algorithm samples answers, refines each unique answer using a hint-based prompt, and updates the distribution via marginalization. Each iteration collects ⌊Br/M⌋ samples per unique answer, applies refinement prompts with hints like "The answer is near to ym", and updates probabilities using the refined distributions. The process continues until a stopping criterion is met (fixed iterations, convergence, or budget exhaustion). The core mechanism relies on probability flow: if p(correct | Refine(correct)) is high and p(correct | Refine(incorrect)) > p(correct), the correct answer probability increases through marginalization.

## Key Results
- RAD achieves 2.8%-10.6% accuracy improvements across six arithmetic benchmarks and two BIG-Bench Hard tasks
- Consistent improvements observed across GPT-3.5 Turbo, GPT-4 Turbo, GPT-4o-mini, and Llama models
- RAD outperforms Self-Consistency and Progressive Hint Prompting baselines while making more efficient use of LLM calls
- Higher probability assigned to correct answers for majority of difficult questions, even with weaker models like Llama-3-8b-instruct

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Iterative marginalization over answer distributions improves the probability of the correct answer.
- **Mechanism:** RAD maintains a distribution over answers, refining it iteratively by conditioning on previous answers and updating probabilities via marginalization. Each refinement step increases the probability of the correct answer if the "probability flow" condition is met (flow into the correct answer exceeds flow out).
- **Core assumption:** The LLM can recognize the correct answer more reliably when presented with it (i.e., p(correct | Refine(correct)) is high) and can often identify incorrect answers (i.e., p(correct | Refine(incorrect)) > p(correct)).
- **Evidence anchors:** Binary example shows probability increasing from 0.4 to 0.56 through refinement; abstract mentions identifying the mode of answer distribution.
- **Break condition:** If the LLM cannot reliably identify correct answers when presented with them (i.e., p(correct | Refine(correct)) is low), or if it is equally likely to stick with incorrect answers (i.e., p(correct | Refine(incorrect)) ≈ p(correct)).

### Mechanism 2
- **Claim:** Maintaining a distribution over answers reduces sampling variance and makes more efficient use of LLM calls compared to Self-Consistency.
- **Mechanism:** Instead of sampling multiple independent chains-of-thought and taking a majority vote (Self-Consistency), RAD iteratively refines the answer distribution, focusing computational budget on promising candidates.
- **Core assumption:** The distribution of answers has structure that can be exploited through iterative refinement, and this structure is more informative than treating each sample independently.
- **Evidence anchors:** Abstract states "By maintaining a distribution, we reduce sampling variance and make more efficient usage of the LLM calls."
- **Break condition:** If the initial answer distribution is too flat (uniform) or if the LLM's refinement capability is too weak to create meaningful probability shifts.

### Mechanism 3
- **Claim:** RAD can be combined with different prompting strategies (CoT, PHP) and still improve reasoning accuracy.
- **Mechanism:** RAD is agnostic to the initialization method for the answer distribution. It can start with CoT+SC, PHP+SC, or other answer aggregation methods, then iteratively refine.
- **Core assumption:** The refinement mechanism works independently of how the initial distribution is generated, as long as the refinement strategy satisfies the probability flow condition.
- **Evidence anchors:** Abstract mentions RAD is "orthogonal to prompt engineering approaches"; implementation uses hint-based prompting from Zheng et al. (2023).
- **Break condition:** If the refinement strategy is incompatible with the initialization method (e.g., if PHP hints are too vague to provide meaningful refinement).

## Foundational Learning

- **Concept:** Monte Carlo approximation of distributions
  - Why needed here: RAD approximates intractable answer distributions through sampling, requiring understanding of Monte Carlo methods.
  - Quick check question: How does the law of large numbers justify using sampled answers to approximate a distribution?

- **Concept:** Probability flow and marginalization
  - Why needed here: The core mechanism relies on understanding how probability mass flows between answers during refinement.
  - Quick check question: In the binary example, why does the probability of the correct answer increase after refinement?

- **Concept:** Chain-of-Thought reasoning and answer aggregation
  - Why needed here: RAD builds on CoT prompting and answer aggregation methods like Self-Consistency.
  - Quick check question: What is the key difference between Self-Consistency and RAD in how they use multiple LLM responses?

## Architecture Onboarding

- **Component map:** Initialization module -> Sampling module -> Refinement module -> Marginalization module -> Stopping criteria module

- **Critical path:**
  1. Initialize answer distribution p1(˜y|x)
  2. For each iteration r:
     - Identify unique answers {ym} from pr(˜y|x)
     - For each unique answer ym:
       - Apply refinement prompt with hint "The answer is near to ym"
       - Sample ⌊Br+1/M⌋ answers to approximate p(˜y|x, Refine(ym))
     - Update distribution using marginalization to get pr+1(˜y|x)
     - Check stopping criteria

- **Design tradeoffs:**
  - Budget allocation: Equal budget per unique answer vs. adaptive allocation based on answer probability
  - Refinement granularity: Using full answers vs. using only answer values as hints
  - Stopping criteria: Fixed iterations vs. convergence-based vs. budget-based

- **Failure signatures:**
  - No improvement across iterations: Indicates weak refinement capability or poor initial distribution
  - Oscillating probabilities: May indicate over-refinement or contradictory hints
  - Budget exhaustion without convergence: Suggests need for more efficient allocation or better stopping criteria

- **First 3 experiments:**
  1. Implement basic RAD with fixed budget allocation and CoT initialization on AddSub dataset
  2. Compare RAD with Self-Consistency on MultiArith to measure efficiency gains
  3. Test RAD with PHP initialization on GSM8K to verify prompt-agnostic capability

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How does the effectiveness of Refined Answer Distributions (RAD) vary across different types of reasoning tasks beyond quantitative reasoning, such as verbal reasoning or logical deduction?
- **Basis in paper:** Explicit - The paper mentions the potential to extend RAD to tasks requiring verbal responses or logical deduction by using appropriate answer refinement strategies.
- **Why unresolved:** The experiments in the paper primarily focus on arithmetic and mathematical reasoning tasks. There is no empirical evidence or theoretical analysis of RAD's performance on other types of reasoning tasks.
- **What evidence would resolve it:** Conducting experiments on a diverse set of reasoning tasks (e.g., verbal reasoning, logical deduction, commonsense reasoning) and comparing RAD's performance to baselines would provide evidence of its generalizability.

### Open Question 2
- **Question:** What is the optimal strategy for allocating LLM calls non-uniformly to different answers during the refinement process, and how does it impact the efficiency and effectiveness of RAD?
- **Basis in paper:** Explicit - The paper mentions that the current version of RAD assigns the same number of LLM calls to each unique answer from the previous round, but suggests that investigating more efficient strategies for non-uniform allocation could be a worthwhile direction.
- **Why unresolved:** The paper does not provide any analysis or experimental results on the impact of different allocation strategies on RAD's performance. It is unclear whether allocating more calls to certain answers (e.g., those with higher probabilities) would improve efficiency or effectiveness.
- **What evidence would resolve it:** Conducting experiments with different allocation strategies (e.g., proportional to probability, inversely proportional to probability, based on answer diversity) and analyzing their impact on RAD's accuracy, sample efficiency, and computational cost would provide insights into the optimal strategy.

### Open Question 3
- **Question:** How sensitive is RAD to the choice of hyperparameters, such as the number of iterations, the sampling budget, and the initial prompt, and what are the best practices for tuning these parameters?
- **Basis in paper:** Inferred - The paper mentions that RAD can be combined with different prompting methods for initialization and that it is not overly sensitive to the choice of hyperparameters. However, it does not provide a systematic analysis of the impact of these parameters on RAD's performance.
- **Why unresolved:** The paper does not provide any guidelines or recommendations for choosing the optimal values of hyperparameters. It is unclear how sensitive RAD is to these choices and what factors should be considered when tuning them.
- **What evidence would resolve it:** Conducting a comprehensive sensitivity analysis of RAD's performance to different hyperparameters (e.g., number of iterations, sampling budget, initial prompt) and identifying the factors that influence their optimal values would provide insights into best practices for tuning RAD.

## Limitations
- Experimental focus primarily on arithmetic reasoning tasks limits generalizability to other reasoning domains
- Effectiveness depends on LLM's ability to recognize correct answers when presented with them, which may vary across models
- The hint-based refinement strategy from Zheng et al. (2023) is only partially specified in the paper

## Confidence

**High Confidence**: The marginalization mechanism for updating answer distributions is mathematically sound and clearly explained. The iterative refinement process is well-specified with clear pseudocode and algorithmic steps.

**Medium Confidence**: The empirical results showing RAD outperforming baselines across multiple models and datasets are robust, but the relative improvements are modest. The claim that RAD reduces sampling variance compared to Self-Consistency is supported by theoretical reasoning but would benefit from direct empirical validation.

**Low Confidence**: The generalization of RAD beyond arithmetic reasoning tasks remains uncertain. The effectiveness of the hint-based refinement strategy across different model families and reasoning domains is not fully established.

## Next Checks

1. **Distribution Convergence Analysis**: Track how answer probabilities evolve across RAD iterations for both correct and incorrect answers. Specifically, measure whether p3(y|x) ≥ p1(y|x) for correct answers and whether incorrect answers are effectively suppressed. This would validate the core probability flow mechanism.

2. **Budget Efficiency Validation**: Compare RAD's accuracy with varying total budgets (e.g., 20, 40, 80 LLM calls) against Self-Consistency baselines. Measure the accuracy gain per LLM call to quantify the claimed efficiency improvements. Include a direct comparison of sampling variance between RAD and SC at equivalent computational budgets.

3. **Cross-Domain Generalization**: Apply RAD to non-mathematical reasoning tasks from BIG-Bench or other reasoning datasets (e.g., commonsense reasoning, logical inference). Measure whether the same probability flow mechanism holds when answers aren't numerical values, and whether the hint-based refinement strategy remains effective without clear "near to" relationships between answers.