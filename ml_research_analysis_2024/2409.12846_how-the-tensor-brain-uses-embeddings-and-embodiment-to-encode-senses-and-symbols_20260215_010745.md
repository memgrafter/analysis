---
ver: rpa2
title: How the (Tensor-) Brain uses Embeddings and Embodiment to Encode Senses and
  Symbols
arxiv_id: '2409.12846'
source_url: https://arxiv.org/abs/2409.12846
tags:
- memory
- brain
- layer
- sparky
- episodic
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'The Tensor Brain (TB) model addresses the problem of integrating
  symbolic and subsymbolic processing in perception, memory, and semantic understanding.
  The core method uses a two-layer architecture: a representation layer modeling the
  brain''s global workspace (cognitive brain state) and an index layer containing
  symbolic concepts, predicates, and episodic indices.'
---

# How the (Tensor-) Brain uses Embeddings and Embodiment to Encode Senses and Symbols

## Quick Facts
- arXiv ID: 2409.12846
- Source URL: https://arxiv.org/abs/2409.12846
- Reference count: 40
- The Tensor Brain (TB) model achieves unified perception, episodic memory, and semantic memory processing through a two-layer architecture with embedding vectors as connection weights.

## Executive Summary
The Tensor Brain (TB) model addresses the challenge of integrating symbolic and subsymbolic processing in perception, memory, and semantic understanding. It employs a two-layer architecture consisting of a representation layer (modeling the brain's global workspace) and an index layer (containing symbolic concepts, predicates, and episodic indices). The model uses embedding vectors as connection weights between layers, enabling both bottom-up symbolic encoding from perception to symbols and top-down symbolic decoding from symbols to embodied representations.

The TB demonstrates unified processing of perception, episodic memory, and semantic memory through self-supervised learning with self-generated labels. This bootstrap learning mechanism generates labels from the cognitive state and feeds them back to update the representation layer. The approach naturally handles multimodality and supports both embedded and symbolic reasoning through chaining and materialization operations, providing a computational framework that connects to cognitive neuroscience theories of consciousness and memory.

## Method Summary
The Tensor Brain employs a two-layer architecture where the representation layer acts as a global workspace capturing the cognitive brain state, while the index layer contains symbolic concepts, predicates, and episodic indices. Embedding vectors serve as connection weights between these layers, enabling bidirectional information flow. The model uses self-supervised learning where perception itself generates symbolic labels through stochastic sampling, which are then treated as actual training targets. This bootstrap learning adapts embedding vectors to integrate perceptual experiences, episodic memories, and semantic relationships. The system also employs attention mechanisms and sampling as complementary approaches to symbolic interpretation, with attention providing fast holistic processing and sampling enabling symbolic commitment and exploration of joint dependencies.

## Key Results
- The TB achieves unified perception, episodic memory, and semantic memory processing through a two-layer architecture with embedding vectors as connection weights.
- Self-supervised learning with self-generated labels enables the TB to learn without external training data through bootstrap learning mechanisms.
- Attention mechanisms and sampling provide complementary approaches to symbolic interpretation, with attention enabling parallel holistic processing and sampling enabling serial focused processing.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The Tensor Brain (TB) achieves unified perception, episodic memory, and semantic memory processing through a two-layer architecture with embedding vectors as connection weights.
- Mechanism: The representation layer acts as a global workspace capturing the cognitive brain state (CBS), while the index layer contains symbolic concepts, predicates, and episodic indices. Embedding vectors connect these layers bidirectionally, enabling both bottom-up symbolic encoding and top-down symbolic decoding.
- Core assumption: Symbols are measurement outcomes from sensory input that feed back to influence processing layers through embodiment.
- Evidence anchors:
  - [abstract] "The model employs embedding vectors as connection weights between layers, enabling both bottom-up symbolic encoding (from perception to symbols) and top-down symbolic decoding (from symbols to embodied representations)."
  - [section 3.1] "The CBS is the activation vector ⃗γ of the representation layer."
  - [section 4] "The post-activation γi can be interpreted as a Bernoulli parameter, i.e., P(Xi = 1) = γi."
- Break condition: If the representation layer cannot maintain coherent activation patterns across different sensory modalities or if embedding vectors fail to capture sufficient semantic relationships.

### Mechanism 2
- Claim: Self-supervised learning with self-generated labels enables the TB to learn without external training data.
- Mechanism: During perception, the TB samples from label distributions and treats generated labels as actual training targets. This bootstrap learning adapts embedding vectors to integrate perceptual experiences, episodic memories, and semantic relationships.
- Core assumption: The brain can generate meaningful symbolic labels from subsymbolic representations without external supervision.
- Evidence anchors:
  - [abstract] "The model achieves this through a sampling mechanism that generates labels from the cognitive state and feeds them back to update the representation layer."
  - [section 8.3] "No external agent provides training data, i.e., symbolic labels. In the TB, perception itself produces labels by stochastic sampling and the brain then treats the generated labels as actual labels and trains on them."
  - [section 8.2] "This is a type of bootstrap learning where model predictions are used as targets for learning."
- Break condition: If generated labels become inconsistent or fail to capture meaningful patterns in the data, or if the sampling mechanism becomes trapped in local optima.

### Mechanism 3
- Claim: Attention mechanisms and sampling provide complementary approaches to symbolic interpretation.
- Mechanism: Attention provides fast, parallel holistic consideration of all interpretations by weighting embedding vectors by probabilities. Sampling commits to specific interpretations and explores joint dependencies between labels through sequential activation.
- Core assumption: Both parallel holistic processing and serial focused processing are needed for complete cognitive function.
- Evidence anchors:
  - [section 6.4] "Thus, one obtains an update of the CBS, even without sampling but by rather considering label probabilities. Note that the equation is a form of an attention mechanism with a skip connection from generative AI."
  - [section 6.5] "Sampling commits the brain to a specific interpretation. For example, if the black thing in the scene is sampled to be Sparky, then the brain can add a lot of background information about Sparky."
  - [section 7.4.2] "For semantic memory recall for a symbol s, e.g., Sparky... We start with Algorithm 3 with a neutral input q = 0 and where we set k ← s and do not sample."
- Break condition: If either mechanism becomes computationally prohibitive or if the combination fails to produce coherent cognitive states.

## Foundational Learning

- Concept: Recurrent Neural Networks (RNNs) with skip connections
  - Why needed here: The representation layer uses RNN dynamics with skip connections to model temporal dependencies and maintain stable activation patterns across time steps.
  - Quick check question: What is the purpose of the skip connection term q(τ-1) in the recurrence equation q(τ)i ← q(τ-1)i + gi(v(τ)) + fNN i(⃗γ(τ-1))?

- Concept: Embeddings as distributed representations
  - Why needed here: Embedding vectors encode semantic relationships between concepts and enable transfer of information between subsymbolic and symbolic processing layers.
  - Quick check question: How does the TB use embedding vectors differently from standard word embedding approaches in NLP?

- Concept: Probabilistic graphical models and sampling
  - Why needed here: The probabilistic interpretation of the cognitive brain state enables sampling-based inference and provides a bridge between deterministic neural network operations and stochastic cognitive processes.
  - Quick check question: What is the relationship between the Bernoulli interpretation of activations and the sampling mechanism in the TB?

## Architecture Onboarding

- Component map:
  - Representation layer: Global workspace model with n ensembles, implements cognitive brain state through post-activations γi
  - Index layer: Symbolic concepts, predicates, and episodic indices with ensemble-based representations
  - Embedding vectors: Connection weights between representation and index layers, serve as concept signatures
  - Evolution neural network: Recurrent dynamics with skip connections and hidden layer for temporal processing
  - Visual processing pipeline: Deep convolutional network mapping input to representation layer

- Critical path: Input → Visual processing → Representation layer activation → Index layer sampling → Top-down embedding activation → Embodiment → Earlier processing layers

- Design tradeoffs:
  - Parallel attention vs serial sampling: Attention provides holistic processing but no symbolic commitment; sampling enables symbolic reasoning but is sequential
  - Sparse vs dense embeddings: Sparse embeddings reduce computational load but may miss semantic relationships; dense embeddings capture more relationships but increase complexity
  - Temperature scaling: Low temperature for focused decision-making vs high temperature for exploration and creativity

- Failure signatures:
  - Catastrophic forgetting: If episodic memories overwrite important semantic relationships during learning
  - Label inconsistency: If self-generated labels become contradictory or fail to converge
  - Bottleneck congestion: If the representation layer becomes saturated and cannot encode complex scenes
  - Sampling collapse: If the sampling mechanism consistently selects only high-probability labels, reducing diversity

- First 3 experiments:
  1. Implement basic perception with scene classification: Feed visual scenes through the convolutional network to the representation layer, sample entity and attribute labels, verify bottom-up encoding works
  2. Test episodic memory recall: Store scene embeddings with episodic indices, activate episodic indices to reconstruct scenes, verify top-down decoding and embodiment
  3. Evaluate semantic memory chaining: Activate concept indices, verify sequential sampling produces chained relationships (e.g., Sparky → Dog → Mammal), test embedded symbolic reasoning

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the Tensor Brain model account for the apparent capacity limit of working memory (approximately seven items) observed in human cognition?
- Basis in paper: [explicit] The paper discusses cognitive control and working memory in Section 9.2, suggesting that cognitive control might consider seven or fewer episodic or concept indices and their embeddings in problem-solving, but does not provide a detailed explanation for the specific capacity limit.
- Why unresolved: The paper acknowledges the capacity limit but does not explain the underlying mechanism or why this specific number appears to be the limit.
- What evidence would resolve it: Computational simulations showing how the TB architecture naturally leads to a capacity limit of approximately seven items, or empirical data from neuroimaging studies demonstrating the relationship between TB component activation and working memory performance.

### Open Question 2
- Question: What is the precise neurobiological implementation of the sampling mechanism in the Tensor Brain model, and how does it relate to quantum measurement theory?
- Basis in paper: [explicit] The paper explicitly draws parallels between the sampling mechanism and quantum measurement collapse in Section 6.4 and mentions that these questions are addressed in reference [42], but does not provide a complete explanation.
- Why unresolved: The paper acknowledges the similarity to quantum measurement but does not provide a detailed neurobiological mechanism or experimental evidence for how sampling occurs in the brain.
- What evidence would resolve it: Neurobiological evidence showing how specific neural circuits implement the sampling process, or quantum coherence measurements in neural tissue during decision-making tasks that demonstrate quantum-like behavior.

### Open Question 3
- Question: How does the Tensor Brain model handle probabilistic reasoning and decision-making when dealing with non-IID (independent and identically distributed) labels and limited sample sizes?
- Basis in paper: [explicit] The paper discusses this in Section 7.4.3, noting that "Probabilities are tricky, also for the brain" and mentions issues with non-IID labels, limited sample size, and the authority/uncertainty of the source, but does not provide a complete solution.
- Why unresolved: The paper acknowledges the challenges of probabilistic reasoning in the brain but does not provide a detailed algorithm or mechanism for how the brain deals with these complexities.
- What evidence would resolve it: Computational models demonstrating how the TB architecture can perform Bayesian inference with non-IID data and limited samples, or behavioral experiments showing how humans perform probabilistic reasoning under these conditions compared to TB predictions.

## Limitations

- The paper lacks detailed implementation specifications for critical components, particularly the "evolution neural network" with hidden layers and skip connections.
- Computational complexity and scalability limitations of the sampling mechanism versus attention approaches are not quantitatively analyzed.
- The model's performance on real-world data is not demonstrated with quantitative metrics, baseline comparisons, or error analysis.

## Confidence

**High Confidence**: The basic two-layer architecture with representation and index layers, and the bidirectional information flow through embedding vectors as connection weights. This core architectural concept is clearly specified and internally consistent.

**Medium Confidence**: The self-supervised learning mechanism with self-generated labels. While the general concept is well-articulated, the specific implementation details and update rules remain underspecified, making it difficult to assess the mechanism's practical viability.

**Low Confidence**: The computational efficiency and scalability claims, particularly regarding the sampling mechanism versus attention approaches. Without quantitative analysis or empirical validation, these claims remain theoretical.

## Next Checks

1. **Implementation Fidelity Check**: Implement the basic two-layer architecture with simple synthetic data to verify that bottom-up encoding (sensory input to symbolic labels) and top-down decoding (symbolic activation to embodied representations) function as described. This would test the core architectural claims without requiring full self-supervised learning implementation.

2. **Label Generation Consistency Test**: Create a controlled experiment where the model generates labels from known input patterns, then tracks whether these self-generated labels remain consistent across multiple iterations. This would validate the bootstrap learning mechanism and identify potential label inconsistency failure modes.

3. **Scalability Analysis**: Measure the computational complexity of the sampling mechanism as the number of concepts and predicates increases. Compare this with the attention mechanism's computational requirements to empirically validate the claimed complementary benefits and identify potential bottlenecks in large-scale applications.