---
ver: rpa2
title: Comparative Study on the Performance of Categorical Variable Encoders in Classification
  and Regression Tasks
arxiv_id: '2401.09682'
source_url: https://arxiv.org/abs/2401.09682
tags:
- encoder
- performance
- encoders
- datasets
- categorical
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper provides theoretical and empirical analysis on the performance
  of categorical variable encoders in classification and regression tasks. It proves
  that the one-hot encoder is the best choice for ATI models (e.g.
---

# Comparative Study on the Performance of Categorical Variable Encoders in Classification and Regression Tasks

## Quick Facts
- arXiv ID: 2401.09682
- Source URL: https://arxiv.org/abs/2401.09682
- Reference count: 40
- Primary result: One-hot encoder is best for ATI models (e.g., MLPNN) with sufficient data, while target encoders excel for tree-based models under the same condition; GLMM encoder is most robust for ATI models with insufficient data, and MinHash is a viable alternative for tree-based models.

## Executive Summary
This paper provides a comprehensive analysis of categorical variable encoders for machine learning tasks, examining both theoretical foundations and empirical performance across 28 datasets and 8 model types. The study establishes that one-hot encoding is theoretically optimal for affine transformation-based models (ATI) as it can mimic any other encoding through learned weights, while target encoders preserve optimal splits in decision trees. The research also identifies data sufficiency (measured by average samples per level) as a critical factor in encoder selection, with different recommendations for sufficient versus insufficient data scenarios.

## Method Summary
The study conducts comprehensive experiments using 28 datasets (15 binary classification and 13 regression tasks) with 8 machine learning models (4 ATI models including logistic regression, SVM, MLPNN, and GBDT; 4 tree-based models including Random Forest, XGBoost, and LightGBM) and 14 different categorical encoders. The experimental procedure involves preprocessing datasets, splitting them into 80% training and 20% test sets, training each model-encoder combination with default hyperparameters, and evaluating performance using F1 score for classification tasks and RMSE for regression tasks. Results are averaged over 10 repetitions with different random seeds, and time costs for encoding and model training are also recorded.

## Key Results
- One-hot encoder performs best for ATI models when data is sufficient (high ASPL), while target encoders excel for tree-based models under the same condition
- When data is insufficient (low ASPL), GLMM encoder achieves more robust performance for ATI models, and MinHash encoder is a viable alternative for tree-based models
- Performance differences between encoders converge to zero as ASPL increases, making one-hot encoding increasingly advantageous for ATI models

## Why This Works (Mechanism)

### Mechanism 1
- Claim: One-hot encoding can mimic any other encoder for ATI models when data is sufficient
- Mechanism: The affine transformation in ATI models allows learned weights to replicate the effect of any encoder mapping, since the one-hot representation spans the input space and the model can adjust weights accordingly
- Core assumption: Sufficient data per categorical level exists so that the model can accurately learn the required weights
- Evidence anchors: [abstract]: "the one-hot encoder is the best choice for ATI models in the sense that it can mimic any other encoders by learning suitable weights from the data"; [section II.A]: "For a categorical variable V = {v1, v2, ..., vc} and any encoder ϕ : V → Rl... there exists an equivalent linear transformation h(x) = W OHϕOH(x)"

### Mechanism 2
- Claim: Target encoders perform well for tree-based models when data is sufficient
- Mechanism: Target encoders preserve optimal splits in decision trees by encoding levels with values close to the conditional mean, maintaining contiguous partitions that minimize impurity measures
- Core assumption: The conditional distribution of the target variable is similar across nodes for splits not involving the encoded variable
- Evidence anchors: [abstract]: "we explain why the target encoder and its variants are the most suitable encoders for tree-based models"; [section III.A]: "Mean Encoder will preserve the optimal split at root node when impurity is measured by mean squared error (MSE)" and "Mean Encoder will preserve the optimal split at root node when impurity is measured by entropy"

### Mechanism 3
- Claim: Performance differences between encoders converge to zero as ASPL increases
- Mechanism: With more samples per level, the empirical estimates of conditional distributions and model parameters become more accurate, reducing the gap between the best encoder and alternatives like one-hot or target encoders
- Core assumption: The true conditional distributions are stable and can be estimated with sufficient data
- Evidence anchors: [abstract]: "one-hot encoder performs best for ATI models when data is sufficient, while target encoders excel for tree-based models under the same condition"; [section II.B]: "As ASPL increases, the average performance of the one-hot encoder converges towards the best encoder"; [section III.B]: "the performance of the Mean Encoder drops as ASPL decreases... When ASPL is sufficiently large... the Mean encoder performs very well"

## Foundational Learning

- Concept: Affine transformations in machine learning models
  - Why needed here: Understanding how ATI models process inputs is key to explaining why one-hot encoding can replicate other encoders
  - Quick check question: Can you write the mathematical form of an affine transformation applied to an input vector in a neural network layer?

- Concept: Decision tree splitting criteria and impurity measures
  - Why needed here: Necessary to understand why target encoders preserve optimal splits in tree-based models
  - Quick check question: What are the common impurity measures used in decision tree algorithms, and how do they relate to the target variable?

- Concept: Conditional expectation and empirical estimation
  - Why needed here: Core to understanding how target encoders work and their reliance on data sufficiency
  - Quick check question: How does the law of large numbers justify using sample means to estimate conditional expectations when ASPL is large?

## Architecture Onboarding

- Component map: Data preprocessing (handling missing values, encoding categorical variables) -> Model training (various ATI and tree-based models) -> Evaluation (performance metrics across datasets)
- Critical path: For a new dataset, first determine ASPL to assess data sufficiency. Then select model type (ATI or tree-based) to guide encoder choice. Apply the selected encoder, train the model, and evaluate performance.
- Design tradeoffs: One-hot encoding introduces high dimensionality, which can increase training time and memory usage, especially for high-cardinality features. Target encoders are more compact but require sufficient data per level to avoid overfitting.
- Failure signatures: Poor performance with one-hot encoding despite sufficient data may indicate model complexity issues or improper hyperparameter tuning. Target encoder failure with low ASPL manifests as unstable or inaccurate splits in tree models.
- First 3 experiments:
  1. Test one-hot encoding on an ATI model (e.g., logistic regression) with a dataset having high ASPL to verify convergence to best encoder performance
  2. Apply a target encoder (e.g., Mean Encoder) to a tree-based model (e.g., Random Forest) with high ASPL and observe split quality
  3. Repeat experiment 1 with a low ASPL dataset to observe the breakdown in one-hot encoding performance and compare with alternative encoders like GLMM

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of categorical encoders vary when applied to ensemble models that combine ATI and tree-based components, such as gradient boosting with neural network base learners?
- Basis in paper: [inferred] The paper systematically compares encoders across pure ATI and pure tree-based models, but does not address hybrid architectures that combine both paradigms
- Why unresolved: The paper's theoretical analysis focuses on the affine transformation properties of ATI models and the split optimization properties of tree-based models, without considering how these properties interact in ensemble methods that combine both
- What evidence would resolve it: Empirical results comparing the performance of various encoders on ensemble models that incorporate both ATI and tree-based components, across datasets with different characteristics

### Open Question 2
- Question: What is the impact of categorical variable cardinality distribution across levels (e.g., power-law vs uniform) on the effectiveness of different encoders, beyond just the average samples per level metric?
- Basis in paper: [inferred] The paper uses average samples per level (ASPL) as the primary metric for data sufficiency, but doesn't explore how the distribution of samples across levels affects encoder performance
- Why unresolved: The theoretical analysis assumes uniform distribution of samples across levels, and empirical results are aggregated without examining the impact of different cardinality distributions on encoder effectiveness
- What evidence would resolve it: Comparative experiments showing encoder performance across datasets with varying cardinality distributions (uniform, power-law, bimodal) while controlling for total sample count and ASPL

### Open Question 3
- Question: How do different categorical encoders affect the interpretability of machine learning models, particularly in terms of feature importance and decision boundaries?
- Basis in paper: [explicit] The paper mentions that contrast encoders are "usually used in statistics" and "less frequently used in machine learning," suggesting an awareness of interpretability considerations
- Why unresolved: The experimental evaluation focuses solely on predictive performance metrics (F1 score, RMSE) without examining how different encoders impact model interpretability or the ability to understand model decisions
- What evidence would resolve it: Analysis of feature importance rankings, decision boundary visualizations, and interpretability metrics across different encoder-model combinations to quantify the trade-off between performance and interpretability

## Limitations
- The study's conclusions rely heavily on empirical validation without rigorous statistical significance testing for performance differences between encoders
- Theoretical claims about one-hot encoding's ability to mimic any encoder assume linear transformations in ATI models, which may not hold for more complex architectures
- The study does not extensively address high-cardinality categorical variables, where one-hot encoding's dimensionality explosion can be problematic

## Confidence
- **High Confidence**: The experimental methodology is sound, with proper dataset splitting, repeated trials, and comprehensive coverage of encoder-model combinations
- **Medium Confidence**: The theoretical mechanisms explaining why certain encoders work best for specific model types are plausible but rely on assumptions that may not always hold in practice
- **Low Confidence**: The study does not provide robust statistical tests for performance differences between encoders, making it difficult to determine if observed differences are significant or due to random variation

## Next Checks
1. Apply appropriate statistical tests (e.g., paired t-tests or Wilcoxon signed-rank tests) to determine if performance differences between encoders are statistically significant across datasets
2. Conduct experiments specifically focusing on high-cardinality categorical variables to assess how one-hot encoding's dimensionality impact affects performance and whether target encoders remain superior for tree-based models
3. Perform cross-validation where encoders are selected based on performance on a subset of datasets and tested on held-out datasets to evaluate the generalizability of the proposed encoder selection guidelines