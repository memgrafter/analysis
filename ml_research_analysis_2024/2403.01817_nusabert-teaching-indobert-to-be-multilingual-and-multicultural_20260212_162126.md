---
ver: rpa2
title: 'NusaBERT: Teaching IndoBERT to be Multilingual and Multicultural'
arxiv_id: '2403.01817'
source_url: https://arxiv.org/abs/2403.01817
tags:
- languages
- language
- nusabert
- indonesian
- tasks
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: NusaBERT enhances IndoBERT through vocabulary expansion and continued
  pre-training on a multilingual corpus covering Indonesian and 12 regional languages.
  The extended vocabulary introduces 1,511 new tokens, and the model is pre-trained
  for 500,000 steps on 16B tokens.
---

# NusaBERT: Teaching IndoBERT to be Multilingual and Multicultural

## Quick Facts
- arXiv ID: 2403.01817
- Source URL: https://arxiv.org/abs/2403.01817
- Reference count: 14
- Primary result: State-of-the-art multilingual NLU performance across Indonesian and 12 regional languages

## Executive Summary
NusaBERT enhances IndoBERT through vocabulary expansion and continued pre-training on a multilingual corpus covering Indonesian and 12 regional languages. The model adds 1,511 new tokens to improve handling of regional language text and code-mixed scenarios. Pre-trained for 500,000 steps on 16B tokens, NusaBERT demonstrates significant improvements on multilingual benchmarks while maintaining strong Indonesian NLU performance. The approach particularly benefits sequence labeling tasks with 2-3% average improvement and shows robust performance across diverse Indonesian languages.

## Method Summary
NusaBERT builds upon IndoBERT by expanding the tokenizer vocabulary from 30,521 to 32,032 tokens, adding 1,511 regional language-specific tokens. The model undergoes continued pre-training using RoBERTa-style MLM objective for 500,000 steps on a combined corpus of CulturaX, Wikipedia dumps, and KoPI-NLLB covering 13 languages. The pre-training uses bfloat16 precision, AdamW optimizer, and sequence length 128. Fine-tuning is performed on IndoNLU, NusaX, and NusaWrites benchmarks using task-specific hyperparameters from the respective benchmarks.

## Key Results
- State-of-the-art performance on multilingual NLU tasks across Indonesian and 12 regional languages
- 2-3% average improvement in sequence labeling tasks compared to IndoBERT
- Improved robustness to code-mixed text while maintaining strong Indonesian language performance

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Vocabulary expansion with 1,511 new tokens improves model robustness to code-mixed and regional language text.
- Mechanism: The extended tokenizer incorporates regional language-specific tokens, allowing the model to recognize and process words that were previously tokenized incorrectly or unseen, thus improving semantic understanding.
- Core assumption: Adding tokens representing regional language vocabulary will improve downstream task performance on those languages.
- Evidence anchors: [abstract] "The extended vocabulary introduces 1,511 new tokens"; [section] "we increased the target vocabulary size to 10,000 and found 1,511 new, non-overlapping tokens to be added"

### Mechanism 2
- Claim: Continued pre-training on multilingual corpus preserves IndoBERT's Indonesian NLU performance while adding regional language capability.
- Mechanism: The pre-training procedure fine-tunes IndoBERT's embeddings on a balanced mix of Indonesian and regional languages, enabling cross-lingual transfer while retaining strong Indonesian task performance.
- Core assumption: IndoBERT's strong Indonesian performance can be preserved during continued pre-training on multilingual data.
- Evidence anchors: [abstract] "NusaBERT demonstrates state-of-the-art performance in tasks involving multiple languages of Indonesia"; [section] "NusaBERT significantly improves the sequence labeling results of IndoBERT, increasing the average score by about 2-3%"

### Mechanism 3
- Claim: Cross-lingual transfer from Indonesian to regional languages improves performance on low-resource tasks.
- Mechanism: Shared linguistic features and vocabulary between Indonesian and regional languages allow knowledge transfer during pre-training, improving performance on regional language downstream tasks.
- Core assumption: Regional languages share enough linguistic similarity with Indonesian to enable effective cross-lingual transfer.
- Evidence anchors: [abstract] "NusaBERT demonstrates state-of-the-art performance in tasks involving multiple languages of Indonesia"; [section] "NusaBERT significantly improves the results on most languages that were included during the continued pre-training phase such as Acehnese (ace), Balinese (ban), Banjarese (bjn), Buginese (bug), Javanese (jav), and Sundanese (sun)"

## Foundational Learning

- Concept: Masked Language Modeling (MLM) objective
  - Why needed here: MLM is the core pre-training objective that teaches the model to predict masked tokens, building contextual understanding.
  - Quick check question: Why does MLM use 15% masking with different replacement strategies (80% mask, 10% random, 10% unchanged)?

- Concept: Vocabulary expansion and tokenizer training
  - Why needed here: Adding new tokens allows the model to represent regional language words that weren't in the original vocabulary.
  - Quick check question: What is the difference between static and dynamic masking in tokenizer training?

- Concept: Continued pre-training vs. training from scratch
  - Why needed here: Continued pre-training leverages existing IndoBERT knowledge while adding multilingual capability, saving compute and preserving performance.
  - Quick check question: How does continued pre-training mitigate catastrophic forgetting compared to training from scratch?

## Architecture Onboarding

- Component map:
  IndoBERT phase one checkpoint → vocabulary expansion → new tokenizer (32,032 tokens) → continued pre-training (16B tokens, 500K steps) → fine-tuning on downstream tasks

- Critical path:
  1. Tokenizer training on multilingual corpus
  2. Continued pre-training with MLM objective
  3. Fine-tuning on downstream tasks
  - Dependencies: High-quality corpus → proper tokenizer training → stable pre-training → effective fine-tuning

- Design tradeoffs:
  - Vocabulary size: 32,032 vs. larger (PhayaThaiBERT's approach) - balances performance gains vs. parameter increase
  - Sequence length: 128 for most tasks vs. 512 for NusaParagraph - balances context vs. memory/compute
  - Pre-training steps: 500K vs. more - balances convergence vs. compute cost

- Failure signatures:
  - Training instability: Large vocabulary expansion without proper initialization
  - Performance degradation: Imbalanced pre-training corpus or poor quality data
  - Limited transfer: Regional languages too distant from Indonesian

- First 3 experiments:
  1. Test tokenizer tokenization quality on regional language Wikipedia samples
  2. Run pre-training for 100K steps and evaluate loss curve stability
  3. Fine-tune on one NusaX language (e.g., Javanese) and compare to IndoBERT baseline

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can we improve the robustness of NusaBERT against code-switching and code-mixing across diverse linguistic contexts?
- Basis in paper: The paper explicitly discusses the limitations of NusaBERT in handling code-switching, particularly in the context of intra-sentential code-switching. It notes that while the model is effective in many tasks, it has yet to address the challenge of code-switching and suggests potential room for improvement.
- Why unresolved: The current evaluation methods, such as using synthetically generated code-mixed examples, do not definitively represent the model's robustness against code-switching. The paper suggests that having an expert-curated code-mixing benchmark would be valuable for future evaluations.
- What evidence would resolve it: Developing and evaluating NusaBERT on a comprehensive, expert-curated code-mixing benchmark that includes a wide range of code-switching scenarios and linguistic contexts. This would provide a clearer understanding of the model's robustness and areas for improvement.

### Open Question 2
- Question: What strategies can be employed to extend NusaBERT's language support to include extremely low-resource languages in Indonesia that are highly distinct from Indonesian?
- Basis in paper: The paper highlights the challenge of extending NusaBERT to extremely low-resource languages that are highly distinct from Indonesian, noting the significant difference in the amount of available text corpus and the lack of quality data for these languages.
- Why unresolved: The current approach of using continued pre-training on available corpora is insufficient for these languages due to their unique linguistic characteristics and limited data. The paper suggests exploring alternative strategies such as leveraging bilingual lexicons and non-text data, but these are not yet fully developed or tested.
- What evidence would resolve it: Implementing and evaluating new strategies for extending NusaBERT to extremely low-resource languages, such as using bilingual lexicons for synthetic text generation and incorporating non-text data like transcribed speech. This would demonstrate the effectiveness of these approaches in improving the model's performance on these languages.

### Open Question 3
- Question: How can we improve the cultural and lexical diversity of NusaBERT's pre-training corpus to better handle tasks like NusaParagraph that involve culturally rich and colloquial language use?
- Basis in paper: The paper discusses the limitations of NusaBERT's pre-training corpus, which is predominantly based on social media texts and online documents, and how this affects its performance on tasks like NusaParagraph that involve more culturally rich and colloquial language use.
- Why unresolved: The current pre-training corpus lacks the cultural and lexical diversity needed to effectively handle tasks that involve nuanced and colloquial language use in very low-resource and linguistically distinct local languages. The paper suggests exploring alternative texts and non-text data but does not provide a definitive solution.
- What evidence would resolve it: Developing and incorporating a more diverse pre-training corpus that includes culturally rich texts and non-text data, such as transcribed speech, to enhance the model's ability to handle tasks involving colloquial and culturally specific language use. This would involve evaluating the model's performance on tasks like NusaParagraph before and after incorporating these diverse data sources.

## Limitations

- Limited quantitative validation of token overlap between added vocabulary and downstream datasets
- No systematic analysis of catastrophic forgetting mechanisms during continued pre-training
- Cross-lingual transfer effectiveness varies significantly across different regional languages

## Confidence

- High Confidence: Vocabulary expansion mechanism and basic pre-training procedure are well-documented and technically sound. Experimental results on NusaX and NusaWrites benchmarks are directly measurable and reproducible.
- Medium Confidence: Claims about preserving IndoBERT's Indonesian performance during continued pre-training are supported by benchmark results but lack ablation studies. The mechanism for knowledge retention is plausible but not thoroughly validated.
- Low Confidence: Cross-lingual transfer mechanism's effectiveness across all 12 regional languages is not well-established. The paper shows aggregate improvements but doesn't analyze transfer effectiveness per language pair or identify which linguistic features enable transfer.

## Next Checks

1. **Token Frequency Analysis**: Analyze the frequency distribution of the 1,511 new tokens in the downstream datasets (NusaX, NusaWrites, IndoNLU) to quantify actual overlap. Create a histogram showing how many added tokens appear at least once, five times, and ten times in each benchmark. This validates whether vocabulary expansion directly benefits downstream tasks.

2. **Catastrophic Forgetting Abation**: Run controlled experiments with different pre-training corpus compositions (100% Indonesian, 50/50 Indonesian-regional, 100% regional) and measure IndoBERT task performance degradation. This quantifies the preservation mechanism and identifies safe pre-training ratios.

3. **Language Pair Transfer Matrix**: Create a heatmap showing performance improvement per regional language when IndoBERT is fine-tuned on that language, broken down by whether the language was included in pre-training. This validates cross-lingual transfer effectiveness and identifies which language pairs benefit most from shared vocabulary and linguistic features.