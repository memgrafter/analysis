---
ver: rpa2
title: Assessing Modality Bias in Video Question Answering Benchmarks with Multimodal
  Large Language Models
arxiv_id: '2408.12763'
source_url: https://arxiv.org/abs/2408.12763
tags:
- questions
- modality
- video
- modalities
- question
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a method to detect modality bias in video
  question-answering datasets using multimodal large language models. The modality
  importance score quantifies whether a question can be answered using a single modality
  or requires integration across multiple modalities.
---

# Assessing Modality Bias in Video Question Answering Benchmarks with Multimodal Large Language Models

## Quick Facts
- arXiv ID: 2408.12763
- Source URL: https://arxiv.org/abs/2408.12763
- Reference count: 21
- This paper introduces a method to detect modality bias in video question-answering datasets using multimodal large language models

## Executive Summary
This paper addresses the critical issue of modality bias in video question-answering benchmarks by introducing a method to quantify the importance of visual and textual modalities using multimodal large language models. The authors propose a modality importance score that measures whether questions require single-modality or multimodal reasoning. Through experiments on three major datasets, they find that the vast majority of questions (89.8-94.8%) can be answered using only one modality or are modality-agnostic, while truly multimodal questions requiring integration are rare (0.6-2%).

## Method Summary
The authors introduce a modality importance score (MIS) to quantify the degree to which questions in video QA datasets require specific modalities for answering. They use a multimodal large language model (GPT-4V) to generate answers for different question-variant pairs that vary in which modalities are available. The MIS is calculated as the average absolute difference in predicted answers when modalities are ablated. Questions are then categorized as modality-agnostic, single-modality (text or video-biased), or multimodal based on their MIS values. Feature permutation experiments validate the model's reliance on textual information even for video-biased questions.

## Key Results
- 89.8-94.8% of questions in three major datasets are modality-agnostic or solvable with single modalities
- Truly multimodal questions requiring integration are rare (0.6-2% of questions)
- Feature permutation experiments confirm models rely heavily on textual information even for video-biased questions
- MIS-based categorization correlates with human perception of modality relevance

## Why This Works (Mechanism)
The approach works by leveraging the reasoning capabilities of multimodal large language models to evaluate which modalities are necessary for answering specific questions. By systematically ablating visual and textual information and measuring the impact on answer prediction, the method quantifies the inherent modality bias in existing datasets.

## Foundational Learning
- **Modality importance score (MIS)**: A metric that quantifies the necessity of different modalities for answering questions; needed to objectively measure dataset bias
- **Multimodal large language models (MLLMs)**: AI models capable of processing both visual and textual information; needed as the evaluation mechanism for modality importance
- **Feature permutation**: A technique to assess feature importance by systematically altering input components; needed to validate the model's reliance patterns
- **Question-variant generation**: Creating multiple versions of questions with different modality availability; needed to systematically test modality requirements
- **Modality categorization**: Classifying questions based on their modality dependencies; needed to organize findings and identify bias patterns

## Architecture Onboarding
**Component map**: Question -> MLLM processing -> Answer prediction -> Modality ablation -> MIS calculation -> Categorization
**Critical path**: Question generation → MLLM answer prediction → MIS computation → Dataset categorization
**Design tradeoffs**: Using MLLMs provides powerful reasoning but introduces potential model-specific biases; smaller sample sizes enable computational feasibility but may miss edge cases
**Failure signatures**: Incorrect categorization when questions contain implicit cross-modal references; over-reliance on textual information in visually complex scenarios
**First experiments**: 1) Test MIS calculation on synthetic questions with known modality requirements; 2) Validate MIS correlation with human annotations on sample questions; 3) Compare feature permutation results across different MLLM architectures

## Open Questions the Paper Calls Out
None

## Limitations
- The methodology assumes MLLMs can reliably assess modality importance, but this may not reflect true understanding versus pattern exploitation
- The study uses relatively small sample sizes (1,000 samples per dataset) which may not capture full diversity of video QA challenges
- Focus on English-language datasets limits generalizability to multilingual contexts

## Confidence
- **High confidence**: Most questions are modality-agnostic or solvable with single modalities (89.8-94.8%)
- **Medium confidence**: Existing datasets are inadequate for evaluating true multimodal reasoning
- **Medium confidence**: Current models rely heavily on textual information even for video-biased questions

## Next Checks
1. Test the MIS methodology across additional video QA datasets with different domains and characteristics to assess generalizability
2. Conduct ablation studies using smaller subsets of questions to determine minimum sample size needed for reliable assessment
3. Compare MLLM-based modality importance scores with human annotations on the same question sets to validate model accuracy and identify potential biases