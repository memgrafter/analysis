---
ver: rpa2
title: 'ConSiDERS-The-Human Evaluation Framework: Rethinking Human Evaluation for
  Generative Large Language Models'
arxiv_id: '2405.18638'
source_url: https://arxiv.org/abs/2405.18638
tags:
- human
- evaluation
- title
- such
- association
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a human evaluation framework called ConSiDERS
  for generative large language models. The framework emphasizes the need for human-centric
  evaluation that accounts for cognitive biases, user experience, and responsible
  AI considerations.
---

# ConSiDERS-The-Human Evaluation Framework: Rethinking Human Evaluation for Generative Large Language Models

## Quick Facts
- arXiv ID: 2405.18638
- Source URL: https://arxiv.org/abs/2405.18638
- Authors: Aparna Elangovan; Ling Liu; Lei Xu; Sravan Bodapati; Dan Roth
- Reference count: 40
- Primary result: Proposes a human evaluation framework for generative LLMs emphasizing cognitive bias mitigation, consistency, and responsible AI considerations.

## Executive Summary
This paper introduces the ConSiDERS framework, a human evaluation methodology for generative large language models that addresses the limitations of current evaluation practices. The framework recognizes that human evaluations often fail to capture LLM capabilities and weaknesses due to cognitive biases, task complexity, and inadequate differentiation of model performance. ConSiDERS consists of six pillars: Consistency, Scoring Criteria, Differentiating, User Experience, Responsible, and Scalability, providing a comprehensive approach to more reliable and representative human evaluation of LLM outputs.

## Method Summary
The ConSiDERS framework is a systematic approach to human evaluation of generative LLMs that addresses cognitive biases, task complexity, and evaluation consistency. It emphasizes breaking down complex evaluation tasks into simpler atomic components, defining clear scoring criteria, and ensuring evaluator diversity to mitigate bias effects. The framework incorporates mechanisms to handle processing fluency bias and the halo effect, implements effective test sets that differentiate LLM capabilities, and includes responsible AI considerations such as bias, safety, and privacy. Key methodological components include qualification exams for evaluators, shuffling of model outputs to reduce ordering bias, fact extraction techniques for consistency, and denoising algorithms to improve inter-rater agreement scores.

## Key Results
- Human evaluation of LLMs is significantly impacted by cognitive biases like processing fluency and the halo effect, which can conflate fluent outputs with factual correctness.
- Simplifying evaluation tasks into atomic components and using clear, well-defined guidelines improves inter-rater agreement and reduces cognitive load.
- Less than 3% of existing papers report evaluator demographics, highlighting a critical gap in addressing bias and generalizability in LLM evaluation.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Human evaluation must explicitly account for cognitive biases to avoid conflating fluency with truthfulness.
- Mechanism: Cognitive biases like processing fluency and the halo effect lead evaluators to overvalue fluent, easy-to-process outputs even when they are not factually correct.
- Core assumption: NLP tasks requiring factual correctness are heavily impacted by linguistic fluency, which can mislead human raters.
- Evidence anchors:
  - [abstract] Highlights how cognitive biases can conflate fluent information and truthfulness.
  - [section 3.1.1] Explains the halo effect and how it impacts individual attribute evaluations when overall fluency is high.
  - [corpus] No direct evidence; paper argues this based on psychology literature and UX findings.
- Break condition: If fluency and factual correctness are perfectly aligned in the model outputs, bias effects diminish.

### Mechanism 2
- Claim: Consistency of human evaluation improves when tasks are simplified and guidelines are well-defined.
- Mechanism: Breaking complex evaluation tasks into simpler, atomic components reduces cognitive load and improves inter-rater agreement.
- Core assumption: Complex tasks with multiple criteria lead to inconsistent evaluations due to cognitive overload and ambiguous instructions.
- Evidence anchors:
  - [abstract] Emphasizes the need for consistency in human evaluation findings.
  - [section 3.2] Details how high task complexity and ill-defined guidelines reduce agreement and provides strategies to mitigate them.
  - [corpus] Weak evidence; primarily based on general cognitive science and HCI principles rather than specific NLP studies.
- Break condition: If tasks are inherently subjective and cannot be broken into atomic components, simplification may lose essential nuance.

### Mechanism 3
- Claim: Effective human evaluation requires diverse evaluators to mitigate bias effects.
- Mechanism: Homogeneous evaluator groups introduce selection bias, limiting the generalizability and representativeness of results.
- Core assumption: Evaluator diversity in demographics and background reduces systemic bias in evaluation outcomes.
- Evidence anchors:
  - [abstract] Stresses the importance of responsible AI evaluation including bias considerations.
  - [section 3.5] Calls for reporting evaluator demographics to mitigate bias effects in evaluations.
  - [corpus] Limited evidence; highlights that less than 3% of papers report evaluator demographics, indicating a gap.
- Break condition: If evaluator diversity does not correlate with reduced bias in evaluation results, this mechanism weakens.

## Foundational Learning

- Concept: Cognitive biases in human judgment
  - Why needed here: Understanding biases like processing fluency and halo effect is critical to designing fair and accurate human evaluation frameworks.
  - Quick check question: Can you name two cognitive biases that affect how humans rate fluent versus factually correct language model outputs?

- Concept: Inter-rater agreement (IRA) metrics
  - Why needed here: Measuring IRA ensures reliability of human evaluation and helps identify design flaws in evaluation tasks.
  - Quick check question: What is the difference between percentage agreement and Krippendorff's alpha in measuring inter-rater agreement?

- Concept: Usability testing criteria (the 5 Es)
  - Why needed here: Incorporating usability principles helps design human evaluation that is easy to learn, efficient, effective, error tolerant, and engaging.
  - Quick check question: How can the 5 Es of usability improve the design of human evaluation guidelines?

## Architecture Onboarding

- Component map:
  - Evaluation Design Module → Task Simplification → Cognitive Bias Mitigation → IRA Measurement → Result Aggregation
  - Data Input: Model outputs, evaluation guidelines, evaluator demographics
  - Output: Consistency scores, bias-adjusted ratings, responsibility metrics

- Critical path:
  1. Define clear, atomic evaluation tasks
  2. Train diverse evaluators with qualification exams
  3. Implement bias mitigation strategies (shuffling, fact extraction)
  4. Measure IRA with multiple metrics
  5. Aggregate results with denoising algorithms

- Design tradeoffs:
  - Simplicity vs. granularity: Breaking tasks into atomic facts improves consistency but may lose qualitative nuance.
  - Diversity vs. coordination: More diverse evaluators reduce bias but increase coordination complexity.
  - Automation vs. human judgment: Automating parts reduces cost but may introduce new biases.

- Failure signatures:
  - Low IRA scores consistently across multiple metrics
  - Systematic rating bias favoring fluent but less accurate outputs
  - Lack of evaluator demographic reporting

- First 3 experiments:
  1. Compare IRA scores before and after simplifying evaluation tasks into atomic facts.
  2. Test the effect of evaluator demographic diversity on bias mitigation.
  3. Measure the impact of rating denoising algorithms on consistency.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal number of raters and test set size needed to achieve reliable and generalizable results in human evaluation of LLMs?
- Basis in paper: [inferred] from the discussion on non-random inconsistencies arising from small sample sizes of evaluators and test sets.
- Why unresolved: The paper highlights the importance of a large pool of evaluators and a large test set, but does not provide specific guidelines on the optimal number.
- What evidence would resolve it: Empirical studies comparing the reliability and generalizability of results across different sample sizes of evaluators and test sets.

### Open Question 2
- Question: How can we effectively differentiate the capabilities and weaknesses of LLMs using test sets that represent end-user use cases?
- Basis in paper: [explicit] from the section on differentiating, which emphasizes the need for test sets that can differentiate model capabilities and weaknesses.
- Why unresolved: The paper acknowledges the challenges in curating effective test sets that represent end-user use cases, but does not provide a concrete solution.
- What evidence would resolve it: Research on developing and evaluating test sets that cover a wide range of end-user use cases and accurately reflect the capabilities and limitations of LLMs.

### Open Question 3
- Question: What are the most effective strategies for mitigating cognitive biases in human evaluation of LLMs?
- Basis in paper: [explicit] from the section on user experience, which highlights the impact of cognitive biases on evaluation and provides recommendations to mitigate common biases.
- Why unresolved: The paper provides recommendations to mitigate some cognitive biases, but does not explore the effectiveness of these strategies or propose additional methods.
- What evidence would resolve it: Empirical studies comparing the effectiveness of different strategies in mitigating cognitive biases and improving the reliability of human evaluation.

## Limitations
- Limited empirical validation of the framework's effectiveness in real-world LLM evaluation scenarios.
- Does not provide specific guidelines on optimal sample sizes for evaluators and test sets.
- Relies heavily on general cognitive science principles rather than NLP-specific validation studies.

## Confidence

**High Confidence**: The need to account for cognitive biases in human evaluation is well-established in psychology literature and HCI research. The mechanisms for how processing fluency and halo effect confound evaluations are supported by existing studies.

**Medium Confidence**: The claim that simplifying tasks improves consistency is grounded in cognitive science principles, but lacks direct NLP-specific validation. The framework's effectiveness in practice needs empirical testing.

**Low Confidence**: The scalability claims and the impact of evaluator diversity on bias mitigation are based on general principles rather than concrete evidence from LLM evaluation contexts.

## Next Checks

1. **Empirical IRA Comparison**: Conduct a controlled study comparing inter-rater agreement scores before and after implementing task simplification and atomic fact evaluation in LLM output assessment.

2. **Demographic Impact Study**: Design an experiment with systematically varied evaluator demographics to measure the actual impact of diversity on bias reduction in LLM evaluation outcomes.

3. **Denoising Algorithm Validation**: Test the effectiveness of rating denoising algorithms in improving consistency scores across multiple LLM evaluation datasets, comparing results with and without denoising.