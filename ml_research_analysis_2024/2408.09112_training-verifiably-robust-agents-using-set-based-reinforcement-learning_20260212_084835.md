---
ver: rpa2
title: Training Verifiably Robust Agents Using Set-Based Reinforcement Learning
arxiv_id: '2408.09112'
source_url: https://arxiv.org/abs/2408.09112
tags:
- set-based
- learning
- neural
- training
- reinforcement
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of training robust reinforcement
  learning agents using neural networks in continuous state and action spaces, where
  standard neural networks are sensitive to input perturbations. The authors propose
  a novel set-based reinforcement learning algorithm that trains agents to maximize
  the worst-case reward by propagating entire sets of perturbed inputs through the
  actor and critic networks during training.
---

# Training Verifiably Robust Agents Using Set-Based Reinforcement Learning

## Quick Facts
- **arXiv ID**: 2408.09112
- **Source URL**: https://arxiv.org/abs/2408.09112
- **Reference count**: 22
- **Primary result**: Novel set-based RL algorithm trains agents that maximize worst-case reward by propagating entire sets of perturbed inputs through neural networks, achieving verifiably better robustness than standard and adversarial training methods.

## Executive Summary
This paper addresses the challenge of training robust reinforcement learning agents in continuous state and action spaces where standard neural networks are sensitive to input perturbations. The authors propose a novel set-based reinforcement learning algorithm that trains agents to maximize worst-case reward by propagating entire sets of perturbed inputs through actor and critic networks during training. A set-based loss function is developed that combines standard training loss with a volume loss to penalize the size of output sets, and set-based policy gradients are derived for backpropagation. The method is evaluated on four benchmarks including navigation, inverted pendulum, and quadcopter control tasks, demonstrating that agents trained with the proposed set-based approach achieve better worst-case performance and are verifiably more robust to input uncertainties compared to standard and adversarial training methods.

## Method Summary
The proposed method trains neural networks utilizing entire sets of perturbed inputs rather than single points, maximizing worst-case reward through set-based reinforcement learning. The approach propagates zonotopes (a specific set representation) through actor and critic networks to model input uncertainty, using a set-based loss function that combines standard training loss with a volume penalty to minimize output set sizes. Set-based policy gradients are derived for backpropagation through these set representations. Two variants are proposed: SA-PC (set-based actor, point-based critic) and SA-SC (set-based actor and critic). The method allows formal verification of trained controllers using reachability analysis, making it applicable in safety-critical environments.

## Key Results
- Agents trained with set-based approach achieve better worst-case performance than standard and adversarial training methods on four benchmark tasks
- Set-based agents maintain robustness guarantees even for large perturbation sets, while other methods degrade significantly
- The proposed method enables formal verification of trained controllers using reachability analysis
- SA-PC (set-based actor, point-based critic) achieves similar performance to SA-SC with significantly reduced computational cost

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Training with entire sets of perturbed inputs improves worst-case performance over standard point-based RL.
- Mechanism: By propagating zonotopes through actor and critic networks, the algorithm directly optimizes the expected value under input uncertainty rather than relying on sampled perturbations. This yields a formal guarantee on worst-case reward via reachability analysis.
- Core assumption: The zonotope approximation error is small enough that minimizing the volume of output sets leads to better worst-case performance.
- Evidence anchors:
  - [abstract]: "train neural networks utilizing entire sets of perturbed inputs and maximize the worst-case reward"
  - [section]: "We construct an input set Xi around each training sample xi modeling the uncertainty, which is typically an ℓ∞-ball around xi"
  - [corpus]: No direct comparison of worst-case performance against set-based methods in corpus; evidence is indirect.
- Break condition: If the zonotope approximation becomes too conservative (many generators), the volume loss may dominate and degrade nominal performance.

### Mechanism 2
- Claim: Set-based policy gradients simultaneously train accuracy and robustness by backpropagating through set representations.
- Mechanism: The gradient includes both the center gradient (standard policy gradient) and a generator matrix gradient weighted by ημ, which penalizes the volume of action sets. This is achieved via set-based backpropagation as defined in Definition 3.
- Core assumption: The generator matrix gradient is meaningful for improving robustness in the actor.
- Evidence anchors:
  - [section]: "the set-based policy gradient simultaneously trains an accurate and robust actor"
  - [section]: "∇GAi JSet(µϕ) = Esi∼ρβ [ω lnDia′(GAi) + (1 − ω) ∇GAi lnDia′(GQi)]"
  - [corpus]: No corpus evidence that similar gradient formulations have been validated.
- Break condition: If ω is poorly tuned, the robustness term may either be ineffective (ω too small) or harm performance (ω too large).

### Mechanism 3
- Claim: Set-based loss functions are derived from probabilistic assumptions that align with the training objective.
- Mechanism: The set-based regression loss is motivated by modeling the critic output as uniformly distributed over its enclosing interval, leading to a likelihood function that combines mean-squared error with a volume penalty.
- Core assumption: Uniform distribution over the enclosing interval is a reasonable model for the true distribution of outputs under input uncertainty.
- Evidence anchors:
  - [section]: "we assume that qi is uniformly distributed over the interval [lQi , uQi ] ⊇ Q i ⊂ R"
  - [section]: "Taking the logarithm obtains us: −ln(N (yi|qi, β−1) dia( Qi)−1) ∝ EReg(yi, Qi)"
  - [corpus]: No corpus evidence that uniform prior over intervals is optimal for RL regression tasks.
- Break condition: If the true posterior is highly non-uniform, the volume penalty may misalign with the true robustness objective.

## Foundational Learning

- Concept: Zonotope arithmetic (Minkowski sum, affine maps, diameter computation)
  - Why needed here: The entire algorithm relies on propagating zonotopes through neural networks to model input uncertainty
  - Quick check question: Given a zonotope Z = ⟨c, G⟩Z, what is the zonotope representation after applying an affine transformation A·Z + b?

- Concept: Neural network set propagation (interval enclosure, ReLU linearization)
  - Why needed here: The algorithm needs to compute output sets from input sets through each layer to evaluate the set-based loss
  - Quick check question: For a ReLU activation with input interval [l, u], what is the linear approximation slope m that preserves the expected value?

- Concept: Deep deterministic policy gradient (actor-critic architecture, policy gradients)
  - Why needed here: The algorithm is built on DDPG, so understanding the standard update rules is essential before adding the set-based modifications
  - Quick check question: In DDPG, how is the target Q-value computed for the critic update?

## Architecture Onboarding

- Component map:
  Actor network (µϕ) -> Zonotope propagator -> Critic network (Qθ) -> Set-based loss module -> Replay buffer (B)

- Critical path:
  1. Get observation st
  2. Construct state zonotope St = ⟨st, εI⟩Z
  3. Propagate through actor: At = enclose(µϕ, St)
  4. Add exploration noise: Ãt = At + et
  5. Propagate through critic: Qt = enclose(Qθ, St × Ãt)
  6. Store transition in buffer
  7. Sample batch and compute targets
  8. Update critic with set-based regression loss
  9. Update actor with set-based policy gradient

- Design tradeoffs:
  - SA-PC vs SA-SC: SA-PC is faster (no critic set propagation) but potentially less robust
  - Number of generators: More generators = better approximation but higher computational cost
  - Weighting factor ω: Balances between minimizing action set volume vs critic output set volume

- Failure signatures:
  - Degraded performance on small ε: SA-PC may be too conservative
  - Memory overflow: Large number of generators in action sets
  - Vanishing gradients: If volume loss dominates in set-based loss

- First 3 experiments:
  1. Implement basic DDPG on a simple continuous control task (e.g., Pendulum-v1) and verify performance matches literature
  2. Add zonotope propagation through a small neural network and verify diameter computation matches manual calculation
  3. Implement SA-PC on Pendulum-v1 with ε = 0.1 and compare against standard DDPG for increasing εtest

## Open Questions the Paper Calls Out
None

## Limitations
- Computational complexity of set propagation grows rapidly with perturbation set size and network depth
- Zonotope approximation error may accumulate through multiple layers, limiting scalability
- The weighting factor ω between accuracy and robustness terms requires careful tuning and lacks principled selection methods
- Empirical evaluation is limited to 4 benchmark tasks with modest state-action dimensions

## Confidence

- Worst-case reward maximization claims: **High confidence** given explicit mathematical formulations
- Set-based policy gradients effectiveness: **High confidence** based on consistent set-based optimization principles
- Computational scalability: **Medium confidence** based on limited evaluation on simple tasks
- Uniform distribution assumption in set-based loss: **Low confidence** due to lack of corpus validation

## Next Checks

1. Implement the set-based algorithm on a more complex continuous control task (e.g., humanoid locomotion) to evaluate scalability and performance in higher-dimensional state-action spaces

2. Conduct ablation studies varying the number of generators in zonotope representations to quantify the approximation-accuracy tradeoff

3. Compare the proposed uniform distribution assumption for output sets against alternative distributions (e.g., Gaussian) in the set-based loss function to assess sensitivity to modeling choices