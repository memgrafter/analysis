---
ver: rpa2
title: 'AdaCQR: Enhancing Query Reformulation for Conversational Search via Sparse
  and Dense Retrieval Alignment'
arxiv_id: '2407.01965'
source_url: https://arxiv.org/abs/2407.01965
tags:
- query
- reformulation
- retrieval
- adacqr
- labels
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes AdaCQR, a framework for enhancing conversational
  query reformulation (CQR) in information retrieval. The key challenge addressed
  is the need for queries that generalize well across both sparse and dense retrieval
  systems.
---

# AdaCQR: Enhancing Query Reformulation for Conversational Search via Sparse and Dense Retrieval Alignment

## Quick Facts
- **arXiv ID**: 2407.01965
- **Source URL**: https://arxiv.org/abs/2407.01965
- **Reference count**: 40
- **Primary result**: Achieves state-of-the-art performance in conversational query reformulation, with MRR of 38.5 and NDCG of 37.6 on TopiOCQA dataset

## Executive Summary
AdaCQR addresses the challenge of conversational query reformulation (CQR) by aligning reformulation models with both sparse (BM25) and dense (ANCE) retrieval systems. The framework employs a two-stage training strategy that first builds basic reformulation ability using superior labels generated via few-shot LLM prompting, then aligns the model using contrastive loss with diverse candidates. Experiments on TopiOCQA and QReCC datasets demonstrate significant performance improvements over existing methods, with AdaCQR achieving state-of-the-art results in both sparse and dense retrieval scenarios.

## Method Summary
AdaCQR employs a two-stage training approach for conversational query reformulation. Stage 1 uses cross-entropy loss with superior reformulation labels generated through few-shot LLM prompting to establish basic reformulation capability. Stage 2 introduces contrastive alignment using diverse candidates generated by Diverse Beam Search, ranked by a fusion metric that combines sparse and dense retrieval performance. The framework generates multiple reformulation candidates, evaluates them across both retrieval systems, and uses the relative rankings as supervision for contrastive learning, ultimately producing queries that generalize well across different retrieval architectures.

## Key Results
- Achieves MRR of 38.5 and NDCG of 37.6 on TopiOCQA dense retrieval, outperforming existing methods
- Shows consistent improvements across both sparse and dense retrieval systems on QReCC and TopiOCQA datasets
- AdaCQR with query expansion demonstrates superior performance compared to baseline methods and the RETPO approach using LLaMA2-7B

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: AdaCQR aligns reformulation models with both term-based and semantic-based retrievers, enabling queries that generalize across sparse and dense retrieval systems.
- **Mechanism**: The framework employs a two-stage training strategy. Stage 1 uses cross-entropy loss with superior reformulation labels to build basic query reformulation ability. Stage 2 employs contrastive loss using diverse candidates to align the model with both sparse (BM25) and dense (ANCE) retrievers.
- **Core assumption**: A good information-seeking query must have both precise term overlap and high semantic similarity to perform well across both retrieval types.
- **Evidence anchors**:
  - [abstract]: "aligning reformulation models with both term-based and semantic-based retrieval systems"
  - [section]: "We propose a fusion metric to evaluate the performance of reformulation query ˆQ more comprehensively, similar to reciprocal rank fusion"
  - [corpus]: Weak - no direct evidence found in corpus
- **Break condition**: If the fusion metric fails to balance term and semantic aspects, or if contrastive loss cannot properly rank candidates across both retrieval types, alignment will break.

### Mechanism 2
- **Claim**: Using large language models to generate superior reformulation labels via few-shot prompting reduces reliance on sub-optimal human annotations while improving query quality.
- **Mechanism**: Few-shot prompting with representative demonstrations implicitly guides the LLM to generate labels that meet the needs of retrievers, rather than relying on costly human annotations.
- **Core assumption**: LLMs can effectively learn what makes a good reformulation query through in-context learning with contrastive demonstrations.
- **Evidence anchors**:
  - [section]: "To mitigate the dependency on costly and sub-optimal human-annotated reformulation labels, we utilize LLMs to generate superior reformulation labels"
  - [section]: "Leveraging excellent in-context learning capabilities of LLMs... we propose a prompting strategy that implicitly selects representative demonstrations"
  - [corpus]: Weak - no direct evidence found in corpus
- **Break condition**: If the LLM cannot distinguish between good and bad reformulations from demonstrations, or if the few-shot examples are not representative, label quality will degrade.

### Mechanism 3
- **Claim**: Diverse Beam Search generates multiple reformulation candidates that can be evaluated across both retrieval systems to establish relative rank orders for contrastive learning.
- **Mechanism**: The model generates diverse candidates using Diverse Beam Search, which are then ranked using the fusion metric across both sparse and dense retrieval systems. This ranking serves as implicit supervision for contrastive alignment.
- **Core assumption**: Among the generated candidates, one will be exceptional while others can provide relative ranking information for learning.
- **Evidence anchors**:
  - [section]: "Diverse Beam Search (Vijayakumar et al., 2016) is used to generate multiple candidates simultaneously"
  - [section]: "Among the generated candidates, one oracle candidate exhibits exceptional performance, while the remaining candidates are relatively ranked"
  - [corpus]: Weak - no direct evidence found in corpus
- **Break condition**: If Diverse Beam Search fails to generate sufficiently diverse candidates, or if the fusion metric cannot reliably rank them, contrastive learning will lack proper supervision signals.

## Foundational Learning

- **Concept: Contrastive Learning**
  - Why needed here: To align the reformulation model with retrievers by learning relative preferences between candidates rather than absolute labels.
  - Quick check question: What is the key difference between contrastive loss and cross-entropy loss in this context?

- **Concept: Fusion Metrics**
  - Why needed here: To evaluate reformulation queries across both sparse and dense retrieval systems simultaneously, capturing both term overlap and semantic similarity.
  - Quick check question: How does the proposed fusion metric differ from simple average of two rankings?

- **Concept: Diverse Beam Search**
  - Why needed here: To generate multiple reformulation candidates that capture different aspects of the query reformulation space for robust contrastive learning.
  - Quick check question: Why is diversity in candidate generation important for contrastive alignment?

## Architecture Onboarding

- **Component map**: Input (current query + historical context) → Label Generator (LLM with few-shot prompting) → Stage 1 Model (T5-base trained with cross-entropy loss) → Candidate Generator (Diverse Beam Search) → Evaluation System (BM25 + ANCE with fusion metric) → Stage 2 Model (multi-task training with cross-entropy + contrastive loss) → Output (reformulated query)

- **Critical path**: Input → Label Generation → Stage 1 Training → Candidate Generation → Stage 2 Training → Output

- **Design tradeoffs**:
  - Using T5-base instead of larger models for efficiency vs. potential performance gains
  - Few-shot prompting vs. zero-shot or fine-tuning for label generation
  - Multi-task loss balancing vs. separate training stages

- **Failure signatures**:
  - Poor retrieval performance on either sparse or dense systems indicates fusion metric imbalance
  - Training instability suggests contrastive loss weight γ needs adjustment
  - Low diversity in candidates suggests beam search parameters need tuning

- **First 3 experiments**:
  1. Train with only cross-entropy loss (no alignment) to establish baseline performance
  2. Test different γ values (0.1, 1, 10, 100) to find optimal contrastive loss weighting
  3. Compare diverse beam search vs. regular beam search for candidate generation quality

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions, but several implications arise from the work.

## Limitations
- Reliance on specific retrieval systems (BM25 and ANCE) may limit generalizability to other retrieval architectures
- The effectiveness of few-shot prompting assumes access to suitable demonstration examples, which may not always be available
- Computational overhead from generating multiple candidates and evaluating them across two retrieval systems

## Confidence
- **High**: Two-stage training approach, few-shot prompting for label generation
- **Medium**: Fusion metric's ability to balance sparse and dense retrieval performance, Diverse Beam Search candidate quality
- **Low**: None explicitly identified

## Next Checks
1. Perform ablation study on the fusion metric by comparing against simple average fusion and reciprocal rank fusion baselines to quantify the contribution of the proposed metric.
2. Test the framework's robustness by replacing BM25 and ANCE with alternative sparse and dense retrievers (e.g., BM25 with different parameters, DPR instead of ANCE).
3. Evaluate candidate diversity quantitatively using metrics like Self-BLEU or n-gram overlap to verify that Diverse Beam Search produces meaningfully different reformulation candidates.