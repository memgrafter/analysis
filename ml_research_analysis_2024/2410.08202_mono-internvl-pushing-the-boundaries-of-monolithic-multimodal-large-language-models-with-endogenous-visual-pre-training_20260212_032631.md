---
ver: rpa2
title: 'Mono-InternVL: Pushing the Boundaries of Monolithic Multimodal Large Language
  Models with Endogenous Visual Pre-training'
arxiv_id: '2410.08202'
source_url: https://arxiv.org/abs/2410.08202
tags:
- visual
- arxiv
- mono-internvl
- learning
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of unstable optimization and
  catastrophic forgetting in monolithic multimodal large language models (MLLMs).
  The authors propose Mono-InternVL, a novel monolithic MLLM that embeds visual experts
  into a pre-trained language model using a multimodal mixture-of-experts structure,
  along with an innovative Endogenous Visual Pre-training (EViP) strategy that progressively
  learns visual knowledge from noisy to high-quality data.
---

# Mono-InternVL: Pushing the Boundaries of Monolithic Multimodal Large Language Models with Endogenous Visual Pre-training

## Quick Facts
- arXiv ID: 2410.08202
- Source URL: https://arxiv.org/abs/2410.08202
- Reference count: 40
- Primary result: Mono-InternVL achieves 80 points higher on OCRBench compared to Emu3, while reducing first token latency by up to 67% compared to modular baseline InternVL-1.5

## Executive Summary
This paper addresses the challenge of unstable optimization and catastrophic forgetting in monolithic multimodal large language models (MLLMs). The authors propose Mono-InternVL, a novel monolithic MLLM that embeds visual experts into a pre-trained language model using a multimodal mixture-of-experts structure, along with an innovative Endogenous Visual Pre-training (EViP) strategy that progressively learns visual knowledge from noisy to high-quality data. Experimental results show Mono-InternVL outperforms existing monolithic MLLMs on 13 out of 16 benchmarks, achieving 80 points higher on OCRBench compared to Emu3, while reducing first token latency by up to 67% compared to modular baseline InternVL-1.5.

## Method Summary
Mono-InternVL integrates visual experts into a pre-trained language model through a multimodal mixture-of-experts (MMoE) architecture. The model uses a patch embedding approach with 28×28 patches and a lightweight visual tokenizer to process high-resolution images up to 10,240 patches. The key innovation is Endogenous Visual Pre-training (EViP), which employs a progressive learning strategy across three stages: concept learning from noisy data, semantic learning from synthetic captions, and alignment learning from task-specific data. During visual pre-training, the language parameters are frozen to prevent catastrophic forgetting while visual experts learn progressively richer visual representations.

## Key Results
- Achieves 80 points higher performance on OCRBench compared to Emu3
- Reduces first token latency by up to 67% compared to InternVL-1.5
- Outperforms existing monolithic MLLMs on 13 out of 16 multimodal benchmarks
- Maintains strong language capabilities while significantly improving visual understanding

## Why This Works (Mechanism)

### Mechanism 1: Catastrophic Forgetting Prevention
Embedding visual experts into a pre-trained LLM prevents catastrophic forgetting by freezing the language parameters during visual pre-training. The multimodal mixture-of-experts (MMoE) structure routes visual tokens to dedicated visual experts while keeping textual tokens processed by frozen textual experts. This isolation allows visual learning without modifying language parameters.

### Mechanism 2: Progressive Visual Knowledge Acquisition
Endogenous Visual Pre-training (EViP) uses a progressive learning approach from noisy to high-quality data. The three-stage process (concept learning → semantic learning → alignment learning) builds visual understanding incrementally, starting with basic concepts from noisy data, then adding semantic understanding from synthetic captions, and finally achieving task-specific alignment.

### Mechanism 3: Efficient High-Resolution Processing
Patch embedding with lightweight visual tokenizer enables efficient processing of high-resolution images while maintaining model scalability. Direct patchification (28×28 patches) with additional thumbnail for global context allows arbitrary resolution processing up to 10,240 patches, balancing efficiency with visual information capture.

## Foundational Learning

- **Concept: Catastrophic forgetting in neural networks**
  - Why needed here: Understanding why freezing language parameters is crucial - without it, visual pre-training would degrade language capabilities
  - Quick check question: What happens to a neural network's performance on task A when it's fine-tuned on task B without any regularization?

- **Concept: Mixture-of-experts (MoE) routing mechanisms**
  - Why needed here: The MMoE structure requires understanding how tokens are routed to appropriate experts for effective visual-textual separation
  - Quick check question: How does static routing differ from dynamic routing in MoE architectures, and what are the trade-offs?

- **Concept: Progressive learning and curriculum design**
  - Why needed here: EViP's staged approach relies on understanding how learning order affects knowledge acquisition and generalization
  - Quick check question: Why might learning from noisy data first be beneficial before moving to high-quality data in visual pre-training?

## Architecture Onboarding

- **Component map**: Image → PatchEmbed → Visual Experts → MMoE → LLM → Text Generation
- **Critical path**: Visual expert initialization and routing logic
- **Design tradeoffs**:
  - Patch size (28×28) vs. fine-grained visual detail capture
  - Static vs. dynamic routing in MMoE (simplicity vs. flexibility)
  - Progressive learning stages vs. end-to-end training (stability vs. efficiency)
- **Failure signatures**:
  - Language degradation: Check if textual experts are being unintentionally updated
  - Visual performance plateau: Verify visual experts are learning effectively from staged data
  - Resolution limitations: Test performance drop with high-resolution images beyond 10,240 patches
- **First 3 experiments**:
  1. Ablation: Remove visual expert freezing - measure language capability degradation vs. visual improvement
  2. Routing analysis: Visualize expert activation patterns across different image types and resolutions
  3. Stage effectiveness: Compare performance when skipping concept learning or semantic learning stages

## Open Questions the Paper Calls Out

### Open Question 1
What is the optimal strategy for dynamically adjusting the number of image patches during different pre-training stages to balance computational efficiency and visual understanding? The paper shows that Mono-InternVL progressively increases the maximum number of image patches from 1,280 to 3,328 during pre-training, suggesting a need to optimize this parameter.

### Open Question 2
How does the performance of Mono-InternVL scale with increasing model depth beyond the current 24 layers? The paper mentions that Mono-InternVL is inferior to InternVL-1.5 on high-resolution benchmarks like InfoVQA, attributing this to relatively shallow model depth.

### Open Question 3
What is the optimal mix of noisy versus high-quality data during pre-training for maximizing downstream performance? The paper uses 922M noisy samples in concept learning and 258M synthetic samples in semantic learning, suggesting data quality plays a crucial role.

## Limitations

- The freezing mechanism for language experts may create a rigid architecture that cannot effectively handle tasks requiring deep cross-modal reasoning
- The progressive learning approach assumes a specific learning hierarchy that may not generalize to all visual domains or languages
- The 28×28 patch embedding may miss fine-grained visual details critical for tasks like medical imaging or satellite imagery analysis

## Confidence

- **High Confidence**: The experimental results showing performance improvements over baseline models (80 points on OCRBench, 67% latency reduction) are well-documented and reproducible
- **Medium Confidence**: The catastrophic forgetting prevention mechanism is theoretically sound but lacks ablation studies proving the necessity of complete language expert freezing
- **Low Confidence**: The claim that the progressive learning stages are optimally ordered is not rigorously tested against alternative curriculum designs

## Next Checks

1. **Cross-Modal Dependency Test**: Remove the freezing mechanism for language experts and measure performance degradation in both language and multimodal tasks to quantify the actual risk of catastrophic forgetting

2. **Patch Resolution Scaling**: Systematically evaluate model performance across patch sizes (14×14, 28×28, 56×56) to determine the optimal trade-off between efficiency and visual detail capture

3. **Alternative Progressive Schedules**: Compare the proposed three-stage learning approach against alternative curriculum designs (e.g., quality-based sampling, difficulty-based ordering) to validate the optimality of the current progression