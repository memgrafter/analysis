---
ver: rpa2
title: 'StreamSpeech: Simultaneous Speech-to-Speech Translation with Multi-task Learning'
arxiv_id: '2406.03049'
source_url: https://arxiv.org/abs/2406.03049
tags:
- speech
- streamspeech
- translation
- target
- source
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: StreamSpeech introduces a direct simultaneous speech-to-speech
  translation (Simul-S2ST) model that jointly learns translation and simultaneous
  policy in a unified multi-task learning framework. The model employs a two-pass
  architecture with chunk-based Conformer for streaming encoding and CTC decoders
  for learning alignments via auxiliary ASR and NAR-S2TT tasks.
---

# StreamSpeech: Simultaneous Speech-to-Speech Translation with Multi-task Learning

## Quick Facts
- **arXiv ID**: 2406.03049
- **Source URL**: https://arxiv.org/abs/2406.03049
- **Reference count**: 40
- **One-line primary result**: Achieves state-of-the-art performance on both offline S2ST and Simul-S2ST tasks with significant improvements over cascaded approaches

## Executive Summary
StreamSpeech introduces a direct simultaneous speech-to-speech translation model that addresses the challenge of streaming speech translation with optimal timing control. The model employs a two-pass architecture with chunk-based Conformer encoding and multi-task learning framework to jointly optimize translation quality and simultaneous policy. By leveraging CTC decoders and auxiliary tasks, StreamSpeech learns alignments that guide when to read and write during streaming, achieving superior performance across different latency requirements while maintaining high-quality intermediate results.

## Method Summary
StreamSpeech uses a two-pass architecture where source speech is first encoded using a chunk-based Conformer that processes streaming inputs in local bidirectional chunks, then translated into target text hidden states, and finally converted to target speech. The model employs multi-task learning across four tasks (S2UT, AR-S2TT, ASR, NAR-S2TT) with CTC decoders to learn alignments that guide the simultaneous policy. Multi-chunk training with randomly sampled chunk sizes enables the model to adapt to different latency requirements during inference.

## Key Results
- Achieves state-of-the-art performance on CVSS-C benchmark for both offline S2ST and Simul-S2ST tasks
- Outperforms cascaded approaches with significant improvements in translation quality and latency control
- Demonstrates adaptability to different latency requirements with a single model trained using multi-chunk training

## Why This Works (Mechanism)

### Mechanism 1
StreamSpeech uses multi-task learning to jointly optimize translation and simultaneous policy. The model optimizes ASR, NAR-S2TT, AR-S2TT, and S2UT tasks together, using CTC decoders to learn alignments that guide when to read and write during streaming. Core assumption: Text alignment between source and target speech is sufficient to control the policy for simultaneous translation.

### Mechanism 2
Chunk-based Conformer enables streaming encoding while retaining bidirectional encoding within local chunks. The model divides streaming speech into chunks, where self-attention and convolution operations are bidirectional within each chunk but unidirectional between chunks. Core assumption: Local bidirectional encoding within chunks is sufficient for capturing context needed for translation while maintaining streaming capability.

### Mechanism 3
Multi-chunk training improves performance across various latency levels with a single model. During training, the chunk size C is randomly sampled from a uniform distribution, allowing the model to adapt to different latency requirements during inference. Core assumption: Training with varying chunk sizes enables the model to generalize across different latency conditions.

## Foundational Learning

- **Connectionist Temporal Classification (CTC)**: Used to model alignment between speech and text sequences of unequal lengths, which is crucial for guiding the simultaneous translation policy. Quick check: What is the purpose of the collapsing function Π in CTC?
- **Two-pass architecture**: Allows the model to first generate target text hidden states and then convert them into target speech, which is essential for direct speech-to-speech translation. Quick check: What are the two passes in the StreamSpeech architecture?
- **Multi-task learning**: Used to jointly optimize translation and simultaneous policy, providing intermediate supervision and learning alignments to guide the policy. Quick check: What tasks are jointly optimized in StreamSpeech's multi-task learning framework?

## Architecture Onboarding

- **Component map**: Streaming speech encoder (chunk-based Conformer) → Simultaneous text decoder (with CTC decoders) → Non-autoregressive text-to-unit generation (with T2U encoder and unit CTC decoder)
- **Critical path**: Streaming speech input → Chunk-based encoding → Text generation (guided by CTC decoders) → Unit generation → Speech synthesis
- **Design tradeoffs**: Chunk size C affects latency and translation quality; multi-chunk training improves generalization but may increase training complexity; two-pass architecture improves translation quality but may increase latency.
- **Failure signatures**: Poor alignment between source and target speech; excessive latency or disfluency in generated speech; failure to adapt to different latency requirements.
- **First 3 experiments**:
  1. Train StreamSpeech with fixed chunk size C and evaluate latency and translation quality under different latency conditions.
  2. Implement multi-chunk training and compare performance with fixed chunk size training.
  3. Evaluate the impact of CTC decoder accuracy on the simultaneous translation policy.

## Open Questions the Paper Calls Out

### Open Question 1
How does StreamSpeech handle out-of-vocabulary (OOV) words in real-time simultaneous translation scenarios? The paper does not discuss how the model deals with words outside its vocabulary during streaming translation, which is a critical issue for real-world applications.

### Open Question 2
What is the impact of different latency requirements on the quality of intermediate ASR results produced during simultaneous translation? While the paper claims high-quality intermediate results, it does not provide empirical evidence showing how these results vary with different latency settings.

### Open Question 3
How does StreamSpeech's performance scale when translating between languages with significantly different syntactic structures? The paper focuses on Indo-European languages with relatively similar structures to English, leaving open questions about performance on languages with more distant linguistic features.

## Limitations

- The claimed improvements over cascaded approaches may be attributable to differences in training data and hyperparameter tuning rather than the proposed architecture alone.
- The multi-task learning framework's assumption that CTC-based alignment learning provides sufficient guidance for simultaneous policy is not empirically validated.
- The model's adaptability to varying latency requirements lacks systematic evaluation across the full spectrum of real-world latency constraints.

## Confidence

- **High Confidence**: The technical feasibility of the two-pass architecture with chunk-based Conformer for streaming speech encoding.
- **Medium Confidence**: The claim of achieving state-of-the-art performance on both offline and simultaneous S2ST tasks.
- **Low Confidence**: The assertion that CTC-based alignment learning provides sufficient guidance for optimal simultaneous policy control.

## Next Checks

1. **Alignment-Policy Correlation Analysis**: Measure correlation between CTC decoder alignment quality and simultaneous policy performance metrics across varying speech rates and domain conditions.

2. **Cascaded System Benchmarking**: Implement and train a cascaded S2ST system using identical data and optimization procedures, then conduct controlled experiments comparing performance across different latency requirements.

3. **Latency Spectrum Evaluation**: Evaluate StreamSpeech's performance across a comprehensive range of latency conditions (AL from 0.5s to 5s) with speech samples varying in rate, complexity, and domain.