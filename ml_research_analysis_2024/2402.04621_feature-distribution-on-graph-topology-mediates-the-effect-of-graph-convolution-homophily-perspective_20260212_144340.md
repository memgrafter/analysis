---
ver: rpa2
title: 'Feature Distribution on Graph Topology Mediates the Effect of Graph Convolution:
  Homophily Perspective'
arxiv_id: '2402.04621'
source_url: https://arxiv.org/abs/2402.04621
tags:
- feature
- node
- graph
- features
- class
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper investigates how the dependence between graph topology
  and node features (A-X dependence) affects Graph Neural Networks (GNNs). Surprisingly,
  shuffling features among nodes of the same class consistently improves GNN performance,
  a phenomenon not well-explained by prior literature.
---

# Feature Distribution on Graph Topology Mediates the Effect of Graph Convolution: Homophily Perspective

## Quick Facts
- arXiv ID: 2402.04621
- Source URL: https://arxiv.org/abs/2402.04621
- Reference count: 40
- Primary result: Feature shuffling within classes consistently improves GNN performance by reducing Class-controlled Feature Homophily (CFH)

## Executive Summary
This paper investigates how the dependence between graph topology and node features affects Graph Neural Network performance. Surprisingly, shuffling features among nodes of the same class consistently improves GNN performance, contradicting prior assumptions. The authors propose a new measure called Class-controlled Feature Homophily (CFH) to quantify this dependence while controlling for node class effects. They develop a random graph model (CSBM-X) to control CFH and establish theory showing that CFH moderates graph convolution effects, with smaller CFH leading to stronger pull of features toward class means and improved classification.

## Method Summary
The authors propose a new measure called Class-controlled Feature Homophily (CFH) to quantify the dependence between graph topology and features while controlling for node class effects. They design a random graph model (CSBM-X) to control CFH by varying a parameter τ. The theoretical framework establishes that CFH moderates the effect of graph convolution, with smaller CFH leading to stronger pull of node features toward their class mean, improving GNN-based node classification. Empirical validation is performed on both synthetic and real-world graphs using GCNII, GPR-GNN, and AERO-GNN architectures with feature shuffling within classes.

## Key Results
- Feature shuffling within classes consistently improves GNN performance in high class-homophily graphs (mean increase of 4%)
- The effect is smaller in low class-homophily graphs (mean increase of 0.5%) and is moderated by feature informativeness
- CFH scores tend to approach zero after feature shuffles, confirming the disruption of topology-feature dependence
- Results are consistent across different GNN architectures and feature types

## Why This Works (Mechanism)

### Mechanism 1
- Claim: CFH measures dependence between graph topology and features while controlling for node class effects
- Mechanism: By subtracting class-specific feature means from each node's feature vector, CFH isolates topology-feature dependence from topology-class dependence
- Core assumption: Linear relationship between node classes and features exists, allowing class-specific means to capture class effects
- Evidence anchors:
  - [abstract] Proposes CFH to control for potential confounds while measuring A-X dependence
  - [section 3.2] Shows class-controlled features are obtained by subtracting class mean: Xi|Y = Xi − (1/|C+Yi| ∑vj∈C+Yi Xj)
  - [corpus] Weak - no direct corpus evidence found for class-controlled features concept
- Break condition: If relationship between classes and features is non-linear, class means won't adequately control for class effects

### Mechanism 2
- Claim: Smaller CFH leads to stronger pull of node features toward their class mean, improving GNN-based node classification
- Mechanism: When CFH is low, graph convolution causes features to converge toward class means more effectively, reducing intra-class variance and improving separability
- Core assumption: Graph convolution acts as a feature denoiser that pulls features toward class means
- Evidence anchors:
  - [abstract] Establishes theory that CFH moderates effect of graph convolution
  - [section 4.2] Shows Bayes error rate is minimized when CFH approaches zero (τ = 0)
  - [section 4.3] Demonstrates empirical validation showing performance improves as CFH approaches zero
- Break condition: If features are already perfectly separated or if class means are too close, reducing CFH won't help

### Mechanism 3
- Claim: Feature shuffle reduces CFH by disrupting topology-feature dependence while preserving topology-class and feature-class dependence
- Mechanism: Randomizing features within classes breaks correlation between node connectivity patterns and feature similarity
- Core assumption: Shuffling features within classes maintains feature-class dependence while reducing topology-feature dependence
- Evidence anchors:
  - [abstract] Feature shuffle disrupts dependence between graph topology and features
  - [section 3.4] Shows CFH scores tend to approach zero after feature shuffles
  - [section 5] Demonstrates consistent performance improvements with increasing shuffled node ratio
- Break condition: If feature distributions are identical across classes or if shuffling introduces too much noise

## Foundational Learning

- Concept: Graph Neural Networks (GNNs)
  - Why needed here: Core architecture being analyzed for how it responds to CFH
  - Quick check question: What is the basic operation performed by a GNN layer on node features?

- Concept: Homophily and Heterophily
  - Why needed here: CFH builds on concepts of homophily but extends to continuous features
  - Quick check question: How does CFH differ from traditional class-homophily measures?

- Concept: Stochastic Block Models (SBMs)
  - Why needed here: CSBM-X extends SBMs to control for feature dependence
  - Quick check question: What limitation of traditional SBMs does CSBM-X address?

## Architecture Onboarding

- Component map:
  CFH measure -> CSBM-X model -> Theoretical framework -> Feature shuffle algorithm -> GNN models -> Experimental validation

- Critical path: CFH → CSBM-X → Theory → Real-world validation
  Start with CFH measure design, implement CSBM-X model, derive theoretical relationship with GNNs, validate on real data

- Design tradeoffs:
  - CFH assumes linear class-feature relationship vs. capturing non-linear dependencies
  - Feature shuffle requires test labels vs. practical applicability
  - CSBM-X complexity vs. modeling real-world graph properties

- Failure signatures:
  - CFH doesn't approach zero after feature shuffle (class-control issues)
  - Performance degrades with feature shuffle (unusual graph structures like Roman-Empire)
  - Theory predictions don't match empirical results (model assumptions violated)

- First 3 experiments:
  1. Measure CFH on real-world graphs before and after feature shuffle
  2. Generate CSBM-X graphs with varying τ and test GNN performance
  3. Apply feature shuffle algorithm to high class-homophily graphs and measure performance gains

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does CFH interact with feature dimensionality in real-world graphs?
- Basis in paper: [inferred] The authors vary feature dimensionality in CSBM-X experiments and observe consistent patterns, but do not explore this systematically in real-world graphs where feature dimensionality varies widely
- Why unresolved: The paper does not analyze how CFH affects GNN performance across graphs with different feature dimensionalities in real-world datasets
- What evidence would resolve it: Empirical analysis of CFH effects on GNN performance across real-world graphs with varying feature dimensionalities (e.g., Cora-Full with 8,710 features vs. Cora with 1,433 features)

### Open Question 2
- Question: Can CFH be estimated or controlled without access to test labels?
- Basis in paper: [explicit] The feature shuffle algorithm requires test labels, which are not available in practice. The authors propose a pseudo-label-based extension but do not explore more sophisticated estimation methods
- Why unresolved: The paper only briefly mentions this limitation and proposes a simple pseudo-labeling approach without exploring alternative estimation strategies
- What evidence would resolve it: Development and validation of methods to estimate or control CFH using only training data, or exploration of semi-supervised CFH estimation techniques

### Open Question 3
- Question: How does CFH affect GNN generalization and robustness to distribution shifts?
- Basis in paper: [inferred] The authors mention that CFH may influence generalization capacity of GNNs but do not investigate this systematically
- Why unresolved: The paper focuses on node classification accuracy but does not analyze how CFH impacts GNN performance on out-of-distribution data or under distribution shifts
- What evidence would resolve it: Experiments measuring GNN performance on out-of-distribution data or under synthetic distribution shifts, with varying levels of CFH

## Limitations
- The feature shuffle algorithm requires test labels, limiting practical applicability
- The theoretical framework assumes linear class-feature relationships which may not hold for all datasets
- Some datasets with unusual graph structures show anomalous behavior not fully explained by the theory

## Confidence

**High Confidence:** The core mechanism that CFH moderates graph convolution effects and that smaller CFH improves GNN performance through feature convergence toward class means. The empirical validation across multiple architectures and datasets provides robust support for this claim.

**Medium Confidence:** The theoretical framework connecting CFH to Bayes error rates and the specific mathematical relationship between CFH and GNN performance. While theoretically sound, some assumptions may not hold perfectly in practice.

**Low Confidence:** The practical applicability of the feature shuffle algorithm without test labels, and the generalizability of the theory to all graph types, particularly those with unusual structures or non-linear feature-class relationships.

## Next Checks

1. **Non-linear CFH Extension:** Implement and validate a non-linear version of the CFH measure that can capture more complex class-feature relationships beyond the current linear assumption.

2. **Real-world Shuffle Implementation:** Develop and test the pseudo-label-based feature shuffle algorithm on real-world datasets to evaluate its practical effectiveness and compare results with the idealized version requiring test labels.

3. **Structure-specific Analysis:** Conduct a detailed analysis of datasets showing anomalous behavior (like Roman-Empire) to understand what specific graph properties cause deviations from theoretical predictions, potentially leading to extended theory modifications.