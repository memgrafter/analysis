---
ver: rpa2
title: Stochastic Gradient Descent for Two-layer Neural Networks
arxiv_id: '2407.07670'
source_url: https://arxiv.org/abs/2407.07670
tags:
- neural
- convergence
- networks
- algorithm
- holds
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper analyzes the convergence rates of stochastic gradient
  descent (SGD) applied to overparameterized two-layer neural networks using the Neural
  Tangent Kernel (NTK) framework. The authors establish sharp convergence rates for
  the last iterate of SGD, improving upon previous work that required exponentially
  many neurons.
---

# Stochastic Gradient Descent for Two-layer Neural Networks

## Quick Facts
- arXiv ID: 2407.07670
- Source URL: https://arxiv.org/abs/2407.07670
- Authors: Dinghao Cao; Zheng-Chu Guo; Lei Shi
- Reference count: 39
- This paper analyzes SGD convergence rates for overparameterized two-layer neural networks using NTK framework

## Executive Summary
This paper establishes sharp convergence rates for stochastic gradient descent (SGD) applied to overparameterized two-layer neural networks using the Neural Tangent Kernel (NTK) framework. The authors demonstrate that SGD converges at a rate of $O(T^{-2r/(2r+1)})$ or $O(T^{-2r/(2r+1)}+\epsilon)$ for target functions with regularity $r$, where $T$ is the number of iterations. Crucially, they show that the number of neurons needed depends polynomially on $T$ rather than exponentially, specifically $O(T^{1+5/(2r+1)})$, representing a significant improvement over previous work.

## Method Summary
The authors analyze SGD convergence for two-layer neural networks by combining NTK approximation theory with convergence analysis in the Reproducing Kernel Hilbert Space (RKHS) generated by NTK. They establish a key approximation error bound showing that the NTK of a two-layer network with $m$ neurons approximates the target function with error $O(m^{-r/(2r+1)})$. This is then combined with a refined analysis of the SGD iterates in the RKHS to obtain the sharp convergence rates. The analysis carefully tracks the dependence on the number of neurons and shows that polynomial scaling is sufficient to maintain optimal convergence rates.

## Key Results
- SGD converges at rate $O(T^{-2r/(2r+1)})$ or $O(T^{-2r/(2r+1)}+\epsilon)$ for target functions with regularity $r$
- Number of neurons needed scales polynomially as $O(T^{1+5/(2r+1)})$ rather than exponentially
- Analysis combines NTK approximation with RKHS convergence theory
- Relaxes previous constraints on neuron numbers while maintaining optimal rates

## Why This Works (Mechanism)
The paper leverages the NTK framework where infinitely wide two-layer networks behave linearly during training. In this regime, SGD can be analyzed as a linear dynamical system in the function space. The key insight is that the NTK approximation error scales as $O(m^{-r/(2r+1)})$ with $m$ neurons, and this error can be balanced against the SGD optimization error by choosing $m$ polynomially in $T$. This allows achieving the optimal convergence rate without requiring exponentially many neurons.

## Foundational Learning
**Neural Tangent Kernel (NTK):** The kernel describing the behavior of infinitely wide neural networks during training. Needed because it allows linearizing the network dynamics for analysis. Quick check: verify NTK is positive definite and characterizes the function space.

**Reproducing Kernel Hilbert Space (RKHS):** The function space associated with a kernel where evaluation functionals are bounded. Needed because convergence analysis can be performed in this space. Quick check: confirm the NTK is Mercer and generates a valid RKHS.

**Approximation Error Bounds:** Quantify how well finite networks approximate target functions. Needed to relate network size to function approximation quality. Quick check: verify the polynomial decay rate with respect to network width.

## Architecture Onboarding
**Component Map:** NTK Approximation -> SGD Dynamics in RKHS -> Convergence Rate Analysis
**Critical Path:** Target function → NTK approximation → SGD iterates → Convergence bound
**Design Tradeoffs:** Neuron count vs. convergence rate (polynomial vs. exponential scaling)
**Failure Signatures:** If neuron count is too small, convergence rate degrades; if too large, computational cost increases
**First Experiments:**
1. Verify NTK approximation error scales as predicted with varying neuron count
2. Test SGD convergence rate on synthetic target functions with known regularity
3. Compare theoretical vs. empirical neuron requirements for achieving target accuracy

## Open Questions the Paper Calls Out
None

## Limitations
- Theoretical guarantees rely on NTK regime with infinite-width assumptions
- Polynomial neuron scaling ($O(T^{1+5/(2r+1)})$) still represents significant resource demands
- Analysis focuses on regression with square loss, may not extend to classification
- Limited applicability to deep networks beyond two layers

## Confidence
- **High** confidence in mathematical derivation and convergence proofs within NTK framework
- **Medium** confidence in practical relevance given gap between theory and practice
- **Low** confidence in direct applicability to deep networks beyond two-layer case

## Next Checks
1. Empirical validation on synthetic datasets with known target functions to verify predicted convergence rates match theoretical predictions
2. Numerical experiments comparing actual neuron requirements against theoretical $O(T^{1+5/(2r+1)})$ bound across different values of $r$ and $T$
3. Extension tests to verify whether similar polynomial bounds can be established for multi-layer networks or alternative loss functions beyond square loss