---
ver: rpa2
title: Fine-tuning the SwissBERT Encoder Model for Embedding Sentences and Documents
arxiv_id: '2405.07513'
source_url: https://arxiv.org/abs/2405.07513
tags:
- language
- swissbert
- zeitung
- articles
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The authors fine-tuned the SwissBERT model for sentence and document
  embedding using contrastive learning on Swiss news articles. The resulting SentenceSwissBERT
  model was evaluated on document retrieval and text classification tasks in German,
  French, Italian, and Romansh.
---

# Fine-tuning the SwissBERT Encoder Model for Embedding Sentences and Documents

## Quick Facts
- arXiv ID: 2405.07513
- Source URL: https://arxiv.org/abs/2405.07513
- Reference count: 13
- Primary result: SentenceSwissBERT achieved up to 94.33% accuracy on document retrieval and 78.49 weighted F1-score on text classification across German, French, Italian, and Romansh.

## Executive Summary
This paper presents SentenceSwissBERT, a fine-tuned version of the SwissBERT model for sentence and document embedding using contrastive learning on Swiss news articles. The model demonstrates significant improvements over both the original SwissBERT and a multilingual baseline, particularly for the low-resource Romansh language. The authors evaluate on document retrieval and text classification tasks using the 20 Minuten dataset, showing that domain-specific fine-tuning can substantially enhance performance for Switzerland-related NLP tasks.

## Method Summary
The authors fine-tuned SwissBERT using SimCSE contrastive learning with title-body pairs from Swiss news articles. The model uses language adapters for German, French, Italian, and Romansh, and employs MEAN pooling for generating 768-dimensional embeddings. Training was conducted on over 1.5 million Swiss news articles with a batch size of 512, learning rate of 1e-5, and temperature of 0.05 for one epoch. Evaluation was performed on the 20 Minuten dataset using cosine similarity for document retrieval and nearest neighbor approach for text classification.

## Key Results
- Document retrieval accuracy reached up to 94.33%
- Text classification achieved up to 78.49 weighted F1-score
- Romansh performance improved by up to 55 percentage points over the original SwissBERT baseline

## Why This Works (Mechanism)

### Mechanism 1
Contrastive learning with title-body pairs improves sentence embedding quality by enforcing semantic similarity for related pairs and dissimilarity for unrelated ones. The SimCSE training objective uses cosine similarity between title and body embeddings with temperature-scaled softmax loss, pushing semantically related pairs close together in vector space.

### Mechanism 2
Language adapters allow the model to maintain separate linguistic representations for each language while sharing core encoder parameters. The X-MOD architecture with language adapters means each language has its own adapter module that is switched based on input language, enabling efficient multilingual embeddings.

### Mechanism 3
Fine-tuning on Swiss-specific news articles provides domain adaptation that improves performance on Switzerland-related tasks. Training on 21M+ Swiss news articles and fine-tuning on 1.5M from the same domain creates embeddings that better capture semantic relationships in Switzerland-related text.

## Foundational Learning

- Concept: Contrastive learning and SimCSE training objective
  - Why needed here: Understanding how the contrastive loss works is crucial for debugging training issues and interpreting results
  - Quick check question: What happens to the loss if we remove the temperature hyperparameter τ from the SimCSE objective?

- Concept: Language adapter architecture (X-MOD)
  - Why needed here: The adapter mechanism is central to how the model handles multiple languages and why it can be fine-tuned efficiently
  - Quick check question: How does the model know which language adapter to use during training and inference?

- Concept: Sentence embedding techniques (MEAN pooling vs CLS token)
  - Why needed here: Different pooling strategies can significantly impact embedding quality for downstream tasks
  - Quick check question: Why did the authors choose MEAN pooling over CLS token for this task?

## Architecture Onboarding

- Component map: SwissBERT base model (X-MOD architecture) -> Language adapters (4 total: DE, FR, IT, RM) -> Contrastive learning head (SimCSE loss) -> Input pipeline (title-body pairs from Swiss news) -> Output layer (768-dim embeddings via MEAN pooling)

- Critical path: 1. Input Swiss news article → split into title and body 2. Language detection → select appropriate adapter 3. Forward pass through base model + adapter 4. MEAN pooling of hidden states → embedding 5. Compute SimCSE loss with title-body pairs 6. Backpropagate and update parameters

- Design tradeoffs: Adapter-based vs full fine-tuning (more parameter-efficient but may capture less nuance), domain-specific vs general pre-training (relevant domain knowledge vs reduced generalization), SimCSE vs supervised contrastive learning (simpler unsupervised vs potentially better with labeled data)

- Failure signatures: Poor multilingual performance (adapter switching not working correctly), degraded embeddings (contrastive loss not converging - check temperature, batch size), overfitting (model memorizing training pairs - monitor validation set), language imbalance (one language dominating - check data distribution)

- First 3 experiments: 1. Ablation study: Train without language adapters to see impact on multilingual performance 2. Cross-lingual transfer test: Fine-tune on German only, evaluate on all four languages 3. Domain adaptation test: Fine-tune on non-Swiss news articles, compare performance on same evaluation tasks

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions, but several emerge from the limitations section regarding generalizability to non-Swiss domains and the impact of using cross-lingual examples during fine-tuning.

## Limitations
- Dramatic improvements for Romansh (up to 55 percentage points) raise concerns about potential data leakage or overfitting
- Evaluation relies entirely on the 20 Minuten dataset, which may not represent the diversity of Swiss news content or other domains
- Contrastive learning assumes titles and bodies are semantically related, but this relationship may not always hold for clickbait or poorly written articles

## Confidence
- High confidence: Document retrieval performance claims (up to 94.33% accuracy)
- Medium confidence: Text classification performance (up to 78.49 weighted F1)
- Medium confidence: Romansh improvement claims
- Low confidence: Generalizability to non-Swiss domains

## Next Checks
1. Cross-domain evaluation: Test SentenceSwissBERT on news articles from non-Swiss sources to assess domain-specific fine-tuning impact
2. Robustness testing for Romansh: Verify the 55 percentage point improvement using multiple Romansh datasets and ablation studies
3. Quality filtering analysis: Implement data quality filtering for title-body pairs to determine impact of noisy or misleading pairs on embedding quality