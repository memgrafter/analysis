---
ver: rpa2
title: On the benefits of pixel-based hierarchical policies for task generalization
arxiv_id: '2407.19142'
source_url: https://arxiv.org/abs/2407.19142
tags:
- hierarchical
- tasks
- policy
- policies
- task
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Hierarchical reinforcement learning (HRL) has been explored for
  improving task generalization in multi-task environments. While flat policies can
  learn new tasks, they often struggle with compositionality and long-horizon challenges.
---

# On the benefits of pixel-based hierarchical policies for task generalization

## Quick Facts
- arXiv ID: 2407.19142
- Source URL: https://arxiv.org/abs/2407.19142
- Authors: Tudor Cristea-Platon; Bogdan Mazoure; Josh Susskind; Walter Talbott
- Reference count: 2
- Hierarchical reinforcement learning outperforms flat policies on training speed, reward generalization, and state-space generalization in pixel-based robotic control tasks

## Executive Summary
This paper explores how hierarchical reinforcement learning (HRL) can improve task generalization in multi-task environments compared to flat policies. While flat policies can learn individual tasks, they struggle with compositionality and long-horizon challenges that HRL addresses by decomposing tasks into smaller subtasks. The authors implement a hierarchical policy architecture and evaluate it on simulated robotic control tasks from pixel observations. Results show that HRL achieves faster training, better generalization to out-of-distribution rewards and state spaces, and requires less complex fine-tuning for novel tasks.

## Method Summary
The study compares Director (a hierarchical policy) against DreamerV3 (a flat policy) on multi-task pixel-based robotic control tasks from DeepMind Control Suite. Both architectures use world models to encode image observations into latent states, but Director introduces a high-level manager that sets latent space goals for a low-level worker policy. Task conditioning is provided through reward signals or visual targets. The hierarchical approach aims to reduce effective task horizon and enable zero-shot generalization through composability of learned skills. Performance is evaluated on training tasks, out-of-distribution speed variations, and novel maze navigation tasks.

## Key Results
- Hierarchical policies show improved performance on training tasks compared to flat policies
- HRL demonstrates better reward and state-space generalization to out-of-distribution tasks
- Fine-tuning hierarchical policies requires less complexity, with frozen low-level components achieving rapid adaptation to novel tasks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Hierarchical policies reduce effective task horizon, leading to faster learning.
- Mechanism: By decomposing a long-horizon task into shorter subtasks, the high-level policy only needs to make fewer decisions, while the low-level policy handles shorter sequences of primitive actions.
- Core assumption: The low-level policies are composable and transferable across tasks.
- Evidence anchors:
  - [abstract] "HRL addresses these issues by decomposing tasks into smaller subtasks, allowing for better generalization and sample efficiency."
  - [section 3] "Consider the task of bipedal locomotion... decomposing a long horizon problem of length H into k smaller sub-problem of length H/k can accelerate learning."
  - [corpus] Weak; no direct corpus support for horizon reduction claims.
- Break condition: If low-level policies are not composable or transferable, the benefit of horizon reduction disappears.

### Mechanism 2
- Claim: Hierarchical policies enable zero-shot generalization through compositionality of learned skills.
- Mechanism: The high-level policy can recombine learned low-level skills in novel ways to solve tasks with different reward structures or state-space configurations without retraining low-level policies.
- Core assumption: Learned low-level policies are temporally interchangeable and can be recombined to solve novel tasks.
- Evidence anchors:
  - [section 3] "If the learned low-level policies are indeed composable, they can be used to solve tasks with different reward structures and topologies of the state space."
  - [section 4.2] "For out-of-distribution speeds we notice that the hierarchical policy is able to consistently outperform the flat one."
  - [corpus] Weak; no direct corpus support for compositionality claims.
- Break condition: If low-level policies are not composable, the hierarchical policy cannot generalize to novel tasks.

### Mechanism 3
- Claim: Hierarchical policies decrease fine-tuning complexity for novel tasks.
- Mechanism: By freezing low-level policies and only updating the high-level policy, the number of parameters that need adaptation is reduced, leading to faster convergence on novel tasks.
- Core assumption: Low-level policies learned during training are sufficient for the novel task and only need appropriate goal selection.
- Evidence anchors:
  - [section 4.3] "Fine-tuning only the higher level policy and the world model is sufficient to achieve success in a rapid and stable manner."
  - [section 4.3] "Fine-tuning the world model is required as the task presents new geometries, such as corners, and a doubling in the number of wall colors."
  - [corpus] Weak; no direct corpus support for fine-tuning complexity claims.
- Break condition: If low-level policies are not sufficient for the novel task, fine-tuning the entire hierarchy becomes necessary.

## Foundational Learning

- Concept: Temporal abstraction in reinforcement learning
  - Why needed here: Hierarchical policies operate at different time scales, with high-level policies setting goals that low-level policies execute over extended periods.
  - Quick check question: What is the difference between primitive actions and composite actions in a hierarchical setting?

- Concept: World models for pixel-based control
  - Why needed here: The hierarchical policy uses a learned world model to encode image observations into latent states, which are then used for planning and goal setting.
  - Quick check question: How does the world model help the agent plan in latent space rather than pixel space?

- Concept: Task conditioning in multi-task reinforcement learning
  - Why needed here: The agent receives task information (like reward signals or target locations) as input to adapt its behavior to different tasks.
  - Quick check question: What are the different ways task information can be provided to a reinforcement learning agent?

## Architecture Onboarding

- Component map:
  - Image observations → World Model → Manager (High-level policy) → VAE decoding → Worker (Low-level policy) → Environment

- Critical path: Image observations → World Model → Manager → VAE decoding → Worker → Environment

- Design tradeoffs:
  - Goal horizon length: Longer horizons reduce manager decisions but require more capable low-level policies
  - World model complexity: More complex models may better capture environment dynamics but increase training time
  - Task conditioning method: Different conditioning approaches may affect generalization capabilities

- Failure signatures:
  - Poor performance on training tasks: May indicate issues with world model learning or policy training
  - Lack of generalization: Could suggest low-level policies are not sufficiently composable
  - Slow fine-tuning: Might indicate over-reliance on low-level policy adaptation

- First 3 experiments:
  1. Train hierarchical and flat policies on a simple locomotion task with varying speeds to compare convergence rates
  2. Evaluate both policies on out-of-distribution speeds to test generalization capabilities
  3. Fine-tune both policies on a novel navigation task with frozen low-level components to assess adaptation efficiency

## Open Questions the Paper Calls Out
None

## Limitations
- Claims about low-level policy composability lack direct experimental validation
- Evidence for reduced fine-tuning complexity is based on a single novel task (maze navigation)
- The paper doesn't directly measure the claimed reduction in effective task horizon

## Confidence

- **High confidence**: Hierarchical policies outperform flat policies on training tasks and show improved sample efficiency
- **Medium confidence**: Hierarchical policies demonstrate better generalization to out-of-distribution speeds and novel state spaces
- **Low confidence**: Claims about reduced fine-tuning complexity and inherent composability of low-level policies

## Next Checks

1. **Horizon Analysis**: Measure and compare the effective task horizon between hierarchical and flat policies during training to quantify the claimed reduction in decision complexity.

2. **Composability Stress Test**: Systematically evaluate the ability to recombine low-level policies learned in different training tasks to solve novel composite tasks, testing the core assumption of policy compositionality.

3. **Fine-tuning Ablation**: Compare fine-tuning efficiency when freezing only the high-level policy versus freezing the entire hierarchy, and test on multiple novel tasks with varying degrees of state-space similarity to training environments.