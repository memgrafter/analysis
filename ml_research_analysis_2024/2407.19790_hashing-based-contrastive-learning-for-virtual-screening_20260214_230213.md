---
ver: rpa2
title: Hashing based Contrastive Learning for Virtual Screening
arxiv_id: '2407.19790'
source_url: https://arxiv.org/abs/2407.19790
tags:
- drughash
- protein
- learning
- methods
- hashing
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: DrugHash is a hashing-based contrastive learning method for virtual
  screening that addresses the memory and time costs of screening billion-scale molecular
  databases. The core method treats virtual screening as a retrieval task using efficient
  binary hash codes, enabling end-to-end learning of binary hash codes for both protein
  and molecule modalities through a simple yet effective hashing strategy.
---

# Hashing based Contrastive Learning for Virtual Screening

## Quick Facts
- arXiv ID: 2407.19790
- Source URL: https://arxiv.org/abs/2407.19790
- Authors: Jin Han; Yun Hong; Wu-Jun Li
- Reference count: 40
- Primary result: DrugHash achieves state-of-the-art accuracy with 32× memory saving and 3.5× speed improvement

## Executive Summary
DrugHash is a hashing-based contrastive learning method for virtual screening that addresses the memory and time costs of screening billion-scale molecular databases. The core method treats virtual screening as a retrieval task using efficient binary hash codes, enabling end-to-end learning of binary hash codes for both protein and molecule modalities through a simple yet effective hashing strategy. DrugHash outperforms existing methods in accuracy, achieving state-of-the-art results with significant improvements across multiple metrics on DUD-E and LIT-PCBA benchmarks.

## Method Summary
DrugHash uses contrastive learning with binary hash codes for proteins and molecules, with a SE(3) Transformer encoder pre-trained on protein pockets and molecules. The model is trained using a contrastive learning objective (infoNCE loss) and cross-modal hashing objective on augmented PDBBind data. The training procedure involves optimizing with gradient accumulation every four steps. DrugHash treats virtual screening as a retrieval task using efficient binary hash codes, enabling end-to-end learning of binary hash codes for both protein and molecule modalities.

## Key Results
- Achieves state-of-the-art accuracy on DUD-E and LIT-PCBA benchmarks
- Memory saving of 32× compared to existing methods
- Speed improvement of 3.5× for retrieval tasks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Binary hash codes reduce memory footprint by 32× while preserving discriminative power for retrieval.
- Mechanism: Real-valued vectors of length d require d×4 bytes per entry; binary hash codes require 1 bit per entry. The hashing strategy maps each protein/molecule into d-bit codes while preserving cosine similarity through a contrastive loss and a regularization term that pulls representations toward binary values.
- Core assumption: The sign function approximates the optimal binarization without destroying the contrastive structure learned in embedding space.
- Evidence anchors:
  - [abstract] "memory saving of 32×"
  - [section] "Lhash = 1/nd Σ_k (∥yp_k − bp_k∥² + ∥ym_k − bm_k∥²)"
  - [corpus] No direct evidence; missing comparison of binary vs real-valued similarity preservation.
- Break condition: If the regularization λ is too small, hash codes deviate from true binary values, destroying the memory advantage; if too large, contrastive alignment collapses.

### Mechanism 2
- Claim: Binary Hamming distance enables 3.5× faster similarity search than real-valued vector comparison.
- Mechanism: Hamming distance between d-bit codes is computed with a single XOR and popcount, O(d) bit operations, whereas cosine similarity on real vectors requires O(d) floating-point multiplies and sums. In billion-scale databases, this difference is multiplicative.
- Core assumption: Hamming distance correlates strongly with the cosine similarity of the underlying real embeddings.
- Evidence anchors:
  - [abstract] "speed improvement of 3.5×"
  - [section] "rank the molecules ... based on Hamming distance"
  - [corpus] No direct throughput or latency measurements; only asymptotic claim.
- Break condition: If hash codes are poorly aligned (high collision rate), retrieval accuracy degrades and speed advantage diminishes due to many false positives.

### Mechanism 3
- Claim: The hashing strategy acts as a regularizer, improving generalization and outperforming real-valued baselines.
- Mechanism: The Lhash term constrains the embedding space to a binary lattice, reducing model capacity and forcing features to be more discriminative. This combats overfitting observed in contrastive models without hashing.
- Core assumption: The binary constraint does not remove essential variance needed for task performance.
- Evidence anchors:
  - [abstract] "outperform existing methods in terms of accuracy"
  - [section] "serves as a regularization term to reduce model overfitting"
  - [corpus] No ablation showing performance with vs without hashing during training; only zero-shot sign comparison.
- Break condition: If λ is mis-tuned, the regularization either under-constrains (no benefit) or over-constrains (accuracy loss).

## Foundational Learning

- Concept: Contrastive learning objective (infoNCE)
  - Why needed here: Aligns protein and molecule representations in shared embedding space without labels.
  - Quick check question: What role does the temperature τ play in the infoNCE loss formulation?

- Concept: Cross-modal hashing
  - Why needed here: Enables efficient retrieval across two different data modalities (protein pockets vs small molecules).
  - Quick check question: Why must hash functions be learned jointly rather than independently for each modality?

- Concept: Regularization via binarization
  - Why needed here: Prevents overfitting in high-capacity encoders when training data is limited.
  - Quick check question: How does the Lhash term differ from typical L2 weight decay in effect?

## Architecture Onboarding

- Component map:
  - Input: Atom coordinates and pairwise distance encodings
  - Encoder: Pre-trained SE(3) Transformer (Ep, Em)
  - Contrastive head: InfoNCE loss on real embeddings
  - Hashing head: Lhash loss pulling embeddings toward binary codes
  - Output: Binary hash codes (bp_k, bm_k) for retrieval

- Critical path:
  1. Encode protein and molecule → real vectors
  2. Apply sign function → binary codes
  3. Compute Hamming distances → ranked retrieval

- Design tradeoffs:
  - Longer codes → better accuracy but higher memory/time
  - Larger λ → stronger regularization but risk of underfitting
  - Pre-trained vs. trained-from-scratch encoders → speed vs. domain specificity

- Failure signatures:
  - High collision rate in hash codes → poor precision
  - Loss divergence in Lhash → unstable training
  - Slow retrieval despite binary codes → bug in Hamming distance implementation

- First 3 experiments:
  1. Train with λ=0 (no hashing) and compare validation BEDROC to baseline to confirm overfitting claim.
  2. Vary code length {64,128,256,512} and plot accuracy vs. memory usage.
  3. Measure actual retrieval latency on a 1M molecule subset vs. real-valued baseline.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the proposed hashing strategy affect the model's performance on larger molecular databases beyond the REAL database (e.g., 100B molecules)?
- Basis in paper: [inferred] The paper discusses memory savings and speed improvements on the REAL database but does not explore performance on larger databases.
- Why unresolved: The experiments were conducted on databases with up to 6.75 billion molecules, but the scalability to larger databases is not addressed.
- What evidence would resolve it: Testing DrugHash on molecular databases with 100 billion or more molecules and comparing its performance, memory usage, and speed with existing methods.

### Open Question 2
- Question: What is the impact of using different protein and molecule encoders on DrugHash's performance?
- Basis in paper: [explicit] The paper mentions that various encoders can be adopted but only uses the same encoder as DrugCLIP for demonstration.
- Why unresolved: The paper does not explore the effect of using different encoders on the model's accuracy and efficiency.
- What evidence would resolve it: Conducting experiments with different encoders and comparing the performance, memory usage, and speed of DrugHash with each encoder.

### Open Question 3
- Question: How does the choice of code length affect DrugHash's performance on different virtual screening tasks?
- Basis in paper: [explicit] The paper studies the influence of code length on accuracy but does not explore its impact on different tasks.
- Why unresolved: The experiments focus on the overall accuracy, but the trade-off between code length and task-specific performance is not addressed.
- What evidence would resolve it: Testing DrugHash with different code lengths on various virtual screening tasks (e.g., early recognition, binding affinity prediction) and analyzing the trade-offs.

### Open Question 4
- Question: What is the impact of using larger training datasets (e.g., BioLip and ChEMBL) on DrugHash's performance?
- Basis in paper: [inferred] The paper mentions that larger complex datasets could be used for training but does not explore their impact on performance.
- Why unresolved: The experiments are conducted using PDBBind as the training set, but the effect of using larger datasets is not addressed.
- What evidence would resolve it: Training DrugHash using larger datasets like BioLip and ChEMBL and comparing its performance, memory usage, and speed with the current implementation.

## Limitations
- No direct experimental evidence comparing binary vs real-valued similarity preservation
- No concrete throughput/latency measurements for the claimed 3.5× speed improvement
- No ablation studies showing the impact of hashing during training versus only at inference

## Confidence

- Memory saving of 32×: High (direct calculation from binary encoding)
- Speed improvement of 3.5×: Low (no direct measurements provided)
- Accuracy improvements on DUD-E and LIT-PCBA: Medium (benchmark results shown but limited ablation)
- Hashing as effective regularizer: Low (no training-time hashing ablation)

## Next Checks

1. Measure actual retrieval latency on 1M molecule subset comparing binary Hamming distance vs real-valued cosine similarity
2. Train with λ=0 during training (no hashing regularization) and compare validation BEDROC to confirm regularization claim
3. Perform ablation study varying code length {64,128,256,512} to establish accuracy vs memory/time tradeoff curve