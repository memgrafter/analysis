---
ver: rpa2
title: 'LlamaTurk: Adapting Open-Source Generative Large Language Models for Low-Resource
  Language'
arxiv_id: '2405.07745'
source_url: https://arxiv.org/abs/2405.07745
tags:
- language
- tuning
- instruction
- training
- task-specific
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study systematically evaluates methods for adapting open-source
  generative large language models to low-resource languages. The core approach involves
  continual training, instruction fine-tuning, task-specific fine-tuning, and vocabulary
  extension.
---

# LlamaTurk: Adapting Open-Source Generative Large Language Models for Low-Resource Language

## Quick Facts
- **arXiv ID**: 2405.07745
- **Source URL**: https://arxiv.org/abs/2405.07745
- **Reference count**: 18
- **Primary result**: Continual training, instruction fine-tuning, and task-specific tuning improve low-resource language LLM adaptation; vocabulary extension provides no benefits.

## Executive Summary
This study systematically evaluates methods for adapting open-source generative large language models to low-resource languages, using Turkish as a case study. The research demonstrates that continual training improves language comprehension (lower perplexity), task-specific fine-tuning enhances downstream task performance, and larger models (13B vs 7B) improve few-shot task performance. Surprisingly, vocabulary extension with 1000 Turkish-specific tokens provided no benefits. The adapted LlamaTurk-7b-c-i-t model achieved 77.65% accuracy on Turkish sentiment analysis, 55.25% on XCOPA, and 32.83% on Belebele, outperforming both the original Llama and various adaptation configurations.

## Method Summary
The LlamaTurk adaptation pipeline involves four main methods applied to Llama-7b and Llama-13b models: continual training with Turkish Wikipedia corpus, instruction fine-tuning using translated Alpaca instructions, task-specific fine-tuning on Turkish datasets, and vocabulary extension with 1000 Turkish tokens. The study systematically evaluates all combinations of these methods, comparing them against the original Llama models and multilingual models. Evaluation uses perplexity scores for language comprehension and accuracy scores on sentiment analysis, XCOPA, and Belebele datasets for downstream task performance.

## Key Results
- Continual training reduces perplexity from 6.32 to 5.67 on Turkish data
- Task-specific fine-tuning improves downstream task accuracy (77.65% on sentiment analysis, 55.25% on XCOPA, 32.83% on Belebele)
- Vocabulary extension with 1000 Turkish tokens provides no performance benefits
- Larger 13B models improve few-shot task performance but not zero-shot or perplexity
- Multilingual models perform worse than adapted monolingual models

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Continual training with low-resource raw text improves language comprehension (lower perplexity) for adapted models.
- Mechanism: Gradual exposure to new language data allows the model to incrementally learn language-specific structures without catastrophically forgetting prior knowledge.
- Core assumption: The low-resource language corpus is sufficiently large and representative to capture essential language properties.
- Evidence anchors:
  - [abstract] "The results show that continual training improves language comprehension, as reflected in perplexity scores"
  - [section 3.1] "continual training is the process of extending the pretraining phase of LLMs by incorporating new data corpus"
  - [corpus] Weak - corpus neighbors don't directly address continual training effectiveness
- Break condition: If the low-resource language corpus is too small or unrepresentative, the model cannot learn meaningful language structures.

### Mechanism 2
- Claim: Task-specific fine-tuning improves downstream task performance even though it may worsen perplexity scores.
- Mechanism: Specialized training on task-specific instructions creates task-optimized representations that generalize better to the target task, even at the cost of general language modeling ability.
- Core assumption: The task-specific instruction set is well-designed and covers the task space adequately.
- Evidence anchors:
  - [abstract] "task-specific tuning generally enhances performance of downstream tasks"
  - [section 4.2] "task-specific tuning improves the performance of sentiment analysis" and "task-specific tuned model is good at zero-shot inference"
  - [corpus] Weak - corpus neighbors don't directly address task-specific tuning
- Break condition: If the task-specific dataset is too small or unrepresentative, the model may overfit to training examples without learning generalizable task patterns.

### Mechanism 3
- Claim: Adapting larger models (13B vs 7B) improves few-shot task performance but not zero-shot or perplexity.
- Mechanism: Larger models have more parameters to capture task-specific patterns, making them more effective when given even limited task examples.
- Core assumption: The relationship between model size and task performance follows the same pattern as for high-resource languages.
- Evidence anchors:
  - [abstract] "while larger models improve task performance with few-shot tuning"
  - [section 4.4] "LlamaTurk-13b improves the performance of downstream task when it is applied with task-specific tuning and few-shot evaluation"
  - [corpus] Weak - corpus neighbors don't directly address model size effects
- Break condition: If the task-specific data is insufficient to leverage the larger model's capacity, the performance gains may not materialize.

## Foundational Learning

- **Concept**: Perplexity as a measure of language model quality
  - **Why needed here**: The paper uses perplexity to evaluate how well adapted models understand the target language
  - **Quick check question**: What does a lower perplexity score indicate about a language model's performance?

- **Concept**: Catastrophic forgetting in continual learning
  - **Why needed here**: The paper discusses continual training while avoiding catastrophic forgetting of original English knowledge
  - **Quick check question**: What is catastrophic forgetting and why is it a concern when adapting models to new languages?

- **Concept**: Few-shot vs zero-shot learning
  - **Why needed here**: The paper evaluates task performance under different shot scenarios and observes different effects
  - **Quick check question**: How does few-shot learning differ from zero-shot learning in terms of evaluation methodology?

## Architecture Onboarding

- **Component map**: Llama-7b/13b → continual training → instruction tuning → task-specific tuning → vocabulary extension
- **Critical path**: For maximum performance, the critical path appears to be: continual training → instruction tuning → task-specific tuning (LlamaTurk-7b-c-i-t)
- **Design tradeoffs**: Vocabulary extension adds parameters but provides no benefit; larger models improve few-shot performance but increase computational cost
- **Failure signatures**: High perplexity scores indicate poor language comprehension; low downstream task accuracy indicates poor task adaptation; vocabulary extension that increases perplexity indicates ineffective token integration
- **First 3 experiments**:
  1. Run perplexity evaluation on xquad and dbricks datasets for the base Llama model
  2. Apply continual training and measure perplexity improvement
  3. Apply instruction tuning and compare perplexity scores to continual training results

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: How would vocabulary extension performance change with different numbers of new tokens added to the vocabulary?
- **Basis in paper**: Inferred from the paper's observation that vocabulary extension with 28k new tokens did not improve perplexity scores, and the authors suggest more experimentation is needed to understand if a different number of new tokens would work better.
- **Why unresolved**: The paper only tested vocabulary extension with one specific number of new tokens (28k). The impact of different vocabulary sizes on adaptation performance remains unexplored.
- **What evidence would resolve it**: Experiments varying the number of new tokens added during vocabulary extension, testing multiple configurations (e.g., 10k, 50k, 100k tokens) and measuring their impact on perplexity and downstream task performance.

### Open Question 2
- **Question**: What is the optimal combination of adaptation methods for different types of downstream tasks and datasets?
- **Basis in paper**: Inferred from the observation that different adaptation method combinations performed differently on various datasets (e.g., continual training with instruction tuning worked well on databricks-instruction but not on other datasets).
- **Why unresolved**: The paper only tested a limited set of combinations on specific datasets. The effectiveness of different method combinations may vary depending on task characteristics, dataset properties, and target language features.
- **What evidence would resolve it**: Systematic experiments testing all possible combinations of adaptation methods (continual training, instruction tuning, task-specific tuning, vocabulary extension) across diverse downstream tasks and datasets, measuring performance metrics like perplexity and accuracy.

### Open Question 3
- **Question**: How does the quality and quantity of training data affect the performance of each adaptation method?
- **Basis in paper**: Inferred from the paper's observation that vocabulary extension might not be suitable for small-scale continual training and that task-specific tuning improved performance with 5k instances but not with smaller datasets.
- **Why unresolved**: The paper used fixed amounts of data for each adaptation method. The relationship between data quality/quantity and adaptation method effectiveness remains unexplored.
- **What evidence would resolve it**: Controlled experiments varying the size and quality of training data for each adaptation method, measuring how performance scales with data quantity and how data quality affects method effectiveness.

## Limitations
- The evaluation of vocabulary extension lacks sufficient negative results analysis to understand why it failed
- The study focuses exclusively on Turkish as the low-resource language case study
- Only one multilingual model (MaLA-10b) was tested for comparison

## Confidence
- **High confidence**: The finding that continual training improves perplexity scores is well-supported by experimental results
- **Medium confidence**: The claim that task-specific fine-tuning enhances downstream task performance is supported by accuracy improvements
- **Low confidence**: The conclusion that vocabulary extension offers no benefits lacks sufficient negative results analysis

## Next Checks
1. Conduct ablation studies on the 1000 added Turkish tokens to identify which specific tokens contribute to perplexity degradation
2. Test additional multilingual models (e.g., BLOOM, XGLM) to determine if the poor performance is model-specific
3. Apply the LlamaTurk adaptation pipeline to at least two additional low-resource languages with different linguistic properties