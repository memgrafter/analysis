---
ver: rpa2
title: 'On normalization-equivariance properties of supervised and unsupervised denoising
  methods: a survey'
arxiv_id: '2402.15352'
source_url: https://arxiv.org/abs/2402.15352
tags:
- image
- denoising
- noisy
- neural
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This survey examines supervised and unsupervised image denoising
  methods, focusing on normalization-equivariance properties. The paper reviews neural
  network architectures including MLPs, CNNs, and Transformers, along with unsupervised
  techniques like sparsity methods and Bayesian approaches.
---

# On normalization-equivariance properties of supervised and unsupervised denoising methods: a survey

## Quick Facts
- arXiv ID: 2402.15352
- Source URL: https://arxiv.org/abs/2402.15352
- Reference count: 40
- Primary result: Traditional denoising methods exhibit normalization-equivariance while most deep learning approaches do not, except for bias-free CNNs

## Executive Summary
This survey systematically examines normalization-equivariance properties in both supervised and unsupervised image denoising methods. The authors analyze how different denoising approaches behave when input data is normalized by a constant factor, revealing a fundamental distinction between traditional methods (like NL-Means and NL-Ridge) which are normalization-equivariant, and most deep learning approaches which are not. The paper demonstrates that only bias-free convolutional neural networks maintain normalization-equivariance, while popular architectures like DnCNN, SCUNet, and Restormer break this property due to bias terms and specific nonlinearities.

The authors propose solutions to restore normalization-equivariance in neural networks through affine convolutions and sort pooling nonlinearities. These modifications allow neural networks to maintain the desirable property of normalization-equivariance without sacrificing denoising performance. The survey provides both theoretical analysis and experimental validation across different network architectures, establishing a framework for understanding when and why normalization-equivariance matters in image denoising applications.

## Method Summary
The survey employs a comprehensive analytical approach to examine normalization-equivariance across denoising methods. The authors first establish theoretical definitions of normalization-equivariance for both supervised and unsupervised methods, then systematically analyze traditional approaches (non-local means, sparsity-based methods, Bayesian approaches) and deep learning architectures (MLPs, CNNs, Transformers). For each method, they derive conditions under which normalization-equivariance holds or fails, focusing on architectural components like convolution operations, activation functions, and normalization layers. The theoretical analysis is complemented by experimental validation using benchmark datasets and various noise levels, with performance measured through PSNR and SSIM metrics. The proposed solutions (affine convolutions and sort pooling) are evaluated for their ability to restore normalization-equivariance while maintaining or improving denoising quality.

## Key Results
- Traditional denoising methods (NL-Means, NL-Ridge) exhibit normalization-equivariance, maintaining consistent performance across different noise scales
- Most deep learning denoising networks (DnCNN, SCUNet, Restormer) are not normalization-equivariant due to bias terms and specific activation functions
- Bias-free CNNs are the exception among deep learning methods, maintaining normalization-equivariance when properly configured
- The proposed affine convolution and sort pooling modifications can restore normalization-equivariance in neural networks without performance degradation

## Why This Works (Mechanism)
Normalization-equivariance ensures that denoising methods produce consistent results when input images are scaled by a constant factor. This property is crucial for generalization across different noise levels and imaging conditions. Traditional methods naturally satisfy this property through their mathematical formulation, while deep learning methods often break it due to learnable bias parameters and non-equivariant activation functions. The mechanism works by ensuring that the denoising operation commutes with input scaling, allowing the network to adapt its behavior proportionally to the input magnitude rather than learning noise-specific patterns.

## Foundational Learning

**Normalization-equivariance**: The property where a function commutes with input scaling, essential for consistent performance across varying noise levels. Quick check: Verify that f(αx) = αf(x) for input scaling factor α.

**Additive Gaussian noise model**: The standard noise assumption where observed image y = x + n, with n ~ N(0, σ²). Quick check: Confirm noise follows normal distribution with zero mean and known variance.

**Bias-free convolutions**: Convolutional layers without learnable bias terms, crucial for maintaining normalization-equivariance in deep networks. Quick check: Ensure convolution operation is purely linear without additive offsets.

**Non-local means**: Traditional denoising method using weighted averaging of similar patches across the image. Quick check: Verify weight computation depends only on patch similarity, not absolute intensities.

## Architecture Onboarding

Component map: Input -> Normalization -> Convolutional Layers -> Activation Functions -> Output

Critical path: The denoising pipeline flows through normalization handling, convolution operations, and activation functions, with normalization-equivariance determined by the interaction of these components.

Design tradeoffs: Traditional methods prioritize mathematical consistency and normalization-equivariance, while deep learning methods optimize for empirical performance, often sacrificing theoretical properties. The choice between methods depends on application requirements for generalization versus peak performance.

Failure signatures: Loss of normalization-equivariance manifests as inconsistent denoising quality across different noise scales, with performance degrading more rapidly at higher noise levels for non-equivariant methods.

First experiments:
1. Test normalization-equivariance by scaling input images by factors of 0.5, 1.0, and 2.0 and measuring consistency in denoising output
2. Compare PSNR/SSIM performance of traditional vs. deep learning methods across noise levels σ = 15, 25, 50
3. Evaluate proposed affine convolution modifications on standard architectures (DnCNN, UNet) for normalization-equivariance restoration

## Open Questions the Paper Calls Out
None identified in the source material.

## Limitations
- Analysis primarily focused on additive Gaussian noise, limiting applicability to other noise distributions
- Heavy reliance on theoretical analysis with limited comprehensive empirical validation across diverse datasets
- Proposed solutions require additional validation across different network architectures beyond the tested examples

## Confidence

High confidence:
- Theoretical analysis of normalization-equivariance properties for traditional methods (NL-Means, NL-Ridge)
- Mathematical foundations underlying the proposed affine convolution and sort pooling solutions

Medium confidence:
- Analysis of deep learning architectures based on specific implementation details that may vary
- Experimental validation results limited to selected architectures and datasets
- Generalization of findings to non-Gaussian noise distributions and real-world scenarios

## Next Checks

1. Conduct comprehensive empirical validation of proposed affine convolution and sort pooling approaches across multiple network architectures (CNNs, Transformers) and diverse datasets to verify robustness and performance consistency.

2. Extend normalization-equivariance analysis to non-Gaussian noise distributions including Poisson noise, multiplicative noise, and real-world noise captured from actual imaging sensors.

3. Systematically evaluate the trade-off between normalization-equivariance and denoising performance across different noise levels and image types, establishing guidelines for when normalization-equivariance is critical versus when it can be relaxed for improved performance.