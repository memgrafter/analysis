---
ver: rpa2
title: Utilizing Description Logics for Global Explanations of Heterogeneous Graph
  Neural Networks
arxiv_id: '2405.12654'
source_url: https://arxiv.org/abs/2405.12654
tags:
- graph
- explanations
- node
- class
- graphs
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a novel approach to globally explain graph
  neural networks (GNNs) using class expressions (CEs) from description logic. Current
  graph-based explanation methods are limited in their ability to capture disjunctions,
  negations, and cardinality restrictions, which are important for explaining complex
  patterns.
---

# Utilizing Description Logics for Global Explanations of Heterogeneous Graph Neural Networks

## Quick Facts
- arXiv ID: 2405.12654
- Source URL: https://arxiv.org/abs/2405.12654
- Reference count: 40
- Key outcome: Novel approach using description logic class expressions to globally explain GNNs, overcoming limitations of subgraph-based methods in capturing disjunctions, negations, and cardinality restrictions

## Executive Summary
This paper addresses the challenge of globally explaining graph neural networks (GNNs) on heterogeneous graphs by introducing class expressions from description logic. Traditional subgraph-based explanation methods struggle to capture complex logical constructs like disjunctions and negations, which are essential for explaining GNN behavior. The authors propose using EL description logic to generate expressive class expressions that can better represent the patterns learned by GNNs. Through a beam search approach with mutation, they generate and score class expressions using either GNN output maximization or fidelity-based scoring on validation data. The method is evaluated on a heterogeneous version of the BA-Shapes dataset, demonstrating high fidelity and accuracy in explaining GNN predictions while also identifying spurious correlations.

## Method Summary
The approach involves generating class expressions from description logic using beam search with mutation, then scoring these expressions through two methods: (1) constructing graphs from class expressions and maximizing the GNN's predictions, or (2) comparing GNN predictions to class expression predictions on a validation set. The best class expression is selected based on these scores. The method leverages the expressive power of description logic to capture complex patterns including conjunctions, disjunctions, and negations that subgraph-based methods cannot represent.

## Key Results
- Class expressions from description logic can capture disjunctions, negations, and cardinality restrictions that subgraph-based explanations cannot
- Fidelity-based scoring produces more representative global explanations than GNN-output-based scoring
- The approach successfully detects spurious correlations in GNN learning processes
- High explanation accuracy and fidelity achieved on heterogeneous BA-Shapes dataset

## Why This Works (Mechanism)

### Mechanism 1
- Claim: CE-based explanations can capture disjunctions, negations, and cardinality restrictions that subgraph-based explanations cannot.
- Mechanism: Description Logic allows construction of class expressions that directly encode these logical constructs, which are then used to synthesize graphs and score them based on GNN predictions.
- Core assumption: The GNN's learned representations correspond to the logical structure captured by the DL expressions.
- Evidence anchors:
  - [abstract] "Current graph-based methods are limited in their ability to explain classes with multiple sufficient explanations... To provide more expressive explanations, we propose utilizing class expressions (CEs) from the field of description logic (DL)."
  - [section] "Whereas common families of GNNs can learn intricate patterns expressed in the description logic ALCQ [6] supporting conjunctions, disjunctions, negations, and cardinality restrictions, a subgraph of a metagraph is unable to capture disjunctions, negations, and cardinality restrictions."
- Break condition: If the GNN's learned representations do not align with the logical structure of the CEs, the explanations will not accurately reflect the model's reasoning.

### Mechanism 2
- Claim: Maximizing fidelity on a validation dataset produces more representative global explanations than maximizing GNN output on synthesized graphs.
- Mechanism: By scoring CEs based on their ability to predict the GNN's behavior on real validation data rather than on synthetic graphs, the approach captures the general patterns the GNN has learned.
- Core assumption: The validation dataset is representative of the GNN's typical behavior and contains the relevant patterns.
- Evidence anchors:
  - [section] "Our results indicate that learning explanations using metrics as fidelity creates explanations for the general behavior of the model while maximizing the GNN output identifies outliers."
  - [section] "The existence of spurious correlations on homogeneous motif datasets was additionally observed by [9]. Furthermore, we notice that maximizing fidelity leads to CEs which can also serve as ground-truth CEs, hence correctly explaining the GNN."
- Break condition: If the validation dataset is not representative of the GNN's typical behavior or contains significant noise, fidelity-based scoring may produce misleading explanations.

### Mechanism 3
- Claim: Beam search with mutation effectively explores the space of possible CEs to find high-quality explanations.
- Mechanism: The algorithm starts with random CEs, scores them, and iteratively refines them through mutation (adding new constructs) while keeping the best candidates.
- Core assumption: The mutation operations preserve syntactic validity and the search space is sufficiently explored with the chosen beam width and iterations.
- Evidence anchors:
  - [section] "Our approach works as follows: We randomly generate CEs via beam search and select the CE maximizing our scoring function, making the explanation independent of the dataset [26]."
  - [section] "In each iteration, for each candidate, we add a mutation of this candidate (lines 7-10). This is done by adding another CE of the form âˆƒr.CLAS S. From the resulting 2k candidates, we again choose the best k candidates by a scoring function for the next iteration (lines 12-13)."
- Break condition: If the mutation operations are too restrictive or the beam width/iterations are insufficient, the search may get stuck in local optima and miss better explanations.

## Foundational Learning

- Concept: Description Logic (DL) and EL
  - Why needed here: The approach relies on EL class expressions to represent and reason about the logical structure of the graph data.
  - Quick check question: What are the key constructs of EL (intersection, existential restriction) and how do they map to graph structures?

- Concept: Graph Neural Networks (GNNs) and message passing
  - Why needed here: The explanations are generated based on the predictions of a trained GNN, so understanding how GNNs process graph data is crucial.
  - Quick check question: How does the GNN's message passing mechanism influence the type of patterns it can learn and explain?

- Concept: Heterogeneous graphs and node/edge types
  - Why needed here: The approach is designed for heterogeneous graphs with multiple node and edge types, which adds complexity to the explanation process.
  - Quick check question: How do the different node and edge types in a heterogeneous graph affect the construction and interpretation of DL-based explanations?

## Architecture Onboarding

- Component map: Heterogeneous graph data -> Trained GNN model -> Description Logic class expression generator -> Beam search algorithm with mutation -> Scoring functions -> Explanation evaluation metrics

- Critical path:
  1. Load heterogeneous graph data and trained GNN model
  2. Generate initial random EL class expressions
  3. Score expressions using chosen scoring function
  4. Perform beam search with mutation to refine expressions
  5. Select best explanation(s) based on scores
  6. Evaluate explanations using accuracy and fidelity metrics

- Design tradeoffs:
  - Beam width and iterations vs. computational cost
  - GNN-output-based scoring vs. fidelity-based scoring
  - EL expressiveness vs. complexity of generated explanations

- Failure signatures:
  - Low explanation accuracy despite high GNN scores (spurious correlations)
  - Inconsistent explanations across different scoring functions
  - Very long/complex explanations that are hard to interpret

- First 3 experiments:
  1. Verify that the approach can generate correct explanations for a simple synthetic heterogeneous graph with known patterns
  2. Compare the quality of explanations generated using GNN-output-based scoring vs. fidelity-based scoring on a benchmark dataset
  3. Test the sensitivity of the approach to different beam widths and mutation strategies on a larger heterogeneous graph dataset

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the incorporation of description logic ALCQ, with unions and cardinality constraints, impact the quality and expressiveness of global explanations for graph neural networks?
- Basis in paper: [explicit] The authors mention future work involving the incorporation of ALCQ description logic, which includes unions and cardinality constraints, to enhance the expressivity of explanations.
- Why unresolved: The paper only implements the approach using the EL description logic, which is a subset of ALCQ. The potential benefits and challenges of incorporating ALCQ are not explored.
- What evidence would resolve it: Conducting experiments comparing the quality and expressiveness of explanations generated using EL and ALCQ description logics on various graph datasets would provide evidence to answer this question.

### Open Question 2
- Question: What are the implications of using fidelity as a scoring function compared to maximizing the GNN output for generating global explanations?
- Basis in paper: [explicit] The authors discuss the difference between maximizing the GNN output and maximizing fidelity for scoring class expressions. They note that maximizing fidelity leads to explanations that can serve as ground-truth CEs, while maximizing GNN output identifies outliers.
- Why unresolved: The paper does not provide a comprehensive analysis of the trade-offs between using fidelity and maximizing GNN output as scoring functions. The implications for the quality and usefulness of explanations are not fully explored.
- What evidence would resolve it: Conducting a detailed study comparing the quality, interpretability, and usefulness of explanations generated using fidelity and GNN output maximization on various graph datasets would provide evidence to answer this question.

### Open Question 3
- Question: How does the runtime of the proposed approach scale with the complexity of the graph neural network and the size of the input graphs?
- Basis in paper: [explicit] The authors mention that the algorithm runtime is primarily determined by the graph-creation step during scoring, which takes significantly longer for longer class expressions. They also note that for GNNs with more layers, the explanations become longer, and larger graphs must be created for scoring.
- Why unresolved: The paper does not provide a detailed analysis of the runtime complexity of the proposed approach. The impact of GNN complexity and input graph size on runtime is not fully explored.
- What evidence would resolve it: Conducting experiments to measure the runtime of the proposed approach on graphs of varying sizes and GNNs of different complexities would provide evidence to answer this question.

## Limitations

- The beam search approach may struggle with scalability and efficiency on larger, more complex heterogeneous graphs
- The fidelity-based scoring approach heavily relies on the quality and representativeness of the validation dataset
- Limited evaluation on real-world heterogeneous graph datasets, with results primarily based on a synthetic benchmark

## Confidence

- High Confidence: The fundamental claim that description logic class expressions can express patterns (disjunctions, negations, cardinality restrictions) that subgraph-based methods cannot.
- Medium Confidence: The claim that fidelity-based scoring produces more representative global explanations than GNN-output-based scoring.
- Low Confidence: The scalability and practical applicability of the approach to large, real-world heterogeneous graphs.

## Next Checks

1. Apply the approach to a larger heterogeneous graph dataset (e.g., OGB-Hetero or a real-world knowledge graph) and measure computational time and memory requirements to evaluate scalability.

2. Systematically vary the composition and size of the validation set used for fidelity scoring and measure the impact on explanation quality and consistency to understand robustness.

3. Train GNNs on multiple heterogeneous graph datasets with different characteristics and apply the explanation approach to each, comparing the quality and interpretability of resulting CEs across datasets to assess generalizability.