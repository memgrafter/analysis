---
ver: rpa2
title: Incremental Learning of Retrievable Skills For Efficient Continual Task Adaptation
arxiv_id: '2410.22658'
source_url: https://arxiv.org/abs/2410.22658
tags:
- skill
- learning
- task
- tasks
- evolving
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces IsCiL, an adapter-based continual imitation
  learning framework designed to improve knowledge sharing and task adaptation in
  non-stationary environments. The core innovation is the use of prototype-based skill
  retrieval combined with skill-specific adapters, enabling efficient learning from
  incomplete demonstrations.
---

# Incremental Learning of Retrievable Skills For Efficient Continual Task Adaptation

## Quick Facts
- arXiv ID: 2410.22658
- Source URL: https://arxiv.org/abs/2410.22658
- Authors: Daehee Lee; Minjong Yoo; Woo Kyung Kim; Wonje Choi; Honguk Woo
- Reference count: 40
- Key outcome: IsCiL achieves 84.5% to 97.2% of oracle performance on Franka-Kitchen and Meta-World environments, outperforming adapter-based baselines in sample efficiency and task adaptation.

## Executive Summary
This paper introduces IsCiL, an adapter-based continual imitation learning framework designed to improve knowledge sharing and task adaptation in non-stationary environments. The core innovation is the use of prototype-based skill retrieval combined with skill-specific adapters, enabling efficient learning from incomplete demonstrations. The framework incrementally learns shareable skills from different tasks and retrieves them as needed during evaluation. Experiments on Franka-Kitchen and Meta-World environments demonstrate that IsCiL outperforms several adapter-based baselines in sample efficiency and task adaptation, achieving 84.5% to 97.2% of the oracle performance. The method also supports task unlearning for privacy protection, with minimal performance loss.

## Method Summary
IsCiL operates by incrementally learning skill prototypes from demonstrations in a continual learning setting. The framework uses a fixed state encoder to map (observation, goal) pairs into a state embedding space, where demonstrations are clustered into multifaceted skill prototypes. During evaluation, the skill retriever maps current states to the most similar bases within these prototypes to select appropriate skill adapters. Each skill has a dedicated LoRA adapter that conditions a pre-trained base model to generate task-specific actions. This approach enables knowledge sharing across tasks while preventing catastrophic forgetting through parameter isolation.

## Key Results
- IsCiL outperforms adapter-based baselines (Seq-FT, Seq-LoRA, EWC, L2M, TAIL) in both sample efficiency and task adaptation
- Achieves 84.5% to 97.2% of oracle performance across Franka-Kitchen and Meta-World environments
- Demonstrates effective task unlearning capability with minimal performance degradation
- Shows robust performance in both task adaptation and sample-efficiency metrics (FWT, BWT, AUC)

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Prototype-based skill retrieval with state embedding mapping enables effective knowledge sharing across tasks.
- **Mechanism:** Demonstrations are encoded into state embeddings using a fixed function f, then mapped to skill prototypes. The skill retriever uses similarity between current state embeddings and stored bases within prototypes to select the appropriate skill adapter. This allows sharing of skills across tasks without requiring complete demonstrations for each task.
- **Core assumption:** The state embedding space captures meaningful similarity relationships between different tasks' states, and the fixed encoding function f is sufficient to maintain these relationships across stages.
- **Evidence anchors:**
  - [abstract]: "demonstrations are mapped into the state embedding space, where proper skills can be retrieved upon input states through prototype-based memory"
  - [section]: "For this, we use multifaceted skill prototypes χz ∈ X, where X is the set of learned skill prototypes. These prototypes capture the sequential patterns of expert demonstrations associated with specific goal-reaching tasks."
  - [corpus]: Weak - the related papers focus on different CIL approaches (parameter-efficient tuning, adapter-based methods) but don't specifically address prototype-based skill retrieval.
- **Break condition:** If the state embedding space becomes too distorted across stages, or if the similarity function S fails to capture meaningful relationships between states from different tasks.

### Mechanism 2
- **Claim:** Parameter-efficient adapter conditioning prevents catastrophic forgetting while enabling sample-efficient task adaptation.
- **Mechanism:** Each skill is associated with a dedicated adapter that modifies the pre-trained base model's output. When evaluating a task, the skill decoder uses both the pre-trained parameters θpre and the retrieved skill adapter parameters θz. This isolation of parameters for each skill prevents overwriting of previously learned knowledge.
- **Core assumption:** The base model has sufficient capacity to accommodate all skill adapters without interference, and the Low-Rank Adaptation (LoRA) parameterization is appropriate for capturing task-specific knowledge.
- **Evidence anchors:**
  - [abstract]: "These retrievable skills are incrementally learned on their corresponding adapters. Our CiL experiments with complex tasks in Franka-Kitchen and Meta-World demonstrate robust performance of IsCiL in both task adaptation and sample-efficiency."
  - [section]: "To effectively use the knowledge of the pre-trained base model without forgetting, even in a non-stationary changing environment, the skill decoder is conditioned based on parameters."
  - [corpus]: Weak - related papers discuss adapter-based continual learning but focus on different approaches (parameter isolation vs. shared adapters with task identifiers).
- **Break condition:** If the adapter parameters become too large relative to the base model capacity, or if the skill distribution shifts too dramatically between stages for the adapters to capture.

### Mechanism 3
- **Claim:** Task unlearning through adapter removal provides privacy protection with minimal performance impact.
- **Mechanism:** Since each skill adapter is tagged with task information, unlearning requests can be handled by simply removing the corresponding skill prototypes and adapters. This maintains output distribution equality with models trained on retained data.
- **Core assumption:** The separation of skill adapters by task is complete and accurate, and removing adapters doesn't create gaps in the skill retrieval space that affect other tasks.
- **Evidence anchors:**
  - [abstract]: "The method also supports task unlearning for privacy protection, with minimal performance loss."
  - [section]: "In IsCiL, the separation of skill adapters for each task facilitates easy tagging of task information on each skill. When an unlearning request is given with a task identifier τ, the corresponding skill prototypes and adapters are removed."
  - [corpus]: Missing - no related papers in the corpus specifically address task unlearning in continual learning.
- **Break condition:** If skills are shared across tasks (violating the assumption of task-specific adapters), or if the skill retrieval space becomes too sparse after unlearning.

## Foundational Learning

- **Concept:** Continual Learning and Catastrophic Forgetting
  - **Why needed here:** IsCiL operates in a continual learning setting where tasks arrive sequentially, and preventing catastrophic forgetting is essential for maintaining performance across stages.
  - **Quick check question:** What happens to a neural network's performance on previous tasks when it's trained on new tasks without any mechanism to preserve old knowledge?

- **Concept:** Parameter-Efficient Tuning (PET) and Adapter Methods
  - **Why needed here:** IsCiL uses LoRA adapters to condition the skill decoder, requiring understanding of how adapter methods modify model behavior without full fine-tuning.
  - **Quick check question:** How does LoRA differ from full fine-tuning in terms of parameter updates and computational efficiency?

- **Concept:** State Embedding and Similarity-Based Retrieval
  - **Why needed here:** The skill retriever relies on mapping states to embeddings and finding similar bases within skill prototypes, requiring understanding of embedding spaces and similarity metrics.
  - **Quick check question:** What properties should a state embedding function have to ensure that similar states from different tasks are close in the embedding space?

## Architecture Onboarding

- **Component map:** (Observation, Goal) → State Encoder f → Skill Retriever πR → Skill Adapter θz → Skill Decoder πD → Action
- **Critical path:** (Observation, Goal) → State Encoder → Skill Retriever → Skill Adapter → Skill Decoder → Action
- **Design tradeoffs:**
  - Fixed vs. learnable state encoder: Fixed encoder ensures consistency but may not adapt to distribution shifts
  - Number of bases per prototype: More bases capture diversity but increase memory and computation
  - Adapter rank in LoRA: Higher rank captures more complex patterns but increases parameters and risk of overfitting
- **Failure signatures:**
  - Retrieval failure: Wrong skill adapter selected (often due to state embedding mismatch or prototype initialization issues)
  - Adaptation failure: Retrieved skill doesn't produce appropriate actions (often due to poor adapter initialization or insufficient training)
  - Forgetting: Performance degradation on previously learned tasks (often due to inadequate adapter isolation or base model interference)
- **First 3 experiments:**
  1. Single task adaptation: Test if IsCiL can learn and adapt to a single task using the prototype-based retrieval system
  2. Two-task transfer: Test if skills learned from one task can be successfully retrieved and used for a second task
  3. Skill retrieval ablation: Test retrieval accuracy by providing states from known demonstrations and checking if the correct skill adapter is retrieved

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the choice of the number of bases per skill prototype affect the overall performance and scalability of IsCiL in larger, more complex environments?
- Basis in paper: [explicit] Table 4 shows ablation study results on the number of bases for skill prototypes in the Evolving Kitchen-Complete scenario, indicating that increasing the number of bases improves performance and stability, particularly around K=10.
- Why unresolved: The paper only tests up to 50 bases and in a single environment (Evolving Kitchen), leaving questions about scalability and optimal base numbers for more complex environments.
- What evidence would resolve it: Testing IsCiL with varying numbers of bases (e.g., 10, 50, 100, 200) across multiple complex environments (e.g., larger Meta-World variations or other robotics benchmarks) to identify performance trends and scalability limits.

### Open Question 2
- Question: How does IsCiL's performance compare to rehearsal-based continual learning methods when given the same computational resources?
- Basis in paper: [explicit] Figure 4 compares IsCiL to Experience Replay (ER) in terms of sample efficiency, showing that IsCiL outperforms ER in AUC across all environments without storing rehearsal data.
- Why unresolved: The comparison focuses on sample efficiency but does not directly address computational efficiency when both methods use the same amount of stored data and training resources.
- What evidence would resolve it: Conducting experiments where both IsCiL and a rehearsal-based method (e.g., ER or other replay-based approaches) are given identical computational budgets (e.g., same number of rehearsals and training epochs) and comparing their performance in terms of both sample efficiency and task adaptation.

### Open Question 3
- Question: Can IsCiL effectively handle scenarios where task sub-goals are not explicitly provided but must be inferred from context or language descriptions?
- Basis in paper: [inferred] The paper mentions that sub-goals are represented through language embeddings and that the framework adapts to novel tasks by modifying the goal gt. However, it assumes sub-goals are known and provided.
- Why unresolved: The framework's reliance on sub-goal sequences for skill retrieval and adaptation suggests potential limitations in scenarios where sub-goals must be inferred rather than explicitly given.
- What evidence would resolve it: Testing IsCiL in environments where tasks are described only through natural language instructions without explicit sub-goal sequences, and evaluating its ability to infer and complete the necessary sub-goals.

## Limitations
- The fixed state encoder may not adapt well to distribution shifts between stages in non-stationary environments
- Prototype initialization using a single demonstration per skill may not capture task diversity for complex or multimodal tasks
- Task unlearning claims lack comprehensive empirical validation for privacy guarantees

## Confidence
- **High confidence:** The core mechanism of using adapter-based skill conditioning to prevent catastrophic forgetting is well-established and empirically validated
- **Medium confidence:** The prototype-based skill retrieval approach is novel but the evaluation primarily focuses on quantitative performance rather than qualitative analysis of retrieval quality
- **Low confidence:** The task unlearning capability is described conceptually but lacks comprehensive experimental validation

## Next Checks
1. Conduct ablation studies on state encoder design - compare fixed vs. learnable encoders and evaluate sensitivity to distribution shifts between stages
2. Analyze retrieval accuracy across stages - measure whether the correct skill adapters are being selected and identify failure modes when retrieval fails
3. Perform task unlearning stress tests - evaluate performance degradation after removing adapters for different numbers of tasks and assess privacy implications through membership inference attacks