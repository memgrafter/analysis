---
ver: rpa2
title: Enhancing Chain of Thought Prompting in Large Language Models via Reasoning
  Patterns
arxiv_id: '2404.14812'
source_url: https://arxiv.org/abs/2404.14812
tags:
- reasoning
- patterns
- language
- llms
- these
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of improving chain-of-thought
  (CoT) prompting in large language models (LLMs) by addressing noise and interpretability
  issues in demonstration selection. The core method, Pattern-CoT, leverages reasoning
  patterns - sequences of reasoning operations extracted from rationales - to select
  diverse and representative demonstrations.
---

# Enhancing Chain of Thought Prompting in Large Language Models via Reasoning Patterns

## Quick Facts
- **arXiv ID:** 2404.14812
- **Source URL:** https://arxiv.org/abs/2404.14812
- **Reference count:** 11
- **Primary result:** Pattern-CoT improves LLM reasoning accuracy by 1-4% across eight datasets by extracting and clustering task-specific reasoning patterns

## Executive Summary
This paper addresses limitations in chain-of-thought (CoT) prompting by introducing Pattern-CoT, a method that selects diverse demonstrations based on reasoning patterns rather than semantic similarity. The approach extracts task-specific reasoning operations from rationales, clusters these patterns, and selects representative demonstrations from each cluster. Experiments show consistent improvements over baseline methods across multiple reasoning tasks and model sizes, with accuracy gains ranging from 1-4%. The method also provides better interpretability through explicit reasoning pattern extraction and feature attribution analysis.

## Method Summary
Pattern-CoT improves demonstration selection for CoT prompting by leveraging reasoning patterns extracted from rationales. The method first collects seed demonstrations using Zero-Shot-CoT to generate rationales for questions. It then identifies task-specific operation tokens (e.g., "+", "-", "*", "/") to extract condensed reasoning patterns from these rationales. These patterns are encoded using Sentence-BERT, clustered using k-means, and diverse demonstrations are selected from each cluster to form the final context. The approach uses an adaptive formula to determine the number of clusters based on dataset characteristics, ensuring appropriate diversity in the selected demonstrations.

## Key Results
- Pattern-CoT consistently outperforms baseline methods (Zero-Shot-CoT, Random-CoT, Auto-CoT) across eight reasoning datasets
- Accuracy improvements range from 1-4% on tasks including MultiArith, GSM8K, AQuA, and others
- The method demonstrates robustness across different model sizes (7B and 13B LLaMA-2) and types (GPT-3.5-turbo, Qwen-7B)
- Feature attribution analysis shows how diverse reasoning patterns reduce bias and improve LLM reasoning performance

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Extracting reasoning patterns from rationales reduces noise by focusing on logical operations rather than overall semantics
- Mechanism: The method tokenizes rationales using task-specific operation tokens to form condensed sequences of reasoning steps, capturing logical flow while discarding irrelevant semantic content
- Core assumption: Logical operations in a rationale are more informative for downstream task success than semantic content
- Break condition: If operation tokens don't adequately capture reasoning steps for a task, extracted patterns may miss critical information

### Mechanism 2
- Claim: Diverse reasoning patterns in demonstration set improve LLM generalization by exposing model to multiple solution strategies
- Mechanism: Clustering patterns and sampling one demonstration from each cluster ensures representation of different reasoning approaches
- Core assumption: LLMs benefit from seeing multiple reasoning patterns rather than multiple examples of same pattern
- Break condition: If clustering algorithm fails or number of clusters is poorly chosen, diversity benefit may be lost

### Mechanism 3
- Claim: Method provides interpretability by making reasoning patterns explicit and showing how different patterns influence LLM outputs
- Mechanism: Extracted patterns serve as bridge between demonstration rationales and reasoning operations, with feature attribution analysis revealing pattern influence
- Core assumption: Making reasoning process transparent through pattern extraction helps identify effective strategies
- Break condition: If extracted patterns are too abstract or attribution analysis can't clearly link patterns to outputs, interpretability benefit is limited

## Foundational Learning

- **Concept:** Chain-of-Thought (CoT) prompting
  - Why needed here: Paper builds on CoT prompting as baseline method
  - Quick check: What is key idea behind CoT prompting, and how does it differ from direct question-answer prompting?

- **Concept:** Pattern extraction and clustering
  - Why needed here: Proposed method relies on extracting reasoning patterns from rationales and clustering them
  - Quick check: How does clustering reasoning patterns help select diverse demonstrations, and why is diversity important?

- **Concept:** Feature attribution analysis
  - Why needed here: Paper uses feature attribution to analyze how reasoning patterns influence LLM outputs
  - Quick check: What is feature attribution analysis, and how can it help understand impact of reasoning patterns on LLM outputs?

## Architecture Onboarding

- **Component map:** Seed demonstration collection -> Pattern discovery -> Pattern clustering -> Demonstration selection -> LLM inference
- **Critical path:** Seed demonstration collection → Pattern discovery → Pattern clustering → Demonstration selection → LLM inference
- **Design tradeoffs:**
  - Operation token selection: More specific tokens capture patterns better but require more domain knowledge and reduce generalization
  - Number of clusters (k): More clusters provide more diversity but may lead to overfitting or insufficient examples per cluster
  - Demonstration quality: Method tolerates incorrect demonstrations as long as they contain useful patterns, but very noisy examples may still harm performance
- **Failure signatures:**
  - Poor performance on tasks with highly implicit reasoning where operation tokens don't capture reasoning steps
  - Overfitting to specific patterns if number of clusters is too large relative to data diversity
  - Limited interpretability if extracted patterns are too abstract or attribution analysis can't clearly link patterns to outputs
- **First 3 experiments:**
  1. Compare proposed method against baseline CoT methods on simple arithmetic dataset like MultiArith
  2. Test impact of different operation token sets on performance by removing tokens from GSM8K dataset
  3. Analyze effect of number of clusters (k) on performance by running experiments with different k values on AQuA dataset

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do reasoning patterns in CoT demonstrations affect model bias across different reasoning domains (e.g., arithmetic vs. symbolic reasoning)?
- Basis in paper: Paper discusses how diverse patterns reduce bias but doesn't extensively explore how different domains interact with pattern diversity
- Why unresolved: Experiments focus on demonstrating effectiveness rather than dissecting how patterns influence bias differently across reasoning types
- What evidence would resolve it: Comparative studies showing bias metrics across domains with varying pattern diversity

### Open Question 2
- Question: What is optimal method for dynamically adjusting number of demonstrations (k) in Pattern-CoT based on task complexity?
- Basis in paper: Authors propose adaptive formula for k but acknowledge it's empirical and may not fully capture task complexity
- Why unresolved: Adaptive k formula uses fixed operations and sample size but doesn't account for intrinsic difficulty or variability of reasoning steps
- What evidence would resolve it: Empirical validation of k adjustment across tasks with varying complexity and principled method for k selection

### Open Question 3
- Question: Can reasoning patterns extracted from CoT demonstrations be used to improve interpretability in non-reasoning tasks (e.g., summarization or classification)?
- Basis in paper: Paper emphasizes interpretability gains from pattern-based selection but focuses solely on reasoning tasks
- Why unresolved: Methodology and benefits of pattern extraction are demonstrated only for reasoning, leaving applicability to other domains unexplored
- What evidence would resolve it: Experiments applying Pattern-CoT to non-reasoning tasks and measuring interpretability improvements

### Open Question 4
- Question: How does quality of seed demonstrations impact effectiveness of Pattern-CoT compared to semantic-based methods?
- Basis in paper: Authors note method doesn't require correct rationales and still outperforms baselines, but don't systematically study impact of seed quality
- Why unresolved: Robustness to incorrect demonstrations is mentioned but not quantified against seed quality variations
- What evidence would resolve it: Controlled experiments varying seed demonstration quality and comparing performance degradation across methods

## Limitations
- Reliance on task-specific operation tokens may not generalize well to domains with highly implicit reasoning or extremely broad action spaces
- Quality of seed demonstrations directly impacts pattern extraction quality, with noise potentially propagating through the process
- Interpretability claims require further validation through systematic human evaluation to confirm practical benefits

## Confidence
- **High Confidence:** Core claim that diverse demonstrations improve CoT performance is well-supported by empirical results across eight datasets
- **Medium Confidence:** Claim about better interpretability through pattern extraction and feature attribution is partially supported but lacks systematic human evaluation
- **Low Confidence:** Claim that method robustly handles incorrect demonstrations as long as they contain useful patterns is not empirically validated

## Next Checks
1. **Cross-domain robustness test:** Evaluate Pattern-CoT on substantially different domain (e.g., commonsense reasoning) where current operation token approach may not capture reasoning patterns effectively
2. **Noise tolerance quantification:** Systematically inject varying levels of noise into seed demonstrations (0%, 10%, 25%, 50% incorrect examples) and measure how Pattern-CoT performance degrades compared to baselines
3. **Human interpretability validation:** Conduct user study where human evaluators assess extracted reasoning patterns and attributions, judging whether patterns accurately represent reasoning strategies and attributions align with their understanding