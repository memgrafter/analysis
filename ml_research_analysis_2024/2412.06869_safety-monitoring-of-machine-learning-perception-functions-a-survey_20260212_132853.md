---
ver: rpa2
title: 'Safety Monitoring of Machine Learning Perception Functions: a Survey'
arxiv_id: '2412.06869'
source_url: https://arxiv.org/abs/2412.06869
tags:
- safety
- detection
- ieee
- systems
- monitoring
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This survey systematically examines the state of safety monitoring
  for ML perception functions in safety-critical autonomous systems. It structures
  the literature around five key considerations: threat identification, requirements
  elicitation, detection mechanisms, recovery actions, and evaluation.'
---

# Safety Monitoring of Machine Learning Perception Functions: a Survey

## Quick Facts
- arXiv ID: 2412.06869
- Source URL: https://arxiv.org/abs/2412.06869
- Reference count: 40
- This survey provides a systematic framework for understanding safety monitoring approaches for ML perception functions in autonomous systems

## Executive Summary
This survey systematically examines safety monitoring approaches for ML perception functions in safety-critical autonomous systems. The authors structure the literature around five key considerations: threat identification, requirements elicitation, detection mechanisms, recovery actions, and evaluation. While ML models excel at perception tasks, their lack of transparency and potential for errors necessitates runtime monitoring to ensure safety. The survey highlights that current approaches face significant challenges in aligning monitoring objectives with system-level safety requirements and developing standardized evaluation frameworks for certification purposes.

## Method Summary
The paper presents an extensive literature review of safety monitoring approaches for ML perception functions in safety-critical contexts. The authors systematically searched through 40+ research papers, categorizing them according to a five-key consideration framework: threat identification, requirements elicitation, detection mechanisms, recovery actions, and evaluation. The review synthesizes current approaches, identifies research gaps, and provides guidance for future development of safety monitoring systems.

## Key Results
- Detection mechanisms are categorized by integration method (internal vs. external) and monitoring point (input, internal layers, output)
- Evaluation remains challenging due to ambiguous definitions of safety and lack of standardized benchmarks
- Current approaches struggle to align monitoring specifications with system-level safety objectives
- Recovery mechanisms (prediction enhancement vs. control switching) need better integration with detection systems
- No consensus exists on evaluation metrics and test datasets across different autonomous system domains

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The survey's top-down safety consideration framework enables comprehensive safety monitor design beyond detection
- Mechanism: By structuring literature around threat identification, requirements elicitation, detection, reaction, and evaluation, the survey ensures all critical safety monitoring aspects are considered
- Core assumption: Safety monitoring encompasses more than just error detection, requiring a holistic view of system safety
- Evidence anchors:
  - [abstract] "This paper presents an extensive literature review on safety monitoring of perception functions using ML in a safety-critical context... structured around the principal safety considerations intrinsic to SM design"
  - [section] "Traditional safety engineering, however, recognizes error detection as just one component of runtime monitoring and other aspects are also crucial to designing and implementing safe monitors"

### Mechanism 2
- Claim: Categorizing detection mechanisms by integration method (internal vs external) and monitoring point provides clearer guidance for implementation
- Mechanism: This taxonomy helps practitioners select appropriate monitoring strategies based on system constraints and integration requirements
- Core assumption: Different integration approaches have distinct advantages and limitations that must be matched to application context
- Evidence anchors:
  - [abstract] "Our categorization of detection mechanisms is constructed around how the monitors are integrated within the entire system"
  - [section] "Internal detection mechanisms are approaches where the ML model itself is trained to predict its failures" vs "External detection mechanisms are independent components"

### Mechanism 3
- Claim: Identifying specific runtime threats enables targeted safety monitor development and evaluation
- Mechanism: By classifying threats (novelty, distributional shift, adversarial) the survey provides a framework for understanding what monitors should detect and how to test them
- Core assumption: Different threat types require different detection and recovery mechanisms
- Evidence anchors:
  - [section] "The identification of potential threats is pivotal when designing a dependable SM... Adverse input data are classified as threats if they can be detected or mitigated using similar approaches"
  - [section] "We distinguish between covariate and semantic shifts" and "An adversarial input is an intentional modification of in-distribution data"

## Foundational Learning

- Concept: Bayesian uncertainty estimation
  - Why needed here: Provides principled approach to quantifying model confidence for safety monitoring
  - Quick check question: What is the key difference between traditional softmax outputs and Bayesian uncertainty estimates?

- Concept: Out-of-distribution detection
  - Why needed here: Core capability for safety monitors to identify potentially unsafe inputs
  - Quick check question: Why is OOD detection alone insufficient for comprehensive safety monitoring?

- Concept: Fault injection testing
  - Why needed here: Essential for evaluating safety monitor performance against known threats
  - Quick check question: What are the key considerations when designing fault injection tests for safety monitors?

## Architecture Onboarding

- Component map:
  - ML perception model (primary system)
  - Safety monitor (detection mechanism)
  - Recovery mechanism (control switching or prediction enhancement)
  - Evaluation framework (test datasets and metrics)
  - Integration layer (system interfaces and timing constraints)

- Critical path: Input → Detection → Decision → Recovery → System safety state
- Design tradeoffs:
  - Detection accuracy vs computational overhead
  - False positive rate vs false negative rate
  - Integration complexity vs independence
  - Real-time constraints vs thorough analysis

- Failure signatures:
  - High false positive rate in benign conditions
  - Missed detection of known threat types
  - Excessive computational latency
  - Inconsistent behavior across similar inputs

- First 3 experiments:
  1. Implement basic softmax confidence threshold monitor on CIFAR-10, measure TPR@95TNR
  2. Add Monte-Carlo dropout uncertainty estimation, compare detection performance
  3. Implement recovery mechanism (input reconstruction) for detected threats, measure impact on system safety

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can safety monitoring requirements be better aligned with system-level safety objectives in ML perception functions?
- Basis in paper: [explicit] The paper identifies this as a key limitation, noting that most detection mechanisms based on out-of-distribution detection suffer from high false positives/negatives when evaluated for detecting ML model failures, due to misalignment between SM specifications and system-level objectives.
- Why unresolved: Current approaches focus on generic OOD detection rather than considering how specific ML errors translate to safety-critical system states, making it difficult to establish meaningful monitoring requirements.
- What evidence would resolve it: Development and validation of a framework that maps system-level safety objectives to specific ML monitoring requirements, demonstrated through case studies showing improved detection of safety-critical errors.

### Open Question 2
- Question: What is the optimal combination of detection and recovery mechanisms for different types of runtime threats in ML perception?
- Basis in paper: [explicit] The paper states that different types of detection and reaction mechanisms must be properly combined with the task at hand, and this choice is highly dependent on the application context, but remains a difficult challenge.
- Why unresolved: The literature lacks systematic studies mapping specific threat types to optimal detection/recovery mechanism combinations, and current research presents conflicting results about individual mechanism effectiveness.
- What evidence would resolve it: Comprehensive experimental studies comparing different detection/recovery mechanism combinations across multiple threat types and application domains, with clear performance metrics tied to safety outcomes.

### Open Question 3
- Question: How can we develop standardized evaluation benchmarks for ML safety monitors that enable meaningful comparison across different approaches?
- Basis in paper: [explicit] The paper identifies the lack of standardized evaluation as a major challenge, noting that different test datasets and evaluation metrics are used across domains, making it difficult to compare different safety monitoring approaches.
- Why unresolved: Current evaluation practices vary widely between domains (automotive, avionics, etc.) and often use different metrics, making cross-comparison impossible and hindering certification efforts.
- What evidence would resolve it: Development and community adoption of unified evaluation benchmarks that include multiple autonomous system use cases, standardized threat injection methods, and comprehensive metrics covering both detection performance and safety impact.

## Limitations
- Survey relies on published literature, potentially underrepresenting negative results or proprietary industrial approaches
- Categorization framework may not capture all emerging monitoring approaches as the field evolves rapidly
- Fundamental limitation in evaluating approaches without standardized benchmarks or agreed-upon safety definitions
- Focus on technical aspects may underrepresent human factors, certification requirements, and real-world deployment challenges

## Confidence

**High Confidence:** The categorization of detection mechanisms (internal vs. external) and the threat taxonomy (novelty, distributional shift, adversarial) are well-supported by the literature and provide practical guidance for implementation. The identification of key research gaps, particularly around evaluation frameworks and recovery mechanisms, is strongly evidenced.

**Medium Confidence:** The framework's applicability across different autonomous systems (AVs, drones, industrial robots) is reasonable but may require adaptation for specific domains. The assertion that current evaluation approaches are insufficient for certification purposes is supported but would benefit from more concrete examples from regulatory bodies.

**Low Confidence:** The survey's ability to predict future research directions is inherently uncertain, and some identified gaps may be resolved quickly as the field advances. The relative importance of different safety considerations may vary significantly across application contexts.

## Next Checks

1. **Benchmark Implementation Test:** Implement three different monitoring approaches (e.g., softmax threshold, MC dropout, input reconstruction) on a common dataset with controlled distributional shifts to empirically validate the survey's categorization and performance claims.

2. **Cross-Domain Applicability:** Apply the five-consideration framework to a non-autonomous vehicle domain (e.g., medical imaging or industrial inspection) to assess framework transferability and identify domain-specific adaptations needed.

3. **Recovery Mechanism Evaluation:** Design and execute a systematic comparison of different recovery mechanisms (prediction enhancement vs. control switching) under various threat scenarios to quantify their relative effectiveness and limitations in maintaining system safety.