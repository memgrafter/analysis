---
ver: rpa2
title: 'LoFi: Neural Local Fields for Scalable Image Reconstruction'
arxiv_id: '2411.04995'
source_url: https://arxiv.org/abs/2411.04995
tags:
- image
- lofi
- reconstruction
- ieee
- images
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces LoFi, a scalable coordinate-based local reconstruction
  pipeline for solving imaging inverse problems. Unlike conventional methods that
  process the entire image simultaneously, LoFi processes local information at each
  coordinate separately using multi-layer perceptrons (MLPs), enabling image reconstruction
  at any resolution or arbitrary continuous coordinate.
---

# LoFi: Neural Local Fields for Scalable Image Reconstruction

## Quick Facts
- arXiv ID: 2411.04995
- Source URL: https://arxiv.org/abs/2411.04995
- Reference count: 40
- Primary result: LoFi achieves comparable or better performance than CNNs and ViTs for image reconstruction tasks while offering resolution-agnostic training and strong generalization.

## Executive Summary
LoFi introduces a scalable coordinate-based local reconstruction pipeline that processes images locally at each coordinate using MLPs, rather than handling entire images simultaneously. This approach enables image reconstruction at any resolution or arbitrary continuous coordinates with memory-efficient training that scales independently of output resolution. The method demonstrates strong performance across multiple imaging domains including low-dose CT, image denoising, and dark matter mapping, while maintaining excellent generalization capabilities even when trained on small datasets.

## Method Summary
LoFi processes images by treating each coordinate independently using multi-layer perceptrons that take local information as input. Unlike conventional methods that process entire images at once, this local processing design allows the model to reconstruct images at any resolution without requiring additional memory during training. The architecture leverages coordinate-based neural fields to represent images continuously, enabling reconstruction at arbitrary coordinates beyond the discrete pixel grid. This local design also facilitates using LoFi as a plug-and-play denoising prior for solving general inverse problems.

## Key Results
- Achieves comparable or superior performance to CNNs and ViTs on low-dose CT, image denoising, and dark matter mapping tasks
- Enables resolution-agnostic training with memory usage independent of output resolution
- Demonstrates strong generalization to out-of-distribution data and can be trained effectively on extremely small datasets without overfitting

## Why This Works (Mechanism)
LoFi's effectiveness stems from its local processing approach that breaks away from the global processing paradigm of conventional image reconstruction methods. By handling each coordinate independently, the model avoids the memory bottlenecks associated with processing large images entirely and can generalize across resolutions. The coordinate-based neural field representation provides a continuous image model that naturally extends to arbitrary resolutions and coordinates. The local design also enables the model to learn robust local patterns that transfer well to new data distributions.

## Foundational Learning
- **Coordinate-based neural fields**: Continuous function representations using neural networks; needed for resolution-agnostic reconstruction; quick check: can the model output values at arbitrary coordinates
- **Local vs global processing**: Processing individual coordinates versus entire images; needed for memory efficiency and scalability; quick check: memory usage scales with patch size not full image size
- **Inverse problem formulation**: Mathematical framework for reconstructing images from incomplete/indirect measurements; needed for applications like CT and denoising; quick check: loss function properly reflects measurement model
- **Plug-and-play priors**: Using pre-trained denoisers as regularizers in iterative reconstruction; needed for general inverse problem solving; quick check: convergence behavior in iterative schemes

## Architecture Onboarding

Component Map: Input Coordinates -> Local MLP -> Output Value -> Image Assembly

Critical Path: Coordinate input → Local feature extraction → MLP processing → Value prediction → Coordinate assembly into final image

Design Tradeoffs: Local processing provides memory efficiency and resolution flexibility but may lose global context; continuous representation enables arbitrary resolution but requires careful coordinate handling; MLPs are universal approximators but may need careful architecture design for optimal performance.

Failure Signatures: Poor reconstruction quality at image boundaries due to lack of global context; overfitting when trained on very small datasets despite regularization; slow convergence if MLP architecture is too shallow or narrow.

First Experiments: 1) Test single coordinate reconstruction accuracy on synthetic data, 2) Compare memory usage during training across different output resolutions, 3) Evaluate generalization by training on low resolution and testing on high resolution images.

## Open Questions the Paper Calls Out
None identified in the provided information.

## Limitations
- Evaluation scope is limited to three imaging domains without exploring more diverse or challenging scenarios
- Real-world scaling behavior with extremely large datasets or higher-dimensional data remains untested
- Effectiveness for general inverse problems beyond denoising has not been thoroughly validated

## Confidence

Claims about scalability and resolution-agnostic training: **High**
Performance comparisons with CNNs and ViTs: **Medium** (limited to specific tasks and datasets)
Generalization to out-of-distribution data: **Medium** (based on limited out-of-distribution testing)
Effectiveness as a plug-and-play prior: **Low-Medium** (preliminary validation only)

## Next Checks
1. Test LoFi's performance on additional inverse problems beyond denoising, such as deblurring or inpainting, to evaluate its versatility as a plug-and-play prior.
2. Conduct experiments with significantly larger datasets and higher-dimensional imaging data to validate scalability claims under more demanding conditions.
3. Perform extensive out-of-distribution testing across diverse imaging modalities and noise profiles to better quantify generalization capabilities.