---
ver: rpa2
title: 'Active Causal Structure Learning with Latent Variables: Towards Learning to
  Detour in Autonomous Robots'
arxiv_id: '2410.20894'
source_url: https://arxiv.org/abs/2410.20894
tags:
- variables
- agent
- variable
- learning
- causal
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a framework for active causal structure learning
  with latent variables (ACSLWL) to enable autonomous robots to adapt to new environmental
  situations. The method allows an agent to detect and model previously unobserved
  latent variables that influence its interactions with the environment, transforming
  unexpected and inefficient situations into predictable ones with optimal operating
  plans.
---

# Active Causal Structure Learning with Latent Variables: Towards Learning to Detour in Autonomous Robots

## Quick Facts
- **arXiv ID**: 2410.20894
- **Source URL**: https://arxiv.org/abs/2410.20894
- **Reference count**: 40
- **Primary result**: Framework enables autonomous robots to discover and model previously unobserved latent variables that influence environmental interactions

## Executive Summary
This paper presents a framework for active causal structure learning with latent variables (ACSLWL) that enables autonomous robots to adapt to new environmental situations by detecting and modeling unobserved latent variables. The approach combines causal discovery algorithms, a novel surprise divergence measure, and hidden variable structure learning to update an agent's internal causal models when unexpected observations occur. Applied to a simulated robot learning to detour around an unexpected barrier, the framework successfully enabled the agent to discover a hidden variable representing the barrier's presence, update its decision network, and develop effective detour behavior while maintaining its ability to reach the target.

## Method Summary
The ACSLWL framework detects latent variables when significant surprise occurs in the utility function during environmental interactions. The agent uses causal discovery algorithms with transfer entropy to learn initial causal structures, then monitors for utility-based surprise divergence to identify potential hidden variables. When detected, the framework learns the causal structure of these hidden variables by identifying observation variables with significant surprise as potential parent/child relationships, then updates model parameters using hard weighted Expectation Maximization. The approach is demonstrated on a simulated robot learning to detour around a barrier by discovering the hidden variable representing the barrier's presence and updating its decision-making accordingly.

## Key Results
- Successfully enabled a simulated robot to discover a hidden variable representing an unexpected barrier in its environment
- Transformed inefficient, surprising behavior into predictable, optimal detour behavior while maintaining goal-directedness
- Demonstrated the framework's ability to update causal models and decision-making processes in response to environmental changes

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The agent detects latent variables by measuring surprise divergence in the utility function when observed transitions become inconsistent with predictions.
- Mechanism: When the agent executes an action, it computes the expected utility distribution based on its current causal model. If the actual utility outcome deviates significantly from this expectation, the surprise divergence coefficient quantifies this mismatch. A high surprise divergence triggers latent variable detection.
- Core assumption: The observed surprise in utility is caused by an unobserved latent variable rather than sensor noise or model error.
- Evidence anchors:
  - [abstract]: "detecting possible latent variables when unexpected observations occur"
  - [section]: "We propose latent variable detection when a significant surprise in the utility function takes place"
  - [corpus]: Weak - corpus papers focus on causal structure learning but don't address surprise-based latent detection
- Break condition: If the observed surprise is due to sensor noise rather than latent variables, the framework will incorrectly introduce hidden variables.

### Mechanism 2
- Claim: The agent learns the causal structure of hidden variables by identifying observation variables with significant surprise divergence as potential parents/children.
- Mechanism: After detecting a latent variable, the agent computes surprise coefficients for each observation variable. Variables with high surprise become candidates for parent/child relationships with the hidden variable. The "XM" topology connects observed variables at time t to the hidden variable and from the hidden variable to observations at time t+1.
- Core assumption: Observation variables experiencing surprise are causally related to the latent variable.
- Evidence anchors:
  - [section]: "The selection of candidates for the hidden variable's children and parents is also based on the relevance of each particular chance variable on the coefficient of surprise"
  - [section]: "For the purposes of visualization, the surprises were normalized by dividing them by the sum of the surprises present in the observed variables"
  - [corpus]: Weak - corpus papers discuss causal structure learning but not surprise-based parent/child selection
- Break condition: If multiple latent variables influence the same observations, the framework may incorrectly group them under a single hidden variable.

### Mechanism 3
- Claim: The agent updates its causal model parameters using hard weighted EM, with weights based on utility differences to prioritize learning from relevant observations.
- Mechanism: After introducing the hidden variable and its connections, the agent runs hard weighted EM. Initial weights are based on utility differences between consecutive observations - larger utility changes get higher weights. This prioritizes learning transitions that significantly impact behavior.
- Core assumption: Observations with larger utility differences are more informative for learning the hidden variable's causal structure.
- Evidence anchors:
  - [section]: "The introduction of these wi serves to enhance the weighting of observations that exhibit minimal surprise but a significant impact on utility"
  - [section]: "wi = 1 + |U(xi−1)−U(xi)| ∀ i = 1, ..., tmax"
  - [corpus]: Weak - corpus papers discuss EM for parameter estimation but not utility-weighted variants
- Break condition: If utility differences don't correlate with causal relevance, the weighting scheme may bias learning away from important transitions.

## Foundational Learning

- Concept: Causal discovery and transfer entropy
  - Why needed here: The agent needs to learn causal relationships between variables, both intratemporal (within the same time step) and intertemporal (across time steps)
  - Quick check question: How does transfer entropy differ from simple correlation when measuring causal relationships across time?

- Concept: Dynamic Decision Networks (DDNs)
  - Why needed here: The agent represents its causal knowledge and decision-making process using DDNs, which extend Bayesian networks to include actions and utilities
  - Quick check question: What are the three types of nodes in a DDN and what does each represent?

- Concept: Surprise divergence and information theory
  - Why needed here: The framework uses surprise divergence to detect when observations deviate from predictions, indicating potential latent variables
  - Quick check question: How does surprise divergence combine entropy and information dispersion to measure unexpectedness?

## Architecture Onboarding

- Component map: Environment -> Observations -> Surprise Detection -> Latent Variable Detection -> Structure Learning -> Parameter Estimation -> Updated DDN -> Decision Making
- Critical path: Environment → Observations → Surprise Detection → Latent Variable Detection → Structure Learning → Parameter Estimation → Updated DDN → Decision Making
- Design tradeoffs: Discrete vs continuous variables (simplicity vs realism), surprise-based vs model-based latent detection (adaptability vs precision), hard weighted EM vs other optimization methods (simplicity vs convergence speed)
- Failure signatures: Incorrect hidden variable introduction (too many false positives), poor structure learning (wrong parent/child relationships), parameter estimation divergence (EM doesn't converge), decision-making degradation (updated model performs worse than original)
- First 3 experiments:
  1. Implement surprise divergence calculation and test on simple distributions to verify it correctly identifies unexpected outcomes
  2. Build a basic DDN with known structure and verify the agent can learn it using causal discovery algorithms
  3. Create a simple environment with one observable variable and one latent variable, verify the agent detects and models the latent variable correctly

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the ACSLWL framework perform in environments with multiple interacting latent variables compared to single latent variable scenarios?
- Basis in paper: [explicit] The paper mentions this as a future work item, noting that "we have actually assumed that there is only one latent variable affecting the agent at any given point in time" and that "the impact of latent variables is observed during periods of relevant surprises rather than surprises at a specific time."
- Why unresolved: The current framework has only been tested with single binary latent variables, and the authors acknowledge this as a limitation that needs to be addressed.
- What evidence would resolve it: Empirical results comparing the framework's performance in environments with varying numbers of interacting latent variables, showing how detection accuracy and learning efficiency scale with complexity.

### Open Question 2
- Question: What is the computational complexity of the surprise divergence measure compared to traditional divergence measures like KL divergence, and how does it scale with high-dimensional data?
- Basis in paper: [inferred] The paper introduces a novel surprise divergence measure and mentions plans for complexity analysis and optimization in future work, but does not provide empirical complexity comparisons.
- Why unresolved: While the theoretical properties of the surprise divergence are established, practical performance metrics and scalability analysis are not presented.
- What evidence would resolve it: Benchmark studies comparing computational time, memory usage, and accuracy of the surprise divergence versus KL divergence across datasets of varying dimensionality and sample sizes.

### Open Question 3
- Question: How robust is the hidden variable detection mechanism to sensor noise and imperfect observations, and what modifications are needed to maintain accuracy in real-world robotic applications?
- Basis in paper: [explicit] The authors note that "sensors and actuators are sometimes very noisy, leading to cases where the observed value does not match what should have been observed" and discuss this as a limitation in their work in progress section.
- Why unresolved: The current framework assumes accurate sensors and does not address the challenge of distinguishing between surprises caused by latent variables versus sensor noise.
- What evidence would resolve it: Experiments showing detection accuracy and false positive rates under various noise conditions, along with proposed modifications to the surprise coefficient that account for sensor uncertainty.

## Limitations
- The framework assumes accurate sensors and doesn't address distinguishing between latent variable surprises versus sensor noise
- Only tested with single binary latent variables, not scalable to multiple interacting latent variables
- Limited empirical validation of the novel surprise divergence measure against traditional divergence measures

## Confidence

- **Mechanism 1 (Surprise-based detection)**: Medium - Conceptually sound but relies on assumption that surprise indicates latent variables rather than noise
- **Mechanism 2 (Structure learning from surprise)**: Low - Novel approach without strong empirical validation in literature
- **Mechanism 3 (Utility-weighted EM)**: Medium - Intuitive weighting scheme but untested against alternative parameter estimation methods
- **Overall framework**: Medium - Promising approach but limited to single latent variable scenario

## Next Checks

1. **Noise robustness test**: Introduce varying levels of sensor noise and evaluate whether the framework incorrectly detects latent variables, comparing against baseline approaches using statistical significance thresholds

2. **Multi-latent scalability**: Extend the environment to include multiple interacting barriers and evaluate whether the framework can correctly identify and model multiple latent variables versus conflating them

3. **Parameter sensitivity analysis**: Systematically vary the surprise threshold, utility weighting parameters, and EM convergence criteria to identify conditions under which the framework succeeds or fails