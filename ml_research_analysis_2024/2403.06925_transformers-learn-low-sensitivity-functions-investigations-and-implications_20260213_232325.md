---
ver: rpa2
title: 'Transformers Learn Low Sensitivity Functions: Investigations and Implications'
arxiv_id: '2403.06925'
source_url: https://arxiv.org/abs/2403.06925
tags:
- sensitivity
- bias
- training
- learning
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates the inductive biases of Transformers compared
  to other neural network architectures like MLPs, CNNs, ConvMixers, and LSTMs. The
  key finding is that Transformers learn functions with lower sensitivity to token-wise
  input perturbations, across both vision and language tasks.
---

# Transformers Learn Low Sensitivity Functions: Investigations and Implications

## Quick Facts
- **arXiv ID**: 2403.06925
- **Source URL**: https://arxiv.org/abs/2403.06925
- **Reference count**: 40
- **Primary result**: Transformers learn functions with lower sensitivity to token-wise input perturbations compared to MLPs, CNNs, ConvMixers, and LSTMs across vision and language tasks.

## Executive Summary
This paper investigates the inductive biases of Transformers compared to other neural network architectures by measuring their sensitivity to token-wise input perturbations. The key finding is that Transformers consistently learn functions with lower sensitivity across both vision and language tasks. This low-sensitivity bias correlates with improved robustness to common corruptions, flatter minima in the loss landscape, and can serve as a progress measure for understanding training dynamics. The authors provide theoretical analysis showing Transformers exhibit a weak form of spectral bias, preferring lower-order Fourier coefficients which leads to lower sensitivity functions.

## Method Summary
The paper introduces a unified notion of sensitivity for high-dimensional data and measures it by adding Gaussian noise to individual tokens and computing the probability that the model's prediction changes. Experiments are conducted across multiple vision tasks (CIFAR-10, ImageNet, Fashion-MNIST, SVHN) and language tasks (MRPC, QQP) comparing Transformers with MLPs, CNNs, ConvMixers, and LSTMs. Sensitivity regularization is implemented as an additional loss term that minimizes the difference between predictions on clean and noisy inputs. Theoretical analysis leverages Fourier analysis on Boolean functions and Gaussian Process behavior of wide neural networks to explain the observed sensitivity patterns.

## Key Results
- Transformers exhibit lower sensitivity than MLPs, CNNs, ConvMixers, and LSTMs across both vision and language tasks
- Lower sensitivity correlates with improved robustness to common corruptions in vision tasks
- Sensitivity can be used as an efficient intervention to improve robustness of Transformers
- Lower sensitivity correlates with flatter minima in the loss landscape
- Sensitivity serves as a progress measure for understanding training dynamics like grokking

## Why This Works (Mechanism)

### Mechanism 1
Transformers learn functions with lower sensitivity to token-wise input perturbations compared to other architectures across vision and language tasks. This is tied to their architecture's bias toward learning functions that depend on lower-order Fourier coefficients in the frequency domain, which inherently leads to lower sensitivity. The sensitivity metric is defined as the average change in output when a token is replaced with a small perturbation.

### Mechanism 2
Lower sensitivity correlates with improved robustness to common corruptions in vision tasks. Functions with lower sensitivity are less affected by small perturbations in the input, which translates to better performance under data corruptions that act like noise (e.g., Gaussian noise, blur, weather effects). The low sensitivity acts as a form of implicit regularization that stabilizes predictions.

### Mechanism 3
Lower sensitivity correlates with flatter minima in the loss landscape, which is associated with better generalization. Sensitivity can be measured by how much the model's output changes when the input is perturbed. This is analogous to measuring the flatness of the loss landscape by perturbing the weights. Lower sensitivity implies that small changes in input lead to small changes in output, which is similar to a flat minimum where small changes in weights do not drastically change the loss.

## Foundational Learning

- **Fourier analysis on Boolean cube and sensitivity in Boolean functions**: Needed to understand how Fourier coefficients relate to sensitivity and how this connects to the spectral bias of transformers. Quick check: Can you explain why low-degree Fourier terms correspond to low-sensitivity functions on the Boolean cube?

- **Gaussian Process behavior of wide neural networks and Neural Tangent Kernel (NTK)**: Needed because the proof of Proposition 3.1 uses the fact that transformers with linear attention converge to a Gaussian Process in the infinite width limit, and the NTK/Kernel captures the implicit bias. Quick check: What is the relationship between the eigenvalues of the NTK and the degree of the Fourier terms the network learns?

- **Robustness and flatness of minima in optimization**: Needed to understand the implications of low sensitivity and how robustness and generalization are related to the geometry of the loss landscape. Quick check: How does the sharpness of a minimum relate to the model's generalization performance?

## Architecture Onboarding

- **Component map**: Input tokenization -> Self-attention layer -> Feed-forward network -> Sensitivity measurement
- **Critical path**: Tokenization → Self-attention → Feed-forward → Sensitivity measurement
- **Design tradeoffs**: Token size vs. sequence length (smaller tokens increase sequence length, computational cost); noise variance (too small may not capture meaningful sensitivity, too large may not reflect local perturbations); model depth and width (deeper/wider models may have different sensitivity profiles)
- **Failure signatures**: High sensitivity values (model is very sensitive to small input changes, may indicate overfitting or lack of robustness); low sensitivity but poor accuracy (model may be too simple or not learning relevant features); sensitivity not distinguishing architectures (metric may not be capturing relevant differences)
- **First 3 experiments**:
  1. Train a single-layer self-attention model on synthetic data with sparse and frequent tokens and measure sensitivity to see if the model prefers the lower sensitivity solution
  2. Train ViT and CNN on CIFAR-10 and measure sensitivity to compare their inductive biases
  3. Train RoBERTa and LSTM on QQP and measure sensitivity to see if transformers are less sensitive to individual tokens

## Open Questions the Paper Calls Out

### Open Question 1
Does the low-sensitivity bias of transformers extend to more complex architectures like GPT-3 or GPT-4, and how does it compare to their performance on diverse tasks? The paper primarily focuses on transformer models like ViT and RoBERTa, but does not explore larger models like GPT-3 or GPT-4. Comparative studies on GPT-3 or GPT-4, analyzing their sensitivity and performance across various tasks would resolve this.

### Open Question 2
How does the low-sensitivity bias of transformers influence their ability to generalize to unseen data distributions beyond the tested scenarios? The paper's experiments focus on specific datasets and corruptions, leaving the generalization aspect to broader distributions unexplored. Experiments testing transformers on diverse, unseen data distributions, measuring sensitivity and generalization performance would resolve this.

### Open Question 3
Can the sensitivity metric be effectively used to guide the design of new neural network architectures that balance accuracy and robustness? The paper suggests sensitivity as a potential metric for understanding and improving model robustness, but does not explore its application in designing new architectures. Development and testing of new architectures optimized for low sensitivity, evaluating their accuracy and robustness compared to existing models would resolve this.

## Limitations

- Theoretical analysis relies heavily on assumptions about infinite-width networks and Gaussian Process behavior that may not hold for practical implementations
- Sensitivity metric may not fully capture all aspects of model robustness or generalization behavior
- Correlation between sensitivity and robustness could be influenced by other architectural factors not controlled for in experiments
- The leap from Boolean functions to real-valued high-dimensional data requires additional justification

## Confidence

- **High confidence**: Empirical finding that Transformers consistently show lower sensitivity than other architectures across vision and language tasks
- **Medium confidence**: Theoretical analysis linking Transformer architecture to low sensitivity through Fourier analysis
- **Medium confidence**: Claim that lower sensitivity directly causes improved robustness

## Next Checks

1. **Ablation on tokenization**: Test whether sensitivity differences persist when using different tokenization strategies (patch sizes, subword tokenizers) to isolate the effect of tokenization from architectural properties

2. **Controlled synthetic experiments**: Create synthetic datasets where sensitivity can be precisely controlled and verify that models learn lower-sensitivity solutions when sensitivity is rewarded, and that this directly improves robustness

3. **Cross-task generalization**: Evaluate whether sensitivity measured on one task (e.g., image classification) predicts robustness on completely different tasks (e.g., object detection or segmentation) to test if sensitivity captures a fundamental architectural property rather than task-specific behavior