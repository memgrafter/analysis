---
ver: rpa2
title: 'NegMerge: Sign-Consensual Weight Merging for Machine Unlearning'
arxiv_id: '2410.05583'
source_url: https://arxiv.org/abs/2410.05583
tags:
- task
- unlearning
- forget
- performance
- merge
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: NegMerge proposes a novel machine unlearning method that leverages
  multiple fine-tuned models instead of selecting a single one. It computes task vectors
  from models trained with varied hyperparameters and merges only the elements with
  consistent shared signs, then negates the merged task vector to induce unlearning.
---

# NegMerge: Sign-Consensual Weight Merging for Machine Unlearning

## Quick Facts
- arXiv ID: 2410.05583
- Source URL: https://arxiv.org/abs/2410.05583
- Reference count: 30
- Primary result: Achieves better unlearning performance than state-of-the-art methods while requiring similar or fewer computational resources

## Executive Summary
NegMerge introduces a novel machine unlearning approach that leverages multiple fine-tuned models instead of selecting a single one. The method computes task vectors from models trained with varied hyperparameters and merges only elements with consistent shared signs, then negates the merged task vector to induce unlearning. Evaluated on twelve datasets across four backbone architectures, NegMerge outperforms existing techniques like Task Arithmetic, Uniform Merge, and Greedy Merge while maintaining minimal degradation on the retain set.

## Method Summary
NegMerge works by fine-tuning multiple models on the forget set using varied hyperparameters, computing task vectors (differences between fine-tuned and original model weights) for each, and merging these vectors by retaining only elements with consistent signs across all task vectors. The merged vector is then negated and applied to the original model to achieve unlearning. This approach differs from traditional methods that select a single optimal fine-tuned model, instead using all fine-tuned models to create a more robust task vector through sign-consistency filtering.

## Key Results
- Outperforms state-of-the-art unlearning methods on zero-shot and standard image recognition tasks
- Maintains better retain-set performance while achieving stronger forget-set reduction
- Requires similar or fewer computational resources compared to baseline methods
- Shows consistent improvements across twelve datasets and four backbone architectures

## Why This Works (Mechanism)

### Mechanism 1
Merging only sign-consistent task vector elements reduces interference with the retain set while targeting the forget set. When multiple fine-tuned models produce task vectors, elements with consistent signs across all vectors likely encode information about the forget set, while inconsistent signs likely encode retain-set knowledge affected by training variation. This filtering preserves retain-set performance while erasing forget-set knowledge.

### Mechanism 2
Using multiple fine-tuned models with varied hyperparameters yields a more robust task vector than selecting a single optimal model. Hyperparameter variation creates diverse task vectors, and combining them via sign-consensus filtering produces a final vector that better captures the essential forget-set direction while averaging out training-noise variation.

### Mechanism 3
Negating the merged task vector achieves machine unlearning without requiring retain-set data. Task arithmetic shows that subtracting a task vector trained on the forget set reduces performance on that set. By using a merged vector that targets only forget-set elements, negation removes forget-set knowledge while preserving retain-set performance.

## Foundational Learning

- **Task arithmetic and model merging fundamentals**: Needed because NegMerge builds directly on task arithmetic by modifying how task vectors are constructed before negation. Quick check: How does task vector subtraction in task arithmetic relate to continual learning's catastrophic forgetting problem?

- **Weight space geometry and sign-consistency filtering**: Understanding why sign-consistency across multiple fine-tuned models indicates forget-set relevance is crucial for grasping NegMerge's design. Quick check: What does it mean geometrically when task vector elements have inconsistent signs across fine-tuned models?

- **Hyperparameter sensitivity and model diversity**: NegMerge's effectiveness depends on creating diverse fine-tuned models through hyperparameter variation. Quick check: How does hyperparameter variation in fine-tuning create different task vectors that can be meaningfully combined?

## Architecture Onboarding

- **Component map**: Fine-tuning pipeline -> Task vector computation -> Sign-consistency filter -> Merged vector construction -> Negation module -> Evaluation suite

- **Critical path**: 1) Fine-tune multiple models on forget set with varied hyperparameters, 2) Compute task vectors (θforget_ft - θoriginal) for each fine-tuned model, 3) Apply sign-consistency filtering across all task vectors, 4) Merge filtered elements into final task vector, 5) Negate merged vector from original model, 6) Evaluate performance on forget and retain sets

- **Design tradeoffs**: More fine-tuned models → better coverage of forget-set space but higher computational cost; stricter sign-consistency → cleaner forget-set targeting but potentially smaller merged vector; different hyperparameter ranges → affects diversity and effectiveness of aggregation

- **Failure signatures**: High retain-set degradation likely caused by insufficient sign-consistency filtering or systematic bias in fine-tuning; low forget-set reduction may indicate inadequate hyperparameter diversity or too strict sign-consistency filtering; unstable performance could result from inconsistent sign patterns that don't generalize

- **First 3 experiments**: 1) Reproduce baseline task arithmetic with single best fine-tuned model on a simple dataset, 2) Implement NegMerge with 5-10 fine-tuned models and compare performance to baseline, 3) Analyze sign-consistency patterns by visualizing which elements are retained vs. filtered out

## Open Questions the Paper Calls Out

- **Open Question 1**: How does NegMerge perform when the forget set is extremely small (e.g., less than 1% of the training data)? The paper mentions that methods like Random Labeling and Influence Unlearning struggle when the forget set is much smaller than the retain set, suggesting potential limitations for very small forget sets.

- **Open Question 2**: Can the sign-consistency merging approach be extended to other model editing tasks beyond unlearning, such as task addition or capability enhancement? The paper only evaluates the sign-consistency approach in the context of machine unlearning, not for other model editing applications.

- **Open Question 3**: What is the theoretical justification for why elements with consistent signs across task vectors correspond to the forget set? The paper provides empirical evidence supporting this assumption but does not offer a formal theoretical justification.

## Limitations

- The paper lacks comprehensive ablation studies examining how many fine-tuned models are needed for optimal performance
- No detailed computational overhead analysis is provided beyond claiming "similar or fewer computational resources"
- The method's effectiveness may depend heavily on the quality and diversity of fine-tuned models, but these factors are not thoroughly explored
- Potential privacy risks if fine-tuned models leak information from the forget set before merging are not addressed

## Confidence

- **High Confidence**: The empirical results showing improved unlearning performance compared to state-of-the-art methods across multiple datasets and architectures
- **Medium Confidence**: The theoretical justification for why sign-consistency filtering effectively targets forget-set knowledge while preserving retain-set performance
- **Low Confidence**: The claim about requiring "similar or fewer computational resources" without detailed resource usage comparisons

## Next Checks

1. Conduct ablation studies varying the number of fine-tuned models (e.g., 2, 5, 10, 20) to determine the point of diminishing returns and optimal configuration

2. Perform detailed computational resource analysis comparing training time, memory usage, and inference overhead between NegMerge and baseline methods

3. Test the method's robustness to different hyperparameter selection strategies (random search vs. structured search) and evaluate how this affects sign-consistency patterns and final performance