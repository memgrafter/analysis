---
ver: rpa2
title: Differentially Private Prototypes for Imbalanced Transfer Learning
arxiv_id: '2406.08039'
source_url: https://arxiv.org/abs/2406.08039
tags:
- equivalent
- rough
- private
- data
- privacy
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Differentially Private Prototype Learning
  (DPPL), a novel approach for private transfer learning that addresses challenges
  in high privacy regimes, low data settings, and imbalanced datasets. The core idea
  is to use publicly pre-trained encoders to extract features from private data and
  generate differentially private prototypes that represent each class in the embedding
  space.
---

# Differentially Private Prototypes for Imbalanced Transfer Learning

## Quick Facts
- **arXiv ID**: 2406.08039
- **Source URL**: https://arxiv.org/abs/2406.08039
- **Reference count**: 40
- **Primary result**: DPPL achieves strong privacy-utility trade-offs, especially in high privacy regimes and for imbalanced data

## Executive Summary
This paper introduces Differentially Private Prototype Learning (DPPL), a novel approach for private transfer learning that addresses challenges in high privacy regimes, low data settings, and imbalanced datasets. The core idea is to use publicly pre-trained encoders to extract features from private data and generate differentially private prototypes that represent each class in the embedding space. These prototypes can be publicly released for inference without additional privacy costs. The method includes two main variants: DPPL-Mean, which estimates private class means using CoinPress, and DPPL-Public, which privately selects public prototypes from publicly available data.

## Method Summary
DPPL uses publicly pre-trained encoders to extract features from private data, then generates differentially private prototypes for each class in the embedding space. DPPL-Mean estimates private class means using CoinPress, while DPPL-Public privately selects prototypes from public datasets using the exponential mechanism. The prototypes are then used for nearest-neighbor classification without additional privacy costs due to DP's post-processing property. The approach is particularly effective for imbalanced datasets and high privacy regimes.

## Key Results
- DPPL outperforms existing baselines including DP-LS and DPSGD-Global-Adapt in high privacy regimes
- Maintains high accuracy for minority classes in imbalanced datasets
- Achieves strong privacy-utility trade-offs across four vision datasets and four state-of-the-art encoders

## Why This Works (Mechanism)

### Mechanism 1
- Claim: DPPL-Mean improves utility in low-dimensional embedding spaces by using CoinPress to estimate class means with minimal noise addition.
- Mechanism: CoinPress iteratively clips and averages embeddings, reducing the noise scale compared to standard mean estimation under DP.
- Core assumption: The embedding space dimensionality (≤ 2048) is low enough for CoinPress to be effective, and class means are close to the origin.
- Evidence anchors:
  - [abstract] "DPPL-Mean, which estimates private class means using CoinPress"
  - [section] "We use CoinPress in our work as our relevant dimensionality is d ≤ 2048 and the means of our embeddings are generally close to the origin"
- Break condition: If embedding dimensionality grows significantly or class means move far from the origin, CoinPress utility degrades.

### Mechanism 2
- Claim: DPPL-Public leverages public data to reduce privacy costs by privately selecting prototypes instead of estimating them from private data.
- Mechanism: Uses exponential mechanism with cosine similarity utility function to select public data points that best represent each private class.
- Core assumption: Public dataset is large enough to contain good representatives for each private class, and cosine similarity is a meaningful distance metric.
- Evidence anchors:
  - [abstract] "we can privately sample our DP prototypes from the publicly available data points"
  - [section] "we privately select public prototypes for each training class"
- Break condition: If public dataset is too small or not representative of private class distributions, prototype quality suffers.

### Mechanism 3
- Claim: DPPL achieves high utility in high-privacy regimes by avoiding iterative noise addition through prototype-based inference.
- Mechanism: Prototypes are computed once with DP guarantees, then used for inference without additional privacy cost due to post-processing property.
- Core assumption: A strong pre-trained encoder provides sufficiently discriminative embeddings for prototype-based classification.
- Evidence anchors:
  - [abstract] "Since our DP prototypes can be obtained from only a few private training data points and without iterative noise addition"
  - [section] "using them for predictions will not incur additional privacy costs due to the DP post-processing guarantees"
- Break condition: If encoder quality degrades significantly, prototype-based classification accuracy drops.

## Foundational Learning

- Concept: Differential Privacy (DP) and zCDP
  - Why needed here: Understanding privacy guarantees and composition properties is essential for evaluating DPPL methods
  - Quick check question: What's the relationship between ε-DP and ρ-zCDP?

- Concept: Prototype-based learning
  - Why needed here: DPPL fundamentally relies on representing classes with prototypes in embedding space
  - Quick check question: How does prototype-based classification differ from parametric models?

- Concept: Transfer learning with pre-trained encoders
  - Why needed here: DPPL leverages public pre-trained encoders to extract features from private data
  - Quick check question: Why is feature extraction from a public encoder privacy-preserving?

## Architecture Onboarding

- Component map: Encoder → Embedding Space → Prototype Selection (DPPL-Public) or Mean Estimation (DPPL-Mean) → Inference
- Critical path: Data → Encoder → Embedding → Prototype Computation → Classification
- Design tradeoffs: DPPL-Mean vs DPPL-Public (privacy cost vs prototype quality), dimensionality reduction vs accuracy
- Failure signatures: High privacy cost with low utility (means diverging), poor accuracy with strong encoders (prototype quality issues)
- First 3 experiments:
  1. Test DPPL-Mean on small, balanced dataset with low dimensionality to verify CoinPress behavior
  2. Compare DPPL-Public prototype quality with ground truth class means on balanced dataset
  3. Benchmark privacy-utility tradeoff of both methods across varying privacy budgets on CIFAR10

## Open Questions the Paper Calls Out
None

## Limitations
- Effectiveness limited to embedding spaces with dimensionality ≤ 2048
- Performance heavily relies on availability and quality of public datasets
- Single prototype per class may be insufficient for complex, multimodal distributions

## Confidence

- **High Confidence**: The core DPPL framework and its two main variants (DPPL-Mean and DPPL-Public) are well-defined and the privacy guarantees are theoretically sound.

- **Medium Confidence**: The experimental results showing DPPL's effectiveness, particularly for imbalanced datasets and high privacy regimes, are convincing but could benefit from additional ablation studies.

- **Low Confidence**: The generalizability of DPPL to domains beyond vision tasks and to future embedding spaces with significantly higher dimensionality.

## Next Checks
1. **Dimensionality Stress Test**: Evaluate DPPL-Mean's performance as the embedding dimensionality increases beyond 2048 to quantify the breakdown point of the CoinPress algorithm.

2. **Public Dataset Sensitivity**: Systematically vary the size and quality of the public dataset used in DPPL-Public to measure its impact on prototype quality and downstream accuracy.

3. **Distribution Complexity Analysis**: Test DPPL on datasets with known multimodal class distributions to assess whether a single prototype per class is sufficient or if more complex prototype representations are needed.