---
ver: rpa2
title: The representation landscape of few-shot learning and fine-tuning in large
  language models
arxiv_id: '2409.03662'
source_url: https://arxiv.org/abs/2409.03662
tags:
- shot
- high
- school
- college
- layer
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper compares how in-context learning (ICL) and supervised
  fine-tuning (SFT) shape internal representations in large language models (LLMs)
  when solving the same question-answering task. Using a density-based clustering
  approach, the authors analyze the geometry of hidden representations across model
  layers, tracking how probability modes evolve and relate to semantic content.
---

# The representation landscape of few-shot learning and fine-tuning in large language models

## Quick Facts
- **arXiv ID**: 2409.03662
- **Source URL**: https://arxiv.org/abs/2409.03662
- **Reference count**: 40
- **Primary result**: ICL and SFT shape internal representations differently, with ICL creating semantically organized early-layer clusters and SFT producing sharp answer-related clustering in later layers.

## Executive Summary
This paper investigates how in-context learning (ICL) and supervised fine-tuning (SFT) differently shape internal representations in large language models when solving the same question-answering task. Using density-based clustering and intrinsic dimension analysis, the authors reveal distinct computational strategies: ICL produces hierarchically organized, semantically meaningful representations in early layers aligned with question subjects, while SFT sharpens clustering of answer-related modes in later layers. Both methods show a sharp transition in middle layers, but with different characteristics. These findings illuminate how LLMs process information under different learning paradigms and suggest optimal methods for extracting task-relevant information.

## Method Summary
The authors analyze hidden representations from multiple Llama and Mistral model families on the MMLU dataset, comparing 0-shot, few-shot (1-5 shots), and fine-tuned models. They use the Gride algorithm to estimate intrinsic dimension, ADP clustering with confidence Z=1.6 to identify density peaks and hierarchical structure, and compute ARI to measure cluster-subject/answer alignment. LoRA fine-tuning is performed on the union of MMLU dev and validation sets with specific hyperparameters. The analysis tracks how probability modes evolve across layers, examining both the geometry of representations and their semantic alignment.

## Key Results
- ICL creates semantically organized, hierarchically structured representations in early layers aligned with question subjects
- SFT produces sharp clustering of answer-related modes in later layers, better encoding answer identities
- Both methods exhibit a sharp transition in intrinsic dimension around middle layers, but with different structural characteristics
- Fine-tuned models show more distinct answer-related clusters while ICL maintains more mixed subject clusters

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Few-shot learning (ICL) shapes interpretable, semantically organized representations in early network layers.
- Mechanism: ICL leverages in-context examples to activate early layer processing that clusters data by subject, creating a hierarchical organization reflecting semantic relationships.
- Core assumption: Early layers are capable of forming semantically meaningful clusters when provided with contextual examples.
- Evidence anchors:
  - [abstract] "ICL shapes interpretable representations hierarchically organized according to their semantic content."
  - [section 4.2] "Few-shot learning creates more interpretable representations in early layers of the network, organized according to the underlying semantics of data."
  - [corpus] No direct corpus support found; weak external corroboration.
- Break condition: If ICL examples are too few or semantically unrelated, the hierarchical semantic clustering fails.

### Mechanism 2
- Claim: Supervised fine-tuning (SFT) sharpens clustering of answer-related modes in later network layers.
- Mechanism: SFT modifies later layers to produce distinct, sharp density peaks aligned with answer identities, improving answer encoding.
- Core assumption: Fine-tuning can reshape the representation landscape in later layers without disrupting early layer semantics.
- Evidence anchors:
  - [abstract] "Fine-tuned representations develop probability modes that better encode the identity of answers."
  - [section 4.3] "Fine-tuning enhances the sharp emergence of clustered representations according to answers in the second half of the network."
  - [corpus] No direct corpus support found; weak external corroboration.
- Break condition: If fine-tuning dataset is too small or noisy, the later-layer clustering degrades.

### Mechanism 3
- Claim: A sharp transition in intrinsic dimension (ID) separates layers encoding high-level semantic information from those generating answers.
- Mechanism: The ID peaks at middle layers indicate a shift from semantic abstraction to answer-specific encoding.
- Core assumption: Changes in intrinsic dimension reliably signal shifts in computational function within the network.
- Evidence anchors:
  - [abstract] "Both methods exhibit a sharp transition in the middle of the network."
  - [section 4.1] "A marked peak in the intrinsic dimension of the dataset, and a sudden change in the geometrical structure of the representations."
  - [corpus] No direct corpus support found; weak external corroboration.
- Break condition: If the task does not involve semantic abstraction followed by answer generation, the ID transition may not appear.

## Foundational Learning

- Concept: Density-based clustering (ADP algorithm)
  - Why needed here: To analyze the multimodal structure of hidden representations without assuming convex cluster shapes.
  - Quick check question: What are the three main steps of the ADP algorithm?

- Concept: Intrinsic dimension (ID) estimation
  - Why needed here: To quantify the global structure of the data manifold and detect transitions in information processing.
  - Quick check question: What algorithm is used to estimate intrinsic dimension in this paper?

- Concept: Adjusted Rand Index (ARI)
  - Why needed here: To measure the consistency between cluster partitions and semantic labels (subjects or answers).
  - Quick check question: What does an ARI of 1 indicate?

## Architecture Onboarding

- Component map: Data -> Hidden representations -> ADP clustering -> ID measurement -> Dendrogram construction -> ARI calculation
- Critical path: Data -> Hidden representations -> ADP clustering -> ID measurement -> Dendrogram construction -> ARI calculation
- Design tradeoffs: ADP allows automatic discovery of clusters but requires good sampling; Gride estimates ID without explicit dimensionality reduction but assumes manifold structure
- Failure signatures: Low ARI values, unstable cluster counts across Z values, ID profiles lacking clear peaks
- First 3 experiments:
  1. Run ADP clustering on 0-shot representations and count clusters per layer.
  2. Compute ID profiles for 5-shot vs. 0-shot setups and compare peak locations.
  3. Measure ARI between clusters and subjects/answers for fine-tuned model in late layers.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do different prompt structures affect the formation of subject-related clusters in early layers during ICL?
- Basis in paper: [explicit] The paper mentions "the prompt structure can also be made more general" and that "further investigations on more differentiated contexts would strengthen our findings."
- Why unresolved: The current study used a specific few-shot prompting format, but didn't systematically explore variations in prompt structure (e.g., different ordering, formatting, or contextual information).
- What evidence would resolve it: Experiments comparing cluster formation across diverse prompt structures (varying shot order, prompt formatting, contextual cues) while measuring subject-cluster ARI and comparing against the current few-shot setup.

### Open Question 2
- Question: What specific mechanisms in the middle layers cause the sharp transition in intrinsic dimension and cluster structure?
- Basis in paper: [inferred] The paper identifies a critical transition around layer 16-17 with peak intrinsic dimension and structural changes, but states "the description of the transition...can be analyzed more in detail, for instance, by providing an interpretation of the mechanism underlying the information flow."
- Why unresolved: The paper identifies when the transition occurs but doesn't explain the underlying mechanisms driving this change in information processing.
- What evidence would resolve it: Detailed mechanistic studies examining attention patterns, information flow, and feature representations specifically in the transition layers to identify what triggers the dimensionality and clustering changes.

### Open Question 3
- Question: How generalizable are these representation landscape differences across different task domains and datasets?
- Basis in paper: [explicit] The paper acknowledges limitations, stating "the analysis of other QA datasets and generic textual sources would make our observations more general."
- Why unresolved: The current study focuses on MMLU, a specific question-answering dataset with a particular subject structure, limiting generalizability.
- What evidence would resolve it: Replication of the analysis on diverse datasets (different domains, question types, or open-ended text) while comparing representation landscapes across ICL and fine-tuning to identify consistent vs. task-specific patterns.

## Limitations
- Findings based on a single dataset (MMLU) and question-answering tasks may not generalize to other domains
- Does not investigate other learning paradigms like RLHF or RAG that could produce different representational landscapes
- Statistical significance of differences between ICL and SFT is not explicitly quantified

## Confidence
- **High confidence**: The claim that both ICL and SFT exhibit a sharp transition in intrinsic dimension around middle layers is well-supported by the data and aligns with established understanding of transformer architectures.
- **Medium confidence**: The assertion that ICL creates semantically organized representations in early layers and SFT sharpens answer-related clustering in later layers is supported by the clustering analysis and ARI measurements, but could benefit from additional statistical validation.
- **Low confidence**: The broader implications about optimal methods for extracting information from LLMs based on these representational differences are speculative and not directly tested in the paper.

## Next Checks
1. Perform statistical tests (e.g., permutation tests) to quantify the significance of differences in ID profiles, ARI values, and clustering metrics between ICL and SFT conditions across multiple random seeds and dataset subsets.
2. Replicate the analysis using a different dataset (e.g., Natural Questions or SQuAD) to assess generalizability of the representational landscape findings beyond MMLU.
3. Investigate the robustness of the ADP clustering results by varying the confidence threshold Z and comparing how cluster stability and ARI values change across ICL and SFT conditions.