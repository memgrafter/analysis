---
ver: rpa2
title: 'LaDiMo: Layer-wise Distillation Inspired MoEfier'
arxiv_id: '2408.04278'
source_url: https://arxiv.org/abs/2408.04278
tags:
- arxiv
- experts
- training
- layers
- layer
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: LaDiMo introduces a two-stage approach to convert dense Transformer
  models into sparse Mixture-of-Experts (MoE) models. The first stage uses layer-wise
  knowledge distillation to train individual experts, splitting feed-forward network
  weights into multiple experts initialized from a pre-trained model.
---

# LaDiMo: Layer-wise Distillation Inspired MoEfier

## Quick Facts
- arXiv ID: 2408.04278
- Source URL: https://arxiv.org/abs/2408.04278
- Authors: Sungyoon Kim; Youngjun Kim; Kihyo Moon; Minsung Jang
- Reference count: 9
- Key outcome: Reduced activated parameters by >20% while maintaining 97% of original MMLU accuracy using only 100K tokens

## Executive Summary
LaDiMo presents a two-stage approach to convert dense Transformer models into sparse Mixture-of-Experts (MoE) models. The method uses layer-wise knowledge distillation to train individual experts by splitting feed-forward network weights, then implements an adaptive routing policy that determines per-layer expert activation based on routing weight distributions. When applied to LLaMA2-7B, the approach achieves significant computational savings while maintaining high accuracy, demonstrating a 10% throughput improvement.

## Method Summary
LaDiMo converts dense Transformer models to sparse MoE models through layer-wise knowledge distillation followed by adaptive routing. First, FFN weights are split into multiple experts per layer, initialized from a pre-trained dense model, and trained independently using MSE loss to match the original layer's output on a small dataset. Second, an adaptive router profiles routing weight distributions to determine per-layer policies (top-1, top-2, top-3, or dynamic) without additional training. The method selectively converts bottom-most layers to MoE layers based on empirical observation that later layers have less impact on accuracy.

## Key Results
- Reduced activated parameters by over 20% compared to dense baseline
- Maintained 97% of original MMLU accuracy with only 100K tokens of training data
- Achieved 10% throughput improvement through selective MoE layer placement

## Why This Works (Mechanism)

### Mechanism 1: Layer-wise Knowledge Distillation
- Claim: Layer-wise distillation compresses the original dense FFN by training each expert to mimic the original layer's output independently
- Mechanism: Each expert is initialized by splitting the original FFN weights into submatrices, then trained with MSE loss to match the dense layer's output on a small dataset
- Core assumption: The original model's learned representations can be preserved through independent expert training without interference between experts
- Evidence anchors: [abstract] "LaDiMo consists of two stages: layer-wise expert construction and routing policy decision. By harnessing the concept of Knowledge Distillation, we compress the model and rapidly recover its performance." [section 4.2] "LaDiMo trains the MoE block to mimic an FFN as in layer-wise distillation methods... the training data should be composed of inputs for the FFN, namely hidden states."

### Mechanism 2: Adaptive Routing Policy
- Claim: The adaptive router dynamically selects the number of experts per token based on routing weight distribution without additional training
- Mechanism: Routes tokens to top-1, top-2, top-3, or dynamic experts per layer by profiling maximal routing weights and using global/local quantiles to determine routing policies
- Core assumption: The routing weight distribution for a given layer is stable enough across inference tasks to allow pre-computed routing policies
- Evidence anchors: [abstract] "Furthermore, we develop an adaptive router that optimizes inference efficiency by profiling the distribution of routing weights and determining a layer-wise policy that balances accuracy and latency." [section 4.4] "LaDiMo adaptively and dynamically route experts in a training-free way, mitigating the lack of training data."

### Mechanism 3: Selective MoE Layer Placement
- Claim: Selective MoE layer placement preserves accuracy while maximizing computational savings
- Mechanism: Only the bottom-most layers are converted to MoE layers based on empirical observation that later layers have less impact on accuracy degradation
- Core assumption: Later layers in the transformer architecture contribute less to overall model accuracy than earlier layers
- Evidence anchors: [section 5.2] "We observed that as the number of layers with its FFN replaced with trained MoE block increases, so does the throughput of the assembled partially MoEfied model, while the accuracy decreases." [section 5.2] "if we replace the FFN from a single layer with a trained MoE block, the negative effect on its accuracy declines as the layer index goes to the end."

## Foundational Learning

- Concept: Knowledge Distillation
  - Why needed here: Enables training sparse experts to mimic dense model behavior without full retraining from scratch
  - Quick check question: What loss function is used to train MoE experts to match the original FFN output?

- Concept: Mixture-of-Experts Architecture
  - Why needed here: Understanding how gating routers distribute tokens across experts and the computational benefits of sparse activation
  - Quick check question: How many experts are activated per token in the Mixtral-8x7B model mentioned?

- Concept: Routing Weight Distribution Analysis
  - Why needed here: Critical for implementing the adaptive router that determines per-layer routing policies based on weight distributions
  - Quick check question: What metric is used to decide whether to route to top-1, top-2, top-3, or dynamic experts?

## Architecture Onboarding

- Component map: Original dense model (e.g., LLaMA2-7B) → Layer-wise MoE block creation → Expert weight splitting → Knowledge distillation training → Adaptive router profiling → Final MoE model assembly
- Critical path: Dataset collection (100K tokens) → MoE block training per layer → Adaptive router policy computation → Model assembly and evaluation
- Design tradeoffs:
  - Number of MoE layers vs. accuracy retention (more layers = higher throughput but lower accuracy)
  - Expert size vs. computational savings (larger experts = better accuracy but fewer savings)
  - Routing policy complexity vs. inference efficiency (dynamic routing = better accuracy but higher latency)
- Failure signatures:
  - Accuracy drops >5% from baseline indicate poor expert training or routing policy selection
  - Training loss plateaus early suggest initialization issues or insufficient data
  - Load imbalance among experts indicates need for auxiliary loss adjustment
- First 3 experiments:
  1. Convert single bottom layer to MoE and measure accuracy degradation with different routing policies (top-1, top-2, top-3)
  2. Train MoE blocks with and without auxiliary loss to measure impact on load balancing
  3. Profile routing weight distributions across different datasets to validate adaptive router assumptions

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal number of layers to convert to MoE layers for different model sizes and tasks?
- Basis in paper: [explicit] The paper states "we performed experiments by replacing the bottom-most layers with MoE blocks, varying the number of the chosen layers" and shows trade-offs between throughput and accuracy in Figure 4.
- Why unresolved: The paper only tested LLaMA2-7B and found that converting the last 12 layers worked well, but this may not generalize to other model sizes or tasks.
- What evidence would resolve it: Systematic experiments varying model sizes (5B, 13B, 70B), tasks (code generation, translation, reasoning), and layer positions would establish optimal conversion patterns.

### Open Question 2
- Question: How does the adaptive routing policy perform compared to trained dynamic routing methods?
- Basis in paper: [explicit] The paper states "most existing studies suffer from the limitation of requiring additional training, whereas our proposed method determines the layer-wise routing policy in a training-free manner" and mentions this as a contribution.
- Why unresolved: The paper only compares against static top-k routing and doesn't benchmark against other dynamic routing methods that require training.
- What evidence would resolve it: Direct comparison of LaDiMo's adaptive routing against trained methods like Li et al. (2023) or Wu et al. (2024) on accuracy and efficiency metrics would show relative performance.

### Open Question 3
- Question: What is the impact of dataset size on MoE block training quality and task-specific performance?
- Basis in paper: [explicit] The paper notes "training for a single layer took about 5 hours" using 100K tokens and shows "a partially MoEfied model with MoE blocks trained with a limited amount of text dataset recovers its generation capability in a general sense but still lacks the ability to generate in a specific field" when tested on GSM8K.
- Why unresolved: The paper only tested with 100K tokens and shows GSM8K performance suffers, but doesn't explore intermediate dataset sizes or their effect on general performance.
- What evidence would resolve it: Experiments training with varying dataset sizes (10K, 100K, 1M, 10M tokens) and measuring MMLU, GSM8K, and other task performance would reveal dataset size requirements.

## Limitations

- Limited hyperparameter disclosure: The paper lacks specific values for learning rates, training steps, and routing policy thresholds needed for exact reproduction
- Single model architecture: Results are only demonstrated on LLaMA2-7B, limiting generalizability to other dense models and tasks
- Small evaluation dataset: The 100K token training dataset may not be representative of diverse inference scenarios for adaptive routing policy validation

## Confidence

- Layer-wise distillation mechanism (High): The approach of training individual experts to mimic original FFN outputs is well-established in knowledge distillation literature and the paper provides clear procedural details for this component.
- Adaptive routing policy (Medium): While the concept of routing based on weight distributions is sound, the lack of specific threshold values and limited evaluation across diverse datasets reduces confidence in the robustness of this mechanism.
- Selective layer placement strategy (Medium): The empirical observation that bottom layers can be converted without significant accuracy loss is supported by results, but lacks theoretical justification for why this pattern holds and whether it extends to other architectures.
- Overall performance claims (Medium): The 97% accuracy retention and 20% parameter reduction are demonstrated, but the small evaluation dataset (100K tokens) and single model architecture limit confidence in generalizability.

## Next Checks

1. **Hyperparameter Sensitivity Analysis**: Systematically vary the learning rate, batch size, and training steps for MoE block training to determine how sensitive the accuracy retention is to these parameters. This would reveal whether the 97% accuracy target is robust or requires precise tuning.

2. **Routing Policy Generalization**: Test the adaptive router on multiple datasets with different input distributions (formal text, code, dialogue) to verify that the pre-computed routing policies remain effective. This would validate the core assumption that routing weight distributions are stable across inference tasks.

3. **Layer Placement Transferability**: Apply the selective MoE layer placement strategy to a different dense model architecture (e.g., OPT-7B or Falcon-7B) and measure whether the same pattern holds where bottom layers can be converted without significant accuracy degradation. This would test the generalizability of the empirical observation about layer importance.