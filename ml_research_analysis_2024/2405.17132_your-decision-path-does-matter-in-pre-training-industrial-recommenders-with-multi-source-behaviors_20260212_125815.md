---
ver: rpa2
title: Your decision path does matter in pre-training industrial recommenders with
  multi-source behaviors
arxiv_id: '2405.17132'
source_url: https://arxiv.org/abs/2405.17132
tags:
- hier
- graph
- decision
- recommendation
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: HIER introduces a hierarchical decision path framework to enhance
  cross-domain recommendation by explicitly modeling user intents and item exemplars.
  It leverages graph neural networks to capture high-order topological information
  from a knowledge graph and employs exemplar-level and information bottleneck based
  contrastive learning to adaptively learn decision paths.
---

# Your decision path does matter in pre-training industrial recommenders with multi-source behaviors

## Quick Facts
- arXiv ID: 2405.17132
- Source URL: https://arxiv.org/abs/2405.17132
- Reference count: 25
- Key outcome: HIER introduces a hierarchical decision path framework to enhance cross-domain recommendation by explicitly modeling user intents and item exemplars. It leverages graph neural networks to capture high-order topological information from a knowledge graph and employs exemplar-level and information bottleneck based contrastive learning to adaptively learn decision paths. The model significantly improves recommendation performance in both few-shot and zero-shot settings, achieving up to 22.90% relative CTR improvement in online A/B testing across four target domains.

## Executive Summary
HIER is a novel pre-training framework for industrial recommenders that leverages hierarchical decision paths to improve cross-domain recommendation. By explicitly modeling user intents and item exemplars through a knowledge graph enhanced with graph neural networks, HIER captures the complex relationships between user behaviors across multiple domains. The framework employs contrastive learning techniques at both exemplar and intent levels to adaptively learn decision paths, significantly improving recommendation performance in both few-shot and zero-shot scenarios. Online A/B testing demonstrates substantial improvements in CTR across four target domains.

## Method Summary
HIER addresses cross-domain recommendation by pre-training on multi-source behaviors and transferring knowledge to domain-agnostic scenarios. The framework builds upon a knowledge graph connecting users and items across different domains, using a 2-layer GNN for initial node representations. It then applies a hierarchical decision path framework that separates user intents from item exemplars, using L0 regularization to select important paths. Exemplar-level contrastive learning clusters similar entities while information bottleneck contrastive learning improves user representations by retaining task-relevant information. The model is pre-trained on source domains and fine-tuned on target domains using universal encoding.

## Key Results
- Achieves 22.90% relative CTR improvement in online A/B testing across four target domains
- Significantly outperforms baseline methods in both few-shot and zero-shot settings
- Ablation studies demonstrate the effectiveness of each component, with the GL module being foundational

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Hierarchical decision path modeling improves representation learning by explicitly separating user intents from item exemplars.
- Mechanism: The model defines a latent decision path network connecting user intents to item exemplars, using sparsity regularization (L0) to select the most important paths. This structure allows the model to capture the hierarchical relationship between why a user interacts (intent) and what they interact with (exemplar).
- Core assumption: User behaviors are driven by distinct intents that can be modeled as separate latent variables, and items can be clustered into exemplars that share common traits.
- Evidence anchors:
  - [abstract] "we propose HIER, a novel Hierarchical decIsion path Enhanced Representation learning for cross-domain recommendation"
  - [section] "we formally define the decision path network from user intents to item exemplars and optimize the decision path selection via sparsity regularization"
  - [corpus] Weak evidence - no direct citations about hierarchical decision path in related works
- Break condition: If user intents are not separable or if item exemplars cannot be meaningfully clustered, the decision path structure becomes meaningless.

### Mechanism 2
- Claim: Exemplar-level contrastive learning improves item representations by clustering similar entities and distancing dissimilar ones.
- Mechanism: The model learns a matrix of latent exemplars and computes similarity scores between each item and these exemplars. It then uses InfoNCE loss to pull related views (original item and exemplar-based perspective) close while pushing unrelated views apart.
- Core assumption: Items that share traits can be meaningfully grouped into exemplars, and contrastive learning can effectively learn these clusters.
- Evidence anchors:
  - [section] "we propose a contrastive exemplar learning module to improve entity representation"
  - [section] "the goal of the proposed contrastive exemplar learning is to pull related views close and push others away"
  - [corpus] No direct evidence in related works about exemplar-level contrastive learning for recommendation
- Break condition: If the number of exemplars is poorly chosen (too few or too many), the contrastive learning may fail to capture meaningful clusters.

### Mechanism 3
- Claim: Information bottleneck contrastive learning improves user representations by retaining only task-relevant information.
- Mechanism: The model creates an intent-enhanced view of user representations and uses information bottleneck principles to encourage divergence between original and augmented views while ensuring the augmented view retains maximal task-relevant information.
- Core assumption: Users act dynamically based on different recommended items, and retaining only task-relevant information improves robustness to noise.
- Evidence anchors:
  - [section] "different from traditional contrastive learning that encourages the commonality between representations of different views, instead we propose to encourage the divergence of the original view and the augmentation view"
  - [section] "Inspired by Information Bottleneck that stems from information theory"
  - [corpus] No direct evidence in related works about information bottleneck contrastive learning for recommendation
- Break condition: If the intent matrix cannot capture meaningful user dynamics, or if the information bottleneck constraint is too strict, the user representations may lose important information.

## Foundational Learning

- Concept: Graph Neural Networks for knowledge graph representation
  - Why needed here: To capture high-order topological information between multi-source behaviors and bridge the domain gap across heterogeneous miniapps
  - Quick check question: How does a 2-layer GNN with entity-entity-entity aggregation differ from a user-entity-entity aggregation for users?

- Concept: Contrastive learning with InfoNCE loss
  - Why needed here: To learn universal representations that cluster similar entities and push dissimilar ones apart, improving generalization to unseen domains
  - Quick check question: What role does the temperature parameter τ play in controlling the strength of penalties on hard negative samples?

- Concept: L0 regularization for sparse path selection
  - Why needed here: To explicitly model and keep only the most important decision paths among user intents and item exemplars, avoiding overfitting to noise
  - Quick check question: How does the binary concrete distribution enable gradient-based optimization of binary decision path variables?

## Architecture Onboarding

- Component map: Knowledge Graph Encoder -> Hierarchical Decision Path Network -> Exemplar-level Contrastive Learning Module -> Information Bottleneck Contrastive Learning Module -> Pre-training Stage -> Fine-tuning Stage
- Critical path: Knowledge graph encoding → Hierarchical decision path learning → Exemplar and intent contrastive learning → Universal representation → Target domain fine-tuning
- Design tradeoffs: Number of exemplars vs. granularity of clustering (Figure 4 shows middle values perform best), number of intents vs. capturing subtle differences (Figure 5 shows larger values perform better)
- Failure signatures: Poor performance in low-frequency intents suggests knowledge graph structure is insufficient; degradation when removing GL component confirms its foundational importance
- First 3 experiments:
  1. Verify knowledge graph encoding by comparing representations with and without GNN layers on source domain task
  2. Test exemplar-level contrastive learning by varying exemplar count and measuring clustering quality
  3. Validate information bottleneck contrastive learning by comparing user representations with and without intent divergence constraint

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the number of exemplars and intents in HIER impact recommendation performance across different target domains?
- Basis in paper: [explicit] The paper presents ablation studies varying the number of exemplars (100-500) and intents (10-50) and their impact on NDCG metrics.
- Why unresolved: The paper only examines a limited range of exemplar and intent numbers, and the optimal values may vary across different target domains and datasets.
- What evidence would resolve it: Experiments testing a wider range of exemplar and intent numbers across multiple target domains, and analyzing the performance trends to determine optimal values for different scenarios.

### Open Question 2
- Question: How does HIER's performance compare to other state-of-the-art cross-domain recommendation methods when considering user intents and decision paths?
- Basis in paper: [inferred] The paper mentions that existing CDR methods overlook the impact of decision paths and user intents, but does not provide a direct comparison with other methods that explicitly model these factors.
- Why unresolved: The paper only compares HIER to other methods that do not explicitly model user intents and decision paths, so it is unclear how HIER would perform against methods that do.
- What evidence would resolve it: Experiments comparing HIER to other state-of-the-art cross-domain recommendation methods that explicitly model user intents and decision paths, using the same datasets and evaluation metrics.

### Open Question 3
- Question: How does HIER's performance scale with the size of the knowledge graph and the number of source domains?
- Basis in paper: [inferred] The paper mentions that HIER is built upon a knowledge graph bridging users/items across different domains, but does not discuss how its performance is affected by the size of the knowledge graph or the number of source domains.
- Why unresolved: The paper only evaluates HIER on a specific knowledge graph and set of source domains, so it is unclear how its performance would be affected by changes in these factors.
- What evidence would resolve it: Experiments varying the size of the knowledge graph and the number of source domains, and analyzing HIER's performance trends to determine its scalability and limitations.

## Limitations
- Implementation details of exemplar-level and information bottleneck contrastive learning modules are underspecified, including specific hyperparameters and noise distribution
- L0 regularization implementation for decision path selection lacks concrete architectural details
- Results are based on a single industrial dataset (Alipay), raising questions about generalizability to other platforms or recommendation scenarios

## Confidence
- **Hierarchical decision path effectiveness**: High confidence - Multiple ablation studies (Table 3) demonstrate consistent performance improvements when the hierarchical structure is present.
- **Contrastive learning contributions**: Medium confidence - While ablation results show improvements, the specific mechanisms (exemplar-level vs. information bottleneck) are less clearly separated in their individual contributions.
- **Knowledge graph encoding**: High confidence - The foundational role of the GNN component is well-established through systematic removal experiments (Figure 3).
- **Online A/B testing results**: Medium confidence - The 22.90% CTR improvement is impressive but reported as a single aggregate figure without domain-specific breakdowns or statistical power analysis.

## Next Checks
1. **Ablation on exemplar count sensitivity**: Replicate the experiments varying the number of exemplars (K) across a wider range to determine the optimal granularity and test the robustness of the clustering approach beyond the reported range.

2. **Cross-platform generalization test**: Apply HIER to a different industrial recommendation dataset (e.g., e-commerce or streaming platforms) with different domain structures to validate whether the hierarchical decision path approach generalizes beyond Alipay's specific ecosystem.

3. **User intent interpretability analysis**: Conduct a qualitative analysis of the learned user intents to verify that they capture meaningful behavioral patterns, such as examining whether distinct intents emerge for different types of user activities and whether these align with domain expertise expectations.