---
ver: rpa2
title: Multimodal Audio-based Disease Prediction with Transformer-based Hierarchical
  Fusion Network
arxiv_id: '2410.09289'
source_url: https://arxiv.org/abs/2410.09289
tags:
- fusion
- disease
- each
- modality
- representation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces AuD-Former, a transformer-based hierarchical
  fusion network for multimodal audio-based disease prediction. The key idea is to
  integrate intra-modal and inter-modal fusion hierarchically to capture dependencies
  within and across modalities.
---

# Multimodal Audio-based Disease Prediction with Transformer-based Hierarchical Fusion Network

## Quick Facts
- arXiv ID: 2410.09289
- Source URL: https://arxiv.org/abs/2410.09289
- Reference count: 40
- Primary result: AuD-Former achieves 91.13% accuracy on Coswara and 96.39% on IPVS for disease prediction

## Executive Summary
This paper introduces AuD-Former, a transformer-based hierarchical fusion network for multimodal audio-based disease prediction. The key innovation is integrating intra-modal and inter-modal fusion hierarchically to capture dependencies within and across modalities. Intra-modal transformers learn unimodal representations by mining intra-modal correlations within each modality, while inter-modal transformers fuse these unimodal representations into a high-level multimodal representation by encoding cross-modal dependencies. Evaluated on five datasets for COVID-19, Parkinson's disease, and pathological dysarthria, AuD-Former achieves state-of-the-art performance, outperforming existing methods with 91.13% accuracy on Coswara and 96.39% on IPVS. Ablation studies confirm the importance of each component in the model.

## Method Summary
AuD-Former is a hierarchical transformer network that first uses intra-modal transformers to learn unimodal representations from each audio modality (cough, breathing, speech, vowel sounds), then employs inter-modal transformers to fuse these representations while encoding cross-modal dependencies. The model processes sequences of acoustic features extracted from seven domains (ZCR, STE, SC, Log-Mel, MFCC, GFCC, CQCC) at 44.1 kHz. Training uses binary cross-entropy loss with SGD optimizer and 10-fold cross-validation, evaluating with accuracy, F1 score, AUC, sensitivity, and specificity metrics. The architecture includes temporal convolution layers, positional embeddings, and multi-head attention mechanisms.

## Key Results
- AuD-Former achieves 91.13% accuracy on the Coswara dataset and 96.39% on IPVS for disease prediction
- The model outperforms existing methods across all five evaluated datasets for COVID-19, Parkinson's disease, and pathological dysarthria classification
- Ablation studies confirm that both intra-modal and inter-modal representation learning components are essential for optimal performance

## Why This Works (Mechanism)

### Mechanism 1
Hierarchical fusion enables more effective multimodal representation by progressively integrating intra-modal and inter-modal information. The model first uses intra-modal transformers to learn unimodal representations that capture dependencies within each modality, then uses inter-modal transformers to fuse these unimodal representations while encoding cross-modal dependencies. This sequential approach leverages complementary information within and between modalities. Break condition: If intra-modal and inter-modal dependencies are not complementary but redundant, hierarchical fusion may not provide advantage over parallel fusion approaches.

### Mechanism 2
Attention-based representation learning captures complex dependencies within modality-specific and modality-shared spaces. Multi-head self-attention in intra-modal transformers learns weighted combinations of features within each modality, while cross-modal attention in inter-modal transformers learns which cross-modal information to incorporate into each unimodal representation. This allows the model to capture non-linear relationships that simple concatenation or averaging operations cannot. Break condition: If dependencies are primarily linear, attention mechanisms may overfit or provide marginal benefit over simpler methods.

### Mechanism 3
Unimodal representations become more robust through cross-modal information exchange before final fusion. Each unimodal representation is enhanced by cross-modal attention that allows it to selectively incorporate complementary information from other modalities, creating more informative inputs for final fusion. This cross-modal enhancement strengthens individual representations before they are combined. Break condition: If modalities contain conflicting rather than complementary information, cross-modal enhancement may introduce noise.

## Foundational Learning

- **Concept**: Transformer architecture and multi-head attention
  - Why needed here: The model relies on transformer-based layers for both intra-modal and inter-modal representation learning, requiring understanding of self