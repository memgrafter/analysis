---
ver: rpa2
title: Convex Formulations for Training Two-Layer ReLU Neural Networks
arxiv_id: '2410.22311'
source_url: https://arxiv.org/abs/2410.22311
tags:
- neural
- networks
- convex
- training
- optimization
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a convex optimization framework for training
  two-layer ReLU neural networks with infinite width. The authors reformulate the
  training problem as a completely positive program (CPP), which is convex but NP-hard
  to solve exactly.
---

# Convex Formulations for Training Two-Layer ReLU Neural Networks

## Quick Facts
- arXiv ID: 2410.22311
- Source URL: https://arxiv.org/abs/2410.22311
- Authors: Karthik Prakhya; Tolga Birdal; Alp Yurtsever
- Reference count: 31
- One-line primary result: A convex optimization framework for training infinite-width two-layer ReLU neural networks with competitive test accuracy compared to NNGP and NTK methods.

## Executive Summary
This paper presents a convex optimization framework for training two-layer ReLU neural networks with infinite width. The authors reformulate the training problem as a completely positive program (CPP), which is convex but NP-hard to solve exactly. To address this, they introduce a semidefinite programming (SDP) relaxation that can be solved in polynomial time. The approach is evaluated on synthetic and real datasets, showing competitive performance compared to established methods like NNGP and NTK.

## Method Summary
The paper reformulates infinite-width two-layer ReLU network training as a convex completely positive program (CPP) in a finite-dimensional lifted space. This exact formulation is then relaxed to a tractable semidefinite program (SDP) that can be solved in polynomial time. A rounding heuristic based on three-operator splitting extracts network weights from the SDP solution. The method is evaluated on both synthetic and real-world datasets, achieving competitive test accuracy compared to Neural Network Gaussian Process (NNGP) and Neural Tangent Kernel (NTK) approaches.

## Key Results
- An exact CPP formulation for infinite-width ReLU networks that becomes convex through lifting
- An SDP relaxation with approximation ratios above 76.9% on synthetic data
- Test accuracy metrics on real datasets competitive with kernel methods (NNGP and NTK)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The infinite-width two-layer ReLU network training problem can be exactly reformulated as a convex completely positive program (CPP) in a finite-dimensional lifted space.
- Mechanism: By introducing a lifted variable Λ that represents the expected outer product of network weights and activations, the non-convex training problem becomes convex. The key insight is that when the network is sufficiently wide (beyond a critical threshold), the empirical distribution over weights and activations converges to a probability measure, allowing exact representation via a convex program.
- Core assumption: The network width exceeds a finite critical threshold R that depends on input/output dimensions and number of data points, such that the empirical distribution of weights and activations can be exactly represented by a probability measure.
- Evidence anchors:
  - [abstract]: "we reformulate the problem of training infinite-width two-layer ReLU networks as a convex completely positive program in a finite-dimensional (lifted) space"
  - [section]: "there exists a finite critical width beyond which the network's expressivity saturates, making the training problem equivalent to the proposed copositive formulation"
  - [corpus]: Found 25 related papers with average neighbor FMR=0.478, suggesting moderate relatedness to convex optimization of neural networks
- Break condition: The width is below the critical threshold R, or the data distribution prevents exact representation by a probability measure.

### Mechanism 2
- Claim: The completely positive program, while convex, is NP-hard to solve exactly due to the complete positivity constraint, necessitating an SDP relaxation.
- Mechanism: The completely positive cone CP is a subset of the positive semidefinite cone PSD, but the reverse is not true. By relaxing CP to the doubly non-negative cone (intersection of PSD and entrywise non-negative matrices), we obtain a tractable semidefinite program that can be solved in polynomial time.
- Core assumption: The SDP relaxation provides a tight approximation to the original CPP, such that the optimal solution of the relaxed problem is close to the true optimal solution.
- Evidence anchors:
  - [abstract]: "we introduce a semidefinite relaxation that can be solved in polynomial time"
  - [section]: "we propose a semidefinite programming relaxation of the original formulation"
  - [corpus]: Weak evidence - corpus contains papers on convex relaxations of ReLU networks but lacks specific evidence on SDP tightness
- Break condition: The SDP relaxation becomes loose, leading to significant approximation error between the relaxed and true optimal values.

### Mechanism 3
- Claim: The SDP solution can be rounded to extract network weights that achieve competitive test accuracy compared to established methods like NNGP and NTK.
- Mechanism: After solving the SDP relaxation to obtain Λ⋆, a rounding procedure based on three-operator splitting finds the closest feasible point λλ⊤ to Λ⋆ in the original problem space. This provides a concrete set of network weights that can be used for prediction.
- Core assumption: The rounding procedure successfully recovers a good approximation of the network weights from the SDP solution, despite the non-convex nature of the rounding problem.
- Evidence anchors:
  - [abstract]: "combined with a rounding heuristic, our approach achieves competitive test accuracy compared to Neural Network Gaussian Process (NNGP) and Neural Tangent Kernel (NTK) methods"
  - [section]: "we propose an effective heuristic based on the three-operator splitting (TOS) method"
  - [corpus]: Weak evidence - corpus lacks specific evidence on rounding heuristics for SDP solutions of neural networks
- Break condition: The rounding procedure fails to converge or produces weights that yield poor test performance.

## Foundational Learning

- Concept: Convex optimization and semidefinite programming
  - Why needed here: The entire approach relies on transforming a non-convex neural network training problem into a convex optimization problem, specifically a completely positive program and its semidefinite relaxation
  - Quick check question: What is the difference between the completely positive cone CP and the positive semidefinite cone PSD?

- Concept: Neural tangent kernel and neural network Gaussian process
  - Why needed here: These are the benchmark methods against which the proposed approach is compared, representing established infinite-width neural network training methods
  - Quick check question: How do NNGP and NTK differ in their treatment of infinite-width neural networks?

- Concept: ReLU activation and two-layer neural networks
  - Why needed here: The paper specifically addresses two-layer ReLU networks, requiring understanding of how ReLU activations introduce non-convexity and how they can be decomposed into positive and negative parts
  - Quick check question: How does decomposing Xu into positive and negative parts help in formulating the convex program?

## Architecture Onboarding

- Component map:
  Data preprocessing -> lifted variable construction -> CPP formulation -> SDP relaxation -> rounding heuristic -> weight extraction -> evaluation

- Critical path: Solve SDP relaxation -> apply rounding heuristic -> extract weights -> evaluate performance
  - Dependencies: SDP solver must converge -> rounding must find feasible solution -> extracted weights must generalize

- Design tradeoffs:
  - Exactness vs. tractability: CPP is exact but NP-hard; SDP is tractable but approximate
  - Width vs. expressiveness: Larger width provides better approximation but increases computational cost
  - Regularization strength: Higher γ provides smoother optimization landscape but may underfit

- Failure signatures:
  - SDP solver fails to converge or reports numerical issues
  - Rounding procedure produces infeasible solutions or fails to converge
  - Test accuracy is significantly worse than baseline methods (NNGP/NTK)
  - Approximation ratio is much lower than reported bounds (76.9%)

- First 3 experiments:
  1. Verify CPP formulation on small synthetic dataset with known solution
  2. Compare SDP relaxation tightness on Random dataset with varying regularization parameters
  3. Test rounding heuristic convergence and solution quality on Spiral dataset

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the theoretical approximation ratio of the SDP relaxation to the original CP problem, and how does it scale with problem dimensions?
- Basis in paper: [explicit] The paper computes empirical approximation ratios of 76.90% and above on synthetic data, but notes these are lower bounds due to SGD limitations and critical width constraints.
- Why unresolved: The authors only provide empirical bounds rather than theoretical guarantees for the approximation ratio of the SDP relaxation.
- What evidence would resolve it: A formal proof establishing upper and lower bounds on the approximation ratio as a function of problem dimensions, data characteristics, and regularization parameters.

### Open Question 2
- Question: How does the proposed convex formulation scale to larger neural networks with more than two layers or different activation functions?
- Basis in paper: [inferred] The paper focuses specifically on two-layer ReLU networks and mentions related work on extending to deeper networks, but doesn't explore this directly.
- Why unresolved: The authors only demonstrate their approach on two-layer networks and leave multi-layer extensions as future work.
- What evidence would resolve it: Experimental results and theoretical analysis of the convex formulation applied to deeper networks with various activation functions, including scalability benchmarks.

### Open Question 3
- Question: What is the relationship between the critical width identified in the paper and the over-parameterization threshold required for good generalization in practice?
- Basis in paper: [explicit] The paper identifies a critical width beyond which network expressivity saturates, but notes that practical networks often require even wider architectures for good generalization.
- Why unresolved: The authors establish theoretical bounds on critical width for exact reformulation but don't connect this to practical generalization requirements.
- What evidence would resolve it: Empirical studies comparing critical width to generalization performance across different datasets and network architectures, potentially revealing scaling laws.

## Limitations
- The SDP relaxation's tightness varies by dataset, with empirical approximation ratios potentially not reflecting true bounds
- Critical width threshold depends on theoretical assumptions that may not hold for all data distributions
- Rounding heuristic convergence and solution quality need further validation across diverse datasets

## Confidence
- **High confidence**: The theoretical reformulation of infinite-width ReLU networks as a completely positive program (CPP) is mathematically sound and well-established in the convex optimization literature.
- **Medium confidence**: The SDP relaxation approach and its polynomial-time solvability are technically valid, though the practical tightness of the relaxation varies by dataset.
- **Low confidence**: The rounding heuristic's effectiveness across diverse real-world datasets and its generalization performance compared to established methods need more empirical validation.

## Next Checks
1. **Empirical validation of approximation bounds**: Test the SDP relaxation on additional real-world datasets beyond those reported to verify the claimed approximation ratios and compare with SGD-trained networks.

2. **Critical width analysis**: Conduct experiments varying network width to empirically verify the existence and practical implications of the critical width threshold R.

3. **Rounding procedure robustness**: Evaluate the three-operator splitting rounding method across diverse initialization strategies and compare with alternative rounding approaches to assess its reliability and convergence properties.