---
ver: rpa2
title: 'DOTS: Learning to Reason Dynamically in LLMs via Optimal Reasoning Trajectories
  Search'
arxiv_id: '2410.03864'
source_url: https://arxiv.org/abs/2410.03864
tags:
- reasoning
- planner
- llms
- trajectory
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces DOTS, a method that enables large language
  models (LLMs) to dynamically select optimal reasoning actions for given questions.
  Unlike static prompting strategies, DOTS learns to plan reasoning trajectories by
  defining atomic action modules (query analysis, solution, verification) and searching
  for the most effective combination per question and LLM capability.
---

# DOTS: Learning to Reason Dynamically in LLMs via Optimal Reasoning Trajectories Search

## Quick Facts
- arXiv ID: 2410.03864
- Source URL: https://arxiv.org/abs/2410.03864
- Reference count: 18
- Outperforms static prompting and vanilla instruction tuning across eight reasoning tasks

## Executive Summary
DOTS introduces a dynamic approach to reasoning in large language models by learning to select optimal reasoning trajectories rather than applying static strategies. The method defines atomic reasoning action modules (query analysis, solution, verification) and searches for the most effective combination per question and LLM capability. Implemented in two paradigms—external planner guiding solver or internalized planning capability—DOTS demonstrates consistent accuracy improvements across mathematical and logical reasoning tasks, with gains up to 8.7% over baselines. The approach adapts to question complexity by allocating deeper reasoning to harder problems, showing robustness and efficiency compared to static methods.

## Method Summary
DOTS operates through three key steps: defining atomic reasoning action modules that can be composed into various reasoning trajectories, searching for optimal action trajectories through iterative exploration and evaluation for each specific task-solving LLM, and fine-tuning LLMs to plan optimal reasoning trajectories. The method uses three layers of atomic actions: analysis (query rewriting, decomposition), solution (Chain-of-Thought, Program-of-Thought), and verification (self-verification, empty). For training data collection, DOTS iteratively searches all possible reasoning trajectories for each question, evaluating them using the solver LLM and retaining top-performing paths. The planner LLM is then fine-tuned using supervised learning to predict optimal trajectories, either as an external guide or internalized into the solver itself.

## Key Results
- DOTS consistently outperforms static reasoning techniques and vanilla instruction tuning across eight reasoning tasks
- Achieves accuracy gains up to 8.7% over baseline methods on mathematical reasoning tasks
- Demonstrates adaptation to both question characteristics and solver capabilities through distribution analysis
- Shows improved performance on in-distribution, few-shot, and out-of-distribution tasks

## Why This Works (Mechanism)

### Mechanism 1
DOTS dynamically adapts reasoning actions based on question characteristics and solver capability by searching through all possible reasoning trajectories for each training question, evaluating them using the specific solver LLM, and selecting the optimal trajectory that balances success rate and computational efficiency. This assumes the solver LLM's performance varies predictably across different reasoning actions for different question types.

### Mechanism 2
Fine-tuning with optimal trajectories produces better generalization than static prompting because DOTS trains the planner to select from multiple reasoning actions rather than using a single static format. This allows the model to match question characteristics to the most effective reasoning strategy, assuming different question types benefit from different reasoning strategies.

### Mechanism 3
The dual learning paradigm (external vs internalized planner) provides flexibility across different LLM accessibility constraints. For closed-source/compute-intensive solvers, an external lightweight planner is fine-tuned; for open-source/small solvers, the planning capability is internalized. This assumes the planner and solver can be effectively trained separately or jointly without performance degradation.

## Foundational Learning

- Concept: Supervised fine-tuning with custom trajectories
  - Why needed here: Standard SFT uses static reasoning formats, but DOTS requires training on dynamically selected optimal trajectories
  - Quick check question: How does the loss function differ between vanilla SFT and DOTS training?

- Concept: Reinforcement learning vs search-based optimization
  - Why needed here: DOTS uses iterative search with evaluation rather than RL, which is more sample-efficient for this problem
  - Quick check question: What's the computational complexity difference between MCTS and DOTS's iterative search approach?

- Concept: Multi-task learning across heterogeneous reasoning strategies
  - Why needed here: The planner must learn to select among diverse reasoning actions (CoT, PoT, verification, etc.) for different question types
  - Quick check question: How does the model handle the distribution shift when training on multiple reasoning strategies?

## Architecture Onboarding

- Component map: Query → Analysis Layer (rewrite/decompose) → Solution Layer (CoT/PoT) → Verification Layer (self-verify/empty) → Answer
- Critical path: Query → Search → Optimal Trajectory Selection → Execution → Answer
- Design tradeoffs: Search comprehensiveness vs computational cost, trajectory length vs success rate
- Failure signatures: Planner consistently selects suboptimal trajectories, search fails to find any working trajectory, solver performance varies unpredictably
- First 3 experiments:
  1. Run search on a small dataset with a single solver to validate trajectory selection
  2. Compare planner performance with random trajectory selection baseline
  3. Test generalization by evaluating planner on held-out question types

## Open Questions the Paper Calls Out

### Open Question 1
How does the performance of DOTS scale with the number of atomic reasoning action modules? Could increasing the number of modules beyond the three layers (analysis, solution, verification) lead to better performance or introduce additional computational overhead and potential errors? The paper only considers a fixed set of atomic reasoning actions and does not explore the impact of varying the number or types of modules.

### Open Question 2
How does the optimal trajectory searching process handle noisy or ambiguous training data? What are the limitations of the current search algorithm in dealing with questions that have multiple valid solutions or unclear ground truth answers? The paper describes an iterative search process but doesn't address how it handles noisy or ambiguous data.

### Open Question 3
Can the DOTS framework be extended to handle multi-modal reasoning tasks, such as those involving images or structured data alongside text? What modifications would be necessary to incorporate non-textual reasoning actions? The paper focuses exclusively on text-based reasoning tasks and doesn't explore multi-modal applications.

## Limitations

- Computational complexity of searching through all possible trajectories for each question is substantial but not quantified
- Limited evaluation scope focused primarily on mathematical reasoning tasks with only one non-mathematical task
- No detailed analysis of whether selected trajectories are truly optimal versus merely different from static approaches

## Confidence

- High Confidence: Experimental results showing DOTS outperforms vanilla instruction tuning and static reasoning methods on tested mathematical reasoning tasks
- Medium Confidence: Claims that DOTS adapts to both question characteristics and solver capabilities based on correlational evidence
- Low Confidence: Scalability to larger models or more diverse question types, and computational efficiency claims due to lack of detailed timing analysis

## Next Checks

1. Measure the total compute cost and inference time per question for the search process across different dataset sizes to determine computational viability
2. Conduct ablation studies forcing the planner to use specific trajectory types for different question categories to validate true optimality
3. Apply DOTS to non-mathematical reasoning tasks (commonsense reasoning, code generation) to test cross-domain generalization