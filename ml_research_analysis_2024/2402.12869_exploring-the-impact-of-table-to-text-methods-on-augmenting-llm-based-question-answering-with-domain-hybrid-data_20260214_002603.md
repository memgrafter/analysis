---
ver: rpa2
title: Exploring the Impact of Table-to-Text Methods on Augmenting LLM-based Question
  Answering with Domain Hybrid Data
arxiv_id: '2402.12869'
source_url: https://arxiv.org/abs/2402.12869
tags:
- table
- data
- methods
- answer
- device
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This study investigates how different table-to-text generation\
  \ methods impact the performance of LLM-based question answering systems augmented\
  \ with domain hybrid data. Four representative methods\u2014Markdown format, Template\
  \ serialization, TPLM-based, and LLM-based\u2014are evaluated across two QA paradigms\
  \ (DSFT and RAG) using real-world ICT domain data."
---

# Exploring the Impact of Table-to-Text Methods on Augmenting LLM-based Question Answering with Domain Hybrid Data

## Quick Facts
- arXiv ID: 2402.12869
- Source URL: https://arxiv.org/abs/2402.12869
- Reference count: 35
- Primary result: LLM-based table-to-text generation consistently outperforms other methods in domain-specific QA systems, with method choice significantly impacting performance

## Executive Summary
This study investigates how different table-to-text generation methods affect the performance of LLM-based question answering systems when augmented with domain hybrid data. The researchers evaluated four representative methods—Markdown format, Template serialization, TPLM-based, and LLM-based—across two QA paradigms (DSFT and RAG) using real-world ICT domain data. Results demonstrate that table-to-text methods significantly influence QA performance, with relative score differences ranging from 2.8% to 9.0% in human evaluation and 4.8% to 16% in GPT-4 evaluation. The LLM-based method using ChatGPT emerged as the most reliable across both frameworks, while TPLM-based served as a good alternative in DSFT and Markdown in RAG.

## Method Summary
The study collected 6GB of ICT domain hybrid data containing text and semi-structured tables, creating the ICTQA dataset with 9,000 QA pairs. Four table-to-text methods were applied to convert tables into natural language: Markdown format (script-based), Template serialization (pre-defined templates), TPLM-based (fine-tuned BART models), and LLM-based (ChatGPT with one-shot prompting). The resulting corpora were used to build QA systems using two paradigms: DSFT (Domain-Specific Fine-Tuning with OPT/Llama2 + QLoRA) and RAG (Retrieval-Augmented Generation with FAISS + Llama2-chat/GPT-3.5-turbo). Performance was evaluated using both human experts and GPT-4, with additional analysis of domain term frequencies and semantic representations.

## Key Results
- LLM-based table-to-text method consistently outperformed others in both DSFT and RAG paradigms
- In DSFT, LLM-based and TPLM-based methods showed superior performance with relative score differences of 2.8%-9.0% in human evaluation
- In RAG, LLM-based and Markdown methods performed best, with RSD of 4.8%-16% in GPT-4 evaluation
- Performance differences were attributed to varying frequencies of domain-specific terms/verbs and quality of semantic representations in generated text chunks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Different table-to-text generation methods produce corpora with varying frequencies of domain-specific terms and verbs, which directly affects the performance of LLM-based QA systems.
- Mechanism: The frequency of domain-specific terms and verbs in the generated text influences the LLM's ability to answer questions accurately. Methods that generate text with higher frequencies of these terms and verbs are more likely to improve QA performance.
- Core assumption: The LLM's performance in answering domain-specific questions is positively correlated with the frequency of relevant terms and verbs in its training corpus.
- Evidence anchors:
  - [abstract]: "Key factors influencing performance differences include the frequency of domain-specific terms and verbs in generated text..."
  - [section]: "we also observe that different table-to-text methods have inconsistent preferences for domain verbs when describing tables... methods with higher frequencies, especially the TPLM and LLM-based methods, correspond to superior QA capabilities in the DSFT systems."
  - [corpus]: "We extract domain term sets and related verb sets from the QA pairs in the ICTQA test set. We then calculate the absolute frequency of these terms and verbs as they appear in the corpora generated by different table-to-text methods."
- Break condition: If the frequency of domain-specific terms and verbs does not correlate with QA performance, or if the LLM's training data already contains sufficient domain knowledge.

### Mechanism 2
- Claim: The quality of semantic representations in the generated text chunks affects the retrieval accuracy in the RAG paradigm, influencing overall QA performance.
- Mechanism: Text chunks with better semantic representations are more likely to be retrieved correctly when a query is made, leading to improved QA performance. Methods that generate text with superior semantic representations enhance the effectiveness of the RAG system.
- Core assumption: The retrieval accuracy in the RAG paradigm is directly related to the quality of semantic representations in the text chunks used for indexing and retrieval.
- Evidence anchors:
  - [abstract]: "the varying quality of semantic representations in the generated text chunks, which appear to be pivotal factors influencing performance disparities across the two systems."
  - [section]: "Under the same LLM reader setup, retrieval accuracy in this semantic space crucially impacts RAG performance... chunks generated by the LLM-based and Markdown methods, which perform well in Table 2, are closer to the query in the semantic space."
  - [corpus]: "To investigate the impact of different methods on retrieval effectiveness, we use t-SNE to visualize the clustering of a query and related chunks in the semantic space."
- Break condition: If the semantic representations do not significantly impact retrieval accuracy, or if the retrieval method does not rely on semantic similarity.

### Mechanism 3
- Claim: The choice of table-to-text generation method should align with the QA paradigm (DSFT vs. RAG) to optimize performance.
- Mechanism: Different QA paradigms have different requirements for the corpus used. In DSFT, methods that generate text with higher frequencies of domain-specific terms and verbs are preferred, while in RAG, methods that produce text with better semantic representations are more effective.
- Core assumption: The effectiveness of a table-to-text method is dependent on the specific requirements of the QA paradigm in which it is used.
- Evidence anchors:
  - [abstract]: "In the DSFT paradigm, LLM-based and TPLM-based consistently outperform others... In the RAG paradigm, while the LLM-based method still performs excellently, the Markdown has shown unexpected effectiveness."
  - [section]: "For the DSFT paradigm... we know that the LLM-based strategy with ChatGPT is outstanding and reliable in both frameworks. In case its drawbacks... are unacceptable, the TPLM-based strategy... is a good alternative in the DSFT paradigm. In the RAG paradigm, the simple and easy-to-use Markdown strategy is also a viable substitute."
  - [corpus]: "The LLM-based strategy with ChatGPT is outstanding and reliable in both frameworks... the TPLM-based strategy... is a good alternative in the DSFT paradigm. In the RAG paradigm, the simple and easy-to-use Markdown strategy is also a viable substitute."
- Break condition: If the alignment between table-to-text methods and QA paradigms does not significantly impact performance, or if a single method consistently outperforms others across all paradigms.

## Foundational Learning

- Concept: Table-to-Text Generation
  - Why needed here: Understanding how different table-to-text methods convert semi-structured data into natural language text is crucial for evaluating their impact on QA system performance.
  - Quick check question: What are the key differences between Markdown format, Template serialization, TPLM-based, and LLM-based table-to-text generation methods?
- Concept: Domain-Specific Fine-Tuning (DSFT)
  - Why needed here: DSFT involves training LLMs on domain-specific corpora to enhance their performance on domain-specific tasks. Knowing how different corpora affect this process is essential for optimizing QA systems.
  - Quick check question: How does the frequency of domain-specific terms and verbs in the training corpus influence the performance of a DSFT-based QA system?
- Concept: Retrieval-Augmented Generation (RAG)
  - Why needed here: RAG uses a domain-specific corpus as an external knowledge base to enhance LLM performance. Understanding how the quality of semantic representations in the corpus affects retrieval accuracy is key to optimizing RAG systems.
  - Quick check question: Why is the quality of semantic representations in text chunks important for the performance of a RAG-based QA system?

## Architecture Onboarding

- Component map:
  - Table-to-Text Generation Methods: Markdown, Template, TPLM-based, LLM-based
  - Domain Corpus: Generated text from tables merged with original document text
  - QA Systems: DSFT (Domain-Specific Fine-Tuning) and RAG (Retrieval-Augmented Generation)
  - Evaluation Metrics: Human evaluation, GPT-4 evaluation
- Critical path:
  - Generate domain corpus using different table-to-text methods
  - Use the generated corpus to build QA systems (DSFT and RAG)
  - Evaluate the performance of the QA systems using human and automated metrics
- Design tradeoffs:
  - Complexity vs. performance: More complex methods (TPLM-based, LLM-based) may yield better performance but require more resources
  - Resource usage: Simpler methods (Markdown, Template) are less resource-intensive but may not perform as well
  - Flexibility: LLM-based methods offer customization through in-context learning but may pose data leakage risks
- Failure signatures:
  - Poor QA performance due to low frequency of domain-specific terms and verbs in the corpus
  - Ineffective retrieval in RAG systems due to poor semantic representations in text chunks
  - Resource constraints limiting the use of more complex table-to-text methods
- First 3 experiments:
  1. Compare the performance of QA systems built using corpora generated by different table-to-text methods.
  2. Analyze the frequency of domain-specific terms and verbs in the generated corpora and correlate with QA performance.
  3. Evaluate the semantic representations of text chunks in the corpora and their impact on retrieval accuracy in RAG systems.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do the relative performance differences between table-to-text methods change when applied to datasets from different domains (e.g., medical, legal, scientific literature)?
- Basis in paper: [inferred] The paper only evaluates methods on an ICT domain dataset and notes that domain-specific terms and verbs influence performance, but does not test cross-domain generalizability.
- Why unresolved: The paper's experiments are limited to one domain (ICT), leaving uncertainty about whether the observed performance differences would hold across other domains with different terminology and table structures.
- What evidence would resolve it: Experiments applying the same four table-to-text methods to domain-specific QA datasets from medical, legal, and scientific domains, measuring performance differences and analyzing domain-specific term frequency impacts.

### Open Question 2
- Question: What is the computational trade-off between the TPLM-based method and the LLM-based method in terms of GPU hours per quality of generated text, and how does this scale with dataset size?
- Basis in paper: [explicit] The paper mentions that TPLM-based methods allow customized adjustment through fine-tuning while requiring more computational resources, and that LLM-based methods might pose risks of domain data leakage, but does not provide detailed computational cost analysis.
- Why unresolved: While the paper notes resource requirements in Table 1, it does not quantify the exact GPU hours needed for fine-tuning TPLM-based models versus API costs for LLM-based methods, nor how these scale with dataset size.
- What evidence would resolve it: Detailed benchmarking of GPU hours required for fine-tuning TPLM-based models (MVP, T5, BART) versus API costs for ChatGPT across varying dataset sizes, correlated with text quality metrics.

### Open Question 3
- Question: How do table-to-text methods affect retrieval accuracy in RAG systems when using different embedding models (e.g., BGE, OpenAI, sentence-transformers) beyond the BGE embedding model used in this study?
- Basis in paper: [inferred] The paper uses BGE embeddings and finds that Markdown and LLM-based methods produce more retrieval-friendly semantic representations, but does not test other embedding models.
- Why unresolved: The paper's findings about semantic representation quality are tied to a specific embedding model (BGE), leaving uncertainty about whether the same retrieval advantages would persist with other embedding approaches.
- What evidence would resolve it: Experiments repeating the RAG evaluation using multiple embedding models (OpenAI embeddings, sentence-transformers, etc.) to determine if the retrieval advantages of certain table-to-text methods are consistent across different semantic representations.

## Limitations

- Findings are based on a single domain (ICT products), limiting generalizability to other domains
- Relatively small test set (500 QA pairs) may not capture full performance variability across different question types
- One-shot setting for LLM-based table-to-text generation may not represent optimal performance that could be achieved with few-shot or fine-tuned approaches

## Confidence

- **High Confidence**: The comparative performance ranking of table-to-text methods within the ICT domain (LLM-based > TPLM-based > Markdown > Template in DSFT; LLM-based ≈ Markdown > TPLM-based > Template in RAG)
- **Medium Confidence**: The generalizability of findings across different domains and the robustness of the "RSD threshold of 2%" as a meaningful performance difference metric
- **Medium Confidence**: The attribution of performance differences primarily to domain term/verb frequencies and semantic representations, as other confounding factors may exist

## Next Checks

1. Replicate the study with datasets from different domains (e.g., healthcare, legal, finance) to test generalizability of method rankings
2. Conduct ablation studies to isolate the specific contribution of domain term frequency versus semantic representation quality to performance differences
3. Test alternative evaluation metrics and thresholds for determining meaningful performance differences beyond the 2% RSD criterion