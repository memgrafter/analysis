---
ver: rpa2
title: 'HumMUSS: Human Motion Understanding using State Space Models'
arxiv_id: '2404.10880'
source_url: https://arxiv.org/abs/2404.10880
tags:
- human
- pose
- hummuss
- pages
- vision
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes HumMUSS, a novel attention-free spatiotemporal
  architecture for human motion understanding using State Space Models (SSMs). HumMUSS
  matches the performance of transformer-based models on tasks like 3D pose estimation,
  mesh recovery, and action recognition, while offering advantages such as adaptability
  to variable frame rates, faster training and inference, and efficient real-time
  sequential prediction.
---

# HumMUSS: Human Motion Understanding using State Space Models

## Quick Facts
- **arXiv ID**: 2404.10880
- **Source URL**: https://arxiv.org/abs/2404.10880
- **Reference count**: 40
- **Primary result**: Attention-free SSM architecture matches transformer performance on human motion tasks while offering faster training and variable frame rate adaptability

## Executive Summary
HumMUSS introduces a novel attention-free architecture for human motion understanding using State Space Models (SSMs). The model replaces traditional transformer attention mechanisms with diagonal state space models and multiplicative gating, achieving competitive performance on 3D pose estimation, mesh recovery, and action recognition tasks. By leveraging the continuous-time formulation of SSMs, HumMUSS can adapt to variable frame rates without retraining, offering both computational efficiency and practical deployment advantages.

## Method Summary
HumMUSS uses a spatiotemporal architecture built from stacked spatiotemporal blocks containing alternating spatial and temporal Gated Diagonal SSM (GDSSM) layers. The model employs a lifting layer to expand input features, followed by N spatiotemporal blocks that process information through forward and backward state space model pathways with multiplicative gating. The continuous-time formulation allows adaptation to different frame rates through discretization parameter adjustment, while the diagonal state matrix with HiPPO initialization enables efficient long-range temporal modeling without attention mechanisms.

## Key Results
- Matches transformer-based MotionBERT performance on 3D pose estimation (MPJPE 37.8mm on MPI-INF-3DHP)
- Achieves 91.3% top-1 accuracy on NTU-RGBD action recognition with only 16M parameters
- Demonstrates 2-3x faster training and inference compared to transformer baselines on longer sequences
- Generalizes to variable frame rates without retraining by adjusting the discretization parameter ∆

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Diagonal State Space Models (DSSM) can efficiently replace transformer attention in human motion understanding.
- **Mechanism**: DSSM replaces the quadratic attention operation with a linear convolution via a precomputed kernel, reducing complexity from O(F²) to O(F log F).
- **Core assumption**: The diagonal state matrix A with HiPPO initialization can capture long-range temporal dependencies without full attention.
- **Evidence anchors**:
  - [abstract]: "HumMUSS consists of several stacked blocks, each containing two streams of alternating spatial and temporal Gated Diagonal SSM blocks (GDSSM)"
  - [section]: "While the naive approach to this computation requires O(L²) multiplications, it can be done in O(L log(L)) time using the Fast Fourier Transform (FFT)"
- **Break condition**: If the learned kernel K fails to capture long-range dependencies, performance will degrade significantly compared to transformers.

### Mechanism 2
- **Claim**: Multiplicative gating in GDSSM blocks can recover complex interactions that would otherwise require attention.
- **Mechanism**: The gating unit σ((xf ⊙ xb)Wcb) multiplies forward and backward processed features, allowing selective combination of temporal information.
- **Core assumption**: Hadamard product can approximate the selective interaction patterns of attention without computing full attention maps.
- **Evidence anchors**:
  - [abstract]: "HumMUSS inherits the advantages of DSSM, such as faster training and inference for longer sequences"
  - [section]: "Following [62] and [91], we combine the forward and backward aggregated information using multiplicative gating"
- **Break condition**: If the gating weights collapse to uniform values, the model loses its ability to selectively focus on relevant temporal features.

### Mechanism 3
- **Claim**: Continuous-time formulation enables adaptation to variable frame rates without retraining.
- **Mechanism**: The discretization parameter ∆ can be adjusted to match new frame rates, effectively resampling the continuous signal.
- **Core assumption**: The learned state transition dynamics are valid across different temporal resolutions when properly scaled.
- **Evidence anchors**:
  - [abstract]: "Moreover, it also can generalize to unseen and variable frame rates due to SSMs' inherent continuous time formulation"
  - [section]: "HumMUSS, being a continuous-time model, does not require positional encodings and enables training and testing on data with different frame rates"
- **Break condition**: If the signal dynamics are highly non-stationary across different frame rates, performance will degrade despite ∆ adjustment.

## Foundational Learning

- **Concept**: State Space Models (SSM) as continuous-time systems
  - **Why needed here**: Understanding how SSMs map between continuous-time scalar inputs and outputs through differential equations
  - **Quick check question**: What matrices define the continuous-time state space model, and how do they transform to discrete-time?

- **Concept**: Diagonal state space models and HiPPO initialization
  - **Why needed here**: Knowing why diagonal A matrices with specific initialization enable efficient long-range memory
  - **Quick check question**: How does the HiPPO parameterization relate to stable training with long sequences?

- **Concept**: Multiplicative gating as a surrogate for attention
  - **Why needed here**: Understanding how Hadamard product can selectively combine information without full attention computation
  - **Quick check question**: What property of multiplicative gating allows it to approximate attention-like behavior?

## Architecture Onboarding

- **Component map**: Input → lifting layer → spatial GDSSM block → temporal GDSSM block → spatiotemporal combination → final projection
- **Critical path**: Input → lifting layer → spatial GDSSM block → temporal GDSSM block → spatiotemporal combination → final projection. The temporal GDSSM blocks are critical for capturing motion dynamics.
- **Design tradeoffs**: Using k=2 dimension reduction in temporal blocks speeds up kernel computation but may lose some temporal resolution. Bidirectional vs causal variants trade future information for real-time applicability.
- **Failure signatures**: If the model fails to converge, check the initialization of Λre, Λim, C, and ∆. If performance is poor on longer sequences, verify the FFT implementation and kernel computation.
- **First 3 experiments**:
  1. Test basic 3D pose estimation on MPI-INF-3DHP with a small HumMUSS model to verify the architecture works
  2. Compare training speed and memory usage between HumMUSS and MotionBERT on sequences of increasing length
  3. Evaluate generalization to different frame rates by subsampling the test set and adjusting ∆ accordingly

## Open Questions the Paper Calls Out

The paper does not explicitly call out open questions, but several important areas remain unexplored based on the current work.

## Limitations

- Evaluation focuses primarily on controlled motion capture conditions with limited testing on highly dynamic or occluded scenarios
- Continuous-time formulation's effectiveness across extremely variable frame rates (e.g., 15fps to 120fps) is not thoroughly validated
- HiPPO initialization scheme requires careful implementation that may affect reproducibility

## Confidence

- **High Confidence**: Computational efficiency claims (O(F log F) complexity vs O(F²) for transformers) are well-supported by theoretical framework and implementation details
- **Medium Confidence**: Performance claims on 3D pose estimation and action recognition are supported by quantitative results but lack extensive ablation studies on architectural components
- **Medium Confidence**: Continuous-time adaptation claim is theoretically sound but empirical validation across diverse frame rates is limited

## Next Checks

1. **Frame Rate Robustness Test**: Evaluate HumMUSS on sequences with extreme frame rate variations (2x to 8x differences) to verify continuous-time adaptation claims, measuring performance degradation as ∆ varies
2. **Long Sequence Stability Analysis**: Test the model on sequences longer than those used in training (e.g., 1000+ frames) to assess whether the HiPPO initialization maintains performance for extended temporal dependencies
3. **Occlusion and Noise Resilience**: Introduce systematic occlusions (15-50% joint masking) and motion noise to test the model's robustness in challenging real-world conditions, comparing against transformer baselines under identical perturbations