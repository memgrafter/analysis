---
ver: rpa2
title: 'Adam on Local Time: Addressing Nonstationarity in RL with Relative Adam Timesteps'
arxiv_id: '2412.17113'
source_url: https://arxiv.org/abs/2412.17113
tags:
- adam
- learning
- adam-rel
- gradient
- update
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of nonstationarity in reinforcement
  learning, particularly how abrupt changes in gradient magnitude due to target network
  updates can cause large, unstable updates in the Adam optimizer. The authors introduce
  Adam-Rel, which resets Adam's timestep parameter to zero after each target change,
  preventing the accumulation of outdated normalization factors.
---

# Adam on Local Time: Addressing Nonstationarity in RL with Relative Adam Timesteps

## Quick Facts
- **arXiv ID**: 2412.17113
- **Source URL**: https://arxiv.org/abs/2412.17113
- **Reference count**: 40
- **Primary result**: Adam-Rel significantly outperforms standard Adam and Adam with momentum resets on Atari-10, Atari-57, and Craftax-Classic benchmarks by preventing gradient magnitude-induced instability through local timestep resetting.

## Executive Summary
This paper addresses the problem of nonstationarity in reinforcement learning, particularly how abrupt changes in gradient magnitude due to target network updates can cause large, unstable updates in the Adam optimizer. The authors introduce Adam-Rel, which resets Adam's timestep parameter to zero after each target change, preventing the accumulation of outdated normalization factors. Theoretically, this ensures bounded update sizes even with large gradient increases, and empirically reduces to learning rate annealing when such increases don't occur. Experiments on Atari-10, Atari-57, and Craftax-Classic show Adam-Rel significantly outperforms both standard Adam and Adam with momentum resets, with PPO achieving higher IQM scores and DQN showing marked improvement. The method also performs well with Polyak averaging.

## Method Summary
Adam-Rel is a modification to the Adam optimizer that resets the timestep parameter to zero after each target network change or training epoch. This prevents the exponential normalization factors in Adam from accumulating outdated information when the optimization objective changes. The method requires only a single-line code change to Adam's update loop, replacing the global timestep with a local epoch timestep that resets to zero at each target change. The authors implement this in both on-policy (PPO) and off-policy (DQN) RL algorithms, demonstrating significant performance improvements across multiple benchmark suites including Atari-10, Atari-57, and Craftax-Classic.

## Key Results
- Adam-Rel achieves significantly higher IQM scores than standard Adam on Atari-10 with PPO, with Adam-MR performing worse than both.
- On Atari-57, Adam-Rel shows consistent improvement over Adam across 57 games with PPO.
- DQN trained with Adam-Rel demonstrates marked improvement on Atari-10, validating the method's effectiveness for off-policy algorithms.
- Adam-Rel maintains good performance with Polyak averaging, suggesting it addresses the fundamental nonstationarity problem rather than being tied to specific target update mechanisms.

## Why This Works (Mechanism)

### Mechanism 1
Adam's normalization divides momentum estimates by `(1 - beta1^t)` and `(1 - beta2^t)`. In nonstationary RL, these exponential factors grow without accounting for objective changes, causing large overshoots. Resetting `t` corrects this by recalculating normalization as if starting fresh. Core assumption: The sudden gradient magnitude increases due to target network changes are significant enough to destabilize Adam's update if not corrected. Evidence: Theorem 3.1 shows update size becomes approximately `1 - beta1 / sqrt(1 - beta2)` for large gradient increases with vanilla Adam.

### Mechanism 2
Without large gradient increases, Adam-Rel's reset causes the update norm to decay rapidly then slowly increase, functionally mimicking a learning rate schedule over the epoch. Core assumption: Over typical RL epoch lengths, this behavior approximates standard learning rate annealing schedules. Evidence: Figure 1 shows Adam-Rel's update size rapidly decays then increases for small `k` values.

### Mechanism 3
Adam-MR resets all momentum estimates, discarding historical gradient information. Adam-Rel only resets `t`, keeping momentum estimates which can guide optimization toward better directions. Core assumption: Momentum estimates from previous objectives contain useful information for the new objective, not just noise. Evidence: Adam-MR performs significantly worse than Adam-Rel on both Atari and Craftax, supporting the value of retained momentum.

## Foundational Learning

- **Nonstationary optimization objectives**: RL's target networks and bootstrapped updates create discrete changes in the optimization landscape, breaking Adam's assumptions. Quick check: What causes the objective function to change during RL training?
- **Momentum-based optimizers and normalization**: Adam's normalization factors depend on cumulative timesteps, which fail when the gradient distribution changes abruptly. Quick check: How does Adam's `(1 - beta^t)` normalization factor work, and why does it assume stationarity?
- **Target networks in deep RL**: Target networks create periodic objective changes that trigger the gradient magnitude increases Adam-Rel addresses. Quick check: How often do target networks typically update in DQN, and what effect does this have on gradients?

## Architecture Onboarding

- **Component map**: Adam optimizer -> Local timestep replacement -> Target change detection -> Timestep reset -> Stable updates
- **Critical path**: Initialize Adam state → Collect experience → Reset timestep to 0 → Compute gradients → Update Adam with local timestep → Apply update → Repeat
- **Design tradeoffs**: Resetting timestep prevents overshoot but may slow initial convergence within epochs. Alternative: adaptive reset frequency based on gradient change detection.
- **Failure signatures**: Large spikes in parameter updates, training instability after target changes, poor performance on tasks requiring rapid adaptation.
- **First 3 experiments**:
  1. Implement Adam-Rel on a simple DQN variant on CartPole and verify it prevents parameter explosion during target updates.
  2. Compare Adam-Rel vs Adam on Atari-10 with identical hyperparameters to measure performance impact.
  3. Test Adam-Rel with different reset frequencies (every epoch vs every N steps) to find optimal balance.

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions, but identifies several areas for future work including exploring Adam-Rel in continuous control settings and investigating the optimal frequency for timestep resets.

## Limitations
- The theoretical analysis assumes infinite gradient increases, which may not fully capture realistic RL scenarios.
- Empirical evaluation focuses primarily on Atari and Craftax benchmarks, leaving open questions about performance on other RL domains.
- The method's effectiveness may depend on the frequency and magnitude of target network updates.

## Confidence

- **High Confidence**: The core mechanism of resetting Adam's timestep is straightforward and the theoretical bound on update size is mathematically sound under the stated assumptions.
- **Medium Confidence**: The empirical improvements on Atari-10, Atari-57, and Craftax-Classic are substantial, but the ablation studies on reset frequency and the comparison with Adam-MR, while suggestive, could benefit from more systematic hyperparameter sweeps.
- **Medium Confidence**: The claim that Adam-Rel reduces to learning rate annealing when gradient changes are absent is supported by Figure 1, but a more rigorous mathematical characterization would strengthen this claim.

## Next Checks

1. **Ablation on Reset Frequency**: Systematically vary the reset frequency (not just per epoch) and measure the impact on performance and stability across different RL algorithms.
2. **Cross-Domain Evaluation**: Test Adam-Rel on continuous control benchmarks (e.g., MuJoCo) and other non-Atari RL tasks to assess generalizability.
3. **Hyperparameter Sensitivity Analysis**: Conduct a comprehensive study of Adam-Rel's sensitivity to learning rate, beta parameters, and other optimizer hyperparameters to provide clearer guidance for practitioners.