---
ver: rpa2
title: Zero-Shot NAS via the Suppression of Local Entropy Decrease
arxiv_id: '2411.06236'
source_url: https://arxiv.org/abs/2411.06236
tags:
- architecture
- entropy
- proxies
- network
- architectures
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a novel zero-shot neural architecture search
  (NAS) method called SED (Suppression of Local Entropy Decrease) that accelerates
  architecture evaluation by three orders of magnitude compared to state-of-the-art
  proxies. The key insight is that irrational network architectures decrease the local
  entropy of feature maps, which degrades specific features to biases and reduces
  network performance.
---

# Zero-Shot NAS via the Suppression of Local Entropy Decrease

## Quick Facts
- arXiv ID: 2411.06236
- Source URL: https://arxiv.org/abs/2411.06236
- Authors: Ning Wu; Han Huang; Yueting Xu; Zhifeng Hao
- Reference count: 10
- Primary result: Achieves 0.61 Spearman's ρ on DARTS space, outperforming state-of-the-art zero-cost proxies by 0.09

## Executive Summary
This paper introduces SED (Suppression of Local Entropy Decrease), a novel zero-shot neural architecture search method that evaluates architectures by quantifying how well they suppress local entropy decrease in feature maps. The key insight is that irrational network topologies create biased feature representations by reducing feature map entropy, which degrades performance. SED computes a topology-only proxy score without requiring data or network execution, achieving three orders of magnitude speedup compared to existing proxies while selecting architectures with higher accuracy and fewer parameters.

## Method Summary
SED is based on the theoretical observation that certain network topologies decrease local entropy in feature maps, creating biased representations that harm performance. The method defines three entropy-based scores for different operation types (skip connections, convolutions, pooling) and combines them into a final architecture score. Crucially, SED requires only architecture topology information—no data or network execution—making it extremely fast. The entropy suppression is quantified through mathematical propositions that relate operation parameters (kernel sizes, strides) to local entropy changes, enabling a data-free proxy that captures performance-relevant architectural properties.

## Key Results
- Achieves 0.61 Spearman's ρ on DARTS space, 0.09 higher than existing zero-cost proxies
- Selects architectures with 81.09% test accuracy, 0.42% higher than state-of-the-art proxies
- Reduces architecture evaluation time by three orders of magnitude compared to SOTA proxies
- Outperforms most SOTA zero-cost proxies on five benchmarks while using fewer parameters

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Local entropy decrease in feature maps caused by certain network topologies degrades network performance.
- Mechanism: When feature map subtensors have low entropy (e.g., zero one-dimensional entropy or multivariate Gaussian entropy approaching negative infinity), they become biased representations that provide poor gradient feedback during training, leading to suboptimal network performance.
- Core assumption: Classification tasks require diverse feature representations, and biased feature maps reduce the network's ability to learn discriminative features.
- Evidence anchors:
  - [abstract] "We prove that particular architectural topologies decrease the local entropy of feature maps, which degrades specific features to a bias, thereby reducing network performance."
  - [section] "Proposition 1 For the classification task, given a feature map X ∈ Rw×h×1, a convolution kernel K ∈ Rkw×kh×1 and stride s = ( s1, s2, 0), where kw and kh are odd and less than w and h, respectively. The convolution operation (X[αi,kw ; βj,kh ]∗K)(kw, kh, kc) works as a bias ofK, when H1(X[αi,kw ; βj,kh ]) = 0 under the rectified linear unit activation function."

### Mechanism 2
- Claim: Network architecture topology directly influences local entropy through operation parameters.
- Mechanism: Large pooling kernels create zero-entropy subtensors, small convolution kernels fail to recover entropy, and skip connections help maintain entropy by combining feature maps with different statistics.
- Core assumption: The relationship between operation parameters (kernel sizes, strides) and local entropy is deterministic and predictable from topology alone.
- Evidence anchors:
  - [section] "Proposition 3 For the maximum pooling Omax of size ow × oh × 1 taking the feature map X(l) ∈ Rw×h×1 as input and s = ( s1, s2, 0) as stride, where ow and oh are less than w and h, respectively. At the probability greater than 1 − 2(max{ow,oh}−2)(w+h)/wh , there exists at least one subtensor X(l+1)[αi,⌈ow/s1⌉; βj,⌈oh/sw⌉] such that H1(X(l+1)[αi,⌈ow/s1⌉; βj,⌈oh/sw⌉]) = 0 ."
  - [section] "Proposition 4 For the given convolution kernel K1 ∈ Rkw1×kh1×kc1, K2 ∈ Rkw2×kh2×kc2, and the feature map X ∼ N whc(µ, Σ), H2(X[αi,kw1 ; βj,kh1 ; γv,kc1 ]) ≥ H2(X[αi,kw2 ; βj,kh2 ; γv,kc2 ]) when kw1kh1kc1 ≥ kw2kh2kc2 and kw1kh1kc1 ̸= 2kw2kh2kc2."

### Mechanism 3
- Claim: SED proxy accurately ranks architectures by quantifying entropy suppression without requiring data or network execution.
- Mechanism: SED computes a topology-based score that captures how well an architecture suppresses local entropy decrease, which correlates with performance because low-entropy architectures perform worse.
- Core assumption: The entropy-based ranking captures the most important performance-determining factors in the architecture space.
- Evidence anchors:
  - [abstract] "Experimental results show that SED outperforms most state-of-the-art (SOTA) zero-cost proxies on multiple benchmarks, reducing the time consumed in architecture evaluation by three orders of magnitude compared to SOTA proxies."
  - [section] "Definition 4 (SED) Given a network architecture A ∈ A consisted of blocks (B1, B2, · · · , Bn), its SED is calculated by the following formula: [formulas provided]"

## Foundational Learning

- Concept: Local entropy in feature maps
  - Why needed here: SED proxy relies on understanding how feature map entropy relates to network performance, which is the core theoretical foundation of the method.
  - Quick check question: What happens to the gradient feedback from a feature map subtensor when its entropy approaches zero under ReLU activation?

- Concept: Multivariate Gaussian entropy
  - Why needed here: The paper uses multivariate Gaussian entropy to measure feature map information content, which is more sophisticated than simple histogram-based entropy.
  - Quick check question: How does the covariance matrix of a feature map subtensor affect its multivariate Gaussian entropy?

- Concept: Neural architecture search evaluation metrics
  - Why needed here: Understanding how to evaluate NAS methods (Spearman correlation, accuracy rankings) is crucial for interpreting SED's performance claims.
  - Quick check question: Why might Spearman correlation be preferred over Pearson correlation when evaluating architecture ranking proxies?

## Architecture Onboarding

- Component map: SED proxy consists of entropy calculation rules (skip SED, conv SED, pool SED) combined into a final score, requiring only architecture topology information without data or network execution.
- Critical path: Parse architecture topology → Calculate skip SED, conv SED, pool SED for each block → Combine into final SED score → Rank architectures by SED value.
- Design tradeoffs: Topology-only computation makes SED extremely fast but may miss data-dependent performance factors; the entropy-based approach is theoretically grounded but complex to implement correctly.
- Failure signatures: Poor correlation with actual performance on new datasets, architectures with similar SED values showing large performance differences, or the proxy ranking architectures opposite to their actual performance.
- First 3 experiments:
  1. Implement SED calculation for a simple 2-block architecture and verify it produces reasonable values for different kernel size configurations.
  2. Compare SED rankings with actual performance on a small architecture space (e.g., 100 architectures) to verify the correlation mechanism.
  3. Benchmark computation time of SED vs. a simple zero-cost proxy (like gradient norm) on the same architecture to verify the claimed speedup.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the local entropy decrease vary across different neural network architectures beyond the ones tested in this study?
- Basis in paper: [explicit] The paper demonstrates that specific network architectures reduce local entropy, which degrades network performance.
- Why unresolved: The study primarily focuses on a limited set of architectures and benchmarks, leaving open the question of how different architectural choices might impact local entropy in other contexts.
- What evidence would resolve it: Conducting experiments with a wider variety of neural network architectures and datasets to observe the relationship between local entropy and network performance.

### Open Question 2
- Question: What is the theoretical limit of the SED proxy's effectiveness in selecting optimal architectures for tasks beyond classification?
- Basis in paper: [explicit] The SED proxy is validated for classification tasks, but its effectiveness for other types of tasks is not explored.
- Why unresolved: The paper does not provide theoretical or empirical evidence for the SED proxy's applicability to tasks such as regression or reinforcement learning.
- What evidence would resolve it: Extending the SED proxy to non-classification tasks and evaluating its performance to determine if it remains effective.

### Open Question 3
- Question: How does the computational efficiency of the SED proxy scale with increasing network depth and complexity?
- Basis in paper: [explicit] The SED proxy is shown to be significantly faster than existing proxies, but its performance on very deep or complex networks is not discussed.
- Why unresolved: The study does not address the computational scalability of the SED proxy for networks with more layers or higher complexity.
- What evidence would resolve it: Analyzing the computational time and resource usage of the SED proxy as network depth and complexity increase to identify any potential bottlenecks or limitations.

## Limitations
- Theoretical dependence on ReLU activation and classification tasks may limit generalizability to other domains
- Assumes simplified Gaussian feature map distributions that may not accurately represent real-world feature maps
- Focus on topology-only computation may miss important data-dependent performance factors

## Confidence
- Core mechanism effectiveness: Medium-High (supported by theoretical propositions and experimental results)
- Universality across architecture spaces: Medium (evaluation focuses primarily on image classification benchmarks)
- Three orders of magnitude speedup claim: High confidence (topology-only computation is inherently fast)
- Cross-domain generalization: Medium (limited testing beyond classification benchmarks)

## Next Checks
1. Test SED's ranking correlation on non-classification tasks (e.g., object detection or semantic segmentation) to verify assumption independence from classification-specific requirements.

2. Evaluate SED's performance on architectures with non-ReLU activations (e.g., GELU, Swish) to assess activation function dependence.

3. Conduct ablation studies removing theoretical assumptions (e.g., Gaussian feature maps) to determine which components contribute most to performance and which are critical assumptions.