---
ver: rpa2
title: 'AtGCN: A Graph Convolutional Network For Ataxic Gait Detection'
arxiv_id: '2410.22862'
source_url: https://arxiv.org/abs/2410.22862
tags:
- gait
- atgcn
- graph
- proposed
- ataxic
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of detecting ataxic gait and estimating
  its severity using 2D videos, a task made challenging by the subtle nature of gait
  deviations and the small size of available datasets. To tackle this, the authors
  propose AtGCN, a Graph Convolutional Network that leverages spatiotemporal graph
  convolutions to capture gait-related features from body part coordinates extracted
  via pose estimation.
---

# AtGCN: A Graph Convolutional Network For Ataxic Gait Detection

## Quick Facts
- arXiv ID: 2410.22862
- Source URL: https://arxiv.org/abs/2410.22862
- Authors: Karan Bania; Tanmay Verlekar
- Reference count: 29
- One-line primary result: AtGCN achieves 93.46% accuracy for ataxic gait detection and 0.4169 MAE for severity prediction

## Executive Summary
This paper addresses the challenge of detecting ataxic gait and estimating its severity using 2D videos, a task complicated by subtle gait deviations and limited dataset sizes. The authors propose AtGCN, a Graph Convolutional Network that leverages spatiotemporal graph convolutions to capture gait-related features from body part coordinates extracted via pose estimation. The model is pre-trained on an action recognition dataset and fine-tuned on the ataxia dataset through truncation and augmentation strategies that segment videos into gait cycles. The proposed AtGCN outperforms the state-of-the-art in detecting ataxic gait with an accuracy of 93.46% and in severity prediction with a mean absolute error (MAE) of 0.4169. These results highlight the effectiveness of spatiotemporal graph convolutions and the model's ability to handle small, imbalanced datasets.

## Method Summary
The method involves preprocessing 2D videos using YOLO for person detection, DeepSORT for tracking, and OpenPose for 2D pose estimation. Videos are segmented into gait cycles based on ankle distance analysis with smoothing filters. A spatiotemporal graph is constructed where nodes represent body part coordinates and edges encode anatomical and temporal connections. The AtGCN model is obtained by truncating a pre-trained ST-GCN model (from 10 to 6 blocks) and fine-tuning it on the Auto-Gait dataset using SGD with learning rate 3×10^-5, batch size 64, and 500 epochs. The model outputs both classification (ataxic/healthy) and regression (severity score) predictions.

## Key Results
- AtGCN achieves 93.46% accuracy for ataxic gait detection, outperforming state-of-the-art methods
- The model achieves a mean absolute error of 0.4169 for severity prediction on the Auto-Gait dataset
- Performance is validated through 10-fold cross-validation with standard metrics including F1 score, ROC AUC, and Pearson's correlation coefficient

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Spatiotemporal graph convolutions can capture subtle gait deviations by modeling body part relationships both spatially and temporally.
- Mechanism: The model constructs a graph for each gait cycle where nodes represent body part coordinates and edges encode anatomical connections and temporal continuity. This allows the network to learn complex patterns in how body parts move relative to each other over time.
- Core assumption: The spatial and temporal relationships between body parts contain sufficient information to distinguish ataxic from healthy gait.
- Evidence anchors:
  - [abstract]: "special spatiotemporal graph convolution that successfully captures important gait-related features"
  - [section]: "The proposed AtGCN model then operates on a graph of body part locations belonging to a single gait cycle"
- Break condition: If the subtle differences between ataxic and healthy gait are not captured in the body part relationships, the model will fail to distinguish them effectively.

### Mechanism 2
- Claim: Truncation and fine-tuning of a pre-trained action recognition model enables effective learning on small, specialized datasets.
- Mechanism: The authors systematically truncate a deep spatiotemporal graph convolution network pre-trained on an action recognition dataset and fine-tune it on the ataxia dataset. This transfers general motion understanding while adapting to the specific characteristics of ataxic gait.
- Core assumption: Pre-training on action recognition provides useful general features that can be adapted to ataxic gait detection.
- Evidence anchors:
  - [abstract]: "To handle the small dataset size, a deep spatiotemporal graph convolution network pre-trained on an action recognition dataset is systematically truncated and then fine-tuned"
  - [section]: "The proposed AtGCN is obtained by truncating and fine-tuning a deep pre-trained spatiotemporal graph convolution model"
- Break condition: If the action recognition dataset is too different from ataxic gait patterns, the transferred features may not be useful for fine-tuning.

### Mechanism 3
- Claim: Augmenting the dataset by splitting videos into multiple gait cycles increases training data and improves model performance.
- Mechanism: The system segments each video into distinct gait cycles and uses each cycle as a separate training sample. This effectively multiplies the dataset size and provides more examples for the model to learn from.
- Core assumption: Individual gait cycles contain sufficient information to classify gait as ataxic or healthy.
- Evidence anchors:
  - [abstract]: "The paper also presents an augmentation strategy that segments a video sequence into multiple gait cycles"
  - [section]: "It increases the dataset size by up to about threefold, allowing the training of the AtGCN model"
- Break condition: If ataxic gait patterns require longer temporal context than a single gait cycle provides, this augmentation strategy may lose important information.

## Foundational Learning

- Concept: Graph Convolutional Networks
  - Why needed here: GCNs can model the complex relationships between body parts in a non-Euclidean space, which is essential for capturing the spatial dependencies in human movement.
  - Quick check question: How do GCNs differ from traditional convolutional neural networks in handling data structure?

- Concept: Spatiotemporal modeling
  - Why needed here: Ataxic gait involves both spatial relationships (how body parts are positioned relative to each other) and temporal patterns (how these relationships change over time), requiring a model that can capture both dimensions.
  - Quick check question: What challenges arise when extending graph convolutions to the temporal domain?

- Concept: Transfer learning and fine-tuning
  - Why needed here: The small size of the ataxia dataset makes direct training difficult, so leveraging a pre-trained model and adapting it through fine-tuning is necessary to achieve good performance.
  - Quick check question: What are the risks and benefits of truncating a pre-trained model before fine-tuning?

## Architecture Onboarding

- Component map: Video input -> Person detection and tracking -> Pose estimation -> Skeleton sequence generation -> Gait cycle segmentation -> Multiple input samples per video -> Graph construction -> Spatiotemporal graph representation -> AtGCN processing -> Classification and severity prediction

- Critical path:
  1. Video input → Person detection and tracking
  2. Pose estimation → Skeleton sequence generation
  3. Gait cycle segmentation → Multiple input samples per video
  4. Graph construction → Spatiotemporal graph representation
  5. AtGCN processing → Classification and severity prediction

- Design tradeoffs:
  - Using 2D videos instead of depth sensors for privacy but with less information
  - Truncating pre-trained model to balance between general features and task-specific adaptation
  - Using gait cycles as units rather than full videos to increase dataset size but potentially lose long-term context

- Failure signatures:
  - High loss during training despite good data augmentation
  - Poor performance on videos with unusual camera angles or occlusions
  - Overfitting on the small dataset despite dropout and augmentation

- First 3 experiments:
  1. Test the complete pipeline on a single video to verify all components work together
  2. Evaluate model performance with different numbers of spatiotemporal graph convolution blocks (1-10) to find optimal truncation point
  3. Compare performance with and without gait cycle segmentation augmentation to validate its effectiveness

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the AtGCN model generalize to other gait-related pathologies beyond ataxia, and what are the performance implications?
- Basis in paper: [inferred] The paper mentions future work will consider merging datasets belonging to different gait-related pathologies to increase training set size, implying this is an open question.
- Why unresolved: The current evaluation is limited to ataxia, and the model's performance on other gait disorders has not been tested.
- What evidence would resolve it: Training and evaluating the AtGCN model on datasets of other gait disorders (e.g., Parkinson's disease, cerebral palsy) and comparing performance metrics to those achieved on ataxia.

### Open Question 2
- Question: How does the proposed gait cycle segmentation and augmentation strategy affect the model's ability to capture long-term gait patterns and dependencies?
- Basis in paper: [explicit] The paper describes an augmentation strategy that segments video sequences into multiple gait cycles and uses each cycle as input to the model.
- Why unresolved: While the strategy improves dataset size, it is unclear whether it might lead to loss of information about longer-term gait patterns or dependencies between cycles.
- What evidence would resolve it: Comparative studies evaluating the model's performance on full video sequences versus segmented gait cycles, and analysis of the impact on capturing temporal dependencies.

### Open Question 3
- Question: What are the limitations of using 2D video data for gait analysis, and how can depth information or 3D pose estimation improve the model's performance?
- Basis in paper: [explicit] The paper acknowledges that the lack of depth information in 2D videos can make certain features (e.g., distance between feet) unreliable, and suggests that depth-sensing cameras could capture skeletal sequences.
- Why unresolved: The current model relies solely on 2D data, and the potential benefits of incorporating depth or 3D information have not been explored.
- What evidence would resolve it: Training and evaluating the AtGCN model on datasets with depth or 3D pose information, and comparing performance metrics to those achieved on 2D data.

## Limitations
- Small dataset size (149 videos, 89 participants) limits generalizability and raises concerns about overfitting
- Lack of external validation on independent datasets makes it unclear how well the model generalizes to different populations and clinical settings
- The truncation strategy was determined through limited experimentation (only 1-10 block variations), leaving uncertainty about whether the optimal configuration was found

## Confidence
- Detection accuracy (93.46%): High confidence - well-validated through 10-fold cross-validation with standard metrics
- Severity prediction (MAE 0.4169): Medium confidence - while MAE is reported, the clinical significance of this error magnitude is not established
- Transfer learning effectiveness: Medium confidence - the mechanism is sound but relies on assumptions about feature transferability that aren't empirically tested
- Gait cycle segmentation augmentation: Medium confidence - the approach is reasonable but the optimal segmentation parameters are not explored

## Next Checks
1. Test the model on an independent ataxia dataset from a different source to verify generalizability beyond the Auto-Gait dataset
2. Conduct ablation studies to determine the impact of each component: compare with and without pre-training, with different truncation points (beyond just 1-10 blocks), and with different augmentation strategies
3. Perform a clinical validation study with neurologists to assess whether the model's severity predictions align with clinical assessments and to determine the practical utility of the MAE 0.4169 error rate in real-world settings