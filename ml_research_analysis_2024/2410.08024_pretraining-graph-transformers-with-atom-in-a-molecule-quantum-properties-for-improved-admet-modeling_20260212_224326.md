---
ver: rpa2
title: Pretraining Graph Transformers with Atom-in-a-Molecule Quantum Properties for
  Improved ADMET Modeling
arxiv_id: '2410.08024'
source_url: https://arxiv.org/abs/2410.08024
tags:
- pretraining
- properties
- pretrained
- atomic
- molecular
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This study evaluates pretraining strategies for Graph Transformer
  architectures to improve ADMET property prediction in drug discovery. Three pretraining
  approaches were compared: atom-level quantum mechanical properties (charges, NMR
  shifts, Fukui indices), molecular-level HOMO-LUMO gap, and self-supervised atom
  masking.'
---

# Pretraining Graph Transformers with Atom-in-a-Molecule Quantum Properties for Improved ADMET Modeling

## Quick Facts
- arXiv ID: 2410.08024
- Source URL: https://arxiv.org/abs/2410.08024
- Reference count: 37
- Models pretrained on atomic quantum mechanical properties outperform other strategies on both public benchmarks and large internal datasets for ADMET property prediction.

## Executive Summary
This study evaluates pretraining strategies for Graph Transformer architectures to improve ADMET property prediction in drug discovery. Three pretraining approaches were compared: atom-level quantum mechanical properties (charges, NMR shifts, Fukui indices), molecular-level HOMO-LUMO gap, and self-supervised atom masking. Models were fine-tuned on 22 ADMET tasks from the Therapeutics Data Commons and tested on a large internal microsomal clearance dataset. Results show that models pretrained on atom-level quantum mechanical properties outperform other strategies on both public benchmarks and the larger internal dataset. Specifically, multitask pretraining on all four atomic properties yielded the best results, followed by models pretrained on NMR shifts and atomic charges. Analysis of latent representations revealed that quantum property pretraining preserves pretraining information after fine-tuning, produces higher latent expressivity across layers, and captures more low-frequency Laplacian eigenmodes of molecular graphs.

## Method Summary
The paper investigates three pretraining strategies for Graphormer architectures: (1) atom-level quantum mechanical properties including atomic charges, NMR shielding constants, and Fukui indices from the QMugs dataset (~136k molecules), (2) molecular HOMO-LUMO gap from the PCQM4Mv2 dataset (~2M molecules), and (3) self-supervised atom masking with 15% random node masking. Pretrained models are fine-tuned on 22 ADMET tasks from the Therapeutics Data Commons and an internal microsomal clearance dataset of 130k compounds. The Graphormer architecture uses 20 self-attention layers with 256 embedding dimensions and 32 attention heads. Models are trained using L1 loss for regression tasks and evaluated using MAE for regression and ROC-AUC for classification on public benchmarks, while internal datasets use R² and Spearman correlation.

## Key Results
- Models pretrained on atomic quantum mechanical properties (charges, NMR shifts, Fukui indices) outperform other pretraining strategies on both public benchmarks and internal datasets
- Multitask pretraining on all four atomic properties yields the best performance, followed by NMR shift pretraining and atomic charge pretraining
- Pretraining information is preserved after fine-tuning, with NMR shift models showing the highest correlation (R²) with original pretraining labels

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Atom-level quantum mechanical pretraining preserves pretraining information after fine-tuning, leading to better downstream performance.
- Mechanism: The Graphormer learns atomic embeddings that encode both the quantum properties and molecular context during pretraining. During fine-tuning, these embeddings retain linear correlation with the original quantum labels, allowing the model to transfer learned atomic-level representations to molecular property prediction tasks.
- Core assumption: Atomic quantum properties contain transferable information relevant to ADMET properties.
- Evidence anchors: [abstract] "models pretrained with atomic quantum mechanical properties produce in general better results" and "supervised strategies preserve the pretraining information after finetuning"; [section] "models pretrained on NMR shifts seems to preserve its pretraining information the most" and analysis showing R2 coefficients for linear regression of pretraining labels from fine-tuned model representations.
- Break condition: If atomic quantum properties are not predictive of the downstream ADMET tasks, or if the pretraining dataset is too small to capture relevant chemical space.

### Mechanism 2
- Claim: Pretraining on atomic quantum properties captures more low-frequency Laplacian eigenmodes of molecular graphs through attention weights.
- Mechanism: The attention rollout matrix analysis shows that models pretrained on atomic quantum properties have higher spectral perception (ζ metric) compared to other pretraining strategies. This means the model's attention mechanism is more sensitive to global molecular structure patterns encoded in low-frequency graph Laplacian eigenmodes.
- Core assumption: Low-frequency Laplacian eigenmodes capture important global molecular structural information relevant to ADMET properties.
- Evidence anchors: [abstract] "models pretrained on atomic quantum mechanical properties capture more low-frequency laplacian eigenmodes of the input graph via the attention weights"; [section] "spectral analysis of the attention rollout matrix documented a non-trivial effect arising in pretrained GTs where the model shows hints of filtered spectral graph convolution".
- Break condition: If the spectral analysis method is not a valid proxy for the actual model behavior, or if low-frequency eigenmodes are not actually important for the ADMET tasks.

### Mechanism 3
- Claim: Atomic quantum property pretraining produces higher latent expressivity across layers compared to other strategies.
- Mechanism: The analysis of ρL (expressivity metric) shows that models pretrained on atomic quantum properties achieve higher maximum latent expressivity values across layers. This indicates the model maintains better representational capacity and can capture more complex relationships between atomic environments and molecular properties.
- Core assumption: Higher latent expressivity correlates with better molecular property prediction performance.
- Evidence anchors: [section] "models pretrained on atomic quantum properties achieve the highest maximum latent expressivity" and "while models pretrained on HLG are characterized by a constant level of expressivity across layers, models pretrained using masking have more similar atomic latent representations in the first few layers and more dissimilar in the last ones"; [abstract] "different pretrainings produce different trends in latent expressivity across layers".
- Break condition: If higher latent expressivity does not actually translate to better performance on the downstream tasks, or if the expressivity metric is not a valid measure of model capability.

## Foundational Learning

- Concept: Graph Neural Networks and Graph Transformers
  - Why needed here: The paper uses Graphormer, a Graph Transformer architecture, as the base model. Understanding how GNNs and GTs process molecular graphs is essential to grasp the pretraining strategies and analysis methods.
  - Quick check question: What is the key difference between message-passing GNNs and attention-based Graph Transformers in terms of information propagation?

- Concept: Quantum Mechanical Properties of Atoms in Molecules
  - Why needed here: The pretraining strategy uses atomic quantum properties (charges, NMR shifts, Fukui indices) which require understanding their chemical significance and how they relate to molecular properties.
  - Quick check question: How do Fukui indices relate to reactivity and why would they be useful for predicting metabolism-related properties?

- Concept: Spectral Graph Theory and Graph Laplacians
  - Why needed here: The paper performs spectral analysis of attention rollout matrices and relates them to graph Laplacian eigenmodes, requiring understanding of graph signal processing concepts.
  - Quick check question: What do low-frequency eigenvectors of the graph Laplacian typically represent in terms of molecular structure?

## Architecture Onboarding

- Component map: Input molecular graph (atoms as nodes, bonds as edges) -> Graphormer encoder (20 layers, 256 dim, 32 heads) -> Pretraining heads (multiple regression heads for different atomic properties or single head for molecular property) -> Fine-tuning task-specific heads (regression/classification) -> Analysis tools (attention rollout computation, spectral analysis, expressivity metrics, neighbor sensitivity analysis)

- Critical path: 1. Load pretraining dataset with atomic quantum properties; 2. Pretrain Graphormer on atomic properties (regression task); 3. Load downstream ADMET dataset; 4. Fine-tune pretrained model on specific task; 5. Perform representation analysis on fine-tuned models; 6. Compare performance across different pretraining strategies

- Design tradeoffs: Number of layers (20 used) vs. training time and overfitting risk; Pretraining dataset size vs. computational cost (atomic properties dataset much smaller than HLG dataset); Multitask pretraining (all atomic properties) vs. individual property pretraining; Choice of loss function (L1 vs L2) for regression tasks

- Failure signatures: Pretraining information not preserved after fine-tuning (low R2 in linear regression analysis); Low spectral perception (ζ close to 0) indicating lack of graph structure awareness; Expressivity collapse across layers (ρL approaching 1); Poor performance on both public benchmarks and internal datasets

- First 3 experiments: 1. Train a baseline Graphormer from scratch on a single ADMET task and establish performance metrics; 2. Pretrain on atomic charges only, then fine-tune on the same ADMET task and compare performance; 3. Analyze the attention rollout matrix spectrum for both models to check for differences in graph spectral perception

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of models pretrained on atomic QM properties scale with the size of the pretraining dataset?
- Basis in paper: [inferred] The paper mentions that atomic QM properties datasets contain ~20x fewer molecular structures than the HLG pretraining dataset, yet achieve comparable performance.
- Why unresolved: The paper does not systematically explore how varying the size of the atomic QM properties pretraining dataset affects model performance.
- What evidence would resolve it: Experiments varying the size of the atomic QM properties pretraining dataset while keeping the model architecture and downstream tasks constant would clarify this relationship.

### Open Question 2
- Question: What is the impact of different graph transformer architectures (e.g., number of layers, attention heads) on the effectiveness of atomic QM property pretraining?
- Basis in paper: [inferred] The paper uses a specific Graphormer implementation with 20 layers and 32 attention heads.
- Why unresolved: The paper does not explore the impact of different graph transformer architectures on the effectiveness of atomic QM property pretraining.
- What evidence would resolve it: Experiments comparing the performance of different graph transformer architectures (e.g., varying the number of layers, attention heads) when pretrained on atomic QM properties would provide insights into the optimal architecture for this pretraining strategy.

### Open Question 3
- Question: How do the learned representations from atomic QM property pretraining generalize to other molecular property prediction tasks beyond ADMET?
- Basis in paper: [explicit] The paper demonstrates that models pretrained on atomic QM properties outperform other strategies on ADMET tasks and a large internal microsomal clearance dataset.
- Why unresolved: The paper only evaluates the performance of atomic QM property pretraining on ADMET tasks and microsomal clearance.
- What evidence would resolve it: Experiments fine-tuning models pretrained on atomic QM properties on a diverse set of molecular property prediction tasks beyond ADMET (e.g., protein-ligand binding affinity, quantum mechanical properties) would demonstrate the generalizability of these representations.

## Limitations

- Public benchmark datasets may not fully represent real-world drug discovery challenges
- The relationship between representation learning properties (expressivity, spectral perception) and downstream performance remains largely correlational
- The claim that atomic quantum properties are inherently more transferable than molecular-level pretraining is not directly tested

## Confidence

**High Confidence:**
- Pretraining on atomic quantum mechanical properties improves ADMET prediction performance compared to training from scratch
- Multitask pretraining on all four atomic properties yields better results than single-property pretraining
- Pretraining information is preserved after fine-tuning (R² analysis)

**Medium Confidence:**
- Models pretrained on NMR shifts preserve pretraining information the most effectively
- Atomic quantum property pretraining captures more low-frequency Laplacian eigenmodes
- Higher latent expressivity correlates with better downstream performance

**Low Confidence:**
- Public benchmarks are insufficient for evaluating pretraining strategies in real-world drug discovery
- Atomic quantum properties are inherently more transferable than molecular-level pretraining
- The specific ranking of pretraining strategies (multitask QM > NMR > charges > HLG > masking) generalizes beyond tested tasks

## Next Checks

1. **Causality validation**: Systematically ablate representation learning properties (expressivity, spectral perception) through architectural modifications to determine if these properties are causal drivers of performance or merely correlates. For example, design models with constrained attention mechanisms that preserve expressivity but reduce spectral perception, or vice versa.

2. **Generalization test**: Evaluate pretraining strategies on a diverse set of molecular property prediction tasks beyond ADMET, including quantum mechanical properties, synthetic accessibility, and toxicity endpoints, to determine if atomic quantum pretraining shows consistent advantages across chemical domains.

3. **Alternative pretraining comparison**: Implement and evaluate geometric pretraining approaches (3D-aware models, equivariant networks) and self-supervised graph-level pretraining strategies to directly compare against atomic quantum property pretraining, controlling for pretraining dataset size and computational resources.