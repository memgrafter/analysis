---
ver: rpa2
title: 'Tracing Representation Progression: Analyzing and Enhancing Layer-Wise Similarity'
arxiv_id: '2406.14479'
source_url: https://arxiv.org/abs/2406.14479
tags:
- layers
- similarity
- training
- layer
- aligned
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper studies layer-wise representation similarity in transformers
  using a sample-wise cosine similarity metric. The authors show that this simple
  metric aligns with more complex CKA and reveals that representations become more
  similar as layers get closer.
---

# Tracing Representation Progression: Analyzing and Enhancing Layer-Wise Similarity

## Quick Facts
- arXiv ID: 2406.14479
- Source URL: https://arxiv.org/abs/2406.14479
- Authors: Jiachen Jiang; Jinxin Zhou; Zhihui Zhu
- Reference count: 40
- Key outcome: Simple sample-wise cosine similarity metric captures layer-wise representation similarity in transformers, aligns with CKA, and when enhanced through aligned training leads to better multi-exit performance with a single classifier

## Executive Summary
This paper investigates layer-wise representation similarity in transformer models using a sample-wise cosine similarity metric. The authors demonstrate that this simple metric effectively captures representation similarity and aligns with the more complex CKA metric. They provide theoretical justification showing that increased representation similarity leads to higher prediction probabilities and explain saturation events. The paper introduces an aligned training method that enhances layer-wise similarity by applying a shared classifier across layers, resulting in improved multi-exit performance and better identification of minimal effective depth.

## Method Summary
The method involves training transformers with a modified loss function that applies the last-layer classifier to all intermediate layer representations during training. The loss is a weighted average of cross-entropy losses across all layers, with weights increasing linearly from shallow to deep layers. This encourages features to progressively align with the classifier throughout the network. The approach uses a single classifier for all layers, improving parameter efficiency. During inference, multi-exit allows early prediction based on confidence thresholds, enabling computational savings.

## Key Results
- Sample-wise cosine similarity effectively captures layer-wise representation similarity and aligns with CKA
- Aligned training enhances layer-wise similarity, leading to more early saturation events and improved multi-exit performance
- Single classifier approach achieves up to 1.36× speedup in inference while maintaining accuracy on ImageNet
- Method maintains transferability to downstream tasks and shows strong performance gains across vision and NLP tasks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Sample-wise cosine similarity captures layer-wise representation similarity and aligns with CKA
- Mechanism: For transformer models with residual connections, features across layers have the same dimension and minimal rotation difference, making simple cosine similarity sufficient to measure representational similarity
- Core assumption: Transformer residual connections create smooth transitions between layers that prevent significant feature rotation
- Evidence anchors:
  - [abstract] "we show that a simple sample-wise cosine similarity metric is capable of capturing the similarity and aligns with the complicated CKA"
  - [section] "our experimental results on common transformers reveal that representations across layers are positively correlated, with similarity increasing when layers get closer"
  - [corpus] Weak evidence - only 1/8 related papers mention cosine similarity metrics
- Break condition: If residual connections are removed or if transformer blocks have different dimensions across layers

### Mechanism 2
- Claim: Increased representation similarity implies increased predicted probability and explains saturation events
- Mechanism: When features progressively align with the last-layer classifier through residual updates following a geodesic curve, the predicted probability for the correct class increases monotonically across layers
- Core assumption: The transformer with weight decay learns the geodesic curve in Wasserstein space, and neural collapse holds at the terminal phase of training
- Evidence anchors:
  - [abstract] "We then show that an increase in representation similarity implies an increase in predicted probability when directly applying the last-layer classifier to any hidden layer representation"
  - [section] "This offers a justification for saturation events, stating that if a sample is correctly predicted at the ℓ-th layer, it will continue to be correctly predicted in subsequent layers"
  - [corpus] No direct evidence found in related papers
- Break condition: If weight decay is removed or if the geodesic curve assumption doesn't hold in practice

### Mechanism 3
- Claim: Aligned training enhances shallow layer effectiveness by improving layer-wise feature similarity
- Mechanism: By applying the last-layer classifier to all intermediate layers during training and minimizing the weighted average cross-entropy loss, features are encouraged to align with the classifier from shallow to deep layers, improving representation similarity and early saturation events
- Core assumption: The self-duality between class-mean features and linear classifiers (neural collapse phenomenon) can be leveraged to align features across layers
- Evidence anchors:
  - [abstract] "We propose an aligned training method to improve the effectiveness of shallow layer by enhancing the similarity between internal representations"
  - [section] "our aligned training approach deploys the common classifier to each layer and then minimizes the average of the cross-entropy losses from all the layers"
  - [corpus] No evidence in related papers about this specific training approach
- Break condition: If the weight decay is too high or if the model architecture prevents effective feature alignment

## Foundational Learning

- Concept: Transformer architecture with residual connections
  - Why needed here: The residual connections are critical for preventing feature rotation and enabling the simple cosine similarity metric to work effectively
  - Quick check question: What would happen to layer-wise similarity measurements if we removed residual connections from the transformer blocks?

- Concept: Neural collapse phenomenon
  - Why needed here: The aligned training method leverages neural collapse's self-duality property to align features across layers with the classifier
  - Quick check question: How does neural collapse relate to the alignment between class-mean features and linear classifiers?

- Concept: Geodesic curve in Wasserstein space
  - Why needed here: This theoretical foundation explains why representation similarity increases as layers get closer in transformers with weight decay
  - Quick check question: What mathematical property of the residual update allows us to model it as a discretization of a geodesic curve?

## Architecture Onboarding

- Component map:
  Transformer blocks with residual connections -> Common classifier applied to all layers -> Aligned training loss function -> Multi-exit inference mechanism -> Cosine similarity measurement module

- Critical path:
  1. Forward pass through transformer layers
  2. Extract features from each layer
  3. Apply common classifier to all layer features
  4. Compute weighted cross-entropy loss across layers
  5. Backward pass to update parameters
  6. Inference with early exit based on confidence threshold

- Design tradeoffs:
  - Simple cosine similarity vs. complex CKA: Tradeoff between computational efficiency and theoretical robustness
  - Single classifier vs. multiple classifiers: Parameter efficiency vs. potential performance gains
  - Linear increasing weights vs. uniform weights: Better final layer performance vs. balanced shallow layer improvement

- Failure signatures:
  - Low cosine similarity between adjacent layers: May indicate broken residual connections or excessive weight decay
  - Non-monotonic accuracy progression: Could suggest geodesic curve assumption doesn't hold
  - Degraded transfer learning: May indicate shallow layers lost universal pattern learning

- First 3 experiments:
  1. Compare cosine similarity and CKA on a small transformer with and without residual connections
  2. Measure accuracy progression and saturation events on a standard vs. aligned trained model
  3. Evaluate multi-exit performance with single classifier vs. multiple classifiers on ImageNet

## Open Questions the Paper Calls Out

The paper doesn't explicitly call out open questions, but based on the limitations section and the discussion, several important questions remain:

- How does the aligned training method affect the interpretability of transformer models, particularly in terms of feature attribution and understanding decision boundaries?
- Can the aligned training approach be extended to other architectures beyond transformers, such as convolutional neural networks or recurrent neural networks, and what modifications would be necessary?
- How does the alignment of features across layers influence the robustness of transformer models to adversarial attacks or distribution shifts?
- What is the computational overhead of aligned training during the training phase, and how does it compare to the efficiency gains during inference?
- How sensitive is the aligned training method to the choice of layer weights (λℓ) and what is the optimal strategy for selecting these weights?

## Limitations

- The geodesic curve assumption for transformer residual updates may not hold universally across all architectures and training configurations
- The method's effectiveness could diminish for transformers without strong residual connections or in architectures with significant feature rotation between layers
- The theoretical justification relies on specific conditions including weight decay and neural collapse, which may not be present in all practical scenarios

## Confidence

- **High confidence**: The empirical observation that cosine similarity aligns with CKA and that layer-wise similarity increases as layers get closer
- **Medium confidence**: The theoretical mechanism linking representation similarity to prediction probability increases and saturation events
- **Medium confidence**: The aligned training method's effectiveness in enhancing shallow layer performance

## Next Checks

1. Test the aligned training method on transformer architectures without residual connections to verify the mechanism's dependence on this architectural feature
2. Evaluate the impact of removing weight decay from training to test the geodesic curve assumption's importance
3. Compare the aligned training method's effectiveness across different learning rate schedules and weight decay values to identify optimal hyperparameters and potential failure conditions