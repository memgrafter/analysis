---
ver: rpa2
title: 'Onco-Retriever: Generative Classifier for Retrieval of EHR Records in Oncology'
arxiv_id: '2404.06680'
source_url: https://arxiv.org/abs/2404.06680
tags:
- onco-retriever
- data
- chunks
- patient
- information
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents Onco-Retriever, a generative classifier designed
  for retrieving Electronic Health Record (EHR) records in oncology. The key challenge
  addressed is the difficulty in creating query-document support pairs for EHR data.
---

# Onco-Retriever: Generative Classifier for Retrieval of EHR Records in Oncology

## Quick Facts
- arXiv ID: 2404.06680
- Source URL: https://arxiv.org/abs/2404.06680
- Reference count: 26
- This paper presents Onco-Retriever, a generative classifier designed for retrieving Electronic Health Record (EHR) records in oncology, outperforming proprietary counterparts like Ada and Mistral by 30-50 F1 points.

## Executive Summary
This paper addresses the challenge of retrieving relevant EHR records in oncology by introducing Onco-Retriever, a specialized generative classifier. The key innovation is using GPT-3 to generate synthetic query-document support pairs, enabling effective training of smaller retrievers without expensive manual labeling. Onco-Retriever achieves significant performance gains over both embedding-based models and larger generative models while maintaining a compact size under 500 million parameters. The approach demonstrates the potential for healthcare organizations to build domain-specific retrieval systems tailored to their specific data needs.

## Method Summary
The Onco-Retriever framework addresses the scarcity of query-document pairs in EHR data by using GPT-3.5 to generate synthetic labeled datasets. The process involves: (1) chunking patient EHR documents using rule-based semantic chunking, (2) generating 30 concept-specific queries using GPT-4, (3) creating a silver dataset by having GPT-3.5 label chunks with Chain-of-Thought reasoning for 13 oncology concepts, and (4) distilling this labeled data into smaller retrievers (Small, Optimized, and Large variants). The models are evaluated against baselines including OpenAI's Ada, Mistral, and fine-tuned PubMedBERT using precision, recall, and F1 score metrics on a held-out test set.

## Key Results
- Onco-Retriever (Small) achieved precision of 0.62 and recall of 0.73, surpassing Ada (precision: 0.31, recall: 0.54) and Mistral (precision: 0.23, recall: 0.37)
- The 500M parameter model outperformed larger generative models while being considerably smaller
- The Chain-of-Thought approach in GPT-3.5 labeling contributed to the 30-50 F1 point improvement over proprietary counterparts

## Why This Works (Mechanism)

### Mechanism 1
Synthetic data generation using GPT-3 alleviates the scarcity of query-document pairs for EHR retrieval. GPT-3 generates realistic EHR-style chunks and their relevance labels for each predefined oncology concept, producing a silver-annotated dataset that can train retrievers without costly manual labeling. Core assumption: GPT-3 outputs are sufficiently accurate for distillation into smaller models that outperform the original teacher. Evidence anchors: The paper reports 30-50 F1 point improvements over proprietary models and leverages CoT approach for better reasoning. Break condition: If GPT-3 outputs include many hallucinated facts, distillation may learn incorrect patterns and degrade performance.

### Mechanism 2
Fine-tuning a small encoder (500M parameters) on concept-specific silver data yields better precision and recall than embedding-based retrievers for oncology concepts. By distilling from GPT-3.5's labeled silver dataset, the compact Onco-Retriever learns oncology-specific relevance patterns, outperforming general-purpose embedding models like Ada and Mistral. Core assumption: The concept-specific training data is rich enough for the small model to generalize well within oncology. Evidence anchors: Onco-Retriever (Small) achieved precision of 0.62 and recall of 0.73, surpassing Ada (precision: 0.31, recall: 0.54) and Mistral (precision: 0.23, recall: 0.37). Break condition: If the silver dataset is too noisy, the small model will overfit and not generalize.

### Mechanism 3
Chain-of-Thought reasoning in GPT-3.5 labeling improves the quality of silver annotations, which in turn boosts distillation performance. GPT-3.5's CoT prompt structure forces stepwise reasoning when labeling chunks, producing more consistent and accurate relevance judgments for training the retriever. Core assumption: CoT prompts reduce labeling noise compared to direct classification outputs. Evidence anchors: The paper leverages a Chain of Thought (CoT) approach to derive labels, enhancing model performance through better reasoning processes. Break condition: If CoT reasoning introduces extra noise or hallucinations, performance will degrade.

## Foundational Learning

- Concept: EHR data structure and chunking
  - Why needed here: Retriever must process realistic EHR notes split into manageable chunks; understanding chunking rules is essential for reproducing or modifying the pipeline.
  - Quick check question: What rule-based criteria are used to chunk a note so that semantic units stay together?

- Concept: Dense passage retrieval vs embedding similarity
  - Why needed here: Baseline methods use embedding similarity; understanding the difference clarifies why dense retrieval with fine-tuning can outperform embeddings in oncology.
  - Quick check question: How does a dense retriever score similarity differently from cosine similarity in embedding space?

- Concept: Model distillation and teacher-student training
  - Why needed here: The Onco-Retriever is distilled from GPT-3.5's silver labels; knowing distillation mechanics explains the parameter reduction and performance gains.
  - Quick check question: What is the objective function when distilling from soft labels produced by a teacher model?

## Architecture Onboarding

- Component map: EHR ingestion -> rule-based chunking -> query expansion -> GPT-4 generation of 30 concept-specific queries -> silver dataset creation -> GPT-3.5 labeling with CoT -> retriever training -> evaluation
- Critical path: Chunking -> silver labeling -> distillation -> evaluation. Any delay in silver labeling slows the entire pipeline.
- Design tradeoffs: Small vs Optimized vs Large models: Small offers highest precision but higher latency; Optimized balances speed and performance; Large maximizes recall at highest compute cost. Silver vs gold annotations: Silver is fast and cheap but noisier; gold is expensive but more reliable for final evaluation.
- Failure signatures: Low precision spikes -> likely noisy silver labels or overgeneralization. High latency -> model size too large for CPU deployment or inefficient chunking. Recall plateauing -> concept-specific training data insufficient for some oncology concepts.
- First 3 experiments: 1) Generate silver dataset for one concept only and train a 500M retriever; compare against embedding baseline on that concept. 2) Swap GPT-3.5 labeling for direct GPT-4 classification; measure impact on precision and latency. 3) Evaluate the effect of CoT prompt structure by training one model with and one without CoT-labeled data on the same concept.

## Open Questions the Paper Calls Out

### Open Question 1
Can the Onco-Retriever framework be extended to work effectively with generic retrieval queries beyond oncology-specific concepts? The authors acknowledge that while the current model is specific to well-defined oncology concepts, there is potential to adapt the approach to encompass generic queries, but they have not validated whether the observed performance gains would be analogous in a broader context. This remains unresolved because the authors have not tested the model's performance on generic queries and acknowledge that creating a comprehensive dataset representing the vast spectrum of possible generic queries presents a difficult challenge. Experiments demonstrating the model's performance on a diverse set of generic retrieval queries, along with a comparison to existing generic retrieval models, would provide insights into the feasibility and effectiveness of extending the Onco-Retriever framework.

### Open Question 2
How would integrating the Onco-Retriever into a complete Retrieval-Augmented Generation (RAG) pipeline or real-world search use case impact its performance and the impact of the 20% information potentially missed by the model? The authors mention that their evaluation did not extend to integrating the retriever within a complete RAG pipeline or in a real-world search use case, which might provide deeper insights into the impact of the information potentially missed by their model. This remains unresolved because the authors have not conducted experiments to assess the model's performance in a real-world setting or within a complete RAG pipeline, which would provide a more comprehensive understanding of its practical impact. Evaluating the Onco-Retriever's performance in a real-world search use case or within a complete RAG pipeline, and assessing the impact of missed information on the final output, would provide valuable insights into its practical applicability.

### Open Question 3
Can the Onco-Retriever be further optimized for real-time execution to enable its use in scenarios where immediate retrieval is crucial, such as during patient consultations or real-time decision-making contexts? The authors acknowledge that while the Onco-Retriever operates with decent speed when running in the background, it is not optimized for real-time execution, which presents a significant hurdle for deployment in scenarios where immediate retrieval is important. This remains unresolved because the authors have not explored techniques or optimizations to improve the model's latency for real-time applications, and further research is needed to make the Onco-Retriever suitable for such use cases. Developing and evaluating techniques to optimize the Onco-Retriever's latency for real-time execution, and demonstrating its performance in scenarios where immediate retrieval is crucial, would provide evidence of its feasibility for real-time clinical use.

## Limitations

- Reliance on GPT-3.5 for synthetic data generation introduces potential hallucination risks that could affect downstream model quality
- The specific rule-based chunking algorithm remains unspecified, making exact replication difficult
- Evaluation depends on a relatively small test set of 50 patients, which may limit generalizability across different oncology populations

## Confidence

- **High Confidence**: The basic distillation approach and overall methodology are well-established; the comparative F1 scores against Ada and Mistral are clearly reported.
- **Medium Confidence**: The performance improvements over PubMedBERT and the effectiveness of CoT prompting are demonstrated but could benefit from additional ablation studies.
- **Low Confidence**: The generalizability of results to other healthcare institutions and different oncology datasets, given the proprietary nature of the training data and unspecified chunking rules.

## Next Checks

1. Replicate the end-to-end pipeline on a publicly available oncology dataset with different patient demographics to verify generalization.
2. Conduct an ablation study comparing Onco-Retriever performance with and without CoT-labeled data on identical concepts.
3. Evaluate the impact of different chunking strategies on retrieval performance by testing multiple rule-based approaches on the same dataset.