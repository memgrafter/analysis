---
ver: rpa2
title: 'PiCO: Peer Review in LLMs based on the Consistency Optimization'
arxiv_id: '2402.01830'
source_url: https://arxiv.org/abs/2402.01830
tags:
- grade
- eliminated
- llms
- evaluation
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes PiCO, an unsupervised LLM evaluation method
  based on peer-review and consistency optimization. The method enables open-source
  and closed-source LLMs to answer unlabeled questions and evaluate each other's responses
  without human annotations.
---

# PiCO: Peer Review in LLMs based on the Consistency Optimization

## Quick Facts
- arXiv ID: 2402.01830
- Source URL: https://arxiv.org/abs/2402.01830
- Authors: Kun-Peng Ning; Shuo Yang; Yu-Yang Liu; Jia-Yu Yao; Zhen-Hui Liu; Yong-Hong Tian; Yibing Song; Li Yuan
- Reference count: 40
- Primary result: Unsupervised LLM evaluation method achieving 0.90, 0.89, and 0.84 Spearman correlation on MT-Bench, Chatbot Arena, and AlpacaEval respectively

## Executive Summary
PiCO introduces an unsupervised method for evaluating large language models through peer review and consistency optimization. The approach enables open-source and closed-source LLMs to answer unlabeled questions and evaluate each other's responses without human annotations. Each model is assigned a learnable capability parameter, and the ranking is optimized to maximize consistency between model capabilities and scores. Experiments on three crowdsourcing datasets with 15 diverse LLMs demonstrate superior performance compared to baseline methods, with significant improvements in Spearman's rank correlation coefficient.

## Method Summary
PiCO implements a peer-review system where multiple LLMs answer open-ended questions, then evaluate each other's responses in anonymous pairs. Each model receives a learnable capability weight that influences its evaluation strength. The method uses consistency optimization to maximize the correlation between model capabilities and their evaluation scores, iteratively refining weights and eliminating the weakest reviewers. The elimination mechanism removes models that consistently provide poor evaluations, with experiments showing optimal performance when 60% of weaker reviewers are removed.

## Key Results
- Achieves 0.90 Spearman correlation on MT-Bench dataset
- Achieves 0.89 Spearman correlation on Chatbot Arena dataset
- Achieves 0.84 Spearman correlation on AlpacaEval dataset
- Outperforms existing SOTA methods like PRD and PRE
- Demonstrates effectiveness in reducing evaluation bias through capability-based reweighting

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Models with higher capability weights assign higher scores to better responses, creating a consistent alignment between capability and scoring accuracy
- Mechanism: Each model receives a learnable capability weight that influences how strongly it evaluates responses. During consistency optimization, weights are adjusted to maximize correlation between capability weights and response scores
- Core assumption: High-capability models evaluate responses more accurately than low-capability models, and these models also generate higher-quality responses themselves
- Evidence anchors:
  - [abstract] "high-level LLM can evaluate others' answers more accurately than low-level ones, while higher-level LLM can also achieve higher response scores"
  - [section 2.2.2] "we maximize the consistency of each LLM's capability w and score G with constrained optimization"
  - [corpus] Weak evidence - only 5 papers in related corpus, none directly validate this specific consistency mechanism
- Break condition: If the correlation between capability weights and scores fails to improve over random initialization, or if eliminating reviewers actually degrades performance

### Mechanism 2
- Claim: The peer review process reduces systematic evaluation bias by reweighting evaluations based on model capabilities
- Mechanism: Models that exhibit consistent bias toward their own responses receive lower weights in the evaluation process, effectively reducing their influence on the final ranking
- Core assumption: Models tend to overvalue their own responses and undervalue others', creating systematic bias that can be corrected through capability-based reweighting
- Evidence anchors:
  - [section 3.2] "The learned confidence weight w can significantly mitigate the preference gaps of the whole evaluation system"
  - [section 2.2.1] "we construct anonymous answer pairs, while randomly selecting other LLMs as 'reviewers'"
  - [corpus] No direct evidence - related papers focus on automated evaluation but not bias mitigation through capability weighting
- Break condition: If reweighting based on capabilities fails to reduce observed preference gaps between models

### Mechanism 3
- Claim: Unsupervised elimination of weakest reviewers improves overall evaluation quality by removing noisy evaluations
- Mechanism: Models with the lowest scores are iteratively removed from the reviewer pool, with the process stopping when system loss reaches a minimum
- Core assumption: Weaker models provide noisier, less reliable evaluations that degrade the quality of the overall peer review system
- Evidence anchors:
  - [section 3.3] "removing weaker reviewers reduces the average loss of the entire system"
  - [section 3.3] "when 60% (or 9) of the weaker reviewers are removed, the system's loss reaches its minimum"
  - [corpus] Weak evidence - no direct validation of this elimination threshold approach in related literature
- Break condition: If eliminating reviewers beyond the optimal threshold (60%) continues to improve performance, suggesting the mechanism is not properly calibrated

## Foundational Learning

- Concept: Consistency optimization as constrained optimization
  - Why needed here: The core innovation relies on maximizing the correlation between model capabilities and evaluation scores under the constraint that scores must be derived from peer evaluations
  - Quick check question: How does the Pearson correlation coefficient measure consistency between capability weights and scores in this context?

- Concept: Permutation entropy as a ranking evaluation metric
  - Why needed here: The paper uses permutation entropy to measure how close the learned ranking is to human preferences, with lower entropy indicating better alignment
  - Quick check question: Why would a ranking with many inversions have higher permutation entropy than a more ordered ranking?

- Concept: Peer review mechanism design
  - Why needed here: The method depends on constructing anonymous answer pairs and having models evaluate each other's responses without knowing the source
  - Quick check question: How does the anonymity requirement prevent self-favoring bias in the peer review process?

## Architecture Onboarding

- Component map:
  Data Collection -> Model responses -> Answer pairs -> Peer Review -> Anonymous evaluation -> Confidence-weighted scoring -> Consistency Optimization -> Capability weight initialization -> Correlation maximization -> Iterative refinement -> Elimination Mechanism -> Score-based filtering -> Threshold learning -> Reviewer reduction -> Final Ranking

- Critical path: Data Collection -> Peer Review -> Consistency Optimization -> Final Ranking
  The most time-consuming step is the peer review evaluation phase where each model must evaluate multiple response pairs

- Design tradeoffs:
  - Token efficiency vs. evaluation quality: More reviewers per pair increases accuracy but also token consumption
  - Elimination threshold selection: Too aggressive elimination removes useful evaluators; too conservative retains noisy ones
  - Weight initialization: Random initialization works but informed initialization (e.g., baseline scores) might converge faster

- Failure signatures:
  - Inconsistent rankings across different seeds indicate instability in the optimization process
  - Elimination mechanism fails to find a clear minimum loss point
  - Learned capability weights show no correlation with actual model performance

- First 3 experiments:
  1. Validate consistency assumption: Manually assign capability weights in forward/backward order and measure ranking correlation
  2. Test elimination mechanism: Vary elimination thresholds and measure impact on final ranking quality
  3. Compare scoring mechanisms: Test Elo vs Rank-based scoring to determine which produces more stable results

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What are the optimal elimination thresholds for different model pools and dataset sizes?
- Basis in paper: [explicit] The paper mentions that 60% elimination of weaker reviewers minimizes system loss, but this may not be optimal for all scenarios
- Why unresolved: The paper only tested one specific elimination rate (60%) and three datasets, without exploring how optimal thresholds vary with model pool composition, dataset size, or task complexity
- What evidence would resolve it: Systematic experiments varying elimination percentages (e.g., 30%, 50%, 70%, 80%) across diverse model pools, dataset sizes, and task types, measuring how the optimal threshold changes

### Open Question 2
- Question: How does PiCO's performance degrade when reviewer models are intentionally poisoned or adversarially manipulated?
- Basis in paper: [inferred] The paper does not test robustness to adversarial scenarios where some models might provide misleading evaluations
- Why unresolved: The current evaluation assumes all models are either competent or incompetent, but doesn't test scenarios where models might be deliberately deceptive or have hidden biases
- What evidence would resolve it: Experiments introducing controlled adversarial reviewers that systematically favor certain model types or provide random evaluations, measuring PiCO's resilience

### Open Question 3
- Question: What is the theoretical convergence guarantee for the consistency optimization algorithm?
- Basis in paper: [explicit] The paper presents the consistency optimization framework but doesn't provide theoretical analysis of convergence properties
- Why unresolved: While the algorithm appears to work empirically, there's no proof of convergence, uniqueness of solutions, or bounds on how close the learned ranking gets to optimal
- What evidence would resolve it: Mathematical proofs establishing convergence conditions, uniqueness of solutions under different consistency measures, or approximation bounds showing how close PiCO gets to the true ranking

## Limitations

- The consistency optimization mechanism's exact mathematical formulation remains unclear despite claims of superior performance
- The elimination threshold of 60% weaker reviewers appears to be an empirical finding without theoretical justification
- The absence of baseline comparisons with simpler ensemble methods makes it difficult to assess whether the peer-review mechanism provides additional value

## Confidence

- High confidence: The peer-review framework's basic architecture and experimental methodology are well-documented and reproducible
- Medium confidence: The correlation between model capabilities and evaluation accuracy, though intuitively sound, lacks direct experimental validation in the paper
- Low confidence: The optimal elimination threshold and its dependence on dataset characteristics are not theoretically grounded

## Next Checks

1. **Ablation Study**: Remove the elimination mechanism and test whether the same ranking quality can be achieved through simpler consistency optimization without reviewer pruning
2. **Cross-Dataset Stability**: Apply PiCO to a different domain (e.g., code generation or reasoning tasks) to test whether the 60% elimination threshold generalizes beyond conversational AI
3. **Theoretical Analysis**: Derive the expected correlation between capability weights and evaluation accuracy under different noise models to determine if the consistency optimization is mathematically sound