---
ver: rpa2
title: 'ProGRes: Prompted Generative Rescoring on ASR n-Best'
arxiv_id: '2409.00217'
source_url: https://arxiv.org/abs/2409.00217
tags:
- hypotheses
- rescoring
- speech
- n-best
- llms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces ProGRes, a novel method for improving ASR
  n-best rescoring by leveraging modern LLMs. ProGRes extends the initial set of n-best
  hypotheses provided by the ASR with hypotheses generated by prompt-based LLMs, then
  reranks this extended set using scores derived from both linguistic information
  (via open-weight LLMs) and acoustic information (from the ASR).
---

# ProGRes: Prompted Generative Rescoring on ASR n-Best

## Quick Facts
- arXiv ID: 2409.00217
- Source URL: https://arxiv.org/abs/2409.00217
- Authors: Ada Defne Tur; Adel Moumen; Mirco Ravanelli
- Reference count: 0
- Primary result: WER reduced from 42.94% to 40.84% (ASR1) and 12.38% to 9.32% (ASR2)

## Executive Summary
This paper introduces ProGRes, a novel method for improving ASR n-best rescoring by leveraging modern LLMs. ProGRes extends the initial set of n-best hypotheses provided by the ASR with hypotheses generated by prompt-based LLMs, then reranks this extended set using scores derived from both linguistic information (via open-weight LLMs) and acoustic information (from the ASR). Experiments on CommonVoice test set show significant WER reductions: from 42.94% to 40.84% for one ASR system and 12.38% to 9.32% for another, representing relative improvements of 4.9% to 24.7%. The method outperforms both prompt-only and LLM rescoring approaches, demonstrating the effectiveness of combining ASR and LLM scores.

## Method Summary
ProGRes uses prompt-based LLMs to generate additional hypotheses from ASR n-best lists, then re-ranks the extended set using linear interpolation of ASR and LLM scores. The method operates in a zero-shot manner without fine-tuning, making it applicable across different ASR systems. For hypothesis generation, the LLM analyzes the n-best hypotheses and generates a more plausible transcription. The extended hypothesis set is then scored using both the ASR's confidence scores and an LLM scorer, with the final score computed as a weighted combination. The method was evaluated on two ASR systems trained on different datasets (SPGISpeech and CommonVoice) using the CommonVoice test set.

## Key Results
- WER reduced from 42.94% to 40.84% (4.9% relative improvement) for ASR1
- WER reduced from 12.38% to 9.32% (24.7% relative improvement) for ASR2
- ProGRes outperforms both prompt-only and LLM rescoring approaches
- GPT-4 achieves the highest performance among tested LLMs

## Why This Works (Mechanism)

### Mechanism 1
- LLM-generated hypotheses augment ASR n-best lists with linguistically plausible alternatives that capture knowledge beyond acoustic evidence
- Core assumption: The LLM has sufficient knowledge to identify and correct errors in the ASR hypotheses that stem from phonetic ambiguity or named entity recognition failures
- Evidence anchors: Abstract states LLMs can predict phonetic patterns that correlate to meaningful replacements; paper section notes intelligent analysis of n-best hypotheses
- Break condition: If the LLM lacks knowledge about the domain or specific named entities in the audio, generated hypotheses may be incorrect or hallucinated

### Mechanism 2
- Interpolation between ASR acoustic scores and LLM linguistic scores produces better final hypotheses than either score alone
- Core assumption: Both ASR and LLM scores provide complementary information - acoustic match and linguistic plausibility respectively
- Evidence anchors: Linear interpolation equation used to combine scores; paper demonstrates ProGRes leads to significant WER improvements
- Break condition: If either score type is unreliable, interpolation may degrade performance

### Mechanism 3
- Zero-shot prompting without fine-tuning enables effective hypothesis generation across different ASR systems
- Core assumption: General-purpose LLMs possess sufficient linguistic knowledge to correct ASR errors without domain-specific fine-tuning
- Evidence anchors: Paper introduces a zero-shot method and explicitly states no models were fine-tuned throughout the process
- Break condition: If the zero-shot approach fails to capture domain-specific terminology or style, performance may suffer

## Foundational Learning

- Concept: Beam search decoding in ASR systems
  - Why needed here: Understanding how n-best hypotheses are generated and their relationship to acoustic scores is crucial for integrating LLM rescoring
  - Quick check question: What is the relationship between beam width and hypothesis quality in ASR systems?

- Concept: Log probability scoring for sequence evaluation
  - Why needed here: Both ASR confidence scores and LLM sequence scores are computed using log probabilities, which must be understood for proper interpolation
  - Quick check question: How does summing log probabilities differ from multiplying raw probabilities in sequence scoring?

- Concept: Prompt engineering for instruction-tuned LLMs
  - Why needed here: The quality of LLM-generated hypotheses depends heavily on prompt design, as shown by the ablation study in the paper
  - Quick check question: How does constraining LLM output versus allowing open generation affect hallucination rates?

## Architecture Onboarding

- Component map: ASR system (ESPnet/ST) → n-best hypothesis generation → LLM generator (GPT-4/Llama-3) → hypothesis generation → LLM scorer (Llama-3) → sequence scoring → Interpolation module → score combination → Final selection → best hypothesis output

- Critical path: 1. ASR generates n-best hypotheses with confidence scores 2. LLM generates new hypothesis from n-best list 3. Extended hypothesis set (n-best + LLM) scored by LLM scorer 4. Interpolation of ASR and LLM scores 5. Selection of highest-scoring hypothesis

- Design tradeoffs:
  - Computational cost vs. accuracy: LLM inference is expensive but provides significant WER improvements
  - Prompt constraints vs. creativity: More constrained prompts reduce hallucinations but may miss creative corrections
  - Weight tuning vs. robustness: Optimal interpolation weight varies by ASR system and dataset

- Failure signatures:
  - High WER with certain LLM weights suggests poor complementarity between ASR and LLM scores
  - Persistent hallucinations in LLM output indicates problematic prompt design
  - Minimal improvement over baseline suggests LLM lacks relevant knowledge

- First 3 experiments:
  1. Baseline: Measure WER of ASR systems without any LLM intervention
  2. Prompt-only: Test LLM hypothesis generation without rescoring to establish upper bound
  3. Interpolation sweep: Vary LLM weight α from 0 to 1 to find optimal balance point

## Open Questions the Paper Calls Out

- Question: How does contamination from training data affect the performance of LLM-based ASR rescoring, and can this be quantified?
  - Basis in paper: The paper discusses contamination concerns, noting that CommonVoice includes text transcriptions from Wikipedia, which is commonly incorporated into LLM training datasets
  - Why unresolved: The paper acknowledges contamination as a concern but does not provide a quantitative analysis of its impact on rescoring performance
  - What evidence would resolve it: A controlled experiment comparing LLM rescoring performance on datasets with varying levels of overlap with LLM training data would provide quantitative evidence of contamination effects

- Question: What is the optimal balance between LLM and ASR scores in the interpolation, and how does this vary across different ASR and LLM configurations?
  - Basis in paper: The paper uses linear interpolation between LLM and ASR scores but does not explore the optimal balance in detail
  - Why unresolved: The paper mentions that the optimal value of the language model weight α can be determined through linear search optimization, but it does not provide a comprehensive analysis of how this balance affects performance across different configurations
  - What evidence would resolve it: A systematic study varying the LLM weight α across a wide range of ASR and LLM configurations would identify the optimal balance for each scenario

- Question: How does the computational complexity of ProGRes compare to other ASR rescoring methods, and what are the practical implications for real-time applications?
  - Basis in paper: The paper mentions that ProGRes has computational complexity due to the resource-intensive nature of LLMs, but it does not provide a detailed comparison with other methods
  - Why unresolved: The paper does not provide a quantitative analysis of the computational complexity of ProGRes compared to other rescoring methods
  - What evidence would resolve it: A benchmarking study comparing the computational complexity of ProGRes with other rescoring methods would provide insights into its practical implications for real-time applications

## Limitations

- The evaluation is limited to two ASR systems and a single test set (CommonVoice), raising questions about generalization to other domains and languages
- The computational cost of LLM inference is substantial, with no analysis of inference time or resource requirements provided
- The paper doesn't explore more sophisticated fusion methods beyond simple linear interpolation that might better capture the relationship between acoustic and linguistic information

## Confidence

- Confidence Level: Medium for WER improvements
  - Significant WER reductions demonstrated but limited to two ASR systems and one test set
- Confidence Level: Low for zero-shot approach
  - Reliance on general-purpose LLMs without fine-tuning may not work for specialized domains
- Confidence Level: Medium for interpolation methodology
  - Linear interpolation assumes comparable score scales but more sophisticated methods weren't explored

## Next Checks

1. **Cross-Domain Validation**: Test ProGRes on ASR systems trained on different domains (e.g., medical, technical, or conversational speech) to verify if the zero-shot approach maintains effectiveness across specialized vocabularies and terminology.

2. **Ablation of Computational Cost**: Measure and analyze the inference time and resource requirements for LLM generation and rescoring across different LLM sizes to quantify the practical deployment costs and identify potential optimization opportunities.

3. **Alternative Fusion Methods**: Compare the linear interpolation approach against learned fusion methods (e.g., neural network-based score fusion or non-linear combinations) to determine if more sophisticated score combination strategies yield better performance than simple weighted averaging.