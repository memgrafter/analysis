---
ver: rpa2
title: Scaling Up Deep Clustering Methods Beyond ImageNet-1K
arxiv_id: '2406.01203'
source_url: https://arxiv.org/abs/2406.01203
tags:
- clustering
- imagenet21k
- benchmarks
- label
- class
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper studies large-scale image clustering by developing new
  benchmarks on ImageNet21K and evaluating deep clustering methods against feature-based
  k-means. The benchmarks vary in class imbalance, granularity, and difficulty to
  classify.
---

# Scaling Up Deep Clustering Methods Beyond ImageNet-1K

## Quick Facts
- arXiv ID: 2406.01203
- Source URL: https://arxiv.org/abs/2406.01203
- Authors: Nikolas Adaloglou; Felix Michels; Kaspar Senft; Diana Petrusheva; Markus Kollmann
- Reference count: 40
- Key outcome: TEMI achieves 69.79% accuracy on ImageNet-1K, surpassing a supervised GoogLeNet

## Executive Summary
This paper establishes new benchmarks for large-scale image clustering by evaluating deep clustering methods (TEMI and SCANv2) against feature-based k-means on ImageNet21K-based datasets. The study reveals that k-means is unfairly evaluated on balanced datasets and underperforms on easy-to-classify benchmarks due to its inability to model irregular cluster shapes. Deep clustering methods consistently outperform k-means on most large-scale benchmarks by leveraging nearest-neighbor-based self-distillation, which captures non-isotropic, non-linear cluster boundaries. The findings demonstrate that deep clustering methods can capture multiple semantic labels per image, providing meaningful secondary cluster predictions beyond the top assignment.

## Method Summary
The study evaluates deep clustering methods (TEMI and SCANv2) against feature-based k-means on ImageNet21K-based benchmarks. TEMI and SCANv2 use nearest-neighbor sampling in feature space combined with self-distillation frameworks, while k-means relies on cosine similarity with L2-normalized features. The methods are tested on various ImageNet21K subsets including imbalanced, coarse, and model-based datasets, alongside the standard ImageNet-1K benchmark. Evaluation metrics include clustering accuracy, top-1→1, top-1→L, top-5→1, top-5→L, linear probing accuracy, and calibration (ECE). Experiments use pre-trained feature extractors (iBOT ViT-L, MAE-R ViT-H, DINOv2 ViT-g, OpenCLIP ViT-G) with TEMI and SCANv2 trained for 50 epochs on ImageNet-1K and 25 epochs on ImageNet21K.

## Key Results
- TEMI achieves 69.79% accuracy on ImageNet-1K, surpassing supervised GoogLeNet
- Deep clustering methods consistently outperform k-means on most large-scale benchmarks
- K-means demonstrates suboptimal performance on imbalanced and easy-to-classify benchmarks
- Non-primary cluster predictions capture meaningful classes such as coarser or coexisting labels

## Why This Works (Mechanism)

### Mechanism 1
- Claim: TEMI and SCANv2 outperform k-means on large-scale benchmarks due to their use of nearest-neighbor-based self-distillation, which captures irregular cluster shapes missed by k-means.
- Mechanism: These methods learn cluster assignments by sampling from nearest neighbors in feature space, allowing them to adapt to non-isotropic, non-linear cluster boundaries.
- Core assumption: Nearest-neighbor pairs in the learned feature space are more likely to share semantic similarity than random pairs.
- Evidence anchors:
  - [abstract] "deep clustering methods like TEMI and SCANv2 consistently outperform it on most large-scale benchmarks"
  - [section] "well-separated clustering benchmarks may have irregular class shapes that cannot always be modeled with k-means"
  - [corpus] No direct corpus evidence; corpus papers focus on different clustering algorithms, so evidence is weak here.
- Break condition: If nearest-neighbor pairs become less semantically meaningful due to poor feature representations, the advantage diminishes.

### Mechanism 2
- Claim: Evaluating k-means only on balanced datasets unfairly disadvantages it compared to deep clustering methods that enforce class uniformity.
- Mechanism: Balanced datasets assume uniform cluster sizes, while k-means tends to create imbalanced clusters in real-world data, leading to lower accuracy when evaluated on imbalanced benchmarks.
- Core assumption: Class imbalance is a realistic property of large-scale real-world datasets.
- Evidence anchors:
  - [abstract] "feature-based k-means is often unfairly evaluated on balanced datasets"
  - [section] "k-means demonstrates suboptimal performance even on imbalanced benchmarks"
  - [corpus] No direct corpus evidence; corpus papers do not discuss k-means evaluation fairness, so evidence is weak here.
- Break condition: If class distributions in real-world data become more balanced, the fairness issue diminishes.

### Mechanism 3
- Claim: Deep clustering methods can capture multiple semantic labels per image, such as coarser or coexisting classes, while k-means cannot.
- Mechanism: Methods like TEMI and SCANv2 provide probability distributions over clusters, enabling identification of secondary meaningful clusters beyond the top prediction.
- Core assumption: Images often contain multiple relevant concepts not captured by single-label ground truth.
- Evidence anchors:
  - [abstract] "non-primary cluster predictions capture meaningful classes (i.e. coarser classes)"
  - [section] "non-primary cluster predictions of clustering methods capture meaningful classes such as coarser or coexisting labels"
  - [corpus] No direct corpus evidence; corpus papers do not discuss multi-label capture, so evidence is weak here.
- Break condition: If ground truth becomes truly multi-label annotated, this advantage becomes standard for all methods.

## Foundational Learning

- Concept: Nearest-neighbor sampling in feature space
  - Why needed here: This is the core mechanism by which TEMI and SCANv2 learn meaningful cluster assignments beyond simple distance metrics.
  - Quick check question: Why do TEMI and SCANv2 use nearest neighbors instead of random sampling when learning cluster assignments?

- Concept: Self-distillation framework
  - Why needed here: Understanding how the teacher-student architecture works is essential to grasp why these methods outperform traditional approaches.
  - Quick check question: How does the exponential moving average of the student model parameters create a more stable teacher in the self-distillation framework?

- Concept: Feature normalization and distance metrics
  - Why needed here: Both k-means and deep clustering methods rely on proper feature representations; understanding L2 normalization and cosine similarity is crucial.
  - Quick check question: Why does k-means use cosine similarity instead of Euclidean distance when features are L2-normalized?

## Architecture Onboarding

- Component map: Pre-trained feature extractor (iBOT ViT-L) -> Clustering heads (TEMI or SCANv2) -> Nearest-neighbor index (FAISS) -> Evaluation pipeline
- Critical path: 1) Precompute features using pre-trained model, 2) Build nearest-neighbor index, 3) Train clustering heads using self-distillation on nearest-neighbor pairs, 4) Evaluate using Hungarian matching against ground truth
- Design tradeoffs: TEMI vs SCANv2 tradeoff involves memory usage (SCANv2 requires larger batch sizes for entropy regularization) vs calibration quality (SCANv2 produces better-calibrated outputs). Feature extractor choice impacts both accuracy and computational cost.
- Failure signatures: Poor performance on imbalanced benchmarks suggests k-means limitations; high variance across feature extractors indicates method dependence on representation quality; out-of-memory errors for SCANv2 on large-scale benchmarks indicate batch size constraints.
- First 3 experiments:
  1. Verify that TEMI and SCANv2 outperform k-means on ImageNet-1K using the same feature extractor
  2. Test clustering performance on a small imbalanced subset of ImageNet21K to observe the fairness issue
  3. Compare top-1 vs top-5 predictions to verify that non-primary clusters capture meaningful additional concepts

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do deep clustering methods perform on extremely large-scale datasets (e.g., 100 million+ images) compared to k-means?
- Basis in paper: [inferred] The paper scales experiments up to ImageNet21K (11 million images) and shows deep clustering methods outperform k-means. However, the authors note that k-means has been applied to proprietary billion-scale datasets, suggesting a potential performance gap at larger scales.
- Why unresolved: The paper does not provide empirical evidence for performance on datasets significantly larger than ImageNet21K. Scaling deep clustering methods to such sizes may introduce computational challenges or algorithmic limitations not yet explored.
- What evidence would resolve it: Experiments comparing TEMI and SCANv2 against k-means on datasets with 100 million+ images, measuring clustering accuracy, computational efficiency, and scalability.

### Open Question 2
- Question: Can the performance gap between k-means and deep clustering methods on easy-to-classify benchmarks be closed by modifying k-means to handle irregular class shapes?
- Basis in paper: [explicit] The authors observe that k-means underperforms significantly on easy-to-classify benchmarks, attributing this to its inability to model irregular class shapes. Deep clustering methods leverage nearest neighbors to capture such shapes.
- Why unresolved: The paper does not explore modifications to k-means (e.g., using different distance metrics or clustering strategies) that could address this limitation. It remains unclear if k-means can be adapted to match deep clustering methods on these benchmarks.
- What evidence would resolve it: Experiments comparing modified k-means variants (e.g., using cosine similarity, density-based clustering) against TEMI and SCANv2 on easy-to-classify benchmarks, measuring clustering accuracy and robustness.

### Open Question 3
- Question: How does the choice of feature extractor impact the performance of deep clustering methods across diverse datasets and tasks?
- Basis in paper: [explicit] The authors note that TEMI and SCANv2 consistently outperform k-means across feature extractors on ImageNet-1K, but no method consistently surpasses when varying pre-trained feature extractors. This suggests dependence on feature quality.
- Why unresolved: The paper evaluates a limited set of feature extractors and does not explore how performance varies with feature quality, domain specificity, or task alignment. The impact of feature extractor choice on clustering in other domains (e.g., medical imaging, video) is also unexplored.
- What evidence would resolve it: Systematic experiments comparing deep clustering methods using feature extractors from diverse domains and tasks, measuring clustering accuracy, generalization, and sensitivity to feature quality.

## Limitations
- Lack of corpus evidence for key mechanisms, relying primarily on internal experimental observations
- Memory constraints for SCANv2 on large-scale benchmarks may limit practical applicability
- Transferability of findings to non-image domains remains untested

## Confidence
- Major claims about benchmarking results: Medium to High
- Theoretical mechanisms explanations: Low to Medium
- Evidence anchors: Primarily from direct experimental results with limited corpus support

## Next Checks
1. **Mechanism validation**: Test whether random sampling (instead of nearest-neighbor sampling) in TEMI/SCANv2 degrades performance on the same benchmarks to confirm the nearest-neighbor mechanism
2. **Fairness assessment**: Create additional balanced/imbalanced datasets with known cluster shapes to systematically test k-means evaluation fairness claims
3. **Multi-label capture verification**: Annotate a subset of ImageNet21K with ground truth multi-labels and verify whether deep clustering methods' secondary predictions align with these labels more than k-means predictions