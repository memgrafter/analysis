---
ver: rpa2
title: 'Do Not Wait: Learning Re-Ranking Model Without User Feedback At Serving Time
  in E-Commerce'
arxiv_id: '2406.14004'
source_url: https://arxiv.org/abs/2406.14004
tags:
- online
- last
- user
- learning
- re-ranking
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: LAST enables online learning for re-ranking models without waiting
  for delayed user feedback. It uses a surrogate evaluator to provide real-time instructional
  signals and applies transient, request-specific model modifications at serving time.
---

# Do Not Wait: Learning Re-Ranking Model Without User Feedback At Serving Time in E-Commerce

## Quick Facts
- arXiv ID: 2406.14004
- Source URL: https://arxiv.org/abs/2406.14004
- Reference count: 33
- Key outcome: LAST enables online learning for re-ranking models without waiting for delayed user feedback, achieving 2.08% improvement in purchase numbers per user in online A/B tests

## Executive Summary
LAST addresses the challenge of online learning for re-ranking models in e-commerce without waiting for delayed user feedback. The method uses a surrogate evaluator to provide real-time instructional signals and applies transient, request-specific model modifications at serving time. These modifications are tailored to individual request contexts and discarded after each recommendation to prevent error propagation. The parallel version efficiently explores model modifications through gradient-based suggestions, optimizing recommendation lists before presenting them to users. Comprehensive experiments show LAST significantly outperforms state-of-the-art re-ranking models while maintaining user engagement without requiring engineering system upgrades.

## Method Summary
LAST is a re-ranking framework that learns online without waiting for user feedback by using a surrogate evaluator to provide real-time quality assessments. For each incoming request, LAST computes context-specific parameter modifications that optimize recommendations for that specific user-item context, then discards these modifications after serving. The parallel version explores multiple modifications simultaneously using gradient-based suggestions, selecting the best-performing recommendation list based on evaluator feedback. This approach enables immediate model improvement while avoiding the pitfalls of error accumulation that plague traditional online learning methods.

## Key Results
- Achieves 2.08% improvement in purchase numbers per user in online A/B tests
- Significantly outperforms state-of-the-art re-ranking models
- Maintains user engagement metrics while improving purchase rates
- Works without requiring engineering system upgrades

## Why This Works (Mechanism)

### Mechanism 1
LAST achieves online learning without waiting for delayed user feedback by using a surrogate evaluator to provide real-time instructional signals. The surrogate evaluator model approximates the list reward function R(u,L,yL) and provides immediate feedback to guide model modifications during serving time, bypassing the need to wait for actual user actions like purchases.

### Mechanism 2
LAST creates request-specific, transient model modifications that are discarded after each recommendation to prevent error propagation. For each incoming request, LAST computes a context-specific parameter modification Δθ that optimizes the recommendation list for that specific user-item context, then discards this modification after serving.

### Mechanism 3
The parallel version of LAST achieves efficient exploration through gradient-based suggestions without requiring full gradient computation of the evaluator. Instead of computing partial gradients of the evaluator with respect to model parameters, LAST uses normalized gradients of the generation probability P(L) combined with step sizes to explore model modifications in parallel.

## Foundational Learning

- **Actor-Evaluator Framework**: Why needed here: LAST builds on this framework by using the evaluator to provide real-time feedback instead of waiting for actual user feedback. Quick check: What is the key difference between the evaluator's role in offline training versus LAST's online serving?

- **Online Learning vs. Batch Learning**: Why needed here: LAST is a form of online learning that updates the model during serving rather than in batch updates after collecting user feedback. Quick check: How does LAST's approach to handling delayed feedback differ from traditional online learning methods?

- **Gradient-based Optimization**: Why needed here: The parallel version of LAST uses gradient-based exploration to efficiently search for model modifications. Quick check: Why can't LAST directly use the gradient of the evaluator with respect to model parameters?

## Architecture Onboarding

- **Component map**: Request Handler -> Actor Model (G) -> Evaluator Model (E) -> Gradient Exploration Module -> Modification Application Layer

- **Critical path**: Request → Actor Generation → Evaluator Assessment → Modification Exploration → Best List Selection → Response

- **Design tradeoffs**:
  - Latency vs. Quality: More exploration steps improve quality but increase latency
  - Accuracy vs. Efficiency: Using P(L) gradients is faster but potentially less accurate than E(·) gradients
  - Adaptability vs. Stability: Request-specific modifications improve adaptation but discarding them may lose useful patterns

- **Failure signatures**:
  - High latency responses indicate exploration is taking too long
  - Degraded recommendation quality suggests the evaluator is poorly calibrated
  - System instability suggests modification application/reversion is error-prone

- **First 3 experiments**:
  1. Implement the parallel version with a fixed small step size list [0.01, 0.02, 0.05] and verify it can modify recommendations in real-time
  2. Add the evaluator component and test that it can accurately score recommendation lists compared to actual user engagement
  3. Implement the modification discard mechanism and verify the base model parameters remain stable over multiple requests

## Open Questions the Paper Calls Out

1. **How does LAST perform compared to classic online learning methods when both are available in the system?** The authors state that LAST can be seamlessly integrated with existing feedback-based online learning methods, but note they don't have classic online learning deployed due to engineering limitations. The paper lacks direct comparison between LAST and traditional online learning approaches, leaving uncertainty about the relative performance benefits of each method.

2. **What is the optimal size of the parameter subset to modify in LAST for different types of recommender systems?** The paper mentions that modifying all billions of parameters is not necessary or efficient, and suggests modifying only a key subset, but doesn't provide guidance on determining the optimal subset size.

3. **How does LAST's performance degrade when the surrogate model's predictions are significantly inaccurate?** The authors acknowledge that "predictions of the surrogate model may be inaccurate" and design LAST to discard modifications after each request to prevent error propagation, but the paper doesn't quantify how LAST's performance is affected by surrogate model accuracy.

## Limitations
- Reliance on a surrogate evaluator that must accurately approximate the true reward function, with limited validation of generalization across different contexts
- Transient modification mechanism may discard useful patterns that could benefit future recommendations
- No direct comparison with traditional online learning methods to establish relative performance benefits

## Confidence
- Mechanism 1: Medium - Actor-evaluator framework is well-established, but real-time application in this specific context needs more validation
- Mechanism 2: Medium - Transient modifications are theoretically sound but practical effectiveness is uncertain
- Mechanism 3: Low - Gradient-based parallel exploration is innovative but has limited external validation

## Next Checks
1. **Evaluator Calibration Test**: Systematically compare the surrogate evaluator's predictions against actual user engagement across diverse recommendation scenarios to quantify approximation error and identify failure modes.

2. **Ablation Study on Modification Discarding**: Test whether retaining successful modifications (with decay) versus discarding them affects long-term performance, particularly for users with recurring needs or preferences.

3. **Latency-Accuracy Trade-off Analysis**: Measure the relationship between exploration depth (number of parallel modifications) and both recommendation quality and serving latency across different traffic patterns and model sizes.