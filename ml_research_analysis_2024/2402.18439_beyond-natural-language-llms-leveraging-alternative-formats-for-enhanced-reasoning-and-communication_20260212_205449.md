---
ver: rpa2
title: 'Beyond Natural Language: LLMs Leveraging Alternative Formats for Enhanced
  Reasoning and Communication'
arxiv_id: '2402.18439'
source_url: https://arxiv.org/abs/2402.18439
tags:
- answer
- statement
- person
- house
- table
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper challenges the default use of natural language (NL)\
  \ in Large Language Model (LLM) reasoning and multi-agent communication by exploring\
  \ alternative formats. The proposed AutoForm method enables LLMs to autonomously\
  \ select the most suitable format\u2014such as code, logical expressions, or structured\
  \ tables\u2014before reasoning or communicating."
---

# Beyond Natural Language: LLMs Leveraging Alternative Formats for Enhanced Reasoning and Communication

## Quick Facts
- arXiv ID: 2402.18439
- Source URL: https://arxiv.org/abs/2402.18439
- Reference count: 40
- Primary result: AutoForm improves reasoning efficiency by 3.3-5.7% and reduces multi-agent communication tokens by up to 72.7% while maintaining effectiveness

## Executive Summary
This paper challenges the default use of natural language in Large Language Model reasoning and multi-agent communication by exploring alternative formats. The proposed AutoForm method enables LLMs to autonomously select the most suitable format—such as code, logical expressions, or structured tables—before reasoning or communicating. Experiments demonstrate that this approach improves reasoning efficiency across various tasks and significantly reduces token usage in multi-agent scenarios without sacrificing effectiveness. The findings suggest that structured, non-natural language formats can enhance both the efficiency and effectiveness of LLM-based systems.

## Method Summary
The AutoForm method prompts LLMs to autonomously select optimal non-natural language formats (code, tables, logical expressions) before reasoning or communicating. The approach modifies standard Chain-of-Thought prompting by adding instructions encouraging format exploration and selection based on task structure. For reasoning tasks, LLMs choose formats like code or tables to solve problems more efficiently. For multi-agent communication, agents use structured formats (JSON-like messages with clear performatives) to convey information more concisely than natural language. Performance is evaluated by comparing accuracy and token usage against standard natural language approaches across diverse reasoning tasks and communication scenarios.

## Key Results
- 3.3-5.7% improvement in reasoning efficiency across tasks like logic grids, coin flip, and information essentiality
- Up to 72.7% reduction in token usage in multi-agent communication while maintaining effectiveness
- LLMs can devise generalizable formats from limited examples and transfer these formats across different models

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLMs can autonomously select optimal non-NL formats for reasoning tasks when explicitly prompted.
- Mechanism: The AutoForm prompt encourages LLMs to explore structured formats before solving problems, allowing them to choose the format that best fits task structure.
- Core assumption: LLMs have sufficient pre-training exposure to recognize and generate non-NL formats and can evaluate their suitability for given tasks.
- Evidence anchors: Abstract shows 3.3-5.7% improvement in reasoning efficiency; section confirms LLMs can devise transferable formats.
- Break condition: If LLM fails to generate structured format or consistently selects suboptimal formats across tasks.

### Mechanism 2
- Claim: Structured communication formats reduce token usage in multi-agent scenarios without sacrificing effectiveness.
- Mechanism: Using concise, structured formats (e.g., JSON-like messages) enables agents to convey information more efficiently than verbose natural language.
- Core assumption: Reduction in token usage translates directly to communication efficiency without loss of critical information.
- Evidence anchors: Abstract shows up to 72.7% token reduction while maintaining communicative effectiveness; section notes parallels with established agent communication languages.
- Break condition: If token reduction leads to decreased task performance or communication breakdown.

### Mechanism 3
- Claim: Format selection capability is transferable between different LLMs, enabling heterogeneous settings where one model selects format and another uses it.
- Mechanism: A more capable LLM (e.g., GPT-4) can determine optimal format for a task, which a less capable LLM (e.g., GPT-3.5) can effectively interpret and use.
- Core assumption: Format devised by advanced LLM is interpretable by other LLMs regardless of size or capability.
- Evidence anchors: Abstract confirms devised format is effectively transferable across different LLMs; section shows mixed results when transferring formats between models.
- Break condition: If less capable LLM cannot interpret or effectively use format selected by more capable LLM.

## Foundational Learning

- Concept: Chain-of-Thought (CoT) prompting
  - Why needed here: AutoForm builds upon CoT by modifying prompts to encourage non-NL formats. Understanding CoT is essential to grasp how AutoForm differs and improves upon baseline.
  - Quick check question: What is the primary difference between AutoForm and standard Chain-of-Thought prompting?

- Concept: Multi-agent communication and collaboration
  - Why needed here: Paper explores how different communication formats affect efficiency and effectiveness of multi-agent problem-solving. Understanding multi-agent systems is crucial to evaluate communication format experiments.
  - Quick check question: How does the paper define communication task between agents, and what metrics are used to evaluate their performance?

- Concept: Agent Communication Languages (ACLs) and KQML
  - Why needed here: Paper draws parallels between formats chosen by LLMs and traditional ACLs like KQML. Understanding ACLs provides context for why LLM-generated formats are considered efficient and structured.
  - Quick check question: What are key features of traditional Agent Communication Languages, and how do LLM-generated formats align with these features?

## Architecture Onboarding

- Component map: Prompt engineering module -> Format selection engine -> Reasoning/communication module -> Evaluation module
- Critical path: 1) Receive task input and initial prompt, 2) Generate AutoForm prompt with format selection instruction, 3) LLM processes prompt and implicitly selects optimal format, 4) LLM uses selected format to reason or communicate, 5) Evaluate performance and analyze results
- Design tradeoffs:
  - Format flexibility vs. consistency: Allowing LLMs to choose formats for each instance provides flexibility but may lack consistency. Task-based format selection offers consistency but may miss instance-specific optimizations.
  - Efficiency vs. interpretability: Structured formats are more efficient but may be less interpretable than natural language for human users.
  - Transferability vs. optimization: Using format devised by one LLM for another enables heterogeneous settings but may sacrifice some optimization compared to homogeneous settings.
- Failure signatures:
  - LLM fails to generate any structured format and defaults to natural language
  - Selected format is inappropriate for task, leading to decreased performance
  - Format is not transferable between LLMs, causing communication breakdown in heterogeneous settings
  - Token reduction leads to loss of critical information and decreased task performance
- First 3 experiments:
  1. Implement AutoForm prompt and test on single reasoning task (e.g., Coin Flip) with one LLM (e.g., GPT-3.5) to verify format selection and performance improvement
  2. Test AutoForm on multiple reasoning tasks (e.g., Logic Grid, Information Essentiality) with same LLM to evaluate generalizability across task types
  3. Implement multi-agent communication scenario with two agents (e.g., GPT-4 and GPT-3.5) and test AutoForm to measure token reduction and effectiveness compared to natural language communication

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does effectiveness of AutoForm vary across different types of reasoning tasks, such as logical reasoning versus symbolic reasoning?
- Basis in paper: [explicit] Paper mentions varying performance improvements across different tasks (e.g., 5.7% for Gemini Pro on Coin Flip vs. 3.3% for GPT-4 overall).
- Why unresolved: Paper does not provide detailed analysis of task-specific effectiveness or explain why certain formats are more beneficial for certain types of reasoning.
- What evidence would resolve it: Detailed breakdown of AutoForm's performance across different reasoning task categories, with analysis of format preferences for each type.

### Open Question 2
- Question: What are limitations of format transferability between LLMs, and under what conditions does it fail?
- Basis in paper: [explicit] Paper notes that transferring formats between models can lead to slightly inferior performance and provides some examples.
- Why unresolved: Paper does not explore boundaries of transferability, such as model size, training data, or format complexity.
- What evidence would resolve it: Systematic experiments testing transferability across diverse LLM pairs, with failure cases analyzed to identify patterns.

### Open Question 3
- Question: How does complexity of communication format chosen by LLMs scale with complexity of task or amount of information exchanged?
- Basis in paper: [inferred] Paper mentions token reduction (up to 72.7%) but does not explore how format complexity evolves with task difficulty.
- Why unresolved: Paper focuses on efficiency gains but does not analyze structural complexity of chosen formats.
- What evidence would resolve it: Study comparing format complexity (e.g., nesting depth, number of fields) across tasks of varying difficulty, with correlation analysis.

## Limitations

- Experimental setup lacks complete transparency about prompt variations and format selection processes
- Transferability claims face significant uncertainty with performance decreasing in most tasks when transferring formats between different LLM models
- Weak corpus support for autonomous format selection, relying primarily on experimental results rather than broader evidence from field

## Confidence

**High Confidence Claims:**
- Methodology for implementing AutoForm prompts is clearly specified
- Experimental results showing token reduction in multi-agent communication are robust
- Basic mechanism of encouraging non-NL format selection through prompts is validated

**Medium Confidence Claims:**
- Generalizability of format selection across different reasoning tasks
- Effectiveness of format transfer between different LLM models
- Parallel between LLM-generated formats and traditional Agent Communication Languages

**Low Confidence Claims:**
- Universal applicability of format selection autonomy across all LLM tasks
- Complete absence of information loss in token reduction process
- Scalability of these findings to larger, more complex multi-agent systems

## Next Checks

1. **Format Selection Consistency Test**: Run multiple trials of same reasoning task with identical prompts across different sessions to verify whether LLMs consistently select same optimal format or if selections vary significantly between runs.

2. **Cross-Domain Transferability Analysis**: Test whether formats devised for reasoning tasks (like Logic Grid) can be effectively transferred to entirely different task domains (like creative writing or summarization) to assess broader applicability of format selection.

3. **Human Interpretability Benchmark**: Conduct user study where human evaluators assess interpretability and usability of LLM-selected structured formats versus natural language outputs, particularly focusing on whether efficiency gains come at cost of human comprehension.