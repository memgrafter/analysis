---
ver: rpa2
title: Why Does Dropping Edges Usually Outperform Adding Edges in Graph Contrastive
  Learning?
arxiv_id: '2412.08128'
source_url: https://arxiv.org/abs/2412.08128
tags:
- graph
- learning
- edge
- edges
- dmax
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates why edge dropping typically outperforms
  edge adding in graph contrastive learning. The authors introduce Error Passing Rate
  (EPR) as a metric to quantify how well a graph fits a Graph Neural Network, measuring
  the ratio of wrongly passed messages during message passing.
---

# Why Does Dropping Edges Usually Outperform Adding Edges in Graph Contrastive Learning?

## Quick Facts
- arXiv ID: 2412.08128
- Source URL: https://arxiv.org/abs/2412.08128
- Authors: Yanchen Xu; Siqi Huang; Hongyuan Zhang; Xuelong Li
- Reference count: 34
- This paper investigates why edge dropping typically outperforms edge adding in graph contrastive learning through the lens of Error Passing Rate (EPR).

## Executive Summary
This paper addresses the empirical observation that edge dropping consistently outperforms edge adding in graph contrastive learning (GCL). The authors introduce Error Passing Rate (EPR) as a metric to quantify how well a graph fits a Graph Neural Network by measuring the ratio of wrongly passed messages during message passing. Through theoretical analysis, they show that dropping edges reduces EPR while adding edges often increases it, explaining the observed performance difference. Based on this insight, they propose EPAGCL, a novel method that uses both edge adding and dropping as augmentations, with probabilities derived from EPR to maintain low EPR values. Experiments on seven real-world datasets demonstrate that EPAGCL outperforms existing contrastive learning methods, achieving up to 0.61% accuracy improvement over GCA with better stability.

## Method Summary
EPAGCL is a graph contrastive learning framework that uses both edge adding and dropping as augmentations, with probabilities derived from Error Passing Rate (EPR) to maintain low EPR values. The method computes weights wd (for dropping) and wa (for adding) using αᵢ,ⱼ values, which are inversely related to EPR change. High-degree nodes are pre-filtered to reduce computational burden. The framework uses a GCN backbone with two layers, a projection head, and InfoNCE contrastive loss. EPAGCL applies edge dropping to both views and edge adding to only one view, maximizing augmentation diversity while maintaining stability.

## Key Results
- EPAGCL outperforms existing GCL methods, achieving up to 0.61% accuracy improvement over GCA
- The method demonstrates better stability with lower standard deviation across runs compared to baseline methods
- EPAGCL is efficient in terms of memory and computation time, scaling to large datasets like ogbn-arxiv
- Theoretical analysis shows that dropping edges reduces EPR while adding edges often increases it

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Edge dropping maintains lower Error Passing Rate (EPR) compared to edge adding because dropping preserves correct message passing more reliably.
- Mechanism: Theorem 3.5 shows that EPR change is proportional to αᵢ,ⱼ, where αᵢ,ⱼ = 2/√(dᵢ+1)(dⱼ+1). Dropping an edge between same-class nodes reduces EPR by k·αᵢ,ⱼ, while adding between different-class nodes increases EPR by (1-k)·αᵢ,ⱼ.
- Core assumption: The EPR of the graph is less than 0.5 (k < 0.5) and degrees satisfy dmax ≤ 4dmin - 1.
- Evidence anchors:
  - [abstract]: "dropping edges reduces EPR while adding edges often increases it"
  - [section 3.3]: Theorem 3.5 formalizes the proportional relationship
  - [corpus]: Weak - corpus papers discuss GCL augmentations but don't mention EPR metric
- Break condition: If graph becomes too sparse, dropping edges may degrade performance; if EPR ≥ 0.5, adding edges could become more beneficial.

### Mechanism 2
- Claim: Adaptive edge selection based on αᵢ,ⱼ weights ensures stability by prioritizing perturbations with minimal EPR impact.
- Mechanism: Algorithm 1 computes weights wd (for dropping) and wa (for adding) using αᵢ,ⱼ values. Edges with higher αᵢ,ⱼ (larger degree product) are less likely to be dropped or added, maintaining lower EPR.
- Core assumption: Edge importance correlates with αᵢ,ⱼ magnitude, and maintaining EPR stability improves contrastive learning performance.
- Evidence anchors:
  - [section 3.4]: "the possibility is high if the magnitude of effect is relatively low"
  - [section 3.3]: αᵢ,ⱼ formula derivation shows inverse relationship with EPR change
  - [corpus]: Weak - no corpus evidence of αᵢ,ⱼ-based augmentation
- Break condition: If degree distribution becomes highly skewed, αᵢ,ⱼ may not accurately reflect importance.

### Mechanism 3
- Claim: Pre-filtering high-degree nodes reduces computational burden while preserving augmentation quality.
- Mechanism: Line 5 in Algorithm 1 selects vertices of top √(2l) degrees (where l = |E|), limiting candidate edges to O(l) instead of O(N²).
- Core assumption: High-degree nodes contribute most to augmentation diversity and EPR sensitivity.
- Evidence anchors:
  - [section 3.4]: "nodes can be pre-screened to reduce the memory and time burden"
  - [section 4.4]: Table 3 shows EPAGCL scales to ogbn-arxiv with reasonable preprocessing time
  - [corpus]: Weak - no corpus evidence of degree-based node filtering for GCL
- Break condition: If graph has uniform degree distribution, filtering may remove important nodes.

## Foundational Learning

- Concept: Graph Neural Networks (GNNs) and message passing
  - Why needed here: EPR measures "wrongly passed messages" during GNN aggregation, so understanding GNN message passing is essential
  - Quick check question: How does the adjacency matrix A relate to message passing in GCN?

- Concept: Contrastive learning objectives (InfoNCE)
  - Why needed here: EPAGCL uses InfoNCE-like loss to contrast positive pairs from different views
  - Quick check question: What is the difference between positive and negative pairs in InfoNCE?

- Concept: Graph augmentation strategies
  - Why needed here: EPAGCL specifically uses edge perturbation (adding/dropping) as augmentation method
  - Quick check question: Why is edge dropping more commonly used than edge adding in GCL?

## Architecture Onboarding

- Component map: Graph preprocessing -> Augmentation module -> GNN backbone -> Projection head -> Contrastive loss
- Critical path: Preprocess → Augment (edge perturbation + feature mask) → GNN forward pass → Projection head → Compute InfoNCE loss → Update parameters
- Design tradeoffs: Using edge dropping on both views provides stability (Theorem 3.5) but edge adding to only one view maximizes augmentation diversity. Feature masking is simple but may lose information.
- Failure signatures: 
  - High standard deviation across runs indicates unstable augmentation
  - Out-of-memory during preprocessing suggests degree filtering threshold too low
  - Performance worse than GCA suggests αᵢ,ⱼ weighting not capturing edge importance correctly
- First 3 experiments:
  1. Run EPAGCL on Cora with default hyperparameters and verify accuracy > 86%
  2. Compare EPAGCL vs GCA on same dataset with identical hyperparameters to confirm 0.61% improvement
  3. Test degree filtering by varying √(2l) threshold and measuring preprocessing time/memory usage

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does Error Passing Rate (EPR) behave in heterogeneous graphs compared to homogeneous graphs, and what modifications might be needed to adapt EPR for heterogeneous graph analysis?
- Basis in paper: [inferred] The paper mentions that EPR is defined to quantify the impact of 1-hop neighbors and suggests that for heterogeneous graphs, bridging nodes might be considered noise unless the receptive field is large enough. They propose defining high-order EPR for better explanations on heterogeneous graphs.
- Why unresolved: The paper only briefly mentions this as a possibility and doesn't provide experimental validation or theoretical analysis for heterogeneous graphs.
- What evidence would resolve it: Experiments comparing EPR behavior in homogeneous vs heterogeneous graphs, and validation of the proposed high-order EPR metric on real heterogeneous datasets.

### Open Question 2
- Question: What is the relationship between the initial graph density and the effectiveness of edge-adding augmentation strategies, particularly when the graph is already sparse?
- Basis in paper: [explicit] The paper states "As for graphs with a significant number of nodes, thanks to the nodes filtering steps (line 5 in Algorithm 1), the computation is greatly accelerated and can be finished within an acceptable timeframe" and mentions that edge dropping may not work when the graph is too sparse.
- Why unresolved: While the paper acknowledges this limitation, it doesn't provide a quantitative analysis of how graph density affects the performance of different augmentation strategies or offer solutions for extremely sparse graphs.
- What evidence would resolve it: Systematic experiments varying initial graph density across multiple datasets, and analysis of how different augmentation strategies (edge adding vs dropping) perform at different density levels.

### Open Question 3
- Question: How does the choice of the assumption that "constantly k of all the message passed in is error message" affect the theoretical conclusions and practical performance of the EPAGCL method?
- Basis in paper: [explicit] The paper introduces Assumption 3.4 stating "For each node in the graph G, constantly k of all the message passed in is error message, i.e. Mwp,i/Mi = k, ∀i ∈ {1, 2, · · · , N}" and uses this assumption to derive key equations (5) and (6).
- Why unresolved: The paper doesn't validate this assumption empirically or explore how sensitive the results are to variations in k, which is a critical parameter in their theoretical framework.
- What evidence would resolve it: Empirical validation of the assumption across different graph types and datasets, sensitivity analysis showing how varying k affects both theoretical predictions and actual performance, and potentially alternative assumptions that might better capture real-world graph characteristics.

## Limitations
- EPR metric assumes access to ground truth node classes for error edge identification, which is not available during self-supervised training
- Theoretical bounds depend on specific graph properties (degree distribution, EPR threshold) that may not generalize
- No ablation study isolating the impact of αᵢ,ⱼ-based probability selection versus random selection

## Confidence
- Theoretical analysis confidence: Medium - relies on assumptions that may not hold in practice
- Experimental results confidence: Medium-High - demonstrates improvements across multiple datasets
- Practical applicability confidence: Medium - efficiency shown but scalability to extremely large graphs not fully tested

## Next Checks
1. Implement an ablation study comparing EPAGCL with random edge selection probabilities versus the αᵢ,ⱼ-based approach to isolate the benefit of adaptive selection
2. Test EPAGCL on synthetic graphs with controlled EPR values to verify the relationship between EPR reduction and performance improvement
3. Evaluate EPAGCL's sensitivity to the degree filtering threshold (√(2l)) by measuring performance and efficiency across a range of values