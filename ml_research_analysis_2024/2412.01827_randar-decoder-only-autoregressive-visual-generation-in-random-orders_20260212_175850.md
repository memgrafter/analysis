---
ver: rpa2
title: 'RandAR: Decoder-only Autoregressive Visual Generation in Random Orders'
arxiv_id: '2412.01827'
source_url: https://arxiv.org/abs/2412.01827
tags:
- randar
- image
- tokens
- generation
- token
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces RandAR, a decoder-only autoregressive model
  for visual generation that operates in arbitrary token orders. By inserting a "position
  instruction token" before each image token to represent spatial location, RandAR
  removes the fixed raster-order inductive bias of previous models.
---

# RandAR: Decoder-only Autoregressive Visual Generation in Random Orders

## Quick Facts
- **arXiv ID**: 2412.01827
- **Source URL**: https://arxiv.org/abs/2412.01827
- **Reference count**: 40
- **Key outcome**: Decoder-only autoregressive model achieves 2.5x inference acceleration via parallel decoding while maintaining image quality, enabling zero-shot inpainting/outpainting and resolution extrapolation from 256x256 to 512x512.

## Executive Summary
RandAR introduces a novel decoder-only autoregressive model for visual generation that operates in arbitrary token orders rather than fixed raster sequences. The key innovation is inserting position instruction tokens before each image token to represent spatial location, enabling the model to predict tokens in any order while maintaining spatial awareness. This design removes the raster-order inductive bias of previous models, unlocking new capabilities including parallel decoding acceleration (2.5x speedup), zero-shot inpainting/outpainting, and resolution extrapolation. RandAR achieves comparable generation quality to raster-order counterparts on ImageNet with FID scores around 2.2-2.5, while demonstrating enhanced detail in high-resolution generation through spatial contextual guidance.

## Method Summary
RandAR is a decoder-only autoregressive transformer that generates images by predicting tokens in random orders. The model inserts a shared learnable position instruction token before each image token, where the instruction token is rotated with 2D coordinates using 2D-RoPE to represent spatial location. During training, token sequences are randomly permuted and the model learns to predict tokens at any location based on previously generated tokens. At inference, this flexibility enables parallel decoding where multiple tokens can be predicted simultaneously in a single forward pass, achieving 2.5x acceleration without quality loss. The model is trained on ImageNet 256x256 images for 300 epochs using AdamW optimizer with constant learning rate for 250 epochs followed by linear decay.

## Key Results
- Achieves 2.5× inference acceleration through parallel decoding without sacrificing generation quality
- Demonstrates zero-shot inpainting, outpainting, and resolution extrapolation capabilities
- Generates high-quality images with FID scores around 2.2-2.5 on ImageNet 256×256
- Enables bi-directional feature extraction using a causal transformer architecture

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Position instruction tokens enable random order generation by providing spatial context for each next token prediction.
- Mechanism: A shared learnable embedding is "rotated" with 2D coordinates of the next image token using 2D RoPE, creating a position instruction token that precedes each image token in the sequence. This allows the model to predict tokens in arbitrary orders while maintaining spatial awareness.
- Core assumption: The model can learn to interpret these position instruction tokens as spatial location indicators regardless of generation order.
- Evidence anchors:
  - [abstract]: "Our essential design enables random order by inserting a 'position instruction token' before each image token to be predicted, representing the spatial location of the next image token."
  - [section]: "For each position, we use a shared learnable embedding e 'rotated' with the 2D coordinates of the next image token to be predicted, following 2D-RoPE [44]."
- Break condition: If the model cannot learn to associate position instruction tokens with spatial locations, or if the 2D RoPE rotation fails to provide meaningful spatial encoding across different orders.

### Mechanism 2
- Claim: Random order training enables parallel decoding acceleration by 2.5x without quality loss.
- Mechanism: Training on randomly permuted token sequences teaches the model to predict tokens at any location based on previously generated tokens. This flexibility allows multiple tokens to be predicted simultaneously in a single forward pass during inference, reducing the number of decoding steps.
- Core assumption: The model learns non-local correlations sufficiently well during random order training to support parallel predictions.
- Evidence anchors:
  - [abstract]: "For the efficiency bottleneck of AR models, RandAR adopts parallel decoding with KV-Cache at inference time, enjoying 2.5 × acceleration without sacrificing generation quality."
  - [section]: "RandAR inherently supports parallel decoding with no post-training required, accelerating sampling speed by 2.5 × without compromising quality."
- Break condition: If parallel decoding introduces significant quality degradation beyond 88 steps, or if the KV cache becomes a bottleneck for longer sequences.

### Mechanism 3
- Claim: Random order training enables zero-shot capabilities like inpainting, outpainting, and resolution extrapolation.
- Mechanism: By removing the fixed raster-order inductive bias, the model learns to aggregate contextual information from any part of the image. This flexibility allows it to handle tasks requiring bidirectional context without additional training.
- Core assumption: The model can effectively use non-local context learned during random order training for these specialized tasks.
- Evidence anchors:
  - [abstract]: "Additionally, RandAR supports inpainting, outpainting and resolution extrapolation in a zero-shot manner."
  - [section]: "Random-order prediction provides the decoder-only model with a level of flexibility exceeding that of raster-order models, thereby unlocking new applications."
- Break condition: If these zero-shot capabilities fail when applied to domains significantly different from ImageNet training data.

## Foundational Learning

- Concept: Autoregressive modeling and next-token prediction
  - Why needed here: RandAR is fundamentally an autoregressive model that predicts the next image token based on previously generated tokens.
  - Quick check question: Can you explain how the probability decomposition p(x) = ∏p(xn|x1,...,xn-1) enables sequential generation?

- Concept: Transformer architecture with causal attention
  - Why needed here: RandAR uses a decoder-only transformer with causal attention to ensure predictions only depend on past tokens, maintaining the autoregressive property.
  - Quick check question: What is the key difference between causal attention and bidirectional attention in transformers?

- Concept: Positional encoding (2D RoPE)
  - Why needed here: 2D RoPE provides relative positional information that is crucial for the position instruction tokens to represent spatial locations.
  - Quick check question: How does 2D RoPE differ from standard 1D RoPE, and why is this important for image generation?

## Architecture Onboarding

- Component map: Input (Image tokens + position instruction tokens) -> Decoder-only transformer with causal attention and 2D RoPE -> Output (Predicted next image token)

- Critical path:
  1. Tokenize image to 2D tokens (16x16 for 256x256)
  2. Randomly permute token sequence and insert position instruction tokens
  3. Pass through decoder-only transformer with causal attention
  4. Predict next image token based on position instruction token and previous context
  5. Repeat until all tokens are generated

- Design tradeoffs:
  - Single shared position instruction embedding vs. unique embeddings per location: Shared embedding reduces parameters but may limit fine-grained spatial encoding.
  - Random order vs. structured orders: Random order provides maximum flexibility but is harder to learn than structured orders.
  - Parallel decoding step size: Larger step sizes increase speed but may reduce quality.

- Failure signatures:
  - Training instability or slow convergence: May indicate the model struggles to learn from random orders.
  - Quality degradation with parallel decoding: Suggests the model cannot effectively use non-local context for simultaneous predictions.
  - Poor performance on zero-shot tasks: Indicates the random order training didn't sufficiently teach bidirectional context modeling.

- First 3 experiments:
  1. Compare FID scores between raster-order and random-order models with identical architectures and training setups.
  2. Test parallel decoding acceleration by measuring latency and quality at different step sizes (e.g., 256, 128, 88, 48 steps).
  3. Evaluate zero-shot inpainting capability by masking random regions and measuring reconstruction quality compared to a raster-order baseline.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can RandAR's parallel decoding capabilities be extended to multi-modal generation tasks (text-to-image, video generation) while maintaining quality and efficiency gains?
- Basis in paper: [explicit] The paper mentions the trend of joint visual language generation with decoder-only transformers and suggests RandAR could be scaled up from ImageNet to image-text and image-video datasets
- Why unresolved: The current RandAR implementation focuses solely on image generation. The paper acknowledges this as a future direction but doesn't provide empirical evidence of how parallel decoding would perform in multi-modal settings
- What evidence would resolve it: Comparative experiments showing RandAR's parallel decoding performance on text-to-image and video generation benchmarks against existing multi-modal AR models

### Open Question 2
- Question: What is the optimal training strategy for RandAR to achieve data efficiency comparable to raster-order models, given the (N!) complexity of random order permutations?
- Basis in paper: [explicit] The paper explicitly states "a meaningful future investigation would be improving the data efficiency of training a random-order model" and notes that learning from a much larger number of orders is significantly more challenging
- Why unresolved: The paper demonstrates RandAR achieves comparable performance despite this challenge, but doesn't explore whether different training strategies (curriculum learning, better sampling of orders, etc.) could reduce the training burden
- What evidence would resolve it: Ablation studies comparing various training strategies on data efficiency metrics while maintaining generation quality

### Open Question 3
- Question: How does RandAR's random order generation affect long-term coherence in outpainting and resolution extrapolation tasks, particularly for complex scenes with multiple objects?
- Basis in paper: [inferred] While the paper demonstrates RandAR's capabilities in outpainting and resolution extrapolation, the results show some struggles with intricate structures and boundaries (e.g., space shuttle example)
- Why unresolved: The paper provides qualitative examples but lacks quantitative analysis of coherence metrics for complex scenes across different extrapolation ratios and orders
- What evidence would resolve it: Quantitative metrics measuring spatial consistency and object coherence in outpainting/extrapolation tasks, along with user studies comparing RandAR to sliding-window approaches on complex scenes

### Open Question 4
- Question: What is the relationship between the layer depth in RandAR and its ability to extract bi-directional contextual features, and can this be optimized for different downstream tasks?
- Basis in paper: [explicit] The paper shows that RandAR's 24th layer performs best for feature encoding and that bi-directional context extraction works differently across layers, but doesn't systematically explore this relationship
- Why unresolved: While the paper identifies the 24th layer as optimal for their 775M model, it doesn't investigate how this varies with model size, task type, or whether layer-wise optimization could improve performance
- What evidence would resolve it: Systematic analysis of feature quality across layers for different model sizes and tasks, potentially leading to task-specific layer selection or attention mechanisms

## Limitations

- The paper lacks systematic evaluation of parallel decoding across different step sizes to validate the claimed 2.5× acceleration maintains consistent quality.
- Zero-shot capabilities are demonstrated qualitatively but not quantitatively compared to trained baselines or evaluated on diverse datasets.
- The paper doesn't address potential training instability from random order sampling or KV cache bottlenecks for very long sequences.

## Confidence

- **High confidence**: The core mechanism of using position instruction tokens with 2D RoPE for spatial encoding is well-supported by the architectural description and consistent with established transformer practices.
- **Medium confidence**: The claims about 2.5× acceleration and zero-shot capabilities are supported by experimental results but lack comprehensive validation and systematic comparison.
- **Low confidence**: The scalability claims to higher resolutions and the effectiveness of spatial contextual guidance are demonstrated with limited quantitative evidence and lack ablation studies.

## Next Checks

1. **Systematic parallel decoding evaluation**: Measure FID scores and sampling latency across multiple decoding step sizes (256, 128, 88, 48, 32 steps) with direct comparison to a raster-order model trained with identical architecture and hyperparameters.

2. **Zero-shot capability benchmarking**: Implement a raster-order baseline model and evaluate both models on zero-shot inpainting/outpainting tasks using standardized metrics like MSE, SSIM, and LPIPS across multiple datasets.

3. **Position instruction token ablation**: Train three variants (full RandAR, variant without position instruction tokens, raster-order baseline) and compare FID scores, convergence speed, and zero-shot task performance to quantify the contribution of the position instruction token mechanism.