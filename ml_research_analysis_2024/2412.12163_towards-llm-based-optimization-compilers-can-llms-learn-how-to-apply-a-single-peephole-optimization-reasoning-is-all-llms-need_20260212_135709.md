---
ver: rpa2
title: Towards LLM-based optimization compilers. Can LLMs learn how to apply a single
  peephole optimization? Reasoning is all LLMs need!
arxiv_id: '2412.12163'
source_url: https://arxiv.org/abs/2412.12163
tags:
- code
- llms
- llama2
- basic
- optimization
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigates the capability of Large Language Models
  (LLMs) to perform peephole optimization on AArch64 assembly code. A 7B-parameter
  Llama2 model is fine-tuned using 100,000 LLVM-generated basic block samples, and
  its performance is compared to advanced OpenAI models GPT-4o and GPT-o1 (preview).
---

# Towards LLM-based optimization compilers. Can LLMs learn how to apply a single peephole optimization? Reasoning is all LLMs need!

## Quick Facts
- arXiv ID: 2412.12163
- Source URL: https://arxiv.org/abs/2412.12163
- Authors: Xiangxin Fang; Lev Mukhanov
- Reference count: 40
- Key outcome: GPT-o1 with chain-of-thought reasoning significantly outperforms fine-tuned Llama2 and GPT-4o on AArch64 assembly optimization tasks

## Executive Summary
This study investigates whether Large Language Models can perform peephole optimization on AArch64 assembly code. The researchers fine-tune a 7B Llama2 model on 100,000 LLVM-generated basic block samples and compare its performance against advanced OpenAI models GPT-4o and GPT-o1 (preview). Using metrics including BLEU, Exact Match Rate, Syntactic Accuracy, and IO Accuracy across 24,000 test samples, the study finds that GPT-o1's chain-of-thought reasoning mechanism enables it to systematically analyze and optimize code, outperforming other models by significant margins. The research highlights the importance of reasoning capabilities over fine-tuning for code optimization tasks and suggests rethinking traditional evaluation metrics for LLM-based code generation.

## Method Summary
The researchers generate 100,000 LLVM assembly basic blocks using the instcombine optimization pass, then fine-tune a 7B Llama2 model using QLoRA with a learning rate of 2e-4 for 6 epochs. They evaluate three models (fine-tuned Llama2, GPT-4o, and GPT-o1) on 24,000 test samples from competitive programming datasets, measuring performance using BLEU score, Exact Match Rate (EMR), Syntactic Accuracy, and IO Accuracy. The study investigates the impact of inference time and reasoning steps on optimization quality, finding that GPT-o1's chain-of-thought mechanism significantly improves performance compared to direct optimization approaches.

## Key Results
- GPT-o1 achieves 14% higher EMR, 24% higher Syntactic Accuracy, and 30% higher IO Accuracy compared to GPT-4o
- Fine-tuning Llama2 improves BLEU and EMR but reduces generalization, leading to higher compilation errors
- Chain-of-thought reasoning in GPT-o1 enables systematic code analysis through intermediate pseudo-code generation
- Increasing inference steps and time improves GPT-o1's optimization quality

## Why This Works (Mechanism)

### Mechanism 1
- Claim: GPT-o1's chain-of-thought reasoning enables systematic analysis of assembly code transformations.
- Mechanism: The model breaks down optimization into multiple inference steps, building intermediate pseudo-code representations to guide correct transformations.
- Core assumption: Complex reasoning tasks require intermediate symbolic representations rather than direct pattern matching.
- Evidence anchors:
  - [abstract] "Our findings indicate that this advantage is largely due to the chain-of-thought reasoning implemented in GPT-o1."
  - [section] "Figure 8 shows the chain-of-thought applied by GPT-o1 to optimize the presented above code sample. We see that it is trying to understand the overall purpose of the provided code sample and examine the function flow."
  - [corpus] Weak - no corpus evidence specifically on chain-of-thought effectiveness.
- Break condition: If intermediate reasoning steps don't improve with more inference time/steps (Figure 7 shows performance improves with steps).

### Mechanism 2
- Claim: Fine-tuning conventional LLMs on compiler-specific data reduces generalization capability.
- Mechanism: The model adjusts weights to mimic specific compiler outputs rather than learning general optimization principles.
- Core assumption: Overfitting to training distribution harms performance on diverse real-world code.
- Evidence anchors:
  - [abstract] "Our findings indicate that this advantage is largely due to the chain-of-thought reasoning implemented in GPT-o1."
  - [section] "Finding: Based on these observations, we conclude that fine-tuning tends to adjust LLMs to completely mimic the behavior of compilers, which may negatively affect their generalization capabilities and overall performance."
  - [corpus] Weak - corpus doesn't provide evidence about fine-tuning vs generalization tradeoff.
- Break condition: If fine-tuned models showed better generalization across diverse datasets.

### Mechanism 3
- Claim: Conventional LLMs have fundamental limitations in generating syntactically correct assembly opcodes.
- Mechanism: The probabilistic nature of token generation leads to non-existent instruction names.
- Core assumption: Current LLM architectures cannot guarantee syntactic validity for low-level code generation.
- Evidence anchors:
  - [abstract] "Our findings indicate that this advantage is largely due to the chain-of-thought reasoning implemented in GPT-o1."
  - [section] "Fundamental limitation of conventional LLMs for code generation. Overall, our findings indicate that no instructions have an error probability of zero a, and the vast majority of errors are due to incorrectly generated instruction names."
  - [corpus] Weak - corpus evidence doesn't directly address opcode generation errors.
- Break condition: If reasoning-enhanced models eliminate opcode errors entirely.

## Foundational Learning

- Concept: Assembly language syntax and semantics
  - Why needed here: Understanding AArch64 instruction formats, register naming conventions, and addressing modes is essential for evaluating LLM optimization outputs.
  - Quick check question: What's the difference between a 32-bit register (w0) and 64-bit register (x0) in AArch64?

- Concept: Compiler optimization techniques
  - Why needed here: Understanding peephole optimization, basic blocks, and common transformation patterns helps interpret why LLMs succeed or fail at optimization.
  - Quick check question: Why would a peephole optimizer remove a store followed by a load from the same stack location?

- Concept: Chain-of-thought reasoning
  - Why needed here: The study's key finding is that GPT-o1's reasoning mechanism enables better optimization through intermediate steps.
  - Quick check question: How does generating pseudo-code help an LLM optimize assembly instructions?

## Architecture Onboarding

- Component map: LLVM compiler -> AArch64 assembly generation -> Basic block extraction -> LLM prompt formatting
- Critical path: Code generation -> Basic block extraction -> LLM optimization -> Compilation verification -> Output correctness check
- Design tradeoffs:
  - Fine-tuning vs generalization: Training on specific compiler outputs improves BLEU/EMR but reduces ability to handle diverse code patterns
  - Model size vs reasoning: Larger models (GPT-o1) with reasoning mechanisms outperform smaller fine-tuned models
  - Inference time vs quality: GPT-o1's chain-of-thought requires more steps/time but produces better results
- Failure signatures:
  - High opcode error rates indicate fundamental LLM limitations
  - BLEU/EMR improvement without Syntactic/IO Accuracy improvement suggests overfitting
  - Compilation failures indicate lack of syntactic understanding
- First 3 experiments:
  1. Test baseline Llama2 on simple peephole optimizations (constant folding, null sequence removal) to establish baseline error rates
  2. Compare GPT-4o vs GPT-o1 on identical optimization tasks to measure reasoning impact
  3. Vary GPT-o1 inference steps/time to find optimal tradeoff between performance and computational cost

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do reasoning capabilities in LLMs affect their ability to optimize code across multiple basic blocks, where dependencies exist between instructions in different blocks?
- Basis in paper: [inferred] The paper mentions that LLMs optimize each basic block separately and discusses limitations related to not seeing the entire program representation.
- Why unresolved: The study focuses on peephole optimization within individual basic blocks, leaving open the question of how LLMs would handle optimizations that require reasoning across multiple basic blocks.
- What evidence would resolve it: Experiments comparing LLM performance on multi-block optimizations versus single-block optimizations, with metrics measuring correctness across block boundaries.

### Open Question 2
- Question: What is the relationship between the number of inference steps in chain-of-thought reasoning and the quality of code optimization, and is there a point of diminishing returns?
- Basis in paper: [explicit] The paper investigates the impact of inference time and number of inference steps on GPT-o1's performance, finding that optimal code generation increases with both factors.
- Why unresolved: While the paper shows that more inference steps improve performance, it doesn't determine the optimal number of steps or whether there's a point where additional steps don't provide meaningful improvements.
- What evidence would resolve it: Systematic experiments varying the number of inference steps while measuring optimization quality and computational efficiency to identify the optimal balance.

### Open Question 3
- Question: How do different quantization strategies affect the performance of fine-tuned LLMs in code optimization tasks, and is there an optimal balance between model size and accuracy?
- Basis in paper: [explicit] The paper mentions using QLoRA for efficient fine-tuning and discusses the trade-offs between model size and performance.
- Why unresolved: The study uses a specific quantization approach but doesn't explore how different quantization strategies or levels might affect optimization accuracy and generalization.
- What evidence would resolve it: Comparative experiments testing various quantization methods and levels on code optimization tasks, measuring both performance and computational efficiency.

## Limitations
- The study uses synthetic LLVM-generated assembly samples for training and competitive programming datasets for testing, which may not fully represent real-world compiler optimization scenarios
- The research focuses on peephole optimization within individual basic blocks, leaving open questions about cross-block optimizations
- BLEU and Exact Match Rate metrics may be misleading for code optimization tasks, yet remain primary evaluation measures

## Confidence
- High Confidence: The fundamental finding that GPT-o1 with reasoning capabilities outperforms fine-tuned Llama2 and GPT-4o in assembly optimization tasks
- Medium Confidence: The claim that chain-of-thought reasoning is the primary mechanism for GPT-o1's advantage
- Low Confidence: The assertion that fine-tuning inherently reduces generalization capabilities

## Next Checks
1. Conduct ablation studies by systematically removing GPT-o1's intermediate reasoning steps to quantify their exact contribution to performance improvements
2. Test the optimized models on assembly code generated by different compilers (GCC, Clang) to assess true generalization beyond LLVM-specific patterns
3. Perform detailed analysis of opcode generation errors to determine if they stem from training data distribution or fundamental LLM limitations in low-level code generation