---
ver: rpa2
title: 'SkipViT: Speeding Up Vision Transformers with a Token-Level Skip Connection'
arxiv_id: '2401.15293'
source_url: https://arxiv.org/abs/2401.15293
tags:
- tokens
- attention
- token
- dropping
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: SkipViT is a method to speed up training of vision transformers
  by dropping less important tokens and reusing them later via a skip connection.
  It uses attention scores from the [CLS] token to identify unimportant patches and
  removes them from intermediate layers.
---

# SkipViT: Speeding Up Vision Transformers with a Token-Level Skip Connection

## Quick Facts
- **arXiv ID**: 2401.15293
- **Source URL**: https://arxiv.org/abs/2401.15293
- **Reference count**: 3
- **Primary result**: 55% of tokens can be dropped while achieving 13.23% faster training throughput with negligible loss in Top-1 accuracy on Huawei Ascend910A

## Executive Summary
SkipViT introduces a method to accelerate Vision Transformer training by selectively dropping less important tokens based on attention scores from the [CLS] token, while maintaining accuracy through a skip connection that reintroduces dropped tokens in later layers. The approach achieves significant computational savings by avoiding redundant interactions between unimportant tokens while preserving the ability to incorporate potentially useful background information. Experiments on ViT-small with ImageNet1K demonstrate that 55% of tokens can be dropped, resulting in 13.23% faster training throughput with minimal impact on Top-1 accuracy.

## Method Summary
SkipViT optimizes Vision Transformer training by identifying and dropping unimportant tokens using attention scores from the [CLS] token, then reintroducing them via skip connections in later layers. The method separates tokens into important and unimportant groups, routing unimportant tokens through a low-cost computational path during intermediate layers. This approach maintains the original model architecture while reducing unnecessary computations, achieving a favorable trade-off between training speed and accuracy preservation.

## Key Results
- 55% of tokens can be dropped from intermediate layers while maintaining accuracy
- 13.23% faster training throughput on Huawei Ascend910A
- Negligible loss in Top-1 accuracy on ImageNet1K test set

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Attention scores from the [CLS] token identify unimportant image patches for removal
- **Mechanism**: The first row of the attention score matrix represents how much each token contributes to forming the new [CLS] token. By averaging attention scores across all heads and selecting tokens with low scores, the method identifies patches that contribute minimally to classification
- **Core assumption**: The [CLS] token's attention distribution reliably indicates patch importance for the final classification task
- **Evidence anchors**:
  - [abstract] "Our method does not add any parameters to the ViT model and aims to find the best trade-off between training throughput and achieving a 0% loss in the Top-1 accuracy of the final model"
  - [section] "Based on the attention Eq. 1, we can say that each row i in the attention score matrix are coefficients by which other tokens will attend in forming the new i token at the attention unit output"
  - [corpus] Weak evidence - the corpus neighbors discuss skip connections but don't directly address attention-based token importance detection
- **Break condition**: If the [CLS] token's attention distribution doesn't correlate with classification-relevant information, or if important patches are consistently misclassified as unimportant

### Mechanism 2
- **Claim**: Skipping unimportant tokens through a separate computational path reduces redundant computation
- **Mechanism**: By routing unimportant tokens through a low-cost path and dropping them from intermediate layers, the method avoids expensive multi-head self-attention and feed-forward network computations on irrelevant information
- **Core assumption**: The computational savings from skipping tokens outweighs any overhead from managing the skip connection
- **Evidence anchors**:
  - [abstract] "In this work, we propose a method to optimize the amount of unnecessary interactions between unimportant tokens by separating and sending them through a different low-cost computational path"
  - [section] "Our method does not add any parameters to the ViT model and aims to find the best trade-off between training throughput and achieving a 0% loss in the Top-1 accuracy of the final model"
  - [corpus] Weak evidence - corpus neighbors discuss skip connections but not specifically for computational efficiency through token dropping
- **Break condition**: If the overhead of managing token skipping exceeds the computational savings, or if the low-cost path becomes computationally expensive

### Mechanism 3
- **Claim**: Reintroducing dropped tokens via skip connection maintains classification accuracy
- **Mechanism**: The skip connection returns previously dropped tokens to their original position in later layers, allowing the model to incorporate potentially useful background information that might have been discarded too early
- **Core assumption**: Background and contextual information in "unimportant" tokens can still contribute to classification when reintroduced at appropriate layers
- **Evidence anchors**:
  - [abstract] "The dropped tokens are reintroduced in later layers to maintain accuracy"
  - [section] "We propose the use of a skip connection for the tokens that would otherwise be discarded. This approach selectively excludes these tokens from contributing to certain transformer layers within the model, while still incorporating them in the final layers"
  - [corpus] Weak evidence - corpus neighbors discuss skip connections generally but not specifically for token reintroduction to maintain accuracy
- **Break condition**: If reintroduced tokens arrive too late to be useful, or if their reintroduction disrupts the learned representations in later layers

## Foundational Learning

- **Concept**: Multi-head self-attention mechanism
  - **Why needed here**: Understanding how attention scores are computed and what they represent is crucial for implementing the token importance detection
  - **Quick check question**: How is the attention score matrix dimensioned relative to the number of input tokens, and what does each row represent?

- **Concept**: Vision Transformer tokenization process
  - **Why needed here**: The method operates on image patches converted to tokens, so understanding this conversion is essential
  - **Quick check question**: How are image patches converted to token embeddings, and what is the relationship between the number of patches and the number of tokens?

- **Concept**: Skip connections in neural networks
  - **Why needed here**: The proposed method uses skip connections to reintroduce dropped tokens, requiring understanding of how skip connections work in transformer architectures
  - **Quick check question**: How does a skip connection differ from residual connections, and what are the potential benefits and drawbacks of using skip connections for token reintroduction?

## Architecture Onboarding

- **Component map**: Input image → Patch tokenization → Multi-head self-attention with [CLS] token → Attention score computation → Token importance filtering → Computational path routing (important vs unimportant tokens) → Intermediate transformer layers → Skip connection reintroduction → Final classification
- **Critical path**: Token importance detection → Token routing → Skip connection management → Accuracy maintenance
- **Design tradeoffs**: Computational speedup vs. potential accuracy loss from dropping tokens; complexity of managing multiple token streams vs. benefits of targeted computation reduction
- **Failure signatures**: Accuracy degradation beyond acceptable thresholds; training instability; unexpected computational overhead; skip connection timing issues
- **First 3 experiments**:
  1. Implement token importance detection using [CLS] attention scores on a small dataset, verify that identified "unimportant" tokens are indeed less relevant to classification
  2. Add basic token dropping with fused token replacement, measure computational savings and accuracy impact before implementing skip connections
  3. Implement skip connection reintroduction at different layers, systematically evaluate the trade-off between computational speedup and accuracy maintenance

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: How does the effectiveness of SkipViT scale with larger Vision Transformer models and datasets beyond ViT-small and ImageNet1K?
- **Basis in paper**: [explicit] The authors state: "Due to resource constraints we where only able to apply our experiments on the small version of ViT using the Imagenet1K dataset. This method shows promising results on the current setup, However it is limited by the size of the model and dataset and should be extended to the larger versions of ViT."
- **Why unresolved**: The paper only reports results on ViT-small, so scalability to larger models and datasets remains untested.
- **What evidence would resolve it**: Experiments applying SkipViT to larger ViT variants (e.g., ViT-base, ViT-large) and larger datasets (e.g., ImageNet21K, JFT-300M) showing maintained or improved speedup and accuracy.

### Open Question 2
- **Question**: What is the impact of SkipViT on memory footprint during training, and how does this compare to baseline ViT?
- **Basis in paper**: [explicit] The authors mention: "Another positive aspect of this approach is the reduced memory footprint of the model, which needs to be examined in the future works."
- **Why unresolved**: Memory usage was not measured or reported in the experiments.
- **What evidence would resolve it**: Quantitative measurements of GPU/TPU memory consumption for SkipViT versus baseline ViT during training, showing the reduction in memory footprint.

### Open Question 3
- **Question**: How does the warm-up period affect the quality of token importance detection, and what is the optimal warm-up duration?
- **Basis in paper**: [explicit] The authors observe: "the warm-up epochs are a essential part of our token dropping strategy which helps the model to select a more informative set of tokens to keep."
- **Why unresolved**: The paper only tests one warm-up duration (15 epochs) and does not explore the sensitivity to different warm-up lengths.
- **What evidence would resolve it**: Experiments varying the warm-up duration (e.g., 0, 5, 10, 15, 20 epochs) and measuring the resulting accuracy and token importance detection quality to identify the optimal warm-up period.

## Limitations
- Limited to ViT-small model and ImageNet1K dataset, with uncertain scalability to larger models and datasets
- No ablation studies to isolate the contribution of each mechanism (attention-based detection, computational routing, skip connections)
- Modest computational speedup of 13.23% may not justify the added implementation complexity for some applications

## Confidence
- **High Confidence**: The core observation that attention scores from the [CLS] token can indicate token importance, and the general concept of token dropping for computational efficiency
- **Medium Confidence**: The overall approach of combining token dropping with skip connections to balance speed and accuracy. The 55% token dropping rate achieving acceptable accuracy is empirically demonstrated
- **Low Confidence**: The specific mechanisms by which each component contributes to the overall performance, and whether the method would maintain effectiveness on larger models or different datasets

## Next Checks
1. **Ablation Study on Mechanism Contributions**: Implement and evaluate each proposed mechanism independently - first test attention-based token importance detection alone, then add computational path routing, and finally incorporate skip connection reintroduction. This will quantify the individual contribution of each component to the overall speedup and accuracy.

2. **Generalization Across Model Sizes**: Test SkipViT on larger ViT variants (Base, Large) and different vision transformer architectures (Swin, PVT) to determine if the 55% token dropping rate and computational benefits generalize beyond the small model used in the paper.

3. **Cross-Dataset Performance Validation**: Evaluate the method on multiple datasets beyond ImageNet1K (e.g., CIFAR-100, COCO detection) to assess whether the attention-based importance detection and skip connection reintroduction maintain effectiveness across different visual recognition tasks and data distributions.