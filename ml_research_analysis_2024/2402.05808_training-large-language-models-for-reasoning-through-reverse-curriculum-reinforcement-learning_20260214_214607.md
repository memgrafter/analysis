---
ver: rpa2
title: Training Large Language Models for Reasoning through Reverse Curriculum Reinforcement
  Learning
arxiv_id: '2402.05808'
source_url: https://arxiv.org/abs/2402.05808
tags:
- reasoning
- learning
- arxiv
- training
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces R3, a novel reinforcement learning method
  that improves large language models' reasoning by employing outcome supervision
  to achieve an effect similar to process supervision. R3 works by starting the model's
  exploration from intermediate states of correct demonstrations and gradually sliding
  the starting point backwards, creating a reverse curriculum that provides step-level
  supervision signals.
---

# Training Large Language Models for Reasoning through Reverse Curriculum Reinforcement Learning

## Quick Facts
- arXiv ID: 2402.05808
- Source URL: https://arxiv.org/abs/2402.05808
- Authors: Zhiheng Xi; Wenxiang Chen; Boyang Hong; Senjie Jin; Rui Zheng; Wei He; Yiwen Ding; Shichun Liu; Xin Guo; Junzhe Wang; Honglin Guo; Wei Shen; Xiaoran Fan; Yuhao Zhou; Shihan Dou; Xiao Wang; Xinbo Zhang; Peng Sun; Tao Gui; Qi Zhang; Xuanjing Huang
- Reference count: 40
- Key outcome: R3 improves LLM reasoning performance by 5.4 points over SFT and 4.1 points over RL baselines, achieving 11.4 points improvement on program-based reasoning for GSM8K

## Executive Summary
This paper introduces R3, a novel reinforcement learning method that improves large language models' reasoning by employing outcome supervision to achieve an effect similar to process supervision. R3 works by starting the model's exploration from intermediate states of correct demonstrations and gradually sliding the starting point backwards, creating a reverse curriculum that provides step-level supervision signals. This approach addresses the challenge of sparse rewards in outcome supervision while avoiding the need for extensive human annotations required by process supervision.

## Method Summary
R3 (Learning Reasoning through Reverse Curriculum Reinforcement Learning) is a reinforcement learning method that trains LLMs for reasoning tasks by starting exploration from intermediate states of correct demonstrations. The approach uses a reverse curriculum where the model begins by generating only the final steps of reasoning chains, then progressively moves the starting point backward through the reasoning process. During training, all difficulty levels are mixed together to prevent overfitting to easy patterns. The method employs outcome supervision with optional partial rewards for numeric answers to provide denser signals during training.

## Key Results
- R3 outperforms SFT and RL baselines by 5.4 and 4.1 points on average across eight reasoning tasks
- On program-based reasoning for GSM8K, R3 achieves an average improvement of 11.4 points over SFT and 4.2 points over RL across three backbone models
- Codellama-7B + R3 performs comparably to larger models or closed-source models without requiring extra data

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Starting exploration from intermediate states of correct demonstrations reduces the effective reasoning horizon and provides step-level supervision signals through outcome supervision alone.
- **Mechanism**: By initializing the model's exploration from states sampled from the middle or end of correct demonstrations, the model only needs to generate the remaining steps. This creates a reverse curriculum where each stage presents an easier exploration problem. The outcome supervision reward (correct/incorrect final answer) is then attributed back to the generated actions, providing approximate step-level signals.
- **Core assumption**: The model can effectively learn to complete reasoning chains when given a head start from a correct intermediate state, and this knowledge transfers when the starting point moves backward.
- **Evidence anchors**:
  - [abstract]: "R3 progressively slides the start state of reasoning from a demonstration's end to its beginning, facilitating easier model exploration at all stages. Thus, R3 establishes a step-wise curriculum, allowing outcome supervision to offer step-level signals and precisely pinpoint errors."
  - [section 3.2]: "If N reasoning steps are required to obtain a reward, this reasoning can now be learned in a time that is linear in N, rather than exponential."
  - [corpus]: Weak - No direct evidence found for reverse curriculum effectiveness in LLMs.

### Mechanism 2
- **Claim**: Mixing start states from different difficulty levels during training prevents overfitting to easy patterns and improves generalization.
- **Mechanism**: Instead of training on one difficulty level at a time (staged RL), R3 mixes all difficulty levels together during training. This multi-task learning approach ensures the model doesn't overfit to patterns from early, easier stages and maintains knowledge across all difficulty levels.
- **Core assumption**: Training on a mixture of difficulty levels provides better generalization than sequential stage-by-stage training.
- **Evidence anchors**:
  - [section 3.3]: "Models might overfit to simple patterns presented in the early stages of the curriculum and fail to generalize effectively when the difficulty increases, leading to a degradation of previously acquired knowledge."
  - [section 4.2]: "Staged RL is only a bit better than the RL baseline, possibly due to overfitting and ineffective stage-to-stage adaptation mentioned before."
  - [corpus]: Weak - No direct evidence found for mixed-difficulty training in RL for LLMs.

### Mechanism 3
- **Claim**: Outcome supervision with partial rewards on numeric answers provides denser signals than binary correct/incorrect rewards.
- **Mechanism**: For mathematical reasoning tasks, the reward function gives a partial reward (e.g., 0.1 or 0.2) when the final answer is numeric but incorrect, rather than zero. This provides more frequent feedback during training and helps the model learn faster.
- **Core assumption**: Partial rewards on numeric answers help the model learn better than pure binary rewards.
- **Evidence anchors**:
  - [section 3.4]: "We employ partial reward ϵ (e.g., ϵ = 0.1) on mathematical reasoning tasks when answer can be extracted and of numeric type to make the reward denser."
  - [section 5.1]: "If we set a small partial reward ϵ or remove it, R3 obtains a lower performance yet it still outperforms RL and SFT. On the other hand, if we set ϵ to a bigger value 0.3, the performance also drops as too large partial reward might lead the model to settle for obtaining simple rewards (outputting numbers) rather than striving for the correct answer."
  - [corpus]: Weak - No direct evidence found for partial rewards in LLM reasoning.

## Foundational Learning

- **Concept**: Reinforcement Learning with sparse rewards
  - Why needed here: The core challenge is that outcome supervision provides rewards only at the end of reasoning chains, making it difficult to identify which actions led to success or failure.
  - Quick check question: Can you explain why sparse rewards make it difficult to train models for multi-step reasoning tasks?

- **Concept**: Curriculum learning
  - Why needed here: The reverse curriculum approach starts from easy problems (near the goal) and gradually increases difficulty, which helps the model learn more effectively than starting from scratch.
  - Quick check question: How does starting from intermediate states of correct demonstrations make the exploration problem easier for the model?

- **Concept**: Multi-task learning
  - Why needed here: Mixing different difficulty levels during training prevents overfitting and improves generalization, similar to how multi-task learning works.
  - Quick check question: Why might training on a mixture of difficulty levels be better than sequential stage-by-stage training?

## Architecture Onboarding

- **Component map**: Demonstration → Intermediate state sampling → Rollout generation → Reward calculation → Policy update → Evaluation

- **Critical path**: Demonstration → Intermediate state sampling → Rollout generation → Reward calculation → Policy update → Evaluation

- **Design tradeoffs**:
  - Number of stages (M) vs. training efficiency: More stages provide finer-grained curriculum but increase training cost
  - Partial reward magnitude vs. learning behavior: Too small provides insufficient signal, too large encourages suboptimal solutions
  - KL penalty coefficient (β) vs. exploration freedom: Higher values constrain exploration but prevent collapse

- **Failure signatures**:
  - Performance plateaus or decreases during training (overfitting to easy patterns)
  - High variance in training rewards (unstable exploration)
  - Low reward density (sparse signals not providing enough guidance)

- **First 3 experiments**:
  1. Baseline comparison: Run R3 vs. vanilla RL vs. SFT on a simple reasoning task (e.g., GSM8K) to verify the core improvement mechanism
  2. Stage mixing ablation: Compare R3 with and without mixed stages to confirm the overfitting prevention benefit
  3. Partial reward ablation: Test different values of the partial reward parameter to find the optimal setting for mathematical reasoning tasks

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the impact of scaling up the model size on R3's performance, and how does it compare to other methods?
- Basis in paper: The paper states "In the future, we will attempt to scale up the model size for better performance."
- Why unresolved: The experiments in the paper were conducted using Llama2-Base-7B, Galatica, and Codellama-7B models. The paper does not explore the impact of scaling up the model size on R3's performance.
- What evidence would resolve it: Experiments using larger models, such as Llama2-13B or Llama2-70B, to compare the performance of R3 with other methods.

### Open Question 2
- Question: How does the diversity and scale of training data affect the performance of R3?
- Basis in paper: The paper mentions "We will explore the impact of training data with larger scale and diversity on R3."
- Why unresolved: The paper does not provide experiments or analysis on how the diversity and scale of training data impact R3's performance.
- What evidence would resolve it: Experiments using different sizes and types of training data to analyze the impact on R3's performance.

### Open Question 3
- Question: Can R3 be applied to other types of reasoning tasks beyond mathematical and logical reasoning?
- Basis in paper: The paper focuses on mathematical and logical reasoning tasks. However, it does not explore the application of R3 to other types of reasoning tasks.
- Why unresolved: The paper does not provide experiments or analysis on the application of R3 to other types of reasoning tasks.
- What evidence would resolve it: Experiments using R3 on other types of reasoning tasks, such as commonsense reasoning or causal reasoning, to evaluate its effectiveness.

## Limitations
- The absolute performance on complex reasoning tasks remains below human-level, suggesting fundamental limitations in the approach's scalability
- The effectiveness of the reverse curriculum approach appears highly dependent on specific design choices with limited ablation studies
- The evaluation focuses primarily on mathematical and logical reasoning tasks, with effectiveness for other reasoning domains unexplored

## Confidence
- **High Confidence**: The core mechanism of using reverse curriculum to make exploration easier is theoretically sound and supported by experimental results
- **Medium Confidence**: The claim that mixing difficulty levels prevents overfitting is supported by staged RL ablation study but needs more extensive experiments
- **Low Confidence**: The effectiveness of partial rewards for numeric answers is demonstrated but optimal parameter setting remains unclear

## Next Checks
1. **Curriculum design ablation study**: Systematically vary the number of stages (M), the sampling strategy for intermediate states, and the difficulty progression to determine which components are essential for the approach's success
2. **Generalization across reasoning domains**: Apply R3 to a broader set of reasoning tasks beyond mathematical and logical reasoning, including causal reasoning, commonsense reasoning, and multi-modal reasoning tasks
3. **Training stability and efficiency analysis**: Compare the stability and computational efficiency of R3 training to standard RL approaches through extensive experiments measuring training curves, variance, and wall-clock time across multiple runs and hardware configurations