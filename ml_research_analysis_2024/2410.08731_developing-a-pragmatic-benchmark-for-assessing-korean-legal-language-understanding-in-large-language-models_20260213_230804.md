---
ver: rpa2
title: Developing a Pragmatic Benchmark for Assessing Korean Legal Language Understanding
  in Large Language Models
arxiv_id: '2410.08731'
source_url: https://arxiv.org/abs/2410.08731
tags:
- legal
- korean
- tasks
- exam
- llms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces KBL, the first pragmatic benchmark for assessing
  Korean legal language understanding in large language models. The benchmark comprises
  7 legal knowledge tasks, 4 legal reasoning tasks, and the Korean bar exam, totaling
  3,308 examples.
---

# Developing a Pragmatic Benchmark for Assessing Korean Legal Language Understanding in Large Language Models

## Quick Facts
- arXiv ID: 2410.08731
- Source URL: https://arxiv.org/abs/2410.08731
- Reference count: 40
- This paper introduces KBL, the first pragmatic benchmark for assessing Korean legal language understanding in large language models, showing that even powerful models like GPT-4 have limited performance on Korean legal tasks.

## Executive Summary
This paper introduces KBL (Korean Benchmark for Legal Language Understanding), the first comprehensive benchmark designed to evaluate Korean legal language understanding in large language models. The benchmark includes 7 legal knowledge tasks, 4 legal reasoning tasks, and the Korean bar exam, totaling 3,308 examples developed in collaboration with lawyers and verified by licensed professionals. The study evaluates models in both closed-book and retrieval-augmented generation settings using Korean statutes and precedents corpora. Results show that even powerful models like GPT-4 and Claude-3 have limited performance on Korean legal tasks, with accuracy improvements of up to 8.6% in the open-book setting.

## Method Summary
The KBL benchmark was developed through collaboration with legal professionals to create 7 knowledge tasks (510 examples), 4 reasoning tasks (288 examples), and the Korean bar exam (4 domains, 53 tasks, 2,510 examples). The evaluation uses the lm-eval-harness framework to assess various models including EEVE-10.8B, KULLM3-10.7B, LG-Exaone-3.0-7.8B, Qwen2-7B, Qwen2-72B, Claude-3 series, GPT-3.5, and GPT-4. For retrieval-augmented generation, BM25 retrieval is used with two legal corpora: Korean statutes (220k articles, 52M tokens) and Korean precedents (150k precedents, 320M tokens).

## Key Results
- GPT-4 and Claude-3 models show limited performance on Korean legal tasks despite strong general performance
- Retrieval-augmented generation improves accuracy by up to 8.6% in open-book settings
- Expert-verified data quality significantly reduces errors compared to freely available legal datasets (up to 21% error reduction)
- Korean LLMs like Qwen2-72B perform better than other Korean models but still lag behind GPT-4

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Legal domain knowledge and reasoning require specialized benchmarks distinct from general LLM evaluations.
- Mechanism: KBL introduces a task taxonomy (knowledge, reasoning, bar exam) tailored to Korean legal ontology, ensuring evaluation of both factual recall and complex reasoning.
- Core assumption: Standard benchmarks like MMLU or general legal benchmarks inadequately assess Korean legal understanding due to linguistic and systemic differences.
- Evidence anchors: [abstract] "their efficacy remains limited for non-standardized tasks and tasks in languages other than English." [section] "GPT-4 has not passed the Korean bar exam, despite passing the US bar exam."

### Mechanism 2
- Claim: Retrieval-augmented generation (RAG) improves LLM performance on Korean legal tasks, but gains depend on corpus type and LLM capability.
- Mechanism: KBL provides two legal corpora (statutes, precedents) and evaluates RAG impact via BM25 retrieval, revealing that benefits vary by task type and domain.
- Core assumption: Legal practitioners use statutes and precedents; thus, LLMs should be evaluated with access to these documents.
- Evidence anchors: [abstract] "considering legal practitioners' frequent use of extensive legal documents for research, we assess LLMs in both a closed book setting...and a retrieval-augmented generation (RAG) setting." [section] "GPT-4 achieves higher scores on the knowledge tasks, scoring +2.4 with precedents, +0.4 with statutes, and +3.3 with both."

### Mechanism 3
- Claim: Expert collaboration and verification ensure benchmark quality and task difficulty.
- Mechanism: Tasks are developed with lawyers, verified by licensed lawyers, and corrected based on feedback, resulting in high-quality, challenging examples.
- Core assumption: Domain expertise is essential to create meaningful legal benchmarks; freely available data contain significant errors.
- Evidence anchors: [section] "we developed the tasks in close corporation with legal professionals, and all the answers have been verified by 8 licensed lawyers." [section] "freely available data, often created by individuals with semi-expertise, frequently include substantial amounts of errors (up to 21% in our study)."

## Foundational Learning

- Concept: Legal ontology differences between jurisdictions
  - Why needed here: Korean civil law system has unique concepts and terms not covered by US or other benchmarks.
  - Quick check question: Can GPT-4 pass the Korean bar exam? (No—shows system differences.)

- Concept: Retrieval-augmented generation in domain-specific tasks
  - Why needed here: Legal research relies on statutes and precedents; RAG simulates real-world use.
  - Quick check question: Does RAG always improve LLM accuracy? (No—depends on task type and corpus.)

- Concept: Expert-annotated data quality control
  - Why needed here: Legal language is subtle; errors in data mislead model training and evaluation.
  - Quick check question: What error rate was found in freely available legal QA data? (Up to 21%.)

## Architecture Onboarding

- Component map:
  - Task datasets (knowledge, reasoning, bar exam) -> Legal corpora (statutes, precedents) -> BM25 retriever -> LLM evaluation harness (lm-eval-harness) -> Expert verification pipeline

- Critical path:
  1. Load task examples (Q&A pairs)
  2. If RAG enabled, retrieve relevant documents from corpora
  3. Format question + options for LLM
  4. Generate answer label
  5. Compare to ground truth for accuracy

- Design tradeoffs:
  - Closed-book vs. open-book: Simpler evaluation but underestimates real-world utility.
  - BM25 vs. learned retriever: Simpler, reproducible baseline but may miss nuanced relevance.
  - Expert verification vs. crowd-sourcing: Higher quality but more resource intensive.

- Failure signatures:
  - Low accuracy across all tasks: Model lacks legal knowledge or reasoning.
  - RAG not improving accuracy: Corpus retrieval or relevance issues; model cannot leverage external docs.
  - High variance in repeated runs: LLM internal randomness; need to average over multiple trials.

- First 3 experiments:
  1. Evaluate baseline (closed-book) accuracy on knowledge tasks.
  2. Enable RAG with statutes corpus; compare accuracy on knowledge tasks.
  3. Test RAG with precedents corpus; compare accuracy on reasoning tasks.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do Korean LLMs compare to GPT-4 on Korean legal tasks when trained with domain-specific legal corpora?
- Basis in paper: [inferred] The paper shows that open-source Korean LLMs like Qwen2-72B perform better than other Korean models but still lag GPT-4, suggesting potential improvements from domain-specific training.
- Why unresolved: The paper evaluates off-the-shelf models but does not train any models on the provided Korean legal corpus.
- What evidence would resolve it: Training experiments comparing legal-domain fine-tuned Korean LLMs against GPT-4 on KBL benchmark tasks.

### Open Question 2
- Question: Does retrieval-augmented generation (RAG) with Korean legal corpora improve LLM performance differently across legal task types (knowledge vs reasoning vs bar exam)?
- Basis in paper: [explicit] The paper shows RAG improves performance on Rule-type questions but not Application-type questions in the bar exam.
- Why unresolved: The paper only tests simple BM25 retrieval without exploring other retrieval methods or analyzing task-type differences systematically.
- What evidence would resolve it: Systematic evaluation of various retrieval methods (semantic, re-ranking, etc.) across all KBL task categories.

### Open Question 3
- Question: What is the impact of legal document format (precedents vs statutes) on RAG effectiveness for different legal reasoning tasks?
- Basis in paper: [explicit] The paper shows GPT-4 performance varies significantly depending on whether precedent or statute corpus is used, with opposite effects in Criminal/Civil vs Public law domains.
- Why unresolved: The paper does not analyze why certain corpora work better for specific domains or tasks, nor does it explore hybrid retrieval approaches.
- What evidence would resolve it: Detailed analysis of retrieval effectiveness by legal document type across all KBL tasks, including hybrid corpus experiments.

## Limitations
- The relatively small number of reasoning tasks (288 examples) compared to knowledge tasks (510 examples) and bar exam questions (2,510 examples) may affect generalizability of reasoning task results.
- The BM25 retrieval method used for RAG may not represent state-of-the-art retrieval techniques, potentially underestimating the true impact of retrieval augmentation.
- The evaluation focuses on Korean language legal understanding, which may not translate directly to other languages or legal systems.

## Confidence

- **High Confidence**: The claim that expert-verified data quality significantly outperforms freely available legal datasets (up to 21% error reduction) is well-supported by direct evidence of lawyer involvement and verification processes.
- **Medium Confidence**: The finding that GPT-4 and Claude-3 models show limited performance on Korean legal tasks (accuracy improvements up to 8.6% with RAG) is supported by benchmark results, though the relatively small reasoning task set tempers generalizability.
- **Medium Confidence**: The assertion that Korean legal ontology requires specialized benchmarks distinct from general evaluations is supported by GPT-4's failure to pass the Korean bar exam, but lacks direct comparative analysis with other non-English legal systems.

## Next Checks
1. Replicate the benchmark evaluation using a learned retriever (e.g., dense retrieval) instead of BM25 to assess whether advanced retrieval methods yield substantially different RAG performance results.
2. Conduct cross-linguistic validation by testing the KBL benchmark framework with Japanese or other non-English legal datasets to evaluate the claim of broader applicability for non-English legal domains.
3. Perform error analysis on the 21% error rate found in freely available legal datasets to identify specific error types and verify that expert verification systematically reduces these errors across all task categories.