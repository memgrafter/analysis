---
ver: rpa2
title: 'Critic-CoT: Boosting the reasoning abilities of large language model via Chain-of-thoughts
  Critic'
arxiv_id: '2408.16326'
source_url: https://arxiv.org/abs/2408.16326
tags:
- step
- critique
- critic
- correct
- refinement
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Critic-CoT, a framework that enhances the
  reasoning capabilities of large language models (LLMs) by pushing them toward System-2-like
  critique and refinement. Unlike previous methods that rely on simple, intuitive
  feedback resembling System-1 processes, Critic-CoT employs step-wise Chain-of-Thought
  critique to enable slow, analytic self-critique.
---

# Critic-CoT: Boosting the reasoning abilities of large language model via Chain-of-thoughts Critic

## Quick Facts
- arXiv ID: 2408.16326
- Source URL: https://arxiv.org/abs/2408.16326
- Authors: Xin Zheng; Jie Lou; Boxi Cao; Xueru Wen; Yuqiu Ji; Hongyu Lin; Yaojie Lu; Xianpei Han; Debing Zhang; Le Sun
- Reference count: 40
- Primary result: Introduces a framework that enhances LLM reasoning through step-wise Chain-of-Thought critique and weak supervision data construction

## Executive Summary
This paper introduces Critic-CoT, a framework that enhances the reasoning capabilities of large language models (LLMs) by pushing them toward System-2-like critique and refinement. Unlike previous methods that rely on simple, intuitive feedback resembling System-1 processes, Critic-CoT employs step-wise Chain-of-Thought critique to enable slow, analytic self-critique. The framework automatically constructs weak-supervision training data without human annotation, allowing LLMs to iteratively analyze, criticize, and refine their reasoning. Experiments on GSM8K and MATH datasets demonstrate significant improvements in task-solving performance through iterative refinement and majority vote filtering.

## Method Summary
Critic-CoT uses a weak supervision approach to construct training data without human annotation. It employs a generator model to produce solutions, a critic model to analyze solutions step-by-step and identify errors, and a refinement module to correct identified errors. The framework trains the critic model through a two-stage process: initial critique ability and self-critique enhancement. During inference, it uses iterative refinement and majority vote filtering to improve solution accuracy. The method is evaluated on GSM8K and MATH datasets, with out-of-domain testing on StrategyQA, AGIEval, and HumanEval.

## Key Results
- Significant improvements in task-solving performance on GSM8K and MATH datasets through iterative refinement
- Majority vote filtering further enhances performance by selecting high-quality solutions
- Mutual reinforcement between critique and task-solving abilities within LLMs, rather than conflict
- Out-of-domain generalization on StrategyQA, AGIEval, and HumanEval datasets

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Step-wise CoT critique enables the model to identify and correct errors at the specific step level, rather than restarting from scratch.
- Mechanism: By breaking down the reasoning process into discrete steps and critiquing each step individually, the model can pinpoint where errors occur and focus refinement efforts on those specific steps. This avoids the inefficiency of completely discarding an entire solution when only one step is wrong.
- Core assumption: The model can effectively analyze and critique its own reasoning at the step level when given explicit step-wise format.
- Evidence anchors:
  - [abstract]: "Through a step-wise CoT reasoning paradigm and the automatic construction of distant-supervision data without human annotation, Critic-CoT enables LLMs to engage in slow, analytic self-critique and refinement"
  - [section 3.1]: "Given the question Q and the corresponding gold answer Ans, we have the n-step attempt Att = [s1, ..., sn] with predicted answer Pred sampled by generator G. The corresponding critique Cri then can be represented as L = [l1, ..., ln], where the step label li = +1 indicates that step i is predicted to be correct, and li = −1 to be incorrect."

### Mechanism 2
- Claim: The mutual reinforcement between critique and task-solving abilities creates a positive feedback loop that enhances both capabilities simultaneously.
- Mechanism: As the model improves its ability to critique reasoning steps, it gains better understanding of problem structure and solution patterns. This enhanced understanding then feeds back into better task-solving performance. The improved task-solving performance provides more high-quality solutions to critique, further strengthening critique abilities.
- Core assumption: The skills required for critiquing reasoning and solving problems are not independent but share underlying cognitive capabilities that can reinforce each other.
- Evidence anchors:
  - [abstract]: "we investigate the intrinsic correlation between critique and task-solving abilities within LLMs, discovering that these abilities can mutually reinforce each other rather than conflict"
  - [section 4.2]: "we find that for LLMs, the ability of critique and refinement could mutually reinforce, which may shed light on designing more advanced self-critic framework designs in future work"

### Mechanism 3
- Claim: Weak supervision through automated data construction eliminates the need for expensive human annotation while maintaining high-quality training data.
- Mechanism: The framework automatically generates critique-refine pairs by sampling solutions, having the model critique them, and collecting only those pairs where critique successfully identifies errors and refinement produces correct answers. This creates high-quality training data without human involvement.
- Core assumption: Automated generation of critique-refine pairs can produce training data of sufficient quality to effectively teach critique and refinement skills.
- Evidence anchors:
  - [section 3.1]: "Through a series of experiments on the in-domain dataset of GSM8K and MATH, together with out-of-domain evaluation on StrategyQA, AGIEval and HumanEval, we find that our trained critic model can fairly distinguish incorrect solutions from correct ones"
  - [section 5.1]: "We sample 300 entries... and conduct a manual verification to verify the accuracy of the step-wise critiques... with about 85% accuracy on wrong-answer critique, and more than 90% on refinement and correct-answer critique"

## Foundational Learning

- Concept: Chain-of-Thought (CoT) reasoning
  - Why needed here: The framework relies on breaking down reasoning into explicit steps that can be individually analyzed and critiqued. Understanding CoT is essential for implementing the step-wise critique mechanism.
  - Quick check question: Can you explain how Chain-of-Thought prompting differs from direct answer generation, and why it's particularly useful for complex reasoning tasks?

- Concept: Weak supervision and distant supervision
  - Why needed here: The framework uses automated methods to generate training data without human labels. Understanding these concepts is crucial for grasping how the training data is constructed and why it's feasible.
  - Quick check question: What are the key differences between weak supervision, distant supervision, and traditional supervised learning, and what are the trade-offs involved?

- Concept: Mutual reinforcement in skill acquisition
  - Why needed here: The paper claims that critique and task-solving abilities mutually reinforce each other. Understanding this concept helps explain why joint training of both skills is beneficial.
  - Quick check question: Can you provide an example from another domain where improving one skill naturally enhances another related skill through mutual reinforcement?

## Architecture Onboarding

- Component map: Generator model -> Critic model -> Refinement module -> Evaluation
- Critical path:
  1. Problem input → Generator produces solution
  2. Solution → Critic analyzes each step
  3. Critique results → Refinement corrects errors
  4. Refined solution → Evaluation against ground truth
  5. Data collection → Training of critic model
- Design tradeoffs:
  - Step-wise critique vs. instance-level critique: More granular control vs. simpler implementation
  - Automated data construction vs. human annotation: Scalability vs. potential noise in data
  - Iterative refinement vs. majority vote: Sample efficiency vs. computational cost
  - Training on critique ability vs. task-solving ability: Specialization vs. generalization
- Failure signatures:
  - Critic consistently misses errors → Problem with critique model training or step-wise analysis capability
  - Refinement introduces new errors → Problem with refinement logic or over-correction
  - No improvement from iterative refinement → Problem with error detection or correction mechanism
  - Training data quality too low → Problem with weak supervision pipeline or sampling strategy
- First 3 experiments:
  1. Implement basic step-wise CoT critique on a small dataset and measure error detection accuracy vs. instance-level critique
  2. Test automated data construction pipeline by generating critique-refine pairs and manually evaluating quality
  3. Implement iterative refinement on GSM8K and measure improvement over base model performance

## Open Questions the Paper Calls Out

The paper does not explicitly call out specific open questions, but it raises several important considerations:
- How does the quality of the constructed critic-refine data impact the final model performance, and what is the upper bound of improvement achievable through this approach?
- Can the Critic-CoT framework be effectively extended to domains beyond mathematics where correctness verification is more complex or subjective?
- What is the optimal balance between System-1-like intuition and System-2-like analysis in LLMs for different types of reasoning tasks?

## Limitations
- The framework's effectiveness depends on having access to strong LLMs for generating critique data (e.g., GPT-4 Turbo), which may not be available to all researchers
- The computational overhead of iterative refinement and majority vote filtering could be prohibitive for real-time applications
- The step-wise critique approach may not generalize well to domains where reasoning doesn't naturally decompose into discrete, checkable steps

## Confidence

**High Confidence**: The empirical results showing performance improvements on GSM8K and MATH datasets through iterative refinement and majority vote filtering

**Medium Confidence**: The mutual reinforcement hypothesis between critique and task-solving abilities, as the ablation studies are suggestive but don't definitively prove causation rather than correlation

**Medium Confidence**: The weak supervision data construction methodology, as the quality metrics are based on manual verification of only 300 samples

## Next Checks
1. Conduct ablation studies varying the number of critique-refine iterations to determine the optimal trade-off between performance gains and computational cost
2. Test the framework's robustness by introducing controlled noise into the automatically generated critique data and measuring the impact on final model performance
3. Evaluate the framework on domains with different reasoning characteristics (e.g., code generation vs. mathematical reasoning) to assess generalizability beyond the GSM8K and MATH datasets