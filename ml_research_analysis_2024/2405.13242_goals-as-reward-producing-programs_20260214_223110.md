---
ver: rpa2
title: Goals as Reward-Producing Programs
arxiv_id: '2405.13242'
source_url: https://arxiv.org/abs/2405.13242
tags:
- game
- games
- block
- preference
- agent
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a framework for modeling human goal generation
  as synthesizing reward-producing programs. The authors collected a dataset of human-generated
  playful goals (single-player games) and translated them into a domain-specific language
  (DSL) of reward-producing programs.
---

# Goals as Reward-Producing Programs

## Quick Facts
- arXiv ID: 2405.13242
- Source URL: https://arxiv.org/abs/2405.13242
- Reference count: 40
- Primary result: Model-generated games indistinguishable from human-created games when generated from program space partitions close to human examples

## Executive Summary
This paper introduces a novel framework for modeling human goal generation as synthesizing reward-producing programs. The authors translate human-generated playful goals into a domain-specific language (DSL) of reward-producing programs and develop a Goal Program Generator (GPG) model that learns a fitness function over programs to capture human-likeness. Using quality-diversity optimization, the model generates novel human-like goals that are indistinguishable from human-created games in human evaluations. The learned fitness function also predicts games that are more fun to play and more human-like, providing a new approach to modeling human goal generation with implications for building artificial agents with richer, more human-like goals.

## Method Summary
The authors collected a dataset of human-generated playful goals (single-player games) and translated them into a domain-specific language (DSL) of reward-producing programs. They developed a Goal Program Generator (GPG) model that learns a fitness function over programs to capture human-likeness using contrastive learning. The model then uses quality-diversity optimization (MAP-Elites) to generate diverse, high-fitness goals. Human evaluations compared model-generated games to human-created games, finding that games generated from program space partitions close to human examples were rated similarly to human-created games, while those further away were rated lower.

## Key Results
- Model-generated games from program space partitions close to human examples were indistinguishable from human-created games in human evaluations
- The learned fitness function predicted games that were more fun to play and more human-like
- Quality-diversity optimization enabled generation of diverse, human-like goals while exploring novel regions of program space

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Reward-producing programs capture the rich semantics of goals through symbolic operations
- Mechanism: Programs are executable mappings from agent behavior to reward scores, enabling compositional reuse of goal motifs
- Core assumption: People create goals using compositional structures that can be represented symbolically
- Evidence anchors:
  - [abstract]: "Reward-producing programs capture the rich semantics of goals through symbolic operations that compose, add temporal constraints, and allow for program execution on behavioral traces to evaluate progress."
  - [section]: "Our choice of representation makes goal semantics explicit. The particular grammatical elements of our representation each fulfill particular roles, such as predicates... and temporal modals..."
  - [corpus]: Weak - no corpus evidence available
- Break condition: If people create goals using non-compositional, holistic representations rather than symbolic building blocks

### Mechanism 2
- Claim: A learned fitness function over program space captures human-likeness and predicts fun ratings
- Mechanism: The fitness function is trained contrastively to distinguish human-generated games from corrupted versions, then used to guide evolutionary search
- Core assumption: Human-like goals have distinct structural and semantic properties that can be learned from examples
- Evidence anchors:
  - [abstract]: "To build a generative model of goals, we learn a fitness function over the infinite set of possible goal programs and sample novel goals with a quality-diversity algorithm."
  - [section]: "Our Goal Program Generator model (GPG, illustrated in Figure 3) operates over a high-dimensional program space and learns how to generate goals maximizing a fitness measure."
  - [corpus]: Weak - no corpus evidence available
- Break condition: If the fitness function fails to capture key aspects of human goal creation or overfits to superficial patterns

### Mechanism 3
- Claim: Quality-diversity optimization enables generation of diverse, human-like goals
- Mechanism: MAP-Elites maintains an archive of programs across behavioral characteristics, ensuring diversity while optimizing fitness
- Core assumption: Human goals occupy distinct regions in program space that can be identified through behavioral characteristics
- Evidence anchors:
  - [abstract]: "To build a generative model of goals, we learn a fitness function over the infinite set of possible goal programs and sample novel goals with a quality-diversity algorithm."
  - [section]: "Our Goal Program Generator model (GPG, illustrated in Figure 3) operates over a high-dimensional program space and learns how to generate goals maximizing a fitness measure."
  - [corpus]: Weak - no corpus evidence available
- Break condition: If the behavioral characteristics fail to capture meaningful diversity or if the archive becomes too sparse

## Foundational Learning

- Concept: Domain-specific language (DSL) for goal representation
  - Why needed here: Provides a structured, executable representation of goals that captures their semantics
  - Quick check question: What are the key components of the DSL and how do they map to goal semantics?

- Concept: Quality-diversity optimization (MAP-Elites)
  - Why needed here: Enables generation of diverse goals while optimizing for human-likeness
  - Quick check question: How does MAP-Elites maintain diversity and what are the behavioral characteristics used?

- Concept: Contrastive learning for fitness function
  - Why needed here: Trains the fitness function to distinguish human-like goals from corrupted versions
  - Quick check question: How does the contrastive learning objective work and what are the negative examples?

## Architecture Onboarding

- Component map:
  - DSL parser and interpreter: Translates natural language goals to executable programs
  - Fitness function: Learns to score programs based on human-likeness
  - MAP-Elites optimizer: Generates diverse, high-fitness goals
  - Human evaluation pipeline: Collects feedback on generated goals

- Critical path:
  1. Collect human-generated goals and translate to DSL
  2. Train fitness function on DSL programs
  3. Use MAP-Elites to generate diverse, high-fitness goals
  4. Evaluate generated goals with human raters

- Design tradeoffs:
  - DSL expressiveness vs. tractability: More expressive DSLs allow richer goal representation but may be harder to learn and search
  - Fitness function complexity vs. data efficiency: More complex fitness functions may capture human-likeness better but require more data
  - Diversity vs. quality: Prioritizing diversity may lead to less human-like goals, while prioritizing quality may lead to less diverse goals

- Failure signatures:
  - Generated goals are too similar to training examples (overfitting)
  - Generated goals are incoherent or nonsensical (underfitting or poor DSL design)
  - Fitness function fails to predict human ratings (poor learning or evaluation setup)

- First 3 experiments:
  1. Evaluate the diversity and quality of generated goals using human ratings
  2. Ablate key components (e.g., fitness function, MAP-Elites) to assess their impact
  3. Test the generalization of the model to new environments or goal types

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do the model's generated goals compare in terms of complexity and novelty to human-generated goals?
- Basis in paper: [explicit] The paper discusses the model's ability to generate novel and human-like goals, but does not provide a detailed comparison of complexity and novelty with human-generated goals.
- Why unresolved: The paper focuses on the model's performance in generating human-like goals but does not delve into a quantitative comparison of complexity and novelty between model-generated and human-generated goals.
- What evidence would resolve it: A detailed analysis comparing the complexity and novelty of model-generated goals with human-generated goals using metrics such as structural complexity, diversity, and novelty scores.

### Open Question 2
- Question: How does the model's performance vary with different sizes of the training dataset?
- Basis in paper: [inferred] The paper mentions that the model can generate goals without a large number of examples, but does not explore the impact of dataset size on performance.
- Why unresolved: The paper does not provide experiments or analysis on how the model's performance changes with varying sizes of the training dataset.
- What evidence would resolve it: Experiments comparing the model's performance on datasets of different sizes, measuring metrics such as fitness scores, human-likeness ratings, and diversity of generated goals.

### Open Question 3
- Question: How does the model's performance change when using different quality-diversity algorithms?
- Basis in paper: [explicit] The paper uses MAP-Elites for quality-diversity optimization, but does not explore the impact of using different algorithms.
- Why unresolved: The paper does not provide experiments or analysis on how the choice of quality-diversity algorithm affects the model's performance.
- What evidence would resolve it: Experiments comparing the model's performance using different quality-diversity algorithms, such as Novelty Search or MAP-Elites with different behavioral characteristics, measuring metrics such as fitness scores, human-likeness ratings, and diversity of generated goals.

### Open Question 4
- Question: How does the model's performance change when incorporating additional cognitive capacities, such as planning or physical simulation?
- Basis in paper: [explicit] The paper mentions the potential integration of planning or physical simulation to improve the model's understanding of physics.
- Why unresolved: The paper does not provide experiments or analysis on how incorporating additional cognitive capacities affects the model's performance.
- What evidence would resolve it: Experiments incorporating planning or physical simulation into the model and measuring the impact on performance metrics such as fitness scores, human-likeness ratings, and diversity of generated goals.

### Open Question 5
- Question: How does the model's performance change when using different representations for goals, such as natural language or other symbolic languages?
- Basis in paper: [inferred] The paper uses a domain-specific language (DSL) for goal representation, but does not explore the impact of using different representations.
- Why unresolved: The paper does not provide experiments or analysis on how the choice of goal representation affects the model's performance.
- What evidence would resolve it: Experiments comparing the model's performance using different goal representations, such as natural language or other symbolic languages, measuring metrics such as fitness scores, human-likeness ratings, and diversity of generated goals.

## Limitations

- The framework relies heavily on the quality and expressiveness of the DSL for goal representation, which may not fully capture the richness of human goal generation
- The learned fitness function is trained on a relatively limited dataset of 37 human-generated games, raising concerns about generalizability
- Quality-diversity optimization may still produce semantically meaningless or incoherent goals when exploring less populated regions of program space

## Confidence

- **High Confidence:** The basic premise that goals can be represented as reward-producing programs and that such programs can be executed to evaluate progress
- **Medium Confidence:** The ability of the learned fitness function to accurately capture human-likeness and predict fun ratings
- **Low Confidence:** The generalizability of the model to other domains or more complex goal types beyond simple single-player games

## Next Checks

1. Validate the model's performance on a larger and more diverse dataset of human-generated goals, including goals from different domains
2. Conduct an ablation study to determine the relative importance of different DSL components in capturing human-likeness
3. Evaluate the model's performance on generating goals that are significantly different from the training examples, both in terms of structure and semantics