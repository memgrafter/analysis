---
ver: rpa2
title: Graph-enhanced Large Language Models in Asynchronous Plan Reasoning
arxiv_id: '2402.02805'
source_url: https://arxiv.org/abs/2402.02805
tags:
- step
- task
- language
- time
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work investigates the ability of large language models (LLMs)
  to perform asynchronous planning tasks, which require combining sequential and parallel
  planning to optimize time costs. The authors introduce a new benchmark, AsyncHow,
  containing 1.6k naturalistic planning instances.
---

# Graph-enhanced Large Language Models in Asynchronous Plan Reasoning

## Quick Facts
- **arXiv ID**: 2402.02805
- **Source URL**: https://arxiv.org/abs/2402.02805
- **Reference count**: 40
- **Key outcome**: LLMs struggle with asynchronous planning tasks unless provided graph-enhanced prompts; PLaG technique significantly improves performance across models.

## Executive Summary
This paper investigates how large language models perform on asynchronous planning tasks, which require combining sequential and parallel reasoning to optimize time costs. The authors introduce AsyncHow, a benchmark of 1.6k naturalistic planning instances, and find that LLMs perform poorly without explicit task-solving illustrations. They propose Plan Like a Graph (PLaG), a novel prompting technique that combines graph representations with natural language prompts, achieving state-of-the-art results. While PLaG significantly boosts performance across all tested models and task complexities, LLMs still suffer from drastic performance degradation as task complexity increases, revealing fundamental limitations in their planning capabilities.

## Method Summary
The authors create the AsyncHow benchmark by combining WikiHow and ProScript datasets to generate 1.6k asynchronous planning tasks. Each task consists of steps with durations and ordering constraints, requiring computation of the shortest completion time. They evaluate multiple LLMs (GPT-3.5, GPT-4, Command, LLaMA-2-70B-chat, Mistral-7B-Instruct) across various prompting strategies: zero-shot, few-shot, Chain-of-Thought, and their proposed PLaG method. PLaG represents planning problems as graphs alongside natural language descriptions, testing both explicit graph representations and Bag-of-Graphs (BaG) approaches. Performance is measured by accuracy in computing correct completion times across varying task complexities.

## Key Results
- LLMs including GPT-4 and LLaMA-2 perform poorly on asynchronous planning without task-solving illustrations
- PLaG technique boosts performance of all tested models, achieving state-of-the-art results
- Despite PLaG improvements, all models show drastic performance degradation as task complexity increases
- Explicit graph representations in PLaG outperform natural language prompts alone

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLMs perform poorly on asynchronous planning tasks unless supplied with explicit task-solving illustrations.
- Mechanism: Without structured guidance, LLMs struggle to combine sequential and parallel reasoning, leading to suboptimal or incorrect plans.
- Core assumption: LLMs lack inherent ability to translate natural language task descriptions into optimal planning graphs.
- Evidence anchors:
  - [abstract] "We find that a representative set of closed and open-source LLMs, including GPT-4 and LLaMA-2, behave poorly when not supplied with illustrations about the task-solving process in our benchmark AsyncHow."
  - [section] "We find that while GPT-4 with few-shot task solution illustrations dominates other models in terms of accuracy, all models perform poorly without illustrations about how to solve the task."
- Break condition: If task descriptions are highly structured and unambiguous, LLMs might perform better without explicit illustrations.

### Mechanism 2
- Claim: Representing planning problems as graphs significantly improves LLM performance on asynchronous planning.
- Mechanism: Graph representations provide explicit structure for sequential and parallel relationships, making it easier for LLMs to reason about optimal plans.
- Core assumption: LLMs can effectively process graph-structured information when provided in the prompt.
- Evidence anchors:
  - [abstract] "We propose a novel technique called Plan Like a Graph (PLaG) that combines graphs with natural language prompts and achieves state-of-the-art results."
  - [section] "We find that our method boosts the performance of all tested models."
- Break condition: If graph representations become too complex or abstract, LLMs may struggle to interpret them correctly.

### Mechanism 3
- Claim: LLM performance degrades drastically as task complexity increases, even with graph-enhanced prompting.
- Mechanism: Complex planning tasks require more sophisticated reasoning and graph traversal, exceeding LLM capabilities.
- Core assumption: There is a limit to the complexity of planning problems that LLMs can handle effectively.
- Evidence anchors:
  - [abstract] "We show that although PLaG can boost model performance, LLMs still suffer from drastic degradation when task complexity increases."
  - [section] "We find that model performance still drastically downgrades with increasing task complexity."
- Break condition: If task complexity is artificially reduced or LLM capabilities significantly improve, performance degradation may be less severe.

## Foundational Learning

- Concept: Directed Acyclic Graphs (DAGs)
  - Why needed here: DAGs are used to model the sequential and parallel relationships between steps in asynchronous planning tasks.
  - Quick check question: Can you explain how a DAG can represent both sequential and parallel dependencies between actions in a plan?

- Concept: Graph Theory and Path Finding
  - Why needed here: Finding the optimal time for an asynchronous plan is equivalent to finding the longest path in a DAG.
  - Quick check question: How would you modify a shortest path algorithm to find the longest path in a DAG?

- Concept: Prompt Engineering and In-Context Learning
  - Why needed here: Different prompting techniques are used to improve LLM performance on asynchronous planning tasks.
  - Quick check question: What are the key differences between zero-shot, few-shot, and Chain-of-Thought prompting?

## Architecture Onboarding

- Component map:
  Benchmark Generation -> Model Evaluation -> Graph-Enhanced Prompting -> Analysis

- Critical path:
  1. Generate AsyncHow benchmark
  2. Evaluate LLMs without graph-enhanced prompting
  3. Implement and evaluate PLaG technique
  4. Analyze performance across task complexities

- Design tradeoffs:
  - Complexity vs. Performance: More complex graph representations may improve performance but increase computational cost.
  - Prompt Length vs. Effectiveness: Longer prompts with detailed illustrations may improve performance but could lead to context truncation.

- Failure signatures:
  - Poor performance on complex tasks: Indicates limitations in LLM reasoning capabilities.
  - Sensitivity to prompt variations: Suggests need for robust prompting techniques.
  - Inconsistent results across graph representations: May indicate issues with graph interpretation.

- First 3 experiments:
  1. Evaluate a baseline LLM (e.g., GPT-3.5) on a simple asynchronous planning task without any prompting enhancements.
  2. Implement and test PLaG with explicit graph representations on the same task.
  3. Gradually increase task complexity and observe performance degradation to establish baseline limits.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do Large Language Models (LLMs) perform on asynchronous planning tasks when given explicit step-by-step instructions compared to implicit hints?
- Basis in paper: [explicit] The paper discusses the performance of LLMs like GPT-4 and LLaMA-2 in asynchronous planning tasks, highlighting that they perform poorly without illustrations about the task-solving process.
- Why unresolved: The paper introduces a novel prompting technique, Plan Like a Graph (PLaG), which combines graphs with natural language prompts to improve performance. However, it does not directly compare the performance of LLMs when given explicit step-by-step instructions versus implicit hints.
- What evidence would resolve it: An experiment comparing the accuracy of LLMs in solving asynchronous planning tasks using explicit step-by-step instructions versus implicit hints would provide insight into their performance differences.

### Open Question 2
- Question: What is the impact of task complexity on the performance of LLMs in asynchronous planning, and how does this compare to their performance in prototypical graph search problems?
- Basis in paper: [explicit] The paper notes that LLMs, including GPT-3.5 and GPT-4, suffer from drastic performance degradation as task complexity increases in asynchronous planning tasks.
- Why unresolved: While the paper discusses the performance degradation with increasing complexity, it does not directly compare this to how LLMs perform on prototypical graph search problems, which are structurally similar.
- What evidence would resolve it: A comparative study analyzing the performance of LLMs on both asynchronous planning tasks and prototypical graph search problems across varying levels of complexity would clarify the impact of task complexity on LLM performance.

### Open Question 3
- Question: How does the use of different graph representations (e.g., adjacency list, edge list, adjacency matrix, compressed sparse row) affect the performance of LLMs in asynchronous planning tasks?
- Basis in paper: [explicit] The paper mentions that each planning task is eventually coupled by four types of graph representations and that these representations can be used to aid LLMs in structural reasoning.
- Why unresolved: The paper does not provide a detailed analysis of how each graph representation affects LLM performance, only that they are available for use.
- What evidence would resolve it: An experiment testing LLM performance on asynchronous planning tasks using each of the four graph representations would reveal which representation, if any, is most effective in aiding LLM reasoning.

## Limitations
- Results are based on a single benchmark (AsyncHow) and may not generalize to all planning domains
- The exact contribution of graph structure versus improved prompting in PLaG is unclear
- Performance degradation with complexity suggests fundamental limitations in LLM reasoning capabilities

## Confidence
- High confidence: LLMs perform poorly on asynchronous planning without explicit illustrations
- Medium confidence: PLaG effectiveness is demonstrated but exact mechanism unclear
- Medium confidence: Performance degradation with complexity is well-documented but scaling relationships need further validation

## Next Checks
1. **Ablation Study on Graph Components**: Test whether the improvements from PLaG come specifically from the graph structure or from any structured representation by comparing PLaG against other structured formats (e.g., tables, flowcharts) on the same tasks.

2. **Cross-Domain Generalization**: Evaluate PLaG on planning tasks from different domains (robotics, logistics, software development) to assess whether the approach generalizes beyond the AsyncHow benchmark.

3. **Complexity Scaling Analysis**: Systematically vary task complexity using controlled parameters (number of steps, depth of parallel chains, branching factor) to establish precise scaling laws for LLM performance degradation and identify potential breakpoints.