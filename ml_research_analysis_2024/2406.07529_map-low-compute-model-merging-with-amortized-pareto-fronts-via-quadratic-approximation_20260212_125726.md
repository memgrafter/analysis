---
ver: rpa2
title: 'MAP: Low-compute Model Merging with Amortized Pareto Fronts via Quadratic
  Approximation'
arxiv_id: '2406.07529'
source_url: https://arxiv.org/abs/2406.07529
tags:
- pareto
- task
- merging
- tasks
- front
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of efficiently merging multiple
  single-task models into a multitask model while capturing trade-offs between tasks.
  The proposed MAP (Model Merging with Amortized Pareto Fronts) algorithm uses quadratic
  approximation surrogate models to identify Pareto optimal solutions without retraining,
  amortizing the computational cost of evaluating trade-offs.
---

# MAP: Low-compute Model Merging with Amortized Pareto Fronts via Quadratic Approximation

## Quick Facts
- arXiv ID: 2406.07529
- Source URL: https://arxiv.org/abs/2406.07529
- Reference count: 40
- This paper introduces MAP, an efficient algorithm for merging multiple single-task models into a multitask model while capturing trade-offs between tasks using quadratic approximation surrogate models.

## Executive Summary
This paper addresses the challenge of efficiently merging multiple single-task models into a multitask model while capturing trade-offs between tasks. The proposed MAP (Model Merging with Amortized Pareto Fronts) algorithm uses quadratic approximation surrogate models to identify Pareto optimal solutions without retraining, amortizing the computational cost of evaluating trade-offs. Two variants, Bayesian MAP and Nested MAP, further reduce computation for high-task scenarios. Experiments on vision and NLP tasks demonstrate MAP's effectiveness in finding diverse Pareto fronts, outperforming baselines like brute-force search and MOEA/D while being significantly more efficient. MAP serves as a flexible plug-in for existing model merging methods.

## Method Summary
MAP addresses multitask model merging by computing task vectors (differences between fine-tuned and pretrained model parameters) and sampling scaling coefficients to evaluate merged models. It fits quadratic surrogate models to approximate task evaluation metrics, then applies multi-objective optimization (NSGA-III) to find Pareto optimal scaling coefficients. The algorithm amortizes computational costs by replacing expensive model evaluations with surrogate optimization. Bayesian MAP adds adaptive sampling to focus on uncertain regions, while Nested MAP reduces complexity from O(N · 2^N) to O(N log N) through sequential pairwise merging.

## Key Results
- MAP efficiently finds Pareto optimal solutions for multitask model merging without retraining
- Bayesian MAP and Nested MAP variants significantly reduce computation for high-task scenarios
- Experiments demonstrate MAP's effectiveness across vision and NLP tasks, outperforming baselines
- MAP serves as a flexible plug-in that can be integrated with existing model merging methods

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Quadratic approximation of evaluation metrics allows efficient Pareto front estimation without retraining.
- Mechanism: Uses second-order Taylor expansion to approximate task evaluation metrics as quadratic functions of scaling coefficients. This reduces the problem from expensive model evaluations to optimizing a low-dimensional surrogate.
- Core assumption: Task vectors have small norms relative to pretrained model parameters, making higher-order terms negligible.
- Evidence anchors:
  - [abstract]: "amortizes the substantial computational cost of evaluations needed to estimate the Pareto front by using quadratic approximation surrogate models"
  - [section 3.1]: Derivation shows Mn(c) ≈ Mn(θpre) + ∇Mn(θpre)⊤V c + 1/2 (Vc)⊤ Hn(θpre) (Vc)
  - [corpus]: Weak - no direct evidence of quadratic approximation use in related work
- Break condition: If task vectors are large or evaluation metrics are highly non-smooth, the quadratic approximation becomes inaccurate.

### Mechanism 2
- Claim: Nested merging reduces computational complexity from O(N · 2^N) to O(N log N).
- Mechanism: Instead of evaluating all possible combinations of N tasks simultaneously, pairs of tasks are merged sequentially, building up to the full solution. This exploits the structure of the merging problem.
- Core assumption: Merging can be performed hierarchically without losing significant Pareto front quality.
- Evidence anchors:
  - [section 3.3]: "Nested MAP (NMMAP) reduces the number of evaluations from O(N · 2^N) to O(N log N)"
  - [section E.2.2]: Detailed calculation showing computational savings for N=8 tasks
  - [corpus]: Weak - no direct evidence of nested merging in related work
- Break condition: If tasks have strong interactions that cannot be captured by pairwise merging, the nested approach may miss important Pareto solutions.

### Mechanism 3
- Claim: Bayesian adaptive sampling focuses computational resources on uncertain regions of the solution space.
- Mechanism: Uses Bayesian optimization with acquisition functions (like upper confidence bound) to iteratively select scaling coefficients where the surrogate model is most uncertain, improving Pareto front estimation efficiency.
- Core assumption: Regions with high uncertainty in the surrogate model are most valuable to evaluate for improving Pareto front quality.
- Evidence anchors:
  - [section 3.3]: "Bayesian adaptive sampling method, inspired by Bayesian optimization"
  - [section E.3.1]: Algorithm details showing iterative sampling based on posterior distribution
  - [corpus]: Weak - only indirect evidence from general Bayesian optimization literature
- Break condition: If the surrogate model's uncertainty estimates are poorly calibrated, Bayesian sampling may focus on irrelevant regions.

## Foundational Learning

- Concept: Multi-objective optimization and Pareto optimality
  - Why needed here: The paper addresses the problem of finding multiple trade-off solutions rather than a single optimal model
  - Quick check question: What defines a Pareto optimal solution in the context of model merging?

- Concept: Second-order Taylor expansion and quadratic approximation
  - Why needed here: Forms the mathematical foundation for the efficient surrogate model that replaces expensive model evaluations
  - Quick check question: Under what conditions is the second-order Taylor expansion a good approximation?

- Concept: Bayesian optimization and acquisition functions
  - Why needed here: Provides the framework for the adaptive sampling strategy that improves computational efficiency
  - Quick check question: How does the upper confidence bound acquisition function balance exploration and exploitation?

## Architecture Onboarding

- Component map:
  Task vectors -> Surrogate model fitting -> Pareto front optimization -> Bayesian sampling (optional)

- Critical path:
  1. Compute task vectors from fine-tuned models
  2. Sample initial scaling coefficients and evaluate metrics
  3. Fit quadratic surrogate models for each task
  4. Apply NSGA-III to find Pareto front on surrogate models
  5. (Optional) Use Bayesian sampling to refine the surrogate model

- Design tradeoffs:
  - Accuracy vs. computation: More sampling points improve Pareto front quality but increase computational cost
  - Model complexity vs. generalization: Higher-order approximations might be more accurate but require more data
  - Sequential vs. parallel evaluation: Parallel evaluation speeds up computation but may miss adaptive sampling benefits

- Failure signatures:
  - Poor R² values in surrogate model fitting indicate the quadratic approximation is inadequate
  - Sparse or degenerate Pareto fronts suggest tasks don't have meaningful trade-offs
  - High variance in Bayesian acquisition function values indicates uncertainty estimation problems

- First 3 experiments:
  1. Verify quadratic approximation accuracy on a simple two-task problem with known ground truth
  2. Test nested merging on three tasks to confirm computational savings and Pareto front quality
  3. Implement Bayesian sampling on a two-task problem to validate improved efficiency over random sampling

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How robust is MAP's quadratic approximation when the assumption of small task vector norms is violated?
- Basis in paper: [explicit] The paper states Assumption 1 about small task vector norms and smoothness of evaluation metrics, but doesn't explore scenarios where this assumption breaks down.
- Why unresolved: The paper only validates the assumption with empirical evidence showing ~1-2% norm ratios, but doesn't test MAP's performance when task vectors are larger or less smooth.
- What evidence would resolve it: Systematic experiments varying task vector norms and testing MAP's accuracy in approximating Pareto fronts under different violation levels of Assumption 1.

### Open Question 2
- Question: How does MAP's performance scale with extremely high-dimensional task spaces (N >> 8)?
- Basis in paper: [inferred] The paper demonstrates MAP on up to 8 tasks and introduces Nested MAP to reduce computational complexity, but doesn't explore scenarios with hundreds or thousands of tasks.
- Why unresolved: The paper only provides complexity analysis for Nested MAP (O(N log N)) but doesn't empirically test how well MAP scales to very high task dimensions.
- What evidence would resolve it: Experiments scaling MAP to high-dimensional task spaces with N in the hundreds or thousands, measuring accuracy of Pareto front approximation and computational efficiency.

### Open Question 3
- Question: Can MAP be extended to handle dynamic task sets where tasks are added or removed over time?
- Basis in paper: [inferred] The paper treats tasks as fixed and pre-defined, but doesn't address scenarios where the task set changes during the merging process.
- Why unresolved: The current MAP algorithm requires re-computing the entire quadratic approximation when tasks change, which may be inefficient for dynamic environments.
- What evidence would resolve it: An extension of MAP that can efficiently update the quadratic approximation and Pareto front when tasks are dynamically added or removed, with benchmarks comparing update efficiency to re-running the full algorithm.

## Limitations
- Reliance on quadratic approximation may limit accuracy when task vector norms are large or evaluation metrics are highly non-smooth
- Computational complexity of O(N · 2^N) for standard MAP may be prohibitive for very large numbers of tasks
- Paper doesn't address potential catastrophic forgetting when merging multiple task-specific models

## Confidence
High confidence: The mathematical framework for quadratic approximation and its use in surrogate modeling is well-established and clearly presented. The computational complexity claims for nested MAP appear sound based on the algorithmic description.

Medium confidence: The experimental results showing MAP's effectiveness across different task combinations and model architectures are promising but limited in scope. The paper doesn't provide extensive ablation studies on the impact of different sampling strategies or the sensitivity to hyperparameter choices.

Low confidence: The paper's claims about MAP being a "plug-in" for existing model merging methods are somewhat vague and not thoroughly validated. The specific benefits and limitations of this integration are not clearly articulated.

## Next Checks
1. Test MAP's performance on a diverse set of task combinations (e.g., vision + NLP) to assess cross-domain generalization and identify potential failure modes.

2. Conduct a detailed ablation study comparing different sampling strategies (random vs. Bayesian) and their impact on Pareto front quality and computational efficiency.

3. Implement MAP with different model architectures (e.g., transformers vs. convolutional networks) to evaluate its flexibility and identify any architecture-specific limitations.