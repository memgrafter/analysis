---
ver: rpa2
title: Enhancing Document-level Translation of Large Language Model via Translation
  Mixed-instructions
arxiv_id: '2401.08088'
source_url: https://arxiv.org/abs/2401.08088
tags:
- translation
- document-level
- llms
- sentence-level
- instructions
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the problem of document-level translation with
  large language models (LLMs), which typically struggle with documents containing
  over 512 tokens. To overcome this, the authors propose "translation mixed-instructions,"
  a fine-tuning approach that combines sentence-level and document-level translation
  instructions of varying lengths.
---

# Enhancing Document-level Translation of Large Language Model via Translation Mixed-instructions

## Quick Facts
- arXiv ID: 2401.08088
- Source URL: https://arxiv.org/abs/2401.08088
- Authors: Yachao Li; Junhui Li; Jing Jiang; Min Zhang
- Reference count: 12
- Primary result: Translation mixed-instructions enable LLMs to maintain consistent translation performance from sentence level to documents containing up to 2048 tokens

## Executive Summary
This paper addresses the challenge of document-level translation with large language models (LLMs), which typically struggle with documents containing over 512 tokens due to sentence-level coverage issues. The authors propose "translation mixed-instructions," a fine-tuning approach that combines sentence-level and document-level translation instructions of varying lengths. By incorporating instructions with different lengths, LLMs learn to map input and output translations of different lengths, enabling robust translation abilities at both sentence and document levels. The approach significantly enhances document-level translation capabilities of Llama-2 7B and 13B models across 10 language pairs while maintaining consistent performance up to 2048 tokens.

## Method Summary
The method involves fine-tuning Llama-2 models (7B and 13B) using LoRA on translation mixed-instructions that combine sentence-level and document-level translation instructions of varying lengths. Long documents are segmented into sub-documents with maximum lengths of 512, 1024, 1536, or 2048 tokens. The fine-tuning process uses lora_r=8 and lora_alpha=16 for one epoch with batch size 32. The approach is evaluated on 10 language pairs using BLEU, COMET, and discourse phenomena metrics (tense consistency, conjunction presence, pronoun