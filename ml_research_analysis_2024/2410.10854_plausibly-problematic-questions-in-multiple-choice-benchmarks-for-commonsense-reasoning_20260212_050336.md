---
ver: rpa2
title: Plausibly Problematic Questions in Multiple-Choice Benchmarks for Commonsense
  Reasoning
arxiv_id: '2410.10854'
source_url: https://arxiv.org/abs/2410.10854
tags:
- question
- answer
- choices
- choice
- plausibility
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a new method for identifying problematic commonsense
  multiple-choice questions by using human plausibility ratings. The authors collect
  individual plausibility ratings for each answer choice and compare the most plausible
  answer to the gold answer.
---

# Plausibly Problematic Questions in Multiple-Choice Benchmarks for Commonsense Reasoning

## Quick Facts
- arXiv ID: 2410.10854
- Source URL: https://arxiv.org/abs/2410.10854
- Reference count: 23
- Primary result: Human plausibility ratings reveal that over 20% of commonsense reasoning questions have answer choices that are more plausible than the gold answer

## Executive Summary
This paper introduces a novel approach to identifying problematic multiple-choice questions in commonsense reasoning benchmarks by leveraging human plausibility ratings. Rather than relying solely on expert review or statistical analysis, the authors collect individual plausibility judgments for each answer option and compare the most plausible choice against the gold standard answer. They find that in a significant portion of cases, these do not align, suggesting the presence of questions that may not effectively measure the intended reasoning capabilities.

The study reveals that questions where human-rated plausibility diverges from gold answers exhibit higher rates of known quality issues such as ambiguity, vagueness, and semantic mismatches. Furthermore, experiments demonstrate that large language models perform worse on these "plausibly problematic" questions, showing both lower accuracy and higher variance in their predictions. This work provides both a methodological contribution for benchmark evaluation and empirical evidence that current commonsense reasoning datasets contain questions that may not effectively measure the intended reasoning capabilities.

## Method Summary
The authors propose a method for identifying problematic multiple-choice questions by collecting human plausibility ratings for each answer option. Using Amazon Mechanical Turk, they recruit annotators to rate the plausibility of each answer choice on a 1-5 scale, ensuring annotators only see questions from benchmarks they haven't previously worked with. For each question, they compare the most plausible answer (highest average rating) against the gold answer to identify "plausibly problematic" questions where these don't align. The method also captures annotator confidence through Likert scale responses, providing additional signal about question quality. The approach is validated against known question quality issues through annotation of a subset of questions for specific problems like ambiguity and semantic mismatch.

## Key Results
- Over 20% of questions show a mismatch between the most plausible answer and the gold answer
- Plausibly problematic questions have significantly higher rates of known quality issues (ambiguity, vagueness, semantic mismatch)
- Large language models show lower accuracy and higher variance when answering plausibly problematic questions compared to non-problematic ones

## Why This Works (Mechanism)
The mechanism works because human plausibility ratings capture intuitive judgments about answer reasonableness that may not align with the intended reasoning task. When the most plausible answer differs from the gold answer, it suggests the question may be testing something other than the intended commonsense reasoning capability - perhaps world knowledge, test-taking strategy, or simply identifying poorly constructed questions. The correlation between plausibility mismatches and known quality issues validates that this method effectively surfaces questions with structural problems that undermine their utility as benchmarks.

## Foundational Learning
- **Plausibility rating methodology**: Why needed - to systematically identify questions where intuitive answer reasonableness diverges from intended answers; Quick check - compare inter-annotator agreement rates across different question types
- **Quality issue annotation framework**: Why needed - to validate that plausibility mismatches correlate with known problematic question characteristics; Quick check - calculate Cohen's kappa for agreement on quality issue identification
- **LLM evaluation methodology**: Why needed - to demonstrate that plausibility-mismatched questions are genuinely more difficult for models to answer correctly; Quick check - compare accuracy and variance metrics across different model families

## Architecture Onboarding

**Component Map:**
AMT workers -> Plausibility ratings -> Most plausible answer identification -> Comparison with gold answer -> Problematic question classification -> Quality issue annotation -> LLM evaluation

**Critical Path:**
Human annotation (plausibility ratings) → Most plausible answer identification → Mismatch detection → Quality issue correlation → LLM performance analysis

**Design Tradeoffs:**
The methodology trades the scalability of automated quality assessment for the nuanced judgment of human annotators. While crowdsourcing plausibility ratings is more resource-intensive than statistical analysis alone, it captures intuitive answer reasonableness that purely quantitative methods might miss. The approach also assumes gold answers are correct, which may not always be true, but this simplifies the analysis and focuses on plausibility as the key metric.

**Failure Signatures:**
- High inter-annotator disagreement on plausibility ratings may indicate questions with genuinely ambiguous or borderline answers
- Systematic plausibility mismatches in certain question types may reveal benchmark construction biases
- LLM accuracy patterns that don't correlate with plausibility mismatches may suggest the method isn't capturing the intended difficulty factors

**Three First Experiments:**
1. Replicate the plausibility rating collection on a held-out validation set to confirm consistency
2. Test whether removing plausibly problematic questions improves benchmark reliability metrics
3. Apply the methodology to a different commonsense reasoning benchmark to test generalizability

## Open Questions the Paper Calls Out
None

## Limitations
- The methodology relies on human annotators whose cultural and demographic backgrounds may introduce biases in plausibility judgments
- The study focuses on a limited number of benchmarks, which may not generalize to all commonsense reasoning datasets
- The approach assumes gold answers are always correct, without accounting for potential errors in the original benchmarks

## Confidence
- **High Confidence**: LLM performance is demonstrably lower on plausibly problematic questions (accuracy decreases, variance increases)
- **Medium Confidence**: Correlation between plausibility mismatches and known quality issues is supported but requires broader validation
- **Medium Confidence**: Plausibility rating methodology is innovative but needs systematic validation across more benchmarks

## Next Checks
1. Apply the plausibility rating methodology to at least two additional commonsense reasoning benchmarks to test generalizability and identify benchmark-specific patterns
2. Conduct detailed inter-annotator agreement studies on plausibility ratings, including statistical tests for rater consistency and potential systematic biases
3. Perform targeted review of gold answers from identified problematic questions to assess whether the gold standard itself may contain errors or cultural biases