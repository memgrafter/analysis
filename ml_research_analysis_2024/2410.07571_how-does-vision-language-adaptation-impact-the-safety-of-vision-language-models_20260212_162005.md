---
ver: rpa2
title: How Does Vision-Language Adaptation Impact the Safety of Vision Language Models?
arxiv_id: '2410.07571'
source_url: https://arxiv.org/abs/2410.07571
tags:
- safety
- multimodal
- adaptation
- balloon
- have
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigates safety degradation in vision-language models
  (LVLMs) during vision-language adaptation, revealing that safety capabilities inherent
  in large language models (LLMs) are significantly compromised even when training
  data is safe. Through systematic experiments and internal weight analysis, we identify
  that VL adaptation disrupts safety-related layers and diverges from safety tuning
  objectives, resulting in harmful outputs and exaggerated refusals.
---

# How Does Vision-Language Adaptation Impact the Safety of Vision Language Models?

## Quick Facts
- **arXiv ID**: 2410.07571
- **Source URL**: https://arxiv.org/abs/2410.07571
- **Reference count**: 40
- **Primary result**: Vision-language adaptation significantly degrades safety in large language models, even with safe training data

## Executive Summary
This study investigates safety degradation in vision-language models (LVLMs) during vision-language adaptation, revealing that safety capabilities inherent in large language models (LLMs) are significantly compromised even when training data is safe. Through systematic experiments and internal weight analysis, we identify that VL adaptation disrupts safety-related layers and diverges from safety tuning objectives, resulting in harmful outputs and exaggerated refusals. We evaluate existing safety tuning approaches (supervised fine-tuning and RLHF) and find they either degrade helpfulness or fail to ensure complete safety. To address these challenges, we propose model weight merging as an efficient solution, demonstrating that merging safety-focused and multimodal-optimized models effectively balances safety and performance without extensive retraining.

## Method Summary
The study adapts LLaMA-2 Chat 7B and Tulu-2 7B using LLaV A-Pretrain and LLaV A-Instruct datasets, filtering for safety with LLaMA-Guard-3 8B and NSFW detection. Safety tuning employs VLGuard dataset with multitask and sequential learning approaches, while RLHF uses SafeRLHF and SPA-VL preference data. The proposed weight merging solution combines safety-tuned and multimodal-tuned models to balance performance. Models are evaluated across text-only safety (SorryBench, WildJailbreak), multimodal safety (MM-Safetybench, SIUO, Figstep), exaggerated safety (XSTest), and multimodal helpfulness (MMBench, MME, SEEDBench) benchmarks.

## Key Results
- VL adaptation causes significant safety degradation, increasing Attack Success Rate (ASR) on safety benchmarks despite safe training data
- Safety tuning methods (SFT, RLHF) either reduce helpfulness or fail to ensure complete safety
- Model weight merging effectively balances safety and multimodal performance without extensive retraining
- Internal layer analysis reveals VL adaptation disrupts safety-critical layers (layers 6-14), reducing the model's ability to recognize harmful content

## Why This Works (Mechanism)

### Mechanism 1
- Claim: VL adaptation alters safety-critical layers in LLMs, leading to loss of safety behaviors.
- Mechanism: The adaptation process changes the weights in layers responsible for safety judgment (Li et al., 2024), reducing the model's ability to recognize and refuse harmful content.
- Core assumption: Safety knowledge in LLMs is encoded in specific layers, and these layers are disrupted by vision-language fine-tuning.
- Evidence anchors:
  - [abstract] "Further analysis of internal model weights suggests that VL adaptation may impact certain safety-related layers, potentially lowering overall safety levels."
  - [section] "Our findings reveal that VL adaptation significantly alters weights in key safety-related layers."
  - [corpus] "Cross-Modal Safety Mechanism Transfer in Large Vision-Language Models" suggests this is a recognized problem.
- Break condition: If safety mechanisms are distributed evenly across layers, or if safety tuning can restore the original safety-layer activations regardless of VL adaptation.

### Mechanism 2
- Claim: Safety tuning and VL adaptation have divergent optimization objectives, causing interference when applied together.
- Mechanism: Multitask learning and sequential fine-tuning create conflicting parameter updates, where one objective degrades the performance of the other (Wei et al., 2024).
- Core assumption: Safety tuning and VL adaptation pull the model in different directions, and joint training cannot optimize both simultaneously.
- Evidence anchors:
  - [abstract] "our findings demonstrate that the objectives of VL adaptation and safety tuning are divergent, which often results in their simultaneous application being suboptimal."
  - [section] "safety tuning and visual instruction tuning alter the model's behavior in fundamentally different ways."
  - [corpus] "Bootstrapping LLM Robustness for VLM Safety via Reducing the Pretraining Modality Gap" suggests modality gap alignment is a separate problem.
- Break condition: If a unified loss function or architecture can align both objectives without conflict.

### Mechanism 3
- Claim: Weight merging can combine safety and multimodal capabilities without retraining.
- Mechanism: By linearly combining the weights of safety-tuned and multimodal-tuned models, the merged model inherits both capabilities without the interference seen in multitask learning.
- Core assumption: The parameters encoding safety and multimodal skills are complementary and can be linearly interpolated.
- Evidence anchors:
  - [abstract] "we suggest the weight merging approach as an optimal solution effectively reducing safety degradation while maintaining helpfulness."
  - [section] "Model weight merging is particularly beneficial when the objectives of two tasks...are different with each other."
  - [corpus] "Cross-Modal Safety Mechanism Transfer in Large Vision-Language Models" and "Bootstrapping LLM Robustness for VLM Safety" suggest complementary methods exist.
- Break condition: If the parameter spaces are incompatible or if safety and multimodal skills are encoded in overlapping, conflicting regions.

## Foundational Learning

- Concept: Catastrophic forgetting
  - Why needed here: VL adaptation causes the model to "forget" safety behaviors learned during pretraining, a form of catastrophic forgetting.
  - Quick check question: If you fine-tune a model on task A, then on task B, what happens to performance on task A? (Answer: It degrades unless preserved by special methods.)

- Concept: Layer-wise specialization in LLMs
  - Why needed here: Safety knowledge is hypothesized to reside in specific layers (Li et al., 2024), so understanding layer roles is critical for diagnosing degradation.
  - Quick check question: In a transformer, which layers typically handle high-level reasoning vs. low-level syntax? (Answer: Deeper layers handle higher-level reasoning.)

- Concept: Cosine similarity for hidden state comparison
  - Why needed here: The paper uses cosine similarity between hidden states to measure how VL adaptation changes internal representations.
  - Quick check question: If two hidden states are identical, what is their cosine similarity? (Answer: 1.0; if orthogonal, 0.0.)

## Architecture Onboarding

- Component map:
  - Vision encoder (frozen CLIP) → MLP projection layer → LLM (partially or fully fine-tuned)
  - Safety layers: layers 6-14 (Li et al., 2024)
  - Merging: Linear combination of two fine-tuned models

- Critical path:
  VL adaptation → safety degradation → safety tuning attempts → merging solution

- Design tradeoffs:
  - Full fine-tuning vs. partial freezing (SPPFT)
  - Multitask learning vs. sequential tuning vs. merging
  - Safety vs. helpfulness (exaggerated refusal)

- Failure signatures:
  - High ASR on safety benchmarks after VL adaptation
  - Low refusal rate on exaggerated safety benchmarks
  - Sharp drop in cosine similarity in safety layers during adaptation

- First 3 experiments:
  1. Run VL adaptation and measure ASR at each checkpoint to confirm safety degradation.
  2. Apply SPPFT (freeze safety layers) and compare ASR to full fine-tuning.
  3. Merge a safety-tuned model with a multimodal-tuned model and evaluate both ASR and accuracy.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the safety degradation observed in LVLMs apply universally across different model architectures and pretraining methods, or is it specific to the LLaMA-2 architecture?
- Basis in paper: [inferred] The paper primarily uses LLaMA-2 Chat 7B and Tulu-2 7B as base models, but acknowledges that "our focus on the LLaMA-2 architecture, though representative, may not capture the full range of safety impacts across other architectures."
- Why unresolved: The study's findings are based on experiments with LLaMA-2-based models. While the results show consistent safety degradation patterns, the authors explicitly state that broader exploration is needed for generalizability.
- What evidence would resolve it: Systematic experiments applying the same VL adaptation and safety tuning methods to diverse LLM architectures (e.g., GPT, Mistral, Vicuna) would demonstrate whether the safety degradation phenomenon is universal or architecture-specific.

### Open Question 2
- Question: What is the precise mechanism by which VL adaptation alters the safety layers in LVLMs, and can this process be mathematically modeled or predicted?
- Basis in paper: [explicit] "By analyzing the internal representations of the model, our findings reveal that VL adaptation significantly alters weights in key safety-related layers" and the authors suggest that "safety-related characteristics might be affected early on" during adaptation.
- Why unresolved: While the paper identifies that safety layers are altered during VL adaptation and provides cosine similarity analysis showing divergence, it does not provide a complete mechanistic explanation of how vision-language adaptation specifically disrupts safety-related representations.
- What evidence would resolve it: Detailed mathematical modeling of how visual information processing interferes with safety-related representations, combined with controlled experiments that isolate specific aspects of vision-language adaptation (e.g., multimodal projection, visual instruction tuning) would clarify the underlying mechanism.

### Open Question 3
- Question: How can safety tuning objectives be effectively aligned with VL adaptation objectives to prevent the observed trade-offs between safety and multimodal performance?
- Basis in paper: [explicit] "Our findings demonstrate that the objectives of safety tuning can be divergent from those of VL adaptation" and the authors note that "jointly training both tasks can lead to suboptimal outcomes."
- Why unresolved: The paper demonstrates that safety tuning and VL adaptation have competing objectives but does not provide a complete solution for aligning these objectives. While model weight merging is proposed as a practical solution, the fundamental challenge of objective alignment during training remains unresolved.
- What evidence would resolve it: Development and validation of new training methodologies that can simultaneously optimize for both safety and multimodal capabilities, potentially through novel loss functions, architectural modifications, or curriculum learning approaches that gradually integrate safety objectives during VL adaptation.

## Limitations

- The mechanism connecting layer-wise weight changes to safety loss remains correlational rather than definitively causal
- The proposed weight merging solution lacks comprehensive ablation studies to determine optimal merging ratios
- Evaluation relies heavily on automated benchmarks without extensive human evaluation to capture nuanced safety failures

## Confidence

**High Confidence:** The empirical observation that VL adaptation degrades safety performance, as measured by increased ASR on safety benchmarks. This finding is directly observable and consistently reproduced across different safety datasets.

**Medium Confidence:** The hypothesis that safety knowledge is encoded in specific layers that are disrupted by VL adaptation. While layer-wise analysis shows changes, the causal relationship requires further validation.

**Medium Confidence:** The claim that safety tuning and VL adaptation have fundamentally divergent optimization objectives. The interference is observable, but whether this represents irreconcilable conflict versus a tuning challenge remains uncertain.

## Next Checks

1. **Layer Ablation Study:** Systematically freeze or modify weights in safety-critical layers (layers 6-14) during VL adaptation to determine if targeted preservation of these layers prevents safety degradation. This would provide stronger causal evidence for the layer disruption hypothesis.

2. **Merging Ratio Optimization:** Conduct a comprehensive grid search over merging ratios (α values) with statistical significance testing to identify optimal combinations and determine whether benefits exceed simple ensemble effects.

3. **Human Evaluation Protocol:** Deploy the safety-degraded, safety-tuned, and merged models to human evaluators using standardized safety and helpfulness rubrics to validate whether automated metrics accurately reflect real-world performance and identify any safety failures that benchmarks miss.