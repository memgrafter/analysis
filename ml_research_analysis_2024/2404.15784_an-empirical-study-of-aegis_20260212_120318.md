---
ver: rpa2
title: An Empirical Study of Aegis
arxiv_id: '2404.15784'
source_url: https://arxiv.org/abs/2404.15784
tags:
- data
- mnist
- aegis
- accuracy
- adversarial
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper empirically studies the Aegis framework, which defends
  against bit-flip attacks on neural networks by using a dynamic-exit strategy with
  internal classifiers and robustness training. The authors evaluate Aegis on MNIST
  and CIFAR10 datasets, comparing models with robustness training, without it, and
  with data augmentation.
---

# An Empirical Study of Aegis

## Quick Facts
- arXiv ID: 2404.15784
- Source URL: https://arxiv.org/abs/2404.15784
- Authors: Daniel Saragih; Paridhi Goel; Tejas Balaji; Alyssa Li
- Reference count: 12
- Primary result: Aegis reduces accuracy on perturbed data and adversarial examples compared to baselines

## Executive Summary
This paper empirically studies the Aegis framework, which defends against bit-flip attacks on neural networks using a dynamic-exit strategy with internal classifiers and robustness training. The authors evaluate Aegis on MNIST and CIFAR10 datasets, comparing models with robustness training, without it, and with data augmentation. They find that robustness training and the dynamic-exit strategy have significant drawbacks: models with robustness training show reduced accuracy on perturbed data and adversarial examples compared to baselines. Data augmentation improves robustness to both bit-flip attacks and other adversarial attacks. The dynamic-exit strategy also becomes less uniform on simpler datasets like MNIST. Overall, while Aegis shows defensive capabilities, it has limitations in generalization and robustness to diverse attacks.

## Method Summary
The study evaluates Aegis by training ResNet and VGG models on MNIST and CIFAR10 datasets. The framework is tested with and without robustness training, and with data augmentation as an alternative to robustness training. The evaluation measures accuracy on clean data, perturbed data (bit-flip attacks), and standard adversarial examples (PGD, FGSM). The study also analyzes the exit distribution uniformity across internal classifiers to assess the effectiveness of the dynamic-exit strategy.

## Key Results
- Robustness training reduces accuracy on both perturbed data and adversarial examples compared to baseline models
- Data augmentation improves robustness to bit-flip attacks and other adversarial attacks
- Dynamic-exit distributions become non-uniform on simpler datasets like MNIST

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Aegis uses a dynamic-exit strategy with internal classifiers to mitigate bit-flip attacks.
- Mechanism: Multiple internal classifiers (ICs) are attached to hidden layers of the base model. During inference, a random subset of these ICs is sampled uniformly. The model exits at the first IC whose prediction confidence exceeds a threshold, making it harder for attackers to predict which layer will be used.
- Core assumption: Uniform sampling of ICs prevents attackers from targeting specific layers.
- Evidence anchors:
  - [abstract] "dynamic-exit strategy with internal classifiers and robustness training."
  - [section] "The first involves a dynamic-exit shallow-deep network (DESDN) where samples randomly exit the network early with a uniform probability distribution on the layers."
  - [corpus] Weak evidence; related papers focus on different defenses (e.g., adversarial learning, prompt injection).
- Break condition: If the dataset is too simple (like MNIST), the uniform exit distribution breaks down because early layers often reach high confidence.

### Mechanism 2
- Claim: Aegis uses robustness training to simulate bit-flip attacks during fine-tuning.
- Mechanism: During fine-tuning, a copy of the model is created with vulnerable bits flipped (selected by gradient ranking). The loss from both the clean and flipped models is summed and backpropagated to train the internal classifiers.
- Core assumption: Training with flipped bits makes the internal classifiers robust to actual bit-flip attacks.
- Evidence anchors:
  - [abstract] "evaluates a pre-trained model with the mechanisms fine-tuned on MNIST" and "robustness training of Aegis."
  - [section] "A second mechanism is robustness training (ROB) wherein the authors deliberately flipped vulnerable bits and trained the early exit classifiers using these features."
  - [corpus] Weak evidence; no direct mention of bit-flip robustness training in related papers.
- Break condition: Robustness training may reduce accuracy on clean data and other adversarial attacks.

### Mechanism 3
- Claim: Data augmentation improves robustness against both bit-flip and other adversarial attacks.
- Mechanism: Label-invariant transformations (rotation, translation, scaling, blur, erasing) are applied to training data. This exposes the model to a wider variety of perturbations, increasing its resilience.
- Core assumption: Augmentation simulates the effect of robustness training but with more informative perturbations.
- Evidence anchors:
  - [abstract] "We also compare the use of data augmentation to the robustness training of Aegis."
  - [section] "The augmentation techniques we applied include rotation, translation, scaling, Gaussian blur, and random erasing."
  - [corpus] Weak evidence; related papers do not discuss data augmentation in the context of bit-flip attacks.
- Break condition: Augmentation may not help if the attack type is not well-represented in the augmented data.

## Foundational Learning

- Concept: Neural network robustness and adversarial attacks
  - Why needed here: Understanding how bit-flip attacks work and why defenses like Aegis are needed.
  - Quick check question: What is a bit-flip attack and how does it differ from adversarial example attacks?
- Concept: Dynamic-exit strategies and early exiting
  - Why needed here: The Aegis framework relies on randomly exiting at internal classifiers to confuse attackers.
  - Quick check question: How does a uniform exit distribution help defend against targeted attacks?
- Concept: Data augmentation and its role in training
  - Why needed here: Augmentation is used as an alternative to robustness training to improve generalization.
  - Quick check question: Why might augmentation be more informative than flipped-bit training?

## Architecture Onboarding

- Component map:
  Base model (ResNet or VGG) -> Internal classifiers at hidden layers -> Robustness training module (optional) -> Data augmentation pipeline (optional)
- Critical path:
  1. Train base model on dataset.
  2. Attach internal classifiers.
  3. Fine-tune with robustness training or data augmentation.
  4. Evaluate on clean, perturbed, and adversarial data.
- Design tradeoffs:
  - Robustness training improves bit-flip defense but may hurt clean accuracy.
  - Data augmentation is more general but may not be as targeted.
  - Dynamic exit adds randomness but can break on simple datasets.
- Failure signatures:
  - Accuracy drops on clean data after robustness training.
  - Non-uniform exit distribution on simple datasets.
  - Poor performance on adversarial examples despite bit-flip defense.
- First 3 experiments:
  1. Train base model on MNIST, attach internal classifiers, evaluate on clean data.
  2. Apply robustness training, evaluate on clean and perturbed data.
  3. Replace robustness training with data augmentation, compare results.

## Open Questions the Paper Calls Out
None

## Limitations

- Robustness training paradoxically reduces accuracy on both perturbed data and standard adversarial examples
- Dynamic-exit strategy shows fundamental fragility on simpler datasets where uniform distributions break down
- Framework shows poor generalization to diverse attack types beyond bit-flip attacks

## Confidence

- Aegis reduces accuracy on perturbed data and adversarial examples compared to baselines: High confidence
- Data augmentation improves robustness while robustness training degrades it: Medium confidence
- Dynamic-exit distributions become non-uniform on simpler datasets: High confidence

## Next Checks

1. Evaluate Aegis on larger, more complex datasets (ImageNet, CIFAR-100) to assess scalability and whether the dynamic-exit strategy's breakdown on simple data extends to more challenging scenarios.

2. Compare computational costs quantitatively by measuring training time, memory usage, and inference latency between Aegis variants with and without robustness training and data augmentation.

3. Test Aegis against a broader spectrum of adversarial attacks beyond PGD and FGSM, including optimization-based attacks, physical-world attacks, and adaptive attacks specifically designed to circumvent dynamic-exit mechanisms.