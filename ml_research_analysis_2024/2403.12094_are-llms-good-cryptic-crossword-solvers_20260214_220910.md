---
ver: rpa2
title: Are LLMs Good Cryptic Crossword Solvers?
arxiv_id: '2403.12094'
source_url: https://arxiv.org/abs/2403.12094
tags:
- llms
- answer
- cryptic
- clue
- prompt
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper establishes benchmark results for three popular large
  language models (LLMs) on the challenging task of solving cryptic crossword puzzles.
  The authors evaluate LLaMA2, Mistral, and ChatGPT under zero-shot, few-shot, and
  fine-tuning scenarios using various data splits to test generalization.
---

# Are LLMs Good Cryptic Crossword Solvers?

## Quick Facts
- arXiv ID: 2403.12094
- Source URL: https://arxiv.org/abs/2403.12094
- Authors: Abdelrahman Sadallah; Daria Kotova; Ekaterina Kochmar
- Reference count: 5
- Primary result: All tested LLMs (LLaMA2, Mistral, ChatGPT) achieve accuracy far below human-level performance on cryptic crosswords, with best model reaching only 13% accuracy

## Executive Summary
This paper establishes benchmark results for three popular large language models on the challenging task of solving cryptic crossword puzzles. The authors evaluate LLaMA2, Mistral, and ChatGPT under zero-shot, few-shot, and fine-tuning scenarios using various data splits to test generalization. Results show that while ChatGPT performs better than open-source models, all LLMs struggle significantly with this task, achieving accuracy far below human-level performance. The best model (Mistral) reaches 13% accuracy on naive splits but drops to 1.2% on the most challenging disjoint split. The authors conclude that cryptic crosswords remain a difficult task for LLMs, highlighting opportunities for improvement through techniques like curriculum learning, chain-of-thought prompting, and mixture-of-experts models.

## Method Summary
The authors created a benchmark dataset of 1,500 cryptic crossword clues from various sources, with each clue paired with its correct answer. They evaluated three model families (LLaMA2, Mistral, ChatGPT) across three training paradigms: zero-shot (no training), few-shot (5 examples), and fine-tuning (training on subsets of data). The evaluation used five different data splits to test generalization: naive split, disjoint words split, unique words split, and two additional splits with varying word overlap between training and test sets. Performance was measured using exact match accuracy.

## Key Results
- ChatGPT achieved the highest accuracy at 8.5% on naive splits, while LLaMA2 and Mistral scored 5.3% and 13% respectively
- All models showed dramatic performance drops on disjoint splits, with Mistral falling from 13% to 1.2% accuracy
- Fine-tuning generally improved performance compared to zero-shot, but gains were limited
- Zero-shot performance was particularly poor across all models, indicating cryptic crosswords require specialized training

## Why This Works (Mechanism)
None

## Foundational Learning
### Cryptic Crossword Structure
- Why needed: Understanding the dual-layered wordplay and definition structure is essential for both human and AI solvers
- Quick check: Can identify the "straight" definition vs. the wordplay component in a sample clue

### Natural Language Processing
- Why needed: LLMs must parse complex linguistic patterns and semantic relationships in clues
- Quick check: Can correctly interpret ambiguous wordplay indicators like "about," "around," or "in"

### Machine Learning Fine-tuning
- Why needed: Models need task-specific training to learn the conventions of cryptic crossword construction
- Quick check: Can demonstrate improved performance on training data after fine-tuning

### Evaluation Metrics
- Why needed: Exact match accuracy may be too strict for measuring partial progress in cryptic solving
- Quick check: Can distinguish between completely wrong answers and partially correct ones

## Architecture Onboarding
### Component Map
LLM Architecture -> Training Paradigm -> Data Split -> Evaluation Metric

### Critical Path
Data preparation → Model selection → Training/fine-tuning → Evaluation → Analysis

### Design Tradeoffs
The paper chose exact match accuracy over partial credit scoring, which provides clear benchmarks but may underestimate model capabilities for solving components of clues.

### Failure Signatures
All models showed consistent failure patterns across data splits, with accuracy dropping dramatically when training and test sets had minimal word overlap, suggesting memorization rather than genuine understanding.

### First Experiments
1. Test chain-of-thought prompting by having models show their reasoning steps before providing final answers
2. Implement curriculum learning by training on progressively harder clue types
3. Compare mixture-of-experts models against single-model LLMs on the same datasets

## Open Questions the Paper Calls Out
### Open Question 1
- Question: Can chain-of-thought prompting improve LLM performance on cryptic crosswords?
- Basis in paper: [explicit] Authors suggest this as a promising future direction in the "Conclusions and Future Work" section.
- Why unresolved: The paper focuses on evaluating LLMs' current capabilities rather than testing advanced prompting techniques. Chain-of-thought prompting was not implemented in the experiments.
- What evidence would resolve it: Experiments comparing standard prompting vs. chain-of-thought prompting on the same models and datasets, measuring accuracy improvements.

### Open Question 2
- Question: Would curriculum learning improve LLM performance on cryptic crosswords?
- Basis in paper: [explicit] Authors mention this as a promising direction, citing Rozner et al. (2021) who showed curriculum learning improved T5 performance.
- Why unresolved: The paper did not implement curriculum learning with LLMs. The comparison with T5 + curriculum in Table 8 shows this approach has potential but wasn't tested with LLMs.
- What evidence would resolve it: Experiments training LLMs with curriculum learning (starting with simpler clues and progressing to harder ones) and measuring accuracy improvements compared to standard fine-tuning.

### Open Question 3
- Question: Would mixture-of-experts models outperform single-model LLMs on cryptic crosswords?
- Basis in paper: [explicit] Authors suggest this as a promising direction in the "Conclusions and Future Work" section, referencing the success of Mixtral.
- Why unresolved: The paper only tested single-model LLMs (LLaMA2, Mistral, ChatGPT) and did not explore mixture-of-experts architectures.
- What evidence would resolve it: Experiments comparing mixture-of-experts models (like Mixtral) against single-model LLMs on the same cryptic crossword datasets, measuring accuracy differences.

## Limitations
- Small dataset size (1,500 clues) may not fully represent the diversity of cryptic crossword clues
- Exact match accuracy metric is overly strict and doesn't capture partial credit or reasoning quality
- Only three model families tested, missing potentially better-performing architectures
- Limited exploration of advanced prompting strategies like chain-of-thought reasoning

## Confidence
- Dataset creation methodology: High
- Experimental design and execution: Medium
- Generalization of results to broader LLM capabilities: Medium
- Conclusions about fundamental LLM limitations: Medium

## Next Checks
1. Expand model evaluation to include newer LLMs and alternative prompting strategies - Test models like GPT-4, Claude, and Gemini using chain-of-thought prompting and step-by-step reasoning approaches to determine if architectural improvements or prompting techniques significantly impact performance.

2. Conduct qualitative error analysis - Perform detailed examination of model failures to identify whether errors stem from semantic understanding, wordplay parsing, or other specific aspects of cryptic clues. This would help pinpoint whether current architectures fundamentally lack certain reasoning capabilities.

3. Implement and evaluate curriculum learning approaches - Design a training curriculum that progressively introduces more complex clue types, starting with simpler anagrams and double definitions before advancing to more sophisticated cryptic constructions. Measure whether this approach yields better generalization than traditional fine-tuning.