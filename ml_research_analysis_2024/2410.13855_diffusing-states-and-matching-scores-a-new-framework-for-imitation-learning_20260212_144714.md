---
ver: rpa2
title: 'Diffusing States and Matching Scores: A New Framework for Imitation Learning'
arxiv_id: '2410.13855'
source_url: https://arxiv.org/abs/2410.13855
tags:
- learning
- score
- expert
- diffusion
- function
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces SMILING, a novel imitation learning framework
  that replaces adversarial discriminator training with score-matching techniques
  inspired by diffusion models. The method first trains a score function on expert
  state distributions, then iteratively estimates score functions for learner state
  distributions and uses the combination to define a cost function for policy updates.
---

# Diffusing States and Matching Scores: A New Framework for Imitation Learning

## Quick Facts
- **arXiv ID**: 2410.13855
- **Source URL**: https://arxiv.org/abs/2410.13855
- **Reference count**: 40
- **Primary result**: Introduces SMILING, an imitation learning framework using score-matching instead of adversarial training, achieving better stability and performance across continuous control tasks.

## Executive Summary
This paper introduces SMILING, a novel imitation learning framework that replaces adversarial discriminator training with score-matching techniques inspired by diffusion models. The method first trains a score function on expert state distributions, then iteratively estimates score functions for learner state distributions and uses the combination to define a cost function for policy updates. Theoretical analysis establishes first- and second-order instance-dependent regret bounds with linear scaling in horizon, demonstrating avoidance of compounding errors. Empirically, SMILING outperforms both GAN-style imitation learning baselines and discriminator-free methods across continuous control tasks, including complex humanoid behaviors like walking, sitting, crawling, and navigating obstacles, without requiring expert actions.

## Method Summary
SMILING addresses imitation learning from state-only demonstrations by using diffusion score matching instead of adversarial training. The approach involves three main steps: (1) pre-training a score function on expert state data using regression-based score matching with a diffusion model (DDPM architecture with 5K discretization steps), (2) iteratively estimating score functions on learner states from the replay buffer using data aggregation, and (3) updating the policy via RL (SAC for simpler tasks, DreamerV3 for complex humanoid tasks) on a cost function derived from the difference between expert and learner score functions. The method aims to achieve more stable training and better performance by avoiding the instability of adversarial discriminator training while maintaining strong theoretical guarantees.

## Key Results
- SMILING achieves superior performance over GAN-style imitation learning baselines and discriminator-free methods across continuous control tasks
- The method demonstrates better stability and sample efficiency than existing approaches
- SMILING successfully learns complex humanoid behaviors including walking, sitting, crawling, and obstacle navigation without requiring expert actions
- Theoretical analysis provides first- and second-order instance-dependent regret bounds that avoid compounding errors common in offline approaches

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Score-matching based diffusion models provide more stable and expressive training than adversarial discriminators for imitation learning
- **Mechanism:** Instead of training a discriminator to approximate f-divergences or IPMs between expert and learner state distributions, SMILING trains score functions to predict noise added by diffusion processes. This converts the adversarial training problem into a regression problem, which is more stable and less prone to mode collapse
- **Core assumption:** The score function of a distribution can be learned effectively via regression-based score-matching, and this score function contains sufficient information to measure distributional differences
- **Evidence anchors:**
  - [abstract] "diffusion models have emerged as a non-adversarial alternative to GANs that merely require training a score function via regression, yet produce generations of higher quality"
  - [section] "diffusion models are known to be more stable to train and produce higher quality samples in domains like audio and video"
  - [corpus] Weak evidence - related papers focus on different IL approaches but don't directly compare diffusion vs GAN-style methods
- **Break condition:** If the function class G for score functions cannot represent the true score functions, or if the diffusion process discretization introduces too much error

### Mechanism 2
- **Claim:** DS Divergence (Diffusion Score Divergence) provides tighter theoretical bounds than f-divergences or IPMs
- **Mechanism:** By measuring differences between diffusion score functions along the forward diffusion process, DS Divergence captures both the mean and variance differences between distributions. This leads to second-order instance-dependent regret bounds that scale with the minimum of expert and learner variances
- **Core assumption:** The diffusion score functions contain enough information to bound KL divergence, Hellinger distance, and total variation distance
- **Evidence anchors:**
  - [abstract] "we prove first- and second-order instance-dependent regret bounds, proving that our approach avoids the compounding errors that stymie offline approaches"
  - [section] "DS divergence is a strong divergence in the sense that, whenever the DS divergence between the two distributions is small, the KL divergence, Hellinger distance, and total variation distance are all small"
  - [corpus] Weak evidence - corpus doesn't provide theoretical comparisons of divergence measures
- **Break condition:** If the diffusion process doesn't converge properly, or if the Lipschitz continuity assumption on score functions is violated

### Mechanism 3
- **Claim:** The variance estimation technique allows faithful approximation of DS Divergence
- **Mechanism:** Direct score matching of the difference between expert and learner score functions can introduce large errors due to variance issues. SMILING instead estimates the variance term separately via additional score matching and subtracts it, ensuring accurate approximation of the true DS Divergence
- **Core assumption:** The variance term can be accurately estimated via score matching, and this estimation doesn't introduce additional bias
- **Evidence anchors:**
  - [section] "we need to get both the expectation (i.e. have an unbiased estimator) as well as the variance correct"
  - [section] "we need to estimate the variance term and subtract it from the score matching objective"
  - [corpus] Weak evidence - corpus focuses on different variance estimation techniques not directly applicable here
- **Break condition:** If the function class G for variance estimation is insufficient, or if the variance term is too large relative to the mean term

## Foundational Learning

- **Concept:** Diffusion processes and score functions
  - Why needed here: The entire algorithm relies on forward diffusion of state distributions and training score functions to reverse this diffusion
  - Quick check question: What is the relationship between the score function ∇ log pt and the noise added in the Ornstein-Uhlenbeck diffusion process?

- **Concept:** Integral Probability Metrics (IPMs) and f-divergences
  - Why needed here: Understanding why SMILING's approach differs from traditional IL methods that minimize IPMs or f-divergences via adversarial training
  - Quick check question: Why might a discriminator-based approach struggle to represent the optimal discriminator for exponential family distributions?

- **Concept:** Instance-dependent regret bounds and their relationship to variance
  - Why needed here: The theoretical results show that SMILING achieves bounds that depend on the minimum of expert and learner variances, which is tighter than traditional bounds
  - Quick check question: How does the second-order bound scale differently from the first-order bound when one of the policies has low variance?

## Architecture Onboarding

- **Component map:** Expert demonstrations → pre-train ge → for each iteration: collect states → train g(k) → compute cost c(k) → RL update → repeat
- **Critical path:** Expert demonstrations → pre-train ge → for each iteration: collect states → train g(k) → compute cost c(k) → RL update → repeat
- **Design tradeoffs:**
  - Diffusion steps vs. training stability: More discretization steps improve accuracy but increase computation
  - Function class expressiveness vs. sample complexity: More expressive score functions need more data to train
  - RL algorithm choice vs. task complexity: SAC works well for simpler tasks, DreamerV3 needed for complex humanoid behaviors
- **Failure signatures:**
  - Score functions diverging to infinity: Indicates numerical instability in diffusion training
  - RL policy collapsing to single behavior: Suggests discriminator collapse in DAC baseline
  - Slow convergence despite good expert demonstrations: May indicate insufficient function class expressiveness
- **First 3 experiments:**
  1. Implement score function training on expert data only (validate ge training)
  2. Add learner score function training with fixed expert score (validate g(k) training)
  3. Combine both score functions with simple RL update (validate full algorithm)

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the choice of diffusion process (e.g., Ornstein-Uhlenbeck vs other SDEs) affect the theoretical bounds and empirical performance of SMILING?
- Basis in paper: [explicit] The paper uses the Ornstein-Uhlenbeck process but mentions that the choice of diffusion process is flexible and could affect performance
- Why unresolved: The paper only evaluates the OU process and doesn't explore how alternative diffusion processes might impact results
- What evidence would resolve it: Comparative experiments using different diffusion processes (e.g., Variance Exploding/Amplifying processes) with theoretical analysis of how bound constants scale with different choices

### Open Question 2
- Question: What is the computational overhead of score matching compared to discriminator training, and how does this scale with state dimensionality and horizon length?
- Basis in paper: [inferred] The paper claims score matching is "significantly easier and more stable to train" but doesn't provide concrete runtime comparisons or complexity analysis
- Why unresolved: No empirical timing data or theoretical complexity analysis is provided to quantify the claimed advantages
- What evidence would resolve it: Detailed runtime comparisons across tasks of varying dimensionality, plus analysis of how computational complexity scales with state space dimension and horizon length

### Open Question 3
- Question: How does SMILING perform when the expert policy is stochastic or multi-modal, and can it capture and reproduce such behavior?
- Basis in paper: [explicit] The paper mentions that diffusion models have "strong ability to capture complex and multi-modal data" but only tests deterministic expert policies
- Why unresolved: All experiments use deterministic expert policies; the method's ability to handle stochastic or multi-modal expert behaviors is untested
- What evidence would resolve it: Experiments with stochastic expert policies (e.g., through action noise injection) and analysis of whether SMILING can reproduce the multi-modal behavior in the learned policy

## Limitations

- The theoretical analysis relies on Lipschitz continuity of score functions and bounded KL divergence assumptions that may not hold for all continuous control tasks
- The algorithm's performance depends heavily on diffusion discretization steps, score function architecture, and RL hyperparameters, with sensitivity not fully characterized
- The claim that DS Divergence provides fundamentally tighter bounds than all other divergence measures lacks direct comparative evidence

## Confidence

- **High confidence**: The core algorithmic framework (using score matching instead of adversarial training) is sound and well-supported by diffusion model literature. The experimental results demonstrating superior performance over baselines are reproducible.
- **Medium confidence**: The theoretical regret bounds are correctly derived under stated assumptions, but their practical implications for real-world imitation learning scenarios need further validation.
- **Low confidence**: The claim that DS Divergence provides fundamentally tighter bounds than all other divergence measures in practice lacks direct comparative evidence.

## Next Checks

1. **Score function sensitivity**: Systematically vary the diffusion discretization steps (e.g., 1K, 5K, 10K) and measure impact on training stability and final performance across multiple tasks.
2. **Function class expressiveness**: Compare performance using different score function architectures (linear, MLP with varying widths, residual networks) to quantify the impact of representational capacity.
3. **Baseline comparison refinement**: Implement additional GAN-style IL methods with improved discriminator architectures (beyond linear functions) to better isolate whether performance gains come from score matching or discriminator improvements.