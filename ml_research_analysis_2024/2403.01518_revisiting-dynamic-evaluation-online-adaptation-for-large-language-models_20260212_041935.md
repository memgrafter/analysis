---
ver: rpa2
title: 'Revisiting Dynamic Evaluation: Online Adaptation for Large Language Models'
arxiv_id: '2403.01518'
source_url: https://arxiv.org/abs/2403.01518
tags:
- online
- context
- learning
- dynamic
- adaptation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper explores online adaptation of large language models
  during test-time evaluation, known as dynamic evaluation. The authors investigate
  methods to extend context beyond the fixed window of transformers through online
  gradient updates, treating parameters as a form of memory that can adapt to distributional
  shifts in the data.
---

# Revisiting Dynamic Evaluation: Online Adaptation for Large Language Models

## Quick Facts
- arXiv ID: 2403.01518
- Source URL: https://arxiv.org/abs/2403.01518
- Authors: Amal Rannen-Triki; Jorg Bornschein; Razvan Pascanu; Marcus Hutter; Andras GyÃ¶rgy; Alexandre Galashov; Yee Whye Teh; Michalis K. Titsias
- Reference count: 12
- Primary result: Online adaptation consistently improves LLM performance during test-time evaluation, with Transformer-XL style adaptation being computationally efficient and LoRA providing favorable compute-performance trade-offs.

## Executive Summary
This paper explores online adaptation of large language models during test-time evaluation, treating parameters as a form of memory that can adapt to distributional shifts in the data. The authors investigate methods to extend context beyond fixed transformer windows through online gradient updates, comparing overlapping and Transformer-XL style adaptation approaches. They find the latter more computationally efficient while achieving similar performance. The primary experimental setup involves a 1B parameter transformer pretrained on C4, then fine-tuned on PG-19 books before being evaluated on a long sequence of PG-19 test books.

## Method Summary
The method involves pretraining transformer models (150M, 400M, 1B parameters) on the C4 dataset, fine-tuning on subsets of PG-19 training books, and evaluating on a concatenated sequence of 100 PG-19 test books (11.8M tokens). Online adaptation is implemented using Transformer-XL style streaming with varying increment sizes and update frequencies. LoRA adaptation is applied to MLPs as a parameter-efficient alternative. The evaluation compares cumulative log-loss between static and dynamic evaluation across different model sizes and context lengths.

## Key Results
- Online adaptation consistently improves performance over static evaluation across all model sizes and context lengths
- Models with shorter context windows can achieve competitive performance to those with longer contexts when using online adaptation, especially under significant distribution shift
- Transformer-XL style adaptation is more computationally efficient than overlapping adaptation while maintaining similar performance
- LoRA adaptation provides favorable compute-performance trade-offs with significantly lower memory requirements for trainable parameters

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Online adaptation improves performance by turning parameters into temporally changing states that serve as a form of memory beyond the fixed context window.
- Mechanism: Gradient updates during test-time evaluation allow the model to adapt to distributional shifts in the data, encoding information that exceeds the length of the context window.
- Core assumption: The distributional shift between training and evaluation data is significant enough to warrant online adaptation.
- Evidence anchors:
  - [abstract]: "we here emphasize the perspective that online adaptation turns parameters into temporally changing states and provides a form of context-length extension with memory in weights"
  - [section 1]: "LLMs rely heavily on context to condition the model towards desired behaviour. However, the prompt is a precious resource, and for transformers the cost of inference grows with the size of the attention window"
  - [corpus]: Weak evidence - neighboring papers focus on LoRA and fine-tuning efficiency, but don't directly address online adaptation as context extension

### Mechanism 2
- Claim: Models with shorter context windows can achieve competitive performance to those with longer contexts when using online adaptation.
- Mechanism: Online adaptation compensates for limited context by updating parameters to capture longer-term information and adapt to distributional changes.
- Core assumption: The cost of inference grows with the size of the attention window, making shorter contexts more desirable when possible.
- Evidence anchors:
  - [section 3.2]: "models with shorter context and online adaptation can achieve a competitive performance to the models with longer context"
  - [section 3.2]: "when the model is faced with a significant distribution shift, online learning with a smaller context window and/or a smaller model can lead to a better compute-performance Pareto front"
  - [corpus]: Moderate evidence - neighboring papers discuss LoRA and efficient fine-tuning, which aligns with the idea of using smaller models/contexts with adaptation

### Mechanism 3
- Claim: Online LoRA adaptation provides a favorable compute-performance trade-off with much lower memory requirements for trainable parameters.
- Mechanism: Low-rank matrices added to transformer layers can be adapted during online learning, reducing the number of parameters that need to be updated.
- Core assumption: The low-rank approximation is sufficient to capture the necessary information for adaptation.
- Evidence anchors:
  - [section 2]: "A successful approach proposed in Hu et al. [2021] consists of adding low-rank matrices to the transformer layers, and only adapting the parameters of these matrices during finetuning"
  - [section E]: "This regret plot shows that LoRA models achieve a lower performance than the fully finetuned model, but a significantly higher performance than the static model"
  - [corpus]: Strong evidence - multiple neighboring papers discuss LoRA and its variants for efficient fine-tuning

## Foundational Learning

- Concept: Distributional shift
  - Why needed here: The paper's main argument is that online adaptation is particularly beneficial when there is a significant distributional shift between training and evaluation data.
  - Quick check question: What is the difference between in-distribution and out-of-distribution data in the context of this paper?

- Concept: Transformer architecture
  - Why needed here: The paper experiments with different model sizes and context windows, and the effectiveness of online adaptation depends on the transformer's ability to attend to previously computed keys and values.
  - Quick check question: How does the Transformer-XL style adaptation differ from the overlapping approach in terms of computational efficiency?

- Concept: Gradient descent and parameter updates
  - Why needed here: Online adaptation involves updating model parameters at test time using gradient descent, which requires additional computational resources and memory for the optimizer state.
  - Quick check question: What are the trade-offs between updating parameters every nth forward step versus updating at every step?

## Architecture Onboarding

- Component map: Pretrained transformer on C4 -> Finetuned on PG-19 subset -> Evaluated on PG-19 test sequence with online adaptation

- Critical path:
  1. Pretrain transformer on C4 dataset
  2. Finetune on subset of PG-19 training set
  3. Evaluate on sequence of PG-19 test books using online adaptation
  4. Compare performance against static evaluation

- Design tradeoffs:
  - Context window size vs. computational cost
  - Update frequency vs. performance
  - Full parameter adaptation vs. parameter-efficient methods (e.g., LoRA)

- Failure signatures:
  - Poor performance despite online adaptation: distribution shift may be too small, or adaptation method may be insufficient
  - High computational cost: context window may be too large, or update frequency may be too high

- First 3 experiments:
  1. Compare static evaluation vs. dynamic evaluation on a 1B parameter model with 2048 context size
  2. Test Transformer-XL style adaptation with varying increment sizes relative to overlapping adaptation
  3. Explore compute-performance trade-offs by varying context size, update frequency, and number of finetuning samples for a 1B parameter model

## Open Questions the Paper Calls Out
None

## Limitations
- Contextual dependency on distributional shift: The paper's core claims about online adaptation's effectiveness hinge on the presence of significant distributional shift between training and evaluation data, which is not fully characterized.
- Computational overhead quantification: The paper lacks precise measurements of the additional computational cost incurred by online adaptation, particularly for the Transformer-XL style approach.
- Generalization across domains: All experiments focus on a single domain shift (C4 to PG-19), limiting the external validity of the claims.

## Confidence
**High confidence**: Claims about online adaptation improving performance over static evaluation are well-supported by experimental results across multiple model sizes and context lengths.

**Medium confidence**: Claims about models with shorter contexts achieving competitive performance through online adaptation are supported by experiments but rely on assumptions about distributional shift that aren't fully validated.

**Low confidence**: Claims about the fundamental mechanism of parameters serving as memory beyond the context window are more speculative, with experimental evidence primarily showing performance improvements rather than directly demonstrating the memory mechanism.

## Next Checks
1. Characterize the distributional shift between C4 and PG-19 using statistical measures (KL divergence, vocabulary overlap, topic modeling) to validate the assumption that significant shift drives adaptation benefits.

2. Measure and report the precise computational overhead (memory, FLOPs, wall-clock time) for different online adaptation configurations, including optimizer state storage and gradient computation costs.

3. Conduct experiments with LoRA on both MLPs and attention layers, varying the rank parameter systematically, to establish the full parameter-efficiency landscape and identify the optimal configuration for different adaptation scenarios.