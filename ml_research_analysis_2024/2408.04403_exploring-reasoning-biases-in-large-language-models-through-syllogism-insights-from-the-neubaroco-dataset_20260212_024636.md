---
ver: rpa2
title: 'Exploring Reasoning Biases in Large Language Models Through Syllogism: Insights
  from the NeuBAROCO Dataset'
arxiv_id: '2408.04403'
source_url: https://arxiv.org/abs/2408.04403
tags:
- reasoning
- task
- answer
- premise
- llms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The study evaluates whether current large language models (LLMs)
  exhibit human-like reasoning biases using a bilingual syllogism dataset called NeuBAROCO.
  The dataset contains 790 problems in English and Japanese, annotated for belief
  bias, conversion errors, and atmosphere effects.
---

# Exploring Reasoning Biases in Large Language Models Through Syllogism: Insights from the NeuBAROCO Dataset

## Quick Facts
- **arXiv ID**: 2408.04403
- **Source URL**: https://arxiv.org/abs/2408.04403
- **Reference count**: 11
- **Primary result**: LLMs exhibit human-like reasoning biases on syllogism tasks, with GPT-4 showing higher few-shot accuracy, but struggling particularly on neutral inferences

## Executive Summary
This study evaluates whether current large language models exhibit human-like reasoning biases using a bilingual syllogism dataset called NeuBAROCO. The dataset contains 790 problems in English and Japanese, annotated for belief bias, conversion errors, and atmosphere effects. Experiments on GPT-3.5, GPT-4, Llama-2, and Swallow models reveal that while LLMs perform well on entailment and contradiction tasks, they struggle with neutral inferences. GPT-4 notably shows higher accuracy in the few-shot setting. The study also introduces a Translate-and-Explain task, finding that errors primarily stem from reasoning rather than interpretation. Overall, LLMs demonstrate reasoning biases similar to humans, but there is significant room for improvement in neutral cases.

## Method Summary
The study constructs the NeuBAROCO dataset containing 790 syllogism problems in English and Japanese, annotated for four types of reasoning biases: belief bias, conversion errors, atmosphere effects, and neutral inferences. Four LLMs (GPT-3.5, GPT-4, Llama-2, and Swallow) are evaluated on these problems in both zero-shot and few-shot settings. The Translate-and-Explain task is introduced to distinguish between translation and reasoning errors. Performance is measured across different reasoning bias types, with particular attention to accuracy differences between entailment/contradiction versus neutral inferences.

## Key Results
- LLMs achieve high accuracy on entailment and contradiction tasks but struggle significantly with neutral inferences
- GPT-4 demonstrates notably higher accuracy in few-shot settings compared to other models
- The Translate-and-Explain task reveals that reasoning errors, rather than translation issues, are the primary source of mistakes
- LLMs exhibit reasoning biases similar to humans across belief bias, conversion errors, and atmosphere effects

## Why This Works (Mechanism)
The study leverages the structured nature of syllogistic reasoning to probe LLM reasoning capabilities. By using a bilingual dataset with controlled bias types, the researchers can isolate specific reasoning phenomena and compare LLM behavior to established human cognitive biases. The controlled experimental design allows for systematic evaluation of whether LLMs process logical relationships similarly to humans, despite potentially different underlying mechanisms.

## Foundational Learning
1. **Syllogistic reasoning** - Deductive logic using two premises to reach a conclusion; needed to understand the task structure and bias types
   - Quick check: Verify understanding of valid/invalid syllogisms and common reasoning errors

2. **Belief bias** - Tendency to accept conclusions that align with prior beliefs regardless of logical validity; needed to interpret performance on believability manipulations
   - Quick check: Identify examples where conclusions align or conflict with common knowledge

3. **Conversion errors** - Mistakenly reversing subject-object relationships in categorical statements; needed to understand one specific bias type
   - Quick check: Practice converting "All A are B" to "All B are A" and identifying errors

4. **Atmosphere effects** - Influence of premise quantifiers on conclusion selection; needed to interpret performance patterns
   - Quick check: Analyze how "all" vs "some" premises affect conclusion choices

5. **Neutral inferences** - Logical conclusions without clear entailment or contradiction; needed to understand performance gaps
   - Quick check: Distinguish between valid, invalid, and neutral syllogistic conclusions

## Architecture Onboarding

**Component map**: NeuBAROCO dataset -> LLM inference engine -> Reasoning bias classification -> Performance evaluation

**Critical path**: Problem encoding → Model inference → Output parsing → Bias type classification → Accuracy calculation

**Design tradeoffs**: Bilingual dataset construction balances cultural bias control with increased annotation complexity; few-shot vs zero-shot evaluation trades prompt engineering effort for generalization assessment

**Failure signatures**: High error rates on neutral inferences indicate difficulty with uncertain logical relationships; systematic errors on specific bias types suggest model susceptibility to cognitive biases

**3 first experiments**:
1. Test model performance on syllogisms with varying levels of semantic complexity while holding logical structure constant
2. Evaluate whether providing explicit logical form representations improves neutral inference accuracy
3. Compare performance across different prompt formulations (instruction-tuned vs chat-tuned) on the same reasoning tasks

## Open Questions the Paper Calls Out
None identified in the provided material.

## Limitations
- Dataset construction may introduce selection bias by focusing specifically on reasoning biases rather than comprehensive syllogism sampling
- Evaluation limited to four specific reasoning biases, potentially missing other human-like reasoning phenomena
- Study restricted to four LLMs, which may not represent the broader landscape of available models

## Confidence
- LLMs exhibit human-like reasoning biases: Medium
- GPT-4 shows higher few-shot accuracy: High
- Errors primarily stem from reasoning in Translate-and-Explain task: Medium

## Next Checks
1. Replicate the study using a larger and more diverse set of LLMs, including open-source models of varying sizes and architectures, to determine if the observed reasoning biases are consistent across the broader LLM landscape.

2. Conduct a human study with participants from diverse cultural and linguistic backgrounds to validate whether the NeuBAROCO dataset accurately captures human-like reasoning biases and to establish baseline human performance for comparison.

3. Perform a detailed error analysis on the neutral inferences, using techniques such as model introspection or attention visualization, to identify the specific factors contributing to the observed performance gap and to inform targeted improvements in LLM reasoning capabilities.