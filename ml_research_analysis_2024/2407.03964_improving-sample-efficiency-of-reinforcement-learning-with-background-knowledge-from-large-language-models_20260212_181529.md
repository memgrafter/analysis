---
ver: rpa2
title: Improving Sample Efficiency of Reinforcement Learning with Background Knowledge
  from Large Language Models
arxiv_id: '2407.03964'
source_url: https://arxiv.org/abs/2407.03964
tags:
- knowledge
- tasks
- background
- llms
- reward
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a framework that harnesses large language models
  (LLMs) to extract background knowledge of an environment and accelerate downstream
  reinforcement learning (RL) tasks via potential-based reward shaping. The key idea
  is to use pre-collected experience data to ground LLMs and request them to provide
  feedback on data samples based on their general understanding of the environment,
  forming task-agnostic background knowledge.
---

# Improving Sample Efficiency of Reinforcement Learning with Background Knowledge from Large Language Models

## Quick Facts
- **arXiv ID**: 2407.03964
- **Source URL**: https://arxiv.org/abs/2407.03964
- **Reference count**: 40
- **Primary result**: Framework achieves significant sample efficiency improvements in RL tasks by using LLMs to extract environment background knowledge via potential-based reward shaping.

## Executive Summary
This paper proposes a framework that leverages large language models (LLMs) to extract background knowledge of an environment and accelerate downstream reinforcement learning (RL) tasks through potential-based reward shaping. The key innovation is using pre-collected experience data to ground LLMs and request them to provide feedback on data samples based on their general understanding of the environment, forming task-agnostic background knowledge. This knowledge is then represented as potential functions for potential-based reward shaping, which maintains policy optimality while providing heuristic guidance. Experiments on Minigrid and Crafter domains show significant sample efficiency improvements across multiple downstream tasks, with the acquired background knowledge generalizing to unseen tasks beyond the scope of provided data.

## Method Summary
The framework collects diverse trajectories using an RL algorithm (e.g., RND) without specifying task goals, then grounds LLMs with this pre-collected data to extract background knowledge. Three variants instantiate this approach: BK-CODE has LLMs write Python functions to compute potential values; BK-PREF has LLMs rank trajectory pairs, with a preference predictor learning the potential function; and BK-GOAL has LLMs suggest potential goals, with a retrieval mechanism computing goal similarity as potential. The extracted knowledge is represented as potential functions for potential-based reward shaping, applied to downstream RL tasks using algorithms like PPO. The framework is evaluated on Minigrid and Crafter domains, demonstrating improved sample efficiency and generalization to unseen tasks.

## Key Results
- Framework achieves significant sample efficiency improvements across multiple downstream RL tasks on Minigrid and Crafter domains
- Background knowledge extracted from pre-collected data generalizes to unseen tasks beyond the scope of provided data
- Three LLM feedback variants (code programming, preference annotation, goal suggestion) all improve performance, with BK-PREF showing particularly strong results

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Pre-collected experience data grounds LLMs to extract task-agnostic background knowledge without requiring environment interaction during knowledge extraction.
- Mechanism: By providing LLMs with pre-collected trajectories from multiple tasks, the LLM can infer general environmental patterns (e.g., avoid walls, collect resources) that are independent of specific task goals. This knowledge is then encoded as potential functions for reward shaping.
- Core assumption: LLMs can generalize from diverse task experiences to extract environmental principles that are task-agnostic.
- Evidence anchors:
  - [abstract]: "We ground LLMs by feeding a few pre-collected experiences and requesting them to delineate background knowledge of the environment."
  - [section III-A]: "We pre-collect a dataset D = {(s, a)} by directly deploying an RL algorithm in an open environment without specifying its task goal."
  - [corpus]: Weak evidence - no direct citations support LLM generalization from pre-collected data, though related works exist on LLM grounding.

### Mechanism 2
- Claim: Representing background knowledge as potential functions maintains policy optimality while accelerating learning through potential-based reward shaping.
- Mechanism: The background knowledge is encoded as a potential function ϕ(s) that defines shaped rewards F(s,s') = γϕ(s') - ϕ(s). This preserves the optimal policy while providing heuristic guidance.
- Core assumption: Potential-based reward shaping maintains policy optimality as proven in Ng et al. (1999).
- Evidence anchors:
  - [section III-A]: "potential-based reward shaping [26], [45] is a useful technique for altering RL processes...it can maintain the policy optimality of original problems."
  - [section III-A]: "We can define the potential function ϕ : S → R instead of defining F : S × S → R."
  - [corpus]: Strong evidence - cited works on potential-based reward shaping (Ng et al., 1999) and recent applications (Goyal et al., 2019; Mazumder et al., 2022).

### Mechanism 3
- Claim: Three variants (code programming, preference annotation, goal suggestion) provide different trade-offs in background knowledge representation flexibility and computational requirements.
- Mechanism: 
  - BK-CODE: LLMs write Python functions that directly compute potential values
  - BK-PREF: LLMs rank trajectory pairs, and a preference predictor learns the potential function
  - BK-GOAL: LLMs suggest potential goals, and a retrieval mechanism computes goal similarity as potential
- Core assumption: Different LLM feedback modalities can effectively capture the same underlying environmental knowledge.
- Evidence anchors:
  - [section IV]: "We design three variants including code programming...preference annotation...and goal suggestion."
  - [section IV-A]: "We prompt LLMs to write code for a function def compute_reward(obs, past_obs, past_actions)."
  - [section IV-B]: "We sample two trajectories τ 0, τ 1 from pre-collected data and ask LLMs to provide preferences over the given pair."
  - [corpus]: Moderate evidence - related works on LLM-generated code (Ma et al., 2023; Yu et al., 2023) and preference learning (Klissarov et al., 2023; Lee et al., 2023).

## Foundational Learning

- **Concept**: Markov Decision Process (MDP)
  - Why needed here: The paper models reinforcement learning tasks as MDPs, and reward shaping operates within this framework.
  - Quick check question: What are the five components of an MDP and how does reward shaping modify the return calculation?

- **Concept**: Potential-based reward shaping
  - Why needed here: This is the core technique used to incorporate background knowledge while preserving policy optimality.
  - Quick check question: What mathematical property ensures that potential-based reward shaping maintains the optimal policy?

- **Concept**: Large Language Model (LLM) prompting techniques
  - Why needed here: The framework relies on effectively prompting LLMs to extract useful background knowledge from trajectories.
  - Quick check question: What are the key components of an effective prompt for asking an LLM to analyze agent trajectories and provide feedback?

## Architecture Onboarding

- **Component map**: Data Collection Module -> LLM Grounding Interface -> Knowledge Representation Engine -> Potential Function Module -> RL Training Pipeline -> Evaluation Suite

- **Critical path**: Data collection → LLM grounding → Knowledge representation → Potential function creation → RL training → Evaluation

- **Design tradeoffs**:
  - BK-CODE offers no additional model overhead but requires LLM to generate valid code
  - BK-PREF requires a parameterized preference predictor but works with any observation type
  - BK-GOAL needs text captions during training but provides interpretable goal-based guidance
  - Tradeoff between prompt simplicity (fewer tokens) and instruction clarity

- **Failure signatures**:
  - Poor sample efficiency improvement → Background knowledge may not be generalizable or potential function poorly designed
  - Runtime errors in generated code → BK-CODE variant failure
  - No convergence in preference learning → BK-PREF variant failure
  - Slow retrieval in goal library → BK-GOAL variant bottleneck

- **First 3 experiments**:
  1. Verify reward shaping preserves optimality: Train PPO with BK-CODE on simple GoTo task and compare final performance with unshaped rewards
  2. Test knowledge generalization: Train with data from GoToRedBall tasks, evaluate on GoToPurpleKey (unseen object/color)
  3. Ablate data diversity: Compare performance using RND-collected data vs. random policy data for background knowledge extraction

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions in the provided text.

## Limitations
- Framework performance depends heavily on LLM generalization capabilities from pre-collected data, which may not hold for environments with sparse rewards or complex dynamics
- Approach requires diverse pre-collected data to extract meaningful background knowledge, limiting applicability to novel environments
- Performance sensitive to prompt quality and data diversity, with no systematic exploration of optimal data collection strategies

## Confidence
- **High confidence**: The mechanism of potential-based reward shaping preserving policy optimality (Mechanism 2) is well-established in RL literature
- **Medium confidence**: The effectiveness of the three LLM feedback variants (Mechanism 3) depends on prompt quality and environment type, which may vary
- **Low confidence**: The generalizability of background knowledge to unseen tasks (Mechanism 1) is promising but not extensively validated across diverse environments

## Next Checks
1. Test the framework on environments with sparse rewards or complex dynamics to assess LLM generalization limits
2. Compare the performance of the three feedback variants (BK-CODE, BK-PREF, BK-GOAL) across multiple environment types to identify optimal use cases
3. Evaluate the impact of pre-collected data diversity on the quality of background knowledge extraction by varying the number and types of tasks used for grounding