---
ver: rpa2
title: 'Prompt-RAG: Pioneering Vector Embedding-Free Retrieval-Augmented Generation
  in Niche Domains, Exemplified by Korean Medicine'
arxiv_id: '2401.11246'
source_url: https://arxiv.org/abs/2401.11246
tags:
- document
- medicine
- korean
- prompt-rag
- embedding
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study introduces Prompt-RAG, a novel retrieval-augmented generation
  approach that eliminates reliance on vector embeddings by using natural language
  prompts and table of contents for document retrieval. Experiments in the Korean
  Medicine domain show that LLM-based vector embeddings are heavily influenced by
  token overlaps rather than human-assessed document relatedness, leading to suboptimal
  performance.
---

# Prompt-RAG: Pioneering Vector Embedding-Free Retrieval-Augmented Generation in Niche Domains, Exemplified by Korean Medicine

## Quick Facts
- arXiv ID: 2401.11246
- Source URL: https://arxiv.org/abs/2401.11246
- Reference count: 0
- Primary result: Prompt-RAG eliminates vector embeddings by using natural language prompts and table of contents for document retrieval, achieving higher relevance and informativeness scores than conventional RAG models in Korean Medicine domain

## Executive Summary
This paper introduces Prompt-RAG, a novel retrieval-augmented generation approach that eliminates the need for vector embeddings by using natural language prompts and table of contents for document retrieval. Experiments in the Korean Medicine domain demonstrate that LLM-based vector embeddings are heavily influenced by token overlaps rather than human-assessed document relatedness, leading to suboptimal performance. Prompt-RAG-based chatbot outperforms ChatGPT and conventional vector embedding-based RAG models in relevance (1.956 vs 1.833-1.711) and informativeness (1.589 vs 0.667-1.033), particularly for direct retrieval (5.5/6) and comprehensive understanding (5.389/6) questions, though with longer response times (24.84s vs 6.45-7.03s). The approach demonstrates potential for niche domains requiring RAG methods.

## Method Summary
Prompt-RAG operates through a two-stage LLM process: first, GPT-4 selects the five most relevant document sections by interpreting the query and consulting a table of contents; second, GPT-3.5-turbo-16k generates answers using the retrieved sections as reference. The system preprocesses Korean Medicine documents by extracting or generating table of contents and segmenting the document body by headings. The approach is evaluated against ChatGPT and conventional chunk retrieval baselines using 30 Korean Medicine questions across three types (direct retrieval, comprehensive understanding, functional robustness), with responses assessed by three Korean Medicine doctors on relevance, readability, and informativeness scales.

## Key Results
- Prompt-RAG achieved relevance score of 1.956 compared to 1.833-1.711 for baselines
- Informativeness score of 1.589 outperformed baselines at 0.667-1.033
- Direct retrieval questions scored 5.5/6 and comprehensive understanding scored 5.389/6
- Response time of 24.84s was significantly longer than baselines (6.45-7.03s)

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Prompt-RAG eliminates the dependency on vector embeddings by using natural language prompts and a table of contents for document retrieval, simplifying the RAG architecture and improving performance in niche domains.
- **Mechanism**: The model leverages the natural language processing capabilities of large language models to directly interpret queries and select relevant document sections using a table of contents, bypassing the need for vector embeddings and complex retrieval algorithms.
- **Core assumption**: The latest large language models have sufficient emergent reasoning capabilities to accurately select relevant document sections based on natural language queries without relying on vector similarity metrics.
- **Break condition**: If the LLM cannot reliably interpret the query or if the table of contents is too large or poorly structured, the retrieval step will fail and degrade performance.

### Mechanism 2
- **Claim**: In niche domains like Korean Medicine, LLM-based vector embeddings are heavily influenced by token overlaps rather than human-assessed document relatedness, leading to suboptimal retrieval performance.
- **Mechanism**: The embedding models capture surface-level lexical similarity (token overlap) more than semantic relatedness in specialized domains, causing irrelevant documents to be retrieved when using conventional RAG methods.
- **Core assumption**: Token overlap is a weaker proxy for semantic relatedness in domains with specialized terminology and limited training data, compared to more general domains.
- **Break condition**: If the domain becomes less niche or if embeddings are fine-tuned with domain-specific data, the correlation between token overlap and human relatedness may improve.

### Mechanism 3
- **Claim**: Prompt-RAG's direct retrieval approach, combined with LLM reasoning, achieves higher relevance and informativeness scores than both ChatGPT (without retrieval) and conventional vector embedding-based RAG models in the Korean Medicine domain.
- **Mechanism**: By using the LLM to interpret queries and select document sections, Prompt-RAG retrieves more contextually relevant information, which leads to more accurate and informative responses, even though response time is longer.
- **Core assumption**: The LLM's ability to understand nuanced queries and select appropriate sections outweighs the latency cost of the additional retrieval step.

## Foundational Learning

- **Concept**: Understanding the difference between token overlap and semantic relatedness in document embeddings.
  - **Why needed here**: The paper argues that in niche domains, embeddings correlate more with token overlap than with human-assessed relatedness, which is a key reason why Prompt-RAG is needed.
  - **Quick check question**: Why might two documents with high token overlap still be semantically unrelated in a specialized domain like Korean Medicine?

- **Concept**: How table of contents-based retrieval works in contrast to vector-based retrieval.
  - **Why needed here**: Prompt-RAG uses the table of contents to guide the LLM in selecting relevant sections, which is fundamentally different from retrieving based on vector similarity.
  - **Quick check question**: What are the advantages and disadvantages of using a table of contents for retrieval compared to vector embeddings?

- **Concept**: The structure and evaluation criteria for retrieval-augmented generation systems.
  - **Why needed here**: The paper evaluates models on relevance, readability, and informativeness, which are standard metrics for RAG systems.
  - **Quick check question**: How do the three evaluation criteria (relevance, readability, informativeness) differ, and why is each important for assessing RAG performance?

## Architecture Onboarding

- **Component map**: Query → Heading selection (LLM) → Section retrieval → Answer generation (LLM)
- **Critical path**: Query → Heading selection (LLM) → Section retrieval → Answer generation (LLM)
- **Design tradeoffs**:
  - Latency vs. quality: Prompt-RAG is slower but more accurate than vector embedding-based RAG.
  - Token usage: Requires careful management of context window and token limits for both heading selection and answer generation.
  - ToC dependency: Performance depends on the quality and structure of the table of contents.
- **Failure signatures**:
  - Headings selected are irrelevant → Retrieval fails, answer is poor.
  - Reference too large for context window → Truncation or summarization needed, possible loss of context.
  - LLM misinterprets query → Wrong sections selected, irrelevant answer.
- **First 3 experiments**:
  1. Compare heading selection accuracy with different LLM models (e.g., GPT-4 vs. GPT-3.5) on a small set of queries.
  2. Measure the impact of ToC size on retrieval quality and latency.
  3. Test the effect of varying the number of headings selected (e.g., 3 vs. 5) on answer quality.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: How do the vector embeddings of LLM-based models differ in their representation of domain-specific terminology between Korean Medicine and Conventional Medicine, and what are the implications for RAG performance?
- **Basis in paper**: Explicit - The paper explicitly compares the performance of LLM-based vector embeddings in Korean Medicine (KM) and Conventional Medicine (CM) domains, finding that KM document embeddings correlate more with token overlaps and less with human-assessed document relatedness compared to CM embeddings.
- **Why unresolved**: While the paper identifies the issue, it does not delve into the specific reasons behind the differing performance of vector embeddings in KM and CM domains, nor does it explore the potential implications for RAG performance in other niche domains.
- **What evidence would resolve it**: Further research could involve a detailed analysis of the vector embeddings generated for domain-specific terminology in KM and CM, comparing the frequency and distribution of tokens, and examining the impact on RAG performance in both domains.

### Open Question 2
- **Question**: Can Prompt-RAG be effectively applied to other niche domains beyond Korean Medicine, and what are the potential challenges and limitations?
- **Basis in paper**: Explicit - The paper discusses the potential of Prompt-RAG to be applied to other domains in need of RAG methods, but it does not provide concrete examples or explore the specific challenges and limitations that may arise in different domains.
- **Why unresolved**: While the paper highlights the versatility of Prompt-RAG, it does not provide sufficient evidence or analysis to support its effectiveness in other niche domains or address the potential challenges and limitations that may arise.
- **What evidence would resolve it**: Future research could involve applying Prompt-RAG to other niche domains and evaluating its performance in terms of relevance, readability, and informativeness. Additionally, researchers could explore the potential challenges and limitations of Prompt-RAG in different domains, such as the availability of domain-specific knowledge and the complexity of the domain.

### Open Question 3
- **Question**: How does the performance of Prompt-RAG compare to other state-of-the-art RAG methods, such as those using fine-tuned language models or hybrid approaches?
- **Basis in paper**: Inferred - While the paper compares the performance of Prompt-RAG to ChatGPT and conventional vector embedding-based RAG models, it does not explicitly compare it to other state-of-the-art RAG methods.
- **Why unresolved**: The paper does not provide a comprehensive comparison of Prompt-RAG to other RAG methods, making it difficult to assess its relative performance and identify its strengths and weaknesses.
- **What evidence would resolve it**: Future research could involve benchmarking Prompt-RAG against other state-of-the-art RAG methods using standardized datasets and evaluation metrics. This would provide a more comprehensive understanding of Prompt-RAG's performance and its potential advantages and disadvantages compared to other approaches.

## Limitations
- Findings are confined to Korean Medicine domain with domain-specific documents and 30 evaluation questions
- Heavy reliance on GPT-4 for heading selection creates potential cost and scalability barriers
- The claim about token overlap correlation lacks broader empirical validation beyond this specific domain
- Longer response times (24.84s) may limit practical deployment despite improved accuracy

## Confidence
- **High confidence**: Prompt-RAG outperforms both ChatGPT and conventional RAG models in the Korean Medicine domain based on human evaluation metrics
- **Medium confidence**: The mechanism that LLM-based embeddings correlate more with token overlap than human-assessed relatedness in niche domains is plausible but requires validation across multiple specialized domains
- **Medium confidence**: The architectural claim that Prompt-RAG eliminates vector embedding dependencies is technically sound but depends on the LLM's consistent reasoning capabilities

## Next Checks
1. Test Prompt-RAG across 3-5 additional niche domains (e.g., legal, medical specialties, technical domains) to validate if token overlap correlation with embeddings is a general phenomenon in specialized fields
2. Conduct ablation studies varying the number of headings selected (2, 3, 5, 7) to determine optimal tradeoff between retrieval comprehensiveness and context window constraints
3. Implement performance benchmarking with different LLM models for heading selection to assess whether the approach scales economically beyond GPT-4, particularly for production deployments