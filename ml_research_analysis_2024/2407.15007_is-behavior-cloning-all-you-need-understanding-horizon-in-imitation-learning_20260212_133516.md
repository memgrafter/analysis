---
ver: rpa2
title: Is Behavior Cloning All You Need? Understanding Horizon in Imitation Learning
arxiv_id: '2407.15007'
source_url: https://arxiv.org/abs/2407.15007
tags:
- learning
- policy
- imitation
- online
- expert
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the sample complexity of imitation learning
  (IL), focusing on the role of horizon. It revisits the gap between offline and online
  IL, specifically whether online methods truly offer superior sample complexity.
---

# Is Behavior Cloning All You Need? Understanding Horizon in Imitation Learning

## Quick Facts
- arXiv ID: 2407.15007
- Source URL: https://arxiv.org/abs/2407.15007
- Authors: Dylan J. Foster; Adam Block; Dipendra Misra
- Reference count: 40
- The paper shows that offline IL with logarithmic loss can achieve horizon-independent sample complexity for deterministic experts, challenging the assumption that online IL is inherently more sample-efficient.

## Executive Summary
This paper revisits the fundamental question of whether online imitation learning is truly more sample-efficient than offline methods. Through a new analysis of behavior cloning with logarithmic loss, the authors demonstrate that horizon-independent sample complexity is achievable under certain conditions. The key insight is that the logarithmic loss allows for tighter generalization bounds through Hellinger distance rather than total variation, enabling horizon-independent performance for both deterministic and stochastic experts under specific conditions.

## Method Summary
The core method involves behavior cloning using logarithmic loss (LogLossBC) rather than traditional indicator or MSE losses. The approach interprets LogLossBC as maximum likelihood estimation over trajectory distributions, enabling the use of Hellinger distance bounds. For deterministic experts, the analysis exploits the symmetry of trajectory-wise distance measures. For stochastic experts, variance-dependent bounds are derived using stopping time arguments to truncate advantage sums. The method is validated across MuJoCo Walker2d-v4, Atari BeamRiderNoFrameskip-v4, Car racing, and Dyck language environments with varying horizon lengths.

## Key Results
- Offline IL with LogLossBC achieves linear dependence on horizon for deterministic, stationary policies, matching online IL performance
- Online IL cannot improve upon offline IL with logarithmic loss without additional assumptions on the policy class
- For stochastic policies, quadratic dependence on horizon is necessary under dense rewards, but logarithmic loss can sidestep this under recoverability conditions
- Experimental validation shows horizon-independent performance of logarithmic loss BC across multiple environments

## Why This Works (Mechanism)

### Mechanism 1
Log-loss behavior cloning avoids quadratic horizon dependence by bounding Hellinger distance instead of total variation. The log-loss naturally induces maximum likelihood estimation over trajectory distributions, allowing tighter change-of-measure arguments. Core assumption: realizability and controlled supervised learning complexity for the policy class.

### Mechanism 2
For deterministic experts, trajectory-wise distance is symmetric and equivalent to Hellinger distance up to a constant factor. This allows switching from imitator's state distribution to expert's for free, avoiding error amplification. Core assumption: expert policy is deterministic.

### Mechanism 3
For stochastic experts, variance-dependent bounds replace horizon dependence when cumulative rewards are bounded. A stopping time argument truncates the sum of advantages at a bounded value L, allowing concentration arguments to apply. Core assumption: cumulative rewards are normalized and expert policy has controlled variance.

## Foundational Learning

- **Hellinger distance**: A divergence measure for probability distributions that provides tighter bounds than total variation for log-loss generalization. Why needed: Enables horizon-independent sample complexity bounds. Quick check: Why is Hellinger distance more suitable than total variation for bounding log-loss generalization error?

- **Stopping time arguments**: Techniques from martingale theory that allow truncation of random sums at bounded values. Why needed: Enables variance-dependent bounds for stochastic experts by controlling the sum of advantages. Quick check: How does defining a stopping time based on the sum of advantages help control regret for stochastic experts?

- **Realizability**: The assumption that the expert policy is contained in the policy class. Why needed: Enables clean sample complexity bounds by ensuring the optimal policy is learnable. Quick check: How does realizability simplify the analysis of imitation learning sample complexity?

## Architecture Onboarding

### Component Map
Expert RL training -> Trajectory dataset collection -> Behavior cloning (LogLoss/MSE) -> Policy evaluation and regret computation

### Critical Path
The critical path flows from expert policy training through dataset collection to imitator training and evaluation. The key bottleneck is ensuring proper reward normalization and expert policy quality before behavior cloning begins.

### Design Tradeoffs
- LogLoss vs MSE: LogLoss enables horizon-independent bounds but requires proper probability modeling
- Online vs Offline: Online methods offer potential representational benefits but not necessarily better sample complexity
- Policy class design: Parameter sharing can reduce complexity but may limit expressiveness

### Failure Signatures
- MSE loss shows poor convergence or high variance across horizons
- Horizon dependence persists despite log-loss usage, indicating reward normalization issues
- Imbalanced performance across different horizon lengths suggests expert policy quality problems

### First 3 Experiments
1. Verify expert policy quality by testing horizon-independent performance with normalized rewards
2. Compare LogLossBC vs MSE loss training stability and convergence across multiple random seeds
3. Test policy class properties by measuring supervised learning complexity empirically

## Open Questions the Paper Calls Out

### Open Question 1
Can offline imitation learning algorithms achieve sub-quadratic horizon dependence for general policy classes? The paper shows this is possible under specific conditions but doesn't address all policy classes.

### Open Question 2
What are the benefits of online interaction in imitation learning beyond horizon dependence? The paper discusses potential benefits but lacks comprehensive empirical validation.

### Open Question 3
How does misspecification affect the role of horizon in imitation learning? The paper focuses on realizable settings but acknowledges misspecification as important.

### Open Question 4
Can the logarithmic loss be beneficial for imitation learning in a minimax sense? The paper demonstrates benefits but questions remain about generalizability.

### Open Question 5
What are the optimal complexity measures for characterizing minimax sample complexity of online and offline IL for any policy class? The paper highlights this need but doesn't provide definitive answers.

## Limitations

- Theoretical claims rely heavily on idealized assumptions including realizability and bounded cumulative payoffs
- Empirical validation is limited to specific environments rather than comprehensive coverage
- Core mechanisms are primarily theoretical contributions without extensive experimental validation

## Confidence

- **High confidence**: LogLossBC achieving horizon-independent sample complexity for deterministic experts under realizability
- **Medium confidence**: Variance-dependent bounds for stochastic experts depending on expert variance control
- **Medium confidence**: Symmetry of trajectory-wise distance for deterministic experts requiring careful verification

## Next Checks

1. Verify expert policy quality by testing whether trained experts achieve horizon-independent performance when normalized rewards are bounded by 1

2. Experimentally validate whether supervised learning complexity Npol(Π, ε) = O(1) holds for neural network architectures used in practice

3. Test scenarios where expert policies are stochastic or have variance scaling with horizon to verify theoretical predictions about quadratic dependence