---
ver: rpa2
title: 'Trusting Fair Data: Leveraging Quality in Fairness-Driven Data Removal Techniques'
arxiv_id: '2405.12926'
source_url: https://arxiv.org/abs/2405.12926
tags:
- data
- fairness
- fair
- discrimination
- problem
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper addresses the trustworthiness issues of fairness-driven
  data removal techniques that exclude relevant data to achieve fair representations.
  The authors propose two additional criteria: group coverage and minimal data loss,
  to enhance the trustworthiness of such methods.'
---

# Trusting Fair Data: Leveraging Quality in Fairness-Driven Data Removal Techniques

## Quick Facts
- arXiv ID: 2405.12926
- Source URL: https://arxiv.org/abs/2405.12926
- Reference count: 25
- Key outcome: Proposes FairDo method using multi-objective optimization to balance fairness and data retention in fairness-driven data removal techniques

## Executive Summary
This paper addresses critical trustworthiness issues in fairness-driven data removal techniques that often exclude relevant data to achieve fair representations. The authors identify two key problems: lack of coverage (entire groups being excluded) and excessive data loss (removing too much relevant information). To address these concerns, they propose FairDo, a methodology that introduces group coverage and minimal data loss as additional criteria alongside fairness objectives. Using NSGA-II to find Pareto-optimal solutions, the approach enables users to balance fairness and data quality when selecting subsets for machine learning applications.

## Method Summary
The FairDo method employs multi-objective optimization to balance fairness and data retention when removing data points for fair representation. It uses NSGA-II with a variable initializer to generate diverse initial populations, finding Pareto-optimal solutions that minimize discrimination while maximizing data retention. A coverage constraint ensures all groups remain represented in the final subset. The method is evaluated on three real-world datasets (Adult, Bank, COMPAS) by training machine learning models on pre-processed data and comparing their fairness and performance metrics against models trained on original datasets.

## Key Results
- Pre-processed datasets enable reliable ML model training with minimal impact on fairness and performance
- FairDo successfully balances the trade-off between fairness and data loss through Pareto-optimal solutions
- The method provides guidance for selecting subsets that best fit specific application needs while maintaining group coverage

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Multi-objective optimization balances fairness and data retention through Pareto-optimal solutions
- Mechanism: NSGA-II generates subsets that are not dominated in both fairness (minimized discrimination) and data loss (maximized retention) objectives
- Core assumption: Users can effectively select from the Pareto front using weighted combinations of objectives
- Evidence anchors: [abstract] introduces multi-objective optimization for fairness and data loss; [section 5.1] describes Pareto front as non-dominated solutions

### Mechanism 2
- Claim: Coverage constraint prevents exclusion of entire groups
- Mechanism: Penalty applied to discrimination score if any group is missing, set higher than maximum achievable discrimination
- Core assumption: Users value group representation over absolute lowest discrimination
- Evidence anchors: [abstract] addresses lack of coverage as critical fairness issue; [section 4.1] describes coverage preference in fairness metric

### Mechanism 3
- Claim: Variable initializer creates diverse initial population for better solution exploration
- Mechanism: Uses range of probabilities [pmin, pmax] instead of uniform 50% inclusion, leading to individuals with varying data removal levels
- Core assumption: More diverse initial population leads to better Pareto front solutions
- Evidence anchors: [section 6.1] describes default range [0.5, 0.99] for variable initializer; [section 7.1] shows significant performance improvement

## Foundational Learning

- Concept: Genetic Algorithms (GAs)
  - Why needed here: GAs solve NP-hard combinatorial optimization for optimal data point removal
  - Quick check question: What is the role of fitness function in a GA, and how is it defined in this paper?

- Concept: Pareto Optimality
  - Why needed here: Essential for interpreting multi-objective optimization results and selecting solutions
  - Quick check question: What does it mean for a solution to be "dominated" in multi-objective optimization?

- Concept: Fairness Metrics (e.g., Statistical Parity)
  - Why needed here: Quantify discrimination levels and guide optimization process
  - Quick check question: How is statistical parity defined, and what does it mean for a dataset to satisfy it?

## Architecture Onboarding

- Component map: Data Input -> Preprocessing (80-20 train-test split) -> Optimization (NSGA-II) -> Output (Pareto front) -> Evaluation (ML model training and comparison)

- Critical path: Data preprocessing → NSGA-II optimization → Pareto front solution selection → ML model training on fair subset → Fairness and performance evaluation

- Design tradeoffs: Population size vs. computational cost; generations vs. solution quality; β normalization choice vs. fairness emphasis

- Failure signatures: Dominated solutions only in Pareto front; high discrimination in fair subsets; significant ML performance degradation

- First 3 experiments:
  1. Run NSGA-II with default parameters on small dataset to verify convergence and Pareto front generation
  2. Evaluate different β values' impact on Pareto solution selection and resulting metrics
  3. Compare NSGA-II performance with different genetic operators to identify optimal configuration

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do different choices of normalization factor β impact Pareto-optimal solution selection?
- Basis in paper: [explicit] Authors discuss absolute vs. relative discrimination choices for β but lack empirical comparison
- Why unresolved: Paper mentions β choice depends on user needs without providing experimental evidence
- What evidence would resolve it: Experiments comparing Pareto fronts with different β values and analyzing fairness-data loss trade-offs

### Open Question 2
- Question: Impact of varying [pmin, pmax] range in variable initializer on population diversity and quality?
- Basis in paper: [explicit] Authors propose variable initializer with default [0.5, 0.99] range but don't investigate range variations
- Why unresolved: Paper sets default range without exploring how different ranges affect initialization
- What evidence would resolve it: Experiments varying [pmin, pmax] range and analyzing resulting population diversity and quality

### Open Question 3
- Question: How does method handle non-binary protected attributes and fairness metric challenges?
- Basis in paper: [explicit] Authors mention framework extensibility to other metrics but focus on binary groups
- Why unresolved: Paper lacks details on non-binary attribute handling or fairness metric challenges
- What evidence would resolve it: Experiments using non-binary protected attributes and analyzing method performance

## Limitations

- Scalability concerns for larger datasets with more groups and features
- Sensitivity to hyperparameter choices (population size, mutation rates, etc.)
- Uncertainty about effectiveness across different fairness metrics and dataset structures

## Confidence

- High confidence: Core multi-objective optimization mechanism for balancing fairness and data retention
- Medium confidence: Coverage constraint and variable initializer effectiveness in improving solutions
- Medium confidence: Practical utility of Pareto front approach for real-world decision-making

## Next Checks

1. **Ablation study on genetic operators**: Systematically test different combinations of selection, crossover, and mutation operators to quantify their individual contributions to solution quality

2. **Cross-metric validation**: Apply FairDo using alternative fairness metrics (equal opportunity, equalized odds) to assess robustness across different fairness definitions

3. **Stress test with synthetic data**: Generate synthetic datasets with controlled properties (number of groups, feature correlations, noise levels) to evaluate method performance under varying conditions and identify breaking points