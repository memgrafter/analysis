---
ver: rpa2
title: A Course Shared Task on Evaluating LLM Output for Clinical Questions
arxiv_id: '2408.00122'
source_url: https://arxiv.org/abs/2408.00122
tags:
- task
- llms
- students
- shared
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a shared task on evaluating harmful LLM outputs
  for clinical questions, organized as part of a Foundations of Language Technology
  course. The task required students to annotate and categorize LLM-generated answers
  to health-related clinical questions using a dataset of 500 Cochrane Clinical Answers,
  ultimately compiling 1,800 annotated answers from five different LLMs.
---

# A Course Shared Task on Evaluating LLM Output for Clinical Questions

## Quick Facts
- arXiv ID: 2408.00122
- Source URL: https://arxiv.org/abs/2408.00122
- Reference count: 3
- Key outcome: A shared task on evaluating harmful LLM outputs for clinical questions, organized as part of a Foundations of Language Technology course, resulting in 1,800 annotated answers from five different LLMs and insights into course design for NLP education

## Executive Summary
This paper presents a shared task organized as part of a Foundations of Language Technology course, where students evaluated LLM-generated answers to health-related clinical questions for harmful content. The task involved annotating and categorizing LLM outputs using a dataset of 500 Cochrane Clinical Answers, ultimately compiling 1,800 annotated answers from five different LLMs. Students participated in four sub-tasks: data annotation, data processing, analysis of human annotations, and model development/prompt engineering for harmfulness detection. The course received 121-130 participants per sub-task, with 87 students completing all four sub-tasks and 74 earning bonus points. The study provides insights into course design for NLP education, highlighting that while annotation tasks were time-consuming for computer science students, most found the analysis sub-task most engaging and the model development sub-task most challenging.

## Method Summary
The shared task involved four sequential sub-tasks: (1) data annotation where students labeled LLM-generated answers for clinical questions using six fine-grained categories including contradictions and exaggerations, (2) data processing to consolidate annotations and prepare datasets, (3) analysis of human annotations to understand patterns and quality, and (4) model development and prompt engineering for harmfulness detection. The dataset consisted of 500 Cochrane Clinical Answers published between 2021-2023, with LLM outputs from five models (Llama-2-70b-chat, ChatGPT, BingChat, PerplexityAI) covering 360 clinical questions, resulting in 1,800 annotated answers. Students were graded through leaderboards and performance metrics, with bonus points available for completing all sub-tasks and passing the final exam.

## Key Results
- 1,800 annotated LLM answers compiled from five different models for 360 clinical questions
- 87 students completed all four sub-tasks, with 74 earning bonus points that upgraded their final grades
- Students found the analysis sub-task most engaging and the model development sub-task most challenging
- Dataset of 850 annotated LLM answers for 130 clinical questions released for future research

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Structured multi-stage task design effectively scaffolds student learning in NLP education
- Mechanism: Breaking down complex real-world problems into sequential sub-tasks allows students to build competence progressively, with each stage building on previous work while maintaining manageable cognitive load
- Core assumption: Students can effectively transfer knowledge and skills from one sub-task to the next when tasks are properly sequenced and dependencies are clear
- Evidence anchors:
  - [abstract] Students participated in four sub-tasks: data annotation, data processing, analysis of human annotations, and model development/prompt engineering for harmfulness detection
  - [section] The course is structured into 14 lectures, supplemented by 9 hands-on coding tutorials that allow the students to reinforce their understanding of key concepts learned in the previous lectures
  - [corpus] The average neighbor FMR score of 0.457 suggests moderate relevance to similar educational NLP tasks, indicating this approach aligns with broader pedagogical patterns
- Break condition: If students fail to complete earlier sub-tasks, they cannot effectively contribute to later stages, creating a cascading failure effect

### Mechanism 2
- Claim: Using real-world, high-quality datasets increases student engagement and learning outcomes
- Mechanism: Authentic datasets from trusted sources (Cochrane Clinical Answers) provide realistic context for applying NLP techniques, making abstract concepts concrete and demonstrating practical relevance
- Core assumption: Students are more motivated and learn more effectively when working with datasets that have clear real-world applications and established quality standards
- Evidence anchors:
  - [section] We utilize Cochrane Clinical Answers, a trusted resource that provides concise, evidence-based responses to clinical questions grounded in rigorous Cochrane systematic reviews
  - [section] We collected a dataset of 500 CCAs published between 2021 and 2023, assuming that the answers written by clinical professionals represent accurate and truthful responses to the target questions
  - [corpus] The dataset compilation resulted in 1,800 annotated answers from five different LLMs for 360 CCAs, demonstrating substantial data volume for meaningful analysis
- Break condition: If the dataset quality is compromised or students cannot relate to the domain, engagement and learning outcomes may decrease

### Mechanism 3
- Claim: Competitive elements and performance-based grading motivate student participation and quality work
- Mechanism: Leaderboards and bonus points for high performance create intrinsic motivation through competition and extrinsic motivation through tangible rewards, driving both participation and quality
- Core assumption: Students respond positively to competitive structures and performance-based incentives when the evaluation criteria are transparent and achievable
- Evidence anchors:
  - [section] The credits for the fourth sub-task is divided into three components including Performance on the leaderboards of the closed and open tracks (40 credits)
  - [section] To qualify for a bonus point, which upgrades their final grade in the course (e.g., from 2.0 to 1.7), students must meet two conditions: 1) pass the final written exam, and 2) participate in all four sub-tasks and obtain at least 70% of all points
  - [section] Overall, 87 students participated in all four sub-tasks, and 74 of them received the bonus points, indicating successful motivation
- Break condition: If the competition becomes too intense or the grading criteria seem unfair, it may demotivate some students or create unhealthy stress

## Foundational Learning

- Concept: Annotation task design and inter-annotator agreement calculation
  - Why needed here: Students must understand how to create reliable labeled datasets, a fundamental skill in NLP that affects all downstream model performance
  - Quick check question: If three annotators label an example as "Contradiction," "Exaggeration," and "Cannot access" respectively, what is the Fleiss' Kappa score?

- Concept: Baseline model implementation and comparison
  - Why needed here: Understanding simple models (decision trees, neural networks) provides foundation for appreciating more complex approaches and recognizing their limitations
  - Quick check question: What are the key differences in how a decision tree and a simple neural network would approach the harmfulness detection task?

- Concept: Prompt engineering principles and evaluation
  - Why needed here: Effective prompting is crucial for LLM-based systems, and students need to understand how different prompt formulations affect model behavior and output quality
  - Quick check question: How might changing from a neutral prompt to an instruction-tuned prompt affect the distribution of harmful outputs across different LLM models?

## Architecture Onboarding

- Component map: Annotation pipeline (Label Studio setup → individual annotation → adjudication → consolidation) → Analysis scripts → Baseline models (decision tree, neural network) → LLM prompting system → Evaluation framework (leaderboards, metrics)
- Critical path: Annotation completion → Data consolidation → Analysis generation → Model development → Evaluation submission
- Design tradeoffs: Balancing annotation workload with learning outcomes vs. selecting topics requiring less domain-specific knowledge
- Failure signatures: Low inter-annotator agreement indicates unclear annotation guidelines; poor baseline model performance suggests data quality issues; leaderboard stagnation indicates prompting challenges
- First 3 experiments:
  1. Test annotation consistency by having team members independently annotate the same 10 examples and calculate inter-annotator agreement
  2. Compare baseline model performance on the dev set using different feature sets (text features, embedding features, hybrid approaches)
  3. Evaluate prompt effectiveness by testing different prompt formulations on a small validation set before submitting to the leaderboard

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the shared task be improved to reduce the time-consuming annotation burden on computer science students while maintaining the educational value of the task?
- Basis in paper: [explicit] The paper states that students with a computer science background found the annotation process too time-consuming.
- Why unresolved: The paper only mentions this as a feedback point and suggests reducing annotation load or selecting topics requiring less domain-specific knowledge, but does not provide specific solutions or evaluate their effectiveness.
- What evidence would resolve it: Implementing and testing alternative annotation strategies or topic selections, then measuring their impact on student engagement and learning outcomes.

### Open Question 2
- Question: What is the optimal balance between student involvement in dataset creation and preventing overfitting in the test set, particularly in educational settings?
- Basis in paper: [inferred] The authors acknowledge that students helped create the test set, which could lead to implicit knowledge affecting prompt design, though they believe the small proportion of annotated CCAs minimizes this risk.
- Why unresolved: The paper only suggests keeping the test set completely hidden in future iterations but does not explore other mitigation strategies or evaluate the actual impact of student involvement on model performance.
- What evidence would resolve it: Comparative studies of student performance with varying degrees of test set visibility or involvement in dataset creation, measuring both learning outcomes and model evaluation robustness.

### Open Question 3
- Question: How does the performance of LLMs in detecting harmful content vary across different clinical domains, and what factors contribute to these variations?
- Basis in paper: [explicit] The paper mentions analyzing whether LLMs produce less harmful content in certain topics but does not provide detailed findings or discuss contributing factors.
- Why unresolved: The analysis questions are listed but the results are not discussed in detail, leaving open the relationship between clinical domain and LLM performance in harm detection.
- What evidence would resolve it: Detailed analysis of LLM performance across various clinical topics, identifying patterns in harm generation and factors such as topic complexity, terminology familiarity, or evidence availability.

## Limitations

- Reliance on student annotators with limited clinical domain expertise raises potential quality concerns for the annotated dataset
- Only 850 annotated answers for 130 clinical questions (47% of total) were released, limiting dataset accessibility
- Course design requires significant computational resources and access to multiple LLM APIs, limiting generalizability to other educational contexts

## Confidence

**High Confidence**: The basic framework of organizing a shared task around LLM evaluation for clinical questions is sound and well-executed. The four-stage structure (annotation → processing → analysis → model development) is clearly defined and achieved the stated educational objectives, as evidenced by 74 students earning bonus points.

**Medium Confidence**: The effectiveness of the annotation task design and its impact on learning outcomes is moderately supported but limited by the lack of detailed agreement statistics and student feedback beyond general sentiment. The claim that "most found the analysis sub-task most engaging" is based on instructor observations rather than systematic student surveys.

**Low Confidence**: The generalizability of the course design to other NLP education contexts is uncertain, as the study doesn't provide comparative data with other teaching approaches or explore how the specific focus on clinical LLM evaluation affects learning outcomes compared to more general NLP tasks.

## Next Checks

1. **Annotation Quality Verification**: Calculate and report detailed inter-annotator agreement statistics (including per-category agreement) for the released dataset subset, and conduct a domain expert review of a sample of annotated examples to validate the quality of student annotations.

2. **Baseline Model Reproducibility**: Implement and evaluate the described baseline models (decision tree, simple neural network) on the released dataset using the exact feature sets and preprocessing steps outlined in the course materials, then compare results with the student submissions.

3. **Prompt Engineering Impact Analysis**: Systematically test the prompt formulations described in Appendix A across the five LLM models on a held-out validation set to quantify how different prompting strategies affect harmfulness detection performance, replicating the leaderboard dynamics described in the paper.