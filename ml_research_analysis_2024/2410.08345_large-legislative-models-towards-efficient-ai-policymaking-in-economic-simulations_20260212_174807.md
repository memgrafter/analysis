---
ver: rpa2
title: 'Large Legislative Models: Towards Efficient AI Policymaking in Economic Simulations'
arxiv_id: '2410.08345'
source_url: https://arxiv.org/abs/2410.08345
tags:
- agents
- each
- apples
- lever
- principal
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a novel LLM-based approach for automated economic
  policymaking in multi-agent reinforcement learning environments. The method addresses
  sample inefficiency issues in existing RL-based approaches by leveraging pre-trained
  LLMs' in-context learning capabilities to directly output economic policies, rather
  than learning policy generators.
---

# Large Legislative Models: Towards Efficient AI Policymaking in Economic Simulations

## Quick Facts
- **arXiv ID**: 2410.08345
- **Source URL**: https://arxiv.org/abs/2410.08345
- **Reference count**: 40
- **Primary result**: LLM-based economic policymaking achieves up to two orders of magnitude better sample efficiency than RL baselines while maintaining similar or better final payoffs

## Executive Summary
This paper introduces a novel approach for automated economic policymaking in multi-agent reinforcement learning environments using large language models. The method leverages LLMs' in-context learning capabilities to directly output economic policies rather than learning policy generators, addressing sample inefficiency issues in existing RL-based approaches. By incorporating contextualization and historical observations into prompts, the LLM policymaker demonstrates significantly improved sample efficiency across three economic simulation environments, converging faster while achieving comparable or superior final payoffs compared to five baseline methods.

## Method Summary
The approach uses pre-trained LLMs with in-context learning to directly generate economic policies for multi-agent environments. The method constructs prompts containing contextualization (natural language descriptions of the problem), demonstrations (previous action-payoff pairs), and the current query. The LLM outputs policy decisions that are then evaluated in the environment, with results fed back into the prompt for subsequent iterations. This creates an iterative loop where the LLM refines its policy choices based on historical performance data, bypassing the need for complex optimization of policy generator networks.

## Key Results
- LLM policymakers converge up to two orders of magnitude faster than RL baselines across three environments
- LLMs achieve similar or better final payoffs compared to AI Economist and MetaGrad baselines
- Performance significantly improves with contextualization, demonstrating LLMs can leverage pre-training for economic reasoning
- Historical observations provide valuable feedback without detrimental effects from irrelevant information

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLMs achieve superior sample efficiency by directly outputting economic policies rather than learning policy generators
- Mechanism: The method leverages pre-trained LLMs' in-context learning capabilities to map contextual information and historical observations directly to optimal policy decisions, bypassing the need for complex optimization of policy generator networks
- Core assumption: The LLM's pre-training has embedded relevant economic reasoning capabilities that can be effectively accessed through in-context learning
- Evidence anchors:
  - [abstract] "proposes a novel LLM-based approach for automated economic policymaking in multi-agent reinforcement learning environments"
  - [section 3] "our ablations demonstrate this is an overly complicated way to approach AI policymaking"
  - [corpus] No direct corpus evidence found - weak signal
- Break condition: If the economic environments become too complex for the LLM's context window or if the required policies fall outside the distribution of the LLM's pre-training

### Mechanism 2
- Claim: Contextualization enables LLMs to leverage their pre-training for improved policymaking
- Mechanism: Providing natural language descriptions of the problem setting allows LLMs to draw upon their extensive pre-training data distribution, effectively "reusing" learned economic principles
- Core assumption: The LLM's pre-training corpus contains relevant economic concepts that can be applied to novel policymaking scenarios
- Evidence anchors:
  - [abstract] "LLMs can also be given a contextualization of the problem setting they are applied to, potentially allowing them to draw upon their extensive pre-training data distribution"
  - [section 5.3] "LLM performance significantly improves when contextualization is included"
  - [corpus] Weak signal - corpus mentions "Large Language Models in Politics and Democracy" but lacks direct evidence
- Break condition: If the contextualization becomes too specific or complex for the LLM to process effectively within its context window

### Mechanism 3
- Claim: Historical observations provide valuable feedback for policy improvement
- Mechanism: The method appends successive action choices and their corresponding payoffs and historical observations to the prompt, creating an iterative learning loop that refines policy decisions over time
- Core assumption: The historical observations contain actionable information that the LLM can process and learn from within its context window
- Evidence anchors:
  - [section 4] "iteratively adds to this contextualization, appending successive action choices and their corresponding payoffs and historical observations"
  - [section 5.2] "Historical information given to policymakers may at times be flawed or inaccurate"
  - [corpus] Weak signal - corpus mentions "algorithmic tradeoffs in fair lending" but lacks direct evidence
- Break condition: If the historical observations become too numerous to fit within the LLM's context window or if they contain misleading information that the LLM cannot properly contextualize

## Foundational Learning

- Concept: Stackelberg-Markov Games
  - Why needed here: The economic policymaking problem is formalized as finding equilibria in Stackelberg-Markov games, where the principal (policymaker) commits to an action that induces a POMG for the followers
  - Quick check question: In a Stackelberg-Markov game, who commits to an action first - the principal or the followers?

- Concept: In-Context Learning (ICL)
  - Why needed here: The method relies on the LLM's ability to learn from examples provided within the prompt context rather than through weight updates
  - Quick check question: What is the key difference between in-context learning and traditional fine-tuning in terms of how the model parameters are updated?

- Concept: Multi-Agent Reinforcement Learning (MARL)
  - Why needed here: The economic environments involve multiple agents whose interactions and equilibria need to be modeled and influenced by the policymaker
  - Quick check question: In MARL environments, what is the primary challenge that distinguishes them from single-agent RL problems?

## Architecture Onboarding

- Component map: LLM principal (with context window and generation capabilities) → prompt builder (contextualization + historical observations) → economic environment (multi-agent simulation) → payoff calculator → prompt updater
- Critical path: Prompt generation → LLM policy output → Environment simulation → Payoff evaluation → Prompt update → Next iteration
- Design tradeoffs: Direct policy output (simple but limited expressiveness) vs. policy generator networks (complex but potentially more flexible); context window size vs. amount of historical information; temperature parameter vs. exploration vs. exploitation
- Failure signatures: LLM outputs nonsensical policies; convergence fails due to insufficient exploration; performance degrades as context window fills with historical observations; contextualization is misinterpreted
- First 3 experiments:
  1. Test LLM policy generation with simple contextualization and no historical observations on a basic economic environment
  2. Add historical observations to the prompt and measure impact on sample efficiency
  3. Compare performance with and without contextualization across different economic environments

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do LLM-based policymaking methods scale with increasingly complex economic environments?
- Basis in paper: [inferred] The paper mentions that prompts given to LLM policymakers will presumably increase in complexity as environments scale, raising questions about prompt design scalability and context window constraints.
- Why unresolved: The experiments only tested on three relatively simple environments, and did not explore how performance degrades or improves as environmental complexity increases.
- What evidence would resolve it: Experiments testing LLM policymakers across a graduated series of increasingly complex environments, measuring performance degradation and identifying breaking points.

### Open Question 2
- Question: What is the impact of irrelevant information in historical observations on LLM policymaking performance in realistic settings?
- Basis in paper: [explicit] The paper conducted an experiment showing that deliberate irrelevant information had no detrimental effect on Gemini-1.5 flash performance in the Harvest environment.
- Why unresolved: This experiment only tested one model on one environment with deliberately added irrelevant information; real-world scenarios might present different types or patterns of irrelevant information.
- What evidence would resolve it: Systematic experiments testing multiple LLM models across various environments with different types of irrelevant information, measuring performance degradation under realistic conditions.

### Open Question 3
- Question: Can contextualization be effectively utilized by baseline RL-based methods to improve their performance?
- Basis in paper: [explicit] The paper states that it is unclear how contextualizations could be effectively utilized by baseline methods, and their ablation showed that LLM performance significantly improved with contextualization.
- Why unresolved: The paper only tested contextualization with LLM methods and did not attempt to incorporate contextual information into the RL-based baselines.
- What evidence would resolve it: Modified RL-based methods that incorporate contextual information through techniques like natural language processing of descriptions or multi-task learning, with performance comparisons to standard implementations.

## Limitations

- Context window constraints limit the amount of historical observations that can be incorporated, potentially degrading performance as iterations accumulate
- Pre-training data distribution assumptions are never empirically validated for economic policymaking relevance
- Results only demonstrate effectiveness on three specific MARL environments, limiting generalizability claims

## Confidence

**High confidence**: LLM-based policymaking achieves superior sample efficiency compared to baseline methods - directly supported by quantitative experiments across three environments with clear convergence metrics.

**Medium confidence**: Mechanism claims about in-context learning leveraging pre-training - experimental results show improvement with contextualization, but ablation studies isolating pre-training contribution are lacking.

**Low confidence**: Scalability claims - paper doesn't address performance as environments become more complex or when context windows become saturated with historical observations.

## Next Checks

1. **Context window saturation test**: Run the LLM policymaker for extended iterations (10x the current maximum) in the Harvest environment and measure performance degradation as the context window fills. Track the exact point where additional historical observations stop improving and start degrading performance.

2. **Pre-training relevance validation**: Analyze the pre-training corpus of GPT-4o mini to quantify the presence of economic policymaking concepts. Compare performance on economic vs. non-economic domains (e.g., social coordination games) to validate that pre-training provides domain-specific advantages.

3. **Baseline hyperparameter sensitivity**: Conduct a systematic hyperparameter sweep for the AI Economist and MetaGrad baselines across the three environments. Report the best-performing configurations and their variance to establish whether the LLM advantage persists under optimal baseline tuning.