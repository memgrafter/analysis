---
ver: rpa2
title: Counterfactual Token Generation in Large Language Models
arxiv_id: '2409.17027'
source_url: https://arxiv.org/abs/2409.17027
tags:
- counterfactual
- generation
- token
- output
- factual
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a causal model of token generation that enhances
  large language models (LLMs) with the ability to reason about counterfactual alternatives
  to their outputs. The authors augment the autoregressive process of LLMs with a
  Gumbel-Max structural causal model (SCM), enabling counterfactual token generation
  at minimal computational cost.
---

# Counterfactual Token Generation in Large Language Models

## Quick Facts
- arXiv ID: 2409.17027
- Source URL: https://arxiv.org/abs/2409.17027
- Authors: Ivi Chatzi; Nina Corvelo Benz; Eleni Straitouri; Stratis Tsirtsis; Manuel Gomez-Rodriguez
- Reference count: 40
- One-line primary result: Introduces a causal model enabling LLMs to reason about counterfactual alternatives to their outputs with minimal computational overhead.

## Executive Summary
This paper presents a novel approach to counterfactual token generation in large language models by augmenting the autoregressive generation process with a Gumbel-Max structural causal model (SCM). The method enables LLMs to generate alternative outputs while maintaining consistency with their original outputs, a capability absent in current stateless LLMs. The approach is demonstrated through bias detection experiments on census data, revealing measurable effects of demographic attributes on generated income and education levels.

## Method Summary
The authors augment the autoregressive token generation process of LLMs with a Gumbel-Max structural causal model, enabling counterfactual reasoning. The method works by storing random number generator states during factual generation and using these states to regenerate the same noise values during counterfactual generation. This ensures that counterfactual outputs remain similar to original outputs while allowing controlled modifications through interventions on specific tokens. The approach is implemented on Llama 3 8B-Instruct and Ministral-8B-Instruct, demonstrating minimal computational overhead compared to vanilla generation.

## Key Results
- Counterfactual outputs generated using this approach exhibit lower Levenshtein edit distance (approximately 0.45-0.75) compared to interventional generation methods, indicating higher similarity to original outputs.
- Bias detection application reveals measurable effects of sex and race attributes on generated income and education levels, with some effects being more pronounced in total effects compared to direct effects.
- The methodology provides insights into the implicit causal structures within LLMs while offering a practical tool for exploring alternative outputs and detecting potential biases.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The Gumbel-Max SCM allows LLMs to maintain consistency between factual and counterfactual token sequences by preserving the same noise values during generation.
- Mechanism: The SCM defines the token sampling process as a function of the token distribution and noise values. By fixing the noise values during counterfactual generation, the model ensures that the same tokens are likely to be selected, maintaining similarity to the original output.
- Core assumption: The Gumbel-Max distribution used in the SCM satisfies the counterfactual stability property, which ensures that the token sampled in the factual generation remains the most probable in the counterfactual scenario.
- Evidence anchors:
  - [abstract] "Our model allows any large language model to perform counterfactual token generation at almost no cost in comparison with vanilla token generation"
  - [section] "Using the Gumbel-Max SCM, our approach to counterfactual token generation is guaranteed to satisfy the property of counterfactual stability"
  - [corpus] Weak evidence - corpus shows related work on story generation but doesn't directly address the Gumbel-Max mechanism
- Break condition: If the token distribution changes significantly after intervention, the counterfactual stability property may not hold, leading to different tokens being selected.

### Mechanism 2
- Claim: The autoregressive process augmented with SCM can efficiently generate counterfactual tokens with minimal additional memory requirements.
- Mechanism: Instead of storing all noise values, the implementation stores only the states of the random number generator. During counterfactual generation, these states are used to regenerate the same noise values on the fly.
- Core assumption: The random number generator states can be used to deterministically regenerate the same sequence of noise values that were used in the original generation.
- Evidence anchors:
  - [section] "Algorithm 1 employs a simple idea: it stores the state of the random number generator rj used at each time stepj of the factual generation"
  - [section] "Then, during the counterfactual generation, it regenerates the valuesuj = GenGumbel(rj) on the fly"
  - [corpus] Weak evidence - corpus contains related work on story generation but doesn't address the efficiency of the Gumbel-Max implementation
- Break condition: If the random number generator implementation changes between factual and counterfactual generation, the regenerated noise values may not match the original ones.

### Mechanism 3
- Claim: The Gumbel-Max SCM-based counterfactual generation reveals biases in LLMs by comparing factual and counterfactual outputs under interventions on sensitive attributes.
- Mechanism: By generating counterfactual outputs where sensitive attributes (e.g., sex, race) are modified while keeping other attributes fixed, the model can reveal how these attributes influence the generated content.
- Core assumption: The LLM's world model encodes causal relationships between attributes that can be uncovered through counterfactual generation.
- Evidence anchors:
  - [section] "We demonstrate the use of counterfactual token generation to investigate potential biases of an LLM towards demographic groups"
  - [section] "The bias detection application reveals that sex and race attributes in the LLM's world model have measurable effects on generated income and education levels"
  - [corpus] Weak evidence - corpus contains related work on story generation but doesn't directly address bias detection using counterfactual generation
- Break condition: If the LLM's world model doesn't encode meaningful causal relationships between attributes, the counterfactual comparisons may not reveal genuine biases.

## Foundational Learning

- Concept: Structural Causal Models (SCMs)
  - Why needed here: SCMs provide the formal framework for modeling the autoregressive token generation process and enabling counterfactual reasoning by specifying how variables respond to interventions.
  - Quick check question: What are the three components of an SCM and how do they relate to the token generation process?

- Concept: Counterfactual Stability Property
  - Why needed here: This property ensures that counterfactual outputs remain similar to factual outputs by maintaining the relative probabilities of tokens when the distribution changes.
  - Quick check question: How does the Gumbel-Max distribution satisfy the counterfactual stability property in the context of token generation?

- Concept: Levenshtein Edit Distance
  - Why needed here: This metric quantifies the similarity between factual and counterfactual token sequences, providing a measure of how well the counterfactual generation preserves the original output.
  - Quick check question: What does a lower Levenshtein edit distance between factual and counterfactual outputs indicate about the quality of counterfactual generation?

## Architecture Onboarding

- Component map: LLM backbone -> Gumbel-Max SCM -> Random number generator state tracker -> Intervention handler -> Comparison module

- Critical path:
  1. Generate factual output sequence using LLM
  2. Store random number generator states during generation
  3. Apply intervention to modify specific tokens
  4. Generate counterfactual output using stored states
  5. Compare factual and counterfactual outputs

- Design tradeoffs:
  - Memory vs. accuracy: Storing all noise values vs. regenerating them from RNG states
  - Computational cost: Minimal overhead compared to vanilla generation
  - Flexibility vs. stability: More complex SCMs might offer better control but could break counterfactual stability

- Failure signatures:
  - Large edit distances between factual and counterfactual outputs (indicates loss of stability)
  - Inconsistent results across different seeds (indicates RNG state issues)
  - Bias detection results that don't align with expectations (indicates world model limitations)

- First 3 experiments:
  1. Compare edit distances between factual and counterfactual outputs for different temperature settings
  2. Test bias detection by intervening on sex attribute and measuring income changes
  3. Validate RNG state regeneration by comparing outputs with and without state storage

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How sensitive is counterfactual token generation to the choice of SCM, and what alternative SCMs could be used while maintaining counterfactual stability?
- Basis in paper: [explicit] The paper discusses the Gumbel-Max SCM and its counterfactual stability property, and mentions that alternative SCMs obeying counterfactual stability could be explored.
- Why unresolved: The paper only implements counterfactual token generation using the Gumbel-Max SCM and briefly mentions inverse transform sampling as an alternative that does not satisfy counterfactual stability. A systematic comparison of different SCMs that satisfy counterfactual stability is missing.
- What evidence would resolve it: Experiments comparing counterfactual token generation using multiple SCMs that satisfy counterfactual stability (e.g., different Gumbel-based approaches, or other SCMs designed for categorical distributions) on the same datasets and tasks would reveal the sensitivity to SCM choice.

### Open Question 2
- How does the performance of counterfactual token generation scale with LLM size and architecture?
- Basis in paper: [inferred] The paper implements the method on Llama 3 8B-Instruct and Ministral-8B-Instruct, but notes it would be insightful to see whether sensitivity changes as the number of parameters increases.
- Why unresolved: The experiments are limited to two 8B parameter models. The relationship between model scale, architecture, and counterfactual generation quality remains unexplored.
- What evidence would resolve it: Implementing counterfactual token generation across LLMs of varying sizes (e.g., 7B, 13B, 70B) and architectures (decoder-only, decoder-encoder) and measuring edit distance, bias detection capabilities, and computational efficiency would clarify scaling effects.

### Open Question 3
- Can counterfactual token generation be effectively combined with human feedback to improve LLM causal reasoning?
- Basis in paper: [explicit] The discussion section mentions that counterfactual statements generated using the model may be inconsistent with the underlying causal model of the world shared by humans, and suggests exploring the use of the methodology in conjunction with human feedback.
- Why unresolved: The paper does not implement or test any human-in-the-loop approach for refining counterfactual generations.
- What evidence would resolve it: Experiments where human annotators rate the plausibility of counterfactual outputs, followed by fine-tuning the LLM on high-quality counterfactual examples, would demonstrate whether human feedback improves the causal consistency of generated text.

## Limitations
- The methodology relies heavily on the assumption that the Gumbel-Max distribution maintains counterfactual stability when token distributions change due to interventions, which requires more extensive empirical validation.
- The experimental validation is limited to two specific LLM architectures (Llama 3 8B-Instruct and Ministral-8B-Instruct), raising questions about generalizability to other model families or scales.
- The interpretation of detected effects as causal biases requires careful consideration, as the LLM's world model may encode correlations rather than true causal relationships.

## Confidence

**High Confidence**: The technical implementation of storing and regenerating random number generator states for counterfactual generation is straightforward and well-defined. The use of Levenshtein edit distance as a similarity metric is standard and appropriate for measuring output similarity.

**Medium Confidence**: The claim that counterfactual generation maintains consistency with original outputs through counterfactual stability is theoretically sound but requires more extensive empirical validation across different intervention types and magnitudes. The bias detection results showing measurable effects of demographic attributes are promising but need replication with more diverse datasets and attribution methods.

**Low Confidence**: The broader claim that this approach reveals the "implicit causal structures within LLMs" is ambitious and would require more rigorous causal inference validation. The practical utility of counterfactual generation for real-world applications beyond bias detection remains largely unproven.

## Next Checks

1. **Multi-step intervention stability test**: Systematically vary the magnitude and number of simultaneous interventions on different tokens, measuring how Levenshtein edit distance and output coherence change. This would validate whether counterfactual stability holds under complex modification scenarios.

2. **Cross-architecture generalization**: Implement the counterfactual generation framework on at least three additional LLM architectures (different families/sizes) and compare: (a) computational overhead, (b) edit distance consistency, and (c) bias detection sensitivity across models.

3. **Causal inference validation**: Design a controlled experiment where known causal relationships are embedded into a synthetic dataset, then use counterfactual generation to detect these relationships. Compare detection accuracy against established causal discovery methods to quantify the approach's causal inference capability.