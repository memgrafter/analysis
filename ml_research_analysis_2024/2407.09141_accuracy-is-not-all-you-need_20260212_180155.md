---
ver: rpa2
title: Accuracy is Not All You Need
arxiv_id: '2407.09141'
source_url: https://arxiv.org/abs/2407.09141
tags:
- flips
- accuracy
- chat
- baseline
- table
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: When LLMs are compressed using quantization, accuracy alone fails
  to capture significant changes in model behavior. The authors identify a phenomenon
  of "flips," where correct answers become incorrect and vice versa, despite similar
  overall accuracy.
---

# Accuracy is Not All You Need

## Quick Facts
- arXiv ID: 2407.09141
- Source URL: https://arxiv.org/abs/2407.09141
- Authors: Abhinav Dutta; Sanjeev Krishnan; Nipun Kwatra; Ramachandran Ramjee
- Reference count: 40
- Key outcome: Accuracy alone fails to capture significant changes in LLM behavior during compression; flips and KL-Divergence are better evaluation metrics

## Executive Summary
This paper challenges the conventional wisdom that accuracy is sufficient for evaluating compressed large language models (LLMs). The authors demonstrate that quantization can preserve overall accuracy while causing significant changes in individual predictions - a phenomenon they call "flips" where correct answers become incorrect and vice versa. They propose using KL-Divergence and the flips metric alongside accuracy to better capture the true impact of compression techniques on model behavior.

## Method Summary
The authors evaluate various quantization schemes (BnB W8A8, GPTQ W8A16, SQ W8A8, etc.) on multiple LLMs (Llama2-13b, Yi chat) across several benchmarks (MMLU, PIQA, Hellaswag). They measure accuracy, flips (changes in correct/incorrect answers), and KL-Divergence between baseline and compressed models. They also validate findings using MT-Bench for generative tasks and analyze the correlation between different evaluation metrics.

## Key Results
- Quantization can preserve accuracy while causing significant flips in answers (correct to incorrect and vice versa)
- KL-Divergence correlates strongly with flips (Spearman correlation of 0.981 on MMLU benchmark)
- Flips correlate with degradation in generative task performance (MT-Bench scores)
- Answers with low "top margin" (small probability difference between top two options) are more susceptible to flips

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Quantization introduces symmetric noise in token probabilities that preserves overall accuracy but shifts individual answers
- Mechanism: Lower-bit quantization reduces precision of weights and activations, perturbing the probability distribution over output tokens. This noise is symmetric enough that increases for some incorrect options are balanced by decreases for others, maintaining aggregate accuracy
- Core assumption: The perturbations introduced by quantization are sufficiently symmetric and small that they don't systematically favor correct answers over incorrect ones
- Evidence anchors:
  - [abstract] "even when the accuracy of baseline and compressed model are similar, we observe the phenomenon of flips, wherein answers change from correct to incorrect and vice versa in proportion"
  - [section 4.3] "perplexity may be interpreted as the inverse of the geometric mean of token probabilities, lower probabilities for some tokens in the test dataset may be cancelled by higher probabilities of other tokens"
  - [corpus] Weak evidence; corpus focuses on general compression evaluation, not the specific noise symmetry mechanism

### Mechanism 2
- Claim: Flips occur more frequently on questions with low "top margin" (small probability difference between top two options)
- Mechanism: When the model is uncertain between two options (low top margin), small perturbations from quantization can easily flip which option is most likely. Correct answers tend to have higher top margins, making them more stable
- Core assumption: The probability difference between top two options correlates with answer stability under perturbations
- Evidence anchors:
  - [section 5] "Answers are likely to change when top margin is low... Quantization introduces some noise in the weights and activations... Thus, we expect that answers are more likely to change when top margin is low"
  - [section 5] "Table 1 shows the top margins for questions for which the LLM's answer is correct and when the answer is incorrect. We observe that, top margin when correct is, on average, greater than the top margin when incorrect"
  - [corpus] Weak evidence; corpus papers discuss compression faithfulness but don't address the top margin mechanism specifically

### Mechanism 3
- Claim: KL-Divergence between baseline and compressed model distributions correlates with flips, making it a good proxy for measuring behavioral divergence
- Mechanism: KL-Divergence measures the overall distributional difference between models. Since flips represent changes in the most likely answer, they contribute to KL-Divergence, and the correlation suggests KL-Divergence captures these behavioral changes
- Core assumption: Changes in the most likely answer (flips) are a significant contributor to overall distributional differences measured by KL-Divergence
- Evidence anchors:
  - [abstract] "We demonstrate that well-known distance metrics like KL-Divergence on a given dataset can better identify the differences created due to various compression techniques and this metric correlates well with flips"
  - [section 4.1] "From Figure 5 in Appendix, we observe that the two distance metrics KL-Divergence and %flips are well correlated. For example, Spearman correlation on the MMLU benchmark is 0.981"
  - [corpus] Weak evidence; corpus papers discuss evaluation metrics but don't specifically address the KL-Divergence/flips correlation

## Foundational Learning

- Concept: Probability distributions over discrete outcomes
  - Why needed here: The paper analyzes how quantization changes the probability distribution over answer options, which is fundamental to understanding flips and KL-Divergence
  - Quick check question: If a model outputs probabilities [0.4, 0.35, 0.15, 0.1] for four options and quantization changes them to [0.38, 0.37, 0.13, 0.12], what happens to the most likely answer?

- Concept: KL-Divergence as a measure of distributional difference
  - Why needed here: The paper proposes KL-Divergence as a distance metric to evaluate compression quality, requiring understanding of how it quantifies differences between probability distributions
  - Quick check question: If two models output identical probability distributions over answers, what is their KL-Divergence?

- Concept: Multiple-choice question evaluation metrics
  - Why needed here: The paper evaluates compression techniques on multiple-choice benchmarks, requiring understanding of accuracy, normalized accuracy, and how these metrics work for MCQ tasks
  - Quick check question: For a 4-option MCQ where the model outputs probabilities [0.1, 0.2, 0.3, 0.4] and the correct answer is option 4, what is the normalized accuracy?

## Architecture Onboarding

- Component map: Baseline model -> Compression (quantization/pruning) -> Benchmark evaluation (accuracy, flips, KL-Divergence) -> Generative task validation (MT-Bench) -> Correlation analysis
- Critical path: The most important evaluation flow is: Compression → Benchmark Evaluation (accuracy, flips, KL-Divergence) → Correlation Analysis → Generative Task Validation
- Design tradeoffs: The paper trades computational cost of KL-Divergence calculation against the interpretability of flips metric; KL-Divergence is more comprehensive but flips is easier to understand for end users
- Failure signatures: If accuracy appears similar but flips are high, this indicates the compressed model behaves differently despite similar aggregate performance; if KL-Divergence is high but flips are low, the models differ in probability distributions but not in most likely answers
- First 3 experiments:
  1. Apply different quantization schemes (BnB W8A8, GPTQ W8A16, etc.) to Llama2-13b on MMLU and measure accuracy, flips, and KL-Divergence
  2. Vary the number of layers dropped in layer dropping compression and observe the relationship between accuracy, flips, and KL-Divergence
  3. Test the correlation between flips on MCQ tasks and MT-Bench scores to validate flips as a proxy for generative task performance

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the phenomenon of flips impact the reliability of LLM compression in safety-critical applications?
- Basis in paper: [explicit] The paper demonstrates that flips occur even when overall accuracy is preserved, and that this phenomenon is correlated with degradation in generative tasks
- Why unresolved: The paper does not specifically analyze the implications of flips for safety-critical applications where model reliability is paramount
- What evidence would resolve it: Empirical studies measuring flip rates and their impact on safety-critical tasks across different compression methods and model sizes

### Open Question 2
- Question: Can the correlation between flips and KL-Divergence be leveraged to develop more efficient evaluation metrics for LLM compression?
- Basis in paper: [explicit] The authors show that flips correlates well with KL-Divergence and with degradation in generative tasks
- Why unresolved: While the correlation is demonstrated, the paper does not explore whether flips could serve as a practical proxy for KL-Divergence in large-scale evaluations
- What evidence would resolve it: Comparative studies evaluating the efficiency and accuracy of using flips versus KL-Divergence across diverse model architectures and compression techniques

### Open Question 3
- Question: What are the underlying mechanisms that cause flips to be nearly balanced between correct→incorrect and incorrect→correct transitions?
- Basis in paper: [explicit] The authors analyze the top margin metric and show that incorrect answers have lower top margins and are more likely to change, explaining the balance
- Why unresolved: The paper provides an intuitive explanation but does not investigate the mathematical or architectural reasons for this balanced flip phenomenon
- What evidence would resolve it: Theoretical analysis and empirical experiments exploring how quantization affects probability distributions near decision boundaries

## Limitations
- The flips phenomenon may not generalize beyond the specific models and quantization schemes tested
- The paper demonstrates correlation but does not establish causation between KL-Divergence and behavioral changes
- The interpretation of KL-Divergence as the definitive superior metric for compression evaluation is not conclusively proven

## Confidence

### Confidence Labels
- Medium confidence: The flips phenomenon and its correlation with KL-Divergence
- Medium confidence: The top margin mechanism explaining when flips occur
- Low confidence: The interpretation of KL-Divergence as the definitive superior metric for compression evaluation

## Next Checks
1. Test the flips phenomenon on additional model architectures (beyond Llama2 and Yi) and compression methods (beyond quantization, such as pruning or knowledge distillation) to verify generalizability
2. Conduct ablation studies varying the magnitude of quantization noise to determine the threshold where accuracy degrades versus where flips remain correlated with KL-Divergence
3. Evaluate whether the correlation between flips and MT-Bench scores holds when tested on held-out generative tasks not used in the original correlation analysis