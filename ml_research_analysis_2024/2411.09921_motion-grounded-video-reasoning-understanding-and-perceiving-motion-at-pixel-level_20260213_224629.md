---
ver: rpa2
title: 'Motion-Grounded Video Reasoning: Understanding and Perceiving Motion at Pixel
  Level'
arxiv_id: '2411.09921'
source_url: https://arxiv.org/abs/2411.09921
tags:
- video
- reasoning
- motion
- temporal
- understanding
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Motion-Grounded Video Reasoning, a new task
  for comprehensive motion understanding that requires generating spatiotemporal segmentation
  masks as visual answers to motion-related questions. The task addresses the limitations
  of existing video understanding tasks by incorporating both implicit reasoning and
  pixel-level spatiotemporal grounding.
---

# Motion-Grounded Video Reasoning: Understanding and Perceiving Motion at Pixel Level

## Quick Facts
- arXiv ID: 2411.09921
- Source URL: https://arxiv.org/abs/2411.09921
- Reference count: 40
- Key outcome: Introduces Motion-Grounded Video Reasoning task and GROUNDMORE dataset; proposes MORA baseline achieving 21.5% relative improvement over existing visual grounding methods

## Executive Summary
This paper introduces Motion-Grounded Video Reasoning, a new task requiring spatiotemporal segmentation masks as visual answers to motion-related questions. Unlike existing video understanding tasks that focus on explicit action detection or spatial grounding, this task combines implicit reasoning with pixel-level spatiotemporal grounding to capture comprehensive motion understanding. The authors create GROUNDMORE, a large-scale dataset with 1,715 videos, 7,577 questions, and 249K object masks across four question types (Causal, Sequential, Counterfactual, Descriptive), and propose MORA, a novel baseline model that integrates multimodal reasoning from LLaVA, pixel-level perception from SAM, and temporal awareness through a dedicated localization head.

## Method Summary
The MORA baseline integrates multimodal reasoning from LLaVA, pixel-level perception from SAM, and temporal localization via a dedicated [LOC] token and lightweight localization head. The model uses spatiotemporal pooling to encode video frames, with [SEG] token generating spatial segmentation masks through SAM and [LOC] token predicting binary temporal masks. Training follows a two-stage approach: pre-training on Ref-YouTubeVOS and MeViS datasets (converted to QA format) for 20 epochs without temporal module, then fine-tuning on GROUNDMORE training split with temporal localization for another 20 epochs using Jaccard Index, F-measure, and J&F combined score for evaluation.

## Key Results
- MORA achieves state-of-the-art performance on GROUNDMORE dataset with significant improvements over existing visual grounding baselines
- The model shows average 21.5% relative improvement compared to the best existing visual grounding method
- Temporal localization component provides modest improvements for sequential questions but performance declines for counterfactual questions, indicating room for improvement in handling complex temporal reasoning

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The task requires both implicit reasoning and pixel-level spatiotemporal grounding to fully capture motion understanding.
- Mechanism: By converting motion-related scenarios into questions that require reasoning about both spatial context (objects involved in motion) and temporal context (chronologically adjacent motions), the model must understand not just what is moving but when and with what it interacts.
- Core assumption: Motion is inherently a spatiotemporal concept that cannot be fully understood through spatial-only or temporal-only approaches.
- Evidence anchors:
  - [abstract] "This task extends existing spatiotemporal grounding work focusing on explicit action/motion grounding, to a more general format by enabling implicit reasoning via questions."
  - [section] "First, understanding specific motions requires analyzing their spatial contexts... Second, temporal context, which provides chronological order to distinguish different motions, is also crucial for motion understanding."

### Mechanism 2
- Claim: The MORA model achieves superior performance by integrating multimodal reasoning, pixel-level perception, and temporal localization.
- Mechanism: MORA combines the reasoning capabilities of LLaVA with SAM's pixel-level perception and adds a temporal localization head using a dedicated [LOC] token, allowing it to generate accurate spatiotemporal masks rather than just text answers.
- Core assumption: Effective motion-grounded video reasoning requires both understanding the question and precisely localizing the relevant objects in both space and time.
- Evidence anchors:
  - [abstract] "MORA incorporates the multimodal reasoning ability from the Multimodal LLM, the pixel-level perception capability from the grounding model (SAM), and the temporal perception ability from a lightweight localization head."
  - [section] "To enable the temporal localization ability, MORA utilizes the extra [LOC] token to learn a binary temporal mask, which refines the direct SAM outputs."

### Mechanism 3
- Claim: The GROUNDMORE dataset's four question types comprehensively evaluate different aspects of motion understanding.
- Mechanism: By including Causal (understanding motivations), Sequential (understanding temporal order), Counterfactual (reasoning about hypothetical scenarios), and Descriptive (understanding abstract attributes) questions, the dataset tests models' ability to reason about motion from multiple perspectives.
- Core assumption: Comprehensive motion understanding requires the ability to reason about motivations, temporal sequences, hypothetical scenarios, and abstract attributes simultaneously.
- Evidence anchors:
  - [abstract] "To facilitate the development of the new task, we collect a large-scale dataset called GROUNDMORE, which comprises 1,715 video clips, 249K object masks that are deliberately designed with 4 question types (Causal, Sequential, Counterfactual, and Descriptive) for benchmarking deep and comprehensive motion reasoning abilities."
  - [section] "As shown in Figure 1(e), Causal questions explore the motivations behind motions, Sequential questions probe the order of temporally adjacent motions, Counterfactual questions are designed for imagining and reasoning about false reality and Descriptive questions ask about the general dynamic scene or abstract motion-related attributes."

## Foundational Learning

- Concept: Multimodal reasoning and grounding
  - Why needed here: The task requires understanding both visual content (motion, objects, spatial relationships) and textual queries (questions about motion) to generate appropriate spatiotemporal masks.
  - Quick check question: Can you explain how a model would need to process both the visual content of a basketball game and the question "Who needs to be passed or else the man in grey cannot easily score?" to generate the correct mask?

- Concept: Spatiotemporal segmentation and localization
  - Why needed here: Unlike traditional video understanding tasks that might only identify actions or objects, this task requires generating pixel-level masks that are localized both spatially and temporally.
  - Quick check question: How would you modify a standard video object segmentation approach to also incorporate temporal localization based on a question about when a specific motion occurs?

- Concept: Question-answering formulation for visual tasks
  - Why needed here: Converting motion understanding into a question-answering format allows for implicit reasoning about motion contexts rather than just explicit detection or recognition.
  - Quick check question: What are the advantages of formulating motion understanding as a question-answering task versus traditional action recognition or object detection approaches?

## Architecture Onboarding

- Component map:
  - LLaVA (Multimodal LLM) -> Handles reasoning about questions and visual content
  - SAM (Segment Anything Model) -> Provides pixel-level perception for object segmentation
  - [SEG] token -> Encodes spatial segmentation information
  - [LOC] token -> Encodes temporal localization information
  - Temporal localization head -> Decodes binary temporal masks
  - Spatiotemporal pooling -> Efficiently encodes video frames

- Critical path: Question → LLaVA reasoning → [SEG] token generation → SAM segmentation → [LOC] token generation → Temporal mask → Final spatiotemporal mask output

- Design tradeoffs:
  - Using frame-by-frame processing with spatiotemporal pooling vs. full temporal modeling (efficiency vs. temporal coherence)
  - End-to-end training of reasoning and grounding vs. two-stage approach (integration vs. modularity)
  - Binary temporal masks vs. continuous temporal attention (simplicity vs. granularity)

- Failure signatures:
  - Incorrect object segmentation (SAM failure or misalignment with reasoning output)
  - Wrong temporal boundaries (LOC token or temporal head failure)
  - Answering wrong question (LLaVA reasoning failure)
  - No temporal localization (missing or ineffective temporal component)

- First 3 experiments:
  1. Ablation study: Test MORA with and without the temporal localization branch to quantify its contribution
  2. Zero-shot evaluation: Test MORA on the dataset without fine-tuning to assess generalization
  3. Dataset diagnostics: Evaluate model performance with and without temporal context to validate the importance of temporal reasoning

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can Motion-Grounded Video Reasoning models be improved to better handle the temporal localization challenges, particularly for counterfactual and sequential questions?
- Basis in paper: [explicit] The paper notes that even their proposed MORA model shows substantial room for improvement, particularly in sequential and counterfactual questions where temporal awareness is crucial.
- Why unresolved: The paper identifies temporal localization as a key challenge but doesn't provide concrete solutions beyond their basic [LOC] token approach, which shows only modest improvements for sequential questions and performance decline for counterfactual ones.
- What evidence would resolve it: A model achieving significantly higher performance on sequential and counterfactual questions while maintaining overall performance would demonstrate successful temporal localization improvements.

### Open Question 2
- Question: What architectural modifications could enhance Motion-Grounded Video Reasoning models' ability to handle implicit reasoning challenges without relying on ground truth answers?
- Basis in paper: [explicit] The dataset diagnosis shows that providing ground truth answers leads to an average of 14.29 improvement in J&F, highlighting the difficulty of implicit reasoning.
- Why unresolved: The paper demonstrates that implicit reasoning is a major bottleneck but doesn't explore architectural solutions beyond their current baseline model that could bridge this gap.
- What evidence would resolve it: A model achieving comparable performance on implicit reasoning questions versus those with explicit object information would demonstrate successful architectural solutions.

### Open Question 3
- Question: How can Motion-Grounded Video Reasoning models be optimized to reduce false positives while maintaining high recall for motion-related objects?
- Basis in paper: [inferred] The paper notes that PG-Video-LLaV A tends to ground all salient objects given scene descriptions, resulting in more false positives, suggesting this is a general challenge for the task.
- Why unresolved: The paper identifies false positives as an issue with current approaches but doesn't provide specific solutions or metrics for balancing precision and recall in this task.
- What evidence would resolve it: A model achieving high J&F scores while maintaining low false positive rates across all question types would demonstrate successful optimization for this challenge.

## Limitations
- Dataset generalization may be limited as videos were sourced from Internet platforms and cropped to average 9.61 seconds, potentially restricting temporal context available for reasoning
- Model architecture inherits constraints from LLaVA and SAM components, with the two-stage approach potentially introducing cascading errors
- Evaluation metrics focusing on segmentation quality may not fully capture semantic correctness of motion understanding or adequately assess temporal reasoning quality

## Confidence
- High Confidence: The core claim that motion understanding requires both spatial and temporal context is well-supported by literature on video understanding
- Medium Confidence: MORA's effectiveness in integrating multimodal reasoning, pixel-level perception, and temporal localization is demonstrated through quantitative results, but improvements may be partially dataset-specific
- Low Confidence: The claim that four question types comprehensively evaluate all aspects of motion understanding is asserted but not empirically validated

## Next Checks
1. **Cross-Dataset Generalization Test**: Evaluate MORA on established video understanding benchmarks like Kinetics, Something-Something V2, or Ego4D, converting their action recognition labels to spatiotemporal mask format to validate whether improvements generalize beyond GROUNDMORE dataset.

2. **Ablation Study with Alternative Components**: Replace LLaVA with other multimodal models (e.g., Flamingo, GIT) and SAM with alternative segmentation models (e.g., Mask2Former, PointRend) while keeping temporal localization component constant to isolate contributions of each component.

3. **Human Evaluation of Motion Understanding**: Conduct a human study where participants evaluate semantic correctness of MORA's spatiotemporal masks beyond segmentation metrics, assessing whether generated masks correctly identify objects relevant to motion context and whether reasoning is sound.