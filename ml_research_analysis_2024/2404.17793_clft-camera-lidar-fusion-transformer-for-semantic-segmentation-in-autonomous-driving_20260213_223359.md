---
ver: rpa2
title: 'CLFT: Camera-LiDAR Fusion Transformer for Semantic Segmentation in Autonomous
  Driving'
arxiv_id: '2404.17793'
source_url: https://arxiv.org/abs/2404.17793
tags:
- fusion
- lidar
- segmentation
- camera
- ieee
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a vision transformer-based network for camera-LiDAR
  fusion for semantic segmentation in autonomous driving. The method uses a progressive-assemble
  strategy of vision transformers on a double-direction network and integrates the
  results in a cross-fusion strategy over the transformer decoder layers.
---

# CLFT: Camera-LiDAR Fusion Transformer for Semantic Segmentation in Autonomous Driving

## Quick Facts
- **arXiv ID**: 2404.17793
- **Source URL**: https://arxiv.org/abs/2404.17793
- **Reference count**: 40
- **Primary result**: Vision transformer-based network achieves up to 10% IoU improvement for semantic segmentation in challenging dark-wet conditions compared to FCN-based fusion methods

## Executive Summary
This paper introduces CLFT, a camera-LiDAR fusion transformer network for semantic segmentation in autonomous driving. The method employs a progressive-assemble strategy that combines multi-scale features from different transformer encoder layers with a cross-fusion approach over decoder layers. The network is evaluated on the Waymo Open Dataset across challenging conditions including rain and low illumination, demonstrating robust performance improvements over traditional FCN-based approaches, particularly for underrepresented classes like pedestrians.

## Method Summary
CLFT uses a double-direction transformer encoder-decoder architecture with progressive feature assembly and cross-fusion. The encoder processes camera RGB images (384x384) and LiDAR point clouds (projected to three 2D views) through a vision transformer backbone. The decoder progressively assembles tokens from four selected encoder layers into image-like representations at different resolutions, then fuses camera and LiDAR features using residual convolution units. The network is trained with weighted cross-entropy loss, Adam optimizer, and ImageNet-pretrained encoder weights, with evaluation on vehicle and human classes using IoU, precision, and recall metrics.

## Key Results
- CLFT achieves up to 10% IoU improvement for challenging dark-wet conditions compared to FCN-based camera-LiDAR fusion
- The method shows robust performance across rain and low illumination scenarios
- CLFT-hybrid variant demonstrates the best performance among tested configurations
- Transformer-based approach particularly benefits underrepresented classes like pedestrians

## Why This Works (Mechanism)

### Mechanism 1
The progressive-assemble strategy improves segmentation accuracy by leveraging multi-scale features from different transformer encoder layers. The decoder progressively assembles tokens from four selected transformer encoder layers into image-like representations at different resolutions, allowing the network to combine both low-level and high-level features for more accurate segmentation.

### Mechanism 2
Cross-fusion of camera and LiDAR features improves segmentation performance by combining complementary sensor information. The decoder uses residual convolution units to progressively fuse camera and LiDAR feature maps at different scales, summing representations with results from previous fusion operations.

### Mechanism 3
Transformer backbone with multi-head attention mechanism improves handling of underrepresented classes like pedestrians. The transformer's self-attention mechanism allows the model to capture long-range dependencies and global context, which helps in identifying and segmenting objects that are less represented in the training data.

## Foundational Learning

- **Vision Transformer (ViT) architecture**: Needed as the foundation for processing camera and LiDAR data. Quick check: What are the key differences between ViT and traditional CNN architectures?
- **Multi-head self-attention mechanism**: Central to the transformer's ability to capture global context. Quick check: How does multi-head attention differ from single-head attention in terms of feature representation?
- **Sensor fusion techniques**: Essential for combining camera and LiDAR data. Quick check: What are the main categories of sensor fusion (early, middle, late) and how do they differ?

## Architecture Onboarding

- **Component map**: Camera/LiDAR input → ViT Encoder → Progressive Assembly → Cross-Fusion → Segmentation Output
- **Critical path**: Camera/LiDAR input → ViT Encoder → Progressive Assembly → Cross-Fusion → Segmentation Output
- **Design tradeoffs**: Patch size (p=16) vs. feature resolution; number of encoder layers to use for assembly (4 layers chosen); fusion strategy (residual connections vs. other methods); memory vs. performance (CLFT-hybrid has best performance but higher memory)
- **Failure signatures**: Poor segmentation in underrepresented classes suggests attention mechanism isn't learning properly; discontinuities in segmentation indicate assembly/fusion issues; slow inference suggests memory bottlenecks
- **First 3 experiments**: 1) Test individual modalities (camera-only, LiDAR-only) to verify each encoder works; 2) Test progressive assembly without fusion to verify multi-scale feature integration; 3) Test fusion with pre-trained encoders to isolate fusion performance from encoder quality

## Open Questions the Paper Calls Out

### Open Question 1
How would the performance of CLFT change if the dataset was more balanced across illumination and weather conditions? The authors note the dataset is heavily unbalanced towards light-dry scenarios (roughly 68% of the total), affecting results particularly for underrepresented classes like humans.

### Open Question 2
What is the impact of using different pre-trained backbones for the transformer encoder on the performance of CLFT? The paper uses ImageNet pre-trained weights but does not explore how different pre-trained backbones might affect performance.

### Open Question 3
How does the performance of CLFT compare to other transformer-based models that use LiDAR input directly, without projecting it to 2D views? The paper projects LiDAR point clouds into 2D views but does not compare against models using direct LiDAR input.

## Limitations
- Lack of detailed ablation studies to isolate the contribution of each component (progressive assembly, cross-fusion, transformer backbone)
- No comparison with state-of-the-art camera-LiDAR fusion methods on the same challenging conditions
- Limited discussion of computational complexity and real-time performance requirements

## Confidence
- **High Confidence**: The core architectural design (transformer-based encoder-decoder with progressive assembly) is clearly described and follows established vision transformer principles
- **Medium Confidence**: The performance improvements (up to 10% IoU) are reported but without comprehensive ablation studies or comparisons to alternative methods
- **Low Confidence**: The specific claims about transformer advantages for underrepresented classes are supported by intuition rather than rigorous analysis

## Next Checks
1. Conduct ablation studies removing the progressive assembly and cross-fusion components separately to quantify their individual contributions
2. Compare performance with state-of-the-art CNN-based fusion methods (e.g., FuseFormer, FuseSeg) under identical challenging conditions
3. Evaluate inference speed and memory usage across different CLFT variants to assess practical deployment feasibility in autonomous vehicles