---
ver: rpa2
title: 'Hyperparameters in Continual Learning: A Reality Check'
arxiv_id: '2403.09066'
source_url: https://arxiv.org/abs/2403.09066
tags:
- decay
- learning
- hyperparameters
- evaluation
- batch
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper identifies a critical flaw in the standard evaluation
  protocol for continual learning (CL) algorithms: hyperparameters are tuned and evaluated
  on the same task sequence, leading to overestimated performance and unrealistic
  assumptions about access to full task data. The authors propose the Generalizable
  Two-phase Evaluation Protocol (GTEP), which separates hyperparameter tuning and
  evaluation onto different datasets while keeping the task configuration identical.'
---

# Hyperparameters in Continual Learning: A Reality Check

## Quick Facts
- arXiv ID: 2403.09066
- Source URL: https://arxiv.org/abs/2403.09066
- Authors: Sungmin Cha; Kyunghyun Cho
- Reference count: 40
- Primary result: State-of-the-art CL algorithms overestimate performance when hyperparameters are tuned on the same task sequence used for evaluation

## Executive Summary
This paper identifies a critical flaw in standard continual learning (CL) evaluation protocols: hyperparameters are tuned and evaluated on the same task sequence, leading to overestimated performance and unrealistic assumptions about access to full task data. The authors propose the Generalizable Two-phase Evaluation Protocol (GTEP), which separates hyperparameter tuning and evaluation onto different datasets while keeping the task configuration identical. This design more realistically assesses how algorithms generalize to unseen CL scenarios. Extensive experiments on class-incremental learning—both with and without pretrained models—show that most state-of-the-art algorithms fail to maintain their reported performance under GTEP, especially recent ones. Many exhibit high sensitivity to hyperparameters and poor stability across task orderings. Even well-generalizing algorithms are often inefficient in training time or parameter count. The results call for a shift in CL evaluation toward protocols that prioritize generalizability and practical efficiency.

## Method Summary
The paper introduces the Generalizable Two-phase Evaluation Protocol (GTEP) that decouples hyperparameter tuning from evaluation by using two distinct datasets. The first phase (DHT - Dataset for Hyperparameter Tuning) is used to sample and optimize hyperparameters across multiple random trials with 5 seeds each. The second phase (DE - Dataset for Evaluation) applies the best hyperparameters found in the tuning phase to assess true algorithm performance. Both phases share identical scenario configurations (same number of tasks, classes per task, etc.) but use different data distributions. The protocol is applied to class-incremental learning with both pretrained and non-pretrained models, evaluating a wide range of algorithms including replay-based, regularization-based, expansion-based, prompt-based, and representation-based methods across CIFAR-100, ImageNet variants, and CUB-200 datasets.

## Key Results
- Most state-of-the-art CL algorithms show significant performance drops when evaluated under GTEP compared to conventional protocols
- Recent algorithms like FOSTER, BEEF, L2P, and DualPrompt show particularly poor generalizability despite strong reported performance
- Many algorithms exhibit high hyperparameter sensitivity and instability across different task orderings and seeds
- Even algorithms that generalize well under GTEP often suffer from long training times or substantial parameter requirements
- The protocol reveals that hyperparameter tuning skill, not just CL capacity, heavily influences reported performance

## Why This Works (Mechanism)

### Mechanism 1
The proposed protocol reduces overestimation of CL capacity by decoupling hyperparameter tuning from evaluation across disjoint datasets. By tuning hyperparameters on one dataset (DHT) and evaluating on another (DE), the method simulates realistic constraints where new tasks arrive without access to previous task data for re-tuning. The core assumption is that performance on a different dataset (DE) is a better proxy for real-world generalizability than performance on the same dataset used for tuning. Evidence shows that algorithms perform significantly worse under GTEP, revealing the artificial inflation in conventional evaluation. Break condition: If DHT and DE are too dissimilar in distribution, hyperparameters tuned on DHT may not transfer well to DE, potentially leading to misleading evaluation results.

### Mechanism 2
The protocol exposes hyperparameter sensitivity and instability hidden in conventional evaluation. By evaluating algorithms across multiple task orderings and different similarity cases, the protocol reveals performance variance and failure modes that standard protocols miss. The core assumption is that high variance across seeds or task orderings indicates poor generalizability and instability. Evidence includes observations that many algorithms suffer from long training times, substantial parameter requirements, or high performance variance, with BEEF producing NaN values for certain seeds. Break condition: If an algorithm is inherently unstable (e.g., due to adversarial learning components), even the proposed protocol may fail to provide reliable evaluation metrics.

### Mechanism 3
The protocol enables fair comparison of algorithms by isolating CL capacity from hyperparameter optimization artifacts. By using the same best hyperparameters across different algorithms for a given scenario, the protocol ensures that differences in performance reflect true CL ability rather than hyperparameter tuning skill. The core assumption is that the best hyperparameters found in the tuning phase are reasonably transferable to the evaluation phase for a given algorithm. Evidence shows that using the best hyperparameters leads to better performance across all algorithms. Break condition: If optimal hyperparameters are highly sensitive to the specific dataset or task ordering, transferring them may not yield fair comparisons.

## Foundational Learning

- Concept: Continual Learning (CL) fundamentals
  - Why needed here: Understanding the plasticity-stability tradeoff is essential to grasp why hyperparameter tuning in CL is problematic.
  - Quick check question: What is catastrophic forgetting, and how do CL algorithms typically address it?

- Concept: Hyperparameter tuning and evaluation separation
  - Why needed here: The core innovation of the paper is separating these phases to avoid overfitting to a single scenario.
  - Quick check question: In traditional ML, why do we use separate validation and test sets?

- Concept: Dataset similarity and domain shift
  - Why needed here: The paper evaluates algorithms under different similarity cases (high vs. low) to assess generalizability.
  - Quick check question: How might performance differ when hyperparameters tuned on Dataset A are evaluated on Dataset B if the datasets have different class distributions?

## Architecture Onboarding

- Component map:
  Data generators -> Hyperparameter tuner -> Evaluator -> Experiment runner

- Critical path:
  1. Generate DHT and DE with same task configuration but different data
  2. Sample hyperparameters and train on DHT, measure validation performance
  3. Select best hyperparameters based on harmonic mean of Acc and AvgAcc
  4. Train on DE using selected hyperparameters, measure final performance
  5. Repeat for multiple seeds and task orderings

- Design tradeoffs:
  - Number of hyperparameter samples vs. computational cost
  - Dataset similarity vs. evaluation realism
  - Fixed vs. adaptive hyperparameter ranges per algorithm

- Failure signatures:
  - NaN values during training (as seen with BEEF)
  - High variance in performance across seeds
  - Performance drop between tuning and evaluation phases

- First 3 experiments:
  1. Replicate conventional protocol results (same dataset for tuning and evaluation) to establish baseline
  2. Apply GTEP with high similarity (splitting same dataset) to check for performance drop
  3. Apply GTEP with low similarity (different datasets) to assess generalizability under domain shift

## Open Questions the Paper Calls Out

### Open Question 1
How sensitive is the generalizability of continual learning algorithms to the specific choice of datasets in the hyperparameter tuning and evaluation phases? While the paper shows that generalizability varies with dataset similarity, it does not explore the full spectrum of possible dataset choices or quantify the impact of specific dataset characteristics. Evidence that would resolve this includes a systematic study varying the datasets used for DHT and DE across a wider range of domains and similarity levels, measuring the impact on algorithm generalizability.

### Open Question 2
Can more efficient hyperparameter tuning methods be developed that maintain or improve generalizability in continual learning? The authors note that implementing GTEP requires repeated training trials, which can be computationally demanding, and suggest developing more sample-efficient tuning strategies as future work. Evidence that would resolve this includes empirical comparison of GTEP using random sampling with alternative hyperparameter tuning methods (e.g., Bayesian optimization, gradient-based methods) in terms of computational cost and generalizability.

### Open Question 3
How robust are continual learning algorithms to unpredictable or adaptive scenarios, such as unknown task boundaries or shifting data distributions? The authors acknowledge that their experiments assume predictable CL scenarios with known task numbers and class distributions, and suggest evaluating algorithms in unpredictable scenarios as future work. Evidence that would resolve this includes experimental evaluation of CL algorithms on benchmarks with dynamic task boundaries, concept drift, or other forms of scenario unpredictability.

## Limitations
- The paper focuses exclusively on class-incremental learning, so findings may not generalize to other CL scenarios like task-incremental or domain-incremental learning.
- GTEP introduces a new assumption about hyperparameter transferability between datasets, but the paper doesn't extensively analyze scenarios where this transfer fails.
- The analysis of algorithm sensitivity and stability is limited to performance metrics, with limited discussion of computational efficiency trade-offs.

## Confidence
- High confidence: The fundamental claim that current CL evaluation protocols lead to overestimation of algorithm performance due to hyperparameter tuning on the same data used for evaluation.
- Medium confidence: The claim that GTEP provides a more realistic evaluation of CL algorithms' generalizability, though protocol sensitivity to dataset choice needs further validation.
- Medium confidence: The assertion that many state-of-the-art CL algorithms fail to maintain performance under GTEP, though investigation into specific failure modes could strengthen this claim.

## Next Checks
1. **Dataset Similarity Analysis**: Systematically vary the similarity between DHT and DE datasets (e.g., using domain adaptation metrics) to identify at what point hyperparameter transfer becomes problematic, and establish guidelines for selecting appropriate dataset pairs.

2. **Cross-Scenario Validation**: Apply GTEP to other CL scenarios beyond class-incremental learning (such as task-incremental or domain-incremental learning) to verify whether the protocol's insights generalize across different CL problem settings.

3. **Algorithm Failure Analysis**: For algorithms that show high sensitivity under GTEP, conduct ablation studies to determine whether failures stem from hyperparameter sensitivity, algorithmic limitations, or implementation issues, providing more granular insights into CL algorithm robustness.