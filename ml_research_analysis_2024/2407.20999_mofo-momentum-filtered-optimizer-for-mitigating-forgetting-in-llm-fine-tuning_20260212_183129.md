---
ver: rpa2
title: 'MoFO: Momentum-Filtered Optimizer for Mitigating Forgetting in LLM Fine-Tuning'
arxiv_id: '2407.20999'
source_url: https://arxiv.org/abs/2407.20999
tags:
- mofo
- fine-tuning
- learning
- forgetting
- adam
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Momentum-Filtered Optimizer (MoFO), a novel
  approach designed to mitigate catastrophic forgetting during large language model
  (LLM) fine-tuning. The key insight is that minimizing the distance between fine-tuned
  and pre-trained models helps preserve pre-trained knowledge.
---

# MoFO: Momentum-Filtered Optimizer for Mitigating Forgetting in LLM Fine-Tuning

## Quick Facts
- arXiv ID: 2407.20999
- Source URL: https://arxiv.org/abs/2407.20999
- Reference count: 40
- Primary result: Momentum-Filtered Optimizer (MoFO) mitigates catastrophic forgetting during LLM fine-tuning while maintaining comparable performance to standard methods

## Executive Summary
MoFO is a novel optimizer designed to address catastrophic forgetting during large language model fine-tuning. The key insight is that minimizing the distance between fine-tuned and pre-trained models helps preserve pre-trained knowledge. MoFO achieves this by updating only parameters with the largest momentum magnitudes at each iteration, inspired by block coordinate descent methods. The approach is theoretically sound, with convergence guarantees matching Adam, and empirically effective across multiple tasks and datasets.

## Method Summary
MoFO modifies the Adam optimizer by adding a momentum filtering mechanism that selects parameters with the largest momentum magnitudes for update. The model parameters are partitioned into blocks, and the filter is applied to each block individually. At each iteration, gradients and momentum are computed for all parameters, then only the top-α parameters in each block (based on momentum magnitude) are updated. This restricted parameter movement helps preserve pre-trained knowledge while still allowing effective task adaptation.

## Key Results
- MoFO achieves comparable fine-tuning performance to standard methods while significantly reducing forgetting of general capabilities
- When fine-tuning Llama-2-7B on MetaMathQA, MoFO preserves general capabilities with only 0.4% average reduction compared to 5.4-9.3% reductions for other methods
- MoFO is effective in continual learning scenarios and works well with other methods like LoRA
- Theoretical analysis shows MoFO converges to critical points of the fine-tuning loss with the same rate as Adam (O(logT/√T))

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: MoFO mitigates forgetting by converging to minima closer to the pre-trained model.
- **Mechanism**: By only updating parameters with the largest momentum magnitudes, MoFO restricts parameter movement and keeps the fine-tuned model closer to the pre-trained state.
- **Core assumption**: The distance between the fine-tuned and pre-trained models correlates with forgetting.
- **Evidence anchors**:
  - [abstract] "minimizing the distance between fine-tuned and pre-trained models helps preserve pre-trained knowledge"
  - [section 2.1] "The closer a fine-tuned model stays to the pretrained parameters, the less forgetting tends to occur"
  - [corpus] Weak - no direct mention of forgetting-mitigation via distance in corpus neighbors
- **Break condition**: If the correlation between distance and forgetting does not hold in practice, or if the momentum filtering mechanism does not effectively restrict parameter movement.

### Mechanism 2
- **Claim**: Momentum filtering in MoFO leads to more stable updates compared to gradient filtering.
- **Mechanism**: Momentum accumulates historical gradients, providing stability by smoothing fluctuations, while 2nd-order moment vt may normalize gradients and average out parameter importance.
- **Core assumption**: Stable updates during training lead to better fine-tuning performance and less forgetting.
- **Evidence anchors**:
  - [section 4.4] "Utilizing momentum instead of gradient filtering leads to more stable updates"
  - [section F.4] "Momentum accumulates historical gradients, so it promotes stability by smoothing out fluctuations"
  - [corpus] Weak - no direct mention of momentum filtering for stability in corpus neighbors
- **Break condition**: If momentum accumulation does not provide stability in practice, or if gradient filtering with vt normalization proves equally effective.

### Mechanism 3
- **Claim**: MoFO achieves comparable convergence to Adam while mitigating forgetting.
- **Mechanism**: MoFO updates only a subset of parameters with large momentum, achieving similar convergence rates to Adam (O(logT/√T)) while restricting parameter movement.
- **Core assumption**: Convergence to critical points of the fine-tuning loss can be achieved with partial parameter updates.
- **Evidence anchors**:
  - [abstract] "Theoretical analysis shows MoFO converges to critical points of the fine-tuning loss with the same rate as Adam"
  - [section 3.1] "Despite updating only a subset of parameters, MoFO maintains the same convergence rate as Adam"
  - [corpus] Weak - no direct mention of convergence rates in corpus neighbors
- **Break condition**: If partial parameter updates fail to converge to critical points, or if the convergence rate is significantly worse than Adam.

## Foundational Learning

- **Concept: Block Coordinate Descent (BCD)**
  - Why needed here: MoFO extends BCD methods to LLM fine-tuning by updating only a subset of parameters at each iteration.
  - Quick check question: How does BCD differ from standard gradient descent in terms of parameter updates?

- **Concept: Momentum in Optimization**
  - Why needed here: MoFO uses momentum magnitudes to select which parameters to update, leveraging the stability benefits of momentum accumulation.
  - Quick check question: What is the difference between using gradient and momentum for parameter selection in optimization?

- **Concept: Catastrophic Forgetting**
  - Why needed here: The primary motivation for MoFO is to mitigate catastrophic forgetting during LLM fine-tuning.
  - Quick check question: How does catastrophic forgetting manifest in LLM fine-tuning, and why is it a problem?

## Architecture Onboarding

- **Component map**: MoFO modifies Adam by adding momentum filtering to select parameters for update. Parameters are partitioned into blocks, with filtering applied to each block individually.

- **Critical path**: 1) Compute gradients and momentum for all parameters, 2) Apply momentum filter to select top-α parameters in each block, 3) Update only selected parameters, 4) Repeat until convergence.

- **Design tradeoffs**: 
  - MoFO vs Full-parameter updates: MoFO restricts parameter movement to mitigate forgetting but may slow convergence.
  - Momentum vs Gradient filtering: Momentum provides stability but may miss important gradient directions.
  - Fixed vs Adaptive α: Fixed α simplifies implementation but may not be optimal for all tasks.

- **Failure signatures**: 
  - Poor fine-tuning performance: May indicate the momentum filter is too restrictive or α is set too low.
  - Significant forgetting: May indicate the momentum filter is not effectively restricting parameter movement.
  - Slow convergence: May indicate the momentum filter is excluding important parameters for task adaptation.

- **First 3 experiments**:
  1. Compare MoFO with Adam on a simple fine-tuning task, measuring both fine-tuning performance and parameter distance to the pre-trained model.
  2. Vary the update fraction α (e.g., 5%, 10%, 15%, 20%) and observe its effect on fine-tuning performance and forgetting.
  3. Test MoFO on a task known to cause significant forgetting (e.g., fine-tuning on a domain-specific dataset) and measure forgetting using held-out pre-training data or general capability benchmarks.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the momentum-filtering threshold α affect the trade-off between fine-tuning performance and forgetting mitigation in different types of tasks (e.g., mathematical reasoning vs. commonsense reasoning)?
- Basis in paper: [explicit] The paper conducts experiments with α values ranging from 5% to 80% and finds that 15% works best for most experiments.
- Why unresolved: The paper does not provide a detailed analysis of how the optimal α value varies across different task domains or model architectures.
- What evidence would resolve it: A systematic study comparing the performance of MoFO with different α values across a diverse set of tasks and model architectures.

### Open Question 2
- Question: Can the momentum filtering mechanism in MoFO be extended to other optimizers beyond Adam and Lion, and what would be the theoretical implications of such extensions?
- Basis in paper: [explicit] The paper mentions that the choice of β1 and β2 in Assumption 1 aligns with that used in analyzing full-batch Adam, and the use of a diminishing learning rate is crucial for stability.
- Why unresolved: The paper focuses on Adam and Lion optimizers, but does not explore the potential of applying momentum filtering to other optimizers like SGD or RMSProp.
- What evidence would resolve it: Empirical studies comparing the performance of MoFO with different optimizers and theoretical analysis of the convergence properties of these extensions.

### Open Question 3
- Question: How does the parameter partitioning strategy in MoFO affect its performance, and can alternative partitioning schemes lead to better results?
- Basis in paper: [explicit] The paper uses the default PyTorch partitioning scheme and also explores an alternative head-level partitioning strategy, finding similar performance.
- Why unresolved: The paper does not thoroughly investigate the impact of different partitioning strategies on MoFO's performance or explore more sophisticated partitioning methods.
- What evidence would resolve it: A comprehensive comparison of MoFO's performance with various partitioning strategies, including those based on the Hessian matrix structure or other model-specific characteristics.

## Limitations
- Theoretical analysis relies on assumptions about Lipschitz continuity and gradient boundedness that may not hold for LLMs
- Evaluation scope is relatively narrow, covering only 3-4 model sizes and a handful of fine-tuning scenarios
- Claims about preserving "general capabilities" are tested primarily through MMLU and a few other benchmarks, which may not comprehensively capture all aspects of pre-trained knowledge
- Effectiveness depends heavily on the choice of update fraction α, but sensitivity analysis is limited

## Confidence

**High Confidence**: The core mechanism of MoFO (momentum-based parameter filtering) is clearly described and the theoretical convergence rate claim (matching Adam at O(logT/√T)) is mathematically specified in the appendix. The experimental methodology for measuring forgetting (comparing fine-tuning performance vs general capability preservation) is sound and reproducible.

**Medium Confidence**: Claims about MoFO's effectiveness across different fine-tuning scenarios (continual learning, LoRA combination) are supported by experiments but haven't been tested on a wide variety of tasks or model architectures. The assertion that momentum filtering is superior to gradient filtering is based on ablation studies with limited parameter sweeps.

**Low Confidence**: The paper's claims about MoFO's advantages over other forgetting mitigation techniques (like L1/L2 regularization) in terms of convergence speed and final performance are not thoroughly validated, particularly for smaller models or when computational resources are constrained.

## Next Checks

1. **Extended Task Diversity**: Validate MoFO's effectiveness on a broader range of fine-tuning tasks including multilingual tasks, code generation, and reasoning tasks not covered in the current experiments. This would test whether the momentum filtering mechanism generalizes beyond mathematical and biomedical domains.

2. **Ablation on Update Fraction**: Conduct a comprehensive sensitivity analysis of the update fraction α parameter across the full range from 5% to 50% on multiple tasks. This would clarify whether the 15% value is optimal or task-dependent, and identify potential failure modes when α is set too high or too low.

3. **Computational Overhead Analysis**: Measure the actual wall-clock time and memory overhead of MoFO compared to standard fine-tuning methods, particularly when using smaller batch sizes or gradient accumulation. This would validate whether the forgetting benefits justify any additional computational costs in resource-constrained settings.