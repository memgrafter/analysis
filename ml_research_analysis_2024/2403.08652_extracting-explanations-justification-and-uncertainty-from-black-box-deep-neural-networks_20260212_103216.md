---
ver: rpa2
title: Extracting Explanations, Justification, and Uncertainty from Black-Box Deep
  Neural Networks
arxiv_id: '2403.08652'
source_url: https://arxiv.org/abs/2403.08652
tags:
- points
- inducing
- gaussian
- data
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a novel Bayesian approach using sparse Gaussian
  processes to extract explanations, justifications, and uncertainty estimates from
  pre-trained deep neural networks. The method replaces the computationally expensive
  support neighborhoods of prior work with a sparse Gaussian process, reducing the
  memory and computation requirements from using all training data to only a small
  set of inducing points.
---

# Extracting Explanations, Justification, and Uncertainty from Black-Box Deep Neural Networks

## Quick Facts
- arXiv ID: 2403.08652
- Source URL: https://arxiv.org/abs/2403.08652
- Authors: Paul Ardis; Arjuna Flenner
- Reference count: 16
- This paper proposes a novel Bayesian approach using sparse Gaussian processes to extract explanations, justifications, and uncertainty estimates from pre-trained deep neural networks.

## Executive Summary
This paper presents a novel Bayesian approach using sparse Gaussian processes (SGPs) to extract explanations, justifications, and uncertainty estimates from pre-trained deep neural networks without retraining. The method replaces computationally expensive support neighborhoods with a sparse Gaussian process, reducing memory and computation requirements from using all training data to only a small set of inducing points. This enables application to large datasets and edge devices while providing example-based justifications and uncertainty estimates based on local exemplar density and coherence. The approach is validated on CIFAR-10, showing it can significantly improve interpretability and reliability of deep neural networks while reducing inference time by two orders of magnitude compared to prior state-of-the-art methods.

## Method Summary
The method extracts explanations, justifications, and uncertainty estimates from pre-trained deep neural networks using sparse Gaussian processes. It takes intermediate layer activations (embeddings) from a pre-trained DNN as input, applies a sparse Gaussian process with inducing points to approximate the full GP posterior, and computes covariance-adjusted distances between test samples and inducing points. The method estimates epistemic uncertainty by counting inducing points within a thresholded neighborhood around test samples, with samples having insufficient inducing points labeled as uncertain. It provides example-based justifications by identifying the most relevant inducing points for each prediction. The approach is evaluated on the CIFAR-10 dataset, demonstrating significant improvements in computational efficiency while maintaining label accuracy and providing meaningful uncertainty estimates.

## Key Results
- Reduces inference time by two orders of magnitude compared to prior state-of-the-art methods
- Maintains label accuracy while providing uncertainty estimates and example-based justifications
- Enables application to large datasets and edge devices by reducing memory requirements from n to m inducing points

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Sparse Gaussian Processes (SGPs) enable scalable uncertainty quantification and justification extraction from large datasets without retraining the DNN.
- Mechanism: SGPs approximate the full Gaussian Process (GP) posterior using a small set of inducing points, reducing the computational complexity from O(n³) to O(nm² + m³), where n is the number of training samples and m is the number of inducing points.
- Core assumption: The inducing points are well-distributed across the embedding space and capture the essential structure of the full dataset.
- Evidence anchors:
  - [abstract]: "replaces the computationally expensive support neighborhoods of prior work with a sparse Gaussian process, reducing the memory and computation requirements from using all training data to only a small set of inducing points."
  - [section]: "Our approach replaces the support neighborhoods of Virani et al. with a SGP; thus, reducing the computational cost and memory footprint from n data points to m inducing points."
  - [corpus]: Found 25 related papers. Average neighbor FMR=0.558. Top related titles: QUCE: The Minimisation and Quantification of Path-Based Uncertainty for Generative Counterfactual Explanations.

### Mechanism 2
- Claim: The SGP's kernel function enables computation of distances that reflect both geometric and probabilistic similarity between test samples and inducing points.
- Mechanism: The covariance-adjusted distance Dcov combines Euclidean distance with a kernel-based probability term to create a metric that reflects both spatial proximity and likelihood of shared labels.
- Core assumption: The SGP's kernel function captures meaningful correlations between samples in the embedding space.
- Evidence anchors:
  - [section]: "We estimate the distance to each inducing point, the covariance and posterior likelihood of generation corresponding to the embedding around that inducing point, and a covariance-adjusted distance: Dcov = Drm + λPrm"
  - [corpus]: Found 25 related papers. Average neighbor FMR=0.558. Top related titles: Relationship between Uncertainty in DNNs and Adversarial Attacks.

### Mechanism 3
- Claim: Epistemic uncertainty can be estimated by counting the number of inducing points within a thresholded neighborhood around a test sample.
- Mechanism: For each test sample, count inducing points within distance threshold ε. If count is below threshold, label the sample as uncertain.
- Core assumption: Sufficient inducing points within a neighborhood indicates reliable evidence for the DNN's prediction.
- Evidence anchors:
  - [section]: "With ε as a threshold parameter, for each new sample in Xr counting the number of inducing points such that Drm < ε or Dcov < ε provide a means to define ε-neighborhoods and support sets."
  - [corpus]: Found 25 related papers. Average neighbor FMR=0.558. Top related titles: A Survey on Uncertainty Quantification Methods for Deep Learning.

## Foundational Learning

- Concept: Gaussian Process regression and its computational complexity
  - Why needed here: Understanding why GPs are computationally expensive (O(n³)) and how SGPs overcome this limitation is fundamental to understanding the method.
  - Quick check question: What is the computational complexity of inverting a matrix of size n×n, and why does this make GPs infeasible for large datasets?

- Concept: Inducing points and variational inference in SGPs
  - Why needed here: The method relies on finding good inducing points to approximate the GP posterior; understanding how these are determined is crucial.
  - Quick check question: How are inducing points typically selected in sparse variational GPs, and what is the trade-off between their number and approximation quality?

- Concept: Epistemic vs aleatoric uncertainty
  - Why needed here: The method specifically addresses epistemic uncertainty (lack of knowledge) rather than inherent randomness in the data.
  - Quick check question: What is the difference between epistemic uncertainty (lack of knowledge) and aleatoric uncertainty (inherent randomness), and which type does this method primarily address?

## Architecture Onboarding

- Component map: Pre-trained DNN -> Embedding extraction -> Sparse GP module -> Distance computation -> Uncertainty estimation -> Justification extraction
- Critical path:
  1. Obtain DNN embeddings for training data
  2. Select inducing points using variational methods
  3. For each test sample, compute distances to inducing points
  4. Count inducing points within threshold
  5. If count < threshold, mark as uncertain
  6. Return predictions with uncertainty estimates and justifications

- Design tradeoffs:
  - Number of inducing points (m) vs. accuracy: More inducing points improve approximation but increase computation
  - Threshold value (ε) vs. sensitivity: Lower thresholds are more conservative but may reject too many samples
  - Choice of kernel function vs. computational efficiency: Different kernels capture different types of relationships

- Failure signatures:
  - Poor accuracy despite low uncertainty: Inducing points not capturing relevant patterns
  - High uncertainty across all samples: Threshold too high or inducing points too sparse
  - High computation time: Number of inducing points too large or inefficient kernel implementation

- First 3 experiments:
  1. Validate that increasing inducing points improves approximation quality on a small dataset
  2. Test different kernel functions to find optimal balance of accuracy and speed
  3. Evaluate sensitivity of uncertainty estimates to threshold parameter ε

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal number of inducing points for sparse Gaussian processes in different applications?
- Basis in paper: [explicit] The paper mentions that the performance plateaus when the number of inducing points exceeds 2500 for the CIFAR-10 dataset, but it doesn't explore the optimal number for other datasets or applications.
- Why unresolved: The paper only tested the approach on the CIFAR-10 dataset, and the optimal number of inducing points may vary depending on the dataset and application.
- What evidence would resolve it: Further experiments on different datasets and applications would provide insights into the optimal number of inducing points for sparse Gaussian processes.

### Open Question 2
- Question: How does the performance of the approach compare to other explainable AI methods?
- Basis in paper: [inferred] The paper mentions that there are other approaches to explainability, such as gradient-based methods and text-based explanations, but it doesn't compare the performance of the proposed approach to these methods.
- Why unresolved: The paper only focuses on the performance of the proposed approach and doesn't provide a comparison with other methods.
- What evidence would resolve it: A comprehensive comparison with other explainable AI methods would provide insights into the strengths and weaknesses of the proposed approach.

### Open Question 3
- Question: How does the approach perform on out-of-distribution data?
- Basis in paper: [explicit] The paper mentions that the approach can be applied to anomaly detection and out-of-distribution detection tasks, but it doesn't provide any results on how well it performs on such data.
- Why unresolved: The paper only tests the approach on the CIFAR-10 dataset, which may not contain out-of-distribution data.
- What evidence would resolve it: Testing the approach on datasets with out-of-distribution data would provide insights into its performance on such data.

## Limitations

- The method's performance on datasets beyond CIFAR-10 is unverified, limiting generalizability claims
- The selection of inducing points and kernel hyperparameters significantly impacts performance but lacks clear optimization guidelines
- Claims about interpretability and usefulness of extracted explanations for end-users lack empirical validation

## Confidence

- **High Confidence**: The computational complexity reduction from O(n³) to O(nm² + m³) is mathematically sound and well-established in the sparse GP literature. The two-order-of-magnitude inference time improvement is a direct consequence of this reduction.
- **Medium Confidence**: The claim that the method works "without retraining the DNN" is valid, but the quality of explanations and uncertainty estimates depends heavily on the quality of embeddings from intermediate layers, which is not thoroughly examined.
- **Low Confidence**: Claims about the interpretability and usefulness of the extracted explanations for end-users lack empirical validation beyond accuracy metrics.

## Next Checks

1. Evaluate the method on diverse datasets (e.g., ImageNet, medical imaging datasets) to assess generalizability and identify domain-specific limitations.
2. Conduct ablation studies varying the number of inducing points and kernel hyperparameters to quantify their impact on accuracy, uncertainty estimates, and computational efficiency.
3. Design user studies to empirically validate whether the extracted explanations and uncertainty estimates actually improve human understanding and trust in DNN predictions.