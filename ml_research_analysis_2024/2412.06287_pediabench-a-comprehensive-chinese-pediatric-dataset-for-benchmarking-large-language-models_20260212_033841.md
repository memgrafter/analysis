---
ver: rpa2
title: 'PediaBench: A Comprehensive Chinese Pediatric Dataset for Benchmarking Large
  Language Models'
arxiv_id: '2412.06287'
source_url: https://arxiv.org/abs/2412.06287
tags:
- questions
- question
- llms
- answer
- medical
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces PediaBench, the first comprehensive Chinese
  pediatric dataset for evaluating large language models (LLMs) on medical question-answering
  tasks. The dataset includes 4,117 objective and 1,632 subjective questions across
  12 pediatric disease groups, covering true/false, multiple-choice, pairing, essay/short-answer,
  and case analysis question types.
---

# PediaBench: A Comprehensive Chinese Pediatric Dataset for Benchmarking Large Language Models

## Quick Facts
- arXiv ID: 2412.06287
- Source URL: https://arxiv.org/abs/2412.06287
- Reference count: 40
- Primary result: Most LLMs struggle with pediatric medical knowledge, with only a few commercial models achieving passing scores

## Executive Summary
This paper introduces PediaBench, the first comprehensive Chinese pediatric dataset designed specifically for evaluating large language models on medical question-answering tasks. The dataset contains 5,749 questions across 12 pediatric disease groups, including both objective questions (true/false, multiple-choice, pairing) and subjective questions (essay/short-answer, case analysis). Extensive experiments with 20 open-source and commercial LLMs reveal significant limitations in current models' ability to handle pediatric medical knowledge, with most models performing poorly on both knowledge retrieval and reasoning tasks.

## Method Summary
PediaBench is constructed through a multi-stage process involving question collection from textbooks and exams, disease group classification using GLM-4 with manual correction, and difficulty assessment based on model performance. The evaluation uses standardized prompts for different question types and employs GPT-4o as an automated examiner for subjective questions. A weighted scoring system accounts for both question type and difficulty level, with higher weights assigned to more challenging questions and subjective question types that test reasoning capabilities.

## Key Results
- Most evaluated LLMs achieved scores below passing thresholds on pediatric medical question-answering
- Commercial models generally outperformed open-source models, but even top performers showed significant knowledge gaps
- Case analysis questions proved particularly challenging, exposing limitations in models' reasoning and clinical integration capabilities
- Objective questions were generally answered more accurately than subjective questions requiring generation and reasoning

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Domain-specific medical benchmarks outperform general benchmarks in diagnosing LLM medical competence
- Mechanism: Pediatric-specific datasets expose domain knowledge gaps that general medical benchmarks overlook
- Core assumption: Pediatric medicine requires specialized knowledge distinct from adult medicine
- Evidence anchors: "Since pediatrics often involves the manifestations and treatments of diseases that differ from those of adults, LLMs with common medical knowledge might not perform equally well on pediatric QA"

### Mechanism 2
- Claim: Mixed question types (objective and subjective) provide comprehensive assessment of LLM capabilities
- Mechanism: Objective questions test knowledge retrieval while subjective questions evaluate generation and reasoning
- Core assumption: Knowledge retrieval and generation require different LLM capabilities
- Evidence anchors: "most of them contain only objective questions... they cannot assess the capacity of LLMs to generate medical texts"

### Mechanism 3
- Claim: Difficulty-weighted scoring reveals model performance nuances better than raw accuracy
- Mechanism: Assigning higher weights to difficult questions amplifies performance differences
- Core assumption: Question difficulty correlates with knowledge depth required
- Evidence anchors: "We calculate the difficulty coefficient Dci of each question i based on the accuracy of all LLMs' answers"

## Foundational Learning

- Concept: Medical domain knowledge specialization
  - Why needed here: Pediatric medicine differs fundamentally from adult medicine in disease presentation and treatment
  - Quick check question: Can you identify three pediatric-specific conditions not found in adult medicine?

- Concept: Question type differentiation
  - Why needed here: Different question types test different LLM capabilities (knowledge retrieval vs. reasoning vs. generation)
  - Quick check question: Which question type would best test an LLM's ability to integrate multiple clinical findings?

- Concept: Difficulty-based evaluation methodology
  - Why needed here: Raw accuracy can mask meaningful performance differences across knowledge depth
  - Quick check question: How would you weight a question that only 10% of models answer correctly versus one 90% answer correctly?

## Architecture Onboarding

- Component map: Question collection pipeline (sources → processing → classification) → Difficulty assessment engine (model performance → coefficient calculation) → Scoring system (difficulty weights + question type weights → total score) → LLM evaluation interface (prompts → responses → automated scoring)

- Critical path: Question collection → Difficulty assessment → LLM evaluation → Scoring calculation

- Design tradeoffs:
  - Domain specificity vs. general applicability
  - Automated vs. human evaluation for subjective questions
  - Question difficulty estimation vs. question quality

- Failure signatures:
  - Low correlation between human and automated scoring
  - Inconsistent difficulty coefficient calculations
  - Poor instruction-following across multiple models

- First 3 experiments:
  1. Test automated scoring correlation with human evaluations on 100 sample questions
  2. Evaluate difficulty coefficient stability across different model populations
  3. Assess instruction-following consistency across 5 open-source models with varying parameter counts

## Open Questions the Paper Calls Out

1. How does PediaBench's pediatric disease group classification accuracy compare when using different LLMs (e.g., GLM-4 vs. GPT-4o) for the annotation process?

2. What is the correlation between LLM performance on PediaBench and their general language understanding capabilities (e.g., scores on C-Eval or SuperCLUE)?

3. How do different prompting strategies (e.g., Chain-of-Thought vs. direct prompting) affect LLM performance on PediaBench's case analysis questions?

## Limitations

- Domain Coverage Constraints: The dataset focuses exclusively on 12 pediatric disease groups, potentially missing important pediatric subspecialties
- Automated Scoring Reliability: Use of GPT-4o as automated examiner introduces potential bias and reliability concerns
- Sample Size Imbalance: Significantly more objective questions than subjective questions may skew evaluation toward knowledge retrieval

## Confidence

- High Confidence: Dataset construction methodology and question type classification are well-documented and follow standard practices
- Medium Confidence: Experimental results showing LLM limitations are supported by data but require further validation for clinical applications
- Low Confidence: Claims about superiority of pediatric-specific benchmarks over general medical benchmarks lack direct comparative evidence

## Next Checks

1. Conduct human evaluation of 100 randomly selected subjective answers to establish ground truth scores and calculate correlation coefficients between GPT-4o automated scoring and human evaluations

2. Recompute difficulty coefficients using different subsets of models to assess robustness across model populations

3. Test whether models performing well on PediaBench also show superior performance on general medical benchmarks to establish unique diagnostic value of pediatric specialization