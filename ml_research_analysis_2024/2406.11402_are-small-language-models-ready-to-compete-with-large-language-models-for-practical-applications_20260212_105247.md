---
ver: rpa2
title: Are Small Language Models Ready to Compete with Large Language Models for Practical
  Applications?
arxiv_id: '2406.11402'
source_url: https://arxiv.org/abs/2406.11402
tags:
- reasoning
- task
- examples
- definition
- types
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a three-tier evaluation framework to analyze
  small, open language models across task types, application domains, and reasoning
  types using Super-Natural Instructions. The authors compare 10 models (1.7B-11B
  parameters) with eight prompt styles and find that appropriate selection can outperform
  or compete with state-of-the-art LLMs like GPT-4o, GPT-4o-mini, Gemini-1.5-Pro,
  and DeepSeek-v2.
---

# Are Small Language Models Ready to Compete with Large Language Models for Practical Applications?

## Quick Facts
- **arXiv ID**: 2406.11402
- **Source URL**: https://arxiv.org/abs/2406.11402
- **Authors**: Neelabh Sinha; Vinija Jain; Aman Chadha
- **Reference count**: 40
- **Primary result**: Small open LMs (1.7B-11B parameters) can compete with or outperform SOTA LLMs when appropriately selected

## Executive Summary
This paper proposes a three-tier evaluation framework to analyze small, open language models across task types, application domains, and reasoning types using Super-Natural Instructions. The authors compare 10 models (1.7B-11B parameters) with eight prompt styles and find that appropriate selection can outperform or compete with state-of-the-art LLMs like GPT-4o, GPT-4o-mini, Gemini-1.5-Pro, and DeepSeek-v2. The study demonstrates that models are robust to paraphrased definitions and provides guidance on selecting optimal prompt styles. Overall, the results show that small, open LMs can effectively replace larger, proprietary models in practical applications when chosen appropriately.

## Method Summary
The study evaluates 10 small open language models (5 pre-trained, 5 instruction-tuned) on the Super-Natural Instructions dataset using 8 prompt styles. Models range from 1.7B to 11B parameters and are tested on 119 tasks across 12 task types, 36 domains, and 18 reasoning types. Semantic correctness is measured using BERTScore recall with roberta-large. The evaluation framework provides hierarchical analysis at task type, domain, and reasoning type levels, comparing performance against SOTA LLMs like GPT-4o and GPT-4o-mini.

## Key Results
- Instruction-tuned models like Mistral-7B-I achieve BERTScore recall of 93.76, matching GPT-4o performance
- Smaller models like Gemma-2B and SmolLM-1.7B-I show strong performance in specific categories
- Appropriate model selection can lead to outperforming SOTA LLMs in certain task categories
- Models demonstrate robustness to paraphrased definitions of tasks

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Three-tier evaluation framework reveals performance patterns that single-dimension evaluations miss.
- **Mechanism**: By hierarchically grouping 12 task types, 36 domains, and 18 reasoning types, the framework captures nuanced capabilities and limitations of small models that aggregate metrics obscure.
- **Core assumption**: Semantic correctness (BERTScore recall) correlates with practical usability in real-world applications.
- **Evidence anchors**: "measuring semantic correctness of outputs across three practical aspects: task types, application domains, and reasoning types" and "Our intention with this is to provide a structure to this study and cover a broad spectrum of entities."
- **Break condition**: If semantic correctness doesn't correlate with downstream task performance, the framework loses practical value.

### Mechanism 2
- **Claim**: Instruction-tuned models outperform pre-trained models across all evaluated dimensions when appropriately selected.
- **Mechanism**: Fine-tuning on conversational datasets enables direct use with chat-style prompts, eliminating need for complex prompt engineering while maintaining competitive performance.
- **Core assumption**: IT models retain general knowledge while gaining instruction-following capability.
- **Evidence anchors**: "instruction-tuned (IT) models will suit out-of-the-box usage on chat-style human-like instructions" and "Mistral-7B-I being the best in all domains" with BERTScore recall of 93.76 matching GPT-4o.
- **Break condition**: If IT models lose general knowledge or become brittle to prompt variations.

### Mechanism 3
- **Claim**: Appropriate model selection can make small LMs compete with or outperform SOTA LLMs despite size differences.
- **Mechanism**: Matching model strengths to specific task requirements exploits architectural and training differences rather than relying on scale alone.
- **Core assumption**: Performance differences between small and large models are not solely due to parameter count.
- **Evidence anchors**: "appropriate selection of open, small LMs can lead to outperforming SOTA LLMs like GPT-4o-mini, Gemini-1.5-Pro, and competing with GPT-4o" with gaps of only 1.44-8.12% compared to GPT-4o-mini in specific categories.
- **Break condition**: If model performance scales linearly with parameter count, selection becomes irrelevant.

## Foundational Learning

- **Concept**: BERTScore recall measures semantic similarity between generated and reference outputs using contextual embeddings.
  - Why needed here: Provides nuanced evaluation beyond exact string matching, capturing meaning preservation.
  - Quick check question: If a model generates "The quick brown fox jumps over the lazy dog" versus reference "A fast brown fox leaps over a sluggish dog", will BERTScore capture semantic equivalence?

- **Concept**: Instruction tuning (IT) vs pre-training: IT models learn to follow natural language instructions while pre-trained models learn next-token prediction.
  - Why needed here: Explains why IT models perform better with chat-style prompts without additional engineering.
  - Quick check question: Can a pre-trained model be used effectively with zero examples if given clear task instructions?

- **Concept**: In-context learning: Models learn from examples provided in the prompt without parameter updates.
  - Why needed here: Determines optimal number of examples to include for different task types and models.
  - Quick check question: Does adding more examples always improve performance, or can it sometimes confuse the model?

## Architecture Onboarding

- **Component map**: Dataset (Super-Natural Instructions) → Prompt generator (8 styles) → Model inference → BERTScore evaluation → Visualization/analysis
- **Critical path**: Prompt generation → Model inference → Evaluation metric computation → Result aggregation
- **Design tradeoffs**: Comprehensive evaluation vs. computational cost; detailed prompt analysis vs. generalizability; semantic correctness vs. other quality dimensions
- **Failure signatures**: Poor correlation between models (indicating performance differences), high variance across prompt styles (indicating sensitivity), large gaps between pre-trained and IT models (indicating adaptation needs)
- **First 3 experiments**:
  1. Run all 8 prompt styles on one task type with one model to verify prompt style impact.
  2. Compare pre-trained vs IT versions of the same model on identical tasks.
  3. Test model performance with paraphrased vs original task definitions to verify robustness.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: How do small, open LMs perform on domain-specific tasks outside the categories covered in the evaluation framework?
- **Basis in paper**: [explicit] The paper acknowledges that there are more entities not included in the framework and suggests leveraging the performance of the groups or choosing the nearest entity to estimate performance.
- **Why unresolved**: The evaluation framework is limited to 12 task types, 36 domains, and 18 reasoning types. Many real-world applications may involve domains not covered in the framework, making it difficult to generalize the findings to all possible use cases.
- **What evidence would resolve it**: Additional experiments evaluating small, open LMs on a broader range of domain-specific tasks, including those not covered in the current framework, would provide insights into their performance in real-world applications.

### Open Question 2
- **Question**: How do small, open LMs perform when fine-tuned or adapted for specific tasks compared to their zero-shot performance?
- **Basis in paper**: [inferred] The paper mentions that pre-trained models can be adapted or aligned further and that the performance of pre-trained models can be taken as a measure of their knowledge of different use-cases. However, it does not provide direct comparisons between zero-shot and fine-tuned performance.
- **Why unresolved**: The paper focuses on evaluating the zero-shot performance of small, open LMs. Fine-tuning or adapting these models for specific tasks may significantly improve their performance, but the extent of this improvement is unknown.
- **What evidence would resolve it**: Experiments comparing the zero-shot performance of small, open LMs with their performance after fine-tuning or adaptation for specific tasks would provide insights into the potential benefits of these techniques.

### Open Question 3
- **Question**: How do small, open LMs perform in multilingual settings compared to their performance in English?
- **Basis in paper**: [explicit] The paper mentions that most LMs are optimized for English and that the experimental dataset consists of English input and output. It does not provide any information on the performance of these models in multilingual settings.
- **Why unresolved**: The paper focuses on evaluating the performance of small, open LMs in English. Many real-world applications require multilingual capabilities, making it important to understand how these models perform in languages other than English.
- **What evidence would resolve it**: Experiments evaluating the performance of small, open LMs on multilingual tasks and comparing their performance across different languages would provide insights into their multilingual capabilities.

## Limitations
- Limited scope of evaluation metrics relying solely on BERTScore recall
- Potential overfitting to Super-Natural Instructions dataset structure
- Hardware and implementation variability affecting reproducibility

## Confidence
- **High confidence**: Instruction-tuned models outperform pre-trained models with chat-style prompts
- **Medium confidence**: Small models can compete with SOTA LLMs in specific categories
- **Low confidence**: Generalizability of the three-tier framework without additional validation

## Next Checks
1. **Benchmark extension**: Validate the three-tier framework performance patterns on at least two additional instruction datasets (e.g., FLAN, Anthropic's Helpful and Harmless) to test generalizability.
2. **Multi-dimensional evaluation**: Supplement BERTScore with additional metrics including factual consistency (e.g., TruthfulQA scores), hallucination detection, and human evaluation for a subset of tasks.
3. **Real-world application testing**: Deploy top-performing small models from this study in at least one practical application scenario (e.g., customer service chatbot, code generation assistant) and measure performance against proprietary alternatives in operational conditions.