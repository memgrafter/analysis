---
ver: rpa2
title: 'Wings: Learning Multimodal LLMs without Text-only Forgetting'
arxiv_id: '2406.03496'
source_url: https://arxiv.org/abs/2406.03496
tags:
- arxiv
- visual
- wings
- text-only
- attention
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the issue of text-only forgetting in multimodal
  large language models (MLLMs), where fine-tuning on multimodal inputs impairs the
  model's ability to respond to text-only instructions. The authors propose WINGS,
  a novel MLLM architecture that introduces visual and textual learners in parallel
  at each layer's attention block.
---

# Wings: Learning Multimodal LLMs without Text-only Forgetting

## Quick Facts
- arXiv ID: 2406.03496
- Source URL: https://arxiv.org/abs/2406.03496
- Authors: Yi-Kai Zhang; Shiyin Lu; Yang Li; Yanqing Ma; Qing-Guo Chen; Zhao Xu; Weihua Luo; Kaifu Zhang; De-Chuan Zhan; Han-Jia Ye
- Reference count: 40
- Primary result: WINGS outperforms equally-scaled MLLMs in both text-only and visual question-answering tasks by introducing parallel visual and textual learners with LoRRA modules

## Executive Summary
This paper addresses the critical problem of text-only forgetting in multimodal large language models (MLLMs), where fine-tuning on multimodal inputs impairs the model's ability to respond to text-only instructions. The authors propose WINGS, a novel architecture that introduces visual and textual learners in parallel at each layer's attention block to compensate for attention shifts that cause text-only forgetting. By using Low-Rank Residual Attention (LoRRA) modules, WINGS maintains efficiency while providing dedicated attention pathways for both modalities.

## Method Summary
WINGS introduces parallel visual and textual learners using LoRRA modules in each layer's attention block. The model is trained in two stages: first, visual learners are fine-tuned to align image features with text; then textual learners are added with an attention-based router to blend outputs. This architecture compensates for attention shifts from pre-image to post-image text that cause text-only forgetting in standard MLLMs.

## Key Results
- WINGS outperforms equally-scaled MLLMs on both text-only and visual question-answering tasks
- Achieves state-of-the-art performance on the newly constructed Interleaved Image-Text (IIT) benchmark
- Maintains text-only capability while improving multimodal understanding

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Text-only forgetting in MLLMs is caused by attention shifts from pre-image to post-image text.
- Mechanism: When visual tokens are inserted into a text sequence, the attention distribution changes such that post-image text receives disproportionately less attention, disrupting continuity in textual processing.
- Core assumption: Attention weights reflect the model's focus, and consistent attention patterns before and after visual tokens are necessary for maintaining text-only capability.
- Evidence anchors:
  - [abstract] "Analyzing MLLM attention in multimodal instructions reveals that text-only forgetting is related to the attention shifts from pre-image to post-image text."
  - [section] "We find that when the MLLM forgets the text-only instructions, the LAWS of the textual sequence after the visual ones show a deviation from the initial trend of rising and then declining."

### Mechanism 2
- Claim: Introducing parallel visual and textual learners compensates for the attention shift.
- Mechanism: By adding separate attention modules for visual and textual processing that operate in parallel to the main attention, the model can maintain focus on both modalities without competition.
- Core assumption: Parallel learners can effectively redistribute attention weights without interfering with the primary transformer attention mechanism.
- Evidence anchors:
  - [abstract] "The complementary visual and textual learners, like 'wings' on either side, are connected in parallel within each layer's attention block."
  - [section] "The outputs of two learners from each layer are then weighted sum to the attention of the main branch."

### Mechanism 3
- Claim: Low-Rank Residual Attention (LoRRA) provides efficient implementation of learners.
- Mechanism: Using low-rank decomposition for attention weights reduces computational complexity while maintaining representational capacity.
- Core assumption: Low-rank approximations can capture the essential attention patterns needed for visual and textual processing.
- Evidence anchors:
  - [abstract] "We design the Low-Rank Residual Attention (LoRRA) to guarantee high efficiency for learners."
  - [section] "Following LoRA [45], LoRRA learners also employ random Gaussian initialization for Wa and sets Wb to zero."

## Foundational Learning

- Concept: Attention mechanisms in transformers
  - Why needed here: Understanding how attention weights shift when visual tokens are introduced is central to the text-only forgetting problem.
  - Quick check question: How does the attention distribution change when visual tokens are inserted into a text sequence?

- Concept: Catastrophic forgetting in continual learning
  - Why needed here: The paper addresses a specific instance of catastrophic forgetting where multimodal training impairs text-only capabilities.
  - Quick check question: What distinguishes catastrophic forgetting from normal learning in neural networks?

- Concept: Low-rank matrix factorization
  - Why needed here: LoRRA uses low-rank decomposition to implement efficient attention learners.
  - Quick check question: How does low-rank approximation reduce computational complexity in matrix operations?

## Architecture Onboarding

- Component map:
  - Main transformer branch: Original LLM attention and feed-forward network
  - Visual learner: Parallel LoRRA module that processes visual features
  - Textual learner: Parallel LoRRA module that processes textual features
  - Router: Attention-based mechanism that blends outputs from visual and textual learners

- Critical path: Visual features → Visual learner → Router → Main attention → Output
  - Textual features → Textual learner → Router → Main attention → Output

- Design tradeoffs:
  - Parallel learners vs. sequential: Parallel design maintains independence but increases parameter count
  - Low-rank vs. full-rank: Low-rank reduces computation but may limit expressiveness
  - Router complexity vs. performance: More sophisticated routing may improve blending but adds overhead

- Failure signatures:
  - Text-only performance degrades despite WINGS implementation
  - Visual understanding degrades when textual learners are added
  - Router consistently favors one modality over another
  - Training instability when integrating LoRRA modules

- First 3 experiments:
  1. Compare attention weight distributions before/after visual token insertion with and without WINGS
  2. Test text-only performance with only visual learners enabled vs. both learners enabled
  3. Measure computational overhead of LoRRA modules compared to standard attention

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal architecture and configuration for the Low-Rank Residual Attention (LoRRA) learners in WINGS to maximize performance across diverse tasks?
- Basis in paper: [explicit] The paper introduces LoRRA as an efficient module for the visual and textual learners, but mentions that its design is crucial for performance.
- Why unresolved: The paper does not provide a detailed analysis of different LoRRA configurations or their impact on performance. It only mentions that the matrix WQ, WK, WV, and WO is low-rank and obtained by the dot product of Wa ∈ Rd×d and Wb ∈ Rd×d.
- What evidence would resolve it: Experiments comparing different LoRRA configurations (e.g., different ranks, initialization strategies) and their impact on WINGS' performance across various benchmarks would provide insights into the optimal architecture.

### Open Question 2
- Question: How does WINGS' performance scale with increasing model size and complexity, and what are the limitations of its current implementation?
- Basis in paper: [inferred] The paper mentions that WINGS achieves state-of-the-art performance on various benchmarks, but does not discuss its scalability or limitations in detail.
- Why unresolved: The paper does not provide a comprehensive analysis of WINGS' performance as the model size and complexity increase. It also does not discuss potential limitations or challenges in scaling up the model.
- What evidence would resolve it: Experiments evaluating WINGS' performance on larger datasets and with more complex architectures, along with an analysis of its computational requirements and limitations, would provide insights into its scalability and potential areas for improvement.

### Open Question 3
- Question: How does WINGS compare to other methods for mitigating text-only forgetting in multimodal models, such as replay-based approaches or interleaved training?
- Basis in paper: [explicit] The paper mentions that existing approaches replay extensive text-only or interleaved training data to mitigate catastrophic forgetting in MLLMs, but does not provide a detailed comparison with WINGS.
- Why unresolved: The paper does not provide a comprehensive comparison of WINGS with other methods for addressing text-only forgetting, such as replay-based approaches or interleaved training. It only mentions that these methods have limitations in terms of computational overhead and data collection challenges.
- What evidence would resolve it: Experiments comparing WINGS with other methods for mitigating text-only forgetting, such as replay-based approaches or interleaved training, on the same benchmarks and with similar resources would provide insights into its relative effectiveness and advantages.

## Limitations

- The paper lacks detailed quantitative analysis of attention distribution changes before and after visual token insertion
- Specific implementation details of LoRRA modules and their impact on model capacity are not fully elaborated
- Critical hyperparameters for the two-stage training process are missing, making it difficult to assess scalability

## Confidence

- **High confidence**: The empirical observation that MLLMs degrade on text-only tasks after multimodal fine-tuning
- **Medium confidence**: The architectural design of WINGS with parallel learners and LoRRA modules
- **Low confidence**: The efficiency claims for LoRRA modules without detailed computational analysis

## Next Checks

1. Conduct ablation studies to isolate the contribution of each component (visual learners, textual learners, LoRRA, router) to overall performance, measuring both accuracy and computational overhead.

2. Perform attention visualization analysis comparing attention weight distributions before and after visual token insertion with and without WINGS, including statistical tests for significance.

3. Test the model's generalization to different types of visual content and text arrangements beyond the IIT benchmark to evaluate robustness of the text-only retention mechanism.