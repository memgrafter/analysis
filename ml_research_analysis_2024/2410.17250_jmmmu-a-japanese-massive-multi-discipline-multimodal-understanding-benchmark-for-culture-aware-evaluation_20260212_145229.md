---
ver: rpa2
title: 'JMMMU: A Japanese Massive Multi-discipline Multimodal Understanding Benchmark
  for Culture-aware Evaluation'
arxiv_id: '2410.17250'
source_url: https://arxiv.org/abs/2410.17250
tags:
- japanese
- image
- subjects
- arxiv
- llav
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces JMMMU, the first large-scale Japanese benchmark
  for evaluating large multimodal models (LMMs) on expert-level tasks within Japanese
  cultural contexts. JMMMU comprises two complementary subsets: culture-agnostic (CA)
  questions translated from MMMU, and culture-specific (CS) questions newly crafted
  to reflect Japanese culture.'
---

# JMMMU: A Japanese Massive Multi-discipline Multimodal Understanding Benchmark for Culture-aware Evaluation

## Quick Facts
- arXiv ID: 2410.17250
- Source URL: https://arxiv.org/abs/2410.17250
- Authors: Shota Onohara; Atsuyuki Miyai; Yuki Imajuku; Kazuki Egashira; Jeonghun Baek; Xiang Yue; Graham Neubig; Kiyoharu Aizawa
- Reference count: 23
- Primary result: First large-scale Japanese multimodal benchmark revealing significant cultural knowledge gaps in current LMMs

## Executive Summary
This paper introduces JMMMU, the first large-scale Japanese benchmark for evaluating large multimodal models (LMMs) on expert-level tasks within Japanese cultural contexts. JMMMU comprises two complementary subsets: culture-agnostic (CA) questions translated from MMMU, and culture-specific (CS) questions newly crafted to reflect Japanese culture. Evaluating 16 open-source and three proprietary LMMs, the study finds overall performance up to 58.6%, with significant drops in CA questions due to language variation and poor performance on CS questions indicating inadequate cultural understanding. Japanese LMMs perform better on CS tasks, highlighting the effectiveness of Japanese dataset fine-tuning. A notable finding is that proprietary models like GPT-4o and Claude 3.5 Sonnet show similar performance on CA questions but diverge significantly on CS questions, revealing that strong English performance does not guarantee Japanese cultural comprehension. This underscores the importance of culturally diverse benchmarks for truly inclusive LMM development.

## Method Summary
The JMMMU benchmark was created by first translating culture-agnostic subjects from the English MMMU benchmark into Japanese for the CA subset, then crafting new culture-specific subjects for the CS subset. The evaluation used 16 open-source and 3 proprietary LMMs with the LMMs-Eval framework, employing prompt-based evaluation with Japanese instructions and rule-based answer extraction. The dual-subset design allows isolation of language proficiency effects from cultural knowledge gaps.

## Key Results
- JMMMU achieves overall accuracy of 58.6% across 16 open-source LMMs
- Japanese LMMs outperform open-source models on CS tasks (up to 1.7% better), validating cultural fine-tuning
- Proprietary models GPT-4o and Claude 3.5 Sonnet show similar CA performance but diverge by 15.7% on CS subset
- "Lack of Knowledge" dominates error types at 53.8% for CS subset questions
- Models frequently fail to follow Japanese instructions, often generating reasoning instead of direct answers

## Why This Works (Mechanism)

### Mechanism 1
- Claim: JMMMU's dual-subset design isolates language proficiency from cultural understanding
- Mechanism: By comparing culture-agnostic (CA) subset (translated from MMMU) and culture-specific (CS) subset (newly created), the benchmark reveals whether performance gaps are due to language variation or cultural knowledge
- Core assumption: Translation preserves semantic equivalence while isolating language effects
- Evidence anchors:
  - [abstract] "Using the CA subset, we observe performance drop in many LMMs when evaluated in Japanese, which is purely attributable to language variation."
  - [section 3.1] "JMMMU features two complementary subsets: (i) culture-agnostic (CA) subset, where the culture-independent subjects (e.g., Math) are selected and translated into Japanese... (ii) culture-specific (CS) subset, comprising newly crafted subjects that reflect Japanese cultural context."
  - [corpus] Weak - no direct neighbor studies on dual-subset benchmarking methodology
- Break condition: If translation introduces cultural bias or semantic drift, the CA subset no longer isolates pure language effects

### Mechanism 2
- Claim: Japanese LMMs outperform open-source models on CS subset due to cultural fine-tuning
- Mechanism: Japanese LMMs (LLaV A CALM2, EvoVLM JP v2) are trained on Japanese datasets, incorporating cultural knowledge that improves performance on CS questions
- Core assumption: Training data distribution affects model's cultural knowledge representation
- Evidence anchors:
  - [abstract] "Japanese LMMs perform better on CS tasks, highlighting the effectiveness of Japanese dataset fine-tuning."
  - [section 4.2] "Japanese-made LMMs... face a minimal drop (up to 1.7%), which implies that incorporating the Japanese dataset successfully mitigates the performance gap between English and Japanese."
  - [corpus] Weak - no neighbor studies specifically measuring cultural fine-tuning impact
- Break condition: If Japanese LMMs' training data lacks sufficient cultural diversity or depth, performance advantage on CS subset disappears

### Mechanism 3
- Claim: Proprietary models show similar CA performance but diverge significantly on CS subset
- Mechanism: GPT-4o and Claude 3.5 Sonnet perform similarly on CA questions (same content, different language) but show 15.7% performance gap on CS questions, revealing superficial Japanese language understanding without cultural depth
- Core assumption: Models can achieve language proficiency without cultural understanding
- Evidence anchors:
  - [abstract] "We reveal a significant discrepancy among the state-of-the-art proprietary models. While they perform similarly on English benchmarks and even on culture-agnostic questions in Japanese, their performances are significantly different on CS subset."
  - [section 4.2] "GPT-4o outperforms Claude 3.5 Sonnet by a substantial 15.7%... This strongly indicates that a model's Japanese language skill and its understanding of Japanese culture should be separately discussed."
  - [corpus] Weak - no neighbor studies comparing proprietary model cultural understanding gaps
- Break condition: If proprietary models' training includes extensive cultural knowledge or if evaluation methodology is flawed

## Foundational Learning

- Concept: Cultural context in language models
  - Why needed here: Understanding how cultural knowledge affects language model performance in non-English contexts
  - Quick check question: Can a model achieve high language proficiency without cultural understanding? (Answer: Yes, based on the GPT-4o vs Claude 3.5 Sonnet comparison)

- Concept: Multimodal benchmark design
  - Why needed here: Understanding how to create benchmarks that isolate specific capabilities (language vs cultural knowledge)
  - Quick check question: What is the purpose of having both CA and CS subsets? (Answer: To separate language proficiency from cultural understanding effects)

- Concept: Translation quality assessment
  - Why needed here: Evaluating whether translations preserve semantic equivalence while isolating language effects
  - Quick check question: How can we verify that CA subset translations don't introduce cultural bias? (Answer: By comparing model performance on identical questions in different languages)

## Architecture Onboarding

- Component map: Benchmark creation → Model evaluation → Error analysis → Cultural knowledge assessment
- Critical path: Data curation (translation + creation) → Model inference → Answer parsing → Performance analysis
- Design tradeoffs: Translation vs. creation (CA subset) - preserves comparison ability but may introduce translation artifacts; creation (CS subset) - captures cultural nuance but loses direct comparison
- Failure signatures: Performance gaps in CA subset indicate language proficiency issues; poor CS performance indicates cultural knowledge gaps
- First 3 experiments:
  1. Evaluate baseline models on both CA and CS subsets to establish performance baselines
  2. Compare model performance on CA subset in English vs Japanese to measure language impact
  3. Analyze error patterns in CS subset to identify specific cultural knowledge gaps

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do performance gaps between Japanese and English in culture-agnostic tasks vary across different linguistic families or language distances from Japanese?
- Basis in paper: [inferred] The paper notes significant performance drops in Japanese versus English for culture-agnostic tasks, suggesting a language variation effect, but doesn't explore whether this varies by language family
- Why unresolved: The study focuses solely on Japanese, so cross-linguistic comparisons aren't possible within this dataset
- What evidence would resolve it: Comparative evaluations of JMMMU-like benchmarks in languages from different families (e.g., Korean, Hindi, Turkish) would reveal whether the performance gap correlates with linguistic distance from English

### Open Question 2
- Question: What specific aspects of Japanese cultural knowledge are most challenging for current multimodal models, and how does this differ from their weaknesses in other cultural contexts?
- Basis in paper: [explicit] The paper identifies "Lack of Knowledge" as the dominant error type (53.8%) in culture-specific subjects, with particularly poor performance on Japanese Heritage, but doesn't analyze which specific cultural domains are most problematic
- Why unresolved: The error analysis remains at a high level without domain-specific breakdowns of cultural knowledge gaps
- What evidence would resolve it: Fine-grained error categorization across different cultural domains (art, history, heritage) within JMMMU and similar benchmarks for other cultures would identify which cultural knowledge types models struggle with most

### Open Question 3
- Question: Does instruction-following ability in Japanese improve with scale, or is it fundamentally limited by training data composition?
- Basis in paper: [explicit] The paper finds that models frequently fail to follow instructions in Japanese, often generating reasoning processes despite being told to answer directly, and notes this leads to overestimation of performance
- Why unresolved: The study doesn't test whether larger models or models with different training data compositions show improved instruction-following in Japanese
- What evidence would resolve it: Comparative evaluation of instruction-following performance across model sizes and training datasets (Japanese-only vs. bilingual vs. English-dominant) would reveal whether scale or data composition drives improvement

## Limitations

- The benchmark's dual-subset design relies heavily on translation quality for the CA subset, with no quantitative validation of translation quality or cross-linguistic semantic drift
- The evaluation protocol uses rule-based answer extraction, which may introduce systematic biases depending on how models format their responses
- The small number of proprietary models tested (only GPT-4o and Claude 3.5 Sonnet) limits generalizability about the broader landscape of commercial LMMs' cultural understanding capabilities

## Confidence

**High Confidence**: The finding that Japanese LMMs perform better on CS tasks due to cultural fine-tuning. This is directly supported by the comparative performance data and the logical connection to dataset composition.

**Medium Confidence**: The claim that translation preserves semantic equivalence for CA subset isolation. While methodologically sound, lacks empirical validation of translation quality.

**Medium Confidence**: The conclusion that strong English performance doesn't guarantee Japanese cultural comprehension. Supported by the proprietary model comparison but limited by small sample size.

## Next Checks

1. **Translation Validation**: Conduct a human evaluation study where bilingual experts rate semantic equivalence between English MMMU questions and their Japanese translations to quantify potential translation artifacts.

2. **Protocol Robustness**: Test the rule-based answer extraction parser against diverse model output formats (including chain-of-thought reasoning) to identify systematic evaluation biases.

3. **Model Landscape Expansion**: Evaluate additional proprietary models (particularly Japanese-developed models like Rakuten's) to strengthen conclusions about the relationship between English proficiency and Japanese cultural understanding.