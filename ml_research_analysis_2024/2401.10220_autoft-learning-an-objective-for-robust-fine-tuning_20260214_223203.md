---
ver: rpa2
title: 'AutoFT: Learning an Objective for Robust Fine-Tuning'
arxiv_id: '2401.10220'
source_url: https://arxiv.org/abs/2401.10220
tags:
- fine-tuning
- auto
- learning
- distribution
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces AutoFT, a data-driven approach for robust
  fine-tuning of foundation models. The core idea is to optimize fine-tuning hyperparameters
  using a small out-of-distribution (OOD) validation set, rather than the standard
  in-distribution (ID) validation set.
---

# AutoFT: Learning an Objective for Robust Fine-Tuning

## Quick Facts
- arXiv ID: 2401.10220
- Source URL: https://arxiv.org/abs/2401.10220
- Authors: Caroline Choi; Yoonho Lee; Annie Chen; Allan Zhou; Aditi Raghunathan; Chelsea Finn
- Reference count: 40
- One-line primary result: AutoFT achieves new state-of-the-art performance on WILDS-iWildCam and FMoW benchmarks, outperforming previous best methods by 6.0% and 1.5% respectively.

## Executive Summary
AutoFT introduces a data-driven approach for robust fine-tuning of foundation models by optimizing hyperparameters using a small out-of-distribution (OOD) validation set instead of the standard in-distribution validation set. The method searches an expressive hyperparameter space including weight coefficients for multiple loss functions and regularizers, along with learning rate and weight decay values. Through bi-level optimization, AutoFT finds hyperparameters that maximize post-adaptation performance on the OOD validation set, significantly improving generalization to unseen OOD inputs across nine natural distribution shifts.

## Method Summary
AutoFT employs bi-level optimization to search for hyperparameters that maximize OOD validation performance. The approach expands the hyperparameter space to include weight coefficients for nine loss functions and regularizers, plus learning rate and weight decay values. Tree-structured Parzen Estimator (TPE) is used for hyperparameter optimization, while foundation models (typically CLIP) are fine-tuned using AdamW optimizer with cosine learning rate scheduler and early stopping on ID validation accuracy. The method requires minimal additional compute, with at most 5% more total compute compared to standard fine-tuning.

## Key Results
- AutoFT significantly improves generalization to unseen OOD inputs, outperforming existing robust fine-tuning methods across nine natural distribution shifts
- Achieves new state-of-the-art performance on WILDS-iWildCam and FMoW benchmarks, outperforming previous best methods by 6.0% and 1.5% respectively
- Requires minimal additional compute, with at most 5% more total compute compared to standard fine-tuning

## Why This Works (Mechanism)
AutoFT works by shifting the optimization objective from in-distribution performance to out-of-distribution performance during hyperparameter search. By using a small OOD validation set to guide hyperparameter selection, the method learns to prioritize features and representations that generalize better to distribution shifts. The expanded hyperparameter space allows for fine-grained control over multiple aspects of the fine-tuning process, including regularization strength and loss function weighting, which helps prevent overfitting to the training distribution while maintaining good performance on OOD data.

## Foundational Learning
- **Bi-level optimization**: Optimizing hyperparameters by considering how they affect the training process; needed to properly tune hyperparameters for OOD generalization
- **Tree-structured Parzen Estimator (TPE)**: A sequential model-based optimization algorithm for hyperparameter search; needed for efficient exploration of the large hyperparameter space
- **Out-of-distribution validation**: Using data that differs from training distribution to validate hyperparameters; needed to ensure robustness to real-world distribution shifts
- **Loss function weighting**: Combining multiple loss functions with learned coefficients; needed to balance different aspects of model performance
- **Regularization techniques**: Methods to prevent overfitting during fine-tuning; needed to maintain generalization capabilities

## Architecture Onboarding

**Component Map**: Foundation Model (CLIP) -> Bi-level Optimizer -> Hyperparameter Space -> TPE Sampler -> Fine-tuning Loop -> OOD Validation

**Critical Path**: TPE hyperparameter optimization -> fine-tuning with optimized hyperparameters -> evaluation on OOD test sets

**Design Tradeoffs**: 
- Larger hyperparameter space provides more flexibility but increases computational cost and risk of overfitting
- Smaller OOD validation sets reduce computational burden but may provide less reliable hyperparameter optimization
- More hyperparameter trials improve optimization quality but increase computational requirements

**Failure Signatures**: 
- Overfitting to OOD validation set if hyperparameter space is too expressive
- Poor OOD generalization if validation set doesn't represent test distributions
- Computational inefficiency from excessive hyperparameter trials

**3 First Experiments**:
1. Test AutoFT on a single dataset with different sizes of OOD validation sets (100, 500, 1000 examples)
2. Compare performance using different foundation models (CLIP vs ResNet) with the same AutoFT hyperparameters
3. Evaluate transfer learning by using hyperparameters optimized on one dataset to fine-tune on another dataset

## Open Questions the Paper Calls Out
### Open Question 1
- Question: How does the effectiveness of AutoFT hyperparameters transfer across different foundation model architectures beyond ViT-B/16 and ViT-L/14?
- Basis in paper: The paper mentions using hyperparameters learned on ViT-B/16 to fine-tune ViT-L/14@336px for WILDS-iWildCam and WILDS-FMoW, suggesting some transferability between architectures.
- Why unresolved: The experiments only demonstrate transfer between two specific ViT architectures, leaving open the question of how well AutoFT hyperparameters generalize to other foundation models like ResNets, ConvNeXt, or other vision transformers with different designs.
- What evidence would resolve it: Experiments testing AutoFT on multiple foundation model architectures (ResNet, ConvNeXt, Swin Transformer, etc.) using hyperparameters optimized for one architecture to fine-tune others on the same or different datasets.

### Open Question 2
- Question: What is the minimum size of the OOD validation set needed for AutoFT to maintain robust performance improvements?
- Basis in paper: The paper uses validation sets of various sizes (100 to 1000 examples) across different datasets but doesn't systematically study the effect of validation set size on performance.
- Why unresolved: The paper shows AutoFT works with small OOD validation sets but doesn't establish the relationship between validation set size and performance, nor does it identify the minimum effective size.
- What evidence would resolve it: Controlled experiments varying the OOD validation set size (e.g., 10, 50, 100, 500, 1000 examples) on multiple datasets to identify the point of diminishing returns and the minimum size needed for consistent improvements.

### Open Question 3
- Question: How does AutoFT perform on distribution shifts that are more extreme or out-of-distribution than the validation set used for hyperparameter optimization?
- Basis in paper: The paper assumes the OOD validation set serves as a proxy for various distribution shifts, but doesn't test performance on shifts more severe than those in the validation set.
- Why unresolved: All experiments use validation sets that are representative of the test distribution shifts, leaving unknown how well AutoFT generalizes to novel, more extreme distribution shifts not represented in the validation set.
- What evidence would resolve it: Experiments introducing progressively more severe distribution shifts (e.g., different corruption intensities, domain gaps, or synthetic shifts) to test the limits of AutoFT's generalization beyond the validation set distribution.

## Limitations
- Missing implementation details regarding exact CLIP model versions, hyperparameters, and text templates used for each dataset
- Optimal number of hyperparameter trials and inner-loop gradient steps may vary depending on dataset and computational resources
- Potential for overfitting to OOD validation set if hyperparameter space is too expressive

## Confidence
- High confidence in overall methodology and experimental results
- Medium confidence in specific implementation details due to missing hyperparameters and text templates
- Medium confidence in computational efficiency claims based on rough estimates provided

## Next Checks
1. Verify exact CLIP model versions, hyperparameters, and text templates used for each dataset by cross-referencing with provided code or contacting authors
2. Conduct experiments to determine optimal number of hyperparameter trials and inner-loop gradient steps for each dataset
3. Investigate potential for overfitting to OOD validation set by experimenting with different validation set sizes and compositions