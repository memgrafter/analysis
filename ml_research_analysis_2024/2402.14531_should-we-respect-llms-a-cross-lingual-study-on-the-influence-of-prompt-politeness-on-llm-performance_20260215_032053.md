---
ver: rpa2
title: Should We Respect LLMs? A Cross-Lingual Study on the Influence of Prompt Politeness
  on LLM Performance
arxiv_id: '2402.14531'
source_url: https://arxiv.org/abs/2402.14531
tags:
- politeness
- answer
- language
- bias
- level
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigates how the politeness of prompts affects the
  performance of large language models (LLMs) across English, Chinese, and Japanese.
  The researchers designed prompts with varying politeness levels and evaluated LLMs
  on summarization, language understanding benchmarks, and stereotypical bias detection
  tasks.
---

# Should We Respect LLMs? A Cross-Lingual Study on the Influence of Prompt Politeness on LLM Performance

## Quick Facts
- arXiv ID: 2402.14531
- Source URL: https://arxiv.org/abs/2402.14531
- Authors: Ziqi Yin; Hao Wang; Kaito Horio; Daisuke Kawahara; Satoshi Sekine
- Reference count: 40
- Key outcome: This study investigates how the politeness of prompts affects the performance of large language models (LLMs) across English, Chinese, and Japanese.

## Executive Summary
This study investigates how the politeness of prompts affects the performance of large language models (LLMs) across English, Chinese, and Japanese. The researchers designed prompts with varying politeness levels and evaluated LLMs on summarization, language understanding benchmarks, and stereotypical bias detection tasks. The study finds that impolite prompts often lead to poor performance, including increased bias, incorrect answers, or refusal to respond, while moderate politeness generally yields better results. However, the optimal level varies by language and model, with LLMs showing sensitivity to cultural nuances in politeness expressions, reflecting human social behavior.

## Method Summary
The study designed prompts with varying politeness levels for English, Chinese, and Japanese, then evaluated multiple LLMs (GPT-3.5, GPT-4, Llama2-70B, ChatGLM3, Swallow-70B) on three task types: summarization (using CNN/Dailymail and XL-Sum datasets), language understanding (MMLU, C-Eval, JMMLU benchmarks), and stereotypical bias detection (CrowS-Pairs, CHBias, Japanese gender bias dataset). Performance was measured using BERTScore and ROUGE-L for summarization, accuracy for language understanding, and Bias Index for bias detection. The researchers compared performance across politeness levels and languages to identify patterns in how prompt politeness affects LLM outputs.

## Key Results
- Impolite prompts often lead to poor performance, including increased bias, incorrect answers, or refusal to respond
- Moderate politeness generally yields better results, but the optimal level varies by language and model
- LLMs show sensitivity to cultural nuances in politeness expressions, reflecting human social behavior

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLM responses vary systematically with politeness levels in prompts due to training data reflecting human social norms.
- Mechanism: Prompts embedding politeness cues activate learned associations between respectful language and cooperative behavior, leading to improved task performance and reduced refusal.
- Core assumption: The LLM has been exposed to sufficient culturally varied examples of polite and impolite language during pretraining and alignment.
- Evidence anchors:
  - [abstract] "LLMs mirror human communication traits, suggesting they align with human cultural norms."
  - [section] "We hypothesize that impolite prompts may lead to a deterioration in model performance... overly polite language does not guarantee better outcomes."
  - [corpus] Related work on politeness and LLM tone shows measurable performance shifts across politeness levels.
- Break condition: If the training corpus lacks sufficient culturally varied examples of polite language, or if the model has not undergone fine-tuning that reinforces respectful communication norms.

### Mechanism 2
- Claim: The optimal politeness level is language- and culture-specific, reflecting the training corpus and cultural norms embedded in the model.
- Mechanism: Different languages encode politeness in distinct syntactic and lexical systems (e.g., Japanese honorifics vs. Chinese formality markers), so the model's sensitivity to politeness cues is calibrated differently per language.
- Core assumption: The pretraining corpus for each language includes culturally representative samples of polite language use, and the model has been fine-tuned on language-specific alignment data.
- Evidence anchors:
  - [abstract] "The best politeness level is different according to the language."
  - [section] "Due to the existence of a politeness system in the Japanese language, store staff almost always use honorific language when speaking to customers."
  - [corpus] The corpus includes related work on cross-cultural NLP and politeness systems.
- Break condition: If the pretraining or fine-tuning data does not adequately represent the cultural norms of a target language, or if the politeness system is not syntactically encoded in that language.

### Mechanism 3
- Claim: RLHF and SFT influence how strongly politeness cues affect LLM performance, with fine-tuned models showing moderated sensitivity.
- Mechanism: Supervised fine-tuning and reinforcement learning from human feedback tune the model to balance task performance with respectful interaction, reducing extreme sensitivity to politeness extremes.
- Core assumption: The model has undergone alignment fine-tuning, which shapes its response to politeness cues.
- Evidence anchors:
  - [section] "We consider the roles of Supervised Fine-Tuning (SFT) and Reinforcement Learning from Human Feedback (RLHF)."
  - [section] "In the MMLU tests, the base model demonstrates a positive correlation between scores and the politeness level... its sensitivity to politeness is primarily governed by RLHF and SFT."
  - [corpus] The corpus includes studies on the impact of RLHF on model behavior.
- Break condition: If the model has not been fine-tuned with RLHF/SFT, or if the fine-tuning corpus does not include diverse politeness examples.

## Foundational Learning

- Concept: Politeness and respect as cultural-linguistic constructs
  - Why needed here: Understanding how politeness is encoded differently across languages is essential for interpreting model behavior and designing prompts.
  - Quick check question: What are the key differences between how politeness is expressed in English, Chinese, and Japanese?

- Concept: Cross-lingual prompt engineering
  - Why needed here: Designing effective prompts requires adapting not just language but also cultural context and politeness norms.
  - Quick check question: How would you adapt a polite prompt for a Chinese-speaking user versus a Japanese-speaking user?

- Concept: Bias detection and mitigation in LLMs
  - Why needed here: Evaluating model outputs for stereotypical bias is a core part of the experimental design.
  - Quick check question: What methods can be used to detect bias in LLM outputs, and how do they differ across languages?

## Architecture Onboarding

- Component map:
  - Prompt generator (politeness templates per language) -> Task runner (summarization, language understanding, bias detection) -> Evaluation pipeline (BERTScore, ROUGE-L, bias index, benchmark scoring) -> Data storage (results per politeness level, language, model)
- Critical path:
  - Design prompts → Run tasks on models → Evaluate outputs → Analyze bias and performance trends
- Design tradeoffs:
  - Balancing politeness level granularity vs. prompt diversity
  - Choosing between full benchmark vs. sampled subset for cost efficiency
  - Prioritizing languages with most available resources vs. most cross-cultural insight
- Failure signatures:
  - High refusal rates at low politeness levels
  - Unexpected performance drops at high politeness levels
  - Inconsistent bias patterns across languages or models
- First 3 experiments:
  1. Test summarization task with politeness levels 1-8 on GPT-3.5 in English; measure length and quality.
  2. Run MMLU benchmark with politeness levels 1-8 on Llama2-70B in English; analyze score variance.
  3. Evaluate bias index on Crows-Pairs for politeness levels 1-8 on GPT-4 in English; compare to Chinese and Japanese.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the politeness sensitivity of LLMs vary across different task types beyond summarization, language understanding, and bias detection?
- Basis in paper: [explicit] The paper tested three task types and found varying sensitivity to politeness levels, but acknowledges limitations in task diversity.
- Why unresolved: The study only evaluated three task categories, leaving open whether the politeness sensitivity generalizes to other tasks like creative writing, code generation, or complex reasoning.
- What evidence would resolve it: Testing the same politeness prompts across a broader range of task types and comparing performance sensitivity patterns would reveal task-specific politeness effects.

### Open Question 2
- Question: What specific training data patterns cause LLMs to be more sensitive to politeness in certain languages versus others?
- Basis in paper: [inferred] The paper observes that models trained in specific languages show higher sensitivity to politeness in those languages, but doesn't examine the underlying training data distribution.
- Why unresolved: The study doesn't analyze the training corpora to identify politeness-related patterns that might explain the differential sensitivity across languages.
- What evidence would resolve it: Analyzing the frequency and context of polite vs. impolite expressions in the training data for each language model would reveal correlations with politeness sensitivity.

### Open Question 3
- Question: How do different RLHF and SFT training approaches affect politeness sensitivity in LLMs?
- Basis in paper: [explicit] The paper compares Llama2-70B with and without RLHF/SFT and finds significant differences in politeness sensitivity, but doesn't explore variations in these training methods.
- Why unresolved: The study only tests one base model and one fine-tuned version, not exploring how different RLHF/SFT approaches (e.g., different reward functions, human feedback quality) affect politeness sensitivity.
- What evidence would resolve it: Testing multiple RLHF/SFT variants on the same base model and comparing their politeness sensitivity would reveal which training approaches increase or decrease politeness effects.

## Limitations

- The study's findings hinge on several critical assumptions about the representativeness of training data and the universality of politeness effects.
- A key limitation is the potential for cultural bias in how politeness is defined and measured across different languages.
- The generalizability of results to smaller or specialized models remains unclear, as the study primarily focuses on larger, more capable LLMs.

## Confidence

- **High Confidence**: The core finding that impolite prompts can lead to decreased performance (refusals, bias, incorrect answers) is well-supported by the experimental results across multiple languages and models.
- **Medium Confidence**: The claim that optimal politeness levels vary by language and culture is supported by the data but requires more nuanced interpretation.
- **Medium Confidence**: The assertion that RLHF and SFT moderate politeness sensitivity is supported by comparative analysis between base and fine-tuned models.

## Next Checks

1. **Cross-Model Validation**: Test the politeness effects on a broader range of models including smaller, task-specific models and open-source alternatives to verify if the observed patterns generalize beyond the current model selection.

2. **Fine-tuning Data Analysis**: Conduct an audit of the pretraining and fine-tuning corpora for each model/language combination to quantify the representation of polite vs. impolite language and its correlation with observed sensitivity.

3. **Prompt Strategy Interaction**: Design experiments to test how different prompting strategies (chain-of-thought, few-shot examples) interact with politeness levels, potentially revealing whether politeness effects are additive or multiplicative with other prompting techniques.