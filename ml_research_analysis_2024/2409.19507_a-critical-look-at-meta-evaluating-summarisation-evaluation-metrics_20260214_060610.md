---
ver: rpa2
title: A Critical Look at Meta-evaluating Summarisation Evaluation Metrics
arxiv_id: '2409.19507'
source_url: https://arxiv.org/abs/2409.19507
tags:
- evaluation
- metrics
- summary
- summaries
- human
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This position paper critically examines current practices for meta-evaluating
  summarization evaluation metrics. The authors find that most benchmarks rely on
  news summarization datasets and focus primarily on content quality, especially faithfulness.
---

# A Critical Look at Meta-evaluating Summarisation Evaluation Metrics

## Quick Facts
- arXiv ID: 2409.19507
- Source URL: https://arxiv.org/abs/2409.19507
- Reference count: 16
- Primary result: Current meta-evaluation practices for summarization metrics are biased toward news datasets, lack standardization, and inadequately assess metrics' ability to distinguish similar-quality systems

## Executive Summary
This position paper critically examines current practices for meta-evaluating summarization evaluation metrics. The authors find that most benchmarks rely on news summarization datasets and focus primarily on content quality, especially faithfulness. They identify key gaps including limited domain diversity, narrow quality dimensions, inconsistent definitions of quality criteria, lack of standardized human evaluation practices, and inadequate assessment of metrics' ability to distinguish between similar-quality systems. The paper calls for building more diverse benchmarks across domains and applications, standardizing human evaluation practices, and developing multi-stage assessment protocols that test metrics' ability to detect significant errors, distinguish similar-quality systems, and identify fine-grained issues.

## Method Summary
The paper reviews recent meta-evaluation benchmarks and their methodologies for collecting human judgments and defining quality dimensions. It analyzes the distribution of automatic evaluation scores across different domains and task constraints, comparing the correlation between automatic metrics and human judgments across different datasets and quality dimensions. The authors consider both system-level and summary-level protocols to identify limitations in current meta-evaluation practices and provide recommendations for improving the reliability and generalizability of summarization evaluation metrics.

## Key Results
- Most meta-evaluation benchmarks rely on news summarization datasets (CNN/DM, XSUM), limiting metric generalizability to other domains
- Human evaluation practices lack standardization in annotator selection, quality control, and annotation guidelines
- High system-level correlation between metrics and human judgments can mask poor summary-level performance
- Current benchmarks inadequately assess metrics' ability to distinguish between similar-quality systems

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Current meta-evaluation practices are biased toward news summarization datasets, limiting metric generalizability.
- **Mechanism:** Most benchmarks use news articles from CNN/DM and XSUM, creating evaluation metrics that work well for news but fail in other domains like biomedical or dialogue.
- **Core assumption:** Named entity recognition tools and question generation strategies that work for news entities will fail for domain-specific entities in other text types.
- **Evidence anchors:**
  - [abstract]: "evaluation metrics are primarily meta-evaluated on datasets consisting of examples from news summarisation datasets"
  - [section 3.1]: "most of the widely used meta-evaluation benchmarks use source texts from news summarisation datasets"
- **Break condition:** When metrics are tested on truly diverse domains (biomedical, legal, dialogue) and fail to maintain correlation with human judgments.

### Mechanism 2
- **Claim:** Human evaluation practices lack standardization, making benchmark results non-reproducible and non-extensible.
- **Mechanism:** Different studies use different annotator types (experts vs crowd workers), different quality control methods, and different annotation interfaces, leading to inconsistent results.
- **Core assumption:** Without standardized protocols, collected human judgments cannot be reliably reused or extended to new models.
- **Evidence anchors:**
  - [abstract]: "lack of standardized human evaluation practices"
  - [section 3.3]: Discusses variations in annotator expertise, quality control practices, and the impact of reference summaries on judgments
- **Break condition:** When two studies using different protocols get conflicting results on the same set of summaries.

### Mechanism 3
- **Claim:** Evaluation metrics that correlate well with human judgments on diverse systems may fail to detect significant errors in individual summaries.
- **Mechanism:** High system-level correlation can mask poor summary-level performance; metrics may distinguish between good and bad systems but fail to identify specific problematic summaries.
- **Core assumption:** Correlation at the system level doesn't guarantee the metric can detect fine-grained issues within summaries from similar-quality systems.
- **Evidence anchors:**
  - [abstract]: "inadequate assessment of metrics' ability to distinguish between similar-quality systems"
  - [section 3.4]: "high correlation is usually attributed to the capability of distinguishing between systems with large performance gaps"
- **Break condition:** When metrics show high system-level correlation but poor performance on detecting known corruptions or distinguishing summaries from similar-quality systems.

## Foundational Learning

- **Concept: Correlation coefficients (Pearson, Spearman, Kendall)**
  - Why needed here: Understanding how meta-evaluation measures the relationship between automatic metric scores and human judgments
  - Quick check question: If a metric has Spearman correlation of 0.8 with human judgments, what does this tell us about the relationship between the metric's rankings and human rankings?

- **Concept: Domain adaptation and generalization**
  - Why needed here: Understanding why metrics trained on news summarization may not transfer to other domains like biomedical or legal text
  - Quick check question: If a metric performs well on CNN/DM but poorly on biomedical summaries, what does this suggest about the metric's generalization ability?

- **Concept: Human evaluation protocols and reliability**
  - Why needed here: Understanding how different annotation approaches (expert vs crowd workers, with vs without reference summaries) affect the reliability of human judgments used in meta-evaluation
  - Quick check question: How might using crowd workers instead of expert annotators affect the reliability of human judgments in a meta-evaluation benchmark?

## Architecture Onboarding

- **Component map:**
  - Data collection: Source texts, reference summaries, system-generated summaries
  - Human evaluation: Quality dimension definitions, annotation protocols, annotator selection
  - Automatic metrics: Various approaches (summary-only, similarity-based, entailment-based, etc.)
  - Meta-evaluation: Correlation calculation (system-level and summary-level), classification/ranking protocols

- **Critical path:** 
  1. Collect diverse source texts and summaries from multiple domains
  2. Establish standardized human evaluation protocols with quality controls
  3. Define clear quality dimensions with consistent terminology
  4. Calculate both system-level and summary-level correlations
  5. Test metrics on error detection, system discrimination, and fine-grained issue identification

- **Design tradeoffs:**
  - Expert annotators vs crowd workers: Higher quality vs lower cost
  - Reference summary vs reference-free: Easier annotation vs potential bias
  - System-level vs summary-level correlation: Distinguishes good vs bad systems vs identifies specific problematic summaries
  - Correlation vs classification/ranking: Continuous scores vs binary judgments

- **Failure signatures:**
  - High system-level correlation but poor summary-level performance
  - Domain-specific performance drops (e.g., good on news, poor on biomedical)
  - Inconsistent results across different human evaluation protocols
  - Failure to detect known corruptions or errors

- **First 3 experiments:**
  1. Test existing metrics on a small, diverse dataset (news + biomedical + dialogue) to observe domain-specific performance differences
  2. Compare system-level and summary-level correlations for the same metric to identify potential masking effects
  3. Evaluate metrics on known corrupted summaries (from benchmarks like SEAHORSE or SUMM EDITS) to assess error detection capability

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What specific evaluation metrics should be developed to assess the effectiveness of automatic metrics in detecting significant errors in summaries?
- Basis in paper: [explicit] The paper discusses the need for evaluation metrics to be tested on their effectiveness in detecting significant errors, such as corruptions in human-written summaries.
- Why unresolved: While the paper highlights the importance of this aspect, it does not provide specific guidelines or metrics for evaluating the effectiveness of automatic metrics in detecting significant errors.
- What evidence would resolve it: Research that develops and validates specific metrics or benchmarks for evaluating the ability of automatic metrics to detect significant errors in summaries would resolve this question.

### Open Question 2
- Question: How can human evaluation practices be standardized to ensure reproducible and extensible judgments across different annotators and summarizer models?
- Basis in paper: [explicit] The paper emphasizes the need for standardizing human evaluation practices to ensure reproducible human judgments over time and across different annotators.
- Why unresolved: The paper suggests the importance of standardization but does not provide concrete methods or frameworks for achieving this goal.
- What evidence would resolve it: Development and implementation of standardized annotation guidelines, training protocols, and quality control mechanisms that can be consistently applied across different studies and domains would resolve this question.

### Open Question 3
- Question: What are the key factors that influence the generalization ability of evaluation metrics across different domains and applications?
- Basis in paper: [explicit] The paper discusses the need to build more diverse benchmarks to analyze the generalization ability of existing evaluation metrics across different domains.
- Why unresolved: While the paper identifies the need for diverse benchmarks, it does not specify the key factors that influence the generalization ability of evaluation metrics.
- What evidence would resolve it: Empirical studies that systematically investigate the impact of various factors (e.g., domain characteristics, task constraints, user preferences) on the generalization ability of evaluation metrics would resolve this question.

## Limitations

- The analysis is primarily based on reviewing existing meta-evaluation practices rather than conducting new experiments
- The actual impact of domain bias on metric generalization has not been empirically validated across diverse domains
- The relationship between system-level and summary-level correlation remains theoretical without direct experimental comparison

## Confidence

- **High Confidence**: Identification of domain bias in current benchmarks and recognition that human evaluation protocols lack standardization
- **Medium Confidence**: Claim that system-level correlation can mask poor summary-level performance and assertion that current benchmarks inadequately assess metrics' ability to distinguish similar-quality systems
- **Low Confidence**: Specific quantitative thresholds for when domain bias becomes problematic or when system-level correlation becomes misleading

## Next Checks

1. **Cross-domain validation study**: Test existing metrics on a balanced dataset containing news, biomedical, and dialogue summarization tasks to empirically measure performance drops across domains
2. **Correlation comparison experiment**: For a set of similar-quality summarization systems, measure both system-level and summary-level correlations to identify masking effects and determine when summary-level performance becomes critical
3. **Protocol standardization test**: Implement two different human evaluation protocols (expert vs crowd workers, with vs without reference summaries) on the same set of summaries to measure the impact of protocol variations on metric meta-evaluation results