---
ver: rpa2
title: 'RALI@TREC iKAT 2024: Achieving Personalization via Retrieval Fusion in Conversational
  Search'
arxiv_id: '2412.07998'
source_url: https://arxiv.org/abs/2412.07998
tags:
- query
- retrieval
- search
- conversational
- fusion
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of over-personalization in conversational
  search, where incorporating user profile information into query reformulation can
  lead to drift from the original query intent. The authors propose a retrieval fusion
  approach that combines ranking lists generated from de-contextualized non-personalized
  queries, expanded queries, and de-contextualized personalized queries using a linear
  combination method.
---

# RALI@TREC iKAT 2024: Achieving Personalization via Retrieval Fusion in Conversational Search

## Quick Facts
- arXiv ID: 2412.07998
- Source URL: https://arxiv.org/abs/2412.07998
- Authors: Yuchen Hui, Fengran Mo, Milan Mao, Jian-Yun Nie
- Reference count: 40
- Key outcome: Retrieval fusion approach combining personalized and non-personalized queries achieves MRR of 84.1 and NDCG@10 of 47.7 on TREC iKAT 2024

## Executive Summary
This paper addresses the over-personalization challenge in conversational search where incorporating user profile information into query reformulation can cause drift from original query intent. The authors propose a retrieval fusion approach that combines ranking lists generated from de-contextualized non-personalized queries, expanded queries, and de-contextualized personalized queries using a linear combination method. Their fusion-then-reranking approach achieves strong results on the TREC iKAT 2024 passage ranking task, demonstrating the effectiveness of combining multiple query reformulation strategies to balance personalization benefits with relevance preservation.

## Method Summary
The approach generates three types of query reformulations per conversational turn using GPT-4o: de-contextualized non-personalized, expanded, and de-contextualized personalized queries. BM25 retrieval is performed for each query type, retrieving top 1000 documents. The three ranking lists are fused using a score-based linear combination method, then the top 50 documents are reranked using a MonoT5 transformer model. Finally, the top 3 documents are used to generate responses via GPT-4o. The method balances the benefits of personalization against the risk of query drift by leveraging complementary retrieval signals from different query types.

## Key Results
- Fusion method alone achieves MRR of 75.2 and NDCG@10 of 35.6
- Fusion-then-reranking improves to MRR of 84.1 and NDCG@10 of 47.7
- Demonstrates effectiveness of combining multiple query reformulation strategies
- Identifies assessment bias in iKAT collections where human rewrites receive preferential treatment

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Linear combination of personalized and non-personalized ranking lists improves retrieval performance by leveraging the "Chorus Effect" principle.
- Mechanism: By combining document scores from both personalized and non-personalized queries, the method identifies documents that perform well across multiple retrieval strategies, filtering out irrelevant documents that rank highly due to personalization drift.
- Core assumption: Documents relevant to both personalized and non-personalized queries are more likely to be truly relevant to the user's information need.
- Evidence anchors: [abstract] "It is difficult to disregard any of them, whereas introducing an excessive number of these pieces risks drifting from the original query and hinders search performance. This is a challenge we denote as over-personalization."
- Break condition: If the personalization drift is severe enough that no documents perform well in both ranking lists, the fusion approach would fail to identify relevant documents.

### Mechanism 2
- Claim: The fusion-then-reranking approach significantly improves retrieval metrics compared to either method alone.
- Mechanism: First fusing ranking lists from different query reformulations creates a broader candidate pool, then reranking with a transformer-based model refines the results based on contextual understanding.
- Core assumption: The reranker can effectively distinguish between documents that score well in fusion due to breadth versus those that are genuinely most relevant.
- Evidence anchors: [section 3.2.1] "The top 50 documents from the fused ranking list are reranked using the MonoT54 [30] reranker."
- Break condition: If the reranker is not trained to handle the specific characteristics of fused ranking lists, it may not improve upon the fusion results.

### Mechanism 3
- Claim: Using different types of query reformulations (de-contextualized non-personalized, expanded, and de-contextualized personalized) provides complementary retrieval signals.
- Mechanism: Each query type captures different aspects of the user's information need - the non-personalized version focuses on core intent, the expanded version adds context, and the personalized version incorporates user preferences.
- Core assumption: The different query reformulations will retrieve overlapping but distinct sets of relevant documents, creating complementary signals when fused.
- Evidence anchors: [section 3.2.1] "For each turn, three types of query reformulations are generated via prompting the GPT-4o model."
- Break condition: If the query reformulations are too similar or too divergent, the complementary signals may not materialize, reducing the effectiveness of fusion.

## Foundational Learning

- Concept: Linear combination methods in information retrieval
  - Why needed here: The paper uses a score-based linear combination method to fuse ranking lists from different query reformulations
  - Quick check question: What is the mathematical formula for combining scores from multiple ranking lists using weighted linear combination?

- Concept: Conversational query reformulation techniques
  - Why needed here: The paper relies on generating different types of reformulated queries using LLMs as the basis for retrieval fusion
  - Quick check question: What are the key differences between de-contextualized non-personalized, expanded, and de-contextualized personalized query reformulations?

- Concept: Retrieval fusion methods and their tradeoffs
  - Why needed here: Understanding when and why to use score-based versus rank-based fusion methods is critical for implementing the proposed approach
  - Quick check question: What are the main advantages and disadvantages of score-based linear combination versus rank-based fusion methods?

## Architecture Onboarding

- Component map: Query reformulation engine (LLM) -> BM25 retrieval system (3 instances) -> Linear combination fusion module -> Reranking module (MonoT5) -> Response generation module (GPT-4o) -> Evaluation pipeline

- Critical path:
  1. Generate three query reformulations per turn
  2. Retrieve top 1000 documents for each query using BM25
  3. Fuse ranking lists using linear combination with fixed weights
  4. Rerank top 50 documents from fused list
  5. Generate response using top 3 documents

- Design tradeoffs:
  - Fixed weights vs. adaptive weights for linear combination
  - Top K candidates for fusion (1000 vs. smaller number)
  - Reranking threshold (top 50 vs. all candidates)
  - Use of PTKB in query reformulation

- Failure signatures:
  - Performance degrades significantly when personalization is not needed
  - Fusion weights need frequent adjustment across different domains
  - Reranker fails to improve upon fused results
  - Assessment bias makes evaluation unreliable

- First 3 experiments:
  1. Test the effect of different fusion weight combinations on a small validation set
  2. Compare fusion-then-reranking versus reranking-then-fusion approaches
  3. Evaluate the impact of including/excluding the expanded query formulation on overall performance

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal weighting strategy for linear combination of personalized and non-personalized ranking lists in conversational search?
- Basis in paper: [explicit] The paper uses fixed weights across all conversational turns for each reformulation type, but notes that "it is worth exploring other reasonable weight assignment approaches since no public training data is available"
- Why unresolved: The authors only experimented with fixed weights optimized on iKAT-23 test collection, but acknowledge that more sophisticated weighting assignment methods could be explored
- What evidence would resolve it: Empirical comparison of different weighting strategies (dynamic vs fixed, learned vs heuristic) across multiple test collections showing which approach yields best retrieval performance

### Open Question 2
- Question: How can assessment bias in iKAT test collections be mitigated to enable fair evaluation of non-participant retrieval strategies?
- Basis in paper: [explicit] The authors demonstrate that human rewrites receive preferential treatment in pooling processes, with only 5 of top 20 documents assessed for improved queries compared to 14 for original human rewrites
- Why unresolved: The paper identifies the problem but doesn't propose solutions, only noting that "deeper pooling depth may help" but acknowledging this is "a costly remedy"
- What evidence would resolve it: Experimental results showing evaluation metrics from alternative assessment methods (deeper pools, separate relevance judgments) that correlate with actual retrieval performance across diverse query reformulations

### Open Question 3
- Question: What are the fundamental limitations of using linear combination methods for retrieval fusion in personalized conversational search?
- Basis in paper: [inferred] While the authors achieve good results with linear combination, they only explore one fusion strategy and don't investigate potential limitations or alternatives
- Why unresolved: The paper presents a successful application but doesn't critically examine when or why linear combination might fail, or how it compares to other fusion approaches
- What evidence would resolve it: Systematic analysis comparing linear combination to rank-based fusion methods (RRF, Borda count) and score-based alternatives across various personalization levels and query types, identifying conditions where each approach excels or fails

## Limitations
- Fixed weight combination approach may not generalize optimally across different domains or user types
- Assessment bias in iKAT collections remains a concern, potentially inflating performance metrics
- Method's effectiveness in scenarios where personalization is genuinely beneficial versus when it causes harmful drift is not fully characterized

## Confidence
- **High confidence**: The retrieval fusion approach demonstrates measurable performance improvements over individual retrieval strategies
- **Medium confidence**: The chorus effect mechanism and specific weight values' optimality are supported but not definitively proven
- **Low confidence**: Assessment bias impacts and generalizability to non-TREC domains require further validation

## Next Checks
1. Test the fusion approach on multiple conversational search datasets to evaluate generalizability beyond TREC iKAT
2. Conduct ablation studies varying fusion weights to determine optimal configurations across different query types and user profiles
3. Implement deeper pooling and manual relevance assessment on a subset of results to quantify potential assessment bias effects