---
ver: rpa2
title: 'SciQAG: A Framework for Auto-Generated Science Question Answering Dataset
  with Fine-grained Evaluation'
arxiv_id: '2405.09939'
source_url: https://arxiv.org/abs/2405.09939
tags:
- scientific
- science
- question
- questions
- pairs
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: SciQAG is a framework for automatically generating high-quality
  science question-answer pairs from scientific literature using large language models.
  The framework combines a QA generator that extracts information and generates diverse
  question-answer pairs with a QA evaluator that assesses their quality.
---

# SciQAG: A Framework for Auto-Generated Science Question Answering Dataset with Fine-grained Evaluation

## Quick Facts
- **arXiv ID**: 2405.09939
- **Source URL**: https://arxiv.org/abs/2405.09939
- **Reference count**: 40
- **Primary result**: Auto-generated dataset of 188,042 QA pairs from 22,743 scientific papers across 24 domains using LLM-based framework

## Executive Summary
SciQAG is a framework for automatically generating high-quality science question-answer pairs from scientific literature using large language models. The framework combines a QA generator that extracts information and generates diverse question-answer pairs with a QA evaluator that assesses their quality. Applied to 22,743 scientific papers across 24 domains, SciQAG produced a dataset of 188,042 QA pairs. The framework's effectiveness was demonstrated through extensive experiments showing that fine-tuning language models on the generated data significantly improves their performance on both open-ended question answering and various scientific tasks. The generated questions showed high diversity (average similarity score of 0.186), and the answers demonstrated strong coverage of source papers (average 78.6%).

## Method Summary
The SciQAG framework employs a two-stage approach: first, a QA generator extracts information from scientific papers and generates diverse question-answer pairs, and second, a QA evaluator assesses the quality of these pairs using automated metrics. The framework was applied to 22,743 scientific papers spanning 24 different domains, resulting in a dataset of 188,042 QA pairs. The generated dataset was then used to fine-tune language models, demonstrating significant improvements in performance on both open-ended question answering tasks and various scientific benchmarks.

## Key Results
- Generated dataset of 188,042 QA pairs from 22,743 scientific papers across 24 domains
- High question diversity with average similarity score of 0.186
- Strong answer coverage of source papers at 78.6%
- Fine-tuning on generated data significantly improved model performance on scientific tasks

## Why This Works (Mechanism)
The framework leverages large language models' ability to understand and extract scientific information from literature, then generate relevant questions and answers. By combining extraction and generation with quality evaluation, it creates a self-reinforcing system that produces diverse, high-quality QA pairs at scale. The multi-domain approach ensures broad coverage of scientific knowledge while maintaining domain-specific accuracy.

## Foundational Learning

1. **Scientific literature parsing** - Needed for extracting structured information from unstructured scientific papers. Quick check: Ability to identify key concepts, methodologies, and findings from abstracts and conclusions.

2. **Question generation diversity** - Critical for creating varied QA pairs that test different aspects of understanding. Quick check: Generated questions should cover different question types (what, how, why) and difficulty levels.

3. **Automated quality evaluation** - Essential for scaling dataset creation without manual annotation. Quick check: QA pairs should meet criteria for factual accuracy, relevance, and completeness.

## Architecture Onboarding

**Component Map:** Scientific Papers -> QA Generator -> QA Pairs -> QA Evaluator -> Filtered Dataset

**Critical Path:** The QA generator extracts information from papers and creates question-answer pairs, which are then evaluated for quality before inclusion in the final dataset. This ensures only high-quality pairs are retained.

**Design Tradeoffs:** Single-hop vs. multi-hop questions (favoring single-hop for simplicity and coverage), automated vs. human evaluation (favoring automated for scalability), domain-specific vs. general knowledge (favoring multi-domain approach).

**Failure Signatures:** Low diversity in generated questions, poor coverage of source material, factual inaccuracies in answers, or failure to meet quality thresholds during evaluation.

**First Experiments:**
1. Generate QA pairs from a small sample of papers and manually evaluate quality
2. Test framework on papers from a single domain before scaling to multi-domain
3. Compare generated questions against human-written questions for similarity and diversity

## Open Questions the Paper Calls Out
None

## Limitations
- Heavy reliance on automatic metrics and LLM-based assessments for quality evaluation
- Limited human evaluation (only 100 samples) may not represent overall dataset quality
- Focus on single-hop questions may limit applicability to complex scientific reasoning tasks

## Confidence

**High Confidence:** The framework's ability to generate a large-scale dataset from scientific papers and the observed improvements in downstream task performance when fine-tuning models on the generated data.

**Medium Confidence:** The diversity metrics of generated questions and the coverage of answers with respect to source papers, as these rely on automated similarity calculations.

**Low Confidence:** The generalizability of the framework across different scientific domains beyond the 24 tested domains, and the long-term stability of generated questions as scientific knowledge evolves.

## Next Checks

1. Conduct extensive human evaluation across multiple scientific domains and expertise levels to validate the quality metrics reported in the paper.

2. Test the framework's performance on cross-domain transfer learning tasks to assess its generalizability beyond the specific domains used in training.

3. Implement longitudinal studies to evaluate how well the generated questions maintain their relevance and accuracy as scientific knowledge advances over time.