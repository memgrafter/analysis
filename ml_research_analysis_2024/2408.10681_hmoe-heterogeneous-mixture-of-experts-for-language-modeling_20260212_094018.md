---
ver: rpa2
title: 'HMoE: Heterogeneous Mixture of Experts for Language Modeling'
arxiv_id: '2408.10681'
source_url: https://arxiv.org/abs/2408.10681
tags:
- experts
- hmoe
- expert
- activated
- more
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Heterogeneous Mixture of Experts (HMoE),
  addressing the lack of specialization in traditional homogeneous MoE models where
  all experts have identical capacity. The proposed HMoE assigns different sizes to
  experts, enabling more specialized handling of varying token complexities.
---

# HMoE: Heterogeneous Mixture of Experts for Language Modeling

## Quick Facts
- **arXiv ID**: 2408.10681
- **Source URL**: https://arxiv.org/abs/2408.10681
- **Reference count**: 24
- **Key outcome**: HMoE achieves lower loss with fewer activated parameters and outperforms conventional homogeneous MoE on pre-training evaluation benchmarks, with average improvements of 1.50% at higher training costs.

## Executive Summary
This paper introduces Heterogeneous Mixture of Experts (HMoE), addressing the lack of specialization in traditional homogeneous MoE models where all experts have identical capacity. The proposed HMoE assigns different sizes to experts, enabling more specialized handling of varying token complexities. To counter the tendency of larger experts being over-activated, the authors introduce a parameter penalty loss that encourages the activation of smaller experts, improving computational efficiency and parameter utilization. Experiments show that HMoE achieves lower loss with fewer activated parameters and outperforms conventional homogeneous MoE on pre-training evaluation benchmarks.

## Method Summary
HMoE implements a Transformer decoder-only architecture with 12 layers, replacing FFN layers with MoE layers containing 8 experts of varying sizes following an arithmetic progression (9, 11, 13, 15, 17, 19, 21, 23). The parameter penalty loss function penalizes the use of larger experts to encourage their more efficient utilization. The model is trained on the RedPajama dataset using AdamW optimizer with specific learning rate schedule, weight decay, and batch size, evaluated with both Top-K and Top-P routing strategies on six benchmarks.

## Key Results
- HMoE achieves lower loss with fewer activated parameters compared to homogeneous MoE
- Outperforms conventional homogeneous MoE on pre-training evaluation benchmarks with average improvements of 1.50% at higher training costs
- Arithmetic progression of expert sizes provides optimal heterogeneity balance compared to geometric or hybrid strategies
- P-Penalty loss successfully balances expert activation by penalizing the use of larger experts

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Larger experts capture complex patterns while smaller experts efficiently handle simpler tokens, improving computational efficiency.
- Mechanism: HMoE introduces varying expert sizes, allowing larger experts to specialize in complex reasoning tasks and smaller experts to process simpler inputs. The P-Penalty loss encourages activation of smaller experts, reducing overall computational load.
- Core assumption: Expert size directly correlates with representational capacity, and token complexity can be effectively differentiated by the router.
- Evidence anchors:
  - [abstract]: "The proposed HMoE assigns different sizes to experts, enabling more specialized handling of varying token complexities."
  - [section]: "In HMoE models, the presence of both large and small experts introduces a challenge where the optimization goal of the language model naturally favors the frequent activation of larger experts due to their superior performance."
  - [corpus]: Found 25 related papers. Average neighbor FMR=0.478. Top related titles: Grove MoE: Towards Efficient and Superior MoE LLMs with Adjugate Experts, On Expert Estimation in Hierarchical Mixture of Experts: Beyond Softmax Gating Functions.

### Mechanism 2
- Claim: The P-Penalty loss function successfully balances expert activation by penalizing the use of larger experts, promoting efficient parameter utilization.
- Mechanism: P-Penalty loss incorporates expert size into the optimization objective, making it costly to activate larger experts. This encourages the model to use smaller experts more frequently while still allowing larger experts for complex tasks.
- Core assumption: The P-Penalty loss effectively modifies the optimization landscape to favor smaller expert activation without harming model performance.
- Evidence anchors:
  - [section]: "We introduce a novel training objective parameter penalty (P-Penalty) loss LP-Penalty as: LP-Penalty = N ∑ i=1 Mi ∗ ˆPi, Mi represents the average dimension of the hidden state of the expert ei on the entire input x."
  - [section]: "This imbalance led to a decline in the model's overall representational capacity. The root cause is that the larger experts possess stronger capabilities compared to the smaller ones, prompting the router to preferentially activate the larger experts more often."
  - [corpus]: Found 25 related papers. Average neighbor FMR=0.478.

### Mechanism 3
- Claim: Arithmetic progression of expert sizes provides optimal heterogeneity balance, avoiding both excessive variance and complete homogeneity.
- Mechanism: Expert sizes follow an arithmetic sequence (e.g., {9, 11, 13, 15, 17, 19, 21, 23}), providing consistent variation while maintaining sufficient differentiation. This design avoids the instability of geometric progression and the limitations of homogeneous experts.
- Core assumption: Arithmetic progression provides the right balance of heterogeneity to enable effective specialization without causing training instability or representation collapse.
- Evidence anchors:
  - [section]: "We mainly explore three types of heterogeneity structures: (1) Geometric strategy... (2) Arithmetic strategy... (3) Hybrid strategy."
  - [section]: "Our results show that the geometric distribution performs the worst. Figure 8(right) illustrates that smaller experts in the geometric progression are less frequently activated, even with P-Penalty loss, suggesting their capacity is insufficient because of their too-small size."
  - [corpus]: Found 25 related papers. Average neighbor FMR=0.478.

## Foundational Learning

- Concept: Mixture of Experts (MoE) architecture
  - Why needed here: Understanding how MoE differs from dense models is fundamental to grasping HMoE's innovations
  - Quick check question: How does MoE achieve computational efficiency compared to dense models?

- Concept: Router and expert interaction in MoE
  - Why needed here: The router's role in selecting experts is critical to understanding how HMoE modifies this process
  - Quick check question: What determines which experts are activated for a given token in traditional MoE?

- Concept: Parameter utilization and activation patterns
  - Why needed here: HMoE's efficiency gains depend on understanding how parameters are activated and utilized
  - Quick check question: How does the number of activated parameters affect computational efficiency in MoE models?

## Architecture Onboarding

- Component map:
  - Token input -> Router assessment -> Expert selection -> Expert processing -> Model output

- Critical path:
  1. Token input → Router assessment
  2. Router selects experts based on token complexity
  3. Selected experts process token
  4. P-Penalty loss influences future routing decisions
  5. Model output generated from expert contributions

- Design tradeoffs:
  - Expert size variance vs. training stability
  - P-Penalty coefficient strength vs. performance
  - Arithmetic vs. geometric vs. hybrid heterogeneity strategies
  - Computational efficiency vs. representational capacity

- Failure signatures:
  - Excessive activation of larger experts despite P-Penalty loss
  - Training instability or divergence
  - Performance degradation compared to homogeneous MoE
  - Insufficient differentiation between expert capabilities

- First 3 experiments:
  1. Compare loss and activated parameters with/without P-Penalty loss on small dataset
  2. Test different expert size distributions (geometric, arithmetic, hybrid) on validation set
  3. Evaluate performance across token complexity levels to verify expert specialization

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the optimal level of heterogeneity in expert sizes vary with different model scales and training datasets?
- Basis in paper: [explicit] The authors found that the geometric distribution strategy performed poorly due to large gaps in expert ability, and they adopted a relatively balanced heterogeneous distribution in their main experiment. They also noted that as the ratio between the largest and smallest experts increases, the model's performance initially degrades but then improves.
- Why unresolved: The paper only explored a limited set of heterogeneity strategies (geometric, arithmetic, and hybrid) and did not systematically investigate how the optimal heterogeneity level changes with model scale and dataset characteristics.
- What evidence would resolve it: Comprehensive experiments comparing different heterogeneity distributions across various model sizes and diverse training datasets, measuring both performance and efficiency metrics.

### Open Question 2
- Question: What are the long-term effects of the parameter penalty loss on model generalization and robustness?
- Basis in paper: [explicit] The authors introduced a parameter penalty loss to encourage the activation of smaller experts, leading to more efficient utilization of experts and preventing the disproportionate reliance on larger experts. They found that this loss significantly improved model performance.
- Why unresolved: The paper primarily focused on pre-training evaluation benchmarks and did not extensively study the impact of the parameter penalty loss on long-term generalization and robustness across diverse downstream tasks and domains.
- What evidence would resolve it: Extensive experiments evaluating the model's performance on a wide range of downstream tasks, including out-of-distribution data and adversarial examples, comparing models trained with and without the parameter penalty loss.

### Open Question 3
- Question: How can the efficiency of heterogeneous MoE models be further improved through advanced hardware adaptation and optimization techniques?
- Basis in paper: [explicit] The authors acknowledged that while their optimized model and training processes achieved faster training speeds for HMoEs compared to traditional MoEs, there is still room for improvement, particularly in hardware adaptation. They mentioned that ES-MoE introduces expert-wise offloading and dynamic expert placement strategy to mitigate computation and communication imbalance.
- Why unresolved: The paper did not extensively explore advanced hardware adaptation techniques, such as specialized kernels for heterogeneous experts, distributed training strategies, or model compression methods tailored for HMoE models.
- What evidence would resolve it: Comprehensive studies comparing different hardware optimization techniques, including specialized kernels, distributed training strategies, and model compression methods, measuring their impact on training efficiency, inference speed, and model quality for HMoE models.

## Limitations

- The paper lacks comprehensive ablation studies isolating the contribution of P-Penalty loss versus heterogeneous expert sizes
- The arithmetic progression of expert sizes appears arbitrary without justification for why this specific progression outperforms other distributions
- Evaluation focuses on pre-training benchmarks but lacks extensive testing on diverse downstream tasks to verify generalization

## Confidence

- Mechanism 1 (Expert Specialization by Size): Medium confidence - The concept is theoretically sound but lacks comprehensive ablation studies
- Mechanism 2 (P-Penalty Loss Effectiveness): Medium confidence - The mathematical formulation is clear but empirical validation of routing influence is limited
- Mechanism 3 (Arithmetic Progression Optimality): Low confidence - The comparison appears superficial without exploring the full hyperparameter space

## Next Checks

1. **Router Accuracy Validation**: Implement a diagnostic tool to track which experts are activated for tokens of known complexity levels (e.g., tokens requiring simple vs. complex reasoning). Measure the correlation between token complexity and expert selection to verify that the router effectively routes to appropriate expert sizes.

2. **P-Penalty Loss Ablation Study**: Train three variants: (a) HMoE with P-Penalty loss, (b) HMoE without P-Penalty loss, and (c) homogeneous MoE with identical total parameters. Compare expert activation distributions, computational efficiency, and performance to isolate the contribution of the penalty mechanism.

3. **Generalization Across Domains**: Evaluate HMoE performance on at least three additional diverse language tasks (e.g., code generation, scientific writing, dialogue systems) beyond the six benchmarks provided. Measure both task-specific performance and computational efficiency to assess whether the heterogeneous architecture generalizes across different language domains.