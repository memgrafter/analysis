---
ver: rpa2
title: Towards a RAG-based Summarization Agent for the Electron-Ion Collider
arxiv_id: '2403.15729'
source_url: https://arxiv.org/abs/2403.15729
tags:
- claims
- agent
- information
- data
- retrieval
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper presents RAGS4EIC, a Retrieval-Augmented Generation (RAG)-based
  AI agent designed to assist researchers in navigating the vast information resources
  of the Electron-Ion Collider (EIC) collaboration. The agent queries a vector database
  of EIC-related documents and uses a Large Language Model (LLM) to generate concise
  summaries with citations.
---

# Towards a RAG-based Summarization Agent for the Electron-Ion Collider

## Quick Facts
- arXiv ID: 2403.15729
- Source URL: https://arxiv.org/abs/2403.15729
- Authors: Karthik Suresh; Neeltje Kackar; Luke Schleck; Cristiano Fanelli
- Reference count: 15
- Key outcome: RAGS4EIC achieves 96.4% claim recognition, 88.9% claim accuracy, and 85.3% citation frequency on synthetic evaluation set

## Executive Summary
This paper introduces RAGS4EIC, a Retrieval-Augmented Generation (RAG) agent designed to assist researchers in navigating the vast information resources of the Electron-Ion Collider (EIC) collaboration. The system queries a vector database of EIC-related documents and uses a Large Language Model (LLM) to generate concise summaries with citations. The agent significantly reduces hallucinations by grounding responses in the knowledge base, demonstrating its potential as a valuable tool for the EIC community.

## Method Summary
The RAGS4EIC agent uses a two-step approach: first, it queries a comprehensive vector database of EIC-related documents using cosine similarity-based retrieval; second, it utilizes an LLM (GPT-3.5-turbo-1106) to generate concise summaries with citations. The system was evaluated using a benchmark dataset created with LLM assistance, achieving high performance across standard metrics including claim recognition rate, claim accuracy rate, and source citation frequency.

## Key Results
- Achieved 96.4% claim recognition rate on synthetic evaluation dataset
- Demonstrated 88.9% claim accuracy rate in generated summaries
- Maintained 85.3% source citation frequency across responses
- RAGAs evaluation showed 87.4% faithfulness, 77.2% answer relevance, and 72.3% answer correctness

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The RAGS4EIC agent reduces hallucinations by grounding LLM responses in a curated vector database.
- Mechanism: Instead of relying solely on the LLM's parametric memory, the agent retrieves relevant semantic chunks from a PineCone vector database and uses them as context for generation, forcing the model to cite real sources.
- Core assumption: Cosine similarity-based retrieval of text chunks preserves semantic relevance for downstream summarization.
- Evidence anchors:
  - [abstract]: "The agent significantly reduces hallucinations by grounding responses in the knowledge base"
  - [section]: "Cosine similarity is utilized to assess the degree of similarity between two sets of semantic information"
  - [corpus]: No direct corpus evidence for hallucination reduction; the claim is primarily based on the abstract's assertion.
- Break condition: If the vector database is incomplete or the chunks are too large/small, semantic relevance degrades and hallucinations may reappear.

### Mechanism 2
- Claim: Instruction-tuned prompt templates improve the alignment of LLM responses for summarization tasks.
- Mechanism: The agent uses structured prompt templates that include sample responses and claim breakdowns, guiding the LLM to produce concise, cited summaries rather than open-ended prose.
- Core assumption: GPT-3.5/gpt-3.5-turbo-1106 can follow structured instructions reliably when given explicit claim-response mapping.
- Evidence anchors:
  - [section]: "Prompts that contain sample responses have been shown to improve summary performance"
  - [section]: "Individual responses to the claims, and a complete ideal response"
  - [corpus]: No explicit corpus evidence for prompt template effectiveness; inferred from LLM literature.
- Break condition: If the LLM cannot parse the template structure, outputs may be incomplete or omit citations.

### Mechanism 3
- Claim: LLM-assisted synthetic dataset generation enables scalable evaluation without domain-expert manual annotation.
- Mechanism: GPT-4.0 is prompted to generate question-answer pairs with explicit claim breakdowns from arXiv papers, producing a rich benchmark for automated RAG evaluation.
- Core assumption: GPT-4.0 can accurately extract and summarize claims from EIC physics papers even though its training cutoff predates the source papers.
- Evidence anchors:
  - [section]: "an advanced language model such as GPT-4.0 to generate a synthetic dataset for evaluation purposes"
  - [section]: "Each question in the dataset is associated with a clearly defined set of 'claims'"
  - [corpus]: No corpus evidence for accuracy of synthetic data; effectiveness inferred from reported evaluation scores.
- Break condition: If GPT-4.0 misinterprets the paper content, the synthetic claims will not match the ground truth, lowering evaluation validity.

## Foundational Learning

- Concept: Vector embeddings and similarity metrics
  - Why needed here: The retrieval step depends on embedding semantic chunks and measuring cosine similarity to find relevant context for the LLM.
  - Quick check question: If two documents have cosine similarity of 0.9, are they guaranteed to be semantically identical? (Answer: No; high similarity suggests strong overlap but not identity.)

- Concept: Prompt engineering and instruction tuning
  - Why needed here: Structured prompts with claim-response mapping guide the LLM to produce concise, cited summaries instead of verbose, uncited prose.
  - Quick check question: What happens if you omit the "cite source" instruction from the prompt template? (Answer: The LLM may generate uncited summaries, increasing hallucination risk.)

- Concept: RAG evaluation metrics (RAGAs)
  - Why needed here: Automated scoring of faithfulness, relevance, and correctness requires understanding of metrics like claim recognition rate and context entity recall.
  - Quick check question: If context entity recall is 98.7%, what does that imply about the retrieved context? (Answer: Nearly all ground-truth claims are present in the retrieved context, indicating high retrieval quality.)

## Architecture Onboarding

- Component map: User interface (Streamlit) -> LangChain-based RAG pipeline -> Vector database (PineCone) -> OpenAI embedding model -> LLM (GPT-3.5-turbo-1106) -> Evaluation harness (RAGAs)
- Critical path:
  1. User submits query → 2. Similarity search retrieves top-20 vectors → 3. LLM generates response using prompt template → 4. Output rendered as GitHub markdown → 5. Evaluation metrics computed
- Design tradeoffs:
  - Chunk size (120 chars) vs. semantic coherence: smaller chunks risk losing context, larger chunks risk irrelevant retrieval
  - Fixed k=20 retrieved contexts vs. adaptive retrieval: fixed reduces latency but may include redundant info
  - Local vs. cloud vector DB: local offers low latency but limited scalability; cloud offers scalability but higher latency
- Failure signatures:
  - Low claim recognition rate: retrieval step missing relevant chunks
  - High hallucination frequency: prompt template not enforcing citations or LLM ignoring context
  - Poor context relevance score: similarity metric not capturing semantic fit
- First 3 experiments:
  1. Vary chunk size (60, 120, 180 chars) and measure cosine similarity vs. claim recognition rate
  2. Swap cosine similarity for MMR (Maximal Marginal Relevance) and compare context relevance and redundancy
  3. Replace GPT-3.5 with GPT-4.0 and evaluate changes in faithfulness and correctness scores

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal chunk size and overlap configuration for maximizing retrieval accuracy while minimizing information loss in EIC-related documents?
- Basis in paper: [inferred] The paper mentions exploring chunk sizes of 120 characters with 10-character overlap, and variations of 60 and 180 characters, but states that optimization is deferred to future work.
- Why unresolved: The paper identifies this as a parameter optimization challenge but does not conduct systematic experiments to determine the optimal configuration for the specific domain of EIC documents.
- What evidence would resolve it: Empirical studies comparing retrieval accuracy and information completeness across different chunk sizes and overlap configurations using domain-specific EIC documents.

### Open Question 2
- Question: How can the RAG Agent be improved to better handle physics equations and LaTeX characters in EIC documents?
- Basis in paper: [explicit] The paper explicitly states that the RAG Agent's performance significantly decreases when dealing with questions involving physics equations and special LaTeX characters, suggesting improvements through better chunking strategies and LLM fine-tuning.
- Why unresolved: The paper acknowledges this limitation but does not provide specific solutions or test methods to enhance equation comprehension.
- What evidence would resolve it: Comparative studies of different chunking methods and LLM fine-tuning approaches on documents containing physics equations, measuring improvements in retrieval accuracy and response quality.

### Open Question 3
- Question: What is the optimal number of retrieved contexts (k) that balances context relevance and answer accuracy for the RAG Agent?
- Basis in paper: [inferred] The paper mentions that context relevancy and answer relevance strongly depend on the total number of retrieved contexts, with a fixed k=20 used in evaluations leading to redundant information.
- Why unresolved: The paper uses a fixed number of contexts for evaluation but does not explore how varying k affects the performance metrics.
- What evidence would resolve it: Systematic evaluation of the RAG Agent's performance across different values of k, analyzing the trade-offs between context relevance, answer accuracy, and redundancy.

## Limitations
- Performance metrics rely entirely on LLM-generated synthetic evaluation set rather than human-annotated ground truth
- Claims about hallucination reduction lack direct comparison to non-RAG baseline using same corpus
- RAGAs scores are internally consistent but unverified against manual annotation

## Confidence
- **High confidence**: Retrieval mechanism (cosine similarity, PineCone vector DB) and architectural components (LangChain pipeline, Streamlit UI) are clearly specified and reproducible
- **Medium confidence**: Performance metrics are valid within synthetic evaluation framework but uncertain for real-world use without human validation
- **Low confidence**: Claims about hallucination reduction and superiority over non-RAG approaches lack direct empirical support

## Next Checks
1. Human evaluation study: Have EIC domain experts manually annotate a subset of outputs for faithfulness and relevance to establish ground truth correlation with RAGAs scores
2. Ablation study: Compare RAG performance against a non-retrieval baseline (LLM-only generation from same corpus) using identical evaluation questions
3. Generalization test: Evaluate the agent on out-of-distribution EIC papers not included in vector database to assess retrieval robustness and hallucination control