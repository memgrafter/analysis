---
ver: rpa2
title: 'Audio Mamba: Bidirectional State Space Model for Audio Representation Learning'
arxiv_id: '2406.03344'
source_url: https://arxiv.org/abs/2406.03344
tags:
- audio
- mamba
- sequence
- classification
- state
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Audio Mamba (AuM), the first self-attention-free,
  purely SSM-based model for audio classification. It replaces the computationally
  intensive self-attention mechanism with bidirectional state space models (SSMs)
  to efficiently handle long audio sequences.
---

# Audio Mamba: Bidirectional State Space Model for Audio Representation Learning

## Quick Facts
- arXiv ID: 2406.03344
- Source URL: https://arxiv.org/abs/2406.03344
- Reference count: 34
- Primary result: First self-attention-free, purely SSM-based model for audio classification that achieves comparable or better performance than Audio Spectrogram Transformer (AST) while being more computationally and memory efficient

## Executive Summary
This paper introduces Audio Mamba (AuM), the first self-attention-free, purely State Space Model (SSM)-based model for audio classification. By replacing the computationally intensive self-attention mechanism with bidirectional SSMs, AuM efficiently handles long audio sequences while eliminating the quadratic complexity of attention mechanisms. The model processes audio spectrograms using bidirectional SSMs with a strategically positioned classification token in the middle of the sequence, enabling linear scaling with sequence length and achieving comparable or better performance than the well-established Audio Spectrogram Transformer (AST) across six benchmarks.

## Method Summary
Audio Mamba processes audio spectrograms by first converting them into patches (16x16), then applying patch embedding and positional encoding with a learnable classification token placed in the middle of the sequence. The core of the model consists of bidirectional State Space Models (SSMs) organized in encoder blocks. These bidirectional SSMs scan the sequence both forward and backward, maintaining state information while processing each patch sequentially. The model uses Mamba parameterization to convert time-invariant SSM parameters into time-variant ones, enabling selective state updates based on input content. The middle-positioned classification token receives balanced context from both directions, which is then processed by a classification head to produce the final prediction.

## Key Results
- On VGGSound dataset, AuM-B/16 achieves 42.58% accuracy compared to AST-B/16's 37.25%
- AuM demonstrates clear computation and memory efficiency, with AuM-B trainable with 80-second sequences while AST-B runs out of memory with 20-second sequences
- Across six benchmarks (AudioSet, VGGSound, VoxCeleb, Speech Commands V2, EPIC-Sounds), AuM achieves comparable or better performance than AST
- AuM-B/32 achieves 43.50% accuracy on VGGSound with fewer parameters than AST-B/16

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Replacing self-attention with bidirectional SSMs reduces computational complexity from O(n²) to O(n) for long audio sequences
- Mechanism: Self-attention computes pairwise interactions between all tokens, leading to quadratic scaling. SSMs process sequences sequentially with linear recurrence, maintaining state information while scanning forward and backward. The bidirectional design captures global context efficiently without computing all pairwise interactions
- Core assumption: The sequential scanning in both directions preserves enough global context for accurate audio classification
- Evidence anchors:
  - [abstract]: "AuM processes audio spectrograms using bidirectional SSMs, eliminating quadratic complexity and enabling linear scaling with sequence length"
  - [section]: "AuM demonstrates clear computation and memory efficiency... While AuM-B can be trained with 80 seconds, AST-B will run out of memory in the setting that uses only 20 seconds audios"
  - [corpus]: Weak. No direct corpus evidence comparing SSM vs attention complexity for audio tasks
- Break condition: If bidirectional scanning fails to capture sufficient global context, performance degrades compared to self-attention models

### Mechanism 2
- Claim: The middle-positioned classification token enables effective bidirectional context modeling
- Mechanism: Placing the classification token in the middle allows both forward and backward SSMs to process information from all directions before reaching the classification token. This provides balanced context from both temporal directions
- Core assumption: The classification token needs balanced context from both forward and backward directions for optimal performance
- Evidence anchors:
  - [section]: "AuM strategically positions the classification token at the midpoint of the input sequence... This setup has shown improved performance in bidirectional processing setup, as demonstrated in ViM"
  - [section]: "Our extensive experiments reveal that positioning the class token in the middle of the sequence is the most suitable choice for our design"
  - [corpus]: Weak. No corpus evidence specifically about classification token positioning in SSM models
- Break condition: If the classification token is placed at the beginning or end, the model may receive unbalanced context, leading to degraded performance

### Mechanism 3
- Claim: Time-varying SSM parameters (Mamba parameterization) enable selective state updates for efficient long-sequence modeling
- Mechanism: Traditional SSMs use time-invariant parameters, limiting their ability to selectively update states based on input content. Mamba converts these to time-variant parameters that depend on the input, allowing the model to selectively update hidden states based on relevance
- Core assumption: Selective state updates based on input content improve modeling efficiency and performance for audio classification
- Evidence anchors:
  - [section]: "Mamba upgrades existing works... by converting the time-invariant parameters into a time-variant format, enabling efficient derivation of parameters from time-varying inputs"
  - [section]: "Mamba then utilizes these parameters by adopting a modern hardware-oriented scanning method that processes the input sequence from beginning to end in a unidirectional manner"
  - [corpus]: Weak. No direct corpus evidence about Mamba parameterization for audio tasks
- Break condition: If selective state updates don't improve over time-invariant SSMs, the complexity of Mamba parameterization may not be justified

## Foundational Learning

- Concept: State Space Models (SSMs)
  - Why needed here: SSMs provide the fundamental mechanism for replacing self-attention with linear-time sequence modeling
  - Quick check question: What is the key difference between SSMs and recurrent neural networks in terms of parameter sharing and computation?

- Concept: Bidirectional processing in sequence models
  - Why needed here: Understanding how forward and backward processing complement each other in capturing global context
  - Quick check question: How does bidirectional processing differ from attention mechanisms in terms of information flow and computational complexity?

- Concept: Patch embedding and positional encoding
  - Why needed here: These are critical preprocessing steps that transform raw audio spectrograms into a format suitable for SSM processing
  - Quick check question: Why is positional encoding necessary for patch-based models like AuM, and how does it differ from the need for positional encoding in transformers?

## Architecture Onboarding

- Component map: Audio waveform → Spectrogram (F x T) → Patchifier (16x16 patches) → Patch embedding layer (D-dimensional tokens) → Classification token (middle) → Positional embeddings → Audio Mamba encoder (L bidirectional SSM blocks) → Classification head → Prediction

- Critical path: Spectrogram → Patch embedding → Bidirectional SSM blocks → Classification token → Prediction

- Design tradeoffs:
  - Bidirectional vs unidirectional SSM: Bidirectional provides better global context but doubles computation
  - Classification token position: Middle position optimal for bidirectional processing but may be less intuitive than transformer's first-position token
  - Patch size selection: Larger patches reduce sequence length but may lose fine-grained temporal information

- Failure signatures:
  - Poor performance on long sequences: May indicate insufficient bidirectional context or inadequate patch sizing
  - Memory inefficiency: Could suggest improper SSM configuration or batch sizing issues
  - Training instability: Might result from incorrect classification token positioning or learning rate issues

- First 3 experiments:
  1. Verify bidirectional vs unidirectional performance: Train two versions (forward-only vs bidirectional SSM) on a small dataset to confirm the bidirectional advantage
  2. Test classification token positioning: Train models with token at head, middle, and end positions to validate the optimal placement
  3. Evaluate patch size impact: Compare performance across different patch sizes to find the optimal balance between sequence length and feature granularity

## Open Questions the Paper Calls Out
- How does Audio Mamba perform on even longer audio sequences (e.g., 10+ minutes) compared to AST, and what are the memory and computational implications?
- How does the performance of Audio Mamba vary with different patch sizes in the spectrogram, and what is the optimal patch size for various audio classification tasks?
- How does Audio Mamba perform in self-supervised learning setups, such as masked autoencoders or contrastive learning, compared to AST?

## Limitations
- Claims about computational efficiency rely on theoretical complexity analysis rather than direct runtime measurements across different sequence lengths
- Limited analysis of how performance scales with sequence length compared to AST
- Bidirectional Mamba implementation details remain unclear, particularly how forward and backward passes are coordinated

## Confidence
- **High confidence**: Claims about memory efficiency improvements and comparable/better performance metrics on benchmarks
- **Medium confidence**: Claims about linear computational complexity scaling and the effectiveness of middle-positioned classification token
- **Low confidence**: Claims about the superiority of Mamba parameterization for audio tasks without comparison to standard SSMs

## Next Checks
1. Measure actual training/inference time across different sequence lengths (20s, 40s, 80s) for both AuM and AST to verify the claimed O(n) vs O(n²) scaling relationship
2. Implement both sequential and parallel bidirectional SSM variants to confirm that the parallel approach maintains the same performance while improving efficiency
3. Train AuM variants using standard time-invariant SSMs to determine if the Mamba parameterization provides measurable benefits for audio classification tasks