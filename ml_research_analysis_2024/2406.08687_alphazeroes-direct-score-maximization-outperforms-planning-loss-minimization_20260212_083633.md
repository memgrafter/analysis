---
ver: rpa2
title: 'AlphaZeroES: Direct score maximization outperforms planning loss minimization'
arxiv_id: '2406.08687'
source_url: https://arxiv.org/abs/2406.08687
tags:
- learning
- planning
- problem
- agent
- neural
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates whether AlphaZero's planning loss minimization
  can be outperformed by directly maximizing the episode score in single-agent environments.
  The authors propose AlphaZeroES, which uses evolution strategies to optimize the
  agent's parameters directly for episode score rather than minimizing value and policy
  losses.
---

# AlphaZeroES: Direct score maximization outperforms planning loss minimization

## Quick Facts
- arXiv ID: 2406.08687
- Source URL: https://arxiv.org/abs/2406.08687
- Authors: Carlos Martin; Tuomas Sandholm
- Reference count: 31
- Key result: Direct score maximization via evolution strategies outperforms AlphaZero's planning loss minimization in single-agent environments

## Executive Summary
This paper challenges the conventional wisdom of planning loss minimization in AlphaZero by proposing AlphaZeroES, which directly maximizes episode scores using evolution strategies. The authors demonstrate that this approach consistently outperforms AlphaZero across navigation, combinatorial optimization, and planning tasks, achieving better episode scores while showing minimal improvement in traditional value and policy losses. The results suggest that direct score optimization may be a more effective training objective for single-agent environments using Monte Carlo Tree Search.

## Method Summary
AlphaZeroES replaces AlphaZero's planning loss minimization with direct episode score maximization using evolution strategies. The method employs a DeepSets neural network architecture to process set-based state representations and uses Gumbel MuZero with a limited MCTS budget of 8 simulations. Instead of optimizing value and policy losses, AlphaZeroES directly optimizes network parameters to maximize the episode return, treating MCTS as a non-differentiable black box function.

## Key Results
- AlphaZeroES consistently achieves higher episode scores than AlphaZero across all tested domains
- Direct score optimization shows little to no improvement in value and policy losses
- The approach performs well with as few as 8 MCTS simulations, suggesting computational efficiency
- Results hold across diverse problems including navigation, Sokoban, TSP, vertex k-center, and maximum diversity

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Direct score maximization bypasses the planning loss proxy that may introduce bias
- Mechanism: AlphaZeroES optimizes agent parameters to maximize actual episode return directly using evolution strategies, rather than minimizing the planning loss (value loss + policy loss) that AlphaZero uses. The planning loss is an indirect proxy that may not perfectly correlate with the true objective of maximizing episode score.
- Core assumption: The episode score is the true objective we want to optimize, and evolution strategies can effectively optimize it despite the non-differentiable MCTS planning
- Evidence anchors:
  - [abstract] "we explore an intriguing question: In single-agent environments, can we outperform AlphaZero by directly maximizing the episode score instead of minimizing this planning loss"
  - [section 4.3] "Instead of minimizing the planning loss, we seek to maximize the episode score directly"
  - [corpus] Found 25 related papers - weak evidence, no direct citations of this work yet
- Break condition: If the relationship between planning loss and episode score is actually strong, or if evolution strategies are ineffective at optimizing the non-differentiable objective

### Mechanism 2
- Claim: AlphaZeroES's DeepSets architecture is better suited for set-based problems
- Mechanism: The paper uses DeepSets, a neural network architecture that can process sets of inputs in a way that is equivariant or invariant with respect to the inputs. This is particularly well-suited for problems like TSP, vertex k-center, and maximum diversity where the state can be naturally represented as a set of objects (cities, points, etc.).
- Core assumption: The set-based representation of problems is important for the architecture's effectiveness
- Evidence anchors:
  - [section 4.2] "we seek a neural network architecture that can process a set of vectors, rather than just a single vector"
  - [section 4.2] "In problems where the action space matches the set of inputs... the predicted action logits are read out via an affine dense layer following the permutation-equivariant layer"
- Break condition: If the problems were not naturally set-based, or if a different architecture performed better

### Mechanism 3
- Claim: AlphaZeroES can achieve similar or better performance with fewer planning steps
- Mechanism: The paper uses Gumbel MuZero, which can learn reliably with as few as 2 MCTS simulations, and experiments use an MCTS simulation budget of 8. By directly optimizing for the episode score, AlphaZeroES may be able to achieve good performance with less computational overhead from MCTS.
- Core assumption: Direct optimization can compensate for less extensive planning
- Evidence anchors:
  - [section 5] "an MCTS simulation budget of 8 (Gumbel MuZero, the AlphaZero variant we use, can learn reliably with as few as 2 simulations"
- Break condition: If extensive planning is actually necessary for good performance, or if the computational savings are not worth any potential performance trade-offs

## Foundational Learning

- Concept: Monte Carlo Tree Search (MCTS)
  - Why needed here: MCTS is the core planning algorithm used by both AlphaZero and AlphaZeroES. Understanding how it works is crucial to understanding the differences between the approaches.
  - Quick check question: What are the three main phases of each MCTS iteration?

- Concept: Evolution Strategies (ES)
  - Why needed here: ES is the black-box optimization algorithm used by AlphaZeroES to directly maximize the episode score, since MCTS is not differentiable.
  - Quick check question: How does OpenAI-ES estimate the gradient using random perturbations?

- Concept: Planning loss vs. episode score
  - Why needed here: The paper directly compares optimizing planning loss (used by AlphaZero) vs. episode score (used by AlphaZeroES). Understanding the difference and why one might be preferable is key.
  - Quick check question: What are the two components of the planning loss, and how do they relate to the episode score?

## Architecture Onboarding

- Component map:
  Environment -> DeepSets network -> MCTS algorithm -> Environment -> Episode score -> Evolution strategies optimizer

- Critical path:
  1. Environment generates initial state
  2. DeepSets network processes state and outputs action probabilities and value estimate
  3. MCTS uses network predictions to select action
  4. Environment transitions to new state based on action
  5. Episode score is computed
  6. Evolution strategies optimizer updates network parameters to maximize episode score

- Design tradeoffs:
  - Using ES instead of gradient-based optimization allows direct optimization of non-differentiable MCTS but may be less sample-efficient
  - DeepSets architecture is well-suited for set-based problems but may be overkill for simpler state representations
  - Using fewer MCTS simulations (budget of 8) trades off some planning depth for computational efficiency

- Failure signatures:
  - Poor performance on problems not naturally represented as sets
  - Difficulty optimizing with ES due to high-dimensional parameter space or noisy rewards
  - Overfitting to specific problem instances if training data is not diverse enough

- First 3 experiments:
  1. Verify that AlphaZeroES can learn on a simple navigation task where the optimal policy is known
  2. Compare performance of AlphaZeroES vs. AlphaZero on a small TSP instance
  3. Test the effect of varying the MCTS simulation budget on performance

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does AlphaZeroES perform in multi-agent environments where planning loss minimization has been successful?
- Basis in paper: [explicit] The paper explicitly states that AlphaZeroES was only tested in single-agent environments, while AlphaZero has been applied to both single-agent and multi-agent environments.
- Why unresolved: The authors chose to focus on single-agent environments and did not test AlphaZeroES in multi-agent settings.
- What evidence would resolve it: Experimental results comparing AlphaZeroES to AlphaZero in multi-agent environments like chess and Go would determine if direct score maximization can outperform planning loss minimization in these domains.

### Open Question 2
- Question: What is the impact of different MCTS algorithms on the performance of AlphaZeroES compared to AlphaZero?
- Basis in paper: [inferred] The paper mentions using Gumbel MuZero as the MCTS algorithm but does not explore other MCTS variants or their impact on the performance of both approaches.
- Why unresolved: The authors used a specific MCTS implementation (Gumbel MuZero) and did not investigate how other MCTS algorithms might affect the comparison between AlphaZeroES and AlphaZero.
- What evidence would resolve it: Comparing the performance of AlphaZeroES and AlphaZero using different MCTS algorithms (e.g., standard MCTS, UCT, etc.) across various environments would reveal the impact of MCTS choice on their relative performance.

### Open Question 3
- Question: How does the performance of AlphaZeroES scale with the complexity of the environment or the size of the state space?
- Basis in paper: [inferred] The paper presents results on a range of environments but does not explicitly analyze how the performance of AlphaZeroES scales with increasing complexity or state space size.
- Why unresolved: The authors tested AlphaZeroES on various environments but did not investigate the relationship between environment complexity, state space size, and the performance gap between AlphaZeroES and AlphaZero.
- What evidence would resolve it: Conducting experiments with progressively more complex environments or larger state spaces, and measuring the performance difference between AlphaZeroES and AlphaZero, would reveal how their relative performance scales with environmental complexity.

## Limitations

- Results are confined to single-agent environments - findings may not extend to multi-agent or competitive settings
- Evolution strategies approach may not scale well to very high-dimensional parameter spaces
- Performance gains could be partially attributed to architectural choices (DeepSets) rather than direct score optimization itself

## Confidence

Medium confidence. The experimental results consistently show AlphaZeroES outperforming AlphaZero across multiple domains, but the sample size of 25 related papers with zero citations suggests this work may be relatively new and untested in the broader research community. The comparison is limited to specific problem types that may favor the DeepSets architecture.

## Next Checks

1. Test AlphaZeroES on multi-agent environments to verify if direct score optimization remains effective when planning loss becomes more relevant
2. Compare against alternative architectures (non-set-based) to isolate the contribution of direct optimization vs. DeepSets architecture
3. Conduct ablation studies varying MCTS budget and ES hyperparameters to understand their interaction effects on performance