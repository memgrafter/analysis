---
ver: rpa2
title: '"Let''s Agree to Disagree": Investigating the Disagreement Problem in Explainable
  AI for Text Summarization'
arxiv_id: '2410.18560'
source_url: https://arxiv.org/abs/2410.18560
tags:
- agreement
- scores
- rank
- xsum
- across
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates the disagreement problem in explainable
  AI (XAI) for text summarization, where different XAI methods yield conflicting explanations.
  The authors propose Regional Explainable AI (RXAI), a segmentation-based approach
  that divides articles into coherent segments using sentence transformers and clustering.
---

# "Let's Agree to Disagree": Investigating the Disagreement Problem in Explainable AI for Text Summarization

## Quick Facts
- arXiv ID: 2410.18560
- Source URL: https://arxiv.org/abs/2410.18560
- Reference count: 40
- Key outcome: RXAI segmentation approach significantly reduces disagreement between XAI methods for text summarization (e.g., attention vs. DeepLIFT reaching ~0.85 agreement on XSum for k=4)

## Executive Summary
This paper addresses the critical problem of disagreement among different XAI methods in text summarization, where methods like attention, DeepLIFT, LIME, and Gradient SHAP often produce conflicting explanations for the same model outputs. The authors propose Regional Explainable AI (RXAI), a segmentation-based approach that divides articles into semantically coherent segments using sentence transformers and k-means clustering. This localized analysis reduces the complexity and contextual noise that causes XAI methods to disagree, with evaluation showing significant improvements in feature agreement scores and statistically significant reductions in disagreement across benchmark datasets.

## Method Summary
The study employs a two-phase approach: first, global analysis of full articles using four XAI methods (attention, DeepLIFT, LIME, Gradient SHAP) to establish baseline disagreement metrics including Feature Agreement, Rank Agreement, and Semantic Alignment Score (SAS). Second, the RXAI framework segments articles using sentence transformer embeddings and k-means clustering (with optimal k determined by Silhouette score), computes segment-level attributions, and aggregates them to article-level for comparison. The framework also includes an interactive JavaScript visualization tool for exploring sentence-level attributions. Statistical validation uses Wilcoxon signed-rank tests to confirm significant improvements.

## Key Results
- Feature agreement between attention and DeepLIFT methods reaches ~0.85 on XSum dataset for k=4 segments
- RXAI approach achieves statistically significant reduction in disagreement (p<0.05) across most method pairs
- Computational overhead increases by 34-45.7% with RXAI, from 29 to 39 hours for XSum dataset analysis
- LIME-based method pairs consistently show lowest agreement scores, indicating fundamental methodological differences

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Segmentation reduces disagreement by creating semantically coherent text regions that align better with XAI attribution methods
- Mechanism: The Regional Explainable AI (RXAI) framework uses sentence transformers and k-means clustering to divide articles into smaller, coherent segments. This localized analysis focuses each XAI method on semantically consistent text regions rather than entire documents, reducing the noise and contextual complexity that causes conflicting explanations.
- Core assumption: Sentence embeddings capture sufficient semantic information to create coherent segments that preserve meaningful context while reducing complexity
- Evidence anchors:
  - [abstract] "we propose Regional Explainable AI (RXAI) a novel segmentation-based approach, where each article is divided into smaller, coherent segments using sentence transformers and clustering"
  - [section] "To tackle the disagreement problem among XAI methods we employed the RXAI approach, hypothesizing that segmenting the input article into semantically coherent segments would reduce the level of disagreement between explanations"
- Break condition: If clustering fails to create semantically coherent segments, or if segmentation destroys contextual relationships critical for accurate attribution, disagreement may increase rather than decrease

### Mechanism 2
- Claim: Different XAI methods have fundamentally different attribution generation approaches that lead to disagreement on full-text analysis
- Mechanism: Attention methods focus on global token importance, DeepLIFT compares activations to reference states, LIME uses perturbation-based local approximation, and Gradient SHAP combines SHAP values with integrated gradients. These distinct mathematical foundations produce different feature importance rankings when applied to entire documents with complex contextual relationships.
- Core assumption: The mathematical foundations of each XAI method are sufficiently different that they will produce divergent results on complex, context-rich text
- Evidence anchors:
  - [section] "attention vs. DeepLIFT consistently shows the highest feature agreement for the Xsum dataset, which indicates that the two methods mostly highlight the same top features"
  - [section] "LIME-based method pairs exhibited lowest agreement scores" suggesting fundamental methodological differences
- Break condition: If all XAI methods were based on similar mathematical principles, or if text complexity was artificially reduced, the disagreement might be less pronounced

### Mechanism 3
- Claim: Semantic alignment scores reveal that disagreement patterns differ between syntactic and semantic understanding of explanations
- Mechanism: The proposed Semantic Alignment Score (SAS) uses cosine similarity between attribution-weighted sentence embeddings to measure semantic agreement between XAI methods, capturing whether methods agree on the meaning conveyed rather than just the specific tokens they highlight.
- Core assumption: Sentence embeddings can capture semantic relationships that transcend surface-level token differences between XAI explanations
- Evidence anchors:
  - [section] "we introduce a novel semantic similarity-based metric SAS that quantifies disagreement at the semantic level by comparing attribution-weighted sentence embeddings using cosine similarity"
  - [section] "SAS scores remain relatively unchanged after applying RXAI approach" suggesting semantic understanding is more stable than syntactic agreement
- Break condition: If sentence embeddings fail to capture domain-specific semantics or if attribution weighting distorts semantic relationships, SAS may not accurately reflect true semantic agreement

## Foundational Learning

- Concept: K-means clustering with silhouette score optimization
  - Why needed here: RXAI framework uses k-means clustering to segment articles, with silhouette score determining optimal k-values to ensure coherent segments
  - Quick check question: What does a high silhouette score indicate about cluster quality, and why is this important for creating coherent text segments?

- Concept: Cosine similarity between sentence embeddings
  - Why needed here: SAS metric relies on cosine similarity between attribution-weighted sentence embeddings to measure semantic alignment between XAI explanations
  - Quick check question: How does cosine similarity between embeddings capture semantic similarity, and why is this preferred over simple token overlap for measuring explanation agreement?

- Concept: Feature importance and ranking metrics in XAI
  - Why needed here: The study uses multiple metrics (Feature Agreement, Rank Agreement, Spearman Rank Correlation) to quantify disagreement between XAI methods at different levels of analysis
  - Quick check question: What's the difference between feature agreement and rank agreement, and why might methods agree on important features but disagree on their relative ranking?

## Architecture Onboarding

- Component map: Data preprocessing → Text segmentation (Sentence Transformers + k-means) → XAI attribution generation (LIME, Gradient SHAP, Attention, DeepLIFT) → Agreement metric computation (Feature, Rank, SAS) → Visualization (JavaScript tool)
- Critical path: Input articles → Segmentation → Attribution generation → Agreement computation → Results visualization
- Design tradeoffs: Segmentation improves agreement but increases computational cost (39 hours for XSum vs 29 hours for global analysis); sentence-level attribution provides granularity but requires careful preprocessing
- Failure signatures: High variability in agreement scores across batches suggests sampling bias; low SAS scores indicate semantic misalignment; computational bottlenecks occur with LIME on segmented data
- First 3 experiments:
  1. Test segmentation quality by comparing semantic similarity between original articles and concatenated segments using pre-trained sentence transformers
  2. Validate agreement improvement by running Wilcoxon signed-rank test on global vs regional scores for each method pair
  3. Measure coherence preservation by comparing coherence scores of full articles versus segmented versions using the Coherence Momentum model

## Open Questions the Paper Calls Out

- Question: How can Large Language Models (LLMs) be effectively utilized as mediators to aggregate explanations from different XAI methods into more consistent explanations?
  - Basis in paper: [explicit] The paper discusses potential future directions including using LLMs as mediators to aggregate explanations of different XAI methods into more consistent explanations.
  - Why unresolved: While the paper identifies this as a promising direction, it does not explore or demonstrate how LLMs could be implemented for this purpose, leaving the methodology and effectiveness unexplored.
  - What evidence would resolve it: A study demonstrating LLM-based aggregation of XAI explanations, showing improved consistency metrics compared to individual XAI methods, with ablation studies isolating LLM contribution.

- Question: What is the root cause of disagreement among different XAI methods in text summarization?
  - Basis in paper: [explicit] The paper acknowledges that it demonstrates the existence of XAI disagreement but does not investigate the root cause of why these methods disagree.
  - Why unresolved: The paper focuses on measuring and mitigating disagreement through segmentation but explicitly states that understanding why methods disagree remains an open research question.
  - What evidence would resolve it: An analysis comparing the theoretical foundations and computational approaches of different XAI methods to identify systematic sources of disagreement, potentially through controlled experiments varying model architecture or input characteristics.

- Question: How can the computational overhead of the RXAI framework be optimized for time-sensitive applications?
  - Basis in paper: [explicit] The paper notes that RXAI introduces significant computational overhead (34-45.7% increase) and mentions this as a limitation for time-sensitive scenarios.
  - Why unresolved: While the paper acknowledges the computational cost as a limitation, it does not propose or evaluate optimization strategies to address this issue.
  - What evidence would resolve it: Performance benchmarks comparing the original RXAI implementation with optimized versions using techniques like parallel processing, more efficient clustering algorithms, or approximate methods that maintain explanation quality while reducing computation time.

## Limitations

- Computational overhead significantly increases analysis time (34-45.7% increase, from 29 to 39 hours for XSum dataset)
- The study does not fully explore whether segmentation preserves semantic coherence of original articles
- Generalization of RXAI beyond text summarization to other NLP tasks remains untested

## Confidence

**High Confidence**: The core finding that segmentation reduces disagreement between XAI methods is well-supported by statistical significance (p<0.05 across most method pairs) and substantial improvements in feature agreement scores. The mechanism by which semantic segmentation reduces contextual complexity is logically sound.

**Medium Confidence**: The Semantic Alignment Score (SAS) metric shows promise in capturing semantic agreement beyond surface-level token matching, but the claim that SAS scores remain "relatively unchanged" after RXAI application needs more rigorous validation. The stability of semantic understanding versus syntactic agreement is plausible but requires deeper investigation across different domains.

**Low Confidence**: The generalizability of RXAI beyond text summarization to other NLP tasks remains untested. While the framework shows strong results on XSum and CNN/DM datasets, different text structures and summarization approaches may yield different agreement patterns that segmentation alone cannot resolve.

## Next Checks

1. **Segmentation Coherence Validation**: Compute coherence scores and semantic similarity between original articles and their segmented versions using the Coherence Momentum model and sentence transformer embeddings. This would quantify whether segmentation preserves semantic integrity and identify failure modes where clustering creates incoherent segments.

2. **Cross-Dataset Generalization Test**: Apply RXAI to a third summarization dataset with different characteristics (e.g., Newsroom or Reddit TIFU) and measure whether the observed agreement improvements (~0.85 for attention vs DeepLIFT on XSum) hold across diverse text structures and domains.

3. **Alternative Clustering Approach Comparison**: Replace k-means with hierarchical clustering or DBSCAN and compare agreement improvements and computational costs. This would validate whether k-means is optimal for RXAI or if alternative clustering methods could achieve similar or better results with reduced computational overhead.