---
ver: rpa2
title: Improving Sequential Recommender Systems with Online and In-store User Behavior
arxiv_id: '2412.02122'
source_url: https://arxiv.org/abs/2412.02122
tags:
- online
- data
- in-store
- user
- behavior
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper studies the problem of integrating online and in-store
  user behavior data into sequential recommender systems. The authors propose a hybrid
  data pipeline to unify these data sources and introduce a model-agnostic store transaction
  encoder based on self-attention to enable transformer-based models to process heterogeneous
  behavior sequences.
---

# Improving Sequential Recommender Systems with Online and In-store User Behavior

## Quick Facts
- **arXiv ID:** 2412.02122
- **Source URL:** https://arxiv.org/abs/2412.02122
- **Reference count:** 21
- **Primary result:** Hybrid data pipeline and attention-based encoder improves sequential recommendations by 4.29% HitRate@10 and 7.50% NDCG@10

## Executive Summary
This paper addresses the challenge of integrating online and in-store user behavior data into sequential recommender systems. The authors propose a hybrid data pipeline to unify these heterogeneous data sources and introduce a model-agnostic store transaction encoder based on self-attention. Their approach enables transformer-based models to process sequences containing both online interactions and in-store purchase sets, achieving significant performance improvements over baselines that use only online data.

## Method Summary
The method involves a two-pronged approach: first, implementing a hybrid data pipeline that combines streaming online behavior with batched in-store transactions through a feature registry and stores system; second, developing a store transaction encoder using self-attention to convert sets of simultaneously purchased in-store items into single embeddings. This encoder is integrated with a transformer-based sequential recommender (SASRec) to create a unified model that can process hybrid user behavior sequences. The system uses negative sampling and cross-entropy loss for training, with evaluation based on HitRate@10 and NDCG@10 metrics.

## Key Results
- Attention-based encoder outperforms simpler averaging approaches for processing in-store item sets
- Incorporating in-store behavior improves recommendation performance by 4.29% HitRate@10 and 7.50% NDCG@10
- SASRecattnE achieves best performance among evaluated models on real-world e-commerce dataset

## Why This Works (Mechanism)

### Mechanism 1
The self-attention-based encoder successfully aggregates in-store item sets into single embeddings without losing permutation invariance. Self-attention computes similarity weights between items in the same set, then normalizes and aggregates them into a single weighted sum. This preserves permutation invariance since attention scores depend only on relative item similarities, not their order.

### Mechanism 2
The hybrid data pipeline successfully unifies online streaming and in-store batch data with different schemas and update frequencies. Online data processed via streaming (Spark/Flink) with sliding windows, in-store data loaded in batches. Feature registry manages schema differences and ensures consistent feature definitions across online/offline environments.

### Mechanism 3
Adding in-store behavior improves sequential recommendation performance for future online interactions. In-store purchases reflect fulfillment of shopping interests that may have been initiated online, providing additional signal for predicting next online behavior.

## Foundational Learning

- **Transformer architecture and self-attention mechanism**: Understanding how standard transformers work and why they fail with item sets at same timestamp
  - Why needed here: Understanding how standard transformers work and why they fail with item sets at same timestamp
  - Quick check question: How does self-attention handle variable-length input sequences, and why can't standard transformers process multiple items at one timestamp?

- **Feature engineering and data pipeline design**: Building hybrid pipeline requires understanding streaming vs batch processing, schema management, and feature registry patterns
  - Why needed here: Building hybrid pipeline requires understanding streaming vs batch processing, schema management, and feature registry patterns
  - Quick check question: What are the key differences between online streaming and batch data processing, and how do you ensure consistent feature definitions?

- **Sequential recommendation metrics (HitRate@N, NDCG@N)**: Evaluating recommendation performance requires understanding these ranking-based metrics
  - Why needed here: Evaluating recommendation performance requires understanding these ranking-based metrics
  - Quick check question: How do HitRate@10 and NDCG@10 differ in what they measure about recommendation quality?

## Architecture Onboarding

- **Component map:** Data Ingestion Layer (online streaming + batch in-store) → Streaming Processor (Spark/Flink with sliding windows) → Feature Registry (schema management, versioning) → Feature Stores (online cache + offline storage) → Model Components (SASRec backbone + store transaction encoder) → Training/Inference Pipeline (negative sampling, cross-entropy loss)

- **Critical path:** Data Ingestion → Streaming Processor → Feature Registry → Feature Stores → Model Training → Evaluation

- **Design tradeoffs:**
  - Encoder complexity: attention vs average pooling (SASRecattnE vs SASRecavgE)
  - Window size: longer windows capture more history but increase latency
  - Feature freshness: real-time vs batched in-store data affects model responsiveness

- **Failure signatures:**
  - Poor performance on HitRate@N/NDCG@N: check data alignment, encoder functionality, or feature quality
  - High inference latency: check streaming window size or feature store access patterns
  - Training-serving skew: verify feature registry consistency between training and inference

- **First 3 experiments:**
  1. Baseline SASRec on pure online data only
  2. SASRecw/store on hybrid data without encoder (simple concatenation)
  3. SASRecavgE on hybrid data with average pooling encoder

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions but raises several implicit ones regarding the generalizability and limitations of their approach.

## Limitations
- No detailed implementation specifications for transformer architecture and hyperparameters
- Lack of explicit validation for correlation assumption between online and in-store behaviors
- Missing ablation studies comparing attention encoder to other permutation-invariant alternatives

## Confidence
- **High confidence:** Hybrid data pipeline architecture and components are well-specified and technically sound
- **Medium confidence:** Attention-based encoder improves over simple averaging, though magnitude needs verification
- **Low confidence:** Claim that in-store behavior specifically improves online behavior prediction requires causal analysis

## Next Checks
1. **Encoder Ablation:** Compare self-attention encoder against alternative permutation-invariant methods (set transformers, DeepSets) on same dataset
2. **Correlation Analysis:** Conduct statistical analysis measuring actual correlation strength between in-store and subsequent online behaviors
3. **Data Volume Control:** Run experiments controlling for total training examples to determine if gains come from behavioral complementarity or more data