---
ver: rpa2
title: 'Beyond the Black Box: Do More Complex Deep Learning Models Provide Superior
  XAI Explanations?'
arxiv_id: '2405.08658'
source_url: https://arxiv.org/abs/2405.08658
tags: []
core_contribution: This study examines the relationship between deep learning model
  complexity and the quality of Explainable AI (XAI) explanations in medical image
  classification. Four ResNet architectures (ResNet-18, 34, 50, 101) were trained
  and evaluated on 4,369 lung X-ray images for COVID-19 detection.
---

# Beyond the Black Box: Do More Complex Deep Learning Models Provide Superior XAI Explanations?

## Quick Facts
- arXiv ID: 2405.08658
- Source URL: https://arxiv.org/abs/2405.08658
- Reference count: 40
- Four ResNet architectures compared for COVID-19 detection from lung X-rays; simpler models performed as well as complex ones

## Executive Summary
This study challenges the common assumption that more complex deep learning models provide better explanations in medical imaging. The researchers evaluated four ResNet architectures (ResNet-18, 34, 50, 101) for COVID-19 detection from lung X-rays and found that increased model complexity did not lead to improved classification performance or better XAI explanations. Surprisingly, the simplest model (ResNet-18) achieved the highest accuracy (98.4%) and AUC-ROC (0.997). Statistical analysis revealed no significant differences in XAI metrics across all ResNet models, suggesting that simpler models may be preferable for medical imaging applications where both performance and interpretability are crucial.

## Method Summary
The researchers trained four ResNet architectures on 4,369 lung X-ray images for COVID-19 detection, evaluating both classification performance and XAI explanation quality. They used two XAI metrics - Relevance Rank Accuracy and Positive Attribution Ratio - to assess explanation quality across models. Statistical analysis compared the performance and XAI metrics between different ResNet architectures to determine if increased complexity provided any benefits. The study focused on the relationship between model complexity and explanation quality, testing the hypothesis that more complex models would provide superior explanations.

## Key Results
- ResNet-18 achieved the highest accuracy (98.4%) and AUC-ROC (0.997) despite being the simplest model
- No significant differences found in XAI metrics (Relevance Rank Accuracy and Positive Attribution Ratio) across all ResNet models
- Increased model complexity did not lead to improved classification performance or better XAI explanations

## Why This Works (Mechanism)
Assumption: The results suggest that model complexity does not inherently improve either the accuracy of predictions or the quality of explanations in medical imaging tasks. This may be because simpler models can focus on the most relevant features without being distracted by spurious correlations that more complex models might learn.

## Foundational Learning
- **XAI Metrics**: Standardized measures to evaluate explanation quality (why needed: to quantify interpretability objectively; quick check: consistent metric definitions across studies)
- **Model Complexity**: Architectural depth and parameter count in neural networks (why needed: fundamental concept in DL; quick check: parameter count and layer structure)
- **Statistical Significance**: Methods to determine if differences are meaningful (why needed: ensures results aren't due to chance; quick check: p-values and confidence intervals)
- **Medical Image Classification**: Applying DL to healthcare imaging tasks (why needed: domain-specific application; quick check: dataset characteristics and task definition)
- **AUC-ROC**: Area under the Receiver Operating Characteristic curve (why needed: standard performance metric; quick check: threshold independence)

## Architecture Onboarding
- **Component Map**: Input X-ray -> ResNet-18/34/50/101 -> Classification Output -> XAI Explanation
- **Critical Path**: X-ray preprocessing -> Model inference -> Performance metrics calculation -> XAI metric evaluation
- **Design Tradeoffs**: Simplicity vs. theoretical capacity, computational efficiency vs. potential performance gains
- **Failure Signatures**: Overfitting with complex models, computational inefficiency without performance gains
- **First Experiments**: 1) Compare parameter counts across ResNet variants, 2) Evaluate inference time differences, 3) Test explanation consistency across random seeds

## Open Questions the Paper Calls Out
Unknown: The paper does not explicitly state open questions, but implicit questions include: Would these findings hold for other medical imaging tasks? Do different XAI methods yield similar results? How do ensemble methods compare to single models in terms of explanation quality?

## Limitations
- Analysis limited to single medical imaging task (COVID-19 detection from lung X-rays)
- Only two XAI metrics used, potentially missing other aspects of explanation quality
- Limited generalizability due to single dataset and specific model architectures tested

## Confidence
- Model complexity does not improve performance: High confidence
- Model complexity does not improve XAI explanations: Medium confidence (limited to specific metrics)
- Simpler models are preferable for medical imaging: Medium confidence (domain-specific finding)

## Next Checks
1. Replicate the analysis across multiple medical imaging datasets and different XAI techniques (e.g., SHAP, LIME) to assess generalizability
2. Conduct user studies with medical professionals to evaluate the practical interpretability and clinical utility of explanations from different model complexities
3. Test whether the findings hold for more complex architectures beyond ResNet (e.g., vision transformers, ensemble methods) and non-medical image classification tasks