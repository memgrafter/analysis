---
ver: rpa2
title: A RAG Approach for Generating Competency Questions in Ontology Engineering
arxiv_id: '2409.08820'
source_url: https://arxiv.org/abs/2409.08820
tags:
- ontology
- knowledge
- domain
- llms
- engineering
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a retrieval-augmented generation (RAG) approach
  using large language models (LLMs) to automatically generate competency questions
  (CQs) for ontology engineering tasks. Unlike previous work that relies on existing
  ontologies, this method uses scientific papers as a domain knowledge base.
---

# A RAG Approach for Generating Competency Questions in Ontology Engineering

## Quick Facts
- arXiv ID: 2409.08820
- Source URL: https://arxiv.org/abs/2409.08820
- Reference count: 19
- This paper introduces a RAG approach using scientific papers as knowledge bases to generate competency questions for ontology engineering, showing improved performance over zero-shot prompting for concrete ontologies.

## Executive Summary
This paper presents a retrieval-augmented generation (RAG) approach for automatically generating competency questions (CQs) in ontology engineering using large language models (LLMs) and scientific papers as domain knowledge. Unlike previous methods that rely on existing ontologies, this approach constructs knowledge bases from domain-specific research papers to guide CQ generation. The method was tested on two ontology engineering tasks: constructing a knowledge graph of empirical research in requirement engineering (KG-EmpiRE) and a core reference ontology in human-computer interaction (HCIO). Experiments using GPT-4 demonstrated that the RAG approach outperformed zero-shot prompting for concrete ontologies requiring extensive domain knowledge, with precision as the primary evaluation metric.

## Method Summary
The method employs a RAG pipeline where scientific papers are chunked, embedded, and stored in a vector database as a knowledge base. For a given ontology engineering task, relevant papers are retrieved based on similarity to task descriptions, and an LLM (GPT-4) generates competency questions using a zero-shot prompt template with four variables: domain, ontology purpose, CQ definition, and number of CQs. The generated CQs are evaluated against ground truth using cosine similarity with a threshold of 0.6 to calculate precision. The approach varies the number of papers in the knowledge base and temperature settings to study their effects on performance.

## Key Results
- RAG approach outperformed zero-shot prompting for concrete ontologies requiring domain knowledge, with best precision achieved using only a visionary paper for KG-EmpiRE
- Increasing the number of papers in the knowledge base generally improved precision for concrete ontologies
- Temperature settings (0.5-1.5) had no significant effect on CQ generation consistency or quality

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Retrieval-augmented generation with scientific papers improves CQ generation for concrete ontologies compared to zero-shot prompting.
- Mechanism: RAG provides LLMs with domain-specific knowledge that enhances their understanding of ontology requirements, particularly when the target ontology is concrete and domain-heavy.
- Core assumption: The selected scientific papers contain relevant, structured knowledge that directly supports the ontology's purpose and scope.
- Evidence anchors:
  - [abstract] "compared to zero-shot prompting, adding relevant domain knowledge to the RAG improves the performance of LLMs on generating CQs for concrete ontology engineering tasks"
  - [section] "the best precision is achieved when we use the only visionary paper as input to the knowledge base in the RAG pipeline"
- Break condition: If selected papers lack coverage of key ontology concepts or are too abstract for the target domain.

### Mechanism 2
- Claim: Increasing the number of papers in the RAG knowledge base generally improves precision for concrete ontologies.
- Mechanism: More papers provide broader coverage of domain concepts, enabling LLMs to generate more comprehensive and relevant CQs.
- Core assumption: Papers in the knowledge base are diverse enough to cover different aspects of the domain without introducing conflicting information.
- Evidence anchors:
  - [section] "increasing the number of papers Npaper in the RAG pipeline generally improves the precision of generating CQs for domain ontologies/KGs"
  - [section] "the best precision is achieved when we use the only visionary paper as input" (suggesting quality over quantity)
- Break condition: When additional papers introduce noise or redundancy that degrades retrieval quality.

### Mechanism 3
- Claim: Temperature settings do not significantly affect CQ generation consistency or quality.
- Mechanism: The task of generating CQs from retrieved documents is sufficiently constrained that randomness in token sampling has minimal impact.
- Core assumption: The retrieved documents provide clear context that guides the LLM toward similar outputs regardless of temperature.
- Evidence anchors:
  - [section] "there are no obvious patterns about how consistency changes by different temperatures regardless of different consistency metrics"
  - [section] "the temperature setting would not affect the task performance of generating CQs in our RAG pipeline"
- Break condition: If task becomes more open-ended or when working with highly abstract ontologies.

## Foundational Learning

- Concept: RAG (Retrieval-Augmented Generation)
  - Why needed here: The paper's core innovation relies on combining document retrieval with LLM generation to improve CQ quality.
  - Quick check question: What are the three main steps in a RAG pipeline and what happens in each?

- Concept: Competency Questions (CQs)
  - Why needed here: CQs are the target output and central evaluation metric for ontology engineering quality.
  - Quick check question: How are informal CQs defined in ontology engineering and what distinguishes them from formal CQs?

- Concept: Cosine similarity for text matching
  - Why needed here: Used to determine if generated CQs match ground truth CQs in evaluation.
  - Quick check question: What threshold value was used to determine if a generated CQ is valid?

## Architecture Onboarding

- Component map: Scientific papers → Document chunking and embedding → Vector database → Retrieval → Prompt engineering → LLM generation → Evaluation
- Critical path: Document selection → Knowledge base construction → Retrieval → Prompt → Generation → Evaluation
- Design tradeoffs: Quality vs. quantity of papers (one visionary paper performed best vs. more papers generally better), token cost vs. performance
- Failure signatures: Low precision when papers are too abstract for concrete ontologies, high variance in outputs when papers are poorly selected
- First 3 experiments:
  1. Test RAG vs. zero-shot prompting on a simple ontology using 1-3 carefully selected papers
  2. Vary Npaper from 1 to 5 while keeping other parameters constant to measure precision impact
  3. Test different temperature settings (0.5, 1.0, 1.5) to verify temperature has no significant effect on this task

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the selection of scientific papers for the knowledge base impact the quality and relevance of generated competency questions?
- Basis in paper: [explicit] The paper discusses the importance of selecting relevant papers for the knowledge base and identifies three principles for selection, but does not systematically investigate how different selection strategies affect CQ quality.
- Why unresolved: The paper only mentions selection principles but does not compare different paper selection methods or evaluate their impact on CQ generation performance.
- What evidence would resolve it: Experimental results comparing CQ quality when using different paper selection strategies (e.g., citation-based vs. keyword-based selection, or papers from different time periods).

### Open Question 2
- Question: Can open-source LLMs achieve comparable performance to GPT-4o for competency question generation while being more cost-effective?
- Basis in paper: [explicit] The authors mention that token costs are a concern with closed-source models like GPT-4o and suggest exploring open-source alternatives like Meta's Llama and Google's Gemma.
- Why unresolved: The paper only uses GPT-4o for experiments and does not evaluate open-source models, despite acknowledging their potential for cost reduction.
- What evidence would resolve it: Comparative experiments using open-source LLMs with the same RAG approach to measure precision, consistency, and token costs.

### Open Question 3
- Question: What is the optimal number of competency questions to generate for different types of ontologies or knowledge graphs?
- Basis in paper: [inferred] The paper uses a fixed number of CQs matching ground truth (77 for KG-EmpiRE, 15 for HCIO) but does not investigate how varying this number affects utility or evaluation effectiveness.
- Why unresolved: The paper follows the ground truth CQ count but does not explore whether this is optimal for different ontology engineering scenarios or evaluation purposes.
- What evidence would resolve it: Experiments varying the number of generated CQs and measuring their utility in ontology evaluation or design processes, potentially using human expert feedback.

### Open Question 4
- Question: How do different embedding models and similarity metrics affect the precision of competency question generation in the RAG approach?
- Basis in paper: [explicit] The paper uses SentenceBERT for embedding and cosine similarity for matching, but acknowledges that RAG systems face challenges with knowledge base content.
- Why unresolved: The paper does not compare alternative embedding models or similarity metrics, nor does it investigate how these choices impact CQ generation quality.
- What evidence would resolve it: Comparative experiments using different embedding models (e.g., different versions of SentenceBERT, other transformer-based models) and similarity metrics (e.g., Euclidean distance, dot product) to measure their impact on CQ precision.

## Limitations

- Unknown PDF processing and chunking methodology for scientific papers, which could significantly affect retrieval quality
- Evaluation relies solely on precision without considering recall or F1-score, potentially missing cases where CQs were omitted but not counted as errors
- The abstract/concrete ontology distinction that drives performance differences is not precisely defined, making it difficult to predict when the RAG approach will outperform zero-shot prompting

## Confidence

- High confidence: The RAG approach generally outperforms zero-shot prompting for concrete ontologies requiring domain knowledge, and the four-variable prompt structure effectively guides LLM output
- Medium confidence: The finding that temperature has no significant effect on CQ generation quality and consistency, as this could be task-specific
- Medium confidence: The optimal balance between knowledge base size and precision, given the conflicting evidence about whether more papers always improves performance

## Next Checks

1. Conduct ablation studies varying the number of papers from 1 to 10 while keeping all other parameters constant to precisely measure the impact on precision for both concrete and abstract ontologies
2. Test the RAG pipeline with additional temperature values (0.3, 0.7, 1.2, 1.7) and larger sample sizes to verify the temperature independence claim
3. Apply the approach to a new ontology engineering task with a different domain to test generalizability beyond the KG-EmpiRE and HCIO examples used in the study