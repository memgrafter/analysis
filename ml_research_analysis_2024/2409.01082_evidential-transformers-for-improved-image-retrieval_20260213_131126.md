---
ver: rpa2
title: Evidential Transformers for Improved Image Retrieval
arxiv_id: '2409.01082'
source_url: https://arxiv.org/abs/2409.01082
tags:
- image
- retrieval
- evidential
- vision
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of improving image retrieval by
  incorporating uncertainty quantification into transformer models. The core method
  introduces Evidential Transformers, which use evidential deep learning to produce
  Dirichlet distribution parameters instead of point predictions, allowing the model
  to reason about prediction uncertainty.
---

# Evidential Transformers for Improved Image Retrieval

## Quick Facts
- arXiv ID: 2409.01082
- Source URL: https://arxiv.org/abs/2409.01082
- Authors: Danilo Dordevic; Suryansh Kumar
- Reference count: 40
- Primary result: Achieves state-of-the-art Recall@1 of 80.79% on CUB-200-2011 using uncertainty-driven reranking

## Executive Summary
This paper introduces Evidential Transformers for image retrieval, a novel approach that incorporates uncertainty quantification into transformer models using evidential deep learning. The method outputs Dirichlet distribution parameters instead of point predictions, enabling the model to reason about prediction uncertainty and improve retrieval quality, reliability, and interpretability. The approach demonstrates state-of-the-art performance on the CUB-200-2011 dataset and establishes evidential classification as a strong baseline for deep metric learning.

## Method Summary
The method employs Global Context Vision Transformers (GC ViT) with evidential deep learning to produce Dirichlet distribution parameters (α) representing uncertainty-aware predictions. Four retrieval paradigms are explored: evidential classification as a deep metric learning baseline, using Dirichlet parameters as neural codes (α-embeddings), uncertainty-driven reranking of top-N results, and distribution-based distance comparisons using Bhattacharyya distance. The evidential loss function encourages both accurate predictions and low-variance outputs, while the α-parameters capture both aleatoric and epistemic uncertainty.

## Key Results
- Evidential classification achieves 80.22% Recall@1 on CUB-200-2011
- Uncertainty-driven reranking method reaches 80.79% Recall@1
- GC ViT architecture significantly outperforms DeiT-S baseline
- Demonstrates superior separation between positive and negative pairs compared to standard approaches

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Evidential transformers improve image retrieval by modeling uncertainty in Dirichlet distribution parameters rather than point predictions
- Mechanism: The model outputs parameters (α) of a Dirichlet distribution over class probabilities, capturing both aleatoric uncertainty (inherent data variability) and epistemic uncertainty (model uncertainty). This allows the system to downrank uncertain retrievals.
- Core assumption: Uncertainty in predictions is useful for ranking and filtering retrieval results
- Evidence anchors:
  - [abstract] "we propose an evidential learning-driven approach to the transformer model for image retrieval"
  - [section] "Such uncertainty can arise from out-of-distribution samples in the dataset, or from images that have embeddings similar to the query but belong to a different class"
  - [corpus] Weak - no direct corpus evidence for this specific uncertainty-quantification mechanism
- Break condition: If uncertainty estimates do not correlate with retrieval quality, or if computational overhead outweighs benefits

### Mechanism 2
- Claim: Using Dirichlet distribution parameters (α-embeddings) as neural codes improves retrieval performance
- Mechanism: Instead of using softmax probabilities or CLS token embeddings, the α parameters themselves serve as image representations. These are compared using L2 distance, treating each image as a distribution in embedding space.
- Core assumption: Distribution parameters contain richer information than point estimates for similarity comparison
- Evidence anchors:
  - [abstract] "This work introduce a novel continuous embedding method, where each image is mapped into a distribution space and subsequently compared using the Bhattacharyya distance"
  - [section] "we employ the final-layer outputs of the network, which now represent the parameters α of the Dirichlet distribution rather than the softmax probability distribution p"
  - [corpus] Weak - corpus lacks direct evidence for α-parameters as effective neural codes
- Break condition: If α-embeddings perform worse than traditional neural codes, or if distributional comparisons fail to capture semantic similarity

### Mechanism 3
- Claim: Uncertainty-driven reranking improves retrieval quality by filtering top-N results
- Mechanism: A baseline retrieval is performed first, then an evidential model computes uncertainty for the top N results. These are reranked in ascending order of uncertainty, pushing uncertain but potentially relevant items higher.
- Core assumption: Uncertainty correlates with relevance for the retrieval task
- Evidence anchors:
  - [abstract] "uncertainty-driven reranking method reaching 80.79% Recall@1"
  - [section] "we utilize a contrastively-trained GC ViT model for feature extraction. Subsequently, an evidential GC ViT is employed to derive uncertainty values for each of the top N results"
  - [corpus] Weak - corpus lacks evidence for uncertainty-based reranking in retrieval contexts
- Break condition: If reranking by uncertainty degrades overall Recall@K metrics, or if computational cost is prohibitive

## Foundational Learning

- Concept: Dirichlet distribution and its parameters
  - Why needed here: The model outputs Dirichlet parameters (α) that represent a distribution over class probabilities, which is fundamental to evidential learning
  - Quick check question: What does it mean when α parameters are close to uniform versus concentrated?
- Concept: Uncertainty quantification in neural networks
  - Why needed here: Understanding how evidential learning differs from traditional softmax outputs and how uncertainty is computed (belief mass and total evidence)
  - Quick check question: How does the evidential loss function encourage both accuracy and low-variance predictions?
- Concept: Vision Transformer architectures (ViT and GC ViT)
  - Why needed here: The paper uses GC ViT as the backbone, which requires understanding attention mechanisms and how CLS tokens are used as image embeddings
  - Quick check question: How does GC ViT differ from standard ViT in terms of local and global context modeling?

## Architecture Onboarding

- Component map: Backbone (GC ViT) -> Evidential layer (outputs α) -> Embedding comparison -> Retrieval output
- Critical path: Backbone → Evidential transformation → Embedding comparison → Retrieval output
- Design tradeoffs:
  - Using α-embeddings vs CLS tokens: α provides uncertainty info but may be higher-dimensional
  - Reranking overhead: Improves quality but adds computation
  - Distributional vs vector comparisons: More expressive but computationally heavier
- Failure signatures:
  - Low separation between positive/negative pairs in embedding space
  - Uncertainty estimates not correlating with retrieval errors
  - Computational overhead making real-time retrieval impractical
- First 3 experiments:
  1. Baseline: Train GC ViT with standard cross-entropy, measure Recall@K on CUB-200-2011
  2. Evidential classification: Replace softmax with evidential output, compare performance metrics
  3. α-embeddings: Use Dirichlet parameters directly as neural codes, evaluate against CLS token baseline

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do evidential transformers perform on large-scale real-world datasets compared to controlled benchmark datasets?
- Basis in paper: [explicit] The paper demonstrates state-of-the-art performance on CUB-200-2011 and Stanford Online Products datasets, but acknowledges these are relatively controlled benchmark datasets
- Why unresolved: The paper focuses on two specific benchmark datasets but doesn't test performance on larger, more diverse real-world image retrieval scenarios
- What evidence would resolve it: Experimental results on larger-scale datasets like ImageNet-1K, OpenImages, or real-world commercial image retrieval systems

### Open Question 2
- Question: How does the uncertainty-driven reranking method scale computationally for real-time applications?
- Basis in paper: [explicit] The paper introduces an uncertainty-driven reranking method that outperforms baseline methods, but doesn't discuss computational complexity or real-time feasibility
- Why unresolved: The reranking approach requires computing uncertainties for top N results, but the computational overhead and scalability for large-scale retrieval systems remains unexplored
- What evidence would resolve it: Runtime analysis, memory requirements, and performance benchmarks for varying dataset sizes and reranking depths

### Open Question 3
- Question: What is the impact of evidential transformers on adversarial robustness in image retrieval?
- Basis in paper: [inferred] The paper discusses uncertainty quantification but doesn't explore adversarial robustness, which is mentioned as a potential future research direction
- Why unresolved: While the paper establishes that uncertainty estimates improve retrieval quality and reliability, it doesn't investigate how these uncertainty estimates protect against adversarial attacks
- What evidence would resolve it: Adversarial attack experiments showing how evidential uncertainty helps detect or resist manipulated query images, compared to standard transformers

## Limitations

- The study demonstrates effectiveness primarily on the CUB-200-2011 dataset, with limited evaluation on Stanford Online Products, raising questions about generalization across diverse retrieval scenarios.
- The uncertainty-driven reranking approach, while showing improved Recall@1, introduces computational overhead that may limit practical deployment.
- The distributional comparison method using Bhattacharyya distance, though theoretically appealing, requires verification on larger-scale datasets to assess scalability.

## Confidence

- Evidential classification performance: High
- α-embeddings effectiveness: Medium
- Uncertainty-driven reranking scalability: Low
- Generalization to diverse datasets: Low

## Next Checks

1. **Dataset Generalization Test**: Evaluate the evidential transformer approach on multiple retrieval datasets (e.g., CARS196, In-Shop Clothes) to verify that performance improvements are not dataset-specific artifacts.

2. **Uncertainty-Utility Correlation**: Systematically analyze the relationship between uncertainty estimates and retrieval errors across different query types to determine if uncertainty genuinely predicts retrieval quality or if improvements stem from other factors.

3. **Computational Overhead Assessment**: Measure end-to-end latency and resource requirements for uncertainty-driven reranking in real-world retrieval pipelines to quantify practical deployment constraints.