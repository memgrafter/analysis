---
ver: rpa2
title: "Sparsest Models Elude Pruning: An Expos\xE9 of Pruning's Current Capabilities"
arxiv_id: '2407.04075'
source_url: https://arxiv.org/abs/2407.04075
tags:
- pruning
- search
- combinatorial
- nonzero
- accuracy
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates whether current pruning algorithms can
  recover the sparsest possible neural networks. The authors conduct extensive experiments
  (485,838 total) using a novel combinatorial search algorithm on a synthetic "Cubist
  Spiral" dataset to identify ideal sparse models.
---

# Sparsest Models Elude Pruning: An Exposé of Pruning's Current Capabilities

## Quick Facts
- **arXiv ID**: 2407.04075
- **Source URL**: https://arxiv.org/abs/2407.04075
- **Reference count**: 40
- **Primary result**: Current pruning algorithms fail to recover the sparsest possible neural networks, creating disconnected paths and sub-optimal sparsity even on simple tasks

## Executive Summary
This paper investigates whether current pruning algorithms can recover the sparsest possible neural networks by comparing them against ideal sparse models identified through exhaustive combinatorial search. The authors conduct 485,838 experiments on a synthetic Cubist Spiral dataset using four-layer MLPs with width 16. Their key finding is a significant performance gap: state-of-the-art pruning methods (including LTH, GMP, RigL, and others) fail to match the accuracy of optimally sparse networks identified by combinatorial search. This gap persists even when pruning is given optimal network widths and weight initializations. The authors attribute this failure to pruning's tendency to create disconnected paths between layers and its poor behavior under overparameterization, concluding that current pruning techniques are inadequate for achieving true network sparsity.

## Method Summary
The authors use a novel combinatorial search algorithm to exhaustively identify ideal sparse networks on a synthetic Cubist Spiral dataset, then compare these against 9 state-of-the-art pruning algorithms (LTH, GMP, RigL, GraSP, SNIP, SynFlow, Iter SNIP, FORCE, ProsPr) across various network widths. The combinatorial search employs a two-phase approach: first identifying structured sparsity masks that maintain connectivity, then exploring unstructured masks within those structures. All experiments use four-layer MLPs trained for 50 epochs with SGD+momentum on 50,000 synthetic data points, with accuracy measured against the number of nonzero parameters to evaluate the trade-off between sparsity and performance.

## Key Results
- Pruning algorithms consistently fail to match the accuracy of optimally sparse networks identified by combinatorial search
- The performance gap persists even with optimal network widths and weight initializations
- Pruning creates disconnected paths between layers, contributing to apparent sparsity without adding expressivity
- Overparameterization systematically degrades pruning performance across different algorithms

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Pruning algorithms fail to recover the sparsest networks because they create disconnected paths between layers.
- Mechanism: Current pruning techniques do not properly align weights between consecutive layers, resulting in some neurons becoming disconnected from the input or output. These disconnected paths contribute to apparent sparsity without adding expressivity.
- Core assumption: Disconnected paths are a primary cause of pruning inefficiency and can be identified through visualization.
- Evidence anchors:
  - [abstract] "tendency to induce disconnected paths throughout the network"
  - [section 5.2] "Current pruning algorithms are unable to properly align the weights between consecutive layers leading to disconnected paths."
  - [corpus] Weak - no direct citations on disconnected paths in pruning literature

### Mechanism 2
- Claim: Overparameterization impedes pruning effectiveness by increasing the likelihood of disconnected paths.
- Mechanism: As network width increases, the probability that randomly pruned weights create disconnected paths between layers approaches one, even when optimal sparsity masks exist.
- Core assumption: Random pruning at initialization (used by methods like RigL) is more susceptible to creating disconnected paths than structured approaches.
- Evidence anchors:
  - [section 6] "overparameterization harms the performance of most pruning techniques"
  - [section L] Formal proof that "the probability that there is no connected path in the model tends to one when the width of the model goes to infinity"
  - [corpus] Weak - limited corpus evidence on overparameterization's negative impact on pruning

### Mechanism 3
- Claim: Pruning algorithms forego achievable sparsity because they require pre-determining the final nonzero budget.
- Mechanism: Pruning methods must commit to a target sparsity level before training, which prevents them from discovering that models could be pruned further after training without loss of accuracy.
- Core assumption: Post-training pruning could reveal additional sparsification opportunities that weren't apparent during the initial pruning process.
- Evidence anchors:
  - [section 5.3] "Suboptimal Sparsity... the model depicted in Figure 9 is foregoing a lot of sparsity that could be attained by magnitude pruning the model after training"
  - [section 7.2] "pruning BENCH -995 obtained from the first combinatorial search after training will lead to a sparser model"
  - [corpus] Weak - no direct citations on post-training pruning revealing additional opportunities

## Foundational Learning

- Concept: Combinatorial search algorithms
  - Why needed here: The paper uses a novel combinatorial search to identify optimal sparse networks as a benchmark against pruning methods
  - Quick check question: What is the time complexity of exhaustive combinatorial search over all possible sparsity masks for a four-layer network with width 16?

- Concept: Network pruning techniques
  - Why needed here: The paper compares 9 state-of-the-art pruning algorithms against the combinatorial search results
  - Quick check question: What are the three main categories of pruning strategies mentioned in the background section?

- Concept: Sparsity masks and connectivity
  - Why needed here: Understanding how pruning creates/disrupts connectivity between layers is central to the paper's findings
  - Quick check question: In a four-layer MLP, what condition must be met for a path to exist between the input and output?

## Architecture Onboarding

- Component map: Synthetic dataset generation (Cubist Spiral) -> Combinatorial search algorithm -> Pruning algorithm implementations -> Visualization tool -> Evaluation framework
- Critical path: Combinatorial search → Identify optimal sparse models → Compare against pruned models → Analyze failure modes → Draw conclusions about pruning limitations
- Design tradeoffs:
  - Synthetic dataset simplicity vs. real-world applicability
  - Small network architectures (width 16) vs. computational feasibility
  - Exhaustive search completeness vs. computational cost
  - Visualization clarity vs. detailed information
- Failure signatures:
  - Disconnected paths in pruned networks
  - Accuracy plateaus despite increasing sparsity
  - Performance degradation with overparameterization
  - Suboptimal sparsity masks that could be further pruned
- First 3 experiments:
  1. Run combinatorial search on Cubist Spiral with target accuracy 95% and 99.5% to establish baseline sparse models
  2. Apply each pruning algorithm to the same dataset with various sparsity budgets and compare against combinatorial search results
  3. Visualize both combinatorial search models and pruned models to identify disconnected paths and connectivity issues

## Open Questions the Paper Calls Out

The paper does not explicitly call out specific open questions, but implicitly raises several important directions:
- How can pruning algorithms be modified to maintain connectivity between layers while achieving high sparsity?
- Can the negative impact of overparameterization on pruning be mitigated through algorithmic improvements?
- Is there a way to adaptively determine optimal sparsity levels during training rather than committing to them upfront?

## Limitations

- Experiments are limited to synthetic data and small four-layer networks, raising questions about generalization to real-world datasets
- The paper focuses exclusively on magnitude-based pruning methods, potentially missing other pruning paradigms
- Combinatorial search is computationally prohibitive for larger networks, limiting its practical applicability as a general approach

## Confidence

- **High confidence**: Pruning algorithms create disconnected paths that reduce expressivity (supported by visualizations and formal proof)
- **Medium confidence**: Overparameterization systematically degrades pruning performance across different algorithms
- **Medium confidence**: Current pruning methods cannot recover the sparsest possible networks, even with optimal conditions
- **Low confidence**: Post-training pruning could reveal additional sparsification opportunities not captured by current methods

## Next Checks

1. Test the combinatorial search approach on a real-world dataset (e.g., MNIST or CIFAR-10) to assess whether the pruning limitations persist beyond synthetic data
2. Implement and evaluate structured pruning methods that maintain layer-wise connectivity to determine if they avoid the disconnected path problem
3. Conduct ablation studies varying network depth (not just width) to understand how architectural factors beyond width influence pruning effectiveness