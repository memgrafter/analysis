---
ver: rpa2
title: 'State-Separated SARSA: A Practical Sequential Decision-Making Algorithm with
  Recovering Rewards'
arxiv_id: '2403.11520'
source_url: https://arxiv.org/abs/2403.11520
tags:
- rewards
- smax
- algorithm
- state
- policy
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the recovering bandit problem, where rewards
  depend on the number of rounds since an arm was last pulled. The authors propose
  a new reinforcement learning algorithm called State-Separated SARSA (SS-SARSA) that
  treats rounds as states to efficiently learn optimal policies.
---

# State-Separated SARSA: A Practical Sequential Decision-Making Algorithm with Recovering Rewards

## Quick Facts
- arXiv ID: 2403.11520
- Source URL: https://arxiv.org/abs/2403.11520
- Authors: Yuto Tanimoto; Kenji Fukumizu
- Reference count: 18
- Key outcome: SS-SARSA reduces computational complexity in recovering bandit problems by constructing State-Separated Q-functions that depend only on states of associated and pulled arms, achieving better performance than Q-learning, SARSA, and dRGP-TS.

## Executive Summary
This paper addresses the recovering bandit problem where rewards depend on the number of rounds since an arm was last pulled. The authors propose State-Separated SARSA (SS-SARSA), which treats rounds as states and constructs State-Separated Q-functions for each arm, reducing state combinations from sKmax × K to s²max × K². The algorithm updates K SS-Q-functions per round in linear time, avoiding the combinatorial complexity of standard Q-learning/SARSA. Experiments show SS-SARSA outperforms related methods across various reward settings including monotone-increasing and increasing-then-decreasing rewards.

## Method Summary
SS-SARSA addresses the recovering bandit problem by treating rounds as states and constructing State-Separated Q-functions (SS-Q-functions) for each arm. Each SS-Q-function depends only on the state of its associated arm and the pulled arm, reducing the state space from sKmax × K to s²max × K². The algorithm uses a Uniform-Explore-First policy that explores all Q-function states uniformly before exploiting learned information. Updates occur in linear time (O(K)) per round by updating K SS-Q-functions, avoiding the exponential complexity of standard tabular RL methods. The SARSA update rule is applied to each SS-Q-function, and the algorithm converges to the optimal policy under mild assumptions including finite reward variance and GLIE policies.

## Key Results
- SS-SARSA outperforms Q-learning, SARSA, and dRGP-TS in cumulative regret across monotone-increasing and increasing-then-decreasing reward settings
- The algorithm achieves better rates of optimal policy identification compared to baseline methods
- Uniform-Explore-First policy provides more efficient exploration than random exploration (ε-greedy), particularly for states with smaller smax values

## Why This Works (Mechanism)

### Mechanism 1
- Claim: SS-SARSA reduces computational complexity by constructing SS-Q-functions that depend only on states of associated and pulled arms
- Mechanism: By limiting state combinations to s²max × K² instead of sKmax × K, SS-SARSA avoids combinatorial explosion typical in standard Q-learning/SARSA
- Core assumption: Reward structure depends only on state of pulled arm and not other arm states
- Evidence anchors:
  - [abstract] "The SS-SARSA algorithm achieves efficient learning by reducing the number of state combinations required for Q-learning/SARSA, which often suffers from combinatorial issues for large-scale RL problems."
  - [section 4.1] "Computation with SS-Q-functions requires only s²max × K² variables, while the naive implementation of the original Q-function needs sKmax × K."
- Break condition: If reward depends on states of multiple arms simultaneously, SS-Q-function structure would no longer be valid

### Mechanism 2
- Claim: Uniform-Explore-First policy ensures efficient uniform exploration by pulling least frequently selected arm for given states during exploration phase
- Mechanism: This policy addresses limitation of random exploration which can lead to uneven updates of SS-Q-functions, particularly favoring states with smax
- Core assumption: Uniform exploration of all Q-function states is beneficial for learning optimal policies in recovering bandit settings
- Evidence anchors:
  - [section 4.2] "Under random exploration, we can compute the probability that arm k is pulled at state sk = i in round t... This result implies that even when applying policies that randomly select an arm given a state... SS-Q-functions are frequently updated at smax compared to other states."
  - [section 4.2] "In this paper, we propose an alternative policy, named Uniform-Explore-First, which follows a strategy of exploring information uniformly initially and subsequently exploiting this explored information."
- Break condition: If optimal policy requires biased exploration toward certain states, uniform exploration might be suboptimal

### Mechanism 3
- Claim: SS-SARSA converges to optimal policy asymptotically under mild assumptions, similar to standard SARSA
- Mechanism: By treating rounds as states and updating SS-Q-functions with SARSA update rule, algorithm can be shown to converge to Bellman optimality equation for Q-functions
- Core assumption: Learning rate satisfies Robbins-Monro scheme and policy is GLIE (Greedy in the Limit with Infinite Exploration)
- Evidence anchors:
  - [section 5] "Suppose that the variance of the (stochastic) reward r is finite and αt = 1/(t+t0)... Suppose that πt(s|a) is a GLIE policy... For any (s, a) ∈ V, Qerr,t(s, a) → 0 with probability 1 as T → ∞."
  - [section 4.2] "Compared to the related works, SS-SARSA has three advantages: reduced combinations of the estimated SS-Q-functions, low time complexity, and long-term lookahead."
- Break condition: If reward variance is infinite or policy fails to visit each (s,a) pair infinitely often, convergence is not guaranteed

## Foundational Learning

- Concept: Markov Decision Process (MDP)
  - Why needed here: Recovering bandit problem is formulated as MDP where rounds are treated as states, allowing application of RL algorithms
  - Quick check question: What are the components of an MDP, and how does treating rounds as states help in solving the recovering bandit problem?

- Concept: Q-learning and SARSA algorithms
  - Why needed here: SS-SARSA builds upon these standard RL algorithms by modifying Q-function structure to handle recovering bandit setting efficiently
  - Quick check question: How do Q-learning and SARSA update their Q-functions, and what is the key difference between them?

- Concept: Bellman equation and optimality
  - Why needed here: Convergence of SS-SARSA to optimal policy relies on properties of Bellman equation and its optimality conditions
  - Quick check question: What is the Bellman equation, and how does it relate to finding optimal policy in an MDP?

## Architecture Onboarding

- Component map: States -> SS-Q-functions -> Policy (Uniform-Explore-First) -> Action selection -> Reward/State update -> SS-Q-function update

- Critical path:
  1. Initialize SS-Q-functions and states
  2. Select arm based on Uniform-Explore-First policy
  3. Receive reward and update states
  4. Update SS-Q-functions using SARSA update rule
  5. Repeat until convergence or maximum rounds reached

- Design tradeoffs:
  - Exploration vs. exploitation: Balancing uniform exploration to gather information and exploitation to maximize rewards
  - State representation: Treating rounds as states simplifies problem but may not capture all dependencies
  - Computational efficiency: Reducing state combinations improves efficiency but may limit algorithm's ability to handle complex reward structures

- Failure signatures:
  - Slow convergence: Indicates insufficient exploration or inappropriate learning rate
  - Suboptimal policy: Suggests algorithm failed to explore all relevant state-action pairs or reward structure is too complex for current approach
  - High variance in cumulative rewards: May indicate need for more stable exploration policy or different reward distribution assumption

- First 3 experiments:
  1. Small-scale problem with K=3 and smax=3 to verify basic functionality and compare against standard Q-learning/SARSA
  2. Monotone-increasing rewards with K=6 and smax=3 to test algorithm's ability to handle larger state spaces and more complex reward structures
  3. Increasing-then-decreasing rewards with K=10 and smax=5 to evaluate performance in most challenging setting and compare against dRGP-TS algorithm

## Open Questions the Paper Calls Out
- The paper identifies the need for regret bounds and sample complexity analysis without relying on strong reward structure assumptions as future work
- The authors suggest functional approximation as a potential solution for scaling to problems with many arms and large smax
- Non-stationary recovering bandit problems where reward functions change over time are not addressed but represent a natural extension

## Limitations
- State-separated structure may not generalize well to problems where rewards depend on more than two arm states simultaneously
- Algorithm's performance in non-stationary environments or with changing reward structures is not evaluated
- Theoretical analysis assumes finite reward variance, which may not hold in all practical applications

## Confidence

- High confidence in computational efficiency claims and theoretical convergence proof under stated assumptions
- Medium confidence in empirical performance claims due to synthetic nature of experiments and limited comparison to alternative methods
- Medium confidence in Uniform-Explore-First policy's general effectiveness, as paper demonstrates benefit in specific settings but doesn't explore edge cases

## Next Checks

1. Test SS-SARSA on a recovering bandit problem where rewards depend on three or more arm states to assess algorithm's robustness beyond two-state assumption

2. Implement and compare against alternative exploration strategies (e.g., UCB-based approaches) to validate superiority of Uniform-Explore-First in diverse settings

3. Evaluate algorithm's performance on a real-world dataset (e.g., recommendation systems with user fatigue) to assess practical applicability beyond synthetic environments