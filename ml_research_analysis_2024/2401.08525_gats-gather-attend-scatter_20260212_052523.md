---
ver: rpa2
title: 'GATS: Gather-Attend-Scatter'
arxiv_id: '2401.08525'
source_url: https://arxiv.org/abs/2401.08525
tags:
- gats
- language
- video
- agent
- vision
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: GATS is a novel module that enables seamless integration of pretrained
  foundation models into larger multimodal networks while keeping the original models
  frozen. The key idea is to gather activations from component models, attend to relevant
  information, and scatter the combined representation back to all models.
---

# GATS: Gather-Attend-Scatter

## Quick Facts
- arXiv ID: 2401.08525
- Source URL: https://arxiv.org/abs/2401.08525
- Authors: Konrad Zolna; Serkan Cabi; Yutian Chen; Eric Lau; Claudio Fantacci; Jurgis Pasukonis; Jost Tobias Springenberg; Sergio Gomez Colmenarejo
- Reference count: 15
- One-line primary result: GATS enables seamless integration of pretrained foundation models into larger multimodal networks while keeping the original models frozen, achieving state-of-the-art performance in Language-Table environment with up to 89% success rate.

## Executive Summary
GATS introduces a novel module that allows pretrained foundation models to be integrated into larger multimodal networks without updating their weights. The key innovation is a gather-attend-scatter mechanism that reprograms model behavior through activation manipulation rather than weight updates. This approach preserves knowledge acquired during pretraining while enabling coordination across multiple modalities processed at different rates.

The method demonstrates significant improvements across diverse domains including games, robotics, and multimodal input-output systems. By keeping foundation models frozen and using lightweight attention mechanisms, GATS provides an efficient solution for multimodal reasoning that scales to multiple modalities without requiring full model retraining.

## Method Summary
GATS works by gathering recent activations from all component models, attending to the most relevant information in a shared low-dimensional space, and scattering the combined representation back to modify the original activations before they enter the next layer. This reprograms the behavior of frozen foundation models through activation manipulation rather than weight updates. Each modality processes information independently using its own local context length, allowing asynchronous processing while maintaining temporal coordination through the GATS layer.

## Key Results
- Achieves state-of-the-art performance in Language-Table environment with up to 89% success rate
- Demonstrates effective coordination across multiple modalities processed at different rates
- Enables knowledge transfer between modalities without modifying frozen foundation models
- Shows versatility across games, robotics, and multimodal input-output systems

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** GATS avoids catastrophic forgetting by keeping pretrained models frozen and instead steering their behavior through activation manipulation.
- **Mechanism:** The module gathers recent activations from all modalities, attends over them in a shared low-dimensional space, and scatters the combined representation back to modify the original activations before they enter the next layer.
- **Core assumption:** A pretrained network's behavior can be reprogrammed by modifying its activations rather than its weights.
- **Evidence anchors:** [abstract] "GATS allows for the original component models to remain frozen, avoiding the risk of them losing important knowledge acquired during the pretraining phase."
- **Break condition:** If the frozen models have incompatible embedding sizes or incompatible attention patterns that cannot be bridged in the shared projection space.

### Mechanism 2
- **Claim:** GATS enables different modalities to process information at different rates while maintaining coordinated multimodal reasoning.
- **Mechanism:** Each modality has its own local context length (N₁, N₂, ..., Nₘ), so recent embeddings from slower or less frequent modalities are still visible to GATS when processing faster ones.
- **Core assumption:** Multimodal information arrives at different rates and requires coordination mechanisms that don't force synchronous processing.
- **Evidence anchors:** [abstract] "GATS empowers AI systems to process and generate information across multiple modalities at different rates."
- **Break condition:** If context lengths are set too short relative to the rate differences, or if temporal dependencies span beyond the local context.

### Mechanism 3
- **Claim:** GATS achieves efficient multimodal integration by parallelizing unimodal processing and adding minimal overhead through lightweight attention.
- **Mechanism:** Each input embedding is processed by only its corresponding unimodal model, then GATS adds overhead only for the gather-attend-scatter steps.
- **Core assumption:** The computational overhead of GATS layers is negligible compared to running full multimodal models sequentially.
- **Evidence anchors:** [section 2.6] "The processing is, however, conditioned on the other modalities thanks to GATS steering. Importantly, only already processed activations from other modalities are gathered by GATS."
- **Break condition:** If the number of GATS layers becomes comparable to component model layers, or if projected embedding size needs to be large for complex reasoning.

## Foundational Learning

- **Concept:** Transformer attention mechanisms and positional encodings
  - Why needed here: GATS layers are built on vanilla transformer layers with local attention over gathered embeddings
  - Quick check question: How does local attention in GATS differ from standard self-attention in terms of context length and modality handling?

- **Concept:** Knowledge transfer vs. fine-tuning trade-offs
  - Why needed here: GATS explicitly chooses to keep pretrained models frozen rather than fine-tuning them to preserve knowledge while adding new capabilities
  - Quick check question: What are the advantages and disadvantages of steering pretrained models through activation manipulation versus fine-tuning their weights?

- **Concept:** Multimodal representation alignment and fusion
  - Why needed here: GATS projects different modality embeddings to a common space for attention, requiring understanding of how to align heterogeneous representations
  - Quick check question: How does the projection size (d) affect the quality of multimodal fusion and the computational efficiency of GATS?

## Architecture Onboarding

- **Component map:** Input embeddings → Unimodal model processing → GATS gather-attend-scatter → Modified activations → Next unimodal layer → Final output
- **Critical path:** Input → Unimodal model processing → GATS gather-attend-scatter → Modified activations → Next unimodal layer → Final output
- **Design tradeoffs:** Frozen foundation models preserve knowledge but limit flexibility; steering adds overhead but enables coordination; small projected embeddings save compute but may limit reasoning capacity
- **Failure signatures:** Poor multimodal coordination suggests context length issues; catastrophic forgetting suggests steering isn't working; slow inference suggests too many GATS layers or large projections
- **First 3 experiments:**
  1. Single-modality ablation: Remove GATS from a bimodal setup and compare performance to ensure GATS is providing benefit
  2. Context length sweep: Vary N₁, N₂, ..., Nₘ to find optimal trade-off between coordination and computational overhead
  3. Projection size scaling: Test different values of d to understand impact on fusion quality and efficiency

## Open Questions the Paper Calls Out
None

## Limitations
- The claims about computational efficiency lack quantitative validation with runtime comparisons
- The scalability to many modalities is theoretical without empirical testing beyond three modalities
- The limits of asynchronous processing when modality rate differences are extreme are not explored

## Confidence

**High Confidence:** The basic GATS architecture (gather-attend-scatter) is well-specified and implementable. The experimental results showing improved performance in Language-Table and robotics tasks appear reliable.

**Medium Confidence:** The claims about avoiding catastrophic forgetting through activation manipulation are plausible but not directly tested. The efficiency claims are reasonable given the architecture but lack quantitative validation.

**Low Confidence:** The assertion that GATS enables "seamless" integration is overstated without showing failure cases or limitations. The claim about being able to "seamlessly add or remove modalities" lacks experimental validation.

## Next Checks

1. **Catastrophic Forgetting Test:** Unfreeze the foundation models in a GATS system and fine-tune them on the target task, then compare performance degradation on the original pretraining task versus the frozen+steering approach.

2. **Computational Overhead Benchmark:** Implement GATS with varying numbers of layers and projection sizes, then measure wall-clock inference time and memory usage compared to baseline approaches.

3. **Modality Rate Sensitivity Analysis:** Create controlled experiments with synthetic data where modality rates differ by orders of magnitude (e.g., 10x, 100x, 1000x), then measure performance degradation as the rate difference increases.