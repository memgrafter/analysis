---
ver: rpa2
title: Assisted Debate Builder with Large Language Models
arxiv_id: '2405.13015'
source_url: https://arxiv.org/abs/2405.13015
tags:
- argumentation
- language
- arguments
- argument
- large
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces ADBL2, an assisted debate builder tool that
  leverages large language models (LLMs) to help users construct high-quality argumentation
  frameworks. ADBL2 addresses the challenge of creating clear, well-structured arguments
  by automating the verification of existing argument relations and assisting in the
  creation of new arguments.
---

# Assisted Debate Builder with Large Language Models

## Quick Facts
- arXiv ID: 2405.13015
- Source URL: https://arxiv.org/abs/2405.13015
- Authors: Elliot Faugier; Frédéric Armetta; Angela Bonifati; Bruno Yun
- Reference count: 40
- One-line primary result: Fine-tuned Mistral-7B achieves 90.59% F1-score for relation-based argument mining across multiple domains

## Executive Summary
This paper introduces ADBL2, an assisted debate builder tool that leverages large language models (LLMs) to help users construct high-quality argumentation frameworks. ADBL2 addresses the challenge of creating clear, well-structured arguments by automating the verification of existing argument relations and assisting in the creation of new arguments. The tool uses relation-based argument mining (RBAM) to classify arguments as supporting and attacking, and is highly modular, allowing it to work with various open-source LLMs. As a key contribution, the authors fine-tuned Mistral-7B, a smaller LLM, for RBAM, achieving an overall F1-score of 90.59% across multiple domains, outperforming existing approaches.

## Method Summary
The paper presents ADBL2, which uses fine-tuned Mistral-7B with LoRA and QLoRA for relation-based argument mining (RBAM). The model is trained on argument triples from Kialo debates across law, politics, and sports domains, then evaluated on 10 different domains. The fine-tuning process uses a 77.8/22.2 train/test split with specific parameters (learning_rate=1e-4, per_device_train_batch_size=16, LoRA rank=8). The tool employs LMQL to constrain LLM outputs to the desired labels (attack or support) and features a modular architecture that allows integration with various open-source LLMs.

## Key Results
- Fine-tuned Mistral-7B achieves 90.59% macro F1-score for relation-based argument mining across multiple domains
- The fine-tuned model outperforms larger models using few-shot prompting approaches
- Strong generalization capabilities demonstrated across domains not included in training data

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Fine-tuning a smaller LLM (Mistral-7B) with LoRA and QLoRA achieves better performance than using a large LLM with few-shot prompting for relation-based argument mining
- Mechanism: By fine-tuning the model on domain-specific data (law, politics, sports), the model learns the nuanced patterns of argumentative relations and generalizes well to unseen domains, outperforming larger models that rely on few-shot examples
- Core assumption: The smaller LLM has sufficient capacity to learn the complex patterns of argumentative relations when fine-tuned appropriately, and that domain-specific fine-tuning provides better generalization than few-shot prompting
- Evidence anchors: The paper states that the fine-tuned Mistral-7B model "outperforms existing approaches for this task with an overall F1-score of 90.59% across all domains"

### Mechanism 2
- Claim: The modular architecture of ADBL2 allows it to work with various open-source LLMs, providing flexibility and adaptability to different use cases and computational constraints
- Mechanism: By designing the tool to be highly modular, users can plug in different LLMs as needed, allowing for customization based on available computational resources and specific requirements of the debate domain
- Core assumption: The modular design does not compromise the core functionality of the tool and that different LLMs can be seamlessly integrated without significant re-engineering
- Evidence anchors: The paper mentions that ADBL2 "is highly modular and can work with any open-source large language models that are used as plugins"

### Mechanism 3
- Claim: The use of relation-based argument mining (RBAM) allows ADBL2 to automatically identify and classify argumentative relations, assisting users in creating high-quality argumentation frameworks
- Mechanism: By leveraging the capabilities of LLMs to perform RBAM, the tool can automatically classify relations as supports or attacks, helping users to structure their debates more effectively and identify potential weaknesses in their arguments
- Core assumption: The RBAM model is accurate enough to provide meaningful assistance to users and that the automatic classification of relations is reliable and useful in practice
- Evidence anchors: The paper states that ADBL2 leverages "the capability of large language models to generalise and perform relation-based argument mining in a wide-variety of domains"

## Foundational Learning

- Concept: Relation-based argument mining (RBAM)
  - Why needed here: RBAM is the core task that ADBL2 is built upon, allowing it to automatically identify and classify argumentative relations
  - Quick check question: What is the difference between relation-based argument mining and other forms of argument mining?

- Concept: Fine-tuning techniques (LoRA and QLoRA)
  - Why needed here: These techniques allow for efficient fine-tuning of smaller LLMs, reducing computational requirements while maintaining performance
  - Quick check question: How do LoRA and QLoRA differ from traditional fine-tuning approaches?

- Concept: Modular software architecture
  - Why needed here: The modular design of ADBL2 allows for flexibility in choosing and integrating different LLMs, making the tool adaptable to various use cases and computational constraints
  - Quick check question: What are the key principles of modular software architecture, and how do they apply to ADBL2?

## Architecture Onboarding

- Component map: Web UI -> Inference Core -> LLM Plugins -> LMQL
- Critical path:
  1. User imports a debate tree into the Web UI
  2. User interacts with the tree (e.g., edits an argument, adds a new argument)
  3. The Inference Core translates the user input into a prompt for the selected LLM
  4. The LLM performs RBAM and returns the classification probabilities
  5. The Web UI displays the results to the user

- Design tradeoffs:
  - Using a smaller LLM (Mistral-7B) instead of a larger one reduces computational requirements but may limit the model's capacity to learn complex patterns
  - The modular architecture provides flexibility but may introduce inconsistencies in performance across different LLMs
  - Relying on RBAM for automatic classification may not always be accurate, potentially leading to misleading assistance for users

- Failure signatures:
  - Poor performance on unseen domains or debate topics
  - Inconsistent results when using different LLMs
  - Difficulty in integrating new LLMs into the tool
  - Users find the automatic assistance unhelpful or misleading

- First 3 experiments:
  1. Test the performance of ADBL2 on a new debate domain not included in the training data
  2. Compare the results of using different LLMs (e.g., Llama 2, Gemma) as plugins in ADBL2
  3. Evaluate the accuracy of the RBAM model on a manually annotated dataset of argumentative relations

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does ADBL2's performance compare to other argumentation tools that leverage large language models for relation-based argument mining?
- Basis in paper: [explicit] The paper mentions that ADBL2 is the first open-source tool that leverages relation-based mining for the verification of existing relations in a debate and the assisted creation of new arguments. However, it does not provide a direct comparison with other argumentation tools
- Why unresolved: The paper does not provide a comprehensive comparison of ADBL2's performance with other argumentation tools, making it difficult to assess its relative effectiveness
- What evidence would resolve it: A comparative study evaluating ADBL2 against other argumentation tools, focusing on metrics such as accuracy, efficiency, and user satisfaction, would provide insights into its relative performance

### Open Question 2
- Question: What are the limitations of using relation-based argument mining for argumentation frameworks, and how can they be addressed?
- Basis in paper: [inferred] The paper acknowledges that while RBAM has been used for several tasks, there is a need to assess its generalization capabilities on other argumentative datasets and extend the model to perform ternary RBAM
- Why unresolved: The paper highlights the potential limitations of RBAM but does not provide a detailed analysis of these limitations or propose solutions to address them
- What evidence would resolve it: A thorough investigation of the limitations of RBAM, including an analysis of its performance on various argumentative datasets and the development of techniques to overcome these limitations, would provide a clearer understanding of its applicability

### Open Question 3
- Question: How does the performance of ADBL2's fine-tuned Mistral-7B model compare to other fine-tuned large language models for relation-based argument mining?
- Basis in paper: [explicit] The paper mentions that the fine-tuned Mistral-7B model outperforms existing approaches for relation-based argument mining with an overall F1-score of 90.59% across all domains. However, it does not provide a direct comparison with other fine-tuned large language models
- Why unresolved: The paper does not provide a comprehensive comparison of the fine-tuned Mistral-7B model's performance with other fine-tuned large language models, making it difficult to assess its relative effectiveness
- What evidence would resolve it: A comparative study evaluating the fine-tuned Mistral-7B model against other fine-tuned large language models, focusing on metrics such as accuracy, efficiency, and generalization capabilities, would provide insights into its relative performance

## Limitations

- Evaluation focuses on 10 specific debate domains, raising questions about generalization to other argumentation types
- Comparison with few-shot prompting approaches lacks direct quantitative evidence
- Practical impact on debate quality and user experience is not empirically validated through user studies

## Confidence

**High Confidence**: The technical implementation of fine-tuning Mistral-7B with LoRA and QLoRA is well-documented and follows established practices. The RBAM task definition and evaluation methodology are clearly specified.

**Medium Confidence**: The claim that fine-tuning outperforms few-shot prompting is supported by comparative results but lacks direct experimental validation. The modular architecture's flexibility is theoretically sound but lacks empirical demonstration.

**Low Confidence**: The practical impact on debate quality and user experience is not empirically validated through user studies or real-world deployment scenarios.

## Next Checks

1. **Domain Generalization Test**: Evaluate ADBL2 on argumentation datasets from completely different domains (e.g., scientific debates, legal arguments) not represented in the training data to verify the claimed generalization capabilities.

2. **User Experience Validation**: Conduct a controlled study comparing debate quality and construction efficiency between users with and without ADBL2 assistance, measuring both objective metrics (argumentation structure quality) and subjective user satisfaction.

3. **Modular Architecture Performance Analysis**: Test ADBL2 with multiple different LLM backends (Llama 2, Gemma, etc.) to empirically verify that the modular design maintains consistent performance and usability across different models.