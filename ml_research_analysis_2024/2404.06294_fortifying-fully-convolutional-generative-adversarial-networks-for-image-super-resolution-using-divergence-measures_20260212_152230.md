---
ver: rpa2
title: Fortifying Fully Convolutional Generative Adversarial Networks for Image Super-Resolution
  Using Divergence Measures
arxiv_id: '2404.06294'
source_url: https://arxiv.org/abs/2404.06294
tags:
- surge
- super-resolution
- image
- psnr
- output
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a fully convolutional Generative Adversarial
  Network (GAN) architecture, called SuRGe, for image super-resolution (SR). SuRGe
  employs skip connections to preserve hierarchical features, and learnable convex
  weights to optimally combine these features.
---

# Fortifying Fully Convolutional Generative Adversarial Networks for Image Super-Resolution Using Divergence Measures

## Quick Facts
- arXiv ID: 2404.06294
- Source URL: https://arxiv.org/abs/2404.06294
- Reference count: 40
- Primary result: Achieves state-of-the-art performance on 10 benchmark datasets with 3.51% average PSNR and 5.45% SSIM improvement for 4x super-resolution

## Executive Summary
This paper introduces SuRGe, a fully convolutional GAN architecture for image super-resolution that employs skip connections and learnable convex weights to optimally combine hierarchical features. The method introduces novel uses of Jensen-Shannon divergence loss between SR and HR distributions and Gromov-Wasserstein distance loss between LR and SR distributions to improve generator training. The approach achieves state-of-the-art performance on ten benchmark datasets with significant improvements in PSNR and SSIM metrics while maintaining low inference time and a lightweight model.

## Method Summary
SuRGe is a fully convolutional GAN that uses skip connections to preserve low-level features from early network layers. The generator employs learnable convex weights through weighted feature mixing modules to optimally combine features from different depths. The method introduces JS divergence loss between SR-HR pairs and GW distance loss between LR-SR pairs as additional generator objectives. The discriminator uses Wasserstein loss with gradient penalty. A dynamic weighted convex combination of the three loss components (adversarial, JS divergence, and GW distance) is used to improve training stability and performance.

## Key Results
- Achieves 3.51% average PSNR improvement and 5.45% SSIM improvement on four common 4x super-resolution benchmarks
- Delivers 15.19% PSNR improvement on six complex 4x super-resolution datasets
- Maintains low inference time and lightweight model architecture
- Demonstrates state-of-the-art performance across ten benchmark datasets including Set5, Set14, BSD100, Urban100, and others

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The adaptive convex combination of loss functions improves training stability and performance.
- Mechanism: Dynamic weights for adversarial, JS divergence, and GW distance losses are computed using Softmax on their values, preventing any single loss from dominating and allowing the model to adjust focus during training.
- Core assumption: The relative importance of the three loss components changes during training, and a fixed weighting scheme cannot adapt to these changes.
- Evidence anchors: [abstract] mentions employing JS and GW losses; [section] discusses the inefficiency of static weights and the use of convex combination with dynamically calculated weights.

### Mechanism 2
- Claim: Skip connections combined with learnable feature mixing modules preserve and optimally combine hierarchical features.
- Mechanism: Skip connections carry low-level features from early layers to later layers, while weighted feature mixing modules use learnable convex weights to combine features from different depths, enabling the network to adaptively focus on the most useful information for super-resolution.
- Core assumption: Low-level features (e.g., edges, textures) are crucial for recovering fine details after upscaling, and simply concatenating or adding features without adaptation is suboptimal.
- Evidence anchors: [abstract] shows that distinct convolutional features can be optimally combined by learnable convex weights; [section] introduces mixing modules that operate in a learnable fashion.

### Mechanism 3
- Claim: The use of Gromov-Wasserstein distance explicitly enforces structural similarity between LR and SR distributions in different metric spaces.
- Mechanism: GW loss minimizes the discrepancy between LR and SR distributions by comparing their internal geometry (pairwise distances) rather than their absolute positions, which is crucial since LR and SR reside in different dimensional spaces.
- Core assumption: Preserving the structural relationships between features in LR and SR is essential for meaningful super-resolution, beyond just matching pixel values.
- Evidence anchors: [abstract] mentions employing JS and GW losses; [section] explains that GW enables penalizing deviation from the ideal SR transformation.

## Foundational Learning

- Concept: Generative Adversarial Networks (GANs) and their training dynamics.
  - Why needed here: SuRGe is fundamentally a GAN-based architecture, and understanding the interplay between generator and discriminator is crucial for grasping the proposed improvements.
  - Quick check question: What is the primary objective of the generator and discriminator in a GAN, and how do they achieve this through adversarial training?

- Concept: Divergence measures (Jensen-Shannon, Gromov-Wasserstein) and their applications in distribution matching.
  - Why needed here: The paper introduces novel uses of JS and GW divergences as loss functions to improve super-resolution quality by aligning distributions of HR-SR and LR-SR pairs.
  - Quick check question: How do JS divergence and GW distance differ in their approach to measuring dissimilarity between probability distributions?

- Concept: Convolutional neural networks and feature hierarchy.
  - Why needed here: SuRGe uses a fully convolutional architecture with skip connections to preserve and combine hierarchical features at different depths.
  - Quick check question: How do features extracted by early vs. late convolutional layers in a CNN differ in terms of abstraction and locality?

## Architecture Onboarding

- Component map: LR image → C0 → R0 → I0 → F0 → U0 → C1 → R1 → I1 → F1 → U1 → SR image (G). For D: HR/SR image → B blocks → H → classification probability.
- Critical path: LR image → C0 → R0 → I0 → F0 → U0 → C1 → R1 → I1 → F1 → U1 → SR image (G). For D: HR/SR image → B blocks → H → classification probability.
- Design tradeoffs:
  - Skip connections vs. dense connections: Skip connections are chosen for efficiency and ease of training with limited data, while dense connections might offer richer feature propagation but at higher computational cost.
  - Dynamic vs. static loss weighting: Dynamic weighting adapts to training progress but adds complexity, while static weighting is simpler but may not optimize performance.
  - Two-step upscaling vs. single-step: Two-step upscaling allows for feature recovery after each upsample, potentially reducing distortion, but adds computational overhead.
- Failure signatures:
  - Mode collapse in G: Discriminator becomes too strong, causing G to produce limited variations of SR outputs.
  - Training instability: Loss values fluctuate wildly or fail to converge.
  - Poor detail recovery: SR outputs lack fine textures and resemble blurred versions of HR images.
- First 3 experiments:
  1. Train G alone (without D) with only reconstruction loss to establish a baseline.
  2. Train full GAN with static weights for all three losses to compare against dynamic weighting.
  3. Ablate the GW loss to assess its contribution to performance.

## Open Questions the Paper Calls Out
- Question: How do the proposed SuRGe's performance metrics compare to other methods when trained and evaluated on datasets with different image characteristics (e.g., textures, colors, object types)?
- Question: How does the choice of kernel size in the discriminator (D) affect the overall performance of SuRGe?
- Question: How does the choice of loss functions in the generator (G) impact the quality of the generated super-resolution images?

## Limitations
- Architectural details are underspecified, particularly the exact configuration of residual blocks, mixing modules, and upscaling blocks
- The contribution of Gromov-Wasserstein distance lacks validation through ablation studies
- The paper doesn't provide a detailed analysis of performance across different image characteristics

## Confidence
- High: The general GAN architecture with skip connections and adversarial training is well-established
- Medium: The dynamic weighted convex combination of losses is theoretically sound but lacks empirical validation through ablation
- Low: The specific contributions of Gromov-Wasserstein distance and learnable convex weights for feature mixing are not independently validated

## Next Checks
1. Conduct ablation studies isolating the contributions of JS divergence loss, GW distance loss, and dynamic weighting versus static alternatives
2. Provide detailed architectural specifications for all components (residual block structure, mixing module implementation, upscaling configuration)
3. Validate the proposed mechanisms on additional datasets beyond the ten benchmarks mentioned, particularly testing on real-world low-resolution images rather than synthetically downscaled ones