---
ver: rpa2
title: 'DocFinQA: A Long-Context Financial Reasoning Dataset'
arxiv_id: '2401.06915'
source_url: https://arxiv.org/abs/2401.06915
tags:
- what
- financial
- docfinqa
- question
- finqa
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces DocFinQA, a long-document financial QA dataset
  that extends the FinQA dataset by providing full SEC report context (average 123k
  words) instead of short excerpts (under 700 words). The dataset contains 7,437 questions
  with associated Python programs for answer generation.
---

# DocFinQA: A Long-Context Financial Reasoning Dataset
## Quick Facts
- arXiv ID: 2401.06915
- Source URL: https://arxiv.org/abs/2401.06915
- Reference count: 23
- Primary result: 7,437 questions with Python programs for financial reasoning on 123k-word SEC documents

## Executive Summary
This paper introduces DocFinQA, a long-document financial QA dataset extending FinQA by providing full SEC report context instead of short excerpts. The dataset contains 7,437 questions with associated Python programs for answer generation, targeting documents averaging 123k words. The authors evaluate both retrieval-based QA pipelines and long-context language models, finding that even state-of-the-art systems struggle significantly with this task. Human evaluation shows non-expert performance at 41% accuracy on 100K+ token documents, while retrieval-free models achieve only 11-23% accuracy.

## Method Summary
The dataset construction process involves collecting SEC filings, converting HTML/XML to markdown using Kensho Extract, chunking documents into 2,750-character segments with 20% overlap, and aligning questions to relevant chunks using four-gram similarity scoring. For evaluation, the authors implement retrieval-based pipelines using ColBERT, Sentence-BERT, and OpenAI Ada models to retrieve relevant document chunks, followed by few-shot in-context learning with LLMs to generate Python programs that compute answers. They also evaluate retrieval-free approaches using long-context models with S2A and Iterative methods.

## Key Results
- Human non-experts achieve only 41% accuracy on 100K+ token documents
- Retrieval-based approaches outperform retrieval-free methods, but both struggle significantly
- Code generation models (CodeLlama, CodeStarCoder) outperform general LLMs on DocFinQA
- Even with retrieval, state-of-the-art systems show substantial room for improvement

## Why This Works (Mechanism)
### Mechanism 1
- Domain-specific finetuning of ColBERT on DocFinQA training data improves retrieval accuracy by 91% HR over Sentence-BERT
- Core assumption: Training data contains sufficient signal for learning financial retrieval patterns
- Evidence: FT ColB achieves 0.35 (HR@1) and 0.55 (HR@3) on test set

### Mechanism 2
- Code generation models outperform general LLMs due to their ability to produce executable Python programs for financial calculations
- Core assumption: Python programs accurately capture financial reasoning steps
- Evidence: CodeLlama and CodeStarCoder show higher accuracy with longer context

### Mechanism 3
- Retrieval-based approaches reduce search space compared to retrieval-free methods, though both struggle with 100K+ token documents
- Core assumption: Retrieval effectively identifies relevant chunks containing needed information
- Evidence: Mistral and GPT-4 outperform retrieval-free counterparts with retrieval

## Foundational Learning
- Financial document structure and terminology: Understanding 10-K reports and financial terminology is crucial for interpreting questions and identifying relevant information
  - Quick check: Can you explain the difference between a 10-K report and a 10-Q report?
- Numerical reasoning and financial calculations: Questions require performing calculations with financial data
  - Quick check: Given share repurchase data, can you write Python to calculate percentage of shares repurchased?
- Long-context processing and retrieval techniques: Documents averaging 123K words require effective search and retrieval methods
  - Quick check: How would you chunk a 200K word document for retrieval with appropriate overlap?

## Architecture Onboarding
- Component map: SEC filing collection → HTML/XML parsing → Markdown conversion → Chunking → Retrieval (ColBERT/Sentence-BERT/OpenAI Ada) → QA with few-shot in-context learning → Answer generation
- Critical path: Document parsing → Chunking and alignment → Retrieval → QA with few-shot learning → Answer generation and evaluation
- Design tradeoffs: Chunk size vs. overlap, dense vs. sparse retrieval, few-shot examples vs. context window space
- Failure signatures: Low HR@k scores indicate poor retrieval; LLM failing to generate correct programs suggests insufficient few-shot examples; performance gap between dev/test sets indicates overfitting
- First 3 experiments: 1) Compare HR@k scores for different retrieval models, 2) Test few-shot settings with different chunk contexts, 3) Evaluate code generation models vs. general LLMs

## Open Questions the Paper Calls Out
- How do retrieval-free methods scale beyond 128K tokens for documents exceeding 200K tokens?
- What is the impact of document structure and formatting on retrieval accuracy for financial documents?
- How does the quality of automatically generated Python programs affect model performance and what are the error rates?

## Limitations
- Evaluation focuses primarily on English SEC filings and may not generalize to other financial reporting formats or languages
- Dataset inherits potential biases from FinQA question distribution
- Human evaluation was limited to 10 examples from development set, potentially not representative

## Confidence
- High confidence: Dataset construction methodology and basic retrieval results
- Medium confidence: Comparative performance of different models and human non-expert accuracy baseline
- Medium confidence: Conclusions about retrieval vs. retrieval-free performance

## Next Checks
1. Conduct larger-scale human evaluation (100+ examples) across diverse question types to validate 41% non-expert accuracy baseline
2. Test retrieval pipeline with different chunk sizes and overlap percentages to optimize context coverage
3. Evaluate model performance on held-out SEC filings from different time periods to assess temporal generalization