---
ver: rpa2
title: 'Efficient Data Generation for Source-grounded Information-seeking Dialogs:
  A Use Case for Meeting Transcripts'
arxiv_id: '2405.01121'
source_url: https://arxiv.org/abs/2405.01121
tags:
- meeting
- response
- mised
- query
- should
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper presents a semi-automated method for generating source-grounded
  information-seeking dialog datasets. The approach uses LLM prompting to simulate
  the Wizard-of-Oz process, generating user queries and agent responses, followed
  by human validation and attribution identification.
---

# Efficient Data Generation for Source-grounded Information-seeking Dialogs: A Use Case for Meeting Transcripts

## Quick Facts
- arXiv ID: 2405.01121
- Source URL: https://arxiv.org/abs/2405.01121
- Reference count: 40
- Creates MISeD dataset: 432 dialogs with 4,161 query-response pairs from meeting transcripts

## Executive Summary
This paper presents a semi-automated method for generating source-grounded information-seeking dialog datasets using LLM prompting combined with human validation. Applied to meeting transcripts, the approach simulates the Wizard-of-Oz process by generating user queries and agent responses with LLMs, then having humans validate and edit the results. The resulting MISeD dataset enables training smaller models that can approach the performance of much larger pre-trained LLMs on both the MISeD test set and a novel fully-manual WOZ test set.

## Method Summary
The method uses LLM prompting to simulate Wizard-of-Oz dialog generation, with templates guiding query diversity and instructions ensuring grounded, neutral responses. Generated dialogs undergo human validation where annotators discard low-quality queries (6% rate) and correct responses (11% rate). Attribution spans are manually identified post-generation. The dataset is used to fine-tune models like LongT5 (3B parameters), which then achieve performance approaching much larger models like Gemini Ultra on both the MISeD test set and a novel fully-manual WOZ test set.

## Key Results
- MISeD dataset created with 432 dialogs and 4,161 query-response pairs from meeting transcripts
- Human evaluation shows MISeD responses are of comparable or slightly better quality than fully-manual responses
- Finetuned LongT5 models approach the performance of much larger pre-trained LLMs (0.3 points difference in 4-point human score)
- Models finetuned on MISeD demonstrate superior performance on both MISeD and WOZ test sets

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Using LLM-generated dialog followed by human validation reduces the cost of creating source-grounded dialog datasets compared to fully manual WOZ methodology.
- Mechanism: LLM prompts simulate both user query and agent response generation, creating candidate dialog turns that humans then verify and correct, eliminating the need for two annotators per dialog turn.
- Core assumption: LLM-generated dialogs are of sufficient quality to reduce but not eliminate human effort, and human verification can catch errors efficiently.
- Evidence anchors:
  - [abstract] "Our method partially automates the dialog creation process by combining LLM-based generation with human verification and editing."
  - [section 4.2] "During validation, annotators eliminated 6% of the queries, yielding 4161 turns. Of these remaining turns, annotators corrected 11% of the responses."
  - [corpus] Weak - corpus provides no direct evidence on cost comparison, though the paper notes WOZ is "time-consuming and expensive."
- Break condition: If LLM-generated dialogs require extensive human correction or if human validation time exceeds WOZ time savings.

### Mechanism 2
- Claim: Finetuning smaller models on MISeD data enables them to approach the performance of much larger pre-trained LLMs.
- Mechanism: MISeD training provides task-specific knowledge that compensates for the smaller model size, allowing LongT5 (3B parameters) to match or exceed Gemini Pro and approach Gemini Ultra performance.
- Core assumption: The task-specific patterns learned from MISeD are sufficient to overcome the inherent knowledge gap between smaller and larger models.
- Evidence anchors:
  - [abstract] "Models finetuned with MISeD demonstrate superior performance on our test set, as well as on a novel fully-manual WOZ test set and an existing query-based summarization benchmark, suggesting the utility of our approach."
  - [section 7.3.1] "The LongT5 model finetuned on MISeD achieves results close to the other much larger models (about 0.3 points difference in the 4-points human score)."
  - [corpus] Weak - corpus doesn't provide direct evidence of this mechanism, though related work on model scaling is cited.
- Break condition: If finetuned smaller models cannot generalize beyond MISeD patterns or if larger models retain significant advantages.

### Mechanism 3
- Claim: MISeD's template-based query generation strategy produces sufficiently diverse queries to train effective dialog agents.
- Mechanism: Query templates derived from QMSum schema (general, specific, yes/no, unanswerable, context-dependent) guide LLM to generate varied queries that cover typical meeting discussion patterns.
- Core assumption: A finite set of query templates can represent the diversity of real user questions about meeting transcripts.
- Evidence anchors:
  - [section 4.1.1] "Following their query types and templates, we created a corresponding set of query templates adapted for guiding LLMs (rather than humans)."
  - [section 5.1.2] Query distribution shows balanced representation across template types: "query type: general 20.91%, query type: specific 52.37%, query type: yes/no 26.72%."
  - [corpus] Weak - corpus doesn't evaluate whether template coverage matches real query diversity.
- Break condition: If models trained on MISeD fail to handle query types outside the template schema or if human evaluation reveals insufficient query diversity.

## Foundational Learning

- Concept: Source-grounded dialog task definition and evaluation metrics
  - Why needed here: Understanding how to evaluate responses and attributions is critical for developing and testing dialog agents
  - Quick check question: What are the two main dimensions for evaluating agent models in this task?

- Concept: LLM prompting strategies for controlled generation
  - Why needed here: The method relies on carefully crafted prompts to guide LLM behavior for both queries and responses
  - Quick check question: How do the query and response prompts differ in their instructions and inputs?

- Concept: Attribution evaluation methods
  - Why needed here: Attribution quality is a key component of the task, requiring specialized evaluation approaches
  - Quick check question: What protocol is used for human evaluation of attribution quality?

## Architecture Onboarding

- Component map: LLM prompt generation -> Automatic dialog generation -> Human validation/annotation -> Dataset creation -> Model finetuning -> Evaluation
- Critical path: Automatic dialog generation -> Human validation -> Model finetuning -> Evaluation
- Design tradeoffs:
  - Automation vs quality: Full automation would be faster but current attribution detection isn't reliable enough
  - Model size vs performance: Smaller finetuned models can approach larger models when trained on MISeD
  - Template diversity vs coverage: More templates increase diversity but require more manual effort
- Failure signatures:
  - Low human scores with high automatic scores suggests attribution evaluation issues
  - WOZ test set performance much lower than MISeD test set suggests overfitting to generation protocol
  - High annotation correction rates (>20%) suggests LLM generation quality issues
- First 3 experiments:
  1. Run LLM generation on 10 meeting transcripts, measure human correction rate and time
  2. Finetune LongT5 on MISeD training set, evaluate on both MISeD and WOZ test sets
  3. Test attribution evaluation methods on sample responses to validate automatic metrics

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can attribution generation be automated effectively for long-form meeting transcripts?
- Basis in paper: [explicit] The authors note that current LLM-based attribution detection "does not perform sufficiently well" and leave attribution generation as a fully-manual task, highlighting it as a key limitation.
- Why unresolved: Automating attribution is challenging due to the unstructured nature of meeting transcripts, the length of documents, and the need for factual consistency between responses and source text.
- What evidence would resolve it: Development and evaluation of models that can reliably identify attribution spans for responses in meeting dialog data, demonstrating comparable quality to human annotation.

### Open Question 2
- Question: What are the optimal strategies for handling long meeting transcripts that exceed LLM context limits?
- Basis in paper: [explicit] The authors discuss that long transcripts exceeding model context length require either truncation or integrating retrieval systems to condense the transcript.
- Why unresolved: Different truncation strategies and retrieval-based approaches have trade-offs in terms of computational cost, information loss, and response quality that need systematic evaluation.
- What evidence would resolve it: Comparative studies of different approaches (truncation methods, retrieval systems, hierarchical models) on response quality and attribution accuracy for long transcripts.

### Open Question 3
- Question: How can prompt template diversity be improved to generate more varied and natural user queries?
- Basis in paper: [explicit] The authors acknowledge their method "depends on manually crafted prompt templates" and suggest "research into more flexible and diverse query generation strategies" could improve the approach.
- Why unresolved: While the current template-based approach generates useful data, it may not capture the full diversity of natural user queries, potentially limiting the generalization of trained models.
- What evidence would resolve it: Development and evaluation of methods that generate more diverse queries (e.g., few-shot learning, learned prompt generation) and demonstrate improved model performance on out-of-distribution queries.

## Limitations
- Reliance on human validation and attribution identification prevents full automation of dataset creation
- 6% of generated queries discarded and 11% of responses corrected indicates LLM generation quality remains insufficient
- Manual attribution identification is time-consuming and potentially inconsistent across annotators
- Attribution detection automation is not yet reliable enough for this task

## Confidence

High:
- Cost reduction mechanism (6% discard and 11% correction rates provide direct evidence)
- Performance gains from finetuning (0.3 point difference between LongT5 and larger models explicitly measured)

Medium:
- Template diversity claims (balanced template coverage shown but no evidence matching real-world distributions)

## Next Checks

1. **Test generalization beyond templates**: Evaluate model performance on queries that don't fit the QMSum template schema to determine if the finetuned models can handle novel query types not present in MISeD training data.

2. **Measure true cost reduction**: Conduct a time-motion study comparing actual human hours required for the semi-automated method versus traditional WOZ methodology, including both LLM generation time and human validation time.

3. **Validate attribution consistency**: Have multiple annotators independently identify attributions for the same responses and measure inter-annotator agreement to assess the reliability and consistency of the manual attribution process.