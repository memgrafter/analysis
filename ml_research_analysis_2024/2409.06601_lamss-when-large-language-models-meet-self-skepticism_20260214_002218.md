---
ver: rpa2
title: 'LaMsS: When Large Language Models Meet Self-Skepticism'
arxiv_id: '2409.06601'
source_url: https://arxiv.org/abs/2409.06601
tags:
- skepticism
- lamss
- tokens
- uni00000013
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: LaMsS addresses LLM hallucination by embedding self-skepticism
  into model outputs. It augments the vocabulary with skepticism tokens, uses CPT
  and SFT to learn to emit these tokens alongside normal tokens, and estimates uncertainty
  from token probabilities.
---

# LaMsS: When Large Language Models Meet Self-Skepticism

## Quick Facts
- arXiv ID: 2409.06601
- Source URL: https://arxiv.org/abs/2409.06601
- Reference count: 22
- Primary result: LaMsS achieves higher accuracy, AUC, and AP than vanilla fine-tuning and R-tuning on multi-choice and QA benchmarks, with strong out-of-domain generalization.

## Executive Summary
LaMsS is a method for mitigating LLM hallucination by embedding self-skepticism into model outputs. It augments the vocabulary with skepticism tokens, uses continual pretraining (CPT) and supervised fine-tuning (SFT) to learn to emit these tokens alongside normal tokens, and estimates uncertainty from token probabilities. On multi-choice and QA benchmarks, LaMsS achieves higher accuracy, AUC, and AP than vanilla fine-tuning and R-tuning, with strong generalization to out-of-domain tasks. The skepticism threshold ϵ=0.5 yields optimal performance. Ablation confirms both CPT and the augmented self-assessment prompt are essential.

## Method Summary
LaMsS introduces skepticism tokens (<s0> to <s9>) into the LLM vocabulary. During continual pretraining (CPT), the model learns to pair normal tokens with skepticism tokens derived from discretized log-probabilities. Supervised fine-tuning (SFT) further calibrates skepticism using augmented queries ("Are you sure?") and ground-truth labels. At inference, responses are filtered by a skepticism threshold (ϵ=0.5), allowing the model to decline answering when uncertainty is high. The approach is evaluated on MMLU, WiCE, FEVER, ParaRel, and HotpotQA, with both in-domain (ID) and out-of-domain (OD) splits.

## Key Results
- LaMsS outperforms vanilla fine-tuning and R-tuning on accuracy, AUC, and AP across multiple benchmarks.
- Strong generalization to out-of-domain tasks, with consistent gains in both ID and OD settings.
- Optimal performance at skepticism threshold ϵ=0.5.
- Both CPT and skepticism token augmentation are necessary for peak performance.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Augmenting vocabulary with skepticism tokens allows the model to emit probabilistic self-assessment signals alongside factual responses.
- Mechanism: The model is trained to output normal tokens paired with skepticism tokens, where skepticism levels are derived from log-probabilities of preceding tokens. During inference, skepticism levels are extracted either from the skepticism token sequence or via an augmented "Are you sure?" query.
- Core assumption: Token probabilities reflect the model's confidence in the plausibility of the generated text.
- Evidence anchors:
  - [abstract] "By calculating the response skepticism given a query, one can define a new self-aware LLM which is only willing to answer with relative lower skepticism level than the threshold."
  - [section] "We quantitatively correlate the skepticism tokenization with the log scale of log probability of precedent norm tokens."
- Break condition: If the mapping between log-probabilities and skepticism levels is not stable across domains or tasks, the self-assessment may become unreliable.

### Mechanism 2
- Claim: Dual-stage training (CPT + SFT) enables both generalization of skepticism detection and task-specific calibration.
- Mechanism: CPT learns to associate low-probability tokens with high skepticism across diverse text. SFT fine-tunes this behavior on QA data with explicit "sure/unsure" labels, improving calibration on specific tasks.
- Core assumption: Skepticism learned in CPT transfers to task-specific settings when refined with SFT.
- Evidence anchors:
  - [abstract] "By introducing a series of skepticism tokens and augmenting them into the vocabulary, we conduct both pertaining and finetuning..."
  - [section] "The query-response finetuning stage then follows, with an extra rethinking question augmented similar to R-tuning, to further strengthen the skeptical feeling accuracy..."
- Break condition: If skepticism tokens are not sufficiently expressive or the fine-tuning data is too narrow, calibration may not generalize.

### Mechanism 3
- Claim: Thresholding on skepticism probability filters out low-confidence responses, improving accuracy.
- Mechanism: At inference, responses are only accepted if the associated skepticism level is below a predefined threshold (ϵ=0.5 in experiments).
- Core assumption: There is a monotonic relationship between skepticism level and the likelihood of hallucination.
- Evidence anchors:
  - [abstract] "one can define a new self-aware LLM which is only willing to answer with relative lower skepticism level than the threshold."
  - [section] "To be consistent with the training setting, we use the same skeptical threshold ϵ = 0 .5 to judge if the model is 'willing' to answer."
- Break condition: If the threshold is not well-calibrated for a new domain, the model may become overly conservative or permissive.

## Foundational Learning

- Concept: Probability discretization and tokenization
  - Why needed here: Skepticism tokens are derived from discretized log-probability values of preceding tokens.
  - Quick check question: How does discretizing log-probabilities into skepticism levels affect model calibration?

- Concept: Continual pretraining (CPT) with auxiliary objectives
  - Why needed here: CPT adds skepticism token prediction as an auxiliary loss to adapt the base LLM to the new paired-token paradigm.
  - Quick check question: Why is it important to include both normal token and skepticism token losses during CPT?

- Concept: Supervised fine-tuning (SFT) with confidence-augmented data
  - Why needed here: SFT uses augmented queries ("Are you sure?") and ground-truth confidence labels to refine skepticism calibration.
  - Quick check question: How does augmenting queries with "Are you sure?" improve the model's ability to self-assess?

## Architecture Onboarding

- Component map: Tokenizer → CPT → SFT → Inference → Threshold filter
- Critical path: Tokenizer → CPT → SFT → Inference → Threshold filter
- Design tradeoffs:
  - Token vocabulary expansion vs. model capacity and speed
  - Discrete skepticism levels vs. continuous confidence scores
  - Threshold calibration vs. coverage of answerable questions
- Failure signatures:
  - High rejection rate (threshold too strict)
  - Low accuracy despite high willingness (threshold too permissive)
  - Poor generalization to out-of-domain data
- First 3 experiments:
  1. Verify skepticism token decoding and threshold filtering on a small in-domain QA set.
  2. Compare CPT-only vs. CPT+SFT calibration on multi-choice benchmarks.
  3. Test ablation of skepticism token augmentation on model accuracy and willingness.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of LaMsS vary when using different skepticism token granularities (e.g., 10 levels vs. 20 levels)?
- Basis in paper: [inferred] The paper uses 10 skepticism tokens (s0 to s9) and shows sensitivity analysis around the threshold of 0.5, but does not explore varying the granularity of skepticism levels.
- Why unresolved: The paper does not provide results or discussion on the impact of increasing or decreasing the number of skepticism levels, which could affect model performance and calibration.
- What evidence would resolve it: Experimental results comparing models trained with different numbers of skepticism tokens (e.g., 5, 10, 20 levels) on the same benchmarks, showing changes in accuracy, AUC, and AP.

### Open Question 2
- Question: How does LaMsS perform when applied to non-text modalities, such as image or audio inputs, where hallucination is also a concern?
- Basis in paper: [inferred] The paper focuses exclusively on text-based LLMs and does not address multimodal applications or how the skepticism framework might generalize to other data types.
- Why unresolved: The methodology is described only in the context of textual data, leaving open the question of adaptability to other modalities.
- What evidence would resolve it: Empirical studies applying LaMsS or a similar skepticism token framework to multimodal models (e.g., vision-language or speech-language models) and measuring hallucination reduction.

### Open Question 3
- Question: What is the impact of the skepticism threshold (ϵ) on the model's willingness to answer versus its accuracy, and is there an optimal trade-off?
- Basis in paper: [explicit] The paper performs sensitivity analysis on ϵ and finds a peak at 0.5, but does not deeply explore the trade-off between willingness to answer and accuracy across different thresholds.
- Why unresolved: While the paper identifies an optimal threshold, it does not analyze the balance between coverage (willingness to answer) and correctness (accuracy) at different ϵ values.
- What evidence would resolve it: Detailed analysis plotting willingness to answer and accuracy as functions of ϵ, identifying regions where one metric improves at the expense of the other.

### Open Question 4
- Question: How does the inclusion of skepticism tokens affect the computational efficiency and inference speed of the model compared to vanilla fine-tuning?
- Basis in paper: [inferred] The paper does not discuss or measure the computational overhead introduced by the additional skepticism tokens and corresponding processing during inference.
- Why unresolved: The methodology introduces new tokens and stages, but there is no mention of inference latency, memory usage, or training time compared to baselines.
- What evidence would resolve it: Benchmarks comparing inference time, memory footprint, and training duration of LaMsS versus vanilla fine-tuning and R-tuning on identical hardware.

## Limitations
- Skepticism token calibration may not be stable across all domains or tasks.
- Vocabulary expansion and additional inference steps may reduce computational efficiency.
- Performance depends on the quality and diversity of pretraining and fine-tuning data.

## Confidence
- **High Confidence**: Experimental results show consistent improvements across multiple benchmarks compared to baselines; ablation confirms necessity of CPT and skepticism token augmentation.
- **Medium Confidence**: The mechanism of using skepticism tokens derived from log-probabilities is plausible and supported by experimental design, but stability across domains is not fully validated.
- **Low Confidence**: Generalizability of the ϵ=0.5 threshold to unseen domains or tasks is not rigorously tested; sensitivity to different threshold values or dataset variations is not explored.

## Next Checks
1. Evaluate LaMsS on a diverse set of out-of-domain tasks to assess whether skepticism token calibration and threshold (ϵ=0.5) remain effective.
2. Systematically vary the skepticism threshold (e.g., ϵ ∈ {0.3, 0.5, 0.7}) and measure its impact on accuracy, willingness to answer, and calibration across multiple benchmarks.
3. Repeat the CPT and SFT stages using a different pretraining corpus (e.g., C4 or The Stack) to determine whether the approach is robust to changes in pretraining data and whether skepticism token calibration is sensitive to corpus characteristics.