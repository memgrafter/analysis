---
ver: rpa2
title: Dual-Space Knowledge Distillation for Large Language Models
arxiv_id: '2406.17328'
source_url: https://arxiv.org/abs/2406.17328
tags:
- teacher
- student
- space
- framework
- dskd
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper identifies a fundamental limitation in the standard
  knowledge distillation framework for large language models: the output distributions
  from teacher and student models are in different output spaces due to different
  prediction heads, which leads to suboptimal similarity on both representation and
  distribution levels and prevents distillation between models with different vocabularies.
  To address this, the authors propose Dual-Space Knowledge Distillation (DSKD), which
  unifies the output spaces by projecting teacher and student hidden states into each
  other''s representation space and using shared prediction heads.'
---

# Dual-Space Knowledge Distillation for Large Language Models

## Quick Facts
- arXiv ID: 2406.17328
- Source URL: https://arxiv.org/abs/2406.17328
- Reference count: 22
- Key outcome: Standard KD fails when teacher/student vocabularies differ; DSKD unifies output spaces via dual projection and achieves 18.69% vs 17.63% Rouge-L with GPT2-120M, 25.05% vs 23.53% with TinyLLaMA-1.1B

## Executive Summary
This paper addresses a fundamental limitation in knowledge distillation for large language models: standard approaches fail when teacher and student models use different tokenizers or vocabularies. The authors identify that this occurs because output distributions exist in different spaces due to distinct prediction heads. They propose Dual-Space Knowledge Distillation (DSKD), which projects teacher and student hidden states into each other's representation spaces using shared prediction heads. The method also introduces Cross-Model Attention (CMA) to automatically align tokens from differently tokenized sequences, enabling effective distillation between models with incompatible vocabularies.

## Method Summary
DSKD operates by creating a unified framework where both teacher and student models share prediction heads in each other's output spaces. The method projects hidden states bidirectionally: teacher states are projected into student space and vice versa. This allows knowledge transfer despite vocabulary differences. The Cross-Model Attention mechanism automatically aligns tokens from sequences that are tokenized differently by the two models, addressing the core challenge of vocabulary mismatch. During training, the student learns from both its own predictions and the teacher's adapted representations, creating a more robust distillation process that works across different tokenizer architectures.

## Key Results
- DSKD achieves 18.69% average Rouge-L score with GPT2-120M student vs 17.63% for standard KD
- DSKD achieves 25.05% average Rouge-L score with TinyLLaMA-1.1B student vs 23.53% for standard KD
- DSKD significantly outperforms standard white-box KD across various distance functions on instruction-following benchmarks

## Why This Works (Mechanism)
Standard KD fails when vocabularies differ because the teacher and student produce distributions in incompatible output spaces. DSKD solves this by creating a dual-space framework where both models can communicate through shared projections. The bidirectional projection ensures that knowledge flows correctly regardless of tokenizer differences, while CMA handles the token alignment challenge automatically.

## Foundational Learning

- **Knowledge Distillation**: Technique for transferring knowledge from large teacher models to smaller students; needed because training small models from scratch is inefficient; quick check: can student match teacher performance on downstream tasks
- **Tokenization/Vocabulary Mismatch**: Different models use different tokenizers creating incompatible output spaces; needed because modern LLMs use varied tokenization strategies; quick check: do teacher/student vocabularies overlap sufficiently
- **Cross-Model Attention**: Mechanism for aligning tokens across different tokenization schemes; needed because standard alignment fails with different vocabularies; quick check: does CMA correctly map corresponding tokens
- **Dual-Space Projection**: Bidirectional mapping of hidden states between teacher and student spaces; needed to unify incompatible output distributions; quick check: do projected distributions maintain semantic coherence
- **Instruction-Following Benchmarks**: Evaluation tasks measuring model ability to follow natural language instructions; needed to validate practical utility; quick check: do improvements generalize beyond specific tasks

## Architecture Onboarding

**Component Map**: Input -> Teacher Model -> Hidden States -> Dual Projection -> Shared Prediction Heads -> Student Model -> Hidden States -> Dual Projection -> Shared Prediction Heads -> Output

**Critical Path**: Teacher forward pass → Hidden state projection → Shared prediction head → Student forward pass → Hidden state projection → Shared prediction head → Loss computation

**Design Tradeoffs**: 
- Dual projection adds computational overhead but enables cross-vocabulary training
- CMA increases complexity but provides automatic token alignment
- Shared prediction heads reduce parameter count but require careful initialization

**Failure Signatures**: 
- Poor performance indicates projection misalignment or CMA failure
- Training instability suggests incompatible teacher/student architectures
- No improvement over standard KD indicates dual-space mechanism not functioning

**First Experiments**: 
1. Validate bidirectional projections preserve semantic meaning
2. Test CMA token alignment accuracy on known correspondences
3. Compare standard KD vs DSKD on same-vocabulary pairs

## Open Questions the Paper Calls Out
None

## Limitations
- Scalability to extremely large models (10B+ parameters) not fully characterized
- CMA robustness across diverse tokenizer architectures beyond GPT2 and LLaMA variants
- Evaluation focuses primarily on Rouge-L scores, which may not capture all quality aspects

## Confidence

High: Identification of vocabulary mismatch as fundamental KD limitation
Medium: DSKD effectiveness within tested model scale and tasks
Medium: CMA mechanism handling different tokenizers given limited scope

## Next Checks

1. Evaluate DSKD performance and computational overhead when scaling to models with 10B+ parameters
2. Test CMA with broader range of tokenizer architectures including sentencepiece and byte-level tokenizers
3. Conduct ablation studies isolating contributions of dual-projection mechanism versus CMA to performance gains