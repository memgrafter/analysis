---
ver: rpa2
title: 'OMPO: A Unified Framework for RL under Policy and Dynamics Shifts'
arxiv_id: '2405.19080'
source_url: https://arxiv.org/abs/2405.19080
tags:
- uni00000013
- uni00000048
- policy
- uni00000056
- uni00000055
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes OMPO, a unified framework for online reinforcement
  learning under policy and dynamics shifts. The key idea is to match transition occupancy
  distributions across varying policies and dynamics by introducing a discriminator
  to distinguish on-policy from historical data, enabling effective learning from
  data with distribution shifts.
---

# OMPO: A Unified Framework for RL under Policy and Dynamics Shifts

## Quick Facts
- arXiv ID: 2405.19080
- Source URL: https://arxiv.org/abs/2405.19080
- Reference count: 40
- Primary result: Unified framework handles policy and dynamics shifts via transition occupancy matching

## Executive Summary
OMPO presents a unified framework for online reinforcement learning under both policy and dynamics shifts. The key innovation is matching transition occupancy distributions across varying policies and dynamics by using a discriminator to distinguish on-policy from historical data. This enables effective learning from data with distribution shifts. The method is instantiated as a tractable min-max optimization problem with an actor-critic structure. Experiments across stationary dynamics, domain adaptation, and non-stationary dynamics scenarios show OMPO consistently outperforms specialized baselines in all settings, particularly when combined with domain randomization.

## Method Summary
OMPO addresses the challenge of training RL policies using data collected under varying policies or dynamics. The framework introduces transition occupancy matching to handle both policy and dynamics shifts in a unified manner. It employs a discriminator to distinguish between on-policy and historical data samples, enabling effective learning from distribution-shifted data. The method uses a small-size local buffer for fresh on-policy samples and a global buffer for historical data. The optimization problem is reformulated as a tractable min-max problem solvable via an actor-critic structure. Experiments validate OMPO across three scenarios: stationary dynamics, domain adaptation, and non-stationary dynamics.

## Key Results
- Consistently outperforms specialized baselines across all three shift scenarios
- Achieves strong performance when combined with domain randomization
- Demonstrates superior sample efficiency and final performance
- Handles both policy and dynamics shifts without requiring separate algorithms

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Transition occupancy matching unifies policy and dynamics shift handling by modeling distribution discrepancies as (s,a,s') tuples
- Mechanism: The framework explicitly models transition occupancy distribution ρπT(s,a,s′) and uses a discriminator h(s,a,s′) to distinguish between on-policy samples from DL and historical samples from DG, thereby correcting distribution shifts
- Core assumption: The Bellman flow constraint holds and the feasible assumption (at least one pair (s,a) satisfies the constraint) is satisfied
- Evidence anchors:
  - [abstract]: "transition occupancy matching" and "discriminator to distinguish on-policy from historical data"
  - [section 4.1]: "transition occupancy distribution considers the normalized discounted marginal distributions of state-actions pair (s,a) as well as the next states s′"
  - [corpus]: Weak - related papers focus on offline RL and model-based RL but don't explicitly address transition occupancy matching
- Break condition: If the Bellman flow constraint is violated or the feasible assumption fails, the dual reformulation breaks down

### Mechanism 2
- Claim: Dual reformulation converts the constrained optimization problem into a tractable min-max problem
- Mechanism: By introducing Lagrangian multiplier Q(s,a) and applying Fenchel conjugate, the framework transforms the surrogate objective into a min-max optimization problem solvable via actor-critic structure
- Core assumption: Slater's condition holds, ensuring strong duality between the primal and dual problems
- Evidence anchors:
  - [section 4.2]: "the primal problem can be converted to the following equivalent unconstrained min-max optimization problem"
  - [section 4.3]: "the min-max problem can be transformed as" with Fenchel conjugate
  - [corpus]: Weak - related works use DICE methods but don't explicitly mention the same dual reformulation approach
- Break condition: If Slater's condition fails or the Fenchel conjugate transformation is invalid, the min-max formulation breaks

### Mechanism 3
- Claim: Local buffer DL captures current policy dynamics while global buffer DG stores historical data, enabling effective shift handling
- Mechanism: DL stores fresh on-policy samples under target/local consistent dynamics, while DG stores historical data with various policy and dynamics shifts. This separation allows the discriminator to learn effective distinction
- Core assumption: Local consistency assumption - current dynamics T can be captured in recent environmental interaction data
- Evidence anchors:
  - [abstract]: "small-size local buffer" and "global buffer for historical data"
  - [section 4.3]: "DL is used to store the fresh data sampled by the current policy under the target/local consistent dynamics; while DG stores the historical data"
  - [corpus]: Weak - related works mention replay buffers but don't explicitly discuss the local/global buffer separation strategy
- Break condition: If the local consistency assumption fails or the buffer sizes are inappropriate, the shift handling breaks down

## Foundational Learning

- Concept: Transition occupancy distribution ρπT(s,a,s′)
  - Why needed here: It extends state-action occupancy to include next state s′, capturing dynamics shifts that state-action alone cannot
  - Quick check question: What additional information does ρπT(s,a,s′) capture compared to ρπ(s,a)?

- Concept: Fenchel conjugate for f-divergence transformation
  - Why needed here: It transforms the f-divergence term into a form that can be optimized using the accessible distribution ρbπbT
  - Quick check question: How does the Fenchel conjugate help eliminate the unknown distribution ρπbT from the optimization?

- Concept: Bellman flow constraint for occupancy distributions
  - Why needed here: It provides the constraint that allows reformulation of the optimization problem and variable substitution
  - Quick check question: What equation represents the Bellman flow constraint for transition occupancy distributions?

## Architecture Onboarding

- Component map: Local buffer DL -> Discriminator training -> Compute R(s,a,s′) -> Update critic Q -> Update policy π
- Critical path: DL → Discriminator training → Compute R(s,a,s′) → Update critic Q → Update policy π
- Design tradeoffs:
  - Buffer size DL vs stability: Smaller DL captures shifts better but causes discriminator instability
  - Weighted factor α vs reward subordination: Larger α overemphasizes distribution discrepancy, smaller α weakens discriminator
  - Order q of Fenchel conjugate vs performance: q ∈ [1.2, 2] works, q=1.5 optimal in experiments
- Failure signatures:
  - Discriminator collapse: h(s,a,s′) outputs constant values
  - Critic divergence: Q-values explode or become NaN
  - Policy collapse: Policy outputs degenerate distributions
- First 3 experiments:
  1. Stationary environment test: Compare OMPO vs SAC on Hopper-v3 with standard hyperparameters
  2. Domain adaptation test: Train on source dynamics, evaluate on target dynamics (e.g., double gravity in Ant-v3)
  3. Buffer size ablation: Test |DL| = 500, 1000, 1500 on non-stationary Walker2d task

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of OMPO compare to other methods when dealing with non-stationary environments where the dynamics change more frequently than assumed by the local consistency assumption?
- Basis in paper: [inferred] The paper mentions the local consistency assumption, stating that "the current dynamics would keep a period before varying," but does not explore scenarios with more rapid dynamics changes
- Why unresolved: The paper's experiments focus on dynamics changes occurring at the episode level or with stochastic variations at each time step, but do not investigate more frequent changes
- What evidence would resolve it: Experiments comparing OMPO's performance to other methods in environments with more frequent dynamics changes, such as multiple changes within a single episode

### Open Question 2
- Question: Can OMPO be extended to handle partially observable Markov decision processes (POMDPs) or other settings where the state information is incomplete or noisy?
- Basis in paper: [inferred] The paper does not discuss POMDPs or settings with incomplete state information, focusing instead on fully observable environments
- Why unresolved: The theoretical framework and experimental evaluation are based on fully observable MDPs, leaving the extension to POMDPs unexplored
- What evidence would resolve it: An extension of OMPO's framework to handle POMDPs, along with experimental results demonstrating its effectiveness in partially observable environments

### Open Question 3
- Question: How does the choice of the weighted factor α impact the trade-off between exploration and exploitation in OMPO?
- Basis in paper: [explicit] The paper discusses the weighted factor α in the context of balancing the scale of the distribution discrepancy term, but does not explore its impact on exploration-exploitation trade-off
- Why unresolved: The paper focuses on the theoretical justification for including α and its impact on stability, but does not investigate its role in balancing exploration and exploitation
- What evidence would resolve it: Experiments varying α across a wider range and analyzing its impact on the agent's exploration behavior and eventual performance

## Limitations
- Performance relies on Bellman flow constraint satisfaction and local consistency assumption, neither empirically verified
- Buffer size of 1000 samples may not be universally optimal across different shift scenarios
- Framework assumes fully observable MDPs, extension to POMDPs remains unexplored

## Confidence
- High confidence: The transition occupancy matching framework provides a mathematically sound approach to handling distribution shifts in RL
- Medium confidence: The dual reformulation and Fenchel conjugate transformations are correctly derived and lead to tractable optimization
- Low confidence: The local buffer strategy (DL vs DG separation) is optimal and the buffer size of 1000 samples is universally appropriate

## Next Checks
1. **Constraint validation test**: Systematically test the Bellman flow constraint satisfaction under varying degrees of dynamics shifts to quantify when the framework breaks down
2. **Buffer size sensitivity analysis**: Evaluate performance across a wider range of local buffer sizes (100-5000) to determine optimal sizing for different shift scenarios
3. **Discriminator collapse detection**: Implement monitoring for discriminator collapse (h(s,a,s′) outputting constant values) and test regularization techniques to prevent this failure mode