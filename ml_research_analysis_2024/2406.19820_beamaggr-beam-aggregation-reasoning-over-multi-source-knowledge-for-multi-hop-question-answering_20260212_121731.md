---
ver: rpa2
title: 'BeamAggR: Beam Aggregation Reasoning over Multi-source Knowledge for Multi-hop
  Question Answering'
arxiv_id: '2406.19820'
source_url: https://arxiv.org/abs/2406.19820
tags:
- reasoning
- knowledge
- question
- answer
- questions
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: BeamAggR is a novel reasoning framework for knowledge-intensive
  multi-hop question answering that decomposes complex questions into trees and performs
  bottom-up reasoning. The method introduces beam aggregation to explore multiple
  reasoning trajectories at each step, while also integrating knowledge from multiple
  sources including internal knowledge, Wikipedia, and web search.
---

# BeamAggR: Beam Aggregation Reasoning over Multi-source Knowledge for Multi-hop Question Answering

## Quick Facts
- arXiv ID: 2406.19820
- Source URL: https://arxiv.org/abs/2406.19820
- Authors: Zheng Chu; Jingchang Chen; Qianglong Chen; Haotian Wang; Kun Zhu; Xiyuan Du; Weijiang Yu; Ming Liu; Bing Qin
- Reference count: 40
- Primary result: 8.5% average improvement over previous state-of-the-art on four open-domain multi-hop reasoning datasets

## Executive Summary
BeamAggR introduces a novel reasoning framework for knowledge-intensive multi-hop question answering that decomposes complex questions into trees and performs bottom-up reasoning. The method introduces beam aggregation to explore multiple reasoning trajectories at each step, while also integrating knowledge from multiple sources including internal knowledge, Wikipedia, and web search. By combining these two innovations, BeamAggR significantly outperforms previous state-of-the-art methods across four open-domain multi-hop reasoning datasets while maintaining computational efficiency.

## Method Summary
BeamAggR parses complex questions into tree structures with atomic and composite questions, then performs bottom-up reasoning with beam aggregation. For each atomic question, it uses four reasoning strategies (closed-book, parametric, Wikipedia retrieval, web search) and aggregates the answers probabilistically. At each intermediate node, it enumerates combinations of candidate answers from sub-questions, conducts reasoning on all combinations, and then aggregates the results based on their marginal probabilities. This approach reduces cascading errors and leverages complementary knowledge from multiple sources.

## Key Results
- Achieves 8.5% average improvement over previous state-of-the-art methods across four datasets
- Demonstrates superior knowledge collaboration and answer aggregation capabilities
- Maintains computational efficiency while exploring multiple reasoning trajectories
- Shows consistent performance gains across HotpotQA, MuSiQue, 2WikiMQA, and Bamboogle datasets

## Why This Works (Mechanism)

### Mechanism 1
- Claim: BeamAggR reduces cascading errors by exploring multiple reasoning trajectories at each hop and probabilistically selecting the most promising path.
- Mechanism: Instead of committing to a single reasoning path at each hop, BeamAggR enumerates combinations of candidate answers from sub-questions, conducts reasoning on all combinations, and then aggregates the results based on their marginal probabilities. This allows the model to explore a broader reasoning space and correct errors that may have occurred in earlier hops.
- Core assumption: The most promising reasoning path at each hop has the highest marginal probability when considering all possible combinations of sub-question answers.
- Evidence anchors:
  - [abstract]: "BeamAggR explores and prioritizes promising answers at each hop of question."
  - [section 3.3]: "In intermediate nodes, we need to conduct reasoning based on the answers derived from previous sub-questions, and each sub-question is associated with a set of answers and probabilities, termed candidates (or beams)."
  - [corpus]: Weak evidence. The corpus mentions iterative retrieval methods like IRCoT and FLARE, but doesn't explicitly discuss beam aggregation or probabilistic path selection. The closest is "Multi-granular Training Strategies for Robust Multi-hop Reasoning Over Noisy and Heterogeneous Knowledge Sources," which suggests exploring multiple granularities but doesn't mention beams or probabilities.
- Break condition: If the marginal probability distribution becomes too flat (high entropy), the model loses the ability to distinguish promising from unpromising paths, leading to random selection.

### Mechanism 2
- Claim: Complementary multi-source reasoning mitigates knowledge omissions and conflicts by dynamically selecting and integrating knowledge from multiple sources with fine granularity.
- Mechanism: For each atomic question, BeamAggR performs reasoning on four different knowledge sources: closed-book reasoning (internal knowledge), parametric knowledge generation, Wikipedia retrieval, and web search. The answers from these sources are then aggregated through voting and normalized into a probability distribution. This allows the model to leverage different types of knowledge and avoid over-reliance on any single source.
- Core assumption: Different knowledge sources provide complementary information, and combining them improves overall reasoning accuracy compared to using any single source.
- Evidence anchors:
  - [abstract]: "BeamAggR dynamically integrates multi-source knowledge in fine granularity during reasoning, fostering knowledge collaboration."
  - [section 3.2]: "To avoid the hallucination caused by lack of knowledge, we use four reasoning strategies combined with answer normalization to fuse information from diverse knowledge sources."
  - [corpus]: Weak evidence. The corpus mentions "Multi-source multi-hop question answering" but doesn't discuss the specific strategy of combining four knowledge sources with voting and normalization. The closest is "Multi-granular Training Strategies for Robust Multi-hop Reasoning Over Noisy and Heterogeneous Knowledge Sources," which suggests handling heterogeneous sources but doesn't mention the specific approach.
- Break condition: If one knowledge source becomes significantly more reliable than others (e.g., Wikipedia is complete and accurate for the domain), the aggregation becomes wasteful and the model should rely primarily on that source.

### Mechanism 3
- Claim: Question decomposition into trees enables more accurate retrieval by creating clearer, more specific retrieval queries for sub-questions.
- Mechanism: Instead of using the original complex question for retrieval, BeamAggR decomposes it into a tree structure with atomic sub-questions at the leaves and composite questions at internal nodes. Each atomic sub-question serves as a more focused retrieval query, yielding more relevant documents for that specific sub-task. The answers are then combined bottom-up through beam aggregation.
- Core assumption: Decomposing complex questions into simpler sub-questions results in more precise retrieval queries that better match the required knowledge, compared to using the full complex question.
- Evidence anchors:
  - [abstract]: "Concretely, we parse the complex questions into trees, which include atom and composite questions, followed by bottom-up reasoning."
  - [section 3.1]: "Multi-hop questions entail complex structures, such as bridge, comparison, composition and their integration. To address this, we parse complex questions into trees to express the reasoning structure."
  - [corpus]: Weak evidence. The corpus mentions "TreeRare: Syntax Tree-Guided Retrieval and Reasoning for Knowledge-Intensive Question Answering" which suggests using tree structures, but doesn't specifically discuss question decomposition or how it improves retrieval accuracy. The closest is "An Iterative Question-Guided Framework for Knowledge Base Question Answering," which mentions iterative approaches but not tree decomposition.
- Break condition: If the decomposition creates sub-questions that are too simple or too disconnected from the original intent, the aggregated answers may not properly address the complex question.

## Foundational Learning

- Concept: Probability theory and Bayesian inference
  - Why needed here: BeamAggR uses probabilistic aggregation to combine answers from different reasoning paths and knowledge sources. Understanding how to compute and interpret probability distributions is essential for grasping how the model selects the most promising answers.
  - Quick check question: If a composite question has two sub-questions with answer distributions P(A) = [0.7, 0.3] and P(B) = [0.6, 0.4], what is the joint probability distribution for the Cartesian product of answers?

- Concept: Information retrieval and BM25
  - Why needed here: BeamAggR uses BM25 for Wikipedia retrieval and Google search for external knowledge. Understanding how these retrieval methods work and their limitations is important for understanding the quality of the knowledge sources used in reasoning.
  - Quick check question: What is the main difference between BM25 and TF-IDF retrieval methods, and why might BM25 be preferred for open-domain question answering?

- Concept: Tree data structures and post-order traversal
  - Why needed here: BeamAggR represents questions as trees and processes them in post-order (bottom-up). Understanding tree traversal algorithms is essential for understanding how the model breaks down and solves complex questions.
  - Quick check question: Given a tree with root Q, children Q1 and Q2, and Q1 having children Q11 and Q12, what is the post-order traversal sequence?

## Architecture Onboarding

- Component map:
  - Question Decomposition Module -> Multi-source Knowledge Retriever -> LLM Reasoning Engine -> Beam Aggregation Module -> Answer Normalization Layer

- Critical path:
  1. Decompose question into tree
  2. For each leaf node, perform multi-source reasoning
  3. Aggregate leaf answers into probability distributions
  4. For each internal node, enumerate answer combinations from children
  5. Perform reasoning on each combination
  6. Probabilistically aggregate answers
  7. Propagate to root node
  8. Select final answer with highest probability

- Design tradeoffs:
  - Beam size vs. computational cost: Larger beam sizes explore more reasoning paths but increase API calls and latency
  - Number of knowledge sources vs. complexity: More sources provide better coverage but increase implementation complexity and potential for conflicting information
  - Depth of decomposition vs. coherence: Deeper trees create more focused sub-questions but may lose the holistic context of the original question

- Failure signatures:
  - Poor decomposition quality: If the LLM fails to properly decompose questions, the entire reasoning chain fails
  - Knowledge source imbalance: If one source dominates (e.g., web search always provides better answers), the aggregation becomes biased
  - Probability distribution collapse: If beam aggregation consistently selects the same path, the exploration capability is lost

- First 3 experiments:
  1. Run BeamAggR on a single HopotQA question with beam size=1 to verify basic functionality and identify decomposition issues
  2. Compare performance with and without multi-source reasoning on 10 questions to quantify the knowledge collaboration benefit
  3. Test different beam sizes (1, 2, 4) on 20 questions to find the optimal tradeoff between performance and computational cost

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does BeamAggR handle cases where the question decomposition produces incorrect or invalid structures?
- Basis in paper: [inferred]
- Why unresolved: The paper mentions that "the model may produce structural incorrect decomposition, which needs post-filtering to ensure the validity of decomposition" but does not elaborate on what filtering methods are used or how effective they are.
- What evidence would resolve it: Detailed analysis of decomposition errors and their frequency, along with quantitative evaluation of post-filtering effectiveness.

### Open Question 2
- Question: What is the impact of beam size on reasoning quality versus computational efficiency?
- Basis in paper: [explicit]
- Why unresolved: The paper mentions that "larger candidate sizes result in broader reasoning breadth, but they also increase reasoning overhead" and discusses greedy aggregation as a cost-efficient variant, but doesn't provide a comprehensive analysis of the trade-offs.
- What evidence would resolve it: Empirical results showing performance curves across different beam sizes, with analysis of computational costs.

### Open Question 3
- Question: What is the relative contribution of each knowledge source across different question types?
- Basis in paper: [explicit]
- Why unresolved: The paper shows that "Disparities in reasoning contributions across various datasets" exist but doesn't provide detailed analysis of how different knowledge sources contribute to solving different types of questions.
- What evidence would resolve it: Detailed breakdown of knowledge source contributions by question type, with statistical significance testing.

## Limitations
- Question Decomposition Reliability: The method depends on accurate tree decomposition using few-shot prompts, but doesn't validate decomposition quality across different question types
- Knowledge Source Integration Complexity: The framework doesn't address potential conflicts between sources or provide detailed analysis of when each source is most valuable
- Computational Efficiency Claims: While claimed, the method requires multiple API calls per reasoning step and runtime comparisons aren't fully substantiated

## Confidence
**High Confidence Claims**:
- The 8.5% average improvement over baselines is well-supported by reported results on four datasets
- The fundamental architecture of bottom-up tree reasoning with beam aggregation is clearly described and implementable
- The approach of combining multiple knowledge sources is technically sound and has face validity

**Medium Confidence Claims**:
- The mechanism by which beam aggregation reduces cascading errors is theoretically plausible but not empirically validated through ablation studies
- The claim about dynamic multi-source knowledge integration fostering "knowledge collaboration" lacks quantitative analysis of source complementarity
- The efficiency improvements are asserted but not rigorously demonstrated with runtime benchmarks

**Low Confidence Claims**:
- The specific temperature parameter (Ï„) values and their impact on aggregation quality are not reported
- The optimal beam size for different question types is not analyzed
- The generalization capability beyond the four tested datasets is not established

## Next Checks
1. Perform an ablation study comparing BeamAggR with and without beam aggregation on 50 randomly selected questions from HotpotQA to quantify the actual contribution of the beam aggregation mechanism.
2. Analyze knowledge source reliability by measuring individual source performance and correlation across 100 questions to reveal whether sources truly provide complementary information.
3. Conduct runtime profiling on 20 complex questions to measure actual API call counts, latency, and cost compared to baseline methods to validate computational efficiency claims.