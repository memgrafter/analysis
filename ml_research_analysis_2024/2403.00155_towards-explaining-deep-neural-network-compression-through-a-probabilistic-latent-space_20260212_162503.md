---
ver: rpa2
title: Towards Explaining Deep Neural Network Compression Through a Probabilistic
  Latent Space
arxiv_id: '2403.00155'
source_url: https://arxiv.org/abs/2403.00155
tags:
- pruning
- network
- latent
- performance
- neural
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a theoretical framework for explaining deep
  neural network compression through probabilistic latent spaces. The core idea is
  to map network weights before and after pruning to probability spaces and use divergence
  measures to explain the behavior of compressed networks.
---

# Towards Explaining Deep Neural Network Compression Through a Probabilistic Latent Space

## Quick Facts
- arXiv ID: 2403.00155
- Source URL: https://arxiv.org/abs/2403.00155
- Reference count: 40
- Primary result: Introduces AP2 and AP3 metrics that explain deep neural network compression behavior through probabilistic latent spaces

## Executive Summary
This paper presents a theoretical framework for explaining deep neural network compression by mapping network weights before and after pruning to probability spaces and using divergence measures to quantify the behavior of compressed networks. The authors introduce two new concepts - analogous projected patterns (AP2) and analogous-in-probability projected patterns (AP3) - and prove a relationship between these properties and network performance. The framework is validated through extensive experiments on AlexNet, ResNet50, VGG16, and ViT models using CIFAR10, CIFAR100, and Tiny-ImageNet datasets, demonstrating that both AP2 and AP3 effectively explain network sparsity and provide insights into the behavior of different pruning methods.

## Method Summary
The method projects weight matrices of DNN layers into probability spaces using a projection map P, creating probabilistic latent representations of the network state. For pruned networks, the framework measures divergence between original and pruned distributions using KL divergence (AP3) and direct weight differences in projected space (AP2). The approach leverages information-theoretic measures to explain how compressed networks maintain performance during training, with validation showing correlation between AP metrics and performance differences across multiple architectures and pruning methods.

## Key Results
- AP2 and AP3 metrics correlate with performance differences between original and pruned networks
- The framework explains how compressed networks maintain performance during training
- Different pruning methods (lowest, highest, random magnitude) show distinct patterns in the probabilistic latent space

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: KL divergence between probabilistic latent spaces of original and pruned networks quantifies the information loss due to pruning.
- **Mechanism**: Weight matrices are projected into probability distributions (e.g., Gaussian), and their KL divergence captures the distributional shift caused by pruning.
- **Core assumption**: Weight matrices can be modeled as parameters of latent distributions that capture the network's representational state.
- **Evidence anchors**:
  - [abstract] "leverages a probabilistic latent space of DNN weights and explains the optimal network sparsity by using the information-theoretic divergence measures."
  - [section 2] "In our approach, ðœ”(ð‘™) is projected, using a projection map P, to a probability space Z with random variables ð‘ and with a distribution, denoted by ð‘ƒðœ”(ð‘™), which has ðœ”(ð‘™) as its parameter set."
  - [corpus] Weak - related works discuss Bayesian priors but not this exact projection framework.
- **Break condition**: If the latent space assumption fails (e.g., weights don't follow the assumed distribution), KL divergence won't accurately reflect pruning impact.

### Mechanism 2
- **Claim**: AP2 and AP3 metrics correlate with performance degradation in pruned networks.
- **Mechanism**: AP2 measures direct weight differences in projected space; AP3 measures distributional divergence. Smaller values indicate better preservation of network behavior.
- **Core assumption**: Network performance depends on how closely pruned weights approximate original weights in the latent space.
- **Evidence anchors**:
  - [section 2] "We say layer ð‘™ and its sparse version have AP3 if for small ðœ–(ð‘™), ð¾ð¿(ð‘ƒðœ”(ð‘™) âˆ¥ ð‘ƒeðœ”(ð‘™)) â‰¤ ðœ–(ð‘™)."
  - [section 5.2] "In this section, we aim to validate Lemma 3.1 by examining the projections of the last layer weights... We utilize a coefficient of the identity covariance matrix for both latent spaces."
  - [corpus] Weak - related works discuss pruning metrics but not AP2/AP3 specifically.
- **Break condition**: If pruning methods create non-linear effects that aren't captured by linear divergence measures, the correlation with performance may break down.

## Foundational Learning

### Probabilistic Latent Space Projection
- **Why needed**: To transform discrete weight matrices into continuous probability distributions that can be compared using information-theoretic measures
- **Quick check**: Verify that the projection operator P maps weight matrices to valid probability distributions with well-defined moments

### Information-Theoretic Divergence
- **Why needed**: To quantify the information loss when weights are pruned by measuring distributional differences between original and pruned networks
- **Quick check**: Confirm KL divergence calculations are symmetric and properly normalized for the chosen distributions

### Analogous Projected Patterns
- **Why needed**: To provide interpretable metrics (AP2 for direct differences, AP3 for distributional divergence) that correlate with network performance
- **Quick check**: Validate that smaller AP2/AP3 values consistently correspond to smaller performance degradation across multiple pruning scenarios

## Architecture Onboarding

### Component Map
Pre-trained model -> Magnitude-based pruning (lowest/highest/random) -> Fine-tuning (20 epochs) -> AP2/AP3 calculation -> Performance evaluation

### Critical Path
Training baseline -> Pruning at specified sparsity levels -> Fine-tuning -> Compute AP2 (L2 norm) and AP3 (KL divergence) -> Compare with performance differences

### Design Tradeoffs
- **Tradeoff 1**: Choice of probability distribution affects AP3 calculation accuracy vs computational complexity
- **Tradeoff 2**: Projection method determines how well weight differences translate to distributional differences
- **Tradeoff 3**: Pruning percentage vs performance preservation - higher sparsity increases divergence but reduces model size

### Failure Signatures
- AP2/AP3 values don't correlate with performance differences when pruning creates non-linear effects
- Divergence measures become unstable in high-dimensional spaces with extreme sparsity
- Distribution assumptions break down when weights don't follow assumed parametric forms

### First Experiments
1. Verify AP2 calculation by comparing L2 norm of weight differences between original and pruned networks at 0.1 sparsity level
2. Compute KL divergence between original and pruned weight distributions using Gaussian approximation
3. Check correlation between AP2/AP3 values and performance differences across all three pruning methods

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Under what specific conditions does AP2 outperform AP3 (or vice versa) in explaining performance differences between original and pruned networks?
- Basis in paper: [inferred] The paper compares AP2 and AP3 metrics and suggests there may be potential for enhancing AP3 over AP2 by adjusting covariance matrices or changing the distribution, but does not provide definitive conclusions about when each metric is superior.
- Why unresolved: The paper mentions this as an area for future investigation but does not conduct a systematic comparison across different network architectures, pruning methods, and dataset characteristics to determine the conditions favoring each metric.
- What evidence would resolve it: A comprehensive empirical study comparing AP2 and AP3 across multiple network architectures (CNNs, transformers), various pruning methods (magnitude-based, structured, iterative), and different datasets with varying complexity levels would clarify the conditions where each metric excels.

### Open Question 2
- Question: How does the relationship between AP2/AP3 values and performance differences vary with different sparsity levels, and what is the optimal pruning percentage for maintaining performance?
- Basis in paper: [explicit] The paper shows that AP2 and AP3 values correlate with performance differences, but acknowledges that the lowest magnitude pruning method sometimes shows overlapping results, particularly at certain pruning percentages (30% and 50%), and notes that 80% pruning may exceed optimal sparsity levels.
- Why unresolved: While the paper demonstrates the correlation between AP metrics and performance, it does not systematically identify the optimal pruning percentage across different networks and datasets, nor does it fully explain why certain pruning percentages lead to deviations from expected behavior.
- What evidence would resolve it: Systematic experiments across multiple pruning percentages (0-90%) on various network architectures and datasets, combined with theoretical analysis of the trade-off between sparsity and performance maintenance, would identify optimal pruning levels and explain the relationship between sparsity and AP metric behavior.

### Open Question 3
- Question: Can the probabilistic latent space framework be extended to explain the behavior of non-magnitude-based pruning methods, such as structured pruning or filter pruning?
- Basis in paper: [explicit] The paper states that "all weight-based pruning techniques can benefit from our analytical investigations" and mentions that "other pruning techniques impact the weights of the network directly or indirectly, they benefit from our analytical investigations," but focuses primarily on weight magnitude pruning.
- Why unresolved: The paper acknowledges the potential applicability to other pruning methods but does not provide empirical validation or theoretical analysis of how the framework applies to structured or filter-based pruning approaches that remove entire neurons or feature maps.

## Limitations

- The probabilistic latent space assumption may not hold for extremely sparse networks where weight distributions deviate significantly from assumed parametric forms
- Computational complexity of KL divergence estimation in high-dimensional spaces could limit practical applicability for very large networks
- The framework focuses primarily on magnitude-based pruning and may require adaptation for structured or filter-based pruning methods

## Confidence

- **High confidence** in theoretical framework and mathematical proofs (Mechanisms 1 and 2)
- **Medium confidence** in experimental validation results, as the correlation between AP2/AP3 metrics and performance is demonstrated but could benefit from additional ablation studies
- **Medium confidence** in generalizability across different network architectures and pruning methods, as experiments focused primarily on magnitude-based pruning

## Next Checks

1. **Distribution sensitivity analysis**: Test how sensitive the AP2/AP3 metrics are to different choices of probability distributions (Gaussian, T-Student, etc.) and verify if the theoretical guarantees hold across these variations.

2. **Cross-architecture validation**: Apply the framework to architectures not included in the original study (e.g., EfficientNet, MobileNet) to assess generalizability and identify any architecture-specific limitations.

3. **Alternative divergence measures**: Compare KL divergence with other information-theoretic measures (Jensen-Shannon divergence, Wasserstein distance) to evaluate whether the choice of divergence measure affects the explanatory power of the framework.