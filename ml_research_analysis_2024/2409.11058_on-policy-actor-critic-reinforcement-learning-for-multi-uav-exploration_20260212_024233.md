---
ver: rpa2
title: On-policy Actor-Critic Reinforcement Learning for Multi-UAV Exploration
arxiv_id: '2409.11058'
source_url: https://arxiv.org/abs/2409.11058
tags:
- agent
- learning
- policy
- time
- agents
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses multi-UAV exploration in unknown 2D environments
  using on-policy reinforcement learning. The proposed method employs Proximal Policy
  Optimization (PPO) with actor-critic networks combining deep convolutional neural
  networks (CNN) and long short-term memory (LSTM).
---

# On-policy Actor-Critic Reinforcement Learning for Multi-UAV Exploration

## Quick Facts
- arXiv ID: 2409.11058
- Source URL: https://arxiv.org/abs/2409.11058
- Reference count: 27
- Primary result: Multi-UAV exploration using PPO with CNN-LSTM actor-critic networks in unknown 2D environments

## Executive Summary
This paper proposes an on-policy actor-critic reinforcement learning approach for multi-UAV exploration in unknown 2D environments. The method employs Proximal Policy Optimization (PPO) with actor-critic networks that combine deep convolutional neural networks (CNN) and long short-term memory (LSTM) components. UAVs operate in a distributed manner, avoiding obstacles and each other while exploring the environment. The approach demonstrates superior performance compared to policy gradient and A3C methods, with CNN-LSTM architectures showing improved exploration capabilities.

## Method Summary
The proposed method uses PPO as the on-policy actor-critic algorithm for multi-UAV exploration. The actor network, responsible for action selection, consists of CNN and LSTM layers that process environmental observations. The critic network, which estimates value functions, also combines CNN and LSTM architectures. UAVs explore in a distributed fashion without central coordination, using their local observations to make decisions about movement while avoiding obstacles and maintaining safe distances from other UAVs. The system is trained in simulation environments and shows capability to generalize to new, untrained environments.

## Key Results
- PPO outperforms policy gradient and A3C methods in multi-UAV exploration tasks
- LSTM-CNN critic combinations improve exploration performance compared to other architectures
- RL approach demonstrates efficiency advantages over heuristic methods (MOEA/D and GA) in processing time for online exploration
- Method successfully handles new, untrained environments during testing
- System shows sensitivity to hyperparameter tuning affecting performance

## Why This Works (Mechanism)
The success of this approach stems from combining the stability of PPO's clipped objective function with the spatial processing capabilities of CNNs and temporal reasoning of LSTMs. The actor-critic framework enables efficient policy optimization through bootstrapping from value estimates, while the distributed nature allows each UAV to make independent decisions based on local observations. The CNN layers extract spatial features from sensor data, and LSTM layers capture temporal dependencies in the exploration process, enabling better prediction of future states and more informed action selection.

## Foundational Learning
- Proximal Policy Optimization (PPO): A reinforcement learning algorithm that optimizes policies while preventing large policy updates through clipping. Why needed: Provides stable training and prevents catastrophic policy collapse common in on-policy methods.
- Actor-Critic Architecture: Framework where an actor network selects actions and a critic network evaluates them. Why needed: Enables efficient learning through bootstrapping and reduces variance compared to pure policy gradient methods.
- Convolutional Neural Networks (CNN): Deep learning architecture for processing grid-like data. Why needed: Extracts spatial features from environmental observations for action decision-making.
- Long Short-Term Memory (LSTM): Recurrent neural network architecture for sequence processing. Why needed: Captures temporal dependencies in exploration trajectories and maintains state memory across time steps.
- Multi-UAV Coordination: Distributed control of multiple autonomous vehicles. Why needed: Enables parallel exploration while avoiding collisions and maintaining coverage efficiency.

## Architecture Onboarding

**Component Map:** Environment -> Sensor Data -> CNN-LSTM Network -> Action Selection -> UAV Movement -> New State

**Critical Path:** Sensor observations → CNN feature extraction → LSTM temporal processing → Action output → Environment response → Value estimation → Policy update

**Design Tradeoffs:** 
- Distributed vs centralized control: Distributed approach reduces communication overhead but may lead to suboptimal exploration patterns
- CNN vs LSTM: CNNs excel at spatial feature extraction while LSTMs handle temporal dependencies, requiring both for effective exploration
- Exploration vs exploitation: The policy must balance thorough coverage with efficient path planning

**Failure Signatures:**
- Poor obstacle avoidance indicating insufficient training or inadequate reward shaping
- UAVs clustering together suggesting failure in collision avoidance or reward design
- Inefficient coverage patterns indicating suboptimal policy or hyperparameter issues
- High processing times suggesting network architecture complexity issues

**First 3 Experiments:**
1. Single UAV exploration in simple environments to validate basic functionality
2. Multi-UAV coordination testing with varying numbers of UAVs
3. Performance comparison across different network architectures (CNN only, LSTM only, CNN-LSTM)

## Open Questions the Paper Calls Out
None

## Limitations
- Simulation environment restricted to 2D scenarios, limiting real-world applicability
- Comparison with heuristic methods focuses primarily on processing time without comprehensive evaluation of solution quality
- Hyperparameter sensitivity analysis lacks systematic investigation of parameter space and robustness

## Confidence

**Major claim confidence:**
- PPO superiority over baseline methods: **Medium** - Results show performance improvements, but the comparison lacks statistical significance testing and evaluation across diverse environment types
- LSTM-CNN architecture benefits: **Medium** - Performance improvements are demonstrated, but ablation studies examining component contributions are absent
- Distributed exploration capability: **High** - The methodology inherently supports distributed operation, though specific coordination mechanisms are not detailed

## Next Checks
1. Conduct systematic hyperparameter sensitivity analysis using design of experiments to identify robust parameter ranges across different environment complexities
2. Implement real-world flight testing or high-fidelity 3D simulation to validate transfer from 2D to 3D exploration scenarios
3. Perform comprehensive comparison with state-of-the-art heuristic methods including solution quality metrics beyond processing time (e.g., coverage rate, obstacle collision frequency)