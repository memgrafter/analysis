---
ver: rpa2
title: Exploring the limits of decoder-only models trained on public speech recognition
  corpora
arxiv_id: '2402.00235'
source_url: https://arxiv.org/abs/2402.00235
tags:
- dota
- audio
- speech
- whisper
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates whether decoder-only models trained on
  public speech recognition corpora alone can deliver competitive performance compared
  to industrial-scale models like Whisper. The authors train a Decoder-Only Transformer
  for ASR (DOTA) on 93K hours of public English speech-text data and evaluate it on
  various English ASR benchmarks.
---

# Exploring the limits of decoder-only models trained on public speech recognition corpora

## Quick Facts
- **arXiv ID**: 2402.00235
- **Source URL**: https://arxiv.org/abs/2402.00235
- **Authors**: Ankit Gupta; George Saon; Brian Kingsbury
- **Reference count**: 10
- **Primary result**: DOTA achieves competitive ASR performance on public data alone, outperforming OWSM and matching Whisper on many benchmarks

## Executive Summary
This paper investigates whether decoder-only models trained exclusively on public speech recognition corpora can deliver competitive performance compared to industrial-scale models like Whisper. The authors train a Decoder-Only Transformer for ASR (DOTA) on 93K hours of public English speech-text data and evaluate it across various English ASR benchmarks. DOTA comprehensively outperforms the open-source replication of Whisper (OWSM) on nearly all English ASR benchmarks and achieves comparable or better performance than Whisper large-v3 on 7 out of 15 test sets, despite having fewer than half as many parameters. The results demonstrate that competitive ASR performance can be achieved using decoder-only models trained on public data alone, with bidirectionality over audio frames being identified as critical for high performance.

## Method Summary
The authors develop a Decoder-Only Transformer for ASR (DOTA) architecture trained on 93K hours of public English speech-text data. The model employs a decoder-only transformer architecture with specific attention mechanisms that incorporate bidirectionality over audio frames. Training was conducted using standard ASR techniques including CTC loss and teacher forcing during decoding. The model was evaluated on multiple English ASR benchmarks to compare performance against both the open-source Whisper replication (OWSM) and the original Whisper large-v3 model.

## Key Results
- DOTA comprehensively outperforms OWSM on nearly all English ASR benchmarks
- DOTA outperforms Whisper large-v3 on 7 out of 15 test sets despite having fewer than half as many parameters
- Bidirectionality over audio frames is identified as critical for achieving high performance
- Competitive ASR performance can be achieved using decoder-only models trained on public data alone

## Why This Works (Mechanism)
The success of DOTA stems from effectively leveraging decoder-only transformer architectures with bidirectional audio frame processing. The model's ability to capture contextual information from both past and future audio frames enables better acoustic modeling compared to strictly causal approaches. The substantial training corpus of 93K hours of public data provides sufficient diversity for learning robust speech representations. The decoder-only architecture simplifies the training pipeline while maintaining competitive performance through effective attention mechanisms and loss functions.

## Foundational Learning
- **Decoder-only transformer architecture**: Why needed - simplifies training pipeline compared to encoder-decoder models; Quick check - verify model can generate sequences autoregressively
- **Bidirectional audio frame processing**: Why needed - captures contextual information from both past and future frames for better acoustic modeling; Quick check - compare performance with strictly causal vs bidirectional approaches
- **CTC loss for ASR**: Why needed - enables efficient training and alignment-free sequence modeling; Quick check - verify training stability and convergence
- **Teacher forcing during decoding**: Why needed - accelerates training by providing ground truth context during autoregressive generation; Quick check - monitor training loss curves for expected behavior
- **Public speech-text corpus curation**: Why needed - ensures model learns from diverse, representative data without proprietary restrictions; Quick check - analyze data distribution and coverage across different speech characteristics
- **Attention mechanisms in decoder-only models**: Why needed - enables long-range dependencies and context modeling in speech sequences; Quick check - verify attention patterns capture relevant acoustic-phonetic relationships

## Architecture Onboarding
**Component map**: Speech audio frames -> Bidirectional Transformer encoder -> Decoder layers with causal attention -> Output distribution over vocabulary
**Critical path**: Audio input → Bidirectional frame processing → Self-attention across frames → Autoregressive decoding → Text output
**Design tradeoffs**: Simpler training pipeline vs. potential limitations in handling long-range dependencies compared to encoder-decoder models; Public data constraints vs. proprietary data advantages
**Failure signatures**: Poor performance on out-of-domain speech, degraded accuracy with increased background noise, difficulties with rare vocabulary or accents not well-represented in training data
**First experiments**: 1) Train with and without bidirectional frame processing to isolate its impact; 2) Compare different attention mechanisms (full vs. sparse attention); 3) Evaluate performance degradation with reduced training data volume

## Open Questions the Paper Calls Out
None identified in the source material.

## Limitations
- Exclusive focus on English speech recognition constrains generalizability to multilingual settings
- Comparison against Whisper is somewhat selective, not evaluating all variants on all test sets
- Parameter count comparison may not fully account for architectural differences between models
- Lack of ablation studies to definitively establish the causal relationship between bidirectionality and performance

## Confidence
- **High confidence**: Comparative performance results showing DOTA outperforming OWSM and matching Whisper on many test sets
- **Medium confidence**: Claim about decoder-only models achieving competitive performance with fewer parameters, pending more comprehensive architectural comparisons
- **Medium confidence**: Importance of bidirectionality for audio frames, pending direct ablation studies

## Next Checks
1. Conduct ablation studies specifically isolating the impact of bidirectional audio frame processing versus other architectural choices
2. Test model performance on non-English languages and cross-lingual transfer to validate generalizability claims
3. Evaluate model robustness across diverse acoustic conditions and domain shifts not represented in the training corpora