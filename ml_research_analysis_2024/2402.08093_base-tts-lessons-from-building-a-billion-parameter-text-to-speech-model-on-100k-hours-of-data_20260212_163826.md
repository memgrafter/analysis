---
ver: rpa2
title: 'BASE TTS: Lessons from building a billion-parameter Text-to-Speech model on
  100K hours of data'
arxiv_id: '2402.08093'
source_url: https://arxiv.org/abs/2402.08093
tags:
- speech
- speaker
- arxiv
- data
- base
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: BASE TTS is a 1-billion-parameter autoregressive Transformer trained
  on 100K hours of public domain speech data, achieving state-of-the-art naturalness
  in text-to-speech synthesis. It uses WavLM-based speechcodes with speaker disentanglement
  and byte-pair encoding, decoded into waveforms via a fast, streamable convolution-based
  decoder.
---

# BASE TTS: Lessons from building a billion-parameter Text-to-Speech model on 100K hours of data

## Quick Facts
- **arXiv ID:** 2402.08093
- **Source URL:** https://arxiv.org/abs/2402.08093
- **Reference count:** 40
- **One-line result:** State-of-the-art 1B-parameter autoregressive TTS trained on 100K hours of public domain data

## Executive Summary
BASE TTS is a 1-billion-parameter autoregressive Transformer trained on 100K hours of public domain speech data, achieving state-of-the-art naturalness in text-to-speech synthesis. It uses WavLM-based speechcodes with speaker disentanglement and byte-pair encoding, decoded into waveforms via a fast, streamable convolution-based decoder. Evaluations against baselines like YourTTS, Bark, and TortoiseTTS show significant improvements in naturalness, with linguistic expert assessments revealing emergent abilities in handling complex prosody, emotions, and syntactic structures as model scale increases.

## Method Summary
BASE TTS uses a WavLM-based speech tokenizer to convert waveforms into discrete speechcodes with speaker disentanglement, followed by autoregressive modeling using a 1B-parameter GPT-2 architecture that predicts speechcodes conditioned on text and reference speech. The speechcodes are then decoded into waveforms using a fast, streamable convolution-based decoder, with final waveform generation handled by BigVGAN. The system is trained in three stages: speech tokenizer, autoregressive model, and decoder, using 100K hours of public domain speech data.

## Key Results
- BASE TTS achieves state-of-the-art naturalness in MUSHRA evaluations compared to baselines like YourTTS, Bark, and TortoiseTTS
- Expert linguistic evaluation shows monotonic improvement across BASE-small, BASE-medium, and BASE-large variants in handling complex prosody, emotions, and syntactic structures
- The speechcode decoder provides 3X improvement in inference speed compared to diffusion-based approaches while maintaining quality

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Speech naturalness improves as model scale (parameters and data) increases beyond certain thresholds
- Mechanism: Larger autoregressive models trained on more diverse data develop emergent abilities to handle complex prosody, emotions, and syntactic structures
- Core assumption: TTS models, like LLMs, acquire qualitatively new capabilities when scaled beyond specific parameter and data thresholds
- Evidence anchors:
  - [abstract]: "BASE TTS variants built with 10K+ hours and 500M+ parameters begin to demonstrate natural prosody on textually complex sentences"
  - [section 4.3]: Expert linguistic evaluation shows monotonic improvement across BASE-small, BASE-medium, and BASE-large variants
  - [corpus]: Weak - only general TTS scaling papers found, no specific evidence of emergent TTS abilities

### Mechanism 2
- Claim: WavLM-based speechcodes with speaker disentanglement outperform autoencoder-based speechcodes
- Mechanism: WavLM-based speechcodes capture semantic and prosodic information while removing speaker identity, allowing the autoregressive model to focus on content and prosody prediction
- Core assumption: Speaker information is better removed at the tokenization stage rather than relying on the autoregressive model to learn this disentanglement
- Evidence anchors:
  - [abstract]: "Further, our speechcodes are built using a novel speech tokenization technique that features speaker ID disentanglement and compression with byte-pair encoding"
  - [section 4.1]: MUSHRA evaluation shows WavLM-based system outperforms VQ-VAE on Spanish voices
  - [corpus]: Weak - only general speech tokenization papers found, no specific evidence comparing WavLM vs VAE for TTS

### Mechanism 3
- Claim: Speechcode decoder is faster and more efficient than diffusion-based decoder while maintaining quality
- Mechanism: The speechcode decoder directly generates waveforms from the autoregressive model's hidden states, eliminating the need for a separate diffusion step and vocoder
- Core assumption: The dense hidden states from the autoregressive model contain sufficient information for high-quality waveform generation
- Evidence anchors:
  - [abstract]: "We also show that they can be decoded to high quality waveforms with a simple, fast, and streamable decoder"
  - [section 4.2]: MUSHRA evaluation shows speechcode decoder performs as well or better than diffusion-based decoder
  - [corpus]: Weak - only general TTS decoder papers found, no specific evidence comparing speechcode vs diffusion decoders

## Foundational Learning

- **Discrete speech representations (speechcodes)**: Why needed here: Enable direct application of LLM techniques to TTS by converting continuous speech into discrete tokens that can be modeled autoregressively. Quick check question: What is the primary advantage of using discrete speech representations over continuous representations like mel-spectrograms in autoregressive TTS models?

- **Byte-pair encoding (BPE) for speech**: Why needed here: Compress speechcode sequences to reduce memory requirements and allow modeling of longer audio segments with Transformers. Quick check question: How does byte-pair encoding reduce the sequence length of speechcodes, and why is this important for autoregressive TTS models?

- **Speaker disentanglement in speech representations**: Why needed here: Separate speaker identity from content and prosodic information, allowing the autoregressive model to focus on generating appropriate prosody for the text. Quick check question: What is the benefit of disentangling speaker identity from content and prosody information in the speechcodes used by an autoregressive TTS model?

## Architecture Onboarding

- **Component map**: WavLM-based speech tokenizer -> Autoregressive Transformer (SpeechGPT) -> Speechcode decoder -> BigVGAN vocoder -> Waveform

- **Critical path**: Text → SpeechGPT → Speechcode decoder → BigVGAN → Waveform

- **Design tradeoffs**:
  - Discrete vs continuous representations: Discrete representations enable autoregressive modeling but may lose some fine-grained acoustic information
  - Speaker disentanglement: Improves content and prosody modeling but adds complexity to the tokenization process
  - End-to-end vs separate components: End-to-end approach simplifies the pipeline but may be less flexible for fine-tuning individual components

- **Failure signatures**:
  - Poor speech quality: May indicate issues with speechcode representation, autoregressive model, or decoder
  - Speaker mismatch: Could be due to insufficient speaker disentanglement in speechcodes or reference speaker embedding issues
  - Incorrect prosody: May result from inadequate conditioning on text or reference speech, or limitations in the autoregressive model's ability to capture complex prosody

- **First 3 experiments**:
  1. Ablation study: Compare WavLM-based speechcodes with VQ-VAE speechcodes to verify the importance of speaker disentanglement and semantic information in the speechcodes
  2. Scaling study: Train BASE TTS variants with different parameter counts and data sizes to confirm the emergence of complex prosody handling abilities
  3. Decoder comparison: Evaluate the speechcode decoder against the diffusion-based decoder on a held-out test set to ensure the new approach maintains quality while improving efficiency

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the choice of discrete speech representation affect the overall quality of BASE TTS?
- Basis in paper: [explicit] The paper mentions that "more research is needed to establish how different properties of speechcodes translate into end-to-end system quality" and that "We only report results for one speechcode configuration and leave more comprehensive study for future work."
- Why unresolved: The paper only uses one configuration of WavLM-based speechcodes and does not explore the impact of different speech tokenization methods on the final TTS quality
- What evidence would resolve it: Comparative studies using various speech tokenization techniques (e.g., VQ-VAE, WavLM, RepCodec) with BASE TTS, evaluating their impact on naturalness, intelligibility, and speaker similarity

### Open Question 2
- Question: Can scaling BASE TTS to even larger datasets and model sizes lead to further improvements in naturalness and emergent abilities?
- Basis in paper: [inferred] The paper demonstrates that scaling BASE TTS from 1K to 100K hours and from 150M to 1B parameters leads to improvements in naturalness and emergent abilities. However, the paper does not explore the upper limits of scaling
- Why unresolved: The paper only explores three variants of BASE TTS with increasing dataset sizes and model parameters. It is unclear if further scaling would continue to yield improvements
- What evidence would resolve it: Training BASE TTS on datasets larger than 100K hours and with models exceeding 1B parameters, and evaluating their performance on naturalness and emergent abilities

### Open Question 3
- Question: How does the inclusion of textual knowledge from text-only LLMs impact the performance of BASE TTS on complex prosody and emotions?
- Basis in paper: [explicit] The paper states that "we remain hopeful that further scaling and injection of textual knowledge from text-only LLM can help us close remaining performance gaps" in handling emotions and paralinguistics
- Why unresolved: The paper does not explore the integration of text-only LLM knowledge into BASE TTS, leaving the potential benefits of such integration unexplored
- What evidence would resolve it: Training BASE TTS with additional text-only LLM knowledge, either through fine-tuning or knowledge injection, and evaluating its impact on the model's ability to handle complex prosody and emotions

## Limitations

- The claims about emergent abilities at billion-parameter scale are primarily supported by internal expert linguistic evaluations rather than strong external validation from the broader scientific literature
- The superiority of WavLM-based speechcodes over VAE alternatives is demonstrated only through limited MUSHRA evaluations on a small number of speakers
- The comparison of speechcode decoder performance against diffusion-based approaches is made against a single baseline with limited test conditions

## Confidence

- **High Confidence**: Claims about BASE TTS achieving state-of-the-art naturalness on standard TTS benchmarks (MUSHRA scores)
- **Medium Confidence**: The architectural approach of using WavLM-based speechcodes with speaker disentanglement and the end-to-end autoregressive framework
- **Low Confidence**: The specific claims about emergent abilities at billion-parameter scale and the superiority of WavLM-based speechcodes over VAE alternatives

## Next Checks

**Validation Check 1**: Conduct a systematic scaling study across multiple TTS architectures (not just autoregressive Transformers) to determine whether the observed improvements in handling complex linguistic phenomena are specific to BASE TTS or represent general scaling laws in TTS.

**Validation Check 2**: Perform ablation studies comparing WavLM-based speechcodes with multiple alternative tokenization approaches (VQ-VAE, HuBERT, Wav2Vec) across diverse languages and speaker conditions to rigorously establish the claimed advantages.

**Validation Check 3**: Evaluate the speechcode decoder's performance on out-of-distribution test sets, including challenging acoustic conditions, novel speakers, and complex prosodic patterns, to verify the claimed robustness and efficiency benefits.