---
ver: rpa2
title: 'GPT vs RETRO: Exploring the Intersection of Retrieval and Parameter-Efficient
  Fine-Tuning'
arxiv_id: '2407.04528'
source_url: https://arxiv.org/abs/2407.04528
tags:
- retro
- arxiv
- peft
- retrieval
- fine-tuning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper compares the performance of GPT and RETRO models across
  various sizes (823M to 48B parameters) when combined with parameter-efficient fine-tuning
  (PEFT) methods such as P-tuning, Adapters, and LoRA. The study evaluates these models
  on six datasets, focusing on document question answering, multiple-choice question
  answering, and summarization tasks.
---

# GPT vs RETRO: Exploring the Intersection of Retrieval and Parameter-Efficient Fine-Tuning

## Quick Facts
- **arXiv ID**: 2407.04528
- **Source URL**: https://arxiv.org/abs/2407.04528
- **Reference count**: 13
- **Primary result**: RETRO models outperform GPT models in zero-shot settings due to their unique pre-training approach and focus on retrieval tasks.

## Executive Summary
This paper compares GPT and RETRO models across various sizes (823M to 48B parameters) when combined with parameter-efficient fine-tuning (PEFT) methods such as P-tuning, Adapters, and LoRA. The study evaluates these models on six datasets, focusing on document question answering, multiple-choice question answering, and summarization tasks. Key findings include: RETRO models outperform GPT models in zero-shot settings due to their retrieval-augmented pre-training; both model types reach performance saturation around 8B parameters; P-tuning underperforms compared to Adapters and LoRA, particularly in smaller models; and GPT models have higher performance potential with PEFT compared to RETRO models.

## Method Summary
The study applies P-tuning, Adapters, and LoRA to GPT and RETRO models across five different sizes (823M, 2.25B, 8.5B, 22B, and 43B parameters) on six datasets including Natural Questions, TriviaQA, NarrativeQA, Qasper, QuALITY, and QMSum. RETRO models incorporate a retrieval encoder and chunked cross-attention with retrieved neighbors, while GPT models rely on standard autoregressive generation. All experiments use the Dragon+ retriever to obtain 5 relevant chunks for each input. The study compares zero-shot performance, PEFT performance, and full fine-tuning across these configurations, measuring F1 score for document QA, exact match for multiple-choice QA, and ROUGE-1/2/L for summarization.

## Key Results
- RETRO models achieve superior zero-shot performance compared to GPT models due to their retrieval-augmented pre-training
- Both model types reach performance saturation around 8B parameters, indicating optimal cost-performance balance
- P-tuning underperforms Adapters and LoRA, especially in smaller models, with Adapters and LoRA showing better parameter efficiency
- GPT models demonstrate higher performance potential with PEFT methods compared to RETRO models

## Why This Works (Mechanism)

### Mechanism 1
- Claim: RETRO models leverage retrieval during pre-training to learn context-aware question answering, while GPT models only rely on next-token prediction.
- Mechanism: RETRO's chunked cross-attention with retrieved neighbors during pre-training teaches it to integrate external knowledge into generation, whereas GPT's autoregressive loss focuses purely on token prediction without external retrieval.
- Core assumption: The retrieval-augmented pre-training process provides RETRO with more targeted contextual learning than GPT's standard autoregressive pre-training.
- Evidence anchors:
  - [abstract]: "RETRO models outperform GPT models in zero-shot settings due to their unique pre-training process"
  - [section]: "RETRO models outperform GPT models in zero-shot settings due to their unique pre-training approach and focus on retrieval tasks. By learning to extract salient information from retrieved text and integrate it into its generation process, RETRO develops the capability to harness relevant contextual knowledge"
- Break condition: If retrieval quality degrades significantly, RETRO's advantage in zero-shot settings would diminish.

### Mechanism 2
- Claim: GPT models have higher performance potential with PEFT because their pre-training is not focused on retrieval-augmented generation.
- Mechanism: Since GPT models don't learn to integrate external retrieval during pre-training, PEFT provides more room for improvement by teaching them retrieval-augmented capabilities that were absent in the original training.
- Core assumption: The absence of retrieval integration in GPT's pre-training creates a larger gap between base performance and optimal performance that PEFT can fill.
- Evidence anchors:
  - [abstract]: "GPT models have higher performance potential with PEFT compared to RETRO models"
  - [section]: "Since GPT pre-training is not focused on retrieval-augmented generation, it allows for a larger room for improvement when fine-tuned on these tasks"
- Break condition: If GPT models already possess strong retrieval capabilities from pre-training, PEFT would show minimal improvement.

### Mechanism 3
- Claim: Both RETRO and GPT models reach performance saturation around 8B parameters, indicating an optimal cost-performance balance.
- Mechanism: Beyond 8B parameters, the marginal improvement in performance does not justify the additional computational cost, creating a saturation point for both model types.
- Core assumption: The relationship between model size and performance follows a diminishing returns pattern after a certain threshold.
- Evidence anchors:
  - [abstract]: "our study indicates that 8B parameter models strike an optimal balance between cost and performance"
  - [section]: "Both RETRO and GPT models exhibit saturation points around 8B parameters"
- Break condition: If future architectures demonstrate different scaling laws, the 8B parameter sweet spot may shift.

## Foundational Learning

- Concept: Retrieval-augmented generation (RAG)
  - Why needed here: The paper compares GPT (without built-in retrieval) against RETRO (with built-in retrieval), requiring understanding of how retrieval integration affects model performance
  - Quick check question: How does retrieval augmentation differ between the two model types, and why does this create different baseline performances?

- Concept: Parameter-efficient fine-tuning (PEFT)
  - Why needed here: The paper evaluates multiple PEFT methods (P-tuning, Adapters, LoRA) across different model sizes and architectures
  - Quick check question: What distinguishes P-tuning from Adapters and LoRA in terms of parameter count and insertion strategy?

- Concept: Model scaling and saturation
  - Why needed here: The paper identifies 8B parameters as an optimal point, requiring understanding of how model size affects performance and cost
  - Quick check question: Why do both RETRO and GPT models show performance saturation around 8B parameters despite their architectural differences?

## Architecture Onboarding

- Component map:
  GPT -> PEFT layers in attention blocks
  RETRO -> Retrieval encoder -> Chunked cross-attention -> PEFT layers in both encoder and decoder
  Retriever -> Dragon+ dual encoder for context retrieval
  PEFT methods -> P-tuning (virtual tokens), Adapters (parallel FC layers), LoRA (low-rank decomposition)

- Critical path:
  1. Retrieve 5 most relevant chunks using Dragon+
  2. For GPT: Concatenate retrieved chunks with input question
  3. For RETRO: Pass through retrieval encoder, then cross-attention with retrieved chunks
  4. Apply PEFT layers during fine-tuning
  5. Generate output

- Design tradeoffs:
  - RETRO vs GPT: RETRO has better zero-shot retrieval performance but lower PEFT improvement potential
  - PEFT method choice: P-tuning uses fewer parameters but underperforms Adapters/LoRA in smaller models
  - Model size: 8B parameters offers optimal cost-performance balance

- Failure signatures:
  - RETRO underperformance: Poor retrieval quality or inappropriate chunking strategy
  - P-tuning failure: Mismatch between virtual token count and task complexity
  - PEFT non-convergence: Inappropriate learning rate or batch size for model scale

- First 3 experiments:
  1. Compare zero-shot performance of RETRO vs GPT on NQ dataset with Dragon+ retriever
  2. Apply LoRA fine-tuning to 8B RETRO model on QASPER dataset
  3. Compare P-tuning vs Adapters performance on 2.25B GPT model across all six datasets

## Open Questions the Paper Calls Out
No specific open questions were identified in the paper text.

## Limitations
- Dataset Generalization: Findings may not generalize to domains outside question answering and summarization
- PEFT Method Sensitivity: Performance gaps between methods could be implementation-dependent rather than inherent
- Retrieval Quality Dependency: RETRO's advantage is heavily dependent on Dragon+ retriever quality

## Confidence

**High Confidence Claims**:
- RETRO models outperform GPT models in zero-shot settings due to their retrieval-augmented pre-training
- Both model types reach performance saturation around 8B parameters
- Adapters and LoRA outperform P-tuning in parameter-efficient fine-tuning scenarios

**Medium Confidence Claims**:
- GPT models have higher performance potential with PEFT compared to RETRO models
- The 8B parameter size represents an optimal cost-performance balance across all scenarios
- P-tuning consistently underperforms due to its virtual token approach rather than inherent limitations

**Low Confidence Claims**:
- The specific performance margins between PEFT methods are stable across all model sizes and datasets
- RETRO's retrieval-augmented pre-training provides uniform advantages across all downstream tasks
- The saturation point of 8B parameters applies equally to all task types and domains

## Next Checks

**Check 1: Retriever Robustness Test**
Validate whether RETRO's performance advantage persists when using different retrievers (e.g., BM25, dense retrievers) or when retrieval quality is intentionally degraded. This would confirm whether the zero-shot advantage is due to RETRO's architecture or the specific Dragon+ retriever used.

**Check 2: Cross-Domain Generalization**
Test the 8B parameter saturation claim on datasets from different domains (e.g., medical, legal, technical) and task types (e.g., code generation, translation) to determine if the scaling behavior is consistent across diverse applications.

**Check 3: PEFT Method Sensitivity Analysis**
Systematically vary implementation details of P-tuning, Adapters, and LoRA (prompt encoder architecture, adapter dimensions, LoRA rank) to isolate whether the performance differences are method-specific or implementation-dependent.