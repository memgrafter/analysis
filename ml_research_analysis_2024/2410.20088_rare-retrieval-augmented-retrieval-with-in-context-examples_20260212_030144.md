---
ver: rpa2
title: 'RARe: Retrieval Augmented Retrieval with In-Context Examples'
arxiv_id: '2410.20088'
source_url: https://arxiv.org/abs/2410.20088
tags:
- in-context
- examples
- query
- rare
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper studies in-context learning for encoder-only models
  in text retrieval tasks. The authors propose RARe (Retrieval Augmented Retrieval
  with In-Context Examples), which fine-tunes a pre-trained model with in-context
  examples whose queries are semantically similar to the target query.
---

# RARe: Retrieval Augmented Retrieval with In-Context Examples

## Quick Facts
- arXiv ID: 2410.20088
- Source URL: https://arxiv.org/abs/2410.20088
- Reference count: 40
- Key outcome: Achieves up to +2.72% nDCG improvements over baseline methods across open-domain retrieval datasets (BeIR, RAR-b)

## Executive Summary
This paper introduces RARe (Retrieval Augmented Retrieval with In-Context Examples), a method that enhances encoder-only models for text retrieval tasks through in-context learning. The approach fine-tunes pre-trained models using semantically similar in-context examples that are prepended to target queries during training. By leveraging contrastive loss training with augmented query formats, RARe achieves significant performance gains over baseline methods while demonstrating strong out-of-domain generalization capabilities.

## Method Summary
RARe fine-tunes pre-trained encoder-only models by augmenting queries with semantically similar in-context examples retrieved using BM25. During training, k=5 similar queries are found and prepended to the target query, creating an augmented format that is then optimized using contrastive loss. At inference, the same retrieval mechanism is used to construct RARe-formatted queries. The method modifies how queries are processed rather than changing model parameters, enabling encoder-only models to leverage contextual information for improved retrieval performance.

## Key Results
- Achieves up to +2.72% nDCG improvements over baseline methods across open-domain retrieval datasets
- Demonstrates stronger out-of-domain generalization compared to models using queries without in-context examples
- Performance gains depend on semantic similarity between in-context examples and target queries rather than example quantity alone

## Why This Works (Mechanism)

### Mechanism 1
In-context examples improve encoder-only models by providing semantically relevant guidance that enriches the query representation space. The encoder-only model's embedding of the query is influenced by the augmented context, effectively expanding the representational capacity for that specific query without changing parameters. This works because the in-context examples share semantic similarity with the target query, creating a more focused embedding subspace. Core assumption: Semantically similar in-context examples provide task-relevant information that the model can effectively incorporate into its fixed-dimensional output.

### Mechanism 2
Fine-tuning with in-context examples enables the encoder-only model to learn how to effectively process augmented query formats. During training, the model learns to map the longer, augmented query format (containing in-context examples) to appropriate document embeddings through contrastive loss optimization. This adaptation allows the model to leverage the additional context during inference. Core assumption: The model can learn to process and utilize the additional information in the augmented query format through continued fine-tuning.

### Mechanism 3
The quality and semantic similarity of in-context examples matters more than their quantity for performance gains. High-quality, semantically similar examples provide focused guidance that helps the model disambiguate the target query's intent, while random examples may introduce noise that confuses the embedding process. Core assumption: Semantic similarity between in-context example queries and target queries correlates with performance improvements.

## Foundational Learning

- Concept: Contrastive learning with in-batch negatives
  - Why needed here: The training objective relies on contrastive loss with in-batch negatives to learn effective query-document representations in a shared embedding space.
  - Quick check question: How does the contrastive loss function help the model distinguish between relevant and irrelevant documents?

- Concept: Dense retrieval and embedding space geometry
  - Why needed here: The approach assumes that semantically similar queries and documents cluster in the embedding space, and that augmenting queries with relevant examples helps position them closer to their target documents.
  - Quick check question: What properties of the embedding space make dense retrieval effective for this task?

- Concept: BM25 for semantic similarity retrieval
  - Why needed here: BM25 is used to retrieve semantically similar queries to serve as in-context examples, forming the foundation for the augmented query construction.
  - Quick check question: Why might BM25 be preferred over learned similarity measures for retrieving in-context examples during training?

## Architecture Onboarding

- Component map: BM25 retriever -> Query augmentation module -> Encoder-only model -> Contrastive loss trainer -> FAISS index
- Critical path: BM25 retrieval → Query augmentation → Embedding generation → Contrastive training → FAISS indexing → Retrieval
- Design tradeoffs:
  - Semantic similarity vs. computational efficiency in BM25 retrieval
  - Number of in-context examples (k) vs. query length and model capacity
  - Training with in-context examples vs. zero-shot adaptation capability
- Failure signatures:
  - Performance degradation when adding in-context examples at inference only
  - Overfitting when training with too many in-context examples
  - Suboptimal performance when in-context examples are semantically dissimilar
- First 3 experiments:
  1. Implement BM25-based retrieval of similar queries and measure semantic similarity distribution
  2. Create augmented query format and test inference-only performance drop
  3. Train model with in-context examples and compare performance on a small validation set

## Open Questions the Paper Calls Out

### Open Question 1
What is the optimal number of in-context examples for different retrieval tasks, and how does this vary across domains? While the paper provides evidence that the optimal number varies, it doesn't establish a clear rule or framework for determining the optimal k across different task types or domains. Systematic experiments varying k across a broader range of retrieval tasks with different characteristics would help establish patterns for optimal k selection.

### Open Question 2
How do architectural differences in base models (e.g., causal vs. bidirectional attention, pooling strategies) affect the effectiveness of in-context learning for retrieval? The paper observes performance differences but doesn't conduct controlled experiments isolating specific architectural features to determine their impact on in-context learning effectiveness. Controlled ablation studies comparing models with different architectural choices while keeping all other factors constant would reveal which architectural features most influence in-context learning performance.

### Open Question 3
Can negative documents be effectively incorporated into in-context examples to improve retrieval performance, and if so, what is the optimal format for presenting them? The paper only tests one format for presenting negative documents and finds no benefit, but doesn't explore alternative formats or ways to make negative examples more informative. Experiments testing different formats for presenting negative documents would determine if there's an effective way to incorporate them.

## Limitations

- In-context learning effectiveness for encoder-only models represents a key uncertainty as these models were not originally designed for in-context learning scenarios, and performance gains may be model-specific
- Computational overhead from retrieving semantically similar queries during both training and inference introduces additional costs that weren't fully analyzed
- BM25-based similarity assumptions may fail when semantically similar queries use different vocabulary or when lexical matching doesn't align with retrieval relevance

## Confidence

- High confidence: The observation that training with in-context examples improves performance compared to inference-only modifications is well-supported by experimental results across multiple datasets
- Medium confidence: The claim that semantic similarity of in-context examples matters more than quantity is supported by analysis but could benefit from more rigorous statistical validation
- Medium confidence: The assertion of strong out-of-domain generalization is demonstrated on BeIR datasets but would benefit from testing on additional diverse domains

## Next Checks

1. **Ablation study on retrieval mechanism**: Replace BM25 with a learned similarity model (e.g., SBERT) for retrieving in-context examples and measure impact on performance to validate the importance of the retrieval mechanism choice.

2. **Latency analysis**: Measure the end-to-end inference latency of RARe compared to baseline methods, including the BM25 retrieval time for in-context examples, to quantify the practical computational overhead.

3. **Model architecture sensitivity**: Test RARe on different encoder-only architectures (beyond the two models evaluated) to assess generalizability and identify potential model-specific limitations or advantages.