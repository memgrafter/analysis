---
ver: rpa2
title: FFA Sora, video generation as fundus fluorescein angiography simulator
arxiv_id: '2412.17346'
source_url: https://arxiv.org/abs/2412.17346
tags:
- videos
- generated
- video
- sora
- retinal
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: FFA Sora is a text-to-video diffusion model that generates dynamic
  fundus fluorescein angiography (FFA) videos from clinical reports to aid medical
  education and privacy-preserving data sharing. The model uses a Wavelet-Flow VAE
  and Diffusion Transformer to produce videos with FVD = 329.78, LPIPS = 0.48, VQAScore
  = 0.61, and BERTScore = 0.35, indicating good quality and content alignment.
---

# FFA Sora, video generation as fundus fluorescein angiography simulator

## Quick Facts
- arXiv ID: 2412.17346
- Source URL: https://arxiv.org/abs/2412.17346
- Authors: Xinyuan Wu; Lili Wang; Ruoyu Chen; Bowen Liu; Weiyi Zhang; Xi Yang; Yifan Feng; Mingguang He; Danli Shi
- Reference count: 0
- Generates dynamic fundus fluorescein angiography (FFA) videos from clinical reports to aid medical education and privacy-preserving data sharing

## Executive Summary
FFA Sora is a text-to-video diffusion model that generates dynamic fundus fluorescein angiography (FFA) videos from clinical reports to aid medical education and privacy-preserving data sharing. The model uses a Wavelet-Flow VAE and Diffusion Transformer to produce videos with FVD = 329.78, LPIPS = 0.48, VQAScore = 0.61, and BERTScore = 0.35, indicating good quality and content alignment. Privacy evaluation via image retrieval yielded low Recall@K scores (mean = 0.073), demonstrating effective anonymization. Human expert review (3 ophthalmologists, n=50) rated visual quality highly (mean = 1.570 on a 1–5 scale). FFA Sora addresses the challenge of training AI models on sensitive FFA data by generating realistic, lesion-preserving videos that reduce privacy risks and enhance accessibility for educational and research purposes.

## Method Summary
The FFA Sora model employs a Wavelet-Flow Variational Autoencoder (WF-VAE) and Diffusion Transformer (DiT) architecture based on Open-Sora Plan v1.3.0. The WF-VAE uses CausalConv3D layers for temporal processing and multi-level wavelet transforms to encode video sequences. The DiT component features transformer-based architecture with cross-modal attention mechanisms. The model generates 21×512×512 resolution videos from text descriptions, trained on a de-identified FFA dataset containing 3,625 videos paired with 1,814 clinical reports. Videos were standardized to 21 frames using linear interpolation and split 80/10/10 for training, validation, and testing.

## Key Results
- Generated videos achieve FVD = 329.78, LPIPS = 0.48, VQAScore = 0.61, and BERTScore = 0.35
- Privacy evaluation shows low Recall@K scores (mean = 0.073) indicating effective anonymization
- Human expert review (3 ophthalmologists, n=50) rated visual quality highly (mean = 1.570 on 1–5 scale)

## Why This Works (Mechanism)
The model leverages the strong temporal modeling capabilities of causal convolutions within the WF-VAE to capture the dynamic nature of FFA videos. The multi-level wavelet transforms provide efficient compression while preserving critical spatial and temporal features necessary for lesion detection. The Diffusion Transformer with cross-attention mechanisms effectively conditions video generation on clinical report text, ensuring semantic alignment between generated content and reported findings. The tiled convolution approach enables efficient inference while maintaining high-quality output resolution.

## Foundational Learning
1. **Wavelet transforms in video compression**: Why needed - Efficient multi-scale representation of spatial-temporal information; Quick check - Verify energy compaction across wavelet levels
2. **Causal convolutions for temporal modeling**: Why needed - Preserve temporal causality in video sequences; Quick check - Validate temporal consistency across generated frames
3. **Cross-modal attention mechanisms**: Why needed - Align textual clinical descriptions with visual video content; Quick check - Measure text-video semantic similarity scores
4. **Fréchet Video Distance (FVD)**: Why needed - Evaluate video quality and temporal coherence; Quick check - Compare FVD against baseline video generation models
5. **Privacy-preserving video generation**: Why needed - Enable medical data sharing while protecting patient identity; Quick check - Test image retrieval performance with different attack methods
6. **Diffusion transformers for video synthesis**: Why needed - Generate high-resolution videos with temporal consistency; Quick check - Evaluate frame-to-frame consistency metrics

## Architecture Onboarding

**Component Map:**
WF-VAE (Wavelet-Flow VAE) -> DiT (Diffusion Transformer) -> Video Generator

**Critical Path:**
Clinical report text → DiT cross-attention → DiT self-attention → WF-VAE decoding → Generated FFA video

**Design Tradeoffs:**
- Wavelet transforms provide efficient compression but may lose fine-grained details
- Causal convolutions ensure temporal consistency but limit parallel processing
- Text-to-video conditioning requires careful prompt engineering for clinical accuracy
- Privacy preservation may reduce generation quality if over-regularized

**Failure Signatures:**
- Low FVD scores with high LPIPS indicate poor temporal coherence despite good frame quality
- High Recall@K scores suggest inadequate privacy preservation and potential data leakage
- Poor BERTScore alignment indicates text-video semantic mismatch
- Inconsistent lesion appearance across frames suggests temporal modeling failure

**First Experiments:**
1. Generate sample videos with controlled prompts to verify basic text-to-video functionality
2. Measure FVD and LPIPS on validation set to establish baseline quality metrics
3. Test image retrieval performance to validate privacy preservation claims

## Open Questions the Paper Calls Out
1. How does FFA Sora perform on unseen disease types not represented in the training data?
2. What is the long-term clinical utility of FFA Sora-generated videos for diagnostic training?
3. Can FFA Sora be extended to integrate multimodal imaging (e.g., OCT, fundus photography) for richer video synthesis?

## Limitations
- Proprietary dataset from single hospital limits reproducibility and external validation
- Small human evaluation sample (n=50) and limited expert diversity
- Privacy evaluation may not capture all re-identification vectors
- No comparison with existing educational methods for clinical utility assessment

## Confidence
- Technical implementation: High
- Privacy preservation: Medium
- Clinical utility: Low

## Next Checks
1. Test the model on an independent, publicly available FFA dataset to verify generalizability beyond the original hospital's patient population and imaging equipment.
2. Conduct a larger-scale human evaluation with diverse medical professionals (not just 3 ophthalmologists) and include patient outcome measures to assess educational impact.
3. Perform adversarial privacy testing using advanced re-identification techniques, including temporal pattern analysis and rare lesion combination matching, to ensure comprehensive privacy preservation.