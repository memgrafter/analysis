---
ver: rpa2
title: Evaluating the Robustness of Deep-Learning Algorithm-Selection Models by Evolving
  Adversarial Instances
arxiv_id: '2406.16609'
source_url: https://arxiv.org/abs/2406.16609
tags:
- instances
- instance
- which
- adversarial
- misclassified
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper evaluates the robustness of deep recurrent networks
  (DRNs) used for algorithm selection in combinatorial optimization, specifically
  in the online bin-packing domain. The authors propose an evolutionary algorithm
  (EA) to evolve adversarial instances that cause trained DRNs to misclassify.
---

# Evaluating the Robustness of Deep-Learning Algorithm-Selection Models by Evolving Adversarial Instances

## Quick Facts
- arXiv ID: 2406.16609
- Source URL: https://arxiv.org/abs/2406.16609
- Authors: Emma Hart; Quentin Renau; Kevin Sim; Mohamad Alissa
- Reference count: 29
- Primary result: Deep recurrent networks for algorithm selection can be successfully attacked with evolved adversarial instances, with up to 56% of bin-packing problem instances manipulated to cause misclassification

## Executive Summary
This paper evaluates the vulnerability of deep learning models used for algorithm selection in combinatorial optimization problems, specifically focusing on online bin-packing. The authors propose an evolutionary algorithm approach to generate adversarial instances that can cause trained deep recurrent networks to misclassify which algorithm would perform best. Through experiments using two datasets and GRU-based models, they demonstrate that adversarial samples can be successfully generated from a significant portion of instances, revealing both fragile and robust instances. The study provides insights into model vulnerabilities and suggests that incorporating adversarial examples during training could improve model robustness.

## Method Summary
The authors employ an evolutionary algorithm (EA) to evolve adversarial instances that cause misclassification in deep recurrent networks (DRNs) trained for algorithm selection in online bin-packing. They train GRU models to predict the best solver (best-fit or first-fit heuristic) for each instance. The EA evolves masks that perturb instances by modifying item sizes within predefined ranges (±10% for DS2, ±5% for DS4), aiming to maximize the difference in output probabilities between correct and incorrect solver predictions. The fitness function rewards masks that increase the output probability for the wrong solver while penalizing those that increase the number of items. Results show successful generation of adversarial instances from up to 56% of original instances, with analysis revealing varying levels of instance fragility and robustness.

## Key Results
- Adversarial samples were successfully generated from up to 56% of original instances across two datasets
- The evolutionary algorithm effectively evolved masks that perturbed item sizes to cause misclassification
- Analysis revealed a spectrum of instance fragility, with some instances being particularly susceptible to perturbation while others remained robust
- Generated adversarial instances spanned a wide range of misclassification confidence levels, providing diverse training data for improving model robustness

## Why This Works (Mechanism)
The evolutionary algorithm works by iteratively evolving perturbation masks that modify item sizes within bounded ranges. These masks are designed to maximize the probability difference between the correct and incorrect solver predictions while maintaining feasibility constraints. The approach leverages the fact that deep learning models can be sensitive to small input perturbations, allowing the EA to find regions in the input space where the model's decision boundary can be crossed. The fitness function balances the goal of misclassification with the need to keep perturbations small, ensuring that generated adversarial instances remain within realistic bounds.

## Foundational Learning

**Combinatorial optimization** - Problems requiring optimal solutions from discrete solution spaces (e.g., bin-packing, TSP)
Why needed: Understanding the problem domain where algorithm selection models operate
Quick check: Can identify common combinatorial optimization problems and their characteristics

**Algorithm selection** - Choosing the most appropriate algorithm for a specific problem instance
Why needed: Core task being modeled by the deep learning approach
Quick check: Can explain how algorithm selection differs from traditional optimization

**Adversarial machine learning** - Techniques for generating inputs that cause model misclassification
Why needed: Framework for understanding how the evolutionary approach creates attacks
Quick check: Can describe different types of adversarial attacks and their goals

**Evolutionary algorithms** - Optimization techniques inspired by natural selection
Why needed: Method used to evolve adversarial instances
Quick check: Can explain basic concepts of mutation, selection, and fitness in EAs

**Recurrent neural networks** - Neural networks with feedback connections for sequence processing
Why needed: Architecture used for modeling algorithm selection
Quick check: Can describe how RNNs process sequential data differently from feedforward networks

## Architecture Onboarding

**Component map**: Input sequences -> GRU layers -> Dense layers -> Output probabilities for each solver
**Critical path**: Instance encoding → GRU processing → Solver probability prediction → EA fitness evaluation
**Design tradeoffs**: Smaller perturbation ranges increase realism but may reduce attack success; larger ranges increase success but reduce practical relevance
**Failure signatures**: High variance in adversarial success across instances; some instances showing complete resistance to perturbation
**First experiments**: 1) Train baseline GRU model on clean data and establish performance metrics 2) Run EA with small perturbation bounds to test basic attack feasibility 3) Analyze instance-level success rates to identify patterns in fragility

## Open Questions the Paper Calls Out
None

## Limitations
- Evaluation is limited to the online bin-packing domain using only two datasets, limiting generalizability to other combinatorial optimization problems
- Adversarial perturbations are constrained to small modifications within predefined ranges, which may not reflect real-world attack scenarios
- Only GRU architectures are evaluated without comparison to other deep learning approaches, leaving uncertainty about architecture-specific vulnerabilities

## Confidence
- High: Core finding that DRNs are vulnerable to adversarial attacks through evolved perturbations
- Medium: Specific perturbation effectiveness and instance fragility analysis under controlled conditions
- Low: Broader implications for algorithm selection robustness across different domains and model architectures

## Next Checks
1. Test the evolutionary adversarial attack methodology on additional combinatorial optimization problems beyond bin-packing, such as TSP or knapsack problems, to assess generalizability
2. Evaluate the effectiveness of the generated adversarial instances against alternative deep learning architectures (LSTM, Transformer-based models) to determine if vulnerability is architecture-specific
3. Investigate whether the same adversarial instances remain effective when models are retrained on augmented datasets containing adversarial examples, measuring improvements in model robustness