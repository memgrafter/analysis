---
ver: rpa2
title: Combining Hierachical VAEs with LLMs for clinically meaningful timeline summarisation
  in social media
arxiv_id: '2401.16240'
source_url: https://arxiv.org/abs/2401.16240
tags:
- timeline
- summaries
- summary
- individual
- mental
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a novel approach for generating clinically
  meaningful summaries of social media timelines for mental health monitoring. The
  method combines a hierarchical variational autoencoder (TH-VAE) with a large language
  model (LLaMA-2) to produce summaries that include both high-level clinical insights
  and temporally sensitive evidence from the user's timeline.
---

# Combining Hierachical VAEs with LLMs for clinically meaningful timeline summarisation in social media

## Quick Facts
- arXiv ID: 2401.16240
- Source URL: https://arxiv.org/abs/2401.16240
- Reference count: 40
- Primary result: TH-VAE outperforms LLM-only approaches in capturing changes over time and produces more factual and logically coherent summaries rich in clinical utility

## Executive Summary
This paper presents a novel approach for generating clinically meaningful summaries of social media timelines for mental health monitoring. The method combines a hierarchical variational autoencoder (TH-VAE) with a large language model (LLaMA-2) to produce summaries that include both high-level clinical insights and temporally sensitive evidence from the user's timeline. The TH-VAE model is adapted to capture long-range dependencies in the timeline data and is guided by LLM-annotated key phrases. The generated timeline summary is then input into LLaMA-2 with instruction prompting to produce the final summary.

## Method Summary
The approach combines a hierarchical VAE (TH-VAE) with LLMs to generate clinically meaningful timeline summaries. The TH-VAE model segments user timelines based on moments of change, extracts key phrases using an LLM, and encodes these with attention mechanisms. The resulting timeline summary is processed by LLaMA-2 with clinical prompts to generate high-level summaries. The method aims to capture both local features within segments and long-range dependencies between segments while focusing on clinically relevant information.

## Key Results
- TH-VAE outperforms LLM-only approaches in capturing changes over time
- Generated summaries show better factual consistency and logical coherence
- Clinical summaries are rich in utility for diagnosis and pattern recognition

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The TH-VAE model can learn long-range dependencies in user timelines by splitting them into segments based on moments of change.
- Mechanism: The model treats each segment as a unit and encodes key phrase-segment pairs with an attention mechanism. The hierarchical structure of TH-VAE partitions the latent variable into multiple sub-latent variables, allowing it to capture different sub-features and establish long-range dependencies between segments.
- Core assumption: Moments of change in mood are meaningful ways to segment user timelines, and consecutive posts with the same label or '0' contain similar mood-related information.

### Mechanism 2
- Claim: The LLM-annotated key phrases guide the TH-VAE to focus on clinically relevant information when generating the timeline summary.
- Mechanism: Key phrases are automatically extracted from each segment using an LLM, then encoded and used to weight the word embeddings in that segment. This representation is input into TH-VAE to generate the timeline summary, biasing it towards clinically important content.
- Core assumption: LLM-annotated key phrases accurately capture the most salient mental health related information in each segment.

### Mechanism 3
- Claim: The multi-stage LLM prompting framework effectively combines the timeline summary with high-level clinical insights.
- Mechanism: The timeline summary is fed into an instruction-tuned LLM with prompts originating from expert annotations. In the map stage, the LLM extracts inferences around clinical topics. In the reduce stage, it combines these into a concise high-level summary.
- Core assumption: The expert-annotated examples and clinical prompts provide sufficient guidance for the LLM to generate a coherent, clinically meaningful high-level summary.

## Foundational Learning

- Concept: Variational Autoencoders (VAEs)
  - Why needed here: VAEs are used as the basis for the TH-VAE model, which learns a latent representation of the timeline data to generate clinically meaningful summaries.
  - Quick check question: How does a VAE differ from a standard autoencoder, and what is the role of the latent variable distribution in a VAE?

- Concept: Hierarchical latent variable models
  - Why needed here: The hierarchical structure of TH-VAE, with multiple layers of latent variables, allows it to capture both local features within segments and long-range dependencies between segments.
  - Quick check question: What is the advantage of using a hierarchical latent variable model compared to a single-layer model, and how does this apply to the timeline summarization task?

- Concept: Attention mechanisms
  - Why needed here: Attention is used to encode the key phrase-segment pairs, allowing the model to focus on the most relevant information when generating the timeline summary.
  - Quick check question: How does an attention mechanism work, and what is its role in the context of the TH-VAE model?

## Architecture Onboarding

- Component map: User timeline → Segmentation (based on moments of change) → Key phrase extraction (via LLM) → Segment encoding (via attention) → TH-VAE (hierarchical VAE) → Timeline summary → High-level summary generation (via LLM with clinical prompts)
- Critical path: The critical path is from the user timeline through the TH-VAE to the timeline summary, as this is the novel component. The high-level summary generation is a downstream process that depends on the quality of the timeline summary.
- Design tradeoffs: The choice of using a hierarchical VAE allows for capturing long-range dependencies but adds complexity. Using an LLM for key phrase extraction leverages its language understanding but introduces potential biases. The multi-stage prompting framework for high-level summaries provides structure but relies heavily on the quality of the prompts.
- Failure signatures: If the timeline summary misses important clinical information, the high-level summary will also be lacking. If the high-level summary is not factually consistent with the timeline, it may be due to issues with the LLM's understanding or the prompts. If the model does not capture changes over time well, it could be an issue with the segmentation or the hierarchical structure.
- First 3 experiments:
  1. Evaluate the quality of the timeline summary generated by TH-VAE compared to baselines (LLM-only, skeleton-based) using automatic metrics (MHIC, FC) and human evaluation (factual consistency, usefulness).
  2. Ablation study: Remove key phrases or clinical prompts from TH-VAE to assess their impact on the timeline and high-level summary quality.
  3. Qualitative analysis: Compare the clinical summaries generated by TH-VAE and baselines to identify strengths and weaknesses in capturing diagnosis, patterns, and moments of change.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What are the limitations of using key phrases annotated by an LLM for timeline summarization, and how can we improve the quality of these key phrases?
- Basis in paper: [explicit] The paper mentions using LLM-annotated key phrases to guide the TH-VAE model in generating timeline summaries.
- Why unresolved: The paper does not discuss the limitations of using LLM-annotated key phrases or potential methods for improving their quality.
- What evidence would resolve it: Experiments comparing the performance of TH-VAE using key phrases annotated by different methods (e.g., human experts, other LLMs) or using different techniques to improve the quality of LLM-annotated key phrases (e.g., fine-tuning the LLM on mental health-related data).

### Open Question 2
- Question: How does the performance of TH-VAE compare to other hierarchical VAE models or different types of neural network architectures for timeline summarization?
- Basis in paper: [inferred] The paper introduces TH-VAE as a novel approach for timeline summarization and compares its performance to LLaMA and skeleton-based models.
- Why unresolved: The paper does not provide a comparison of TH-VAE with other hierarchical VAE models or alternative neural network architectures for timeline summarization.
- What evidence would resolve it: Experiments comparing the performance of TH-VAE to other hierarchical VAE models (e.g., NVAE, VHAE) or different neural network architectures (e.g., Transformers, LSTMs) for timeline summarization.

### Open Question 3
- Question: How can we adapt the TH-VAE model to handle timelines with different levels of granularity (e.g., daily, weekly, monthly posts) and varying lengths?
- Basis in paper: [inferred] The paper focuses on summarizing timelines consisting of consecutive posts between two dates, but does not discuss handling timelines with different levels of granularity or varying lengths.
- Why unresolved: The paper does not provide insights into how TH-VAE can be adapted to handle timelines with different levels of granularity or varying lengths.
- What evidence would resolve it: Experiments evaluating the performance of TH-VAE on timelines with different levels of granularity (e.g., daily, weekly, monthly posts) and varying lengths, and proposing modifications to the model architecture or training process to handle these variations.

## Limitations

- Data scope constraints: Small dataset of 30 manually annotated timelines from TalkLife may not capture full diversity of social media mental health discussions
- Evaluation methodology gaps: Human evaluation lacks details about rater expertise and inter-rater reliability; automated metrics may not fully capture clinical utility
- Implementation ambiguities: Several critical implementation details underspecified, making exact replication challenging

## Confidence

**High confidence**: The architectural approach combining hierarchical VAEs with LLMs for timeline summarization is technically sound and the implementation of basic components (VAE structure, attention mechanisms, LLM prompting) is likely correct based on established methods.

**Medium confidence**: The specific contributions around moments-of-change segmentation and the multi-stage LLM prompting framework are novel but their effectiveness depends heavily on implementation details not fully specified in the paper.

**Low confidence**: The clinical claims about improved diagnostic utility and mental health monitoring capabilities are not sufficiently validated by the evaluation methodology, which lacks involvement of mental health professionals and real-world clinical testing.

## Next Checks

1. **Clinical expert validation**: Conduct a formal study with licensed mental health professionals to evaluate the clinical utility of TH-VAE summaries compared to baseline methods, focusing on diagnostic accuracy, treatment planning relevance, and therapeutic insight.

2. **Dataset generalization test**: Evaluate TH-VAE on multiple social media platforms (Reddit, Twitter, etc.) and languages to assess robustness across different communication styles, cultural contexts, and platform-specific norms.

3. **Component ablation study**: Systematically remove each novel component (moments-of-change segmentation, key phrase guidance, multi-stage prompting) in controlled experiments to quantify their individual contributions to summary quality and clinical utility.