---
ver: rpa2
title: Dense Retrieval with Continuous Explicit Feedback for Systematic Review Screening
  Prioritisation
arxiv_id: '2407.00635'
source_url: https://arxiv.org/abs/2407.00635
tags:
- feedback
- dense
- clef
- screening
- retrieval
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces a dense retrieval approach with continuous
  explicit feedback for screening prioritisation in systematic reviews. The method
  uses dense representations and Rocchio's algorithm to iteratively update the query
  representation based on reviewer feedback during document screening.
---

# Dense Retrieval with Continuous Explicit Feedback for Systematic Review Screening Prioritisation

## Quick Facts
- arXiv ID: 2407.00635
- Source URL: https://arxiv.org/abs/2407.00635
- Reference count: 40
- Dense retrieval with continuous explicit feedback achieves similar or better AP and Last Rel scores compared to state-of-the-art methods while being more computationally efficient

## Executive Summary
This paper introduces a dense retrieval approach with continuous explicit feedback for screening prioritisation in systematic reviews. The method uses dense representations and Rocchio's algorithm to iteratively update the query representation based on reviewer feedback during document screening. Unlike existing methods, it avoids costly model fine-tuning by updating dense query vectors directly. Experiments on CLEF TAR datasets show that the proposed method achieves similar or better Average Precision (AP) and Last Relevant (Last Rel) scores compared to state-of-the-art methods, while being more computationally efficient.

## Method Summary
The method exploits continuous relevance feedback from reviewers during document screening to efficiently update the dense query representation, which is then applied to rank the remaining documents to be screened. It uses dense retrievers with Rocchio's algorithm to iteratively refine the query representation based on explicit feedback (relevant/non-relevant documents) without requiring model fine-tuning. The approach is evaluated across CLEF TAR datasets using domain-specific BERT backbones (BioLinkBERT, PubMedBERT) and shows promising effectiveness compared to previous methods.

## Key Results
- Dense retrievers except BioBERT outperform BM25+RM3 in AP when no feedback is considered
- Dense retrieval with continuous feedback achieves comparable or better effectiveness than TAR active learning methods
- Different Rocchio parameter settings ((1,1,1), (1,0.8,0.2), (1,0.5,0.5), (1,1,0)) show varying effectiveness across feedback sizes

## Why This Works (Mechanism)

### Mechanism 1
Dense retrieval with continuous explicit feedback avoids costly model fine-tuning while achieving similar or better effectiveness than BERT-based rankers. It uses dense representations and Rocchio's algorithm to iteratively update the query representation based on reviewer feedback during screening, without re-training the underlying model. Core assumption: Dense query vectors can be effectively updated using relevance feedback without requiring fine-tuning of the neural encoder.

### Mechanism 2
The Rocchio-based feedback algorithm with continuous updates improves retrieval effectiveness compared to initial rankings. It applies Rocchio's algorithm to dense representations: q_update = αq + βavg(d+) - γavg(d-), where positive and negative feedback from screened documents refines the query vector. Core assumption: The Rocchio algorithm transfers effectively to dense representations in the context of continuous feedback.

### Mechanism 3
Domain-specific BERT variants (BioLinkBERT, PubMedBERT) outperform general domain models and traditional BM25+RM3 approaches. It uses biomedical pre-trained BERT models as dense retriever backbones, leveraging domain-specific knowledge for better initial retrieval and feedback adaptation. Core assumption: Biomedical domain knowledge embedded in specialized BERT models improves retrieval quality for medical systematic review screening tasks.

## Foundational Learning

- Concept: Dense retrieval and dense representations
  - Why needed here: The method relies on dense vector representations of queries and documents rather than traditional sparse representations like TF-IDF or BM25.
  - Quick check question: What is the key difference between dense and sparse representations in information retrieval?

- Concept: Relevance feedback mechanisms
  - Why needed here: The method uses explicit continuous feedback from reviewers to iteratively improve retrieval results through Rocchio's algorithm.
  - Quick check question: How does Rocchio's algorithm update a query representation using relevant and non-relevant documents?

- Concept: Systematic review screening prioritisation
  - Why needed here: Understanding the specific task context where relevant documents need to be identified with high recall and ranked early for efficient review.
  - Quick check question: What are the two main phases of systematic review screening, and why is prioritisation important?

## Architecture Onboarding

- Component map: Document collection -> Dense retriever backbone -> Initial ranking -> Continuous feedback loop (reviewer assessment -> Rocchio update -> re-ranking) -> Evaluation metrics

- Critical path: Query → Initial dense retrieval → Top-k documents for review → Explicit feedback (relevant/non-relevant) → Rocchio update → Re-ranking → Repeat until completion

- Design tradeoffs: 
  - Dense retrieval provides better semantic matching but requires more computational resources than sparse retrieval
  - Continuous feedback improves effectiveness but adds latency to the screening process
  - Domain-specific BERT models improve initial retrieval but may have limited availability

- Failure signatures:
  - Poor initial retrieval quality despite dense representations
  - Feedback updates not improving ranking effectiveness
  - High variance in effectiveness across different topics
  - Computational costs exceeding acceptable thresholds

- First 3 experiments:
  1. Compare initial dense retrieval effectiveness (AP, Last Rel) against BM25+RM3 baseline without any feedback
  2. Test different Rocchio parameter settings ((α, β, γ) combinations) on a single dataset to identify optimal configuration
  3. Measure effectiveness improvement when varying feedback batch size (k) from 5 to 50 documents

## Open Questions the Paper Calls Out

### Open Question 1
How does the performance of dense retrieval with continuous explicit feedback compare to model-based approaches like active learning with logistic regression across different systematic review domains? The paper compares dense retrieval with continuous feedback to TAR active learning with logistic regression, showing dense retrieval's superiority in most cases, but this comparison is limited to CLEF TAR datasets focusing on medical systematic reviews.

### Open Question 2
What is the optimal batch size (k) for relevance feedback that balances computational efficiency and retrieval effectiveness across different dataset sizes? The paper explores varying k values from 5 to 50 but notes that larger feedback sizes are computationally beneficial while effectiveness varies by Rocchio setting, without establishing a general relationship.

### Open Question 3
How does the proposed method perform when integrated with downstream tasks like full-text screening compared to waiting for complete abstract screening? The paper mentions that relevant documents identified during screening can be passed to downstream tasks, but doesn't evaluate the impact on overall review timelines or measure practical impact on total systematic review completion time.

## Limitations
- Limited evaluation scope to CLEF TAR datasets without comparison to recent transformer-based rankers
- Lack of ablation studies and analysis of failure cases or sensitivity to feedback quality
- Sparse implementation details making exact reproduction challenging

## Confidence
- High confidence: Dense retrievers outperform BM25+RM3 baseline; domain-specific BERT variants show effectiveness improvements
- Medium confidence: Rocchio-based feedback mechanism improves effectiveness over initial rankings
- Low confidence: Computational efficiency claims relative to fine-tuning-based methods; generalization to non-CLEF datasets

## Next Checks
1. Implement ablation study comparing dense retrieval with continuous feedback against dense retrieval alone (no feedback) and sparse retrieval with relevance feedback
2. Test the method on additional systematic review datasets (e.g., from PROSPERO) to assess generalizability beyond CLEF TAR
3. Measure actual computational runtime of the feedback loop versus fine-tuning-based methods to validate efficiency claims