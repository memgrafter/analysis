---
ver: rpa2
title: 'Continual Diffuser (CoD): Mastering Continual Offline Reinforcement Learning
  with Experience Rehearsal'
arxiv_id: '2409.02512'
source_url: https://arxiv.org/abs/2409.02512
tags:
- expert
- tasks
- continual
- learning
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses catastrophic forgetting in continual offline
  reinforcement learning by proposing a rehearsal-based diffusion model called Continual
  Diffuser (CoD). The key innovation is integrating experience rehearsal into diffusion-based
  models to balance plasticity and stability when tasks arrive sequentially.
---

# Continual Diffuser (CoD): Mastering Continual Offline Reinforcement Learning with Experience Rehearsal

## Quick Facts
- arXiv ID: 2409.02512
- Source URL: https://arxiv.org/abs/2409.02512
- Authors: Jifeng Hu; Li Shen; Sili Huang; Zhejian Yang; Hechang Chen; Lichao Sun; Yi Chang; Dacheng Tao
- Reference count: 40
- One-line primary result: CoD achieves 478.19±15.84 mean return on Ant-dir tasks versus 270.44±5.54 for best baseline

## Executive Summary
This paper addresses catastrophic forgetting in continual offline reinforcement learning by proposing Continual Diffuser (CoD), a rehearsal-based diffusion model that integrates experience replay with diffusion-based trajectory modeling. The key innovation is using periodic rehearsal with a small portion of previous tasks' data during training to maintain knowledge while adapting to new tasks. The method demonstrates significant improvements over existing diffusion-based methods and continual learning baselines on a novel benchmark of 90 tasks from Continual World and Gym-MuJoCo environments.

## Method Summary
CoD addresses catastrophic forgetting in continual offline RL by combining diffusion models with experience rehearsal. The method processes trajectory datasets into sequences, trains a diffusion model with classifier-free task conditioning, and periodically replays samples from previous tasks during training. The model generates actions through conditional generation at inference, using task-specific information as conditions to guide trajectory generation. Key hyperparameters include 200 diffusion steps, sequence length 48, learning rate 3e-4, rehearsal frequency 2, and sample diversity 10%.

## Key Results
- On Ant-dir tasks, CoD achieves 478.19±15.84 mean return versus 270.44±5.54 for best baseline
- Maintains strong forward transfer and minimal forgetting across CW10 and CW20 tasks
- Demonstrates significant improvements over Diffuser-w/o rehearsal and DD-w/o rehearsal baselines

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Experience rehearsal with diffusion models reduces catastrophic forgetting in continual offline RL.
- **Mechanism:** Periodic replay of previous task data during training maintains knowledge of earlier tasks while adapting to new ones.
- **Core assumption:** Rehearsal is sufficient to prevent catastrophic forgetting without degrading performance on new tasks.
- **Evidence anchors:**
  - [abstract] "we preserve a small portion of previous datasets as the rehearsal buffer and replay it to retain the acquired knowledge"
  - [section 3.2] "CoD reaches mean success rate from 20% to 98% when incorporating experience rehearsal"
  - [corpus] Weak - no direct comparison of rehearsal vs non-rehearsal in related works
- **Break condition:** If rehearsal frequency or sample diversity is too low, catastrophic forgetting may reoccur. If too high, it may prevent adaptation to new tasks.

### Mechanism 2
- **Claim:** Classifier-free task conditioning enables effective control over diffusion model outputs across different tasks.
- **Mechanism:** The model learns to implicitly associate task-specific information with trajectory generation by training on both unconditional and conditional noise predictions.
- **Core assumption:** Task-specific information can be effectively encoded as conditions that guide trajectory generation.
- **Evidence anchors:**
  - [section 4.2] "we adopt the classifier-free guidance due to its simplicity, controllability, and higher performance"
  - [section 4.3] "we propose to use environment-related information as the task condition"
  - [corpus] Weak - no direct comparison of classifier-free vs classifier-guided methods in related works
- **Break condition:** If task conditions are not sufficiently discriminative, the model may generate inappropriate trajectories for specific tasks.

### Mechanism 3
- **Claim:** Diffusion models can effectively model joint state-action distributions for offline RL.
- **Mechanism:** By treating trajectories as data points and learning the reverse diffusion process, the model can generate realistic action sequences given observations.
- **Core assumption:** The trajectory distribution can be adequately modeled by a diffusion process, and the reverse process can be learned to generate valid actions.
- **Evidence anchors:**
  - [section 4.2] "diffusion-based models are proposed to model the distribution of trajectory τ"
  - [section 4.3] "we train our model on each task with sequential modeling of trajectories and make decisions with conditional generation"
  - [corpus] Weak - related works focus on diffusion for RL but don't explicitly validate joint distribution modeling
- **Break condition:** If the trajectory distribution is too complex or multimodal, the diffusion model may fail to capture it accurately.

## Foundational Learning

- **Concept: Diffusion probabilistic models**
  - Why needed here: The core method relies on diffusion models to model trajectory distributions and generate actions, so understanding how diffusion models work is essential.
  - Quick check question: What is the difference between the forward and reverse diffusion processes in a diffusion model?

- **Concept: Catastrophic forgetting in continual learning**
  - Why needed here: The paper addresses catastrophic forgetting, a key challenge in continual learning, so understanding this phenomenon is crucial.
  - Quick check question: What are the main approaches to mitigate catastrophic forgetting in continual learning?

- **Concept: Offline reinforcement learning**
  - Why needed here: The method is designed for offline RL settings, so understanding the challenges and approaches in offline RL is important.
  - Quick check question: What are the main challenges in offline RL compared to online RL?

## Architecture Onboarding

- **Component map:**
  - Noise prediction model (ϵθ) -> Task embedding function (ftask) -> Time embedding function (ftime) -> Rehearsal buffer -> Unet architecture with temporal convolution blocks

- **Critical path:**
  1. Process datasets by splitting trajectories into sequences
  2. Train diffusion model with task and time conditioning
  3. Periodically replay previous task samples during training
  4. Generate actions using conditional generation at inference

- **Design tradeoffs:**
  - Sequence length vs. computational cost - longer sequences capture more complex distributions but increase computation
  - Rehearsal frequency vs. adaptation speed - more frequent rehearsal reduces forgetting but may slow adaptation
  - Diffusion steps vs. generation speed - more steps improve quality but reduce speed

- **Failure signatures:**
  - Severe performance drop on previous tasks indicates catastrophic forgetting
  - Inability to adapt to new tasks suggests over-reliance on rehearsal
  - Poor generation quality may indicate insufficient model capacity or training

- **First 3 experiments:**
  1. Implement basic diffusion model without rehearsal on single task to verify trajectory modeling
  2. Add task conditioning and test on multiple tasks sequentially without rehearsal to observe catastrophic forgetting
  3. Add rehearsal mechanism and compare performance with and without rehearsal on continual tasks

## Open Questions the Paper Calls Out
The paper doesn't explicitly call out open questions, but several important ones emerge from the work:

### Open Question 1
- Question: How does the performance of Continual Diffuser scale with the number of tasks beyond the 90 tasks tested in the current benchmark?
- Basis in paper: [explicit] The paper constructs a benchmark with 90 tasks but states "we will actively maintain and progressively incorporate more datasets into our benchmark."
- Why unresolved: The paper only tests on the current 90 tasks and doesn't explore how performance changes as the task count increases significantly.
- What evidence would resolve it: Systematic testing of CoD on benchmarks with 100+ tasks, measuring performance degradation and memory requirements.

### Open Question 2
- Question: What is the optimal balance between rehearsal frequency (υ) and rehearsal sample diversity (ξ) for different types of continual learning scenarios?
- Basis in paper: [explicit] The paper mentions "larger υ corresponds to aggravated catastrophic forgetting" and "a larger value of ξ will improve the performance and increase the storage burden" but doesn't provide specific guidelines for different scenarios.
- Why unresolved: The paper only provides general observations about these hyperparameters without specific recommendations for different task types or difficulty levels.
- What evidence would resolve it: Comprehensive ablation studies across various task types (CICORL, TICORL, DICORL) showing optimal hyperparameter combinations for each.

### Open Question 3
- Question: How does Continual Diffuser perform in task-free continual learning settings where task boundaries are not provided?
- Basis in paper: [inferred] The paper focuses on task-aware continual learning and mentions "explicit boundaries(task-awareCL)" but doesn't test on task-free scenarios.
- Why unresolved: All experiments in the paper assume task boundaries are known, leaving open whether CoD can handle more realistic scenarios where tasks change without explicit notification.
- What evidence would resolve it: Experiments testing CoD on task-free continual learning benchmarks and comparing performance to task-aware settings.

## Limitations
- The specific mechanisms by which classifier-free task conditioning and diffusion-based trajectory modeling enable effective continual learning are not thoroughly validated against alternatives
- The rehearsal mechanism's parameters (10% diversity, frequency 2) appear arbitrary without systematic sensitivity analysis
- The claim that diffusion models can effectively model joint state-action distributions for offline RL has only weak supporting evidence from related works

## Confidence

### Confidence Assessment
- **High confidence**: The core problem statement (catastrophic forgetting in continual offline RL) is well-established and the experimental methodology (benchmark construction, metric definitions) is sound.
- **Medium confidence**: The overall approach of combining diffusion models with experience rehearsal is reasonable and shows promising results, though the specific design choices lack rigorous justification.
- **Low confidence**: The specific mechanisms by which classifier-free task conditioning and diffusion-based trajectory modeling enable effective continual learning are not thoroughly validated against alternatives.

## Next Checks
1. **Ablation study**: Systematically vary rehearsal frequency and sample diversity to identify optimal parameters and verify robustness across different task sequences.
2. **Alternative comparison**: Implement and compare against a classifier-guided diffusion approach to empirically validate the claimed advantages of classifier-free guidance.
3. **Distribution analysis**: Evaluate the quality of learned joint state-action distributions using metrics like coverage and diversity to verify that diffusion models can adequately capture complex trajectory distributions.