---
ver: rpa2
title: 'VisualWebArena: Evaluating Multimodal Agents on Realistic Visual Web Tasks'
arxiv_id: '2401.13649'
source_url: https://arxiv.org/abs/2401.13649
tags:
- tasks
- agents
- page
- task
- visual
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces VisualWebArena, a benchmark for evaluating
  multimodal web agents on visually grounded tasks. The benchmark includes 910 diverse
  tasks across three web environments (Classifieds, Shopping, Reddit) that require
  visual understanding to solve.
---

# VisualWebArena: Evaluating Multimodal Agents on Realistic Visual Web Tasks

## Quick Facts
- arXiv ID: 2401.13649
- Source URL: https://arxiv.org/abs/2401.13649
- Reference count: 40
- Best VLM agent achieves 16.4% success rate on 910 visual web tasks

## Executive Summary
VisualWebArena is a benchmark for evaluating multimodal web agents on visually grounded tasks across three realistic web environments (Classifieds, Shopping, Reddit). The benchmark includes 910 diverse tasks that require visual understanding to solve, testing agents' abilities to navigate and complete objectives that involve images, colors, and layouts. The authors evaluate state-of-the-art LLM and VLM agents, finding that while multimodal models significantly outperform text-only agents, all models substantially underperform human performance (88.7% success rate). The paper also introduces a new Set-of-Marks (SoM) representation that simplifies the action space and improves performance on visually complex websites.

## Method Summary
The authors created VisualWebArena by developing three self-hosted web environments (Classifieds, Shopping, Reddit) with 910 visually grounded tasks. They evaluated agents using different observation spaces: accessibility trees (text-only), accessibility trees with image captions, screenshots, and the proposed Set-of-Marks representation. Agents were prompted using Chain-of-Thought reasoning with 3 in-context examples. The evaluation measured success rates across different task difficulty levels and web environments, comparing multimodal agents against text-only baselines and human performance.

## Key Results
- Multimodal agents (16.4% success rate) significantly outperform text-only agents (9.6%) on visual web tasks
- Human performance baseline of 88.7% highlights substantial headroom for model improvement
- Set-of-Marks representation improves performance on visually complex websites, particularly Classifieds (12.38% → 17.14%) and Reddit (8.12% → 9.83%)
- All models struggle with OCR-heavy tasks and exact image matching, while easy tasks are solved more reliably than hard tasks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Multimodal agents outperform text-only agents because they can directly process visual information rather than relying on text descriptions.
- Mechanism: The visual understanding capability allows multimodal agents to better interpret webpage content, identify visual elements like colors, shapes, and patterns, and execute actions based on this visual context.
- Core assumption: The visual information in webpages contains complementary data that is not fully captured by text-based accessibility trees or HTML representations.
- Evidence anchors: [abstract] "while multimodal models significantly outperform text-only agents"; [section] "Visual understanding of product images is required for successfully navigating and completing tasks on e-commerce platforms"; [corpus] Weak - no direct corpus evidence, but the performance gap between multimodal and text-only agents supports this claim
- Break condition: If the visual information in webpages becomes redundant or fully capturable by text descriptions, or if the multimodal models fail to properly process visual inputs.

### Mechanism 2
- Claim: The Set-of-Marks (SoM) representation improves performance on visually complex websites by providing a simplified action space with direct element references.
- Mechanism: SoM assigns unique IDs to interactable elements and provides bounding boxes, allowing agents to reference elements directly without complex visual co-referencing or coordinate prediction.
- Core assumption: Strong VLMs can effectively process SoM representations and use the unique IDs to perform precise element selection and interaction.
- Evidence anchors: [abstract] "The paper also proposes a new Set-of-Marks (SoM) representation that simplifies the action space and improves performance on visually complex websites"; [section] "We observe particularly substantial improvements on Classifieds and Reddit, from 12.38% → 17.14% and 8.12% → 9.83% respectively"; [corpus] Weak - only general knowledge that direct element references simplify agent action spaces
- Break condition: If VLMs cannot effectively process SoM representations, or if the visual complexity of websites exceeds the resolution or coverage of SoM bounding boxes.

### Mechanism 3
- Claim: VisualWebArena's tasks require visual understanding because modern web interfaces are designed for human visual perception, not text-only processing.
- Mechanism: The tasks in VisualWebArena leverage visual elements like colors, images, and layouts that are essential for task completion, mimicking real-world web usage where visual cues are crucial.
- Core assumption: Real-world web tasks often require visual information that cannot be adequately represented through text alone.
- Evidence anchors: [abstract] "Given that most computer interfaces cater to human perception, visual information often augments textual data in ways that text-only models struggle to harness effectively"; [section] "VisualWebArena is designed around three realistic web environments that involve visually rich content"; [corpus] Moderate - related work on web agents (Zhou et al., 2024) and multimodal models supports the importance of visual information
- Break condition: If web interfaces shift towards text-only designs or if visual elements become purely decorative rather than functional.

## Foundational Learning

- Concept: Partially Observable Markov Decision Process (POMDP)
  - Why needed here: The web environment is modeled as a POMDP where agents have partial observations (current webpage) and must take actions to reach goal states, which is fundamental to understanding autonomous web agent frameworks.
  - Quick check question: In the context of VisualWebArena, what are the components of the POMDP model and how do they relate to web navigation?

- Concept: Visual grounding and multimodal reasoning
  - Why needed here: VisualWebArena tasks require agents to understand and reason about both visual and textual information, which is essential for solving visually grounded tasks that involve image-text inputs and visual webpage content.
  - Quick check question: How does visual grounding differ from simple image recognition, and why is it crucial for autonomous web agents?

- Concept: Accessibility tree vs. visual representations
  - Why needed here: Understanding the different ways webpage content can be represented (accessibility tree, raw HTML, screenshots, SoM) is crucial for designing effective agents and understanding the performance differences between them.
  - Quick check question: What are the advantages and disadvantages of using accessibility trees versus visual screenshots for web agents, and when might each be preferable?

## Architecture Onboarding

- Component map: Self-hosted web environments (Classifieds, Shopping, Reddit) -> Task specification and reward function framework -> Agent framework (observation processing, action generation, browser automation interface)
- Critical path: Task specification → Observation processing (webpage content + any input images) → Reasoning and planning (what actions to take) → Action generation (click, type, navigate, etc.) → Environment state transition → Reward evaluation → Repeat until task completion or failure. The bottleneck is typically in the reasoning and planning stage where agents must interpret visual information and determine appropriate actions.
- Design tradeoffs: 1) Visual representation choice (accessibility tree vs. screenshots vs. SoM) - accessibility trees provide structured information but may miss visual details, while screenshots provide complete visual context but require more processing; 2) Action space complexity - simplified action spaces (like SoM) are easier for agents but may limit flexibility, while complex action spaces provide more control but are harder to learn; 3) Multimodal model selection - larger models may perform better but are more expensive to run and have longer inference times.
- Failure signatures: Common failure modes include: 1) Getting stuck in loops or oscillating between pages, 2) Giving up too early when the correct element is not immediately visible, 3) Performing useless actions (like opening blank tabs), 4) Failing to scroll to see necessary elements, 5) Repeating actions without learning from previous failures, 6) Poor performance on OCR-heavy tasks or exact image matching tasks, and 7) Difficulty with multi-step reasoning tasks that require maintaining context across multiple pages.
- First 3 experiments:
  1. Compare performance of text-only agent with accessibility tree input vs. the same agent with image captions added to the accessibility tree to quantify the benefit of visual information.
  2. Test the SoM representation on a subset of visually complex tasks to measure the specific improvement in navigation efficiency and accuracy.
  3. Evaluate agent performance on easy vs. hard tasks to understand the difficulty scaling and identify which capabilities are most limiting for current models.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the Set-of-Marks (SoM) representation impact long-term task completion performance across multiple web sessions?
- Basis in paper: [explicit] The paper mentions that the SoM representation improves performance on visually complex websites and is particularly effective for navigation, but does not explore its effectiveness across extended, multi-session tasks.
- Why unresolved: The current evaluation focuses on individual task completion within a single session. Long-term task performance, including memory of past interactions and cross-session navigation, remains unexplored.
- What evidence would resolve it: Empirical results comparing SoM performance on tasks requiring multiple sessions or extended memory, including metrics on task completion rates, navigation efficiency, and memory utilization across sessions.

### Open Question 2
- Question: What is the impact of fine-tuning vision-language models (VLMs) on web-specific data compared to using prompt-based approaches?
- Basis in paper: [inferred] The paper uses prompt-based approaches with few-shot examples and notes that success rates increase with more examples, suggesting potential benefits from fine-tuning. However, it does not explore fine-tuning VLMs on web-specific data.
- Why unresolved: Fine-tuning could potentially improve model performance by adapting VLMs to the specific characteristics of web interfaces and tasks, but the paper only uses prompt-based approaches.
- What evidence would resolve it: Comparative results between prompt-based approaches and fine-tuned VLMs on the same benchmark, including metrics on success rates, task completion times, and error rates.

### Open Question 3
- Question: How do different visual representations (SoM, accessibility tree, screenshots) compare in terms of computational efficiency and model performance?
- Basis in paper: [explicit] The paper introduces SoM as a new visual representation and compares it to accessibility trees and screenshots, but does not provide a comprehensive analysis of computational efficiency.
- Why unresolved: While the paper shows performance differences between representations, it does not quantify the computational costs (e.g., memory usage, inference time) associated with each approach.
- What evidence would resolve it: Detailed benchmarks comparing computational resources required for each representation (memory, inference time) alongside performance metrics (success rates, task completion times) across various tasks and model sizes.

## Limitations
- The 71.3% performance gap between human (88.7%) and best model (16.4%) performance suggests current agents have fundamental limitations in visual understanding and reasoning
- The study does not explore whether SoM representation introduces information loss compared to full visual screenshots, which could affect performance comparisons
- Evaluation focuses primarily on task completion success rates without detailed analysis of agent efficiency or failure mode distributions across different task types

## Confidence
- **High confidence**: Multimodal agents significantly outperform text-only agents (16.4% vs. 9.6% success rates) - well-supported by direct comparisons across multiple environments
- **Medium confidence**: The Set-of-Marks representation improves performance on visually complex websites - improvements observed but mechanism and generalizability require further validation
- **Medium confidence**: VisualWebArena tasks require visual understanding that text-only agents cannot adequately process - performance gap supports this claim but exact nature of required visual information is not fully characterized

## Next Checks
1. Conduct ablation studies on the SoM representation to quantify information loss compared to full screenshots and validate that improvements are not simply due to action space simplification rather than visual understanding benefits.
2. Perform detailed failure mode analysis across different task categories to identify specific capabilities (e.g., OCR, image matching, multi-step reasoning) that limit current agent performance and guide future improvements.
3. Evaluate agent efficiency metrics (action count, completion time) alongside success rates to provide a more comprehensive assessment of agent capabilities and identify optimization opportunities beyond task completion.