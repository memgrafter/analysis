---
ver: rpa2
title: Unified Anomaly Detection methods on Edge Device using Knowledge Distillation
  and Quantization
arxiv_id: '2407.02968'
source_url: https://arxiv.org/abs/2407.02968
tags:
- performance
- anomaly
- quantization
- detection
- which
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This work explores deploying lightweight, unified multi-class\
  \ anomaly detection models on edge devices. Three knowledge-distillation based methods\u2014\
  Uninformed Students, Reverse Distillation, and STFPM\u2014are evaluated on the MVTec\
  \ AD dataset."
---

# Unified Anomaly Detection methods on Edge Device using Knowledge Distillation and Quantization

## Quick Facts
- arXiv ID: 2407.02968
- Source URL: https://arxiv.org/abs/2407.02968
- Reference count: 30
- Primary result: Multi-class anomaly detection models perform comparably to one-class models when deployed on edge devices using knowledge distillation and quantization

## Executive Summary
This work addresses the challenge of deploying lightweight anomaly detection models on edge devices for industrial visual inspection. The authors propose using knowledge distillation to transfer knowledge from teacher networks to smaller student networks, enabling efficient multi-class anomaly detection. They evaluate three knowledge-distillation based methods (Uninformed Students, Reverse Distillation, and STFPM) on the MVTec AD dataset and demonstrate that multi-class models perform at par with traditional one-class models. The study also investigates quantization techniques (post-training and quantization-aware training) to further reduce model size and latency, showing that quantization-aware training preserves near-original performance while significantly reducing computational requirements.

## Method Summary
The authors employ knowledge distillation to train lightweight student networks for multi-class anomaly detection, using three approaches: Uninformed Students (error between student ensemble and teacher predictions), Reverse Distillation (reconstruction error), and STFPM (similarity matching). These methods are trained on combined normal data from all 15 classes of the MVTec AD dataset. For edge deployment, they apply quantization using both post-training quantization (PTQ) with different calibration strategies and quantization-aware training (QAT) to reduce model size and inference latency while maintaining performance.

## Key Results
- Multi-class anomaly detection models achieve comparable performance to one-class models on the MVTec AD dataset
- Random normal data calibration for PTQ outperforms training data calibration in preserving model performance
- QAT preserves near-original FP-32 performance for two high-performing models while significantly reducing model size and latency on NVIDIA Jetson Xavier NX

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Multi-class anomaly detection models perform comparably to one-class models when object classes are significantly different from each other
- Mechanism: When object classes have distinct feature distributions, anomalies that deviate from normal patterns in one class won't overlap with features of other classes, allowing a single model to generalize across classes
- Core assumption: The object classes in the MVTec AD dataset are sufficiently different from each other
- Evidence anchors:
  - [abstract]: "Our experimental study shows that multi-class models perform at par with one-class models for the standard MVTec AD dataset"
  - [section 4.2]: "Hence, the feature distributions of one object class are likely to be different from others"
  - [corpus]: No direct evidence, but related papers on multi-class AD suggest this is a known challenge
- Break condition: If object classes share significant feature overlap, the unified model's performance would degrade significantly compared to one-class models

### Mechanism 2
- Claim: Knowledge distillation from teacher to student networks enables lightweight models to maintain high anomaly detection performance
- Mechanism: The teacher network learns robust feature representations from normal data, which are transferred to the student network through distillation. During inference, the student's predictions deviating from the teacher indicate anomalies
- Core assumption: The teacher network learns meaningful representations that generalize to anomaly detection
- Evidence anchors:
  - [section 3.1]: "The anomaly score is the error between the mean predictions of the students' ensemble and the teacher's prediction"
  - [section 3.2]: "Anomalies are detected based on the deviations of the reconstructed output from the student and the input image"
  - [corpus]: Related papers on knowledge distillation for AD confirm this approach
- Break condition: If the teacher network overfits to the training data or fails to learn generalizable features, the distillation process would transfer poor representations

### Mechanism 3
- Claim: Quantization-aware training (QAT) preserves model performance better than post-training quantization (PTQ) by simulating quantization error during training
- Mechanism: QAT introduces fake-quantization modules during training that simulate the effects of quantization, allowing the model to adapt its weights to minimize quantization error. This results in weights that, when actually quantized, maintain performance closer to the original FP-32 model
- Core assumption: Simulating quantization error during training allows the model to find better minima that are more robust to quantization
- Evidence anchors:
  - [section 3.4.2]: "Due to quantization, the performance drop in PTQ is further compensated by QAT, which yields at par performance with the original 32-bit Floating point in two of the models considered"
  - [section 4.7]: "As QAT involves training during the quantization process, this may imply that the performance of QAT is likely to be better than PTQ"
  - [corpus]: No direct evidence, but this is a standard finding in quantization literature
- Break condition: If the fake-quantization simulation doesn't accurately represent actual quantization effects, QAT would not provide the expected performance benefits

## Foundational Learning

- Concept: Knowledge distillation in neural networks
  - Why needed here: The core methodology relies on transferring knowledge from larger teacher networks to smaller student networks for anomaly detection
  - Quick check question: What is the primary difference between traditional knowledge distillation and the reverse distillation approach used in this paper?

- Concept: Quantization techniques for neural networks
  - Why needed here: The paper compares post-training quantization (PTQ) and quantization-aware training (QAT) for edge deployment
  - Quick check question: What is the key difference between PTQ and QAT in terms of when the quantization error is introduced?

- Concept: Anomaly detection metrics and evaluation
  - Why needed here: Understanding AUROC (Area Under the ROC Curve) and how it's used to evaluate anomaly detection performance across different models and quantization schemes
  - Quick check question: Why might AUROC be a more appropriate metric than accuracy for evaluating anomaly detection models?

## Architecture Onboarding

- Component map:
  - Teacher network (typically ResNet or WideResNet) -> Student network (typically smaller CNN) -> Anomaly scoring mechanism -> Quantization modules -> Edge device runtime

- Critical path:
  1. Train teacher network on normal data from all classes
  2. Train student network with knowledge distillation from teacher
  3. Implement quantization (PTQ or QAT)
  4. Deploy quantized model on edge device
  5. Run inference and compute anomaly scores

- Design tradeoffs:
  - Model size vs. performance: Smaller models are faster but may lose accuracy
  - Calibration strategy: Training data vs. random normal data affects PTQ performance
  - Framework choice: PyTorch vs. TensorRT impacts quantization effectiveness and deployment options

- Failure signatures:
  - High AUROC but poor localization: Model detects anomalies but doesn't accurately identify defect regions
  - Significant performance drop after quantization: Indicates poor calibration or need for QAT instead of PTQ
  - Inconsistent performance across classes: Suggests model doesn't generalize well to all object types

- First 3 experiments:
  1. Train and evaluate one-class vs. multi-class models on MVTec AD to verify generalization claims
  2. Implement PTQ with both training data and random normal data calibration to compare effectiveness
  3. Apply QAT to top-performing models and compare with PTQ results to validate performance preservation

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do unified multi-class anomaly detection models generalize to datasets with overlapping object classes, where feature distributions are not distinct?
- Basis in paper: [inferred] The paper states that multi-class models perform well because object classes in MVTec AD are "significantly different from each other." It raises the question of whether this holds for datasets with overlapping classes.
- Why unresolved: The paper only tested on MVTec AD, which has distinct object classes. No experiments were conducted on datasets with overlapping or similar classes.
- What evidence would resolve it: Testing the same multi-class models on datasets like CIFAR-10 or industrial datasets with overlapping object classes, comparing their performance to one-class models.

### Open Question 2
- Question: What is the optimal calibration strategy for post-training quantization in unsupervised anomaly detection tasks beyond random normal data and training data calibration?
- Basis in paper: [explicit] The paper explores two calibration strategies (training data and random normal data) and finds random normal data performs better, but does not explore other potential strategies.
- Why unresolved: Only two calibration methods were tested. The paper does not investigate whether other strategies (e.g., test data, synthetic data, or adaptive calibration) could yield better results.
- What evidence would resolve it: Comparative experiments using alternative calibration datasets or adaptive calibration techniques, measuring AUROC and latency improvements.

### Open Question 3
- Question: How does the choice of quantization precision (e.g., 4-bit, 2-bit) affect the trade-off between model size, latency, and anomaly detection performance in edge deployments?
- Basis in paper: [inferred] The paper evaluates 8-bit and 16-bit quantization but does not explore lower precision levels, which could further reduce model size and latency.
- Why unresolved: The study stops at 8-bit quantization. Lower precision quantization (e.g., 4-bit or 2-bit) could offer additional benefits but may also introduce higher performance degradation.
- What evidence would resolve it: Systematic evaluation of 4-bit and 2-bit quantization on the same models, comparing AUROC, model size, and latency trade-offs.

## Limitations
- The study only tested on the MVTec AD dataset, which has distinct object classes, limiting generalization to real-world scenarios with similar object classes
- The effectiveness of random normal data for PTQ calibration is shown but not thoroughly explained
- The paper does not explore quantization at lower precisions (e.g., INT-4, INT-2) that could offer additional benefits

## Confidence
- **High Confidence**: Multi-class models performing comparably to one-class models on MVTec AD dataset; quantization-aware training preserving FP-32 performance better than post-training quantization
- **Medium Confidence**: Claims about knowledge distillation transferring meaningful representations; effectiveness of random normal data for PTQ calibration
- **Low Confidence**: Generalization to real-world scenarios with similar object classes; long-term stability of quantized models on edge devices

## Next Checks
1. **Cross-Dataset Validation**: Test unified multi-class models on datasets with varying degrees of class similarity to validate the assumption that distinct object classes enable effective multi-class AD
2. **Quantization Robustness**: Evaluate model performance under different quantization bit-widths (e.g., INT-4, INT-2) and different edge device architectures to assess generalization of QAT benefits
3. **Real-World Deployment**: Deploy the quantized models in an actual industrial setting with continuous operation to measure performance stability and identify potential edge cases not captured in the controlled MVTec AD dataset