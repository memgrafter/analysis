---
ver: rpa2
title: 'PSLM: Parallel Generation of Text and Speech with LLMs for Low-Latency Spoken
  Dialogue Systems'
arxiv_id: '2406.12428'
source_url: https://arxiv.org/abs/2406.12428
tags:
- speech
- text
- tokens
- pslm
- latency
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: PSLM is a multimodal language model that enables parallel generation
  of text and speech tokens, addressing latency issues in spoken dialogue systems.
  It extends the input and output sequences of a pretrained decoder-only Transformer
  to handle both modalities, allowing speech generation to begin immediately after
  speech questions are received.
---

# PSLM: Parallel Generation of Text and Speech with LLMs for Low-Latency Spoken Dialogue Systems

## Quick Facts
- arXiv ID: 2406.12428
- Source URL: https://arxiv.org/abs/2406.12428
- Reference count: 13
- Primary result: PSLM achieves 60-70% latency reduction in spoken question answering while maintaining response quality through parallel text and speech generation

## Executive Summary
PSLM introduces a novel approach to reduce latency in spoken dialogue systems by enabling parallel generation of text and speech tokens. The model extends a pretrained decoder-only Transformer to handle both modalities simultaneously, allowing speech generation to begin immediately after receiving speech questions. By introducing multiple speech streams and employing a weighted loss function, PSLM achieves significant latency improvements compared to sequential chain-of-modality prompting while preserving response quality.

## Method Summary
PSLM modifies a pretrained decoder-only Transformer by extending its input and output sequences to handle both text and speech modalities. The model employs a weighted loss function to balance losses between text and speech streams, and introduces multiple speech streams to accelerate generation. This architecture allows the model to generate text and speech tokens in parallel rather than sequentially, significantly reducing the latency in spoken dialogue systems. The approach is specifically designed for spoken question answering tasks but has potential applications in broader dialogue systems.

## Key Results
- Achieves 60-70% latency reduction compared to chain-of-modality prompting
- Maintains response quality while significantly improving generation speed
- Successfully demonstrates parallel generation capability for both text and speech tokens
- Shows effectiveness on spoken question answering tasks

## Why This Works (Mechanism)
PSLM's parallel generation approach works by modifying the traditional sequential processing pipeline of spoken dialogue systems. Instead of waiting for text generation to complete before starting speech synthesis, PSLM generates both modalities simultaneously from the same model. The weighted loss function ensures proper balance between text and speech quality, while multiple speech streams allow for faster audio output generation. This parallel processing fundamentally reduces the end-to-end latency of the system.

## Foundational Learning
**Transformer architecture**: The core of PSLM is built on decoder-only Transformers, which are essential for understanding the model's extension capabilities and how it processes sequential data. Quick check: Review self-attention mechanisms and masked attention in decoder Transformers.

**Modality fusion**: Understanding how different input/output modalities can be integrated into a single model is crucial for grasping PSLM's approach. Quick check: Study cross-modal attention mechanisms and multi-modal embedding strategies.

**Weighted loss functions**: The model uses weighted loss to balance text and speech generation quality. Quick check: Review how loss weighting affects multi-task learning and modality-specific performance.

## Architecture Onboarding
**Component map**: Input speech/text -> Encoder embedding -> Weighted loss balancing -> Multiple speech streams -> Parallel text/speech output
**Critical path**: Speech input → Text/speech embedding → Parallel generation → Weighted loss optimization → Final output
**Design tradeoffs**: Speed vs. quality balance, complexity of multiple streams vs. latency reduction, model size vs. real-time performance
**Failure signatures**: Text quality degradation when speech streams dominate, audio artifacts from insufficient speech stream training, latency increases when streams become imbalanced
**First experiments**:
1. Single stream parallel generation with balanced loss weighting
2. Multi-stream configuration with varying weight ratios
3. Ablation study on speech stream count impact on latency and quality

## Open Questions the Paper Calls Out
None identified in the provided information.

## Limitations
- Limited ablation studies on weighted loss function impact
- Lack of evaluation on complex multi-turn dialogue scenarios
- No assessment of computational overhead for multiple speech streams
- Unclear handling of natural conversation disfluencies and interruptions

## Confidence
- High confidence in parallel generation capability and latency metrics
- Medium confidence in quality preservation claims
- Low confidence in generalizability to complex dialogue scenarios

## Next Checks
1. Conduct comprehensive ablation studies on weighted loss function with different weighting schemes
2. Evaluate performance on complex dialogue tasks including multi-turn conversations and disfluency handling
3. Perform detailed computational complexity analysis for real-time deployment assessment