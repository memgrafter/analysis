---
ver: rpa2
title: Can In-context Learning Really Generalize to Out-of-distribution Tasks?
arxiv_id: '2410.09695'
source_url: https://arxiv.org/abs/2410.09695
tags:
- task
- tasks
- pretraining
- context
- in-context
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work investigates whether in-context learning (ICL) can truly
  generalize to out-of-distribution (OOD) tasks. The authors conduct synthetic experiments
  with a GPT-2 model to explore ICL's performance on OOD mathematical functions.
---

# Can In-context Learning Really Generalize to Out-of-distribution Tasks?

## Quick Facts
- arXiv ID: 2410.09695
- Source URL: https://arxiv.org/abs/2410.09695
- Authors: Qixun Wang; Yifei Wang; Yisen Wang; Xianghua Ying
- Reference count: 40
- Primary result: In-context learning (ICL) tends to implement pretraining function classes and optimize within them when faced with out-of-distribution tasks, rather than truly generalizing to new input-output mappings.

## Executive Summary
This paper investigates whether in-context learning can truly generalize to out-of-distribution tasks through synthetic experiments with a GPT-2 model. The authors find that ICL's performance on OOD mathematical functions resembles implementing and optimizing functions from the pretraining hypothesis space rather than learning new task functions. Their key findings show that ICL's ability to handle unseen abstract labels only works in in-distribution scenarios, and when pretrained on multiple tasks, ICL exhibits a "low-test-error preference" that selects the pretraining function yielding lowest test error in the testing context. Both empirical and theoretical analyses support these conclusions about ICL's limitations for OOD generalization.

## Method Summary
The authors train GPT-2 from scratch on simple function classes (linear regression, quadratic regression, and 2-layer ReLU network regression) and evaluate its in-context learning performance on out-of-distribution tasks like square root regression and cubic regression. They compare ICL performance with models from the same function classes trained by gradient descent on the in-context examples, observing that ICL performance approaches gradient descent performance on OOD tasks. The experiments use synthetic mathematical functions as inputs, with word embeddings from pretrained models, and measure test error (squared error) for regression tasks and top-5 accuracy for classification tasks.

## Key Results
- ICL performance on OOD tasks resembles implementing and optimizing functions from the pretraining hypothesis space
- ICL's ability to learn unseen abstract labels only manifests in scenarios without distributional shifts
- When pretrained on multiple tasks, ICL exhibits a "low-test-error preference" selecting the pretraining function yielding lowest test error in the testing context

## Why This Works (Mechanism)

### Mechanism 1
- Claim: ICL performance on OOD tasks resembles implementing pretraining function classes and optimizing via gradient descent
- Mechanism: When presented with OOD context, ICL retrieves the closest matching function from its pretraining hypothesis space and optimizes parameters within that class using the in-context examples
- Core assumption: The pretraining distribution contains functions sufficiently close to OOD test functions for this approximation to work
- Evidence anchors:
  - [abstract] "ICL performance resembles implementing a function within the pretraining hypothesis space and optimizing it with gradient descent based on the in-context examples"
  - [section 3.1] "when evaluated on a new task, ICL performs similarly to the corresponding model of the pretraining function class optimized by GD given enough in-context examples"
  - [corpus] Weak evidence - no direct citations supporting this specific mechanism
- Break condition: When test functions lie far outside the pretraining function class manifold, making gradient descent optimization ineffective

### Mechanism 2
- Claim: ICL's abstract label classification ability only manifests in ID scenarios
- Mechanism: ICL performs classification by retrieving similar examples from context using an implicit similarity metric learned during pretraining, not by truly generalizing to new input-output mappings
- Core assumption: The model can learn a similarity metric during pretraining that generalizes to OOD labels when the underlying function is ID
- Evidence anchors:
  - [abstract] "ICL's ability to learn unseen abstract labels in context only manifests in scenarios without distributional shifts"
  - [section 4.2] "ICL can only solve classification with unseen labels over ID test function classes. Once the underlying task function is OOD, ICL fails even if the target label appears in the context"
  - [corpus] Moderate evidence - related work on retrieval-based ICL supports this mechanism
- Break condition: When the underlying prediction rule (function) is OOD, even if labels are retrievable from context

### Mechanism 3
- Claim: ICL exhibits "low-test-error preference" when pretrained on multiple tasks
- Mechanism: ICL implements the pretraining function that yields lowest test error on the downstream context, selecting based on both input distribution similarity and function performance
- Core assumption: The model maintains posterior distributions over pretraining functions that can be updated based on test context
- Evidence anchors:
  - [abstract] "ICL tends to implement the pretraining function that yields low test error in the testing context"
  - [section 5.1] "the ICL prediction prefers to implement the pretraining function with lower test error in the downstream context"
  - [corpus] Weak evidence - algorithm selection literature provides some support but lacks direct mechanism evidence
- Break condition: When no pretraining function performs well on the test context, or when input distributions are completely dissimilar

## Foundational Learning

- Concept: Bayesian inference over latent task concepts
  - Why needed here: Understanding how ICL infers task parameters from context requires Bayesian framework
  - Quick check question: What determines the posterior distribution over task concepts given test context?

- Concept: Function approximation and optimization
  - Why needed here: ICL must approximate functions within its hypothesis space and optimize parameters
  - Quick check question: How does gradient descent optimization manifest within the forward pass of ICL?

- Concept: Similarity metrics and retrieval
  - Why needed here: ICL's ability to classify abstract labels relies on retrieving similar examples from context
  - Quick check question: What implicit similarity metric does ICL use to match query examples to context examples?

## Architecture Onboarding

- Component map: Pretraining function classes → ICL inference mechanism → Test context processing → Posterior function selection → Parameter optimization
- Critical path: Pretraining → Context encoding → Function class selection → Parameter optimization → Prediction
- Design tradeoffs: Model capacity vs. generalization ability; pretraining diversity vs. overfitting; function class selection vs. flexibility
- Failure signatures: Double descent curves; performance collapse on OOD tasks; inability to truly learn novel input-output mappings
- First 3 experiments:
  1. Train GPT-2 on linear regression, evaluate on quadratic regression with varying context lengths
  2. Train on mixed function classes, measure algorithm selection behavior on OOD tasks
  3. Test abstract label classification ability on ID vs. OOD underlying functions

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does increasing the diversity of pretraining tasks beyond the observed effects in the paper lead to improved OOD generalization for ICL?
- Basis in paper: [explicit] The authors mention that "increasing the diversity of training tasks, while keeping them in-distribution, can activate the OOD generalization ability" but the results "suggest a negative conclusion."
- Why unresolved: The paper only tested a limited range of task diversities and did not explore extreme cases or fundamentally different task types.
- What evidence would resolve it: Experiments showing the effect of pretraining on an extremely diverse set of tasks (e.g., including tasks from entirely different domains) on ICL's OOD performance.

### Open Question 2
- Question: What specific architectural or training modifications to Transformers could enable true OOD generalization in ICL?
- Basis in paper: [inferred] The authors conclude that "Transformers may struggle to learn OOD task functions through ICL" and suggest that ICL "tends to implement functions encountered during pretraining."
- Why unresolved: The paper focuses on the limitations of standard Transformers but does not explore potential modifications to overcome these limitations.
- What evidence would resolve it: Experiments demonstrating that specific architectural changes (e.g., attention mechanisms, activation functions) or training procedures (e.g., curriculum learning, meta-learning) enable Transformers to generalize to OOD tasks through ICL.

### Open Question 3
- Question: How does the Bayesian framework for ICL apply to the algorithm selection mechanism observed in the paper?
- Basis in paper: [explicit] The authors mention that "the Bayesian framework shows promise as a potential lens for interpreting our empirical findings" and provide an intuitive interpretation of their findings from a Bayesian perspective.
- Why unresolved: The paper provides an intuitive interpretation but does not develop a formal Bayesian model that explains the algorithm selection mechanism.
- What evidence would resolve it: A formal Bayesian model that predicts ICL's algorithm selection behavior based on the pretraining distribution and the test context.

## Limitations
- The synthetic nature of experimental setup limits generalizability to real-world NLP tasks
- OOD functions may not be truly out-of-distribution from the model's perspective due to potential overlap with pretraining data
- Computational efficiency claims are based on theoretical analysis rather than empirical runtime measurements

## Confidence

**High Confidence:** The empirical observations showing ICL performance resembling gradient descent optimization within pretraining function classes (Mechanism 1).

**Medium Confidence:** The claim that ICL's abstract label classification ability only manifests in ID scenarios (Mechanism 2).

**Medium Confidence:** The "low-test-error preference" mechanism when pretrained on multiple tasks (Mechanism 3).

## Next Checks

1. Conduct empirical measurements comparing actual inference time and computational resources required for ICL versus gradient descent optimization on the same OOD tasks to validate the computational efficiency claims.

2. Test the proposed mechanisms on non-mathematical tasks (e.g., natural language understanding or reasoning tasks) to assess whether the findings generalize beyond synthetic function regression.

3. Investigate how the observed mechanisms change with model scale (small vs. large transformers) and pretraining diversity to identify the conditions under which each mechanism dominates ICL behavior.