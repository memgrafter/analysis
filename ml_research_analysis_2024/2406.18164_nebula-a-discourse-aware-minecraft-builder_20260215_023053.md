---
ver: rpa2
title: 'Nebula: A discourse aware Minecraft Builder'
arxiv_id: '2406.18164'
source_url: https://arxiv.org/abs/2406.18164
tags:
- nebula
- action
- builder
- instructions
- instruction
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the challenge of predicting action sequences
  from natural language instructions in a collaborative Minecraft environment. The
  core idea is to use a large language model (LLM) trained on the Minecraft Dialogue
  Corpus to predict actions based on the entire conversation history and world state,
  rather than just the most recent instruction.
---

# Nebula: A discourse aware Minecraft Builder

## Quick Facts
- arXiv ID: 2406.18164
- Source URL: https://arxiv.org/abs/2406.18164
- Authors: Akshay Chaturvedi; Kate Thompson; Nicholas Asher
- Reference count: 7
- Primary result: NeBuLa model doubles net-action F1 score by using full conversation context and narrative arcs for action prediction

## Executive Summary
This paper addresses the challenge of predicting action sequences from natural language instructions in collaborative Minecraft environments. The authors propose using a large language model (NeBuLa) trained on the Minecraft Dialogue Corpus to predict actions based on entire conversation history and world state, rather than just recent instructions. By incorporating discourse structure through narrative arcs, the model achieves significantly improved performance over baselines, doubling the net-action F1 score. The paper also introduces synthetic datasets with appropriate metrics to better evaluate underspecified instructions and location descriptions.

## Method Summary
The method involves fine-tuning Llama-3-8B (NeBuLa) on the Minecraft Dialogue Corpus using QLoRA, with evaluation on both the original corpus and narrative arcs extracted by a discourse parser. The model uses full conversation context and world state to predict action sequences. Additionally, the authors create synthetic datasets for shapes and locations with binary correctness metrics, and fine-tune NeBuLa on these subsets to address underspecified instructions. The approach combines discourse-aware context selection with synthetic data generation to improve action prediction accuracy.

## Key Results
- NeBuLa model doubles net-action F1 score compared to baseline models using only recent instructions
- Narrative arcs in discourse structure are shown to be necessary and sufficient for accurate action prediction
- High accuracy achieved on synthetic datasets for shape construction and location understanding using binary evaluation metrics

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Using the entire conversation history and world state as context improves LLM-based action prediction over using only the most recent instruction.
- Mechanism: The LLM integrates discourse structure, prior actions, and spatial information to infer underspecified instructions more accurately.
- Core assumption: All prior context is relevant and improves prediction; no information overload or noise from irrelevant context.
- Evidence anchors:
  - [abstract]: "improve the 'language to action' component of such interactions"
  - [section 3]: "We used Llama-2-7B, Llama-2-13B and Llama-3-8B models to take as context all the conversation and action sequences up to action sequence an to predict an"
  - [corpus]: Weak; no explicit corpus evidence, only model results.
- Break condition: If context includes irrelevant or contradictory information that confuses the model rather than helping it.

### Mechanism 2
- Claim: Narrative arcs in the discourse structure are both necessary and sufficient for accurate action prediction.
- Mechanism: By isolating portions of the conversation bounded by Narration relations, the model focuses on coherent instructional segments and avoids noise from unrelated dialogue.
- Core assumption: Narrative arcs capture all necessary information for predicting the next action sequence.
- Evidence anchors:
  - [abstract]: "particular discursive components of the linguistic and nonlinguistic context are necessary and sufficient"
  - [section 4]: "We made a first approximation of these interleaved processes by determining necessary and sufficient situated, conversational conditions"
  - [corpus]: Weak; narrative arc definition relies on discourse parser output, not corpus statistics.
- Break condition: If narrative arc boundaries are incorrectly identified by the parser, leading to incomplete or noisy context.

### Mechanism 3
- Claim: Synthetic datasets with explicit evaluation metrics better capture the semantics of underspecified instructions than the original net-action F1 metric.
- Mechanism: By defining correct shapes and locations with binary functions and allowing multiple valid instantiations, the metric aligns with human interpretation of vague language.
- Core assumption: Multiple correct instantiations exist and are equally valid; synthetic data covers the relevant instruction space.
- Evidence anchors:
  - [abstract]: "We address this problem in two ways: first by further finetuning NeBuLa on a synthetic dataset"
  - [section 6]: "we test NeBuLa on simple scenarios using a more just metric"
  - [corpus]: Weak; corpus issues described but not quantified in terms of instruction ambiguity.
- Break condition: If synthetic data fails to represent the complexity or distribution of real instructions.

## Foundational Learning

- Concept: Discourse structure and narrative arcs
  - Why needed here: To isolate coherent instructional segments from noisy conversation.
  - Quick check question: What discourse relation marks the beginning and end of a narrative arc in this corpus?
- Concept: Embodied spatial reasoning
  - Why needed here: To interpret location descriptions that are relative and underspecified.
  - Quick check question: How does the model distinguish between "on top of" and "to the side of" in terms of block coordinates?
- Concept: Evaluation metrics for underspecified tasks
  - Why needed here: To fairly assess model performance when multiple correct outputs exist.
  - Quick check question: Why is exact coordinate matching inappropriate for location descriptions like "in a corner"?

## Architecture Onboarding

- Component map: LLM (Llama-3-8B) fine-tuned on Minecraft Dialogue Corpus -> Discourse parser (Thompson et al. 2024) for narrative arc extraction -> Synthetic dataset generator for shapes and locations -> Evaluation module with binary shape/location functions
- Critical path: 1. Parse discourse to extract narrative arc 2. Provide world state and arc context to LLM 3. Generate action sequence prediction 4. Evaluate using synthetic metric or net-action F1
- Design tradeoffs:
  - Full context vs. narrative arc: more information vs. cleaner signal
  - Exact vs. relaxed evaluation: stricter correctness vs. realistic assessment
  - Synthetic vs. real data: controlled testing vs. real-world complexity
- Failure signatures:
  - Low precision/recall on net-action F1: context too noisy or narrative arc extraction failing
  - High shape accuracy but low location accuracy: model understands shapes but not spatial relations
  - Poor performance on "not touching" instructions: model conflates contact vs. non-contact
- First 3 experiments:
  1. Compare LLM performance with full context vs. narrative arc only on validation set
  2. Test shape construction accuracy on synthetic dataset before and after finetuning
  3. Evaluate location description understanding using binary functions on synthetic data

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can NeBuLa's understanding of the semantics and pragmatics of indefinite and numerical noun phrases be improved to avoid placing multiple blocks instead of just one?
- Basis in paper: [explicit] The paper mentions that NeBuLa sometimes places multiple blocks instead of just one on a given level-1 structure, indicating a limited understanding of indefinite and numerical noun phrases.
- Why unresolved: The paper does not provide a clear solution to this problem, only suggesting that some of the difficulties NeBuLa had with instructions come from this limited understanding.
- What evidence would resolve it: Testing NeBuLa on a dataset with a variety of indefinite and numerical noun phrases and evaluating its performance in understanding and executing these instructions correctly.

### Open Question 2
- Question: How can the evaluation metric for the Minecraft corpus be revised to better reflect the semantics of location expressions and underspecified instructions?
- Basis in paper: [explicit] The paper discusses the issues with the current net-action F1 metric, which treats vague instructions as completely precise ones and considers one instantiation of an instruction to be the only ground truth.
- Why unresolved: The paper introduces a new evaluation metric but does not provide a detailed analysis of its effectiveness on the entire Minecraft corpus.
- What evidence would resolve it: Applying the new evaluation metric to the entire Minecraft corpus and comparing the results with the original net-action F1 metric to assess improvements in capturing the semantics of location expressions and underspecified instructions.

### Open Question 3
- Question: How can NeBuLa be extended to cover other frequent anaphoric location descriptions in addition to on top of X and to the side of X?
- Basis in paper: [explicit] The paper mentions that locutions like in front of/behind, underneath, hanging off, and next to (X) have underspecified parameters that allow for several correct placements once X has been identified.
- Why unresolved: The paper does not provide a detailed plan for extending NeBuLa to cover these additional anaphoric location descriptions.
- What evidence would resolve it: Evaluating NeBuLa's performance on a dataset containing a variety of anaphoric location descriptions and analyzing its ability to correctly interpret and execute these instructions.

## Limitations

- Context Relevance Assumption: The paper assumes all prior conversation context improves prediction, but doesn't test whether certain types of prior context might degrade performance through information overload or noise.
- Synthetic Dataset Representativeness: While synthetic datasets allow controlled evaluation, they may not capture the full complexity and distribution of real Minecraft dialogues.
- Narrative Arc Extraction Reliability: The effectiveness of narrative arc-based context selection depends entirely on the accuracy of an external discourse parser whose limitations are not fully characterized.

## Confidence

- High Confidence: The architectural approach of using LLM-based action prediction with discourse-aware context is sound and well-implemented.
- Medium Confidence: The claim that narrative arcs are both necessary and sufficient for accurate action prediction is supported by experimental results but relies on an external parser.
- Low Confidence: The assertion that synthetic datasets provide a more appropriate evaluation metric for underspecified instructions lacks validation against human judgments.

## Next Checks

1. **Context Pruning Analysis**: Conduct ablation studies to identify which portions of the conversation history actually contribute to improved prediction accuracy, and which might be safely removed to reduce noise and computational overhead.

2. **Narrative Arc Boundary Verification**: Manually annotate a subset of the Minecraft Dialogue Corpus to verify the accuracy of the Thompson et al. (2024) discourse parser's narrative arc identification, and measure the impact of parser errors on NeBuLa+N performance.

3. **Synthetic-to-Real Transfer Study**: Evaluate whether models trained and tested on synthetic datasets show similar performance gains when deployed on real Minecraft dialogues, particularly for underspecified instructions that are difficult to synthetically generate.