---
ver: rpa2
title: Residual vector quantization for KV cache compression in large language model
arxiv_id: '2410.15704'
source_url: https://arxiv.org/abs/2410.15704
tags:
- quantization
- vector
- codebook
- residual
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proposes using residual vector quantization (RVQ) to
  compress KV caches in large language models (LLMs). Unlike scalar quantization,
  RVQ leverages codebook-based compression to achieve higher fidelity.
---

# Residual vector quantization for KV cache compression in large language model

## Quick Facts
- arXiv ID: 2410.15704
- Source URL: https://arxiv.org/abs/2410.15704
- Authors: Ankur Kumar
- Reference count: 31
- Primary result: RVQ with residual depth 8 achieves 5.5x compression over half precision while recovering most unquantized model performance on standard benchmarks

## Executive Summary
This paper proposes using residual vector quantization (RVQ) to compress KV caches in large language models, achieving 5.5x compression over half precision while maintaining most of the model's performance. Unlike scalar quantization, RVQ leverages codebook-based compression to achieve higher fidelity without additional learnable parameters. The method scales vectors by their standard deviation, groups channels (non-contiguous for keys, contiguous for values), and applies RVQ using K codebooks learned via exponential moving average. Experiments on Llama-3-8B, Mistral-7B, and Gemma-7B show that a residual depth of 8 with 2048 codes per codebook recovers most performance, though GSM8K tasks still see some degradation even with fine-tuning.

## Method Summary
The paper applies residual vector quantization to compress key and value projection outputs in LLM attention layers. The method scales input vectors by their standard deviation, groups channels (non-contiguous for keys, contiguous for values), and applies RVQ using K codebooks with |Ci| codes each. Each vector is encoded as a sum of K nearest codebook entries, reducing memory footprint. Codebooks are learned via exponential moving average with decay 0.99, initialized using k-means clustering on the first batch. The approach requires no additional learnable parameters beyond the pretrained model and can optionally include joint fine-tuning of attention block weights with quantization to further improve performance, particularly on challenging tasks like GSM8K.

## Key Results
- RVQ with residual depth 8 recovers most unquantized model performance across standard benchmarks
- Non-contiguous channel grouping for keys outperforms contiguous grouping in compression quality
- Joint fine-tuning with quantization improves performance on GSM8K tasks
- Achieves 5.5x compression over half precision without significant accuracy loss
- Simple implementation using existing Triton kernels without additional learnable parameters

## Why This Works (Mechanism)

### Mechanism 1
- Claim: RVQ compresses KV cache by replacing high-precision vectors with combinations of discrete codebook entries, preserving most performance
- Mechanism: Scales input vectors by standard deviation, groups channels, applies residual quantization using K codebooks, each vector encoded as sum of K nearest entries
- Core assumption: Performance preserved if quantization error minimized through sufficient codebook depth and appropriate grouping
- Evidence anchors:
  - [abstract] "We find that a residual depth of 8 recovers most of the performance of the unquantized model"
  - [section] "We find that a residual depth of 8 recovers most of the performance of the unquantized model"
  - [corpus] Weak evidence: No direct RVQ mention in neighbors; related works use scalar quantization or other VQ methods
- Break condition: Performance degrades significantly if K too small (K=4 leads to severe degradation on MMLU and GSM8k)

### Mechanism 2
- Claim: Grouping non-contiguous channels yields better compression performance than contiguous grouping for key embeddings
- Mechanism: Non-contiguous grouping (d/32 channels apart) captures more diverse patterns across channel dimension, reducing quantization error
- Core assumption: Key embeddings benefit from distributed channel grouping rather than local patterns due to attention mechanism nature
- Evidence anchors:
  - [section] "We find that grouping non-contiguous channels together works better than grouping contiguous channels for compressing key matrix"
  - [section] "By default, channels in key embeddings are grouped non-contiguously whereas value channels are grouped contiguously"
  - [corpus] No direct evidence; this is a unique finding of the paper
- Break condition: Performance on key-sensitive tasks drops significantly with poor channel grouping choices

### Mechanism 3
- Claim: Fine-tuning LLM weights alongside quantization further improves performance, especially on GSM8k
- Mechanism: Joint fine-tuning adjusts model parameters to better accommodate quantized representations, reducing gap between quantized and unquantized performance
- Core assumption: Quantization introduces domain shift correctable through fine-tuning even with small learning rate (1e-5)
- Evidence anchors:
  - [section] "We see noticeable improvement when we also finetune the weights in all attention blocks along with the quantization"
  - [section] "We see noticeable improvement when we also finetune the weights in all attention blocks along with the quantization"
  - [corpus] No direct evidence; fine-tuning with quantization not discussed in neighbor papers
- Break condition: Fine-tuning may overfit or not converge if learning rate too high or dataset insufficient

## Foundational Learning

- Concept: Vector quantization (VQ) basics
  - Why needed here: Understanding how VQ maps continuous vectors to discrete codebook entries is essential for grasping RVQ's compression mechanism
  - Quick check question: What is the role of the commitment loss in standard VQ, and why is it set to zero in this work?

- Concept: Residual quantization
  - Why needed here: RVQ uses multiple codebooks in sequence, with each step refining the quantization; knowing this helps understand why K=8 is effective
  - Quick check question: How does residual quantization differ from standard VQ in terms of codebook usage and reconstruction?

- Concept: Channel grouping strategies
  - Why needed here: The paper shows non-contiguous grouping outperforms contiguous grouping for keys; understanding why requires knowledge of channel interactions in attention
  - Quick check question: Why might grouping non-contiguous channels capture more diverse patterns than contiguous ones in key embeddings?

## Architecture Onboarding

- Component map: Input vectors -> Scale by standard deviation -> Group channels (non-contiguous for keys, contiguous for values) -> RVQ with K codebooks -> Sum of K nearest codes -> Scale back -> Attention layer
- Critical path:
  1. Scale input vector by its standard deviation
  2. Group channels (non-contiguous for keys, contiguous for values)
  3. For each group, apply RVQ: find K nearest codes across K codebooks
  4. Sum codes, scale back, feed to attention
  5. (Optional) Fine-tune model with quantization
- Design tradeoffs:
  - Higher K improves accuracy but increases computational cost and codebook size
  - Non-contiguous grouping improves key quantization but may complicate codebook learning
  - Joint fine-tuning helps performance but adds training overhead and risk of overfitting
- Failure signatures:
  - Severe performance drop on MMLU/GSM8k: likely K too small or poor channel grouping
  - No improvement with fine-tuning: learning rate too high/low or insufficient data
  - High memory usage: codebook size |Ci| too large or K too high
- First 3 experiments:
  1. Quantize only keys with contiguous grouping, K=4, compare to unquantized baseline
  2. Quantize only keys with non-contiguous grouping, K=8, compare to K=4 result
  3. Quantize both keys and values with non-contiguous grouping for keys, K=8, with and without fine-tuning

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does grouping non-contiguous channels provide consistent benefits across different LLM architectures beyond Llama-3, Mistral, and Gemma?
- Basis in paper: [explicit] The paper found that grouping non-contiguous channels worked better than contiguous channels for compressing key matrices in their experiments with Llama-3-8B, Mistral-7B, and Gemma-7B
- Why unresolved: The experiments were limited to three specific model families. Different architectures may have different internal representations that could respond differently to channel grouping strategies
- What evidence would resolve it: Testing the non-contiguous channel grouping approach on a diverse set of LLM architectures (different sizes, training methods, and architectural variants) to verify if the benefit generalizes

### Open Question 2
- Question: What is the optimal residual depth (number of codebooks K) that balances compression efficiency with computational overhead for different model sizes and inference scenarios?
- Basis in paper: [explicit] The paper found that K=8 recovered most performance but noted that 8 codebooks may introduce computational efficiency challenges, especially in compute-bound scenarios
- Why unresolved: The paper only tested K=4, 6, and 8 on one model size (Llama-3-8B). Different model sizes, batch sizes, and inference contexts (pre-fill vs. decoding) may have different optimal values
- What evidence would resolve it: Systematic experiments varying K across different model sizes, batch sizes, and inference phases to find the optimal tradeoff point for each scenario

### Open Question 3
- Question: How does the performance gap on GSM8K tasks compare when using RVQ versus other KV cache compression methods?
- Basis in paper: [explicit] The paper notes that GSM8K shows significant degradation even with K=8 and mentions that existing quantization works often avoid testing on GSM8K because it's a "hard generation task"
- Why unresolved: The paper only compares RVQ to unquantized baselines and doesn't benchmark against other compression methods on GSM8K specifically
- What evidence would resolve it: Direct comparison of RVQ against other KV cache compression methods (like KIVI, KVQuant) on GSM8K to determine if the performance gap is inherent to quantization or specific to RVQ

## Limitations
- GSM8K tasks show significant performance degradation even with K=8 and fine-tuning, indicating fundamental limitations in representing complex mathematical reasoning through quantized embeddings
- The optimal channel grouping strategy (non-contiguous) may not generalize across different LLM architectures and model sizes
- 8 codebooks may introduce computational efficiency challenges in compute-bound inference scenarios, though this tradeoff isn't fully characterized

## Confidence
- High confidence: The mechanism of residual vector quantization with codebook-based compression and standard deviation scaling
- Medium confidence: The superiority of non-contiguous channel grouping for key embeddings - supported by ablation studies but lacks theoretical explanation
- Medium confidence: The benefit of joint fine-tuning with quantization - improvement is shown but the magnitude may vary with different learning rates or datasets

## Next Checks
1. **Cross-architecture generalization test**: Apply the same non-contiguous grouping strategy (d/32 apart) to a larger model (e.g., Llama-3-70B) and a smaller model (e.g., Phi-3-mini) to verify if the pattern holds across scales, measuring performance drop relative to unquantized baselines

2. **Codebook capacity sensitivity analysis**: Systematically vary the number of codes per codebook (e.g., 512, 1024, 4096) while keeping K=8 fixed to determine the minimal codebook size that achieves GSM8K performance within 5% of the unquantized model

3. **Alternative grouping strategy comparison**: Implement and test a learned channel grouping approach where the grouping pattern is optimized during fine-tuning, comparing against the fixed non-contiguous strategy to determine if dynamic grouping yields better compression-performance tradeoffs