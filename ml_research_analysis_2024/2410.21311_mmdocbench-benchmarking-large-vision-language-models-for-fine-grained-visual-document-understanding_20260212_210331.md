---
ver: rpa2
title: 'MMDocBench: Benchmarking Large Vision-Language Models for Fine-Grained Visual
  Document Understanding'
arxiv_id: '2410.21311'
source_url: https://arxiv.org/abs/2410.21311
tags:
- document
- visual
- lvlms
- fine-grained
- reasoning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces MMDocBench, a comprehensive benchmark designed
  to evaluate Large Vision-Language Models' (LVLMs) fine-grained visual understanding
  capabilities on document images. The benchmark consists of 15 main tasks and 48
  sub-tasks, covering a wide range of document types including research papers, receipts,
  financial reports, and charts.
---

# MMDocBench: Benchmarking Large Vision-Language Models for Fine-Grained Visual Document Understanding

## Quick Facts
- arXiv ID: 2410.21311
- Source URL: https://arxiv.org/abs/2410.21311
- Reference count: 31
- Large Vision-Language Models show significant challenges in fine-grained visual document understanding tasks, with best models achieving only 11.44% IOU for region prediction

## Executive Summary
This paper introduces MMDocBench, a comprehensive benchmark designed to evaluate Large Vision-Language Models' (LVLMs) fine-grained visual understanding capabilities on document images. The benchmark consists of 15 main tasks and 48 sub-tasks, covering diverse document types including research papers, receipts, financial reports, and charts. MMDocBench contains 4,338 QA pairs with 11,353 annotated supporting regions, enabling both answer prediction and region grounding evaluation. The authors conduct extensive experiments with 16 LVLMs, revealing significant challenges in region prediction tasks and large performance gaps between open-source and closed-source models.

## Method Summary
The benchmark uses zero-shot prompting with task-specific instructions to evaluate LVLMs on document images without model training. The evaluation framework includes both answer prediction (measured by Exact Match and F1-score) and region grounding (measured by Intersection over Union). The benchmark provides 2,400 document images, 4,338 QA pairs with 11,353 annotated supporting regions, and comprehensive task instructions for all 15 main tasks and 48 sub-tasks.

## Key Results
- Region prediction remains a significant challenge, with the best model achieving only 11.44% IOU
- Large performance gap exists between open-source and closed-source models in answer prediction
- Region prediction performance is more comparable across model types, with closed-source models showing less advantage
- Text localization and document forgery detection tasks are particularly challenging for current LVLMs

## Why This Works (Mechanism)

### Mechanism 1
- Claim: MMDocBench provides fine-grained visual grounding evaluation by requiring LVLMs to output supporting regions (bounding boxes) for each answer.
- Mechanism: The benchmark includes 11,353 annotated supporting regions across 4,338 QA pairs, enabling evaluation of whether models correctly ground their predictions to relevant image regions rather than just answering textually.
- Core assumption: Visual grounding capability is essential for true fine-grained visual understanding and can be measured through bounding box prediction accuracy.
- Evidence anchors: [abstract] "MMDocBench contains 4,338 QA pairs with 11,353 annotated supporting regions, enabling both answer prediction and region grounding evaluation"

### Mechanism 2
- Claim: Document images provide multi-granularity information that tests fine-grained visual perception across different levels (text, tables, charts, figures).
- Mechanism: The benchmark uses diverse document types including research papers, receipts, financial reports, and charts, each containing information at different granularities from character-level text to complex table structures.
- Core assumption: Different document elements require different levels of visual discrimination, making documents ideal for testing fine-grained capabilities.
- Evidence anchors: [abstract] "document images offer certain advantages as testing data to evaluate LVLMs' fine-grained visual understanding capabilities"

### Mechanism 3
- Claim: The comprehensive task taxonomy (15 main tasks, 48 sub-tasks) provides discriminative evaluation of specific LVLM capabilities.
- Mechanism: By separating tasks into fine-grained visual perception (9 tasks) and fine-grained visual reasoning (6 tasks), the benchmark can identify specific weaknesses rather than just overall performance.
- Core assumption: Different types of visual understanding tasks require distinct capabilities, and separating them reveals more specific model strengths and weaknesses.
- Evidence anchors: [abstract] "MMDocBench defines 15 main tasks with 4,338 QA pairs and 11,353 supporting regions"

## Foundational Learning

- Concept: Intersection over Union (IOU) metric
  - Why needed here: IOU measures the overlap between predicted and ground truth bounding boxes, providing a quantitative assessment of region prediction accuracy
  - Quick check question: If a predicted box is [100, 100, 200, 200] and ground truth is [150, 150, 250, 250], what is the IOU? (Answer: 1/7 or ~14.3%)

- Concept: Exact Match (EM) vs F1-score for text evaluation
  - Why needed here: Different text evaluation metrics capture different aspects - EM requires perfect character matching while F1-score measures word-level overlap, important for assessing answer quality
  - Quick check question: Why might a model get 0% EM but high F1-score on the same answer? (Answer: Different word order or minor spelling variations)

- Concept: Zero-shot prompting
  - Why needed here: The evaluation uses zero-shot prompting to assess model capabilities without task-specific training, providing a fair baseline comparison
  - Quick check question: What's the key difference between zero-shot and few-shot prompting? (Answer: Few-shot includes example demonstrations in the prompt)

## Architecture Onboarding

- Component map: Visual encoder → Visual adapter → LLM backbone → Output layer. For document understanding, additional components may include OCR modules and region prediction heads.
- Critical path: Image → Visual encoding → Feature extraction → Language modeling → Answer generation + Region prediction. The bottleneck is often in visual encoding of fine-grained details.
- Design tradeoffs: High-resolution input vs computational cost, detailed region prediction vs answer accuracy, general vs task-specific training data.
- Failure signatures: Poor IOU scores despite good EM/F1 indicate grounding issues; good region prediction but wrong answers suggest comprehension problems; low performance across all metrics indicates fundamental limitations.
- First 3 experiments:
  1. Evaluate a baseline model on a subset of MMDocBench tasks to identify performance patterns
  2. Test region prediction capability on simple document images with single answer regions
  3. Compare performance across different document types to identify which are most challenging

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can Large Vision-Language Models be improved to achieve better fine-grained visual understanding, particularly in tasks like document forgery detection and text localization?
- Basis in paper: [explicit] The paper highlights significant challenges in these tasks, with models achieving only 20.6% in exact match for forgery detection and struggling with text localization.
- Why unresolved: The paper demonstrates the current limitations but does not propose specific solutions for improving these capabilities.
- What evidence would resolve it: Comparative studies of new model architectures or training methodologies that specifically target these fine-grained tasks, showing improved performance metrics.

### Open Question 2
- Question: What is the impact of different visual encoders on the fine-grained visual understanding capabilities of Large Vision-Language Models?
- Basis in paper: [inferred] The paper notes that visual encoders adopted in models like GPT-4o and GPT-4v may not effectively support fine-grained visual understanding.
- Why unresolved: The paper suggests this as a possible reason for performance issues but does not explore alternative encoders or their effects.
- What evidence would resolve it: Empirical studies comparing the performance of models using different visual encoders on the MMDocBench tasks, highlighting which encoders improve fine-grained understanding.

### Open Question 3
- Question: How does the granularity of supporting regions affect the performance of Large Vision-Language Models in fine-grained visual understanding tasks?
- Basis in paper: [inferred] The paper provides annotations of supporting regions but does not analyze how the granularity of these regions impacts model performance.
- Why unresolved: The analysis focuses on model performance across tasks and document types but does not delve into the granularity aspect of supporting regions.
- What evidence would resolve it: Detailed analysis of model performance variations with different granularity levels of supporting regions, possibly leading to insights on optimal granularity for specific tasks.

## Limitations
- The benchmark's comprehensive nature (15 tasks, 48 sub-tasks) may create evaluation complexity that makes it difficult to isolate specific model capabilities.
- Zero-shot prompting approach may not fully leverage model capabilities that could be achieved with task-specific fine-tuning or few-shot prompting.
- Evaluation of region grounding through IOU may be sensitive to annotation quality and could be influenced by minor coordinate variations.

## Confidence

- **High Confidence**: The benchmark design and task taxonomy are well-structured for comprehensive evaluation (supported by detailed methodology and clear task definitions)
- **Medium Confidence**: The claim that MMDocBench reveals significant challenges in region prediction tasks (supported by IOU scores but limited by single evaluation metric)
- **Medium Confidence**: The observation of performance gaps between open-source and closed-source models (supported by experimental results but may be influenced by model selection and prompt engineering)

## Next Checks

1. Conduct ablation studies by evaluating models on individual task categories (fine-grained visual perception vs. fine-grained visual reasoning) to verify the discriminative value of the task taxonomy
2. Test alternative region evaluation metrics (e.g., weighted IOU, point-based evaluation) to confirm that poor performance isn't solely due to annotation sensitivity
3. Perform few-shot prompting experiments using the same benchmark to determine if performance improvements suggest the zero-shot results underestimate true model capabilities