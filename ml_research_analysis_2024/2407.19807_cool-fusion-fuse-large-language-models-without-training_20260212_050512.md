---
ver: rpa2
title: 'Cool-Fusion: Fuse Large Language Models without Training'
arxiv_id: '2407.19807'
source_url: https://arxiv.org/abs/2407.19807
tags:
- text
- llms
- source
- token
- cool-fusion
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Cool-Fusion is a training-free method to fuse heterogeneous large
  language models (LLMs) without requiring aligned vocabularies or architectural matching.
  It works by iteratively generating short text segments, having all source LLMs evaluate
  their perplexities, and selecting the segment with the lowest average perplexity.
---

# Cool-Fusion: Fuse Large Language Models without Training

## Quick Facts
- arXiv ID: 2407.19807
- Source URL: https://arxiv.org/abs/2407.19807
- Reference count: 7
- Primary result: A training-free method that fuses heterogeneous LLMs without vocabulary alignment, improving GSM8K accuracy by 17.4% over the best individual source model.

## Executive Summary
Cool-Fusion is a training-free method that enables fusion of heterogeneous large language models (LLMs) without requiring aligned vocabularies or architectural matching. The approach works by iteratively generating short text segments, having all source LLMs evaluate their perplexities, and selecting the segment with the lowest average perplexity. This process avoids the high computational costs of fine-tuning or vocabulary alignment. Experiments show that Cool-Fusion significantly improves accuracy on benchmark datasets, outperforming or matching state-of-the-art fusion methods that require training.

## Method Summary
Cool-Fusion fuses multiple LLMs by iteratively generating short text segments, computing perplexities for each segment using all source LLMs, and selecting the segment with the lowest average perplexity. The method defines "aligned text segments" as the shortest text decodable by all tokenizers to ensure fair perplexity comparison. After segment selection, an optional sentence-level reranking step compares full continuations from each LLM. The process repeats until completion, building the output incrementally while maintaining all LLMs' states. This approach works across diverse domains and significantly outperforms individual LLMs on benchmark datasets.

## Key Results
- GSM8K accuracy improved by 17.4% over the best individual source LLM
- CoQA F1 score increased by 6.4% compared to best individual model
- Outperforms or matches state-of-the-art fusion methods requiring training

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Iterative text segment reranking using average perplexity allows heterogeneous LLMs to cooperate without vocabulary alignment.
- Mechanism: At each iteration, all source LLMs independently generate a short text segment, compute perplexities for every segment, and select the segment with the lowest average perplexity. This selected segment is appended to the context and the loop continues.
- Core assumption: Lower average perplexity corresponds to higher quality or more likely continuation text.
- Evidence anchors:
  - [abstract] "iteratively generating short text segments, having all source LLMs evaluate their perplexities, and selecting the segment with the lowest average perplexity"
  - [section] "the text segment with the smallest averaged perplexity is selected as the jointly predicted text segment"
  - [corpus] Weak or missing. Corpus lacks direct evidence for this mechanism.
- Break condition: If average perplexity does not correlate with quality (e.g., in low-resource languages or highly specialized domains).

### Mechanism 2
- Claim: Aligned text segments reduce bias from differing tokenizer segment lengths.
- Mechanism: Define an "aligned text segment" as the shortest text that is decodable by all tokenizers. This ensures fair perplexity comparison by preventing longer segments from being unfairly penalized.
- Core assumption: Perplexity is higher at the start of words, so shorter, aligned segments yield fairer comparisons.
- Evidence anchors:
  - [abstract] "we ensemble LLMs on text level, allowing them to rerank the generated texts by each other with different granularities"
  - [section] "we define a new aligned text segment for each source LLM as the shortest text segment that is generated by the LLM and is decodable by the tokenizers of all source LLMs"
  - [corpus] Weak or missing. Corpus lacks direct evidence for this mechanism.
- Break condition: If tokenizers cannot find common decodable boundaries, making aligned segments impractical.

### Mechanism 3
- Claim: Combining fine-grained segment reranking with coarse-grained sentence-level reranking improves overall accuracy.
- Mechanism: After selecting the best segment via perplexity, also have each LLM predict a full continuation. Rerank all continuations (fine-grained joint prediction + individual predictions) using average perplexities.
- Core assumption: Sentence-level reranking can correct suboptimal fine-grained selections.
- Evidence anchors:
  - [abstract] "we can simultaneously employ an iterative fine-grained text segment selection and a coarse-grained sentence level reranking at the same time"
  - [section] "Rerank turns out to be very effective and it obtains a 12.5% increment over LLaMA-3"
  - [corpus] Weak or missing. Corpus lacks direct evidence for this mechanism.
- Break condition: If sentence-level continuations are too divergent or if computational cost outweighs benefit.

## Foundational Learning

- Concept: Perplexity as a measure of language model uncertainty.
  - Why needed here: Perplexity is the core metric used to select the best text segment across LLMs.
  - Quick check question: What does a lower perplexity value indicate about a language model's prediction?

- Concept: Tokenizer vocabularies and their impact on model interoperability.
  - Why needed here: Different LLMs use different tokenizers, which is the main challenge Cool-Fusion addresses.
  - Quick check question: Why is it problematic to directly ensemble LLMs with different token vocabularies?

- Concept: Greedy decoding and beam search in text generation.
  - Why needed here: Cool-Fusion uses greedy decoding for segment generation but mentions beam search as a possible improvement.
  - Quick check question: How does greedy decoding differ from beam search in terms of output diversity?

## Architecture Onboarding

- Component map: TextGen -> Tokenizer -> Perplexity Evaluator -> Reranker -> Context Updater -> repeat

- Critical path: TextGen → Tokenizer → Perplexity Evaluator → Reranker → Context Updater → repeat.

- Design tradeoffs:
  - Shorter segments → finer-grained selection but more iterations and communication overhead.
  - Aligned segments → fairer perplexity comparison but may limit segment flexibility.
  - Reranking continuations → potential accuracy gain but extra computation.

- Failure signatures:
  - Slow inference: Frequent tokenizer calls and inter-LLM communication.
  - Poor accuracy: Low correlation between average perplexity and actual quality.
  - Memory issues: Storing key-value caches for multiple LLMs.

- First 3 experiments:
  1. Run Cool-Fusion with two LLMs on a simple text completion task; verify that output differs from individual LLMs.
  2. Measure perplexity correlation with human judgment on a small validation set.
  3. Compare accuracy with and without aligned segments on a multilingual dataset.

## Open Questions the Paper Calls Out
None

## Limitations
- Lack of direct experimental evidence validating that lower average perplexity consistently correlates with higher quality text outputs
- Underspecified implementation details for handling aligned text segments across different token vocabularies
- Uncharacterized computational overhead of maintaining multiple LLMs' key-value caches and frequent inter-model communication

## Confidence

**High Confidence**: The basic mechanism of using average perplexity for segment selection is well-defined and the experimental results showing accuracy improvements over individual LLMs are clearly presented.

**Medium Confidence**: The claim that aligned text segments improve fairness in perplexity comparison has theoretical justification but lacks direct experimental validation comparing aligned vs. non-aligned approaches.

**Low Confidence**: The assertion that sentence-level reranking provides meaningful accuracy improvements beyond fine-grained segment selection is supported only by a single percentage improvement figure without ablation studies or error analysis.

## Next Checks

1. **Perplexity-Quality Correlation Validation**: Conduct a controlled experiment where human evaluators rate text quality on a sample of outputs from Cool-Fusion, then compute the correlation between average perplexity scores and human quality ratings. This would directly validate the core assumption underlying the method.

2. **Aligned Segment Boundary Analysis**: Implement both aligned and non-aligned versions of Cool-Fusion and systematically test them on multilingual datasets where token vocabularies differ significantly. Measure both accuracy differences and the frequency of failed segment alignments to quantify the practical impact of this mechanism.

3. **Computational Overhead Characterization**: Profile the inference time and memory usage of Cool-Fusion compared to individual LLMs across different numbers of source models (2, 4, and 8). Measure the breakdown of time spent in generation, perplexity computation, and communication to identify scalability bottlenecks.