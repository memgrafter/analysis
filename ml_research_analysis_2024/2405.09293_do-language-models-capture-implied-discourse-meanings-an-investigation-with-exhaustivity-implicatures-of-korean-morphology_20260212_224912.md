---
ver: rpa2
title: Do language models capture implied discourse meanings? An investigation with
  exhaustivity implicatures of Korean morphology
arxiv_id: '2405.09293'
source_url: https://arxiv.org/abs/2405.09293
tags:
- marker
- discourse
- exhaustivity
- object
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'The study investigates whether large language models (LLMs) can
  capture implied discourse meanings, specifically exhaustivity implicatures in Korean
  morphology. The authors examine three post-positional markers in Korean: lul (accusative
  case), nun (contrastive topic), and null-marking, which can evoke exhaustivity implicatures
  in certain discourse contexts.'
---

# Do language models capture implied discourse meanings? An investigation with exhaustivity implicatures of Korean morphology

## Quick Facts
- arXiv ID: 2405.09293
- Source URL: https://arxiv.org/abs/2405.09293
- Reference count: 13
- Key outcome: LLMs show varying ability to capture exhaustivity implicatures in Korean morphology, with larger models and those with human feedback performing more human-like

## Executive Summary
This study investigates whether large language models can capture implied discourse meanings through an examination of exhaustivity implicatures in Korean morphology. The authors focus on three post-positional markers (lul, nun, and null-marking) and their ability to evoke exhaustivity implicatures in different discourse contexts. The research reveals that while models show some sensitivity to these pragmatic meanings, encoding dual grammatical and discourse functions remains challenging for distributional semantics, though larger models with human feedback demonstrate more human-like patterns.

## Method Summary
The study compares LLMs' (KoGPT-2, KoGPT-Trinity, Polyglot-Ko models, GPT-3, and ChatGPT) and humans' sensitivity to exhaustivity implicatures indicated by Korean object markers lul and nun. Using surprisal measurements, elicited ratings, and probability measurements, the researchers tested models' ability to associate discourse meanings with different object markings. Two main experiments were conducted: one testing discourse meanings in processing (measuring surprisal and ratings) and another testing discourse meanings in production (forced-choice tasks).

## Key Results
- GPT-3 showed sensitivity to the non-cancelable exhaustivity implicature of the nun marker but not to the lul marker's cancelable exhaustivity implicature
- Polyglot-Ko-12B demonstrated some ability to associate the nun marker with exhaustivity status in production tasks, but not the lul marker
- ChatGPT and human participants showed similar patterns in rating discourse continuation appropriateness, but ChatGPT struggled in forced-choice generation tasks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Larger models with human feedback (RLHF) can better capture discourse meanings of grammatical markers because they learn nuanced pragmatic patterns beyond pure distributional statistics.
- Mechanism: RLHF training provides explicit signals about the appropriateness of responses in discourse contexts, allowing the model to associate grammatical markers like lul with their discourse-level implicatures (e.g., exhaustivity).
- Core assumption: Human feedback can effectively encode pragmatic knowledge that is not easily recoverable from raw distributional data alone.
- Evidence anchors: [abstract] "Overall, the results suggest that encoding dual meanings (grammatical and discourse) in multiple domains of language is challenging for distributional semantics, but larger models with human feedback show more human-like patterns."

### Mechanism 2
- Claim: Models learn to associate the nun marker with exhaustivity implicatures because it is primarily used as a discourse marker, making its pragmatic function more salient in distributional patterns.
- Mechanism: The nun marker's primary role as a discourse marker means that its usage patterns in the training data are more likely to co-occur with contexts that require exhaustivity implicatures, making it easier for models to learn this association.
- Core assumption: Discourse markers have more consistent distributional patterns related to their pragmatic functions compared to grammatical markers.
- Evidence anchors: [abstract] "GPT-3 showed sensitivity to the non-cancelable exhaustivity implicature of the nun marker, but not to the lul marker's cancelable exhaustivity implicature."

### Mechanism 3
- Claim: The lul marker's dual role as a grammatical case marker and a marker of exhaustivity implicature makes it harder for models to learn its discourse function because its distributional patterns are more complex and less consistent.
- Mechanism: The lul marker's primary function as a case marker means that its distributional patterns are more tied to syntactic roles rather than pragmatic functions, making it harder for models to associate it with exhaustivity implicatures.
- Core assumption: Grammatical markers have more complex and less consistent distributional patterns related to their discourse functions compared to discourse markers.
- Evidence anchors: [abstract] "Results suggest that discourse meanings of a grammatical marker can be more challenging to encode than that of a discourse marker."

## Foundational Learning

- Concept: Exhaustivity implicatures
  - Why needed here: Understanding how models associate markers with exhaustivity implicatures is central to the study's findings.
  - Quick check question: What is the difference between cancelable and non-cancelable exhaustivity implicatures in the context of Korean morphology?

- Concept: Differential Object Marking (DOM)
  - Why needed here: DOM is the phenomenon being studied, where different markers are used based on both semantic and discourse features of the object.
  - Quick check question: How does DOM in Korean differ from DOM in other languages in terms of the factors influencing marker choice?

- Concept: Distributional semantics
  - Why needed here: The study investigates whether distributional semantics can capture discourse meanings, so understanding this concept is crucial.
  - Quick check question: How does the distributional hypothesis relate to the ability of language models to capture non-literal meanings?

## Architecture Onboarding

- Component map: Pre-trained transformer models (KoGPT-2, KoGPT-Trinity, Polyglot-Ko series, GPT-3, ChatGPT) -> Log probability and surprisal measurements -> Human evaluation tasks (rating appropriateness, forced-choice tasks) -> Corpus data (Korean text with examples of DOM)

- Critical path: 1) Train or access pre-trained models on Korean text data; 2) Design experimental stimuli to test models' understanding of exhaustivity implicatures; 3) Measure models' performance using log probabilities, surprisal, or human evaluations; 4) Analyze results to determine if models can capture discourse meanings

- Design tradeoffs: Using larger models with RLHF vs. smaller models trained purely on distributional data; focusing on production tasks (generating responses) vs. processing tasks (rating appropriateness); including contradictory continuations as a baseline vs. only testing pragmatic infelicity

- Failure signatures: Models consistently failing to distinguish logical contradictions from pragmatic infelicity; models not associating the lul marker with exhaustivity implicatures despite its grammatical function; models not improving with size or human feedback in capturing discourse meanings

- First 3 experiments: 1) Test if models can distinguish between cancelable and non-cancelable exhaustivity implicatures of the lul and nun markers using log probabilities or surprisal; 2) Test if models can generate appropriate responses using the lul and nun markers to convey exhaustivity status using forced-choice tasks; 3) Test if human feedback improves models' ability to capture discourse meanings by comparing models with and without RLHF

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the size of the training corpus, beyond just the number of parameters, influence the ability of language models to capture discourse meanings in Korean morphology?
- Basis in paper: [inferred] The paper mentions that larger models show more human-like patterns but notes that many factors were not controlled across models tested, including the amount of training data.
- Why unresolved: The study did not control for the amount of training data across different models, making it difficult to isolate the effect of corpus size on capturing discourse meanings.
- What evidence would resolve it: Comparative studies of models with similar architectures and parameter counts but different training corpus sizes, specifically focusing on their performance in capturing discourse meanings.

### Open Question 2
- Question: How do the frequency of morphological markers in the training data and the model's ability to generate behavior sensitive to that marker's implicatures relate to each other?
- Basis in paper: [explicit] The paper suggests that future work would benefit from examining the impact of the frequency of a given morphological marker with the model's ability to generate behavior sensitive to that marker's implicatures.
- Why unresolved: The study did not analyze the frequency of morphological markers in the training data and its correlation with the model's performance in generating appropriate discourse continuations.
- What evidence would resolve it: Statistical analysis of the frequency of morphological markers in the training data of various models and their corresponding performance in tasks involving discourse meanings.

### Open Question 3
- Question: Can fine-tuning language models with specific discourse-level training data improve their ability to capture implied discourse meanings in Korean morphology?
- Basis in paper: [inferred] The paper concludes that encoding dual meanings in multiple domains of language is challenging for distributional semantics but suggests that larger models with human feedback show more human-like patterns, implying that fine-tuning might help.
- Why unresolved: The study only evaluated pre-trained models without any fine-tuning on discourse-level data, leaving the potential benefits of such fine-tuning unexplored.
- What evidence would resolve it: Experiments comparing the performance of pre-trained models with models fine-tuned on discourse-level data in tasks involving Korean morphology and discourse meanings.

## Limitations

- Weak corpus evidence for distributional patterns of Korean markers, with no direct citations supporting claims about nun marker's distributional consistency versus lul marker's complexity
- Reliance on API access to proprietary models (GPT-3, ChatGPT) without full control over model parameters or training details
- ChatGPT struggled to generate appropriate responses in forced-choice tasks, but the paper does not provide detailed analysis of why this occurred

## Confidence

- High confidence: The finding that GPT-3 showed sensitivity to the non-cancelable exhaustivity implicature of the nun marker is well-supported by the experimental design and results presented
- Medium confidence: The claim that larger models with human feedback show more human-like patterns is supported but requires cautious interpretation due to the proprietary nature of ChatGPT limiting reproducibility
- Low confidence: The assertion that encoding dual meanings is "challenging for distributional semantics" is plausible but under-supported by weak corpus evidence

## Next Checks

1. **Corpus analysis replication**: Conduct a systematic corpus study to verify the distributional patterns of lul versus nun markers in Korean text, specifically testing whether nun indeed shows more consistent pragmatic associations than lul

2. **Controlled ablation study**: Compare models with identical architectures but different training procedures (with vs. without human feedback) on the Korean exhaustivity tasks to isolate the specific contribution of RLHF to discourse understanding

3. **Cross-linguistic validation**: Test whether the observed pattern of better performance on discourse markers versus grammatical markers holds across multiple languages with similar DOM phenomena, to determine if this is a general principle of distributional semantics or specific to Korean morphology