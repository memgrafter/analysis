---
ver: rpa2
title: Intriguing Differences Between Zero-Shot and Systematic Evaluations of Vision-Language
  Transformer Models
arxiv_id: '2402.08473'
source_url: https://arxiv.org/abs/2402.08473
tags:
- e-10
- e-11
- e-12
- e-09
- embedding
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Vision-language transformer models achieve high zero-shot classification
  accuracy but fail systematic evaluations, with 0% accuracy when embedding-aligned
  images are classified. The authors propose a gradient descent optimization method
  to align image representations with target classes, demonstrating that visually
  indistinguishable images can be classified with high confidence into incorrect classes.
---

# Intriguing Differences Between Zero-Shot and Systematic Evaluations of Vision-Language Transformer Models

## Quick Facts
- arXiv ID: 2402.08473
- Source URL: https://arxiv.org/abs/2402.08473
- Authors: Shaeke Salman; Md Montasir Bin Shams; Xiuwen Liu; Lingjiong Zhu
- Reference count: 40
- Primary result: Vision-language transformer models achieve high zero-shot accuracy but fail systematic evaluations, with 0% accuracy when embedding-aligned images are classified

## Executive Summary
Vision-language transformer models have demonstrated remarkable zero-shot classification performance, but this paper reveals significant limitations when subjected to systematic evaluation. The authors propose a gradient descent optimization method to align image representations with target classes, demonstrating that visually indistinguishable images can be classified with high confidence into incorrect classes. Using a linear approximation framework, they show that Gaussian noise in the input space leads to normal distributions in the representation space, indicating limited generalization. The method is model and dataset-agnostic, with similar results observed across different vision datasets and multimodal models.

## Method Summary
The paper introduces a gradient descent optimization procedure to find images whose embeddings align with target class representations while maintaining visual similarity to original images. The method computes gradients using the Jacobian of the representation function and iteratively updates input images to minimize the distance to target class embeddings. A linear approximation framework is used to analyze how Gaussian noise propagates through the representation function, showing that normally distributed input noise leads to normally distributed representations. The approach is evaluated using zero-shot classification accuracy, systematic evaluation accuracy, embedding alignment metrics (cosine similarity, PSNR, SSIM), and analysis of Jacobian matrix properties.

## Key Results
- Vision-language transformer models achieve high zero-shot classification accuracy but fail systematic evaluations with 0% accuracy when embedding-aligned images are classified
- Visually indistinguishable images can be classified with high confidence into incorrect classes using gradient descent optimization to align embeddings
- Gaussian noise in the input space leads to normal distributions in the representation space under linear approximation, indicating limited generalization

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Vision-language transformer models achieve high zero-shot accuracy by embedding alignment in a shared representation space.
- **Mechanism**: The models use joint image-text contrastive objectives to learn embeddings where semantically similar images and text are close in the shared embedding space. This alignment allows the softmax classification to work effectively in zero-shot settings.
- **Core assumption**: The embedding space has good geometric properties where semantically similar concepts are clustered together.
- **Evidence anchors**:
  - [abstract] "Transformer-based models have dominated natural language processing and other areas in the last few years due to their superior (zero-shot) performance on benchmark datasets."
  - [section 3] "The probability classified x to ti is given by ef(x)T ft(ti) / PC−1j=0 ef(x)T ft(tj)"
  - [corpus] Weak evidence - corpus papers discuss zero-shot ranking and image privacy classification but don't directly address the geometric properties of embedding spaces.
- **Break condition**: When the embedding space has poor geometric properties or when the input distribution differs significantly from training data.

### Mechanism 2
- **Claim**: Linear approximation of the representation function explains why Gaussian noise in input space leads to normal distributions in representation space.
- **Mechanism**: Under local linear approximation, the Jacobian matrix transforms the input noise covariance into the representation space covariance. This creates normal distributions in the embedding space when Gaussian noise is added to inputs.
- **Core assumption**: The transformer model can be locally approximated as linear near the input point.
- **Evidence anchors**:
  - [abstract] "Using a linear approximation, we show that Gaussian noise in the input space leads to normal distributions in the representation space"
  - [section 4.2] "Under the linear assumption, the resulting probability distribution in the representation space will be normal as well"
  - [corpus] No direct evidence - corpus papers don't discuss linear approximations of transformer representations.
- **Break condition**: When the linear approximation breaks down due to strong nonlinearities or when the noise magnitude is too large.

### Mechanism 3
- **Claim**: The embedding matching procedure can find subspaces of visually indistinguishable images that are classified differently.
- **Mechanism**: Gradient descent optimization is used to modify inputs to match target embeddings while keeping visual changes small. This creates a subspace where representations match other classes despite visual similarity.
- **Core assumption**: The representation function is differentiable and the gradient descent can find paths to target embeddings.
- **Evidence anchors**:
  - [abstract] "we are able to explore the embedding space of a commonly used vision-language model" and "visually indistinguishable images can be classified with high confidence into incorrect classes"
  - [section 4.1] "The gradient is approximately determined by ∂L/∂x ≈ (∂f/∂x)^T(f(x0 + ∆x) − f(xtg))"
  - [corpus] Weak evidence - corpus papers discuss adversarial attacks but not the specific embedding matching approach described.
- **Break condition**: When the optimization fails to converge or when the visual changes become too large.

## Foundational Learning

- **Concept: Jacobian matrix and its role in gradient-based optimization**
  - Why needed here: The Jacobian is used to compute gradients for the embedding matching procedure and to analyze how noise propagates through the representation function.
  - Quick check question: If the Jacobian has rank less than the input dimension, what does this imply about the null space of the representation function?

- **Concept: Contrastive learning and shared embedding spaces**
  - Why needed here: Understanding how vision-language models learn to align image and text representations is crucial for interpreting their behavior.
  - Quick check question: How does the softmax classification work when the embedding space is shared between images and text?

- **Concept: Linear approximation and Taylor series expansion**
  - Why needed here: The linear approximation is used to analyze how Gaussian noise in the input space affects the representation space.
  - Quick check question: Under what conditions does the linear approximation become inaccurate for transformer models?

## Architecture Onboarding

- **Component map**: Image → Vision encoder (transformer) → Shared embedding space → Text encoder (transformer) → Softmax classification layer
- **Critical path**: Image → Vision encoder → Shared embedding → Softmax classification. For embedding alignment: Image → Gradient computation → Image modification → New embedding
- **Design tradeoffs**: Zero-shot classification trades off between generalization (working on unseen classes) and robustness (sensitivity to small input changes). The linear approximation trades off between computational efficiency and accuracy.
- **Failure signatures**: Zero-shot accuracy drops when classes are semantically similar. Classification becomes unstable when Gaussian noise is added. Embedding alignment fails when visual changes become noticeable.
- **First 3 experiments**:
  1. Verify zero-shot classification accuracy on a subset of Imagenette classes using ImageBind.
  2. Implement and test the embedding matching procedure on a single image-class pair.
  3. Add Gaussian noise to an image and measure classification accuracy changes using the linear approximation framework.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: How does the proposed method's performance compare across different vision-language transformer architectures beyond CLIP and ImageBind?
- **Basis in paper**: [explicit] The paper demonstrates results using CLIP and ImageBind but states the method is model-agnostic and should generalize.
- **Why unresolved**: The paper only provides results for two specific models, leaving uncertainty about performance on other transformer architectures.
- **What evidence would resolve it**: Systematic evaluation across diverse vision-language models (e.g., LLaVA, MiniGPT-4, Flamingo) using the same methodology.

### Open Question 2
- **Question**: What is the underlying mechanism causing different singular value distributions in the Jacobian matrices between original and embedding-aligned images?
- **Basis in paper**: [explicit] The paper observes different singular value distributions in Fig. 3 but does not explain the mechanism.
- **Why unresolved**: The paper notes the observation but leaves investigation of the exact underlying mechanism as future work.
- **What evidence would resolve it**: Detailed analysis of Jacobian matrix properties and their relationship to embedding alignment optimization.

### Open Question 3
- **Question**: How does the proposed detection method for adversarial modifications perform against other types of adversarial attacks?
- **Basis in paper**: [inferred] The paper proposes a Gaussian noise-based detection method for embedding-aligned images but does not compare it to existing adversarial detection methods.
- **Why unresolved**: The paper only evaluates the method's effectiveness on embedding-aligned images, not other adversarial examples.
- **What evidence would resolve it**: Comparative evaluation of the detection method against established adversarial attack detection techniques.

## Limitations

- The reliance on gradient-based optimization to find embedding-aligned images may not fully capture the behavior of the model across all possible input variations.
- The linear approximation used to analyze Gaussian noise effects is only valid locally and may break down for larger perturbations or in regions with strong nonlinearities.
- Experiments are conducted primarily on Imagenette and Imagenette10 datasets, which may not generalize to all vision-language transformer models or broader image classification tasks.

## Confidence

**High Confidence**: The observation that vision-language transformer models achieve high zero-shot accuracy but fail systematic evaluations is well-supported by experimental evidence. The gradient descent optimization procedure for finding embedding-aligned images is clearly specified and demonstrated.

**Medium Confidence**: The claim that Gaussian noise in the input space leads to normal distributions in the representation space under linear approximation is theoretically sound but relies on assumptions about local linearity that may not hold globally. The analysis provides valuable insights but requires careful interpretation.

**Low Confidence**: The broader implications about the limitations of vision-language transformer models for reliable classification are suggestive but require further validation across different model architectures, datasets, and evaluation protocols.

## Next Checks

1. **Cross-model validation**: Test the embedding alignment procedure and systematic evaluation on at least three different vision-language transformer models (e.g., ImageBind, CLIP, BLIP) to verify that the observed phenomena are not model-specific.

2. **Noise sensitivity analysis**: Systematically vary the magnitude and distribution of Gaussian noise added to inputs and measure how classification accuracy degrades, comparing the linear approximation predictions with empirical results across a broader range of noise levels.

3. **Human perceptual validation**: Conduct human studies to verify that the embedding-aligned images are truly visually indistinguishable from the original images across multiple human observers, using established perceptual metrics and psychophysical methods.