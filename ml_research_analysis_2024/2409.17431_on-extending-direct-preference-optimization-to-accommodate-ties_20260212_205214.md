---
ver: rpa2
title: On Extending Direct Preference Optimization to Accommodate Ties
arxiv_id: '2409.17431'
source_url: https://arxiv.org/abs/2409.17431
tags:
- preference
- dpo-d
- dpo-rk
- pairs
- ties
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: We derive and investigate two DPO variants that explicitly model
  the possibility of declaring a tie in pair-wise comparisons. We replace the Bradley-Terry
  model in DPO with two well-known modeling extensions, by Rao and Kupper and by Davidson,
  that assign probability to ties as alternatives to clear preferences.
---

# On Extending Direct Preference Optimization to Accommodate Ties

## Quick Facts
- arXiv ID: 2409.17431
- Source URL: https://arxiv.org/abs/2409.17431
- Reference count: 40
- Primary result: DPO variants with tie modeling show performance improvements and better regularization

## Executive Summary
This paper addresses a limitation in Direct Preference Optimization (DPO) by extending it to handle tied pairwise comparisons. The authors replace the Bradley-Terry model in DPO with two alternative models - Rao-Kupper and Davidson - that explicitly assign probability to ties as alternatives to clear preferences. Through experiments in neural machine translation and summarization, they demonstrate that explicitly modeling ties can improve task performance and provide stronger regularization effects compared to standard DPO.

## Method Summary
The paper derives two DPO variants that explicitly model ties in pairwise comparisons. The standard DPO uses the Bradley-Terry model, which only models clear preferences between two options. The authors replace this with the Rao-Kupper model and the Davidson model, both of which can handle tied comparisons. These variants are evaluated on neural machine translation and summarization tasks, where ties are present in the preference data. The key insight is that incorporating ties leads to better regularization with respect to the reference policy, as measured by KL divergence.

## Key Results
- Explicitly labeled ties can be added to DPO datasets without the performance degradation observed in standard DPO
- Inclusion of ties leads to stronger regularization with respect to the reference policy
- Performance improvements over DPO are observed in translation and mathematical reasoning tasks
- DPO variants with tie modeling show better handling of tied pairs compared to standard DPO

## Why This Works (Mechanism)
The paper provides a theoretical explanation for the regularization effect using ideal DPO policy theory. By modeling ties explicitly rather than discarding them or treating them as clear preferences, the optimization process becomes more robust and better aligned with the reference policy. The tie-aware models capture uncertainty in human preferences more accurately, leading to improved generalization.

## Foundational Learning

**Bradley-Terry model**: A probabilistic model for pairwise comparisons that assumes one option is always preferred over another. Needed to understand the baseline DPO approach. Quick check: Can represent clear preference probabilities but cannot model ties.

**Rao-Kupper model**: Extends Bradley-Terry to include a tie probability term. Needed as one of the tie-aware alternatives. Quick check: Adds a parameter for tie probability between two options.

**Davidson model**: Another extension of Bradley-Terry that models ties through a different parameterization. Needed as the second tie-aware alternative. Quick check: Uses a different mathematical formulation for tie probability.

**KL divergence**: Measures the difference between two probability distributions. Needed to quantify regularization effects. Quick check: Lower values indicate better alignment with reference policy.

## Architecture Onboarding

**Component map**: Preference dataset -> Tie-aware DPO variant (Rao-Kupper or Davidson) -> Model parameters -> KL regularization -> Performance evaluation

**Critical path**: Dataset preparation with tie labels → Model training with tie-aware loss → Regularization monitoring → Performance evaluation

**Design tradeoffs**: The paper chooses between Rao-Kupper and Davidson models based on their mathematical properties. Rao-Kupper uses a symmetric tie probability, while Davidson uses an asymmetric formulation. The tradeoff involves complexity vs. expressiveness in modeling ties.

**Failure signatures**: Poor performance when tie labels are incorrectly assigned, model instability when tie probability parameters are not well-regularized, and overfitting when the dataset has too few tied examples.

**First experiments**: 1) Compare standard DPO vs. tie-aware variants on a dataset with known ties, 2) Analyze KL divergence trajectories during training for different models, 3) Test sensitivity to tie probability parameter initialization.

## Open Questions the Paper Calls Out

None

## Limitations

- Experiments are limited to two domains (neural machine translation and summarization)
- Relatively small sample sizes for tied pairs in the WMT'22 dataset (only 38 ties)
- The study does not investigate trade-offs between tie-aware optimization and other aspects like diversity or computational efficiency
- Theoretical regularization analysis relies on idealized assumptions that may not fully capture real-world behavior

## Confidence

| Claim | Confidence |
|-------|------------|
| Theoretical extensions are sound | High |
| Performance improvements over DPO | Medium |
| Regularization effects from tie modeling | Low |

## Next Checks

1. Replicate the experiments on additional tasks and datasets to test generalizability
2. Conduct ablation studies to isolate the impact of tie modeling from other experimental factors
3. Investigate the behavior of tie-aware DPO variants in large-scale, industrial applications to assess practical utility