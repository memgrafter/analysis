---
ver: rpa2
title: Semantic Retrieval at Walmart
arxiv_id: '2412.04637'
source_url: https://arxiv.org/abs/2412.04637
tags:
- product
- query
- retrieval
- embedding
- search
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a hybrid semantic retrieval system deployed
  at Walmart that combines traditional inverted index and embedding-based neural retrieval
  to improve product search, particularly for tail queries. The system uses a two-tower
  BERT-based neural model with negative sampling techniques for training, and reduces
  embedding size through linear projection to balance performance and storage requirements.
---

# Semantic Retrieval at Walmart

## Quick Facts
- arXiv ID: 2412.04637
- Source URL: https://arxiv.org/abs/2412.04637
- Reference count: 40
- Hybrid semantic retrieval system improves tail query performance by 18% in category recall@40

## Executive Summary
This paper presents a hybrid semantic retrieval system deployed at Walmart that combines traditional inverted index and embedding-based neural retrieval to improve product search, particularly for tail queries. The system uses a two-tower BERT-based neural model with negative sampling techniques for training, and reduces embedding size through linear projection to balance performance and storage requirements. The hybrid approach significantly improves retrieval relevance, achieving up to 18% lift in category recall@40 and 2% lift in NDCG@5 for tail queries compared to baseline methods.

## Method Summary
The system employs a two-tower BERT-based neural model with negative sampling techniques for training, and reduces embedding size through linear projection to balance performance and storage requirements. The hybrid approach combines approximate nearest neighbor search with traditional inverted index retrieval, using a recall federation component to merge results. The model is trained on 2M queries with product titles, descriptions, and attributes, using sampled softmax loss and hard negative sampling strategies. The system was successfully deployed in production with minimal latency impact, demonstrating the effectiveness of combining semantic and traditional retrieval methods for e-commerce search.

## Key Results
- 18% lift in category recall@40 for tail queries compared to inverted index only
- 2% lift in NDCG@5 for tail queries with hybrid approach
- Successful production deployment with minimal latency impact

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The hybrid approach of combining inverted index and semantic retrieval improves tail query performance by addressing vocabulary mismatch while preserving exact match capabilities.
- Mechanism: Inverted index handles exact matches for rare tokens (e.g., product IDs, model numbers) that semantic embeddings struggle with, while semantic retrieval captures synonyms and query intent for tail queries where vocabulary mismatch is more pronounced.
- Core assumption: Vocabulary mismatch is more problematic for tail queries than head queries in e-commerce search.
- Evidence anchors:
  - [abstract]: "semantic retrieval helps bridge the vocabulary gap especially for tail queries; it helps with synonyms, misspellings, and other query variants"
  - [section 3]: "traditional inverted index has capabilities like facet navigation and category filtering, which are hard to replicate using semantic retrieval alone"
  - [corpus]: Weak - corpus lacks direct comparison of tail vs head query performance

### Mechanism 2
- Claim: Negative sampling strategies significantly improve semantic retrieval model performance by teaching the model to distinguish relevant from irrelevant products.
- Mechanism: Hard negative selection techniques (product type matching, token matching, student-teacher approach) inject more challenging negative examples into training, forcing the model to learn finer distinctions between relevant and irrelevant items.
- Core assumption: The model needs explicit training on negative examples to learn relevance boundaries effectively.
- Evidence anchors:
  - [section 5.2]: "selecting negative items is essential for good performance" and describes three strategies for negative selection
  - [section 8.2]: "When we added hard negatives to the training data, described in section 5.2, we observed 0.87% lift in Recall@40 and 18% lift Category Recall@40"
  - [corpus]: Weak - corpus doesn't provide comparative results without negative sampling

### Mechanism 3
- Claim: Reducing embedding dimensionality through linear projection maintains performance while significantly reducing storage and latency costs.
- Mechanism: A learned linear projection layer maps high-dimensional embeddings (768) to lower dimensions (256) with minimal performance loss, enabling efficient ANN indexing and frequent updates.
- Core assumption: The information captured in the higher dimensional space can be effectively compressed without significant loss of retrieval quality.
- Evidence anchors:
  - [section 5.3]: "We therefore investigated two different strategies to reduce the size of the embeddings. In the first approach, we added a linear projection layer to reduce the embedding size"
  - [section 8.2]: "The linear projection is very effective in reducing the size of the embedding with very little performance cost"
  - [section 7.1]: "for normalized vectors of dimension 256, the ANN services can yield 99% for recall@20"

## Foundational Learning

- Concept: Approximate Nearest Neighbor (ANN) search tradeoffs
  - Why needed here: Understanding latency vs. recall tradeoffs is critical for production deployment decisions
  - Quick check question: What recall@20 percentage is achieved with 256-dimensional embeddings at ~13ms latency according to the paper?

- Concept: Siamese network architecture
  - Why needed here: The two-tower BERT-based model is central to the semantic retrieval component
  - Quick check question: What pooling method was found to work best for the Siamese BERT model in this paper?

- Concept: Negative sampling in contrastive learning
  - Why needed here: Proper negative selection is crucial for training effective retrieval models
  - Quick check question: Which negative sampling strategy (PT match, token match, or student-teacher) provided the best performance improvement?

## Architecture Onboarding

- Component map:
  Query Planner → generates both inverted index query plan and query embedding
  ANN Fetcher → retrieves products using approximate nearest neighbor search
  Inverted Index → retrieves products using traditional text matching
  Recall Federation → merges and deduplicates results from both sources
  Re-ranker → applies GBDT model with features including BERT embedding cosine similarity

- Critical path: Query → Query Planner → (ANN Fetcher + Inverted Index) → Recall Federation → Re-ranker → Results
- Design tradeoffs:
  - Latency vs. recall: 256-dimensional embeddings chosen over 768 for better latency while maintaining performance
  - Storage vs. freshness: Smaller embeddings enable more frequent ANN index updates
  - Complexity vs. maintainability: Hybrid approach adds complexity but provides better overall performance

- Failure signatures:
  - High latency: Check embedding dimensionality and ANN service configuration
  - Poor tail query performance: Verify semantic retrieval model is being used for eligible queries
  - Low overall recall: Check negative sampling implementation and training data quality

- First 3 experiments:
  1. Test recall@40 difference between inverted index only vs hybrid approach on tail query dataset
  2. Compare performance of different negative sampling strategies (PT match, token match, student-teacher)
  3. Evaluate impact of embedding dimensionality reduction on recall@40 and latency metrics

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can we further optimize the trade-off between embedding size and retrieval performance, particularly for rare tokens and long-tail queries?
- Basis in paper: [explicit] The paper discusses reducing embedding size through linear projection and using smaller transformer architectures like MiniLM and XtremeDistil, but notes performance drops with further size reduction.
- Why unresolved: The paper identifies that smaller embeddings reduce storage and latency costs but also decrease recall performance, particularly for tail queries and rare tokens.
- What evidence would resolve it: Empirical studies comparing different compression techniques (e.g., quantization, pruning) on the same model architecture while measuring recall@40, category recall@40, and latency across various query types.

### Open Question 2
- Question: What is the optimal strategy for negative item selection that balances computational efficiency with retrieval quality in e-commerce settings?
- Basis in paper: [explicit] The paper experiments with in-batch negatives, hard negatives using ANCE, and various filtering strategies (PT match, token match, student-teacher), finding different performance impacts.
- Why unresolved: While the paper identifies several strategies and their relative performance, it doesn't determine which approach is optimal across different query distributions or catalog sizes.
- What evidence would resolve it: Comparative analysis of different negative sampling strategies across multiple e-commerce domains with varying catalog sizes, query distributions, and computational constraints.

### Open Question 3
- Question: How can we better leverage product attributes beyond titles and basic attributes (category, brand, color, gender) to improve semantic retrieval performance?
- Basis in paper: [inferred] The paper mentions that adding more product attributes like description didn't improve performance, suggesting limitations in how attributes are currently utilized.
- Why unresolved: The paper only experiments with concatenating attributes as prefixes and doesn't explore alternative methods of incorporating structured product information into the semantic model.
- What evidence would resolve it: Testing alternative approaches such as attribute-specific attention mechanisms, attribute-aware pooling strategies, or graph-based attribute representations and measuring their impact on recall metrics.

## Limitations
- Evaluation focuses primarily on recall-based metrics without extensive analysis of precision or user satisfaction metrics
- Performance on extremely long-tail queries (those with very few historical interactions) is not explicitly characterized
- Detailed performance benchmarks comparing hybrid system to pure approaches are limited

## Confidence
**High Confidence Claims:**
- The hybrid retrieval approach improves tail query performance compared to inverted index alone
- Negative sampling strategies are effective for training semantic retrieval models
- Dimensionality reduction to 256 maintains performance while improving efficiency

**Medium Confidence Claims:**
- The specific 18% lift in category recall@40 is directly attributable to hard negative sampling
- The hybrid approach's benefits generalize beyond the Walmart corpus

**Low Confidence Claims:**
- The optimal embedding dimensionality of 256
- The specific negative sampling strategies chosen

## Next Checks
1. **A/B Test Validation**: Deploy a controlled experiment comparing pure inverted index vs. hybrid retrieval with statistically significant user samples to validate offline metrics against actual user engagement metrics (CTR, conversion rate).

2. **Extreme Tail Analysis**: Conduct a focused study on queries with zero historical interactions to measure how effectively the semantic component performs when no engagement data exists for negative sampling.

3. **Model Generalization Test**: Train and evaluate the same hybrid approach on a different e-commerce corpus (such as from a different retailer or product category) to assess whether the observed performance improvements transfer to new domains.