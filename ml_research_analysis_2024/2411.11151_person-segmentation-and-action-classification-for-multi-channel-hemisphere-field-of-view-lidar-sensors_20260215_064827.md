---
ver: rpa2
title: Person Segmentation and Action Classification for Multi-Channel Hemisphere
  Field of View LiDAR Sensors
arxiv_id: '2411.11151'
source_url: https://arxiv.org/abs/2411.11151
tags:
- person
- detection
- segmentation
- action
- lidar
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a person segmentation and action classification
  approach for hemisphere field of view LiDAR sensors. The authors collected a dataset
  using an Ouster OSDome-64 sensor with four channels (range, signal, reflectivity,
  and NIR) and annotated it for person segmentation and action classification (walking,
  waving, sitting).
---

# Person Segmentation and Action Classification for Multi-Channel Hemisphere Field of View LiDAR Sensors

## Quick Facts
- arXiv ID: 2411.11151
- Source URL: https://arxiv.org/abs/2411.11151
- Reference count: 34
- Person segmentation and action classification with F1-scores above 0.9 on multi-channel hemisphere LiDAR data

## Executive Summary
This paper presents a person segmentation and action classification approach for hemisphere field of view LiDAR sensors. The authors collected a dataset using an Ouster OSDome-64 sensor with four channels (range, signal, reflectivity, and NIR) and annotated it for person segmentation and action classification (walking, waving, sitting). They adapted a MaskDINO model to operate on spherical projected multi-channel LiDAR representations with positional encoding. Their approach achieved strong performance with F1-scores above 0.9 for person segmentation and action classification tasks. An ablation study showed that range and reflectivity channels along with positional encoding contributed significantly to performance. The method runs at 11 Hz on a robot platform, demonstrating real-time capability and generalization to different sensor versions.

## Method Summary
The authors collected 442 annotated LiDAR scans from an Ouster OSDome-64 sensor with four channels (range, signal, reflectivity, and NIR). They converted the 3D point cloud data to spherical projected 2D representations (512×64 pixels) and added positional encoding of XYZ coordinates as additional input channels, resulting in seven-channel input images. The MaskDINO model with SwinL transformer backbone was adapted to process this multi-channel representation. The model was first trained on person segmentation, then fine-tuned for action classification using the same architecture. The approach achieved real-time performance at 11 Hz on a robot platform and demonstrated generalization across different Ouster LiDAR versions.

## Key Results
- Achieved F1-scores above 0.9 for person segmentation and action classification tasks
- Demonstrated real-time capability at 11 Hz on a robot platform
- Showed successful generalization to different Ouster LiDAR sensor versions (OSDome-64 and OSDome-128)
- Ablation study revealed range and reflectivity channels contribute most significantly to performance

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Multi-channel LiDAR fusion improves person segmentation and action classification by providing complementary perceptual cues
- Mechanism: The Ouster OSDome-64 provides four distinct channels (range, signal, reflectivity, NIR) that capture different physical properties of the scene. Range data provides geometric structure, signal intensity captures return strength variations, reflectivity encodes material properties, and NIR captures ambient illumination effects. By combining these channels into a four-channel representation, the model can leverage complementary information for better discrimination of persons from background and for distinguishing action states.
- Core assumption: No single channel consistently provides optimal perceptual separation across all scenarios, but combining channels yields better performance than any individual channel
- Evidence anchors:
  - [abstract]: "we incorporate all LiDAR channels in the person perception, thereby improving the separation of individuals"
  - [section]: "Each measurement channel has advantages and disadvantages in terms of perceptual separation of individuals, depending on the situation. None of the channels consistently provides universal perceptual separation for individuals, though."
  - [corpus]: Weak evidence - corpus neighbors don't directly address multi-channel LiDAR fusion for person segmentation
- Break condition: If the channels contain redundant information or if certain channels introduce noise that overwhelms the useful signals

### Mechanism 2
- Claim: Positional encoding of XYZ coordinates enhances model performance by providing explicit spatial context
- Mechanism: The authors compute positional encoding using Ouster Sensor SDK to encode the 3D coordinates of each measurement point. This positional information is added as additional input channels to the model. By explicitly providing the spatial relationships between points, the model can better understand the 3D structure of persons and their actions, even though the input is processed as 2D images.
- Core assumption: The model benefits from explicit spatial information beyond what can be inferred from the range image alone
- Evidence anchors:
  - [section]: "Positional encoding of the XYZ point coordinates were added as additional input channels, resulting in a seven-channel input image"
  - [section]: "The implementation of positional encoding allowed us to improve the performance of the trained models, resulting in higher precision and F1-score"
  - [corpus]: Weak evidence - corpus neighbors don't specifically address positional encoding in LiDAR-based person segmentation
- Break condition: If the positional encoding introduces redundant information that the model could learn from the range data, or if the encoding computation introduces errors

### Mechanism 3
- Claim: MaskDINO architecture with SwinL backbone can effectively process spherical projected LiDAR data for detection and segmentation tasks
- Mechanism: The authors adapt MaskDINO, a transformer-based model originally designed for 2D images, to operate on the spherical projected LiDAR representation. The SwinL transformer backbone processes the multi-channel image input, while the MaskDINO framework handles both detection and segmentation. This architecture can learn to identify person regions and classify actions from the structured LiDAR data.
- Core assumption: The spatial patterns in LiDAR data can be effectively captured by convolutional and transformer operations originally designed for natural images
- Evidence anchors:
  - [abstract]: "We propose a method based on a MaskDINO model to detect and segment persons and to recognize their actions from combined spherical projected multi-channel representations"
  - [section]: "We adapted MaskDINO [33] with a SwinL transformer backbone"
  - [corpus]: Weak evidence - corpus neighbors don't specifically address MaskDINO for LiDAR-based person detection
- Break condition: If the spherical projection introduces distortions that the model cannot adequately compensate for, or if the transformer architecture is not well-suited to the structured nature of LiDAR data

## Foundational Learning

- Concept: Spherical projection of LiDAR data to 2D images
  - Why needed here: The Ouster OSDome-64 produces 3D point cloud data that needs to be converted to a 2D representation for processing by 2D image models like MaskDINO
  - Quick check question: How does the spherical projection preserve the geometric relationships between points while creating a 2D image format?

- Concept: Multi-channel sensor data fusion
  - Why needed here: Each LiDAR channel captures different physical properties, and understanding how to combine them effectively is crucial for optimal performance
  - Quick check question: What are the specific advantages and disadvantages of each LiDAR channel (range, signal, reflectivity, NIR) for person detection?

- Concept: Transformer-based object detection and segmentation
  - Why needed here: The MaskDINO framework with SwinL backbone is the core architecture used for both person segmentation and action classification
  - Quick check question: How does the SwinL transformer architecture differ from traditional CNN-based detection models, and what advantages does it offer for this application?

## Architecture Onboarding

- Component map: Ouster OSDome-64 LiDAR → Spherical projection → Multi-channel image (4 channels) → Positional encoding (7 channels) → Convolutional encoding layer → SwinL transformer backbone → MaskDINO head → Person detection and action classification
- Critical path: Data acquisition → Spherical projection → Multi-channel fusion → Model inference → Post-processing
- Design tradeoffs: Using a pre-trained MaskDINO model provides strong initialization but may not be optimal for LiDAR data; using all channels provides comprehensive information but increases computational cost; positional encoding adds spatial context but increases input dimensionality
- Failure signatures: Poor performance on partially occluded persons suggests issues with channel fusion; inconsistent action classification across similar poses suggests issues with the action recognition branch; degraded performance on different LiDAR models suggests overfitting to the specific sensor geometry
- First 3 experiments:
  1. Test the model on a held-out validation set with different levels of occlusion to assess robustness
  2. Evaluate the contribution of each LiDAR channel by training models with different channel combinations
  3. Test the model's generalization to different Ouster LiDAR models (e.g., OSDome-128) to verify sensor independence

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the model performance change when using different versions of hemisphere LiDAR sensors with varying numbers of laser beams?
- Basis in paper: [explicit] The authors tested their approach on both Ouster OSDome-64 and OSDome-128 sensors, noting the latter has double the laser beams, and found good generalization across versions
- Why unresolved: The paper only tested with two specific versions (64 and 128 beams) but did not systematically evaluate performance across a broader range of sensor configurations
- What evidence would resolve it: Systematic testing with multiple LiDAR versions having different beam counts (e.g., 32, 64, 128, 256 beams) to establish performance scaling patterns

### Open Question 2
- Question: What is the impact of different combinations of LiDAR channels on person segmentation performance in various environmental conditions?
- Basis in paper: [explicit] The ablation study showed that range and reflectivity channels contribute significantly to performance, but the study was limited to specific conditions and combinations
- Why unresolved: The ablation study only tested excluding individual channels in controlled settings, without exploring all possible combinations or testing across diverse environmental conditions
- What evidence would resolve it: Comprehensive testing of all possible channel combinations across varied environments (different lighting, weather, background complexity) to determine optimal channel sets

### Open Question 3
- Question: Can the approach be extended to recognize more complex person actions beyond the three tested (walking, waving, sitting)?
- Basis in paper: [explicit] The authors successfully demonstrated action classification for three specific actions but did not explore more complex or varied actions
- Why unresolved: The model was only trained and tested on three relatively simple action classes, leaving uncertainty about its ability to generalize to more nuanced or complex human behaviors
- What evidence would resolve it: Training and testing the model on a larger and more diverse action dataset, including complex interactions and subtle behavioral differences

## Limitations
- Dataset size of 442 annotated scans may limit generalization to highly diverse real-world scenarios
- Performance metrics are reported on the same dataset used for training without clear validation on an independent test set
- The approach has not been tested on cross-dataset evaluation to assess generalization beyond the Ouster OSDome-64 sensor

## Confidence

- **High Confidence**: The multi-channel LiDAR fusion approach and its contribution to improved performance (Mechanism 1) is well-supported by the ablation study showing that range and reflectivity channels contribute most significantly
- **Medium Confidence**: The effectiveness of positional encoding for enhancing spatial understanding (Mechanism 2) is demonstrated but could benefit from additional experiments showing the impact of different encoding schemes
- **Medium Confidence**: The adaptation of MaskDINO to LiDAR data (Mechanism 3) shows strong results but lacks comparison with alternative architectures that might be better suited for structured LiDAR data

## Next Checks
1. Conduct cross-dataset evaluation using the SemanticKITTI dataset or similar to assess generalization beyond the Ouster OSDome-64 sensor and the specific collection environment
2. Perform robustness testing with varying levels of occlusion, clutter, and sensor noise to evaluate real-world deployment readiness
3. Compare the proposed approach against alternative transformer architectures (such as PointFormer or 3D CNN-based methods) to validate the architectural choices and quantify the benefits of the MaskDINO adaptation