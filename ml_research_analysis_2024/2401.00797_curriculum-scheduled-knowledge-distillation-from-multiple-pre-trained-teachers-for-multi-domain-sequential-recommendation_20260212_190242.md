---
ver: rpa2
title: Curriculum-scheduled Knowledge Distillation from Multiple Pre-trained Teachers
  for Multi-domain Sequential Recommendation
arxiv_id: '2401.00797'
source_url: https://arxiv.org/abs/2401.00797
tags:
- recommendation
- prms
- knowledge
- ckd-mdsr
- sequential
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proposes CKD-MDSR, a curriculum-scheduled knowledge distillation
  framework that leverages multiple pre-trained recommendation models (PRMs) to enhance
  a small student model for multi-domain sequential recommendation. CKD-MDSR addresses
  the challenge of using heterogeneous PRMs in practical systems by distilling informative
  knowledge from three representative PRMs (UniSRec, Recformer, UniM2Rec) into a student
  model.
---

# Curriculum-scheduled Knowledge Distillation from Multiple Pre-trained Teachers for Multi-domain Sequential Recommendation

## Quick Facts
- arXiv ID: 2401.00797
- Source URL: https://arxiv.org/abs/2401.00797
- Reference count: 40
- Key outcome: CKD-MDSR achieves 0.86%-19.21% relative improvements in Recall@10 over SASRec base model

## Executive Summary
This paper proposes CKD-MDSR, a curriculum-scheduled knowledge distillation framework that leverages multiple pre-trained recommendation models (PRMs) to enhance a small student model for multi-domain sequential recommendation. The framework addresses the challenge of using heterogeneous PRMs by distilling informative knowledge from three representative models (UniSRec, Recformer, UniM2Rec) into a student model. Through curriculum-scheduled user behavior sequence sampling and an instance-level scoring strategy, CKD-MDSR selectively integrates knowledge based on confidence and consistency across PRMs. Extensive experiments on five real-world datasets demonstrate significant improvements over conventional models while maintaining comparable online serving costs.

## Method Summary
CKD-MDSR is a knowledge distillation framework that transfers knowledge from multiple heterogeneous pre-trained recommendation models to a student model for multi-domain sequential recommendation. The framework employs curriculum-scheduled sequence sampling based on difficulty (sequence length and item popularity), distills normalized user-item scores from in-batch negative samples, and uses an instance-level scoring strategy to select informative knowledge based on PRM confidence and consistency. The student model is trained using combined cross-entropy and knowledge distillation losses, with dynamic weighting of PRM contributions during training.

## Key Results
- Achieves 0.86%-19.21% relative improvements in Recall@10 compared to SASRec base model
- Outperforms conventional models and PRMs themselves on five real-world Amazon datasets
- Maintains comparable online serving costs while delivering superior recommendation performance

## Why This Works (Mechanism)

### Mechanism 1
- Claim: CKD-MDSR effectively integrates knowledge from heterogeneous PRMs by distilling normalized user-item scores from in-batch negative samples.
- Mechanism: The framework distills scores from multiple PRMs using in-batch negative sampling, which establishes a common invariant across different PRMs regardless of their varying architectures and outputs.
- Core assumption: In-batch negative samples provide a sufficient and representative subset of the item space for knowledge distillation.
- Evidence anchors: [abstract], [section] on in-batch negative sampling
- Break condition: If in-batch negative samples don't adequately represent the item space or contain noisy information

### Mechanism 2
- Claim: The curriculum-scheduled user behavior sequence sampling strategy improves learning effectiveness by training on progressively more difficult data.
- Mechanism: The framework measures sequence difficulty using sequence length and item popularity, then partitions sequences into buckets ordered by difficulty.
- Core assumption: Sequences with longer lengths and less popular items are inherently more difficult to learn from.
- Evidence anchors: [abstract], [section] on easy-to-hard sampling strategy
- Break condition: If difficulty measurer doesn't accurately reflect learning complexity

### Mechanism 3
- Claim: The instance-level scoring strategy selects informative knowledge from heterogeneous PRMs based on confidence and consistency, filtering out noisy predictions.
- Mechanism: The framework normalizes scores from different PRMs and calculates mean-square error between all pairs to measure consistency, then allocates dynamic weights based on confidence scores.
- Core assumption: PRMs exhibit varying levels of confidence and consistency across different user-item pairs.
- Evidence anchors: [abstract], [section] on instance-level scoring strategy
- Break condition: If consistency measurement fails to accurately identify noisy predictions

## Foundational Learning

- Concept: Pre-trained Language Models (PLMs) and their application in recommendation systems
  - Why needed here: CKD-MDSR leverages multiple PRMs built on PLMs, understanding their capabilities and limitations is crucial
  - Quick check question: What are the key differences between how PRMs like UniSRec, Recformer, and UniM2Rec represent items and model sequences?

- Concept: Knowledge Distillation (KD) techniques in machine learning
  - Why needed here: CKD-MDSR is fundamentally a KD framework transferring knowledge from multiple teacher models to a student model
  - Quick check question: How does the in-batch negative sampling strategy in CKD-MDSR differ from traditional KD approaches?

- Concept: Curriculum Learning and its application to sequential data
  - Why needed here: The framework employs curriculum-scheduled sampling to improve learning effectiveness
  - Quick check question: Why might training on progressively more difficult sequences improve the student model's ability to learn from multiple heterogeneous PRMs?

## Architecture Onboarding

- Component map: Curriculum Scheduler → Knowledge Distillation module → Instance-level Scoring Strategy → Student Model
- Critical path: Sequence sampling → Difficulty measurement → Bucket partitioning → Progressive training → In-batch negative sampling → Score normalization → Consistency measurement → Dynamic weight allocation → Student model optimization
- Design tradeoffs: Trades computational efficiency for knowledge integration quality by using in-batch negatives instead of full corpus; adds training complexity with curriculum scheduling
- Failure signatures: Poor performance on long-tail items suggests restrictive in-batch sampling; inconsistent improvements across datasets indicate curriculum scheduler issues
- First 3 experiments:
  1. Test difficulty measurer on small dataset to verify correct ordering by difficulty
  2. Compare in-batch negative sampling versus full corpus on single PRM to verify efficiency-quality tradeoff
  3. Test instance-level scoring strategy on synthetic data with known PRM consistency

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can CKD-MDSR handle dynamic knowledge distillation where PRMs or their strengths change over time?
- Basis in paper: [inferred] Static weighting approach discussed but not how to adapt when PRMs evolve
- Why unresolved: Paper focuses on fixed set of three PRMs without exploring real-world scenarios with PRM updates
- What evidence would resolve it: Experimental results showing performance when dynamically updating PRMs

### Open Question 2
- Question: What is the impact of different difficulty measurers on curriculum learning effectiveness?
- Basis in paper: [explicit] Paper states unclear what difficulty measurers are effective for multi-domain sequential recommendation
- Why unresolved: Only evaluates one specific difficulty measurer
- What evidence would resolve it: Comparative experiments testing multiple difficulty measurers on same datasets

### Open Question 3
- Question: How does CKD-MDSR perform in streaming recommendation scenarios with continuous user interaction data?
- Basis in paper: [inferred] Evaluates on static datasets using leave-one-out evaluation
- Why unresolved: Doesn't address continuous data streams or concept drift in user preferences
- What evidence would resolve it: Results from online experiments or simulations showing performance in streaming scenarios

## Limitations
- In-batch negative sampling may miss long-tail items not appearing in any batch
- Difficulty measurer relying on sequence length and popularity may not generalize across all domains
- Instance-level scoring depends on hyperparameter τ that may require extensive tuning

## Confidence
- High confidence: General framework architecture and experimental methodology
- Medium confidence: Effectiveness of curriculum-scheduled sampling and instance-level scoring strategy
- Low confidence: Scalability to extremely large item catalogs or highly heterogeneous PRM architectures

## Next Checks
1. Compare CKD-MDSR performance using in-batch negatives versus full corpus negatives to quantify efficiency-quality tradeoff
2. Systematically vary number of buckets B and difficulty measurer parameters to determine optimal configurations
3. Evaluate CKD-MDSR's performance when transferring knowledge from PRMs trained on one domain to a student model in a different domain