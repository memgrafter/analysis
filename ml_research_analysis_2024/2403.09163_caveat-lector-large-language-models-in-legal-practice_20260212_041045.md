---
ver: rpa2
title: 'Caveat Lector: Large Language Models in Legal Practice'
arxiv_id: '2403.09163'
source_url: https://arxiv.org/abs/2403.09163
tags:
- legal
- language
- llms
- text
- such
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper highlights the fundamental limitations of large language
  models (LLMs) in legal practice, emphasizing their inability to understand, reason,
  or possess accurate knowledge of the law. While LLMs excel at generating fluent,
  human-like text, they operate solely on statistical word prediction and lack access
  to verified legal facts or common-sense reasoning.
---

# Caveat Lector: Large Language Models in Legal Practice

## Quick Facts
- arXiv ID: 2403.09163
- Source URL: https://arxiv.org/abs/2403.09163
- Authors: Eliza Mik
- Reference count: 0
- Primary result: LLMs cannot reliably perform legal tasks due to fundamental limitations in understanding, reasoning, and distinguishing fact from fiction

## Executive Summary
Large language models (LLMs) like GPT-4 excel at generating fluent, human-like text but fundamentally lack the ability to understand, reason, or possess accurate knowledge of the law. They operate solely on statistical word prediction without grounding in real-world legal concepts or common-sense reasoning. This leads to a high propensity for hallucinations - generating plausible but factually incorrect or legally infeasible statements. The paper warns that overreliance on LLM outputs poses significant risks in legal contexts where accuracy is critical, and that even advanced models cannot reliably replace human legal judgment.

## Method Summary
The paper analyzes LLM behavior through examination of their training methodology (statistical word prediction), architectural limitations (lack of symbol grounding), and performance on legal tasks. It evaluates hallucination propensity across different legal question types and investigates the fundamental barriers to accurate legal reasoning. The analysis draws on existing research about LLM limitations, legal reasoning requirements, and the challenges of fine-tuning models for specialized domains.

## Key Results
- LLMs cannot distinguish fact from fiction because they lack understanding of meaning and cannot ground symbols in real-world referents
- Hallucinations occur frequently in legal contexts, with models generating plausible but incorrect information even on factual questions
- Fine-tuning LLMs for legal tasks is inherently limited by the difficulty of formalizing legal knowledge and establishing ground truth

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** LLMs generate fluent but unreliable legal text because they are trained solely on word prediction without grounding in real-world legal concepts.
- **Mechanism:** The model memorizes word co-occurrence patterns from large text corpora but lacks the ability to associate those words with their real-world referents or legal meanings. This results in plausible-sounding text that may be factually incorrect.
- **Core assumption:** Understanding legal meaning requires more than statistical word patterns; it requires knowledge of the physical and conceptual world.
- **Evidence anchors:**
  - [abstract] "LLMs do not understand text. Without the ability to understand meaning, LLMs will remain unable to use language, to acquire knowledge and to perform complex reasoning tasks."
  - [section II.A] "Meaning cannot, as they claim, be learned from form alone. The authors define 'form' as any observable realization of language, such as text, and 'meaning' as the relation between the form and something external to language."
  - [corpus] Weak - related papers focus on text complexity metrics, not symbol grounding

### Mechanism 2
- **Claim:** Hallucinations occur because LLMs cannot distinguish between high-probability word sequences and factually correct information.
- **Mechanism:** The model treats all text as equal - whether from a Supreme Court opinion or a Reddit comment. It generates text based on statistical likelihood rather than truth-seeking, leading to plausible but incorrect legal statements.
- **Core assumption:** Legal correctness depends on factual accuracy and established doctrine, not just linguistic plausibility.
- **Evidence anchors:**
  - [abstract] "Trained to model language on the basis of stochastic word predictions, LLMs cannot distinguish fact from fiction."
  - [section IV.A] "LLMs are trained to generate text. They are not expected to refuse to provide an answer, express doubt, or uncertainty."
  - [corpus] Weak - related papers mention hallucinations but don't explain the statistical vs. factual distinction

### Mechanism 3
- **Claim:** Fine-tuning LLMs for legal tasks is inherently limited by the difficulty of formalizing legal knowledge and the impossibility of establishing a single "ground truth."
- **Mechanism:** Legal tasks require nuanced interpretation and reasoning that cannot be reduced to binary classification labels. Human annotators must make subjective judgments about what constitutes correct legal reasoning.
- **Core assumption:** Many legal questions have multiple valid answers based on different interpretations of law and policy.
- **Evidence anchors:**
  - [section IV.D.1] "The process involves two groups of persons: those who annotate and those who prepare annotation guidelines... The problem is not, however, limited to the expertise of the creators but concerns the broader difficulty of formalizing legal rules."
  - [section III.A.2] "The principles governing a particular legal area can be scattered across multiple legal sources that are often inconsistent and contradictory."
  - [corpus] Moderate - LegalBench benchmark acknowledges the difficulty of capturing legal reasoning in discrete tasks

## Foundational Learning

- **Concept:** Symbol grounding problem
  - **Why needed here:** Explains why LLMs cannot learn meaning from text alone, which is fundamental to understanding their limitations in legal contexts
  - **Quick check question:** Why can't an LLM understand what "indemnity" means just by reading thousands of contracts containing that word?

- **Concept:** Parametric knowledge vs. true knowledge
  - **Why needed here:** Distinguishes between statistical memorization of word patterns and actual understanding of legal concepts
  - **Quick check question:** If an LLM memorizes that "contracts require mens rea" appears frequently in its training data, does that mean it understands contract law?

- **Concept:** Out-of-distribution robustness
  - **Why needed here:** Explains why LLMs struggle with specialized legal text that differs from their general training data
  - **Quick check question:** Why might an LLM trained on general internet text perform poorly on tax law questions?

## Architecture Onboarding

- **Component map:** Pre-training phase -> Fine-tuning phase -> Prompting interface -> Generation mechanism -> Knowledge retrieval (optional)
- **Critical path:** Prompt → Model processing → Token prediction → Output generation → User evaluation
  - Failure points: Poor prompt quality, model limitations, hallucination detection, user expertise gaps
- **Design tradeoffs:**
  - Model size vs. computational cost and hallucination rate
  - Training data breadth vs. domain specificity
  - Generation fluency vs. factual accuracy
  - User autonomy vs. model control/moderation
- **Failure signatures:**
  - Consistent factual errors in legal citations or case law
  - Confident but incorrect legal reasoning
  - Sensitivity to minor prompt variations producing different outputs
  - Inability to handle novel legal scenarios
- **First 3 experiments:**
  1. Test hallucination rates by asking the model to summarize a known case and checking for factual accuracy
  2. Compare outputs using different prompt formulations for the same legal question to assess sensitivity
  3. Evaluate model performance on legal reasoning tasks requiring multiple steps versus simple fact recall

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can legal hallucinations be reliably detected and measured in open-domain legal question answering scenarios where multiple correct answers exist?
- **Basis in paper:** [explicit] The paper contrasts scenario 1 (factual questions with single correct answers) and scenario 2 (open-ended questions with multiple possible correct answers), noting that in the second scenario "it may be impossible to state whether the generated output constitutes a hallucination, that is, whether it is correct or not."
- **Why unresolved:** The paper argues that in open-ended legal questions, correctness depends on subjective legal expertise and interpretation, making automated detection of hallucinations impossible without a clear "legal ground truth."
- **What evidence would resolve it:** A comprehensive study comparing hallucination detection accuracy between legal experts and non-experts across multiple open-domain legal question types, demonstrating whether reliable detection methods can be developed.

### Open Question 2
- **Question:** Can reinforcement learning from expert human feedback (RLEHF) significantly improve LLM performance on complex legal tasks compared to traditional RLHF using non-expert labelers?
- **Basis in paper:** [explicit] The paper discusses RLHF limitations, noting that "Tasks requiring expert legal knowledge are particularly prone to succumb to subjective or even incorrect interpretations of the law" and proposes RLEHF as a potential solution.
- **Why unresolved:** The paper identifies the need for expert legal feedback but does not provide empirical evidence comparing RLEHF performance against traditional RLHF on legal tasks.
- **What evidence would resolve it:** A controlled experiment comparing model performance on legal tasks using RLEHF versus RLHF, measuring accuracy improvements and hallucination reduction.

### Open Question 3
- **Question:** What is the optimal balance between model size and training data quality for minimizing legal hallucinations while maintaining practical utility?
- **Basis in paper:** [inferred] The paper discusses scaling limitations, noting that "larger models are often less truthful than their smaller predecessors" and that "increasing model size does not significantly improve parametric knowledge with regards to less popular words."
- **Why unresolved:** The paper presents conflicting evidence about scaling benefits but does not empirically determine the optimal trade-off point between model size and data quality.
- **What evidence would resolve it:** A systematic study varying model sizes and training data quality on legal tasks, measuring hallucination rates and task performance to identify optimal parameter configurations.

## Limitations

- Proprietary model details limit understanding of training data composition and architectural specifics
- No universal ground truth for many legal questions makes hallucination detection subjective and unreliable
- The relationship between parametric knowledge and factual accuracy remains poorly understood

## Confidence

**High Confidence:** LLMs generate fluent but potentially unreliable legal text due to their word prediction nature. This is well-established across multiple evaluations and directly observable in practice.

**Medium Confidence:** Fine-tuning for legal tasks faces inherent limitations due to the difficulty of formalizing legal knowledge and establishing ground truth.

**Low Confidence:** The specific hallucination rates and error patterns for different legal task types without standardized benchmarks and evaluation protocols.

## Next Checks

1. **Cross-model hallucination comparison:** Evaluate multiple LLM variants (different sizes, training approaches, legal domain specializations) on identical legal tasks to determine if hallucination patterns are model-specific or fundamental to the approach.

2. **Expert-in-the-loop validation:** Implement a human oversight protocol where legal experts review and correct LLM outputs, measuring both the rate of expert intervention and the downstream impact on final output accuracy.

3. **Grounding augmentation experiment:** Test whether providing LLMs with access to verified legal databases during generation significantly reduces hallucination rates compared to standalone generation, isolating the effect of external knowledge access.