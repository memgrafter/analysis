---
ver: rpa2
title: 'AnnoTheia: A Semi-Automatic Annotation Toolkit for Audio-Visual Speech Technologies'
arxiv_id: '2402.13152'
source_url: https://arxiv.org/abs/2402.13152
tags:
- speech
- pages
- audio-visual
- recognition
- toolkit
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces AnnoTheia, a semi-automatic toolkit for annotating
  audio-visual speech data across multiple languages. The toolkit detects active speakers
  and generates transcriptions, with a flexible modular design allowing adaptation
  to different languages.
---

# AnnoTheia: A Semi-Automatic Annotation Toolkit for Audio-Visual Speech Technologies

## Quick Facts
- arXiv ID: 2402.13152
- Source URL: https://arxiv.org/abs/2402.13152
- Authors: José-M. Acosta-Triana; David Gimeno-Gómez; Carlos-D. Martínez-Hinarejos
- Reference count: 0
- Primary result: Semi-automatic toolkit for annotating audio-visual speech data across multiple languages with ~95% ASD accuracy on Spanish test set

## Executive Summary
AnnoTheia is a semi-automatic annotation toolkit designed to streamline the process of annotating audio-visual speech data, particularly for developing speech technologies in low-resource languages. The toolkit detects active speakers and generates transcriptions through a modular architecture that allows adaptation to different languages. The authors demonstrate this adaptability by fine-tuning a pre-trained active speaker detection (ASD) model to Spanish using the LIP-RTVE dataset, achieving approximately 95% accuracy and 99% AUC on a test set. Additionally, they created ASD-RTVE, a new Spanish ASD corpus. The toolkit aims to lower barriers for developing speech technologies in low-resource languages by streamlining the data collection process.

## Method Summary
The paper introduces AnnoTheia, a semi-automatic annotation toolkit that processes video data through a pipeline of scene detection, face detection, face alignment, active speaker detection, post-processing, scene trimming, and automatic transcription. To demonstrate cross-lingual adaptation, the authors fine-tuned a pre-trained TalkNet-ASD model to Spanish using the LIP-RTVE database, following a standard training procedure with Adam optimizer (0.0001 learning rate, 32 batch size, 9 epochs). The toolkit uses PySceneDetection for scene detection, Dual Shot Face Detector for face extraction, Whisper for transcription, and incorporates post-processing with smoothing and thresholding. The authors also constructed the ASD-RTVE corpus by creatively defining positive and negative samples from existing databases not originally designed for ASD tasks.

## Key Results
- Achieved ~95% accuracy and 99% AUC on Spanish ASD test set after fine-tuning pre-trained model
- Demonstrated modular toolkit design allowing language-specific component replacement
- Created ASD-RTVE, a new Spanish ASD corpus from repurposed audio-visual databases
- Showed effectiveness of context window optimization, with 51-frame windows yielding optimal performance

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** The toolkit achieves cross-lingual adaptation by modular replacement of language-dependent components while preserving the overall architecture.
- **Mechanism:** AnnoTheia's design isolates language-dependent modules (e.g., ASR, face detection, ASD) allowing researchers to substitute them with language-specific models trained on local data. The ASD model fine-tuning process demonstrates this approach by adapting a pre-trained model to Spanish using LIP-RTVE data.
- **Core assumption:** Language-specific modules can be replaced independently without breaking the system's overall workflow or data flow dependencies.
- **Evidence anchors:**
  - [abstract]: "The toolkit aims to lower barriers for developing speech technologies in low-resource languages by streamlining the data collection process."
  - [section]: "One of the most notable aspects of the proposed toolkit is the flexibility to replace a module with another of our preference or, where appropriate, adapted to our language of interest."
  - [corpus]: Weak - corpus provides related papers but doesn't directly validate this mechanism.

### Mechanism 2
- **Claim:** Semi-automatic annotation reduces human workload while maintaining data quality through post-processing smoothing and thresholding.
- **Mechanism:** The system uses automated detection (ASD, face detection) to generate candidate annotations, then applies smoothing techniques and adjustable thresholds to reduce false positives/negatives. Human annotators only validate or correct the remaining cases.
- **Core assumption:** Automated detection can generate high-recall candidates, and post-processing can filter out obvious errors without losing too many valid cases.
- **Evidence anchors:**
  - [section]: "we applied a smoothing average strategy for stabilizing the complete scene detection process" and "we computed the optimum threshold to find the best trade-off between the true-positive and false-positive rates."
  - [abstract]: "AnnoTheia, a semi-automatic annotation toolkit that detects when a person speaks on the scene and the corresponding transcription."
  - [corpus]: Weak - no direct evidence in corpus about workload reduction or quality maintenance.

### Mechanism 3
- **Claim:** The ASD-RTVE corpus construction methodology enables ASD model training even when databases weren't originally designed for this task.
- **Mechanism:** By creatively defining positive samples (aligned audio-video), negative samples (temporal mismatch, partial speaker mismatch, complete speaker mismatch), and leveraging existing databases not intended for ASD, the authors created a suitable training corpus for Spanish ASD.
- **Core assumption:** Existing audio-visual databases can be repurposed for ASD by defining appropriate sample categories and splitting strategies.
- **Evidence anchors:**
  - [section]: "we defined different types of samples, intending to cover all the possible situations we might find when applying our model in a realistic application scenario."
  - [abstract]: "we also describe the adaptation of a pre-trained ASD model to Spanish, using a database not initially conceived for this type of task."
  - [corpus]: Weak - corpus doesn't validate this specific methodology but provides related work on speech technologies.

## Foundational Learning

- **Concept: Active Speaker Detection (ASD)**
  - Why needed here: ASD is the core component that identifies who is speaking in a video, enabling accurate transcription and annotation. Without reliable ASD, the entire toolkit's output quality suffers.
  - Quick check question: What are the three main input modalities used by TalkNet-ASD for detecting active speakers?

- **Concept: Cross-modal feature fusion**
  - Why needed here: The TalkNet-ASD model combines visual (face frames) and acoustic (MFCC) features through cross-attention mechanisms to make speaking/not-speaking decisions.
  - Quick check question: How does the cross-attention network in TalkNet-ASD align audio and visual embeddings?

- **Concept: Data augmentation and negative sampling**
  - Why needed here: The ASD-RTVE corpus construction relies on creating negative samples (mismatched audio-video) to train robust ASD models that don't just associate faces with voices.
  - Quick check question: Why is it important to include "partial speaker mismatch" samples in the ASD training data?

## Architecture Onboarding

- **Component map:** Scene Detection → Face Detection → Face Alignment → Active Speaker Detection → Post-Processing → Scene Trimming → Automatic Transcription → User Interface
- **Critical path:** Scene Detection → Face Detection → Active Speaker Detection → Post-Processing → Scene Trimming → Transcription
- **Design tradeoffs:** The toolkit trades some accuracy for speed by using window-slicing methods and non-overlapping sampling. It also trades perfect automation for semi-automation, requiring human validation to ensure quality.
- **Failure signatures:** Common failures include (1) Face detection missing speakers in profile views or poor lighting, (2) ASD misclassifying dubbed content or voiceovers, (3) Transcription errors in noisy audio or accented speech, (4) Scene detection incorrectly splitting continuous speech.
- **First 3 experiments:**
  1. Run the pipeline on a single video with known ground truth to measure detection accuracy and identify failure modes.
  2. Test the ASD module independently with the ASD-RTVE test set to verify the ~95% accuracy claim.
  3. Evaluate the user interface workflow by having a test annotator process a small batch of scenes and measure time savings versus manual annotation.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How effective is AnnoTheia compared to a fully manual annotation process in terms of time savings and annotation quality?
- Basis in paper: [explicit] The authors state "we plan to conduct a comprehensive experiment to properly assess the effectiveness and advantages of using the AnnoTheia toolkit versus a completely manual procedure" but note this experiment has not yet been conducted.
- Why unresolved: The paper describes the toolkit's design and demonstrates its adaptation to Spanish, but does not provide empirical comparison data against manual annotation workflows.
- What evidence would resolve it: A controlled study comparing annotation time, accuracy, and inter-annotator agreement between AnnoTheia-assisted and purely manual annotation of the same audio-visual datasets.

### Open Question 2
- Question: Can the ASD model trained on Spanish data generalize to other Romance languages with minimal additional training?
- Basis in paper: [inferred] The paper successfully adapts an ASD model from English to Spanish using LIP-RTVE, suggesting cross-linguistic adaptation is possible, but does not test generalization to other languages.
- Why unresolved: The authors only demonstrate adaptation to one specific language (Spanish) and do not explore whether this model could be transferred to related languages with limited additional data.
- What evidence would resolve it: Fine-tuning the Spanish ASD model on Portuguese, Italian, or French datasets and measuring performance degradation or improvement compared to training from scratch.

### Open Question 3
- Question: How does the choice of context window size affect ASD performance across different types of audio-visual content (e.g., news broadcasts vs. informal conversations)?
- Basis in paper: [explicit] The authors show that context window size significantly affects performance on their Spanish dataset, with 51-frame windows achieving optimal results, but they only tested on TV news content.
- Why unresolved: The study only evaluated context window effects on one type of content (Spanish TV news), leaving open whether optimal window sizes vary across different speaking styles, environments, or content types.
- What evidence would resolve it: Testing the ASD model with different context window sizes on diverse datasets including informal conversations, interviews, and scripted content, then comparing optimal window sizes across these domains.

## Limitations
- Limited validation beyond Spanish language adaptation; generalization to other languages untested
- Claims about semi-automatic workflow efficiency lack empirical user studies or time measurements
- Modular design flexibility asserted but not validated through actual component replacement experiments

## Confidence
- **High confidence:** Core technical approach of fine-tuning TalkNet-ASD for Spanish adaptation follows standard transfer learning practices
- **Medium confidence:** Toolkit's overall effectiveness and usability claims supported by authors' experience but lack independent validation
- **Low confidence:** Claims about toolkit's applicability to arbitrary low-resource languages are speculative with only Spanish demonstration

## Next Checks
1. Test the toolkit's language-agnostic claims by adapting the ASD model to a different language (e.g., French or German) using a similar fine-tuning approach and evaluate performance on that language's test set.
2. Conduct a user study comparing annotation time and quality between manual annotation, the AnnoTheia toolkit, and a fully automatic approach to quantify the semi-automatic workflow's efficiency gains.
3. Validate the ASD-RTVE corpus construction methodology by training ASD models on subsets of the data and testing generalization to held-out videos, particularly focusing on the different negative sample types.