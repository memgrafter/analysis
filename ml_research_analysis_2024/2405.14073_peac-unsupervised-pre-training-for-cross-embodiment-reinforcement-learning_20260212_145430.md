---
ver: rpa2
title: 'PEAC: Unsupervised Pre-training for Cross-Embodiment Reinforcement Learning'
arxiv_id: '2405.14073'
source_url: https://arxiv.org/abs/2405.14073
tags:
- learning
- embodiments
- embodiment
- peac
- cross-embodiment
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a novel approach to unsupervised pre-training
  for cross-embodiment reinforcement learning. The authors introduce the concept of
  Cross-Embodiment Unsupervised RL (CEURL), where agents are pre-trained in reward-free
  environments across a distribution of embodiments to learn embodiment-aware and
  task-agnostic knowledge.
---

# PEAC: Unsupervised Pre-training for Cross-Embodiment Reinforcement Learning

## Quick Facts
- arXiv ID: 2405.14073
- Source URL: https://arxiv.org/abs/2405.14073
- Authors: Chengyang Ying; Zhongkai Hao; Xinning Zhou; Xuezhou Xu; Hang Su; Xingxing Zhang; Jun Zhu
- Reference count: 40
- Primary result: PEAC significantly improves adaptation performance and cross-embodiment generalization in unsupervised RL settings.

## Executive Summary
This paper introduces PEAC, a novel approach to unsupervised pre-training for cross-embodiment reinforcement learning. The method pre-trains agents across a distribution of embodiments in reward-free environments to learn embodiment-aware and task-agnostic knowledge. PEAC incorporates a cross-embodiment intrinsic reward function and an embodiment discriminator to enable effective exploration and skill discovery across different embodiments. Extensive experiments demonstrate that PEAC outperforms current state-of-the-art unsupervised RL models on both simulated and real-world environments.

## Method Summary
PEAC addresses cross-embodiment unsupervised RL by formulating the problem as a Controlled Embodiment Markov Decision Process (CE-MDP). The core innovation is a cross-embodiment intrinsic reward (RCE) that encourages the policy to explore regions where an embodiment discriminator is uncertain. During pre-training, agents collect trajectories from multiple embodiments in reward-free environments, while the embodiment discriminator learns to distinguish between different embodiments. The RCE is computed as the difference between a fixed embodiment prior and the discriminator's posterior, guiding the policy to learn embodiment-aware representations. PEAC can be flexibly combined with existing unsupervised RL methods like LBS and DIAYN for enhanced exploration and skill discovery. After pre-training, the policy is fine-tuned on downstream tasks with extrinsic rewards.

## Key Results
- PEAC outperforms state-of-the-art unsupervised RL models across multiple domains including DMC, Robosuite, and Isaacgym
- The method shows significant improvements in both adaptation performance and cross-embodiment generalization
- PEAC demonstrates robust performance across different RL backbones including DDPG, DreamerV2, and PPO
- Zero-shot adaptation results show PEAC can effectively transfer to unseen tasks without additional training

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The cross-embodiment intrinsic reward RCE enables the agent to explore regions where the embodiment discriminator is uncertain, thereby learning representation of embodiment contexts.
- Mechanism: RCE is defined as the difference between a fixed embodiment prior and a learned posterior. Maximizing this reward encourages the policy to visit states where the discriminator cannot confidently predict the embodiment, forcing the model to learn embeddings that capture embodiment-specific features.
- Core assumption: The embodiment discriminator can be trained to reliably distinguish between different embodiments based on trajectories, and the prior is fixed and known.
- Evidence anchors:
  - [abstract] "incorporates an intrinsic reward function specifically designed for cross-embodiment pre-training" and "an embodiment discriminator, which distinguishes between different embodiments"
  - [section 3.2] "we first train an embodiment discriminator qθ(e|τ) to approximate pπ(e|τ)" and "For cross-embodiment pre-training, PEAC then utilizes our cross-embodiment intrinsic reward, which is defined following Eq. (4) as RCE(τ) ≜ log p(e) − log qθ(e|τ)"
- Break condition: If the embodiment discriminator fails to converge or if the prior is misspecified, RCE may not guide the policy toward useful embodiment-aware exploration.

### Mechanism 2
- Claim: PEAC's formulation as a Controlled Embodiment MDP (CE-MDP) correctly captures the additional complexity introduced by varying embodiment contexts, making pre-training more challenging but also more effective.
- Mechanism: By defining a distribution over controlled MDPs and extending information geometry analysis, the paper shows that skill vertices in CE-MDP may no longer correspond to deterministic policies. This complexity necessitates the use of RCE to guide exploration across the entire embodiment space.
- Core assumption: The extension of information geometry analysis from single-embodiment MDPs to CE-MDP is valid and captures the true complexity of the problem.
- Evidence anchors:
  - [section 3.1] "We formulate CEURL as a novel Controlled Embodiment Markov Decision Process (CE-MDP)" and "We then extend the information geometry analyses of the controlled MDP [11] to better explain the complexity of CE-MDP"
  - [section 3.1] "Our findings indicate that skill vertices within CE-MDP may no longer be simple deterministic policies and the behaviors across different embodiments can display substantial variability"
- Break condition: If the information geometry analysis does not accurately model the true complexity, the formulation may not lead to better pre-training strategies.

### Mechanism 3
- Claim: The flexibility of PEAC to combine with existing unsupervised RL methods (like LBS and DIAYN) allows it to leverage both exploration and skill discovery for cross-embodiment learning.
- Mechanism: PEAC can be combined with exploration methods by adding RCE to the existing intrinsic reward, or with skill discovery methods by extending the mutual information objective to be embodiment-aware. This allows PEAC to adapt to different pre-training needs.
- Core assumption: Existing unsupervised RL methods can be extended to the cross-embodiment setting without significant modification to their core algorithms.
- Evidence anchors:
  - [section 4] "PEAC can integrate flexibly with existing single-embodiment unsupervised RL methods to achieve cross-embodiment exploration and skill discovery"
  - [section 4] "We take LBS [38], of which the intrinsic reward is the KL divergence between the latent prior and the approximation posterior, as the PEAC-LBS" and "We take DIAYN [10] as an example, resulting in PEAC-DIAYN"
- Break condition: If the combination with existing methods introduces conflicts or instability, the performance gains may not be realized.

## Foundational Learning

- Concept: Controlled Embodiment Markov Decision Process (CE-MDP)
  - Why needed here: Provides a formal framework for modeling the cross-embodiment reinforcement learning problem, capturing the distribution over controlled MDPs and the additional complexity introduced by varying embodiment contexts.
  - Quick check question: How does the state space in a CE-MDP differ from a standard MDP, and why is this important for cross-embodiment learning?

- Concept: Information Geometry Analysis
  - Why needed here: Used to analyze the complexity of the CE-MDP and understand how the distribution of embodiment contexts affects the optimal policy. Shows that skill vertices may no longer be deterministic policies in the cross-embodiment setting.
  - Quick check question: What is the significance of the discount state distribution in the information geometry analysis, and how does it relate to the optimal policy in a CE-MDP?

- Concept: Unsupervised Reinforcement Learning
  - Why needed here: Forms the basis for pre-training in reward-free environments, allowing the agent to learn embodiment-aware and task-agnostic knowledge before fine-tuning on specific downstream tasks.
  - Quick check question: What are the key differences between exploration-based and skill discovery-based unsupervised RL methods, and how might each be adapted for cross-embodiment learning?

## Architecture Onboarding

- Component map:
  Embodiment Discriminator -> Cross-Embodiment Intrinsic Reward (RCE) -> Policy Network -> RL Backbone

- Critical path:
  1. Collect trajectories from multiple embodiments in a reward-free environment.
  2. Train the embodiment discriminator on these trajectories to predict the embodiment context.
  3. Compute the RCE for each state-action pair using the discriminator's posterior and a fixed prior.
  4. Combine RCE with the existing intrinsic reward (if using a combined method like PEAC-LBS or PEAC-DIAYN).
  5. Update the policy using the RL backbone and the combined intrinsic reward.
  6. Fine-tune the pre-trained policy on downstream tasks with extrinsic rewards.

- Design tradeoffs:
  - Using a fixed embodiment prior vs. learning it: A fixed prior simplifies the RCE calculation but may not capture the true embodiment distribution.
  - Combining with existing methods vs. using PEAC alone: Combining allows leveraging proven exploration/skill discovery techniques but adds complexity.
  - Using a discriminator vs. other context encoders: A discriminator is simple and effective but may not capture all relevant embodiment features.

- Failure signatures:
  - Embodiment discriminator fails to converge or predicts randomly: RCE will not guide the policy effectively, leading to poor pre-training.
  - Policy collapses to a single embodiment: Indicates that the RCE is not encouraging sufficient exploration across the embodiment space.
  - Fine-tuning performance is worse than training from scratch: Suggests that the pre-training did not learn useful cross-embodiment knowledge.

- First 3 experiments:
  1. Train the embodiment discriminator on a small set of trajectories from two distinct embodiments and evaluate its accuracy on a held-out test set.
  2. Implement PEAC with a simple RL backbone (e.g., DDPG) and a small number of embodiments, and evaluate its performance on a basic downstream task compared to training from scratch.
  3. Combine PEAC with an existing unsupervised RL method (e.g., LBS) and evaluate its performance on a more complex downstream task compared to the standalone method.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does PEAC's performance scale when dealing with embodiments that have vastly different morphologies and dynamics, such as combining a humanoid with a quadruped?
- Basis in paper: [inferred] The paper mentions that PEAC may struggle with extremely different embodiments and provides preliminary results with Walker-Humanoid setting showing limitations.
- Why unresolved: The paper only briefly touches on this challenging case with one example (Walker-Humanoid) and doesn't provide comprehensive analysis of how performance degrades as embodiment differences increase.
- What evidence would resolve it: Systematic experiments varying the degree of embodiment dissimilarity (e.g., different numbers of limbs, joint configurations, mass distributions) and measuring PEAC's performance across this spectrum.

### Open Question 2
- Question: What is the theoretical relationship between the number of pre-training embodiments and the generalization capability to unseen embodiments?
- Basis in paper: [inferred] The paper shows that more pre-training timesteps improve performance, but doesn't analyze how the diversity and quantity of training embodiments affects generalization.
- Why unresolved: The paper focuses on specific embodiment distributions but doesn't provide theoretical analysis or empirical studies on how coverage of embodiment space affects generalization.
- What evidence would resolve it: Experiments systematically varying the number and diversity of training embodiments, combined with theoretical analysis of coverage in embodiment space.

### Open Question 3
- Question: How does PEAC compare to other approaches that learn separate policies for each embodiment and then merge them?
- Basis in paper: [inferred] The paper proposes a unified policy approach but doesn't compare against ensemble methods or policy distillation approaches.
- Why unresolved: The paper focuses on comparing PEAC with single-policy unsupervised RL methods but doesn't explore alternative architectural approaches.
- What evidence would resolve it: Direct comparison between PEAC and methods that train separate embodiment-specific policies followed by policy fusion or distillation techniques.

### Open Question 4
- Question: What is the optimal trade-off between exploration and skill discovery in the cross-embodiment setting, and how does this change with different embodiment distributions?
- Basis in paper: [explicit] The paper introduces PEAC-LBS and PEAC-DIAYN as examples of combining PEAC with exploration and skill discovery, but doesn't systematically study the optimal balance.
- Why unresolved: The paper presents these as examples without providing analysis on when each approach is preferable or how to automatically tune the balance.
- What evidence would resolve it: Empirical studies varying the weight between exploration and skill discovery across different embodiment distributions, potentially with an adaptive weighting mechanism.

### Open Question 5
- Question: How does PEAC's embodiment discriminator learn to generalize to unseen embodiments during pre-training?
- Basis in paper: [explicit] The paper mentions that the embodiment discriminator learns embodiment-aware contexts but doesn't analyze how well it generalizes to unseen embodiments during pre-training.
- Why unresolved: The paper evaluates generalization of the final policy but doesn't examine whether the discriminator itself can recognize or categorize unseen embodiments during pre-training.
- What evidence would resolve it: Analysis of the discriminator's predictions on unseen embodiments during pre-training, including metrics like classification accuracy or clustering quality in embedding space.

## Limitations

- The information geometry analysis extending to CE-MDPs is primarily conceptual with limited empirical validation of how well it captures the true complexity of cross-embodiment learning.
- The paper does not thoroughly investigate the sensitivity of PEAC to hyperparameter choices, particularly the balance between intrinsic and extrinsic rewards during fine-tuning.
- The approach may struggle with extremely different embodiments that have vastly different morphologies and dynamics.

## Confidence

- **High Confidence**: The experimental results demonstrating PEAC's superior performance on downstream tasks, particularly the consistent improvements across multiple RL backbones and embodiment distributions.
- **Medium Confidence**: The core claim that cross-embodiment pre-training improves generalization, though the exact mechanism (embodiment-aware exploration vs. skill discovery) is not fully disentangled.
- **Low Confidence**: The theoretical analysis of CE-MDP complexity and its implications for skill vertices, as this remains largely conceptual without rigorous empirical validation.

## Next Checks

1. **Ablation Study on Intrinsic Reward Components**: Systematically remove or modify RCE, the embodiment discriminator, and the fixed prior to quantify each component's contribution to PEAC's performance. This would help validate the claimed mechanism of embodiment-aware exploration.

2. **Analysis of Zero-Shot Generalization**: Evaluate PEAC's performance on completely unseen embodiments (not present during pre-training) to test the true cross-embodiment generalization capability. This would validate whether PEAC learns truly embodiment-agnostic representations.

3. **Visualization of Learned Representations**: Use techniques like t-SNE or UMAP to visualize the state/action embeddings learned by PEAC across different embodiments. This would provide empirical evidence for whether the policy learns to distinguish and adapt to different embodiment contexts as claimed.