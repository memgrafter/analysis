---
ver: rpa2
title: Stealth edits to large language models
arxiv_id: '2406.12670'
source_url: https://arxiv.org/abs/2406.12670
tags:
- stealth
- prompt
- which
- edits
- prompts
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper presents a theoretical foundation for stealth editing\
  \ of large language models (LLMs), introducing methods to surgically correct specific\
  \ hallucinations without retraining or otherwise affecting model behavior. The key\
  \ insight is that a single metric\u2014the intrinsic dimensionality of a model's\
  \ feature space\u2014determines both editability and vulnerability to stealth attacks."
---

# Stealth edits to large language models

## Quick Facts
- arXiv ID: 2406.12670
- Source URL: https://arxiv.org/abs/2406.12670
- Authors: Oliver J. Sutton; Qinghua Zhou; Wei Wang; Desmond J. Higham; Alexander N. Gorban; Alexander Bastounis; Ivan Y. Tyukin
- Reference count: 40
- Key outcome: Theoretical foundation for stealth editing of LLMs using intrinsic dimensionality to determine editability and attack vulnerability

## Executive Summary
This paper presents a theoretical foundation for stealth editing of large language models, introducing methods to surgically correct specific hallucinations without retraining or affecting model behavior. The key insight is that a single metric—the intrinsic dimensionality of a model's feature space—determines both editability and vulnerability to stealth attacks. The authors propose two main approaches: in-place edits that modify existing network weights and jet-pack blocks that insert new optimized editing modules. Extensive experiments on Llama 3 8B, GPT-J, and Mamba 1.4B demonstrate the effectiveness of these approaches, with edit success rates above 95% in later layers and detector false positive rates near zero.

## Method Summary
The paper introduces two approaches for stealth editing: in-place edits that modify existing network weights and jet-pack blocks that insert new optimized editing modules. Both methods leverage the model's inherent normalization layers (like RMSNorm or LayerNorm) to create highly selective edits through linear separators in feature space. The key innovation is encoding a detector neuron that responds to specific prompts using a linear separator threshold. When activated, this neuron modifies the model's output through a gradient descent-optimized replacement vector. The theoretical framework connects edit selectivity and attack stealthiness to the intrinsic dimensionality of feature space, providing strict guarantees on performance.

## Key Results
- Edit success rates above 95% in later layers with false positive rates near zero for in-place edits
- Jet-pack blocks show similar performance with multiple simultaneous edits
- Stealth attacks with corrupted or contextually-embedded triggers show detector false positive rates below 0.1% in intermediate and later layers
- All modern LLM architectures are vulnerable to stealth attacks with computationally simple modifications

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Stealth edits work by inserting a linear separator in feature space that activates only for specific target prompts
- Mechanism: The edit algorithm constructs a detector neuron weight vector that responds positively to the target feature vector and negatively to other vectors. This is achieved by normalizing input vectors to the unit sphere and encoding a linear separator with a threshold θ. When activated, the detector neuron propagates a signal through the activation function σ to modify the model's output.
- Core assumption: Feature vectors of different prompts are well-separated in the model's feature space, allowing a linear separator to distinguish between them
- Evidence anchors:
  - [abstract] "The key insight is that a single metric—the intrinsic dimensionality of a model's feature space—determines both editability and vulnerability to stealth attacks."
  - [section 3] "The activation function σ provides near-zero response when the output from the linear separator is sufficiently negative due to (2)."
  - [corpus] Weak - no direct corpus evidence found for this specific mechanism
- Break condition: If feature vectors of different prompts overlap significantly in feature space, the linear separator cannot reliably distinguish between them

### Mechanism 2
- Claim: The intrinsic dimensionality of feature space determines edit selectivity and attack stealthiness
- Mechanism: The metric n(D, δ) measures how separable data points are in feature space. Higher intrinsic dimensionality means better separation between feature vectors, making edits more selective and attacks harder to detect. The probability of false positives decreases exponentially with intrinsic dimensionality.
- Core assumption: Feature vectors from random prompts form a distribution with measurable intrinsic dimensionality
- Evidence anchors:
  - [abstract] "The key insight is that a single metric—the intrinsic dimensionality of a model's feature space—determines both editability and vulnerability to stealth attacks."
  - [section 4] "Theorem 2 shows that when randomly sampled test prompts produce a feature cloud with high intrinsic dimensionality, the probability of activating a stealth attack with any fixed trigger is very small."
  - [corpus] Weak - no direct corpus evidence found for this specific mechanism
- Break condition: If feature vectors are concentrated in a low-dimensional subspace, the intrinsic dimensionality metric becomes small and edits/attacks become detectable

### Mechanism 3
- Claim: Randomizing triggers makes stealth attacks harder to detect by exploiting feature space geometry
- Mechanism: By randomly corrupting prompts or adding unexpected context, the attacker creates a distribution of trigger feature vectors. Theorem 3 shows that any fixed test prompt has exponentially decreasing probability of activating any trigger from this distribution as intrinsic dimensionality increases.
- Core assumption: Attackers can control trigger distribution to maximize feature space intrinsic dimensionality
- Evidence anchors:
  - [abstract] "Stealth attacks with corrupted or contextually-embedded triggers show detector false positive rates below 0.1% in intermediate and later layers."
  - [section 3] "The attacker may also randomise the trigger. The impact of this randomisation is highlighted by Theorem 3: since the attacker chooses the trigger distribution, Theorem 3 gives them a guarantee on the probability of any fixed test prompt activating their trigger."
  - [corpus] Weak - no direct corpus evidence found for this specific mechanism
- Break condition: If randomized triggers cluster in low-dimensional regions of feature space, they become easier to detect

## Foundational Learning

- Concept: Intrinsic dimensionality and separability-based dimensionality
  - Why needed here: This metric is the theoretical foundation that determines both editability and vulnerability to attacks. Understanding it is crucial for predicting when stealth edits will work.
  - Quick check question: What does it mean when n(D, δ) approaches infinity for a distribution D at threshold δ?

- Concept: Feature normalization and projection to unit sphere
  - Why needed here: All modern LLM architectures use normalization layers (RMSNorm/LayerNorm) that project features to the surface of a sphere, making linear separators effective for detecting specific prompts.
  - Quick check question: Why does projecting feature vectors to the unit sphere make linear separators more effective for detecting specific prompts?

- Concept: Linear separability and hyperplane classification
  - Why needed here: Stealth edits rely on encoding linear separators into weight matrices to distinguish target prompts from others. Understanding linear separability is key to grasping how edits work.
  - Quick check question: What geometric property of feature space makes linear separators effective for detecting specific prompts in normalized models?

## Architecture Onboarding

- Component map: The stealth edit system consists of three main components: (1) the detector neuron that identifies target prompts using linear separators, (2) the normalization functions (RMSNorm/LayerNorm) that project features to the unit sphere, and (3) the weight modification algorithm that inserts edits into existing layers or jet-pack blocks.

- Critical path: The critical path for implementing a stealth edit is: (1) extract feature vector from target prompt, (2) construct linear separator with appropriate threshold, (3) find replacement output vector using gradient descent, (4) modify weights in target layer or jet-pack block.

- Design tradeoffs: In-place edits are computationally cheaper but require modifying existing neurons, while jet-pack blocks add new parameters but avoid neuron pruning. The choice depends on whether model size constraints or edit selectivity is more important.

- Failure signatures: Low edit success rates in early layers, high detector false positive rates, excessive perplexity ratio changes, or failure to find replacement output vectors all indicate problems with the edit implementation.

- First 3 experiments:
  1. Test in-place edit on a simple hallucination with θ=0.005 on layer 17 of Llama to verify basic functionality
  2. Measure detector false positive rate using 20,000 Wikipedia prompts to validate selectivity
  3. Implement jet-pack block with 1000 edits to test scalability and compare with in-place edits

## Open Questions the Paper Calls Out
None identified in the paper.

## Limitations
- Theoretical guarantees rely on idealized feature space geometry assumptions
- Intrinsic dimensionality metric may not accurately reflect real-world feature distributions
- Framework assumes perfect normalization layers without accounting for implementation variations
- Scalability claims for jet-pack blocks with hundreds of simultaneous edits are uncertain

## Confidence
- High confidence in the basic stealth edit mechanism using linear separators in normalized feature space
- Medium confidence in the theoretical bounds on false positive rates based on intrinsic dimensionality
- Low confidence in the scalability claims for jet-pack blocks with hundreds of simultaneous edits

## Next Checks
1. **Feature space validation**: Measure the empirical intrinsic dimensionality of feature vectors from diverse prompt distributions and compare with theoretical predictions. Test whether the exponential decay in false positive rates actually holds across different model architectures and prompt types.

2. **Cross-architecture robustness**: Implement the stealth edit methods on additional architectures beyond Llama 3 8B, GPT-J, and Mamba 1.4B. Test whether the theoretical guarantees hold for transformer variants with different normalization schemes and attention mechanisms.

3. **Practical attack feasibility**: Attempt to detect stealth attacks using existing model inspection tools and fine-tuning-based detection methods. Measure whether the proposed randomization techniques actually make attacks harder to detect in practice versus just in theory.