---
ver: rpa2
title: 'JuStRank: Benchmarking LLM Judges for System Ranking'
arxiv_id: '2412.09569'
source_url: https://arxiv.org/abs/2412.09569
tags:
- judge
- judges
- system
- numeric
- anchor
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces JuStRank, the first large-scale benchmark
  for evaluating LLM-based judges on their ability to rank systems (models or configurations)
  rather than just individual responses. Previous benchmarks focused on instance-level
  judge performance, overlooking how judges perform when aggregating judgments across
  multiple responses from different systems.
---

# JuStRank: Benchmarking LLM Judges for System Ranking

## Quick Facts
- arXiv ID: 2412.09569
- Source URL: https://arxiv.org/abs/2412.09569
- Reference count: 40
- One-line primary result: Instance-level judge performance does not directly translate to system-level ranking ability

## Executive Summary
This paper introduces JuStRank, the first large-scale benchmark for evaluating LLM-based judges on their ability to rank systems rather than just individual responses. The authors collected 1.5 million judgment scores from 48 judge realizations over 63 systems responding to 500 instructions from the Arena Hard dataset. They find that instance-level judge performance doesn't predict system-level ranking ability, with some 8B-parameter reward models performing on par with much larger LLMs. The analysis reveals distinct judge behaviors at the system level, including varying degrees of decisiveness and system-specific biases that are correlated with ranking quality but not with each other.

## Method Summary
The study evaluates 48 judge realizations (8 LLM models × 4 realizations + 6 reward models) on their ability to rank 63 systems based on responses to 500 instructions from Arena Hard dataset. Judges score individual responses, which are then aggregated using Win-rate, Mean, Median, or Bradley-Terry methods to create system rankings. These rankings are compared against a gold ranking from Chatbot Arena using Kendall's Tau correlation as the primary metric.

## Key Results
- Instance-level judge performance does not directly translate to system-level ranking ability
- Some 8B-parameter reward models perform on par with much larger LLMs for system ranking
- Judge traits (decisiveness and system bias) are correlated with ranking quality but not with each other

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLM-based judges can accurately rank systems even when individual response judgments contain errors, provided the errors are distributed evenly across systems.
- Mechanism: System-level ranking aggregates multiple instance judgments per system, smoothing out individual errors. If a judge makes random mistakes across different systems, the aggregated scores preserve the relative system quality.
- Core assumption: Errors in individual judgments are independent of the source system and randomly distributed.
- Evidence anchors:
  - [abstract]: "instance-level judge performance does not directly translate to system-level ranking ability"
  - [section 2]: "instance-level judge evaluations focus on how many errors the judge makes, and do not address the distribution of these errors across systems"
- Break condition: If judge errors systematically favor or disfavor certain systems (bias), the aggregated ranking becomes distorted regardless of overall accuracy.

### Mechanism 2
- Claim: LLM realizations that ask for absolute quality scores (Numeric/Likert) produce better system rankings than comparative realizations.
- Mechanism: Absolute scoring allows judges to calibrate their judgments independently for each response, avoiding anchoring effects that occur when comparing to a reference system.
- Core assumption: Judges can provide consistent absolute quality estimates across diverse responses.
- Evidence anchors:
  - [section 5.1]: "both Numeric and Likert realizations – compared to Anchor and TokenProbs – is statistically significant"
  - [section 4.2.2]: "we see that it is also affected by the characteristics of the judge"
- Break condition: If the judge's calibration is poor or inconsistent, absolute scoring degrades into unreliable ordinal rankings.

### Mechanism 3
- Claim: Decisive judges (those with extreme win-rate predictions) can improve system separation and ranking quality.
- Mechanism: By amplifying differences between strong and weak systems, decisive judges create larger score gaps that make system rankings more distinct and stable.
- Core assumption: Amplifying true quality differences helps distinguish systems more clearly than preserving nuanced intermediate judgments.
- Evidence anchors:
  - [section 6.1]: "some judges exhibit unique prediction patterns, yielding win-rates that are consistently closer to the extremes (0.0 / 1.0)"
  - [section 6.3]: "each of these traits... is correlated to the ranking quality τ, with r = 0.55 for the α decisiveness measure"
- Break condition: If decisiveness amplifies noise or systematic biases rather than true quality differences, it degrades ranking accuracy.

## Foundational Learning

- Concept: Bradley-Terry model for pairwise comparisons
  - Why needed here: The gold ranking and aggregation methods rely on modeling system quality as the probability of winning pairwise comparisons
  - Quick check question: In Bradley-Terry, if system A beats system B 70% of the time and B beats C 70% of the time, what is the expected win rate of A over C?

- Concept: Kendall's Tau correlation for ranking agreement
  - Why needed here: The primary metric for judge quality is how well their system ranking matches the human ranking
  - Quick check question: If two rankings have perfect agreement, what is their Kendall's Tau value?

- Concept: Beta distribution fitting for calibration analysis
  - Why needed here: Used to quantify how "decisive" judges are by fitting their win-rate predictions to a cumulative distribution function
  - Quick check question: What does an alpha parameter of 1 in the beta distribution fit represent in terms of judge behavior?

## Architecture Onboarding

- Component map: 500 instructions -> 63 systems -> 48 judge realizations -> Score Matrix -> Aggregation (Win-rate/Mean/Median/BT) -> Rankings -> Kendall's Tau correlation with gold ranking
- Critical path: Judge → Score Matrix → Aggregation → Ranking → Correlation with gold ranking
- Design tradeoffs:
  - Choice of realization vs. choice of model: Realizations have nearly as much impact as different models
  - Aggregation method: Win-rate and BT perform similarly; mean aggregation is simpler but less robust to ties
  - Number of responses per system: More responses reduce variance but increase computational cost
- Failure signatures:
  - Low Kendall's Tau correlation with gold ranking
  - High variance in judge scores across similar systems
  - Systematic bias toward specific systems (heat map shows concentrated bias)
  - Decisiveness parameter α < 1 indicates indecisiveness rather than appropriate confidence
- First 3 experiments:
  1. Run all judges with Numeric realization and BT aggregation to establish baseline performance
  2. Test each judge realization (Numeric, Likert, Anchor, TokenProbs) separately to identify best performers
  3. Compare Mean vs. Median aggregation for judges with discrete score distributions to observe tie-handling effects

## Open Questions the Paper Calls Out

- Question: How do task-specific LLM judges perform on system-level ranking compared to general-purpose judges?
  - Basis in paper: [inferred] The paper notes it cannot draw conclusions on task-specific judge behavior and focuses on general-purpose LLM usage, suggesting this as an important avenue for future work.
  - Why unresolved: The study was limited to heterogeneous datasets of general-purpose user instructions, preventing analysis of domain-specific judge behavior.
  - What evidence would resolve it: Direct comparison of task-specific judges (e.g., medical, legal, or programming domains) on system-level rankings using domain-specific benchmarks and instructions.

- Question: What is the optimal combination of judge realization and aggregation method for system-level ranking?
  - Basis in paper: [explicit] The authors note that these decisions substantially affect system-level judgments and may change model selection or research conclusions, but do not identify optimal combinations.
  - Why unresolved: While the paper identifies that realization and aggregation matter, it doesn't systematically explore all possible combinations or their interactions.
  - What evidence would resolve it: Systematic testing of all realization-aggregation pairs across diverse datasets, measuring ranking quality, robustness, and computational efficiency to identify optimal configurations.

- Question: Can we train dedicated system-level judges that outperform general-purpose LLM judges and reward models?
  - Basis in paper: [explicit] The authors suggest exploring training dedicated system-level judges as an important avenue for future work.
  - Why unresolved: The paper uses existing judges without exploring whether specialized training for system-level tasks would improve performance.
  - What evidence would resolve it: Head-to-head comparisons between dedicated system-level judges (trained specifically on system ranking tasks) and the best-performing general-purpose judges across multiple benchmarks and datasets.

## Limitations

- The benchmark covers only 63 systems and 500 instructions from a single dataset (Arena Hard), which may limit generalizability to other domains or system populations.
- The study doesn't examine how judge performance scales with different numbers of responses per system or how performance varies across different instruction types.
- Reliance on Chatbot Arena's gold ranking as ground truth introduces potential measurement error and may not fully capture "true" system quality.

## Confidence

**High confidence** in the core finding that instance-level judge performance doesn't directly predict system-level ranking ability, supported by extensive empirical results across 48 judge realizations and clear statistical differences in performance metrics.

**Medium confidence** in the relative performance of different judge realizations (Numeric/Likert vs Anchor/TokenProbs), as this comparison depends on the specific implementation details and prompts used, which aren't fully specified in the paper.

**Medium confidence** in the identified judge traits (decisiveness and system bias) as independent factors affecting ranking quality, though the correlation analysis shows these relationships but doesn't establish causation or explore underlying mechanisms.

## Next Checks

1. **Prompt sensitivity analysis**: Run the same judge realizations with systematically varied prompts to quantify how sensitive system rankings are to prompt engineering, testing whether the observed performance differences persist across prompt variations.

2. **Cross-dataset validation**: Apply the benchmark to a completely different instruction set (e.g., FLASK or Alpaca) to verify that the identified patterns in judge behavior and the disconnect between instance and system performance generalize beyond Arena Hard.

3. **Aggregation method comparison**: Implement and test alternative ranking aggregation methods (such as TrueSkill or Plackett-Luce models) to determine if the observed judge behaviors and performance differences are artifacts of the Bradley-Terry approach or persist across different ranking methodologies.