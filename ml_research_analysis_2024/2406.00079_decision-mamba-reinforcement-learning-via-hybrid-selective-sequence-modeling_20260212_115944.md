---
ver: rpa2
title: 'Decision Mamba: Reinforcement Learning via Hybrid Selective Sequence Modeling'
arxiv_id: '2406.00079'
source_url: https://arxiv.org/abs/2406.00079
tags:
- mamba
- transformer
- dm-h
- learning
- tasks
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates the application of Mamba, a selective state
  space model with linear complexity, to improve the efficiency of in-context reinforcement
  learning (RL). While transformer-based methods like Decision Transformer excel in
  effectiveness, they suffer from quadratic computational costs in long-horizon tasks
  due to self-attention.
---

# Decision Mamba: Reinforcement Learning via Hybrid Selective Sequence Modeling

## Quick Facts
- arXiv ID: 2406.00079
- Source URL: https://arxiv.org/abs/2406.00079
- Reference count: 40
- This paper investigates the application of Mamba, a selective state space model with linear complexity, to improve the efficiency of in-context reinforcement learning (RL).

## Executive Summary
This paper addresses the efficiency limitations of transformer-based reinforcement learning methods like Decision Transformer by introducing Decision Mamba-Hybrid (DM-H), which combines Mamba's linear complexity with transformer's prediction quality. The hybrid architecture uses Mamba to generate high-value sub-goals from long-term context while transformers predict actions conditioned on these sub-goals and short-term context. Experiments on D4RL, Grid World, and Tmaze benchmarks demonstrate that DM-H achieves up to 28x faster online inference on long tasks while maintaining or exceeding the performance of transformer baselines.

## Method Summary
Decision Mamba-Hybrid (DM-H) combines a Mamba module that processes across-episodic contexts to generate sub-goals with a transformer module that predicts actions conditioned on these sub-goals and short-term context. The method leverages Mamba's linear complexity for efficient long-term memory processing while using transformers for high-quality local action prediction. High-value states are selected from offline data and transformed into sub-goal embeddings to align transformer predictions with task objectives. The architecture is trained end-to-end using supervised losses on trajectories sorted by return.

## Key Results
- DM-H achieves up to 28x faster online inference compared to transformer baselines on long-horizon tasks
- The hybrid architecture outperforms both pure transformer and pure Mamba baselines in effectiveness and efficiency
- DM-H maintains superior performance on D4RL, Grid World, and Tmaze benchmarks while reducing computational costs

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Mamba's linear complexity with selective state spaces enables efficient long-term context processing while transformers handle high-quality action prediction locally.
- Mechanism: Mamba generates sub-goals from long-term trajectories, providing compressed memory representations. Transformers use these sub-goals as prompts for local action prediction, avoiding quadratic attention costs on entire trajectories.
- Core assumption: Sub-goals contain sufficient information for transformer to predict accurate actions within local context windows.
- Evidence anchors:
  - [abstract] "DM-H first generates high-value sub-goals from long-term memory through the Mamba model. Then, we use sub-goals to prompt the transformer"
  - [section] "DM-H links Mamba and the transformer through the sub-goal z to efficiently recall long-term contexts while ensuring high-quality predictions"
  - [corpus] Weak evidence - related papers focus on Mamba in RL but don't explicitly validate the hybrid sub-goal mechanism
- Break condition: If sub-goals fail to capture task-relevant information, transformer predictions degrade despite local context processing.

### Mechanism 2
- Claim: The hybrid architecture preserves transformer's effectiveness for sequential decision-making while Mamba handles long-term dependencies efficiently.
- Mechanism: Mamba processes across-episodic contexts independently, generating sub-goals at regular intervals. Transformer maintains sequential attention within local windows, capturing state-action relationships better than Mamba's independent processing.
- Core assumption: Sequential relationships between states are more important for action prediction than long-term context compression.
- Evidence anchors:
  - [abstract] "Regarding efficiency, the online testing of DM-H in the long-term task is 28× times faster than the transformer-based baselines"
  - [section] "Since states in RL tasks commonly exhibit sequential relationships, the attention mechanism is more suitable for capturing the information between states"
  - [corpus] Moderate evidence - Mamba vs Transformer comparison shows efficiency gains but transformer superiority in effectiveness
- Break condition: If task requires extensive long-term planning beyond sub-goal intervals, sequential relationships become insufficient for good decisions.

### Mechanism 3
- Claim: Valuable sub-goals from offline data align transformer predictions with task objectives, improving sample efficiency.
- Mechanism: Offline trajectories provide high-value states selected via weighted average of accumulated rewards. These states are mapped through linear layers to sub-goal dimensions and used to reconstruct local sequences, creating stronger supervision signals.
- Core assumption: High-value states identified offline correlate with optimal decision points in online tasks.
- Evidence anchors:
  - [section] "We select extra high-value states from the offline data and transform them into sub-goals z to align actions generated by the transformer"
  - [section] "We can model this behavior by finding states with high accumulated reward values in the trajectory"
  - [corpus] Missing evidence - no related papers discuss valuable sub-goal selection mechanism
- Break condition: If offline data doesn't represent target task distribution, valuable sub-goals may mislead rather than guide the policy.

## Foundational Learning

- Concept: State Space Models (SSM) and their selective state space variant (Mamba)
  - Why needed here: Understanding how Mamba processes sequences linearly vs transformer's quadratic attention
  - Quick check question: What architectural component allows Mamba to maintain linear complexity while processing long sequences?

- Concept: Transformer self-attention and its quadratic complexity
  - Why needed here: Critical for understanding why pure transformer approaches don't scale to long-horizon tasks
  - Quick check question: How does the attention complexity scale with sequence length in transformers vs Mamba?

- Concept: Reinforcement learning as sequence modeling
  - Why needed here: Decision Transformer framework treats RL as sequential prediction, enabling transformer application to RL
  - Quick check question: What are the three main input modalities for sequence modeling in Decision Transformer?

## Architecture Onboarding

- Component map:
  - Mamba module -> generates sub-goals from across-episodic context
  - Linear layer -> maps high-value states to sub-goal embedding space
  - Transformer module -> predicts actions conditioned on sub-goals and local context
  - Context buffer -> stores n trajectories sorted by return for across-episodic processing

- Critical path:
  1. Sample and sort n trajectories by return
  2. Mamba generates sub-goals from across-episodic context
  3. Select valuable sub-goals from offline data
  4. Transformer predicts actions conditioned on sub-goals
  5. Execute actions and update context buffer

- Design tradeoffs:
  - c parameter: Larger values improve efficiency but reduce transformer's local context
  - Sub-goal frequency: More frequent sub-goals improve guidance but reduce Mamba efficiency gains
  - Context size n: More trajectories provide better long-term memory but increase computational cost

- Failure signatures:
  - Performance degrades when c > 20 for complex tasks (insufficient local context)
  - Training instability when offline data quality is poor (misleading sub-goals)
  - Memory issues when n > 10 (across-episodic context too large for Mamba)

- First 3 experiments:
  1. Verify Mamba generates meaningful sub-goals by visualizing them against trajectory returns
  2. Test transformer predictions with and without sub-goals to measure guidance effectiveness
  3. Benchmark efficiency gains by varying c parameter on medium-length tasks

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the performance gain from valuable sub-goals scale with task horizon or is it consistent across different task lengths?
- Basis in paper: [explicit] The ablation study shows sub-goals significantly improve DM-H's performance in both Grid World and D4RL tasks, but the paper doesn't analyze whether this benefit changes with task horizon.
- Why unresolved: The paper reports ablation results across different environments but doesn't systematically vary task horizon while holding other factors constant to isolate the effect of sub-goals on longer vs shorter tasks.
- What evidence would resolve it: Experiments showing DM-H performance with and without valuable sub-goals across a range of task horizons, particularly comparing very long tasks (like the 10k step Tmaze) versus shorter ones.

### Open Question 2
- Question: What is the optimal value of the hyperparameter "c" and how does it vary across different types of environments or tasks?
- Basis in paper: [explicit] The paper mentions sensitivity analysis on "c" showing DM-H performs well within 10 ≤ c ≤ 20 range, but doesn't explore whether optimal values differ by environment type or task characteristics.
- Why unresolved: The paper uses fixed values of c (20 for D4RL and Large Grid World, 5 for conventional Grid World) without exploring whether these are optimal or task-specific.
- What evidence would resolve it: Systematic experiments varying "c" across different environment types (grid worlds, continuous control, maze tasks) to identify whether optimal values correlate with task properties like state space dimensionality, action space type, or reward sparsity.

### Open Question 3
- Question: How does DM-H's performance compare when using different methods for selecting valuable sub-goals beyond the weighted average of accumulated rewards?
- Basis in paper: [inferred] The paper describes one specific method for selecting sub-goals (weighted average of accumulated rewards divided by distance), but doesn't test alternative selection criteria or compare performance.
- Why unresolved: The paper implements and evaluates only one sub-goal selection strategy without exploring whether other methods (like value function estimates, visitation frequency, or information-theoretic measures) might perform better.
- What evidence would resolve it: Comparative experiments testing DM-H with different sub-goal selection methods (e.g., using learned value functions, entropy-based measures, or expert demonstrations) across the same benchmark tasks.

## Limitations
- The paper provides strong empirical results but limited mechanistic analysis of why the hybrid architecture works
- Limited comparison with other hybrid approaches beyond pure transformer and Mamba baselines
- Sub-goal mechanism effectiveness across diverse task distributions lacks rigorous validation

## Confidence
- **High Confidence**: The efficiency claims regarding computational speedup (28×) and the fundamental advantage of Mamba's linear complexity over transformer attention
- **Medium Confidence**: The effectiveness claims for long-horizon tasks, as results show strong performance but could benefit from more diverse task types and ablation studies
- **Low Confidence**: The theoretical justification for why sub-goals specifically improve performance beyond what local transformer attention alone could achieve

## Next Checks
1. **Ablation study**: Remove the Mamba sub-goal generation entirely and compare transformer performance with full attention vs. the hybrid approach to isolate the contribution of each component
2. **Sub-goal quality analysis**: Visualize and correlate generated sub-goals with trajectory returns across different task types to verify they capture meaningful long-term information
3. **Scalability testing**: Evaluate performance and efficiency on tasks significantly longer than those tested (e.g., 1000+ step horizons) to verify the claimed advantages hold at scale