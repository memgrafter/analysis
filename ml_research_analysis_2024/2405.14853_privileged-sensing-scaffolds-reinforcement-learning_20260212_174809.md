---
ver: rpa2
title: Privileged Sensing Scaffolds Reinforcement Learning
arxiv_id: '2405.14853'
source_url: https://arxiv.org/abs/2405.14853
tags:
- privileged
- policy
- target
- learning
- scaffolded
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "Scaffolder enables reinforcement learning agents to exploit privileged\
  \ sensory information available only during training to improve policies that must\
  \ operate with limited sensing at test time. The method enhances standard model-based\
  \ RL components\u2014world models, critics, exploration policies, and representation\
  \ learning\u2014using these privileged observations."
---

# Privileged Sensing Scaffolds Reinforcement Learning

## Quick Facts
- arXiv ID: 2405.14853
- Source URL: https://arxiv.org/abs/2405.14853
- Reference count: 30
- Primary result: Scaffolder improves RL policies trained with privileged sensing, bridging 79% of the performance gap between impoverished and privileged sensing across ten robotic tasks.

## Executive Summary
Scaffolder is a model-based reinforcement learning method that leverages privileged sensory information available only during training to improve policies that must operate with limited sensing at test time. The approach enhances standard MBRL components—world models, critics, exploration policies, and representation learning—using privileged observations to synthesize improved target policies. Evaluated on a new Sensory Scaffolding Suite of ten diverse robotic tasks, Scaffolder consistently outperforms relevant baselines and demonstrates that training-time privileged sensing can significantly improve real-world robot policy performance even when target sensors are severely limited.

## Method Summary
Scaffolder extends DreamerV3 by incorporating privileged observations during training to scaffold four key components: world models, critics, exploration policies, and representation learning. The method maintains parallel target and scaffolded components, where the scaffolded components use privileged observations (o+) while target components use only limited observations (o-). Key innovations include Nested Latent Imagination (NLI) for improved credit assignment, a privileged exploration policy for better data collection, and representation learning objectives that force target representations to encode information from privileged observations. The method uses a transdecoder to map between privileged and target latent spaces, enabling the critic to evaluate trajectories with privileged information while optimizing the target policy.

## Key Results
- Bridges 79% of the performance gap between impoverished and privileged sensing on ten diverse robotic tasks
- Outperforms baseline methods including DreamerV3, Informed Dreamer, DreamerV3+BC, Guided Observability, RMA+, and AAC
- Shows task-specific contributions from different scaffolded elements, with privileged components improving credit assignment and learning signals
- Demonstrates that privileged sensing can significantly improve policy performance in scenarios with severely limited target sensors

## Why This Works (Mechanism)

### Mechanism 1: Nested Latent Imagination (NLI) Improves Credit Assignment
The scaffolded world model enables more accurate TD(λ) return estimates by providing better credit assignment through privileged observations. NLI translates scaffolded latents into target latents using a transdecoder, allowing the critic to evaluate trajectories with privileged information unavailable to the target policy. This creates more accurate reward and value estimates for policy optimization.

### Mechanism 2: Scaffolded Exploration Policy Provides Better Training Data
The privileged exploration policy collects higher-quality data that improves the target policy's learning trajectory. π+ trained with privileged observations can solve tasks more quickly and gather task-relevant states that the target policy π- cannot reach, populating the replay buffer with valuable experiences.

### Mechanism 3: Privileged Representation Learning Objective Encodes Critical Information
Training the target representation to predict privileged observations forces it to encode task-relevant information that would otherwise be inaccessible. The auxiliary decoder from z- to o+ encourages the target latent representation to capture information present in privileged observations, making the policy more effective despite limited sensing.

## Foundational Learning

- Concept: Partially Observable Markov Decision Processes (POMDPs)
  - Why needed here: The paper operates in a setting where the agent must act with limited observations (o-) while having access to privileged observations (o+) during training
  - Quick check question: What is the key difference between an MDP and a POMDP, and how does this paper's setting relate to that difference?

- Concept: Model-Based Reinforcement Learning (MBRL)
  - Why needed here: Scaffolder builds on DreamerV3, a MBRL method that learns a world model to generate synthetic experience for policy training
  - Quick check question: How does the world model in DreamerV3 enable more efficient policy learning compared to model-free methods?

- Concept: Actor-Critic Methods
  - Why needed here: The method uses both an actor (policy) and critic (value function) that are trained using TD(λ) returns computed from the world model
  - Quick check question: What is the role of the critic in actor-critic methods, and how does Scaffolder modify this role using privileged observations?

## Architecture Onboarding

- Component map: Target World Model (o-) -> Target Policy (z-) -> Target Representation (z-) <- Auxiliary Decoder (z- to o+) -> Privileged Observations (o+)
  - Scaffolded World Model (o+) -> Scaffolded Exploration Policy (z+) -> Transdecoder (z+ to o-) -> Target Representation (z-)

- Critical path:
  1. Collect data using both target and scaffolded policies
  2. Train both world models on their respective observations
  3. Generate trajectories using NLI (Nested Latent Imagination)
  4. Compute TD(λ) returns using scaffolded components
  5. Update target policy to maximize scaffolded TD(λ) return
  6. Update representations and other components

- Design tradeoffs:
  - Training two world models increases computational cost but improves accuracy
  - Using privileged information for exploration may create exploration patterns that don't transfer to the target policy
  - The transdecoder introduces potential information loss that could bias value estimates

- Failure signatures:
  - Target policy performs worse than unprivileged baseline (indicates privileged information is being used incorrectly)
  - High variance in learning curves (suggests instability from mismatched privileged/target representations)
  - Slow convergence (indicates the privileged information isn't providing useful training signals)

- First 3 experiments:
  1. Run Scaffolder on Blind Pick with varying transdecoder architectures to test information preservation
  2. Compare performance when using only privileged exploration vs. only privileged critic vs. both
  3. Test the impact of removing the privileged representation learning objective on final policy performance

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What are the computational and resource implications of running Scaffolder compared to DreamerV3 and other baselines?
- Basis in paper: Section D "Runtime and Resource Comparison"
- Why unresolved: While the paper provides approximate runtimes in hours for each method, it does not discuss the exact resource requirements (e.g., GPU memory, CPU usage) or the impact of training additional models on overall training efficiency.
- What evidence would resolve it: Detailed profiling of resource usage (GPU memory, CPU, etc.) for Scaffolder and baselines during training, including memory consumption and training time per step.

### Open Question 2
- Question: How does Scaffolder perform in real-world robotics scenarios where sensor noise and calibration issues are prevalent?
- Basis in paper: Section G "Scaffolder with rewards estimated from privileged sensors"
- Why unresolved: The paper only evaluates Scaffolder in simulated environments, even when introducing noise in reward estimation. Real-world scenarios may present additional challenges such as sensor calibration, latency, and environmental variability.
- What evidence would resolve it: Experimental results on real-world robotic platforms with various sensor configurations and noise levels, demonstrating Scaffolder's robustness and adaptability.

### Open Question 3
- Question: Can the privileged information be used to improve the target policy's exploration strategy beyond the current implementation of a separate exploration policy?
- Basis in paper: Section 3 "Scaffolded Exploration Policy"
- Why unresolved: The paper uses a separate scaffolded exploration policy to collect data, but it does not explore other ways to leverage privileged information for exploration, such as intrinsic motivation or curiosity-driven exploration.
- What evidence would resolve it: Experiments comparing Scaffolder's performance with alternative exploration strategies that utilize privileged information, such as curiosity-driven exploration or information gain maximization.

## Limitations
- Only evaluated in simulated environments without real-robot validation
- Assumes privileged observations are available throughout training, which may not hold in all scenarios
- Computational overhead from maintaining dual world models and additional scaffolded components

## Confidence
- Confidence in performance claims: Medium (extensive empirical evaluation but limited to simulation)
- Confidence in mechanism claims: Medium (theoretical justification but limited direct evidence)
- Confidence in generalization claims: Low (only tested on simulated tasks, real-world transfer unknown)

## Next Checks
1. Evaluate Scaffolder on tasks with varying degrees of information asymmetry between privileged and target observations
2. Test robustness when privileged observations are occasionally unavailable during training
3. Measure the computational overhead of the scaffolded components compared to standard DreamerV3