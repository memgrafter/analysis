---
ver: rpa2
title: "$\u03C8$DAG: Projected Stochastic Approximation Iteration for DAG Structure\
  \ Learning"
arxiv_id: '2410.23862'
source_url: https://arxiv.org/abs/2410.23862
tags:
- golem
- dagma
- gradient
- runtime
- computations
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "The paper addresses the challenge of learning Directed Acyclic\
  \ Graph (DAG) structures from data, which is fundamental in fields like causal inference\
  \ and machine learning. The proposed method, \u03C8DAG, reformulates the DAG learning\
  \ problem as a stochastic optimization task, using Stochastic Approximation and\
  \ Stochastic Gradient Descent (SGD) to handle the vast combinatorial search space."
---

# $ψ$DAG: Projected Stochastic Approximation Iteration for DAG Structure Learning

## Quick Facts
- arXiv ID: 2410.23862
- Source URL: https://arxiv.org/abs/2410.23862
- Authors: Klea Ziu; Slavomír Hanzely; Loka Li; Kun Zhang; Martin Takáč; Dmitry Kamzolov
- Reference count: 40
- Primary result: Proposed method significantly outperforms existing methods like NOTEARS, GOLEM, and DAGMA in scalability and runtime, handling graphs with up to 10,000 nodes

## Executive Summary
The paper addresses the challenge of learning Directed Acyclic Graph (DAG) structures from data, which is fundamental in fields like causal inference and machine learning. The proposed method, ψDAG, reformulates the DAG learning problem as a stochastic optimization task, using Stochastic Approximation and Stochastic Gradient Descent (SGD) to handle the vast combinatorial search space. ψDAG introduces new projection methods to efficiently enforce DAG constraints, ensuring convergence to a feasible local minimum. The method is well-suited for large-scale problems due to its low iteration complexity and improved computational efficiency.

## Method Summary
ψDAG reformulates DAG structure learning as a stochastic optimization problem. The method uses a two-phase framework: Phase 1 applies unconstrained SGD to explore the solution space, then projects the result to a feasible DAG using a topological sorting heuristic. Phase 2 runs constrained SGD while preserving the topological ordering. The key innovation is the projection method, which finds the "closest" topological sorting and removes edges not permitted by this ordering, reducing the constraint space from $2^{d^2-d}$ to $d!$. This approach enables scalable learning of DAGs with up to 10,000 nodes.

## Key Results
- Significantly outperforms existing methods (NOTEARS, GOLEM, DAGMA) in runtime and scalability on synthetic datasets
- Handles graphs with up to 10,000 nodes compared to baselines that struggle with 3,000+ nodes
- On the causal protein signaling network dataset, achieves superior performance in structural Hamming distance, true positive rate, and false positive rate

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Reformulating the DAG learning problem as a stochastic optimization problem reduces the constraint space from $2^{d^2-d}$ to $d!$, enabling scalable learning.
- Mechanism: By leveraging a fixed topological ordering of vertices, the algorithm can optimize over the space of "full DAGs" which are maximal DAGs consistent with that ordering, instead of exploring the entire combinatorial space of possible DAGs.
- Core assumption: The true DAG has a topological ordering that can be approximated by a projection step.
- Evidence anchors:
  - [abstract] "The key innovation is the projection method, which finds the 'closest' topological sorting and removes edges not permitted by this ordering, reducing the space of constraints from $2^{d^2-d}$ to $d!$"
  - [section 4.1] "From a discrete optimization perspective, this approach significantly reduces the space of constraints from $2^{d^2-d}$ to $d!$"
- Break condition: If the projection method consistently returns incorrect topological orderings, the algorithm may converge to suboptimal solutions.

### Mechanism 2
- Claim: The two-phase optimization framework (unconstrained SGD followed by projection and constrained SGD) ensures convergence to a local minimum of the original problem.
- Mechanism: Phase 1 explores the unconstrained space to find a promising region, Phase 2 projects to a feasible DAG and optimizes within that constrained space while preserving the topological ordering.
- Core assumption: The unconstrained optimization in Phase 1 provides iterates that are "close enough" to the true DAG for the projection to be effective.
- Evidence anchors:
  - [section 4] "Instead of strictly enforcing DAG constraints throughout the entire iteration process, we propose a novel, scalable optimization framework that consists of three main steps"
  - [section 4.1] "Theorem 2. For an L1-smooth function F(W) = Ex∼D [l(W; x)], Algorithm (1), with access to σ1-stochastic gradients, converges to a local minimum of problem(11) at the rate E[F(WT) - F(W*)] ≤ O(σ1R√T + L1R²/T)"
- Break condition: If the projection step is too aggressive or too lenient, it may prevent the algorithm from finding good solutions.

### Mechanism 3
- Claim: The projection method based on row and column norm heuristics efficiently finds a topological ordering without expensive computations.
- Mechanism: By comparing row and column norms of the adjacency matrix, the algorithm identifies vertices that are likely to be at the beginning or end of the topological ordering, then recursively applies this to the remaining vertices.
- Core assumption: Vertices with small column norms are likely sources (early in ordering) and vertices with small row norms are likely sinks (late in ordering).
- Evidence anchors:
  - [section 4.2] "We compute a 'closest' topological sorting and remove all edges not permitted by this ordering. The topological sorting is computed by a heuristic that calculates norms of all rows and columns to find the lowest value vi"
  - [algorithm 2] The projection method is described with O(d²) complexity
- Break condition: If the norm-based heuristic fails to identify the correct ordering, the projection may not preserve important edges.

## Foundational Learning

- Concept: Stochastic Approximation (SA) vs Sample Average Approximation (SAA)
  - Why needed here: Understanding the difference explains why ψDAG can converge to the true solution while SAA-based methods have inherent gaps
  - Quick check question: What is the key advantage of SA over SAA in terms of convergence guarantees?

- Concept: Topological sorting of DAGs
  - Why needed here: The entire algorithm relies on finding and preserving topological orderings to reduce the search space
  - Quick check question: Why does every DAG have at least one topological sorting?

- Concept: Convex vs non-convex optimization
  - Why needed here: The reformulated problem (12) becomes convex for a fixed ordering, while the original problem (11) is non-convex
  - Quick check question: How does fixing a topological ordering transform the original non-convex problem into a convex one?

## Architecture Onboarding

- Component map:
  - Phase 1: Unconstrained SGD (τ₁ iterations)
  - Projection: Topological sorting heuristic with edge removal
  - Phase 2: Constrained SGD (τ₂ iterations) preserving the ordering
  - Main loop: Repeats K times

- Critical path:
  1. Initialize W₀
  2. Run unconstrained SGD to get W^(1/3)
  3. Apply projection to get DAG W^(2/3) and ordering π
  4. Run constrained SGD to get Wk+1
  5. Repeat until convergence

- Design tradeoffs:
  - Projection frequency vs convergence speed: More frequent projections may help but increase overhead
  - Step sizes for Phase 1 vs Phase 2: May need different tuning for exploration vs exploitation
  - Memory vs computation: Storing full d×d matrices enables parallelization but requires O(d²) memory

- Failure signatures:
  - Divergence: If W grows unbounded during unconstrained phase
  - Slow convergence: If projection consistently returns poor orderings
  - Memory errors: When d becomes very large (approaching 10⁴)

- First 3 experiments:
  1. Run on small ER2 graph (d=10) with Gaussian noise, compare runtime vs DAGMA
  2. Test projection heuristic alone on a known DAG to verify correctness
  3. Vary projection frequency (every iteration vs every 10 iterations) to measure impact on convergence

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the proposed projection method always find the "closest" topological ordering in terms of some well-defined distance metric?
- Basis in paper: [inferred] The paper describes a heuristic for computing the closest topological ordering but does not prove it minimizes any specific distance metric between the input matrix and the space of DAGs.
- Why unresolved: The algorithm computes norms of rows and columns to determine vertex ordering but doesn't establish that this yields the minimum distance to the feasible set under any norm.
- What evidence would resolve it: A proof showing the projection minimizes a specific distance (e.g., Frobenius norm) between the input matrix and the set of DAGs, or experimental evidence comparing different projection methods.

### Open Question 2
- Question: How does the convergence rate change when using weighted projections based on second derivatives as described in Appendix C?
- Basis in paper: [explicit] The paper mentions weighted projections in Appendix C but only provides preliminary experimental results without theoretical analysis.
- Why unresolved: The paper shows improved convergence in some cases but doesn't analyze the theoretical implications or provide convergence guarantees for the weighted variant.
- What evidence would resolve it: A theoretical analysis of convergence rates for the weighted projection method, or comprehensive experimental results across multiple graph types and sizes.

### Open Question 3
- Question: What is the impact of the SGD step sizes (τ1 and τ2) on the final solution quality and convergence speed?
- Basis in paper: [inferred] The paper uses fixed step sizes in the experiments but doesn't analyze how different choices affect performance or provide guidance on optimal selection.
- Why unresolved: The experiments use default parameters without exploring the sensitivity to step size choices or providing theoretical bounds on their impact.
- What evidence would resolve it: An analysis of how step size choices affect convergence rates and solution quality, potentially with adaptive step size strategies.

### Open Question 4
- Question: Can the framework be extended to handle nonlinear DAG structures while maintaining similar scalability?
- Basis in paper: [explicit] The paper focuses on linear DAGs and mentions nonlinear models only in the context of related work.
- Why unresolved: The paper establishes a framework for linear structures but doesn't explore whether the stochastic approximation approach can be generalized to nonlinear cases.
- What evidence would resolve it: A modified version of the framework for nonlinear DAGs with experimental results demonstrating scalability and performance.

### Open Question 5
- Question: How does the algorithm perform on real-world graphs with different structural properties (e.g., community structure, hubs)?
- Basis in paper: [explicit] The paper tests on synthetic graphs and one real-world dataset, but doesn't systematically explore different graph properties.
- Why unresolved: The experiments use random graph models but don't investigate how the algorithm handles various real-world graph characteristics.
- What evidence would resolve it: Experiments on diverse real-world datasets with different structural properties, analyzing how performance varies with graph characteristics.

## Limitations

- The projection heuristic's effectiveness on real-world networks with diverse structural properties requires further validation
- Convergence guarantees depend on specific smoothness conditions that may not hold in all practical scenarios
- The method focuses on linear DAGs and may not directly extend to nonlinear structures

## Confidence

- Scalability improvements (High): Multiple experimental results consistently show superior runtime performance across graph sizes up to 10,000 nodes
- Projection method effectiveness (Medium): While theoretically sound, the heuristic's reliability across diverse graph structures needs more extensive testing
- Convergence guarantees (Medium): Theoretical bounds are provided but depend on assumptions about gradient smoothness and noise properties

## Next Checks

1. Test the projection heuristic on benchmark causal discovery datasets with known ground truth to assess accuracy across different graph topologies
2. Conduct sensitivity analysis of convergence rates with varying projection frequencies and step sizes to optimize the two-phase framework
3. Evaluate performance on graphs with varying edge densities and community structures to identify potential failure modes of the norm-based heuristic