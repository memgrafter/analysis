---
ver: rpa2
title: Key ingredients for effective zero-shot cross-lingual knowledge transfer in
  generative tasks
arxiv_id: '2402.12279'
source_url: https://arxiv.org/abs/2402.12279
tags:
- training
- steps
- mbart
- language
- tuning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'The paper conducts a systematic study of zero-shot cross-lingual
  knowledge transfer in generative tasks, focusing on learning rate tuning and intermediate
  tuning. Key findings include: careful learning rate tuning substantially reduces
  generation in wrong languages; simple full finetuning with tuned LR is a strong
  baseline, with advanced methods providing only marginal gains; mBART and mT5 perform
  similarly; NLLB-200 is competitive in summarization but lags in QA; and final zero-shot
  models match or exceed data translation baselines.'
---

# Key ingredients for effective zero-shot cross-lingual knowledge transfer in generative tasks

## Quick Facts
- arXiv ID: 2402.12279
- Source URL: https://arxiv.org/abs/2402.12279
- Authors: Nadezhda Chirkova; Vassilina Nikoulina
- Reference count: 40
- Primary result: Systematic study reveals learning rate tuning and simple fine-tuning are key for effective zero-shot cross-lingual generative task transfer

## Executive Summary
This paper presents a comprehensive empirical study on zero-shot cross-lingual transfer for generative tasks, focusing on the effectiveness of learning rate tuning and intermediate fine-tuning strategies. The authors systematically evaluate multiple multilingual pretrained models (mBART, mT5, NLLB-200) across summarization and question answering tasks in 15 languages. Their findings challenge common assumptions about the necessity of advanced fine-tuning techniques, showing that careful learning rate tuning and simple full fine-tuning can serve as strong baselines that often match or exceed more sophisticated approaches.

The study reveals that language-specific learning rate tuning is crucial for preventing generation in incorrect languages, a common failure mode in zero-shot transfer. Surprisingly, advanced fine-tuning methods like AdapterDrop and GradDrop provide only marginal improvements over simple fine-tuning when learning rates are properly tuned. The authors also demonstrate that final zero-shot models can achieve performance comparable to or better than data translation baselines, particularly when the target language is similar to English or has substantial training data available.

## Method Summary
The authors conduct an extensive empirical study comparing zero-shot cross-lingual transfer approaches for generative tasks. They evaluate three multilingual models (mBART, mT5, NLLB-200) across 15 languages on summarization and question answering tasks. The study systematically examines learning rate tuning strategies, comparing constant learning rates with language-specific tuning, and evaluates both simple fine-tuning approaches and advanced methods like AdapterDrop and GradDrop. Performance is measured using BLEU for QA tasks and ROUGE for summarization, with comparisons against data translation baselines.

## Key Results
- Careful learning rate tuning substantially reduces generation in wrong languages and improves overall performance
- Simple full fine-tuning with tuned learning rates serves as a strong baseline, with advanced methods providing only marginal gains
- mBART and mT5 show similar performance patterns across tasks, while NLLB-200 excels in summarization but lags in QA

## Why This Works (Mechanism)
The effectiveness of learning rate tuning stems from its ability to stabilize the optimization process during fine-tuning, particularly when adapting models to new languages with different linguistic structures. By carefully tuning learning rates for each language, the model can better preserve its multilingual capabilities while adapting to task-specific requirements. This prevents the common failure mode of language mixing during generation.

## Foundational Learning
- **Cross-lingual transfer learning**: Understanding how knowledge transfers between languages in multilingual models
  - Why needed: Core mechanism for zero-shot cross-lingual performance
  - Quick check: Can the model perform tasks in unseen languages after training on English?

- **Learning rate scheduling**: Different approaches to adjusting learning rates during training
  - Why needed: Critical for preventing catastrophic forgetting and optimizing convergence
  - Quick check: Does the learning rate decay schedule affect final performance?

- **Fine-tuning strategies**: Various methods for adapting pretrained models to downstream tasks
  - Why needed: Determines how effectively the model can specialize to new tasks
  - Quick check: Does the fine-tuning approach affect zero-shot transfer quality?

- **Multilingual model architectures**: Understanding mBART, mT5, and NLLB-200 architectures
  - Why needed: Different architectures have different transfer capabilities
  - Quick check: How do architectural differences affect cross-lingual transfer?

## Architecture Onboarding
Component map: Pretrained multilingual model -> Fine-tuning stage -> Zero-shot evaluation -> Automatic metrics
Critical path: Language-specific LR tuning -> Fine-tuning -> Generation -> Evaluation
Design tradeoffs: Simple fine-tuning vs. advanced methods (AdapterDrop, GradDrop)
Failure signatures: Generation in wrong languages, catastrophic forgetting, poor adaptation
First experiments: 1) Test language-specific LR tuning on single model/task, 2) Compare simple vs. advanced fine-tuning, 3) Evaluate zero-shot vs. translation baseline

## Open Questions the Paper Calls Out
None

## Limitations
- Study focuses only on summarization and question answering tasks, excluding other generative tasks
- Evaluation relies solely on automatic metrics, lacking human evaluation for quality assessment
- Conclusions may not generalize to more recent multilingual models and advanced fine-tuning techniques

## Confidence
- Learning rate tuning being critical for preventing language mixing: High
- Simple full fine-tuning serving as a strong baseline: High
- Advanced fine-tuning methods providing only marginal gains: Medium

## Next Checks
1. Replicate experiments on additional generative tasks (translation, dialogue, creative generation) to verify generalizability
2. Conduct human evaluation studies to validate automatic metric results for cross-lingual quality assessment
3. Extend analysis to include more recent multilingual models and advanced fine-tuning techniques to ensure conclusions remain valid