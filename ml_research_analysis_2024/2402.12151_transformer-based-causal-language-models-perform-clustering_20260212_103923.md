---
ver: rpa2
title: Transformer-based Causal Language Models Perform Clustering
arxiv_id: '2402.12151'
source_url: https://arxiv.org/abs/2402.12151
tags:
- task
- clustering
- training
- language
- tasks
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper investigates how Transformer-based causal language models
  learn to follow instructions by clustering task-specific information in their hidden
  representations. The authors create a simplified instruction-following task with
  synthetic data, where tasks are identified by regular expression-based instructions.
---

# Transformer-based Causal Language Models Perform Clustering

## Quick Facts
- arXiv ID: 2402.12151
- Source URL: https://arxiv.org/abs/2402.12151
- Authors: Xinbo Wu; Lav R. Varshney
- Reference count: 19
- Primary result: Transformer-based causal language models learn instruction-following by clustering task-specific information in hidden representations

## Executive Summary
This paper investigates how Transformer-based causal language models learn to follow instructions by analyzing their hidden representations during training. The authors create a simplified instruction-following task with synthetic data where tasks are identified by regular expression-based instructions. They train a small 6-layer Transformer model and analyze its hidden states throughout training, finding that the model forms task-specific clusters in its hidden space. These clusters evolve dynamically during learning and enable the model to handle unseen instances effectively. The clustering phenomenon is validated on real LLMs and leads to practical applications including pre-training with task identities for faster fine-tuning and alignment methods with reduced forgetting.

## Method Summary
The authors generate synthetic instruction-following datasets using regular expression-based instructions where each task maps single-token inputs to single-token outputs. A 6-layer Transformer following GPT-2 architecture is trained on this data using AdamW optimizer with cosine annealing learning rate schedule. Hidden states of input tokens are extracted across layers and analyzed using T-SNE for dimensionality reduction and KMeans clustering. Clustering performance is evaluated using F1 score, adjusted rand index (ARI), and adjusted mutual information (AMI), with task identities serving as ground truth labels. The analysis tracks clustering quality throughout training and correlates it with task performance and K-nearest neighbors (KNN) accuracy on validation data.

## Key Results
- Transformer models form distinct task-specific clusters in hidden representation space that improve during training
- Clustering performance (F1, ARI, AMI) increases across training epochs, with dramatic improvements after initial task accuracy plateaus
- KNN accuracy on validation data correlates with clustering quality, demonstrating that clusters enable generalization to unseen instances
- The clustering phenomenon persists and strengthens after task performance plateaus, suggesting it's an inductive bias of the Transformer architecture

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Transformer models form task-specific clusters in hidden representation space during instruction-following training.
- Mechanism: The model organizes hidden states corresponding to the same task into close proximity, creating distinct clusters that evolve throughout training.
- Core assumption: Task identity can be inferred from the proximity of hidden states in the embedding space.
- Evidence anchors:
  - [abstract] "our findings suggest that the model learns task-specific information by clustering data within its hidden space, with this clustering process evolving dynamically during learning"
  - [section] "We found experimental evidence supporting the former" (task-specific clusters forming in hidden space)
  - [corpus] Corpus signals show related work on clustering and task-specific representations, but direct evidence for this mechanism is limited
- Break condition: If hidden states for the same task become scattered or mixed with other tasks' representations, clustering would fail.

### Mechanism 2
- Claim: Clustering improves the model's ability to generalize to unseen instances of known tasks.
- Mechanism: By bringing instances of the same task closer in hidden space, the model can use nearby training examples to infer correct outputs for unseen instances.
- Core assumption: K-nearest neighbors in hidden space can effectively predict task identity and output for unseen instances.
- Evidence anchors:
  - [abstract] "demonstrate how this phenomenon assists the model in handling unseen instances"
  - [section] "We observe a dramatic improvement in the percentage along the training process, indicating that both training instances and unseen instances are not only close in the hidden space but also become more clustered as the training proceeds"
  - [corpus] Weak corpus evidence - related works exist on KNN approaches but not specifically for this clustering mechanism
- Break condition: If clustering doesn't improve KNN accuracy or if KNN accuracy doesn't correlate with task performance, the mechanism breaks.

### Mechanism 3
- Claim: Task-specific clustering emerges as a strong inductive bias of the Transformer architecture during training.
- Mechanism: The clustering phenomenon persists and even strengthens after task performance plateaus, suggesting it's an inherent property of how Transformers organize information.
- Core assumption: Clustering is not just a byproduct of task learning but a fundamental organizational principle of the model.
- Evidence anchors:
  - [abstract] "we show that the clusters evolve dynamically as the model learns"
  - [section] "particularly noteworthy is the persistence and even improvement of the clustering phenomenon long after the early stopping point, indicating clustering as a strong inductive bias"
  - [corpus] Limited corpus evidence - clustering in Transformers has been studied but not specifically as an inductive bias for instruction-following
- Break condition: If clustering disappears when training is extended or when different architectures are used, it's not an inductive bias.

## Foundational Learning

- Concept: Hidden state representation and how Transformers process sequences across layers
  - Why needed here: Understanding how hidden states evolve across layers is crucial for analyzing clustering behavior
  - Quick check question: Can you explain what happens to a token's hidden state as it passes through each Transformer layer?

- Concept: Clustering algorithms and evaluation metrics (F1, ARI, AMI)
  - Why needed here: The paper uses KMeans clustering and multiple evaluation metrics to quantify task-specific clustering
  - Quick check question: What's the difference between F1 score and Adjusted Rand Index when evaluating clustering results?

- Concept: Regular expressions and synthetic data generation
  - Why needed here: The paper creates synthetic instruction datasets using regular expressions to control task complexity
  - Quick check question: How would you generate 100 different instructions for a task using a regular expression pattern?

## Architecture Onboarding

- Component map: Input instruction + Input token + Output token sequence -> 6-layer Transformer -> Next token prediction -> T-SNE -> KMeans clustering -> Evaluation metrics

- Critical path: 1. Generate synthetic instruction-following dataset 2. Train Transformer model 3. Extract hidden states for input tokens across layers 4. Apply T-SNE dimensionality reduction 5. Perform KMeans clustering 6. Evaluate clustering using task identities as labels

- Design tradeoffs:
  - Synthetic vs. real data: Synthetic data offers control but may not capture all real-world complexities
  - Model size: Smaller models train faster for analysis but may not capture all phenomena
  - Clustering granularity: Task-level vs. instruction-distribution-level clustering

- Failure signatures:
  - Clustering performance doesn't improve during training
  - KNN accuracy doesn't correlate with task accuracy
  - Hard examples aren't separated from original examples
  - Clustering only works on training data, not validation data

- First 3 experiments:
  1. Train a small Transformer on synthetic instruction data and visualize hidden states at different layers
  2. Measure clustering performance (F1, ARI, AMI) at each layer throughout training
  3. Test KNN accuracy on validation set to verify generalization of clustering structure

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the clustering phenomenon persist when the task-specific vocabulary is expanded beyond the small vocabulary used in the simplified setting?
- Basis in paper: [inferred] The paper notes that the simplified task uses a small task-related vocabulary to ensure the model cannot identify tasks solely from inputs. This suggests that scaling up the vocabulary could impact clustering behavior.
- Why unresolved: The experiments are explicitly limited to a simplified setting with constrained vocabularies, and the paper acknowledges this as a limitation.
- What evidence would resolve it: Replicating the experiments with progressively larger vocabularies and analyzing whether clustering persists or degrades would provide clarity.

### Open Question 2
- Question: How does pre-training with task identities impact the generalization performance of the model on entirely unseen tasks not present in the pre-training data?
- Basis in paper: [explicit] The paper introduces pre-training with task identities to accelerate convergence and improve task accuracy but does not evaluate performance on tasks outside the pre-training distribution.
- Why unresolved: The experiments focus on fine-tuning performance within the same task distribution used for pre-training.
- What evidence would resolve it: Testing the pre-trained model on a disjoint set of tasks and comparing its generalization to a model without task-identity pre-training would address this.

### Open Question 3
- Question: What is the relationship between the size of the Transformer model (e.g., hidden dimension) and the quality or stability of the task-specific clustering?
- Basis in paper: [explicit] The paper tests clustering across models with hidden dimensions of 32, 768, and 2048, but does not provide a systematic analysis of how model size affects clustering dynamics or performance.
- Why unresolved: The analysis focuses on demonstrating clustering existence rather than characterizing its dependence on model scale.
- What evidence would resolve it: A controlled study varying model size while keeping other factors constant, followed by clustering analysis and task performance evaluation, would clarify this relationship.

### Open Question 4
- Question: Can the clustering-based alignment method be extended to handle more complex forms of misalignment, such as multi-step instruction corrections or context-dependent toxicity?
- Basis in paper: [inferred] The alignment application described is limited to simple source-target task replacement, which may not capture the complexity of real-world misalignment scenarios.
- Why unresolved: The paper explicitly states that the alignment application is simplified and leaves extension to realistic settings as future work.
- What evidence would resolve it: Implementing the alignment method on more complex misalignment tasks (e.g., multi-turn dialogues with context-dependent behavior) and evaluating its effectiveness would provide insights.

## Limitations
- Results are based on synthetic data with single-token inputs and outputs, which may not fully represent real-world instruction-following complexity
- Experiments focus on small 6-layer Transformers, and it's unclear whether larger models exhibit the same clustering behavior
- The paper demonstrates correlation between clustering and task performance but doesn't establish causation through ablation studies

## Confidence
- High Confidence: The existence of task-specific clusters in hidden representations during training
- Medium Confidence: That clustering is an inductive bias of Transformer architecture
- Medium Confidence: That clustering enables generalization to unseen instances

## Next Checks
1. **Cross-model validation**: Test whether the clustering phenomenon persists in larger Transformers (12-24 layers) and different architectures (BERT, T5) to verify if it's truly an inductive bias rather than a model-specific artifact.

2. **Real-world complexity test**: Apply the clustering analysis to instruction-following tasks with multi-token inputs/outputs and more complex natural language instructions to validate if the phenomenon scales to realistic scenarios.

3. **Causality verification**: Design ablation studies where clustering is explicitly disrupted (e.g., through regularization) to directly test whether clustering causes improved generalization versus merely correlating with it.