---
ver: rpa2
title: 'xGen-MM-Vid (BLIP-3-Video): You Only Need 32 Tokens to Represent a Video Even
  in VLMs'
arxiv_id: '2410.16267'
source_url: https://arxiv.org/abs/2410.16267
tags:
- video
- tokens
- temporal
- blip-3-video
- encoder
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper introduces BLIP-3-Video, a compact multimodal video\
  \ language model that uses an explicit temporal encoder to represent entire videos\
  \ with only 16\u201332 visual tokens, drastically fewer than prior models (e.g.,\
  \ 4608). The temporal encoder, explored in forms such as spatio-temporal attentional\
  \ pooling and a grouped token sequential model, learns to abstract frame-level tokens\
  \ into concise video-level representations."
---

# xGen-MM-Vid (BLIP-3-Video): You Only Need 32 Tokens to Represent a Video Even in VLMs

## Quick Facts
- arXiv ID: 2410.16267
- Source URL: https://arxiv.org/abs/2410.16267
- Authors: Michael S. Ryoo, Honglu Zhou, Shrikant Kendre, Can Qin, Le Xue, Manli Shu, Jongwoo Park, Kanchana Ranasinghe, Silvio Savarese, Ran Xu, Caiming Xiong, Juan Carlos Niebles
- Reference count: 40
- One-line primary result: Achieves video QA accuracy comparable to much larger models using only 16-32 visual tokens

## Executive Summary
BLIP-3-Video introduces a novel approach to video understanding in vision-language models by using an explicit temporal encoder to compress video frames into a minimal set of visual tokens. Unlike prior models that process thousands of tokens per video, BLIP-3-Video achieves competitive performance with only 16-32 tokens through two temporal encoder variants: spatio-temporal attentional pooling and a grouped token sequential model. The model demonstrates that efficient video representation is possible without sacrificing accuracy, achieving 77.7% accuracy on MSVD-QA and 76.4% on NExT-QA while being significantly more efficient than larger counterparts.

## Method Summary
BLIP-3-Video uses a three-stage curriculum learning approach: (1) image caption pretraining using BLIP-3 weights, (2) video caption pretraining on rephrased LLaVA-Hound-DPO data, and (3) video instruction tuning on mixed video QA datasets. The model processes 8 uniformly sampled frames at 384×384 resolution through a SigLIP vision encoder, then uses a Perceiver-Resampler to reduce 729 tokens per frame to 128 tokens. The temporal encoder (either TokenLearner or grouped TTM-based) further compresses these to 16-128 tokens before passing them to a Phi-3 LLM with text prompts. This approach enables the model to achieve competitive video understanding performance while using drastically fewer visual tokens than previous methods.

## Key Results
- Achieves 77.7% accuracy on MSVD-QA using only 32 visual tokens
- Reaches 76.4% accuracy on NExT-QA with 32 tokens, comparable to much larger models
- Outperforms larger models (7B, 34B parameters) on multiple video QA benchmarks despite being only 4B parameters
- Ablation studies confirm the effectiveness of the temporal encoder and grouped sequential design

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The explicit temporal encoder allows the model to abstract frame-level tokens into a compact video-level representation, drastically reducing the number of visual tokens needed for video understanding.
- Mechanism: The temporal encoder processes a sequence of image-level tokens (N tokens per frame over T frames) and outputs a fixed number of M tokens (e.g., 16 or 32) that capture the essential video information through learnable soft selection of informative tokens across space and time.
- Core assumption: The model can learn to select a small subset of spatio-temporally informative tokens that are sufficient for downstream tasks like question answering and captioning.
- Evidence anchors:
  - [abstract]: "BLIP-3-Video takes advantage of the 'temporal encoder' in addition to the conventional visual tokenizer, which maps a sequence of tokens over multiple frames into a compact set of visual tokens."
  - [section]: "The role of the temporal encoder is to build a video-level token representation from such sequence of image-level tokens, serving as a mapping function between a set of N · T image tokens to M video tokens..."
  - [corpus]: Weak evidence - no direct comparison of token reduction effectiveness in the corpus.

### Mechanism 2
- Claim: The grouped token sequential model (based on Token Turing Machines) maintains a separate memory for each visual token over time, better preserving scene details than vanilla sequential models.
- Mechanism: Instead of maintaining a single memory vector, the grouped sequential model maintains a set of memory tokens Ht for each of the N tokens over time, allowing the model to focus on its group and maintain different visual features separately.
- Core assumption: Maintaining separate memory for each visual token over time is more effective at preserving scene details than a single shared memory vector.
- Evidence anchors:
  - [section]: "The main distinction between our new sequential models and the conventional sequential models is that our sequential model maintains a grouped memory, separately processing and maintaining different visual features."
  - [section]: "That is, we enforce the sequential model to focus on its group (specified with j) and maintain a 'set' of memory tokens for every time step t: Ht."
  - [corpus]: No direct evidence in corpus comparing grouped vs. ungrouped sequential models.

### Mechanism 3
- Claim: The spatio-temporal attentional pooling (TokenLearner) is more effective than separate spatial and temporal pooling because it learns to soft-select informative tokens directly from the combined space-time representation.
- Mechanism: Unlike approaches that apply spatial pooling per frame and then temporal pooling across frames, TokenLearner takes all N · T tokens and learns to soft-select M informative tokens by computing attention weights over the entire spatio-temporal representation.
- Core assumption: Learning to select tokens from the combined space-time representation is more effective than applying spatial and temporal pooling sequentially.
- Evidence anchors:
  - [section]: "Unlike previous per-frame use of pooling where spatial pooling and temporal pooling are applied separately, our temporal encoder directly takes all N · T tokens and 'learns' to soft-select M informative tokens spatio-temporally."
  - [section]: "We experimentally confirm that such learnable spatio-temporal attentional pooling has advantages over the conventional approach of non-learnable spatial pooling and temporal pooling..."
  - [corpus]: No direct evidence in corpus comparing TokenLearner to sequential spatial-temporal pooling.

## Foundational Learning

- Concept: **Vision Transformers (ViT) and Tokenization**
  - Why needed here: The model uses a pretrained SigLIP vision encoder to map video frames into visual tokens, which are then processed by the temporal encoder.
  - Quick check question: What is the output dimension of a typical ViT encoder when processing an image, and how are these outputs used as tokens?

- Concept: **Attention Mechanisms and Cross-Attention**
  - Why needed here: The temporal encoder uses attentional pooling (TokenLearner) which relies on attention mechanisms to soft-select informative tokens.
  - Quick check question: How does cross-attention differ from self-attention, and why is it used in the Perceiver-Resampler?

- Concept: **Sequential Models (Mamba, Token Turing Machines)**
  - Why needed here: The grouped token sequential model is based on Token Turing Machines, which are a type of sequential model.
  - Quick check question: What is the key difference between a standard Transformer and a sequential model like Mamba or TTM in terms of how they process input sequences?

## Architecture Onboarding

- Component map: Video frames (8×384×384) → SigLIP → Perceiver-Resampler (729→128 tokens/frame) → Temporal Encoder (128×8→16-128 tokens) → Phi-3 LLM → Text output

- Critical path: Video frames → SigLIP → Perceiver-Resampler → Temporal Encoder → LLM → Text output

- Design tradeoffs:
  - Token reduction vs. information loss: Using fewer tokens (16-32) vs. using all tokens (4608) - the temporal encoder must learn to preserve essential information while reducing token count
  - Model size vs. performance: 4B parameters vs. larger models (7B, 34B) - the temporal encoder enables competitive performance despite smaller size
  - Computational efficiency vs. accuracy: Fewer tokens reduce computation (quadratic to token count) but may impact accuracy if essential information is lost

- Failure signatures:
  - Performance degradation on tasks requiring fine-grained temporal reasoning
  - Inability to handle complex or long video scenes effectively
  - Hallucinations or generation of irrelevant information in captions/answers
  - Overfitting to training data when token reduction is too aggressive

- First 3 experiments:
  1. **Ablation study on temporal encoder types**: Compare performance using different temporal encoders (mean pooling, transformer, TokenLearner, grouped TTM) with fixed token count (128) on MSVD-QA to identify the most effective approach
  2. **Token count sensitivity analysis**: Test model performance with varying numbers of output tokens (16, 32, 64, 128) on MSVD-QA and NExT-QA to find the optimal balance between efficiency and accuracy
  3. **Frame count scaling experiment**: Evaluate performance when increasing input frames (8→16) while keeping output tokens fixed (32 or 128) on NExT-QA to assess scalability to longer videos

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of BLIP-3-Video scale with the number of input frames beyond 16 frames, particularly when maintaining a fixed number of output tokens?
- Basis in paper: [explicit] The paper mentions that increasing the number of input frames has a positive effect on performance, as it increases the pool of tokens the temporal encoder can select from, and suggests this trend will continue until saturation.
- Why unresolved: The paper only tests up to 16 frames and does not explore the upper limits of this scaling behavior or identify the point of diminishing returns.
- What evidence would resolve it: Experiments evaluating BLIP-3-Video with a broader range of frame counts (e.g., 8, 16, 32, 64, 128) while keeping output tokens fixed would clarify how much additional performance is gained and where saturation occurs.

### Open Question 2
- Question: What is the impact of different types of temporal encoders on the model's ability to handle long-form videos (e.g., videos longer than a few minutes)?
- Basis in paper: [inferred] The paper shows that BLIP-3-Video can be combined with hierarchical frameworks like LVNet to handle longer videos, but does not directly investigate how different temporal encoder architectures (e.g., spatio-temporal attentional pooling vs. grouped sequential models) perform on long-form video tasks.
- Why unresolved: The experiments focus on short to medium-length videos, and the paper does not compare temporal encoder effectiveness specifically in long-video scenarios.
- What evidence would resolve it: Comparative studies of BLIP-3-Video variants with different temporal encoders on long-video benchmarks (e.g., EgoSchema, VideoMME) would reveal which encoder designs are most effective for extended temporal reasoning.

### Open Question 3
- Question: How sensitive is BLIP-3-Video's performance to the resolution and quality of input frames, especially when using fewer output tokens?
- Basis in paper: [explicit] The model uses a fixed input resolution of 384×384 and Perceiver-Resampler to reduce tokens per frame, but the paper does not explore how changes in input resolution or visual quality affect performance when the temporal encoder is constrained to output only 16–32 tokens.
- Why unresolved: The experiments assume a standard resolution, and there is no analysis of how lower or higher resolutions impact the model's ability to abstract meaningful video representations with minimal tokens.
- What evidence would resolve it: Systematic evaluation of BLIP-3-Video across multiple input resolutions (e.g., 256×256, 384×384, 512×512) while measuring accuracy and token efficiency would clarify the trade-offs between resolution, token count, and performance.

### Open Question 4
- Question: What are the limitations of the grouped token sequential model temporal encoder in capturing complex, multi-object interactions over time?
- Basis in paper: [explicit] The paper introduces a grouped token sequential model based on Token Turing Machines and shows it outperforms other temporal encoders, but does not deeply analyze its weaknesses in modeling intricate temporal dynamics or interactions between multiple objects.
- Why unresolved: The ablation studies focus on overall accuracy improvements but do not investigate failure cases or limitations in handling complex, multi-agent video scenarios.
- What evidence would resolve it: Detailed qualitative and quantitative analysis of BLIP-3-Video's performance on datasets with complex multi-object interactions (e.g., TGIF-QA action repetition tasks, NExT-QA with multiple subjects) would reveal the strengths and limitations of the grouped sequential model in capturing nuanced temporal relationships.

## Limitations

- The model's performance on very long videos and complex multi-event narratives remains untested, as experiments focus on short to medium-length videos.
- The effectiveness of the grouped token sequential model is demonstrated but not exhaustively compared to simpler sequential approaches, leaving uncertainty about whether the added complexity is justified.
- The claim about token efficiency relies heavily on the temporal encoder, but ablation studies don't fully isolate whether gains come from the encoder architecture or the specific training curriculum.

## Confidence

**High Confidence**: The claim that BLIP-3-Video achieves competitive video QA performance with significantly fewer tokens (16-32 vs 4608) is well-supported by empirical results across multiple benchmarks (MSVD-QA: 77.7%, NExT-QA: 76.4%). The ablation studies demonstrating the importance of the temporal encoder are robust and convincing.

**Medium Confidence**: The assertion that the grouped sequential model (TTM-based) is superior to other temporal encoder designs is supported by experiments, but the comparisons are not exhaustive. The paper shows it works well but doesn't definitively prove it's the optimal approach.

**Low Confidence**: The claim about scalability to long videos and complex scenes is based on integration with hierarchical frameworks, but this is presented more as a proof-of-concept rather than a thoroughly validated extension. The paper doesn't explore the limits of how long or complex videos can be before the 16-32 token representation becomes insufficient.

## Next Checks

1. **Failure Case Analysis**: Systematically test BLIP-3-Video on videos that require fine-grained temporal reasoning or contain complex multi-event narratives to identify the limits of the 16-32 token representation. Compare failure modes with a baseline that uses all 4608 tokens.

2. **Temporal Encoder Architecture Comparison**: Conduct a more comprehensive ablation study comparing TokenLearner, grouped TTM, standard transformer-based temporal encoders, and simpler approaches like mean pooling across time. Include analysis of computational efficiency and memory usage for each variant.

3. **Cross-Dataset Generalization Test**: Evaluate the model trained on the mixed video QA datasets on a held-out, challenging dataset not seen during training (such as EgoSchema or VideoMME) to assess whether the token efficiency generalizes to new video types and question styles.