---
ver: rpa2
title: Rephrasing Electronic Health Records for Pretraining Clinical Language Models
arxiv_id: '2411.18940'
source_url: https://arxiv.org/abs/2411.18940
tags:
- clinical
- language
- notes
- prompt
- llms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study introduces a method to generate synthetic clinical text
  for pretraining language models by rephrasing real clinical notes using small-sized
  LLMs. Four models under 10B parameters (Llama-3.1, Mistral-0.3, Qwen-2, and Gemma-2)
  were used with medically adapted prompts to rephrase discharge summaries from MIMIC-III.
---

# Rephrasing Electronic Health Records for Pretraining Clinical Language Models

## Quick Facts
- arXiv ID: 2411.18940
- Source URL: https://arxiv.org/abs/2411.18940
- Authors: Jinghui Liu; Anthony Nguyen
- Reference count: 12
- Primary result: Synthetic clinical text generated via LLM rephrasing improves pretraining of clinical language models compared to previous methods

## Executive Summary
This study introduces a method to generate synthetic clinical text for pretraining language models by rephrasing real clinical notes using small-sized LLMs. Four models under 10B parameters (Llama-3.1, Mistral-0.3, Qwen-2, and Gemma-2) were used with medically adapted prompts to rephrase discharge summaries from MIMIC-III. The synthetic data was used to pretrain both decoder-based and encoder-based language models. Results showed that models trained on rephrased data achieved significantly lower perplexity than those trained on previous synthetic methods and outperformed ClinicalBERT in downstream tasks. Augmenting real clinical notes with synthetic text further improved performance, demonstrating the effectiveness of the rephrasing approach for clinical language model development.

## Method Summary
The method uses small LLMs (<10B parameters) to rephrase real clinical notes from MIMIC-III, generating synthetic data that captures clinical language patterns while maintaining privacy. Four different LLMs (Llama-3.1, Mistral-0.3, Qwen-2, and Gemma-2) were prompted with three medically adapted prompts to rephrase discharge summaries. The resulting synthetic corpora were used to pretrain both decoder-based and encoder-based language models. The approach was evaluated using perplexity scores on MIMIC-IV test sets and downstream task performance on MedNLI and i2b2 datasets, comparing against previous synthetic methods and ClinicalBERT.

## Key Results
- Models trained on rephrased data achieved significantly lower perplexity than those trained on previous synthetic methods
- Pretrained models outperformed ClinicalBERT on downstream clinical NLP tasks
- Augmenting original clinical notes with synthetic corpora from different LLMs improved performance even at small token budgets
- Different LLMs responded variably to prompts, with Qwen-2 performing better under medically focused Prompt 2

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLM rephrasing reduces perplexity by creating clinically relevant text without requiring large real clinical datasets
- Mechanism: Small LLMs (<10B parameters) rephrase real clinical notes from MIMIC-III, generating synthetic data that captures clinical language patterns while maintaining privacy
- Core assumption: Rephrased clinical text preserves essential semantic content while being distinct enough to serve as effective pretraining material
- Evidence anchors:
  - [abstract] "The method yields better results in language modeling and downstream tasks than previous synthesis approaches without referencing real clinical text"
  - [section 3.2] "The rephrasing method consistently outperforms the approach in Asclepius (Kweon et al., 2024), which does not refer to real clinical text"
  - [corpus] Weak evidence - corpus analysis shows related work but lacks direct comparison of rephrasing quality metrics
- Break condition: If rephrasing causes significant semantic drift or hallucinations that degrade clinical meaning

### Mechanism 2
- Claim: Combining real and synthetic clinical notes improves language modeling performance
- Mechanism: Augmenting original clinical notes with synthetic corpora from different LLMs provides complementary learning signals
- Core assumption: Synthetic data fills coverage gaps in real data while maintaining domain relevance
- Evidence anchors:
  - [abstract] "We find that augmenting original clinical notes with synthetic corpora from different LLMs improves performances even at a small token budget"
  - [section 3.2] "The results confirm the benefit of augmenting pretraining data with synthetic text"
  - [corpus] Moderate evidence - related work shows augmentation benefits but lacks specific clinical note combination studies
- Break condition: If synthetic data introduces conflicting patterns that confuse the model

### Mechanism 3
- Claim: Different LLM architectures respond variably to prompts, affecting synthetic data quality
- Mechanism: Prompt engineering significantly influences how LLMs rephrase clinical text, with medical-specific prompts showing mixed results
- Core assumption: LLMs can be guided to produce clinically appropriate rephrasings through prompt adaptation
- Evidence anchors:
  - [section 2.1] "We explore whether it is beneficial to explicitly leverage the internal knowledge of LLM for synthesis"
  - [section 3.2] "LLMs respond differently to prompts. For example, Qwen-2 performs better under the medically focused Prompt 2"
  - [corpus] Limited evidence - corpus analysis shows related prompt engineering work but lacks clinical-specific studies
- Break condition: If prompt engineering fails to produce consistent quality across different LLM architectures

## Foundational Learning

- Concept: Clinical text characteristics and privacy constraints
  - Why needed here: Understanding why clinical notes are valuable for pretraining and why privacy concerns necessitate synthetic data generation
  - Quick check question: What makes clinical notes particularly challenging for language modeling compared to general text?

- Concept: LLM architecture differences (decoder vs encoder models)
  - Why needed here: The study pretrains both decoder-based and encoder-based language models, requiring understanding of their distinct applications
  - Quick check question: How do decoder-based models differ from encoder-based models in their pretraining objectives?

- Concept: Prompt engineering principles
  - Why needed here: The study explores multiple prompts to guide LLM rephrasing, requiring understanding of how prompt structure affects output
  - Quick check question: What are the key components of effective prompts for specialized domain text generation?

## Architecture Onboarding

- Component map: MIMIC-III data → LLM rephrasing (Llama-3.1, Mistral-0.3, Qwen-2, Gemma-2) → synthetic corpora → pretraining (decoder and encoder models) → evaluation (perplexity and downstream tasks)
- Critical path: Clinical notes → LLM rephrasing → pretraining → evaluation
- Design tradeoffs: Smaller LLMs (efficiency) vs larger models (potential quality), prompt specificity vs generality, synthetic data quantity vs quality
- Failure signatures: High perplexity indicating poor language modeling, degraded downstream task performance, semantic drift in rephrased text
- First 3 experiments:
  1. Test rephrasing quality: Compare semantic similarity between original and rephrased clinical notes using simple metrics
  2. Validate synthetic data: Pretrain a small model on synthetic data and test on held-out real clinical text
  3. Prompt sensitivity: Evaluate how different prompts affect rephrasing quality for each LLM architecture

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do the rephrased clinical notes compare to the original notes in terms of clinical meaning retention and potential hallucinations?
- Basis in paper: [inferred] The paper mentions the need for a deeper comparison between rephrased and real notes to understand content retention and potential hallucinations, but does not provide such analysis.
- Why unresolved: The authors note that they did not keep the indices of generated outputs corresponding to original text, making it difficult to provide rephrasings for all text chunks. They leave this analysis for future work.
- What evidence would resolve it: A systematic comparison of rephrased and original notes, measuring content retention and identifying potential hallucinations, would provide evidence to resolve this question.

### Open Question 2
- Question: How do different prompt settings impact the performance of decoder-based models for instruction tuning?
- Basis in paper: [explicit] The authors note that while Prompt 3 tended to underperform in language modeling, it provided an advantage on fine-tuning performance for specific tasks. They suggest that future research needs to investigate how prompts impact decoder-based models for instruction tuning.
- Why unresolved: The paper focuses on the impact of prompts on encoder-based models and language modeling, but does not explore their impact on decoder-based models for instruction tuning.
- What evidence would resolve it: Experiments comparing the performance of decoder-based models on instruction tuning tasks using different prompts would provide evidence to resolve this question.

### Open Question 3
- Question: How do LLMs handle duplicated content such as copy-and-pasted text in their rephrasing?
- Basis in paper: [explicit] The authors mention exploring whether LLMs reduce or amplify biases and how they handle duplicated contents such as copy-and-pasted text in their rephrasing as important future directions.
- Why unresolved: The paper does not provide any analysis or results regarding how LLMs handle duplicated content in their rephrasing.
- What evidence would resolve it: Analyzing the rephrased notes for changes in duplicated content compared to the original notes would provide evidence to resolve this question.

## Limitations

- Evaluation relies heavily on perplexity metrics, which correlate imperfectly with downstream task performance
- Rephrasing method tested only on discharge summaries from MIMIC-III, limiting generalizability to other clinical note types
- Synthetic data generation performed once per LLM-prompt combination without reporting variability or sensitivity analyses

## Confidence

- **High confidence**: The perplexity improvements over previous synthetic methods (Asclepius) and the outperformance of ClinicalBERT on downstream tasks are well-supported by the experimental results
- **Medium confidence**: The claim that combining real and synthetic data improves performance is supported but could benefit from more extensive ablation studies with varying synthetic-to-real ratios
- **Medium confidence**: The assertion that different LLMs respond variably to prompts is demonstrated but the mechanism behind these differences is not fully explored

## Next Checks

1. **Semantic fidelity validation**: Perform detailed semantic similarity analysis between original and rephrased clinical notes to quantify information preservation and identify potential clinical concept drift
2. **Cross-corpora generalization**: Test the pretrained models on clinical text from different sources (e.g., clinical trials data, different EHR systems) to assess domain transfer capability
3. **Longitudinal stability assessment**: Evaluate whether models trained on rephrased data maintain performance stability over extended inference tasks, particularly for clinical decision support scenarios