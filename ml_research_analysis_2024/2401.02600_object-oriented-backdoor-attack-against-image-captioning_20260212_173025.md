---
ver: rpa2
title: Object-oriented backdoor attack against image captioning
arxiv_id: '2401.02600'
source_url: https://arxiv.org/abs/2401.02600
tags:
- image
- attack
- backdoor
- captioning
- poisoned
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces the first backdoor attack on image captioning
  models using a data poisoning method. It proposes an object-oriented trigger generation
  approach that crafts poisoned samples by iteratively modifying pixel values in object
  regions detected by YOLO-v3.
---

# Object-oriented backdoor attack against image captioning

## Quick Facts
- **arXiv ID:** 2401.02600
- **Source URL:** https://arxiv.org/abs/2401.02600
- **Reference count:** 0
- **Primary result:** First backdoor attack on image captioning models using data poisoning achieves >90% ASR on Flickr8k and >96% on Flickr30k

## Executive Summary
This paper introduces the first backdoor attack on image captioning models using a data poisoning approach. The attack crafts poisoned samples by iteratively modifying pixel values in object regions detected by YOLO-v3 using Gaussian noise. The attack achieves high success rates while maintaining low false trigger rates, demonstrating that poisoned models can generate accurate captions for benign images while producing attacker-specified outputs for poisoned images.

## Method Summary
The attack generates poisoned samples by first detecting objects using YOLO-v3, then iteratively modifying pixel values in each detected object region using a Gaussian noise matrix (M ~ N(0,1)) with noise intensity α=20. The original captions are replaced with attacker-defined captions during training. The model is trained using cross-entropy loss and Adam optimizer with learning rate decay. Evaluation uses Attack Success Rate (ASR) and False Trigger Rate (FTR) metrics on Flickr8k and Flickr30k datasets with 30% poisoning rate.

## Key Results
- Achieves attack success rates over 90% on Flickr8k and 96% on Flickr30k
- Maintains false trigger rates under 5%
- Shows minimal BLEU score degradation (<1%) on benign images
- Demonstrates that the attacked model behaves normally on benign images while producing attacker-specified captions for poisoned images

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Poisoned samples are generated by iteratively modifying pixel values in object regions detected by YOLO-v3.
- **Mechanism:** The attack first detects objects using YOLO-v3, then iteratively modifies pixel values in each detected object region using a Gaussian noise matrix. The noise intensity α is fixed at 20, ensuring the modifications are slight but sufficient to embed the trigger.
- **Core assumption:** Object detection by YOLO-v3 is accurate and covers the objects of interest in the image.
- **Evidence anchors:**
  - [abstract]: "crafts poisoned samples by iteratively modifying pixel values in object regions detected by YOLO-v3"
  - [section]: "an object detector is first used to extract regions that contain objects. Afterwards, for each detected region, a Gaussian noise matrix M ~ N (0, 1)C×W ×H is generated"
  - [corpus]: Weak evidence; related papers focus on backdoor attacks but do not discuss object-oriented trigger generation.
- **Break condition:** If YOLO-v3 fails to detect objects accurately, the poison crafting method would not work as intended.

### Mechanism 2
- **Claim:** The attacked model generates attacker-specified outputs for poisoned images while maintaining normal performance on benign images.
- **Mechanism:** During training, the model learns to associate the poisoned images with the attacker-defined captions. When presented with a poisoned image during inference, the model generates the attacker-specified caption. For benign images, the model generates accurate captions as usual.
- **Core assumption:** The training process allows the model to learn the association between poisoned images and attacker-defined captions without affecting its performance on benign images.
- **Evidence anchors:**
  - [abstract]: "the attacked model behaves normally on benign images, but for poisoned images, the model will generate some sentences irrelevant to the given image"
  - [section]: "if the input is a benign image, the model will correctly generate words describing the image, while if the input image is poisoned, the model will output the target words as the attacker expects"
  - [corpus]: Weak evidence; related papers discuss backdoor attacks but do not provide specific details on maintaining performance on benign images.
- **Break condition:** If the training process does not allow the model to learn the association between poisoned images and attacker-defined captions, or if the performance on benign images degrades significantly, the attack would fail.

### Mechanism 3
- **Claim:** The attack achieves high success rates (over 90% on Flickr8k and 96% on Flickr30k) while maintaining low false trigger rates (under 5%).
- **Mechanism:** The object-oriented poison crafting method and the training process ensure that the model learns to generate attacker-specified captions for poisoned images while maintaining good performance on benign images. The low false trigger rates indicate that the backdoor is stealthy and does not affect the model's behavior on benign images.
- **Core assumption:** The evaluation metrics (ASR and FTR) accurately measure the attack's success and stealthiness.
- **Evidence anchors:**
  - [abstract]: "The attack achieves high success rates (over 90% on Flickr8k and 96% on Flickr30k) while maintaining low false trigger rates (under 5%)"
  - [section]: "We adopt Attack Success Rate (ASR) to evaluate whether the attacked model can generate identical or approximate descriptions specified by the attacker" and "we adopt False Triggered Rate(FTR) to test whether the attacked model will generate target captions for benign images"
  - [corpus]: Weak evidence; related papers discuss backdoor attacks but do not provide specific details on the evaluation metrics used.
- **Break condition:** If the evaluation metrics do not accurately measure the attack's success and stealthiness, or if the attack fails to achieve high success rates or maintain low false trigger rates, the attack would be considered unsuccessful.

## Foundational Learning

- **Concept:** Object detection using YOLO-v3
  - **Why needed here:** The attack relies on YOLO-v3 to detect objects in the image, which are then used to craft the poisoned samples.
  - **Quick check question:** What is the purpose of using YOLO-v3 in this attack, and how does it contribute to the overall mechanism?

- **Concept:** Gaussian noise matrix generation
  - **Why needed here:** The attack uses a Gaussian noise matrix to modify pixel values in the detected object regions, creating the poisoned samples.
  - **Quick check question:** How does the Gaussian noise matrix contribute to the poison crafting process, and why is it used instead of other methods?

- **Concept:** Cross-entropy loss and Adam optimizer
  - **Why needed here:** The attack uses these components during the training process to learn the association between poisoned images and attacker-defined captions.
  - **Quick check question:** What role do cross-entropy loss and Adam optimizer play in the training process, and how do they contribute to the attack's success?

## Architecture Onboarding

- **Component map:** YOLO-v3 object detector -> Gaussian noise matrix generator -> Image captioning model (Show-Attend-and-Tell) -> Training and evaluation components

- **Critical path:**
  1. Detect objects in the image using YOLO-v3
  2. Generate a Gaussian noise matrix for each detected object region
  3. Modify pixel values in the object regions using the noise matrix
  4. Replace original captions with attacker-defined captions
  5. Train the image captioning model on the poisoned dataset
  6. Evaluate the attacked model on both benign and poisoned test sets

- **Design tradeoffs:**
  - Using YOLO-v3 for object detection may introduce errors if the detector fails to identify objects accurately.
  - The Gaussian noise matrix generation and pixel value modification may affect the visual quality of the poisoned images.
  - The training process must balance learning the association between poisoned images and attacker-defined captions while maintaining performance on benign images.

- **Failure signatures:**
  - Low ASR or high FTR during evaluation
  - Significant degradation in BLEU scores on benign test images
  - Inaccurate object detection by YOLO-v3

- **First 3 experiments:**
  1. Evaluate the object detection accuracy of YOLO-v3 on a sample of images from the dataset.
  2. Assess the visual quality of poisoned images by comparing them to the original images.
  3. Train the image captioning model on a small poisoned dataset and evaluate its performance on both benign and poisoned test sets.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How does the attack performance vary with different object detectors and their detection accuracy?
- **Basis in paper:** [inferred] The paper mentions that "due to the limitation of the adopted object detector, the objects in the image may not be detected accurately" and suggests future work to apply semantic segmentation for more precise object region identification.
- **Why unresolved:** The paper only uses YOLO-v3 as the object detector and doesn't explore how different object detection models or their varying accuracy levels would impact the backdoor attack's effectiveness.
- **What evidence would resolve it:** Conducting experiments with multiple object detection models (e.g., Faster R-CNN, RetinaNet) and analyzing the correlation between detection accuracy and attack success rate would provide concrete evidence.

### Open Question 2
- **Question:** What is the minimum poisoning rate required to achieve a successful backdoor attack while maintaining model performance?
- **Basis in paper:** [explicit] The paper mentions that "the poisoning rate in the training and validation dataset is both set to be 30%" but doesn't explore how varying this rate affects attack success and model performance.
- **Why unresolved:** The paper doesn't provide a systematic analysis of how different poisoning rates impact the trade-off between attack success rate and maintaining model performance on benign images.
- **What evidence would resolve it:** Conducting experiments with varying poisoning rates (e.g., 10%, 20%, 30%, 40%, 50%) and analyzing the corresponding attack success rates and model performance metrics would provide concrete evidence.

### Open Question 3
- **Question:** How does the backdoor attack perform against different image captioning model architectures beyond Show-Attend-and-Tell?
- **Basis in paper:** [explicit] The paper states "Although there are already many more advanced models, they share a similar encoder-decoder framework" but only tests the attack on Show-Attend-and-Tell with ResNet101.
- **Why unresolved:** The paper doesn't provide empirical evidence on how the backdoor attack performs against other popular image captioning architectures like Transformer-based models or other CNN-RNN combinations.
- **What evidence would resolve it:** Conducting experiments with multiple image captioning architectures (e.g., X-Transformer, VLP, Oscar) and comparing their vulnerability to the backdoor attack would provide concrete evidence.

### Open Question 4
- **Question:** What is the effectiveness of the backdoor attack on larger, more diverse datasets?
- **Basis in paper:** [inferred] The paper only tests on Flickr8k and Flickr30k datasets, which are relatively small and domain-specific, and mentions the need to test on "more kinds of models" in future work.
- **Why unresolved:** The paper doesn't explore how the attack scales to larger, more diverse datasets with different characteristics (e.g., COCO, Open Images, or domain-specific medical imaging datasets).
- **What evidence would resolve it:** Conducting experiments on larger, more diverse datasets with varying characteristics and analyzing how dataset size, diversity, and domain affect attack success rate and model performance would provide concrete evidence.

## Limitations

- The object detection component (YOLO-v3) is treated as a black box, and performance degradation would directly impact attack success
- The iterative pixel modification process lacks specific stopping criteria, making exact replication uncertain
- Limited dataset scope - only tested on Flickr8k and Flickr30k, which may not generalize to all image captioning scenarios

## Confidence

- **High Confidence:** Attack success rates (ASR) on the two test datasets are clearly reported and demonstrate effectiveness
- **Medium Confidence:** The object-oriented trigger generation mechanism is well-described but depends heavily on YOLO-v3 performance
- **Low Confidence:** The exact implementation details of noise application and iterative modification process are underspecified

## Next Checks

1. **Object Detection Reliability Test:** Evaluate YOLO-v3's object detection accuracy on a stratified sample of images from both datasets to establish baseline detection performance and identify failure modes.

2. **Noise Application Verification:** Create controlled experiments to test whether element-wise addition of the Gaussian noise matrix produces the claimed pixel modifications, comparing results with alternative noise application methods.

3. **Cross-Dataset Generalization Test:** Apply the attack to a different image captioning dataset (e.g., MSCOCO) with varying image content and caption complexity to assess whether the attack maintains its effectiveness beyond the original test sets.