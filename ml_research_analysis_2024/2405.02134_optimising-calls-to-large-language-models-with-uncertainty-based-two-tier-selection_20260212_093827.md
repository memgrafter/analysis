---
ver: rpa2
title: Optimising Calls to Large Language Models with Uncertainty-Based Two-Tier Selection
arxiv_id: '2405.02134'
source_url: https://arxiv.org/abs/2405.02134
tags:
- tasks
- small
- large
- llms
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work investigates the cost-performance trade-off in using
  large language models (LLMs) for text tasks. The authors propose a simpler approach
  than existing methods that require auxiliary models or multiple LLM calls.
---

# Optimising Calls to Large Language Models with Uncertainty-Based Two-Tier Selection

## Quick Facts
- arXiv ID: 2405.02134
- Source URL: https://arxiv.org/abs/2405.02134
- Reference count: 29
- Primary result: Margin Sampling achieves best performance in 25 out of 27 task-LLM setup comparisons

## Executive Summary
This paper proposes Margin Sampling, a method to optimize calls to large language models by using uncertainty from a smaller, cheaper model to decide whether to call the larger, more expensive model. The approach uses the margin between the two most likely tokens from the small model's output as a proxy for uncertainty. Experiments across nine tasks and three pairs of LLMs show that this method outperforms existing approaches like routing and hybrid strategies while being simpler to implement.

## Method Summary
Margin Sampling uses the uncertainty of a small LLM's output (measured by the margin between the two most likely tokens) to decide whether to call a larger LLM. For each input, the small model generates probabilities for the first token, and the difference between the top two probabilities is computed. If this margin falls below a threshold, the large model is called; otherwise, the small model's output is used. The threshold is dynamically adjusted based on early queries to adapt to task difficulty. The method is evaluated against routing, hybrid, and frugal strategies across nine tasks using three pairs of LLMs.

## Key Results
- Margin Sampling outperforms existing methods in 25 out of 27 task-LLM setup comparisons
- The method achieves better cost-performance trade-offs across varying budget constraints
- Dynamic threshold adaptation based on early queries stabilizes performance across different task difficulties

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Using the margin between the two most likely tokens of the small LLM captures uncertainty that correlates with whether the small model's answer will be wrong.
- Mechanism: For short generation tasks, the first token often determines the answer's correctness. A small margin means the small LLM is uncertain, suggesting the answer may be wrong and the large LLM should be called.
- Core assumption: The distribution of the first token contains sufficient signal about overall answer correctness for short generation tasks.
- Evidence anchors:
  - [abstract] "we use only the uncertainty of the generations of the small LLM as the decision criterion"
  - [section 3.2.2] "Margin fs (xi) = Pfs (yi = kt=1 1 | xi) − Pfs (yi = kt=1 2 | xi)"
  - [corpus] Weak: No direct citation to margin-based uncertainty in LLM optimization literature.

### Mechanism 2
- Claim: Cascading with margin sampling avoids training auxiliary models, reducing complexity and cost.
- Mechanism: The decision to call the large LLM is based on a threshold on the margin value computed from the small LLM's logits, with no additional model training or repeated calls needed.
- Core assumption: The marginal distribution of the first token is cheaper to compute than full sequence generation and still informative.
- Evidence anchors:
  - [abstract] "does not require data or running multiple calls to the small model"
  - [section 3.2.2] "we suggest using the uncertainty of the output, namely the margin"
  - [corpus] Missing: No corpus evidence of margin sampling applied to LLM call optimization.

### Mechanism 3
- Claim: Dynamic threshold adaptation based on early queries stabilizes performance across varying task difficulty.
- Mechanism: The threshold is initialized from the first 10 queries and updated as more queries arrive, allowing the policy to adjust to the distribution of margin values.
- Core assumption: The initial 10-query window provides a representative sample of margin distribution for threshold setting.
- Evidence anchors:
  - [section 3.3] "An initial threshold is calculated using the first 10 queries... the threshold is dynamically updated based on all past queries"
  - [corpus] Weak: No direct evidence that 10-query initialization is optimal.

## Foundational Learning

- Concept: Probability distribution over tokens from LLM logits
  - Why needed here: Margin is computed as a difference between probabilities of top two tokens; understanding this is essential to implement the method.
  - Quick check question: How do you convert raw logits from an LLM into a probability distribution over the vocabulary?

- Concept: Cascading vs. routing strategies in model selection
  - Why needed here: The paper compares margin sampling (cascading) against routing and hybrid strategies; knowing the difference is key to understanding the experimental setup.
  - Quick check question: In a cascading strategy, which model is always called first, and when is the second model called?

- Concept: Area Under the Curve (AUC) as a cost-accuracy tradeoff metric
  - Why needed here: Results are reported as AUC of accuracy over budget; understanding this metric is necessary to interpret performance claims.
  - Quick check question: What does a higher AUC indicate about a strategy's performance across different budget constraints?

## Architecture Onboarding

- Component map: Small LLM (inference + logit extraction) → Margin calculation → Threshold comparison → Decision to call large LLM or not → Large LLM (optional) → Accuracy and cost tracking
- Critical path: Small LLM → margin computation → threshold check → large LLM call decision
- Design tradeoffs: Margin sampling trades off no training cost for potentially suboptimal performance on long-generation tasks; dynamic threshold adds robustness but requires initial calibration queries
- Failure signatures: High false positive rate (calling large LLM too often) indicates threshold too low; high false negative rate (not calling large LLM when needed) indicates threshold too high
- First 3 experiments:
  1. Run margin sampling on a balanced binary classification dataset with Mistral 7B → Llama-2 70B pair; verify AUC > random baseline
  2. Compare margin sampling against FrugalGPT on SST-2 with Llama-2 13B → Llama-2 70B; record AUC difference
  3. Test margin sampling on NaturalQuestions with GPT-3 → GPT-4; confirm dynamic threshold stabilizes performance after initial 10 queries

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the proposed Margin Sampling method perform on long-generation tasks compared to short-generation tasks?
- Basis in paper: Inferred
- Why unresolved: The paper explicitly states that the experiments focus on short-generation tasks due to the additional complexity introduced by long-generation tasks for evaluation. The authors mention that generalizing the approach to long-generation tasks is left for future work.
- What evidence would resolve it: Conducting experiments on long-generation tasks using the Margin Sampling method and comparing the results with those obtained from short-generation tasks.

### Open Question 2
- Question: What is the impact of the cost ratio (rcost = cl/cs) on the performance of the Margin Sampling method compared to routing strategies?
- Basis in paper: Inferred
- Why unresolved: The paper investigates the effect of varying the cost ratio on the performance of different strategies, including Margin Sampling. However, it does not provide a detailed analysis of how the cost ratio specifically impacts the performance of Margin Sampling compared to routing strategies.
- What evidence would resolve it: Conducting experiments with different cost ratios and analyzing the performance of Margin Sampling and routing strategies to determine the impact of the cost ratio on their relative performance.

### Open Question 3
- Question: How does the performance of the Margin Sampling method change when using a larger number of training data points for the auxiliary models?
- Basis in paper: Inferred
- Why unresolved: The paper investigates the effect of training data on the performance of auxiliary models, but it does not provide a detailed analysis of how the performance of Margin Sampling changes with an increasing number of training data points.
- What evidence would resolve it: Conducting experiments with different numbers of training data points and analyzing the performance of Margin Sampling to determine the impact of the training data size on its performance.

### Open Question 4
- Question: How does the proposed Margin Sampling method perform in a multi-task setting with a larger number of tasks compared to the nine tasks used in the experiments?
- Basis in paper: Inferred
- Why unresolved: The paper demonstrates the performance of Margin Sampling in a multi-task setting with nine tasks, but it does not explore how the method performs with a larger number of tasks.
- What evidence would resolve it: Conducting experiments with a larger number of tasks and analyzing the performance of Margin Sampling in the multi-task setting to determine its scalability and robustness.

### Open Question 5
- Question: What is the impact of using different uncertainty measures, such as perplexity or entropy, compared to the margin used in the proposed Margin Sampling method?
- Basis in paper: Inferred
- Why unresolved: The paper uses the margin as the uncertainty measure in the proposed Margin Sampling method, but it does not explore the impact of using different uncertainty measures on the performance of the method.
- What evidence would resolve it: Conducting experiments using different uncertainty measures, such as perplexity or entropy, in the Margin Sampling method and comparing the results with those obtained using the margin to determine the impact of the choice of uncertainty measure on the performance.

## Limitations

- The method has only been tested on short-generation tasks, and its effectiveness on longer generation tasks remains unknown.
- The experiments use a limited number of LLM pairs and task types, which may not generalize to all possible use cases.
- The paper doesn't explore whether combining margin sampling with auxiliary models could yield even better performance.

## Confidence

**High Confidence**: The empirical results showing margin sampling outperforms routing and hybrid methods in 25 out of 27 setups. The experimental methodology is sound, with appropriate baselines and clear performance metrics.

**Medium Confidence**: The claim that margin sampling is a generally applicable approach for optimizing LLM calls. While results are positive across multiple domains, the limited scope of task types and LLM capabilities tested prevents strong generalization claims.

**Low Confidence**: The assertion that understanding signals within LLMs is superior to auxiliary models for optimizing calls. The paper doesn't test whether combining margin sampling with auxiliary models would yield even better results, leaving open the question of whether simplicity comes at a performance cost.

## Next Checks

1. Test margin sampling on tasks requiring longer generation sequences (e.g., summarization or creative writing) to validate whether first-token uncertainty remains predictive of final output quality.

2. Evaluate margin sampling with LLM pairs having larger capability gaps (e.g., 1B vs 70B parameter models) to determine if the method scales across different model sizes.

3. Compare margin sampling against a combined approach that uses both margin uncertainty and an auxiliary model to determine if the claimed simplicity advantage comes with measurable performance tradeoffs.