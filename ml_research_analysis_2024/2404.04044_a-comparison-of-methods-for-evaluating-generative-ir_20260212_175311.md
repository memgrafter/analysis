---
ver: rpa2
title: A Comparison of Methods for Evaluating Generative IR
arxiv_id: '2404.04044'
source_url: https://arxiv.org/abs/2404.04044
tags:
- relevance
- human
- query
- methods
- evaluation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper compares multiple methods for evaluating generative
  information retrieval (Gen-IR) systems, where responses are generated rather than
  drawn from a fixed corpus. The authors evaluate five methods: binary relevance,
  graded relevance, subtopic relevance, pairwise preferences, and embeddings-based
  similarity.'
---

# A Comparison of Methods for Evaluating Generative IR

## Quick Facts
- arXiv ID: 2404.04044
- Source URL: https://arxiv.org/abs/2404.04044
- Reference count: 40
- One-line primary result: Subtopic relevance emerges as a promising approach for Gen-IR evaluation, balancing autonomy and auditability

## Executive Summary
This paper compares multiple methods for evaluating generative information retrieval (Gen-IR) systems, where responses are generated rather than drawn from a fixed corpus. The authors evaluate five methods: binary relevance, graded relevance, subtopic relevance, pairwise preferences, and embeddings-based similarity. They validate these methods against human assessments using TREC Deep Learning Track datasets and then apply them to evaluate outputs from several generative models. Subtopic relevance emerges as a promising approach, balancing autonomy and auditability. Pairwise preferences provide the best performance when exemplars are available but are computationally expensive. The study provides a framework for assessing Gen-IR systems and highlights the potential of LLM-based evaluation methods in this emerging field.

## Method Summary
The paper validates five evaluation methods for Gen-IR systems using TREC Deep Learning Track 2019 and 2020 datasets with 43 and 54 queries respectively. The methods include binary relevance, graded relevance, subtopic relevance, pairwise preferences, and embeddings-based similarity. Validation is performed by comparing LLM-generated labels against human qrels. The methods are then applied to evaluate responses from four LLM models (gpt-3.5-turbo, gpt-4, llama2 7b chat, llama2 13b chat) and their "liar" variants. Agreement with human assessment, suitability for human auditing, and autonomous operation are measured as key metrics.

## Key Results
- Subtopic relevance offers the best balance between autonomy and auditability among the evaluated methods
- Pairwise preferences provide the highest agreement with human judgments when exemplars are available
- Embeddings-based similarity performs poorly in validation, showing weak correlation with human relevance judgments
- Binary and graded relevance methods show lower agreement with human assessments compared to subtopic and pairwise approaches

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLM-based evaluation can replace human assessors while maintaining alignment with human judgment.
- Mechanism: LLMs are prompted to judge relevance, subtopic coverage, or pairwise preference, mirroring human judgment processes but at scale and without cost.
- Core assumption: LLMs can replicate human judgment accurately enough to serve as the primary evaluation source.
- Evidence anchors: Abstract states LLMs demonstrate "capabilities similar or superior to crowdsourced labels"; section assumes methods "must largely depend on LLM-generated labels."
- Break condition: LLM outputs diverge significantly from human judgment in validation experiments, especially for nuanced relevance distinctions.

### Mechanism 2
- Claim: Subtopic-based evaluation offers a balance between autonomy and auditability.
- Mechanism: LLMs generate a set of subtopics from the query; responses are scored by the proportion of subtopics they address, allowing automated scoring while keeping evaluation criteria interpretable.
- Core assumption: Subtopics can be automatically generated that reflect human notions of relevance without manual definition.
- Evidence anchors: Section notes "subtopics often include partial answers or relevant information not appearing in the query"; offers "greater explainability and interpretability as it can highlight specific subtopics."
- Break condition: Generated subtopics miss core aspects of relevance or are too numerous/few to meaningfully discriminate responses.

### Mechanism 3
- Claim: Pairwise preference judgments provide the most accurate alignment with human preferences.
- Mechanism: LLMs compare two responses and select the preferred one, directly capturing human preference order without intermediate relevance scales.
- Core assumption: Direct comparison of two responses is more reliable than absolute scoring.
- Evidence anchors: Section states "there are fewer disagreements and ties" in pairwise preference validation; "pairwise preferences provide the best overall performance."
- Break condition: Computational cost becomes prohibitive for large-scale evaluation or exemplars are unavailable/insufficient.

## Foundational Learning

- Concept: Binary relevance assessment
  - Why needed here: Baseline method for determining if a generated response answers the query at all.
  - Quick check question: What is the difference between binary relevance and graded relevance?

- Concept: Subtopic extraction and coverage
  - Why needed here: Allows automated scoring based on multiple aspects of relevance without predefined criteria.
  - Quick check question: How do subtopics differ from nuggets in prior literature?

- Concept: Pairwise preference evaluation
  - Why needed here: Directly captures human preference order between responses, avoiding calibration issues of graded scales.
  - Quick check question: Why might pairwise preferences be more reliable than absolute relevance scores?

## Architecture Onboarding

- Component map: Query → LLM prompt → Generated subtopics (for subtopic method) OR → Direct judgment (binary/graded) OR → Pairwise comparison → Exemplar response storage (for pairwise and embedding methods) → LLM evaluator (gpt-4 used in experiments) → Validation pipeline comparing LLM judgments to human qrels

- Critical path:
  - For subtopic method: Generate subtopics → Score response coverage → Aggregate score
  - For pairwise: Prepare exemplar pairs → LLM preference judgment → Aggregate preferences

- Design tradeoffs:
  - Subtopic: More autonomous but may miss nuances; pairwise: Most accurate but expensive
  - Binary: Simple and auditable but coarse; embeddings: Fast but indirect and unauditable

- Failure signatures:
  - Subtopic: Generated subtopics miss key aspects or are too generic
  - Pairwise: LLM preferences inconsistent across prompt orders or exemplars poor quality
  - Embeddings: Similarity scores don't correlate with human relevance judgments

- First 3 experiments:
  1. Validate binary relevance on a small query set with human-labeled qrels
  2. Generate subtopics for 10 queries and manually audit their quality
  3. Compare pairwise preferences using two different exemplars for the same query

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do different evaluation methods (binary relevance, graded relevance, subtopic relevance, pairwise preferences, embeddings) compare in their ability to detect performance differences between Gen-IR systems across diverse query types?
- Basis in paper: [explicit] The paper compares these five methods and validates them against human assessments, showing varying degrees of agreement and disagreement
- Why unresolved: The paper only tests on two years of TREC DL data with passage-level tasks. Different query types (e.g., factoid vs. complex reasoning) may require different evaluation approaches
- What evidence would resolve it: Testing these methods across multiple diverse datasets with different query types and comparing results to human judgments

### Open Question 2
- Question: What is the optimal balance between autonomy and auditability in Gen-IR evaluation methods?
- Basis in paper: [explicit] The paper identifies three requirements (R1, R2, R3) and discusses the trade-offs between autonomous evaluation and human auditing
- Why unresolved: The paper finds that subtopic relevance offers a reasonable compromise, but doesn't definitively establish the optimal balance point
- What evidence would resolve it: Empirical studies measuring the effectiveness of different evaluation methods with varying levels of human oversight across multiple Gen-IR systems

### Open Question 3
- Question: How well do current Gen-IR evaluation methods handle the challenges of conversational and personalized search?
- Basis in paper: [inferred] The paper acknowledges this limitation in Section 5, noting that their work is limited to traditional "ad hoc" scenarios
- Why unresolved: The experiments only evaluate single responses to queries in isolation, not considering context or personalization
- What evidence would resolve it: Testing evaluation methods on conversational Gen-IR systems with contextual queries and personalized responses, measuring agreement with human preferences in these settings

## Limitations

- The embeddings-based method performs poorly in validation, indicating semantic similarity approaches may not align well with human notions of relevance for generated responses
- The study is limited to traditional "ad hoc" search scenarios and doesn't address challenges in conversational or personalized search
- The paper doesn't definitively establish the optimal balance between autonomy and auditability in Gen-IR evaluation methods

## Confidence

- **High confidence** in the observation that pairwise preferences provide the best alignment with human judgments when exemplars are available
- **Medium confidence** in the conclusion that subtopic relevance offers the best balance between autonomy and auditability
- **Low confidence** in the embeddings-based method's utility for Gen-IR evaluation, given its poor validation performance

## Next Checks

1. Conduct a manual audit of automatically generated subtopics to verify they capture human-relevant aspects of the queries and responses
2. Test the pairwise preference method with different exemplar selection strategies to determine optimal exemplar quality thresholds
3. Evaluate whether few-shot prompting improves the performance of binary and graded relevance methods to narrow the gap with human judgments