---
ver: rpa2
title: Enhancing Diversity in Bayesian Deep Learning via Hyperspherical Energy Minimization
  of CKA
arxiv_id: '2411.00259'
source_url: https://arxiv.org/abs/2411.00259
tags:
- ensemble
- he-cka
- learning
- deep
- layer
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of improving diversity and uncertainty
  estimation in Bayesian deep learning ensembles and hypernetworks. The authors propose
  using Centered Kernel Alignment (CKA) on feature kernels to measure similarity between
  networks, then apply hyperspherical energy (HE) minimization on top of CKA kernels
  to promote diversity.
---

# Enhancing Diversity in Bayesian Deep Learning via Hyperspherical Energy Minimization of CKA

## Quick Facts
- arXiv ID: 2411.00259
- Source URL: https://arxiv.org/abs/2411.00259
- Reference count: 35
- Primary result: Achieves 99.86% AUROC on SVHN and 99.99% on Fashion-MNIST for OOD detection

## Executive Summary
This paper introduces HE-CKA, a method that combines Centered Kernel Alignment (CKA) with hyperspherical energy (HE) minimization to improve diversity in Bayesian deep learning ensembles and hypernetworks. The approach addresses the challenge of optimizing diversity when direct CKA minimization suffers from vanishing gradients for similar models. By applying HE on top of CKA kernels and introducing synthetic out-of-distribution examples with feature repulsive terms, the method significantly improves uncertainty quantification and outlier detection while maintaining competitive accuracy across multiple datasets.

## Method Summary
HE-CKA uses CKA on feature Gram matrices to measure network similarity, then applies HE minimization on the hypersphere to provide stable gradient signals for diversity optimization. The method introduces synthetic OOD examples and feature repulsive terms to enhance uncertainty estimation. Layer-wise weighting allows selective diversity enforcement, focusing on later layers where diversity has the most impact on final predictions. The approach is applied to both ensembles and hypernetworks, with specific training procedures including AdamW optimizer, SGD, and various learning rates depending on the task.

## Key Results
- Achieves near-perfect separation of inliers and outliers (99.86% AUROC on SVHN, 99.99% on Fashion-MNIST)
- Outperforms baselines in uncertainty quantification for both synthetic and realistic outlier detection tasks
- Maintains competitive accuracy while substantially improving OOD detection capabilities
- Demonstrates effectiveness across diverse datasets including MNIST, CIFAR, and TinyImageNet

## Why This Works (Mechanism)

### Mechanism 1
- Claim: HE-CKA provides better gradient signals for diversity optimization than direct CKA minimization.
- Mechanism: When two models are already very similar (CKA near 1), direct CKA minimization yields vanishing gradients due to the cosine derivative approaching zero. HE-CKA transforms CKA values into geodesic distances on the hypersphere, then applies an inverse power law (1/d^s) to maintain strong gradients even for nearby points.
- Core assumption: Uniform distribution of feature Gram matrices on the unit hypersphere maximizes model diversity in function space.
- Evidence anchors:
  - [abstract]: "Noting that CKA projects kernels onto a unit hypersphere and that directly optimizing the CKA objective leads to diminishing gradients when two networks are very similar."
  - [section 3.3]: "the gradient of cos(ϕ) is − sin(ϕ), which is close to 0 when ϕ is close to 0. In other words, if two models are already very similar to each other (their CKA being close to 1), then optimizing with CKA may not provide enough gradient to move them apart."
  - [corpus]: Weak - no direct evidence found about gradient vanishing in similar work, though the mechanism aligns with known issues in high-dimensional similarity optimization.
- Break condition: If the feature Gram matrices cannot be meaningfully projected onto a hypersphere (e.g., rank-deficient Gram matrices or pathological network architectures).

### Mechanism 2
- Claim: Synthetic OOD examples with feature repulsive terms improve uncertainty estimation without degrading in-distribution accuracy.
- Mechanism: By generating synthetic outliers that are clearly non-overlapping with in-distribution data, and applying HE-CKA minimization to these examples, the ensemble learns to produce diverse, uncertain predictions on OOD data while maintaining high confidence on inliers. The entropy maximization term ensures outlier predictions are uncertain.
- Core assumption: Feature diversity on synthetic outliers correlates with uncertainty in prediction space, and these outliers can be generated without requiring proximity to the true data manifold.
- Evidence anchors:
  - [abstract]: "Additionally, by leveraging CKA-based feature kernels, we derive feature repulsive terms applied to synthetically generated outlier examples."
  - [section 4.2]: "Intuitively, we want more diverse features on obvious outlier examples to indicate uncertainty because the networks trained on these examples should not be confident."
  - [corpus]: Weak - no direct evidence found about synthetic outlier generation effectiveness, though the mechanism aligns with known data augmentation principles.
- Break condition: If the synthetic OOD examples are too similar to in-distribution data, causing the repulsive term to interfere with inlier performance.

### Mechanism 3
- Claim: Layer-wise weighting in HE-CKA allows selective diversity enforcement where it's most beneficial.
- Mechanism: Early layers tend to have high similarity across models regardless of diversity efforts, while later layers show more variation. By weighting later layers more heavily in the HE-CKA loss, the method focuses diversity promotion where it matters most for final predictions.
- Core assumption: Feature similarity patterns across layers are consistent enough across different architectures to justify a general weighting scheme.
- Evidence anchors:
  - [section C.1]: "it has been empirically shown that the first few layers of deep neural networks have high similarity (Kornblith et al., 2019), which indicates that initial layers learn more aligned features."
  - [section 4]: "we found that using a weighting scheme in Eq. (4) allowed for finer control of the repulsive term."
  - [corpus]: Weak - no direct evidence found about layer-wise similarity patterns, though the mechanism aligns with known hierarchical feature learning principles.
- Break condition: If the architecture has fundamentally different layer similarity patterns (e.g., extreme depth or unusual skip connections).

## Foundational Learning

- Concept: Centered Kernel Alignment (CKA) for neural network comparison
  - Why needed here: Provides permutation-invariant, high-dimensional similarity metric between network feature representations
  - Quick check question: How does CKA handle the permutation invariance problem that plagues naive parameter or activation distance metrics?

- Concept: Hyperspherical Energy (HE) minimization
  - Why needed here: Provides stable gradient signals for diversity optimization when direct similarity minimization fails
  - Quick check question: What mathematical property of the HE kernel ensures gradients remain meaningful even for very similar models?

- Concept: Particle-based variational inference
  - Why needed here: Frames ensemble training as approximating a posterior distribution, justifying the diversity-promoting priors
  - Quick check question: How does adding a HE-CKA term to the ensemble loss relate to the KL-divergence minimization in particle-based VI?

## Architecture Onboarding

- Component map:
  Feature Gram matrix construction → Centered normalization → Hyperspherical projection → HE-CKA energy calculation → Loss integration
  Synthetic OOD generator → Feature Gram matrix construction → HE-CKA energy calculation → Entropy maximization → Loss integration
  Layer weighting module → Multiplicative scaling of layer-specific HE-CKA terms

- Critical path:
  1. Forward pass through ensemble to extract features at specified layers
  2. Construct Gram matrices from features
  3. Center and normalize Gram matrices
  4. Compute pairwise HE-CKA energies with layer weighting
  5. Backpropagate through the weighted sum of energies
  6. Add entropy term for OOD examples if applicable

- Design tradeoffs:
  - Memory vs. diversity: Computing HE-CKA on all layers maximizes diversity but increases memory usage quadratically with batch size
  - Repulsion strength vs. accuracy: Stronger γ values increase diversity but may harm in-distribution performance
  - OOD generation complexity vs. effectiveness: More sophisticated OOD generation improves uncertainty estimation but increases computational overhead

- Failure signatures:
  - Vanishing gradients: Loss plateaus early, suggesting repulsion term is too weak or smoothing is too aggressive
  - Degraded accuracy: In-distribution performance drops significantly, indicating over-regularization
  - Mode collapse in hypernetworks: Generated networks become too similar, suggesting insufficient diversity promotion
  - Memory overflow: CUDA out of memory errors during Gram matrix computation

- First 3 experiments:
  1. Implement HE-CKA on a simple 2D classification ensemble (like the four-quadrant dataset) to visually verify diversity promotion
  2. Compare HE-CKA vs. direct CKA minimization on synthetic data to demonstrate gradient stability
  3. Test synthetic OOD generation on MNIST ensemble to verify uncertainty improvement without accuracy degradation

## Open Questions the Paper Calls Out
None

## Limitations
- Synthetic OOD generation relies on heuristic noise combinations without rigorous validation of distributional properties
- Layer weighting scheme is tuned per-dataset without theoretical justification for optimal weighting patterns
- Method effectiveness unverified for regression, sequence modeling, or architectures beyond CNNs

## Confidence
- High confidence: Core mechanism of using HE to overcome gradient vanishing in CKA minimization is well-founded theoretically
- Medium confidence: Synthetic OOD generation approach shows strong empirical results but lacks ablation studies on different strategies
- Low confidence: Layer weighting scheme's generalizability across architectures is based on limited empirical observations

## Next Checks
1. **Ablation on synthetic OOD generation**: Systematically replace heuristic noise combinations with different OOD generation strategies to determine if diversity improvements are specific to proposed method or more general.

2. **Layer weighting transferability test**: Apply same layer weighting scheme across different architectures (ResNet, DenseNet, ViT) on same dataset to verify if early-layer similarity pattern is architecture-agnostic.

3. **Computational complexity scaling analysis**: Measure training time and memory usage as function of ensemble size, batch size, and network depth to establish practical scalability limits.