---
ver: rpa2
title: 'Scheduled Curiosity-Deep Dyna-Q: Efficient Exploration for Dialog Policy Learning'
arxiv_id: '2402.00085'
source_url: https://arxiv.org/abs/2402.00085
tags:
- agent
- dialog
- curiosity
- learning
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a curiosity-driven curriculum learning framework,
  SC-DDQ, to improve dialog policy learning efficiency. It combines intrinsic motivation
  via a curiosity model with task-difficulty-based curriculum scheduling, building
  on Deep Dyna-Q.
---

# Scheduled Curiosity-Deep Dyna-Q: Efficient Exploration for Dialog Policy Learning

## Quick Facts
- arXiv ID: 2402.00085
- Source URL: https://arxiv.org/abs/2402.00085
- Authors: Xuecheng Niu; Akinori Ito; Takashi Nose
- Reference count: 40
- Primary result: Curiosity-driven curriculum learning improves dialog policy efficiency, achieving up to 0.92 success rate versus 0.86 for DDQ

## Executive Summary
This paper introduces SC-DDQ, a curiosity-driven curriculum learning framework that combines intrinsic motivation with task-difficulty-based scheduling to improve dialog policy learning efficiency. The framework modifies the ICM curiosity model for task-oriented dialog systems and integrates it with Deep Dyna-Q's planning mechanism. Experiments on a movie-ticket booking dataset with 137 user goals demonstrate that SC-DDQ outperforms baseline methods, particularly when using easy-first scheduling for curiosity-equipped models. The study also reveals that traditional easy-first curriculum learning is not universally optimal, with difficult-first strategies being more suitable for models without curiosity.

## Method Summary
The SC-DDQ framework combines curriculum learning with curiosity-driven exploration for task-oriented dialog policy learning. It modifies the ICM curiosity model to generate intrinsic rewards based on prediction errors between real and predicted next states. The framework uses four variants: DDQ (no curiosity, random), C-DDQ (with curiosity, random), S-DDQ (no curiosity, scheduled), and SC-DDQ (with curiosity, scheduled). Training occurs over 300 epochs with warm start via rule-based agent, then alternating real and simulated experiences. The movie-ticket booking dataset contains 137 user goals categorized by difficulty (61 easy, 33 middle, 43 difficult) based on request slot count.

## Key Results
- SC-DDQ achieves 0.92 success rate versus 0.86 for DDQ baseline
- Easy-first scheduling benefits curiosity-equipped models while difficult-first benefits curiosity-free models
- High action-sampling entropy in early training correlates with better final performance
- SC-DDQ reduces average dialog turns compared to baselines

## Why This Works (Mechanism)

### Mechanism 1
Curiosity-driven exploration with intrinsic reward improves policy learning by encouraging visits to unfamiliar dialog states. The curiosity model outputs rewards proportional to prediction error between real and predicted next states, with higher error meaning higher curiosity reward. Core assumption: prediction error is a valid proxy for state novelty. Break condition: if state space is fully deterministic or prediction model becomes too accurate.

### Mechanism 2
Scheduling curriculum learning with opposite strategies yields complementary benefits depending on whether curiosity is present. Easy-first schedules provide quick success rate boosts but may cause policy collapse when difficulty increases; difficult-first schedules build robustness early but can stall without curiosity to maintain exploration. Core assumption: agent's sampling entropy early in training correlates with later success. Break condition: if task distribution changes during training.

### Mechanism 3
Entropy of sampled actions in early training stages is positively correlated with final task success rate. High entropy means the agent tries many actions early, avoiding premature convergence to suboptimal policies; low entropy in late stages allows exploitation. Core assumption: policy optimization landscape benefits from broad early exploration before narrowing focus. Break condition: if action space is very small or reward signal is sparse.

## Foundational Learning

- **Reinforcement Learning and Q-learning basics**: The policy is optimized via Deep Q-learning and updated with both real and simulated experiences. Quick check: What is the difference between on-policy and off-policy RL in the context of DDQ's planning step?

- **Curriculum Learning principles**: The framework deliberately orders tasks by difficulty to mimic human learning patterns and improve stability. Quick check: How does the easy-first schedule differ from self-paced learning in terms of task selection criteria?

- **Intrinsic motivation and curiosity models**: The curiosity module generates internal reward signals based on prediction error to drive exploration beyond external rewards. Quick check: In what way does the ICM-based curiosity model differ from count-based exploration methods?

## Architecture Onboarding

- **Component map**: User Simulator → Dialog State → Curiosity Policy Agent (DQN + Curiosity MLP) → Action → World Model (MLP) → Reward + Next State → Experience Buffers (Real/Simulated) → Update DQN, World Model, Curiosity Model

- **Critical path**: State observation → curiosity reward + Q-value → action selection → environment step → experience storage → model updates (DQN, world, curiosity)

- **Design tradeoffs**: Curiosity vs computational cost (running curiosity MLP for every candidate action adds overhead); scheduled vs adaptive curricula (fixed schedules are simpler but less flexible); simulated vs real experiences (planning speeds learning but may diverge if world model is inaccurate)

- **Failure signatures**: Vanishing curiosity rewards if prediction model becomes too accurate; catastrophic forgetting if replay buffers are too small or imbalanced; suboptimal policies if curriculum schedule mismatches agent learning dynamics

- **First 3 experiments**: 1) Run DDQ vs DQN on movie-ticket dataset to confirm baseline gap; 2) Run SC-DDQ with EFS schedule, measure entropy and success rate per epoch; 3) Run S-DDQ with DFS schedule, compare against EFS to confirm opposite-strategy benefits

## Open Questions the Paper Calls Out

- **How does the performance of SC-DDQ vary with different weighting schemes for curiosity rewards during training?**: The current implementation uses fixed weighting, and future work plans to control curiosity-driven exploration based on entropy trends.

- **How would the SC-DDQ framework perform in dialog systems with more complex state representations containing interfering factors?**: The current implementation is tailored for movie-ticket booking scenarios, and it's unclear how it would handle more complex environments.

- **What is the optimal schedule for gradually reducing curiosity-driven exploration during training?**: While the paper identifies the trend of decreasing entropy correlating with improved performance, it does not specify how to schedule the reduction of curiosity-driven exploration.

## Limitations

- Performance claims are primarily validated on a single movie-ticket booking dataset with 137 user goals, limiting generalizability
- Curiosity mechanism's effectiveness depends heavily on accurate state prediction, which may degrade in more complex or partially observable environments
- Fixed curriculum schedules may not adapt well to changing task distributions or agent learning dynamics

## Confidence

- **High confidence**: The empirical comparison between DDQ and DQN baselines, showing consistent performance improvements
- **Medium confidence**: The effectiveness of curiosity-driven exploration for dialog policy learning, supported by direct experimental results but limited to one domain
- **Low confidence**: The universal optimality of easy-first schedule for curiosity-equipped models versus difficult-first for curiosity-free models, as this claim is based on a single experimental setup

## Next Checks

1. Test SC-DDQ on a different task-oriented dialog domain (e.g., restaurant booking) to verify cross-domain effectiveness
2. Implement an adaptive scheduling mechanism that adjusts difficulty based on agent performance rather than using fixed schedules
3. Conduct ablation studies to quantify the individual contributions of curiosity rewards versus curriculum scheduling to overall performance improvements