---
ver: rpa2
title: 'Exploring Social Media Image Categorization Using Large Models with Different
  Adaptation Methods: A Case Study on Cultural Nature''s Contributions to People'
arxiv_id: '2410.00275'
source_url: https://arxiv.org/abs/2410.00275
tags:
- learning
- large
- image
- images
- social
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of categorizing social media
  images into Cultural Ecosystem Services (CES) classes, such as "Cultural-Religious,"
  "Fauna-Flora," "Gastronomy," "Landscape-Nature," "Sports," and "Urban-Rural." The
  authors introduce FLIPS, a new dataset of Flickr images from mountain national parks,
  and evaluate various large model-based approaches for this task. These approaches
  use Large Vision Models (LVMs), Large Language Models (LLMs), and Large Vision-Language
  Models (LVLMs) adapted through supervised learning, in-context learning, and hybrid
  methods.
---

# Exploring Social Media Image Categorization Using Large Models with Different Adaptation Methods: A Case Study on Cultural Nature's Contributions to People

## Quick Facts
- **arXiv ID**: 2410.00275
- **Source URL**: https://arxiv.org/abs/2410.00275
- **Reference count**: 10
- **Primary result**: DINOv2 LVM with lightweight fine-tuning achieved 97.08% accuracy on categorizing social media images into Cultural Ecosystem Services classes

## Executive Summary
This paper addresses the challenge of categorizing social media images into Cultural Ecosystem Services (CES) classes using large models. The authors introduce FLIPS, a new dataset of Flickr images from mountain national parks, and evaluate five different approaches combining Large Vision Models (LVMs), Large Language Models (LLMs), and Large Vision-Language Models (LVLMs). The study demonstrates that large models are highly effective for this task, with self-supervised LVMs achieving the highest accuracy when fine-tuned. The research provides a comprehensive evaluation of different adaptation methods including supervised learning, in-context learning, and hybrid approaches for CES image classification.

## Method Summary
The study evaluates five approaches for CES image categorization using different large model architectures and adaptation methods. The FLIPS dataset contains 960 images from eight Spanish/Portuguese mountain national parks, evenly distributed across six CES classes. Approach (4) uses DINOv2 ViT-L/14 with lightweight fine-tuning (freezing the backbone, training a linear classifier head for 100 epochs at learning rate 2×10⁻³). Approach (2) employs LLaVA-1.5 with simple prompt engineering ("Describe the image. Keep your response short."). Approach (3) uses GPT-4o with extended prompts. Approach (5) combines UMAP dimensionality reduction, KMeans clustering, and Flan-T5 zero-shot classification. The study compares supervised learning (fine-tuning), in-context learning (prompting, few-shot), and hybrid (unsupervised + in-context) adaptation methods.

## Key Results
- DINOv2 LVM with lightweight fine-tuning achieved the highest accuracy at 97.08%
- LLaVA-1.5 with simple prompting reached 88.85% accuracy
- GPT-4o with extended prompts also performed well at 88.85% accuracy
- LVLMs demonstrated strong performance using prompt engineering without fine-tuning

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LVMs trained via self-supervised learning encode rich visual representations that transfer effectively to CES categorization when fine-tuned.
- Mechanism: DINOv2 learns visual embeddings through instance discrimination without labels, then adapts to CES classes by training only the classifier head (lightweight fine-tuning).
- Core assumption: Visual patterns relevant to CES categories are present in the learned embedding space and can be mapped via a linear classifier.
- Evidence anchors:
  - The best-performing model was DINOv2 LVM with lightweight fine-tuning, achieving 97.08% accuracy.
  - DINOv2 was trained on large, curated, and diverse image datasets, allowing it to produce high-quality, transferable features suitable for a wide array of applications.

### Mechanism 2
- Claim: LVLMs perform well on CES categorization using zero-shot prompting by combining visual and language understanding.
- Mechanism: LLaVA-1.5 or GPT-4o ingests images, generates a caption, and the language component classifies the caption into CES classes using prompt engineering.
- Core assumption: The semantic definitions of CES classes can be encoded into prompts so that the LVLM's language component can correctly classify captions.
- Evidence anchors:
  - LVLMs like GPT-4o performed well with prompt engineering, reaching 88.85% accuracy.
  - These models allow for a more interactive communication between humans and machines, which make it especially suitable for non AI-expert users.

### Mechanism 3
- Claim: Combining unsupervised clustering with zero-shot classification improves performance on CES tasks by leveraging both visual grouping and language reasoning.
- Mechanism: LLaVA-1.5 generates descriptions, SBERT creates embeddings, UMAP reduces dimensions, clustering groups similar images, and Flan-T5 assigns cluster labels via zero-shot text classification.
- Core assumption: Images within the same CES class will cluster together in embedding space, and cluster-level descriptions can be mapped to CES classes without labeled examples.
- Evidence anchors:
  - We explore a combined approach involving dimensionality reduction followed by clustering of model embeddings, paired with zero-shot learning for classification.
  - UMAP is used for dimensionality reduction, optimized with 15 neighbors, 20 output dimensions, and the cosine distance metric.

## Foundational Learning

- Concept: Self-supervised learning (SSL) for vision
  - Why needed here: Enables training LVMs like DINOv2 on large unlabeled image datasets, creating rich features transferable to CES categorization.
  - Quick check question: What is the pretext task used in DINOv2's self-supervised training?

- Concept: Prompt engineering for zero-shot classification
  - Why needed here: Allows LVLMs to classify images into CES categories without fine-tuning by guiding the model with carefully structured prompts.
  - Quick check question: How does adding class definitions to a prompt improve classification accuracy?

- Concept: Clustering with dimensionality reduction
  - Why needed here: Groups visually similar images before classification, reducing the problem complexity when labels are scarce.
  - Quick check question: Why is UMAP preferred over PCA for preserving local structure in high-dimensional embeddings?

## Architecture Onboarding

- Component map: Input image → Vision encoder (LVM/LVLM) → Embeddings → (Optional) Dimension reduction → Clustering (if hybrid) → Classification (fine-tuned head or zero-shot prompt) → CES label output
- Critical path: Vision model inference → Embedding generation → (Optional) clustering step → Classification → Output
- Design tradeoffs:
  - Accuracy vs. cost: LVMs + fine-tuning achieve highest accuracy but require GPU resources; LVLMs + prompting are cheaper but less accurate.
  - Complexity vs. usability: Hybrid approach can improve accuracy but increases pipeline complexity; prompting is simpler for non-experts.
- Failure signatures:
  - Low accuracy across classes: Embedding space may not capture CES semantics; prompt may be ambiguous.
  - High precision but low recall: Clustering may over-segment or model may be too conservative.
  - Inconsistent results: Prompt sensitivity or randomness in clustering initialization.
- First 3 experiments:
  1. Run DINOv2 with lightweight fine-tuning on a small subset of CES data to verify transfer learning effectiveness.
  2. Test GPT-4o with both simple and extended prompts on the same subset to measure prompt engineering impact.
  3. Apply the hybrid UMAP+KMeans+Flan-T5 pipeline to a balanced sample to assess clustering quality and zero-shot mapping accuracy.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can ensemble approaches combining multiple large models further improve classification accuracy for CES categories?
- Basis in paper: The paper states that "optimal accuracy across all CES classes could potentially be achieved through an ensemble of large models" but notes this would increase computational costs and environmental impact.
- Why unresolved: The study only evaluated individual models and did not implement or test ensemble approaches due to computational constraints.
- What evidence would resolve it: Experimental results comparing single-model performance against various ensemble configurations (e.g., weighted voting, stacking) on the same dataset, including computational cost and accuracy trade-offs.

### Open Question 2
- Question: How well do large models generalize to CES categorization across different geographic regions and cultural contexts beyond mountain national parks?
- Basis in paper: The study used images from eight mountain national parks in Spain and Portugal, but did not test cross-regional generalization or cultural context adaptation.
- Why unresolved: The dataset was limited to a specific geographic and cultural setting, making it unclear if the models would perform similarly in other regions or cultural contexts.
- What evidence would resolve it: Testing the same large models on CES categorization datasets from different countries, cultures, and landscape types, comparing performance across regions.

### Open Question 3
- Question: What is the optimal balance between model accuracy and environmental sustainability when deploying large models for social media image categorization?
- Basis in paper: The paper explicitly discusses the trade-off between higher accuracy through ensemble approaches and increased "computational costs and environmental impact due to higher CO2 emissions."
- Why unresolved: The study did not quantify the environmental impact (e.g., carbon footprint) of different models or adaptation methods, focusing only on accuracy metrics.
- What evidence would resolve it: Empirical measurements of CO2 emissions and energy consumption for different model architectures, adaptation methods, and batch sizes, correlated with classification accuracy to determine optimal trade-offs.

## Limitations

- The FLIPS dataset contains only 960 images from eight mountain national parks in Spain and Portugal, which may not represent the full diversity of social media imagery or CES categories globally.
- The study does not report class imbalance statistics or test on external datasets, raising concerns about overfitting to this specific context.
- While LVLMs achieve reasonable accuracy through prompting, the exact prompt templates beyond brief descriptions are not fully specified, making replication challenging.

## Confidence

**High confidence**: The effectiveness of self-supervised LVMs (DINOv2) for visual feature extraction and transfer learning to CES categorization, supported by strong quantitative results and clear training methodology.

**Medium confidence**: The performance of LVLMs with prompt engineering for CES classification, as results are promising but prompt engineering details are limited and may not generalize across different CES definitions or cultural contexts.

**Low confidence**: The hybrid clustering-plus-zero-shot approach, as the paper provides minimal details on clustering parameters, evaluation methodology, and how cluster assignments map to final CES classes.

## Next Checks

1. **Dataset generalization test**: Evaluate DINOv2 fine-tuning performance on an independent social media image dataset with the same CES classes to verify that the 97.08% accuracy generalizes beyond the FLIPS dataset.

2. **Prompt ablation study**: Systematically test variations of the GPT-4o prompt structure (simple description vs. extended prompts with class definitions) on a held-out validation set to quantify the impact of prompt engineering on the 88.85% accuracy.

3. **Hybrid pipeline component analysis**: Run the UMAP+KMeans+Flan-T5 pipeline with different numbers of clusters (k=3, 6, 9) and measure how clustering quality (silhouette score) correlates with final CES classification accuracy to determine if the hybrid approach provides benefits beyond simple zero-shot prompting.