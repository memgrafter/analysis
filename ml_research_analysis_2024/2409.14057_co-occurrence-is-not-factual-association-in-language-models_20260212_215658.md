---
ver: rpa2
title: Co-occurrence is not Factual Association in Language Models
arxiv_id: '2409.14057'
source_url: https://arxiv.org/abs/2409.14057
tags:
- layer
- ablation
- factual
- knowledge
- direction
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This work investigates how language models learn factual knowledge
  from text. It identifies two forms of knowledge representation: co-occurrence statistics
  (easily learned but poorly generalizable) and factual associations (harder to learn
  but generalizable to reasoning tasks).'
---

# Co-occurrence is not Factual Association in Language Models

## Quick Facts
- arXiv ID: 2409.14057
- Source URL: https://arxiv.org/abs/2409.14057
- Authors: Xiao Zhang; Miao Li; Ji Wu
- Reference count: 40
- Key outcome: Language models learn co-occurrence statistics in middle layers and factual associations in lower layers; training on implicit associations and active forgetting significantly improves reasoning generalization

## Executive Summary
This paper investigates how language models learn factual knowledge from text, distinguishing between co-occurrence statistics (easily learned but poorly generalizable) and factual associations (harder to learn but better for reasoning). The authors show that these two forms of knowledge are stored in different layers of transformers, with co-occurrence in middle layers and factual associations in lower layers. They propose two strategies to improve learning of factual associations: training on text with implicit rather than explicit associations, and actively forgetting co-occurrence statistics through parameter reset. Experiments on synthetic and real-world datasets demonstrate significant improvements in reasoning tasks, with up to 13.3% improvement in multi-hop QA accuracy.

## Method Summary
The authors finetune LLaMA 3 and Gemma models using causal language modeling on synthetic Country-city-animals datasets with both narrative (explicit co-occurrence) and referencing (implicit associations) text formats. They employ layer-wise parameter ablation to identify where different knowledge types are stored, and implement active forgetting by resetting upper layer parameters after initial training. Models are evaluated on QA, multiple choice, reverse QA, indirect reasoning, and 2-hop reasoning tasks using 5-shot accuracy. Hyperparameters include Adam optimizer, batch size 16, learning rate search, and linear decay with warmup.

## Key Results
- Language models learn co-occurrence statistics in middle layers and factual associations in lower layers
- Training on implicit association text significantly improves generalization to indirect and multi-hop reasoning tasks
- Active forgetting of co-occurrence statistics (parameter reset) further enhances factual association learning
- On real-world datasets, these methods improve multi-hop QA accuracy by up to 13.3%

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Language models learn co-occurrence statistics in middle layers and factual associations in lower layers
- Mechanism: The transformer's architecture allows different types of knowledge to be parameterized in different layers. Middle layers encode statistical patterns like word co-occurrence, while lower layers encode abstract factual relationships
- Core assumption: Knowledge representation is layer-segregated in transformers
- Evidence anchors:
  - [section] "Co-occurrence statistics and factual associations are largely parameterized separately in a transformer language model"
  - [section] "Co-occurrence statistics are mainly parameterized across the middle layers of the transformer, while factual associations are only parameterized in the lower 1/3 of the layers"
  - [corpus] Weak - the corpus doesn't directly address layer-segregation mechanisms
- Break condition: If knowledge becomes distributed across all layers rather than layer-segregated, this mechanism would fail

### Mechanism 2
- Claim: Implicit association text forces learning of factual associations rather than co-occurrence statistics
- Mechanism: By removing explicit co-occurrence between head and tail entities through intermediate attributes, the model cannot shortcut by learning simple statistical patterns and must learn the underlying factual relationship
- Core assumption: Word co-occurrence is the primary shortcut language models use to learn facts
- Evidence anchors:
  - [abstract] "training on text with implicit rather than explicit factual associations can force the model to learn factual associations instead of co-occurrence statistics"
  - [section] "Implicit factual associations cannot be modeled by word co-occurrence probabilities and forces the model to learn the underlying factual associations"
  - [corpus] Weak - corpus evidence is primarily from controlled synthetic datasets
- Break condition: If the model develops alternative shortcuts that bypass co-occurrence learning, this mechanism would fail

### Mechanism 3
- Claim: Active forgetting of co-occurrence statistics unblocks factual association learning
- Mechanism: By resetting parameters in layers that learned co-occurrence statistics, the model loses this shortcut and can then learn the underlying factual associations during subsequent training
- Core assumption: Co-occurrence statistics and factual associations compete for learning resources
- Evidence anchors:
  - [abstract] "propose a simple training method to actively forget the learned co-occurrence statistics, which unblocks and enhances the learning of factual associations"
  - [section] "With a non-zero loss, the lower layers of the transformer can undergo further training, and the learning of the true factual associations can continue"
  - [corpus] Weak - corpus doesn't provide direct evidence of this mechanism
- Break condition: If resetting parameters doesn't sufficiently remove co-occurrence knowledge, the model would re-learn it instead of factual associations

## Foundational Learning

- Concept: Layer-wise knowledge representation in transformers
  - Why needed here: Understanding how different types of knowledge are stored in different layers is crucial for both analyzing the problem and developing solutions
  - Quick check question: How does parameter ablation help identify which layers store specific knowledge types?

- Concept: Causal language modeling bias
  - Why needed here: The paper's core argument is that the training objective biases models toward learning statistical patterns rather than true facts
  - Quick check question: Why does the causal language modeling objective encourage learning co-occurrence statistics?

- Concept: Shortcut learning in neural networks
  - Why needed here: The paper relies on understanding how models prefer simple statistical patterns over complex relationships
  - Quick check question: What makes word co-occurrence statistics easier to learn than factual associations?

## Architecture Onboarding

- Component map: Input embeddings -> Lower layers (factual processing) -> Middle layers (statistical processing) -> Upper layers (reasoning) -> Output layer
- Critical path: Text input → lower layers (factual processing) → middle layers (statistical processing) → reasoning tasks
- Design tradeoffs:
  - Layer segregation vs. distributed knowledge
  - Explicit vs. implicit knowledge demonstration
  - Full model vs. parameter-efficient fine-tuning
- Failure signatures:
  - High QA accuracy but poor reasoning performance → co-occurrence statistics dominate
  - Poor performance after layer reset → knowledge wasn't properly segregated
  - No improvement from active forgetting → co-occurrence wasn't properly cleared
- First 3 experiments:
  1. Run layer-wise ablation to identify which layers control different task performances
  2. Compare training on narrative vs. referencing text formats
  3. Test active forgetting by resetting upper layers and measuring reasoning performance changes

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the efficiency of learning factual knowledge differ between pretraining from scratch and finetuning on limited text data?
- Basis in paper: [inferred] The paper mentions that the scope is limited to finetuning and that learning knowledge efficiently is likely more challenging during pretraining from scratch due to the lack of existing knowledge and good language representations in the model.
- Why unresolved: The study only focuses on finetuning, leaving the comparison with pretraining from scratch unexplored.
- What evidence would resolve it: Experiments comparing the efficiency of learning factual knowledge during pretraining from scratch versus finetuning on the same dataset, measuring metrics like sample efficiency, generalization to reasoning tasks, and knowledge retention.

### Open Question 2
- Question: What is the impact of different forms of text expressing factual knowledge on the generalization properties of language models?
- Basis in paper: [explicit] The paper acknowledges that the scope is limited to two forms of text (narrative and referencing) and that facts can be communicated in many different ways in natural language, with the generalization properties of different forms of text remaining unexplored.
- Why unresolved: The study only considers two specific forms of text, and the effect of other forms on knowledge learning and generalization is unknown.
- What evidence would resolve it: Experiments training language models on various forms of text expressing the same factual knowledge, followed by evaluation of their generalization to reasoning tasks, to identify which forms promote better learning of factual associations.

### Open Question 3
- Question: How do knowledge editing, knowledge-aware training, and assisted reasoning techniques affect the learning and utilization of factual knowledge in language models?
- Basis in paper: [explicit] The paper mentions that the study is limited to learning new factual knowledge from raw text, unlike knowledge editing or knowledge-aware training which utilize annotated facts, and that the models are not assisted during reasoning with techniques like chain-of-thought.
- Why unresolved: The study does not explore the interaction between these techniques and the learning of factual knowledge, nor their impact on reasoning performance.
- What evidence would resolve it: Experiments combining the proposed strategies (implicit association and active forgetting) with knowledge editing, knowledge-aware training, or assisted reasoning techniques, measuring their effect on knowledge learning efficiency and reasoning performance.

## Limitations
- Findings are primarily based on synthetic datasets with limited real-world knowledge complexity
- Results may not scale to larger models where knowledge representation could differ significantly
- Modest improvements on real-world datasets compared to synthetic data experiments

## Confidence

*High Confidence:* The empirical evidence strongly supports that language models learn co-occurrence statistics differently from factual associations, with clear layer-wise separation observed through parameter ablation experiments. The improvement from implicit vs explicit training formats and the effectiveness of active forgetting are well-demonstrated across multiple datasets and tasks.

*Medium Confidence:* The proposed mechanisms for why these approaches work (layer segregation, shortcut learning, competition for resources) are plausible but not definitively proven. The evidence is primarily correlational rather than showing causal mechanisms. The effectiveness of these methods on real-world datasets, while positive, shows smaller improvements than synthetic data.

*Low Confidence:* The scalability of these approaches to larger models, more complex knowledge domains, and different training objectives remains untested. The paper doesn't explore whether the layer-wise segregation is an inherent property of transformer architectures or an artifact of the specific training conditions used.

## Next Checks

1. **Real-world Knowledge Transfer Test**: Evaluate whether models trained with implicit associations and active forgetting on the synthetic dataset show improved learning efficiency and generalization when subsequently trained on real-world knowledge graphs or Wikipedia-derived datasets.

2. **Scaling Experiment**: Repeat the synthetic dataset experiments with progressively larger models (8B → 70B → 405B parameters) to determine if the layer-wise segregation pattern holds and whether the relative effectiveness of implicit training vs active forgetting changes with scale.

3. **Alternative Training Objectives**: Test whether the same layer-wise knowledge separation and the effectiveness of the proposed methods hold when using different training objectives like masked language modeling or prefix language modeling, which may change how the model learns to represent knowledge.