---
ver: rpa2
title: Zero Shot Audio to Audio Emotion Transfer With Speaker Disentanglement
arxiv_id: '2401.04511'
source_url: https://arxiv.org/abs/2401.04511
tags:
- speech
- emotion
- speaker
- source
- audio
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proposes ZEST, a zero-shot audio-to-audio emotion transfer
  framework that preserves speaker identity and speech content while transferring
  emotion from a target audio to a source audio. ZEST decomposes speech into semantic
  tokens (HuBERT), speaker embeddings (EASE), and emotion embeddings, and uses a pitch
  contour predictor to generate the pitch contour of the converted speech.
---

# Zero Shot Audio to Audio Emotion Transfer With Speaker Disentanglement

## Quick Facts
- arXiv ID: 2401.04511
- Source URL: https://arxiv.org/abs/2401.04511
- Reference count: 0
- Primary result: Zero-shot emotion transfer framework achieving >85% emotion accuracy while preserving speaker identity and speech content

## Executive Summary
ZEST is a zero-shot audio-to-audio emotion transfer framework that decomposes speech into disentangled semantic, speaker, and emotion representations. The system uses pre-trained HuBERT for content tokens, EASE with emotion adversarial training for speaker embeddings, and SACE for emotion embeddings. A cross-attention pitch predictor generates the pitch contour, which is combined with these factors in HiFi-GAN to reconstruct speech with the target emotion while preserving speaker identity and content.

## Method Summary
ZEST extracts HuBERT semantic tokens, EASE speaker embeddings, and SACE emotion embeddings from source and target audio. During conversion, it uses source semantic tokens and speaker embeddings with the target emotion embedding, and predicts the pitch contour via a cross-attention model. These are fed to HiFi-GAN to reconstruct speech with the target emotion while preserving speaker identity and content. The framework is trained on the ESD dataset with pre-trained models for HuBERT, EASE, and SACE, while HiFi-GAN and the pitch predictor are trained on ESD data.

## Key Results
- Significant improvement in emotion transfer accuracy over baseline systems on ESD dataset
- Maintains similar ASR performance (CER ~12%) and speaker preservation (~75-80% accuracy)
- Subjective listening tests validate effectiveness with naturalness ratings of 4.1/5

## Why This Works (Mechanism)

### Mechanism 1
- Claim: ZEST achieves zero-shot emotion transfer by decomposing speech into disentangled semantic, speaker, and emotion representations, then recombining them with a target emotion embedding.
- Mechanism: The model extracts HuBERT semantic tokens, EASE speaker embeddings, and SACE emotion embeddings from source and target audio. During conversion, it uses source semantic tokens and speaker embeddings with the target emotion embedding, and predicts the pitch contour via a cross-attention model. These are fed to HiFi-GAN to reconstruct speech with the target emotion while preserving speaker identity and content.
- Core assumption: Semantic tokens capture content, EASE vectors are speaker-emotion disentangled, and emotion embeddings encode emotion style independently of speaker and content.
- Evidence anchors:
  - [abstract] "The proposed system builds upon decomposing speech into semantic tokens, speaker representations and emotion embeddings."
  - [section 3.2] "The x-vectors have been shown to encode emotion information [24, 25]. In order to suppress the emotion information, inspired by the disentanglement approach proposed in Li et al. [20], we add two fully connected layers to the x-vector model and further train the model with an emotion adversarial loss [26]."
  - [corpus] Weak evidence; no directly cited proof of cross-attention effectiveness for pitch prediction from the provided text.
- Break condition: If semantic tokens do not fully capture content, or if emotion embeddings are not independent of speaker/content, transfer quality degrades.

### Mechanism 2
- Claim: The pitch contour predictor effectively reconstructs the target pitch contour using cross-attention over semantic and speaker embeddings.
- Mechanism: HuBERT tokens are converted to embeddings (queries), combined with speaker and SACE embeddings (keys/values) in a cross-attention layer, followed by 1D-CNN to predict the F0 contour. This predicted contour is used in HiFi-GAN reconstruction.
- Core assumption: Cross-attention can model the mapping from semantic/speaker context to pitch contour, and the F0 contour is a sufficient summary of pitch-related information.
- Evidence anchors:
  - [section 3.4] "The HuBERT tokens for the speech signal are converted to an sequence of vectors... This sequence, denoted by Q = {q1, .., qT }T t=1, is used as the query sequence for cross-attention. The frame-level SACE embeddings are added with speaker embedding (EASE) to form the key-value pair for the cross attention module [30]."
  - [corpus] Weak evidence; no direct proof of effectiveness of this specific cross-attention pitch prediction setup in the provided text.
- Break condition: If cross-attention fails to capture long-range dependencies or if F0 is insufficient, the reconstructed speech may lose prosody or emotion.

### Mechanism 3
- Claim: Emotion adversarial training on EASE vectors disentangles speaker and emotion information, improving emotion transfer accuracy.
- Mechanism: The EASE model is trained with an emotion adversarial loss (Eq. 1) to suppress emotion information in speaker embeddings. This ensures that speaker embeddings carry only speaker identity, allowing emotion embeddings to carry emotion style exclusively.
- Core assumption: Emotion information can be suppressed from speaker embeddings without harming speaker representation quality, and emotion embeddings can then carry emotion independently.
- Evidence anchors:
  - [section 3.2] "In order to suppress the emotion information, inspired by the disentanglement approach proposed in Li et al. [20], we add two fully connected layers to the x-vector model and further train the model with an emotion adversarial loss [26]."
  - [section 4.3.1] "The adversarial training allows disentangled representations of speaker and emotion, which is shown to improve the objective quality results."
  - [corpus] Weak evidence; no direct evidence of emotion adversarial training improving results in the provided text.
- Break condition: If adversarial training is too aggressive, speaker embeddings may lose speaker identity; if too weak, emotion leakage remains.

## Foundational Learning

- Concept: Disentangled representation learning
  - Why needed here: To isolate emotion style from speaker identity and speech content so that emotion can be transferred without altering speaker or content.
  - Quick check question: Can you explain how adversarial training helps remove one factor (e.g., emotion) from a representation that also contains another (e.g., speaker)?

- Concept: Self-supervised speech representation learning (HuBERT)
  - Why needed here: To obtain semantic tokens that capture speech content without requiring labeled transcriptions or parallel data.
  - Quick check question: Why is using a pre-trained HuBERT model advantageous for content encoding compared to training from scratch?

- Concept: Cross-attention mechanisms in sequence modeling
  - Why needed here: To map semantic and speaker context to the target pitch contour, enabling content and speaker-conditioned pitch prediction.
  - Quick check question: How does cross-attention differ from self-attention, and why is it suitable for this pitch prediction task?

## Architecture Onboarding

- Component map: HuBERT encoder -> semantic tokens (content) -> cross-attention pitch predictor -> F0 contour; EASE model -> speaker embeddings (speaker identity); SACE model -> emotion embeddings (emotion style); HiFi-GAN decoder -> reconstructed speech

- Critical path: 1) Extract semantic tokens, speaker embeddings, and emotion embeddings from source and target audio. 2) Use source tokens and speaker embeddings, target emotion embedding, and predicted F0 to reconstruct speech via HiFi-GAN. 3) During training, train pitch predictor and HiFi-GAN jointly on ESD; freeze HuBERT, EASE, and SACE.

- Design tradeoffs: Using pre-trained models avoids data requirements but limits fine-tuning. Emotion adversarial training improves disentanglement but may hurt speaker accuracy if over-regularized. Pitch prediction adds complexity but allows pitch-based emotion transfer.

- Failure signatures: Poor emotion transfer accuracy: emotion embeddings may not capture emotion independently of speaker. Degraded speaker preservation: adversarial training may over-suppress speaker info. High CER: content tokens or HiFi-GAN reconstruction fails to preserve text. Unnatural prosody: pitch predictor fails to model pitch accurately.

- First 3 experiments: 1) Run ZEST on SSST setting and verify emotion accuracy > baseline. 2) Test with EASE frozen (no adversarial training) and compare emotion accuracy. 3) Remove pitch predictor and use target F0; compare emotion accuracy and CER.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the choice of the HuBERT model layer (e.g., 9th vs 12th) affect the emotion transfer performance in ZEST?
- Basis in paper: [explicit] The paper mentions using the 9th layer of the HuBERT model for clustering and representation extraction.
- Why unresolved: The paper does not explore or compare the performance of ZEST with different layers of the HuBERT model.
- What evidence would resolve it: Experiments comparing the emotion transfer accuracy, speaker preservation, and ASR performance of ZEST when using different layers of the HuBERT model would provide insights into the optimal layer choice.

### Open Question 2
- Question: How does the performance of ZEST compare to other state-of-the-art A2A emotion transfer methods that do not rely on disentangled representations?
- Basis in paper: [inferred] The paper benchmarks ZEST against two baseline methods (V AWGAN and Polyak et al.) but does not explore other disentanglement-free approaches.
- Why unresolved: The paper does not provide a comprehensive comparison with other disentanglement-free A2A emotion transfer methods.
- What evidence would resolve it: Evaluating ZEST against other state-of-the-art A2A emotion transfer methods that do not use disentangled representations would provide a clearer understanding of the benefits of the proposed approach.

### Open Question 3
- Question: How does the size of the emotional speech dataset used for training the F0 predictor and emotion embedding extractor affect the performance of ZEST?
- Basis in paper: [inferred] The paper mentions using the ESD dataset for training these components but does not explore the impact of dataset size on performance.
- Why unresolved: The paper does not investigate how the performance of ZEST scales with the size of the emotional speech dataset used for training the F0 predictor and emotion embedding extractor.
- What evidence would resolve it: Experiments training the F0 predictor and emotion embedding extractor on datasets of varying sizes and evaluating the impact on ZEST's performance would provide insights into the dataset size requirements for optimal results.

### Open Question 4
- Question: How does the proposed emotion adversarial training of the EASE model affect the disentanglement of speaker and emotion representations?
- Basis in paper: [explicit] The paper mentions using emotion adversarial training to suppress emotion information in the EASE model.
- Why unresolved: The paper does not provide a detailed analysis of how the emotion adversarial training affects the disentanglement of speaker and emotion representations.
- What evidence would resolve it: Analyzing the speaker and emotion representations before and after emotion adversarial training, and evaluating the impact on emotion transfer accuracy and speaker preservation, would provide insights into the effectiveness of the proposed training approach.

## Limitations
- The cross-attention pitch predictor architecture lacks specific configuration details, making exact reproduction challenging
- Emotion adversarial training implementation is described conceptually but lacks precise formulation details
- Subjective evaluation involved only 20 raters, limiting statistical confidence
- Zero-shot performance on truly unseen speakers (TIMIT) is claimed but not demonstrated with results

## Confidence
**High Confidence Claims:**
- The three-way disentanglement approach is technically sound and aligns with established representation learning principles
- The framework architecture is coherent with appropriate component choices
- Objective metrics demonstrate successful emotion transfer while preserving content and speaker identity on ESD

**Medium Confidence Claims:**
- The effectiveness of the cross-attention pitch predictor mechanism is assumed but not empirically validated in isolation
- The benefit of emotion adversarial training for disentanglement is theoretically justified but lacks direct ablation evidence
- Subjective naturalness ratings suggest good audio quality, but small rater pool limits generalizability

**Low Confidence Claims:**
- Zero-shot performance on unseen speakers is claimed as a capability but not demonstrated with results
- The pitch contour predictor's effectiveness for emotion transfer is assumed but not validated with ablation studies
- The SACE model's emotion embedding quality is inferred from end-task performance rather than direct emotion classification accuracy

## Next Checks
1. **Cross-Attention Pitch Predictor Ablation**: Remove the pitch predictor entirely and use the target audio's F0 contour directly. Compare emotion accuracy, CER, and speaker accuracy to determine the pitch predictor's contribution.

2. **Adversarial Training Impact**: Train ZEST with EASE and SACE models without emotion adversarial loss (freeze the emotion suppression components). Compare emotion accuracy, speaker accuracy, and CER to quantify the disentanglement benefit.

3. **Unseen Speaker Generalization**: Evaluate ZEST on the TIMIT dataset speakers (truly unseen during training). Measure emotion accuracy, speaker classification accuracy, and CER to validate zero-shot generalization claims.