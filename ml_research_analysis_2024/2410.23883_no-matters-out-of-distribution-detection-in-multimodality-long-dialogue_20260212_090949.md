---
ver: rpa2
title: '''No'' Matters: Out-of-Distribution Detection in Multimodality Long Dialogue'
arxiv_id: '2410.23883'
source_url: https://arxiv.org/abs/2410.23883
tags:
- dialogue
- detection
- image
- score
- pairs
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces a novel framework, Dialogue Image Aligning
  and Enhancing Framework (DIAEF), for detecting out-of-distribution (OOD) instances
  in multimodal long dialogue systems. The framework addresses two key scenarios:
  mismatches between dialogue and image inputs, and input pairs with previously unseen
  labels.'
---

# 'No' Matters: Out-of-Distribution Detection in Multimodality Long Dialogue

## Quick Facts
- arXiv ID: 2410.23883
- Source URL: https://arxiv.org/abs/2410.23883
- Authors: Rena Gao; Xuetong Wu; Siwen Luo; Caren Han; Feng Liu
- Reference count: 14
- Primary result: DIAEF framework outperforms baselines in multimodal OOD detection with improved FPR95, AUROC, and AUPR metrics

## Executive Summary
This paper introduces the Dialogue Image Aligning and Enhancing Framework (DIAEF) for detecting out-of-distribution (OOD) instances in multimodal long dialogue systems. The framework addresses two key scenarios: mismatches between dialogue and image inputs, and input pairs with previously unseen labels. By integrating visual language models with a proposed scoring method that combines dialogue-image similarity and label-based scores, DIAEF effectively identifies OOD samples in multimodal dialogue settings. Experimental results on Visdial and Real MMD datasets demonstrate significant performance improvements over baselines using individual modalities.

## Method Summary
The proposed Dialogue Image Aligning and Enhancing Framework (DIAEF) combines visual language models (CLIP/BLIP) with a novel scoring mechanism for OOD detection in multimodal long dialogue. The framework computes dialogue-image alignment scores using cosine similarity in shared semantic space, then multiplies this with an enhancing term that measures label relevance. A hyperparameter α controls the weighting between image and dialogue contributions, while γ modulates the alignment term. The method is evaluated using FPR95, AUROC, and AUPR metrics on Visdial and Real MMD datasets, with various OOD scoring methods (Probability, MSP, Logits, ODIN, Mahalanobis, JointEnergy) and fusion strategies.

## Key Results
- DIAEF outperforms single-modality baselines on Visdial and Real MMD datasets
- Achieves improved FPR95, AUROC, and AUPR metrics for OOD detection
- Effectively identifies both mismatched dialogue-image pairs and pairs with previously unseen labels
- Demonstrates strong robustness in long dialogue scenarios

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The multiplicative combination of the alignment term s(xT, xI)γ and the enhancing term (αsI + (1 - α)sT) allows the model to simultaneously capture both cross-modal mismatch and label relevance.
- Mechanism: The alignment term detects semantic mismatches between dialogue and image; the enhancing term measures relevance to labels. When multiplied, OOD samples with mismatches have suppressed scores, while ID samples maintain high scores.
- Core assumption: The cosine similarity s(xI, xT) from visual-language models like CLIP is effective at distinguishing semantically matched vs mismatched dialogue-image pairs.
- Evidence anchors:
  - [abstract] "Our proposed score effectively identifies these mismatches and demonstrates strong robustness in long dialogues"
  - [section 4] "The purpose of using a multiplicative combination of the alignment and enhancing terms is: (1) identifying mismatched OOD pairs where either the image or dialogue might have high relevance to the label, making the enhancing term potentially large"
  - [corpus] Weak - related work focuses on single modality OOD detection, not cross-modal alignment
- Break condition: If the visual-language model fails to project dialogue and images into comparable semantic spaces, the alignment term becomes unreliable and the multiplicative combination loses its discriminative power.

### Mechanism 2
- Claim: The hyperparameter α allows adaptive weighting between image and dialogue contributions to OOD detection based on which modality is more reliable in a given context.
- Mechanism: When α is set to 0.5, both modalities contribute equally; higher α values prioritize image-based detection, while lower values prioritize dialogue-based detection.
- Core assumption: Different dialogue-image pairs have varying reliability in their modalities, and adaptive weighting can optimize detection performance.
- Evidence anchors:
  - [section 4] "where the weighting hyperparameter α controls the relative importance of the image: if α is selected to be large, we rely more on images for OOD detection; conversely for a small α, we rely more on the dialogue"
  - [section 5.3] "The results demonstrate that DIEAF performs effectively when combining dialogue and image scores"
  - [corpus] Weak - no direct corpus evidence for adaptive modality weighting in cross-modal OOD detection
- Break condition: If one modality is consistently unreliable across all test cases, adaptive weighting may not improve performance and could introduce unnecessary complexity.

### Mechanism 3
- Claim: The proposed framework establishes a theoretical foundation where the DIEAF score function satisfies the condition E_P[log g(xI, xT, y)] > E_Q[log g(xI, xT, y)] for ID vs OOD distributions under the stated assumptions.
- Mechanism: The theoretical analysis in Appendix B shows that under specific assumptions about the distributions, the DIEAF score function will have higher expected log values for ID samples than OOD samples.
- Core assumption: The three cases in Assumption 1 correctly characterize the relationship between ID and OOD distributions in terms of image-text similarity and label relevance.
- Evidence anchors:
  - [section 4] "we give a theoretical justification for the proposed score in Appendix B"
  - [section 4] "The purpose of using a multiplicative combination of the alignment and enhancing terms is: (2) identifying matched pairs with OOD labels where s(xT, xI) may be large, but the enhancement term is likely to be small"
  - [corpus] Weak - theoretical justification for cross-modal OOD detection is rarely addressed in related work
- Break condition: If the real-world data distributions violate the assumptions in Assumption 1, particularly regarding the relationship between image-text similarity and label relevance, the theoretical guarantees may not hold.

## Foundational Learning

- Concept: Visual-language models (CLIP, BLIP) that project images and text into shared semantic spaces
  - Why needed here: The framework relies on computing cosine similarity between dialogue and image embeddings in the same latent space
  - Quick check question: How does CLIP ensure that images and text are projected into comparable semantic representations?

- Concept: Out-of-distribution detection metrics (FPR95, AUROC, AUPR) and their interpretation
  - Why needed here: The evaluation framework uses these specific metrics to assess OOD detection performance
  - Quick check question: What does FPR95 measure and why is it particularly important for OOD detection?

- Concept: Multi-label classification and handling multiple semantic labels per dialogue-image pair
  - Why needed here: The framework handles cases where pairs can be associated with multiple labels simultaneously
  - Quick check question: How does the framework handle cases where a dialogue-image pair could be relevant to multiple categories?

## Architecture Onboarding

- Component map: Visual language encoder → Embedding extractor → Label extractor → Score combiner → Threshold comparator
- Critical path: Image encoder → Dialogue encoder → Similarity computation → Label scoring → Score fusion → OOD decision
- Design tradeoffs: Multiplicative vs additive combination of alignment and enhancing terms; fixed vs adaptive α weighting; single vs multiple label extractors
- Failure signatures: Poor cross-modal alignment leading to false positives; over-reliance on one modality causing systematic blind spots; threshold selection causing imbalanced precision-recall
- First 3 experiments:
  1. Baseline comparison: Run with only image modality vs only dialogue modality to establish performance floor
  2. α sensitivity analysis: Test different α values (0.0, 0.25, 0.5, 0.75, 1.0) to understand modality importance
  3. γ impact study: Vary γ from 0 to 3 to assess the importance of cross-modal alignment in detection accuracy

## Open Questions the Paper Calls Out
None

## Limitations
- Theoretical justification relies on specific distributional assumptions that may not hold in real-world scenarios
- Framework performance heavily depends on quality of visual language models like CLIP or BLIP
- Paper does not address computational overhead of processing long dialogues and multiple images

## Confidence

- **High Confidence**: Experimental results on Visdial and Real MMD datasets are robust, with clear improvements over baselines in FPR95, AUROC, and AUPR metrics
- **Medium Confidence**: Theoretical analysis in Appendix B provides solid foundation, but assumptions may not fully capture real-world data distributions
- **Low Confidence**: Adaptive weighting mechanism (α) and its impact on OOD detection performance are not thoroughly explored

## Next Checks

1. Validate whether real-world data distributions violate the assumptions in Assumption 1, particularly regarding the relationship between image-text similarity and label relevance
2. Conduct experiments to determine if one modality (dialogue or image) is consistently more reliable across different test cases, and assess impact of adaptive weighting
3. Measure computational overhead of processing long dialogues and multiple images, and explore optimization strategies for practical deployment