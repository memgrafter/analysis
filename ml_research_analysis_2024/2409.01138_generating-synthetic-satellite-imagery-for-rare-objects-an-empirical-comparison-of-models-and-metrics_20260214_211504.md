---
ver: rpa2
title: 'Generating Synthetic Satellite Imagery for Rare Objects: An Empirical Comparison
  of Models and Metrics'
arxiv_id: '2409.01138'
source_url: https://arxiv.org/abs/2409.01138
tags:
- image
- human
- data
- images
- metrics
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'The paper investigates the challenges of generating synthetic
  satellite imagery for rare object classes, specifically nuclear power plants, which
  are difficult to monitor due to limited real-world examples. The authors fine-tune
  a pre-trained text-to-image model, Stable Diffusion, using two approaches: DreamBooth
  and Textual Inversion.'
---

# Generating Synthetic Satellite Imagery for Rare Objects: An Empirical Comparison of Models and Metrics

## Quick Facts
- arXiv ID: 2409.01138
- Source URL: https://arxiv.org/abs/2409.01138
- Reference count: 40
- Primary result: Human evaluation shows fine-tuned models generate realistic satellite imagery for rare objects, but automated metrics poorly align with human perception

## Executive Summary
This paper investigates the challenges of generating synthetic satellite imagery for rare object classes, specifically nuclear power plants, which are difficult to monitor due to limited real-world examples. The authors fine-tune a pre-trained text-to-image model, Stable Diffusion, using two approaches: DreamBooth and Textual Inversion. They also explore using additional image input from a game engine to guide the generation process. Human evaluation is conducted to assess image fidelity, text alignment, and layout alignment. Results show that fine-tuned models generate realistic satellite imagery, with textual control being effective. However, automated metrics like Inception Score (IS) and CLIPScore do not align well with human perception, highlighting the need for large-scale human studies to evaluate synthetic data quality, especially for rare objects and niche domains.

## Method Summary
The authors fine-tune Stable Diffusion v1.5 on 202 real satellite images of nuclear power plants using DreamBooth and Textual Inversion approaches. They generate synthetic layouts using Unity game engine (canny, depth, and sketch modes) and apply T2I-Adapter for layout conditioning. The pipeline involves fine-tuning the base model, generating images with text prompts only and with layout conditioning, then evaluating results through human studies (Likert scale ratings for fidelity, text alignment, layout alignment) and automated metrics (Inception Score, CLIPScore).

## Key Results
- Fine-tuned models generate realistic satellite imagery of nuclear power plants with effective textual control
- T2I-Adapter with Unity layouts improves structural alignment but may reduce overall image quality
- Automated metrics (Inception Score, CLIPScore) show poor correlation with human perception ratings
- Human evaluation reveals discrepancies between perceived image quality and automated metric scores

## Why This Works (Mechanism)

### Mechanism 1
Fine-tuning a pre-trained text-to-image model on a small, rare-class dataset enables generation of realistic synthetic satellite imagery with good text alignment. The model's latent space contains generalizable features that can be specialized for rare classes without catastrophic forgetting.

### Mechanism 2
Incorporating layout conditioning through T2I-Adapter with image input from a game engine improves structural control over generated images. The adapter processes depth maps, sketches, or edge images from Unity-rendered layouts and feeds them into the denoising U-Net at multiple timesteps to guide spatial composition.

### Mechanism 3
Human evaluation reveals discrepancies between automated metrics and perceived image quality, especially for rare classes. Large-scale user studies rate images on fidelity, text alignment, and layout alignment using Likert scales, exposing metric failures not captured by Inception-based scores.

## Foundational Learning

- **Fine-tuning diffusion models with limited data**: Why needed - The target class (nuclear plants) has only ~200 real images, requiring specialized methods to avoid overfitting. Quick check - What are the key differences between DreamBooth and Textual Inversion in how they update model parameters?

- **Latent diffusion model architecture**: Why needed - Understanding the U-Net denoising process and CLIP conditioning is essential for debugging generation quality. Quick check - How does the T2I-Adapter integrate structural guidance into the denoising timesteps?

- **Automated evaluation metrics for generative models**: Why needed - Interpreting IS, FID, and CLIPScore results requires knowledge of their limitations, especially for rare classes. Quick check - Why might Inception Score correlate negatively with human ratings for satellite imagery?

## Architecture Onboarding

- **Component map**: Pre-trained Stable Diffusion v1.5 -> DreamBooth/Textual Inversion fine-tuning -> T2I-Adapter -> CLIP text encoder -> Unity game engine -> Human evaluation pipeline

- **Critical path**: 1. Collect and preprocess satellite imagery of nuclear plants 2. Generate synthetic layouts in Unity 3. Fine-tune base model using DreamBooth/TI 4. Apply T2I-Adapter with structural guidance 5. Generate images with varied prompts/modalities 6. Evaluate using human studies and automated metrics

- **Design tradeoffs**: Fine-tuning vs. prompt engineering (adapt vs. speed); Text-only vs. layout conditioning (semantic vs. spatial control); Automated vs. human evaluation (scalable vs. accurate)

- **Failure signatures**: Poor image fidelity (low human ratings, high IS, unrealistic textures); Weak text alignment (low CLIPScore, prompt-content mismatch); Structural misalignment (layout conditioning fails to preserve configurations)

- **First 3 experiments**: 1. Generate 100 images using base Stable Diffusion with nuclear plant prompts; evaluate IS and CLIPScore 2. Fine-tune with DreamBooth on 200 satellite images; generate and evaluate again 3. Apply T2I-Adapter with Unity layouts; compare layout alignment scores to text-only results

## Open Questions the Paper Calls Out

**Open Question 1**: Can automated metrics for image quality evaluation be developed that align better with human perception for rare object classes and niche domains? The paper shows IS and CLIPScore poorly align with human ratings for nuclear power plant imagery, but doesn't propose new metrics.

**Open Question 2**: How does the addition of structural layout guidance impact the ability to generate consistent imagery of the same object across different viewpoints? The study investigates layout control but only considers limited viewpoints and layouts.

**Open Question 3**: What are the societal implications of using generative AI to create synthetic imagery of rare objects, particularly in terms of misinformation and public perception? The paper discusses potential for misinformation but doesn't explore specific mechanisms or impact on public trust.

## Limitations

- Small dataset of 202 satellite images may lead to overfitting and limit generalization to other rare object classes
- Human evaluation methodology may introduce biases from raters' varying familiarity with satellite imagery and nuclear facilities
- Automated metrics were shown to poorly align with human perception, but the study doesn't explore alternative evaluation methods

## Confidence

- **High Confidence**: Effectiveness of fine-tuning Stable Diffusion using DreamBooth and Textual Inversion for rare object classes
- **Medium Confidence**: Superiority of layout conditioning through T2I-Adapter for structural control (with noted trade-offs)
- **Low Confidence**: Generalizability of findings to other rare object classes or satellite imagery domains

## Next Checks

1. Test the same fine-tuning and conditioning approach on a different rare satellite object class (e.g., specific types of industrial facilities) to assess generalizability beyond nuclear power plants.

2. Conduct a more comprehensive statistical analysis correlating automated metrics with human ratings across multiple rare object datasets to identify which metrics, if any, show promise for rare class evaluation.

3. Assess model performance over extended fine-tuning periods and multiple random seeds to determine the consistency and reliability of generated outputs, particularly for the layout conditioning approach.