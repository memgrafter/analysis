---
ver: rpa2
title: Augmenting Multimodal LLMs with Self-Reflective Tokens for Knowledge-based
  Visual Question Answering
arxiv_id: '2411.16863'
source_url: https://arxiv.org/abs/2411.16863
tags:
- reflectiv
- knowledge
- passages
- retrieval
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Reflective LLaVA (ReflectiVA), a multimodal
  large language model that uses self-reflective tokens to determine when external
  knowledge is needed and to identify relevant retrieved passages for knowledge-based
  visual question answering. The model employs a two-stage, two-model training strategy
  to learn these capabilities, enabling dynamic retrieval-augmented generation.
---

# Augmenting Multimodal LLMs with Self-Reflective Tokens for Knowledge-based Visual Question Answering

## Quick Facts
- arXiv ID: 2411.16863
- Source URL: https://arxiv.org/abs/2411.16863
- Reference count: 40
- Primary result: Introduces ReflectiVA, achieving state-of-the-art performance on knowledge-based visual question answering datasets with 35.5% accuracy on Encyclopedic-VQA

## Executive Summary
This paper introduces Reflective LLaVA (ReflectiVA), a multimodal large language model that uses self-reflective tokens to determine when external knowledge is needed and to identify relevant retrieved passages for knowledge-based visual question answering. The model employs a two-stage, two-model training strategy to learn these capabilities, enabling dynamic retrieval-augmented generation. ReflectiVA achieves state-of-the-art performance on the Encyclopedic-VQA and InfoSeek datasets, outperforming existing retrieval-augmented MLLMs by significant margins while maintaining strong performance on standard MLLM benchmarks.

## Method Summary
ReflectiVA extends the LLaVA-v1.5 MLLM architecture with four reflective tokens (<RET>, <NORET>, <REL>, <NOREL>) to make dynamic decisions about knowledge retrieval and passage relevance. The model uses a two-stage training strategy: first training an in-article reflective model to distinguish relevant from irrelevant passages within the same document, then training the final model using predictions from the first stage. For knowledge retrieval, it employs CLIP-based models for coarse-grained document retrieval followed by fine-grained passage selection. The training data combines Encyclopedic-VQA, InfoSeek, and LLaVA-Instruct datasets, with automatic annotation of positive and negative passages using GPT-4.

## Key Results
- Achieves 35.5% accuracy on Encyclopedic-VQA, outperforming EchoSight (26.4%) and Instruct-VQA (29.6%)
- Maintains strong performance on standard MLLM benchmarks (MMMU, MMBench, POPE, SEED-Img, MME, GQA, TextVQA, ScienceQA, AI2D)
- Shows 4.8% improvement in VQA accuracy and 8.4% improvement in relaxed accuracy on InfoSeek compared to existing methods
- Demonstrates high accuracy in reflective token prediction (>85% for <RET>/<NORET> decisions)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Reflective tokens enable the model to dynamically decide when external knowledge is needed for answering questions
- Mechanism: The model generates either <RET> or <NORET> tokens to indicate whether retrieval from external knowledge sources should be performed, allowing it to bypass retrieval for questions that can be answered from visual information alone
- Core assumption: The model can learn to accurately distinguish between questions requiring external knowledge and those that don't through training with labeled examples
- Evidence anchors:
  - [abstract] "Reflective LLaVA (ReflectiVA), utilizes reflective tokens to dynamically determine the need for external knowledge"
  - [section 3.1] "At test time, the MLLM is prompted with an input image I and a query q, and is asked to produce either the <RET> or the <NORET> token"
  - [corpus] Weak evidence - no direct citations found for this specific mechanism in related works

### Mechanism 2
- Claim: Reflective tokens allow the model to assess the relevance of retrieved passages before generating answers
- Mechanism: After retrieving candidate passages, the model generates <REL> or <NOREL> tokens for each passage to identify which ones are most pertinent to answering the question, then uses only the relevant passages for final answer generation
- Core assumption: The model can learn to effectively discriminate between relevant and irrelevant passages through exposure to both types during training
- Evidence anchors:
  - [abstract] "predict the relevance of information retrieved from an external database"
  - [section 3.1] "we empower the model to identify the relevance of retrieved passages for generation"
  - [corpus] Moderate evidence - related work like EchoSight [71] uses re-ranking components, suggesting passage relevance assessment is valuable

### Mechanism 3
- Claim: The two-stage training strategy with in-article discrimination enables learning to handle both positive and negative passages from different sources
- Mechanism: First stage trains an in-article model to distinguish relevant from irrelevant passages within the same document; second stage uses this model's predictions to train the final model on a balanced mixture including cross-article negative examples
- Core assumption: Learning to discriminate within-document is a useful proxy task that transfers to the more general task of discriminating across different documents
- Evidence anchors:
  - [section 3.2] "Initially, we train an in-article reflective MLLM capable of distinguishing between relevant passages and negative passages from the same article"
  - [section 3.3] "we employ predictions from that model to train the final MLLM with the ability to cope with negative passages taken from the same articles and from different articles"
  - [corpus] Weak evidence - the specific two-stage approach appears novel with no direct citations in related works

## Foundational Learning

- Concept: Multimodal retrieval-augmented generation
  - Why needed here: The task requires combining visual understanding with external knowledge retrieval to answer questions that cannot be answered from images alone
  - Quick check question: What are the key components needed for retrieval-augmented generation in multimodal settings?

- Concept: Token classification for decision making
  - Why needed here: The model needs to make binary decisions (retrieve vs no-retrieve, relevant vs irrelevant) that guide the generation process
  - Quick check question: How does the model use token generation to make routing decisions during inference?

- Concept: Contrastive learning for retrieval
  - Why needed here: The coarse-grained retrieval stage requires finding relevant documents based on similarity between query images and document features
  - Quick check question: What role does CLIP-based retrieval play in identifying relevant knowledge sources?

## Architecture Onboarding

- Component map: Pre-trained LLaVA-v1.5 MLLM → Extended vocabulary with 4 reflective tokens (<RET>, <NORET>, <REL>, <NOREL>) → CLIP-based retrieval system → In-article discriminator (first stage) → Final trained model (second stage)
- Critical path: Image+Question → Reflective token prediction (<RET>/<NORET>) → If <RET>: Retrieval → Relevance token prediction (<REL>/<NOREL>) for each passage → Answer generation using relevant passages
- Design tradeoffs: Using special tokens adds complexity but enables dynamic decision-making; two-stage training increases development time but improves performance; CLIP-based retrieval is efficient but may miss relevant documents
- Failure signatures: Consistently wrong reflective token predictions, poor retrieval recall, failure to discriminate relevant passages, degradation on non-retrieval benchmarks
- First 3 experiments:
  1. Test reflective token prediction accuracy on validation set to ensure basic decision-making capability
  2. Evaluate retrieval relevance prediction by checking if <REL> tokens are assigned to ground truth passages
  3. Compare performance with and without reflective tokens on a small subset of Encyclopedic-VQA to verify improvement

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the model's performance change when using a larger knowledge base (e.g., the full 6M Wikipedia pages from InfoSeek) instead of the 2M subset?
- Basis in paper: [explicit] The paper mentions using a subset of 100k pages for InfoSeek experiments but doesn't explore scaling to the full 6M knowledge base.
- Why unresolved: The paper only evaluates on a reduced knowledge base for InfoSeek, leaving questions about scalability and performance with larger knowledge bases unanswered.
- What evidence would resolve it: Experiments comparing performance on the full 6M InfoSeek knowledge base versus the reduced 100k subset, measuring accuracy and retrieval efficiency.

### Open Question 2
- Question: How sensitive is ReflectiVA's performance to the number of retrieved documents (k) and passages after re-ranking (kp) in different settings?
- Basis in paper: [explicit] The paper varies k and kp in experiments but doesn't provide a comprehensive sensitivity analysis across different datasets or question types.
- Why unresolved: While some ablation studies exist, there's no systematic exploration of optimal k and kp values across diverse scenarios or understanding of the trade-offs involved.
- What evidence would resolve it: A detailed sensitivity analysis showing performance curves for different k and kp combinations across multiple datasets and question difficulty levels.

### Open Question 3
- Question: How does ReflectiVA's reflective token accuracy translate to actual answer quality improvements compared to other retrieval-augmented methods?
- Basis in paper: [inferred] The paper shows high accuracy in predicting reflective tokens but doesn't directly correlate this with improvements in final answer quality versus baseline methods.
- Why unresolved: While reflective token accuracy is reported, the paper doesn't establish a clear link between this accuracy and the actual quality of generated answers compared to other approaches.
- What evidence would resolve it: A correlation study showing how reflective token accuracy relates to improvements in final answer quality, possibly using human evaluation or detailed error analysis.

## Limitations

- The two-stage training strategy lacks extensive ablation studies to validate the necessity of each component
- Reliance on GPT-4 for automatic data annotation introduces potential biases and limitations in training data quality
- Model's performance on encyclopedic knowledge may not generalize to more specialized or domain-specific knowledge bases
- Computational overhead of generating reflective tokens and performing relevance assessment for each retrieved passage could limit practical deployment

## Confidence

- **High confidence**: The basic mechanism of using reflective tokens for retrieval decisions works as described, supported by the clear architecture and implementation details provided.
- **Medium confidence**: The two-stage training strategy provides benefits beyond simpler alternatives, as the paper shows improved performance but lacks comprehensive ablation studies.
- **Medium confidence**: The model maintains strong performance on standard MLLM benchmarks while improving on knowledge-based tasks, though this preservation isn't extensively validated across all tested benchmarks.
- **Low confidence**: The generalization capability to different knowledge bases and domains beyond the Wikipedia-based datasets used in the original experiments.

## Next Checks

1. **Ablation study on training strategy**: Train a version of ReflectiVA using only the second stage (direct training without the in-article discriminator) and compare performance to verify whether the two-stage approach is necessary or if simpler training methods could achieve similar results.

2. **Reflective token prediction analysis**: Conduct a detailed error analysis on the <RET>/<NORET> and <REL>/<NOREL> predictions to identify systematic failure patterns, particularly examining cases where the model makes incorrect decisions and understanding whether these correlate with specific question types or visual content.

3. **Generalization to alternative knowledge bases**: Evaluate ReflectiVA on a different knowledge base (e.g., domain-specific medical or scientific literature) to test whether the learned reflective capabilities transfer beyond the Wikipedia-based training data used in the original experiments.