---
ver: rpa2
title: 'CAST: Cross-modal Alignment Similarity Test for Vision Language Models'
arxiv_id: '2409.11007'
source_url: https://arxiv.org/abs/2409.11007
tags:
- both
- scenes
- images
- have
- image
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: CAST is a method to evaluate self-consistency in vision language
  models by comparing their reasoning across text, image, and combined modalities.
  It generates similarity statements between two scenes and then evaluates whether
  the model remains consistent in its assessments across different input types.
---

# CAST: Cross-modal Alignment Similarity Test for Vision Language Models

## Quick Facts
- arXiv ID: 2409.11007
- Source URL: https://arxiv.org/abs/2409.11007
- Reference count: 40
- Key outcome: CAST evaluates self-consistency in VLMs by comparing their reasoning across text, image, and combined modalities, revealing significant inconsistencies in most models

## Executive Summary
CAST introduces a method to evaluate self-consistency in vision language models by comparing their reasoning across text, image, and combined modalities. The method generates similarity statements between two scenes and then evaluates whether the model remains consistent in its assessments across different input types. Testing multiple open-source and closed-source VLMs, CAST reveals that most models exhibit significant inconsistencies when generating statements from images but evaluating them with text, or vice versa. While models perform best when generating and evaluating within the same modality, cross-modal consistency drops notably. These findings highlight the need for improved robustness and alignment in multimodal AI systems, particularly as generation length increases.

## Method Summary
CAST is a two-step automated approach that evaluates self-consistency in vision language models. First, the model generates similarity statements between scene pairs using text-only, image-only, or combined inputs. Second, the same model evaluates these statements across each modality using three different prompt formats. The method uses the DOCCI dataset with CLIP-based similarity filtering to create 100 pairs of aligned scenes, then calculates consistency scores by averaging model agreement across the first three generated statements for each modality permutation.

## Key Results
- Most VLMs show significant inconsistencies when generating statements from images but evaluating them with text, or vice versa
- Models perform best when generating and evaluating within the same modality, with cross-modal consistency dropping notably
- Consistency scores decrease slightly from Top-1 to Top-3 generated statements, indicating quality decline with additional generations

## Why This Works (Mechanism)

### Mechanism 1
- Claim: CAST evaluates self-consistency by comparing a VLM's own generated statements against evaluations across different input modalities.
- Mechanism: In the generation step, the model creates similarity statements conditioned on image-only, text-only, or combined inputs. In the evaluation step, the same model validates these statements under each modality. If the model affirms its own statements equally across modalities, it is deemed self-consistent; discrepancies reveal modality misalignment.
- Core assumption: A capable VLM should produce stable, coherent reasoning about similarities whether it sees images, reads descriptions, or both.
- Evidence anchors:
  - [abstract] "CAST is a method to evaluate self-consistency in vision language models by comparing their reasoning across text, image, and combined modalities."
  - [section] "Since we focus on self-consistency, we use the same model for both generation and evaluation."
- Break condition: If the model systematically hallucinates or generates ambiguous statements that are true in one modality but not another, consistency scores drop regardless of capability.

### Mechanism 2
- Claim: Focusing on similarities rather than differences reduces superficial cue reliance and encourages deeper semantic reasoning.
- Mechanism: By asking the model to find shared features between two scenes, it must integrate cross-modal information rather than simply attending to one image or highlighting minor attribute changes.
- Core assumption: Similarity tasks force the model to process aligned information across modalities, exposing alignment failures that contrastive (difference) tasks might miss.
- Evidence anchors:
  - [section] "By focusing on shared features, the model is less likely to rely on surface-level distinctions or superficial strategies."
- Break condition: If the model defaults to generic statements (e.g., "both scenes are outdoors") that hold regardless of input, the test may not reveal true modality-specific weaknesses.

### Mechanism 3
- Claim: Using multiple evaluation prompts reduces prompt bias and yields a more robust consistency metric.
- Mechanism: Three distinct prompts ("one/both," "true/false," "yes/no") are applied to each generated statement, and scores are averaged, mitigating the impact of phrasing sensitivity.
- Core assumption: VLMs are brittle to prompt wording; averaging over diverse formulations smooths this noise.
- Evidence anchors:
  - [section] "To mitigate bias towards a certain prompt or phrasing (Pezeshkpour and Hruschka, 2024; Sclar et al., 2024), we use three different evaluation prompts."
- Break condition: If all prompts still trigger the same systematic bias (e.g., always affirming statements about visual attributes), the average may mask underlying modality weaknesses.

## Foundational Learning

- Concept: Self-consistency in AI models
  - Why needed here: CAST's core metric is the absence of contradictions between a model's own outputs across modalities; understanding what consistency means is essential to interpret results.
  - Quick check question: If a model generates "both scenes have a bridge" and later evaluates it as true for both image and text inputs, is that self-consistent?

- Concept: Cross-modal alignment
  - Why needed here: The method tests whether a model aligns vision and language representations consistently; grasping this alignment problem contextualizes CAST's contribution.
  - Quick check question: If a model sees a bridge in an image but the text description omits it, should it still affirm a statement about the bridge when evaluating with text only?

- Concept: Modality as information superset
  - Why needed here: The paper models image information as a superset of text descriptions; understanding this guides interpretation of why image evaluations can be stricter.
  - Quick check question: If H(Simg) â‰¥ H(Stxt), can a statement true in the image always be true in the text?

## Architecture Onboarding

- Component map: Data pipeline -> CLIP-based pair filtering -> DOCCI sampling -> CAST generation prompts -> Greedy sampling -> CAST evaluation prompts -> Parsing -> Consistency scoring
- Critical path: CLIP similarity filtering -> pair selection -> generation step -> evaluation step -> parsing -> scoring
- Design tradeoffs: Greedy sampling ensures determinism but may miss diversity; multiple prompts reduce bias but increase compute; restricting to CLIP-similar pairs ensures semantic overlap but may exclude edge cases
- Failure signatures: Low consistency across all modalities suggests hallucination or reasoning collapse; modality-specific drops hint at encoder misalignment; high parsing failures indicate prompt brittleness
- First 3 experiments:
  1. Run CAST on a single VLM with one pair to verify generation/evaluation flow and parsing logic
  2. Vary CLIP similarity thresholds (0.75, 0.85, 0.95) to see impact on consistency scores
  3. Compare greedy vs. temperature=0.7 sampling for statement generation to gauge robustness

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What specific architectural features or training techniques could improve cross-modal consistency in VLMs?
- Basis in paper: [explicit] The paper shows that most VLMs exhibit significant inconsistencies when generating statements from images but evaluating them with text, or vice versa.
- Why unresolved: The paper identifies the problem but does not explore solutions or modifications to model architecture or training that could address these inconsistencies.
- What evidence would resolve it: Systematic experiments comparing different VLM architectures (e.g., different vision encoders, language models, or alignment strategies) trained on various datasets to measure their cross-modal consistency using CAST.

### Open Question 2
- Question: Does the length of generated statements directly correlate with decreased consistency, and if so, why?
- Basis in paper: [explicit] The paper observes a slight decrease in CAST scores from Top-1 to Top-3 generated statements, indicating quality decline with additional generations.
- Why unresolved: The paper notes the correlation but doesn't investigate the underlying mechanisms or whether this is due to model limitations, prompt design, or information overload.
- What evidence would resolve it: Controlled experiments varying generation length and analyzing the semantic complexity and information content of statements to determine what causes the consistency drop.

### Open Question 3
- Question: How does the information content difference between modalities (image vs. text) contribute to cross-modal inconsistency?
- Basis in paper: [explicit] The paper discusses that text descriptions are a subset of image information content and that text can introduce new information through subjective interpretation.
- Why unresolved: The paper presents this as a theoretical framework but doesn't empirically test how information content differences between modalities affect consistency scores.
- What evidence would resolve it: Quantitative analysis of information entropy between image and text pairs, correlating this with consistency scores across different models to determine if information gap predicts inconsistency.

## Limitations

- Hallucination detection: The method assumes statements are grounded in scenes but doesn't actively filter hallucinated content beyond CLIP-based pair selection
- Parser brittleness: Evaluation relies on strict yes/no parsing of model outputs, potentially biasing results downward
- Dataset constraints: Only 100 CLIP-filtered pairs from DOCCI are used, limiting generalizability

## Confidence

- High confidence: The two-step generation-then-evaluation pipeline is clearly specified and reproducible
- Medium confidence: The claim that cross-modal consistency drops significantly is supported by results, but exact magnitude depends on pair selection and parser accuracy
- Low confidence: The interpretation that similarity-focused prompts reduce superficial cue reliance is plausible but not empirically validated

## Next Checks

1. Hallucination robustness test: Run CAST on pairs with known hallucinatory content to quantify how often false statements are still treated as consistent
2. Parser generalization check: Evaluate the parsing logic on a held-out set of model outputs with varied phrasing to measure robustness to linguistic variation
3. Pair diversity stress test: Expand pair selection to include lower CLIP similarity thresholds and measure how consistency scores change across the full similarity spectrum