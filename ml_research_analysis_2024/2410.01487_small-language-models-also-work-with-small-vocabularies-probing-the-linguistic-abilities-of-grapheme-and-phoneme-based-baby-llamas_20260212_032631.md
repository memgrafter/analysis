---
ver: rpa2
title: 'Small Language Models Also Work With Small Vocabularies: Probing the Linguistic
  Abilities of Grapheme- and Phoneme-Based Baby Llamas'
arxiv_id: '2410.01487'
source_url: https://arxiv.org/abs/2410.01487
tags:
- filtered
- language
- data
- phoneme
- linguistic
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Character-level tokenization enables linguistically plausible language
  models that outperform subword-based models on lexical tasks and achieve comparable
  syntactic performance. Small Llama models trained with grapheme and phoneme vocabularies
  (vocabularies of ~260-360 vs 16,000 for subwords) demonstrate strong linguistic
  abilities across syntactic, lexical, and phonological benchmarks.
---

# Small Language Models Also Work With Small Vocabularies: Probing the Linguistic Abilities of Grapheme- and Phoneme-Based Baby Llamas

## Quick Facts
- **arXiv ID:** 2410.01487
- **Source URL:** https://arxiv.org/abs/2410.01487
- **Reference count:** 25
- **Primary result:** Character-level tokenization enables linguistically plausible language models that outperform subword-based models on lexical tasks and achieve comparable syntactic performance.

## Executive Summary
This study demonstrates that small Llama models (15M parameters) trained with character-level vocabularies can achieve strong linguistic performance on standard syntactic and novel lexical/phonetic benchmarks. By using grapheme and phoneme vocabularies of ~260-360 tokens instead of the typical 16,000 subword units, the models develop linguistically meaningful representations at and below the word level. The research challenges the assumption that subword tokenization is necessary for effective language modeling, showing that character-based approaches not only match subword models on syntactic tasks but often surpass them on lexical and phonological evaluations.

## Method Summary
The study trains four small Llama models (8 hidden layers, 8 attention heads, embedding size 512, context size 64) on the BabyLM 2024 dataset (100M tokens), using grapheme and phoneme tokenization with and without whitespace. Models are evaluated on BLiMP syntactic tests, a lexical decision task distinguishing real words from nonce words, rhyme prediction, and age prediction tasks. Performance is compared against a BabyLlama baseline with subword tokenization (58M parameters, 16K vocabulary). The approach uses G2P conversion (Gi2Pi) for phoneme models and tests the hypothesis that character-level tokenization enables more linguistically plausible representations.

## Key Results
- Character-level tokenization enables linguistically plausible language models that outperform subword-based models on lexical tasks and achieve comparable syntactic performance.
- Grapheme models achieve near-perfect performance on lexical decision tasks, with whitespace-removed versions showing slight improvements.
- Phoneme models excel at phonological tasks like rhyme prediction, while grapheme models show better overall performance across all benchmarks.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Character-level tokenization enables linguistically plausible language models that outperform subword-based models on lexical tasks
- Mechanism: By using individual characters as tokens instead of arbitrary subword units, models learn more meaningful representations of linguistic units at and below the word level. This is particularly effective for lexical decision tasks where the model must distinguish real words from nonce words based on character patterns.
- Core assumption: Individual characters carry sufficient linguistic information to build word-level representations without explicit word boundary information
- Evidence anchors:
  - [abstract] "Character-level tokenization enables linguistically plausible language models that outperform subword-based models on lexical tasks"
  - [section] "Grapheme models achieve almost perfect performance on the lexical decision task â€“ in fact, the model trained without word-separating white space beats its counterpart by a tiny margin"
- Break condition: When tasks require higher-level linguistic abstractions that depend on word-level boundaries or morphological units not easily recoverable from character sequences alone

### Mechanism 2
- Claim: Small Llama models trained with character-level vocabularies achieve strong linguistic performance on standard syntactic and novel lexical/phonetic benchmarks
- Mechanism: The Llama architecture's self-attention mechanism can effectively learn hierarchical representations even from character-level input, allowing the model to implicitly discover word boundaries, morphemes, and syntactic structures through exposure to character sequences
- Core assumption: The Transformer architecture can discover linguistic structure from character-level input without explicit tokenization guidance
- Evidence anchors:
  - [abstract] "small models based on the Llama architecture can achieve strong linguistic performance on standard syntactic and novel lexical/phonetic benchmarks when trained with character-level vocabularies"
  - [section] "Our character-level LMs perform as well as (larger) subword-based LMs on syntactic tests and embedding-based prediction tasks"
- Break condition: When the character sequence becomes too long relative to context size, making it difficult for the model to maintain long-range dependencies necessary for complex syntactic processing

### Mechanism 3
- Claim: Models trained without whitespace show improved lexical/phonological performance by being forced to infer word-level structures from data itself
- Mechanism: Removing explicit word boundary markers compels the model to develop more precise representations of lexical and phonological patterns, as it cannot rely on whitespace as a crutch for word segmentation
- Core assumption: Explicit word boundaries in training data reduce the model's need to develop robust internal representations of word structure
- Evidence anchors:
  - [abstract] "Models trained without whitespace show improved lexical/phonological performance"
  - [section] "the deletion of white spaces has a negative effect on syntactic evaluations, but moderately improves the grapheme models on the BLiMP supplement and the rhyme prediction task"
- Break condition: When the absence of word boundaries makes it computationally infeasible for the model to maintain coherent representations of longer linguistic units

## Foundational Learning

- Concept: Character-level tokenization and its relationship to linguistic units
  - Why needed here: Understanding how treating characters as tokens enables learning of linguistic structures at and below the word level
  - Quick check question: How does character-level tokenization differ from subword tokenization in terms of the linguistic units being modeled?

- Concept: Transformer architecture and self-attention mechanisms
  - Why needed here: The Llama model's ability to learn hierarchical representations from character sequences depends on the Transformer's self-attention mechanism
  - Quick check question: How does self-attention enable the model to discover word boundaries and syntactic structure from character-level input?

- Concept: Perplexity and minimal pair evaluation paradigms
  - Why needed here: These are the primary evaluation metrics used to assess model performance on linguistic tasks
  - Quick check question: What does a lower perplexity score indicate about a model's performance on a linguistic task?

## Architecture Onboarding

- Component map:
  - BabyLM 2024 dataset (100M tokens) -> Preprocessing (grapheme/phoneme conversion, whitespace options) -> Character-level tokenizer (vocabularies of ~260-360 tokens) -> Small Llama architecture (8 layers, 8 heads, 512 embedding size, 64 context) -> Training (5 epochs) -> Evaluation (BLiMP, lexical decision, rhyme prediction, age prediction)

- Critical path:
  1. Data preprocessing (grapheme vs phoneme conversion, whitespace removal options)
  2. Tokenizer creation with character-level vocabularies
  3. Model initialization and training
  4. Evaluation on multiple linguistic benchmarks
  5. Analysis of performance differences across tokenization strategies

- Design tradeoffs:
  - Character-level input increases sequence length but provides more linguistically meaningful tokens
  - Smaller context size (64) vs larger subword models but compensated by character-level granularity
  - Tradeoff between computational efficiency and linguistic plausibility

- Failure signatures:
  - Poor performance on tasks requiring word-level boundaries when whitespace is removed
  - Inability to distinguish real words from nonce words in lexical decision tasks
  - Unexpected performance drops on syntactic tasks when using phoneme representations

- First 3 experiments:
  1. Train grapheme model with and without whitespace to verify lexical decision performance differences
  2. Compare phoneme vs grapheme model performance on rhyme prediction task
  3. Evaluate subword-based BabyLlama vs character-based models on BLiMP to confirm comparable syntactic performance

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does training phoneme-based models on phonetically plausible data with more variation improve their performance compared to G2P-converted data?
- Basis in paper: [inferred] The paper suggests phoneme models suffer from errors introduced through too rigid G2P and could benefit from more fine-grained representations, particularly for phonological tasks requiring subtle distinctions in pronunciation.
- Why unresolved: The paper notes that training on phonetically plausible data requires manually checked transcriptions, which are rare and often limited in size.
- What evidence would resolve it: Training phoneme models on corpora of transcribed spontaneous speech with regional, social, and individual variation, then evaluating them on the same tasks as the current models.

### Open Question 2
- Question: How do the latent vector spaces of character-based models differ from those of subword-based models in terms of encoding higher linguistic units like syllables or morphemes?
- Basis in paper: [explicit] The paper suggests investigating whether and how these models develop representations of higher linguistic units, such as syllables or morphemes, and how their latent vector spaces differ from those of subword-based models.
- Why unresolved: The paper states this requires further research with directed experiments, which have not yet been conducted.
- What evidence would resolve it: Comparative analysis of vector space geometry and probing tasks specifically designed to detect representations of syllables, morphemes, and other intermediate linguistic units in both character-based and subword-based models.

### Open Question 3
- Question: Does excluding explicit word boundaries in grapheme models lead to more precise encoding of phonological and lexical patterns compared to models trained with whitespace?
- Basis in paper: [explicit] The paper observes that models trained without whitespace show moderate improvements at lexical/phonological tasks and suggests this might indicate grapheme models trained without whitespace develop more precise and granular latent representations at the lexical and phonological levels.
- Why unresolved: The paper notes this is a hypothesis that warrants further exploration with directed experiments.
- What evidence would resolve it: Detailed analysis of learned representations using techniques like representational similarity analysis and targeted probing tasks comparing models trained with and without whitespace on specific phonological and lexical phenomena.

## Limitations

- The study uses small-scale models (15M parameters) and limited data (100M tokens), which may not generalize to larger models or more diverse linguistic phenomena.
- The G2P conversion process may introduce inconsistencies or errors that affect phoneme model performance, particularly for contractions and edge cases.
- The absence of human evaluation leaves open the possibility that computational metrics may not fully capture linguistic competence or human-like language understanding.

## Confidence

**High Confidence** in the core finding that character-level tokenization can enable strong linguistic performance on lexical tasks, particularly given the near-perfect scores on the lexical decision task and the clear advantage over subword-based models in distinguishing real words from nonce words.

**Medium Confidence** in the comparative performance between grapheme and phoneme models, as the differences in performance across various tasks suggest that while both approaches work, the choice between them may depend heavily on the specific linguistic task and dataset characteristics.

**Medium Confidence** in the claim that removing whitespace improves lexical/phonological performance, as the results show moderate improvements but also reveal negative impacts on syntactic evaluations.

## Next Checks

1. **Scale-up validation**: Train larger versions of the character-based models (e.g., 100M+ parameters) on more extensive datasets to determine if the lexical advantages of character tokenization persist at scale and whether the gap with subword models widens or narrows.

2. **Cross-linguistic robustness**: Test the character-level approach on morphologically rich languages (e.g., Turkish, Finnish) and languages with non-Latin scripts to verify whether the observed advantages generalize beyond English and whether phoneme-based approaches become more or less advantageous for languages with complex orthographic-to-phonological mappings.

3. **Human evaluation benchmark**: Conduct human evaluations comparing model outputs across tokenization strategies on the same linguistic tasks to validate that computational performance metrics align with human judgments of linguistic competence and to identify any discrepancies between computational and human measures of language understanding.