---
ver: rpa2
title: '1.5-Pints Technical Report: Pretraining in Days, Not Months -- Your Language
  Model Thrives on Quality Data'
arxiv_id: '2408.03506'
source_url: https://arxiv.org/abs/2408.03506
tags:
- language
- dataset
- training
- tokens
- pints
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a compute-efficient approach to pre-training
  a Language Model, achieving strong performance in just 9 days with a 1.56 billion
  parameter model named "1.5-Pints". The key innovation lies in carefully curating
  a high-quality pre-training dataset of 57 billion tokens, prioritizing expository
  and "textbook-like" content to enhance reasoning and logical deduction capabilities.
---

# 1.5-Pints Technical Report: Pretraining in Days, Not Months -- Your Language Model Thrives on Quality Data

## Quick Facts
- arXiv ID: 2408.03506
- Source URL: https://arxiv.org/abs/2408.03506
- Reference count: 40
- 1.5-Pints achieves strong performance in just 9 days with 1.56B parameters, outperforming larger models using 15-25x less training data

## Executive Summary
This paper presents a compute-efficient approach to pre-training language models, demonstrating that high-quality curated data can dramatically reduce training time and resources. The 1.5-Pints model, with 1.56 billion parameters, achieves state-of-the-art performance on MT-Bench benchmark in just 9 days by prioritizing expository and "textbook-like" content over quantity. The approach makes pretraining more accessible and environmentally friendly while maintaining competitive reasoning and logical deduction capabilities.

## Method Summary
The authors developed a high-quality pre-training dataset of 57 billion tokens, carefully curated to prioritize expository and textbook-like content that enhances reasoning capabilities. They trained a 1.56 billion parameter model using architectural modifications including a modified Mistral tokenizer and larger Multi-layer Perceptron layers. The focus on data quality over quantity enabled pretraining in just 9 days while achieving superior performance compared to larger models like Apple's OpenELM and Microsoft's Phi that used 15-25 times more training data.

## Key Results
- 1.5-Pints outperforms state-of-the-art models like Apple's OpenELM and Microsoft's Phi on MT-Bench benchmark
- Achieved training in just 9 days using 15-25 times less data than comparable models
- Model available in two versions: 2K and 16K context windows
- Open-sourced to facilitate further advancements in the field

## Why This Works (Mechanism)
The key innovation lies in carefully curating high-quality training data rather than maximizing dataset size. By prioritizing expository and textbook-like content, the model develops stronger reasoning and logical deduction capabilities. This approach, combined with architectural optimizations like modified tokenization and larger MLP layers, enables significant efficiency gains in both training time and computational resources while maintaining or exceeding the performance of larger models trained on more data.

## Foundational Learning
- **Data curation for reasoning**: Selecting textbook-like and expository content enhances logical deduction capabilities - needed to improve reasoning performance, quick check: compare reasoning task performance with and without curated data
- **Quality vs quantity trade-off**: Smaller, higher-quality datasets can outperform larger, noisier ones - needed to reduce training costs while maintaining performance, quick check: measure performance per training token
- **Tokenization optimization**: Modified Mistral tokenizer improves efficiency - needed to reduce vocabulary size and computational overhead, quick check: compare perplexity with standard vs modified tokenizer
- **MLP scaling**: Larger Multi-layer Perceptron layers enhance model capacity - needed to compensate for reduced dataset size while maintaining representational power, quick check: ablation study varying MLP dimensions

## Architecture Onboarding
- **Component map**: Data curation pipeline -> Modified tokenizer -> Larger MLPs -> 1.56B parameter model -> MT-Bench evaluation
- **Critical path**: High-quality dataset creation → Tokenizer optimization → Architectural modifications → Efficient pretraining → Performance evaluation
- **Design tradeoffs**: Prioritized data quality over quantity, accepting smaller dataset size for improved reasoning capabilities and reduced compute requirements
- **Failure signatures**: Poor reasoning performance despite competitive training metrics would indicate issues with data curation quality or architectural modifications
- **First experiments**: 1) Benchmark comparison with OpenELM and Phi on MT-Bench, 2) Ablation study on dataset size vs quality, 3) Efficiency analysis comparing training time and compute usage

## Open Questions the Paper Calls Out
None

## Limitations
- Limited details on specific dataset filtering criteria and selection process
- Single benchmark evaluation may not fully capture model capabilities across diverse tasks
- Unclear whether results generalize to larger models or different task domains
- Contribution of architectural modifications versus data quality to overall performance remains ambiguous

## Confidence
- High confidence: Compute efficiency improvements are real and measurable
- Medium confidence: Data quality impact on reasoning capabilities
- Medium confidence: MT-Bench performance relative to comparable models
- Low confidence: Claims about specific content types driving reasoning improvements

## Next Checks
1. Replicate the training with alternative high-quality datasets using identical curation methodologies to test whether the performance gains transfer beyond the specific dataset used
2. Conduct ablation studies isolating the impact of architectural modifications (tokenization, MLP sizes) from data quality effects on final model performance
3. Evaluate model performance across a broader suite of benchmarks including reasoning-specific tasks, factual knowledge tests, and generation quality metrics to validate the claimed reasoning capabilities