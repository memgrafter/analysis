---
ver: rpa2
title: 'Multitask Mayhem: Unveiling and Mitigating Safety Gaps in LLMs Fine-tuning'
arxiv_id: '2409.15361'
source_url: https://arxiv.org/abs/2409.15361
tags:
- safety
- tasks
- fine-tuning
- data
- fine-tuned
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigates how fine-tuning large language models (LLMs)
  on benign data for various downstream tasks affects their safety guardrails. We
  evaluate task-specific safety degradation across summarization, code generation,
  translation, and classification, using both proprietary (GPT-4o-mini, Gemini 1.5
  Flash) and open-source (Llama3.1-8B) models.
---

# Multitask Mayhem: Unveiling and Mitigating Safety Gaps in LLMs Fine-tuning

## Quick Facts
- **arXiv ID**: 2409.15361
- **Source URL**: https://arxiv.org/abs/2409.15361
- **Reference count**: 40
- **Primary result**: Fine-tuning LLMs on benign data for various tasks degrades safety guardrails, with translation and classification showing highest vulnerability.

## Executive Summary
This study investigates how fine-tuning large language models (LLMs) on benign data for downstream tasks affects their safety guardrails. Through systematic evaluation of summarization, code generation, translation, and classification tasks across proprietary (GPT-4o-mini, Gemini 1.5 Flash) and open-source (Llama3.1-8B) models, the research reveals that translation and classification tasks exhibit the weakest safety protections with attack success rates ranging from 73-92%. Fine-tuning on code generation and translation data leads to the highest safety degradation. To address these gaps, the authors propose MultiTaskBench, a multitask safety dataset that significantly reduces ASR across all tasks without compromising helpfulness.

## Method Summary
The study fine-tuned three models (GPT-4o-mini, Gemini 1.5 Flash, Llama3.1-8B) on benign datasets for four downstream tasks: text generation (1K scientific papers), code generation (1K GitHub samples), translation (1K English-French pairs), and classification (2K sentiment analysis samples). Safety was evaluated using a GPT-4o judge on 205 harmful prompts from Toxigen, CrowS-Pairs, and task-specific sources. Multiple scenarios were tested: base models, fine-tuned models, models with guard models (Llama3-Guard, OpenAI Moderator), and models fine-tuned with the MultiTaskBench safety dataset. ASR was measured across all tasks to assess safety degradation.

## Key Results
- Translation and classification tasks show highest safety vulnerability with ASR rates of 73-92% across all model types
- Fine-tuning on code generation and translation data causes the most significant safety guardrail degradation
- Existing guard models and safety datasets fail to provide cross-task robustness, particularly for translation and classification
- MultiTaskBench safety dataset reduces ASR across all tasks without compromising model helpfulness

## Why This Works (Mechanism)

### Mechanism 1
Fine-tuning on benign data degrades safety guardrails more for certain tasks (translation, classification) than others. During fine-tuning, models learn task-specific patterns that can inadvertently weaken safety boundaries, especially in tasks where safety alignment was previously weaker. Evidence shows fine-tuning on code generation and translation leads to highest degradation, with open-source models showing highest ASR against translation tasks. This mechanism breaks if safety guardrails are uniformly strong across all tasks initially or if fine-tuning methodology inherently preserves safety boundaries.

### Mechanism 2
Guard models and safety-tuning datasets fail to generalize across different task types because they're typically designed and evaluated for text generation tasks, creating blind spots for other task types like translation and classification. Evidence shows guard models are effective at blocking text and code generation prompts but face challenges with translation and classification prompts. This mechanism breaks if safety solutions are designed with task-agnostic safety principles from the start.

### Mechanism 3
MultiTaskBench safety dataset reduces ASR across all tasks without compromising helpfulness by curating prompts across all task types and violation categories, providing comprehensive safety training that addresses blind spots in existing datasets. Evidence shows fine-tuning with this dataset demonstrates substantial reduction in ASR across all tasks. This mechanism breaks if task-specific safety degradation patterns are too complex to capture with a single dataset, or if safety improvements in one task create vulnerabilities in others.

## Foundational Learning

- **Safety alignment techniques (RLHF, safety-tuning datasets)**: Understanding how safety is initially implemented helps explain why fine-tuning can degrade these guardrails. Quick check: What is the difference between base model safety and instruction-tuned model safety?

- **Task-specific performance vs. task-specific safety**: Fine-tuning improves task performance but can simultaneously weaken safety boundaries in complex ways. Quick check: Why might improving translation accuracy also make a model more likely to answer harmful translation prompts?

- **Guard model architecture and filtering mechanisms**: Understanding how guards work explains their task-specific limitations and failure modes. Quick check: How do input guards differ from output guards in their approach to safety filtering?

## Architecture Onboarding

- **Component map**: Base LLM → Fine-tuning pipeline → Safety guardrails → Task-specific evaluation → Guard models + Safety-tuning datasets → GPT-4o judge + human annotators

- **Critical path**: Fine-tuning → Safety degradation → Guard model evaluation → MultiTaskBench development → Safety improvement

- **Design tradeoffs**: Safety vs. helpfulness (safety measures can increase false refusals), task-specific vs. task-agnostic safety (specialized solutions work better but don't generalize), resource constraints (limited fine-tuning epochs and batch sizes affect results)

- **Failure signatures**: High ASR in translation/classification tasks indicates safety degradation, guard model failures show up as high ASR in specific task categories, overrefusal shows up as low helpfulness scores

- **First 3 experiments**:
  1. Fine-tune base model on benign summarization data and measure ASR across all four tasks
  2. Apply OpenAI moderator to base model and measure task-specific ASR reduction
  3. Fine-tune base model with MultiTaskBench data and compare ASR reduction vs. existing safety datasets

## Open Questions the Paper Calls Out

**Open Question 1**: How does the effectiveness of the MultiTaskBench safety-tuning dataset generalize to other downstream tasks beyond the four investigated? The study's scope is limited to four tasks, and the authors acknowledge that a broader range of tasks would be needed to fully validate the dataset's generalizability.

**Open Question 2**: To what extent does fine-tuning on benign data compromise the effectiveness of LLMs against advanced adversarial and jailbreaking attacks? The study focuses on the impact of fine-tuning on safety guardrails but does not directly investigate the effect on the models' resistance to advanced adversarial attacks.

**Open Question 3**: How do the safety guardrails of LLMs vary across different types of harmful content within the same task category? The study focuses on overall ASR across task categories but does not delve into the nuances of different types of harmful content within each category.

**Open Question 4**: What are the specific factors that contribute to the higher susceptibility of open-source models to safety violations compared to proprietary models? The study identifies the difference in safety performance between open-source and proprietary models but does not investigate the underlying factors contributing to this difference.

## Limitations

- Study scope limited to four task types (summarization, code generation, translation, classification) may not capture all scenarios where fine-tuning degrades safety
- Reliance on GPT-4o judge for safety evaluation introduces potential subjectivity, though partially mitigated by human annotator validation
- Relatively small fine-tuning datasets (1K samples per task) and limited epochs (3) may not reflect real-world deployment conditions

## Confidence

**High Confidence**: Translation and classification tasks show highest ASR rates across all tested models; MultiTaskBench effectiveness in reducing ASR across multiple task types.

**Medium Confidence**: Fine-tuning on code generation and translation data leads to highest safety degradation; generalizability of findings to other task types and model architectures requires further validation.

**Low Confidence**: Existing safety solutions lack cross-task robustness based on limited guard model testing; conclusions about fundamental limitations of current safety alignment techniques would benefit from testing additional safety solutions and model architectures.

## Next Checks

1. **Cross-Architecture Validation**: Test the fine-tuning safety degradation hypothesis on additional model architectures (e.g., Mistral, Phi) and sizes (7B, 13B, 70B) to determine if observed patterns are architecture-agnostic.

2. **Real-World Deployment Simulation**: Conduct longitudinal study with continuous fine-tuning on production-scale datasets to assess whether safety degradation accumulates over time and with increasing training volume.

3. **Alternative Safety Solution Evaluation**: Test additional guard models and safety datasets (e.g., Anthropic's Constitutional AI, DeepMind's Sparrow) to determine if cross-task robustness limitations are universal or specific to tested solutions.