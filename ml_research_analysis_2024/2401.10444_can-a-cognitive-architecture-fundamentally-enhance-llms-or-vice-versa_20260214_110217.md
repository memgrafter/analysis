---
ver: rpa2
title: Can A Cognitive Architecture Fundamentally Enhance LLMs? Or Vice Versa?
arxiv_id: '2401.10444'
source_url: https://arxiv.org/abs/2401.10444
tags:
- llms
- processes
- implicit
- explicit
- cognitive
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper argues that current LLM-centered AI systems suffer from
  fundamental limitations due to their lack of human-like cognition, and proposes
  incorporating insights from computational cognitive architectures to address these
  issues. It emphasizes the importance of dual-process architectures and hybrid neuro-symbolic
  approaches, integrating implicit processes (captured by LLMs) with explicit symbolic
  processes.
---

# Can A Cognitive Architecture Fundamentally Enhance LLMs? Or Vice Versa?

## Quick Facts
- arXiv ID: 2401.10444
- Source URL: https://arxiv.org/abs/2401.10444
- Reference count: 0
- The paper proposes integrating cognitive architectures with LLMs to address fundamental limitations in current AI systems by incorporating human-like cognition, dual-process reasoning, and motivation systems.

## Executive Summary
The paper argues that current LLM-centered AI systems lack fundamental human-like cognitive capabilities due to their absence of dual-process architectures, explicit symbolic reasoning, and motivation systems. It proposes a hybrid approach that combines LLMs (capturing implicit processes) with computational cognitive architectures like Clarion (providing explicit symbolic processes) to create more capable, reliable, and human-like AI systems. The framework emphasizes the importance of integrating human psychology into AI development to achieve better reasoning, memory, motivation, and learning capabilities.

## Method Summary
The proposed method involves integrating LLMs as implicit processing layers within the Clarion cognitive architecture's four subsystems (ACS, NACS, MS, MCS), while maintaining explicit symbolic processes for reasoning and knowledge representation. This hybrid neuro-symbolic approach uses prompt engineering to guide LLMs for explicit reasoning tasks, implements dual-process interaction mechanisms for bottom-up and top-down information flow, and incorporates human motivation theory to enable goal-directed behavior. The architecture aims to leverage LLMs' ability to capture intuition through massive linguistic data while providing structured symbolic guidance for enhanced reasoning and decision-making.

## Key Results
- Proposes a framework for integrating LLMs with cognitive architectures to address fundamental limitations in current AI systems
- Demonstrates how dual-process architectures can combine implicit LLM processing with explicit symbolic reasoning
- Presents a detailed example using the Clarion architecture showing potential enhancements in reasoning, memory, motivation, and learning
- Emphasizes the importance of human psychology and motivation theory in creating more capable and reliable AI systems

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLMs can capture human intuition and instinct through implicit processes learned from vast linguistic data.
- Mechanism: Large-scale training on diverse text data allows LLMs to internalize statistical patterns and structures that correspond to human intuitive understanding of the world.
- Core assumption: Human intuition can be partially captured through language, and repeated exposure to linguistic patterns enables statistical learning that mirrors intuitive cognition.
- Evidence anchors:
  - [abstract] The paper argues that LLMs can capture intuition and instinct, corresponding to implicit processes in humans.
  - [section] "Human intuition results from implicit (unconscious) processes... intuition is often obtained through experience, especially a large amount of repeated experience. On the other hand, a large amount of repeated experience is exactly what the training of LLMs with a large amount of linguistic data provides."
  - [corpus] Weak correlation with corpus evidence; no direct supporting papers found.
- Break condition: If the linguistic data lacks sufficient diversity or fails to capture the statistical patterns underlying human intuitive reasoning, the LLM cannot adequately represent intuition.

### Mechanism 2
- Claim: Explicit symbolic processes can guide and enhance LLM reasoning through structured prompting and interaction.
- Mechanism: Dual-process architectures enable top-down activation where explicit symbolic knowledge directs LLM processing, and bottom-up activation where LLM outputs inform symbolic reasoning.
- Core assumption: The interaction between implicit (LLM) and explicit (symbolic) processes is essential for achieving human-level reasoning capabilities.
- Evidence anchors:
  - [abstract] The paper emphasizes the importance of dual-process architectures and hybrid neuro-symbolic approaches.
  - [section] "Within Clarion, detailed processes of implicit-explicit interaction have been elucidated mechanistically... These interaction processes can be mapped, when LLMs are incorporated into Clarion, to those between LLMs (capturing implicit processes) and explicit, symbolic processes."
  - [corpus] Weak correlation; corpus lacks papers specifically addressing dual-process architectures with LLMs.
- Break condition: If symbolic processes cannot effectively prompt or interpret LLM outputs, the interaction fails to enhance reasoning capabilities.

### Mechanism 3
- Claim: Human-like motivation systems enable autonomous goal-directed behavior in AI systems.
- Mechanism: Intrinsic motives and needs, when integrated into cognitive architectures, provide the foundation for self-directed behavior and alignment with human values.
- Core assumption: Motivation is a fundamental component of intelligence that current LLM-centered systems lack.
- Evidence anchors:
  - [abstract] The paper argues that future systems should draw inspiration from human psychology and models of the human mind.
  - [section] "Motivation is of fundamental importance to humans and human-like behavior... Without motivational processes, they would be aimless; or they would have to rely on prior knowledge coded into them."
  - [corpus] Weak correlation; corpus lacks papers specifically addressing motivation in AI systems.
- Break condition: If the motivational framework cannot effectively translate into goal-directed behavior or value alignment, the system fails to achieve human-like autonomy.

## Foundational Learning

- Concept: Dual-process cognition
  - Why needed here: Understanding the distinction between implicit and explicit processes is crucial for designing architectures that leverage both LLM capabilities and symbolic reasoning.
  - Quick check question: Can you explain the difference between System 1 (intuitive) and System 2 (deliberative) thinking and how they relate to LLM and symbolic processing?

- Concept: Neuro-symbolic integration
  - Why needed here: The architecture must effectively combine neural network capabilities with symbolic reasoning to achieve human-like intelligence.
  - Quick check question: How would you design a system that allows symbolic processes to guide neural network outputs and vice versa?

- Concept: Human motivation theory
  - Why needed here: Incorporating human-like motivation systems is essential for creating autonomous agents that align with human values and exhibit goal-directed behavior.
  - Quick check question: What are the key differences between intrinsic and extrinsic motivation, and how might each be implemented in an AI system?

## Architecture Onboarding

- Component map: MS (selects goals from activated drives) -> MCS (chooses actions/meta-actions to maximize drive satisfaction) -> ACS and NACS (process actions and knowledge) with LLMs integrated at implicit levels of each subsystem.

- Critical path: The primary information flow starts with the MS selecting goals based on activated drives, followed by the MCS choosing actions/meta-actions to maximize drive satisfaction, with the ACS and NACS processing actions and knowledge respectively. The integration with LLMs occurs primarily at the implicit levels of each subsystem.

- Design tradeoffs: The architecture must balance between LLM autonomy and symbolic control, determine when to rely on implicit intuition versus explicit reasoning, and decide how to structure the interaction between neural and symbolic components. The tradeoff between computational efficiency and psychological realism must also be considered.

- Failure signatures: If the system produces inconsistent outputs, lacks goal-directed behavior, or fails to integrate LLM outputs with symbolic reasoning effectively, these indicate problems in the dual-process interaction. Over-reliance on either implicit or explicit processes can lead to brittle or uninterpretable behavior.

- First 3 experiments:
  1. Implement a simple dual-process reasoning task where an LLM generates initial responses that are then refined by explicit symbolic rules, measuring improvement in reasoning accuracy.
  2. Create a basic motivation system where drives influence goal selection and observe how this affects agent behavior in a simulated environment.
  3. Develop an episodic memory component that stores and retrieves LLM-generated descriptions of experiences, testing retrieval accuracy and relevance.

## Open Questions the Paper Calls Out

- Question: What specific architectural changes are needed to effectively integrate LLMs into existing cognitive architectures like Clarion?
  - Basis in paper: [explicit] The paper proposes incorporating LLMs into Clarion but doesn't provide specific implementation details.
  - Why unresolved: The paper presents a high-level framework but lacks concrete technical specifications for integration.
  - What evidence would resolve it: A detailed technical specification showing how LLMs would be connected to each subsystem of Clarion, including data flow and interaction mechanisms.

- Question: How can we empirically validate that a cognitive architecture-enhanced LLM system truly captures human-like dual processes?
  - Basis in paper: [explicit] The paper mentions that Clarion has been validated against psychological data but doesn't address validation of the enhanced system.
  - Why unresolved: Current psychological validation methods may not be sufficient to test the complex interactions between LLMs and symbolic processes.
  - What evidence would resolve it: A comprehensive testing framework that includes both traditional psychological tests and new benchmarks specifically designed for hybrid neuro-symbolic systems.

- Question: What are the limitations of using LLMs to capture implicit processes in cognitive architectures, and how can these limitations be addressed?
  - Basis in paper: [inferred] The paper acknowledges that LLMs have limitations but doesn't explore them in depth.
  - Why unresolved: The paper focuses on the potential benefits of LLMs but doesn't thoroughly examine their drawbacks or limitations.
  - What evidence would resolve it: A detailed analysis of LLM limitations in capturing implicit processes, along with proposed solutions or alternative approaches.

## Limitations

- The proposed dual-process integration between LLMs and symbolic systems rests on untested assumptions about whether linguistic patterns can effectively capture human intuition.
- The effectiveness of current prompt engineering techniques for enabling symbolic processes to guide LLM reasoning has not been demonstrated at scale.
- The translation of psychological motivation theories into computational systems that produce reliable goal-directed behavior in AI has not been established.

## Confidence

- **Medium Confidence**: The architectural framework for integrating cognitive architectures with LLMs is well-grounded in established cognitive science principles, but lacks empirical validation of key mechanisms.
- **Low Confidence**: The effectiveness of LLMs in capturing human intuition through implicit processes and the practical implementation of dual-process interaction for enhanced reasoning.
- **Medium Confidence**: The theoretical importance of motivation in AI systems, though the practical implementation details and behavioral outcomes remain uncertain.

## Next Checks

1. **Empirical Validation of Dual-Process Interaction**: Conduct controlled experiments comparing LLM performance with and without explicit symbolic guidance on reasoning tasks, measuring improvement in accuracy and consistency. This would test whether the proposed dual-process interaction actually enhances reasoning capabilities.

2. **Motivation System Behavioral Testing**: Implement a basic motivational framework within a simulated environment and observe whether the system exhibits coherent, goal-directed behavior aligned with specified drives. Measure how effectively the system balances competing motivations and adapts to changing environmental conditions.

3. **Cross-Domain Intuition Transfer**: Test whether an LLM trained on linguistic data can successfully apply intuitive reasoning to non-linguistic domains (e.g., physical reasoning, social situations) that were not explicitly present in the training data. This would validate the claim that LLMs can capture generalizable intuitive understanding.