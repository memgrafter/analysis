---
ver: rpa2
title: 'Thinking Before Speaking: A Role-playing Model with Mindset'
arxiv_id: '2409.13752'
source_url: https://arxiv.org/abs/2409.13752
tags:
- character
- llms
- dialogue
- knowledge
- role
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of making large language models
  (LLMs) convincingly portray specific roles in dialogues. The authors propose a "Thinking
  Before Speaking" (TBS) model that enhances role-playing by extending the training
  data with character-specific scenarios, historical dialogues, and pre-dialogue mindset
  generation.
---

# Thinking Before Speaking: A Role-playing Model with Mindset

## Quick Facts
- **arXiv ID**: 2409.13752
- **Source URL**: https://arxiv.org/abs/2409.13752
- **Reference count**: 40
- **Primary result**: TBS model achieves 6.81/7.0 overall score, outperforming baselines in role-playing tasks

## Executive Summary
This paper addresses the challenge of making large language models convincingly portray specific roles in dialogues. The authors propose a "Thinking Before Speaking" (TBS) model that enhances role-playing by extending the training data with character-specific scenarios, historical dialogues, and pre-dialogue mindset generation. The method involves generating character backgrounds, creating realistic scenarios, producing dialogues that mimic the character's tone, and incorporating character-specific thinking logic before each response. Additionally, the model is trained with "hallucination knowledge" - content beyond the character's knowledge - to improve its ability to refuse answering questions outside the character's scope. Experimental results show that the TBS model outperforms baseline models like CharacterLLM and RoleLLM across all metrics.

## Method Summary
The TBS model combines special prompts with fine-tuning to improve role-playing. It extends training data by generating character-specific scenarios and dialogues that mimic the character's tone, then adds pre-dialogue mindset generation where the model creates internal reasoning before producing responses. The approach also incorporates "hallucination knowledge" - training examples that include content beyond the character's knowledge - to teach the model to refuse answering questions outside its scope. The model is fine-tuned using LoRA on the constructed dataset with batch size 64, learning rate 5e-5, and 10 epochs.

## Key Results
- TBS Llama3 achieves highest overall score of 6.81 out of 7.0
- Outperforms baseline models (CharacterLLM, RoleLLM) across all evaluation metrics
- Shows improvements in contextual immersion, emotional resonance, language style, logical thinking, and adaptability
- Ablation studies confirm role thinking, foresight knowledge, and special prompts are crucial for effectiveness

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Thinking-before-speaking mindset generation improves role consistency by embedding character-specific thought logic before each dialogue response.
- **Mechanism**: The model generates a "thinking" segment that reflects the character's internal reasoning, relationship context, and situational awareness before producing the actual spoken response.
- **Core assumption**: Character responses are better aligned with their persona when preceded by explicit internal thought modeling that incorporates personality, relationships, and current scenario context.
- **Evidence anchors**: [abstract] "supplementing each pair of dialogue with the character's mindset"; [section] "we generated the mindset of the characters' pre-dialogue thinking for each pair of dialogues based on the character backgrounds and the character relationships"
- **Break condition**: If the thinking segment does not actually reflect the character's internal logic or is disconnected from the current dialogue context, the improvement in role consistency disappears.

### Mechanism 2
- **Claim**: Including "hallucination knowledge" during fine-tuning teaches the model to refuse answering questions beyond the character's scope.
- **Mechanism**: The model is trained on negative samples that include scenarios and questions the character would not know, with the expectation that it learns to identify and decline such queries.
- **Core assumption**: Fine-tuning on out-of-scope examples will generalize to real cases where the character lacks relevant knowledge, improving refusal behavior.
- **Evidence anchors**: [abstract] "add few data points that include elements beyond the role's knowledge, and fine-tune the LLMs"; [section] "we add few data points that include elements beyond the role's knowledge, and fine-tune the LLMs"
- **Break condition**: If the model fails to generalize from the training examples to unseen out-of-scope questions, it may still hallucinate or incorrectly attempt to answer.

### Mechanism 3
- **Claim**: Data expansion through scenario generation and dialogue mimicry allows the model to learn richer character representation beyond limited historical dialogue.
- **Mechanism**: The approach segments character life experiences, generates plausible scenarios, creates dialogues that mimic the character's tone, and pairs them with mindset generation to enrich the training data.
- **Core assumption**: Expanding the dialogue dataset with generated but contextually consistent scenarios improves the model's ability to emulate the character's knowledge, tone, and logic.
- **Evidence anchors**: [abstract] "extend the data based on the character's real-life scenarios and the historical dialogue"; [section] "we propose mimicking each character while expanding our dialogue dataset"
- **Break condition**: If generated scenarios or dialogues diverge significantly from the character's authentic behavior or historical context, the model may learn incorrect character representations.

## Foundational Learning

- **Concept**: Fine-tuning with LoRA (Low-Rank Adaptation)
  - **Why needed here**: Allows efficient adaptation of large pre-trained LLMs to specific roles without full retraining, enabling task-specific knowledge injection while preserving base capabilities
  - **Quick check question**: What are the key hyperparameters for LoRA fine-tuning in this context (rank, alpha, learning rate, batch size)?

- **Concept**: Prompt engineering for role-play tasks
  - **Why needed here**: Special prompts guide the model to adopt character personas, incorporate mindset thinking, and handle out-of-scope knowledge appropriately
  - **Quick check question**: How does the prompt structure differ between standard instruction following and role-playing with mindset generation?

- **Concept**: Dataset construction for character representation
  - **Why needed here**: High-quality, contextually rich datasets that include character profiles, historical dialogues, scenarios, and mindset generation are essential for effective role-playing model training
  - **Quick check question**: What are the key components that must be included in each training sample to capture character authenticity?

## Architecture Onboarding

- **Component map**: Base LLM (GLM-4-9B-chat, Llama-2-7B, Llama-3-8B) → LoRA Adapter → Special Prompt Layer → Mindset Generation → Dialogue Output
- **Critical path**: Character profile → Scenario generation → Dialogue mimicry → Mindset generation → Training data construction → LoRA fine-tuning → Inference with special prompts
- **Design tradeoffs**: Richer character representation vs. computational cost; more extensive data generation vs. potential hallucination; mindset generation vs. response latency
- **Failure signatures**: Inconsistent character voice across responses; failure to refuse out-of-scope questions; poor adaptation to unexpected scenarios; loss of character knowledge over conversation turns
- **First 3 experiments**:
  1. Test mindset generation independently: Feed a character profile and scenario to the model, check if the thinking output is contextually appropriate and character-consistent
  2. Test hallucination knowledge integration: Evaluate model responses to out-of-scope questions with and without hallucination knowledge training
  3. Test data expansion impact: Compare model performance on role-playing tasks using original vs. expanded datasets

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: How does the Thinking Before Speaking (TBS) model handle the trade-off between maintaining character authenticity and adapting to unexpected user inputs that fall outside the character's knowledge?
- **Basis in paper**: Explicit
- **Why unresolved**: The paper mentions that the TBS model incorporates "hallucination knowledge" to avoid answering questions beyond the character's knowledge, but it does not provide a detailed analysis of how the model balances authenticity with adaptability in real-time interactions.
- **What evidence would resolve it**: A comprehensive evaluation of the model's responses to a diverse set of unexpected questions, comparing the authenticity and adaptability of TBS-generated responses against baseline models.

### Open Question 2
- **Question**: What are the computational costs associated with the TBS model's approach of generating character-specific scenarios and mindsets, and how do these costs scale with the complexity of the character?
- **Basis in paper**: Inferred
- **Why unresolved**: While the paper describes the data construction process and the inclusion of character-specific scenarios and mindsets, it does not discuss the computational resources required for these tasks or how they scale with more complex characters.
- **What evidence would resolve it**: A detailed analysis of the computational resources (e.g., time, memory) required for generating scenarios and mindsets for different characters, along with scalability assessments.

### Open Question 3
- **Question**: How does the inclusion of character-specific thinking logic before each response impact the overall coherence and user engagement in multi-turn dialogues?
- **Basis in paper**: Explicit
- **Why unresolved**: The paper introduces the concept of character-specific thinking logic to enhance role-playing, but it does not provide empirical evidence on how this feature affects the coherence and engagement of dialogues over multiple turns.
- **What evidence would resolve it**: User studies or engagement metrics comparing multi-turn dialogues generated by TBS models with and without the thinking logic component, assessing factors such as coherence, immersion, and user satisfaction.

## Limitations

- The hallucination knowledge training appears to have limited success (5/8 correct refusals), suggesting the approach may not work as intended
- Evaluation relies entirely on automated metrics without human judgment of role-playing quality, which is particularly problematic for subjective aspects like emotional resonance and contextual immersion
- No analysis of model behavior on truly novel scenarios outside the training distribution

## Confidence

- **Mechanism 1 (Mindset Generation)**: Medium - The approach is plausible and novel, but lacks ablation studies showing the necessity of mindset segments for performance gains
- **Mechanism 2 (Hallucination Knowledge)**: Low - Only 5 out of 8 hallucination evaluation questions were answered correctly; the method's effectiveness is questionable given these results
- **Mechanism 3 (Data Expansion)**: Medium - While the approach addresses data scarcity, there's no evidence that the generated dialogues maintain character authenticity

## Next Checks

1. **Mindset Segment Utilization**: Perform attention visualization analysis to confirm the model actually attends to mindset segments during inference and that these attendances correlate with improved role consistency
2. **Hallucination Generalization Test**: Evaluate the model on a broader set of out-of-scope questions (beyond the 8 used in evaluation) to test whether the hallucination knowledge training generalizes
3. **Character Consistency Over Time**: Test the model across extended dialogues (multiple conversation turns) to verify that character consistency is maintained and doesn't degrade with conversation length