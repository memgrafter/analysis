---
ver: rpa2
title: Can Large Language Models Reason about the Region Connection Calculus?
arxiv_id: '2411.19589'
source_url: https://arxiv.org/abs/2411.19589
tags:
- relations
- ntpp
- deptn
- reasoning
- spatial
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: LLMs were evaluated on three spatial reasoning tasks using RCC-8.
  Experiments tested composition table reconstruction, alignment with human composition
  preferences, and conceptual neighborhood reconstruction, both with named and anonymous
  relations.
---

# Can Large Language Models Reason about the Region Connection Calculus?

## Quick Facts
- arXiv ID: 2411.19589
- Source URL: https://arxiv.org/abs/2411.19589
- Authors: Anthony G Cohn; Robert E Blackwell
- Reference count: 26
- LLMs tested on RCC-8 spatial reasoning tasks showed performance above random guessing but below perfect accuracy

## Executive Summary
This paper evaluates the ability of large language models to perform spatial reasoning using the Region Connection Calculus (RCC-8), a formal system for representing spatial relationships. The study tests three types of spatial reasoning tasks: composition table reconstruction, alignment with human composition preferences, and conceptual neighborhood reconstruction. Results show that while LLMs can perform basic spatial reasoning tasks better than random guessing, they struggle with complex composition tasks and show asymmetric behavior in inverse relation prediction. Performance degrades significantly when relation names are anonymized, suggesting reliance on memorized patterns rather than true spatial reasoning.

## Method Summary
The study evaluates multiple LLMs (GPT-3.5T, GPT-4T, GPT-4o, Claude-3.5S, Gemini-1.5P, Llama-3 70B) on three spatial reasoning tasks using RCC-8 relations. Each task was tested with both named and anonymous relations, with 30 repeats per prompt to measure variability. The experiments included composition table reconstruction (49 questions), alignment with human composition preferences from Ragni et al. (2007), and conceptual neighborhood reconstruction. Performance was measured using Jaccard Index, comparing model predictions to ground truth answers and human preferences.

## Key Results
- Claude-3.5S achieved the highest score of 0.69Â±0.008 on composition reconstruction
- All models performed better than random guessing but significantly worse than perfect accuracy
- Anonymous relations led to lower performance across all models, indicating reliance on relation name knowledge
- GPT-4o surprisingly underperformed compared to GPT-4T despite being a newer model

## Why This Works (Mechanism)

### Mechanism 1
LLMs can partially generalize spatial reasoning from training data but struggle with complex composition tasks. The models have been exposed to RCC-8 concepts during pretraining, enabling them to recognize relation names and basic spatial concepts. However, their performance degrades significantly when relations are anonymized, indicating reliance on memorized patterns rather than true reasoning.

### Mechanism 2
LLMs show asymmetric behavior in inverse relation prediction, preferring direct relations over inverse ones. The models demonstrate consistent bias against inverse relations (TPPi, NTPPi) in composition tasks, predicting TPP more frequently than TPPi and NTPP more frequently than NTPPi.

### Mechanism 3
LLMs show better performance on spatial continuity tasks than on composition tasks, suggesting different reasoning mechanisms. The models perform better on conceptual neighborhood reconstruction than on full composition table reconstruction, indicating that simpler spatial reasoning tasks are more amenable to LLM capabilities.

## Foundational Learning

- **Concept: Qualitative Spatial Reasoning (QSR) and Region Connection Calculus (RCC-8)**
  - Why needed here: Understanding the basic spatial relations (DC, EC, PO, TPP, NTPP, TPPi, NTPPi, EQ) and their meanings is essential for interpreting the experiment results
  - Quick check question: What are the eight RCC-8 relations and what does each one represent in terms of spatial relationships?

- **Concept: Relational composition and conceptual neighborhoods**
  - Why needed here: The experiments test two types of reasoning - determining all possible relations from composition and finding immediate spatial neighbors
  - Quick check question: How does relational composition differ from conceptual neighborhood reasoning in spatial calculi?

- **Concept: Jaccard Index for multi-label classification**
  - Why needed here: The accuracy metric uses Jaccard Index to handle cases where multiple relations are possible answers
  - Quick check question: How does the Jaccard Index handle cases where both predicted and ground truth contain multiple relations?

## Architecture Onboarding

- **Component map**: Prompt engineering layer -> LLM inference layer -> Evaluation layer -> Data management layer
- **Critical path**: 1) Generate prompts for each experiment, 2) Send to LLM APIs with standardized parameters, 3) Collect 30 responses per prompt, 4) Calculate Jaccard Index for each response, 5) Aggregate statistics and analyze patterns
- **Design tradeoffs**: Simple prompts vs. optimized prompts (chose simple for baseline), multiple repeats vs. single runs (chose 30 for variability), encrypted data vs. open access (chose encryption to prevent contamination)
- **Failure signatures**: Predicting invalid relations involving y, consistently missing inverse relations, performance drop with anonymous relations, asymmetric composition tables when symmetry is required
- **First 3 experiments**: 1) Run RCC-8 composition experiment with eponymous relations to establish baseline, 2) Repeat with anonymous relations to test reliance on memorized patterns, 3) Run spatial continuity experiment to compare reasoning capabilities across task types

## Open Questions the Paper Calls Out

### Open Question 1
How do different prompt engineering strategies (like chain-of-thought, k-shot, or tree-of-thought) affect LLM performance on spatial reasoning tasks? The paper used simple natural prompting to test baseline performance without optimization, leaving the impact of advanced prompting strategies unexplored.

### Open Question 2
Why does GPT-4o perform worse than GPT-4T on spatial reasoning despite being a newer model? The paper only observes this performance difference without investigating the underlying reasons, such as differences in alignment training or engineering changes.

### Open Question 3
How would multimodal LLMs with diagram-drawing capabilities perform on spatial reasoning tasks compared to text-only models? The paper only tested text-based LLMs and speculates about potential benefits of multimodal approaches without empirical evidence.

## Limitations
- Performance heavily depends on whether relation names are provided, suggesting models rely on memorized patterns rather than genuine spatial reasoning
- Closed nature of most evaluated models prevents verification of their actual training data
- Experiments only tested a specific subset of spatial reasoning tasks, leaving open the question of whether LLMs could perform better on other types of qualitative spatial reasoning problems

## Confidence
- **High Confidence**: LLMs perform better than random guessing on RCC-8 composition tasks but significantly worse than perfect accuracy
- **Medium Confidence**: LLMs rely on relation name knowledge rather than true spatial reasoning
- **Medium Confidence**: Asymmetric behavior in inverse relation prediction is consistently observed across models

## Next Checks
1. Run the same experiments with systematically varied prompt formats, temperatures, and top-p parameters to determine how much performance depends on prompt engineering versus model capabilities
2. Test the same models on other qualitative spatial reasoning frameworks (like Allen's interval algebra) to determine if the observed limitations are specific to RCC-8
3. Conduct controlled experiments comparing LLM performance to human subjects on the same RCC-8 tasks to establish whether the performance gap represents a fundamental limitation or is within the range of human variability