---
ver: rpa2
title: Mechanistic interpretability of large language models with applications to
  the financial services industry
arxiv_id: '2407.11215'
source_url: https://arxiv.org/abs/2407.11215
tags:
- attention
- financial
- language
- figure
- logit
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces mechanistic interpretability techniques to
  analyze large language models (LLMs) for financial services applications, focusing
  on compliance monitoring tasks. The authors investigate GPT-2 Small's internal mechanisms
  when prompted to identify potential violations of Fair Lending laws.
---

# Mechanistic interpretability of large language models with applications to the financial services industry

## Quick Facts
- arXiv ID: 2407.11215
- Source URL: https://arxiv.org/abs/2407.11215
- Reference count: 40
- Key outcome: The paper identifies specific attention heads in GPT-2 Small that contribute to Fair Lending compliance task completion using mechanistic interpretability techniques

## Executive Summary
This paper introduces mechanistic interpretability techniques to analyze large language models for financial services applications, specifically focusing on compliance monitoring tasks. The authors apply direct logit attribution and activation patching methods to investigate GPT-2 Small's internal mechanisms when identifying potential Fair Lending law violations. The study successfully identifies specific attention heads that contribute positively or negatively to task completion, providing insights into how LLMs process financial compliance questions.

## Method Summary
The authors use GPT-2 Small and apply two main mechanistic interpretability techniques: direct logit attribution to measure each layer's contribution to task completion, and activation patching to identify causal components through clean and corrupted prompt interventions. The methodology involves calculating logit differences across layers and attention heads, then performing residual stream and attention head patching to isolate important components. The analysis decomposes attention heads into their building blocks (key, query, value, and attention patterns) to understand which subcomponents drive behavior.

## Key Results
- Heads 10.2, 10.7, and 11.3 contribute positively to Fair Lending task completion
- Heads 9.6 and 10.6 contribute negatively to Fair Lending task completion
- Value vector patching has more significant effect than key/query patching for important heads in later layers
- The methodology successfully identifies task-specific attention heads for compliance monitoring

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Direct logit attribution reveals which attention heads contribute positively or negatively to Fair Lending task completion
- Mechanism: The technique calculates the logit difference for "Yes" vs "No" answers at each layer boundary, identifying specific heads that improve or degrade task performance
- Core assumption: Residual stream decomposition after each layer accurately captures each component's contribution to final output
- Evidence anchors: [abstract] "Using direct logit attribution, we study the contributions of each layer and its corresponding attention heads to the logit difference in the residual stream"

### Mechanism 2
- Claim: Activation patching identifies which components are necessary for correct Fair Lending violation detection
- Mechanism: Clean and corrupted prompts create controlled interventions where patching from clean to corrupted runs measures component importance
- Core assumption: Differences between clean and corrupted runs isolate the specific components responsible for task completion
- Evidence anchors: [abstract] "we design clean and corrupted prompts and use activation patching as a causal intervention method to localize our task completion components further"

### Mechanism 3
- Claim: Value vector patching has more significant effect than key/query patching for important heads in later layers
- Mechanism: Decomposed attention mechanism components (key, query, value, attention patterns) can be patched individually to identify which subcomponent drives behavior
- Core assumption: Attention head behavior can be understood through its constituent mathematical operations
- Evidence anchors: [section] "Instead of just patching on a head's output, we decompose the attention heads by patching into the building blocks"

## Foundational Learning

- Concept: Transformer attention mechanism
  - Why needed here: Understanding how heads read from and write to residual stream is essential for interpreting patching results
  - Quick check question: What mathematical operation combines query, key, and value vectors to produce attention weights?

- Concept: Residual stream decomposition
  - Why needed here: The core technique for attributing contributions requires understanding how information flows through layers
  - Quick check question: How does the residual stream at layer n differ from layer n-1 in terms of information content?

- Concept: Causal intervention methodology
  - Why needed here: Activation patching relies on controlled experiments to establish causal relationships between components and outputs
  - Quick check question: What distinguishes denoising from noising in activation patching experiments?

## Architecture Onboarding

- Component map: Token embedding → positional embedding → 12 transformer blocks (attention + MLP) → unembedding → softmax output
- Critical path: Token embedding → positional embedding → 12 transformer blocks (attention + MLP) → unembedding → softmax output
- Design tradeoffs: Small model size enables comprehensive mechanistic analysis but may limit generalizability to larger models
- Failure signatures: Inconsistent patching results across runs, heads showing both positive and negative contributions depending on context
- First 3 experiments:
  1. Run direct logit attribution on simple compliance task to verify heads 10.2, 10.7, 11.3 show positive contributions
  2. Design clean/corrupted prompt pairs for TCPA compliance task and verify patching localizes to expected token positions
  3. Perform value vs key/query patching on head 10.2 to confirm value vectors dominate its contribution

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can mechanistic interpretability techniques identify specific attention heads responsible for bias in financial compliance tasks?
- Basis in paper: [explicit] The paper identifies specific heads (10.2, 10.7, 11.3 as positive contributors and 9.6, 10.6 as negative contributors) for Fair Lending task completion
- Why unresolved: While the paper identifies these heads for Fair Lending violations, it's unclear if these same techniques can generalize to identify bias in other financial compliance domains
- What evidence would resolve it: Testing the same mechanistic interpretability pipeline on TCPA, UDAAP, and other financial compliance tasks to see if specific heads consistently emerge for different bias types

### Open Question 2
- Question: How do value vectors compare to key/query vectors in their importance for attention head contributions across different financial compliance tasks?
- Basis in paper: [explicit] The paper concludes that value patching has more significant effect than key/query patching for important heads in later layers (9-11)
- Why unresolved: The paper only tests this on Fair Lending tasks; it's unclear if this pattern holds across different types of financial compliance tasks
- What evidence would resolve it: Applying the same vector decomposition analysis to attention heads identified in TCPA, UDAAP, and other financial compliance tasks to compare the relative importance of value vs key/query vectors

### Open Question 3
- Question: Can the identified circuit for Fair Lending violations generalize to other transformer architectures beyond GPT-2 Small?
- Basis in paper: [inferred] The paper notes that universality/convergent learning is a hypothesis in mechanistic interpretability suggesting analogous features may form across models
- Why unresolved: The analysis is limited to GPT-2 Small; larger models like Mistral 7B or Llama-2 7B may have different circuit structures
- What evidence would resolve it: Applying the same mechanistic interpretability pipeline to larger open-source models on the same Fair Lending tasks to see if similar attention heads emerge

## Limitations
- Analysis constrained by small model size (GPT-2 Small) which may not capture complexity of modern financial LLMs
- Task specificity limits generalizability to other financial compliance domains
- Lack of direct validation that identified attention heads genuinely implement Fair Lending logic versus superficial pattern matching

## Confidence
- High confidence in the mechanistic interpretability methodology itself - the direct logit attribution and activation patching techniques are well-established in the literature
- Medium confidence in the specific head identifications for the Fair Lending task - while the methodology is sound, the task's complexity and model size limitations introduce uncertainty
- Low confidence in generalizability to production-scale financial LLMs - GPT-2 Small differs substantially from modern models in architecture, training data, and capabilities

## Next Checks
1. **Ablation validation**: Remove or suppress identified positive-contributing heads (10.2, 10.7, 11.3) and measure actual performance degradation on Fair Lending tasks across multiple examples
2. **Cross-domain testing**: Apply the same attribution methodology to a different financial compliance domain (e.g., TCPA or Reg BI) to verify if similar attention heads emerge as task-relevant
3. **Statistical robustness**: Run the attribution analysis across 50+ random prompt variations with confidence intervals to establish statistical significance of identified head contributions