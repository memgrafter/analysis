---
ver: rpa2
title: 'DialCLIP: Empowering CLIP as Multi-Modal Dialog Retriever'
arxiv_id: '2401.01076'
source_url: https://arxiv.org/abs/2401.01076
tags:
- context
- prompt
- multi-modal
- dialog
- clip
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes DialCLIP, a parameter-efficient prompt-tuning
  method for multi-modal dialog retrieval. The key innovation is introducing context
  prompts that capture dialog history, along with domain prompts to adapt CLIP to
  dialog data, and mixture-of-projection experts to handle different retrieval types.
---

# DialCLIP: Empowering CLIP as Multi-Modal Dialog Retriever

## Quick Facts
- arXiv ID: 2401.01076
- Source URL: https://arxiv.org/abs/2401.01076
- Reference count: 23
- Key outcome: Parameter-efficient prompt tuning achieves state-of-the-art performance on multi-modal dialog retrieval

## Executive Summary
DialCLIP introduces a parameter-efficient prompt-tuning approach that adapts CLIP for multi-modal dialog retrieval tasks. By introducing context prompts that capture dialog history, domain prompts to adapt CLIP to dialog data, and mixture-of-projection experts to handle different retrieval types, DialCLIP achieves significant performance improvements while tuning only 0.04% of parameters. The method demonstrates substantial gains on both PhotoChat and MMDialog datasets, improving R@1 by 12.8 points on MMDialog and the Sum metric by 17.8 points on PhotoChat.

## Method Summary
DialCLIP extends CLIP for dialog retrieval by adding three key components: context prompts generated from dialog history via a multi-modal context encoder, domain prompts that adapt CLIP to dialog-specific knowledge, and mixture-of-projection experts that handle different retrieval types. The context prompt generator processes multi-modal dialog context through a linear mapping network and distills features into prompts using pooling and feed-forward layers. Domain prompts are added to each layer of CLIP encoders using deep prompt tuning. For retrieval, multiple projection experts map CLIP outputs to a common representation space based on the specific retrieval type (text-to-image, image-to-text, or text-to-text).

## Key Results
- Achieves state-of-the-art performance on PhotoChat dataset with 17.8 points improvement in Sum metric
- Improves R@1 by 12.8 points on MMDialog dataset compared to baseline methods
- Demonstrates parameter efficiency by tuning only 0.04% of CLIP parameters while maintaining high performance

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Context prompts learned via multi-modal context encoder improve dialog retrieval by encoding dialog history into CLIP-compatible format
- Mechanism: Dialog context is passed through a multi-modal context encoder that bridges vision and language modalities, with output features distilled into prompts using a prompt generator
- Core assumption: Dialog context features can be effectively encoded and distilled into prompt tokens that CLIP can process without significant information loss
- Evidence anchors: [abstract] "our approach introduces a multi-modal context prompt generator to learn context features which are subsequently distilled into prompts"; [section] "We build context prompt generator (CPG), which consists of a multi-modal context encoder and a prompt generator"
- Break condition: If the multi-modal context encoder cannot effectively bridge the modality gap or if prompt distillation loses critical semantic information needed for dialog retrieval

### Mechanism 2
- Claim: Domain prompts mitigate the discrepancy between CLIP's pre-training data and dialog retrieval tasks by adapting CLIP to dialog-specific knowledge
- Mechanism: Fixed-length learnable domain prompt parameters are added to each layer of the CLIP encoders using deep prompt tuning approach
- Core assumption: Adding learnable parameters to CLIP layers can effectively transfer knowledge from non-dialog pre-training to dialog retrieval without full fine-tuning
- Evidence anchors: [abstract] "we introduce domain prompt to mitigate the discrepancy from the downstream dialog data"; [section] "we introduce a special set of parameters, called domain prompt"
- Break condition: If domain prompts overfit to training data or if they cannot effectively bridge the fundamental differences between image-text pairs and dialog contexts

### Mechanism 3
- Claim: Mixture of Projection (MoP) experts handle different retrieval types by learning specialized mappings from CLIP outputs
- Mechanism: Multiple experts are designed where each expert handles one specific retrieval type, with prompted CLIP encoders processing different combinations of modalities appropriately
- Core assumption: Different retrieval types require different projection mappings and a single projection cannot adequately handle all modality combinations in multi-modal dialog
- Evidence anchors: [abstract] "To facilitate various types of retrieval, we also design multiple experts to learn mappings from CLIP outputs to multi-modal representation space"; [section] "In the multi-modal dialog retrieval scenario, single projection is probably not sufficient"
- Break condition: If the number of retrieval types exceeds the number of experts or if experts cannot effectively learn distinct projections for their assigned retrieval types

## Foundational Learning

- Concept: Vision-Language Pre-training (VLP)
  - Why needed here: Understanding how models like CLIP learn joint representations from image-text pairs is crucial for knowing what knowledge is already captured and what needs adaptation for dialog
  - Quick check question: What is the primary objective function used in CLIP's pre-training that aligns vision and language representations?

- Concept: Prompt Tuning in Pre-trained Models
  - Why needed here: The entire DialCLIP approach relies on efficient parameter tuning through prompt addition rather than full fine-tuning, requiring understanding of how prompts modify model behavior
  - Quick check question: How does adding prompts at different layers of a transformer affect the model's attention patterns and output representations?

- Concept: Multi-modal Retrieval Tasks
  - Why needed here: The paper deals with various retrieval types (text-to-image, image-to-text, text-to-text) which require understanding of how similarity matching works across different modality combinations
  - Quick check question: What metric is typically used to evaluate retrieval performance when matching items of different modalities?

## Architecture Onboarding

- Component map: Multi-modal dialog context + query → Context Prompt Generator → CLIP Encoders (with domain prompts) → Mixture of Projection experts → Relevance scores

- Critical path:
  1. Multi-modal context C → Context Prompt Generator → Context prompts Pc
  2. Current input I + Pc + Domain prompts Pd → CLIP encoders → Intermediate representation
  3. Response Rm + Pd → CLIP encoders → Intermediate representation
  4. MoP experts map both representations to common space
  5. Dot-product similarity between mapped representations

- Design tradeoffs:
  - Parameter efficiency vs. performance: Using only 0.04% of parameters tuned vs. full fine-tuning
  - Prompt length vs. effectiveness: Longer prompts may capture more context but introduce noise
  - Number of MoP experts vs. complexity: More experts for more retrieval types increases complexity

- Failure signatures:
  - Poor performance on text-to-text retrieval: MoP expert for this type may be under-trained
  - Inconsistent results across different context prompt lengths: Context encoder may not be robust
  - Degradation when using longer domain prompts: Overfitting to dialog data

- First 3 experiments:
  1. Test different context prompt lengths (16, 32, 64, 96, 128) on PhotoChat to find optimal length
  2. Vary domain prompt length (2, 4, 8, 16, 64) to identify overfitting threshold
  3. Test different insertion layers for context prompts (2nd, 4th, 6th, 8th, 10th) to find optimal position

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of DialCLIP vary with different lengths of context prompts beyond the tested range (16-128)?
- Basis in paper: [inferred] The paper tested context prompt lengths from 16 to 128 and observed performance trends, but did not explore beyond this range
- Why unresolved: The paper only tested context prompt lengths up to 128, leaving uncertainty about performance with even longer prompts
- What evidence would resolve it: Conducting experiments with context prompt lengths greater than 128 would provide insights into the optimal length for performance

### Open Question 2
- Question: What is the impact of different domain prompt initialization methods on DialCLIP's performance?
- Basis in paper: [explicit] The paper compared three initialization approaches (Random, Pretrain, Task-related) and found pretraining to be most effective
- Why unresolved: While the paper tested three initialization methods, there may be other approaches that could further improve performance
- What evidence would resolve it: Experimenting with additional domain prompt initialization methods, such as using pre-trained embeddings or task-specific prompts, could reveal more effective strategies

### Open Question 3
- Question: How does the choice of prompt layer affect DialCLIP's performance across different datasets?
- Basis in paper: [explicit] The paper found that inserting context prompts into the 2nd layer yielded the best performance on PhotoChat, but did not explore this across other datasets
- Why unresolved: The paper only tested the prompt layer choice on PhotoChat, leaving uncertainty about its impact on other datasets
- What evidence would resolve it: Conducting experiments with different prompt layer choices on various datasets would provide insights into the optimal configuration for each dataset

## Limitations
- Evaluation confined to two specific datasets (PhotoChat and MMDialog), raising questions about generalizability to other dialog domains or languages
- Parameter efficiency comes with the implicit cost of potentially reduced robustness compared to full fine-tuning approaches
- Domain prompt mechanism assumes learnable parameters can effectively bridge the gap between CLIP's pre-training data and dialog retrieval tasks, but this assumption lacks strong empirical validation

## Confidence

**High Confidence**: The core retrieval architecture using prompt tuning with CLIP is well-established in the literature. The reported performance improvements on standard metrics (R@1, Sum) are specific and measurable, with clear baselines provided for comparison.

**Medium Confidence**: The effectiveness of context prompts and domain prompts relies on assumptions about how well prompt-based adaptation can capture dialog-specific knowledge. While the results are positive, the underlying mechanisms remain somewhat opaque and warrant deeper investigation.

**Low Confidence**: The mixture-of-projection experts' contribution is difficult to isolate from the overall performance gain. The paper doesn't provide ablation studies showing how much each component contributes individually, making it challenging to assess whether the complexity of multiple experts is justified.

## Next Checks

1. **Cross-domain robustness test**: Evaluate DialCLIP on dialog datasets from different domains (customer service, healthcare, social media) and languages to assess generalization beyond PhotoChat and MMDialog.

2. **Ablation study on component contributions**: Systematically remove or replace each major component (context prompts, domain prompts, MoP experts) with simpler alternatives to quantify their individual contributions to overall performance.

3. **Prompt interpretability analysis**: Apply attention visualization and probing techniques to analyze what the learned context and domain prompts are actually capturing, examining prompt embeddings for meaningful dialog patterns and checking for overfitting.