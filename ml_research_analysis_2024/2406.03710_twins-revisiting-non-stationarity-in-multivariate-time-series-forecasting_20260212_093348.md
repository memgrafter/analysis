---
ver: rpa2
title: 'TwinS: Revisiting Non-Stationarity in Multivariate Time Series Forecasting'
arxiv_id: '2406.03710'
source_url: https://arxiv.org/abs/2406.03710
tags:
- time
- series
- attention
- periodic
- wavelet
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: TwinS addresses the challenge of non-stationary periodic distributions
  in multivariate time series forecasting by proposing a Transformer-based model with
  three key modules. The Wavelet Convolution module captures nested periods by scaling
  convolution kernels like wavelet transform.
---

# TwinS: Revisiting Non-Stationarity in Multivariate Time Series Forecasting

## Quick Facts
- arXiv ID: 2406.03710
- Source URL: https://arxiv.org/abs/2406.03710
- Authors: Jiaxi Hu; Qingsong Wen; Sijie Ruan; Li Liu; Yuxuan Liang
- Reference count: 20
- One-line primary result: TwinS achieves state-of-the-art performance in multivariate time series forecasting with a maximum MSE improvement of 25.8% over PatchTST

## Executive Summary
TwinS addresses the challenge of non-stationary periodic distributions in multivariate time series forecasting through a Transformer-based architecture with three key modules. The model employs Wavelet Convolution to capture nested periods, Period-Aware Attention to guide attention computation based on period relevance scores, and Channel-Temporal Mixed MLP to capture overall relationships between time series. TwinS demonstrates superior performance compared to eight mainstream TS models across eight popular datasets, showing consistent improvements in both MSE and MAE metrics.

## Method Summary
TwinS is a Transformer-based model designed to handle non-stationary periodic distributions in multivariate time series forecasting. The architecture consists of three key modules: Wavelet Convolution for multi-period embedding using scaled convolutional kernels, Period-Aware Attention for modeling non-stationary periodic distributions through a convolutional sub-network that generates period relevance scores, and Channel-Temporal Mixer MLP for capturing relationships between time series through joint learning of channel and temporal dimensions. The model is trained using Adam optimizer with hyperparameter tuning across learning rates, patch lengths, hidden dimensions, and attention heads.

## Key Results
- Achieves state-of-the-art performance compared to eight mainstream TS models
- Maximum MSE improvement of 25.8% over PatchTST
- Consistently outperforms baselines across eight popular datasets
- Demonstrates efficacy of each module in addressing non-stationary periodic distributions

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Wavelet Convolution scales kernel sizes exponentially (powers of 2) to emulate wavelet transform, enabling nested period detection.
- Mechanism: Each convolutional kernel is expanded in size by a factor of 2^(scale), then centered on shared parameters, creating multi-scale receptive fields that mimic wavelet decomposition.
- Core assumption: Shared parameters across scales preserve relative relationships while enabling multi-scale period extraction.
- Evidence anchors:
  - [abstract]: "Wavelet Convolution models nested periods by scaling the convolution kernel size like wavelet transform."
  - [section]: "We apply scaling transformations to the kernel size of the convolutional kernel to correspond to the scaling transformations of wavelet basis functions."
  - [corpus]: Weak. Corpus neighbors discuss wavelet-guided attention but none explicitly validate kernel scaling for nested period extraction.
- Break condition: If shared parameters across scales lose discriminative power or if period nesting is not truly exponential in frequency domain.

### Mechanism 2
- Claim: Period-Aware Attention uses a depthwise separable convolution sub-network to generate period relevance scores, guiding attention toward non-stationary periodic patterns.
- Mechanism: A 1D depthwise separable convolution scans the time dimension to detect missing period patterns; scores are sigmoid-activated and used to weight attention values directly.
- Core assumption: Periodicity gaps manifest as spatial patterns in the embedding space detectable by local convolution filters.
- Evidence anchors:
  - [abstract]: "The Period-Aware Attention guides attention computation by generating period relevance scores through a convolutional sub-network."
  - [section]: "we employ a Convolution sub-network to aware periodicity absence with their translation invariance, thereby guiding the information allocation in attention computation."
  - [corpus]: Weak. Neighbors mention wavelet-guided attention but no evidence on period relevance scoring via depthwise separable convolution.
- Break condition: If the missing period patterns are not spatially local or if the convolutional sub-network fails to generalize across datasets.

### Mechanism 3
- Claim: Channel-Temporal Mixer MLP jointly models variable relationships without isolating channel and temporal dimensions, preserving hysteresis.
- Mechanism: The feature map is reshaped into a joint space and processed by a single MLP; this avoids decoupled channel/time modeling that could lose cross-series dependencies.
- Core assumption: Variable relationships are best captured when temporal and channel dimensions are not treated separately.
- Evidence anchors:
  - [abstract]: "The Channel-Temporal Mixed MLP captures the overall relationships between time series through channel-time mixing learning."
  - [section]: "we adopt a joint learning approach instead of isolated modeling channels and time dependencies and employ a straightforward MLP to accomplish this."
  - [corpus]: Weak. Corpus neighbors propose joint models but none explicitly validate hysteresis preservation via single MLP.
- Break condition: If cross-series dependencies are not consistent across time or if the MLP cannot scale with number of variables.

## Foundational Learning

- Concept: Wavelet Transform
  - Why needed here: Provides theoretical foundation for multi-scale period decomposition in time series.
  - Quick check question: How does scaling the wavelet basis function affect the frequency resolution of the transform?
- Concept: Multi-Head Attention
  - Why needed here: Core mechanism for capturing long-range dependencies in Transformer models.
  - Quick check question: Why does splitting queries, keys, and values into multiple heads improve attention?
- Concept: Depthwise Separable Convolution
  - Why needed here: Enables lightweight period detection sub-network without exploding parameter count.
  - Quick check question: How does depthwise separable convolution reduce computation compared to standard convolution?

## Architecture Onboarding

- Component map: Input -> Wavelet Convolution -> Reversible Window Patching -> Period-Aware Attention -> CT-MLP -> Output
- Critical path: Input → Wavelet Convolution → Reversible Window Patching → Period-Aware Attention → CT-MLP → Output
- Design tradeoffs:
  - Wavelet Convolution vs FFT-image: Wavelet is more efficient but less interpretable than FFT-based methods.
  - Period-Aware Attention vs standard attention: More complex but better handles non-stationary periods.
  - CT-MLP vs separate channel/time models: Simpler but may lose fine-grained temporal dynamics.
- Failure signatures:
  - Poor performance on stationary datasets: Wavelet module may be overkill.
  - High variance across runs: Reversible window patching may fragment key temporal patterns.
  - Memory overflow on high-variable datasets: CT-MLP hidden dim too large.
- First 3 experiments:
  1. Replace Wavelet Convolution with standard convolution and measure MSE drop.
  2. Remove Period-Aware Attention and revert to standard MHSA; compare convergence curves.
  3. Increase CT-MLP hidden dim from 128 to 256; measure overfitting on traffic dataset.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does TwinS handle cases where the dominant periodic patterns in a time series change over time (e.g., a sudden shift in seasonality)?
- Basis in paper: [explicit] The paper mentions TwinS uses wavelet convolution to capture nested periods at different scales, but doesn't discuss how it adapts to changing dominant periods over time.
- Why unresolved: While TwinS can capture multiple periodic patterns, it's unclear if the model can dynamically adjust its focus as the dominant patterns shift throughout the time series.
- What evidence would resolve it: Experiments comparing TwinS performance on time series with static vs. dynamically changing periodic patterns. Analysis of how TwinS's attention weights shift over time in response to changing periodicities.

### Open Question 2
- Question: What is the computational overhead of TwinS compared to simpler Transformer-based models like PatchTST, and how does this impact its scalability to very long time series?
- Basis in paper: [inferred] The paper claims TwinS has similar or lower complexity than PatchTST in some cases (Equations 14-15), but doesn't provide comprehensive computational benchmarks or discuss scalability to very long sequences.
- Why unresolved: While theoretical complexity is analyzed, practical runtime and memory usage across different dataset sizes and sequence lengths is not reported.
- What evidence would resolve it: Detailed runtime and memory profiling of TwinS vs. baselines on datasets of increasing size and sequence length. Analysis of how TwinS's performance scales with input length.

### Open Question 3
- Question: How sensitive is TwinS's performance to the choice of wavelet basis function, and are there scenarios where a different wavelet (e.g., Morlet vs. Haar) would be more appropriate?
- Basis in paper: [explicit] The paper mentions using wavelet transform methodology but doesn't specify which wavelet basis function is used or discuss the impact of this choice on performance.
- Why unresolved: The choice of wavelet basis can significantly impact the quality of frequency and time-scale decomposition, yet this critical hyperparameter is not explored.
- What evidence would resolve it: Experiments comparing TwinS performance using different wavelet basis functions on datasets with varying periodic characteristics. Analysis of how the choice of wavelet affects TwinS's ability to capture different types of periodic patterns.

## Limitations

- Core mechanisms lack rigorous empirical validation, particularly the Wavelet Convolution's kernel scaling and Period-Aware Attention's depthwise separable convolution approach
- Computational overhead and scalability to very long time series is not comprehensively benchmarked
- Performance sensitivity to wavelet basis function choice remains unexplored

## Confidence

- Wavelet Convolution mechanism: **Medium** - Theoretical justification exists but limited empirical validation
- Period-Aware Attention: **Low** - Core mechanism underspecified and not rigorously tested
- CT-MLP effectiveness: **Medium** - Ablation shows importance but no comparative analysis with alternatives
- Overall performance claims: **High** - Extensive benchmarking against 8 mainstream models with clear metrics

## Next Checks

1. Implement a variant of TwinS replacing Wavelet Convolution with standard convolution while keeping other components unchanged, then measure performance degradation specifically on datasets with strong non-stationary periodic patterns
2. Replace Period-Aware Attention with standard Multi-Head Self-Attention and compare both MSE performance and attention visualization patterns to verify the claimed period-awareness benefit
3. Create an ablation study comparing CT-MLP against two separate MLPs for channel and temporal processing respectively, measuring both performance and parameter efficiency tradeoffs