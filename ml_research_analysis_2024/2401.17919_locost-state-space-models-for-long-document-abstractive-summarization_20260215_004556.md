---
ver: rpa2
title: 'LOCOST: State-Space Models for Long Document Abstractive Summarization'
arxiv_id: '2401.17919'
source_url: https://arxiv.org/abs/2401.17919
tags:
- long
- locost
- input
- summarization
- layer
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces LOCOST, an encoder-decoder architecture based
  on state-space models for conditional text generation with long context inputs.
  By replacing self-attention with state-space models, LOCOST achieves O(L log L)
  computational complexity, enabling efficient processing of extremely long sequences.
---

# LOCOST: State-Space Models for Long Document Abstractive Summarization

## Quick Facts
- arXiv ID: 2401.17919
- Source URL: https://arxiv.org/abs/2401.17919
- Reference count: 22
- Primary result: LOCOST achieves 93-96% of sparse transformer performance on long document summarization while using 50% less training memory and 87% less inference memory

## Executive Summary
This paper introduces LOCOST, an encoder-decoder architecture based on state-space models (SSMs) for long document abstractive summarization. By replacing self-attention with SSM convolutions, LOCOST achieves O(L log L) computational complexity, enabling efficient processing of extremely long sequences. The model reaches near-state-of-the-art performance on multiple summarization benchmarks while using significantly less memory. Notably, LOCOST can process input texts exceeding 600K tokens, setting new state-of-the-art results on full-book summarization tasks.

## Method Summary
LOCOST uses an encoder-decoder architecture where the encoder consists of stacked LOCOST layers (bidirectional SSM + gated feedforward) and the decoder uses standard transformer layers with dense self-attention and cross-attention. The model is pre-trained using gap-sentence generation (GSG) on the C4 dataset, where key sentences are masked and the model learns to reconstruct them. For fine-tuning, the pre-trained model is trained on various summarization datasets with input lengths ranging from 4K to 32K tokens and output lengths of 512 tokens.

## Key Results
- LOCOST achieves 93-96% of sparse transformer performance on long document summarization tasks
- Memory usage reduced by 50% during training and 87% during inference compared to baselines
- Sets new state-of-the-art results on BookSum-Book summarization with inputs exceeding 600K tokens
- Processes sequences up to 600K tokens, demonstrating effective long-range context modeling

## Why This Works (Mechanism)

### Mechanism 1
- Bidirectional SSM convolutions enable effective local and global context modeling without self-attention
- Uses forward and backward state-space kernels to perform causal and anti-causal convolutions, capturing dependencies in both directions
- Assumes learned kernels can model both short-range and long-range dependencies through their spectral radii

### Mechanism 2
- Gated bidirectional SSM layers effectively replace transformer self-attention layers
- Each layer applies a gated bidirectional SSM followed by a gated feedforward network with selective information integration
- Assumes gating mechanisms can control information flow as effectively as multi-head attention

### Mechanism 3
- Pre-training with gap-sentence generation (GSG) enables effective zero-shot summarization capabilities
- Trains model to identify and reconstruct key sentences that maximize ROUGE overlap with remaining text
- Assumes sentences with highest ROUGE overlap are most informative for summarization

## Foundational Learning

- Concept: State-space models and their connection to convolutions
  - Why needed here: Understanding how SSMs can be viewed as convolutions is crucial for grasping why they can replace attention mechanisms
  - Quick check question: How does the convolution theorem allow SSM computations to be performed in O(L log L) time?

- Concept: Spectral radii and their role in context modeling
  - Why needed here: The spectral radii of state matrices determine how far in the past/future a token can influence the output
  - Quick check question: What relationship exists between spectral radius magnitude and the range of context a kernel can capture?

- Concept: Bidirectional processing in sequence models
  - Why needed here: The model needs to aggregate information from both directions without the quadratic complexity of full bidirectional attention
  - Quick check question: How does combining forward and backward convolutions achieve bidirectional context aggregation?

## Architecture Onboarding

- Component map: Input → Embedding → Encoder (LOCOST layers) → Cross-attention with decoder → Generation
- Critical path: Input → Embedding → Encoder (LOCOST layers) → Cross-attention with decoder → Generation
- Design tradeoffs:
  - SSM in encoder reduces complexity but requires standard attention in decoder
  - Smaller model size (250M params) vs larger transformers (500M+ params)
  - Simpler architecture vs more complex sparse-attention patterns
- Failure signatures:
  - Poor performance on very long sequences may indicate kernel initialization issues
  - Memory inefficiency despite O(L log L) complexity could suggest implementation problems
  - Training instability might indicate improper learning rate or gradient clipping needs
- First 3 experiments:
  1. Compare LOCOST layer output distribution with transformer layer output distribution on fixed input
  2. Visualize learned kernel weights across layers to verify they span different context ranges
  3. Test model performance on increasing sequence lengths to verify extrapolation capability

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of LOCOST scale with increasing input length beyond 600K tokens, and what are the theoretical limits of its state-space model architecture for handling extremely long sequences?
- Basis in paper: The paper demonstrates LOCOST can process up to 600K tokens and sets new state-of-the-art results on full-book summarization, but doesn't explore upper limits or analyze performance degradation at extreme lengths.
- Why unresolved: The paper focuses on demonstrating competitive performance and setting new benchmarks, but doesn't investigate asymptotic behavior or theoretical constraints of the state-space model for ultra-long sequences.
- What evidence would resolve it: Experimental results showing performance on input lengths progressively increasing beyond 600K tokens, analysis of computational complexity and memory usage at these extremes, and theoretical analysis of the state-space model's capacity to capture long-range dependencies in ultra-long sequences.

### Open Question 2
- Question: How would LOCOST perform on other long-input abstractive tasks beyond summarization, such as long-form question answering, dialogue systems, or document-level machine translation?
- Basis in paper: The paper states "Although replacing self-attention with state-space encoders drastically reduces the computational complexity, the use of dense cross-attention in the decoder still limits the output sequence length in terms of computation during training."
- Why unresolved: The paper focuses specifically on long document abstractive summarization and doesn't explore LOCOST's applicability or performance on other types of long-input generation tasks that may have different characteristics or requirements.
- What evidence would resolve it: Experimental results evaluating LOCOST on various long-input generation tasks beyond summarization, including tasks with different output length requirements and structural characteristics, to assess its versatility and identify any task-specific limitations.

### Open Question 3
- Question: What is the impact of different state-space model configurations (e.g., varying the state dimension N, number of layers, or initialization schemes) on LOCOST's performance and efficiency, and how sensitive is the model to these hyperparameters?
- Basis in paper: The paper mentions following specific parametrization and initialization schemes from previous work but doesn't provide a comprehensive ablation study or sensitivity analysis of these choices.
- Why unresolved: While the paper presents a working model with specific hyperparameters, it doesn't explore the design space of state-space model configurations or analyze how different choices affect the trade-off between performance and efficiency.
- What evidence would resolve it: Systematic experiments varying key state-space model hyperparameters and analyzing their impact on performance metrics, computational complexity, and memory usage to identify optimal configurations and understand the model's sensitivity to these choices.

## Limitations
- No direct empirical comparison between SSM-based attention and traditional self-attention mechanisms
- Effectiveness of GSG pre-training assumed based on prior work rather than independently validated for this architecture
- Model's ability to generalize beyond tested sequence lengths not systematically analyzed

## Confidence

**High Confidence Claims:**
- LOCOST achieves O(L log L) computational complexity versus O(L²) for traditional transformers
- Memory usage improvements (50% training, 87% inference) are well-documented through controlled experiments
- Performance on BookSum-Book dataset is state-of-the-art, as verified through ROUGE metrics

**Medium Confidence Claims:**
- The 93-96% performance relative to sparse transformers holds across all tested datasets
- The effectiveness of bidirectional SSM convolutions for capturing both local and global context

**Low Confidence Claims:**
- The exact mechanisms by which learned kernel weights capture long-range dependencies
- Whether GSG pre-training is optimal for this architecture versus other self-supervised objectives

## Next Checks

1. **Ablation Study**: Remove the SSM encoder and replace it with a standard transformer encoder while keeping all other components identical. Compare performance across sequence lengths to isolate the contribution of the SSM architecture.

2. **Kernel Analysis**: Quantitatively analyze the learned kernel weights across layers and attention heads. Measure their spectral radii distribution and correlate these with the model's ability to capture long-range dependencies at different sequence lengths.

3. **Scalability Testing**: Systematically evaluate model performance as a function of input length, testing at 10K, 50K, 100K, 250K, 500K, and 600K tokens. Identify the point at which performance begins to degrade and determine whether this follows the expected O(L log L) scaling.