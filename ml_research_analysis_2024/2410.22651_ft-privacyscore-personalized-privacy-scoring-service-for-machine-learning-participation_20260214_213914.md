---
ver: rpa2
title: 'FT-PrivacyScore: Personalized Privacy Scoring Service for Machine Learning
  Participation'
arxiv_id: '2410.22651'
source_url: https://arxiv.org/abs/2410.22651
tags:
- privacy
- data
- lira
- fine-tuning
- scoring
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents FT-PrivacyScore, a privacy scoring service
  that estimates the risk of participating in model fine-tuning tasks. The core method
  employs a batch-based offline Likelihood-ratio Attack (LiRA) that trains multiple
  fine-tuned models with randomly sampled subsets of contributor data, then uses hypothesis
  testing to calculate privacy scores.
---

# FT-PrivacyScore: Personalized Privacy Scoring Service for Machine Learning Participation

## Quick Facts
- arXiv ID: 2410.22651
- Source URL: https://arxiv.org/abs/2410.22651
- Authors: Yuechun Gu; Jiajie He; Keke Chen
- Reference count: 7
- Primary result: Reduces LiRA computational cost from 6.5 hours to 3 minutes per instance while maintaining privacy score accuracy

## Executive Summary
FT-PrivacyScore introduces a personalized privacy scoring service that estimates privacy risks for data contributors participating in model fine-tuning tasks. The system uses a batch-based offline Likelihood-ratio Attack (LiRA) approach that trains multiple fine-tuned models with randomly sampled subsets of contributor data, then applies hypothesis testing to calculate privacy scores. This method significantly improves computational efficiency compared to traditional per-sample LiRA while maintaining comparable accuracy. The service enables contributors to understand their privacy risks before participation and helps model builders make informed decisions about incorporating specific data sources.

## Method Summary
FT-PrivacyScore implements a batch-based offline LiRA testing approach specifically designed for model fine-tuning tasks. Instead of training n models per sample (traditional LiRA), the method trains n models using m/2 samples each, spreading the training cost across all m samples. For each sample, approximately n/2 models contain it in training data, creating a statistical distribution for membership inference testing. The privacy score is calculated as |2∑I(LiRA(t_j, M_i) == G(t_j, M_i))/n - 1|, where higher scores indicate greater privacy risk. The system focuses on fine-tuning tasks to leverage pre-trained base models and reduce computational burden compared to full model training.

## Key Results
- Reduces per-instance computational cost from 6.5 hours to 3 minutes
- Maintains comparable privacy score quality to traditional LiRA methods
- Demonstrates practical deployment potential for large-scale fine-tuning scenarios

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Batch-based offline LiRA testing reduces per-instance computational cost while maintaining accuracy
- Mechanism: Instead of training n models per sample, trains n models using m/2 samples each, spreading training cost across all m samples
- Core assumption: Statistical power of membership inference testing is preserved with randomly partitioned training sets
- Evidence anchors: Traditional per-sample LiRA takes 6.5 hours per instance vs. 3 minutes with batch approach; n models each take m/2 samples

### Mechanism 2
- Claim: Fine-tuning focus enables practical deployment on large models while preserving efficiency
- Mechanism: Leverages pre-trained base models and applies lightweight fine-tuning procedures
- Core assumption: Fine-tuning captures sufficient model behavior changes for membership inference
- Evidence anchors: Method specifically designed for fine-tuning tasks; more practical for large models than full training

### Mechanism 3
- Claim: Privacy score calculation using |2ĉcorrect/n - 1| provides interpretable risk quantification
- Mechanism: Measures how well LiRA predictions match ground truth membership status, normalized to [-1, 1] range
- Core assumption: Binary membership prediction accuracy correlates with actual privacy risk
- Evidence anchors: Score of 0 indicates random guessing, ±1 indicates perfect prediction; worst-case scenario yields score of 0

## Foundational Learning

- **Concept: Membership Inference Attacks (MIA)**
  - Why needed here: FT-PrivacyScore fundamentally relies on membership inference to determine whether a data point was used in model training
  - Quick check question: How does membership inference differ from other privacy attacks like model inversion or attribute inference?

- **Concept: Likelihood-ratio Test (LiRA)**
  - Why needed here: LiRA provides the statistical framework for determining membership by comparing output distributions of models trained with vs without target samples
  - Quick check question: What distinguishes online LiRA from offline LiRA in terms of computational requirements and accuracy trade-offs?

- **Concept: Differential Privacy Fundamentals**
  - Why needed here: Understanding why traditional differential privacy methods damage model utility helps contextualize FT-PrivacyScore's approach of quantifying risk without adding noise
  - Quick check question: How does the ε parameter in differential privacy relate to the privacy scores produced by FT-PrivacyScore?

## Architecture Onboarding

- **Component map**: Contributors → Sample submission → Model builder → Base model + strategy → FT-PrivacyScore → Privacy scores → Contributors
- **Critical path**: Sample submission → Batch collection → n model fine-tuning → LiRA testing per model → Privacy score calculation → Score delivery
- **Design tradeoffs**: Computational efficiency vs. privacy score accuracy; batch size vs. timeliness; model fine-tuning vs. full training capabilities
- **Failure signatures**: High variance in privacy scores across similar samples (indicates insufficient statistical power); consistently low scores (indicates LiRA ineffectiveness); system timeouts during fine-tuning (indicates resource constraints)
- **First 3 experiments**:
  1. Validate baseline: Run traditional per-sample LiRA on small dataset (n=10 samples) and compare with batch approach accuracy
  2. Resource scaling: Test how privacy score quality degrades as m decreases from 100 to 10 samples
  3. Model dependency: Compare privacy scores across different base model architectures (ResNet vs. EfficientNet) to identify sensitivity

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the privacy scoring system perform when applied to real-world datasets with more complex distributions than CIFAR-10, particularly in domains with high class imbalance or multimodal distributions?
- Basis in paper: Paper only validates on CIFAR-10 dataset and mentions need to test on more diverse datasets
- Why unresolved: Current evaluation limited to single dataset with balanced classes; real-world applications may involve skewed distributions
- What evidence would resolve it: Empirical results showing consistent performance across multiple diverse datasets including those with class imbalance and multimodal distributions

### Open Question 2
- Question: What is the optimal batch size that balances computational efficiency with privacy score accuracy, and how does this trade-off vary across different model architectures and fine-tuning strategies?
- Basis in paper: Paper mentions processing batches of records together but doesn't explore optimal batch size or accuracy effects
- Why unresolved: Demonstrates efficiency gains but doesn't systematically investigate batch size effects on score quality
- What evidence would resolve it: Systematic experiments varying batch sizes and measuring both computational efficiency and accuracy across different architectures

### Open Question 3
- Question: How do privacy scores change over time as the fine-tuned model continues to be updated with new data, and what is the temporal stability of these scores?
- Basis in paper: Paper considers continuous fine-tuning scenario but doesn't address score evolution over time
- Why unresolved: Focuses on single evaluation point while real-world involves ongoing model updates
- What evidence would resolve it: Longitudinal studies tracking privacy scores across multiple rounds of model updates

## Limitations
- Limited evaluation scope: Only tested on CIFAR-10 dataset with balanced classes
- Unknown fine-tuning strategy details: Specific implementation and hyperparameters not fully specified
- Statistical reliability thresholds: Doesn't establish clear minimum sample sizes for reliable scoring

## Confidence

- **High Confidence**: Core mechanism of batch-based offline LiRA reducing computational costs from 6.5 hours to 3 minutes per instance
- **Medium Confidence**: Claim that privacy score quality is "comparable" to traditional methods without quantitative comparison metrics
- **Low Confidence**: Generalizability across different fine-tuning tasks and model architectures given limited experimental scope

## Next Checks

1. **Statistical Power Validation**: Systematically test privacy score accuracy as the number of test samples (m) decreases from 100 to 10 to identify minimum threshold for reliable scoring

2. **Cross-Architecture Comparison**: Evaluate the privacy scoring service across at least three different base model architectures to assess sensitivity to architectural differences

3. **Fine-tuning Strategy Impact**: Test the service with varying fine-tuning strategies (different learning rates, epochs, data augmentation) to determine sensitivity to training hyperparameters