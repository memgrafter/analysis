---
ver: rpa2
title: 'Bias in the Mirror: Are LLMs opinions robust to their own adversarial attacks
  ?'
arxiv_id: '2410.13517'
source_url: https://arxiv.org/abs/2410.13517
tags:
- debates
- biases
- bias
- biased
- shifts
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates the robustness of biases in large language
  models (LLMs) by exposing them to self-debates and multilingual prompts. Two instances
  of the same model argue opposing viewpoints to persuade a neutral version, assessing
  how biases shift under structured discourse.
---

# Bias in the Mirror: Are LLMs opinions robust to their own adversarial attacks ?

## Quick Facts
- arXiv ID: 2410.13517
- Source URL: https://arxiv.org/abs/2410.13517
- Reference count: 9
- Two instances of the same model argue opposing viewpoints to persuade a neutral version, assessing how biases shift under structured discourse

## Executive Summary
This paper investigates the robustness of biases in large language models (LLMs) by exposing them to self-debates and multilingual prompts. Two instances of the same model argue opposing viewpoints to persuade a neutral version, assessing how biases shift under structured discourse. Experiments across 14 diverse models, including GPT-4, Llama, Mistral, JAIS, and Qwen in multiple languages, show that models like Qwen exhibit minimal bias shifts, while Mistral and Llama are more flexible. Multilingual evaluations reveal greater bias shifts in Chinese than Arabic prompts. Human comparisons show stronger human bias resilience but greater susceptibility to misinformation. Overall, the study highlights that model biases are not fixed and vary across linguistic and cultural contexts, emphasizing the need for nuanced, multilingual bias evaluations.

## Method Summary
The study uses a self-debate framework where two instances of the same LLM argue opposing viewpoints, with a neutral instance observing and re-evaluating its stance post-debate. Bias is measured using Political Compass test questions scored on a -10 to 10 scale. Experiments span 14 models and include multilingual evaluations in Arabic and Chinese, as well as human comparisons across 16 questions in 8 topics. Bias shifts are tracked before and after debates, and variations in debate fairness are tested.

## Key Results
- Qwen exhibits minimal bias shifts, while Mistral and Llama are more flexible to debate persuasion.
- Multilingual evaluations reveal greater bias shifts in Chinese than Arabic prompts.
- Human biases are more resilient overall but more susceptible to misinformation influence compared to models.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Model biases can be dynamically shifted through structured debate without additional training or fine-tuning.
- Mechanism: Two instances of the same model argue opposing viewpoints, with a neutral instance observing. The neutral instance then re-evaluates its stance post-debate, revealing bias shifts.
- Core assumption: Models retain internal coherence and can be "convinced" by their own arguments.
- Evidence anchors:
  - [abstract] "two instances of an LLM engage in self-debate, arguing opposing viewpoints to persuade a neutral version of the model"
  - [section 3.3.1] "We assess the robustness of both initial biases and post-contradiction biases across different languages"
  - [corpus] Weak: related work focuses on external bias detection but not self-debate, so internal mechanism not directly tested elsewhere.
- Break condition: If the model's internal representations do not change during debate or if the model outputs identical responses pre- and post-debate.

### Mechanism 2
- Claim: Bias expression and resilience vary significantly across linguistic and cultural contexts.
- Mechanism: The same model, when prompted in different languages, shows different degrees of bias, indicating that training data cultural context influences bias manifestation.
- Core assumption: Multilingual models embed cultural biases from their training corpora and respond differently depending on the language of interaction.
- Evidence anchors:
  - [abstract] "Multilingual evaluations reveal greater bias shifts in Chinese than Arabic prompts"
  - [section 3.3.2] "models exhibit different biases in their secondary languages, such as Arabic and Chinese"
  - [corpus] Moderate: some related work discusses multilingual bias, but self-debate in multiple languages is unique here.
- Break condition: If cross-linguistic bias differences are negligible or if prompting language has no measurable effect on bias shifts.

### Mechanism 3
- Claim: Human bias resilience is stronger than model bias resilience, but humans are more susceptible to misinformation influence.
- Mechanism: Human evaluators' opinions shift less in response to debates compared to models, but they shift more on misinformation topics.
- Core assumption: Humans and models process persuasive arguments differently, with humans showing more consistent core biases but greater variability on misinformation.
- Evidence anchors:
  - [section 4.4] "human biases tend to remain stronger on most topics, indicating they are less easily influenced than models"
  - [section 4.4] "humans are more persuadable on topics related to misinformation and nonsensical questions"
  - [corpus] Weak: few direct comparisons of human vs. model debate resilience exist in the corpus.
- Break condition: If human opinion shifts are similar in magnitude to model shifts or if human shifts are consistently larger.

## Foundational Learning

- Concept: Debate-based evaluation of bias
  - Why needed here: The study relies on structured debates to measure bias shifts rather than static questionnaires, requiring understanding of debate design and argument structure.
  - Quick check question: What are the four turns in the debate structure and how do they facilitate bias measurement?
- Concept: Multilingual model evaluation
  - Why needed here: The experiments span multiple languages to capture cross-cultural bias differences, requiring awareness of how language and cultural context influence model outputs.
  - Quick check question: How does prompting in Arabic vs. Chinese affect bias shifts according to the results?
- Concept: Human vs. model comparison methodology
  - Why needed here: The study compares human and model responses to validate findings and contextualize model biases relative to human biases.
  - Quick check question: What are the eight topic categories used for human evaluation and why were they chosen?

## Architecture Onboarding

- Component map: Prompt generation engine -> Debate orchestration module -> Neutral instance interface -> Multilingual adapter -> Human evaluation interface
- Critical path:
  1. Generate initial bias question.
  2. Instantiate two model versions for debate.
  3. Run structured debate.
  4. Capture neutral model's pre/post-debate stance.
  5. Repeat for all languages and questions.
  6. Compare human vs. model shifts.
- Design tradeoffs:
  - Tradeoff between debate depth (more turns) and computational cost.
  - Balancing language diversity vs. depth of cultural context coverage.
  - Deciding whether to use human debates for all topics or a subset.
- Failure signatures:
  - No change in model stance pre/post-debate indicates strong bias entrenchment.
  - High variance in responses suggests instability or randomness rather than bias.
  - Human and model shifts tracking too closely may indicate methodological issues.
- First 3 experiments:
  1. Run a single debate cycle on a simple political question in English to validate setup.
  2. Test multilingual bias shift on one question in Arabic and Chinese to confirm language effect.
  3. Compare human vs. model bias shift on a misinformation question to establish baseline differences.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do specific linguistic features of Arabic and Chinese affect the expression and persistence of biases in LLMs compared to English?
- Basis in paper: Explicit
- Why unresolved: The paper notes differences in bias expression across languages but does not analyze specific linguistic features (e.g., grammar, syntax, cultural connotations) that might drive these differences.
- What evidence would resolve it: Detailed linguistic analysis of model responses in different languages, identifying which features correlate with bias shifts.

### Open Question 2
- Question: Can the robustness of biases in LLMs be improved through targeted training or fine-tuning without introducing new biases?
- Basis in paper: Inferred
- Why unresolved: The paper explores bias persistence but does not investigate methods to enhance bias robustness without compromising neutrality.
- What evidence would resolve it: Experimental results showing improved bias robustness after targeted training while maintaining model fairness.

### Open Question 3
- Question: How do emotional and rhetorical elements in debates influence bias shifts in LLMs compared to humans?
- Basis in paper: Explicit
- Why unresolved: The paper compares human and model responses but does not isolate the impact of emotional versus logical arguments on bias shifts.
- What evidence would resolve it: Controlled experiments varying the emotional and rhetorical content of debates and measuring their effects on bias shifts.

## Limitations
- The mechanism by which debates shift model biases is not fully explained or validated against external benchmarks.
- Cross-linguistic bias comparisons rely on translated Political Compass questions, which may not fully capture cultural nuances in Arabic and Chinese contexts.
- Human vs. model comparisons are based on a limited set of 16 questions across 8 topics, which may not generalize to broader bias types.

## Confidence

- **High**: Bias shifts are measurable across models and languages; the debate framework successfully induces changes in model stances.
- **Medium**: Multilingual bias differences are observed but may be influenced by translation quality and cultural context adaptation.
- **Low**: The exact mechanism by which debates shift model biases is not fully explained or validated against external benchmarks.

## Next Checks

1. **Mechanism validation**: Compare debate-induced bias shifts with shifts from fine-tuning or prompt engineering to isolate the debate mechanism's contribution.
2. **Translation accuracy**: Conduct back-translation and cultural review of Arabic and Chinese Political Compass questions to ensure semantic fidelity.
3. **Human model alignment**: Expand human evaluation to a broader set of bias types and misinformation topics to confirm the pattern of stronger human resilience but higher misinformation susceptibility.