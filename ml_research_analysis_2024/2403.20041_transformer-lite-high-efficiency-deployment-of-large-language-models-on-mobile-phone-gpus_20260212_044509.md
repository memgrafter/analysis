---
ver: rpa2
title: 'Transformer-Lite: High-efficiency Deployment of Large Language Models on Mobile
  Phone GPUs'
arxiv_id: '2403.20041'
source_url: https://arxiv.org/abs/2403.20041
tags:
- inference
- shape
- performance
- arxiv
- memory
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper addresses the challenge of deploying large language
  models (LLMs) efficiently on mobile phone GPUs, where current methods suffer from
  slow inference speeds and poor user experience. To solve this, the authors propose
  four key optimizations: (1) a symbolic expression-based approach to support dynamic
  shape model inference, (2) operator optimizations and execution priority settings
  to enhance speed and reduce phone lagging, (3) an FP4 quantization method termed
  E0M4 to minimize dequantization overhead, and (4) a sub-tensor-based technique to
  eliminate KV cache copying after LLM inference.'
---

# Transformer-Lite: High-efficiency Deployment of Large Language Models on Mobile Phone GPUs

## Quick Facts
- arXiv ID: 2403.20041
- Source URL: https://arxiv.org/abs/2403.20041
- Reference count: 40
- Primary result: Achieves 330 token/s prefill and 30 token/s decoding for Gemma 2B on mobile GPUs

## Executive Summary
Transformer-Lite addresses the critical challenge of deploying large language models on mobile phone GPUs by introducing a comprehensive optimization framework. The system tackles the fundamental bottlenecks of slow inference speeds and poor user experience through four key innovations: symbolic expression-based dynamic shape inference, operator optimizations with execution priority settings, FP4 quantization with minimal dequantization overhead, and sub-tensor-based KV cache elimination. These optimizations are implemented in a mobile inference engine compatible with both Qualcomm and MediaTek processors.

The framework demonstrates significant performance improvements across LLMs ranging from 2B to 14B parameters, achieving over 10x speedup in prefill operations and 2-3x speedup in decoding compared to existing mobile inference solutions. The work represents a substantial advancement in bringing efficient LLM deployment to mobile devices, enabling practical applications that were previously constrained by computational limitations.

## Method Summary
Transformer-Lite implements a multi-faceted optimization strategy specifically designed for mobile GPU deployment of large language models. The core approach centers on four technical innovations: a symbolic expression system that enables dynamic shape inference without sacrificing speed, operator-level optimizations that prioritize execution to minimize phone lagging, an E0M4 quantization scheme that reduces dequantization overhead while maintaining model accuracy, and a sub-tensor technique that eliminates unnecessary KV cache copying during inference. The system is built as a mobile inference engine with cross-platform compatibility for major mobile processor architectures, addressing the unique constraints of mobile computing environments including limited memory, thermal constraints, and the need for responsive user interfaces.

## Key Results
- Achieves 330 token/s prefill and 30 token/s decoding for Gemma 2B
- Attains 121 token/s prefill and 14 token/s decoding for ChatGLM2 6B
- Demonstrates over 10x speedup for prefill and 2-3x speedup for decoding compared to CPU-based FastLLM and GPU-based MLC-LLM

## Why This Works (Mechanism)
The system works by systematically addressing the fundamental bottlenecks in mobile LLM inference. Dynamic shape support through symbolic expressions eliminates the need for runtime shape calculations that typically slow down inference. Operator optimizations with execution priority prevent the GPU from being overwhelmed by managing the computational pipeline more efficiently. The E0M4 quantization method minimizes the computational overhead of converting between storage and computation formats, which is particularly important on resource-constrained mobile devices. The sub-tensor KV cache elimination removes redundant memory operations that consume bandwidth and processing power. Together, these optimizations create a synergistic effect where each component reduces overhead that would otherwise compound in mobile environments.

## Foundational Learning

**Dynamic shape inference**: Needed to support variable-length inputs without sacrificing speed; quick check: verify symbolic expressions correctly handle edge cases in sequence lengths.

**Quantization-aware optimization**: Required to balance model size with computational efficiency on mobile GPUs; quick check: measure accuracy degradation at different quantization levels.

**Operator fusion and scheduling**: Essential for maximizing GPU throughput while preventing memory bottlenecks; quick check: profile memory bandwidth utilization during inference.

**Cache management strategies**: Critical for minimizing redundant memory operations on devices with limited bandwidth; quick check: compare memory access patterns with and without sub-tensor optimization.

## Architecture Onboarding

**Component map**: User input -> Tokenizer -> Symbolic expression engine -> Operator pipeline -> Quantization module -> KV cache manager -> GPU execution -> Output decoder

**Critical path**: Input processing → Symbolic shape resolution → Quantized computation → Cache management → GPU kernel execution → Token generation

**Design tradeoffs**: The system prioritizes inference speed over training capabilities, uses aggressive quantization to save memory at the cost of potential accuracy loss, and sacrifices some model compatibility for optimization-specific implementations.

**Failure signatures**: Performance degradation occurs when input sequences exceed optimized length ranges, quantization introduces significant accuracy loss on complex reasoning tasks, and thermal throttling reduces sustained performance on extended inference sessions.

**First experiments**: 1) Benchmark prefill speed with varying sequence lengths, 2) Measure decoding latency with different temperature settings, 3) Profile memory usage during sustained inference sessions.

## Open Questions the Paper Calls Out
None

## Limitations
- Evaluation limited to two model variants on likely high-end Android phones only
- Performance comparisons against only two baseline systems without broader framework comparison
- E0M4 quantization and sub-tensor techniques lack detailed error analysis and ablation studies
- Does not address memory consumption patterns or thermal throttling effects

## Confidence

**High confidence**: Core architectural approach combining multiple optimizations, basic feasibility of achieving sub-second inference speeds on mobile GPUs for small LLMs.

**Medium confidence**: Specific performance numbers reported, claimed 10x speedup over CPU baselines, cross-platform compatibility without detailed validation.

**Low confidence**: Absolute superiority compared to all existing mobile inference frameworks, robustness of E0M4 quantization across diverse models, long-term stability under sustained usage.

## Next Checks
1. Test Transformer-Lite on additional LLM architectures including LLaMA variants, Mistral, and other 2B-14B parameter models
2. Evaluate the system on budget and mid-range mobile devices from multiple manufacturers
3. Conduct systematic experiments measuring output quality changes when using E0M4 quantization versus standard FP16/FP8 approaches across different task types