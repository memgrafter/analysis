---
ver: rpa2
title: Banishing LLM Hallucinations Requires Rethinking Generalization
arxiv_id: '2406.17642'
source_url: https://arxiv.org/abs/2406.17642
tags:
- training
- llms
- arxiv
- hallucinations
- generalization
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work challenges the conventional view that hallucinations
  in LLMs are an unavoidable trade-off between creativity and factuality. Through
  extensive systematic experiments, the authors show that pre-trained LLMs can easily
  memorize large datasets of random facts without a significant increase in generalization
  error.
---

# Banishing LLM Hallucinations Requires Rethinking Generalization

## Quick Facts
- arXiv ID: 2406.17642
- Source URL: https://arxiv.org/abs/2406.17642
- Reference count: 12
- LLMs can memorize random facts without significant generalization error increase, challenging the view that hallucinations are an unavoidable trade-off.

## Executive Summary
This paper challenges the conventional view that hallucinations in LLMs are an unavoidable trade-off between creativity and factuality. Through systematic experiments, the authors demonstrate that pre-trained LLMs can easily memorize large datasets of random facts without a significant increase in generalization error. This surprising result suggests that LLMs have sufficient capacity to store key facts precisely, but the computational cost of doing so is prohibitively high. To address this, the authors propose Lamini Memory Tuning, a new training approach that selectively targets near-zero training loss for specific facts, and introduce Lamini-1, a first-generation model architecture based on a massive mixture of memory experts (MoME) that enables efficient storage and retrieval of facts.

## Method Summary
The authors propose Lamini Memory Tuning, which targets near-zero training loss for key facts that should not be hallucinated, and Lamini-1, a first-generation model architecture based on a massive mixture of memory experts (MoME). Lamini-1 achieves state-of-the-art factual recall after just one hour of training on 8 MI300X GPUs by efficiently storing and retrieving facts through selective expert updates.

## Key Results
- LLMs can memorize random facts without significant generalization error increase
- Lamini Memory Tuning selectively targets near-zero training loss for specific facts
- Lamini-1 achieves state-of-the-art factual recall after one hour of training on 8 MI300X GPUs

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLMs have sufficient capacity to memorize large datasets of random facts without significant generalization error increase.
- Mechanism: During training, when loss is driven to near zero for specific facts, the model parameters are updated to store exact mappings from question to answer. The remaining capacity is used for general language understanding.
- Core assumption: The model's parameter count exceeds the number of facts to be memorized, allowing exact storage without interfering with generalization.
- Evidence anchors:
  - [abstract] "LLMs augmented with a massive Mixture of Memory Experts (MoME) can easily memorize large datasets of random numbers"
  - [section 2] "we can force the model to memorize random strings without causing the generalization error of the model to jump considerably"
  - [corpus] Weak - corpus papers focus on detecting hallucinations rather than explaining memorization capacity
- Break condition: If the number of facts exceeds the number of trainable parameters, exact memorization becomes impossible and generalization error must increase.

### Mechanism 2
- Claim: Standard training recipes optimized for generalization error amplify hallucinations.
- Mechanism: Training for one epoch on massive datasets creates a balance where loss remains moderate. This moderate loss allows the model to generate plausible but incorrect tokens during inference, leading to hallucinations.
- Core assumption: The optimal training recipe for generalization (one epoch on large datasets) does not minimize loss on individual facts.
- Evidence anchors:
  - [section 5] "we almost always choose our model as the output of running stochastic gradient descent for one epoch on trillions of tokens of internet text"
  - [section 5] "this typical training recipe leads to hallucinations on facts even if they appear in the pretraining data"
  - [corpus] Weak - corpus focuses on hallucination detection rather than training recipe analysis
- Break condition: If training continues until individual fact loss reaches zero, hallucinations on those facts are eliminated.

### Mechanism 3
- Claim: Memory Tuning selectively targets near-zero training loss for specific facts to eliminate hallucinations.
- Mechanism: By freezing the backbone network and updating only a subset of experts for each fact, the computational cost of memorizing individual facts is reduced. This allows the model to store facts exactly without requiring full retraining.
- Core assumption: Selective parameter updates can achieve near-zero loss on specific facts without affecting the generalization performance of the frozen backbone.
- Evidence anchors:
  - [section 5] "In Lamini Memory Tuning, we instead analyze the loss of individual facts, noting that this removes the biggest obstacle"
  - [section 6] "A straightforward solution to training an LLM that does not hallucinate on key facts would increase the training requirements by 100x"
  - [corpus] Weak - corpus papers do not discuss selective parameter updates for fact memorization
- Break condition: If the selected experts are not sufficient to represent the facts, or if the backbone parameters interfere with the memorized facts.

## Foundational Learning

- Concept: Generalization error vs training error
  - Why needed here: Understanding why low generalization error doesn't prevent hallucinations
  - Quick check question: Can a model have low generalization error but still hallucinate frequently?

- Concept: Information retrieval systems
  - Why needed here: Understanding how traditional retrieval methods compare to the MoME approach
  - Quick check question: What are the key differences between vector search and the MoME expert selection mechanism?

- Concept: Neural network capacity and memorization
  - Why needed here: Understanding why LLMs can memorize random labels without affecting generalization
  - Quick check question: According to theoretical results, how many facts can an LLM memorize based on its parameter count?

## Architecture Onboarding

- Component map:
  - Backbone transformer (frozen during Memory Tuning)
  - Mixture of Memory Experts (MoME) - millions of experts for storing facts
  - Cross attention mechanism for expert selection
  - Index for managing expert retrieval
  - Triton kernels for GPU acceleration

- Critical path:
  1. Question input → cross attention → expert selection
  2. Selected experts → response generation
  3. Backpropagation only through selected experts during Memory Tuning

- Design tradeoffs:
  - Memory vs computation: More experts allow storing more facts but increase memory requirements
  - Expert selection granularity: Too few experts per query may miss relevant facts; too many increases computation
  - Backbone freezing: Reduces computation but may limit the model's ability to integrate memorized facts

- Failure signatures:
  - Hallucinations persist: Experts not being selected correctly or insufficient expert capacity
  - Slow inference: Too many experts being retrieved per query
  - Training instability: Learning rate too high for selective updates

- First 3 experiments:
  1. Train MoME on random facts and measure hallucination rate vs baseline
  2. Vary number of experts selected per query and measure performance/memory tradeoff
  3. Test expert selection with frozen vs unfrozen backbone to quantify impact on memorization

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the exact threshold of training loss above which LLMs consistently hallucinate, and how does this threshold vary across different model architectures and dataset sizes?
- Basis in paper: [explicit] The paper states "we corroborate these experimental findings with a theoretical construction showing that simple neural networks trained to predict the next token hallucinate when the training loss is above a threshold"
- Why unresolved: The paper mentions this threshold exists but does not provide specific numerical values or investigate how it varies across different architectures and datasets.
- What evidence would resolve it: Experimental results showing training loss values for various models and datasets, and correlation with hallucination frequency.

### Open Question 2
- Question: How can we efficiently determine which facts should be prioritized for memory tuning versus allowing generalization, and what metrics should guide this selection?
- Basis in paper: [explicit] The paper proposes "Lamini Memory Tuning, that targets near zero training loss for key facts that should not be hallucinated" but doesn't specify how to identify these key facts.
- Why unresolved: The paper introduces the concept but doesn't provide a methodology for determining which facts are critical enough to warrant memory tuning.
- What evidence would resolve it: A framework for evaluating fact importance and experimental results comparing different selection strategies.

### Open Question 3
- Question: What is the computational trade-off between model size and memory expert count in the MoME architecture for achieving optimal factual recall?
- Basis in paper: [explicit] The paper describes "Lamini-1 - that stores facts in a massive mixture of millions of memory experts" but doesn't provide analysis of optimal ratios.
- Why unresolved: The paper presents the architecture but doesn't explore the relationship between model parameters and memory expert parameters.
- What evidence would resolve it: Systematic experiments varying the ratio of backbone parameters to memory expert parameters and measuring factual recall performance.

## Limitations

- The paper's theoretical capacity analysis may not capture the full complexity of modern transformer architectures
- Experimental validation focuses primarily on TruthfulQA and MMLU datasets, which may not represent the full spectrum of hallucination types
- Computational cost analysis may underestimate practical deployment challenges for the MoME architecture

## Confidence

**High Confidence**: Core observation that standard training approaches create a trade-off between generalization and fact memorization
**Medium Confidence**: Proposed Memory Tuning solution
**Low Confidence**: Scalability claims for the MoME architecture

## Next Checks

1. **Capacity Scaling Experiment**: Systematically test the memorization capacity by training models on increasing numbers of random facts (from thousands to millions) and measuring the point at which generalization error begins to increase significantly.

2. **Cross-Domain Generalization Test**: Evaluate the Memory Tuning approach on diverse domains including technical documentation, medical knowledge, and creative writing tasks.

3. **Production Performance Benchmark**: Implement a prototype MoME system with realistic scale (millions of experts) and measure actual inference latency, memory usage, and throughput under realistic query loads.