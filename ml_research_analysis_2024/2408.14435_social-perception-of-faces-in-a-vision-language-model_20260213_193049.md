---
ver: rpa2
title: Social Perception of Faces in a Vision-Language Model
arxiv_id: '2408.14435'
source_url: https://arxiv.org/abs/2408.14435
tags:
- social
- cosine
- images
- perception
- black
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: CLIP, a vision-language model, exhibits human-like social perception
  of faces, including biases based on age, gender, and race. Using a novel experimental
  approach with synthetic face images (CausalFace) that systematically vary six attributes,
  the study reveals that non-protected attributes like facial expression, lighting,
  and pose significantly impact social perception, often as much as protected attributes.
---

# Social Perception of Faces in a Vision-Language Model

## Quick Facts
- arXiv ID: 2408.14435
- Source URL: https://arxiv.org/abs/2408.14435
- Reference count: 40
- Primary result: CLIP exhibits human-like social perception biases across age, gender, and race, with non-protected attributes like expression having comparable effects

## Executive Summary
This study investigates social perception biases in CLIP, a vision-language model, using a novel experimental approach with synthetic face images (CausalFace) that systematically vary six attributes. The research reveals that non-protected attributes like facial expression, lighting, and pose significantly impact social perception, often as much as protected attributes such as age, gender, and race. Notably, smiling has a stronger effect than age, and Black women are consistently outliers across all age groups and expressions. The method controls for confounding variables, providing clearer, more reliable insights into bias than observational studies using real-world data. This approach enables causal inferences and could be applied to study biases in other vision-language models.

## Method Summary
The study employs synthetic face images (CausalFace) systematically varied across six attributes: age, gender, race, facial expression, lighting, and pose. Text prompts derived from validated social psychology models (SCM, ABC) capture dimensions of social perception. The method computes cosine similarity between CLIP's image and text embeddings, introducing a neutral prompt baseline to normalize systematic differences across demographic groups. This approach isolates the causal effect of each attribute on social perception while avoiding confounds present in real-world datasets.

## Key Results
- Non-protected attributes (expression, lighting, pose) cause as much variation in social perception as protected attributes (age, gender, race)
- Smiling has a stronger effect on social perception than age
- Black women are consistent outliers, showing extreme social perception values across all ages and expressions
- Systematic attribute manipulation provides clearer causal insights than observational studies with real photos

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Systematic, independent manipulation of facial attributes in synthetic images enables causal inference about how each attribute influences social perception in CLIP.
- Mechanism: By generating synthetic faces where only one attribute varies at a time while all others are held constant, the study isolates the effect of that attribute on social perception scores. This avoids confounding correlations present in real-world datasets.
- Core assumption: The synthetic dataset's controlled attribute variations are perceptually valid and do not introduce unintended visual artifacts that could distort CLIP's embeddings.
- Evidence anchors:
  - [abstract]: "Independently and systematically manipulating face attributes allows us to study the effect of each on social perception and avoids confounds that can occur in wild-collected data due to uncontrolled systematic correlations between attributes."
  - [section]: "CausalFace allows us to systematically compare variations in perception due to legally protected attributes... as well as unprotected attributes... that are usually not included in bias analyses."
- Break condition: If synthetic face manipulations introduce unintended correlations or CLIP responds to synthetic artifacts rather than real facial features, causal inferences will be invalid.

### Mechanism 2
- Claim: Using socially validated psychological terms as text prompts improves the reliability and interpretability of bias measurements compared to ad-hoc prompt selection.
- Mechanism: Text prompts derived from established social psychology models (SCM, ABC) capture the dimensions of social perception that humans naturally use (warmth, competence, agency, etc.). CLIP's response to these prompts reveals whether it shares human-like biases.
- Core assumption: The CLIP model's embedding space preserves semantic relationships similar to human social perception dimensions.
- Evidence anchors:
  - [abstract]: "Our textual prompts are constructed from well-validated social psychology terms denoting social perception."
  - [section]: "We use text prompts that are best at representing social perceptions based on evidence from social psychology... decades of social psychology literature and use words that have emerged as the consensus to represent the major ways in which people categorize each other."
- Break condition: If CLIP's embedding space does not align with human social perception dimensions, or if the psychological terms don't translate well to CLIP's learned representations, the bias measurements will be unreliable.

### Mechanism 3
- Claim: Introducing a neutral prompt baseline and measuring difference in cosine similarity reveals social biases more clearly than raw similarity scores.
- Mechanism: The study finds that different demographic groups have different baseline cosine similarities with neutral prompts. By subtracting this baseline, the analysis isolates the additional effect of social adjectives, making biases more apparent.
- Core assumption: The baseline cosine similarity differences are consistent across images within a demographic group and can be reliably subtracted.
- Evidence anchors:
  - [abstract]: "We introduced this metric because we observed a bias between demographic groups when using neutral text prompts... Subtracting neutral prompt cosine similarities normalizes away this 'ground' bias and more clearly reveals social biases."
  - [section]: "When we consider the distributions of cosine similarities of CausalFace images using a neutral text prompt... we find that Asian women have significantly higher and Black men significantly lower cosine similarities compared to all other groups (p<= 0.01)."
- Break condition: If the baseline differences are not consistent or if the subtraction process introduces new artifacts, the adjusted metric may misrepresent true social biases.

## Foundational Learning

- Concept: Cosine similarity in embedding space
  - Why needed here: The study measures how similar CLIP's internal representations of face images are to text prompts using cosine similarity, which quantifies the angle between vectors in the embedding space.
  - Quick check question: If two vectors in CLIP's embedding space point in exactly the same direction, what is their cosine similarity value?

- Concept: Synthetic data generation for controlled experiments
  - Why needed here: The study uses a synthetic dataset (CausalFace) where facial attributes are systematically varied to isolate causal effects, requiring understanding of how synthetic data can be generated and validated.
  - Quick check question: Why might synthetic faces be preferable to real photos when studying the causal effect of individual facial attributes on model behavior?

- Concept: Social perception dimensions in psychology
  - Why needed here: The study uses established psychological frameworks (SCM, ABC) to define what social perceptions to measure, requiring knowledge of these models and their validated dimensions.
  - Quick check question: What are the three primary dimensions of social perception according to the Stereotype Content Model?

## Architecture Onboarding

- Component map:
  - CLIP model (ViT-B/32) for generating image and text embeddings
  - Synthetic face dataset (CausalFace) with systematically varied attributes
  - Text prompt construction system using validated psychological terms
  - Cosine similarity calculation pipeline
  - Statistical analysis tools for comparing attribute effects

- Critical path:
  1. Load CLIP model and obtain image/text embedding functions
  2. Generate or load CausalFace dataset with attribute annotations
  3. Construct text prompts from psychological dimensions
  4. Compute cosine similarities between all image-text pairs
  5. Calculate baseline neutral prompt similarities and adjust metrics
  6. Analyze attribute effects through statistical comparison

- Design tradeoffs:
  - Synthetic vs. real images: Synthetic provides control but may lack ecological validity
  - Resolution vs. computational cost: Higher resolution images may improve quality but increase processing time
  - Number of attributes vs. combinatorial complexity: More attributes provide richer analysis but increase computational requirements exponentially

- Failure signatures:
  - CLIP fails to produce meaningful embeddings for synthetic faces
  - Attribute variations in CausalFace don't produce perceptible differences in images
  - Text prompts don't align with CLIP's learned semantic space
  - Statistical analysis shows no significant differences when visual inspection suggests they should exist

- First 3 experiments:
  1. Verify CLIP can classify basic demographic attributes (gender, race) from synthetic faces using simple prompts
  2. Test cosine similarity baseline differences across demographic groups with neutral prompts
  3. Compare social perception scores for smiling vs. non-smiling faces within the same demographic group to validate the method

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does CLIP exhibit human-like social perception in other cultural contexts beyond the English-speaking world?
- Basis in paper: [explicit] The paper states that CLIP's social perception mirrors humans in drawing trait inferences from face images, but all experiments were conducted using English text prompts and synthetic faces.
- Why unresolved: The study only tested CLIP with English text prompts and synthetic faces generated using Western facial features. Cross-cultural validation is needed to determine if these findings generalize.
- What evidence would resolve it: Conducting the same experiments with text prompts translated into multiple languages and using synthetic faces that represent diverse cultural facial features.

### Open Question 2
- Question: Are the age-related social perception patterns for Black women in CLIP causally linked to training data biases or emergent properties of the model architecture?
- Basis in paper: [explicit] The paper identifies unique age-related patterns for Black women across multiple social perception dimensions but doesn't determine the source of these patterns.
- Why unresolved: The experimental approach isolates the effect of age on social perception but cannot distinguish whether the observed patterns originate from biased training data or from the model's internal representation mechanisms.
- What evidence would resolve it: Analyzing CLIP's internal representations across different training iterations and comparing social perception patterns before and after specific training data modifications.

### Open Question 3
- Question: How do non-protected attributes like facial expression, lighting, and pose interact with protected attributes to create emergent social perception biases that are not simply additive?
- Basis in paper: [explicit] The paper shows that non-protected attributes cause as much variation in social perception as protected ones, but doesn't explore their interaction effects.
- Why unresolved: The experimental design systematically varies each attribute independently but doesn't examine higher-order interactions between multiple attributes simultaneously.
- What evidence would resolve it: Creating synthetic face datasets that systematically vary multiple attributes together and measuring the resulting social perception patterns to identify non-additive interactions.

## Limitations
- The synthetic nature of CausalFace may limit ecological validity compared to real-world photos
- Results may be specific to CLIP's architecture and training data, limiting generalizability
- The study doesn't explore higher-order interactions between multiple facial attributes

## Confidence
- High confidence: The methodological framework for using synthetic faces to study causal effects of facial attributes
- Medium confidence: The relative magnitude of effects across different attributes and demographic groups
- Low confidence: The generalizability of specific numerical findings to other vision-language models or real-world applications

## Next Checks
1. Replicate the analysis using a different vision-language model (e.g., BLIP, Flamingo) to test generalizability of findings
2. Conduct human perception studies on the same CausalFace images to validate that CLIP's social perception aligns with human judgments
3. Test the robustness of findings by generating alternative synthetic face datasets with different generation parameters to ensure results aren't artifacts of specific image properties