---
ver: rpa2
title: Sandra -- A Neuro-Symbolic Reasoner Based On Descriptions And Situations
arxiv_id: '2402.00591'
source_url: https://arxiv.org/abs/2402.00591
tags:
- situation
- ontology
- sandra
- descriptions
- description
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Sandra is a neuro-symbolic reasoner that combines vectorial representations
  with deductive reasoning, enabling perspective-based inference over incomplete information.
  It formalizes the Description and Situation (DnS) ontology pattern into a differentiable
  vector space, allowing neural networks to reason about plausible interpretations
  of situations even when facts are missing or conflicting.
---

# Sandra -- A Neuro-Symbolic Reasoner Based On Descriptions And Situations

## Quick Facts
- arXiv ID: 2402.00591
- Source URL: https://arxiv.org/abs/2402.00591
- Reference count: 25
- Primary result: Neuro-symbolic reasoner combining vector representations with deductive reasoning for perspective-based inference over incomplete information

## Executive Summary
Sandra is a neuro-symbolic reasoner that integrates neural networks with symbolic reasoning through the Description and Situation (DnS) ontology pattern. It maps situations into a vector space constrained by an ontology, enabling the system to infer plausible interpretations even with incomplete information. The approach provides both improved accuracy on visual reasoning tasks and interpretable outputs by identifying which descriptions best explain a given situation.

## Method Summary
Sandra implements a neuro-symbolic architecture where neural networks extract features from input data and map them to a vector space V constrained by a DnS ontology. The system defines a function fs that maps situations to vectors, ensuring these vectors satisfy the description constraints through linear combinations of basis vectors. For classification, Sandra computes the probability that a situation satisfies each description, using these probabilities to make predictions while providing interpretable explanations through the inferred descriptions.

## Key Results
- Outperforms baseline models on I-RAVEN (95.1% vs 90.8%) and Rotated-FashionMNIST (92.3% vs 90.6%)
- Adds minimal complexity: 275k vs 205k parameters for I-RAVEN, and 84.1k vs 29k for FashionMNIST
- Provides interpretable outputs by inferring descriptions that explain situations, even when misclassifications occur

## Why This Works (Mechanism)

### Mechanism 1
Sandra achieves perspective-based reasoning by mapping situations into a vector space constrained by a DnS ontology, where each description corresponds to a subspace. The neuro-symbolic architecture enforces that the neural network's output vector lies in the span of the basis vectors derived from the roles of a description. This ensures the vector representation satisfies the description's constraints.

Core assumption: The vector space isomorphism between the DnS ontology and the geometric representation preserves the logical relationships between descriptions and situations.

### Mechanism 2
Sandra provides probabilistic reasoning over descriptions, allowing inference even with incomplete information. The method computes the probability that a situation satisfies a description by measuring how well the situation vector can be expressed as a linear combination of the description's basis vectors. This probability quantifies the degree of satisfiability.

Core assumption: The probability of satisfaction is a valid measure of how well a description explains a situation, even if not all roles are fulfilled.

### Mechanism 3
Sandra improves interpretability by providing a human-readable explanation for the model's predictions. The inferred descriptions that satisfy a situation provide a symbolic explanation for the model's classification, bridging the gap between the neural network's internal representation and human-understandable concepts.

Core assumption: The descriptions in the DnS ontology correspond to meaningful concepts that can be interpreted by humans.

## Foundational Learning

- Concept: Vector space and linear algebra
  - Why needed here: The core of Sandra's reasoning relies on mapping situations and descriptions to vectors and subspaces, requiring a solid understanding of vector spaces, linear combinations, and basis vectors.
  - Quick check question: Can you explain what it means for a vector to be a linear combination of other vectors, and how this relates to the concept of a basis?

- Concept: Description and Situation (DnS) ontology design pattern
  - Why needed here: Sandra's reasoning is based on the DnS pattern, which formalizes the relationship between descriptions (perspectives) and situations (facts). Understanding this pattern is crucial for grasping how Sandra infers plausible interpretations.
  - Quick check question: Can you describe the difference between a description and a situation in the DnS ontology, and how the satisfaction relation is defined?

- Concept: Neuro-symbolic integration
  - Why needed here: Sandra combines neural networks with symbolic reasoning, requiring an understanding of how to integrate these two paradigms effectively. This includes knowledge of how to constrain the neural network's output to satisfy the DnS constraints.
  - Quick check question: Can you explain how a neural network can be used to approximate the function fs, which maps a situation to a vector, while ensuring that the resulting vector satisfies the DnS constraints?

## Architecture Onboarding

- Component map: Neural Network -> Sandra Layer -> DnS Ontology
- Critical path:
  1. Input data is fed into the NN.
  2. The NN's output is projected into the vector space V using the Sandra layer.
  3. The Sandra layer computes the probability of satisfaction for each description.
  4. The descriptions with the highest probabilities are selected as the model's interpretation of the situation.

- Design tradeoffs:
  - Complexity vs. Interpretability: Sandra adds complexity to the model by introducing the DnS ontology and the vector space mapping, but this complexity is traded off for improved interpretability and perspective-based reasoning.
  - Ontology Design: The effectiveness of Sandra depends on the quality of the DnS ontology. A poorly designed ontology may lead to incorrect or meaningless interpretations.

- Failure signatures:
  - Low accuracy on the task: This could indicate that the NN is not learning the fs function effectively, or that the DnS ontology is not well-suited to the task.
  - Uninterpretable descriptions: If the descriptions inferred by Sandra are not meaningful to humans, it may indicate that the DnS ontology does not accurately capture the semantics of the task.
  - Computational inefficiency: If the vector space operations become too expensive for large ontologies, it may be necessary to optimize the implementation or simplify the ontology.

- First 3 experiments:
  1. Train a simple NN (e.g., a linear classifier) on a toy dataset (e.g., MNIST) with a small DnS ontology (e.g., two descriptions: "has digit 0" and "has digit 1"). Verify that Sandra can correctly infer the descriptions satisfied by each input.
  2. Integrate Sandra into a more complex NN (e.g., a CNN) and train it on a more challenging dataset (e.g., CIFAR-10) with a more complex DnS ontology (e.g., descriptions based on object categories and attributes). Evaluate the impact of Sandra on the model's accuracy and interpretability.
  3. Experiment with different ways of defining the DnS ontology for a given task (e.g., using manual annotation vs. automatic extraction from data). Compare the performance and interpretability of the resulting Sandra models.

## Open Questions the Paper Calls Out

### Open Question 1
Can the neuro-symbolic reasoner be extended to support more complex ontologies beyond Description and Situation (DnS), such as those incorporating temporal or probabilistic reasoning? The paper mentions the potential for extending the method to some parts of Description Logic but does not provide a clear methodology for extending it to other ontology types.

### Open Question 2
How does the choice of activation function in the dP-sandra layer affect the performance and interpretability of the neuro-symbolic reasoner? The paper discusses the use of the ReLU function but mentions that other functions could be used for the same purpose without systematic assessment.

### Open Question 3
Can the neuro-symbolic reasoner be applied to other domains beyond visual reasoning and domain generalization, such as natural language processing or robotics? The paper discusses potential applications in various domains but does not provide demonstrations or discuss specific challenges in adapting the method.

## Limitations
- Experimental validation relies on only two datasets, limiting generalizability across different reasoning tasks
- Does not demonstrate how to automatically construct or validate ontologies for new domains
- Computational overhead of vector space operations for large ontologies is not characterized

## Confidence
- **High Confidence**: The theoretical foundation of Sandra's vector space construction and the DnS satisfaction relation is mathematically sound
- **Medium Confidence**: The claim that Sandra provides "interpretability in the classification process" is supported by qualitative examples but lacks systematic evaluation
- **Low Confidence**: The assertion that Sandra can handle "incomplete information" through probabilistic reasoning is only partially supported

## Next Checks
1. Implement an automated method for extracting DnS ontologies from unlabeled data and evaluate performance degradation compared to manually constructed ontologies
2. Systematically remove different proportions of features from datasets (20%, 50%, 80% missing) to measure Sandra's accuracy degradation compared to baselines
3. Benchmark Sandra's inference time and memory usage across ontologies of increasing size (10, 100, 1000 descriptions) to quantify computational tradeoffs