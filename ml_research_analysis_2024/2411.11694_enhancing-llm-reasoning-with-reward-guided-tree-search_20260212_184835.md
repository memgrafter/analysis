---
ver: rpa2
title: Enhancing LLM Reasoning with Reward-guided Tree Search
arxiv_id: '2411.11694'
source_url: https://arxiv.org/abs/2411.11694
tags:
- reward
- reasoning
- search
- training
- policy
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper presents STILL-1, a reward-guided tree search framework
  for enhancing LLM reasoning abilities. The framework integrates three components:
  a policy model for generating reasoning steps, a reward model for providing feedback
  signals, and a search algorithm for navigating the solution space.'
---

# Enhancing LLM Reasoning with Reward-guided Tree Search

## Quick Facts
- arXiv ID: 2411.11694
- Source URL: https://arxiv.org/abs/2411.11694
- Reference count: 40
- Key outcome: STILL-1 achieved 46.9%, 7.3%, 91.6%, and 31.4% accuracy gains on MATH-OAI, GSM-Hard, OlympiadBench, and College Math respectively using reward-guided tree search

## Executive Summary
This paper introduces STILL-1, a reward-guided tree search framework that significantly enhances large language model reasoning capabilities on complex mathematical problems. The framework integrates a policy model for generating reasoning steps, a reward model for providing feedback signals, and a search algorithm for navigating the solution space. Through iterative training between components using preference optimization and active learning, the system achieves substantial performance improvements across four challenging mathematical reasoning benchmarks. The approach demonstrates that test-time scaling through structured search can effectively address the limitations of LLMs in complex reasoning tasks.

## Method Summary
The framework implements a tree search algorithm where a policy model generates reasoning steps guided by a reward model. The policy model is adapted to mathematical reasoning format through instruction tuning on formatted data synthesized via in-context learning. The reward model, based on LLaMA-3.1-8B-Instruct, is domain-adapted through mathematical instruction fine-tuning and trained using outcome-supervised learning with active learning to select high-quality samples. The search algorithm employs an MCTS-like approach with pre-expansion and rollouts for node evaluation, incorporating tool manipulation for calculation verification.

## Key Results
- Achieved 46.9% accuracy gain on MATH-OAI benchmark compared to baseline methods
- Demonstrated 91.6% accuracy improvement on OlympiadBench dataset
- Showed 31.4% performance increase on College Math problems
- Outperformed existing approaches across all four tested mathematical reasoning datasets

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Reward-guided tree search improves LLM reasoning by providing structured exploration of solution space
- Mechanism: The framework integrates policy model, reward model, and search algorithm to navigate a dynamically expanding tree. The policy model generates reasoning steps, the reward model provides feedback signals, and the search algorithm guides the exploration process.
- Core assumption: The reward model can accurately assess solution quality and guide the search process effectively
- Evidence anchors:
  - [abstract]: "This framework is implemented by integrating the policy model, reward model, and search algorithm. It is primarily constructed around a tree search algorithm, where the policy model navigates a dynamically expanding tree guided by a specially trained reward model."
  - [section]: "Within this framework, the policy model generates new reasoning steps based on a partial solution prefix along a given path in the search tree. The search algorithm constructs the search tree to structure the reasoning process, while the reward model provides feedback signals to guide the policy model's actions and the search process."
  - [corpus]: Weak - corpus papers focus on similar tree search approaches but don't directly validate this specific mechanism
- Break condition: If the reward model cannot distinguish between correct and incorrect reasoning paths, the search process becomes ineffective

### Mechanism 2
- Claim: Iterative training between policy and reward models creates mutual improvement cycles
- Mechanism: The policy model generates candidate solutions, which are used to train the reward model. The reward model then provides feedback to improve the policy model through preference optimization, creating a reinforcement learning loop.
- Core assumption: The quality of generated candidate solutions improves sufficiently to train an effective reward model
- Evidence anchors:
  - [abstract]: "The approach employs iterative training between the policy and reward models, using specially constructed preference data and active learning."
  - [section]: "In our framework, the policy model and the reward model are two highly interrelated components: their training involves using data generated or selected by one another, and they jointly perform the reasoning process to arrive at the final solution."
  - [corpus]: Weak - corpus papers discuss iterative training but lack specific validation of this mutual improvement cycle
- Break condition: If the policy model fails to generate sufficiently diverse or accurate candidate solutions, the reward model cannot learn effective discrimination

### Mechanism 3
- Claim: Domain adaptation and format alignment improve model performance on mathematical reasoning
- Mechanism: The policy model is adapted to mathematical reasoning format through instruction tuning, and the reward model is domain-adapted through mathematical instruction fine-tuning.
- Core assumption: Mathematical reasoning requires specialized formatting and domain knowledge that general models lack
- Evidence anchors:
  - [section]: "To align with this desired reasoning format, we synthesize formatted data through in-context learning with a more capable LLM, followed by instruction tuning on the policy model."
  - [section]: "For the backbone model of the reward model, we select LLaMA-3.1-8B-Instruct, which is the same choice as for the policy model in our framework. Although LLMs have demonstrated remarkable performance on various text generation tasks, they still face challenges in complex mathematical reasoning tasks. To address this, we fine-tune the backbone model with mathematical instructions, enabling it to better adapt to mathematical scenarios and tasks."
  - [corpus]: Weak - corpus papers mention domain adaptation but don't provide specific evidence for mathematical reasoning
- Break condition: If the mathematical reasoning format is not sufficiently general or the domain adaptation doesn't capture essential mathematical concepts

## Foundational Learning

- Concept: Tree search algorithms (MCTS, beam search)
  - Why needed here: The framework relies on tree-based exploration of solution space, requiring understanding of search algorithms
  - Quick check question: What is the key difference between MCTS and beam search in terms of exploration-exploitation tradeoff?

- Concept: Reinforcement learning and preference optimization
  - Why needed here: The iterative training between policy and reward models follows RL principles, using preference data for optimization
  - Quick check question: How does direct preference optimization differ from traditional RL approaches in terms of reward signal formulation?

- Concept: Mathematical reasoning formats and notation
  - Why needed here: The policy model must generate mathematically correct reasoning steps in a specific format
  - Quick check question: What are the key components of a well-formatted mathematical reasoning step in this framework?

## Architecture Onboarding

- Component map:
  - Policy Model -> Reward Model -> Search Algorithm -> Solution Output
  - Policy Model -> Reward Model (iterative training loop)
  - Reward Model -> Policy Model (preference optimization)

- Critical path:
  1. Policy model generates candidate solutions
  2. Reward model scores solutions and provides feedback
  3. Preference optimization updates policy model
  4. Search algorithm uses updated policy model for exploration

- Design tradeoffs:
  - Search algorithm choice (MCTS vs beam search): MCTS provides better exploration but is computationally expensive; beam search is faster but may miss optimal paths
  - Reward model granularity (outcome vs process supervision): Outcome supervision is easier to implement but may miss step-level errors
  - Training data selection strategy: Active learning improves quality but requires more computation

- Failure signatures:
  - Policy model generates repetitive or low-quality solutions
  - Reward model cannot distinguish between correct and incorrect reasoning paths
  - Search algorithm fails to explore sufficiently diverse solution paths

- First 3 experiments:
  1. Compare MCTS vs beam search performance on a simple mathematical dataset
  2. Test different reward model architectures (discriminative vs generative) on solution quality assessment
  3. Evaluate the impact of domain adaptation on policy model performance with and without mathematical instruction tuning

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the effectiveness of generative reward models compare to discriminative reward models when trained with limited process-level supervision data?
- Basis in paper: [explicit] The paper mentions that generative reward models outperformed discriminative ones but primarily focuses on outcome-supervised training due to computational constraints of generating process-supervised data.
- Why unresolved: The paper only briefly mentions the potential of outcome-supervised models for step-level assessment but doesn't conduct thorough comparisons under limited process supervision scenarios.
- What evidence would resolve it: Direct comparison experiments between generative and discriminative reward models trained with varying amounts of process-level supervision data.

### Open Question 2
- Question: What is the optimal balance between exploration and exploitation in the UCB formula for mathematical reasoning tasks?
- Basis in paper: [explicit] The paper uses UCB with a constant c but doesn't explore the sensitivity of performance to different c values or alternative exploration strategies.
- Why unresolved: The paper presents the standard UCB formula but doesn't systematically analyze how different exploration parameters affect reasoning performance.
- What evidence would resolve it: Comprehensive ablation studies varying the exploration constant and testing alternative exploration strategies.

### Open Question 3
- Question: How does the performance of reward-guided tree search scale with model size beyond the 8B parameter baseline used in this study?
- Basis in paper: [explicit] The paper uses LLaMA-3.1-8B-Instruct as the backbone but mentions that increasing model size can enhance assessment capability.
- Why unresolved: The study only tests one model size and doesn't explore how the framework's effectiveness changes with larger models.
- What evidence would resolve it: Performance comparisons of the framework using different model sizes (e.g., 8B, 70B, 405B parameters) on the same benchmarks.

## Limitations
- Performance heavily depends on reward model quality and iterative training loop effectiveness
- Limited details on exact prompt templates and hyperparameter configurations may hinder reproducibility
- Active learning component adds computational complexity to the training process

## Confidence

**Major Uncertainties:**
The framework's performance heavily depends on the quality of the reward model and the effectiveness of the iterative training loop. Without access to the exact prompt templates and hyperparameter configurations, reproducing the claimed performance gains may prove challenging. The active learning component for reward model training introduces additional complexity that could affect reproducibility.

**Confidence Labels:**
- High confidence in the general framework architecture and its theoretical soundness
- Medium confidence in the specific implementation details and hyperparameter choices
- Medium confidence in the reported performance improvements due to limited details on ablation studies

## Next Checks
1. Implement a simplified version of the framework using publicly available datasets to verify the core mechanism of reward-guided tree search improves mathematical reasoning performance
2. Conduct ablation studies to isolate the impact of each component (policy model, reward model, search algorithm) on overall performance
3. Test the framework's generalization to mathematical reasoning tasks beyond the four specified datasets to evaluate robustness and scalability