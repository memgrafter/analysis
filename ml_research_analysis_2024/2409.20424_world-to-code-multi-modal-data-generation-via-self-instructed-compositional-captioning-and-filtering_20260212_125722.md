---
ver: rpa2
title: 'World to Code: Multi-modal Data Generation via Self-Instructed Compositional
  Captioning and Filtering'
arxiv_id: '2409.20424'
source_url: https://arxiv.org/abs/2409.20424
tags:
- llav
- data
- visual
- arxiv
- image
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces W2C, a self-instructed data generation pipeline
  for vision-language models that leverages consistency filtering to produce high-quality
  multi-modal data without human intervention. The method uses VLMs to extract visual
  concepts, generate region-level captions and OCR information, then filters outputs
  through generator-validator consistency checking, organizing results into Python
  code format for better cross-modal alignment.
---

# World to Code: Multi-modal Data Generation via Self-Instructed Compositional Captioning and Filtering

## Quick Facts
- arXiv ID: 2409.20424
- Source URL: https://arxiv.org/abs/2409.20424
- Reference count: 24
- Key outcome: W2C improves VQA performance by 8.2 accuracy on 2-shot GQA and achieves 1.5-3.5 IoU gains on grounding tasks

## Executive Summary
W2C introduces a self-instructed data generation pipeline for vision-language models that leverages consistency filtering to produce high-quality multi-modal data without human intervention. The method extracts visual concepts, generates region-level captions and OCR information, then filters outputs through generator-validator consistency checking. By organizing results into Python code format, W2C achieves better cross-modal alignment than traditional detail captions. Experiments demonstrate consistent improvements across 7-9 VQA benchmarks and visual grounding tasks compared to ShareGPT4V, with particular gains in grounding benchmarks.

## Method Summary
The W2C pipeline processes images through four main stages: visual concepts extraction using Grounding DINO, self-instructed information extraction (region captions and OCR), consistency filtering to select high-quality outputs, and structured formatting into Python code. The method generates multiple caption candidates for each visual concept using beam search, then validates each candidate against the image through self-consistency checks. Candidates containing hallucinated concepts receive negative scores while valid concepts receive positive scores. The highest-scoring caption is selected as final output. The Python code format structures image information as classes with attributes representing visual concepts, enabling VLMs to better understand semantic equivalence between visual elements and their textual descriptions.

## Key Results
- Achieves 8.2 accuracy improvement on 2-shot GQA evaluation compared to baseline models
- Improves grounding benchmarks by 1.5-3.5 average IoU across RefCOCO, RefCOCO+, and RefCOCOg
- Consistently outperforms ShareGPT4V across 7-9 VQA benchmarks including GQA, MME, DocVQA, and TextVQA
- Demonstrates effectiveness across both LLaVA-1.5 and LLaVA-NeXT architectures

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Consistency filtering improves data quality by selecting captions that survive multiple validation checks
- Mechanism: The pipeline generates multiple caption candidates for each visual concept using beam search, then validates each candidate against the image through self-consistency checks. Candidates containing hallucinated concepts (marked "No") receive negative scores, while valid concepts receive positive scores. The caption with the highest score is selected as final output.
- Core assumption: Visual concepts that are consistently validated across multiple prompt variations are more likely to be correct and less likely to hallucinate
- Evidence anchors:
  - [abstract] "Experiments have demonstrated the high quality of W2C by improving various visual question answering and visual grounding benchmarks across different VLMs"
  - [section 3.3] "we propose to filter the visual concepts via generation-validation consistency, where we change the region-level captions into multiple visual question answering problems for both counting filtering and caption reranking"
- Break condition: If VLMs generate consistently wrong outputs across prompt variations, or if the scoring mechanism fails to distinguish between valid and hallucinated concepts

### Mechanism 2
- Claim: Organizing data in Python code format improves cross-modal alignment compared to traditional captions
- Mechanism: The pipeline structures image information as Python classes with attributes representing visual concepts, each containing caption, OCR text, and bounding box information. This formal representation enables VLMs to better understand the semantic equivalence between visual elements and their textual descriptions.
- Core assumption: Structured code representations provide clearer semantic relationships between visual elements and their descriptions than free-form captions
- Evidence anchors

## Foundational Learning

### Consistency Filtering
- Why needed: To eliminate hallucinated concepts and ensure data quality without human annotation
- Quick check: Generate captions for the same visual concept with different prompts and verify consistent outputs

### Self-Instructed Generation
- Why needed: Enables autonomous data creation without human-written instructions
- Quick check: Verify that generated questions and answers are coherent and correctly grounded in the image

### Cross-Modal Alignment
- Why needed: Ensures visual concepts and textual descriptions are semantically equivalent
- Quick check: Test whether VLMs can correctly map visual elements to their code representations

## Architecture Onboarding

### Component Map
Visual Concepts Extraction -> Self-Instructed Caption Generation -> Consistency Filtering -> Python Code Formatting -> VLM Training

### Critical Path
The most critical components are the consistency filtering stage and the Python code formatting. Consistency filtering determines which generated captions are accepted, directly impacting data quality. The code formatting is essential for achieving the reported cross-modal alignment improvements.

### Design Tradeoffs
- Beam search vs. greedy decoding: Beam search generates multiple candidates but increases computational cost
- Filtering thresholds: Stricter thresholds improve quality but reduce dataset size
- Code format vs. traditional captions: Code provides better structure but may be harder for some VLMs to parse

### Failure Signatures
- Dataset size significantly smaller than expected: Indicates overly strict filtering thresholds
- Poor performance on OCR-heavy tasks: Suggests issues with OCR extraction or LLaVA-1.5 limitations
- Inconsistent performance across VLMs: Points to VLM-specific generation biases

### First Experiments
1. Generate captions for a small set of images and manually verify consistency filtering removes hallucinated concepts
2. Compare model performance on a few-shot task using code-formatted vs. traditional captions
3. Test the pipeline with different VLMs to identify generation variability

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does W2C's performance scale when applied to VLMs with different architectural designs beyond the LLaVA series?
- Basis in paper: [explicit] The paper mentions that experiments are mainly conducted on LLaVA series models and notes that "The effectiveness of W2C can be further investigated on other VLM structures."
- Why unresolved: The paper only tests W2C on LLaVA models using MLP projectors, leaving open whether the approach generalizes to other VLM architectures like those using transformers or different alignment strategies.
- What evidence would resolve it: Testing W2C on a diverse set of VLM architectures (e.g., Flamingo, BLIP-2, or other transformer-based models) and comparing performance improvements across different visual encoder types.

### Open Question 2
- Question: What is the impact of using different object detection models (instead of Grounding DINO) on the quality and quantity of generated data in the W2C pipeline?
- Basis in paper: [explicit] The paper uses Grounding DINO for concept-to-bounding box mapping and mentions that "Grounding DINO introduces new challenges for counting problems" due to overlapping boxes.
- Why unresolved: While Grounding DINO is used in the current implementation, the paper doesn't explore whether alternative object detection models (e.g., YOLO, DETR) might produce better results in terms of concept coverage or counting accuracy.
- What evidence would resolve it: Replacing Grounding DINO with different object detection models in the W2C pipeline and comparing the resulting dataset quality, concept coverage, and downstream task performance.

### Open Question 3
- Question: How does the quality of W2C-generated data vary across different types of images in the ShareGPT4V dataset?
- Basis in paper: [explicit] The paper mentions that "The experiments are mainly conducted on the SOTA open-source VLM structures" without detailed analysis of image category performance.
- Why unresolved: The paper doesn't provide a detailed analysis of how W2C performs across different image categories (e.g., OCR-heavy vs. natural scenes, simple vs. complex compositions), leaving questions about dataset biases or limitations.
- What evidence would resolve it: Analyzing W2C's data generation quality across different image categories in the ShareGPT4V dataset, identifying which types of images produce the most consistent and accurate annotations, and determining what image characteristics correlate with better or worse performance.

## Limitations
- Incomplete experimental details, particularly regarding specific instruct prompts and training hyperparameters
- Heavy reliance on relative comparisons to ShareGPT4V rather than establishing absolute performance baselines
- Limited testing to LLaVA series models, leaving generalization to other VLM architectures uncertain

## Confidence
- **High confidence**: The consistency filtering mechanism and grounding benchmark improvements (1.5-3.5 IoU) are well-supported
- **Medium confidence**: The code format cross-modal alignment claims are supported but based on limited experiments
- **Low confidence**: Claims about achieving high-quality data "without human intervention" depend heavily on undisclosed prompt engineering

## Next Checks
1. Reconstruct exact instruct prompts from Appendix A.1 templates and test pipeline reproducibility across different VLMs
2. Generate a small sample dataset and conduct human evaluation to verify consistency filtering removes hallucinated concepts
3. Implement Python code formatting and conduct controlled experiments comparing it against traditional captions on GQA dataset