---
ver: rpa2
title: Analysing zero-shot temporal relation extraction on clinical notes using temporal
  consistency
arxiv_id: '2406.11486'
source_url: https://arxiv.org/abs/2406.11486
tags:
- temporal
- before
- consistency
- relation
- pairs
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents the first study of zero-shot temporal relation
  extraction in biomedical text. The authors use two prompt strategies (BatchQA and
  Chain-of-Thought) with five LLMs to extract temporal relations between medical events
  in clinical notes.
---

# Analysing zero-shot temporal relation extraction on clinical notes using temporal consistency

## Quick Facts
- arXiv ID: 2406.11486
- Source URL: https://arxiv.org/abs/2406.11486
- Reference count: 17
- Zero-shot LLMs perform worse than supervised methods by ~0.2 F1 score on clinical temporal relation extraction

## Executive Summary
This paper presents the first study of zero-shot temporal relation extraction in biomedical text using large language models (LLMs). The authors employ two prompt strategies (BatchQA and Chain-of-Thought) with five different LLMs to extract temporal relations between medical events in clinical notes. Their experiments demonstrate that LLMs perform worse than supervised methods in terms of F1 score, achieving approximately 0.2 lower performance. The study also conducts a temporal consistency analysis by calculating consistency scores for uniqueness and transitivity properties, revealing that LLMs struggle with temporal consistency and often provide multiple relations for the same event pair.

## Method Summary
The study uses five LLMs (GPT-3.5, Mixtral, Llama 2, Gemma, and PMC-LLaMA) with two prompting strategies (BatchQA and Chain-of-Thought) to extract temporal relations between medical events in clinical notes from the i2b2 2012 dataset. No fine-tuning is performed; instead, the models are prompted directly with clinical text and temporal relation questions. The researchers generate candidate event pairs using a rule-based approach, construct prompts with either BatchQA or CoT strategies, collect LLM responses, map the five relations to three gold relations, and calculate F1 scores and temporal consistency metrics. They also apply Integer Linear Programming (ILP) to enforce temporal consistency on the predictions.

## Key Results
- LLMs perform worse than supervised methods in zero-shot temporal relation extraction, with approximately 0.2 lower F1 scores
- Chain-of-Thought prompting generally improves LLM performance compared to BatchQA, though effectiveness varies by model
- Even when temporal consistency is achieved through ILP enforcement, predictions can remain inaccurate, with F1 scores decreasing slightly or by 0.066 for CoT
- LLMs struggle with temporal properties, often providing multiple relations for the same event pair and violating uniqueness and transitivity constraints

## Why This Works (Mechanism)

### Mechanism 1
Zero-shot LLMs struggle with temporal relation extraction because they lack domain-specific fine-tuning and the reasoning required for temporal properties (uniqueness and transitivity) is not well captured in their pre-training. Without task-specific training, LLMs default to pattern-matching and general language understanding, which is insufficient for capturing the fine-grained temporal semantics needed in clinical texts.

### Mechanism 2
Consistency in predictions (uniqueness and transitivity) does not guarantee accuracy because enforcing global constraints can override locally correct predictions. Temporal constraints are applied globally after local predictions, potentially modifying correct relations to satisfy consistency, thereby reducing overall accuracy.

### Mechanism 3
Chain-of-Thought prompting improves LLM performance on temporal reasoning by explicitly structuring the reasoning process, but it still underperforms supervised models. CoT prompts decompose the task into sub-questions, allowing the model to reason step-by-step, which partially mitigates the absence of fine-tuning.

## Foundational Learning

- **Temporal relation extraction and its properties (uniqueness, transitivity, symmetry)**: Understanding these properties is crucial for designing consistency checks and interpreting LLM predictions in the context of temporal reasoning. Quick check: Can you explain why each pair of events can only have one temporal relation (uniqueness) and how transitivity applies to event ordering?

- **Zero-shot learning and prompt engineering**: The study relies on zero-shot prompting strategies to elicit temporal reasoning from LLMs without fine-tuning, making it essential to understand how prompts influence model behavior. Quick check: What is the difference between BatchQA and Chain-of-Thought prompting, and how might each affect the LLM's ability to reason about temporal relations?

- **Integer Linear Programming (ILP) for constraint satisfaction**: ILP is used to enforce temporal consistency on LLM predictions, requiring an understanding of how to formulate and solve constraint satisfaction problems. Quick check: How would you set up an ILP to maximize prediction confidence while enforcing uniqueness and transitivity constraints?

## Architecture Onboarding

- **Component map**: Data preprocessing -> Candidate pair generation -> Prompt construction (BatchQA/CoT) -> LLM inference -> Consistency scoring -> ILP constraint enforcement -> Evaluation
- **Critical path**: Prompt construction -> LLM inference -> Consistency scoring -> ILP enforcement -> Evaluation
- **Design tradeoffs**: Zero-shot vs. supervised learning (cost vs. accuracy), BatchQA vs. CoT prompting (efficiency vs. reasoning depth), ILP constraint strictness vs. prediction accuracy
- **Failure signatures**: Low F1 scores despite high consistency, ILP enforcement reducing accuracy, LLM predictions violating temporal properties
- **First 3 experiments**:
  1. Run BatchQA and CoT prompts with a single LLM and compare F1 scores and consistency metrics.
  2. Apply ILP to enforce consistency on the predictions from experiment 1 and measure the impact on F1 score.
  3. Test the effect of combining W-order predictions with LLM predictions on recall and F1 score.

## Open Questions the Paper Calls Out

### Open Question 1
Does improving temporal consistency in zero-shot BioTempRE lead to better accuracy in predicting temporal relations between medical events? The study found that enforcing temporal consistency using an ILP method did not improve the F1 score, indicating that temporal consistency does not necessarily correlate with better accuracy.

### Open Question 2
How do different prompt strategies (BatchQA vs. CoT) impact the performance of large language models in zero-shot BioTempRE, and which strategy is more effective for capturing temporal relations in clinical text? While the paper provides some insights into the impact of prompting strategies, it does not definitively determine which strategy is superior for BioTempRE or explore the reasons behind the varying effectiveness across different LLMs.

### Open Question 3
Can the performance of large language models in zero-shot BioTempRE be improved by incorporating domain-specific knowledge, such as pre-training on biomedical text or fine-tuning on clinical data? The study uses a medical LLM (PMC-LLaMA) but finds that it does not outperform general domain LLMs, suggesting that pre-training on biomedical text alone may not be sufficient.

### Open Question 4
How do the temporal properties of uniqueness and transitivity influence the performance of large language models in zero-shot BioTempRE, and what are the challenges in capturing these properties accurately? The paper identifies the challenges in capturing temporal properties but does not delve into the underlying reasons for these difficulties or explore potential solutions.

## Limitations

- **Data and Annotation Quality**: The study relies on the i2b2 2012 dataset, which may contain inherent noise or ambiguity in annotations that could affect LLM performance and consistency analysis.
- **Prompt Sensitivity**: The effectiveness of zero-shot prompting strategies is highly sensitive to prompt design, and the paper does not explore the full space of possible prompts or conduct ablation studies.
- **ILP Constraint Formulation**: The exact formulation of constraints and handling of confidence scores in the ILP implementation are not fully detailed, affecting reproducibility and interpretation.

## Confidence

**High Confidence**:
- LLMs perform worse than supervised methods in zero-shot temporal relation extraction
- Enforcing temporal consistency using ILP can achieve 100% consistency but may not improve accuracy
- Chain-of-Thought prompting generally improves LLM performance compared to BatchQA

**Medium Confidence**:
- LLMs struggle with temporal properties due to lack of fine-tuning
- Higher consistency does not necessarily correlate with better accuracy

**Low Confidence**:
- Specific mechanisms by which CoT prompting improves temporal reasoning
- Impact of combining W-order predictions with LLM predictions on recall

## Next Checks

1. **Prompt Ablation Study**: Conduct a systematic ablation study by modifying individual elements of the BatchQA and CoT prompts to isolate which components are most critical for LLM performance and consistency.

2. **Cross-Dataset Validation**: Test the zero-shot LLMs on a different temporal relation extraction dataset (e.g., TB-Dense or THYME) to assess whether the observed performance gaps and consistency issues generalize beyond the i2b2 dataset.

3. **ILP Constraint Sensitivity Analysis**: Vary the confidence score threshold and the strictness of ILP constraints to determine how sensitive the consistency enforcement is to these parameters and whether a less strict ILP formulation could improve accuracy while maintaining high consistency.