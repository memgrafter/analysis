---
ver: rpa2
title: '1-Bit FQT: Pushing the Limit of Fully Quantized Training to 1-bit'
arxiv_id: '2408.14267'
source_url: https://arxiv.org/abs/2408.14267
tags:
- uni00000013
- uni00000014
- training
- uni00000011
- uni00000018
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work attempts to achieve 1-bit Fully Quantized Training (FQT)
  by pushing the limits of gradient precision. It introduces theoretical analysis
  showing that gradient variance impacts convergence, with Adam being more suitable
  than SGD for low-bitwidth training.
---

# 1-Bit FQT: Pushing the Limit of Fully Quantized Training to 1-bit

## Quick Facts
- arXiv ID: 2408.14267
- Source URL: https://arxiv.org/abs/2408.14267
- Authors: Chang Gao; Jianfei Chen; Kang Zhao; Jiaqi Wang; Liping Jing
- Reference count: 40
- Key outcome: Achieves 1-bit fully quantized training with ~6% accuracy improvement over per-sample quantization and up to 5.13× training speedup

## Executive Summary
This work pushes fully quantized training (FQT) to 1-bit precision by addressing the fundamental challenge of gradient variance. The authors provide theoretical analysis showing that gradient variance significantly impacts convergence, with Adam being more suitable than SGD for low-bitwidth regimes. They introduce Activation Gradient Pruning (AGP) to reduce variance by pruning less informative gradients and Sample Channel joint Quantization (SCQ) to ensure hardware compatibility. Experiments demonstrate an average 6% accuracy improvement compared to existing methods.

## Method Summary
The paper proposes a 1-bit FQT framework combining AGP and SCQ. AGP identifies and prunes gradient groups with smaller ranges to reduce variance, then uses the saved resources to enhance precision for remaining groups. SCQ ensures the quantization scheme is compatible with low-bitwidth hardware by jointly considering sample and channel dimensions. The method uses binary matrix multiplication (XNOR and bitcount operations) for efficient computation. Adam optimizer is preferred over SGD due to its lower sensitivity to gradient variance.

## Key Results
- Average 6% accuracy improvement compared to per-sample quantization on transfer learning tasks
- Maximum training speedup of 5.13× compared to full precision training
- 1-bit FQT achieves competitive performance while maintaining hardware compatibility

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Gradient variance influences the convergence of Fully Quantized Training (FQT).
- Mechanism: High variance in quantized gradients increases error in weight updates, leading to slower or unstable convergence. Quantizers with larger quantization bins (like 1-bit) have higher variance, exacerbating the problem.
- Core assumption: Unbiased quantizers can be used without systematic bias, and variance is the main source of error.
- Evidence anchors:
  - [abstract] "...revealing that the gradient variance influences the convergence of FQT."
  - [section 4] "From the inquation, it is straightforward to conclude that RSGD (T) T = O(σ2) + O(1), RAdam(T) T = O(σ) + O(1). This implies that the convergence of FQT based on both Adam and SGD is influenced by the gradient variance..."

### Mechanism 2
- Claim: Adam is more suitable than SGD for low-bitwidth FQT due to different sensitivity to gradient variance.
- Mechanism: Adam normalizes gradients by their variance, which mitigates the impact of high gradient variance. SGD is more sensitive to variance because it directly scales gradients by the learning rate.
- Core assumption: Adam's adaptive learning rate based on second moments effectively reduces the negative impact of gradient variance.
- Evidence anchors:
  - [abstract] "...our analysis reveals that Adam is more suitable for FQT than SGD in the low-bitwidth regime, due to their different sensitivity to gradient variance."
  - [section 4] Theoretical bounds show RAdam(T) / T = O(σ) + O(1), while RSGD (T) / T = O(σ2) + O(1).

### Mechanism 3
- Claim: Activation Gradient Pruning (AGP) reduces gradient variance by removing groups with smaller ranges and improving the precision of remaining groups.
- Mechanism: Gradients exhibit heterogeneity, with some samples/groups having large ranges and others having small ranges. Pruning groups with small ranges reduces the overall range, thus reducing variance. Resources saved are used to increase the precision of remaining groups.
- Core assumption: Groups with smaller ranges contribute less information and can be pruned without significant loss.
- Evidence anchors:
  - [abstract] "...introduces an Activation Gradient Pruning (AGP) strategy. The strategy leverages the heterogeneity of gradients by pruning less informative gradients and enhancing the numerical precision of remaining gradients to mitigate gradient variance."
  - [section 5.2] "To mitigate the impact of outliers on variance, per-group quantization is widely employed... However, the variance of PSQ is still too large for 1-bit FQT."

## Foundational Learning

- Concept: Unbiased stochastic quantization
  - Why needed here: Ensures that the quantized gradient is an unbiased estimate of the original gradient, allowing the model to converge to the same point as full-precision training.
  - Quick check question: What property must a quantizer have to ensure unbiased gradients in FQT?

- Concept: Per-group quantization
  - Why needed here: Reduces gradient variance by assigning separate ranges to each group instead of sharing a large range, which is crucial for low-bitwidth training.
  - Quick check question: How does per-group quantization help mitigate the impact of outliers in gradient values?

- Concept: Binary matrix multiplication
  - Why needed here: Enables efficient computation of 1-bit FQT by replacing standard matrix multiplication with XNOR and bitcount operations, significantly reducing computation time.
  - Quick check question: What operations replace standard matrix multiplication in 1-bit FQT?

## Architecture Onboarding

- Component map:
  Forward pass: Binary weights, activations, and deterministic quantization
  Backward pass: Activation Gradient Pruning (AGP), Sample Channel joint Quantization (SCQ), binary matrix multiplication
  Hardware: Low-bitwidth computing units (e.g., XNOR, bitcount)
  Optimizer: Adam (preferred due to lower sensitivity to gradient variance)

- Critical path:
  1. Forward pass: Quantize weights and activations, perform binary matrix multiplication
  2. Backward pass: Compute gradients, apply AGP to reduce variance, apply SCQ for hardware compatibility, perform binary matrix multiplication
  3. Update weights: Apply Adam optimizer with quantized gradients

- Design tradeoffs:
  - Precision vs. variance: Lower precision (1-bit) increases variance, requiring techniques like AGP to mitigate
  - Hardware compatibility vs. accuracy: SCQ ensures compatibility with low-bitwidth hardware but may introduce additional complexity
  - Pruning vs. information loss: AGP prunes less informative groups but may discard useful information if heterogeneity is not present

- Failure signatures:
  - High gradient variance leading to unstable training
  - Significant accuracy drop compared to full-precision training
  - Incompatibility with target hardware due to quantization strategy
  - Inability to converge when using SGD instead of Adam

- First 3 experiments:
  1. Train a simple model (e.g., VGGNet-16) on CIFAR-10 with 1-bit FQT using Adam optimizer and AGP, measure accuracy and training time
  2. Compare the performance of Adam vs. SGD on the same model and dataset to validate the theory on optimizer sensitivity to gradient variance
  3. Implement and test SCQ to ensure compatibility with low-bitwidth hardware and measure any impact on accuracy or training time

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the variance of the gradient quantizer impact the convergence of 1-bit FQT in scenarios beyond transfer learning, such as training from scratch?
- Basis in paper: [explicit] The paper discusses gradient variance's impact on convergence in 1-bit FQT and notes that training from scratch remains an open problem.
- Why unresolved: The paper only evaluates 1-bit FQT on transfer learning tasks and does not explore its performance in training from scratch, where gradient variance might be more pronounced.
- What evidence would resolve it: Experiments comparing 1-bit FQT on training from scratch versus transfer learning, measuring convergence speed and final accuracy.

### Open Question 2
- Question: What is the optimal hyperparameter configuration (e.g., b value, learning rate) for 1-bit FQT across different model architectures and datasets?
- Basis in paper: [explicit] The paper investigates the impact of the hyperparameter b on performance but only reports results for specific configurations.
- Why unresolved: The paper only evaluates a limited set of hyperparameter configurations and does not provide a comprehensive analysis of the optimal settings for different scenarios.
- What evidence would resolve it: A systematic hyperparameter search across various model architectures and datasets, reporting the optimal configurations and their impact on accuracy and training speed.

### Open Question 3
- Question: How does the computational efficiency of 1-bit FQT compare to other low-bitwidth training methods (e.g., 4-bit FQT) in terms of both training speed and model accuracy?
- Basis in paper: [explicit] The paper compares the training speedup of 1-bit FQT to FP32 training and 8-bit PSQ, but does not directly compare it to other low-bitwidth methods like 4-bit FQT.
- Why unresolved: The paper focuses on demonstrating the potential of 1-bit FQT but does not provide a comprehensive comparison to other low-bitwidth training methods in terms of both speed and accuracy.
- What evidence would resolve it: Experiments comparing the training speed and final accuracy of 1-bit FQT, 4-bit FQT, and other low-bitwidth methods on the same tasks and hardware.

## Limitations

- Theoretical analysis relies on idealized assumptions about gradient distributions that may not hold in practice
- Effectiveness of AGP depends critically on the presence of gradient heterogeneity, which may vary across architectures
- Hardware compatibility claims need validation beyond theoretical framework on diverse platforms

## Confidence

- High Confidence: The theoretical framework showing gradient variance impacts convergence and that Adam is more suitable than SGD for low-bitwidth training
- Medium Confidence: The effectiveness of Activation Gradient Pruning in reducing variance and improving accuracy
- Medium Confidence: The Sample Channel joint Quantization method ensuring hardware compatibility

## Next Checks

1. **Cross-architecture validation**: Test the 1-bit FQT approach on transformer-based architectures (e.g., BERT, ViT) to verify if gradient heterogeneity patterns hold and AGP remains effective across different model families.

2. **Hardware compatibility testing**: Implement SCQ on actual low-bitwidth hardware (e.g., binary neural network accelerators) to measure real-world training speedup and verify compatibility claims beyond theoretical analysis.

3. **Gradient variance ablation study**: Systematically vary the degree of gradient pruning in AGP to quantify the tradeoff between variance reduction and information loss, determining the optimal pruning threshold for different task complexities.