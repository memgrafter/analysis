---
ver: rpa2
title: 'Learning from Committee: Reasoning Distillation from a Mixture of Teachers
  with Peer-Review'
arxiv_id: '2410.03663'
source_url: https://arxiv.org/abs/2410.03663
tags:
- student
- llms
- reasoning
- teacher
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a Mistake-Aware Peer-Review Distillation (MAPD)
  method to improve reasoning abilities of smaller language models through knowledge
  distillation from multiple LLMs. The approach collects incorrect student responses,
  generates tailored feedback and rationales from multiple teacher LLMs, and uses
  a simulated peer-review process to filter high-quality instructional data.
---

# Learning from Committee: Reasoning Distillation from a Mixture of Teachers with Peer-Review

## Quick Facts
- arXiv ID: 2410.03663
- Source URL: https://arxiv.org/abs/2410.03663
- Authors: Zhuochun Li; Yuelyu Ji; Rui Meng; Daqing He
- Reference count: 18
- One-line primary result: MAPD improves Llama2-7B performance from 16.55% to 36.24% on GSM8K, 44.71% to 59.50% on SV AMP, 48.53% to 67.67% on StrategyQA, and 16.50% to 36.27% on LogiQA

## Executive Summary
This paper introduces Mistake-Aware Peer-Review Distillation (MAPD), a novel approach for improving reasoning abilities of smaller language models through knowledge distillation from multiple LLMs. The method addresses a key limitation of existing distillation approaches by collecting incorrect student responses, generating tailored feedback and rationales from multiple teacher LLMs, and using a simulated peer-review process to filter high-quality instructional data. Experiments demonstrate significant performance improvements across mathematical, commonsense, and logical reasoning tasks compared to baseline methods.

## Method Summary
MAPD involves a multi-stage process where a student model first takes an "exam" on training data to identify its mistakes. Multiple teacher LLMs then generate both correct rationales and specific feedback about the student's errors. These rationales undergo a simulated peer-review process where each teacher LLM reviews and scores the others' outputs, with only high-scoring rationales being included in the final training dataset. The student model is then instruction-tuned using a combined loss function that balances learning from correct rationales and mistake-specific feedback. The approach uses cross-entropy loss for both learning objectives and leverages multiple teacher models to increase diversity and reduce biased rationales.

## Key Results
- MAPD improves Llama2-7B performance from 16.55% to 36.24% on GSM8K
- MAPD improves Llama2-7B performance from 44.71% to 59.50% on SV AMP
- MAPD improves Llama2-7B performance from 48.53% to 67.67% on StrategyQA
- MAPD improves Llama2-7B performance from 16.50% to 36.27% on LogiQA

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The simulated peer-review process between multiple teacher LLMs reduces the likelihood of flawed rationales even when the final answer is correct.
- Mechanism: When multiple teacher LLMs generate rationales for the same question, each rationale is reviewed by the other teachers. Only rationales that receive scores above a threshold from the peer-review process are included in the training dataset. This filtering mechanism ensures that only high-quality, well-reasoned rationales are passed to the student model.
- Core assumption: Multiple teacher LLMs can effectively identify and filter out flawed rationales through peer-review, even when those rationales happen to lead to correct answers.
- Evidence anchors:
  - [abstract] "we employ multiple LLMs as teachers and ask them the same question. Each teacher LLM's answer is reviewed by the other teachers, and only the responses that pass this peer-review process are included in the instruction training dataset."
  - [section 3.3] "We obtain their rationales... For each rationale, we incorporate it into the designed peer-review prompt... Only the rationale with an average score exceeding the acceptance threshold will be included in the rationale set"
- Break condition: If the peer-review threshold is set too low, flawed rationales might still pass through. If set too high, the dataset size might become too small to be effective.

### Mechanism 2
- Claim: Learning from both correct rationales and mistake-specific feedback provides more comprehensive instruction than learning from correct answers alone.
- Mechanism: The student model first takes an "exam" to identify its mistakes, then teacher LLMs are prompted to generate both the correct rationale and specific feedback about the student's errors. This dual approach helps the student understand not just what the right answer is, but why their original approach was wrong.
- Core assumption: Students benefit more from understanding their mistakes than from simply being shown the correct answer.
- Evidence anchors:
  - [abstract] "our method asks teachers to identify and explain the student's mistakes, providing customized instruction learning data"
  - [section 3.4] "we propose that the student model should also learn from its own mistakes, simulating the typical human learning process"
- Break condition: If the mistake feedback provided by teachers is too generic or not specific to the student's actual errors, the learning benefit may be minimal.

### Mechanism 3
- Claim: Multiple teacher LLMs provide more diverse and less biased training data compared to a single teacher LLM.
- Mechanism: Different LLMs have different reasoning patterns and biases. By using multiple teachers and aggregating their outputs through peer-review, the resulting training data is more representative of diverse reasoning approaches and less influenced by any single model's biases.
- Core assumption: Different LLMs have sufficiently different reasoning patterns that their combination provides better coverage than any single model.
- Evidence anchors:
  - [abstract] "we employ multiple LLMs as teachers and ask them the same question... we believe this peer-review mechanism between teacher LLMs can significantly reduce biased or flawed rationales"
  - [section 3.3] "Unlike most studies that rely on a single teacher LLM... we employ multiple LLMs as teachers to increase the diversity of generated data"
- Break condition: If all teacher LLMs share similar biases or reasoning patterns, the diversity benefit may be minimal.

## Foundational Learning

- Concept: Cross-entropy loss function
  - Why needed here: The paper uses cross-entropy loss (CE) for both learning from gold rationales (Lgr) and learning from mistake feedback (Lmf) in the instruction tuning process.
  - Quick check question: How does cross-entropy loss measure the difference between the student model's output distribution and the target distribution in classification tasks?

- Concept: Chain-of-Thought (CoT) prompting
  - Why needed here: The paper builds on CoT prompting techniques where LLMs generate intermediate reasoning steps to improve performance on reasoning tasks.
  - Quick check question: Why does breaking down complex reasoning problems into intermediate steps often improve LLM performance on tasks like GSM8K?

- Concept: Knowledge distillation
  - Why needed here: The entire paper is about knowledge distillation from large teacher LLMs to a smaller student model, transferring reasoning capabilities.
  - Quick check question: What are the key differences between response-based knowledge distillation and rationale-based knowledge distillation?

## Architecture Onboarding

- Component map: Student model exam -> Teacher rationale and feedback generation -> Peer-review filtering -> Instruction tuning with combined loss function
- Critical path: Student model exam → Teacher rationale and feedback generation → Peer-review filtering → Instruction tuning with combined loss function
- Design tradeoffs: The paper balances learning from mistakes (α parameter) and uses multiple teachers which increases computational cost but improves data quality. The peer-review threshold trades off between data quantity and quality.
- Failure signatures: If the student model performance doesn't improve after distillation, possible causes include: peer-review threshold too high/low, teacher LLMs not providing specific enough feedback, or the exam didn't properly identify student weaknesses.
- First 3 experiments:
  1. Run the student model exam on a small subset of training data to verify the mistake identification mechanism works correctly.
  2. Test the teacher LLM rationale generation with a single question to ensure the prompt templates produce the expected format.
  3. Run a peer-review process with two teacher LLMs on a single question to verify the scoring mechanism works as intended before scaling to the full dataset.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How would the MAPD method perform with larger teacher models like GPT-4, OpenAI o1, or Claude-3 Opus?
- Basis in paper: [explicit] The paper acknowledges this as a limitation, stating that due to cost constraints, experiments primarily used GPT-3.5-turbo, Gemini-1.0-pro, and Mixtral-8x7B-Instruct-v0.1 as teacher models, and suggests future research could benefit from using more powerful models.
- Why unresolved: The paper explicitly notes this limitation and suggests future research could explore more powerful models, indicating that the performance of MAPD with larger models remains untested.
- What evidence would resolve it: Running the MAPD method with GPT-4, OpenAI o1, or Claude-3 Opus as teachers and comparing the results with the current implementation would provide evidence of the method's performance with larger models.

### Open Question 2
- Question: What is the impact of continuously incorporating fresh data throughout training, rather than using a fixed dataset from the initial exam?
- Basis in paper: [explicit] The paper mentions this as a limitation, stating that due to time and cost constraints, the method does not collect the student LM's incorrect rationales and update the instruction dataset after each epoch, and notes that the potential benefits of continuously incorporating fresh data remain unexplored.
- Why unresolved: The paper explicitly states that this aspect was not explored due to time and cost constraints, leaving the potential benefits of continuous data updating untested.
- What evidence would resolve it: Implementing a version of MAPD that updates the instruction dataset after each epoch based on the student's new mistakes and comparing its performance with the fixed dataset approach would provide evidence of the impact of continuous data incorporation.

### Open Question 3
- Question: How would the performance of MAPD change if more sophisticated loss functions like Reinforcement Learning with Human Feedback (RLHF) were used instead of the default cross-entropy loss?
- Basis in paper: [explicit] The paper mentions this as a limitation, stating that the default cross-entropy loss function was employed for instruction tuning and suggests that exploring more sophisticated methods like RLHF and integrating additional techniques into the joint learning framework would be worthwhile.
- Why unresolved: The paper explicitly notes that the default cross-entropy loss was used and suggests exploring more sophisticated methods, indicating that the impact of using alternative loss functions remains untested.
- What evidence would resolve it: Implementing MAPD with RLHF or other sophisticated loss functions and comparing the results with the cross-entropy loss implementation would provide evidence of the impact of different loss functions on performance.

## Limitations

- The paper does not disclose the exact prompt templates used for teacher rationale generation and peer-review, making reproducibility challenging
- The computational overhead of using multiple teacher LLMs and running peer-review processes is substantial but not quantified
- The evaluation is limited to reasoning tasks, with no assessment of whether the method maintains performance on non-reasoning tasks

## Confidence

**High confidence** in the following claims:
- MAPD improves student model performance on the tested reasoning benchmarks compared to baseline distillation methods
- The peer-review mechanism reduces the inclusion of flawed rationales in the training data
- Learning from both correct rationales and mistake-specific feedback provides benefits beyond learning from correct answers alone

**Medium confidence** in the following claims:
- The peer-review mechanism between teacher LLMs effectively identifies and filters out flawed rationales
- The improvements generalize across different types of reasoning tasks (mathematical, commonsense, logical)
- The method scales effectively to larger datasets and different student model sizes

**Low confidence** in the following claims:
- The specific peer-review threshold value is optimal across all tasks
- The method would maintain effectiveness with different teacher LLM combinations
- The computational overhead is justified by the performance gains

## Next Checks

1. **Ablation study on peer-review threshold**: Systematically vary the peer-review acceptance threshold across multiple reasoning tasks to determine the optimal balance between data quality and quantity. This would help validate whether the threshold choice significantly impacts performance.

2. **Teacher model dependency analysis**: Replace one or more teacher LLMs with alternatives (e.g., Claude-3, Llama-2-70B) to test whether the improvements are robust to different teacher combinations, or whether performance degrades significantly with certain teacher pairings.

3. **Zero-shot generalization test**: Evaluate the distilled student model on out-of-distribution reasoning tasks not seen during training to assess whether the improvements generalize beyond the specific benchmark datasets used in the paper.