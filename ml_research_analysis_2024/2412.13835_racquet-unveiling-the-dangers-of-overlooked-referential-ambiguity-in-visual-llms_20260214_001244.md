---
ver: rpa2
title: 'RAcQUEt: Unveiling the Dangers of Overlooked Referential Ambiguity in Visual
  LLMs'
arxiv_id: '2412.13835'
source_url: https://arxiv.org/abs/2412.13835
tags:
- image
- ambiguity
- person
- questions
- responses
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper investigates referential ambiguity in visual question
  answering, a critical challenge in human communication often overlooked by current
  multimodal models. The authors introduce RACQUET, a curated dataset of 740 image-question
  pairs designed to test how well vision-language models handle ambiguity when multiple
  entities could be referenced.
---

# RAcQUEt: Unveiling the Dangers of Overlooked Referential Ambiguity in Visual LLMs

## Quick Facts
- arXiv ID: 2412.13835
- Source URL: https://arxiv.org/abs/2412.13835
- Reference count: 40
- Current VLMs fail to recognize referential ambiguity and often respond with overconfidence

## Executive Summary
The paper investigates referential ambiguity in visual question answering, a critical challenge in human communication often overlooked by current multimodal models. The authors introduce RACQUET, a curated dataset of 740 image-question pairs designed to test how well vision-language models handle ambiguity when multiple entities could be referenced. Evaluations reveal that humans typically acknowledge ambiguity explicitly, while models often respond with overconfidence, describing only one referent without recognition of ambiguity. This overconfidence is particularly problematic in RACQUET-BIAS, where stereotypical responses are prevalent. Chain-of-thought prompting shows some promise in revealing awareness of multiple referents during reasoning, though final answers still often fail to address ambiguity.

## Method Summary
The study evaluates referential ambiguity handling using RACQUET, a curated dataset of 740 image-question pairs split into RACQUET-GENERAL (real images) and RACQUET-BIAS (AI-generated images with controlled bias triggers). The authors conduct zero-shot evaluations on proprietary models (GPT-4o, Gemini 1.5 Pro) and open models (LLaVA-34B, Molmo 7B-D, LLaVA-7B, MolmoE 1B, Qwen-VL-Chat), classifying responses into Explicit, Implicit, and High Risk categories. Additional experiments include chain-of-thought prompting, clarification prompting, and saliency analysis using SAM to map model responses to referents.

## Key Results
- Humans acknowledge referential ambiguity in 91.6% of cases in RACQUET-GENERAL, while models only do so in 14.2% of cases
- RACQUET-BIAS reveals high prevalence of stereotypical responses across all models when ambiguity is unrecognized
- Chain-of-thought prompting increases mentions of multiple referents during reasoning (69% for GPT-4o) but final answers rarely acknowledge ambiguity

## Why This Works (Mechanism)

### Mechanism 1
- Claim: RACQUET's curated dataset reveals that current VLMs fail to recognize referential ambiguity and often respond with overconfidence.
- Mechanism: By isolating referential ambiguity—where multiple referents are present but the question targets a single, unspecified entity—the dataset forces models to either acknowledge ambiguity or default to overconfident descriptions of one referent.
- Core assumption: Ambiguity recognition is necessary for socially unbiased responses.
- Evidence anchors:
  - [abstract] "Through a series of evaluations, we reveal significant limitations and problems of overconfidence of state-of-the-art large multimodal language models in addressing ambiguity in their responses."
  - [section 4.2] "models, on the other hand, tend to be overly confident, acknowledging ambiguity in a minority of instances."
  - [corpus] Weak: corpus only lists related papers, not direct evidence for mechanism.

### Mechanism 2
- Claim: RACQUET-BIAS specifically shows that failing to recognize ambiguity leads to stereotypical responses.
- Mechanism: By constructing images where two individuals differ in one social category (gender, ethnicity, disability) and asking ambiguous questions about an attribute tied to stereotypes, the dataset exposes how models project stereotypes when ambiguity is not addressed.
- Core assumption: Stereotypical interpretation is the default when ambiguity is unrecognized.
- Evidence anchors:
  - [abstract] "The overconfidence issue becomes particularly relevant for RACQUET-BIAS, a subset designed to analyze a critical yet underexplored problem: failing to address ambiguity leads to stereotypical, socially biased responses."
  - [section 6] "A particularly striking finding is the high prevalence of Stereotypical responses across all models."
  - [corpus] Weak: corpus only lists related papers, not direct evidence for mechanism.

### Mechanism 3
- Claim: Chain-of-thought (CoT) prompting reveals awareness of multiple referents during reasoning but does not always translate to final responses.
- Mechanism: When models are asked to explain reasoning step-by-step, they often mention multiple referents, indicating awareness, but final answers may still ignore this awareness.
- Core assumption: Internal reasoning awareness does not guarantee socially responsible final outputs.
- Evidence anchors:
  - [section 4.3] "an analysis of the intermediate reasoning steps reveals that 69% of GPT-4o's CoT responses mention the presence of multiple referents."
  - [abstract] "Chain-of-thought prompting shows some promise in revealing awareness of multiple referents during reasoning, though final answers still often fail to address ambiguity."
  - [corpus] Weak: corpus only lists related papers, not direct evidence for mechanism.

## Foundational Learning

- Concept: Referential ambiguity in language
  - Why needed here: To understand why ambiguous questions are difficult for models to handle without additional context.
  - Quick check question: In the sentence "Where's the bus headed?" what makes the question ambiguous if multiple buses are present?

- Concept: Social stereotypes and their visual association
  - Why needed here: To grasp why ambiguous questions about social categories can lead to biased model outputs.
  - Quick check question: Why might a model assume "assertive" refers to a male-presenting person in an image?

- Concept: Chain-of-thought reasoning in VLMs
  - Why needed here: To understand how intermediate reasoning steps can reveal model awareness even when final answers do not.
  - Quick check question: How does CoT prompting differ from direct question answering in terms of model output?

## Architecture Onboarding

- Component map: Dataset curation -> Ambiguity annotation -> Model response generation -> Response classification -> Analysis of bias/stereotypes
- Critical path:
  1. Curate RACQUET dataset with ambiguous questions.
  2. Generate model responses using various VLMs.
  3. Classify responses into Explicit, Implicit, High Risk (or Stereotypical/Anti-stereotypical for RACQUET-BIAS).
  4. Analyze patterns of overconfidence and bias.
- Design tradeoffs:
  - Using real-world images vs. AI-generated images: Real images are more diverse but harder to control for saliency; AI-generated images allow precise control but may lack ecological validity.
  - Manual annotation vs. automated evaluation: Manual annotation is more reliable but less scalable; automated evaluation is faster but may miss nuanced ambiguities.
- Failure signatures:
  - High rate of Class C (High Risk) responses indicates overconfidence.
  - High rate of Stereotypical responses in RACQUET-BIAS indicates failure to address ambiguity.
  - Low agreement between human and model annotations suggests models struggle with ambiguity recognition.
- First 3 experiments:
  1. Run RACQUET-GENERAL on a new VLM and compare response class distribution to human responses.
  2. Apply clarification prompting to RACQUET-BIAS and measure change in Stereotypical vs. Anti-stereotypical responses.
  3. Use SAM to map model responses to referents and quantify influence of saliency features.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can conversational grounding strategies be effectively integrated into VLMs to improve their handling of referential ambiguity?
- Basis in paper: [explicit] The paper highlights that humans naturally use conversational grounding strategies, such as asking clarifying questions or providing multiple referent descriptions, to address ambiguity, while models often fail to do so.
- Why unresolved: The paper shows that current VLMs rarely employ such strategies, and even when prompted, they often only describe multiple referents without explicitly acknowledging ambiguity. The effectiveness of fine-tuning or multi-turn protocols to encourage clarification behavior remains unexplored.
- What evidence would resolve it: Experiments comparing VLM performance with and without fine-tuning on datasets that explicitly train for ambiguity acknowledgment, or testing interactive protocols where models are encouraged to ask clarifying questions.

### Open Question 2
- Question: What specific saliency features beyond size and proximity to center influence VLM referent selection in ambiguous contexts?
- Basis in paper: [inferred] The paper investigates size and proximity to center as saliency features but acknowledges these are not exhaustive, noting that other factors like foreground/background distinctions could play a role.
- Why unresolved: The analysis is limited to two measurable features due to technical constraints, leaving open the question of whether other visual saliency cues (e.g., color contrast, texture, depth) systematically bias VLM responses.
- What evidence would resolve it: Systematic ablation studies varying different saliency attributes (e.g., brightness, edge sharpness, depth ordering) while keeping others constant, to identify which features most strongly influence referent selection.

### Open Question 3
- Question: Does the use of Chain-of-Thought prompting genuinely improve VLM awareness of ambiguity, or does it merely reveal awareness that is not acted upon in final responses?
- Basis in paper: [explicit] The paper finds that CoT prompting increases mentions of multiple referents during reasoning steps, but final answers still rarely acknowledge ambiguity, suggesting a disconnect between reasoning and output.
- Why unresolved: The paper does not explore whether this reasoning awareness can be leveraged through techniques like self-improvement or reinforcement learning to translate intermediate reasoning into final responses that address ambiguity.
- What evidence would resolve it: Experiments applying self-improvement algorithms (e.g., STaR) to CoT reasoning, or training models to align their final answers with their intermediate reasoning about ambiguity.

### Open Question 4
- Question: How do cultural differences in stereotypical interpretations affect VLM responses to ambiguous questions in RACQUET-BIAS?
- Basis in paper: [explicit] The paper acknowledges that stereotypes can vary across cultures but bases its analysis on a fixed set of stereotypical interpretations from prior research, without testing cultural variability.
- Why unresolved: The study uses a Western-centric view of stereotypes and does not account for how VLMs trained on diverse data might respond differently across cultural contexts, or how stereotypes might shift in non-Western populations.
- What evidence would resolve it: Cross-cultural studies evaluating VLM responses to RACQUET-BIAS using annotators from different cultural backgrounds, or testing VLMs trained on non-Western datasets for shifts in stereotypical responses.

## Limitations
- The RACQUET dataset contains only 740 examples, which may not fully capture the diversity of real-world ambiguous scenarios
- RACQUET-BIAS uses AI-generated images that may not fully represent natural social contexts
- Response classification relies on human annotation which may introduce subjectivity in distinguishing between Explicit, Implicit, and High Risk responses

## Confidence
- High confidence: Models exhibit overconfidence and fail to recognize referential ambiguity in RACQUET-GENERAL
- Medium confidence: Chain-of-thought prompting reveals awareness of multiple referents during reasoning steps
- Medium confidence: RACQUET-BIAS successfully demonstrates that unrecognized ambiguity leads to stereotypical responses

## Next Checks
1. Test RACQUET on additional VLMs beyond those evaluated in the paper, particularly newer models and open-source alternatives
2. Conduct a follow-up study using real-world ambiguous scenarios from conversational datasets to validate whether RACQUET findings generalize
3. Implement the STaR self-improvement method mentioned in the paper to determine whether iterative refinement can help models better translate internal reasoning awareness into socially responsible final responses