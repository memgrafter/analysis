---
ver: rpa2
title: Offline Actor-Critic Reinforcement Learning Scales to Large Models
arxiv_id: '2402.05546'
source_url: https://arxiv.org/abs/2402.05546
tags:
- data
- learning
- offline
- task
- which
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work demonstrates that offline actor-critic reinforcement
  learning can scale to large models like transformers and follows similar scaling
  laws to supervised learning. The authors introduce a Perceiver-based actor-critic
  model that can handle multimodal inputs and large action spaces, and show that offline
  actor-critic methods outperform strong behavioral cloning baselines on 132 continuous
  control tasks.
---

# Offline Actor-Critic Reinforcement Learning Scales to Large Models

## Quick Facts
- arXiv ID: 2402.05546
- Source URL: https://arxiv.org/abs/2402.05546
- Reference count: 40
- One-line primary result: Offline actor-critic methods can scale to large transformer models and follow similar scaling laws to supervised learning

## Executive Summary
This work demonstrates that offline actor-critic reinforcement learning can scale to large transformer models and follows similar scaling laws to supervised learning. The authors introduce a Perceiver-based actor-critic model that handles multimodal inputs and large action spaces, showing that offline actor-critic methods outperform behavioral cloning baselines on 132 continuous control tasks. The method enables a gradual transition between BC and RL while processing multimodal data efficiently for real-time control.

## Method Summary
The method uses a Perceiver-Actor-Critic (PAC) architecture that combines KL-regularized offline RL with a Perceiver backbone for efficient multimodal processing. The algorithm trains a policy and Q-function using data from multiple sources, starting with pure behavioral cloning (α=1) and gradually reducing α to enable reinforcement learning. The Perceiver architecture uses cross-attention to efficiently summarize multimodal inputs into a small latent set, which is then processed by self-attention layers. The policy and Q-value decoders use cross-attention to attend to the latent representations and output action distributions and distributional Q-values respectively.

## Key Results
- PAC outperforms BC baselines, achieving 87.7% of expert performance on Control Suite tasks and 92.1% with optimal tuning
- PAC can learn from sub-optimal data, improving from 28% to 93.2% success on a real robot stacking task
- Scaling analysis shows PAC benefits more from additional parameters than BC, with optimal model sizes of 954M-1.33B parameters for the dataset

## Why This Works (Mechanism)

### Mechanism 1
- Claim: KL-regularized actor-critic enables smooth interpolation between behavioral cloning and reinforcement learning, stabilizing large-scale offline RL
- Mechanism: By introducing a temperature-scaled entropy-regularized RL objective, the algorithm blends BC and RL via the α parameter. When α is high, the policy is heavily regularized toward the behavior policy; when α is low, the policy is optimized more directly against the learned Q-function
- Core assumption: A single KL divergence term is sufficient to prevent overestimation of out-of-distribution actions
- Evidence anchors:
  - [abstract] "simple offline actor critic algorithms are a natural choice for gradually moving away from the currently predominant paradigm of behavioral cloning"
  - [section 3.2] "we find that regularizing the policy towards the data distribution (via BC) is sufficient to stabilize offline RL for large models"
- Break condition: If the data contains large portions of very poor-quality or adversarial samples, the KL regularization may be insufficient to prevent exploitation of overestimated Q-values

### Mechanism 2
- Claim: Perceiver-style cross-attention enables efficient processing of multimodal inputs at scale, allowing 1B-parameter models to run at 20Hz for real-time control
- Mechanism: The model first encodes each modality into embedding vectors, then uses a small number of learned latent queries to cross-attend to the full input sequence. This shifts the quadratic self-attention cost from the full input sequence to the much smaller latent set
- Core assumption: The latent queries can sufficiently summarize the rich multimodal input information for downstream policy and Q-value decoding
- Evidence anchors:
  - [section 3.3] "this effectively reduces the computation and memory usage to O(NZ²)"
  - [section 3.3] "This effectively reduces the computation and memory usage to O(NZ²)"
- Break condition: If the latent set is too small relative to the complexity of the input, important information may be lost

### Mechanism 3
- Claim: Scaling laws for offline actor-critic methods mirror those of supervised learning, enabling predictable performance gains with increased model size and compute
- Mechanism: The return profile of the offline actor-critic model follows a logistic growth pattern as training steps increase, and the number of optimal parameters and tokens consumed follow power-law relationships with compute
- Core assumption: The offline RL objective exhibits smooth, predictable scaling behavior similar to next-token prediction in supervised settings
- Evidence anchors:
  - [section 4.1] "we etablish, for the first time, that they follow similar scaling laws to those observed in the supervised learning regime"
  - [section 4.1] "The scaling laws are different for the BC and offline RL settings"
- Break condition: If the data quality varies drastically across tasks, or if the reward structure is highly non-stationary, the scaling laws may not hold

## Foundational Learning

- Concept: Markov Decision Processes (MDPs) and the Bellman equation
  - Why needed here: The actor-critic algorithm relies on estimating Q-values and value functions, which are defined in terms of the Bellman equation and MDP dynamics
  - Quick check question: What is the difference between the state-value function V(s) and the action-value function Q(s,a)?

- Concept: KL divergence and its use as a regularization term
  - Why needed here: The algorithm uses KL divergence to regularize the policy toward the behavior policy (BC term) and to measure divergence between learned and target Q-value distributions
  - Quick check question: How does the KL divergence term prevent overestimation in offline RL?

- Concept: Cross-attention and self-attention in transformer architectures
  - Why needed here: The Perceiver architecture uses cross-attention to efficiently summarize multimodal inputs into latents, and self-attention to process these latents
  - Quick check question: What is the computational complexity of self-attention vs cross-attention, and why does this matter for scaling?

## Architecture Onboarding

- Component map: Modality encoders (proprioception, vision, language) -> Perceiver backbone (cross-attention + self-attention on latents) -> Policy decoder (cross-attention + action distribution output) -> Q-value decoder (cross-attention + distributional Q-value output) -> Target networks and TD learning updates

- Critical path:
  1. Encode multimodal inputs into embeddings
  2. Cross-attend embeddings to latents (Perceiver)
  3. Self-attend latents to form context representation
  4. Decode policy logits from latents
  5. Decode Q-value logits from latents + action encoding
  6. Update policy and Q-value networks using KL losses

- Design tradeoffs:
  - Larger latent sets increase expressiveness but also computation
  - More bins in Q-value discretization improve resolution but increase output dimension
  - Higher α favors stability (more BC) but may limit improvement; lower α risks overestimation

- Failure signatures:
  - Degraded performance with small latent sets (insufficient input summarization)
  - Training instability when α is too low (overfitting to Q-function errors)
  - Slow inference when action cross-attention is not cached

- First 3 experiments:
  1. Train a small model (32M params) on a subset of Control Suite tasks, varying α from 0.0 to 1.0, and compare BC vs RL performance
  2. Ablation: Replace Perceiver backbone with standard transformer; measure FLOPs and performance on the same task subset
  3. Scaling test: Train models of 32M, 73M, and 164M parameters on the full data mix for 1M steps; evaluate scaling of average return vs parameters

## Open Questions the Paper Calls Out
None

## Limitations
- Computational requirements for training large transformer-based models, with optimal performance requiring 954M-1.33B parameters
- Scaling laws were derived from a single large-scale dataset mixture and may not generalize to datasets with different quality distributions
- Perceiver architecture's effectiveness depends critically on the choice of latent set size, which may require tuning for different input complexities

## Confidence
- High confidence: PAC's ability to scale to large models and outperform BC baselines on Control Suite tasks
- Medium confidence: The Perceiver architecture's efficiency claims and real-time control capabilities
- Medium confidence: Scaling laws following supervised learning patterns

## Next Checks
1. Test the scaling laws on datasets with varying quality distributions and reward structures to verify their robustness
2. Conduct ablation studies on latent set size and its impact on multimodal input processing for different task complexities
3. Measure actual inference latency on target hardware to validate the claimed 20Hz real-time control capability