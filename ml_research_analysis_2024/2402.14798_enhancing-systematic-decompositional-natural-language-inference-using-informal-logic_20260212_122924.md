---
ver: rpa2
title: Enhancing Systematic Decompositional Natural Language Inference Using Informal
  Logic
arxiv_id: '2402.14798'
source_url: https://arxiv.org/abs/2402.14798
tags:
- entailment
- hypothesis
- tree
- rdte
- decompositions
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper tackles the problem of determining what constitutes a
  valid compositional entailment in natural language inference, which is essential
  for building explainable AI systems that use entailment trees. The authors propose
  a new protocol inspired by informal logic's RAS (Relevance, Acceptability, Sufficiency)
  criteria, along with a redundancy check, to annotate decompositional textual entailment
  data.
---

# Enhancing Systematic Decompositional Natural Language Inference Using Informal Logic

## Quick Facts
- arXiv ID: 2402.14798
- Source URL: https://arxiv.org/abs/2402.14798
- Reference count: 40
- Primary result: RDTE protocol improves decompositional entailment annotation consistency by +9% and enhances TREE WISE's QA accuracy and tree quality

## Executive Summary
This paper addresses the challenge of determining valid compositional entailment in natural language inference, crucial for explainable AI systems using entailment trees. The authors propose a novel RDTE protocol inspired by informal logic's RAS criteria (Relevance, Acceptability, Sufficiency) plus redundancy check to annotate decompositional textual entailment data. They collect the RDTE dataset of over 1000 expert annotations demonstrating substantially higher internal consistency than prior datasets. The paper also develops a knowledge distillation pipeline using GPT-4 annotations to train smaller entailment classifiers, which when integrated into the TREE WISE entailment tree engine significantly improves both accuracy and proof quality.

## Method Summary
The method involves three key components: First, the RDTE protocol replaces binary entailment labels with multi-faceted ordinal ratings (relevance, acceptability, sufficiency, redundancy) to improve annotation consistency. Second, knowledge distillation uses GPT-4 as a teacher to annotate reasoning traces, creating silver training data for student models (RoBERTa and ChatGPT) that emulate GPT-4's behavior while being faster and cheaper. Third, the TREE WISE entailment tree engine integrates these classifiers to filter invalid decompositions during search, combining forward chaining, paraphrasing, and improved decomposition filtering to find more accurate and explainable proofs.

## Key Results
- RDTE dataset shows +9% higher internal consistency than prior decompositional entailment datasets
- Knowledge distillation from GPT-4 improves student models by 5-21 points, sometimes outperforming GPT-4 itself
- TREE WISE with RDTE-trained classifiers significantly improves both QA accuracy and tree integrity quality

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The RDTE protocol improves consistency by replacing ambiguous binary entailment labels with multi-faceted ordinal ratings
- Mechanism: Explicitly breaking down entailment into RAS criteria guides annotators to focus on specific logical properties, reducing subjective interpretation
- Core assumption: Annotators can reliably distinguish relevance, acceptability, and sufficiency when given clear rubrics
- Evidence anchors: RDTE dataset shows +9% higher internal consistency; ordinal scale implementation with specific conditions for each score

### Mechanism 2
- Claim: Knowledge distillation from GPT-4 annotations creates effective small student models
- Mechanism: GPT-4 acts as teacher, labeling reasoning traces; student models fine-tuned on silver data to mimic GPT-4 while being faster and cheaper
- Core assumption: GPT-4's RDTE-guided annotations are of sufficient quality to serve as ground truth
- Evidence anchors: Student models improve by 5-21 points; at times outperform GPT-4 itself

### Mechanism 3
- Claim: TREE WISE's enhanced search algorithm improves grounding and tree quality
- Mechanism: Integrating forward chaining, paraphrasing, and RDTE-trained classifiers to retrieve support documents, generate inferences, and filter invalid decompositions
- Core assumption: RDTE-trained classifiers can reliably identify valid decompositions in diverse domains
- Evidence anchors: Improved QA accuracy and raised quality of generated trees

## Foundational Learning

- Concept: RAS (Relevance, Acceptability, Sufficiency) criteria from informal logic
  - Why needed here: Provides principled framework for judging decompositional entailment beyond binary yes/no labels
  - Quick check question: If a premise is factually true but irrelevant to the hypothesis, what RAS score should it receive?

- Concept: Knowledge distillation in NLP
  - Why needed here: Allows expensive GPT-4 reasoning to be emulated by smaller, faster models suitable for real-time inference
  - Quick check question: What is the trade-off between precision and recall in knowledge distillation for entailment classification?

- Concept: Tree-based reasoning and backward chaining
  - Why needed here: TREE WISE uses recursive decomposition to build explainable proofs from knowledge corpora
  - Quick check question: How does TREE WISE prevent redundant or circular decompositions during search?

## Architecture Onboarding

- Component map: GPT-4 (teacher) -> RDTE protocol -> Silver dataset -> Student model (ChatGPT/RoBERTa) -> TREE WISE engine
- Critical path: Generate reasoning traces -> Annotate with RDTE rubric -> Distill into student model -> Use as entailment filter in TREE WISE
- Design tradeoffs: High-quality annotations vs. annotation cost; precision vs. recall in entailment filtering; GPT-4 quality vs. student model speed
- Failure signatures: Low student model precision -> many false positives in TREE WISE; high student model recall but low precision -> over-filtering valid decompositions; inconsistent annotations -> noisy silver data
- First 3 experiments:
  1. Run RDTE protocol on small set of decompositions to measure inter-annotator agreement
  2. Generate silver data using GPT-4 and train student model; evaluate on held-out RDTE
  3. Integrate student model into TREE WISE and compare QA accuracy and tree integrity vs. baseline

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How would the RDTE protocol perform on annotating decompositions for domains outside of science QA and multi-hop QA over Wikipedia?
- Basis in paper: Explicit - The paper discusses potential application to new domains but acknowledges careful consideration of how facets manifest differently would be necessary
- Why unresolved: RDTE dataset and evaluation were limited to two specific domains (ARC and HotpotQA)
- What evidence would resolve it: Applying RDTE protocol to diverse reasoning tasks and evaluating resulting dataset quality and model performance

### Open Question 2
- Question: What is the impact of using different retrieval methods (e.g., BM25 vs. dense retrieval) on quality of entailment trees generated by TREE WISE?
- Basis in paper: Inferred - The paper mentions using BM25 for first-stage retrieval and SentenceTransformer for reranking but doesn't explore impact of different retrieval methods
- Why unresolved: Retrieval method can significantly affect quality and relevance of support documents, impacting final tree quality
- What evidence would resolve it: Conducting experiments with TREE WISE using different retrieval methods and comparing generated tree quality and QA accuracy

### Open Question 3
- Question: How does size and diversity of knowledge base (e.g., Wikipedia vs. specialized corpus) affect TREE WISE performance on different reasoning tasks?
- Basis in paper: Explicit - The paper evaluates TREE WISE on EBQA using both WorldTree and Wikipedia but doesn't extensively explore impact of knowledge base characteristics
- Why unresolved: Choice of knowledge base can influence availability of relevant information, presence of noise, and overall system effectiveness
- What evidence would resolve it: Conducting experiments with TREE WISE using knowledge bases of varying sizes and domains, evaluating performance on range of reasoning tasks

## Limitations

- The RDTE protocol, while improving consistency, shows only modest gains in downstream QA performance with marginal accuracy improvements
- Knowledge distillation approach heavily relies on GPT-4's annotation quality, which may not generalize to all domains or reasoning tasks
- TREE WISE's enhanced search algorithm still struggles with complex reasoning chains requiring multi-hop inference or commonsense knowledge

## Confidence

- High confidence: RDTE protocol improves annotation consistency compared to prior datasets (directly supported by +9% improvement in internal consistency)
- Medium confidence: Knowledge distillation from GPT-4 creates effective small student models (improvements shown but reliance on GPT-4 quality introduces variability)
- Medium confidence: TREE WISE's enhanced search algorithm improves grounding and tree quality (improvements demonstrated but gains are modest and may not generalize)

## Next Checks

1. **Replicate RDTE Protocol**: Conduct small-scale annotation study using RDTE protocol to measure inter-annotator agreement and validate consistency improvements
2. **Evaluate Student Models**: Test distilled student models on held-out validation set to assess precision-recall trade-off and identify potential overfitting or underfitting issues
3. **Assess TREE WISE Performance**: Run TREE WISE with and without RDTE-trained classifiers on diverse set of QA tasks to evaluate robustness of accuracy and tree integrity improvements