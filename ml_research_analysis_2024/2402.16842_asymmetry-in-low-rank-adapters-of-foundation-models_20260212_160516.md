---
ver: rpa2
title: Asymmetry in Low-Rank Adapters of Foundation Models
arxiv_id: '2402.16842'
source_url: https://arxiv.org/abs/2402.16842
tags:
- lora
- fine-tuning
- matrices
- random
- generalization
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper reveals that the two low-rank matrices in LoRA (B and
  A) have asymmetric roles: B projects the output while A extracts features from the
  input. This asymmetry suggests that updating only B, with A fixed to a random orthogonal
  matrix, can match or exceed the performance of updating both matrices, while reducing
  trainable parameters by nearly half.'
---

# Asymmetry in Low-Rank Adapters of Foundation Models

## Quick Facts
- arXiv ID: 2402.16842
- Source URL: https://arxiv.org/abs/2402.16842
- Reference count: 21
- Key outcome: Updating only the B matrix in LoRA (with A frozen to random orthogonal) matches or exceeds full LoRA performance while reducing trainable parameters by ~50%

## Executive Summary
This paper reveals that the two low-rank matrices in LoRA (B and A) have asymmetric roles: B projects the output while A extracts features from the input. This asymmetry suggests that updating only B, with A fixed to a random orthogonal matrix, can match or exceed the performance of updating both matrices, while reducing trainable parameters by nearly half. Theoretical analysis using multivariate linear regression and information-theoretic bounds supports this conclusion. Empirical results on RoBERTa, BART, LLaMA-2, and ViTs across GLUE, summarization, MMLU, and DomainBed tasks confirm that updating B alone achieves comparable or superior accuracy with fewer parameters and improved generalization.

## Method Summary
The paper analyzes LoRA fine-tuning by examining the asymmetric roles of matrices A and B in the low-rank update BA. Instead of updating both matrices, the method freezes A to a random orthogonal matrix and only trains B. This approach leverages the observation that A serves primarily as a feature extractor while B handles the actual adaptation. The method is validated across multiple foundation models (RoBERTa, BART, LLaMA-2, ViTs) and tasks (GLUE, summarization, MMLU, DomainBed), comparing B-only training against standard LoRA and other variants.

## Key Results
- B-only training matches or exceeds full LoRA performance on GLUE and summarization tasks
- Parameter reduction of ~50% when freezing A (when din ≈ dout)
- Improved generalization bounds by factor of sqrt(2) when only tuning B
- Consistent performance across multiple architectures and tasks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: B matrix extracts useful features while A matrix projects input to a low-rank subspace
- Mechanism: In LoRA, the product BA updates pretrained weights W0. The A matrix acts as a feature extractor that reduces input dimensionality to r dimensions, while B maps these r-dimensional features to desired outputs. Since B can learn to use whatever information A preserves, freezing A to a random orthogonal matrix still allows B to learn useful mappings.
- Core assumption: The information in the input is redundant enough that reducing to r dimensions preserves sufficient signal for B to work with
- Evidence anchors:
  - [abstract]: "A extracts features from the input, while B uses these features to create the desired output"
  - [section 4.1.1]: "A serves to extract r features from ϕ(layerInput), which are then used by B to predict some desired output"
  - [corpus]: "Recent efforts to scale Transformer models have demonstrated rapid progress... parameter-efficient fine-tuning (PEFT) approaches have emerged as a viable alternative" (weak - mentions PEFT but not asymmetry)

### Mechanism 2
- Claim: Freezing A reduces trainable parameters by ~50% while maintaining performance
- Mechanism: Standard LoRA updates both A and B matrices. By freezing A to a random orthogonal matrix and only updating B, the number of trainable parameters is reduced by a factor of dout/(dout+din). Since dout ≈ din in practice, this yields ~2x parameter reduction.
- Core assumption: The parameter reduction from freezing A doesn't significantly impact model capacity when B can compensate
- Evidence anchors:
  - [abstract]: "updating only B, with A fixed to a random orthogonal matrix, can match or exceed the performance of updating both matrices, while reducing trainable parameters by nearly half"
  - [section 4.2.1]: "Fine-tuning B alone as opposed to both A and B reduces the number of parameters by a factor of dout/dout+din, which equals 0.5 when din = dout"
  - [corpus]: "LoRA is also communication-efficient for federated LLMs when multiple users collaboratively fine-tune a global LLM model" (weak - mentions efficiency but not parameter reduction)

### Mechanism 3
- Claim: Freezing A improves generalization bounds
- Mechanism: Information-theoretic generalization bounds depend on the number of parameters being tuned. By freezing A and only tuning B, the bound becomes smaller by a factor of sqrt(2) when din = dout. This allows using larger rank r for B without increasing the generalization bound.
- Core assumption: Smaller generalization bounds directly translate to better out-of-domain performance
- Evidence anchors:
  - [abstract]: "Using an information-theoretic lens, we also bound the generalization of low-rank adapters, showing that the parameter savings of exclusively training B improves the bound"
  - [section 4.2.2]: "Lemma 4.5 (Generalization bounds on adapting A and/or B)... tuning just one factor... is a factor of sqrt(2) smaller than the bound for tuning both factors"
  - [corpus]: "Less is More: Resource-Efficient Low-Rank Adaptation... training... LoRA update matrices to exploit matrix-wise asymmetry" (weak - mentions asymmetry but not generalization bounds)

## Foundational Learning

- Concept: Low-rank matrix factorization
  - Why needed here: LoRA represents weight updates as BA where A and B are low-rank matrices
  - Quick check question: What is the rank of BA if A is r×din and B is dout×r?

- Concept: Information bottleneck principle
  - Why needed here: The A matrix acts as a bottleneck that compresses input features to r dimensions
  - Quick check question: Why might reducing input dimensionality to r dimensions not harm performance?

- Concept: Stiefel manifold and random orthogonal matrices
  - Why needed here: A is initialized as a random orthogonal matrix from the Stiefel manifold
  - Quick check question: What property must a matrix have to be considered "random orthogonal"?

## Architecture Onboarding

- Component map:
  - W0: Pre-trained weight matrix (fixed)
  - A: Input feature extractor matrix (frozen to random orthogonal)
  - B: Output projection matrix (trainable)
  - BA: Low-rank update added to W0

- Critical path:
  1. Input passes through layer, producing ϕ(layerInput)
  2. A extracts r-dimensional features: A·ϕ(layerInput)
  3. B projects these features to output space: B·(A·ϕ(layerInput))
  4. Result added to W0·ϕ(layerInput) for final output

- Design tradeoffs:
  - Parameter efficiency vs. model capacity: Freezing A saves ~50% parameters but may limit expressiveness
  - Generalization vs. fit: Smaller generalization bounds vs. potentially reduced ability to fit complex targets
  - Initialization sensitivity: Random orthogonal A vs. learned A

- Failure signatures:
  - Poor performance: A might need to be trainable if input features are not redundant
  - Overfitting: B might need regularization if rank r is too high
  - Training instability: Random A initialization might need normalization

- First 3 experiments:
  1. Compare training B only vs. both A and B on a simple regression task with synthetic data
  2. Measure parameter reduction and runtime speed-up when freezing A
  3. Test out-of-domain generalization on a vision transformer with different rank r values

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the asymmetry between matrices A and B generalize across different neural network architectures beyond transformers and vision transformers?
- Basis in paper: [inferred] The paper focuses on transformers and vision transformers, but mentions analyzing a general case of rank-r adaptation of any parameter matrix used multiplicatively on some input-dependent vector.
- Why unresolved: The theoretical analysis and empirical validation are limited to specific architectures (transformers, ViTs). Broader validation across diverse architectures is needed to establish the universality of the observed asymmetry.
- What evidence would resolve it: Empirical studies on a wide range of neural network architectures (e.g., recurrent networks, graph neural networks, CNNs) demonstrating consistent asymmetry between A and B matrices during LoRA fine-tuning.

### Open Question 2
- Question: What is the optimal initialization strategy for matrix A when freezing it during LoRA fine-tuning, and how does it affect performance?
- Basis in paper: [explicit] The paper compares different initialization methods (random orthonormal, zero, random uniform) for matrix A and finds that orthogonal initialization performs best.
- Why unresolved: While the paper identifies orthogonal initialization as superior, it doesn't explore the full space of possible initialization strategies or investigate the underlying reasons for this superiority.
- What evidence would resolve it: Systematic exploration of various initialization strategies (e.g., structured random matrices, data-dependent initialization) and theoretical analysis of their impact on LoRA performance when A is frozen.

### Open Question 3
- Question: How does the rank r of matrices A and B affect the observed asymmetry, and is there an optimal rank ratio for A:B when only updating B?
- Basis in paper: [explicit] The paper mentions that the performance advantage of tuning B over A is large when d ≫ r, but doesn't investigate the optimal rank ratio for A:B.
- Why unresolved: The paper doesn't provide guidance on choosing the rank r for A and B, which is crucial for practical applications of the proposed method.
- What evidence would resolve it: Empirical studies varying the rank r for A and B, analyzing the impact on performance and parameter efficiency, to determine an optimal rank ratio for different tasks and model architectures.

### Open Question 4
- Question: How does the observed asymmetry between A and B matrices in LoRA relate to the intrinsic dimensionality of the pre-trained model, as mentioned in related work?
- Basis in paper: [inferred] The paper mentions related work on intrinsic dimensionality explaining the effectiveness of language model fine-tuning, but doesn't explicitly connect this concept to the observed asymmetry.
- Why unresolved: The paper doesn't explore the relationship between the intrinsic dimensionality of the pre-trained model and the asymmetry between A and B matrices during LoRA fine-tuning.
- What evidence would resolve it: Theoretical analysis and empirical studies investigating how the intrinsic dimensionality of the pre-trained model influences the asymmetry between A and B matrices, and how this relationship affects LoRA performance.

## Limitations
- The theoretical claims rely on idealized assumptions about low-rank structure of input features
- The empirical validation uses relatively small rank values (r=8, 16, 32, 64) that may not scale to larger values
- Mixed results on MMLU and DomainBed tasks compared to strong GLUE and summarization performance

## Confidence
- **High Confidence**: The parameter efficiency claim (approximately 50% reduction when freezing A) is well-supported by both theoretical analysis and empirical results across multiple tasks and models
- **Medium Confidence**: The generalization improvement claim is supported by the theoretical bounds and some empirical evidence, but the practical significance may vary depending on the specific task and data distribution
- **Medium Confidence**: The claim that B-only training matches or exceeds full LoRA performance is well-supported by GLUE and summarization experiments but shows more mixed results on MMLU and DomainBed tasks

## Next Checks
1. **Scale Test**: Evaluate the B-only approach with larger rank values (r=128, 256) on GLUE tasks to verify if the performance advantage persists at higher capacities, testing the limits of the random orthogonal initialization hypothesis
2. **Domain Transfer Test**: Apply the B-only method to a cross-domain transfer learning scenario where the source and target domains have significantly different feature distributions, to validate the generalization claims under challenging conditions
3. **Architectural Stress Test**: Test the B-only approach on architectures with highly asymmetric weight matrices (where din ≠ dout by a factor of 2 or more) to verify if the ~50% parameter reduction claim holds under different dimensional configurations