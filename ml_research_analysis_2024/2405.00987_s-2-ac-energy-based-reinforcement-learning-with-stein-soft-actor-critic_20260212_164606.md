---
ver: rpa2
title: 'S$^2$AC: Energy-Based Reinforcement Learning with Stein Soft Actor Critic'
arxiv_id: '2405.00987'
source_url: https://arxiv.org/abs/2405.00987
tags:
- entropy
- s2ac
- distribution
- svgd
- policy
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes Stein Soft Actor-Critic (S2AC), a maximum entropy
  reinforcement learning algorithm that models the policy as an expressive Energy-Based
  Model (EBM) over Q-values. The key contribution is deriving a closed-form expression
  for the entropy of policies induced by Stein Variational Gradient Descent (SVGD),
  which enables efficient entropy computation without sacrificing expressivity.
---

# S$^2$AC: Energy-Based Reinforcement Learning with Stein Soft Actor Critic

## Quick Facts
- arXiv ID: 2405.00987
- Source URL: https://arxiv.org/abs/2405.00987
- Reference count: 40
- Primary result: S2AC learns more optimal solutions to the MaxEnt objective than SQL and SAC in multi-goal environment, and outperforms SAC and SQL on MuJoCo benchmark tasks with 4/5 environments

## Executive Summary
This paper proposes S2AC, a maximum entropy reinforcement learning algorithm that models the policy as an expressive Energy-Based Model (EBM) over Q-values using Stein Variational Gradient Descent (SVGD). The key innovation is deriving a closed-form expression for the entropy of policies induced by SVGD, enabling efficient entropy computation without sacrificing expressivity. S2AC uses a parameterized initial Gaussian distribution in SVGD to improve scalability and sample efficiency. Experiments demonstrate S2AC's superiority over SAC and SQL in both a multimodal multi-goal environment and standard MuJoCo benchmark tasks.

## Method Summary
S2AC is a maximum entropy actor-critic algorithm that uses SVGD to sample actions from an EBM policy parameterized by Q-values. The method introduces a closed-form entropy estimation formula for SVGD-induced distributions and uses a parameterized initial Gaussian distribution to accelerate convergence. The actor is trained to minimize KL divergence to the EBM, while the critic estimates soft Q-values. At test time, an amortized SVGD version provides fast inference. The approach combines the expressivity of EBMs with computational efficiency through analytic entropy computation.

## Key Results
- S2AC yields more optimal solutions to the MaxEnt objective than SQL and SAC in a multi-goal environment
- S2AC outperforms SAC and SQL on MuJoCo benchmark tasks with 4/5 environments
- The closed-form entropy formula is validated on distributions with known entropy values and shown to be robust to SVGD parameters

## Why This Works (Mechanism)

### Mechanism 1
The closed-form entropy formula for SVGD-based policies enables tractable entropy computation without sacrificing expressivity. SVGD update rule is invertible under certain conditions (ϵ ≪ σ for RBF kernel), allowing use of the change-of-variable formula to derive an analytic expression for the policy entropy that depends only on first-order derivatives and vector products.

### Mechanism 2
Parameterized initial distribution (isotropic Gaussian) accelerates convergence to the target distribution. Learning the mean and standard deviation of the initial Gaussian distribution allows it to contour the high-density region of the target distribution, reducing the number of SVGD steps needed for convergence.

### Mechanism 3
SVGD-based policy outperforms SAC and SQL on multimodal tasks by better maximizing expected future entropy. SVGD captures multimodal action distributions through particle-based updates with repulsive forces, while parameterized initialization further refines convergence to relevant modes, resulting in policies that maximize both expected reward and entropy.

## Foundational Learning

- **Maximum Entropy Reinforcement Learning**: Why needed - S2AC is a MaxEnt RL algorithm that aims to maximize both expected reward and expected entropy. Quick check - What is the key difference between the MaxEnt RL objective and standard RL objective in terms of what they maximize?

- **Energy-Based Models (EBMs)**: Why needed - S2AC models the policy as an EBM over Q-values, requiring understanding of EBMs and how they represent complex distributions. Quick check - How does an EBM represent a probability distribution, and why are they considered expressive but computationally challenging?

- **Stein Variational Gradient Descent (SVGD)**: Why needed - SVGD is the core sampling algorithm used to generate action samples from the EBM policy. Quick check - What are the two key terms in the SVGD update rule, and what roles do they play in the sampling process?

## Architecture Onboarding

- **Component map**: Critic network (Qϕ) -> Actor as SVGD sampler with parameterized initial Gaussian -> Entropy computation module using closed-form formula -> Replay buffer for off-policy learning -> Target networks for stability

- **Critical path**: 1) Sample particles from initial Gaussian distribution, 2) Update particles using SVGD dynamics, 3) Compute entropy using closed-form formula, 4) Update critic using Bellman loss with entropy term, 5) Update actor parameters to minimize KL divergence to EBM

- **Design tradeoffs**: SVGD steps vs. computation cost (more steps improve accuracy but increase runtime), Number of particles vs. memory (more particles better capture distribution but require more memory), Kernel variance selection (affects invertibility and convergence properties)

- **Failure signatures**: Poor performance (check if SVGD parameters are appropriately tuned), Instability during training (verify that entropy computation is stable), Suboptimal exploration (ensure initial Gaussian parameters are being learned effectively)

- **First 3 experiments**: 1) Verify closed-form entropy computation on simple distributions with known ground truth, 2) Test SVGD invertibility condition by varying step size and kernel variance on a 2D target distribution, 3) Compare S2AC performance with and without parameterized initial distribution on a simple multimodal task

## Open Questions the Paper Calls Out

### Open Question 1
How does the choice of kernel function affect the invertibility of SVGD and the accuracy of the closed-form entropy estimate? The paper only uses an RBF kernel and does not investigate the invertibility or performance of other kernel functions.

### Open Question 2
Can the parameterized initial distribution q0 be learned more effectively using techniques beyond a simple isotropic Gaussian? The paper uses a parameterized isotropic Gaussian but does not explore more complex parameterizations.

### Open Question 3
How does the choice of entropy weight α impact the robustness and performance of S2AC in environments with varying levels of multimodality and noise? The paper only provides results for a specific multi-goal environment and MuJoCo benchmarks.

## Limitations

- The closed-form entropy formula's computational efficiency relies on the SVGD invertibility condition (ϵ ≪ σ), which may break down in high-dimensional action spaces
- The assumption that isotropic Gaussian initialization is sufficient for capturing complex multimodal distributions in real-world tasks remains largely untested
- Comparison with SAC-NF doesn't directly validate whether SVGD-based approach provides advantages over other particle-based methods

## Confidence

- **High Confidence**: Mathematical derivation of closed-form entropy formula under stated invertibility conditions is sound and validated on simple distributions
- **Medium Confidence**: Empirical performance improvements on MuJoCo benchmarks are convincing, though ablation studies on individual components are limited
- **Low Confidence**: Scalability claims for high-dimensional tasks are not thoroughly validated, and choice of isotropic Gaussian initialization for complex environments remains speculative

## Next Checks

1. **Stress test the entropy formula**: Systematically vary SVGD step size (ϵ) and kernel variance (σ) across a range of values on a 10D Gaussian mixture to identify the breaking point of the closed-form entropy approximation and quantify error growth.

2. **Ablation on initialization**: Implement S2AC with different initial distributions (isotropic Gaussian vs. diagonal Gaussian vs. full covariance Gaussian) and measure the impact on sample efficiency and final performance across all benchmark tasks.

3. **High-dimensional scaling analysis**: Test S2AC on continuous control tasks with larger action spaces (e.g., Humanoid with more degrees of freedom or simulated robotic manipulation tasks) to evaluate whether computational advantages and performance benefits scale to more realistic scenarios.