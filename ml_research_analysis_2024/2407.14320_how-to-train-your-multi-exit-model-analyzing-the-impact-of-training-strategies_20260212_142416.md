---
ver: rpa2
title: How to Train Your Multi-Exit Model? Analyzing the Impact of Training Strategies
arxiv_id: '2407.14320'
source_url: https://arxiv.org/abs/2407.14320
tags:
- training
- mixed
- regime
- joint
- disjoint
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper analyzes training strategies for multi-exit neural networks,
  comparing joint, disjoint, and mixed approaches. While joint training trains the
  entire model simultaneously and disjoint training separates backbone and exit heads,
  the proposed mixed strategy first trains the backbone then jointly optimizes the
  entire model.
---

# How to Train Your Multi-Exit Model? Analyzing the Impact of Training Strategies

## Quick Facts
- **arXiv ID**: 2407.14320
- **Source URL**: https://arxiv.org/abs/2407.14320
- **Reference count**: 40
- **Primary result**: Mixed training strategy improves multi-exit model accuracy by 1-3% at higher computational budgets while reducing training time by 27%

## Executive Summary
This paper analyzes three training strategies for multi-exit neural networks: joint, disjoint, and mixed. Through extensive experiments across architectures (ResNet, ViT) and datasets (ImageNet, CIFAR), the authors demonstrate that mixed training—which first trains the backbone independently then jointly optimizes the entire model—consistently outperforms conventional approaches. The analysis reveals that mixed training produces gradients dominated by deeper classifiers, aligning with performance at higher computational budgets, and creates solution basins similar to disjoint training while achieving superior performance.

## Method Summary
The paper compares three training regimes for multi-exit neural networks. Joint training optimizes all components simultaneously, disjoint training separates backbone and exit heads into distinct phases, and mixed training combines both approaches by first pre-training the backbone then jointly optimizing with internal classifiers. The authors evaluate these strategies across multiple architectures (ResNet, ViT), datasets (ImageNet, CIFAR), and early-exit methods (SDN, GPF), measuring accuracy, FLOPs efficiency, gradient dominance, mode connectivity, numerical rank, and mutual information.

## Key Results
- Mixed training improves accuracy by 1-3% compared to joint training at higher computational budgets
- Mixed training reduces training time by 27% compared to joint training
- Mixed training obviates the need for gradient equilibrium techniques through natural gradient dominance by deeper classifiers

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Mixed training outperforms joint training by 1-3% accuracy at higher computational budgets.
- Mechanism: Mixed training first optimizes the backbone independently, then jointly trains with internal classifiers. This creates solution basins similar to disjoint training while achieving superior performance.
- Core assumption: Pre-training the backbone before joint optimization prevents interference between backbone and classifier gradients.
- Evidence anchors:
  - [abstract]: "mixed training consistently outperforms conventional approaches" and "mixed training improves accuracy by 1-3% compared to joint training at higher computational budgets"
  - [section]: "We observe a distinct change in network expressiveness once intermediate classifiers are attached and permitted to influence the backbone"
  - [corpus]: Weak evidence - no direct citations to mixed training improvements
- Break condition: If backbone pre-training introduces significant bias that cannot be corrected during joint optimization

### Mechanism 2
- Claim: Mixed training produces gradients dominated by deeper classifiers, aligning with performance at higher computational budgets.
- Mechanism: During mixed training, the backbone is pre-trained, then the entire model is jointly optimized. This results in the final intermediate classifier having the strongest influence on the backbone's gradients.
- Core assumption: Deeper classifiers in multi-exit architectures provide more discriminative features that guide backbone optimization.
- Evidence anchors:
  - [abstract]: "mixed training produces gradients dominated by deeper classifiers, aligning well with performance at higher computational budgets"
  - [section]: "In the mixed regime, gradients are dominated by deeper classifiers, causing the early layers to primarily support the learning objectives of later classifiers"
  - [corpus]: Weak evidence - no direct citations to gradient dominance mechanisms
- Break condition: If gradient dominance by deeper classifiers causes earlier classifiers to underperform on simple inputs

### Mechanism 3
- Claim: Mixed training obviates the need for gradient equilibrium techniques.
- Mechanism: The gradient dominance naturally achieved through mixed training eliminates the need for explicit gradient scaling methods proposed in prior work.
- Core assumption: Mixed training inherently balances the contribution of different internal classifiers to backbone optimization.
- Evidence anchors:
  - [abstract]: "we show that when using the mixed training regime, gradient scaling is unnecessary"
  - [section]: "The mixed regime achieve superior results and it obviates the need for application of gradient equilibrium"
  - [corpus]: Weak evidence - no direct citations to gradient equilibrium comparison
- Break condition: If certain architectures require explicit gradient balancing even with mixed training

## Foundational Learning

- Concept: Gradient dominance in multi-exit architectures
  - Why needed here: Understanding which classifier gradients influence the backbone is crucial for explaining performance differences between training regimes
  - Quick check question: Which classifier (early, middle, or late) has the strongest influence on backbone gradients in mixed training?

- Concept: Mode connectivity and loss basins
  - Why needed here: The paper uses mode connectivity to show that mixed and disjoint training produce similar solutions in the loss landscape
  - Quick check question: What does low loss during interpolation between two models indicate about their training regimes?

- Concept: Numerical rank as a measure of network expressiveness
  - Why needed here: The paper analyzes how different training regimes affect the rank of activation matrices, indicating changes in representational capacity
  - Quick check question: How does the numerical rank of activation matrices change when internal classifiers are allowed to influence the backbone?

## Architecture Onboarding

- Component map:
  - Backbone network (θb) - main feature extraction layers
  - Internal classifiers (ICs) - attached at intermediate layers with parameters θ(k)IC
  - Training regimes - disjoint (phases 1+3), joint (phase 2), mixed (phases 1+2)
  - Metrics - gradient dominance, mode connectivity, numerical rank, mutual information

- Critical path:
  1. Pre-train backbone independently (mixed regime only)
  2. Train entire model jointly with all components
  3. Evaluate using FLOPs-accuracy trade-off across computational budgets
  4. Analyze using proposed framework metrics

- Design tradeoffs:
  - Joint training: Simple implementation but suboptimal at higher budgets
  - Disjoint training: Fast convergence but poor early-layer representations
  - Mixed training: Best overall performance but requires two-phase training
  - IC size: Larger ICs improve disjoint regime performance but may degrade mixed regime results

- Failure signatures:
  - Joint regime underperforming at high budgets (1-3% accuracy gap)
  - Disjoint regime struggling with low computational budgets
  - Gradient interference between backbone and classifiers
  - Loss landscape showing distinct basins for different regimes

- First 3 experiments:
  1. Compare accuracy of ResNet-50 on TinyImageNet across regimes at 25%, 50%, 75%, and 100% computational budgets
  2. Measure gradient dominance cosine similarity for each IC during training in joint vs mixed regimes
  3. Evaluate mode connectivity by interpolating weights between models trained with different regimes and measuring loss along the path

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do the proposed training strategies generalize to more complex architectures like vision transformers with deeper layers or larger models?
- Basis in paper: [inferred] The paper demonstrates effectiveness across ResNet, ViT, and MSDNet architectures but doesn't explore deeper or larger variants
- Why unresolved: The study uses relatively standard model sizes (ResNet-34, ViT-Tiny, ViT-Small) but doesn't test whether the mixed training strategy scales to larger models
- What evidence would resolve it: Comprehensive experiments on deeper transformers (ViT-Base, ViT-Large) or other modern architectures showing consistent improvements with mixed training

### Open Question 2
- Question: What is the theoretical explanation for why gradient dominance in the mixed regime leads to better performance at higher computational budgets?
- Basis in paper: [explicit] The paper observes that mixed training produces gradients dominated by deeper classifiers but doesn't provide a theoretical framework explaining why this improves performance
- Why unresolved: While empirical observations show the gradient patterns, the paper doesn't connect this to theoretical understanding of optimization dynamics in multi-exit networks
- What evidence would resolve it: A theoretical analysis showing how gradient dominance patterns affect the loss landscape and optimization trajectory, potentially connecting to mode connectivity findings

### Open Question 3
- Question: How sensitive are the training regime recommendations to different early-exit criteria (entropy, max-softmax, etc.) and how do they interact with loss scaling techniques?
- Basis in paper: [explicit] The paper tests multiple early-exit methods including entropy-based criteria and finds consistent patterns, but doesn't deeply analyze interactions between exit criteria and training regimes
- Why unresolved: The experiments show the mixed regime works across different methods, but don't explore the nuanced interactions between exit criteria design and optimal training strategies
- What evidence would resolve it: Systematic ablation studies varying exit criteria while controlling for training regime, showing how different criteria benefit from different optimization approaches

### Open Question 4
- Question: What is the optimal number and placement of internal classifiers for different computational budget targets under each training regime?
- Basis in paper: [explicit] The paper analyzes IC placement schemes but doesn't provide a systematic framework for determining optimal placement based on target computational budgets
- Why unresolved: While the paper shows that placement affects performance, it doesn't develop a principled approach to designing IC placement strategies for specific efficiency targets
- What evidence would resolve it: A design framework or empirical study mapping computational budget requirements to optimal IC placement patterns for each training regime

## Limitations
- The paper relies heavily on empirical validation without providing theoretical guarantees for why mixed training outperforms other regimes
- The analysis of gradient dominance and mode connectivity provides mechanistic insights but doesn't establish causation
- Claims about obviating the need for gradient equilibrium techniques are based on empirical observation without theoretical justification

## Confidence

- **High Confidence**: The experimental methodology and comparative analysis across multiple datasets and architectures are rigorous and well-executed. The reported accuracy improvements (1-3%) and training time reductions (27%) appear reliable given the extensive ablation studies.
- **Medium Confidence**: The proposed mechanisms explaining why mixed training works (gradient dominance, solution basin similarity) are plausible but not definitively proven. The paper shows correlation between these factors and performance but doesn't establish causation.
- **Low Confidence**: Claims about obviating the need for gradient equilibrium techniques are based on empirical observation without theoretical justification. The generalization of mixed training benefits across all architectures and problem types remains uncertain.

## Next Checks
1. **Theoretical Analysis**: Derive formal conditions under which mixed training would outperform joint training, potentially through analysis of gradient alignment or loss landscape properties.
2. **Architecture Transfer**: Validate mixed training benefits on architectures not explored in the paper (e.g., Vision Transformers with early exits, recurrent models) to test generalizability.
3. **Problem Domain Extension**: Apply mixed training to non-classification tasks (regression, sequence-to-sequence) to evaluate whether the observed benefits extend beyond the primary experimental setup.