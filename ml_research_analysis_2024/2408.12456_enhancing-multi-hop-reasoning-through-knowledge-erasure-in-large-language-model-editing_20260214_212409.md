---
ver: rpa2
title: Enhancing Multi-hop Reasoning through Knowledge Erasure in Large Language Model
  Editing
arxiv_id: '2408.12456'
source_url: https://arxiv.org/abs/2408.12456
tags:
- knowledge
- multi-hop
- editing
- answer
- edited
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of knowledge editing in large
  language models (LLMs) when handling multi-hop reasoning tasks. The authors hypothesize
  that residual single-hop knowledge after editing causes edited models to revert
  to original answers in multi-hop questions.
---

# Enhancing Multi-hop Reasoning through Knowledge Erasure in Large Language Model Editing

## Quick Facts
- arXiv ID: 2408.12456
- Source URL: https://arxiv.org/abs/2408.12456
- Reference count: 15
- This paper addresses the challenge of knowledge editing in large language models (LLMs) when handling multi-hop reasoning tasks.

## Executive Summary
This paper addresses the challenge of knowledge editing in large language models (LLMs) when handling multi-hop reasoning tasks. The authors hypothesize that residual single-hop knowledge after editing causes edited models to revert to original answers in multi-hop questions. To validate this, they introduce a "Retain Score" metric to quantify residual old knowledge and empirically show that higher retention correlates with more original answers in multi-hop reasoning. Based on this finding, they propose KELE (Knowledge Erasure for Large Language Model Editing), a method that integrates knowledge erasure with a rank-one editing framework to eliminate old knowledge while injecting new knowledge. Extensive experiments on GPT-2 XL and GPT-J demonstrate that KELE significantly improves multi-hop reasoning accuracy (e.g., 16.67% and 65.11% improvement over baselines) while maintaining performance on single-hop tasks. The method effectively reduces the retention of outdated knowledge, leading to more accurate responses in complex reasoning scenarios.

## Method Summary
The KELE method integrates knowledge erasure with a rank-one editing framework to address residual knowledge in LLMs after editing. It uses a dual optimization approach: first erasing old knowledge through a max-margin loss that reduces the likelihood of generating original answers, then injecting new knowledge by optimizing for accurate prediction of target objects. The method applies these operations to specific feed-forward network layers where knowledge is stored as key-value memories. A regularization term minimizes the impact on the model's intrinsic capabilities while ensuring effective knowledge removal and integration.

## Key Results
- KELE significantly improves multi-hop reasoning accuracy, achieving 16.67% and 65.11% improvements over baselines on GPT-2 XL and GPT-J respectively
- The method maintains performance on single-hop tasks while enhancing multi-hop reasoning capabilities
- KELE effectively reduces the retention of outdated knowledge, as measured by the Retain Score metric

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Residual single-hop knowledge in edited LLMs triggers generation of original answers in multi-hop reasoning tasks.
- Mechanism: When an LLM is edited to update single-hop knowledge (e.g., "USA president is Obama" → "USA president is Biden"), some traces of the original knowledge remain encoded in the model's parameters. During multi-hop reasoning, these residual traces are reactivated through the model's attention and feed-forward mechanisms, causing the model to revert to the original answer path.
- Core assumption: Knowledge in LLMs is stored in a distributed manner across feed-forward layers as key-value memories, and edited knowledge doesn't completely overwrite the original knowledge.
- Evidence anchors:
  - [abstract] "Drawing on cognitive neuroscience and the operational mechanisms of LLMs, we hypothesize that the residual single-hop knowledge after editing causes edited models to revert to their original answers when processing multi-hop questions"
  - [section 4] "We define the Retain Score as a metric to quantify the residual old knowledge (s, r, o) for each edit sample... the higher the residual old knowledge in the edited LLM, the more likely it is to provide the original answers to multi-hop questions"
  - [corpus] Weak evidence - related papers focus on multi-hop reasoning limitations but don't specifically address residual knowledge reactivation mechanism
- Break condition: If knowledge erasure is complete or if the model architecture prevents cross-layer knowledge interference, this mechanism would not apply.

### Mechanism 2
- Claim: The Retain Score metric effectively quantifies the strength of residual old knowledge activation in LLMs.
- Mechanism: The Retain Score uses the logit value of the original object as a proxy for knowledge activation strength. Higher logit values indicate stronger activation of the old knowledge pathway, making it more likely to be retrieved during multi-hop reasoning.
- Core assumption: Logit values directly correlate with the strength of knowledge activation in the model's memory representations.
- Evidence anchors:
  - [section 4] "we use the logit value as a measure of the model's retention of old knowledge... we use the logit value as a measure of the model's retention of old knowledge"
  - [section 4] "the higher the RS value, the accuracy of the unedited model's responses also increases. This suggests that the model's sensitivity to the corresponding knowledge strengthens as the RS value rises"
  - [corpus] Weak evidence - no direct validation of logit-as-activation proxy in related literature
- Break condition: If logit values are influenced by factors other than knowledge activation strength (e.g., temperature settings, token frequency), the metric would lose validity.

### Mechanism 3
- Claim: The KELE method's dual optimization (erasure + injection) successfully balances knowledge removal with new knowledge integration.
- Mechanism: KELE uses a max-margin loss to suppress old knowledge activation while simultaneously optimizing for new knowledge recall. The regularization term prevents excessive model disruption during the erasure process.
- Core assumption: Joint optimization of opposing objectives (erasure and injection) can achieve a stable equilibrium that removes old knowledge without damaging overall model capabilities.
- Evidence anchors:
  - [abstract] "we design an erasure function for residual knowledge and an injection function for new knowledge. Through joint optimization, we derive the optimal recall vector"
  - [section 5] "we introduce a max-margin loss aimed at reducing the likelihood that the edited LLM generates o in response to the prompt p(s, r)"
  - [section 5] "To mitigate the impact of above operations on the intrinsic of s within the LLM, we minimize the KL divergence between F(vl_s+ = h) and the original model"
  - [corpus] Moderate evidence - related work on rank-one editing provides foundation, but dual optimization approach is novel
- Break condition: If the regularization strength is poorly tuned or if the erasure function is too aggressive, the model may lose general reasoning capabilities.

## Foundational Learning

- Concept: Knowledge editing in LLMs
  - Why needed here: The paper builds on existing knowledge editing techniques but identifies their limitations in multi-hop reasoning scenarios
  - Quick check question: What is the difference between methods that preserve model parameters versus those that modify them directly?

- Concept: Multi-hop reasoning in language models
  - Why needed here: The core problem being addressed is the degradation of multi-hop reasoning performance after knowledge editing
  - Quick check question: How does multi-hop reasoning differ from single-hop reasoning in terms of knowledge dependencies?

- Concept: Feed-forward network (FFN) as key-value memory
  - Why needed here: The paper assumes knowledge is stored in FFN layers as key-value pairs, which is critical for understanding the rank-one editing framework
  - Quick check question: How does the key-value memory hypothesis explain the storage and retrieval of factual knowledge in transformers?

## Architecture Onboarding

- Component map: Input layer -> Subject representation (k*) -> FFN editing -> Recall vector (v*) -> Parameter update -> Output layer

- Critical path: Subject → Subject representation (k*) → FFN editing → Recall vector (v*) → Parameter update → Output generation

- Design tradeoffs:
  - Erasure intensity vs. model stability: Higher k values in the erasure function increase old knowledge removal but may destabilize the model
  - Layer selection: Editing different FFN layers may have varying impacts on knowledge retention and reasoning performance
  - Regularization strength: Balancing between complete erasure and preserving general model capabilities

- Failure signatures:
  - Over-erasure: Model loses general reasoning capabilities and performs poorly on both single-hop and multi-hop tasks
  - Under-erasure: Model continues to generate original answers in multi-hop scenarios
  - Layer mismatch: Editing the wrong FFN layer may not effectively target the relevant knowledge

- First 3 experiments:
  1. Test Retain Score correlation: Verify that higher Retain Scores correlate with original answer generation in multi-hop tasks
  2. Ablation study on k values: Determine optimal erasure intensity by testing various k values on a validation set
  3. Layer sensitivity analysis: Test editing different FFN layers to identify which layers most effectively target the relevant knowledge

## Open Questions the Paper Calls Out
No open questions were explicitly called out in the paper.

## Limitations
- The Retain Score metric, while intuitive, lacks direct validation as a measure of knowledge activation strength in LLMs
- Experimental results are based on relatively small models (GPT-2 XL and GPT-J) and may not generalize to larger, more modern LLMs
- The long-term stability of knowledge erasure and its impact on model generalization remain uncertain

## Confidence
- Hypothesis on Residual Knowledge: Medium - The claim is supported by empirical correlations but lacks causal evidence
- KELE Method Effectiveness: Medium-High - Significant improvements are well-documented but long-term stability is uncertain
- Retain Score Metric Validity: Low-Medium - Shows expected correlations but theoretical foundation needs more rigorous validation

## Next Checks
1. **Ablation Study on Retain Score Components**: Systematically test whether the logit-based Retain Score captures knowledge activation more effectively than alternative metrics by correlating each metric with actual knowledge retrieval success rates across different reasoning tasks.

2. **Cross-Model Generalization Test**: Apply KELE to a larger, more modern LLM (e.g., LLaMA-2 or Mistral) and evaluate whether the same improvement patterns hold. This would validate whether the knowledge erasure approach generalizes beyond the specific architectures tested.

3. **Longitudinal Stability Analysis**: Track edited models' performance over extended inference periods and multiple fine-tuning sessions to assess whether knowledge erasure remains stable or if edited knowledge gradually reverts to original patterns through continued training or inference.