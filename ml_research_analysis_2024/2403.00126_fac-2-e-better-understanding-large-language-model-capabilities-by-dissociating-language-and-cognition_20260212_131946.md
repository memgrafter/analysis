---
ver: rpa2
title: 'FAC$^2$E: Better Understanding Large Language Model Capabilities by Dissociating
  Language and Cognition'
arxiv_id: '2403.00126'
source_url: https://arxiv.org/abs/2403.00126
tags:
- language
- arxiv
- llms
- llama
- knowledge
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "The paper introduces FAC2E, a fine-grained evaluation framework\
  \ for large language models (LLMs) that dissociates language-related and cognition-related\
  \ capabilities. The framework categorizes LLM capabilities into four dimensions\u2014\
  LINGUISTIC KNOWLEDGE, FORMAL KNOWLEDGE, WORLD MODELING, and SOCIAL MODELING\u2014\
  grounded in neuroscience evidence about brain function separation."
---

# FAC$^2$E: Better Understanding Large Language Model Capabilities by Dissociating Language and Cognition

## Quick Facts
- arXiv ID: 2403.00126
- Source URL: https://arxiv.org/abs/2403.00126
- Authors: Xiaoqiang Wang; Lingfei Wu; Tengfei Ma; Bang Liu
- Reference count: 31
- Primary result: Dissociating language and cognition reveals significant performance gaps in LLM capabilities, with knowledge utilization identified as a key bottleneck.

## Executive Summary
This paper introduces FAC2E, a fine-grained evaluation framework that dissociates language-related and cognition-related capabilities in large language models. The framework categorizes capabilities into four dimensions grounded in neuroscience evidence about brain function separation, then breaks down capability application into three sub-steps for detailed assessment. Experiments on 17 benchmarks reveal significant performance gaps between open-source and proprietary models, particularly in cognition-related tasks. The study identifies knowledge utilization as a key limitation and proposes a knowledge-enhanced method that injects reference rationales, achieving approximately 90% of the performance of instruction-tuned models.

## Method Summary
FAC2E evaluates LLMs through few-shot instruction-following with chain-of-thought prompting to elicit intermediate reasoning steps. The framework reformats 17 English benchmarks into unified question-answering format and applies capability-specific instructions to assess four dimensions: linguistic knowledge, formal knowledge, world modeling, and social modeling. Each capability is evaluated across three sub-steps (recalling relevant knowledge, utilizing knowledge, solving problems) using automatic metrics like BARTScore-Recall. The knowledge-enhanced method augments inputs with reference rationales (R1 for crystallized step, R2 for fluid step) to improve knowledge utilization and problem-solving performance.

## Key Results
- Significant performance gap between open-source and proprietary models, especially in cognition-related tasks
- Knowledge utilization identified as a key bottleneck, with models struggling to apply recalled knowledge effectively
- Knowledge-enhanced method with reference rationales achieves ~90% of instruction-tuned model performance
- Language and cognitive capabilities show weak correlation, validating the dissociation approach
- Both model size and fine-tuning data quality significantly impact overall performance

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Disassociating language and cognition allows targeted evaluation of specific model capabilities.
- Mechanism: By categorizing LLM capabilities into four dimensions—LINGUISTIC KNOWLEDGE, FORMAL KNOWLEDGE, WORLD MODELING, and SOCIAL MODELING—the framework isolates distinct neural and functional processes, enabling fine-grained assessment of each capability separately.
- Core assumption: Language processing and cognitive processes (like memory and reasoning) are separable in both human brain function and LLM architecture, allowing meaningful distinction between language-related and cognition-related capabilities.
- Evidence anchors:
  - [abstract] "dissociating the language-related capabilities and the cognition-related ones"
  - [section] "Human language processing... robustly attributes language and cognition to different brain areas, namely 'language network' and 'multi-demand network'"
  - [corpus] Weak evidence - neighboring papers focus on capability assessment but not explicitly on dissociating language vs cognition
- Break condition: If language and cognition are deeply entangled in LLM architecture such that separating them loses meaningful distinctions or if capabilities are too correlated to evaluate independently.

### Mechanism 2
- Claim: Breaking down capability application into three sub-steps (knowledge recall, utilization, problem-solving) reveals specific performance bottlenecks.
- Mechanism: By extracting intermediate reasoning steps through chain-of-thought prompting, the framework evaluates crystallized performance (s1), fluid performance (s2), and problem-solving performance (s3) separately, identifying where models fail in the reasoning pipeline.
- Core assumption: LLM reasoning can be decomposed into sequential steps that map to distinct cognitive processes, and evaluating each step provides actionable insights into model limitations.
- Evidence anchors:
  - [abstract] "through extracting the intermediate reasoning from LLMs, we further break down the process of applying a specific capability into three sub-steps"
  - [section] "we expect that the model outputs as: [Thought], [Action], [Answer]"
  - [corpus] Weak evidence - neighboring papers focus on capability assessment but not on stepwise decomposition of reasoning
- Break condition: If intermediate reasoning steps are too noisy, inconsistent, or if the decomposition doesn't align with how LLMs actually process information.

### Mechanism 3
- Claim: Injecting reference rationales improves problem-solving performance by enhancing knowledge utilization.
- Mechanism: By appending reference rationales (R1 for crystallized step, R2 for fluid step) to the input, the framework provides explicit knowledge guidance that helps models better recall and apply relevant information to solve problems.
- Core assumption: Models have encoded relevant knowledge but struggle to retrieve and apply it effectively, and providing explicit references can bridge this gap.
- Evidence anchors:
  - [abstract] "we suggest a knowledge-enhance remedy by incorporating relevant knowledge text as additional input"
  - [section] "R1 contributes slightly to language-related capabilities, while R2 brings about significant improvements to cognition-related ones"
  - [corpus] Weak evidence - neighboring papers don't explicitly discuss knowledge injection methods
- Break condition: If injected knowledge doesn't improve performance or if models over-rely on provided references without developing genuine reasoning capabilities.

## Foundational Learning

- Concept: Few-shot instruction-following with chain-of-thought prompting
  - Why needed here: Enables the model to generate intermediate reasoning steps that can be evaluated separately, revealing the reasoning process rather than just final answers
  - Quick check question: Can you explain how chain-of-thought prompting differs from standard prompting and why it's useful for evaluating reasoning capabilities?

- Concept: Dissociation of language and cognition in neural architecture
  - Why needed here: The framework's design assumes that language processing and cognitive processes are implemented by different neural mechanisms, which is fundamental to its capability categorization
  - Quick check question: What evidence from neuroscience supports the separation of language and cognition, and how might this apply to LLM architecture?

- Concept: Automatic evaluation metrics for free-form rationale generation
  - Why needed here: The framework uses BARTScore-Recall to evaluate the quality of generated rationales, which is essential for the stepwise evaluation approach
  - Quick check question: How does BARTScore-Recall work, and why is it suitable for evaluating the diversity of rationale generation compared to exact match metrics?

## Architecture Onboarding

- Component map: Data preprocessing (benchmark reformulation to QA format) → Instruction design (capability-specific prompts) → Model inference (few-shot instruction-following with CoT) → Intermediate evaluation (s1, s2 scores) → Final evaluation (s3 score) → Knowledge injection (R1, R2 augmentation) → Performance comparison
- Critical path: The evaluation pipeline from input question through intermediate reasoning steps to final answer, with each step providing actionable diagnostic information about model capabilities
- Design tradeoffs: Fine-grained evaluation vs. computational cost (multiple inference runs per question), comprehensive capability coverage vs. benchmark consistency (using existing benchmarks may introduce domain-specific biases)
- Failure signatures: Low s1 but high s2/s3 suggests poor knowledge encoding; high s1 but low s2/s3 suggests knowledge recall without effective utilization; inconsistent performance across capabilities suggests capability-specific weaknesses
- First 3 experiments:
  1. Evaluate a small set of questions across all capabilities to verify the instruction design and intermediate reasoning extraction works as intended
  2. Compare performance of backbone models vs. instruction-tuned models on a single capability to validate the capability dissociation approach
  3. Test knowledge injection with R1 only on a subset of questions to measure the impact on problem-solving performance before implementing full R1+R2 augmentation

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the specific knowledge utilization mechanism that causes the performance gap between open-source and proprietary models?
- Basis in paper: [explicit] The paper identifies knowledge utilization as a key limitation and notes that models have difficulties applying knowledge effectively, with a significant performance gap between open-source and proprietary models in cognition-related tasks.
- Why unresolved: While the paper identifies knowledge utilization as a bottleneck, it does not provide a detailed explanation of what specific mechanisms cause this gap or why proprietary models excel in this area compared to open-source alternatives.
- What evidence would resolve it: Detailed analysis of intermediate reasoning steps showing the exact differences in knowledge utilization patterns between open-source and proprietary models, along with neuroscientific or computational evidence explaining these differences.

### Open Question 2
- Question: How does the quality of instruction-tuning datasets affect different capability dimensions differently?
- Basis in paper: [explicit] The paper notes that there is no single best instruction-tuning dataset across all tasks and that different datasets bring different benefits to LLMs' capabilities, with the knowledge-enhanced method showing varied improvements across capability dimensions.
- Why unresolved: The paper demonstrates that different instruction-tuning datasets have varying effects on model capabilities but does not provide a systematic analysis of how specific dataset characteristics affect different capability dimensions.
- What evidence would resolve it: Comparative analysis of how different instruction-tuning datasets (human-written vs. model-generated, diverse vs. focused) specifically impact linguistic knowledge, formal knowledge, world modeling, and social modeling capabilities.

### Open Question 3
- Question: What is the optimal balance between crystallized and fluid performance for maximizing problem-solving capabilities?
- Basis in paper: [inferred] The paper shows that both crystallized (s1) and fluid (s2) performance contribute to problem-solving (s3), with different models showing varying balances between these intermediate steps.
- Why unresolved: While the paper demonstrates that both intermediate steps matter, it does not investigate what optimal ratio or balance between crystallized and fluid performance would maximize overall problem-solving capabilities.
- What evidence would resolve it: Systematic experiments varying the emphasis on crystallized versus fluid performance in training or prompting, measuring the resulting impact on problem-solving capabilities across different task types.

## Limitations

- The assumption that neuroscience-based dissociation of language and cognition networks directly applies to LLM capability assessment requires more validation
- The knowledge injection approach may lead to pattern matching rather than genuine knowledge utilization improvements
- The correlation between language and cognitive capabilities is weak but not independent, suggesting some entanglement that the framework may not fully account for

## Confidence

- **High confidence**: The methodology for evaluating intermediate reasoning steps (s1, s2, s3) is well-specified and the experimental results showing performance gaps between open-source and proprietary models are reproducible.
- **Medium confidence**: The claim that knowledge utilization is a key bottleneck for LLM performance is supported by experimental evidence but may be influenced by the specific knowledge injection method used.
- **Low confidence**: The assumption that neuroscience-based dissociation of language and cognition networks directly applies to LLM capability assessment requires more validation, as LLMs don't have brain-like architectures.

## Next Checks

1. **Cross-model consistency test**: Evaluate the same set of questions across multiple model families (different architectures, not just sizes) to determine if the capability dissociation holds consistently or is architecture-dependent.
2. **Ablation study on knowledge injection**: Test the knowledge-enhanced method with progressively less explicit guidance (e.g., partial rationales, keyword hints) to measure how much of the improvement comes from genuine knowledge utilization vs. pattern matching.
3. **Correlation robustness check**: Perform correlation analysis across different benchmark subsets and with additional capability dimensions to verify that the weak correlation between language and cognitive capabilities is consistent and not an artifact of the specific benchmark selection.