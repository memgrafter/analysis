---
ver: rpa2
title: Contrastive Representation for Interactive Recommendation
arxiv_id: '2412.18396'
source_url: https://arxiv.org/abs/2412.18396
tags:
- state
- learning
- representation
- prcl
- user
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the sample inefficiency problem in interactive
  recommendation (IR), which is critical for training effective recommender agents.
  The authors propose Contrastive Representation for Interactive Recommendation (CRIR),
  which employs a Preference Ranking Contrastive Learning (PRCL) method to enhance
  state representation.
---

# Contrastive Representation for Interactive Recommendation

## Quick Facts
- arXiv ID: 2412.18396
- Source URL: https://arxiv.org/abs/2412.18396
- Reference count: 38
- This paper proposes CRIR to improve sample efficiency in interactive recommendation using Preference Ranking Contrastive Learning (PRCL)

## Executive Summary
This paper addresses the critical sample inefficiency problem in interactive recommendation (IR) by proposing Contrastive Representation for Interactive Recommendation (CRIR). The method employs Preference Ranking Contrastive Learning (PRCL) to enhance state representation through contrastive learning that doesn't rely on high-level representations or large action sets. CRIR demonstrates significant improvements in sample efficiency compared to baselines like SAC, CRR, PPO, DRR, and NICF, particularly in cold-start settings where it outperforms others in cumulative reward and click-through rate metrics.

## Method Summary
CRIR improves sample efficiency in interactive recommendation by using a state representation network combined with Preference Ranking Contrastive Learning (PRCL). The state representation network extracts high-level preference ranking features from explicit interaction sequences, while PRCL performs contrastive learning using these rankings to construct positive and negative pairs without requiring large action space computations. The method employs an auxiliary training mechanism where PRCL runs independently from RL training, and uses a mixed sampling strategy that combines random sampling for PRCL with prioritized experience replay for RL training to maximize data utilization.

## Key Results
- CRIR significantly improves sample efficiency compared to baselines (SAC, CRR, PPO, DRR, NICF) on Virtual-Taobao and ML-1M simulators
- CRIR achieves higher cumulative reward and click-through rate metrics, particularly in cold-start settings
- The PRCL and data exploiting mechanisms contribute to CRIR's superior performance in interactive recommendation tasks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Contrastive learning can improve sample efficiency in interactive recommendation by learning better state representations without requiring large action space computations.
- Mechanism: CRIR uses a state representation network combined with PRCL. The state representation network generates interest weights for each user interaction, which are then used to rank behaviors. PRCL uses these rankings to construct positive and negative pairs for contrastive learning, applying a Positional Weighted InfoNCE loss that emphasizes more relevant interactions.
- Core assumption: High-level preference ranking features can be extracted from explicit interaction sequences and used to enhance user representation for RL agents.
- Evidence anchors:
  - [abstract] "CRIR efficiently extracts latent, high-level preference ranking features from explicit interaction, and leverages the features to enhance users' representation."
  - [section] "The key insight of PRCL is that it can perform contrastive learning without relying on computations involving high-level representations or large potential action sets."
  - [corpus] Weak evidence - no direct corpus support for this specific mechanism.

### Mechanism 2
- Claim: Using two different sampling strategies (random and PER) for PRCL and RL training simultaneously improves overall learning efficiency.
- Mechanism: The Mixed Mechanism samples one batch randomly from the replay buffer for PRCL and another batch using PER strategy for RL training. This ensures that transitions used for RL also experience contrastive learning, maximizing data utilization.
- Core assumption: Data used for RL training can simultaneously benefit PRCL without interfering with either optimization goal.
- Evidence anchors:
  - [section] "Considering that PRCL is conducted at different stage with the main DRL task, we design an data sampling mechanism, which can achieve both two optimization goals simultaneously."
  - [section] "This mechanism ensures that every transition undergoing reinforcement learning also experiences contrastive learning at least once."
  - [corpus] Weak evidence - no direct corpus support for this specific sampling strategy.

### Mechanism 3
- Claim: Using an auxiliary training mechanism where PRCL runs independently from RL training leads to better performance than constrained training where PRCL loss is added to RL loss.
- Mechanism: CRIR uses an Auxiliary Mechanism where PRCL is conducted separately from RL training. The state representation network is updated through gradients from both PRCL and RL, but PRCL doesn't constrain RL loss directly.
- Core assumption: Independent optimization of PRCL and RL tasks prevents interference between their respective gradient updates.
- Evidence anchors:
  - [section] "Different from prior contrastive methods in DRL, we apply an data exploiting and agent training mechanism to solve problem (iii). In these two mechanisms, PRCL is conducted separately with main DRL task, but can achieve better effect."
  - [section] "As shown in Figure 4(c), the γ value have little effect on the performance. PRCL seems to have no improvement in Constrained Mechanism. This demonstrates the effectiveness of our Auxiliary training strategy."
  - [corpus] Weak evidence - no direct corpus support for this specific training mechanism.

## Foundational Learning

- Concept: Contrastive Learning
  - Why needed here: Traditional recommendation datasets are sparse and cannot provide explicit ratings for every action in interactive recommendation. Contrastive learning allows the model to learn from implicit feedback by comparing similar and dissimilar interactions.
  - Quick check question: How does contrastive learning differ from supervised learning in terms of data requirements and learning objectives?

- Concept: Reinforcement Learning with High-Dimensional State Spaces
  - Why needed here: Interactive recommendation requires encoding user profiles and interaction histories into high-dimensional observations to capture semantic information, which makes sample efficiency a critical challenge.
  - Quick check question: Why is sample efficiency particularly important in interactive recommendation compared to other RL domains?

- Concept: Experience Replay and Sampling Strategies
  - Why needed here: Off-policy RL methods use replay buffers to store historical interactions. Different sampling strategies (random vs. prioritized) affect how efficiently the agent learns from past experiences.
  - Quick check question: What are the trade-offs between random sampling and prioritized experience replay in terms of exploration vs. exploitation?

## Architecture Onboarding

- Component map:
  - User interaction history → State Representation Network → Interest weights + state representation
  - Interest weights → PRCL ranking → Positive/negative pairs
  - PRCL pairs + state representation → Positional Weighted InfoNCE loss → Updated state representation
  - State representation + action → RL training → Policy update
  - All transitions → Mixed sampling → Both PRCL and RL training

- Critical path:
  1. User interaction history → State Representation Network → Interest weights + state representation
  2. Interest weights → PRCL ranking → Positive/negative pairs
  3. PRCL pairs + state representation → Positional Weighted InfoNCE loss → Updated state representation
  4. State representation + action → RL training → Policy update
  5. All transitions → Mixed sampling → Both PRCL and RL training

- Design tradeoffs:
  - Independent vs. constrained PRCL training: Independent training (Auxiliary Mechanism) prevents interference but may miss beneficial gradient sharing
  - Random vs. PER sampling for PRCL: Random sampling ensures diverse contrastive pairs but may miss important transitions
  - Interest weight ranking vs. other selection methods: Ranking provides structured positive/negative pairs but requires accurate weight estimation

- Failure signatures:
  - PRCL fails: Interest weights don't correlate with actual preferences, contrastive pairs don't improve representation quality, gradients from PRCL interfere with RL
  - RL fails: State representation doesn't capture user preferences, agent doesn't learn effective policy, exploration-exploitation balance is wrong
  - Sampling fails: Random sampling misses important transitions, PER sampling overfits to recent experiences, mixed sampling creates conflicting gradients

- First 3 experiments:
  1. Test PRCL effectiveness: Compare CRIR with CRIR w/o CL on Virtual-Taobao - should show PRCL improves sample efficiency
  2. Test sampling mechanism: Compare Mixed Mechanism vs. Divided vs. Combined - Mixed should perform best
  3. Test training mechanism: Compare Auxiliary vs. Constrained - Auxiliary should outperform constrained training

## Open Questions the Paper Calls Out
- The paper does not explicitly call out any open questions, but there are several areas that warrant further investigation based on the limitations discussed in the paper.

## Limitations
- The effectiveness of PRCL heavily depends on the accuracy of interest weight estimation, which is not thoroughly validated across different user behavior patterns.
- The computational efficiency claims are not fully substantiated - while the paper states PRCL avoids large action space computations, it doesn't provide runtime comparisons or complexity analysis against existing methods.
- The cold-start setting evaluation focuses on new users but doesn't address the critical scenario of new items, which is a significant limitation for real-world deployment.

## Confidence

- **High confidence**: CRIR improves sample efficiency compared to baseline DRL methods (DDPG, PPO, SAC) - supported by quantitative results across both evaluation metrics and datasets
- **Medium confidence**: PRCL mechanism is the primary driver of performance gains - while ablation shows PRCL helps, the contribution of individual components within PRCL is not isolated
- **Low confidence**: The mixed sampling mechanism is superior to other sampling strategies - only compared against two alternatives without exploring the full design space of sampling approaches

## Next Checks
1. Conduct sensitivity analysis on interest weight estimation by adding varying levels of noise to the ranking mechanism and measuring impact on final performance
2. Perform runtime profiling to quantify computational savings from PRCL's avoidance of large action space computations compared to traditional methods
3. Extend cold-start evaluation to include new item scenarios by systematically removing popular items from training data and measuring recommendation performance