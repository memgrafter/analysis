---
ver: rpa2
title: Variational Distillation of Diffusion Policies into Mixture of Experts
arxiv_id: '2406.12538'
source_url: https://arxiv.org/abs/2406.12538
tags:
- diffusion
- learning
- task
- should
- experts
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Variational Diffusion Distillation (VDD),
  a novel method that distills pre-trained diffusion policies into Mixture of Experts
  (MoE) models through variational inference. The key challenge addressed is the trade-off
  between diffusion models' expressiveness for complex, multi-modal distributions
  and their drawbacks of long inference times and intractable likelihoods, which limit
  real-time applications like robot control.
---

# Variational Distillation of Diffusion Policies into Mixture of Experts

## Quick Facts
- arXiv ID: 2406.12538
- Source URL: https://arxiv.org/abs/2406.12538
- Reference count: 40
- This paper introduces VDD, a method that distills diffusion policies into MoE models achieving on-par performance with faster inference and tractable likelihoods.

## Executive Summary
This paper addresses the challenge of distilling complex, multi-modal distributions from pre-trained diffusion policies into Mixture of Experts (MoE) models that can be deployed in real-time applications. VDD leverages variational inference with a decompositional upper bound that allows training each expert separately while elegantly utilizing the gradient of the pre-trained score function. The method achieves on-par performance with diffusion models while being significantly faster during inference and having tractable likelihoods for post-hoc analysis.

## Method Summary
VDD distills pre-trained diffusion policies into MoE models by minimizing a variational upper bound of the reverse KL divergence. The key innovation is a decompositional approach that allows each expert to be trained independently using the gradient of the pre-trained diffusion policy's score function, avoiding direct likelihood evaluation. During training, VDD uses an auxiliary distribution to tighten the upper bound while maintaining the decomposition property. The method is evaluated across nine complex behavior learning tasks, demonstrating superior performance compared to existing distillation methods and conventional MoE training approaches.

## Key Results
- VDD accurately distills complex distributions learned by diffusion models across nine behavior learning tasks
- VDD outperforms existing state-of-the-art distillation methods and conventional MoE training approaches
- Distilled MoE policies achieve on-par performance with diffusion models while being 2-3x faster during inference and having tractable likelihoods

## Why This Works (Mechanism)

### Mechanism 1
VDD leverages a decompositional upper bound of the variational objective that allows training each expert separately, resulting in a robust optimization scheme. By introducing an auxiliary distribution q̃(z|a,s), VDD rewrites the reverse KL objective as an upper bound that decomposes into individual per-expert objectives, allowing each expert to be optimized independently without requiring full reparameterization of the entire MoE.

### Mechanism 2
VDD elegantly leverages the gradient of the pre-trained score function, enabling the MoE to benefit from the diffusion model's properties. During the M-Step for updating experts, VDD uses the score function fθ(a,s,t) ≈ ∇_a log π*(a|s) directly in the gradient computation instead of evaluating the intractable likelihood π(a|s), allowing the MoE to learn from the diffusion model's learned representation without requiring likelihood evaluation.

### Mechanism 3
VDD achieves faster inference times than diffusion models while maintaining on-par performance. The MoE architecture allows VDD to select and execute a single expert during inference instead of running the iterative denoising process of diffusion models, reducing inference time while the MoE can still represent complex multi-modal distributions learned from the diffusion model.

## Foundational Learning

- Concept: Variational Inference and KL Divergence
  - Why needed here: VDD is fundamentally based on minimizing the reverse KL divergence between the MoE policy and the diffusion policy using variational inference techniques
  - Quick check question: What is the difference between forward KL and reverse KL divergence, and why does VDD use reverse KL?

- Concept: Denoising Diffusion Models and Score Functions
  - Why needed here: VDD leverages pre-trained diffusion models and their score functions as the teacher model to distill into the MoE policy
  - Quick check question: How do diffusion models use score functions to generate samples, and why are these score functions intractable to evaluate directly?

- Concept: Mixture of Experts (MoE) Architecture
  - Why needed here: VDD specifically targets distilling into MoE models, which require understanding of gating mechanisms, expert specialization, and the challenges of training such models
  - Quick check question: What are the main challenges in training MoE models, and how does VDD's approach address these challenges?

## Architecture Onboarding

- Component map:
  Transformer backbone (shared across all experts) -> Gating network (categorical distribution over experts) -> Expert heads (mean and covariance prediction for each expert) -> Score function integration module (leverages pre-trained diffusion model scores)

- Critical path:
  1. Forward pass through transformer to get shared features
  2. Gating network computes expert selection probabilities
  3. Selected expert computes action distribution
  4. During training: compute loss using score function gradients
  5. During inference: fast single expert evaluation

- Design tradeoffs:
  - Number of experts vs. model complexity: More experts can capture more modes but increase computational cost
  - Expert specialization vs. gating complexity: Well-specialized experts require sophisticated gating
  - Score function accuracy vs. distillation quality: Poor score functions lead to poor MoE learning

- Failure signatures:
  - Mode averaging: If experts are not well-separated, the MoE may produce averaged actions
  - Gating collapse: If one expert dominates, the MoE loses diversity
  - Score function misalignment: Poor guidance from diffusion model scores leads to poor distillation

- First 3 experiments:
  1. Single expert baseline: Test VDD with Z=1 to establish performance floor
  2. Ablation on number of experts: Vary Z to find optimal tradeoff between performance and diversity
  3. Score function sensitivity: Test with different diffusion timesteps to verify robustness

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions, but based on the limitations discussed, several open areas emerge:
- How VDD's performance scales with the number of experts for high-dimensional data like images
- The impact of pre-defining the number of experts on VDD's adaptability to different tasks
- VDD's computational efficiency and scalability compared to other distillation methods

## Limitations
- VDD is not straightforwardly applicable to very high-dimensional data like images due to the MoE's contextual mean and covariance prediction requirements
- The number of experts needs to be pre-defined by the user, which may require extensive tuning or lead to suboptimal performance
- VDD's performance is sensitive to the quality of the pre-trained score functions from the diffusion model

## Confidence

- Mechanism 1 (Variational decomposition): Medium - The theoretical framework is sound, but lacks extensive ablation studies on auxiliary distribution construction
- Mechanism 2 (Score function utilization): High - The gradient-based approach is well-established in diffusion model literature and clearly demonstrated
- Mechanism 3 (Inference speedup): High - The empirical timing results are directly measured and show consistent improvements

## Next Checks

1. Ablation study on auxiliary distribution design: Systematically vary the construction of q̃(z|a,s) to understand its impact on optimization stability and final performance

2. Robustness to teacher model quality: Test VDD performance when using diffusion models trained with different hyperparameter settings or fewer iterations to quantify sensitivity to score function accuracy

3. Scaling analysis: Evaluate VDD's performance and training efficiency as the number of experts increases beyond the tested range to identify practical limits of the approach