---
ver: rpa2
title: 'Bayes without Underfitting: Fully Correlated Deep Learning Posteriors via
  Alternating Projections'
arxiv_id: '2410.16901'
source_url: https://arxiv.org/abs/2410.16901
tags: []
core_contribution: This paper tackles the problem of underfitting in Bayesian deep
  learning, where approximate posteriors often yield less accurate predictions than
  a simple point estimate. The authors propose projecting approximate posteriors onto
  the kernel (null space) of the generalized Gauss-Newton matrix, guaranteeing that
  the Bayesian prediction does not underfit on training data.
---

# Bayes without Underfitting: Fully Correlated Deep Learning Posteriors via Alternating Projections

## Quick Facts
- **arXiv ID**: 2410.16901
- **Source URL**: https://arxiv.org/abs/2410.16901
- **Authors**: Marco Miani; Hrittik Roy; Søren Hauberg
- **Reference count**: 34
- **Primary result**: Projected posterior prevents underfitting by ensuring Bayesian predictions match point estimates on training data

## Executive Summary
This paper addresses the problem of underfitting in Bayesian deep learning, where approximate posteriors often yield less accurate predictions than simple point estimates. The authors propose projecting approximate posteriors onto the kernel of the generalized Gauss-Newton (GGN) matrix, guaranteeing that Bayesian predictions on training data remain unchanged. They develop a matrix-free algorithm for projecting onto this null space that scales linearly with the number of parameters and quadratically with the number of output dimensions. Extensive empirical evaluation shows the approach scales to large models, including vision transformers with 28 million parameters, and provides better uncertainty estimates while matching point estimate accuracy on in-distribution data.

## Method Summary
The method projects an approximate posterior onto the kernel of the generalized Gauss-Newton (GGN) matrix to prevent underfitting. This is achieved through an alternating projections algorithm that iteratively projects onto per-batch GGN kernels without explicitly forming the large GGN matrix. For high-dimensional outputs like generative models, the method uses a loss-projected posterior that projects onto the kernel of the loss-Jacobian instead of the full Jacobian, making it computationally feasible. The approach works with any differentiable model and can handle various approximate posterior distributions, though the paper focuses on Gaussian distributions.

## Key Results
- Projected posterior matches point estimate accuracy on MNIST, Fashion MNIST, CIFAR-10, and SVHN while providing better uncertainty estimates
- Out-of-distribution detection AUROC scores improve from 0.93 (MAP) to 0.96-0.97 (projected posterior)
- Method scales to vision transformers with 28 million parameters on ImageNet
- For generative models, loss-projected posterior produces better samples than baseline methods on CelebA

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Projecting the posterior onto the kernel of the generalized Gauss-Newton (GGN) matrix prevents underfitting by ensuring predictions on training data remain unchanged.
- Mechanism: The GGN matrix encodes which parameter perturbations leave the model's predictions invariant on the training set. Its kernel is the subspace of parameters that yield identical predictions. By restricting the posterior to this subspace, we guarantee that Bayesian predictions on training data match the point estimate.
- Core assumption: The model is sufficiently overparameterized such that the GGN has a non-trivial kernel, and the linearized model adequately approximates the true model locally.
- Evidence anchors:
  - [abstract] "For linearized models, the null space of the generalized Gauss-Newton matrix corresponds to parameters that preserve the training predictions of the point estimate."
  - [section 3] "The projected posterior never underfits... we avoid underfitting when Jθmap(x)(θ−θmap) = 0 on the training data."
- Break condition: If the model is underparameterized or the linearization is poor, the kernel may not capture the relevant invariances, or the approximation may break down.

### Mechanism 2
- Claim: The alternating projections algorithm efficiently computes projections onto the GGN kernel without explicitly forming the large GGN matrix.
- Mechanism: The GGN kernel is decomposed as the intersection of per-batch GGN kernels. Alternating projections iteratively projects onto each per-batch kernel, converging to the full kernel. This avoids inverting a huge NO×NO matrix.
- Core assumption: The alternating projections algorithm converges sufficiently fast for the truncated number of iterations to yield accurate projections.
- Evidence anchors:
  - [section 4] "We propose a linear-time sampling algorithm that can be implemented entirely using automatic differentiation."
  - [section 4] "von Neumann's [1949] alternating projections algorithm projects onto intersections of subspaces."
- Break condition: If the per-batch kernels are nearly orthogonal or the alternating projections converge very slowly, many iterations may be needed, making the method impractical.

### Mechanism 3
- Claim: The loss-projected posterior scales to high-dimensional outputs by projecting onto the kernel of the loss-Jacobian rather than the full Jacobian.
- Mechanism: The loss-Jacobian stacks per-datum loss gradients, which are linear combinations of Jacobian rows. Its kernel contains the full Jacobian kernel, but with fewer rows (O vs NO), making the projection computationally feasible for generative models.
- Core assumption: The loss-Jacobian captures sufficient information to preserve the relevant invariances for the task, and its kernel is a good approximation to the full Jacobian kernel.
- Evidence anchors:
  - [section 4.1] "The kernel of the stacked loss gradients JL θ ∈RN×P contains the kernel of the full Jacobian Jθ∈RNO×P, i.e. ker(Jθ)⊆ker(JL θ)."
  - [section 4.1] "This approach can, thus, be seen as aggregating each per-datum Jacobian into a single meaningful row, lowering the row count by a factor O."
- Break condition: If the loss function does not adequately summarize the information in the Jacobian, or if the kernel containment is too loose, the approximation may lose important invariances.

## Foundational Learning

- Concept: Generalized Gauss-Newton (GGN) matrix
  - Why needed here: The GGN matrix is central to the method as its kernel defines the subspace onto which the posterior is projected to avoid underfitting.
  - Quick check question: What is the relationship between the GGN matrix and the Fisher information matrix for regression problems?

- Concept: Alternating projections algorithm
  - Why needed here: This algorithm is the key computational tool that enables efficient projection onto the GGN kernel without explicitly forming the large GGN matrix.
  - Quick check question: What is the convergence rate of the alternating projections algorithm in terms of the angles between the subspaces?

- Concept: Jacobian and loss-Jacobian
  - Why needed here: The Jacobian and loss-Jacobian define the kernels that capture parameter invariances. Understanding their structure is crucial for grasping the method.
  - Quick check question: How does the loss-Jacobian relate to the Jacobian for a multi-class classification problem with cross-entropy loss?

## Architecture Onboarding

- Component map:
  Input: Model parameters, training data, prior precision
  -> Core: Alternating projections algorithm for GGN kernel projection
  -> Output: Samples from the projected posterior
  -> Supporting: Jacobian-vector and vector-Jacobian product computations

- Critical path:
  1. Compute model Jacobian at MAP estimate
  2. Set up alternating projections algorithm
  3. Iterate projections onto per-batch GGN kernels
  4. Generate samples from projected posterior

- Design tradeoffs:
  - Number of alternating projections iterations vs. projection accuracy
  - Batch size for per-batch GGN kernel computation vs. memory usage
  - Use of full Jacobian vs. loss-Jacobian for high-dimensional outputs

- Failure signatures:
  - Slow convergence of alternating projections (many iterations needed)
  - Numerical instability in Jacobian computations
  - Posterior samples not preserving training predictions (indicating incorrect projection)

- First 3 experiments:
  1. Toy regression with known ground truth to verify underfitting is prevented
  2. Small CNN on MNIST to test computational efficiency and scaling
  3. Comparison of projected posterior vs. diagonal Laplace on a simple classification task to demonstrate the importance of correlations

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the projected posterior perform on high-noise data, where the posterior should have significant uncertainty on the training data?
- Basis in paper: [explicit] The paper states that the applicability to high-noise data remains unresolved and that the theoretical analysis only applies to low-noise data.
- Why unresolved: The theoretical analysis and experiments focus on low-noise scenarios where underfitting is a concern. The behavior on high-noise data may differ significantly.
- What evidence would resolve it: Experiments comparing the projected posterior's performance on high-noise datasets (e.g., with added observation noise) to standard Bayesian methods and point estimates.

### Open Question 2
- Question: What is the impact of different choices for the projection algorithm (e.g., batch size, number of iterations) on the quality of the projected posterior?
- Basis in paper: [explicit] The paper uses specific hyperparameters (1000 iterations, batch size 16) but doesn't explore the sensitivity to these choices.
- Why unresolved: The paper doesn't investigate how these hyperparameters affect the approximation quality or computational efficiency.
- What evidence would resolve it: A systematic study varying batch size and iteration count, measuring the trade-off between approximation accuracy and computational cost.

### Open Question 3
- Question: Can the projected posterior framework be extended to non-Gaussian approximate posteriors, and what benefits would this provide?
- Basis in paper: [inferred] The paper mentions that the algorithm works for any differentiable model and can project samples from any distribution, but only demonstrates with Gaussian posteriors.
- Why unresolved: The current implementation only considers isotropic Gaussian distributions, leaving open the question of how other distributions might perform.
- What evidence would resolve it: Experiments projecting samples from non-Gaussian distributions (e.g., heavy-tailed distributions) and comparing their performance to Gaussian projections.

## Limitations
- The method requires the model to be sufficiently overparameterized for the GGN kernel to be non-trivial
- Alternating projections algorithm requires careful tuning of iteration count and batch size
- Loss-projected posterior approximation may lose some invariances for high-dimensional outputs

## Confidence

**Major uncertainties**: The method's effectiveness depends critically on the model being sufficiently overparameterized and the linearization being accurate locally. For underparameterized models, the GGN kernel may not capture meaningful invariances, potentially leading to poor approximations. The alternating projections algorithm requires careful tuning of iteration count and batch size, with insufficient iterations risking inaccurate projections.

**Confidence assessment**:
- High confidence in the theoretical framework preventing underfitting for linearized models
- Medium confidence in the alternating projections algorithm's practical convergence and scalability
- Medium confidence in the loss-projected posterior's ability to capture invariances for high-dimensional outputs

## Next Checks
1. Test the method on progressively smaller models to identify the overparameterization threshold where the GGN kernel becomes effective
2. Compare alternating projections convergence rates across different architectures and datasets to establish practical iteration requirements
3. Evaluate the approximation error introduced by using loss-Jacobian versus full Jacobian on a simple generative model where both can be computed