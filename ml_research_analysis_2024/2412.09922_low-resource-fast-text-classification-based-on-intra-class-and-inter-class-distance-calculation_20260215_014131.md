---
ver: rpa2
title: Low-Resource Fast Text Classification Based on Intra-Class and Inter-Class
  Distance Calculation
arxiv_id: '2412.09922'
source_url: https://arxiv.org/abs/2412.09922
tags:
- text
- classification
- data
- lftc
- datasets
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses limitations in existing text classification
  methods, including focus only on inter-sentence similarity, high memory consumption,
  and long processing times. It proposes a low-resource and fast text classification
  model (LFTC) that constructs compressor lists for each class to leverage intra-class
  regularity information, removes redundant data, and computes similarity distances
  for classification.
---

# Low-Resource Fast Text Classification Based on Intra-Class and Inter-Class Distance Calculation

## Quick Facts
- arXiv ID: 2412.09922
- Source URL: https://arxiv.org/abs/2412.09922
- Authors: Yanxu Mao; Peipei Liu; Tiehan Cui; Congying Liu; Datao You
- Reference count: 40
- Key outcome: LFTC achieved 91.4% accuracy on KirNews dataset, outperforming BERT's 87.9% while being 4-15x faster than comparable models

## Executive Summary
This paper introduces LFTC (Low-Resource Fast Text Classification), a parameter-free model that leverages data compression algorithms and KNN classification to achieve fast, accurate text classification with minimal computational resources. The model constructs compressor lists for each class to capture intra-class regularity, removes redundant data, and computes similarity distances using both intra-class and inter-class information. On nine benchmark datasets, LFTC demonstrated state-of-the-art performance among non-pretrained models, particularly excelling in few-shot scenarios where it achieved accuracy scores comparable to or better than pretrained models like BERT.

## Method Summary
LFTC is a lightweight text classification model that uses data compression algorithms (Zstd, Gzip) and KNN classification without requiring pre-training. The approach begins by constructing a compressor list for each class to capture intra-class regularity information, then removes redundant information irrelevant to classification to reduce processing time. For classification, the model computes similarity distances using Normalized Compression Distance (NCD) between the prediction text and Gold data, then applies KNN classification based on these distances. This parameter-free approach is particularly suitable for scenarios with scarce labeled data and limited computational resources.

## Key Results
- On KirNews dataset, LFTC achieved 91.4% accuracy compared to BERT's 87.9%
- Processing speed was 4-15 times faster than the lightweight gzip model across different datasets
- In few-shot experiments, LFTC achieved accuracy scores of 81.8% (5-shot), 87.1% (10-shot), 88.8% (50-shot), and 90.5% (100-shot) on KirNews dataset
- LFTC achieved state-of-the-art scores among non-pretrained models on nine benchmark datasets

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LFTC achieves faster processing by removing redundant data irrelevant to classification.
- Mechanism: The model constructs a compressor list for each class to leverage intra-class regularity information, then removes data not needed for classification, significantly reducing processing time.
- Core assumption: Texts within the same class exhibit more regularity compared to those from different classes, allowing effective compression and redundancy removal.
- Evidence anchors:
  - [abstract]: "Our approach begins by constructing a compressor list for each class to fully mine the regularity information within intra-class data. We then remove redundant information irrelevant to the target classification to reduce processing time."
  - [section]: "By employing innovative and efficient data structures, we significantly reduce time complexity and computational overhead."
- Break condition: If texts within the same class do not exhibit sufficient regularity, the compression approach would fail to effectively remove redundant information, leading to increased processing time rather than reduction.

### Mechanism 2
- Claim: LFTC improves performance by computing similarity distances using both intra-class and inter-class information.
- Mechanism: The model uses Normalized Compression Distance (NCD) to measure similarity between prediction text and Gold data, then applies KNN classification based on these distances.
- Core assumption: The combination of intra-class regularities (captured through compression) and inter-class differences (captured through distance calculations) provides more discriminative information for classification than simple pairwise sentence matching.
- Evidence anchors:
  - [abstract]: "Finally, we compute the similarity distance between text pairs for classification."
  - [section]: "The approach fully leverages multiple inter-class and intra-class correlations to achieve text classification tasks."
- Break condition: If the compression distance fails to capture meaningful semantic differences between classes, the KNN classification would perform no better than random guessing.

### Mechanism 3
- Claim: LFTC achieves resource efficiency by eliminating the need for pre-training and parameter-heavy models.
- Mechanism: The model uses a parameter-free approach with data compression algorithms (Zstd, Gzip) and KNN classification, avoiding the computational overhead of pre-trained language models.
- Core assumption: Effective text classification can be achieved without the complex feature representations and large parameter counts of neural network-based models.
- Evidence anchors:
  - [abstract]: "LFTC is a lightweight model, especially suitable for scenarios with scarce labeled data and limited computational resources."
  - [section]: "This approach achieves rapid processing in environments with constrained computational and data resources by optimizing data handling and classification strategies."
- Break condition: If the classification task requires complex feature representations that cannot be captured through simple compression and distance metrics, the model would underperform compared to pre-trained approaches.

## Foundational Learning

- Concept: Data Compression and Information Theory
  - Why needed here: The model fundamentally relies on compression algorithms (Zstd, Gzip) and information-theoretic concepts like Normalized Compression Distance to measure text similarity.
  - Quick check question: How does the Normalized Compression Distance formula (NCD = (C(TCi Y) - min(C(Y), C(TCi))) / max(C(Y), C(TCi))) measure the similarity between two texts?

- Concept: K-Nearest Neighbors (KNN) Classification
  - Why needed here: After computing compression distances, the model uses KNN to classify texts based on their proximity to labeled examples in the compressed space.
  - Quick check question: What happens when K=1 in KNN classification, and how does this affect the model's decision boundary compared to larger K values?

- Concept: Text Preprocessing and Tokenization
  - Why needed here: The model requires proper text segmentation and preprocessing to construct meaningful compressor lists and calculate accurate compression distances.
  - Quick check question: How does the step size parameter S in the compressor list construction affect the granularity of intra-class regularity capture?

## Architecture Onboarding

- Component map:
  - Compressor List Construction: Divides training data into segments and builds Zstd compressors for each class
  - Multi Compressor Classification: Applies constructed compressors to input text, calculates scores, identifies top candidates
  - Centralized Reasoning: Uses Gzip compression and NCD to measure similarity with Gold data, applies KNN for final classification

- Critical path: Input text → Multi Compressor Classification → Centralized Reasoning → Final prediction
- Design tradeoffs: 
  - Speed vs. accuracy: Using multiple compressors improves accuracy but increases processing time
  - Compression level vs. resource usage: Higher compression ratios provide better regularity capture but require more computational resources
  - K value in KNN vs. generalization: Lower K values make the model more sensitive to noise, higher K values may smooth over important distinctions

- Failure signatures:
  - Poor accuracy: Could indicate insufficient intra-class regularity, inappropriate compression parameters, or inadequate KNN K value
  - Slow processing: May result from too many compressors in the list or inefficient implementation of compression algorithms
  - High memory usage: Could occur if compressor lists become too large or if Gold data storage is not optimized

- First 3 experiments:
  1. Test the compressor list construction with varying step sizes (S) on a small dataset to find the optimal balance between granularity and computational efficiency
  2. Evaluate the effect of different K values in KNN classification on the R8 dataset to determine the best trade-off between accuracy and robustness
  3. Compare processing times and accuracy across different compression algorithms (Zstd vs. Gzip) on the AGnews dataset to identify the most efficient approach for this model

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How would adjusting compression levels for individual data points within datasets impact LFTC's performance compared to the current uniform adjustment approach?
- Basis in paper: Explicit - The authors mention in the Limitations section that they did not adjust compression levels for each individual data point within datasets and speculate that more targeted adjustments could obtain better performance scores.
- Why unresolved: The paper only mentions this as a speculation without empirical validation. Testing different compression levels for individual data points would require extensive experimentation across multiple datasets.
- What evidence would resolve it: Controlled experiments comparing LFTC's performance using uniform vs. individualized compression levels across various datasets, measuring both accuracy and processing time.

### Open Question 2
- Question: Would incorporating a third similar result in the Centralized Reasoning module improve LFTC's classification accuracy without significantly increasing computational overhead?
- Basis in paper: Explicit - The authors mention attempting to consider the third similar result but did not achieve the expected scores, though they don't provide specific performance metrics or explain why the third result was not beneficial.
- Why unresolved: The paper only briefly mentions this attempt without detailed results or analysis of why it failed, leaving uncertainty about whether this approach could be optimized.
- What evidence would resolve it: Systematic experiments testing different K values in KNN (K=1, K=2, K=3) across multiple datasets, comparing accuracy gains against computational cost increases.

### Open Question 3
- Question: How would extending LFTC's compressor architecture to image classification tasks compare to traditional neural network approaches in terms of both performance and computational efficiency?
- Basis in paper: Explicit - The authors discuss plans to extend their compressor architecture to image classification in the Future Work section, citing recent research on compressor distance metrics outperforming traditional models.
- Why unresolved: This is proposed future work that has not been implemented or tested, making it unclear whether the compression-based approach would translate effectively to image data.
- What evidence would resolve it: Comparative studies applying LFTC's compressor architecture to image classification benchmarks, measuring performance against both traditional neural networks and existing compression-based image classification methods.

## Limitations
- The evaluation lacks detailed analysis of memory consumption patterns and scalability beyond tested dataset sizes
- Performance on extremely imbalanced datasets or highly specialized domains remains unclear
- The paper does not provide ablation studies to quantify individual contributions of compressor list construction versus KNN classification to performance gains

## Confidence
- High confidence: The speed improvement claims (4-15x faster than gzip) are well-supported by experimental results across multiple datasets
- Medium confidence: The accuracy improvements over non-pretrained baselines are convincing, but direct comparisons with other lightweight methods would strengthen these claims
- Medium confidence: The few-shot learning performance is impressive, though results are limited to specific shot sizes (5, 10, 50, 100) and may not generalize to other few-shot scenarios

## Next Checks
1. **Ablation study**: Run experiments with only the Multi Compressor Classification module (without Centralized Reasoning) and only the Centralized Reasoning module (without compressor lists) to quantify their individual contributions to accuracy and speed.
2. **Memory profiling**: Measure and report memory usage during both training and inference phases across different dataset sizes to verify the "low-resource" claim beyond just processing speed.
3. **Scalability testing**: Evaluate the model on datasets significantly larger than the current benchmarks (e.g., >100k documents) to assess how the compressor list approach scales and whether processing time advantages are maintained.