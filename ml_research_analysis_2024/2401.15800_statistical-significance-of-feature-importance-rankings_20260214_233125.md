---
ver: rpa2
title: Statistical Significance of Feature Importance Rankings
arxiv_id: '2401.15800'
source_url: https://arxiv.org/abs/2401.15800
tags:
- shapley
- features
- feature
- lime
- samples
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces RankSHAP and RankLIME, two methods for obtaining
  stable feature importance rankings in machine learning models. The key problem addressed
  is the instability of popular feature attribution methods like SHAP and LIME due
  to random sampling, which undermines their credibility and reproducibility.
---

# Statistical Significance of Feature Importance Rankings

## Quick Facts
- arXiv ID: 2401.15800
- Source URL: https://arxiv.org/abs/2401.15800
- Reference count: 40
- One-line primary result: Introduces RankSHAP and RankLIME methods that provide statistically significant feature importance rankings while controlling Family-Wise Error Rate

## Executive Summary
This paper addresses the critical problem of instability in popular feature attribution methods like SHAP and LIME, which rely on random sampling and thus produce inconsistent rankings across runs. The authors propose RankSHAP and RankLIME, two methods that use statistical hypothesis testing to ensure the top-K ranked features are correct with high probability (1-α). These methods iteratively test pairwise comparisons between adjacent features until all K top features are correctly ordered, providing both statistical rigor and computational efficiency.

The core contribution is a framework that transforms the problem of feature ranking from an optimization task to a statistical inference problem. By controlling the Family-Wise Error Rate across multiple hypothesis tests, RankSHAP and RankLIME provide guarantees about the correctness of feature rankings that traditional methods cannot offer. The approach is particularly valuable in high-stakes applications where reproducibility and interpretability of machine learning models are essential.

## Method Summary
RankSHAP and RankLIME introduce statistical hypothesis testing to feature importance ranking. For RankSHAP, the method iteratively tests pairwise comparisons between adjacent features in the ranking using a closed-form estimator for Shapley values. The algorithm continues testing until all K top features are correctly ordered with confidence 1-α. RankLIME applies hypothesis testing to each feature selection in the LASSO path, controlling the Family-Wise Error Rate across multiple comparisons. Both methods adaptively focus computational effort on features whose rankings are both high and ambiguous, providing efficiency gains over naive sampling approaches.

## Key Results
- RankSHAP and RankLIME successfully control the Family-Wise Error Rate (FWER) at the specified level α across multiple benchmark datasets
- RankSHAP is highly efficient, adaptively focusing computational effort on features whose rankings are both high and ambiguous
- RankLIME dramatically improves the stability of LIME feature selections compared to the standard algorithm
- On the Adult dataset, RankSHAP achieved FWERs of 0.92, 0.24, and 0.12 for different configurations, while standard Shapley Sampling had much higher FWERs

## Why This Works (Mechanism)
The methods work by converting the ranking problem into a statistical inference task. Instead of computing point estimates for feature importance and ranking them directly, RankSHAP and RankLIME treat each pairwise comparison as a hypothesis test. By controlling the Family-Wise Error Rate across all comparisons needed to establish the top-K ranking, the methods provide statistical guarantees about the correctness of the final ranking. The adaptive testing strategy focuses computational resources on ambiguous comparisons while quickly confirming clear ones, achieving both statistical rigor and efficiency.

## Foundational Learning
- Family-Wise Error Rate (FWER): The probability of making at least one false discovery when performing multiple hypothesis tests. Critical for understanding the statistical guarantees provided by RankSHAP and RankLIME.
  - Why needed: Ensures that the methods don't produce false positives in feature rankings
  - Quick check: Verify that FWER ≤ α across all hypothesis tests in the ranking procedure

- Shapley values: A game-theoretic approach to feature attribution that fairly distributes credit among features based on their marginal contributions.
  - Why needed: Forms the basis for RankSHAP's statistical testing framework
  - Quick check: Confirm that marginal contributions are computed correctly for each feature subset

- LASSO path: The sequence of models obtained by varying the regularization parameter in LASSO regression.
  - Why needed: Provides the foundation for RankLIME's feature selection procedure
  - Quick check: Verify that feature inclusion/exclusion follows the correct LASSO path

## Architecture Onboarding

Component map: Data -> Feature Importance Estimation -> Pairwise Comparison Tests -> FWER Control -> Final Ranking

Critical path: The algorithm iteratively performs pairwise comparisons between adjacent features in the current ranking. For each comparison, it computes the difference in feature importance and tests whether this difference is statistically significant. The process continues until all K top features are correctly ordered or computational budget is exhausted.

Design tradeoffs: The primary tradeoff is between statistical rigor and computational efficiency. RankSHAP and RankLIME sacrifice some computational efficiency compared to point-estimate methods to gain statistical guarantees. However, the adaptive testing strategy mitigates this by focusing effort where it's most needed.

Failure signatures: If the FWER is not properly controlled, the method may produce incorrect rankings with higher probability than specified. This could manifest as unstable rankings across different runs or datasets. Additionally, if the statistical tests lack power (e.g., due to insufficient samples), the method may fail to distinguish between truly important and unimportant features.

First experiments:
1. Run RankSHAP on a simple dataset with known feature importances to verify correct ranking behavior
2. Compare FWER control across different values of α to ensure statistical guarantees hold
3. Measure computational efficiency gains of RankSHAP versus naive sampling on datasets of varying size

## Open Questions the Paper Calls Out
None

## Limitations
- Computational efficiency gains are primarily demonstrated on relatively small benchmark datasets, and scalability to high-dimensional problems remains an open question
- The comparison framework focuses primarily on SHAP and LIME, leaving unexplored how these methods compare to other stable feature importance approaches
- The paper assumes feature independence in its statistical testing framework, which may not hold in many real-world datasets with correlated features

## Confidence
- Scalability to high-dimensional problems: Medium confidence
- Comparison to alternative methods: Low confidence
- Validity under feature correlation: Medium confidence

## Next Checks
1. Test RankSHAP and RankLIME on high-dimensional datasets (>100 features) to evaluate scalability and computational efficiency claims
2. Compare RankSHAP's performance against permutation-based feature importance methods under correlated feature scenarios
3. Evaluate the impact of feature correlation on the statistical validity of pairwise comparisons in the ranking procedure