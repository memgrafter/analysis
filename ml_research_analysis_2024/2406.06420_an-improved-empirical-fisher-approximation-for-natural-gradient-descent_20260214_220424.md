---
ver: rpa2
title: An Improved Empirical Fisher Approximation for Natural Gradient Descent
arxiv_id: '2406.06420'
source_url: https://arxiv.org/abs/2406.06420
tags:
- training
- loss
- update
- approximation
- which
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the poor approximation quality of empirical
  Fisher (EF) methods in Natural Gradient Descent (NGD) optimization for deep learning
  models. The EF method, while easy to implement, suffers from an "inversely-scaled
  projection issue" that biases updates towards well-trained samples and leads to
  inefficient training trajectories.
---

# An Improved Empirical Fisher Approximation for Natural Gradient Descent

## Quick Facts
- arXiv ID: 2406.06420
- Source URL: https://arxiv.org/abs/2406.06420
- Authors: Xiaodong Wu; Wenyi Yu; Chao Zhang; Philip Woodland
- Reference count: 40
- Key outcome: The paper proposes an improved empirical Fisher (iEF) method that resolves the inversely-scaled projection issue in standard empirical Fisher methods, providing better approximation quality to exact Natural Gradient updates and strong convergence across various fine-tuning tasks.

## Executive Summary
This paper addresses the limitations of empirical Fisher (EF) methods in Natural Gradient Descent (NGD) optimization for deep learning. The EF method, while easy to implement, suffers from an "inversely-scaled projection issue" that biases updates towards well-trained samples and leads to inefficient training trajectories. The authors propose an improved empirical Fisher (iEF) method that resolves this issue by introducing a diagonal scaling matrix to the standard EF formulation. The iEF method is motivated as an efficient approximation to the Gauss-Newton algorithm from a loss reduction perspective and is shown to guarantee global convergence under mild assumptions.

## Method Summary
The paper proposes an improved empirical Fisher (iEF) method to address the poor approximation quality of empirical Fisher (EF) methods in Natural Gradient Descent (NGD) optimization. The EF method suffers from an "inversely-scaled projection issue" that biases updates towards well-trained samples. The iEF method resolves this issue by introducing a diagonal scaling matrix to the standard EF formulation, motivated as an efficient approximation to the Gauss-Newton algorithm from a loss reduction perspective. The iEF method guarantees global convergence under mild assumptions and demonstrates strong convergence and generalization across various tasks, including fine-tuning pre-trained models with LoRA and Prompt-Tuning on GLUE tasks, and ViT with LoRA for CIFAR100.

## Key Results
- The iEF method resolves the inversely-scaled projection issue in standard EF methods, providing better approximation quality to exact Natural Gradient updates.
- iEF demonstrates strong convergence and generalization across various tasks, including fine-tuning pre-trained models with LoRA and Prompt-Tuning on GLUE tasks, and ViT with LoRA for CIFAR100.
- The proposed iEF method is robust to damping across tasks and training stages, outperforming both EF and sampled Fisher methods.

## Why This Works (Mechanism)
The iEF method works by introducing a diagonal scaling matrix to the standard EF formulation, which resolves the inversely-scaled projection issue that biases updates towards well-trained samples. This improvement is motivated as an efficient approximation to the Gauss-Newton algorithm from a loss reduction perspective, ensuring better approximation quality to exact Natural Gradient updates and global convergence under mild assumptions.

## Foundational Learning

1. **Empirical Fisher (EF) Method**: A computationally efficient approximation of the Fisher Information Matrix used in Natural Gradient Descent (NGD) optimization.
   - Why needed: To enable efficient implementation of NGD in deep learning models.
   - Quick check: Verify that the EF method is a diagonal approximation of the Fisher Information Matrix.

2. **Inversely-Scaled Projection Issue**: A limitation of the EF method where updates are biased towards well-trained samples, leading to inefficient training trajectories.
   - Why needed: Understanding this issue is crucial for developing improved methods that address the bias in EF updates.
   - Quick check: Confirm that the iEF method resolves this issue by introducing a diagonal scaling matrix.

3. **Gauss-Newton Algorithm**: A method for solving non-linear least squares problems, used as a motivation for the iEF method.
   - Why needed: To provide a theoretical foundation for the iEF method as an efficient approximation.
   - Quick check: Verify that the iEF method is motivated as an efficient approximation to the Gauss-Newton algorithm from a loss reduction perspective.

## Architecture Onboarding

**Component Map:**
Model Parameters -> Empirical Fisher Matrix -> Improved Empirical Fisher Matrix -> Parameter Updates

**Critical Path:**
1. Compute gradients for each training sample.
2. Estimate the empirical Fisher matrix using the gradients.
3. Apply the diagonal scaling matrix to obtain the improved empirical Fisher matrix.
4. Compute parameter updates using the iEF matrix and apply them to the model.

**Design Tradeoffs:**
- The iEF method introduces additional computational overhead compared to the standard EF method due to the diagonal scaling matrix.
- However, the improved approximation quality and convergence properties of iEF may outweigh the increased computational cost.

**Failure Signatures:**
- If the diagonal scaling matrix is not properly tuned, the iEF method may not effectively resolve the inversely-scaled projection issue.
- Poor initialization of the scaling matrix can lead to suboptimal convergence or even divergence.

**First Experiments:**
1. Fine-tune a pre-trained T5-base model with LoRA and Prompt-Tuning on a GLUE task using the iEF method and compare its performance to the standard EF method.
2. Train a ViT model with LoRA for CIFAR100 classification using the iEF method and evaluate its convergence and generalization compared to the EF method.
3. Analyze the sensitivity of the iEF method to different damping parameters and initialization strategies to identify optimal configurations.

## Open Questions the Paper Calls Out
None

## Limitations
- The global convergence guarantees of the iEF method are based on mild assumptions that require further clarification and practical validation.
- The experimental results are limited to specific fine-tuning scenarios, and the scalability of iEF to larger models and datasets needs to be assessed.
- The sensitivity of iEF to different damping parameters and initialization strategies requires further analysis to identify optimal configurations.

## Confidence
- **High confidence**: The proposed iEF method effectively resolves the inversely-scaled projection issue in empirical Fisher methods, as supported by theoretical motivation and experimental results.
- **Medium confidence**: The global convergence guarantees of iEF under mild assumptions require further clarification and practical validation.
- **Medium confidence**: The iEF method shows consistent improvement over EF and sampled Fisher methods across diverse tasks, but the experimental results are limited to specific fine-tuning scenarios.

## Next Checks
1. Test iEF on larger-scale models and datasets to assess its scalability and generalization capabilities.
2. Analyze the sensitivity of iEF to different damping parameters and initialization strategies to identify optimal configurations.
3. Compare iEF with other second-order optimization methods, such as K-FAC and Shampoo, to evaluate its relative performance and computational efficiency.