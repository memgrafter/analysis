---
ver: rpa2
title: 'Understanding Warmup-Stable-Decay Learning Rates: A River Valley Loss Landscape
  Perspective'
arxiv_id: '2410.05192'
source_url: https://arxiv.org/abs/2410.05192
tags:
- learning
- river
- loss
- rate
- will
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper studies Warmup-Stable-Decay (WSD) learning rate schedules
  for training language models. The key idea is that WSD maintains a constant learning
  rate during most of training, then rapidly decays near the end, enabling flexible
  checkpointing without predefining total compute.
---

# Understanding Warmup-Stable-Decay Learning Rates: A River Valley Loss Landscape Perspective

## Quick Facts
- arXiv ID: 2410.05192
- Source URL: https://arxiv.org/abs/2410.05192
- Reference count: 40
- Key outcome: WSD-S learning rate schedule outperforms WSD and cyclic-cosine, achieving competitive results with oracle-tuned cosine schedules while enabling flexible checkpointing

## Executive Summary
This paper introduces Warmup-Stable-Decay (WSD) learning rate schedules for training language models, which maintain constant learning rates during most training then rapidly decay near the end. The key innovation is WSD-S, which reuses decay phases from previous checkpoints without rollback, enabling efficient continual training. The authors propose a theoretical framework based on a "river valley" loss landscape where large learning rates enable faster progress along the river direction despite oscillations in mountain directions, while decay phases reveal this progress by reducing oscillations.

## Method Summary
The WSD-S schedule implements an inverse proportional decay where the learning rate decreases as LR = LR_peak * (1 - t/T) during the decay phase, with the decay starting at a fraction of total training steps. The method trains LLaMA models (0.1B-1.2B parameters) on the Pile dataset using context length 4096, batch size 4M tokens, with peak learning rates of 6e-4 for smaller models and 4e-4 for larger ones. The decay phase timing is critical, with optimal performance achieved when decay occurs between 8-12% of total training steps.

## Key Results
- WSD-S outperforms both WSD and cyclic-cosine learning rate schedules on LLaMA models
- The method achieves competitive validation loss with oracle-tuned cosine schedules
- Performance is robust to decay phase timing within 8-12% of total training steps
- WSD-S enables efficient continual training by reusing decay phases from previous checkpoints

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Large learning rates enable faster progress along the "river" direction of the loss landscape despite increased oscillations in the "mountain" direction.
- **Mechanism:** The loss landscape is modeled as a river valley with a deep valley (river) and steep hillsides (mountains). Large learning rates cause larger oscillations between hillsides but accelerate implicit progress along the river.
- **Core assumption:** The loss exhibits a river valley landscape where the gradient aligns with the minimal eigenvector of the Hessian along a 1D manifold (the river).
- **Evidence anchors:**
  - [abstract] "During the stable phase, the iterate undergoes large oscillations due to the high learning rate, yet it progresses swiftly along the river."
  - [section] "A large learning rate yields larger bouncing due to the stochasticity of the gradient, increasing the hill component of the loss, but it also makes faster progress in the river direction."
  - [corpus] Weak corpus support - only one related paper mentions "valley-river model" but without detailed mechanism analysis.
- **Break condition:** If the loss landscape does not exhibit the river valley structure, large learning rates may only cause harmful oscillations without the compensating river progress.

### Mechanism 2
- **Claim:** Decay phases reveal the true optimization progress by reducing oscillations and moving iterates closer to the river.
- **Mechanism:** During decay, the rapidly dropping learning rate minimizes oscillations in the mountain direction, allowing the iterate to converge near the river and reveal the progress made during the stable phase.
- **Core assumption:** The hill component of the loss (caused by oscillations) masks the true progress in the river direction during the stable phase.
- **Evidence anchors:**
  - [abstract] "During the decay phase, the rapidly dropping learning rate minimizes the iterate's oscillations, moving it closer to the river and revealing true optimization progress."
  - [section] "As training nears completion, it becomes essential to reduce the learning rate. This decay minimizes the oscillations in the mountain direction to decrease the hill component and ensures that the iterates converge to a point close to the river."
  - [corpus] No direct corpus evidence for this specific mechanism.
- **Break condition:** If decay occurs too early or too late relative to the optimization trajectory, the benefit of revealing true progress may be lost.

### Mechanism 3
- **Claim:** Data uncertainty variation shapes the river valley landscape, with deterministic tokens forming the river and uncertain tokens creating steep hillsides.
- **Mechanism:** Tokens with high predictability (deterministic facts) correspond to flatter loss directions (river), while ambiguous tokens create sharper loss components (hillsides). This heterogeneity creates the river valley structure.
- **Core assumption:** Different tokens have varying levels of predictability, and this uncertainty directly translates to loss landscape curvature.
- **Evidence anchors:**
  - [abstract] "We hypothesize the river valley landscape can naturally arise from the heterogeneity in the stochasticity of different tokens: highly deterministic tokens... contribute to the 'river' direction, while uncertain tokens... create the steep hillsides."
  - [section] "When predicting a deterministic fact, a large learning rate can boost the model's confidence, accelerating learning. However, when the next token is inherently ambiguous... the model must learn a calibrated distribution, which may necessitate a smaller step size."
  - [corpus] No corpus evidence supporting this specific hypothesis about token uncertainty shaping loss landscapes.
- **Break condition:** If token uncertainty does not correlate with loss landscape sharpness, this mechanism fails to explain the river valley structure.

## Foundational Learning

- **Concept:** Stochastic Gradient Descent (SGD) dynamics on non-convex loss landscapes
  - Why needed here: The paper analyzes how SGD trajectories behave on the river valley landscape, requiring understanding of SGD's behavior on complex geometries
  - Quick check question: How does SGD's stochasticity interact with sharp and flat directions in a loss landscape?

- **Concept:** Eigenvalue decomposition and Hessian analysis
  - Why needed here: The river valley assumption relies on the Hessian having a clear eigengap between the minimal eigenvalue (river direction) and others (mountain directions)
  - Quick check question: What conditions on the Hessian ensure a well-defined river direction?

- **Concept:** Learning rate schedule design principles
  - Why needed here: Understanding why different learning rate schedules (cosine vs WSD vs WSD-S) affect optimization differently requires knowledge of schedule design tradeoffs
  - Quick check question: How do different learning rate decay schedules affect convergence speed and final loss?

## Architecture Onboarding

- **Component map:** WSD-S schedule implementation -> LLaMA model training -> Pile dataset processing -> TPU v3-256 training with Levanter framework -> Validation loss evaluation

- **Critical path:**
  1. Model initialization and warmup
  2. Stable phase with constant learning rate
  3. Decay phase with rapid learning rate reduction
  4. Checkpointing and potential resumption
  5. Evaluation of multiple compute budgets

- **Design tradeoffs:**
  - Large learning rates: faster river progress vs higher oscillation cost
  - Decay timing: revealing true progress vs maintaining optimization momentum
  - Checkpointing strategy: simplicity of WSD-S vs flexibility of WSD

- **Failure signatures:**
  - Excessive loss spikes during stable phase (learning rate too high)
  - Insufficient final loss improvement (decay too late or too mild)
  - Poor performance across multiple compute budgets (schedule parameters misaligned)

- **First 3 experiments:**
  1. Implement WSD schedule on small LLaMA model (0.1B parameters) with varying decay fractions (4%, 8%, 10%, 12%) to test sensitivity
  2. Compare WSD vs WSD-S on medium model (0.6B parameters) with fixed decay timing to validate performance gains
  3. Validate river valley landscape hypothesis by interpolating between checkpoints during stable and decay phases and measuring loss behavior

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the river valley landscape theory be extended to k-dimensional rivers where k > 1?
- Basis in paper: [inferred] The paper explicitly mentions this as a potential extension in Section 3.5, stating "There are several other potential technical improvements to our theory that could be explored in future works. First, the analysis of the stochastic setting may be extended to include a river that is not a straight line."
- Why unresolved: The current theory assumes a 1-dimensional river, and extending to higher dimensions would require new mathematical tools to handle the coupling between the dynamics in the original river valley landscape and the dynamics constrained on the generalized river.
- What evidence would resolve it: A formal proof showing that the optimization dynamics can be decomposed into progress along the k-dimensional river and oscillations in the orthogonal directions, with similar properties to the 1D case.

### Open Question 2
- Question: What is the optimal fraction of training steps to spend in the decay phase for different model sizes and datasets?
- Basis in paper: [explicit] The paper shows in Figure 13 that WSD-S is relatively insensitive to decay fraction between 8-12%, but notes this was tested on specific models (0.1B-1.2B) and dataset (Pile).
- Why unresolved: The paper only tests a narrow range of decay fractions (4-14%) on four model sizes with one dataset, leaving open whether this insensitivity holds across different scales, architectures, or data distributions.
- What evidence would resolve it: Systematic experiments varying decay fraction across diverse model sizes (e.g., 0.01B to 100B parameters), architectures (e.g., transformers, MLPs), and datasets with different token uncertainty distributions.

### Open Question 3
- Question: How does the river valley landscape emerge from more complex language model architectures and data distributions beyond the simple bigram model?
- Basis in paper: [explicit] The paper proposes a hypothesis in Section 4 that uncertainty variation in data distribution shapes the river valley landscape, and validates this with a bigram toy model, but acknowledges this is a simplified case.
- Why unresolved: The bigram model is highly simplified compared to real language models with deep architectures trained on complex, heterogeneous data. The connection between token uncertainty and loss landscape sharpness needs validation in more realistic settings.
- What evidence would resolve it: Empirical studies analyzing the relationship between token uncertainty (e.g., entropy of next-token predictions) and local loss landscape curvature across different layers and model sizes on real datasets, potentially using techniques like Hessian analysis or directional sharpness measurements.

## Limitations

- The river valley landscape theory remains a hypothesis without rigorous empirical validation across diverse architectures and datasets
- The mechanism connecting token uncertainty to loss landscape curvature is speculative with no corpus evidence or detailed experimental validation
- The theoretical framework's generalizability to non-transformer architectures and non-language tasks is unproven

## Confidence

- **High confidence**: Empirical results showing WSD-S outperforms WSD and cyclic-cosine on LLaMA models for the specific experimental setup
- **Medium confidence**: Theoretical mechanism explaining why WSD schedules work, though core assumptions require further validation
- **Low confidence**: Claim that river valley landscape naturally arises from token uncertainty heterogeneity, which lacks direct experimental evidence

## Next Checks

1. **Landscape validation experiment**: Train a small transformer on a synthetic dataset with controlled token uncertainty (mix of deterministic and ambiguous tokens), then measure Hessian eigenvalues at various checkpoints to empirically verify the river valley structure.

2. **Architecture generalization test**: Implement WSD-S on non-transformer architectures (CNN for image classification, RNN for sequence modeling) and compare performance against standard schedules to validate whether the river valley landscape is universal.

3. **Ablation on decay timing sensitivity**: Systematically vary decay phase timing (4%, 6%, 8%, 10%, 12%, 14%) across multiple model sizes and measure the resulting validation loss distribution to quantify claimed robustness.