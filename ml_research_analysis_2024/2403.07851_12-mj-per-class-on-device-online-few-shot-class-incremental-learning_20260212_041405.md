---
ver: rpa2
title: 12 mJ per Class On-Device Online Few-Shot Class-Incremental Learning
arxiv_id: '2403.07851'
source_url: https://arxiv.org/abs/2403.07851
tags:
- learning
- class
- classes
- backbone
- o-fscil
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses few-shot class-incremental learning (FSCIL)
  at the extreme edge, enabling models to learn new classes from only a few labeled
  examples without forgetting prior knowledge. The authors introduce O-FSCIL, a lightweight
  architecture combining a pretrained and metalearned feature extractor with an expandable
  explicit memory storing class prototypes.
---

# 12 mJ per Class On-Device Online Few-Shot Class-Incremental Learning

## Quick Facts
- arXiv ID: 2403.07851
- Source URL: https://arxiv.org/abs/2403.07851
- Authors: Yoga Esa Wibowo; Cristian Cioflan; Thorir Mar Ingolfsson; Michael Hersche; Leo Zhao; Abbas Rahimi; Luca Benani
- Reference count: 37
- Primary result: Achieves 68.62% accuracy on FSCIL CIFAR100 benchmark while consuming only 12 mJ per new class

## Executive Summary
This paper introduces O-FSCIL, a lightweight architecture for few-shot class-incremental learning (FSCIL) at the extreme edge. The system combines a frozen pretrained backbone with an expandable explicit memory storing class prototypes, enabling models to learn new classes from only a few labeled examples without forgetting prior knowledge. By employing orthogonal regularization during pretraining and multi-margin loss during metalearning, the approach improves feature separability and generalization. The architecture is specifically designed for ultra-low-power microcontrollers, achieving state-of-the-art accuracy while consuming minimal energy per class.

## Method Summary
The O-FSCIL methodology involves pretraining a MobileNetV2 backbone with feature orthogonality regularization and data augmentation (Mixup and Cutmix) to generate meaningful feature representations. Metalearning is then performed using a multi-margin loss to enhance feature clustering and prevent overfitting to few labeled samples. The quantized model is deployed on the GAP9 MCU, where the frozen backbone extracts features that are projected to prototypes and stored in an explicit memory. When learning new classes, the system extends the memory with novel class prototypes while keeping the rest of the architecture frozen, enabling online learning with minimal energy consumption.

## Key Results
- Achieves state-of-the-art accuracy of 68.62% on the FSCIL CIFAR100 benchmark
- Consumes as little as 12 mJ per new class on GAP9 MCU
- Demonstrates real-time online learning capabilities with minimal energy usage
- Successfully prevents catastrophic forgetting by freezing the backbone and updating only the memory

## Why This Works (Mechanism)

### Mechanism 1
Freezing the backbone and only updating the explicit memory prevents catastrophic forgetting while enabling online learning. The pretrained backbone provides stable feature extraction, while the explicit memory stores class prototypes as averaged feature vectors that can be appended without modifying the backbone. This decouples representation learning from class addition, assuming the frozen backbone produces discriminative features that remain relevant as new classes are added.

### Mechanism 2
Orthogonal regularization during pretraining improves feature separability and generalization to new classes by penalizing non-orthogonal feature vectors. This encourages the network to spread class representations across orthogonal dimensions, creating more available feature space for new classes to occupy without overlap. The approach assumes feature orthogonality in the pretraining space correlates with better clustering and separation during incremental learning.

### Mechanism 3
Multi-margin loss during metalearning improves feature robustness near classification boundaries by pushing features away from other class prototypes with a margin. This creates wider separation between clusters, especially near boundaries where new classes may appear. The mechanism assumes wider inter-class margins during metalearning translate to better separation for novel classes during incremental learning.

## Foundational Learning

- Concept: Catastrophic forgetting in continual learning
  - Why needed here: FSCIL requires learning new classes without degrading performance on old ones; understanding forgetting mechanisms is essential to designing solutions.
  - Quick check question: What happens to model accuracy on base classes when fine-tuning on new classes without any regularization?

- Concept: Prototype-based classification
  - Why needed here: O-FSCIL uses cosine similarity between query features and stored class prototypes; understanding prototype methods is key to grasping the approach.
  - Quick check question: How does cosine similarity between a query and class prototype determine classification in a nearest-prototype classifier?

- Concept: Feature orthogonality and its regularization
  - Why needed here: The paper proposes orthogonal regularization to improve feature space utilization; understanding orthogonality helps grasp why this helps FSCIL.
  - Quick check question: Why might orthogonalizing feature vectors help when adding new classes to an existing feature space?

## Architecture Onboarding

- Component map: Query image → Backbone (MobileNetV2) → FCR → Cosine similarity with EM → Classification
- Critical path: Query image → Backbone → FCR → Cosine similarity with EM → Classification
- Design tradeoffs:
  - MobileNetV2 vs ResNet: 5.7× less compute, 5.2× less storage, slight accuracy drop
  - 3-bit quantization: 9.6 kB memory for 100 classes vs higher precision
  - Frozen backbone: No forgetting but potential representational limitations
- Failure signatures:
  - Accuracy drop on base classes → backbone insufficiently discriminative
  - Poor new class accuracy → EM prototypes not representative or backbone features poor
  - High energy consumption → inefficient deployment or unnecessary fine-tuning
- First 3 experiments:
  1. Measure base session accuracy with and without orthogonal regularization
  2. Test incremental learning accuracy over 8 sessions with frozen vs fine-tuned backbone
  3. Evaluate memory vs accuracy tradeoff by varying prototype bit precision (8-bit, 4-bit, 3-bit)

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the proposed orthogonal regularization affect feature representation in the space between class clusters, and can it be optimized for even better separability?
- Basis in paper: The authors mention that orthogonal regularization addresses dimensionality reduction problems, but the impact on the feature space between class clusters is not fully explored.
- Why unresolved: The paper does not provide a detailed analysis of the feature space between clusters after applying orthogonal regularization, leaving room for further investigation into its optimization.
- What evidence would resolve it: Empirical studies comparing feature distributions with and without orthogonal regularization, along with quantitative metrics on separability improvements, would provide insights into its effectiveness.

### Open Question 2
- Question: What is the impact of different data augmentation techniques, such as Mixup and CutMix, on the model's ability to generalize to novel classes in FSCIL?
- Basis in paper: The authors discuss the use of Mixup and CutMix for feature interpolation, but the specific impact on generalization to new classes is not fully detailed.
- Why unresolved: While the paper mentions improvements in accuracy, it does not provide a comprehensive analysis of how each augmentation technique contributes to generalization in FSCIL scenarios.
- What evidence would resolve it: Comparative studies isolating the effects of Mixup and CutMix on model performance across multiple FSCIL benchmarks would clarify their individual contributions.

### Open Question 3
- Question: How does the proposed O-FSCIL methodology scale with larger datasets and more complex class structures, and what are the potential limitations?
- Basis in paper: The paper focuses on CIFAR100, but does not address scalability to larger datasets or more complex class structures.
- Why unresolved: The scalability of O-FSCIL to more complex and larger datasets is not discussed, leaving questions about its applicability to diverse real-world scenarios.
- What evidence would resolve it: Experiments evaluating O-FSCIL on larger datasets with more intricate class structures, along with analyses of computational and memory requirements, would provide insights into its scalability and limitations.

## Limitations
- The orthogonal regularization's exact formulation and hyperparameters are not specified, making it difficult to assess its specific contribution.
- The multi-margin loss, while showing improved separation near boundaries, lacks comparative analysis against alternative margin-based losses.
- The claim of 12 mJ per class energy consumption is based on deployment on a specific GAP9 MCU without detailing the measurement methodology.

## Confidence
- High confidence: The frozen backbone + prototype memory approach for preventing catastrophic forgetting, as this mechanism is straightforward and well-supported by experimental results.
- Medium confidence: The orthogonal regularization's effectiveness for FSCIL, due to limited theoretical justification and lack of ablation studies.
- Medium confidence: The multi-margin loss contribution, as it shows benefits but lacks comparison to alternative approaches and detailed analysis of margin hyperparameter sensitivity.

## Next Checks
1. Conduct ablation studies removing orthogonal regularization and multi-margin loss to quantify their individual contributions to the 68.62% accuracy.
2. Perform stress tests by evaluating the system's performance when adding classes that are semantically similar to existing classes to assess the limits of the frozen backbone approach.
3. Measure energy consumption across multiple devices and environmental conditions to validate the 12 mJ per class claim and assess its generalizability.