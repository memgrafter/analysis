---
ver: rpa2
title: Parameterizing Federated Continual Learning for Reproducible Research
arxiv_id: '2406.02015'
source_url: https://arxiv.org/abs/2406.02015
tags:
- learning
- clients
- task
- tasks
- continual
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Freddie, the first open-source framework
  for reproducible Federated Continual Learning (FCL) research. Freddie enables scalable,
  configurable emulation of FCL scenarios by combining Kubernetes-based orchestration
  with flexible support for data, task, and resource heterogeneity.
---

# Parameterizing Federated Continual Learning for Reproducible Research

## Quick Facts
- arXiv ID: 2406.02015
- Source URL: https://arxiv.org/abs/2406.02015
- Authors: Bart Cox; Jeroen Galjaard; Aditya Shankar; Jérémie Decouchant; Lydia Y. Chen
- Reference count: 19
- Primary result: Freddie is the first open-source framework for reproducible Federated Continual Learning research

## Executive Summary
This paper introduces Freddie, the first open-source framework for reproducible Federated Continual Learning (FCL) research. Freddie enables scalable, configurable emulation of FCL scenarios by combining Kubernetes-based orchestration with flexible support for data, task, and resource heterogeneity. The framework supports both Task-Interactive Learning (Task-IL) and Domain-Interactive Learning (Domain-IL) through sliding and expanding window mechanisms, and implements state-of-the-art algorithms like EWC and GEM.

## Method Summary
Freddie provides a Kubernetes-based orchestration framework for Federated Continual Learning experiments. The system deploys Federator and Client pods with configurable CPU/memory resources, enabling dynamic scaling and resource heterogeneity emulation. Freddie implements both Task-IL (sliding window) and Domain-IL (expanding window) mechanisms for handling evolving client tasks, along with three task partitioning schemes (Column, Balanced, Shuffled). The framework supports state-of-the-art continual learning algorithms and provides tools for data collection and experiment deployment through its Orchestrator and Extractor components.

## Key Results
- Freddie scales linearly from 5 to 75 clients with round durations scaling proportionally
- Task-IL outperforms Domain-IL in FCL settings due to reduced catastrophic forgetting
- Balanced and shuffled task partitioning schemes yield up to 4% higher average accuracy than column-wise ordering
- Kubernetes containerization enables efficient isolation of client and federator workloads

## Why This Works (Mechanism)

### Mechanism 1
Kubernetes containerization enables scalable federated continual learning experiments by isolating client and federator workloads. Each client and federator runs in separate pods with configurable CPU/memory, allowing dynamic scaling and resource heterogeneity emulation. Core assumption: Kubernetes scheduler can efficiently allocate heterogeneous resources without major overhead. Evidence anchors: [abstract] "Freddie enables scalable, configurable emulation of FCL scenarios by combining Kubernetes-based orchestration with flexible support for data, task, and resource heterogeneity." [section] "The communication between any two parties in the system is asynchronous, allowing the development of FL systems with non-blocking federator-client interactions." Break condition: If Kubernetes scheduling overhead exceeds communication cost, scalability gains vanish.

### Mechanism 2
Task-ID aware sliding window (Task-IL) reduces catastrophic forgetting by restricting output classes to current task. During evaluation, only the sub-classes belonging to the current task are considered valid outputs, preventing interference from previously learned classes. Core assumption: Task IDs are available at evaluation time. Evidence anchors: [abstract] "Results also reveal that Task-IL outperforms Domain-IL in FCL settings due to reduced catastrophic forgetting." [section] "A sliding window restricts the output classes only to those of the task evaluated at a time t." Break condition: If task IDs are not available or unreliable, sliding window mechanism fails.

### Mechanism 3
Partition scheme design (column, balanced, shuffled) controls catastrophic forgetting rates in FCL. Column scheme causes high forgetting by repeating task order; balanced scheme mitigates forgetting by distributing tasks across clients; shuffled scheme relies on randomness. Core assumption: Task order and distribution across clients directly impacts forgetting. Evidence anchors: [section] "We devise three different schemes that partition tasks differently, and that can be used to evaluate a FCL scheme over a representative set of scenarios." [section] "The column scheme suffers more from more pronounced catastrophic forgetting than the shuffled and balanced scheme, resulting in lower accuracy." Break condition: If task distribution across clients is uniform by chance, scheme differences disappear.

## Foundational Learning

- Concept: Federated Learning (FL)
  - Why needed here: FCL builds on FL's distributed optimization framework.
  - Quick check question: How does FL differ from traditional centralized training?

- Concept: Continual Learning (CL)
  - Why needed here: FCL combines FL with CL to handle evolving client tasks.
  - Quick check question: What is catastrophic forgetting and why does it occur?

- Concept: Catastrophic Forgetting
  - Why needed here: Central challenge FCL aims to mitigate through various mechanisms.
  - Quick check question: Which CL techniques are most effective against forgetting?

## Architecture Onboarding

- Component map: Orchestrator -> TrainJob deployment -> Federator <-> Clients communication -> Extractor data collection
- Critical path: Orchestrator → TrainJob deployment → Federator ↔ Clients communication → Extractor data collection
- Design tradeoffs: Kubernetes flexibility vs. overhead; Task-IL accuracy vs. Domain-IL applicability
- Failure signatures: Scaling bottlenecks (round duration increase), memory leaks (pod restarts), communication failures (training stalls)
- First 3 experiments:
  1. Single-client simulation to verify basic FL workflow
  2. Small-scale FCL with 5 clients and 2 tasks to test Task-IL vs Domain-IL
  3. Resource heterogeneity test with clients of different CPU/memory allocations

## Open Questions the Paper Calls Out

### Open Question 1
How does the choice of task partitioning scheme (Column, Balanced, Shuffled) impact the trade-off between short-term and long-term catastrophic forgetting in FCL? Basis in paper: [explicit] The paper compares Column, Balanced, and Shuffled task partition schemes, showing that Column suffers more from catastrophic forgetting than Balanced and Shuffled, with Balanced and Shuffled yielding up to 4% higher average accuracy. Why unresolved: The paper demonstrates differences in accuracy between schemes but does not analyze the underlying mechanisms of short-term versus long-term forgetting in each scheme. What evidence would resolve it: Detailed analysis of parameter drift over time for each task partitioning scheme, comparing forgetting patterns across different time horizons and task sequences.

### Open Question 2
What is the impact of data heterogeneity (non-IID data distribution) on the performance of Task-IL versus Domain-IL in FCL settings? Basis in paper: [explicit] The paper shows that Task-IL outperforms Domain-IL due to reduced catastrophic forgetting, but does not explore how varying levels of data heterogeneity affect this performance difference. Why unresolved: The experiments use a specific data distribution (overlapping CIFAR100) without varying the degree of non-IIDness to see how it interacts with the IL strategies. What evidence would resolve it: Experiments varying the degree of data heterogeneity across clients while keeping the task sequence constant, measuring accuracy differences between Task-IL and Domain-IL.

### Open Question 3
How does the scalability of Freddie's Kubernetes-based deployment handle increasing client heterogeneity in terms of computational resources and network conditions? Basis in paper: [explicit] The paper demonstrates scalability from 5 to 75 clients with linear scaling of round durations, but does not explore heterogeneous resource allocation or network conditions among clients. Why unresolved: The experiments assume homogeneous client resources and network conditions, which does not reflect real-world deployment scenarios. What evidence would resolve it: Experiments with heterogeneous client configurations (varying CPU/memory) and network conditions (latency/bandwidth) to measure impact on round duration and model accuracy.

### Open Question 4
How do different continual learning algorithms (EWC, GEM, etc.) perform under the same FCL experimental conditions, and what factors influence their relative effectiveness? Basis in paper: [explicit] Freddie implements state-of-the-art algorithms like EWC and GEM, but the experiments only demonstrate their basic functionality without comparing their performance under identical conditions. Why unresolved: The paper mentions algorithm support but does not provide comparative analysis of their effectiveness in FCL scenarios. What evidence would resolve it: Head-to-head comparison of multiple CL algorithms (EWC, GEM, etc.) under identical FCL conditions with varying task sequences and data heterogeneity levels.

## Limitations

- No empirical validation of Kubernetes orchestration overhead, leaving scalability claims unverified
- Task-IL vs Domain-IL comparison lacks direct performance metrics and ablation studies
- Task partitioning scheme analysis depends on CIFAR100 specifics not fully detailed

## Confidence

- High confidence: Freddie framework architecture and basic FCL functionality
- Medium confidence: Scaling claims (linear round duration growth) without overhead data
- Low confidence: Catastrophic forgetting mitigation effectiveness across all three partition schemes

## Next Checks

1. Benchmark Kubernetes pod scheduling overhead vs communication costs to verify scalability claims
2. Conduct ablation studies comparing Task-IL and Domain-IL with identical hyperparameters
3. Test task partitioning scheme robustness with synthetic data distributions beyond CIFAR100