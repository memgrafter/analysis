---
ver: rpa2
title: 'FINDER: Stochastic Mirroring of Noisy Quasi-Newton Search and Deep Network
  Training'
arxiv_id: '2410.14270'
source_url: https://arxiv.org/abs/2410.14270
tags:
- optimization
- loss
- stochastic
- function
- finder
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proposes FINDER, a stochastic optimizer that combines
  noise-assisted global search with faster local convergence akin to Newton-like methods.
  FINDER addresses the challenge of optimizing non-convex and possibly non-smooth
  objective functions over large dimensional design spaces, which is common in machine
  learning and engineering applications.
---

# FINDER: Stochastic Mirroring of Noisy Quasi-Newton Search and Deep Network Training

## Quick Facts
- arXiv ID: 2410.14270
- Source URL: https://arxiv.org/abs/2410.14270
- Reference count: 40
- Primary result: FINDER achieves similar or better performance than Adam on deep network training and engineering applications while using fewer iterations

## Executive Summary
FINDER introduces a novel stochastic optimizer that combines noise-assisted global search with faster local convergence properties akin to Newton-like methods. The method addresses the challenge of optimizing non-convex and potentially non-smooth objective functions over large dimensional design spaces through derivative-free updates that mimic Newton search using inverse Hessian approximations. By leveraging nonlinear stochastic filtering equations and introducing simplifications like diagonalization of the gain matrix, FINDER achieves linear scaling with dimension while maintaining competitive performance across diverse applications including deep learning and engineering problems.

## Method Summary
FINDER employs nonlinear stochastic filtering equations to derive a derivative-free update rule that approximates Newton search using the inverse Hessian of the objective function. The method achieves this by imposing vanishing gradient requirements through stochastic filtering, generating a matrix that serves as the stochastic equivalent of the inverse Hessian. To enable practical implementation on large-scale problems, FINDER introduces key simplifications including diagonalization of the gain matrix and a scalar step-size parameter, allowing the method to scale linearly with problem dimension while maintaining the core quasi-Newton search properties.

## Key Results
- FINDER demonstrates competitive performance against Adam on IEEE benchmark functions, achieving similar or better results with fewer iterations
- On deep learning tasks including MNIST and CIFAR10 classification, FINDER achieves comparable or superior loss values relative to Adam
- The method shows promise for physics-informed neural networks, successfully training models for Burgers' equation, 2D elasticity, and strain gradient plasticity problems

## Why This Works (Mechanism)
FINDER works by bridging stochastic approximation theory with quasi-Newton optimization principles through nonlinear stochastic filtering. The key insight is that by imposing vanishing gradient conditions via stochastic filtering equations, one can derive a matrix that approximates the inverse Hessian without explicitly computing second derivatives. This approach naturally incorporates noise into the search process, providing global exploration capabilities while maintaining the fast local convergence properties of Newton-like methods. The diagonal approximation enables practical scaling to high-dimensional problems while preserving the essential curvature information needed for effective optimization.

## Foundational Learning

**Stochastic Filtering Theory**
- Why needed: Provides the mathematical framework for deriving derivative-free updates that approximate inverse Hessian information
- Quick check: Verify that the filtering equations properly converge under the stated assumptions and that the resulting gain matrix behaves as expected

**Quasi-Newton Methods**
- Why needed: Establishes the connection between the proposed approach and established optimization principles that leverage curvature information
- Quick check: Confirm that the diagonal approximation maintains sufficient curvature information for effective search direction computation

**Large-Scale Optimization**
- Why needed: Understanding scaling requirements and trade-offs necessary for practical implementation on high-dimensional problems
- Quick check: Validate that the computational complexity remains linear with respect to problem dimension as claimed

## Architecture Onboarding

**Component Map**
Stochastic Filtering Equations -> Gain Matrix Computation -> Diagonal Approximation -> Parameter Update -> Loss Evaluation

**Critical Path**
The critical computational path involves computing the stochastic filtering equations to obtain the gain matrix, applying the diagonal approximation, and performing the parameter update. This sequence must be efficient to maintain the claimed linear scaling with dimension.

**Design Tradeoffs**
The primary tradeoff involves accuracy versus computational efficiency through the diagonal approximation of the gain matrix. While full matrix computations would provide more accurate curvature information, they would prevent linear scaling. The stochastic filtering approach trades off some precision for computational tractability and noise-assisted exploration capabilities.

**Failure Signatures**
Potential failure modes include poor convergence when the diagonal approximation inadequately captures off-diagonal Hessian correlations, numerical instability in the stochastic filtering equations, and suboptimal performance on problems requiring significant off-diagonal curvature information.

**3 First Experiments**
1. Test FINDER on a simple quadratic optimization problem to verify basic convergence properties and compare against exact Newton method
2. Evaluate FINDER on a low-dimensional non-convex problem with known global optimum to assess global search capabilities
3. Implement FINDER on a small neural network (e.g., 2-layer MLP) on MNIST to validate deep learning integration

## Open Questions the Paper Calls Out

None specified in the provided content.

## Limitations

- The theoretical framework relies on assumptions about stochastic filtering convergence that may not hold in all practical scenarios
- The diagonal approximation of the gain matrix may limit performance on problems where off-diagonal Hessian correlations are significant
- Empirical validation primarily compares against Adam, leaving questions about relative performance to other modern optimizers like LAMB, Lion, or second-order methods

## Confidence

- Theoretical framework: Medium
- Deep learning performance claims: Medium
- Engineering applications: Medium

## Next Checks

1. Conduct ablation studies comparing diagonal vs. full gain matrices and varying stochastic filtering parameters to quantify their impact on convergence
2. Test FINDER on larger-scale deep learning models (e.g., ResNet50, Vision Transformers) and more complex datasets (e.g., ImageNet) to assess scalability
3. Compare FINDER systematically against a broader range of optimizers including LAMB, Lion, and second-order methods like Shampoo on both synthetic and real-world benchmarks