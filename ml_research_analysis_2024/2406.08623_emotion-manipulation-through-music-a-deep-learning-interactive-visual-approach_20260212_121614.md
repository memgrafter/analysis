---
ver: rpa2
title: Emotion Manipulation Through Music -- A Deep Learning Interactive Visual Approach
arxiv_id: '2406.08623'
source_url: https://arxiv.org/abs/2406.08623
tags:
- music
- emotion
- emotional
- audio
- accuracy
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a pipeline that manipulates the emotional
  content of music by altering musical features such as key and instrumentation. Using
  a deep learning classifier based on XLSR-Wav2Vec2, the system evaluates emotional
  content on Russel's Circumplex model, shifting music from happy to sad or vice versa.
---

# Emotion Manipulation Through Music -- A Deep Learning Interactive Visual Approach

## Quick Facts
- arXiv ID: 2406.08623
- Source URL: https://arxiv.org/abs/2406.08623
- Reference count: 34
- Primary result: 70% classifier accuracy on 4Q Emotion Dataset for emotion manipulation via key/instrumentation changes

## Executive Summary
This paper introduces a pipeline that manipulates the emotional content of music by altering musical features such as key and instrumentation. Using a deep learning classifier based on XLSR-Wav2Vec2, the system evaluates emotional content on Russel's Circumplex model, shifting music from happy to sad or vice versa. The approach integrates tools like Music21 for transposition and Accomontage2 for accompaniment generation. On the 4Q Emotion Dataset, the classifier achieves 70% accuracy, close to the state-of-the-art 75%. Visualizations demonstrate effective emotional transformations. This work establishes a foundation for Semantic Manipulation of Music (SMM), with potential applications in custom music generation, automated remixing, and emotion-tailored playlists.

## Method Summary
The method involves classifying music emotion using a retrained XLSR-Wav2Vec2 model on the 4Q Emotion Dataset, then manipulating emotional content by transposing MIDI melodies to different keys and generating accompaniment with Accomontage2. The system evaluates each transformation using the classifier and visualizes emotional shifts on Russell's Circumplex model. The pipeline takes MIDI input, synthesizes audio, classifies baseline emotion, applies key/instrumentation changes, generates new accompaniment, classifies transformed output, and selects the result closest to the target emotional quadrant.

## Key Results
- Achieved 70% classification accuracy on 4Q Emotion Dataset, only 5% below state-of-the-art
- Demonstrated effective emotional transformations through key transposition and instrumentation changes
- Successfully visualized emotional shifts on Russell's Circumplex model
- Proved concept of Semantic Manipulation of Music (SMM) for controlled emotional content modification

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Shifting key and instrumentation alters the perceived emotional content in a predictable way.
- Mechanism: The deep learning classifier maps audio features to Russell's Circumplex quadrants; changing key and SoundFont shifts those features, moving the point in the circumplex toward the target quadrant.
- Core assumption: Musical key and timbre are dominant factors in perceived emotion, and the classifier's mapping from features to emotions is consistent.
- Evidence anchors:
  - [abstract] "Our approach is a proof-of-concept for Semantic Manipulation of Music (SMM), a novel field aimed at modifying the emotional content of existing music."
  - [section 3.2] "Using our classifier output of a vector of four quadrant probabilities, [p1, p2, p3, p4] we convert the probabilities into a (x, y) coordinate: x = (p1 − p3) × radius, y = (p2 − p4) × radius."
  - [corpus] Weak: corpus neighbors discuss emotion recognition but not key/instrumentation manipulation directly.
- Break condition: If the classifier's mapping is non-linear or if key changes do not reliably affect the mapped features, the emotional shift becomes unpredictable.

### Mechanism 2
- Claim: Accompaniment generation conditioned on transposed melody reinforces the emotional shift.
- Mechanism: Accomontage2 selects accompaniment phrases based on input melody features; when the melody is transposed, the phrase selection changes, producing harmonies that reinforce the target emotion.
- Core assumption: Accomontage2's phrase selection correlates with emotional valence/arousal, so altering the input melody changes the output emotional tone.
- Evidence anchors:
  - [section 2.3] "By transposing the music to a different key before input, we manipulate the note-level phrases Accomontage2 matches against which changes the accompaniment generated and the overall mood of the musical work."
  - [section 3.1.2] Mentions using the 4Q dataset for retraining the classifier, implying the model captures emotional labels tied to these features.
  - [corpus] Weak: corpus neighbors focus on emotion detection/recognition, not accompaniment generation.
- Break condition: If Accomontage2's output is too rigid or insensitive to melodic changes, the accompaniment may not contribute to the desired emotional shift.

### Mechanism 3
- Claim: The classifier's accuracy on the 4Q dataset ensures reliable emotional assessment after manipulation.
- Mechanism: The retrained Wav2Vec2 model outputs probabilities for each quadrant; these probabilities are used to verify and visualize the emotional transformation.
- Core assumption: The 4Q dataset labels accurately represent perceived emotion, and the classifier generalizes to unseen melodies.
- Evidence anchors:
  - [section 4.1] "With limited resources, our classifier was able to achieve an accuracy of 70%, only 5% lower than the accuracy of 75% by Panda et al. [25] on their SVM model on the 4Q Emotional Dataset."
  - [section 3.1.1] Describes the XLSR-Wav2Vec2 architecture and its adaptation for music emotion recognition.
  - [corpus] Weak: corpus neighbors do not provide direct accuracy benchmarks for similar classifiers.
- Break condition: If the 4Q dataset is not representative of broader musical styles, or if the classifier overfits, accuracy on real-world inputs may drop.

## Foundational Learning

- Concept: Russell's Circumplex model of emotion
  - Why needed here: Provides the 2D valence-arousal space for visualizing and targeting emotional shifts.
  - Quick check question: What do the x and y axes of Russell's Circumplex represent?
- Concept: Wav2Vec2/XLSR speech representation adaptation to music
  - Why needed here: Allows extraction of audio features from music using a pre-trained speech model, then retraining for emotion classification.
  - Quick check question: Why is the Wav2Vec2 model repurposed from speech to music in this work?
- Concept: MIDI transposition and instrumentation selection
  - Why needed here: Core levers for manipulating the emotional content of the input melody before accompaniment generation.
  - Quick check question: How does transposing a melody to a different key potentially change its perceived emotion?

## Architecture Onboarding

- Component map: MIDI melody -> baseline classification -> key/instrumentation changes -> accompaniment generation -> final classification -> visualization
- Critical path: MIDI → baseline classification → key/instrumentation changes → accompaniment generation → final classification → visualization
- Design tradeoffs:
  - Using pre-trained Wav2Vec2 saves training time but may miss music-specific features
  - Limited compute forces 16kHz sampling, possibly reducing fidelity
  - Accomontage2 is deterministic given input, so diversity depends on key/instrumentation choices
- Failure signatures:
  - Classifier outputs near-uniform probabilities → emotional content not shifting
  - Accompaniment too similar across transpositions → instrumentation choice dominates
  - Russell plot points clustering near origin → weak emotional signal
- First 3 experiments:
  1. Run pipeline on a simple melody (e.g., C major scale) with one key transposition and default SoundFont; verify classification changes.
  2. Vary SoundFont only (keep key constant) on the same melody; observe emotional impact.
  3. Sweep multiple keys for a single melody; plot all results on Russell diagram to confirm systematic shifts.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the distance from the original key in terms of semitones affect the emotional transformation when transposing music?
- Basis in paper: [inferred] The authors hypothesize that maximal emotional difference occurs approximately six keys away from the original key, but this requires further testing.
- Why unresolved: The paper suggests this needs verification through systematic testing across multiple songs, keys, and artists to account for potential biases.
- What evidence would resolve it: Controlled experiments transposing multiple songs by varying semitone distances from the original key, measuring emotional shifts using the classifier, and analyzing patterns across different musical pieces.

### Open Question 2
- Question: What is the relative contribution of key transposition versus accompaniment generation (through Accomontage2) to the overall emotional transformation of music?
- Basis in paper: [explicit] The authors state that both key and SoundFont significantly impact emotional content, but it's difficult to quantify exactly how much impact comes from each component.
- Why unresolved: The current pipeline combines multiple transformations simultaneously, making it challenging to isolate individual effects.
- What evidence would resolve it: Systematic testing where each transformation (key transposition, accompaniment generation, instrumentation selection) is applied independently and in combination, measuring emotional shifts using the classifier.

### Open Question 3
- Question: How accurate is the classifier's emotional categorization compared to human perception of musical emotion, and how does this vary across different demographic groups?
- Basis in paper: [explicit] The authors suggest evaluating classifier accuracy using human testing and note that musical preferences are impacted by lifetime exposure and demographic data.
- Why unresolved: The classifier's performance is only compared against other machine learning models, not against human judgment.
- What evidence would resolve it: User studies where participants rate the emotional content of both original and transformed music, with results compared to classifier predictions across different age groups, cultures, and musical backgrounds.

## Limitations
- Limited to 16kHz sampling due to compute constraints, potentially reducing feature fidelity
- Reliance on Accomontage2 for accompaniment generation introduces potential point of failure
- Generalization to diverse musical genres and emotional nuances remains unproven
- Classifier accuracy of 70% may not translate to reliable emotional manipulation across all musical styles

## Confidence

**High**: The pipeline architecture and classification methodology are sound and reproducible.

**Medium**: The emotional manipulation mechanism works for simple cases but may degrade with complex musical styles.

**Low**: Generalization to diverse musical genres and emotional nuances remains unproven.

## Next Checks
1. Test the pipeline on multiple musical genres (classical, jazz, pop) to assess generalization across styles.
2. Conduct a human listening study to verify if automated emotional classifications align with perceived emotion.
3. Evaluate classifier robustness by testing on out-of-distribution melodies not in the 4Q dataset.