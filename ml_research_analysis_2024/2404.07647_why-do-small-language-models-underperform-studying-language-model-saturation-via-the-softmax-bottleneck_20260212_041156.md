---
ver: rpa2
title: Why do small language models underperform? Studying Language Model Saturation
  via the Softmax Bottleneck
arxiv_id: '2404.07647'
source_url: https://arxiv.org/abs/2404.07647
tags:
- language
- performance
- saturation
- head
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates the performance saturation observed in
  small language models (LMs) during pretraining. The authors attribute this saturation
  to the softmax bottleneck phenomenon, where a mismatch between the hidden dimension
  of small models and the high rank of the target contextual probability distribution
  limits performance.
---

# Why do small language models underperform? Studying Language Model Saturation via the Softmax Bottleneck

## Quick Facts
- arXiv ID: 2404.07647
- Source URL: https://arxiv.org/abs/2404.07647
- Reference count: 17
- Models with hidden dimensions below 1000 suffer from representation degeneration and performance saturation during pretraining

## Executive Summary
This paper investigates the performance saturation observed in small language models during pretraining, attributing it to the softmax bottleneck phenomenon. The authors demonstrate that models with hidden dimensions below approximately 1000 suffer from rank-constrained representations in their language modeling heads, leading to reduced evaluation performance. Through theoretical analysis and empirical experiments on Pythia models, they show that singular value distributions of language modeling heads become increasingly uniform during training, eventually degenerating abruptly and correlating with performance drops.

## Method Summary
The paper analyzes existing Pythia model checkpoints and conducts rank-constrained head experiments to study the softmax bottleneck. The authors compute anisotropy measures through cosine similarity across layers, analyze singular value distributions of language modeling heads during training, and estimate the inherent dimensionality of language using n-gram matrices and SVD. They also implement rank-constrained language modeling heads on frozen pretrained models to measure bottleneck effects and compare performance across different model scales including GPT-2, OPT, and Gemma.

## Key Results
- Small language models (hidden dimension < 1000) show performance saturation correlated with rank-constrained representations in language modeling heads
- Singular value distributions of small model LM heads become increasingly uniform during training, then abruptly degenerate, correlating with performance drops
- Last-layer anisotropy (high cosine similarity) emerges abruptly in small models around the saturation point, while larger models maintain near-isotropic representations

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Small language models underperform due to softmax bottleneck when hidden dimension < 1000
- Mechanism: When the hidden dimension d is smaller than the rank of the target contextual probability distribution, the linear language modeling head cannot accurately represent the output distribution, causing performance degradation
- Core assumption: The rank of the contextual probability distribution is higher than 1000
- Evidence anchors:
  - [abstract] "models based on less than 1000 hidden dimensions tend to adopt degenerate latent representations in late pretraining, which leads to reduced evaluation performance"
  - [section 5.1] "perplexity starts to noticeably decrease when the rank of the language modeling head W is inferior to 1000, regardless of the model size"
  - [corpus] Weak - related papers discuss softmax attention but don't directly confirm the 1000 threshold
- Break condition: If the true rank of the contextual distribution is actually below 1000, the bottleneck wouldn't occur at this threshold

### Mechanism 2
- Claim: Performance saturation correlates with rank saturation in language modeling heads
- Mechanism: During training, the singular value distribution of small LM heads becomes increasingly uniform, then abruptly degenerates, which correlates with the observed performance drop
- Core assumption: The collapse of singular value distribution directly impacts the model's ability to represent the output distribution
- Evidence anchors:
  - [section 4.2] "The heads of small models see their singular value distributions become increasingly uniform, up to a point where they degenerate abruptly, which again correlates with the LM performance drop"
  - [section 4.2] "we use a singular entropy metric, computed as the Kullback-Leibler divergence between the normalized singular value distribution and the uniform distribution"
  - [corpus] Weak - related papers discuss attention expressiveness but not singular value collapse patterns
- Break condition: If the singular value distribution collapse doesn't actually impair representational capacity, the correlation might be coincidental

### Mechanism 3
- Claim: Anisotropy in last-layer representations indicates performance saturation in small models
- Mechanism: Small models develop high last-layer anisotropy (low angular variability) that increases abruptly around the saturation point, while larger models maintain near-isotropic last layers
- Core assumption: Last-layer anisotropy is symptomatic of performance saturation rather than a separate phenomenon
- Evidence anchors:
  - [section 4.1] "the dichotomy aligns with the one of the saturation phenomenon for the Pythia suite, where only models containing 160M or fewer parameters seem affected by last-layer anisotropy"
  - [section 4.1] "we observe that almost all layers of Transformers models are anisotropic to some extent, regardless of the scale"
  - [corpus] Weak - related papers discuss attention expressiveness but don't establish the link between anisotropy and saturation
- Break condition: If anisotropy appears in small models for reasons unrelated to saturation, the correlation would be spurious

## Foundational Learning

- Concept: Singular Value Decomposition (SVD) and matrix rank
  - Why needed here: The paper's core argument relies on understanding how matrix rank limits representational capacity, particularly for the language modeling head
  - Quick check question: What is the maximum rank a matrix with dimensions V×d can have, and how does this constrain the expressiveness of the language modeling head?

- Concept: Softmax function and its properties
  - Why needed here: The softmax bottleneck occurs specifically because the linear layer followed by softmax cannot represent certain distributions when dimensionality is constrained
  - Quick check question: How does the rank of the input matrix to a softmax function limit the possible output distributions it can represent?

- Concept: Cosine similarity and anisotropy measures
  - Why needed here: The paper uses cosine similarity to quantify representation degeneration through anisotropy in the embedding space
  - Quick check question: What does a high average cosine similarity between representations indicate about the geometry of the embedding space?

## Architecture Onboarding

- Component map:
  Input: Tokenized sequences -> Encoder: Transformer layers producing d-dimensional contextual representations -> Head: Linear layer W∈RV×d followed by softmax for next-token prediction -> Loss: Cross-entropy between predicted and actual next tokens

- Critical path:
  1. Forward pass through transformer layers to generate contextual representations
  2. Linear transformation by W to produce logits
  3. Softmax to convert logits to probability distribution
  4. Cross-entropy loss computation
  5. Backward pass with gradients flowing through all components

- Design tradeoffs:
  - Hidden dimension d vs model size: Larger d alleviates softmax bottleneck but increases computational cost
  - Vocabulary size V vs model architecture: Larger V requires larger W matrix, increasing memory usage
  - Layer depth vs width: Deeper models may compensate for smaller d through hierarchical representations

- Failure signatures:
  - Performance plateau or degradation during late training stages
  - Increasing last-layer anisotropy (average cosine similarity approaching 1)
  - Singular value distribution of W becoming increasingly uniform then abruptly spiking
  - Loss gap between actual and extrapolated performance based on scaling laws

- First 3 experiments:
  1. Train Pythia-160M with varying hidden dimensions (128, 256, 512, 768) and measure performance saturation points
  2. Implement rank-constrained language modeling heads (W=AB with varying inner dimension) on pretrained representations and measure performance degradation
  3. Monitor singular value distributions and anisotropy metrics during training of small models to identify correlation with performance saturation

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the singular value distribution degeneracy in the language modeling head relate to the emergence of anisotropy in the last layer representations during training?
- Basis in paper: [explicit] The paper shows that both phenomena occur concurrently in small models and are correlated with the performance saturation, but does not establish a direct causal relationship.
- Why unresolved: While the paper demonstrates a strong correlation between the two phenomena, it does not provide a mechanistic explanation for why the singular value distribution degeneracy leads to or is caused by the emergence of anisotropy.
- What evidence would resolve it: Experiments that manipulate the singular value distribution in the language modeling head and observe the effects on the anisotropy of the last layer representations, or vice versa, would help establish a causal relationship.

### Open Question 2
- Question: What is the relationship between the softmax bottleneck and the emergence of outlier dimensions in the representations of small language models?
- Basis in paper: [inferred] The paper discusses the softmax bottleneck and its impact on the performance of small language models, and mentions that previous works have observed the emergence of outlier dimensions during training. However, it does not explore the connection between these two phenomena.
- Why unresolved: The paper does not investigate whether the softmax bottleneck contributes to the emergence of outlier dimensions, or if these outlier dimensions exacerbate the effects of the softmax bottleneck.
- What evidence would resolve it: Experiments that analyze the relationship between the softmax bottleneck and the emergence of outlier dimensions, such as measuring the correlation between the two phenomena or studying the effects of mitigating the softmax bottleneck on the prevalence of outlier dimensions, would help clarify this connection.

### Open Question 3
- Question: How does the rank of the target contextual probability distribution vary across different domains and languages, and how does this affect the performance of small language models?
- Basis in paper: [explicit] The paper estimates the rank of the contextual probability distribution using a naive 5-gram language model on various datasets and tokenizers, finding that the rank is relatively high compared to the hidden dimensions of small models. However, it does not explore the variability of this rank across different domains or languages.
- Why unresolved: The paper only provides a single estimate of the rank for each dataset and tokenizer, without considering the potential variations in rank across different domains or languages. This variability could have significant implications for the performance of small language models in different settings.
- What evidence would resolve it: Experiments that estimate the rank of the contextual probability distribution across a diverse set of domains and languages, and analyze the relationship between this rank and the performance of small language models in each setting, would help elucidate the impact of domain and language on the softmax bottleneck and performance saturation.

## Limitations
- The analysis primarily focuses on Pythia models and demonstrates correlations rather than establishing definitive causation between softmax bottleneck and performance saturation
- The 1000-dimensional threshold appears empirically derived from Pythia experiments rather than theoretically proven as a universal limit
- The paper does not fully explore whether alternative architectural modifications could mitigate the observed saturation without increasing hidden dimensions

## Confidence

- **High confidence**: The correlation between small hidden dimensions (below 1000) and performance degradation is well-established through multiple experiments and model comparisons
- **Medium confidence**: The theoretical explanation via softmax bottleneck and rank constraints provides a plausible mechanism, though alternative explanations (such as optimization difficulties or data limitations) are not fully ruled out
- **Medium confidence**: The link between singular value distribution collapse and performance saturation is empirically demonstrated but could potentially be coincidental rather than causal

## Next Checks

1. **Cross-architecture validation**: Test the softmax bottleneck hypothesis on architectures beyond Pythia (such as LLaMA or Mistral) to determine if the 1000-dimensional threshold is universal or architecture-dependent

2. **Intervention study**: Implement architectural modifications that explicitly address the softmax bottleneck (such as mixture-of-softmaxes or adaptive embeddings) and measure whether performance saturation can be delayed or eliminated without increasing hidden dimensions

3. **Optimization analysis**: Conduct controlled experiments varying learning rates, batch sizes, and training duration for small models to determine whether the observed saturation could be partially attributed to optimization rather than fundamental representational constraints