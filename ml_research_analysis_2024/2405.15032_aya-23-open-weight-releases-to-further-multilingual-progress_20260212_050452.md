---
ver: rpa2
title: 'Aya 23: Open Weight Releases to Further Multilingual Progress'
arxiv_id: '2405.15032'
source_url: https://arxiv.org/abs/2405.15032
tags:
- languages
- multilingual
- language
- arxiv
- evaluation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Aya 23 is a family of multilingual language models that improves
  on the original Aya 101 model by focusing on 23 languages instead of 101, thereby
  allocating more model capacity to each language and avoiding the "curse of multilinguality."
  The models are based on Cohere's Command series and are fine-tuned on a large multilingual
  instruction dataset. Aya 23 includes 8B and 35B parameter models, with the 35B version
  outperforming both Aya 101 and other widely used models like Gemma, Mistral, and
  Mixtral on a range of discriminative and generative tasks across the 23 supported
  languages.
---

# Aya 23: Open Weight Releases to Further Multilingual Progress

## Quick Facts
- **arXiv ID**: 2405.15032
- **Source URL**: https://arxiv.org/abs/2405.15032
- **Reference count**: 34
- **Primary result**: Aya-23-35B outperforms Aya-101, Gemma, Mistral, and Mixtral on multilingual tasks across 23 languages

## Executive Summary
Aya 23 is a family of multilingual language models that addresses the "curse of multilinguality" by focusing on 23 languages instead of the original Aya 101's 101 languages. This strategic narrowing allows for greater model capacity per language, resulting in superior performance on both discriminative and generative tasks. The models are based on Cohere's Command series and fine-tuned on a large multilingual instruction dataset. Aya-23-35B achieves state-of-the-art results on multilingual MMLU (58.2%) and FLORES-200 translation (43.0 spBLEU), outperforming larger and smaller competing models.

## Method Summary
Aya 23 leverages Cohere's Command series as a foundation, fine-tuning the models on a large multilingual instruction dataset. By reducing the number of supported languages from 101 to 23, the model allocates more capacity to each language, mitigating the performance degradation that occurs when models attempt to support too many languages simultaneously. The approach includes both 8B and 35B parameter versions, with the larger model demonstrating significant improvements across the targeted language set.

## Key Results
- Aya-23-35B achieves 58.2% accuracy on multilingual MMLU compared to Mixtral-8x7B's 57.1%
- Aya-23-35B reaches 43.0 spBLEU on FLORES-200 translation versus Mixtral-8x7B's 36.3
- The 35B model outperforms both Aya 101 and other widely used models including Gemma, Mistral, and Mixtral across discriminative and generative tasks

## Why This Works (Mechanism)
The "curse of multilinguality" occurs when language models spread their capacity too thinly across many languages, resulting in degraded performance for all. By strategically reducing the target language set from 101 to 23, Aya 23 concentrates model capacity where it matters most, allowing for deeper linguistic understanding and better instruction following in each supported language. This focused approach enables the model to achieve higher quality representations for each language rather than superficial coverage of many.

## Foundational Learning
- **Multilingual instruction tuning**: Why needed - enables models to follow instructions across languages; Quick check - test zero-shot instruction following in each supported language
- **Curse of multilinguality**: Why needed - understanding performance degradation across many languages; Quick check - measure performance drop when increasing language count
- **Cross-lingual transfer**: Why needed - leverages knowledge from high-resource to low-resource languages; Quick check - evaluate performance on related but unseen languages
- **Parameter-efficient fine-tuning**: Why needed - allows adaptation without full retraining; Quick check - compare performance with different fine-tuning methods
- **Multilingual evaluation benchmarks**: Why needed - standardized assessment across languages; Quick check - run models on MMLU, FLORES, and XTREME
- **Open-weight licensing**: Why needed - enables community research and deployment; Quick check - verify license compliance for intended use cases

## Architecture Onboarding

**Component map**: Pre-trained Cohere Command model -> Multilingual instruction dataset -> Fine-tuning pipeline -> Aya 23 model family

**Critical path**: Data preparation and cleaning -> Instruction tuning with supervised learning -> Evaluation on multilingual benchmarks -> Release under non-commercial license

**Design tradeoffs**: Reduced language coverage (23 vs 101) for increased depth and performance per language, versus broader but shallower multilingual support

**Failure signatures**: Poor performance on languages outside the 23 supported set, potential overfitting to instruction patterns in the fine-tuning data, and possible degradation in languages with limited representation in the training corpus

**First experiments**:
1. Evaluate zero-shot instruction following capabilities across all 23 supported languages
2. Test translation quality on FLORES-200 for language pairs involving the 23 supported languages
3. Measure performance on multilingual MMLU to establish baseline capabilities

## Open Questions the Paper Calls Out
None

## Limitations
- Evaluation primarily focuses on 23 selected languages, raising questions about generalizability to other languages
- Comparison with models of different parameter counts (8B vs 35B vs 8x7B) may be misleading without accounting for model size differences
- Non-commercial license with separate commercial terms may limit broader adoption and independent validation

## Confidence

**High confidence in**:
- Technical approach of focusing on fewer languages to reduce the curse of multilinguality
- Reported benchmark improvements on the specified 23 languages

**Medium confidence in**:
- Absolute performance gains compared to other models, given different model sizes and potential cherry-picking of evaluation languages

**Low confidence in**:
- Generalizability of results to languages outside the 23 supported ones
- Real-world utility of the models given the licensing restrictions

## Next Checks
1. Evaluate Aya-23 models on held-out languages not included in the 23-language set to verify generalization claims
2. Conduct controlled experiments comparing Aya-23-35B directly against similarly sized models (not just Mixtral-8x7B) on the same multilingual benchmarks
3. Test the models with commercial applications under the provided commercial license terms to assess practical usability and any potential restrictions