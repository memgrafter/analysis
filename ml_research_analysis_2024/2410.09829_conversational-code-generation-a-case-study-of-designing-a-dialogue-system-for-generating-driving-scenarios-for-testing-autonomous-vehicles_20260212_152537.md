---
ver: rpa2
title: 'Conversational Code Generation: a Case Study of Designing a Dialogue System
  for Generating Driving Scenarios for Testing Autonomous Vehicles'
arxiv_id: '2410.09829'
source_url: https://arxiv.org/abs/2410.09829
tags:
- generation
- code
- description
- vehicle
- dialogue
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a dialogue system for generating driving
  scenarios for autonomous vehicle testing using natural language. The system leverages
  retrieval-augmented generation (RAG) with large language models (LLMs) to convert
  user descriptions into executable Scenic programs, which define driving scenarios
  in the CARLA simulator.
---

# Conversational Code Generation: a Case Study of Designing a Dialogue System for Generating Driving Scenarios for Testing Autonomous Vehicles

## Quick Facts
- arXiv ID: 2410.09829
- Source URL: https://arxiv.org/abs/2410.09829
- Reference count: 40
- One-line primary result: Dialogue system using RAG-LLM achieves 4.5x higher success rate (28% vs 6%) for generating driving scenarios from natural language descriptions.

## Executive Summary
This paper presents a dialogue system for generating driving scenarios for autonomous vehicle testing using natural language descriptions. The system leverages retrieval-augmented generation (RAG) with large language models (LLMs) to convert user descriptions into executable Scenic programs that define driving scenarios in the CARLA simulator. Due to data scarcity, the authors manually augment the dataset from 32 to 105 description-program pairs and standardize code style. Human experiments demonstrate that dialogue significantly improves scenario generation success, achieving a 4.5 times higher success rate compared to single-turn generation.

## Method Summary
The system converts natural language descriptions into executable Scenic programs using a RAG-LLM approach. It embeds user descriptions, retrieves semantically similar exemplars from a curated dataset, and constructs prompts for the LLM (CodeLlama 7B) with these exemplars. The generated code is executed in CARLA, and users provide feedback if unsatisfied. The system supports up to four dialogue turns, with each turn updating the description based on user feedback. Error feeding is implemented to handle execution failures, allowing the LLM to self-correct up to three times before requesting user intervention.

## Key Results
- Dialogue increases successful scenario generation from 6% (single turn) to 28% (up to 4 turns)
- RAG with in-context exemplars improves code generation accuracy over random exemplar selection
- Execution errors decrease from 31% to 11% when error feeding is applied
- 63% of execution errors stem from missing correspondence between Scenic API and descriptions

## Why This Works (Mechanism)

### Mechanism 1
- Claim: RAG with in-context exemplars improves code generation accuracy over random selection in low-resource domains.
- Mechanism: Embeddings of natural language descriptions are used to retrieve semantically similar description-program pairs from a manually curated dataset. These exemplars are inserted into the prompt to guide the LLM in generating syntactically correct and contextually appropriate Scenic code.
- Core assumption: The retrieved exemplars are semantically close enough to the target description to provide useful in-context examples.
- Evidence anchors: [abstract] The authors explicitly state they use RAG with in-context learning to construct the code prompt. [section] Section 3.1 describes embedding descriptions, indexing the dataset, and using top-k retrieval (k=3) to construct the CODE prompt. [corpus] Weak evidence; the paper doesn't benchmark RAG against random retrieval on external datasets.
- Break condition: If the dataset is too small or descriptions are too dissimilar, retrieved exemplars may not match the target scenario, causing generation errors.

### Mechanism 2
- Claim: Extended dialogue (up to four turns) significantly improves successful simulation generation by iteratively refining underspecified natural language descriptions.
- Mechanism: After initial code generation, simulation instances are shown to the user. User feedback is summarized into an updated description, which is then used to regenerate code. This iterative loop aligns the generated scenarios with the user's communicative intent.
- Core assumption: User feedback effectively captures missing information not present in the original description, and the LLM can incorporate this into a coherent updated description.
- Evidence anchors: [abstract] "Human experiments show that dialogue is critical to successful simulation generation, leading to a 4.5 times higher success rate than generation without engaging in extended conversation." [section] Section 3.2 details the conversation process, including updating the description via SUMM prompt and regenerating code. [corpus] Weak evidence; no external studies cited on dialogue efficacy for code refinement.
- Break condition: If user feedback is too vague or the summarization step introduces errors, the iterative refinement may fail to converge on a correct scenario.

### Mechanism 3
- Claim: Error feeding (retrying code generation with error context) improves executability of generated Scenic programs.
- Mechanism: When generated code fails to execute, the system feeds the original description, the erroneous code, and the error messages back to the LLM for self-correction, up to three attempts.
- Core assumption: The LLM can understand error messages and correct its own code generation based on them.
- Evidence anchors: [section] Section 3.1 explicitly describes error feeding: "the program ÀÜùëù is passed to the simulator... the natural language description, ÀÜùëù and its errors are passed back to the IFLLM, to attempt to self-correct the output." [abstract] Not directly mentioned; this is a technical detail in the implementation. [corpus] Weak evidence; no external benchmarks on error feeding effectiveness.
- Break condition: If the LLM cannot parse or correct the errors, or if errors persist after three attempts, the user must provide a new description.

## Foundational Learning

- Concept: Retrieval-augmented generation (RAG)
  - Why needed here: The dataset of Scenic programs is extremely small (32‚Üí105 examples), so RAG is used to retrieve relevant exemplars to guide code generation.
  - Quick check question: How does RAG help when training data is scarce?

- Concept: In-context learning with few-shot prompts
  - Why needed here: The LLM is not fine-tuned on Scenic code, so it relies on a few exemplars in the prompt to understand the code structure and semantics.
  - Quick check question: What is the role of the exemplars in the CODE prompt?

- Concept: Semantic parsing from natural language to executable code
  - Why needed here: The system must convert user descriptions into valid Scenic programs that can be executed in the CARLA simulator.
  - Quick check question: Why is execution success a key metric in this domain?

## Architecture Onboarding

- Component map: Natural language description ‚Üí Embedding ‚Üí RAG retrieval (top-k exemplars) ‚Üí CODE prompt construction ‚Üí LLM (CodeLlama) ‚Üí Scenic program ‚Üí CARLA simulator ‚Üí Simulation instances ‚Üí User observation ‚Üí Feedback ‚Üí SUMM prompt ‚Üí Updated description ‚Üí Repeat generation ‚Üí Error handling: Failed execution ‚Üí Error feeding loop (up to 3 retries) ‚Üí User paraphrase request

- Critical path:
  1. User provides description.
  2. RAG retrieves exemplars and constructs CODE prompt.
  3. LLM generates Scenic code.
  4. Code is executed in CARLA.
  5. User observes simulations and provides feedback if unsatisfied.
  6. SUMM prompt updates description; repeat from step 2.

- Design tradeoffs:
  - Using RAG vs. fine-tuning: RAG avoids data scarcity issues but may retrieve irrelevant exemplars if dataset is too small.
  - Error feeding vs. immediate user intervention: Error feeding automates correction but adds latency; user intervention is more reliable but slower.
  - Fixed number of dialogue turns (4) vs. open-ended: Fixed limit ensures termination but may cut off useful refinement.

- Failure signatures:
  - Execution error: Generated code does not run in CARLA (often due to API mismatches or missing primitives).
  - Pass error: Code runs but does not match user intent (often due to underspecification or style differences).
  - Summarization error: Updated description loses or adds information due to imperfect summarization.

- First 3 experiments:
  1. Test RAG retrieval: Input a description and verify that the top-3 retrieved exemplars are semantically relevant.
  2. Test code generation: Input a description with exemplars and verify that the generated code is syntactically correct and executable.
  3. Test dialogue loop: Input a description, observe simulation, provide feedback, and verify that the updated description and regenerated code better match the user's intent.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the data scarcity issue for Scenic programs be addressed more effectively to improve code generation performance?
- Basis in paper: [explicit] The paper notes that only 32 examples of Scenic programs with natural language descriptions are available online, and the authors manually augment this to 105 pairs.
- Why unresolved: The manual augmentation process is time-consuming and may not scale well. The paper does not explore automated data augmentation techniques or ways to generate synthetic training data.
- What evidence would resolve it: Experimental results comparing the proposed approach with automated data augmentation methods or synthetic data generation techniques, showing improved code generation performance with less manual effort.

### Open Question 2
- Question: What specific techniques could improve the summarization process to reduce summarization errors in the dialogue system?
- Basis in paper: [explicit] The paper identifies summarization errors as one of the failure modes, noting that the updated description may fail to include the feedback or contain extra information.
- Why unresolved: The paper uses a simple summarization prompt but does not explore more sophisticated techniques like using specialized summarization models or incorporating attention mechanisms to ensure all relevant information is preserved.
- What evidence would resolve it: Comparative evaluation of different summarization approaches, showing reduced summarization errors and improved dialogue system performance.

### Open Question 3
- Question: How can the dialogue system be extended to dynamically test and evaluate new controller designs in tandem with formal verification methods?
- Basis in paper: [explicit] The paper mentions this as a future direction, noting that the current setup does not dynamically change the controller or use the interface to test it.
- Why unresolved: The paper does not provide any implementation details or preliminary results for this extension. It remains a conceptual idea rather than a concrete research direction.
- What evidence would resolve it: Implementation and evaluation of a system that integrates the dialogue interface with controller testing and formal verification, demonstrating improved safety validation capabilities.

## Limitations
- Small dataset (105 examples) manually augmented from 32, limiting generalizability
- Human evaluation with only 6 participants raises concerns about statistical significance
- Success rate remains relatively low at 28% even with dialogue, indicating persistent challenges
- No comparison against alternative dialogue strategies or baseline models

## Confidence
- High confidence: The core mechanism of using RAG with in-context exemplars for code generation in low-resource settings is well-supported by the implementation details and results.
- Medium confidence: The claim that dialogue significantly improves success rates is supported by human trials, but the small sample size and lack of baseline comparisons reduce confidence in the magnitude of the effect.
- Low confidence: The generalizability of the approach to other domains or larger datasets is unclear due to the study's narrow focus and manual data augmentation.

## Next Checks
1. **Dataset generalization test**: Evaluate the RAG-LLM system on an external dataset of description-program pairs (e.g., CodeXGLUE or similar) to assess performance beyond the manually curated 105 examples.

2. **Baseline comparison**: Implement and compare against a baseline model without RAG (e.g., direct prompting with no exemplars) and a baseline without dialogue (single-turn generation) to quantify the specific contributions of each component.

3. **User study replication**: Conduct a larger-scale human evaluation with at least 20-30 participants and include a control group using a standard chatbot (e.g., ChatGPT) for comparison to validate the system's effectiveness in real-world use.