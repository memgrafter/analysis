---
ver: rpa2
title: Unlocking Large Language Model's Planning Capabilities with Maximum Diversity
  Fine-tuning
arxiv_id: '2406.10479'
source_url: https://arxiv.org/abs/2406.10479
tags:
- planning
- fine-tuning
- llms
- tasks
- block
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper demonstrates that fine-tuning significantly enhances
  large language models' planning capabilities, with GPT-3.5-turbo and Llama-3-8b
  achieving solved rates exceeding 95% after training on thousands of planning task
  examples. To improve sample efficiency, the authors propose a Maximum Diversity
  Fine-Tuning (MDFT) strategy that uses graph representations to encode planning tasks
  and selects diverse, representative training samples.
---

# Unlocking Large Language Model's Planning Capabilities with Maximum Diversity Fine-tuning

## Quick Facts
- arXiv ID: 2406.10479
- Source URL: https://arxiv.org/abs/2406.10479
- Authors: Wenjun Li; Changyu Chen; Pradeep Varakantham
- Reference count: 28
- This paper demonstrates that fine-tuning significantly enhances large language models' planning capabilities, with GPT-3.5-turbo and Llama-3-8b achieving solved rates exceeding 95% after training on thousands of planning task examples.

## Executive Summary
This paper addresses the challenge of improving large language models' planning capabilities through fine-tuning, showing that traditional random sampling methods are inefficient for selecting training data. The authors propose a Maximum Diversity Fine-Tuning (MDFT) strategy that uses graph representations to encode planning tasks and selects diverse, representative training samples. Their MDFT-g algorithm consistently outperforms random sampling and language embedding-based methods across multiple benchmark domains, particularly at smaller sample sizes. Notably, MDFT-g with 100 samples achieves performance comparable to random sampling with 200 samples, demonstrating significant gains in training efficiency while maintaining strong planning performance.

## Method Summary
The authors develop a Maximum Diversity Fine-Tuning (MDFT) framework that improves sample efficiency for LLM fine-tuning in automated planning tasks. The approach uses graph representations to encode planning tasks (initial and goal configurations as nodes and predicates as edges), applies k-means clustering to identify task clusters in the embedding space, and selects representative samples closest to each cluster centroid. This ensures both diversity (different clusters) and representativeness (closest to centroid) in the training data. The method is compared against random sampling and language embedding-based approaches across multiple planning domains using metrics like solved rate, plan validity, and solution length.

## Key Results
- Fine-tuning significantly enhances LLM planning capabilities, with GPT-3.5-turbo and Llama-3-8b achieving solved rates exceeding 95% after training on thousands of examples
- MDFT-g algorithm outperforms random sampling and language embedding methods, particularly at smaller sample sizes
- MDFT-g with 100 samples achieves performance comparable to random sampling with 200 samples, demonstrating significant training efficiency gains
- Graph representations better capture structural task information than language embeddings for planning tasks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Graph representations capture structural task information more effectively than language embeddings for planning tasks.
- Mechanism: Planning tasks consist of initial and goal configurations represented as nodes (objects) and edges (predicates). This graph structure preserves relationships that natural language descriptions lose when converted to embeddings.
- Core assumption: The solution to a planning task depends primarily on the relationships between objects rather than their narrative description.
- Evidence anchors:
  - "planning tasks with distinct solutions can only differ in a few words in the natural language descriptions and result in highly similar language embeddings"
  - "we use graph representations to encode planning tasks. Each planning task consists of an initial configuration of objects and a goal configuration of objects. To solve the task, the LLM needs to perform sequential actions to transform the initial configuration into the goal configuration. In graph representation, each object is a node, and each predicate is a (directed) edge between two nodes."

### Mechanism 2
- Claim: Clustering-based maximum diversity sampling improves sample efficiency by ensuring representative coverage of the task space.
- Mechanism: K-means clustering identifies task clusters in embedding space, then selects the point closest to each cluster centroid. This ensures both diversity (different clusters) and representativeness (closest to centroid).
- Core assumption: Tasks within the same cluster are similar enough that learning from one representative task transfers to others in the cluster, while tasks in different clusters require separate examples.
- Evidence anchors:
  - "we employ clustering-based approaches to identify representative points, thereby ensuring diversity within the dataset"
  - "For each cluster, we find the point closest to the cluster centroid, select it to represent the cluster, and include it in the final subset"
  - "we empirically demonstrate that MDFT-g consistently outperforms existing baseline methods at the same scale"

### Mechanism 3
- Claim: Fine-tuning on planning-specific data significantly improves LLM performance beyond what prompting alone achieves.
- Mechanism: Supervised fine-tuning updates model weights on thousands of planning examples, allowing the model to internalize planning patterns and structures that are difficult to elicit through prompts alone.
- Core assumption: The planning capabilities of LLMs are latent and require exposure to task-specific examples to be activated and refined.
- Evidence anchors:
  - "fine-tuning significantly enhances large language models' planning capabilities, with GPT-3.5-turbo and Llama-3-8b achieving solved rates exceeding 95% after training on thousands of planning task examples"
  - "we found that substantial fine-tuning significantly improves LLM's planning capabilities to a satisfactory level, which contradicts the existing prejudice that LLMs are weak at planning"

## Foundational Learning

- Concept: Automated Planning (PDDL, state-space search)
  - Why needed here: The paper evaluates LLM planning performance against traditional automated planning benchmarks and uses PDDL for validation.
  - Quick check question: What are the key components of a PDDL planning domain definition, and how do they relate to the graph representations used in MDFT-g?

- Concept: Maximum Diversity Problem (MDP) and clustering algorithms
  - Why needed here: The MDFT strategy is built on solving MDP through clustering to select diverse training samples.
  - Quick check question: How does k-means clustering solve (approximately) the maximum diversity problem, and what are the limitations of this approach?

- Concept: Graph embeddings and representation learning
  - Why needed here: MDFT-g uses graph representations of planning tasks, requiring understanding of how to encode graph structures into vector space.
  - Quick check question: What are common methods for converting graph structures into fixed-length vector embeddings, and how do they preserve structural information?

## Architecture Onboarding

- Component map: Task encoding → clustering → sample selection → fine-tuning → evaluation
- Critical path: Task encoding → clustering → sample selection → fine-tuning → evaluation
- Design tradeoffs:
  - Graph vs. language embeddings: Graph embeddings better capture task structure but require graph construction; language embeddings are faster but less discriminative
  - Sample size vs. diversity: Larger k provides better coverage but increases fine-tuning cost
  - Clustering parameters: Number of clusters affects diversity-generality tradeoff
- Failure signatures:
  - Poor performance despite high diversity: Indicates clustering is not grouping similar tasks correctly
  - High variance in results: Suggests instability in sample selection or clustering
  - Rapid performance saturation: May indicate insufficient task diversity in the dataset
- First 3 experiments:
  1. Compare graph embeddings vs. language embeddings on a small set of planning tasks to verify structural preservation
  2. Test clustering effectiveness by checking if selected samples cover different regions of the task space
  3. Evaluate sample efficiency by comparing MDFT-g performance at k=50 vs. random sampling at k=100

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal number of fine-tuning epochs for different LLMs to balance planning performance and computational costs?
- Basis in paper: The paper mentions restricting GPT-3.5-turbo to fewer epochs due to high economic costs while allowing Llama models more epochs, noting that GPT-3.5-turbo was outperformed by Llama-3-8b with 4000 samples due to this limitation.
- Why unresolved: The paper only compares different numbers of samples and two distinct fine-tuning strategies (MDFT-g vs random), but doesn't systematically investigate the impact of fine-tuning epochs on performance or cost-effectiveness.
- What evidence would resolve it: A comprehensive study varying fine-tuning epochs across multiple sample sizes for both closed-source (GPT-3.5-turbo) and open-source (Llama-3-8b) models, measuring both planning performance and economic costs to identify the optimal epoch count for each model.

### Open Question 2
- Question: How can cross-domain transferability be improved for LLMs in automated planning?
- Basis in paper: The paper explicitly states that LLMs exhibit poor cross-domain transferability even worse than un-fine-tuned models, and notes that this is due to significant differences between planning domains in actions, predicates, and objects.
- Why unresolved: The paper only demonstrates the problem exists but doesn't propose or test solutions for improving cross-domain transfer, merely suggesting that fine-tuning on mixed data enables multi-task planning capabilities with higher variance.
- What evidence would resolve it: Experiments testing various approaches to improve cross-domain transfer, such as curriculum learning across domains, meta-learning techniques, or developing domain-agnostic planning representations that capture common planning principles across different domains.

### Open Question 3
- Question: What is the theoretical limit of planning performance achievable through fine-tuning alone, without external tools or feedback mechanisms?
- Basis in paper: The paper shows that fine-tuning achieves solved rates exceeding 95% for certain domains and models, but doesn't explore the absolute ceiling of performance achievable through fine-tuning alone or compare against optimal planners.
- Why unresolved: The experiments focus on relative improvements through different fine-tuning strategies but don't benchmark against theoretical or practical upper bounds of planning performance in these domains.
- What evidence would resolve it: A systematic study comparing fine-tuned LLM performance against optimal planners (like Fast Downward) across a comprehensive set of planning domains, identifying the performance gap and analyzing whether certain planning problem characteristics are particularly challenging for fine-tuned LLMs to master.

## Limitations

- Evaluation is constrained to specific planning benchmarks (ISPY, IPC domains) that may not represent real-world planning complexity
- Graph representation approach assumes planning tasks can be adequately captured through object-predicate structures, which may not hold for tasks requiring rich contextual understanding
- Computational overhead of graph construction and embedding generation for MDFT-g is not thoroughly analyzed compared to simpler methods

## Confidence

- High confidence: The fundamental claim that fine-tuning improves LLM planning performance is strongly supported by empirical results showing >95% solved rates after training
- Medium confidence: The superiority of graph embeddings over language embeddings for task representation is demonstrated within specific planning domains tested
- Medium confidence: The sample efficiency gains (100 vs 200 samples) are statistically significant within the experimental setup

## Next Checks

1. **Cross-domain generalization test**: Evaluate MDFT-g on planning tasks from domains not represented in the training data (e.g., different IPC categories or real-world robotic planning scenarios) to assess whether the diversity gains transfer beyond the benchmark set.

2. **Ablation study on graph construction**: Systematically vary the graph representation granularity (e.g., including/excluding certain predicate types, varying node representations) to determine which aspects of the graph encoding are most critical for the performance improvements.

3. **Computational overhead analysis**: Measure and compare the wall-clock time and resource requirements for graph construction and embedding generation in MDFT-g versus language embedding methods, particularly at scale, to fully understand the practical tradeoffs of the approach.