---
ver: rpa2
title: 'PertEval: Unveiling Real Knowledge Capacity of LLMs with Knowledge-Invariant
  Perturbations'
arxiv_id: '2405.19740'
source_url: https://arxiv.org/abs/2405.19740
tags:
- knowledge
- llms
- question
- choice
- capacity
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper presents PertEval, a toolkit to measure real knowledge
  capacity of large language models (LLMs) using knowledge-invariant perturbations.
  It addresses the issue of overestimated LLM performance on static benchmarks due
  to limited test scenarios and data contamination risks.
---

# PertEval: Unveiling Real Knowledge Capacity of LLMs with Knowledge-Invariant Perturbations

## Quick Facts
- **arXiv ID**: 2405.19740
- **Source URL**: https://arxiv.org/abs/2405.19740
- **Reference count**: 40
- **Primary result**: PertEval toolkit exposes overestimated LLM performance on static benchmarks through knowledge-invariant perturbations, revealing 25.8% absolute overestimation for GPT-4

## Executive Summary
PertEval addresses the critical issue of overestimated LLM performance on static benchmarks by introducing knowledge-invariant perturbations that generate diverse test scenarios while preserving core knowledge content. The toolkit employs human-like restatement techniques to create perturbed test samples and includes response consistency analyses comparing performance on raw versus perturbed test sets. Experiments with six LLMs demonstrate significant overestimation of knowledge capacity on raw benchmarks, with detailed transition analyses exposing various weaknesses in LLMs' knowledge mastery. PertEval serves as an essential tool for unveiling the true knowledge capacity of LLMs when applied alongside any close-ended benchmark.

## Method Summary
PertEval evaluates real knowledge capacity of LLMs using knowledge-invariant perturbations to transform static benchmark questions while preserving answer invariance. The method applies content-level and format-level perturbations to generate diverse test scenarios, then evaluates LLM performance on both original and perturbed data. Key metrics include Consistent Accuracy (ACC@Consist), Performance Drop Rate (PDR), and Recall of Performance (ROP) to quantify the gap between raw benchmark performance and performance under perturbations. The toolkit also includes response pattern analysis to categorize LLM responses and identify specific weaknesses in knowledge acquisition.

## Key Results
- GPT-4 showed 25.8% absolute overestimation of knowledge capacity on raw benchmarks compared to perturbed versions
- Knowledge-invariant perturbations affected LLMs by increasing the ratio of extra multiple choices, revealing potential rote memorization
- Transition analysis metrics (ACC@Consist, PDR, ROP) effectively quantified the gap between raw and real knowledge capacity across all six tested LLMs

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Knowledge-invariant perturbations expose memorization vs. true understanding in LLMs
- **Mechanism**: By altering irrelevant details while preserving semantic and answer invariance, PertEval creates test scenarios that differentiate rote memorization from robust knowledge acquisition
- **Core assumption**: LLMs that truly understand content should maintain performance across knowledge-invariant transformations
- **Evidence anchors**: Response pattern analysis reveals LLMs' potential rote memorization of correct options, leading to inflated performance
- **Break condition**: If perturbations inadvertently alter semantic content or if LLMs develop robustness to these specific transformations during training

### Mechanism 2
- **Claim**: Response pattern analysis reveals specific weaknesses in LLM knowledge acquisition
- **Mechanism**: By categorizing responses into patterns (correct choice, extra multiple choice, etc.), PertEval identifies whether LLMs struggle with filtering incorrect options or understanding question formats
- **Core assumption**: Response patterns provide meaningful insights into LLM reasoning processes
- **Evidence anchors**: Response pattern analysis discovered that PertEval retains LLMs' uncertainty to specious knowledge and revealed their rote memorization to correct options
- **Break condition**: If response patterns become too complex to interpret or if LLMs develop strategies to mask their weaknesses

### Mechanism 3
- **Claim**: Transition analysis metrics provide quantitative measures of real knowledge capacity
- **Mechanism**: Metrics like Consistent Accuracy (ACC@Consist) and Performance Drop Rate (PDR) quantify the gap between raw benchmark performance and performance under perturbations
- **Core assumption**: Consistent performance across perturbations indicates genuine knowledge mastery
- **Evidence anchors**: The transition analysis aims to measure the real knowledge capacity of LLMs from their responses to benchmark datasets
- **Break condition**: If metrics don't correlate with practical LLM performance or if they become gamed by model developers

## Foundational Learning

- **Concept**: Knowledge invariance standards
  - **Why needed here**: To ensure perturbations preserve the core knowledge being tested while altering surface features
  - **Quick check question**: What are the four standards of knowledge invariance defined in the paper?

- **Concept**: Response pattern taxonomy
  - **Why needed here**: To categorize LLM responses and identify specific weaknesses in knowledge acquisition
  - **Quick check question**: What are the five response patterns defined in the response pattern analysis?

- **Concept**: Transition analysis metrics
  - **Why needed here**: To quantify the difference between raw benchmark performance and performance under perturbations
  - **Quick check question**: What is the formula for Consistent Accuracy (ACC@Consist)?

## Architecture Onboarding

- **Component map**: Knowledge-invariant perturbations (content-level and format-level) → Knowledge invariance checking (LLM-based scoring and testing on mastered questions) → Transition analysis (metrics and response pattern analysis)
- **Critical path**: Perturbation generation → Knowledge invariance checking → Transition analysis → Performance evaluation
- **Design tradeoffs**:
  - Content-level vs. format-level perturbations: Balancing knowledge preservation with transformation effectiveness
  - LLM-based vs. testing-based invariance checking: Trade-off between automation and reliability
  - Comprehensive metrics vs. interpretability: Balancing quantitative rigor with actionable insights
- **Failure signatures**:
  - High knowledge invariance scores but poor correlation with real-world performance
  - Response patterns that don't align with expected weaknesses
  - Metrics that show inconsistent results across different LLM architectures
- **First 3 experiments**:
  1. Apply knowledge-invariant perturbations to a small subset of benchmark questions and manually verify invariance
  2. Run response pattern analysis on a diverse set of LLMs and compare results
  3. Calculate transition analysis metrics for a single LLM across multiple perturbations and assess consistency

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: How can the knowledge invariance of perturbations be measured more effectively than traditional string-based metrics like Levenshtein distance?
- **Basis in paper**: The paper found that Levenshtein distance lacks correlation with knowledge invariance scores, indicating a need for better measurement methods
- **Why unresolved**: Current methods either rely on LLM scoring which may be subjective, or on performance consistency which doesn't directly measure knowledge invariance
- **What evidence would resolve it**: Development and validation of a metric that captures knowledge-relevant features of text beyond string-level changes

### Open Question 2
- **Question**: What specific training strategies are most effective for improving LLMs' performance on knowledge-invariant perturbations?
- **Basis in paper**: The authors discuss fine-tuning LLaMA-3-8B-Instruct on perturbed data and observe different effects for format-level vs content-level perturbations
- **Why unresolved**: While the paper demonstrates the feasibility of fine-tuning, it doesn't provide comprehensive experiments across different model sizes and training regimes
- **What evidence would resolve it**: Systematic experiments comparing different fine-tuning approaches and their effects on performance across various perturbation types

### Open Question 3
- **Question**: Can PertEval be extended to evaluate knowledge capacity in open-ended question formats beyond multiple choice?
- **Basis in paper**: The authors discuss this as a future direction, noting challenges in defining correctness and ensuring knowledge invariance for complex questions
- **Why unresolved**: Multiple choice questions offer clear correctness criteria and perturbation boundaries, but open-ended formats lack these advantages
- **What evidence would resolve it**: Successful extension of PertEval to at least one open-ended format with validated perturbation strategies and measurement metrics

## Limitations

- Exact prompt templates for knowledge-invariant paraphrasing are not fully specified, making it difficult to verify true knowledge invariance
- Corpus reveals only 25 related papers with average citations of 0.0, suggesting limited validation from the broader research community
- Knowledge invariance checking relies on LLM-based scoring, which could introduce its own biases and circular validation

## Confidence

- **High confidence**: Overall framework's validity and utility as a toolkit for LLM evaluation
- **Medium confidence**: Specific response pattern analysis methodology and its interpretability
- **Medium confidence**: Transition analysis metrics' ability to quantify real knowledge capacity
- **Low confidence**: Robustness of the perturbation generation process without access to exact implementation details

## Next Checks

1. Conduct ablation studies by systematically removing each perturbation strategy to quantify their individual contributions to performance drops
2. Test PertEval on additional LLM architectures and benchmarks beyond the MMLU datasets to assess generalizability
3. Perform human evaluation studies to verify that the perturbations maintain knowledge invariance according to human judgment