---
ver: rpa2
title: An Electrocardiogram Foundation Model Built on over 10 Million Recordings with
  External Evaluation across Multiple Domains
arxiv_id: '2410.04133'
source_url: https://arxiv.org/abs/2410.04133
tags:
- ecgs
- page
- ecgfounder
- block
- dataset
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents ECGFounder, a foundation model for electrocardiogram
  (ECG) analysis trained on over 10 million annotated ECG recordings from the Harvard-Emory
  ECG Database. The model employs a novel approach to handle incomplete real-world
  annotations using positive unlabeled (PU) learning and demonstrates strong performance
  across multiple clinical tasks.
---

# An Electrocardiogram Foundation Model Built on over 10 Million Recordings with External Evaluation across Multiple Domains

## Quick Facts
- arXiv ID: 2410.04133
- Source URL: https://arxiv.org/abs/2410.04133
- Authors: Jun Li; Aaron Aguirre; Junior Moura; Che Liu; Lanhai Zhong; Chenxi Sun; Gari Clifford; Brandon Westover; Shenda Hong
- Reference count: 40
- Primary result: ECGFounder achieves expert-level performance on ECG analysis with AUROC exceeding 0.95 for 80 diagnoses, trained on over 10 million recordings

## Executive Summary
ECGFounder is a foundation model for electrocardiogram analysis trained on over 10 million annotated ECG recordings from the Harvard-Emory ECG Database. The model employs positive unlabeled (PU) learning to handle incomplete real-world annotations and demonstrates strong performance across multiple clinical tasks. When evaluated on internal validation sets, ECGFounder achieves expert-level performance with AUROC exceeding 0.95 for 80 different diagnoses. The model shows superior generalization across external validation sets from different regions and hospitals.

## Method Summary
ECGFounder utilizes a RegNet architecture with 76.3M parameters, trained on over 10 million ECG recordings using a novel PU learning approach to address incomplete annotations. The model incorporates data augmentation based on electrical axis to enhance single-lead ECG performance and employs contrastive learning for robust feature representation. Pre-training leverages multi-label classification across 150 diagnostic categories, followed by fine-tuning for downstream tasks including demographic analysis, clinical event detection, and cross-modality cardiac rhythm diagnosis.

## Key Results
- ECGFounder achieves AUROC exceeding 0.95 for 80 different diagnoses on internal validation sets
- Outperforms baseline models by 3-5 percentage points in AUROC for downstream fine-tuning tasks
- Demonstrates superior generalization across external validation sets from different regions and hospitals

## Why This Works (Mechanism)

### Mechanism 1
- Claim: PU learning mitigates the impact of missing labels in ECG annotations.
- Mechanism: By treating unlabeled categories as neither negative nor positive, the model avoids the false negative bias that occurs when missing labels are assumed negative.
- Core assumption: In ECG annotations, unlabeled categories are more likely to be missing positives rather than true negatives.
- Evidence anchors:
  - [abstract] "To address the inherent challenges of incomplete annotations in real-world data, we introduce a novel method for pre-processing and training on these annotations, ensuring robust performance even under sub-optimal conditions."
  - [section] "To address the challenge brought by incomplete ECG annotations, we introduce positive unlabeled (PU) learning method."
- Break condition: If the dataset annotations were complete and comprehensive, the need for PU learning would be reduced or eliminated.

### Mechanism 2
- Claim: Data augmentation based on electrical axis improves single-lead ECG model performance.
- Mechanism: By systematically enhancing standard 12-lead ECG data to simulate various clinical scenarios of axis inversion, the model learns robust features applicable to single-lead ECGs.
- Core assumption: Single-lead ECGs can benefit from learning features derived from the relationship between ECG vectors and leads, even when those leads are not directly present.
- Evidence anchors:
  - [abstract] "Moreover,bytrainingthesingle-leadECGmodelbasedonleadaugmentation,weareabletomaintainhighdiagnosticperformanceonsingle-leadECGsaswell."
  - [section] "By systematically enhancing standard 12-lead ECG data, we simulate various clinical scenarios of axis inversion, thereby enhancing the model's robustness and versatility."
- Break condition: If the augmented data does not accurately represent real-world single-lead ECG variations, the model's performance might not improve or could degrade.

### Mechanism 3
- Claim: Foundation model pre-training on large ECG datasets enables effective transfer learning.
- Mechanism: By learning generalizable representations from over 10 million ECGs, the model captures a broad range of cardiac features that can be fine-tuned for specific downstream tasks.
- Core assumption: The learned features are sufficiently general to be applicable across diverse ECG-related tasks and datasets.
- Evidence anchors:
  - [abstract] "When fine-tuned, ECGFounder outperforms baseline models in demographic analysis, clinical event detection, and cross-modality cardiac rhythm diagnosis."
  - [section] "Experimental results demonstrate that ECGFounder achieves expert-level performance on internal validation sets, with AUROC exceeding 0.95 for eighty diagnoses."
- Break condition: If the downstream tasks differ significantly from the pre-training domain, transfer learning benefits might be limited.

## Foundational Learning

- Concept: Multi-label classification
  - Why needed here: ECG diagnoses often involve multiple concurrent conditions, requiring the model to predict multiple labels simultaneously.
  - Quick check question: Does the model output a probability for each of the 150 label categories, allowing for multiple positive predictions?

- Concept: Contrastive learning
  - Why needed here: To learn robust ECG representations that generalize well across different datasets and conditions without relying solely on labeled data.
  - Quick check question: Does the model use contrastive learning techniques during pre-training to learn meaningful ECG features?

- Concept: Transfer learning
  - Why needed here: To leverage the knowledge gained from the large pre-training dataset for improved performance on specific downstream tasks with limited data.
  - Quick check question: Can the pre-trained model be fine-tuned on smaller, task-specific datasets to achieve better performance than training from scratch?

## Architecture Onboarding

- Component map: Input layer for 12-lead ECG signals → Convolutional layers with increasing depth → Skip connections → Fully connected layer for classification
- Critical path: Data preprocessing (resampling, filtering, normalization) → Model input → Convolutional feature extraction → Fully connected classification → Output predictions
- Design tradeoffs: Large model size (76.3M parameters) for improved performance vs. computational efficiency; extensive data augmentation for robustness vs. potential overfitting; PU learning for handling incomplete annotations vs. complexity in implementation
- Failure signatures: Poor performance on external datasets (indicating overfitting); significant drop in performance for single-lead ECGs (indicating issues with data augmentation); low sensitivity for rare conditions (indicating issues with PU learning or class imbalance)
- First 3 experiments:
  1. Evaluate the model's performance on a held-out validation set to check for overfitting
  2. Test the model's ability to generalize to a different ECG dataset (e.g., PTB-XL) to assess external validity
  3. Fine-tune the model on a specific downstream task (e.g., atrial fibrillation detection) and compare performance to a baseline model trained from scratch

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does ECGFounder's performance compare to human cardiologists on extremely rare or highly complex ECG abnormalities not well-represented in the training data?
- Basis in paper: [explicit] The paper mentions that "ECGFounder matches or even exceeds the performance of cardiology experts on the internal review test sets" but notes that "the dataset contains both ICD-9 and ICD-10 formats" and that the model was trained on 150 label categories. The authors also state that "cardiology experts typically use a unified annotation system, the richness of the dataset labels is not very high, often only including common ECG abnormalities and omitting many important but rare diagnostic labels."
- Why unresolved: The paper does not provide specific data on how the model performs on rare conditions or ECG abnormalities that were underrepresented in the training set.
- What evidence would resolve it: Testing the model on a curated dataset of rare ECG abnormalities with known ground truth diagnoses, comparing performance metrics (sensitivity, specificity, F1-score) between ECGFounder and expert cardiologists.

### Open Question 2
- Question: What is the computational and memory overhead of ECGFounder when deployed on resource-constrained wearable devices compared to the optimized single-lead model?
- Basis in paper: [explicit] The authors mention that "we have scaled down the model's parameter size to optimize for wearable devices with limited computational resources" and that the single-lead model was developed through "a novel data augmentation method based on the cardiac axis." However, they do not provide specific computational benchmarks.
- Why unresolved: While the paper discusses model architecture and data augmentation, it does not provide quantitative data on memory usage, inference time, or power consumption on actual wearable hardware.
- What evidence would resolve it: Benchmarking the model on representative wearable device hardware, measuring parameters such as inference time per ECG, memory footprint, and power consumption during continuous monitoring.

### Open Question 3
- Question: How does ECGFounder handle ECG data from patients with atypical cardiac anatomy or congenital heart conditions that deviate significantly from normal cardiac electrical pathways?
- Basis in paper: [explicit] The paper discusses the model's performance on various ECG diagnoses and mentions "its adaptability extends beyond disease classification to tasks like demographics and laboratory measurement detection," but does not specifically address congenital heart conditions or anatomical variations.
- Why unresolved: The study focuses on acquired cardiovascular diseases and common ECG abnormalities, with no specific mention of congenital heart disease or patients with anatomical variations that could affect ECG interpretation.
- What evidence would resolve it: Evaluating the model's performance on ECGs from patients with known congenital heart conditions, comparing diagnostic accuracy and false positive/negative rates against expert cardiologists' assessments.

## Limitations

- Data Domain Limitations: The model was trained exclusively on data from the Harvard-Emory ECG Database, which may not fully represent global ECG patterns and rare conditions.
- Technical Constraints: The 76.3M parameter model requires substantial computational resources for deployment in resource-limited settings.
- Clinical Validation Gaps: There is no mention of prospective clinical trials or real-world deployment studies to assess performance in actual clinical workflows.

## Confidence

- High Confidence (AUROC > 0.95 for 80 diagnoses): Internal validation results showing expert-level performance are well-supported by methodology and evaluation framework
- Medium Confidence (Outperformance of baselines by 3-5 percentage points): Improvements are statistically significant but baseline models and comparison methodology details are limited
- Medium Confidence (Effective handling of incomplete annotations): PU learning approach is theoretically sound but depends on key assumptions about unlabeled data

## Next Checks

1. Evaluate ECGFounder's performance on completely independent datasets not used in any part of the training or validation process, particularly focusing on rare conditions and different demographic populations.

2. Conduct a prospective study in clinical settings to assess the model's performance in actual diagnostic workflows, including time-to-diagnosis, inter-observer agreement, and impact on clinical decision-making.

3. Perform detailed benchmarking of the model's inference time and resource requirements across different hardware configurations to assess its feasibility for deployment in resource-limited healthcare settings and mobile applications.