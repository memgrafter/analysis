---
ver: rpa2
title: 'CoE-SQL: In-Context Learning for Multi-Turn Text-to-SQL with Chain-of-Editions'
arxiv_id: '2405.02712'
source_url: https://arxiv.org/abs/2405.02712
tags:
- clause
- party
- edit
- table
- coe-sql
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proposes CoE-SQL, a method for multi-turn text-to-SQL
  generation using large language models (LLMs) with in-context learning. The core
  idea is to explicitly model the changes in SQL queries across conversation turns
  using a chain of edition rules.
---

# CoE-SQL: In-Context Learning for Multi-Turn Text-to-SQL with Chain-of-Editions

## Quick Facts
- arXiv ID: 2405.02712
- Source URL: https://arxiv.org/abs/2405.02712
- Reference count: 40
- Primary result: CoE-SQL achieves state-of-the-art performance among LLM-based methods on SParC and CoSQL benchmarks

## Executive Summary
CoE-SQL introduces a novel approach for multi-turn text-to-SQL generation that explicitly models context dependencies between consecutive SQL queries using a chain-of-editions framework. Instead of generating complete SQL from scratch, the method applies predefined unit edit rules to modify previous queries based on extracted edition chains. The approach leverages AST comparison to automatically identify necessary modifications and serializes these changes in natural language format for in-context learning with LLMs.

## Method Summary
The method defines 14 unit edit rules for modifying SQL queries, extracts edition chains by comparing ASTs of consecutive queries, and serializes these chains in natural language for LLM prompting. The framework uses few-shot learning with exemplars from the training dataset to demonstrate how to apply edition chains. During inference, the system constructs prompts containing database schema, questions, previous SQL, and the edition chain, then generates SQL using GPT-3.5-turbo-16k with temperature=0.

## Key Results
- Achieves 54.2% exact match on SParC, outperforming baselines like Re2SQL (50.5%) and ChatSQL (49.3%)
- Reaches 59.6% execution accuracy on CoSQL, surpassing other in-context learning methods
- Maintains competitive performance compared to fine-tuned models while using fewer training examples

## Why This Works (Mechanism)

### Mechanism 1
- Claim: CoE-SQL improves performance by modeling context dependency through SQL edition chains rather than full query generation.
- Mechanism: Instead of generating complete SQL from scratch, the model edits previous SQL queries using a sequence of unit edit rules that capture the user's evolving intent across turns.
- Core assumption: Multi-turn text-to-SQL queries can be constructed by applying a small number of predictable modifications to the previous query rather than requiring full regeneration.
- Evidence anchors:
  - [abstract] "the current SQL query can be modified from the preceding SQL query with only a few operations due to the context dependency"
  - [section 3.2] "we totally define 14 unit edit rules shown in Table 13"
- Break condition: When user intent shifts dramatically between turns, requiring structural changes that can't be captured by predefined edit rules, or when the context dependency is minimal and direct generation would be more efficient.

### Mechanism 2
- Claim: Abstract Syntax Tree (AST) comparison enables automatic extraction of edition chains with minimal length.
- Mechanism: The algorithm compares ASTs of consecutive SQL queries to identify structural differences and generates the shortest possible sequence of unit edit rules that transform one query into the next.
- Core assumption: AST comparison provides a precise representation of SQL structure that enables systematic identification of differences between queries.
- Evidence anchors:
  - [section 3.3] "Figure 1 shows an example of a comparison between two ASTs"
  - [section 3.3] "We use the few-shot learning method to activate LLMs' ability of utilizing our pre-defined unit edit rules"
- Break condition: When SQL queries have complex nested structures or when the difference between queries involves semantically equivalent but structurally different representations that AST comparison might miss.

### Mechanism 3
- Claim: Natural language serialization of edition chains achieves better performance than rule-based or code-based representations.
- Mechanism: The system converts unit edit rules into natural language descriptions that align with the LLM's pretraining corpus, making it easier for the model to understand and apply the edits.
- Core assumption: LLMs trained on natural language corpora will better understand and execute instructions presented in natural language format compared to custom rule syntax or code.
- Evidence anchors:
  - [section 3.4] "we find that the NL description performs the best on two benchmarks, SParC and CoSQL"
  - [section 3.4] "The chain-of-editions style with the edit rule performs relatively poor, since our unit edit rules are self-designed and very unlikely to appear in the pretraining corpus"
- Break condition: When the natural language descriptions become too verbose or ambiguous, or when the LLM's pretraining data doesn't contain sufficient examples of SQL-related natural language descriptions.

## Foundational Learning

- Concept: Abstract Syntax Trees (ASTs) and their comparison
  - Why needed here: ASTs provide a structured representation of SQL queries that enables systematic identification of differences between consecutive queries, which is essential for extracting edition chains.
  - Quick check question: Given two simple SQL queries like "SELECT * FROM table" and "SELECT column FROM table", what structural difference would their ASTs reveal?

- Concept: In-context learning and few-shot prompting
  - Why needed here: The method relies on providing the LLM with exemplars that demonstrate how to apply edition chains, requiring understanding of how to structure effective few-shot prompts.
  - Quick check question: If you have 4 database schemas and select 4 examples from each, how many total exemplars will be in your prompt?

- Concept: SQL query structure and components
  - Why needed here: Understanding the components (SELECT, FROM, WHERE, etc.) is essential for defining unit edit rules and recognizing what modifications are possible.
  - Quick check question: What are the minimum components required for a valid SQL query, and which components are optional?

## Architecture Onboarding

- Component map:
  Exemplar Selection -> AST Comparison Engine -> Edition Chain Serializer -> Prompt Constructor -> LLM Interface

- Critical path:
  1. Receive current question and context
  2. Extract edition chain from exemplar comparisons
  3. Serialize edition chain in natural language
  4. Construct prompt with all necessary components
  5. Send to LLM with temperature=0
  6. Post-process and validate SQL output

- Design tradeoffs:
  - Natural language vs. code-based serialization: Natural language performs better but may be less precise; code is more precise but harder for LLM to interpret
  - Maximum edition chain length: Longer chains capture more complex modifications but increase prompt complexity and may confuse the LLM
  - Exemplar selection strategy: Random selection is simple but may miss important patterns; sophisticated selection could improve performance but adds complexity

- Failure signatures:
  - SQL syntax errors in output: May indicate issues with prompt clarity or LLM understanding
  - Missing context dependencies: May suggest edition chains are not being properly applied
  - Overly complex SQL generation: May indicate LLM is ignoring edition chains and generating from scratch
  - Performance degradation with longer conversations: May suggest edition chain approach becomes less effective over time

- First 3 experiments:
  1. Test different serialization styles (edit rules, Python code, natural language) on a small dataset to validate the performance differences
  2. Experiment with maximum edition chain length thresholds to find optimal balance between context capture and complexity
  3. Compare performance with and without the "thought-before-edition" analysis step to validate its contribution to accuracy

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions in the traditional sense, but several limitations and future directions are implied throughout the discussion.

## Limitations
- Edit Rule Coverage: The method relies on 14 predefined edit rules without analysis of rule completeness or coverage percentage
- Prompt Construction Details: Key implementation details like exact prompt templates and temperature settings are missing
- Generalization to New Domains: Evaluation is limited to two specific conversational text-to-SQL datasets

## Confidence
The evidence supporting the core claims is High confidence overall. The experimental results show consistent improvements across both SParC and CoSQL benchmarks with clear statistical significance. The ablation studies directly validate the contribution of the edition chain approach compared to baselines.

## Next Checks
1. Conduct a systematic analysis of the 14 edit rules to determine what percentage of SQL transitions in the test sets they can handle
2. Systematically test different prompt formatting variations and temperature settings to establish the robustness of the method to implementation details
3. Evaluate CoE-SQL on non-conversational text-to-SQL datasets or on text-to-SQL tasks from different domains to assess generalizability beyond the current benchmarks