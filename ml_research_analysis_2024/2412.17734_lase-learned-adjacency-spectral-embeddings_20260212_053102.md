---
ver: rpa2
title: 'LASE: Learned Adjacency Spectral Embeddings'
arxiv_id: '2412.17734'
source_url: https://arxiv.org/abs/2412.17734
tags:
- lase
- graph
- node
- embeddings
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces LASE (Learned Adjacency Spectral Embeddings),
  a neural network architecture designed to learn spectral graph embeddings from adjacency
  matrices in an unsupervised manner. The key idea is to unroll a gradient descent
  algorithm for low-rank matrix factorization into a GNN, combining GCN and GAT modules
  in each layer.
---

# LASE: Learned Adjacency Spectral Embeddings

## Quick Facts
- arXiv ID: 2412.17734
- Source URL: https://arxiv.org/abs/2412.17734
- Authors: Sofía Pérez Casulo; Marcelo Fiori; Federico Larroca; Gonzalo Mateos
- Reference count: 26
- One-line primary result: Neural network architecture that learns spectral graph embeddings from adjacency matrices by unrolling gradient descent, achieving reconstruction accuracy comparable to ASE while being significantly faster.

## Executive Summary
This paper introduces LASE (Learned Adjacency Spectral Embeddings), a novel neural network architecture designed to learn spectral graph embeddings from adjacency matrices in an unsupervised manner. The key innovation is unrolling a gradient descent algorithm for low-rank matrix factorization into a GNN, combining GCN and GAT modules in each layer. This design allows LASE to approximate eigenvectors of the adjacency matrix, overcoming limitations of standard GNNs in capturing global graph structure. Experiments demonstrate that LASE achieves reconstruction accuracy comparable to ASE while being significantly faster, especially for large graphs.

## Method Summary
LASE is trained by unrolling a gradient descent algorithm for low-rank matrix factorization into a GNN with L layers, each combining a Graph Convolutional Network (GCN) and a fully-connected Graph Attention Network (GAT) module. The method minimizes squared reconstruction error on adjacency matrices, optionally with masks for missing edges. For end-to-end tasks, LASE is integrated into a larger pipeline by concatenating its output with node features and training jointly with a GNN using a combined loss. The architecture is parameter-efficient with a fixed number of learnable parameters independent of graph size, enabling transfer of weights across graphs from the same distribution.

## Key Results
- LASE achieves reconstruction accuracy comparable to ASE while being significantly faster, especially for large graphs
- When integrated into end-to-end pipelines for node classification or link prediction, LASE outperforms both traditional GNNs and methods using precomputed spectral positional encodings
- The method is parameter-efficient, transferable across graph sizes, and robust to distribution shifts, even in the presence of missing edges

## Why This Works (Mechanism)

### Mechanism 1
LASE unrolls gradient descent for low-rank matrix factorization into a GNN, combining GCN and GAT modules to approximate eigenvectors of the adjacency matrix. Each iteration of the gradient descent algorithm is mapped to a layer in the neural network. The GCN part computes a graph convolution (first-order term), while the GAT part performs a fully-connected inner-product-based attention. This superposition captures both local and global graph structure, overcoming the limitation of GCNs alone.

### Mechanism 2
LASE is parameter-efficient and transferable across graph sizes because the number of learnable parameters is independent of the number of nodes. The LASE architecture has a fixed number of parameters determined by the number of layers and embedding dimension, not the graph size. This allows training on smaller subgraphs and inference on larger graphs from the same distribution.

### Mechanism 3
LASE can be integrated into an end-to-end pipeline for supervised tasks, learning "discriminative ASEs" that outperform GNNs with precomputed spectral positional encodings. LASE is a differentiable function, allowing it to be trained jointly with a downstream task (e.g., node classification) by combining reconstruction error and task-specific loss. This endows the embeddings with a discriminative bias towards the task.

## Foundational Learning

- Concept: Gradient descent for low-rank matrix factorization
  - Why needed here: LASE is built by unrolling this algorithm into a neural network. Understanding its convergence properties and the resulting update rules is crucial for interpreting the LASE architecture.
  - Quick check question: What is the update rule for gradient descent in the factored form of the adjacency spectral embedding problem, and how does it relate to graph convolutions and attention mechanisms?

- Concept: Graph neural networks (GNNs) and their expressive power
  - Why needed here: LASE combines GCN and GAT modules. Understanding the limitations of GCNs (e.g., inability to distinguish nodes with similar local structure in regular graphs) motivates the need for the GAT component in LASE.
  - Quick check question: Why do standard GNNs fail to produce useful embeddings in a symmetric stochastic block model with two equally-sized communities and the same connection probabilities?

- Concept: Algorithm unrolling
  - Why needed here: LASE is designed using this principle. Understanding how iterative algorithms can be mapped to neural network layers is essential for grasping the LASE architecture and its properties.
  - Quick check question: How does algorithm unrolling transform an iterative optimization algorithm into a neural network, and what are the benefits of this approach?

## Architecture Onboarding

- Component map:
  LASE Block: GCN (graph convolution) + GAT (fully-connected inner-product attention) -> LASE Network: Stack of LASE Blocks (truncated unrolled GD) -> Input: Random initialization (same as GD) -> Output: Estimated spectral embeddings -> Optional: Integration into end-to-end pipeline (concatenate with node features, train with combined loss)

- Critical path:
  1. Initialize LASE weights (randomly)
  2. Forward pass through LASE layers (graph convolution + attention at each layer)
  3. Compute loss (reconstruction error + task-specific loss if end-to-end)
  4. Backward pass and weight update
  5. Inference: Forward pass through trained LASE

- Design tradeoffs:
  - Depth (number of layers) vs. accuracy: More layers may improve approximation but increase computation
  - GCN vs. GAT balance: GCN captures local structure, GAT captures global structure. The ratio may depend on the graph
  - Sparse vs. full attention: Sparse attention reduces computation but may sacrifice accuracy

- Failure signatures:
  - Poor reconstruction error: LASE is not approximating the spectral embeddings well
  - Degraded task performance in end-to-end setting: LASE is not learning useful discriminative embeddings
  - Sensitivity to initialization: LASE may get stuck in bad local minima

- First 3 experiments:
  1. Train LASE on a small synthetic graph (e.g., SBM) and compare the reconstruction error to ASE and GD
  2. Test transferability by training LASE on a subgraph and evaluating on the full graph
  3. Integrate LASE into an end-to-end node classification pipeline and compare to using precomputed ASE embeddings

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the number of negative entries in the diagonal matrix Q affect the performance of GLASE for non-homophilic graphs?
- Basis in paper: The paper mentions that matrix Q can be tuned by selecting the number of negative entries based on minimizing reconstruction error, but does not provide a systematic method or guidelines for choosing this number
- Why unresolved: The paper only suggests trying different possibilities for Q and choosing the one that minimizes reconstruction error, without providing theoretical justification or empirical evidence on how to optimally select the number of negative entries
- What evidence would resolve it: Empirical studies systematically varying the number of negative entries in Q across different types of non-homophilic graphs, with clear performance metrics showing the impact on reconstruction accuracy and downstream task performance

### Open Question 2
- Question: What is the theoretical relationship between the transferability of LASE weights across different graph sizes and the spectral properties of the underlying graph distribution?
- Basis in paper: The paper empirically demonstrates that LASE weights trained on smaller subgraphs can be used for inference on larger graphs, but does not provide theoretical analysis of when and why this transferability works
- Why unresolved: The paper only shows empirical results on transferability without explaining the theoretical conditions under which learned weights remain effective across different graph sizes from the same distribution
- What evidence would resolve it: Theoretical analysis connecting spectral concentration properties of graphons/graphon models to the stability of learned LASE weights across different graph realizations, possibly using techniques from statistical learning theory for graph neural networks

### Open Question 3
- Question: How does the choice of sparse attention mechanism (Erdös-Rényi, Watts-Strogatz, or Big-Bird) affect the trade-off between computational efficiency and embedding quality for different graph topologies?
- Basis in paper: The paper compares different sparse attention mechanisms and shows they can achieve similar performance to full attention with reduced computation time, but does not systematically analyze how graph topology influences the optimal choice
- Why unresolved: The paper only tests these mechanisms on specific SBM graphs without providing a framework for understanding which sparse attention mechanism is optimal for different types of graph structures
- What evidence would resolve it: Systematic experiments varying graph topology (e.g., different clustering coefficients, diameter, degree distributions) and comparing the performance of each sparse attention mechanism, potentially leading to guidelines for selecting the appropriate mechanism based on graph properties

## Limitations
- The paper lacks complete hyperparameter specifications for LASE training and downstream GNN integration, particularly learning rates, batch sizes, training epochs, and downstream architecture details
- Sparse attention mechanisms are mentioned but not fully detailed in terms of parameters and implementation specifics
- The comparison methodology between LASE and baselines could be more rigorous, as some claims about performance improvements are based on relative rather than absolute metrics

## Confidence

- **High**: The core mechanism of unrolling gradient descent for low-rank matrix factorization into a GNN architecture is well-supported by theoretical foundations and clearly explained derivations
- **Medium**: The experimental results showing LASE's performance improvements are convincing, but the lack of complete hyperparameter details and rigorous statistical comparisons introduces some uncertainty
- **Medium**: The transferability claims across graph sizes are theoretically sound given the parameter efficiency, but practical limitations and the extent of distribution shifts are not fully explored

## Next Checks

1. **Hyperparameter Sensitivity Analysis**: Systematically vary learning rates, batch sizes, and number of layers to determine their impact on reconstruction error and task performance

2. **Statistical Significance Testing**: Conduct rigorous statistical tests comparing LASE against baselines across multiple random seeds and graph instances to establish the reliability of performance improvements

3. **Transferability Stress Test**: Evaluate LASE's performance when transferring between graphs with increasingly different distributions to quantify the limits of parameter efficiency benefits