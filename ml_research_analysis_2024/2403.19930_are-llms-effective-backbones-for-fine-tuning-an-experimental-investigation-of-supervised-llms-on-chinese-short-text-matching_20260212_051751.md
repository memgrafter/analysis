---
ver: rpa2
title: Are LLMs Effective Backbones for Fine-tuning? An Experimental Investigation
  of Supervised LLMs on Chinese Short Text Matching
arxiv_id: '2403.19930'
source_url: https://arxiv.org/abs/2403.19930
tags:
- task
- llms
- matching
- text
- figure
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates whether large language models (LLMs) can
  serve as effective backbones for supervised fine-tuning in the task of Chinese short
  text matching. The authors conduct a comprehensive experimental study comparing
  generative and discriminative fine-tuning approaches, as well as the impact of prompt
  formats and the addition of chain-of-thought (CoT) reasoning.
---

# Are LLMs Effective Backbones for Fine-tuning? An Experimental Investigation of Supervised LLMs on Chinese Short Text Matching

## Quick Facts
- arXiv ID: 2403.19930
- Source URL: https://arxiv.org/abs/2403.19930
- Authors: Shulin Liu; Chengcheng Xu; Hao Liu; Tinghao Yu; Tao Yang
- Reference count: 9
- One-line primary result: Generative fine-tuning of LLMs significantly outperforms discriminative approaches for Chinese short text matching, achieving 84.5% accuracy on LCQMC compared to 52.9% for few-shot GPT-4

## Executive Summary
This paper investigates whether large language models can serve as effective backbones for supervised fine-tuning in Chinese short text matching tasks. The authors conduct comprehensive experiments comparing generative and discriminative fine-tuning approaches, prompt formats, and the impact of chain-of-thought reasoning. Their findings demonstrate that generative fine-tuning significantly outperforms discriminative methods, particularly with limited training data, while supervised LLMs show reduced sensitivity to prompt complexity compared to zero/few-shot settings. The study provides evidence that LLMs can indeed be effective backbones for supervised fine-tuning in natural language understanding tasks.

## Method Summary
The study fine-tunes CLLM-7B, a Chinese-enhanced model based on LLaMA-2-7B, on two Chinese short text matching datasets (LCQMC and BQ). The authors compare generative versus discriminative fine-tuning approaches, test different prompt formats (concise vs. complex), and evaluate the impact of chain-of-thought reasoning on training samples. All experiments use full-model fine-tuning with accuracy as the primary evaluation metric, and results are compared against fine-tuned BERT and few-shot GPT-4 baselines.

## Key Results
- Generative fine-tuning achieved 84.5% accuracy on LCQMC compared to 52.9% for few-shot GPT-4
- Supervised LLMs showed no significant sensitivity to prompt complexity, unlike in zero/few-shot settings
- Chain-of-thought reasoning provided greater improvements on challenging datasets like BQ compared to simpler datasets
- Generative approaches outperformed discriminative fine-tuning especially when training data was limited

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Generative fine-tuning aligns with pre-training objectives and enables better knowledge transfer
- Mechanism: LLMs pre-trained with generative objectives (predicting next tokens) can leverage this familiar task structure during generative fine-tuning
- Core assumption: Pre-training has captured relevant semantic knowledge transferable to supervised tasks through generative fine-tuning
- Evidence anchors: Generative fine-tuning significantly outperforms discriminative approaches on limited data; aligns with pre-training procedure

### Mechanism 2
- Claim: Supervised LLMs are less sensitive to prompt complexity because they can learn task definitions from training data
- Mechanism: In supervised settings, explicit input-output examples allow models to learn task structure independently of prompt format
- Core assumption: Models can extract task-relevant patterns from labeled examples without relying on prompt engineering
- Evidence anchors: Supervised LLMs not sensitive to prompt complexity unlike zero/few-shot settings; comparable performance across prompt styles

### Mechanism 3
- Claim: Chain-of-thought reasoning in supervised settings improves performance on challenging tasks by providing intermediate reasoning steps
- Mechanism: CoT provides explicit reasoning traces that help break down complex decisions into manageable steps
- Core assumption: Intermediate reasoning steps are valuable for understanding complex relationships in data
- Evidence anchors: CoT improved performance particularly on challenging BQ dataset compared to simpler LCQMC

## Foundational Learning

- Concept: Generative vs Discriminative modeling
  - Why needed here: The paper compares these two paradigms for fine-tuning LLMs
  - Quick check question: What is the fundamental difference between generative and discriminative models in terms of their output?

- Concept: Chain-of-thought reasoning
  - Why needed here: The paper investigates CoT as a training augmentation technique
  - Quick check question: How does CoT differ from standard prompting approaches in terms of the reasoning process?

- Concept: Prompt engineering
  - Why needed here: The paper explores the impact of different prompt formats in supervised settings
  - Quick check question: Why might prompt sensitivity differ between zero-shot and supervised settings?

## Architecture Onboarding

- Component map: LLM backbone (CLLM-7B) → Fine-tuning method (generative/discriminative) → Prompt format → Output format (with/without CoT) → Task-specific head
- Critical path: Model input → Prompt processing → Task-specific processing → Output generation → Loss calculation → Parameter update
- Design tradeoffs: Generative vs discriminative fine-tuning (knowledge transfer vs explicit feature extraction), prompt complexity (model burden vs task clarity), CoT inclusion (reasoning depth vs training complexity)
- Failure signatures: Poor performance on limited data (generative advantage not realized), high sensitivity to prompts (supervised learning not effective), minimal CoT benefit (task not complex enough)
- First 3 experiments:
  1. Compare generative vs discriminative fine-tuning on small dataset to verify mechanism 1
  2. Test concise vs complex prompts on same dataset to verify mechanism 2
  3. Add CoT to training data and evaluate on challenging dataset to verify mechanism 3

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do supervised LLMs perform on other NLU tasks beyond Chinese short text matching?
- Basis in paper: The authors note their conclusions may apply to other NLU tasks like text classification but acknowledge this requires additional experimentation
- Why unresolved: Only examined one specific NLU task on two datasets
- What evidence would resolve it: Similar experiments on diverse NLU tasks (sentiment analysis, NER, text classification) compared to zero/few-shot LLMs and traditional fine-tuned models

### Open Question 2
- Question: What is the optimal balance between prompt complexity and task-specific training data in supervised settings?
- Basis in paper: Found supervised LLMs not sensitive to prompt complexity but only tested two extreme prompt styles
- Why unresolved: Only compared concise vs. complex prompts without exploring intermediate levels or systematically varying prompt detail
- What evidence would resolve it: Systematic ablation study varying prompt detail levels while keeping training data constant

### Open Question 3
- Question: How does the effectiveness of CoT vary with task difficulty and model scale in supervised settings?
- Basis in paper: Observed CoT provided more substantial improvements on challenging BQ dataset but didn't explore different model sizes or wider range of task difficulties
- Why unresolved: Only used one model size (CLLM-7B) and two datasets of varying difficulty
- What evidence would resolve it: Testing CoT effectiveness across multiple model sizes and broader range of NLU tasks with varying difficulty levels

## Limitations

- Results primarily based on Chinese short text matching tasks, limiting generalizability to other languages and NLP tasks
- Comparison with BERT and few-shot GPT-4 is not comprehensive, lacking wider range of fine-tuning baselines or state-of-the-art models
- Mechanisms proposed to explain phenomena lack direct evidence from underlying corpus or model internals

## Confidence

- **High Confidence**: Generative fine-tuning outperforms discriminative approaches, especially on limited data, well-supported by experimental results
- **Medium Confidence**: Supervised LLMs less sensitive to prompt complexity, supported by experimental findings but lacks direct corpus/model evidence
- **Medium Confidence**: CoT reasoning improves performance on challenging tasks, supported by results but lacks detailed analysis of decision-making process

## Next Checks

1. **Generalization to Other Languages and Tasks**: Conduct experiments on short text matching tasks in other languages and different NLP tasks to assess generalizability of findings

2. **Analysis of Model Internals**: Perform analysis of attention patterns, hidden states, and internal representations during generative and discriminative fine-tuning to understand how models process and learn from training data

3. **Ablation Studies on CoT Reasoning**: Conduct ablation studies to identify critical components of CoT reasoning process by systematically removing or modifying different aspects and evaluating impact on model performance