---
ver: rpa2
title: 'PromptOptMe: Error-Aware Prompt Compression for LLM-based MT Evaluation Metrics'
arxiv_id: '2412.16120'
source_url: https://arxiv.org/abs/2412.16120
tags:
- prompt
- evaluation
- translation
- source
- compression
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper proposes PromptOptMe, a method to reduce token usage
  in large language model (LLM)-based machine translation evaluation metrics by compressing
  prompts using a smaller fine-tuned model. The approach involves two stages: supervised
  fine-tuning to learn prompt compression, followed by preference optimization to
  refine outputs based on evaluation quality.'
---

# PromptOptMe: Error-Aware Prompt Compression for LLM-based MT Evaluation Metrics

## Quick Facts
- arXiv ID: 2412.16120
- Source URL: https://arxiv.org/abs/2412.16120
- Reference count: 20
- Reduces token usage by 2.37× in LLM-based MT evaluation while maintaining or improving quality

## Executive Summary
This paper introduces PromptOptMe, a two-stage fine-tuning approach for compressing prompts used in large language model-based machine translation evaluation metrics. The method combines supervised fine-tuning with preference optimization to create compact prompts that preserve essential evaluation information, particularly error spans. Applied to the GEMBA-MMQ metric, the approach achieves substantial token reduction (2.37×) while maintaining or improving evaluation quality across multiple language pairs, making advanced LLM-based evaluation metrics more cost-effective and accessible.

## Method Summary
PromptOptMe employs a two-stage fine-tuning process using LLaMA 3.2 models (1B or 3B parameters). First, supervised fine-tuning adapts the model to the compression task using MQM-annotated data from WMT Metrics shared tasks. Second, preference optimization refines the compression based on evaluation scores obtained from GPT-4o, using an ORPO algorithm that optimizes for pairwise accuracy and Kendall's tau correlations. The approach focuses on preserving error spans while removing redundant tokens, achieving significant compression rates without sacrificing evaluation quality.

## Key Results
- Achieves 2.37× token reduction (from 19M to 8.3M tokens) using PROMPT OPTME-3B with GPT-4o lite prompt template
- Maintains or improves evaluation quality across multiple language pairs and metrics
- 3B parameter model outperforms 1B parameter version in both compression rate and quality metrics
- Outperforms LLMLingua-2 baseline with significantly less quality degradation

## Why This Works (Mechanism)

### Mechanism 1
Two-stage fine-tuning (supervised + preference optimization) enables targeted prompt compression while preserving evaluation quality. First stage trains model to compress text while retaining error spans; second stage refines compression based on evaluation score preferences from GPT-4o.

### Mechanism 2
Prompt compression reduces computational costs without degrading evaluation accuracy. Compressed prompts use fewer tokens, reducing GPT-4o API costs while maintaining or improving pairwise accuracy and Kendall τ correlations.

### Mechanism 3
Model size matters for compression effectiveness - larger models achieve better compression rates and quality. 3B parameter LLaMA model outperforms 1B parameter version in both compression rate (2.37× vs 2.15×) and evaluation quality metrics.

## Foundational Learning

- Concept: Prompt engineering for LLM-based evaluation
  - Why needed here: Understanding how prompts structure evaluation tasks is crucial for effective compression
  - Quick check question: What are the key components of the GEMBA-MQM prompt that must be preserved for accurate evaluation?

- Concept: Supervised fine-tuning for task adaptation
  - Why needed here: First stage requires adapting general language model to specific compression task
  - Quick check question: How does the supervised fine-tuning stage teach the model to identify and preserve error spans?

- Concept: Preference optimization (ORPO)
  - Why needed here: Second stage refines compression based on actual evaluation performance
  - Quick check question: What is the role of the odds ratio in the ORPO algorithm and how does it guide model training?

## Architecture Onboarding

- Component map: Source text → Supervised fine-tuning → Preference optimization → Compressed output → Evaluation
- Critical path: Source text → Supervised fine-tuning → Preference optimization → Compressed output → Evaluation
- Design tradeoffs:
  - Model size vs compression efficiency: Larger models achieve better compression but require more resources
  - Token reduction vs quality preservation: Need to balance efficiency gains with maintaining evaluation accuracy
  - Task-specific vs general compression: Our approach is tailored to MT evaluation vs more general-purpose methods
- Failure signatures:
  - Catastrophic quality drop (near-zero Kendall τ correlations)
  - Inconsistent compression across similar inputs
  - Failure to preserve critical error spans
  - Preference optimization doesn't improve over supervised fine-tuning alone
- First 3 experiments:
  1. Run baseline evaluation with uncompressed GEMBA-MQM prompt to establish reference metrics
  2. Apply PROMPT OPTME-3B to a small test set and compare compression rates vs quality metrics
  3. Test different compression rates (30%, 50%, 70%) to find optimal balance between efficiency and quality

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does PromptOptMe's performance scale with increasingly complex translation tasks, such as translating long-form documents or specialized technical content?
- Basis in paper: [inferred] The paper mentions that future work could explore adapting PromptOptMe to other NLG evaluation tasks
- Why unresolved: Current study focuses on machine translation evaluation using relatively short source and target texts
- What evidence would resolve it: Experiments with longer documents, technical texts, and low-resource language pairs

### Open Question 2
- Question: What is the impact of PromptOptMe's compression on the ability to detect nuanced translation errors, such as subtle differences in meaning or tone?
- Basis in paper: [explicit] Acknowledges possibility that some fine-grained details may not be fully captured in compressed prompts
- Why unresolved: Current evaluation focuses on overall quality metrics rather than specific error type detection
- What evidence would resolve it: Detailed error analysis comparing original vs compressed prompt error detection capabilities

### Open Question 3
- Question: How does the computational cost of running PromptOptMe compare to the cost savings achieved through prompt compression in large-scale evaluation scenarios?
- Basis in paper: [explicit] Mentions practitioners should consider inference costs for the compression model itself
- Why unresolved: Paper demonstrates token reduction but doesn't provide comprehensive cost analysis including training and inference costs
- What evidence would resolve it: Cost analysis comparing total computational cost (training + inference) versus baseline approach across different scales

## Limitations

- Evaluation relies heavily on GPT-4o as reference standard, limiting reproducibility without access to same model
- Focus on GEMBA-MQM metric and translation evaluation raises questions about generalizability to other tasks
- While 2.37× compression is impressive, absolute token reduction may still be substantial for production deployment

## Confidence

**High confidence:** Two-stage fine-tuning approach effectively achieves prompt compression while maintaining evaluation quality

**Medium confidence:** Superiority of larger models (3B vs 1B) for compression effectiveness

**Low confidence:** Claim that prompt compression for machine translation evaluation is still in its early stages

## Next Checks

1. Apply PromptOptMe to alternative LLM-based MT evaluation metrics (e.g., COMET, BLEURT-style approaches) to verify generalizability beyond GEMBA-MQM

2. Calculate actual cost savings for production deployment using current GPT-4o pricing, accounting for both reduced input tokens and potential changes in evaluation accuracy

3. Systematically test the model at various compression thresholds (20%, 40%, 60%, 80%) to identify optimal balance point where token reduction maximizes without degrading evaluation quality beyond acceptable thresholds