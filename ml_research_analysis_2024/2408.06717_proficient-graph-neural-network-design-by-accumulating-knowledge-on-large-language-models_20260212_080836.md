---
ver: rpa2
title: Proficient Graph Neural Network Design by Accumulating Knowledge on Large Language
  Models
arxiv_id: '2408.06717'
source_url: https://arxiv.org/abs/2408.06717
tags:
- graph
- knowledge
- uni00000055
- arxiv
- uni00000048
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: DesiGNN addresses the challenge of LLM-based Graph Neural Network
  design, which often struggles with both inherent knowledge gaps and external noise
  from descriptive inputs. The core method involves systematically converting past
  model design experiences into structured, fine-grained knowledge priors, aligned
  with empirical property filtering and adaptive elicitation of literature insights
  via LLMs.
---

# Proficient Graph Neural Network Design by Accumulating Knowledge on Large Language Models

## Quick Facts
- arXiv ID: 2408.06717
- Source URL: https://arxiv.org/abs/2408.06717
- Authors: Jialiang Wang; Hanmo Liu; Shimin Di; Zhili Wang; Jiachuan Wang; Lei Chen; Xiaofang Zhou
- Reference count: 40
- Primary result: Delivers top-5.77% initial GNN proposals within seconds without training on unseen data

## Executive Summary
DesiGNN addresses the challenge of designing effective Graph Neural Networks (GNNs) for unseen graph datasets by leveraging accumulated knowledge from benchmarks and literature through Large Language Models (LLMs). The framework systematically converts past model design experiences into structured knowledge priors, aligned with empirical property filtering and adaptive elicitation of literature insights via LLMs. This knowledge-centered approach enables immediate, data-aware GNN recommendations without requiring training on unseen data, delivering initial model proposals ranking in the top 5.77% of all possible architectures within seconds.

## Method Summary
DesiGNN constructs an empirically validated meta-level understanding of graph-GNN-performance from benchmarks and adaptively aligns key properties with graph-specific insights elicited from the graph learning literature. The framework operates in three stages: first, it computes graph properties and task similarity to identify relevant benchmark datasets; second, it constructs a knowledge pool from top-performing models and elicits adaptive property weights via LLMs; third, it samples initial proposals from the knowledge pool and refines them through exploration (crossover) and exploitation (mutation) mechanisms. The system uses a Bayesian-inspired formulation to approximate posterior beliefs over architectures without directly testing on unseen data.

## Key Results
- DesiGNN delivers top-5.77% initial model proposals for unseen datasets within seconds
- Achieves consistently superior performance with minimal search costs compared to baselines
- Demonstrates effectiveness across 3 out-of-distribution datasets and 8 benchmarks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Empirical filtering of graph properties reduces external noise from descriptive inputs
- Mechanism: By correlating filtered graph properties with empirical performance rankings across benchmark datasets, the system identifies properties that reliably predict GNN performance. This empirical foundation reduces reliance on user-provided semantic descriptions that often mislead LLM-based methods
- Core assumption: Certain graph properties have consistent predictive power for GNN performance across diverse datasets
- Evidence anchors: [abstract] "To account for the inherent variability and external noise, DesiGNN aligns empirical property filtering from extensive benchmarks with adaptive elicitation of literature insights via LLMs"

### Mechanism 2
- Claim: LLM alignment with literature insights provides adaptive importance weights for graph properties
- Mechanism: LLMs are used to adaptively elicit importance weights for each graph property based on the unseen dataset's characteristics and literature insights. This creates property-specific weights that reflect graph-aware meta-knowledge documented in research literature
- Core assumption: LLMs pre-trained on graph learning literature can effectively identify which properties matter most for specific graph types
- Evidence anchors: [abstract] "DesiGNN aligns empirical property filtering from extensive benchmarks with adaptive elicitation of literature insights via LLMs"

### Mechanism 3
- Claim: Knowledge pool construction enables immediate, data-aware model proposals without training
- Mechanism: By aggregating top-performing models from benchmark datasets similar to the unseen dataset (based on computed task similarity), DesiGNN creates a knowledge pool that approximates the posterior distribution of effective architectures. This allows immediate sampling of initial proposals without requiring training on the unseen data
- Core assumption: Models that perform well on similar benchmark graphs will also perform well on the unseen graph
- Evidence anchors: [abstract] "By constructing a solid meta-knowledge between unseen graph understanding and known effective architecture patterns, DesiGNN can deliver top-5.77% initial model proposals for unseen datasets within seconds"

## Foundational Learning

- Concept: Bayesian inference in neural architecture search
  - Why needed here: The system uses Bayesian-inspired formulation to approximate posterior beliefs over architectures without directly testing on unseen data
  - Quick check question: How does the system estimate P(Œ∏·µ¢|F(G·µ§)) without actually evaluating Œ∏·µ¢ on G·µ§?

- Concept: Graph topology and GNN performance relationships
  - Why needed here: Understanding how different graph properties (homophily, degree distribution, etc.) influence which GNN architectures perform best is central to the empirical filtering mechanism
  - Quick check question: Why might a GCN perform poorly on heterophilic graphs compared to homophilic ones?

- Concept: Large language model prompting and in-context learning
  - Why needed here: The system relies on LLMs to align empirical knowledge with literature insights and to sample initial architecture proposals
  - Quick check question: What type of prompt would help an LLM identify the most relevant graph properties for a citation network dataset?

## Architecture Onboarding

- Component map: Graph Understanding -> Knowledge Retrieval -> Model Suggestion and Refinement -> Best Model Output
- Critical path: Graph Understanding ‚Üí Knowledge Retrieval ‚Üí Initial Model Suggestion ‚Üí Model Refinement ‚Üí Best Model Output
- Design tradeoffs:
  - More graph properties in filtering increases potential accuracy but also computational cost and noise
  - Larger knowledge pools provide more diverse options but increase processing time and may include irrelevant models
  - Deeper refinement exploration improves solution quality but increases computational cost
- Failure signatures:
  - Initial proposals consistently poor: Likely issue with task similarity computation or property filtering
  - Refinement fails to improve: Likely issue with crossover or mutation mechanisms
  - System runs slowly: Likely issue with graph property computation or knowledge pool size
- First 3 experiments:
  1. Run DesiGNN on Cora dataset with N_f=5 properties and N_s=3 benchmark sources, measure initial proposal accuracy
  2. Test task similarity computation by comparing PubMed against CS, Physics, and Citeseer benchmark datasets
  3. Validate property confidence filtering by computing I(G·µ¢, g‚Çñ) for all properties across the 8 benchmark datasets

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How robust is the task similarity metric S to variations in the confidence threshold ùõø when applied to graphs with vastly different properties (e.g., homophily vs heterophily)?
- Basis in paper: [explicit] The paper discusses the hit rate ùõø* as a measure of task similarity effectiveness, but does not deeply analyze robustness to threshold changes across diverse graph types
- Why unresolved: The empirical validation focuses on overall hit rates rather than sensitivity analysis across graph property extremes
- What evidence would resolve it: Systematic experiments varying ùõø and testing on heterophily-rich graphs like Actor and Flickr, showing how performance changes

### Open Question 2
- Question: Can DesiGNN‚Äôs meta-knowledge construction generalize effectively to emerging graph domains (e.g., spatiotemporal or multimodal graphs) not represented in NAS-Bench-Graph?
- Basis in paper: [inferred] The knowledge pool is built from NAS-Bench-Graph, which may limit generalization to newer or more complex graph types
- Why unresolved: The experiments only test on datasets within or closely related to NAS-Bench-Graph, with no evaluation on truly novel graph domains
- What evidence would resolve it: Testing DesiGNN on out-of-distribution graph datasets like spatiotemporal traffic networks or multimodal molecular graphs

### Open Question 3
- Question: How does the choice of LLM (e.g., GPT-4 vs Llama2) affect the long-term accumulation and evolution of the knowledge pool in DesiGNN?
- Basis in paper: [explicit] The case study shows Llama2 slightly underperforms GPT-4, but does not explore long-term impacts on knowledge quality
- Why unresolved: The paper only evaluates short-term performance differences without tracking how LLM choice influences the quality of accumulated knowledge over time
- What evidence would resolve it: Longitudinal experiments comparing knowledge pool evolution and model performance when using different LLMs over multiple iterations

## Limitations
- Reliance on quality and representativeness of benchmark datasets used for constructing empirical knowledge base
- Dependency on LLMs having sufficient graph-specific training data for effective property weighting
- Assumption that models performing well on similar benchmark graphs will transfer effectively to unseen datasets may break down for unique graph structures

## Confidence
- **High confidence**: The empirical filtering mechanism for reducing noise from descriptive inputs
- **Medium confidence**: The LLM alignment for adaptive property weighting
- **Medium confidence**: The immediate model proposal capability without training

## Next Checks
1. **Transferability assessment**: Systematically evaluate DesiGNN's performance on graphs with structural characteristics not well-represented in the 8 benchmark datasets to test the limits of the knowledge pool transfer assumption
2. **LLM dependency analysis**: Compare DesiGNN's performance using different LLMs (with varying amounts of graph-specific training data) to quantify the impact of LLM quality on adaptive property weighting accuracy
3. **Property correlation stability**: Measure how consistently graph properties correlate with GNN performance across different dataset families to validate the empirical filtering mechanism's robustness to dataset variability