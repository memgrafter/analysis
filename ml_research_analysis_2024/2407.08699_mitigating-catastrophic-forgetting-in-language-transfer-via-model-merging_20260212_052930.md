---
ver: rpa2
title: Mitigating Catastrophic Forgetting in Language Transfer via Model Merging
arxiv_id: '2407.08699'
source_url: https://arxiv.org/abs/2407.08699
tags:
- data
- language
- training
- bulgarian
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proposes Branch-and-Merge (BAM), a method for adapting
  large language models to new languages while mitigating catastrophic forgetting.
  BAM iteratively trains models on subsets of data in parallel and merges them, reducing
  weight change magnitude and improving task vector quality.
---

# Mitigating Catastrophic Forgetting in Language Transfer via Language Transfer via Model Merging

## Quick Facts
- **arXiv ID**: 2407.08699
- **Source URL**: https://arxiv.org/abs/2407.08699
- **Reference count**: 40
- **Primary result**: BAM improves performance in Bulgarian by 10.8% and in English by 1.3% compared to baseline models, outperforming standard training and instruction finetuning

## Executive Summary
This paper introduces Branch-and-Merge (BAM), a method for adapting large language models to new languages while mitigating catastrophic forgetting. BAM works by iteratively training models on subsets of data in parallel and merging them, which reduces weight change magnitude and improves task vector quality. The authors demonstrate that BAM, combined with an effective approximate experience replay data mix, significantly reduces forgetting compared to standard training methods across both continued pretraining and instruction finetuning scenarios.

## Method Summary
BAM operates by partitioning training data into subsets and training separate model branches on each subset in parallel. After training, these branches are merged using a weighted combination of their parameters, where weights are determined by performance on a validation set. This approach reduces the magnitude of weight changes compared to standard fine-tuning, thereby preserving knowledge from previous languages. The method is combined with an approximate experience replay strategy that mixes in relevant data from previously learned languages to further combat forgetting.

## Key Results
- BAM improves performance on Bulgarian by 10.8% and on English by 1.3% compared to baseline models
- Outperforms standard training and instruction finetuning methods across different model architectures
- Effective for both continued pretraining and instruction finetuning scenarios
- Demonstrates reduced catastrophic forgetting compared to traditional adaptation methods

## Why This Works (Mechanism)
BAM reduces catastrophic forgetting by limiting the magnitude of weight changes during adaptation. By training on data subsets in parallel and merging models, the approach creates a more balanced update that preserves knowledge from previously learned languages. The approximate experience replay component further reinforces retention by mixing in relevant data from earlier training stages. This combination addresses the core issue where standard fine-tuning makes large, disruptive changes to model parameters when adapting to new languages.

## Foundational Learning
- **Catastrophic forgetting**: When neural networks overwrite previously learned knowledge when trained on new tasks - needed to understand the problem BAM addresses; quick check: observe performance drop on original task after fine-tuning
- **Model merging**: Combining parameters from separately trained models - needed to understand BAM's core mechanism; quick check: verify merged model performance exceeds individual branches
- **Experience replay**: Reusing past training examples during current training - needed to understand the data mixing strategy; quick check: measure forgetting reduction with and without replay
- **Weight space geometry**: How parameter changes affect model behavior - needed to understand why BAM's approach works; quick check: analyze weight change magnitudes between methods

## Architecture Onboarding

**Component map**: Data partitioner -> Parallel trainers -> Validator -> Merger -> Final model

**Critical path**: Data partitioning → Parallel training → Performance validation → Weighted parameter merging → Evaluation

**Design tradeoffs**: 
- Branch-and-merge adds computational overhead but reduces forgetting
- Approximate replay balances memory efficiency with retention
- Parallel training requires more resources but enables better knowledge preservation

**Failure signatures**:
- Poor performance if data subsets are too small or unrepresentative
- Merging weights may not properly balance contributions if validation data is limited
- Experience replay may be ineffective if past data is too dissimilar to current task

**First experiments**:
1. Run standard fine-tuning on new language and measure baseline forgetting
2. Apply BAM with varying numbers of branches to find optimal configuration
3. Test BAM with and without experience replay to isolate their respective contributions

## Open Questions the Paper Calls Out
None

## Limitations
- Evaluation focuses primarily on two languages (English and Bulgarian), limiting generalizability across diverse language families
- Computational overhead of branch-and-merge approach is not fully characterized for very large models
- Analysis of theoretical advantages in reducing weight change magnitude and improving task vector quality could be more thorough
- Method's robustness to highly imbalanced datasets and extreme domain shifts remains unexplored

## Confidence

| Claim | Confidence Level |
|-------|------------------|
| BAM's effectiveness for specific languages and tasks tested | High |
| BAM's generalizability across diverse languages and domains | Medium |
| BAM's computational efficiency and scalability to extremely large models | Low |

## Next Checks
1. Evaluate BAM across a broader range of languages representing different families and typological features to assess cross-linguistic generalization
2. Conduct a comprehensive ablation study isolating the contributions of the data mix strategy versus the merging mechanism
3. Perform scaling experiments to quantify computational overhead and memory requirements across different model sizes and dataset volumes