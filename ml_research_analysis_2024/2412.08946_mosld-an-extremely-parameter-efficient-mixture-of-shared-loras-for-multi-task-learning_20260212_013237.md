---
ver: rpa2
title: 'MoSLD: An Extremely Parameter-Efficient Mixture-of-Shared LoRAs for Multi-Task
  Learning'
arxiv_id: '2412.08946'
source_url: https://arxiv.org/abs/2412.08946
tags:
- lora
- mixture
- matrix
- single
- mosld
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of applying LoRA for multi-task
  learning in large language models, where heterogeneous and imbalanced training data
  leads to performance degradation. The authors propose MoSLD, a mixture-of-shared-LoRA
  model that shares the upper projection matrix across different experts while retaining
  task-specific lower projection matrices.
---

# MoSLD: An Extremely Parameter-Efficient Mixture-of-Shared LoRAs for Multi-Task Learning

## Quick Facts
- arXiv ID: 2412.08946
- Source URL: https://arxiv.org/abs/2412.08946
- Reference count: 27
- Primary result: MoSLD achieves 71.56% average accuracy on six commonsense reasoning datasets, outperforming best baseline (MoLA) by 1.56%

## Executive Summary
MoSLD addresses the challenge of applying LoRA for multi-task learning in large language models, where heterogeneous and imbalanced training data leads to performance degradation. The authors propose a mixture-of-shared-LoRA model that shares the upper projection matrix across different experts while retaining task-specific lower projection matrices. A dropout strategy is applied to the shared matrix to balance updates and mitigate overfitting. Experiments on six commonsense reasoning datasets demonstrate that MoSLD achieves significant improvements over existing methods while maintaining parameter efficiency through the sharing mechanism.

## Method Summary
MoSLD is a parameter-efficient multi-task learning framework that builds upon LoRA by introducing a mixture-of-shared approach. The key innovation is sharing the upper projection matrix across all task-specific experts while keeping the lower projection matrices task-specific. This sharing mechanism reduces parameter redundancy while maintaining task adaptability. To prevent the shared matrix from dominating updates and causing overfitting, a dropout strategy is applied during training. The model is evaluated on six commonsense reasoning datasets, demonstrating superior performance compared to existing baselines while maintaining computational efficiency.

## Key Results
- MoSLD achieves 71.56% average accuracy on six commonsense reasoning datasets
- Outperforms best baseline (MoLA) by 1.56% (70.00%)
- Maintains parameter efficiency through the sharing mechanism
- Demonstrates effectiveness of dropout strategy in balancing shared matrix updates

## Why This Works (Mechanism)
The core mechanism of MoSLD relies on sharing the upper projection matrix across task-specific experts while maintaining separate lower projection matrices. This architectural choice reduces parameter redundancy and enables knowledge transfer across tasks. The dropout strategy applied to the shared matrix prevents it from dominating updates during training, which could otherwise lead to overfitting or catastrophic forgetting. By balancing the learning rates between the shared and task-specific components, MoSLD achieves better generalization across heterogeneous tasks.

## Foundational Learning

**LoRA (Low-Rank Adaptation)**
Why needed: Enables efficient fine-tuning of large models by learning low-rank matrices instead of full weight updates
Quick check: Verify that LoRA matrices W0 ≈ I + BA where B ∈ R^{d×r}, A ∈ R^{r×d}

**Parameter-Efficient Fine-Tuning**
Why needed: Reduces computational cost and memory requirements for adapting large models to new tasks
Quick check: Confirm parameter count reduction compared to full fine-tuning (typically >90% savings)

**Mixture-of-Experts (MoE)**
Why needed: Distributes computation across specialized components for improved task handling
Quick check: Validate routing mechanism selects appropriate expert(s) based on input characteristics

## Architecture Onboarding

**Component Map**
Input -> LoRA Upper Matrix (shared) -> Dropout -> LoRA Lower Matrix (task-specific) -> Output

**Critical Path**
1. Input token embeddings enter LoRA adaptation layer
2. Shared upper projection matrix transforms input representation
3. Dropout applied to upper matrix outputs
4. Task-specific lower projection matrix applies final transformation
5. Outputs combined with frozen base model weights

**Design Tradeoffs**
- Sharing upper matrix reduces parameters but may limit task-specific expressivity
- Dropout on shared matrix prevents overfitting but may slow convergence
- Separate lower matrices maintain task adaptability but increase parameter count

**Failure Signatures**
- Performance degradation when tasks are highly dissimilar (limited knowledge transfer)
- Overfitting on small datasets despite dropout regularization
- Suboptimal routing in mixture setting leading to incorrect expert selection

**First Experiments**
1. Ablation study: Remove dropout to measure its impact on shared matrix dominance
2. Cross-dataset evaluation: Test model on tasks outside the original six datasets
3. Parameter efficiency analysis: Compare FLOPs and memory usage against baselines

## Open Questions the Paper Calls Out
None

## Limitations
- Evaluation scope remains narrow, focusing primarily on commonsense reasoning without exploring broader NLP tasks or multimodal scenarios
- Performance improvements, while statistically significant, are relatively modest (1.56% over best baseline)
- Lacks ablation studies examining dropout strategy's specific contribution versus sharing mechanism itself

## Confidence
- Technical approach validity: High
- Empirical performance claims: Medium
- Parameter efficiency analysis: Medium
- Theoretical contributions: Low

## Next Checks
1. Conduct ablation studies isolating the effects of matrix sharing versus dropout regularization on overall performance
2. Test MoSLD across diverse NLP tasks (sentiment analysis, QA, summarization) to assess generalizability
3. Perform runtime efficiency benchmarks comparing MoSLD with baseline methods during inference