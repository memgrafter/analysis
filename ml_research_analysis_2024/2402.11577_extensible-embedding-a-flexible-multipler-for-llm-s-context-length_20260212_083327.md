---
ver: rpa2
title: 'Extensible Embedding: A Flexible Multipler For LLM''s Context Length'
arxiv_id: '2402.11577'
source_url: https://arxiv.org/abs/2402.11577
tags:
- context
- extensible
- embedding
- embeddings
- which
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces extensible embedding, a novel approach to
  extend the context length of large language models (LLMs) by using compact representations
  that can encode information from an extensible scope of context. The core idea is
  to replace traditional token embeddings with embeddings that can represent multiple
  tokens or even sentences, thus increasing the information density and allowing the
  LLM to access a larger context within its original window.
---

# Extensible Embedding: A Flexible Multipler For LLM's Context Length

## Quick Facts
- arXiv ID: 2402.11577
- Source URL: https://arxiv.org/abs/2402.11577
- Reference count: 27
- Primary result: Extends LLaMA-2-7B context length up to 100K tokens with superior performance

## Executive Summary
Extensible embedding introduces a novel approach to extend the context length of large language models by using compact representations that encode information from an extensible scope of context. The method replaces traditional token embeddings with higher-density embeddings that can represent multiple tokens or sentences, allowing the LLM to access a larger context within its original window. A lightweight model transforms input chunks into output embeddings, which are then down-scaled to generate compact representations. The approach demonstrates effective context extension up to 100K tokens with superior language modeling and understanding performance compared to existing methods.

## Method Summary
The extensible embedding method works by replacing single-token embeddings with compact representations that encode multiple tokens. A lightweight model called the extensible embedder (using the first 8 layers of LLaMA-2-7B) transforms input chunks into output embeddings, which are then down-scaled through sampling (e.g., every 32nd embedding) to produce compact representations. The model is trained using a two-stream auto-regression task where embeddings are first generated for the entire context and then cached, followed by chunk-by-chunk token prediction. This approach enables direct integration with existing LLMs without affecting their short-context performance and shows strong compatibility with fine-tuned derivatives.

## Key Results
- Successfully extends LLaMA-2-7B context length to 100K tokens and beyond
- Achieves superior language modeling and understanding performance compared to existing methods
- Demonstrates strong compatibility with fine-tuned derivatives without requiring further adaptation

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Extensible embedding increases information density by replacing single-token embeddings with representations that encode multiple tokens or sentences.
- Mechanism: A lightweight model (extensible embedder) transforms input chunks into output embeddings, which are then down-scaled (e.g., by sampling every 32nd embedding) to produce compact representations. These compact embeddings allow the LLM to access more context within its original window.
- Core assumption: The LLM can effectively process these higher-density embeddings without loss of semantic coherence.
- Evidence anchors:
  - [abstract] "represents the information for an extensible scope of context instead of a single token"
  - [section] "compact input units of higher information density, the LLM can access to a vast scope of context even with a small context window"
- Break condition: If the down-sampling rate is too high, semantic information may be lost, degrading performance.

### Mechanism 2
- Claim: Two-stream auto-regression enables efficient training by producing prediction losses for all tokens in long sequences.
- Mechanism: In the first pass, extensible embeddings are generated and cached for the entire context. In the second pass, tokens are predicted chunk-by-chunk using the cached embeddings from preceding chunks and normal embeddings within the current chunk.
- Core assumption: The cached extensible embeddings sufficiently capture the preceding context for accurate token prediction.
- Evidence anchors:
  - [section] "comprehensive training losses can be derived from all tokens within each training sample"
  - [section] "exceptional sample efficiency, which enables the model to be effectively trained with a small amount of data"
- Break condition: If the chunk size is too small, the model may not have enough context to make accurate predictions.

### Mechanism 3
- Claim: Extensible embedding maintains compatibility with fine-tuned derivatives of the downstream LLM without further adaptation.
- Mechanism: Training is performed with the downstream LLM's parameters fixed, allowing the extensible embeddings to function as a plug-in module. The embeddings generalize well to other models fine-tuned from the same base model.
- Core assumption: The fine-tuned derivatives share sufficient architectural and semantic similarity with the base model.
- Evidence anchors:
  - [abstract] "strong compatibility with the existing LLMs, where the extensible embedding can be seamlessly introduced as a plug-in component"
  - [section] "well-trained extensible embeddings for one LLM can be effectively applied to other fine-tuned derivatives of its downstream LLM without any further adaptation"
- Break condition: If the fine-tuned model significantly diverges from the base model's architecture or training data distribution, compatibility may break.

## Foundational Learning

- Concept: Information density in embeddings
  - Why needed here: Understanding how replacing single-token embeddings with multi-token embeddings increases the amount of context accessible within a fixed window.
  - Quick check question: What is the difference between information density in traditional token embeddings and extensible embeddings?

- Concept: Two-stream training process
  - Why needed here: Grasping how generating embeddings in one pass and predicting tokens in another leads to efficient training and better context utilization.
  - Quick check question: How does two-stream auto-regression differ from standard auto-regressive training?

- Concept: Down-sampling strategies
  - Why needed here: Recognizing how different methods of reducing the number of embeddings (e.g., strided sampling) affect information retention and model performance.
  - Quick check question: Why might strided sampling be more effective than random sampling for down-scaling embeddings?

## Architecture Onboarding

- Component map: Input chunks -> Extensible embedder (first 8 layers of LLaMA-2-7B) -> Output embeddings -> Down-scaling function -> Compact extensible embeddings -> Downstream LLM

- Critical path:
  1. Input is partitioned into chunks
  2. Each chunk is processed by the extensible embedder to produce output embeddings
  3. Output embeddings are down-scaled to generate extensible embeddings
  4. The LLM predicts new tokens based on extensible embeddings from preceding chunks and normal embeddings within the current chunk

- Design tradeoffs:
  - Embedder size vs. computational cost: Larger embedders may produce better embeddings but increase computation
  - Down-sampling rate (k) vs. information retention: Higher k allows longer context but may lose semantic details
  - Chunk size vs. context coherence: Smaller chunks may lose contextual information spanning multiple chunks

- Failure signatures:
  - High perplexity on long-context tasks: Indicates loss of semantic information during down-scaling or embedding generation
  - Incompatibility with fine-tuned models: Suggests the extensible embeddings do not generalize well
  - Inefficient training: May result from suboptimal two-stream auto-regression setup

- First 3 experiments:
  1. Test different down-sampling rates (k) on a fixed dataset to find the optimal balance between context length and information retention
  2. Compare the performance of extensible embedding with and without two-stream auto-regression to validate its efficiency
  3. Apply the extensible embeddings trained on LLaMA-2-7B to a fine-tuned derivative model to verify compatibility

## Open Questions the Paper Calls Out
None

## Limitations
- The down-sampling mechanism may lead to information loss when the sampling rate is too high, potentially degrading semantic coherence
- The two-stream auto-regression approach's generalizability across different model architectures or training regimes remains uncertain
- The paper focuses on LLaMA-2-7B, leaving uncertainty about performance on larger models or those with different architectural designs

## Confidence

**High Confidence:**
- The core mechanism of using compact representations to increase information density within a fixed context window is well-supported by the experimental results

**Medium Confidence:**
- The claim that extensible embedding maintains compatibility with fine-tuned derivatives is supported by the experiments, but generalizability to diverse fine-tuned models remains uncertain

**Low Confidence:**
- The assertion that two-stream auto-regression significantly improves sample efficiency lacks comparative analysis with alternative training methods

## Next Checks

1. **Down-sampling Rate Sensitivity Analysis**: Conduct experiments with varying down-sampling rates (e.g., k=16, 32, 64) on a fixed dataset to quantify the trade-off between context length and information retention

2. **Cross-Architecture Compatibility Testing**: Evaluate the performance of extensible embeddings on LLMs with diverse architectures (e.g., GPT, BERT) and training regimes to assess the robustness of compatibility claims

3. **Real-World Deployment Simulation**: Test extensible embedding in scenarios with dynamic input distributions and task-specific requirements (e.g., multi-turn dialogue, long-form document summarization) to identify potential failure modes and scalability challenges in practical applications