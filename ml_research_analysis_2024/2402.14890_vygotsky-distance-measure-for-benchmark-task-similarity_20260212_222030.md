---
ver: rpa2
title: 'Vygotsky Distance: Measure for Benchmark Task Similarity'
arxiv_id: '2402.14890'
source_url: https://arxiv.org/abs/2402.14890
tags:
- tasks
- benchmark
- benchmarks
- which
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Vygotsky distance, a novel metric for measuring
  similarity between benchmark tasks in NLP based on relative model performance. The
  core idea is to represent benchmarks as weighted graphs, where edge weights reflect
  the number of inversions in model rankings between tasks.
---

# Vygotsky Distance: Measure for Benchmark Task Similarity

## Quick Facts
- arXiv ID: 2402.14890
- Source URL: https://arxiv.org/abs/2402.14890
- Authors: Maxim K. Surkov; Ivan P. Yamshchikov
- Reference count: 0
- One-line primary result: Novel metric measures NLP benchmark task similarity through relative model performance inversions

## Executive Summary
This paper introduces Vygotsky distance, a novel metric for measuring similarity between benchmark tasks in NLP based on relative model performance. The core idea is to represent benchmarks as weighted graphs, where edge weights reflect the number of inversions in model rankings between tasks. Experiments show that 40% of tasks in popular NLP benchmarks (GLUE, SuperGLUE, CLUE, RussianSuperGLUE) are redundant and can be removed without significant loss of information. A compression algorithm is proposed that predicts private leaderboard performance using only a subset of tasks, achieving over 80% accuracy in model comparisons. The method has implications for developing more efficient evaluation systems, guiding future benchmark design, and understanding task relationships in NLP.

## Method Summary
The method constructs weighted task graphs where edge weights are defined by the number of inversions between model rankings across tasks (Vygotsky distance). From these graphs, minimum weight spanning trees reveal task relationships and groupings. The compression algorithm selects subsets of tasks that preserve predictive power for model performance on remaining tasks. Classification and regression models (SVM, GP, MLP) predict model comparisons and score estimates respectively, enabling benchmark size reduction while maintaining evaluation quality.

## Key Results
- 40% of tasks in popular NLP benchmarks are redundant and removable
- Benchmark compression achieves over 80% accuracy in model comparisons
- MST analysis reveals task groupings and identifies exceptional tasks
- Optimal compression rate is around 40% for tested benchmarks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Task similarity can be measured by inversions in model ranking permutations
- Mechanism: Represent each task as a permutation of model rankings; similarity is quantified by counting how many pairs of models switch relative order between tasks (inversions)
- Core assumption: The relative performance of models across tasks captures the intrinsic similarity between those tasks
- Evidence anchors:
  - [abstract] "The core idea of this similarity measure is that it is based on relative performance of the 'students' on a given task, rather that on the properties of the task itself."
  - [section 2] "w(π, σ) = inv(π ◦ σ−1) where π and σ are two permutations of model rankings, and inv is the function that calculates the number of inversions"
  - [corpus] Weak - corpus contains papers about general similarity measures but not specifically inversion-based methods for NLP benchmarks
- Break condition: If model rankings are unstable or if a few dominant models skew rankings across all tasks, the inversion count may not reflect true task similarity

### Mechanism 2
- Claim: Benchmark compression is possible by selecting subsets of tasks that preserve predictive power
- Mechanism: Use Vygotsky distance to build a graph representation of the benchmark; select a small subset of tasks (public leaderboard) that allows accurate prediction of model performance on remaining tasks (private leaderboard) using classification or regression models
- Core assumption: Information about model performance on some tasks can be inferred from performance on other tasks within the same benchmark
- Evidence anchors:
  - [abstract] "Experiments on various benchmarks...demonstrate that a vast majority of NLP benchmarks could be at least 40% smaller"
  - [section 3.2] "one can find a public leaderboard with a minimal number of tasks allowing one to predict information about the private leaderboard with the desired accuracy"
  - [corpus] Weak - corpus does not contain papers about benchmark compression using graph-based similarity measures
- Break condition: If tasks are highly diverse with little overlap in model performance patterns, prediction accuracy will degrade significantly

### Mechanism 3
- Claim: Minimum Weight Spanning Tree (MST) reveals task groupings and exceptional tasks
- Mechanism: Convert benchmark tasks into complete graph with Vygotsky distances as edge weights; compute MST to visualize task relationships and identify clusters
- Core assumption: Tasks with small Vygotsky distances tend to be of similar types and should be grouped together in the MST
- Evidence anchors:
  - [section 2] "Let us consider the minimum weight spanning tree of the benchmark graph. This entity (MST) has a series of useful properties that help us to understand the structure of the benchmarks more clearly"
  - [section 2] "Notice that we can select groups of tasks in the benchmark which are close to each other (e.g., WNLI, MNLIM, MNLIMM, QNLI) and belong to the same problem type"
  - [corpus] Weak - corpus contains papers about MST applications in general but not specifically for NLP benchmark analysis
- Break condition: If task relationships are too complex for tree structure to capture, important groupings may be obscured

## Foundational Learning

- Concept: Permutation and inversion counting
  - Why needed here: Vygotsky distance is fundamentally defined as the number of inversions between model ranking permutations
  - Quick check question: Given two model rankings A=[M1,M2,M3] and B=[M2,M1,M3], what is the Vygotsky distance between them?

- Concept: Graph theory (weighted graphs, minimum spanning trees)
  - Why needed here: Benchmark representation as weighted graph and MST analysis are central to the methodology
  - Quick check question: What property of MST guarantees that for any two tasks, the path between them in the MST provides a lower bound on their Vygotsky distance?

- Concept: Machine learning (classification and regression for prediction)
  - Why needed here: The compression algorithm relies on predicting private leaderboard performance from public leaderboard results
  - Quick check question: What is the difference between the classification problem (models comparison) and regression problem (model's score estimation) in the compression framework?

## Architecture Onboarding

- Component map: Data collection module -> Vygotsky distance calculator -> Graph builder -> MST visualizer -> Compression engine -> Prediction models -> Evaluation metrics
- Critical path: Data collection → Vygotsky distance calculation → Graph construction → MST analysis → Benchmark compression → Prediction evaluation
- Design tradeoffs:
  - Computational cost vs. accuracy: Using all model pairs vs. sampling for inversion counting
  - Compression level vs. prediction accuracy: Higher compression reduces evaluation cost but may decrease prediction quality
  - Model complexity vs. interpretability: Simple models (SVM) vs. complex models (MLP) for prediction
- Failure signatures:
  - Inconsistent or noisy model performance data leading to unreliable distance calculations
  - Highly diverse benchmarks where few tasks share performance patterns
  - Overfitting in prediction models due to limited training data or too many features
- First 3 experiments:
  1. Compute Vygotsky distances between GLUE tasks and visualize as MST to verify task groupings
  2. Apply compression algorithm to GLUE with 40% target compression and evaluate prediction accuracy
  3. Test different prediction models (SVM, GP, MLP) on compressed benchmark to determine optimal choice

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can we rigorously classify NLP tasks into distinct types beyond human intuition?
- Basis in paper: [explicit] The paper notes that "one can either find the nearest group of already existing tasks and assume that the new one is of the same type, or if there is no such group, one might, indeed, claim that the task is of a novel type."
- Why unresolved: The current typology of NLP tasks relies on subjective judgment by dataset creators, and the paper suggests using Vygotsky distance for more rigorous classification but doesn't provide a definitive method.
- What evidence would resolve it: A systematic study using Vygotsky distance to categorize tasks and validate the resulting taxonomy against human judgments and task performance patterns.

### Open Question 2
- Question: What specific internal properties of NLP tasks cause them to appear different to models despite being classified as similar by humans?
- Basis in paper: [explicit] The paper observes that "some internal properties of the tasks make them 'look' different from the models' perspective. However, these differences are not evident to humans."
- Why unresolved: The paper identifies this phenomenon but doesn't investigate the specific linguistic or structural features that create these perceptual differences for models.
- What evidence would resolve it: Comparative analysis of task features (vocabulary, syntactic complexity, semantic requirements) that correlate with large Vygotsky distances between tasks classified as the same type by humans.

### Open Question 3
- Question: What is the relationship between Vygotsky distance and model generalization performance across tasks?
- Basis in paper: [inferred] The paper suggests that Vygotsky distance could be used to evaluate new benchmarks and "ensure that they significantly differ from the existing NLP tasks," implying a connection to generalization.
- Why unresolved: While the paper establishes Vygotsky distance as a measure of task similarity, it doesn't empirically investigate how task diversity (as measured by Vygotsky distance) relates to model generalization.
- What evidence would resolve it: Empirical studies measuring model performance degradation when trained on benchmarks with varying average Vygotsky distances between tasks.

## Limitations
- Reliance on stable model performance data with no sensitivity analysis for outliers
- Limited empirical validation beyond a few specific NLP benchmarks
- Weak corpus support for the core inversion-based mechanism

## Confidence
- Vygotsky distance metric validity: **Medium** - While the mathematical formulation is sound, empirical validation is limited to a few benchmarks without extensive ablation studies
- Benchmark compression effectiveness: **Medium** - The 40% compression claim is supported by experiments but relies on specific prediction models and thresholds
- MST-based task analysis: **Low** - The interpretation of MST structure as revealing task relationships is intuitive but lacks rigorous validation or comparison with alternative methods

## Next Checks
1. Conduct sensitivity analysis: Remove 1-2 top-performing models and recalculate Vygotsky distances to assess stability of task similarities
2. Test on diverse benchmarks: Apply the methodology to specialized NLP tasks (e.g., medical text processing, low-resource languages) to evaluate generalizability
3. Compare with alternative metrics: Implement and compare against established benchmark similarity measures (e.g., based on model embedding distances or task characteristic vectors) to validate Vygotsky distance effectiveness