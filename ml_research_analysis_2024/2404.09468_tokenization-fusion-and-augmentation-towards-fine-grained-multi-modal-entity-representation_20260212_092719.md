---
ver: rpa2
title: 'Tokenization, Fusion, and Augmentation: Towards Fine-grained Multi-modal Entity
  Representation'
arxiv_id: '2404.09468'
source_url: https://arxiv.org/abs/2404.09468
tags:
- multi-modal
- entity
- knowledge
- mmkgc
- information
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper tackles multi-modal knowledge graph completion (MMKGC)
  by addressing the coarse processing of multi-modal entity information in existing
  methods. It introduces a novel framework, MYGO, that tokenizes multi-modal entity
  data into fine-grained discrete tokens, fuses them through a hierarchical transformer-based
  architecture, and augments representations using fine-grained contrastive learning.
---

# Tokenization, Fusion, and Augmentation: Towards Fine-grained Multi-modal Entity Representation

## Quick Facts
- arXiv ID: 2404.09468
- Source URL: https://arxiv.org/abs/2404.09468
- Authors: Yichi Zhang; Zhuo Chen; Lingbing Guo; Yajing Xu; Binbin Hu; Ziqi Liu; Wen Zhang; Huajun Chen
- Reference count: 26
- Key outcome: MYGO achieves state-of-the-art MMKGC performance, improving Hit@1 by up to 18.4% and MRR by 4.0% over 19 recent models.

## Executive Summary
This paper introduces MYGO, a novel framework for multi-modal knowledge graph completion (MMKGC) that addresses the coarse handling of multi-modal entity information in existing methods. By tokenizing multi-modal entity data into fine-grained discrete tokens and employing a hierarchical transformer-based architecture for fusion, MYGO preserves nuanced semantic details and enables rich cross-modal interactions. The framework further incorporates fine-grained contrastive learning to augment entity representations, resulting in significant performance gains on standard MMKGC benchmarks.

## Method Summary
MYGO tackles MMKGC by first tokenizing multi-modal entity data (images and text) into fine-grained discrete tokens using pre-trained visual (BEiT) and textual (BERT) tokenizers. These tokens are then processed through a hierarchical triple modeling (HTM) architecture, which includes a cross-modal entity encoder (CMEE) and a contextual triple encoder (CTE), both implemented as transformer layers. The framework employs a Tucker decomposition-based relational decoder to score triples and incorporates fine-grained contrastive learning (FGCL) to augment entity representations via in-batch negative sampling and InfoNCE loss. The model is trained end-to-end on MMKGC benchmarks, with extensive ablation studies and efficiency analyses.

## Key Results
- Achieves state-of-the-art performance on DB15K, MKG-W, and MKG-Y datasets
- Improves Hit@1 by up to 18.4% and MRR by 4.0% over 19 recent MMKGC models
- Demonstrates stability and efficiency, with performance gains maintained across varying token counts
- Shows interpretability through fine-grained contrastive learning and cross-modal interactions

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Tokenizing multi-modal entity data into fine-grained discrete tokens preserves semantic granularity lost in conventional embedding aggregation.
- Mechanism: The modality tokenization (MT) module uses pre-trained visual and textual tokenizers to decompose raw modality data into discrete semantic units. These tokens are then retained as individual embeddings rather than being collapsed into a single vector, enabling richer cross-modal interactions.
- Core assumption: Multi-modal semantic features are not fully captured when modality information is averaged or aggregated into single embeddings.
- Evidence anchors:
  - [abstract]: "resulting in coarse handling of multi-modal entity information, overlooking the nuanced, fine-grained semantic details and their complex interactions"
  - [section]: "Unlike existing MMKGC methods (Li et al. 2023), MYGO performs fine-grained tokenization and obtains a sequence of discrete tokens"
  - [corpus]: Weak corpus support for this specific mechanism; no directly comparable papers cited in the neighbor list.
- Break condition: If the tokenizer fails to segment meaningful semantic units, or if the resulting token sequence becomes too sparse or redundant, the model may not benefit from fine-grained processing.

### Mechanism 2
- Claim: Hierarchical transformer-based modeling (HTM) enables detailed cross-modal and contextual interactions between tokens, improving entity representation quality.
- Mechanism: The HTM module uses a cross-modal entity encoder (CMEE) to process the tokenized sequence, allowing every token to dynamically interact with others across modalities. Contextual triple encoders (CTE) further refine these representations in relational context.
- Core assumption: Cross-modal interactions between fine-grained tokens yield richer entity representations than modality-agnostic aggregation.
- Evidence anchors:
  - [abstract]: "learns entity representations with a cross-modal entity encoder" and "hierarchical triple modeling (HTM) architecture"
  - [section]: "we apply a transformer (Vaswani et al. 2017) layer as the CMEE... The cross-modal entity representation is obtained by e = Pooling(Transformer(Xinput(e)))"
  - [corpus]: Weak; neighbors discuss fusion strategies but not hierarchical token-level transformers for MMKGC.
- Break condition: If the transformer layers overfit to noise in token sequences, or if the context encoder fails to meaningfully incorporate relational signals, performance gains will diminish.

### Mechanism 3
- Claim: Fine-grained contrastive learning (FGCL) augments entity representations by highlighting specificity through multi-scale contrastive samples.
- Mechanism: FGCL generates positive pairs from two forward passes and additional token-level embeddings (global, visual, textual). In-batch negative sampling and InfoNCE loss sharpen distinctions between entities.
- Core assumption: Contrastive learning at multiple granularities (global vs. modality-specific) yields more discriminative entity representations.
- Evidence anchors:
  - [abstract]: "MYGO incorporates fine-grained contrastive learning to highlight the specificity of the entity representations"
  - [section]: "We can define the output representations of the multi-modal tokens... s(e), v(e), w(e) to represent the global, visual, and textual information"
  - [corpus]: No explicit contrastive learning mechanism in neighbors; this is a distinctive design choice.
- Break condition: If contrastive pairs are poorly constructed (e.g., semantically dissimilar positives), the contrastive loss may harm rather than help representation quality.

## Foundational Learning

- Concept: Multi-modal Knowledge Graph Completion (MMKGC)
  - Why needed here: The paper builds on and improves MMKGC methods, so understanding the task—predicting missing triples using structural and multi-modal entity data—is foundational.
  - Quick check question: In MMKGC, what additional information beyond graph structure is leveraged to predict missing triples?

- Concept: Tokenization in NLP and Vision (Vector Quantization)
  - Why needed here: MYGO relies on tokenizers (BERT for text, BEiT for images) to decompose modality data into discrete semantic units.
  - Quick check question: How does vector quantization (VQ) enable non-text modalities like images to be represented as token sequences?

- Concept: Contrastive Learning and InfoNCE Loss
  - Why needed here: FGCL uses contrastive learning to refine entity representations; understanding InfoNCE and in-batch negative sampling is key.
  - Quick check question: What is the role of temperature τ in the InfoNCE loss, and how does it affect contrastive learning?

## Architecture Onboarding

- Component map:
  Modality Tokenization (MT) -> Hierarchical Triple Modeling (HTM) -> Relational Decoder -> Fine-grained Contrastive Learning (FGCL) -> Output

- Critical path:
  1. Tokenize multi-modal data into discrete tokens (MT)
  2. Encode tokens into fine-grained entity representations (CMEE)
  3. Contextualize representations within relational structure (CTE)
  4. Score triples (Relational Decoder)
  5. Augment representations via contrastive learning (FGCL)

- Design tradeoffs:
  - Token granularity vs. sequence length: More tokens preserve detail but increase computation (O((m+n)²))
  - Static vs. adaptive fusion: MYGO’s adaptive transformer-based fusion is more expressive but costlier than static aggregation
  - Contrastive loss weight λ: Must balance MMKGC task loss and representation quality

- Failure signatures:
  - Degraded Hit@1/MRR with stable Hit@10 suggests poor discrimination at top ranks
  - Performance drops with increased token counts may indicate overfitting or computational inefficiency
  - If contrastive loss hurts performance, candidate construction or temperature may be misconfigured

- First 3 experiments:
  1. Ablation: Remove modality tokenization and test MMKGC performance drop.
  2. Ablation: Remove FGCL and observe effect on fine-grained metrics (Hit@1 vs Hit@10).
  3. Efficiency test: Vary (m+n) token counts and measure runtime vs. performance trade-off.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of MYGO scale with increasing amounts of multi-modal data per entity?
- Basis in paper: [explicit] The paper discusses handling multiple modality instances and mentions evaluating performance with varying numbers of entity images (1-5), but doesn't systematically explore scaling beyond this range or with other modalities.
- Why unresolved: The paper only evaluates up to 5 images per entity, leaving the question of performance at larger scales unanswered. The computational efficiency trade-offs at scale are also not fully explored.
- What evidence would resolve it: Comprehensive experiments varying the number of images/texts per entity across multiple orders of magnitude, along with runtime and memory profiling to assess scalability bottlenecks.

### Open Question 2
- Question: What is the optimal balance between visual and textual tokens (m vs n) for different types of knowledge graphs or entity categories?
- Basis in paper: [explicit] The paper mentions tuning m and n in {4, 8, 12} but doesn't systematically explore different ratios or investigate whether optimal values vary by dataset or entity type.
- Why unresolved: The ablation study treats m and n as parameters to be tuned but doesn't analyze their relative importance or investigate domain-specific optimization.
- What evidence would resolve it: Systematic experiments varying the m:n ratio across different datasets and entity categories, along with correlation analysis between token ratios and entity types.

### Open Question 3
- Question: How does MYGO perform when handling modalities beyond images and text, such as audio or video?
- Basis in paper: [explicit] The paper focuses on image and text modalities, mentioning other modalities (audio, videos) only in the context of MMKGs in general, not in MYGO's implementation.
- Why unresolved: The tokenization and hierarchical modeling approach is described for two specific modalities, with no discussion of extending to other data types or the challenges involved.
- What evidence would resolve it: Experiments incorporating additional modalities like audio descriptions or video clips, along with modifications to the tokenization and fusion modules to handle temporal/spatial aspects of these modalities.

## Limitations
- Weak corpus support for the core mechanisms proposed (fine-grained tokenization, hierarchical transformer fusion, fine-grained contrastive learning)
- Limited ablation studies isolating contributions of each module (tokenization, hierarchical modeling, contrastive learning)
- Key preprocessing and hyperparameter details (missing modality handling, contrastive loss temperature, dropout rates) not fully specified

## Confidence
- **High Confidence**: Experimental results demonstrating MYGO's state-of-the-art performance on three MMKGC benchmarks
- **Medium Confidence**: Plausible mechanism claims (fine-grained tokenization preserves semantic granularity, hierarchical transformers enable rich cross-modal interactions, contrastive learning augments specificity) but lack strong corroborating evidence from cited corpus or detailed ablation studies
- **Low Confidence**: Generalizability to datasets with significantly different modality distributions or entity structures not demonstrated; efficiency claims not empirically validated across varying token counts

## Next Checks
1. **Ablation Study**: Perform a detailed ablation study by systematically removing each component (tokenization, hierarchical modeling, contrastive learning) and measuring the impact on key metrics (Hit@1, MRR) to quantify their individual contributions.

2. **Token Granularity Analysis**: Conduct experiments varying the maximum number of tokens (m, n) across a wider range (e.g., 2, 4, 8, 12, 16) to assess the trade-off between performance gains and computational efficiency, and identify optimal settings.

3. **Corpus Gap Analysis**: Investigate the neighboring literature for alternative fusion strategies or contrastive learning approaches in multi-modal settings, and compare their efficacy against MYGO's mechanisms to better contextualize the framework's innovations.