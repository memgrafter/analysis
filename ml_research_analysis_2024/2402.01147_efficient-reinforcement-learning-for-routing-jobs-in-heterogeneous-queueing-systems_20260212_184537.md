---
ver: rpa2
title: Efficient Reinforcement Learning for Routing Jobs in Heterogeneous Queueing
  Systems
arxiv_id: '2402.01147'
source_url: https://arxiv.org/abs/2402.01147
tags:
- policy
- threshold
- servers
- server
- queueing
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses efficient routing of jobs in heterogeneous
  multi-server queueing systems to minimize expected response time. Standard reinforcement
  learning methods are inefficient due to exponential state space growth with the
  number of servers.
---

# Efficient Reinforcement Learning for Routing Jobs in Heterogeneous Queueing Systems

## Quick Facts
- **arXiv ID**: 2402.01147
- **Source URL**: https://arxiv.org/abs/2402.01147
- **Reference count**: 32
- **Primary result**: ACHQ achieves up to 30% improvement in expected response time over greedy baseline

## Executive Summary
This paper addresses the challenge of efficiently routing jobs in heterogeneous multi-server queueing systems to minimize expected response time. Traditional reinforcement learning approaches struggle with exponential state-space growth as the number of servers increases. The authors propose ACHQ, an actor-critic algorithm that leverages the underlying queueing structure through a low-dimensional soft threshold policy parameterization and linear value function approximation. Theoretical analysis proves convergence to a stationary point for general multi-server systems and to an approximate global optimum for the two-server case, while simulations demonstrate significant performance improvements over baseline policies.

## Method Summary
ACHQ is an actor-critic policy gradient method designed specifically for heterogeneous multi-server queueing systems. The actor uses a soft threshold policy parameterization that depends only on queue length and server identity, reducing the policy space from exponential to linear in the number of servers. The critic employs a linear function approximator with normalized state features. The algorithm operates in a discrete-time simulation environment with exponential inter-arrival and service times, updating policy parameters via policy gradients and critic parameters via TD(0) updates. Two timescales are used with step sizes α(t) = O(1/(1+t)ʳᵃ) and β(t) = O(1/(1+t)ʳᵇ) where 0 < rᵇ < rᵃ < 1.

## Key Results
- ACHQ achieves up to 30% improvement in expected response time compared to fastest-available-server baseline
- For two-server systems, ACHQ converges to an approximate global optimum as the soft threshold sharpens
- For general multi-server systems, ACHQ converges to a stationary point under standard assumptions

## Why This Works (Mechanism)

### Mechanism 1
- Claim: ACHQ avoids state-space explosion by using a soft threshold policy parameterization that depends only on queue length and fastest available server.
- Mechanism: The policy selects the fastest idle server only when queue length exceeds a server-specific threshold; otherwise it waits. This reduces the policy space from exponential in number of servers to linear (k-1 parameters).
- Core assumption: The optimal policy for heterogeneous multi-server systems is of threshold type.
- Evidence anchors:
  - [abstract]: "leverages the underlying queueing structure to design a low-dimensional soft threshold policy parameterization"
  - [section 4.2]: Defines the soft threshold policy and explains it as a differentiable version of the conjectured optimal threshold policy.
  - [corpus]: No direct evidence in neighbors; the related papers focus on RL for scheduling but not on threshold-type policies.
- Break condition: If the optimal policy is not threshold-like, the low-dimensional parameterization will underfit and performance will degrade.

### Mechanism 2
- Claim: The linear value function approximation with normalized state features suffices because the optimal value function is approximately linear in the number of jobs.
- Mechanism: Features are s/(lM + k), capturing the intuition that more jobs means higher expected cost. Empirical R² values near 0.94 confirm this approximation is accurate.
- Core assumption: The optimal value function can be approximated well by a linear function of the normalized state.
- Evidence anchors:
  - [section 6.1]: "R2 values of 0.941, 0.943, 0.942, 0.942 respectively. This indicates a very high degree of correlation and hence shows that a linear function is a good approximation."
  - [section 4.2]: Defines the feature vector as ϕ(s) = s/(lM + k).
  - [corpus]: No evidence in neighbors about value function approximation quality.
- Break condition: If the value function is highly nonlinear (e.g., due to complex interactions between servers), linear approximation will yield large errors and slow convergence.

### Mechanism 3
- Claim: ACHQ converges to a stationary point for the general case and to an approximate global optimum for two servers by leveraging convergence results from Wu et al. (2020) and Bhandari & Russo (2024).
- Mechanism: Two-time-scale actor-critic with TD(0) critic update ensures that policy gradient estimates converge under ergodicity and boundedness assumptions; for two servers, the soft threshold policy class is closed under approximate policy improvement.
- Core assumption: The Markov chain induced by the policy is ergodic and the Bellman error of the policy class goes to zero as the soft threshold sharpens.
- Evidence anchors:
  - [section 5.1]: States convergence to stationary point under assumptions 5.1-5.5 and cites Wu et al. (2020).
  - [section 5.2]: Claims approximate global optimality for two servers using Bhandari & Russo (2024) and provides proof sketch.
  - [corpus]: No direct evidence in neighbors about convergence guarantees for policy gradient methods in queueing.
- Break condition: If the Markov chain is not ergodic or the policy class cannot approximate the optimal policy well, convergence guarantees fail.

## Foundational Learning

- **Concept**: Markov Decision Processes (MDPs) and average-cost optimization.
  - Why needed here: The queueing system is modeled as an MDP where the goal is to minimize average expected response time.
  - Quick check question: In an MDP, what does the stationary distribution νθ represent for a given policy πθ?

- **Concept**: Policy Gradient Theorem and actor-critic methods.
  - Why needed here: ACHQ uses policy gradients with a linear critic to update the policy parameters.
  - Quick check question: Why do we subtract a baseline (like V) from Q in policy gradient estimation?

- **Concept**: Threshold policies and their optimality in queueing systems.
  - Why needed here: The algorithm is designed around the conjecture that optimal policies are threshold-like.
  - Quick check question: In a two-server system with one fast and one slow server, under what condition should a job be routed to the slow server?

## Architecture Onboarding

- **Component map**: Router -> Environment -> Actor -> Critic -> Trainer
- **Critical path**:
  1. Router samples action a ~ πθ(·|s)
  2. Environment returns next state s' and cost c
  3. Advantage δ = c - η + ϕ(s')ᵀω - ϕ(s)ᵀω is computed
  4. Actor θ ← θ - α δ ∇θ log πθ(a|s)
  5. Critic ω ← ΠRω (ω + β δ ϕ(s))
  6. Average cost η ← η + ζ (c - η)

- **Design tradeoffs**:
  - Low-dimensional policy vs. expressiveness: Soft threshold may miss optimal non-threshold policies
  - Linear critic vs. bias: Fast and sample-efficient but may approximate poorly for nonlinear value functions
  - Sharpness σ: Higher σ → closer to hard threshold but may cause vanishing gradients

- **Failure signatures**:
  - Divergence: Check if δ is too large; reduce step sizes α, β
  - Slow learning: Verify that policy is actually changing (monitor θ norms)
  - Poor performance: Inspect if thresholds converge to unreasonable values; consider increasing σ or adjusting feature scaling

- **First 3 experiments**:
  1. Two-server case: Compare ACHQ with known optimal threshold policy; verify convergence and optimality gap
  2. Scaling test: Increase number of servers; confirm state-space savings and performance relative to FAS baseline
  3. Heterogeneity sweep: Vary ratio of fast to slow server speeds; observe how thresholds and performance gain change

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the optimal policy for multi-server heterogeneous queueing systems always follow a threshold policy structure, even beyond the two-server case?
- Basis in paper: The paper demonstrates through simulations that the optimal policy obtained via Relative Value Iteration exhibits threshold-like behavior for multi-server systems, but notes that proving this optimality for the general multi-server case has been an open problem for nearly four decades.
- Why unresolved: While the paper shows threshold-like behavior in simulations, a formal proof of optimality for the general multi-server case remains an open problem in queueing theory.
- What evidence would resolve it: A rigorous mathematical proof demonstrating that threshold policies are optimal for multi-server heterogeneous queueing systems, or a counterexample showing a non-threshold optimal policy.

### Open Question 2
- Question: How does the performance of ACHQ scale with increasing system heterogeneity and server count in practical large-scale implementations?
- Basis in paper: The paper shows ACHQ improves expected response time by up to 30% over baseline policies, but notes that as the number of servers increases, the performance gap to the FAS baseline reduces. It also implements a power-of-d-choices version for scalability.
- Why unresolved: The paper demonstrates performance on systems with up to 8 servers and various heterogeneity levels, but doesn't provide extensive empirical analysis for very large-scale systems with hundreds of servers or extreme heterogeneity.
- What evidence would resolve it: Large-scale simulations or real-world implementations with hundreds of servers and extreme heterogeneity, comparing ACHQ against other policies.

### Open Question 3
- Question: What is the impact of using different function approximations for the value function in ACHQ?
- Basis in paper: The paper uses a linear function approximation with normalized state features and shows high R² values (>0.94) in experiments, suggesting good fit. However, it doesn't explore other function approximation methods.
- Why unresolved: The paper only considers linear function approximation, leaving open questions about whether more complex approximations could improve performance or if linear approximation is sufficient.
- What evidence would resolve it: Experiments comparing ACHQ with different function approximation methods (e.g., neural networks, polynomial features) and analyzing their impact on convergence and performance.

## Limitations

- The convergence guarantees for general multi-server systems rely on assumptions about ergodicity and boundedness that are not fully verified in the specific queueing context
- The performance improvements (up to 30%) are demonstrated only for relatively small systems (up to 5 servers) and may not scale to much larger systems
- The claim of approximate global optimality for two servers is based on a proof sketch that relies on another cited result, without full detailed proof in the paper

## Confidence

- **High Confidence**: The mechanism by which ACHQ reduces state-space complexity through soft threshold parameterization is well-established and directly supported by the mathematical formulation in Section 4.2. The empirical evidence of R² values near 0.94 for the linear value function approximation provides strong support for Mechanism 2.

- **Medium Confidence**: The convergence claims for general multi-server systems are supported by citing established results (Wu et al., 2020) and providing assumptions, but the specific application to the ACHQ algorithm is not fully detailed. The approximate global optimality for two servers is claimed but only a proof sketch is provided, relying on another cited result (Bhandari & Russo, 2024).

- **Low Confidence**: The scalability of ACHQ to very large systems (e.g., hundreds of servers) is not demonstrated. The performance claims (up to 30% improvement) are based on specific simulation parameters and may not generalize to all possible system configurations.

## Next Checks

1. **Two-Server Validation**: Implement the ACHQ algorithm for a two-server system with known optimal threshold policy (e.g., M/M/2 queue with different service rates). Measure the optimality gap as σ increases and verify if it approaches zero, confirming the approximate global optimality claim.

2. **Value Function Linearity Test**: For systems with more than two servers, compute the R² values of the linear approximation for the value function across different load conditions and system sizes. Systematically vary the degree of heterogeneity in service rates to determine when the linear approximation breaks down.

3. **Scalability Benchmark**: Implement ACHQ for a larger system (e.g., 10-20 servers) and compare its performance and learning speed against the greedy FAS baseline and a model-based approach. Measure the impact of increasing σ on both performance and convergence stability.