---
ver: rpa2
title: 'With Good MT There is No Need For End-to-End: A Case for Translate-then-Summarize
  Cross-lingual Summarization'
arxiv_id: '2409.00414'
source_url: https://arxiv.org/abs/2409.00414
tags:
- system
- languages
- summarization
- systems
- translation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper challenges the prevailing trend favoring end-to-end cross-lingual
  summarization systems, which are believed to perform on par or better than traditional
  pipeline-based approaches. By evaluating both paradigms across 39 source languages
  into English, the study demonstrates that a simple translate-then-summarize pipeline
  consistently outperforms end-to-end models, even those trained on large parallel
  datasets.
---

# With Good MT There is No Need For End-to-End: A Case for Translate-then-Summarize Cross-lingual Summarization

## Quick Facts
- arXiv ID: 2409.00414
- Source URL: https://arxiv.org/abs/2409.00414
- Reference count: 13
- Primary result: Pipeline approach outperforms end-to-end models for cross-lingual summarization

## Executive Summary
This paper challenges the prevailing assumption that end-to-end cross-lingual summarization systems are superior to traditional pipeline approaches. Through extensive experimentation across 39 source languages into English, the authors demonstrate that a simple translate-then-summarize pipeline consistently outperforms end-to-end models, even those trained on large parallel datasets. The pipeline leverages a strong abstractive summarization model (BRIO) and either a lightweight (OPUS-MT) or powerful (M2M100) translation system, achieving higher ROUGE-1 scores particularly when translation quality is high.

The study reveals a strong correlation between translation quality (measured by BLEU scores) and summarization performance, suggesting that BLEU can serve as an a priori indicator of language pair feasibility. This finding advocates for pipeline-based designs in cross-lingual summarization, emphasizing that advancements in monolingual summarization and translation systems can be effectively combined rather than requiring complex end-to-end architectures.

## Method Summary
The paper evaluates both end-to-end and pipeline approaches for cross-lingual summarization from 39 source languages into English. The pipeline approach involves translating source documents to English using either OPUS-MT (lightweight) or M2M100 (powerful), followed by summarization using the BRIO model. The end-to-end approach uses a model trained on large parallel datasets. Both approaches are evaluated using ROUGE metrics, with particular attention to the correlation between translation quality (BLEU scores) and summarization performance.

## Key Results
- Pipeline approaches consistently outperform end-to-end models across 39 source languages
- Higher translation quality (BLEU scores) correlates with better summarization performance
- The translate-then-summarize pipeline achieves higher ROUGE-1 scores than end-to-end alternatives

## Why This Works (Mechanism)
The success of the pipeline approach stems from leveraging the strengths of specialized models: high-quality translation systems that can accurately convert source text to English, combined with state-of-the-art monolingual summarization models. This separation of concerns allows each component to focus on its core competency rather than forcing a single model to handle both translation and summarization simultaneously. The strong correlation between BLEU and ROUGE scores indicates that translation quality is a critical bottleneck - when translation is accurate, the subsequent summarization can operate effectively on well-formed English text.

## Foundational Learning
- **Cross-lingual summarization**: Generating summaries in a target language different from the source language - needed to understand the problem space and why traditional monolingual summarization doesn't suffice
- **BLEU score**: Metric for evaluating translation quality by comparing n-gram overlap with reference translations - needed to assess translation quality as a predictor of summarization performance
- **ROUGE metrics**: Evaluation metrics for summarization that measure n-gram overlap between generated and reference summaries - needed to quantify summarization quality
- **Pipeline vs. end-to-end architectures**: Trade-offs between modular approaches (separate translation and summarization) versus unified models - needed to understand the fundamental design decision being evaluated
- **Translation quality as bottleneck**: How errors in translation propagate to summarization - needed to explain the correlation between BLEU and ROUGE scores

## Architecture Onboarding

Component Map:
Source Document -> Translation System (OPUS-MT/M2M100) -> Summarization Model (BRIO) -> English Summary

Critical Path:
Translation quality â†’ Summarization quality. The pipeline approach succeeds when translation is accurate enough for the summarization model to work effectively on well-formed English text.

Design Tradeoffs:
- Pipeline approach: Leverages best-in-class components but introduces potential error propagation from translation to summarization
- End-to-end approach: More elegant theoretically but requires large parallel datasets and may struggle with the complexity of handling both tasks simultaneously

Failure Signatures:
- Poor translation quality (low BLEU) leading to degraded summarization performance
- Summarization model failing to capture key information when source text is poorly translated
- End-to-end models struggling with rare language pairs due to limited training data

First Experiments:
1. Compare ROUGE scores of pipeline vs. end-to-end approaches on a held-out test set
2. Analyze correlation between BLEU scores and ROUGE scores across different language pairs
3. Evaluate impact of using different translation systems (OPUS-MT vs M2M100) on final summarization quality

## Open Questions the Paper Calls Out
None

## Limitations
- Evaluation focuses exclusively on English as the target language, limiting generalizability to other target languages
- ROUGE-based evaluation may not fully capture nuanced quality differences between approaches
- Conclusions based on specific model choices may not generalize to other state-of-the-art systems

## Confidence
- High confidence in the core finding that pipeline approaches outperform end-to-end models for English target language summarization
- Medium confidence in the BLEU-to-ROUGE correlation as a predictive indicator, given limited exploration of different model combinations
- Low confidence in generalizability to non-English target languages and other evaluation metrics

## Next Checks
1. Replicate experiments with additional target languages beyond English to assess cross-linguistic validity
2. Conduct human evaluation studies to complement automated metrics and assess qualitative differences
3. Test the pipeline approach with alternative state-of-the-art summarization and translation models to verify robustness of findings