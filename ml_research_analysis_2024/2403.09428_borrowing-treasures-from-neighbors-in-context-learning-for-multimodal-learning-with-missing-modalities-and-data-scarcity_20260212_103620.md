---
ver: rpa2
title: 'Borrowing Treasures from Neighbors: In-Context Learning for Multimodal Learning
  with Missing Modalities and Data Scarcity'
arxiv_id: '2403.09428'
source_url: https://arxiv.org/abs/2403.09428
tags:
- data
- learning
- missing
- performance
- modalities
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses multimodal learning with missing modalities
  and data scarcity. The core method idea is to use retrieval-augmented in-context
  learning to adaptively enhance both missing- and full-modality samples by leveraging
  neighboring full-modality samples from the training set.
---

# Borrowing Treasures from Neighbors: In-Context Learning for Multimodal Learning with Missing Modalities and Data Scarcity

## Quick Facts
- arXiv ID: 2403.09428
- Source URL: https://arxiv.org/abs/2403.09428
- Reference count: 40
- Primary result: 6.1% average improvement over strong baseline with 1% training data

## Executive Summary
This paper addresses multimodal learning with missing modalities and data scarcity by proposing a retrieval-augmented in-context learning approach. The method enhances both missing- and full-modality samples by adaptively leveraging neighboring full-modality samples from the training set, reducing reliance on sample size. Experiments show an average 6.1% improvement over recent baselines across various datasets and missing states when only 1% of training data is available, while also reducing the performance gap between full- and missing-modality data.

## Method Summary
The proposed method uses a pre-trained multimodal transformer to extract features from samples, then retrieves the Q most similar full-modality samples from the training set using cosine similarity on CLS tokens. Both the current sample's pooled features and neighbor features are passed into an in-context learning (ICL) module, which processes this information to predict labels. The approach treats both missing- and full-modality samples similarly by using neighbor information, rather than focusing solely on reconstructing missing modalities. The method employs two ICL configurations: ICL-CA (cross-attention) and ICL-NTP (next-token prediction).

## Key Results
- Average 6.1% improvement over recent strong baseline across datasets with 1% training data
- Reduces performance gap between full-modality and missing-modality data compared to parametric baselines
- Sample-efficient semi-parametric approach that exploits available full-modality data

## Why This Works (Mechanism)

### Mechanism 1
Retrieval-augmented ICL improves performance by fusing neighbor full-modality features with current sample features. The method retrieves Q similar full-modality samples using cosine similarity on CLS tokens, then passes both current and neighbor features into an ICL module for implicit inference of missing modalities and feature refinement.

### Mechanism 2
The method reduces performance gap between modalities by treating both types of samples equally through adaptive neighbor-based enhancement, rather than disproportionately focusing on missing modalities reconstruction.

### Mechanism 3
Semi-parametric approach improves sample efficiency by leveraging existing full-modality data through retrieval instead of learning parameters to handle missing modalities, reducing reliance on large training sets.

## Foundational Learning

- **Multimodal transformer architecture**: Understanding how ViLT-style transformers handle missing modalities through empty token initialization is crucial since the method builds on this foundation. Quick check: How does ViLT handle missing modalities differently from earlier feature-fusion approaches?

- **In-context learning in multimodal settings**: The core innovation uses ICL to condition samples on retrieved neighbors, requiring understanding of ICL mechanisms in multimodal contexts. Quick check: What's the difference between ICL-CA and ICL-NTP in this context?

- **Retrieval mechanisms and similarity measures**: The method relies on cosine similarity of CLS tokens for neighbor retrieval, requiring understanding of when this similarity measure works well in high-dimensional spaces. Quick check: Why use only CLS tokens for similarity calculation rather than all tokens?

## Architecture Onboarding

- **Component map**: Pretrained multimodal transformer (frozen) → Feature extraction → Retrieval module → ICL module (trainable) → Classification layer
- **Critical path**: Input → Transformer feature extraction → Neighbor retrieval → ICL module → Classification
- **Design tradeoffs**: Using only full-modality neighbors improves computational efficiency but may miss context; ICL-CA vs ICL-NTP offers simplicity vs sequential dependency capture; pooled feature length balances information vs computation
- **Failure signatures**: Poor missing-modality performance suggests ineffective neighbor retrieval; performance gap indicates unbalanced treatment; low sample efficiency suggests insufficient contextual information
- **First 3 experiments**: 1) Verify neighbor retrieval quality through cosine similarity scores and modality distribution; 2) Ablation on Q neighbors to find optimal trade-off; 3) Pooled feature length sweep to verify 8 tokens is optimal

## Open Questions the Paper Calls Out

### Open Question 1
Optimal balance between retrieval of full-modality samples and direct training on missing-modality samples. The paper doesn't compare against direct training on missing-modality samples.

### Open Question 2
How performance scales with training dataset size in low-data regime. The paper focuses on low-data scenarios without analyzing scaling behavior.

### Open Question 3
Impact of different similarity measures (Euclidean distance, dot product) on retrieval and performance. The paper uses cosine similarity without exploring alternatives.

### Open Question 4
Handling more than two modalities and challenges of scaling to higher-dimensional multimodal data. The paper assumes two modalities for simplicity but claims framework can handle any number.

## Limitations
- Performance heavily depends on quality of retrieved neighbors and cosine similarity in high-dimensional space
- Additional computational overhead for retrieval process and ICL module
- Limited to binary modality states (full vs. missing), not partial modality availability or noisy data

## Confidence

**High Confidence**: Retrieval-augmented ICL improves performance over baselines with 1% training data; method reduces performance gap between modalities; semi-parametric approach exploits full-modality data.

**Medium Confidence**: Specific performance improvements (6.1%) across all datasets; ICL-CA vs ICL-NTP superiority; optimal Q=4 neighbors.

**Low Confidence**: Generalization to untested datasets and tasks; performance on partial modality scenarios; robustness to retrieval noise.

## Next Checks

1. **Retrieval Quality Analysis**: Analyze retrieved neighbors across different missing modality rates and datasets using both cosine similarity and human evaluation to validate semantic relevance.

2. **Cross-Dataset Generalization Test**: Apply method to speech-text multimodal learning or time-series sensor data to test generalization beyond vision-language and medical domains.

3. **Partial Modality Availability Study**: Create scenarios with partially corrupted modalities (noisy images, truncated text) to understand performance beyond binary missing modality setup.