---
ver: rpa2
title: How good is my story? Towards quantitative metrics for evaluating LLM-generated
  XAI narratives
arxiv_id: '2412.10220'
source_url: https://arxiv.org/abs/2412.10220
tags:
- narrative
- narratives
- metrics
- shap
- feature
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a framework and metrics to evaluate LLM-generated
  narratives that explain SHAP-based predictions for tabular data. The authors propose
  using an extraction model to pull out features, ranks, signs, and assumptions from
  the narratives, then validate them using downstream metrics like accuracy and perplexity.
---

# How good is my story? Towards quantitative metrics for evaluating LLM-generated XAI narratives

## Quick Facts
- arXiv ID: 2412.10220
- Source URL: https://arxiv.org/abs/2412.10220
- Authors: Timour Ichmoukhamedov; James Hinns; David Martens
- Reference count: 0
- Primary result: Framework for evaluating LLM-generated XAI narratives using extraction, faithfulness, human similarity, and assumption metrics

## Executive Summary
This paper introduces a framework and metrics to evaluate LLM-generated narratives that explain SHAP-based predictions for tabular data. The authors propose using an extraction model to pull out features, ranks, signs, and assumptions from the narratives, then validate them using downstream metrics like accuracy and perplexity. They also explore cosine similarity with human-written narratives as a measure of human similarity. The metrics are tested on three datasets and multiple LLMs, showing that manipulated narratives are detected effectively. A key finding is that LLMs often self-correct feature signs in manipulated narratives, suggesting hallucination issues in XAI explanations.

## Method Summary
The framework evaluates LLM-generated narratives by first extracting structured information (features, ranks, signs, values, assumptions) using an extraction LLM with zero-shot prompting. These extracted elements are then validated against ground truth SHAP tables using downstream metrics: faithfulness (rank, sign, value agreement), human similarity (cosine similarity, BLEURT), and assumptions (perplexity). The approach is tested on three tabular datasets (Fifa Man of the Match, German Credit Score, Student Performance) with narratives generated by various LLMs. The method aims to provide automated, quantitative evaluation of narrative quality beyond human judgment.

## Key Results
- Extraction model successfully identifies ranks, signs, and values from narratives with reasonable accuracy
- Perplexity effectively detects unreasonable assumptions in manipulated narratives
- Cosine similarity with modern embeddings captures narrative quality and can match similar narratives
- LLMs demonstrate self-correction behavior, often fixing sign errors when detecting manipulated narratives
- Framework successfully identifies quality issues in automatically generated XAI narratives

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Extraction model identifies and validates rank, sign, and value consistency between generated narrative and original SHAP table.
- Mechanism: Zero-shot prompt instructs extraction LLM to parse narrative into structured feature attributes (rank, sign, value, assumption), which are then checked against ground truth SHAP values using downstream accuracy metrics.
- Core assumption: Extraction LLM reliably extracts structured information from narratives with accuracy comparable to human annotators.
- Evidence anchors:
  - [abstract] "We introduce an extraction model that can extract various quantities of interest in the narrative that could then be checked using downstream metrics of choice."
  - [section] "The extraction model then extracts a dictionary with keys fj ∈ F : j ∈ {0, ..., n − 1} representing the features present in the narrative."
  - [corpus] Weak evidence - no direct citations on extraction model accuracy for this specific task.
- Break condition: Extraction model fails to reliably parse narratives, leading to incorrect faithfulness metrics.

### Mechanism 2
- Claim: Perplexity of extracted assumptions detects unreasonable statements by measuring token likelihood relative to base LLM.
- Mechanism: Extracted assumptions are evaluated using perplexity metric from Llama-3-8B model; lower perplexity indicates more reasonable assumptions.
- Core assumption: Perplexity effectively distinguishes reasonable from unreasonable assumptions in XAI context.
- Evidence anchors:
  - [abstract] "Perplexity measures how unlikely it is for the LLM to produce this string which as a rough proxy has been used for the purposes of fact-checking [24, 25] or other evaluation tasks [26]."
  - [section] "Perplexity is indeed capable of detecting more unreasonable assumptions with good accuracy" (validation experiment).
  - [corpus] Weak evidence - general perplexity literature but limited specific validation for XAI assumptions.
- Break condition: Perplexity fails to capture semantic reasonableness, leading to false negatives in assumption detection.

### Mechanism 3
- Claim: Modern embeddings with cosine similarity effectively match generated narratives to human-written references.
- Mechanism: Narratives are embedded using voyage-large-2-instruct model, and cosine similarity measures semantic distance between LLM-generated and human-written narratives.
- Core assumption: State-of-the-art embedding models capture semantic similarity relevant for XAI narrative evaluation.
- Evidence anchors:
  - [abstract] "we also consider the scenario where a reference set of explanations together with expert-written narratives is available and can be used for evaluation."
  - [section] "even the simple cosine similarity metric used on embeddings created by state-of-the-art embedding models, already captures several interesting properties" (validation results showing 42/60 correct matches).
  - [corpus] Weak evidence - general embedding literature but limited specific validation for narrative matching.
- Break condition: Embeddings fail to capture narrative quality differences, leading to unreliable similarity metrics.

## Foundational Learning

- Concept: SHAP (SHapley Additive exPlanations) method for feature attribution
  - Why needed here: The framework evaluates LLM-generated narratives that explain SHAP-based predictions, requiring understanding of SHAP values and interpretation.
  - Quick check question: How does SHAP combine local linear approximations with Shapley values to estimate feature effects?

- Concept: Feature attribution and explanation faithfulness
  - Why needed here: The evaluation framework measures whether generated narratives faithfully represent the underlying SHAP explanations.
  - Quick check question: What distinguishes faithfulness from plausibility in XAI explanations?

- Concept: Large Language Model prompting and zero-shot learning
  - Why needed here: The framework uses zero-shot prompts to both generate narratives and extract information from them.
  - Quick check question: How does zero-shot prompting differ from fine-tuning when using LLMs for XAI tasks?

## Architecture Onboarding

- Component map: Generation LLM -> Narrative output -> Extraction LLM -> Structured attributes -> Downstream metrics (accuracy, perplexity, cosine similarity) -> Evaluation results
- Critical path: Generation prompt -> LLM narrative generation -> Feature extraction -> Metric calculation -> Quality assessment
- Design tradeoffs: Accuracy vs. computational cost (using quantized models for perplexity), comprehensive vs. focused metrics, automated vs. human evaluation
- Failure signatures: Low rank/sign accuracy indicates faithfulness issues, high perplexity indicates unreasonable assumptions, low cosine similarity indicates narrative quality issues
- First 3 experiments:
  1. Generate narratives with manipulated SHAP tables and verify detection through accuracy drops
  2. Test perplexity detection by injecting unreasonable assumptions and measuring changes
  3. Compare cosine similarity performance against BLEURT baseline on narrative matching task

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can we improve the accuracy of sign correction in manipulated narratives generated by LLMs?
- Basis in paper: [explicit] The paper finds that LLMs exhibit a stronger tendency to self-correct the sign of feature contributions in manipulated narratives, suggesting a potential hallucination issue.
- Why unresolved: The paper does not provide a detailed explanation for this phenomenon or propose solutions to mitigate it.
- What evidence would resolve it: Experiments demonstrating improved sign accuracy in manipulated narratives after applying specific prompting strategies or model modifications.

### Open Question 2
- Question: Can fine-tuned embedding models improve the accuracy of human similarity metrics for XAI narratives?
- Basis in paper: [inferred] The paper suggests that training additional specialized layers on top of embedding models could improve the isolation of various properties of relevance for narrative evaluation.
- Why unresolved: The paper does not explore this avenue and only uses existing embedding models.
- What evidence would resolve it: Experiments comparing the performance of fine-tuned embedding models against existing models in matching similar narratives and detecting shifts in similarity.

### Open Question 3
- Question: How can we develop more robust metrics for evaluating assumptions in XAI narratives beyond perplexity?
- Basis in paper: [explicit] The paper finds that perplexity is not consistent across models when applied to assumptions generated by LLMs in manipulated narratives.
- Why unresolved: The paper does not propose alternative metrics or explore the reasons for the inconsistency.
- What evidence would resolve it: Experiments comparing the performance of alternative metrics, such as factuality or coherence, against perplexity in evaluating assumptions in XAI narratives.

## Limitations

- Extraction model reliability is uncertain without direct validation against human annotators
- Framework is limited to tabular data with SHAP explanations and may not generalize to other XAI methods
- Computational cost of using LLMs for both generation and extraction could limit scalability

## Confidence

- Extraction model accuracy: Medium - lacks direct validation against human annotations
- Perplexity for assumption detection: High - demonstrated effectiveness in controlled experiments
- Cosine similarity for narrative matching: Medium - correctly matched 42/60 narratives but lacks human quality comparison
- Framework generalizability: Low - only tested on specific datasets and XAI method

## Next Checks

1. Validate extraction model accuracy by comparing extracted features against human annotations on a subset of narratives.
2. Test perplexity metric across diverse assumption types to establish robustness boundaries.
3. Conduct user studies comparing narrative quality assessments between automated metrics and human evaluators.