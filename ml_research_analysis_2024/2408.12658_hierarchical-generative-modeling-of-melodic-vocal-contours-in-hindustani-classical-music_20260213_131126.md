---
ver: rpa2
title: Hierarchical Generative Modeling of Melodic Vocal Contours in Hindustani Classical
  Music
arxiv_id: '2408.12658'
source_url: https://arxiv.org/abs/2408.12658
tags:
- pitch
- music
- audio
- melodic
- hindustani
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the challenge of generating Hindustani classical
  vocal music, which has rich melodic intricacies that are not well-captured by previous
  symbolic or MIDI-based approaches. The authors propose GaMaDHaNi, a hierarchical
  generative model that uses finely quantized pitch contours as an intermediate representation.
---

# Hierarchical Generative Modeling of Melodic Vocal Contours in Hindustani Classical Music

## Quick Facts
- arXiv ID: 2408.12658
- Source URL: https://arxiv.org/abs/2408.12658
- Reference count: 0
- The authors propose GaMaDHaNi, a hierarchical generative model using finely quantized pitch contours to capture expressive melodic intricacies in Hindustani classical vocal music, achieving 0.71 mean Pearson correlation for pitch adherence.

## Executive Summary
This paper addresses the challenge of generating Hindustani classical vocal music, which has rich melodic intricacies not well-captured by previous symbolic or MIDI-based approaches. The authors propose GaMaDHaNi, a hierarchical generative model that uses finely quantized pitch contours as an intermediate representation. The model consists of a pitch generator (either autoregressive or diffusion-based) and a spectrogram generator, trained on a 120-hour dataset. In a listening test, participants preferred the model's outputs over non-hierarchical baselines. The spectrogram generator achieved a mean Pearson correlation of 0.71 between the input and extracted pitch. The authors also demonstrate two potential interaction use cases: primed generation (continuing a melodic prompt) and coarse pitch conditioning (guiding generation with solfege-like input).

## Method Summary
The authors propose a hierarchical generative model that uses finely quantized pitch contours as an intermediate representation for modeling Hindustani vocal music. The pipeline involves extracting pitch contours from audio recordings, training a pitch generator (either autoregressive or diffusion-based), and then conditioning a spectrogram generator on the generated pitch and singer identity. The diffusion-based approach uses Iterative α-Deblending (IADB) with classifier-free guidance for conditioning. The model is trained on a 120-hour dataset of Hindustani vocal recordings, and pitch is extracted using CREPE at 100Hz with 10-cent quantization.

## Key Results
- Participants in a listening test preferred the hierarchical model's outputs over non-hierarchical baselines for melodic quality
- The spectrogram generator achieved a mean Pearson correlation of 0.71 between input and extracted pitch
- The diffusion-based pitch generator (IADB) produced more musically interesting variations than autoregressive approaches, with better handling of both slow and fast melodic movements
- Singer conditioning with classifier-free guidance maintained vocal timbre consistency across generated samples

## Why This Works (Mechanism)

### Mechanism 1
Hierarchical modeling with pitch contours as intermediate representation captures expressive melodic intricacies better than direct audio modeling. The two-level hierarchy separates the modeling of pitch (the melodic structure) from the modeling of timbre and spectral content, allowing each to be learned more effectively on the 120-hour dataset. This assumes fine-grained pitch contours contain sufficient information to represent core melodic content of Hindustani vocal music, and this representation is more compact and learnable than raw audio.

### Mechanism 2
Diffusion-based pitch generation (IADB) provides stable and diverse melodic outputs compared to autoregressive approaches. The iterative deblending process in IADB allows for global exploration of the pitch space while maintaining local coherence, producing more musically interesting variations than purely next-token prediction. This assumes the pitch space can be effectively modeled as a continuous distribution amenable to diffusion-based sampling.

### Mechanism 3
Conditioning on singer identity with classifier-free guidance maintains vocal timbre consistency across generated samples. By providing singer embeddings as conditioning during spectrogram generation, the model learns to associate specific spectral characteristics with each singer, producing outputs that maintain consistent voice quality. This assumes the spectrogram generator can effectively condition on singer embeddings to reproduce characteristic timbral features.

## Foundational Learning

- Concept: Understanding Hindustani classical music structure (raga, tala, ornamentation)
  - Why needed here: The model is specifically designed for Hindustani music, and understanding its melodic framework is essential for interpreting results and designing appropriate interactions.
  - Quick check question: What is the difference between a raga and a scale in Western music?

- Concept: Audio signal processing basics (pitch extraction, spectrogram generation)
  - Why needed here: The pipeline relies on accurate pitch extraction and spectrogram generation, requiring knowledge of signal processing fundamentals.
  - Quick check question: How does the sampling rate affect the resolution of pitch detection?

- Concept: Diffusion models and their variants (IADB)
  - Why needed here: The pitch generator uses a diffusion-based approach, requiring understanding of how diffusion models work and why IADB is suitable for this task.
  - Quick check question: What is the key difference between standard diffusion models and Iterative α-Deblending?

## Architecture Onboarding

- Component map: Hindustani vocal audio recordings -> Source separation -> Pitch extraction (CREPE) -> Quantization -> Data augmentation -> Pitch Generator (autoregressive or diffusion) -> Spectrogram Generator (diffusion) -> Vocoder (Griffin-Lim) -> Audio output

- Critical path: Pitch extraction → Pitch Generator → Spectrogram Generator → Vocoder → Audio output

- Design tradeoffs:
  - Pitch quantization resolution (10 cents) vs. model complexity and data requirements
  - Griffin-Lim vs. neural vocoder for audio synthesis (quality vs. speed)
  - Singer conditioning strength (CFG weight) vs. timbre consistency vs. pitch adherence

- Failure signatures:
  - Poor pitch extraction → Noisy or inaccurate conditioning for spectrogram generator
  - Overfitting on small dataset → Lack of diversity in generated outputs
  - Incorrect CFG weighting → Either poor timbre consistency or poor pitch adherence

- First 3 experiments:
  1. Test pitch extraction quality on a subset of the dataset with ground truth annotations
  2. Train pitch generator with autoregressive architecture only, evaluate diversity of outputs
  3. Evaluate spectrogram generator's pitch adherence using Pearson correlation on validation set

## Open Questions the Paper Calls Out

- How does incorporating tonic, raga, and tala as conditioning signals impact the quality and stylistic adherence of generated Hindustani vocal music? The authors explicitly state that future work could involve using these elements as conditioning for generation.

- Can the Spectrogram Generator's pitch adherence be improved by incorporating pitch-specific training objectives or alternative conditioning representations? The authors suggest investigating pitch-specific training objectives and alternative conditioning representations.

- How effective would domain-informed pitch extraction models, such as the Carnatic FTA-Net, be in improving the quality of pitch contours used for training and generation? The authors mention that the Carnatic FTA-Net presents a domain-informed model for extracting pitch contours from Carnatic vocal audio.

- What are the ethical implications and potential risks of generating music that mimics the identities of existing singers, and how can protective guidelines be established? The authors acknowledge the ethical concerns involved in modeling singing voices.

## Limitations

- The evaluation relies on subjective listening tests rather than objective perceptual metrics, limiting generalizability of melodic quality improvements.

- The 0.71 Pearson correlation for pitch adherence is only reported for the spectrogram generator, not for the full end-to-end system, leaving uncertainty about whether pitch degradation occurs through the complete pipeline.

- The use of Griffin-Lim as a placeholder vocoder may significantly impact perceived audio quality compared to neural vocoders, potentially masking the true capabilities of the generative model.

## Confidence

**High Confidence**: The hierarchical architecture design with pitch contours as intermediate representation is technically sound and well-motivated by the limitations of existing symbolic approaches to Hindustani music.

**Medium Confidence**: The listening test results showing preference for the proposed method over baselines, as this relies on subjective human judgment without standardized evaluation protocols.

**Low Confidence**: The claimed advantages of diffusion-based pitch generation over autoregressive approaches, as this is supported by qualitative observations rather than controlled ablation studies.

## Next Checks

1. Conduct an ablation study comparing full end-to-end pitch adherence (input pitch vs. extracted pitch from final audio) against the current spectrogram generator-only evaluation to identify any degradation in the complete pipeline.

2. Replace the Griffin-Lim vocoder with a neural vocoder (e.g., HiFi-GAN) and re-evaluate both pitch adherence and perceptual quality to determine if the vocoder is a bottleneck.

3. Perform a controlled experiment varying the diffusion guidance scale and classifier-free guidance weight to identify optimal trade-offs between pitch adherence and timbre consistency, then validate these settings through both objective metrics and listening tests.