---
ver: rpa2
title: How to Leverage Personal Textual Knowledge for Personalized Conversational
  Information Retrieval
arxiv_id: '2407.16192'
source_url: https://arxiv.org/abs/2407.16192
tags:
- ptkb
- query
- retrieval
- information
- conversational
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The study investigates how to effectively incorporate Personal
  Textual Knowledge Base (PTKB) into personalized conversational information retrieval
  (CIR). The core method involves analyzing different approaches to select relevant
  PTKB sentences and using them for query reformulation with large language models
  (LLMs).
---

# How to Leverage Personal Textual Knowledge for Personalized Conversational Information Retrieval

## Quick Facts
- arXiv ID: 2407.16192
- Source URL: https://arxiv.org/abs/2407.16192
- Reference count: 40
- The study investigates how to effectively incorporate Personal Textual Knowledge Base (PTKB) into personalized conversational information retrieval (CIR).

## Executive Summary
This paper addresses the challenge of personalizing conversational information retrieval by incorporating Personal Textual Knowledge Base (PTKB) information into query reformulation. The researchers develop methods to select relevant PTKB sentences and use them to enhance conversational queries through large language models (LLMs). They compare automatic annotation based on retrieval impact with human judgments and LLM-based selection strategies. The work demonstrates that automatic annotation aligned with retrieval effectiveness outperforms human judgments and that LLM-based strategies like select-then-reformulate and select-and-reformulate improve performance when guided by high-quality examples. The study reveals that PTKB doesn't always improve retrieval when used alone but can be beneficial when integrated through appropriate LLM strategies.

## Method Summary
The method involves analyzing different approaches to select relevant PTKB sentences for query reformulation using LLMs. The researchers implement three PTKB annotation approaches: human annotation comparison, automatic annotation based on retrieval impact (measuring whether a PTKB sentence improves retrieval metrics), and LLM-based annotation using ChatGPT with appropriate prompts. They then implement LLM-aided personalized query reformulation with two strategies: select-then-reformulate (STR) and select-and-reformulate (SAR), evaluating these using BM25 and ANCE retrievers on the full test set and the subset requiring PTKB. The approach leverages in-context learning with few-shot samples to teach LLMs how to identify relevant PTKB sentences.

## Key Results
- Automatic annotation based on retrieval impact outperforms human judgments in selecting useful PTKB sentences
- LLM-based strategies like select-then-reformulate and select-and-reformulate improve performance when guided by high-quality examples
- Using in-context learning with few-shot samples (3-shot optimal) significantly enhances LLM performance in personalized query reformulation
- PTKB does not always improve retrieval when used alone but can be beneficial when integrated through appropriate LLM strategies
- The best strategy varies by query type: selective PTKB application improves overall performance

## Why This Works (Mechanism)

### Mechanism 1
- Claim: PTKB sentences improve retrieval when aligned with retrieval effectiveness, not just human intuition
- Mechanism: Automatic annotation based on retrieval impact (whether a PTKB sentence improves retrieval metrics when used) outperforms human relevance judgments
- Core assumption: A PTKB sentence is only useful for query reformulation if it actually improves retrieval performance, not just if it seems topically related
- Evidence anchors:
  - [abstract]: "automatic annotation aligned with retrieval effectiveness outperforms human judgments"
  - [section 3.1]: "we make automatic annotation of each sentence in PTKB based on its impact on retrieval results: if it increases retrieval effectiveness, then it is deemed relevant"

### Mechanism 2
- Claim: LLM-based query reformulation strategies can implicitly handle PTKB selection when given high-quality guidance
- Mechanism: Using STR (select then reformulate) and SAR (select and reformulate) strategies with in-context learning helps LLMs identify relevant PTKB sentences and generate better queries
- Core assumption: LLMs can learn to distinguish relevant from irrelevant PTKB sentences through demonstration, even without explicit training data
- Evidence anchors:
  - [abstract]: "LLM can help generate a more appropriate personalized query when high-quality guidance is provided"
  - [section 4.3.2]: "the few-shot samples can provide the context to guide LLM to learn how to identify relevant PTKB"

### Mechanism 3
- Claim: Not all conversational queries require personalization; selective application improves overall performance
- Mechanism: Evaluating on the full test set vs. only PTKB-relevant turns shows that using PTKB for all queries can hurt performance, but helps when personalization is actually needed
- Core assumption: The optimal personalization strategy varies by query type and context
- Evidence anchors:
  - [section 4.3.1]: "it is striking to observe that the best strategy is not using any PTKB information" when evaluated on whole test set
  - [section 4.3.1]: "within the subset of queries that need PTKB, the automatic annotation achieves the best results"

## Foundational Learning

- Concept: Retrieval effectiveness metrics (MRR, NDCG, MAP)
  - Why needed here: The paper uses these metrics to evaluate how well PTKB selection improves search results
  - Quick check question: If a PTKB sentence improves MRR from 0.3 to 0.4, what does this tell us about its usefulness?

- Concept: In-context learning and few-shot prompting
  - Why needed here: The paper uses 1-5 shot examples to teach LLMs how to select relevant PTKB sentences
  - Quick check question: Why might 3-shot examples work better than 5-shot examples for this task?

- Concept: Dense vs sparse retrieval (ANCE vs BM25)
  - Why needed here: The paper compares how different retrieval methods respond to PTKB-enhanced queries
  - Quick check question: Why does BM25 perform better than ANCE in this dataset, contrary to typical results?

## Architecture Onboarding

- Component map: User query → PTKB selection module → Query reformulation LLM → Retrieval engine (BM25/ANCE) → Document ranking
- Critical path: PTKB selection → LLM reformulation → Retrieval evaluation
- Design tradeoffs: Using all PTKB vs selective PTKB, zero-shot vs few-shot prompting, BM25 vs ANCE retrieval
- Failure signatures: Retrieval performance decreases when PTKB is added, inconsistent PTKB selection across similar queries, LLM generates queries that drift from original intent
- First 3 experiments:
  1. Compare retrieval performance with no PTKB vs all PTKB vs automatically selected PTKB
  2. Test STR vs SAR strategies with varying numbers of in-context examples
  3. Evaluate human vs automatic vs LLM PTKB annotation methods on a held-out test set

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can we develop a mechanism to determine which queries require personalized information versus those that do not?
- Basis in paper: [explicit] The authors note that the best strategy is not using any PTKB information when evaluated on the whole test set, suggesting that not all queries need personalization.
- Why unresolved: The paper does not provide a method to distinguish between queries that require personalization and those that do not, leaving this as a future work item.
- What evidence would resolve it: A study that identifies features or patterns in queries that indicate the need for personalization, possibly through user behavior analysis or query context.

### Open Question 2
- Question: What are the most effective ways to model PTKB as more than just a set of sentences, incorporating user modeling perspectives?
- Basis in paper: [explicit] The authors suggest that PTKB can be exploited from a more sophisticated user modeling perspective rather than merely being seen as providing a set of sentences.
- Why unresolved: The paper focuses on selecting relevant sentences from PTKB but does not explore deeper user modeling techniques.
- What evidence would resolve it: Research that integrates PTKB with advanced user modeling techniques, such as dynamic user profiles or context-aware personalization models.

### Open Question 3
- Question: How can in-context learning be optimized to improve LLM performance in personalized query reformulation?
- Basis in paper: [explicit] The authors demonstrate that in-context learning with few-shot samples enhances LLM performance, but the optimal number and quality of samples are not fully explored.
- Why unresolved: The paper shows that three-shot achieves the best results, but does not investigate the impact of different sample qualities or strategies.
- What evidence would resolve it: Experiments comparing various in-context learning strategies, sample qualities, and quantities to determine the most effective approach for personalized query reformulation.

## Limitations

- The study relies heavily on automatic annotation methods that use retrieval metrics as proxies for PTKB usefulness, but the correlation between retrieval metric improvement and actual user satisfaction remains unclear
- The effectiveness of in-context learning with only 1-5 examples may not generalize well to more diverse query distributions or different LLM models
- The finding that PTKB doesn't always improve retrieval when used alone is specific to the dataset and evaluation methodology used

## Confidence

- High confidence: The finding that automatic annotation based on retrieval impact outperforms human judgments, as this is directly measured and consistent across experiments
- Medium confidence: The effectiveness of LLM-based strategies (STR and SAR) with in-context learning, as results depend on the quality of demonstration examples and may vary with different LLM models
- Medium confidence: The observation that PTKB doesn't always improve retrieval when used alone, as this finding is specific to the dataset and evaluation methodology used

## Next Checks

1. Test the automatic annotation approach with different retrieval metrics (beyond MRR/NDCG) to verify that improvement in these metrics correlates with actual user satisfaction and information need fulfillment
2. Conduct user studies to validate whether PTKB sentences selected by automatic annotation and LLM strategies actually improve user experience compared to human-selected sentences
3. Evaluate the approach on a different conversational dataset with varying query types and user profiles to test generalizability of the selective PTKB application strategy