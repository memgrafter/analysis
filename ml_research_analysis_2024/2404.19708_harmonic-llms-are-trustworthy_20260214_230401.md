---
ver: rpa2
title: Harmonic LLMs are Trustworthy
arxiv_id: '2404.19708'
source_url: https://arxiv.org/abs/2404.19708
tags:
- orig
- input
- llms
- gpt-4
- output
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This study introduces Harmonic Robustness, a novel model-agnostic\
  \ method for measuring LLM trustworthiness by analyzing the local deviation from\
  \ harmoniticity (\u03B3) of their responses. The method quantifies output stability\
  \ by perturbing inputs with random non-printing ASCII characters and measuring the\
  \ angle between original and averaged perturbed output embeddings."
---

# Harmonic LLMs are Trustworthy

## Quick Facts
- arXiv ID: 2404.19708
- Source URL: https://arxiv.org/abs/2404.19708
- Authors: Nicholas S. Kersting; Mohammad Rahman; Suchismitha Vedala; Yang Wang
- Reference count: 40
- Key outcome: γ → 0 strongly correlates with trustworthy LLM outputs across 10 popular models

## Executive Summary
This study introduces Harmonic Robustness, a novel model-agnostic method for measuring LLM trustworthiness by analyzing the local deviation from harmoniticity (γ) of their responses. The method quantifies output stability by perturbing inputs with random non-printing ASCII characters and measuring the angle between original and averaged perturbed output embeddings. Human annotation experiments across 10 popular LLMs and three domains demonstrate that γ → 0 strongly correlates with trustworthy answers. Low-γ leaders were GPT-4o, GPT-4, and Smaug-72B, showing mid-size open-source models can outperform large commercial ones.

## Method Summary
The method measures harmonic robustness by computing the angle between the original output embedding and the average embedding of perturbed versions of the input. Specifically, it generates N perturbed inputs by adding random non-printing ASCII characters, runs all through the LLM, embeds the outputs using ada-002, and calculates γ as the angular deviation from the mean value property. The approach is unsupervised, requiring only input questions and model responses without ground truth answers.

## Key Results
- Human annotation confirms that γ → 0 indicates trustworthiness
- Low-γ leaders: GPT-4o, GPT-4, and Smaug-72B
- Stochastic gradient ascent in γ efficiently discovers adversarial prompts
- Method works across WebQA, TruthfulQA, and ProgrammingQA domains

## Why This Works (Mechanism)

### Mechanism 1
- Claim: γ → 0 strongly correlates with trustworthy LLM outputs
- Mechanism: Harmonic robustness measures deviation from the mean value property. Adding random non-printing ASCII characters creates input perturbations. If output embeddings for all perturbed inputs are similar to the original output embedding, γ is small, indicating stable interpolation and trustworthiness.
- Core assumption: Low γ means the model is stable and interpolating smoothly in embedding space, which correlates with correctness
- Evidence anchors:
  - [abstract] "human annotation confirms that γ→0 indicates trustworthiness"
  - [section] "low γ < 0.05 was generally in the trustworthy range ~4"
  - [corpus] Human annotation results show strong negative correlation between γ and quality ratings across all models and domains
- Break condition: If model outputs semantically equivalent but syntactically varied responses, γ may be high despite correctness

### Mechanism 2
- Claim: Stochastic gradient ascent in γ efficiently discovers adversarial prompts
- Mechanism: By following the gradient of γ with respect to input perturbations, we can find inputs that cause large output variations. Random UTF-8 characters are more effective than ASCII control characters because they carry semantic weight, causing larger γ values.
- Core assumption: Large γ values indicate unstable regions in the model's input space where small perturbations cause significant output changes
- Evidence anchors:
  - [abstract] "stochastic gradient ascent in γ efficiently exposes adversarial prompts"
  - [section] Table 9 shows how following γ gradient leads to increasingly hallucinated physics content
  - [corpus] Adversarial examples become more severe with each iteration of γ ascent
- Break condition: If the model has built-in robustness to perturbations or if perturbations don't affect embedding space significantly

### Mechanism 3
- Claim: Unsupervised evaluation works because γ measures internal consistency, not ground truth
- Mechanism: γ only requires input questions and the model's responses. It measures how consistently the model responds to similar inputs, making it usable without labeled data. This enables online, automated trustworthiness assessment.
- Core assumption: Internal consistency in responses (low γ) correlates with correctness even without knowing the ground truth
- Evidence anchors:
  - [abstract] "unsupervised approach requires no ground truth, enabling automated online trustworthiness assessment"
  - [section] "only needs questions without ground-truth answers, allowing one to gauge LLM trustworthiness in an online, automated metric"
  - [corpus] γ distributions show clear separation between model qualities without using ground truth
- Break condition: If the training data itself is systematically biased, low γ could indicate consistent but incorrect responses

## Foundational Learning

- Concept: Mean value property of harmonic functions
  - Why needed here: The harmonic condition ∇²f = 0 means f(x) equals the average of f over any ball around x. This property is the mathematical foundation for measuring stability through γ.
  - Quick check question: If f(x) = x², what is the average of f over [x-1, x+1]? Does this equal f(x)? What does that tell you about harmonicity?

- Concept: Semantic embeddings and cosine similarity
  - Why needed here: We convert text outputs to vectors and measure the angle between original and perturbed output embeddings. Understanding how embeddings encode semantic similarity is crucial for interpreting γ.
  - Quick check question: If two sentences have embeddings that differ by 5° in angle, are they semantically similar? What about 45°? How does this relate to γ?

- Concept: Adversarial example generation via gradient ascent
  - Why needed here: Following γ's gradient helps find inputs that cause model instability. This connects to broader ML security concepts about finding model weaknesses.
  - Quick check question: If we want to maximize γ, should we add perturbations that increase or decrease the angle between output embeddings? How does this differ from standard adversarial attacks?

## Architecture Onboarding

- Component map: Input perturbation generator → LLM → Output embedding converter → γ calculator → (optional) Gradient ascent optimizer
- Critical path: Input text → Generate N perturbed versions → Run through LLM → Embed all outputs → Compute average perturbed embedding → Calculate angle with original → Return γ
- Design tradeoffs: N=10 perturbations balances accuracy vs. speed; ASCII control characters vs. UTF-8 for perturbation; angle vs. magnitude for γ definition
- Failure signatures: High γ with correct answers (model rephrases consistently), low γ with incorrect answers (biased training), model hangs on certain inputs
- First 3 experiments:
  1. Run Algorithm 1 on simple factual questions (e.g., "What is 2+2?") to verify γ ≈ 0 for stable answers
  2. Test perturbation effectiveness by measuring embedding angle changes with different character types
  3. Compare γ values across multiple models on the same questions to validate ranking correlation with human ratings

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the choice of random ASCII control characters (0-31) compare to other perturbation methods (like UTF-8 characters or misspellings) in terms of isotropy in embedding space and correlation with trustworthiness?
- Basis in paper: Explicit - The paper mentions this as a limitation and suggests future work.
- Why unresolved: The paper chose ASCII control characters for simplicity but acknowledges other methods could be explored. No direct comparison is provided.
- What evidence would resolve it: Experiments comparing different perturbation methods across various LLMs and domains, measuring both embedding isotropy and correlation with trustworthiness scores.

### Open Question 2
- Question: What is the optimal number of perturbations (N) for balancing computational efficiency and accuracy in the Harmonic Robustness metric?
- Basis in paper: Explicit - The paper discusses N=10 as a middle-ground but suggests this could be optimized.
- Why unresolved: The paper chose N=10 without extensive exploration of the trade-off between computational cost and accuracy.
- What evidence would resolve it: Systematic experiments varying N across different LLMs, domains, and computational resources to find the optimal balance.

### Open Question 3
- Question: How does the Harmonic Robustness metric perform on specialized or domain-specific LLMs compared to general-purpose models?
- Basis in paper: Inferred - The paper focuses on general-purpose models across three domains but doesn't explore specialized LLMs.
- Why unresolved: The study's scope was limited to popular general-purpose models, leaving open questions about performance on specialized LLMs.
- What evidence would resolve it: Testing the γ metric on domain-specific LLMs (e.g., medical, legal, scientific) and comparing performance to general-purpose models.

### Open Question 4
- Question: Can the Harmonic Robustness metric be effectively used in real-time applications, or is it primarily suited for offline evaluation?
- Basis in paper: Explicit - The paper mentions the potential for real-time usage but doesn't provide concrete implementation details.
- Why unresolved: While the paper suggests real-time applicability, it doesn't demonstrate or analyze the practicality of this in actual use cases.
- What evidence would resolve it: Implementation and testing of the γ metric in a real-time LLM application, measuring latency and impact on user experience.

### Open Question 5
- Question: How does the Harmonic Robustness metric correlate with other established trustworthiness or robustness measures for LLMs?
- Basis in paper: Inferred - The paper introduces γ as a novel metric but doesn't compare it to existing measures.
- Why unresolved: The study focuses on demonstrating γ's effectiveness without benchmarking against other established metrics.
- What evidence would resolve it: Comparative studies measuring correlation between γ and other trustworthiness/robustness metrics across various LLMs and evaluation scenarios.

## Limitations
- Method's effectiveness depends heavily on perturbation quality and embedding isotropy assumptions
- Correlation between γ and trustworthiness lacks theoretical grounding explaining why harmonic functions predict semantic correctness
- May struggle with models that consistently rephrase correct answers or those trained on systematically biased data

## Confidence
- **High confidence**: γ measures output stability through embedding angle deviation. The mathematical formulation and computational implementation are straightforward and verifiable.
- **Medium confidence**: γ → 0 correlates with trustworthy outputs across diverse models and domains. While human annotation results are compelling, the relationship may not generalize to all question types or model architectures.
- **Low confidence**: Stochastic gradient ascent in γ efficiently discovers adversarial prompts. The method shows promise but may not find all adversarial examples, particularly those requiring semantic understanding beyond simple perturbations.

## Next Checks
1. **Perturbation space analysis**: Test γ sensitivity to different perturbation types (Unicode, word substitutions, syntactic variations) to determine if ASCII control characters are truly optimal or if richer perturbations provide better discrimination.

2. **Theoretical grounding study**: Analyze whether harmonic function properties actually predict semantic correctness by testing on synthetic datasets where ground truth is known and examining cases where low γ coincides with incorrect answers.

3. **Cross-domain robustness test**: Apply the method to domains outside the original three (e.g., medical advice, legal reasoning) to verify that the γ-trustworthiness correlation holds when models encounter specialized knowledge requiring expert judgment.