---
ver: rpa2
title: Towards More Accurate Prediction of Human Empathy and Emotion in Text and Multi-turn
  Conversations by Combining Advanced NLP, Transformers-based Networks, and Linguistic
  Methodologies
arxiv_id: '2407.18496'
source_url: https://arxiv.org/abs/2407.18496
tags:
- task
- empathy
- values
- training
- distress
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This research addresses empathy and emotion prediction in text
  and multi-turn conversations using advanced NLP techniques. The core approach combines
  transformer-based embeddings, neural networks with advanced dropout and GELU activation,
  stratified sampling, and lexicon-based features.
---

# Towards More Accurate Prediction of Human Empathy and Emotion in Text and Multi-turn Conversations by Combining Advanced NLP, Transformers-based Networks, and Linguistic Methodologies

## Quick Facts
- arXiv ID: 2407.18496
- Source URL: https://arxiv.org/abs/2407.18496
- Reference count: 0
- Primary result: Ensemble model achieved 0.521 Pearson correlation on WASSA 2022 test data (33.59% improvement over baseline) and 0.702 on WASSA 2023 test data (64.02% improvement over baseline)

## Executive Summary
This research addresses empathy and emotion prediction in text and multi-turn conversations using advanced NLP techniques. The core approach combines transformer-based embeddings, neural networks with advanced dropout and GELU activation, stratified sampling, and lexicon-based features. For the primary task (WASSA 2022), an ensemble model achieved a Pearson correlation of 0.521 on test data, improving 33.59% over baseline. For the adaptation task (WASSA 2023), the system scored 0.702 on test data, a 64.02% improvement over baseline. Key innovations include addressing class imbalance through stratified sampling and enriching features with four lexical resources (NRC emotion lexicon, MPQA subjectivity lexicon, NRC VAD lexicon, and verbal polarity shifters). The system successfully adapted from essay-level to conversation-level prediction while maintaining stable training without overfitting.

## Method Summary
The method involves preprocessing WASSA datasets with BPE tokenization, generating embeddings using text-embedding-ada-002, creating 48 additional lexicon features from four word-level resources, and training feed-forward neural networks with advanced dropout and GELU activation. The approach employs stratified sampling to address class imbalance and uses ensemble methods combining FNN predictions with two SVR models (polynomial and RBF kernels). The system adapts from essay-level to conversation-level prediction by applying the same methodology to different data structures while maintaining performance improvements through feature enrichment and careful hyperparameter tuning.

## Key Results
- Achieved 0.521 Pearson correlation on WASSA 2022 test data, representing 33.59% improvement over baseline
- Scored 0.702 Pearson correlation on WASSA 2023 test data, representing 64.02% improvement over baseline
- Successfully adapted from essay-level to conversation-level prediction while maintaining stable training without overfitting

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The ensemble averaging of predictions from three different models improves robustness and reduces overfitting compared to a single model.
- Mechanism: By combining predictions from a feed-forward neural network, a polynomial kernel SVR, and an RBF kernel SVR, the ensemble captures different aspects of the data distribution. Each model has different strengths and weaknesses, and averaging them reduces the impact of individual model errors.
- Core assumption: The individual models make uncorrelated errors, so averaging reduces overall error.
- Evidence anchors:
  - [section]: "The primary task system for the final stage is a small ensemble consisting of our model from the second stage and two Support Vector Regression (SVR) models from the svm module of Scikit-learn. Each ensemble prediction is the average of the predictions of the three models."
  - [abstract]: "The system successfully adapted from essay-level to conversation-level prediction while maintaining stable training without overfitting."

### Mechanism 2
- Claim: Using stratified sampling to address class imbalance improves model performance by ensuring the training and validation sets have the same distribution as the original dataset.
- Mechanism: Stratified sampling preserves the proportion of each target class in the training and validation sets, preventing the model from being biased towards the majority class. This leads to better generalization and more accurate predictions across all classes.
- Core assumption: The original dataset distribution is representative of the true underlying distribution.
- Evidence anchors:
  - [section]: "Stratified Sampling approach has been used to address the class imbalance. This method of sampling preserves the same distribution of each target class in the training and validation sets as in the original dataset."
  - [abstract]: "Key innovations include addressing class imbalance through stratified sampling and enriching features with four lexical resources."

### Mechanism 3
- Claim: Enriching features with lexical resources improves model performance by providing additional semantic and contextual information.
- Mechanism: The four lexical resources (NRC emotion lexicon, MPQA subjectivity lexicon, NRC VAD lexicon, and verbal polarity shifters) provide information about word-level emotions, subjectivity, valence/arousal/dominance, and polarity shifts. These features capture nuances in language that may not be captured by the embedding models alone.
- Core assumption: The lexical resources are accurate and relevant to the task.
- Evidence anchors:
  - [section]: "For the revised system presented in the second stage, we have created 48 additional features based on 4 different word-level lexicons."
  - [abstract]: "Key innovations include addressing class imbalance through stratified sampling and enriching features with four lexical resources (NRC emotion lexicon, MPQA subjectivity lexicon, NRC VAD lexicon, and verbal polarity shifters)."

## Foundational Learning

- Concept: Transformer-based embeddings
  - Why needed here: Transformer-based embeddings capture complex semantic relationships in text that are crucial for understanding empathy and emotion.
  - Quick check question: What is the difference between a transformer-based embedding and a traditional word embedding like Word2Vec?

- Concept: Class imbalance and stratified sampling
  - Why needed here: Class imbalance can lead to biased models that perform poorly on minority classes. Stratified sampling ensures that the training and validation sets have the same distribution as the original dataset.
  - Quick check question: What is the difference between stratified sampling and random sampling?

- Concept: Ensemble methods
  - Why needed here: Ensemble methods combine predictions from multiple models to improve robustness and reduce overfitting.
  - Quick check question: What are the advantages and disadvantages of using an ensemble method compared to a single model?

## Architecture Onboarding

- Component map: Data loading and preprocessing -> BPE tokenization -> Embedding generation (text-embedding-ada-002) -> Lexicon feature extraction (NRC, MPQA, VAD, polarity shifters) -> Model training (FNN with dropout, GELU activation) -> Ensemble prediction (FNN + SVR-polynomial + SVR-RBF) -> Evaluation (Pearson correlation)
- Critical path: Data loading -> preprocessing -> embedding generation -> feature extraction -> model prediction
- Design tradeoffs: The use of ensemble methods increases computational cost but improves robustness. The use of lexical resources increases feature dimensionality but provides additional semantic information. The use of stratified sampling ensures balanced training data but may reduce the size of the training set.
- Failure signatures: Overfitting (high training accuracy, low validation accuracy), underfitting (low training and validation accuracy), class imbalance (biased predictions towards majority class), and poor feature extraction (inaccurate or irrelevant features).
- First 3 experiments:
  1. Train a single model (e.g., FNN) on the training data and evaluate its performance on the validation data.
  2. Implement stratified sampling and compare the performance of the model on the validation data.
  3. Add lexical features and compare the performance of the model on the validation data.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the model performance change when using conversation-level embeddings instead of essay-level embeddings for the adaptation task?
- Basis in paper: [explicit] The paper discusses adapting the model from essay-level to conversation-level prediction but doesn't compare performance using different embedding approaches specifically for conversations.
- Why unresolved: The authors used the same embedding approach (text-embedding-ada-002) for both essay and conversation tasks, but didn't explore whether conversation-specific embeddings might yield better results.
- What evidence would resolve it: Direct comparison of model performance using both essay-level and conversation-level embeddings on the same adaptation task dataset.

### Open Question 2
- Question: What is the impact of using different dropout rates and activation functions on the model's ability to generalize across different datasets?
- Basis in paper: [explicit] The authors discuss hyperparameter tuning including dropout rates and GELU activation function, but don't explore their impact on generalization across datasets.
- Why unresolved: The paper shows improved performance with these changes but doesn't investigate whether these improvements generalize to other datasets or tasks beyond the WASSA tasks.
- What evidence would resolve it: Testing the model with different dropout rates and activation functions on multiple, diverse datasets to measure generalization performance.

### Open Question 3
- Question: How do the lexicon features contribute to the model's performance, and can their contribution be quantified?
- Basis in paper: [explicit] The authors mention adding lexicon features and their positive effect on performance, but don't quantify the individual contribution of each lexicon.
- Why unresolved: While the paper states that lexicon features improve performance, it doesn't provide a detailed analysis of how much each lexicon contributes to the overall improvement.
- What evidence would resolve it: Ablation studies removing each lexicon feature individually and measuring the impact on model performance to quantify each lexicon's contribution.

## Limitations

- Limited generalizability beyond WASSA datasets, with effectiveness on other domains or languages remaining untested
- Lexicon resource dependency and currency concerns, as the resources may not capture contemporary language usage or domain-specific terminology
- Unexplained mechanism of ensemble success, lacking ablation studies to quantify individual component contributions

## Confidence

- High confidence: Technical implementation details of transformer embeddings, stratified sampling, and lexicon feature extraction are well-documented and follow established NLP practices
- Medium confidence: Reported Pearson correlation improvements are credible based on the methodology described, though exact reproducibility cannot be confirmed without access to test sets
- Low confidence: Claim that this approach "successfully adapted from essay-level to conversation-level prediction" lacks comparative analysis showing why adaptation was challenging or what specific modifications were required

## Next Checks

1. **Ablation study of ensemble components**: Remove each ensemble member (FNN, SVR-polynomial, SVR-RBF) individually and measure performance degradation to quantify each component's contribution and determine if the ensemble is essential.

2. **Cross-domain generalization test**: Apply the trained models to empathy prediction tasks from different domains (e.g., clinical conversations, customer service interactions) to assess whether the improvements generalize beyond WASSA datasets.

3. **Lexicon resource validation**: Conduct an error analysis comparing model predictions with and without lexicon features on a held-out validation set, specifically examining cases where lexicon features may introduce noise or incorrect information for domain-specific language.