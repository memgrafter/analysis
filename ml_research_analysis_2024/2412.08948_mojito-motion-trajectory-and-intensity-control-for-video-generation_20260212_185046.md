---
ver: rpa2
title: 'Mojito: Motion Trajectory and Intensity Control for Video Generation'
arxiv_id: '2412.08948'
source_url: https://arxiv.org/abs/2412.08948
tags:
- motion
- video
- intensity
- control
- generation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Mojito introduces a diffusion-based video generation framework
  that integrates both trajectory and intensity control for text-to-video generation.
  It features a training-free Directional Motion Control (DMC) module using cross-attention
  to guide object motion along specified trajectories without additional training,
  and a Motion Intensity Modulator (MIM) that uses optical flow maps to control motion
  intensity levels.
---

# Mojito: Motion Trajectory and Intensity Control for Video Generation

## Quick Facts
- arXiv ID: 2412.08948
- Source URL: https://arxiv.org/abs/2412.08948
- Authors: Xuehai He; Shuohang Wang; Jianwei Yang; Xiaoxia Wu; Yiping Wang; Kuan Wang; Zheng Zhan; Olatunji Ruwase; Yelong Shen; Xin Eric Wang
- Reference count: 37
- Primary result: Training-free diffusion framework with directional motion control (DMC) and motion intensity modulation (MIM) achieving precise trajectory and intensity control in video generation

## Executive Summary
Mojito introduces a novel diffusion-based video generation framework that achieves precise motion trajectory and intensity control without additional training. The framework features two key innovations: a Directional Motion Control (DMC) module that modifies cross-attention maps during inference to guide object motion along specified trajectories, and a Motion Intensity Modulator (MIM) that conditions generation on optical flow-based motion intensity embeddings. Extensive experiments demonstrate that Mojito generates motion patterns closely matching specified directions and intensities while maintaining realistic dynamics and computational efficiency.

## Method Summary
Mojito employs a Conv-Spatial-Temporal Transformer architecture that processes video frames through spatial and temporal transformers. The DMC module dynamically modifies cross-attention during inference to steer attention toward user-defined bounding boxes across frames, enabling training-free trajectory control. The MIM module computes optical flow maps from training videos, normalizes them into discrete intensity levels, and embeds these as word-form representations that condition the diffusion process. A temporal smoothness regularization term ensures coherent motion across frames by penalizing large changes in attention maps between consecutive frames.

## Key Results
- Achieves precise trajectory control without additional training through cross-attention modification
- Controls motion intensity levels using optical flow-based embeddings with high correlation to specified intensities
- Generates videos with realistic dynamics aligned with natural motion while maintaining computational efficiency

## Why This Works (Mechanism)

### Mechanism 1: Training-free Directional Motion Control
Mojito achieves training-free directional motion control by modifying cross-attention maps during inference. The DMC module defines an energy function that encourages attention scores for specified object tokens to concentrate within user-defined bounding boxes across frames. During sampling, gradients of this energy function update latent variables, steering attention toward the desired trajectory without retraining the model.

### Mechanism 2: Motion Intensity Control via Optical Flow
The MIM module controls motion intensity by conditioning the diffusion process on optical flow-based motion embeddings. It computes optical flow maps from training videos, normalizes them into discrete intensity levels, and converts these into word-form embeddings. These embeddings combine with text embeddings and feed into cross-attention layers, conditioning the model to generate videos with specified motion intensity.

### Mechanism 3: Temporal Smoothness Regularization
Temporal smoothness is enforced through a regularization term that penalizes large changes in attention maps between consecutive frames. This term is added to the energy function optimized during inference, ensuring coherent motion across the video sequence by computing the squared difference between attention maps of adjacent frames.

## Foundational Learning

- **Cross-attention in diffusion models**: Why needed - Cross-attention layers are the mechanism through which Mojito guides object motion along specified trajectories and intensities without retraining. Quick check - How does modifying cross-attention maps during inference influence the final generated video content?

- **Optical flow computation**: Why needed - Optical flow provides the quantitative basis for motion intensity control, capturing pixel-wise motion direction and magnitude between frames. Quick check - What information does optical flow capture, and how is it used to represent motion intensity in Mojito?

- **Temporal coherence in video generation**: Why needed - Maintaining temporal consistency across frames is crucial for generating realistic videos with smooth motion. Quick check - Why is temporal coherence important in video generation, and how does Mojito enforce it?

## Architecture Onboarding

- **Component map**: Input text prompt → CLIP text encoder → cross-attention layers → DMC/MIM conditioning → denoising U-Net → VAE decoder → generated video

- **Critical path**: Input text prompt → CLIP text encoder → cross-attention layers → DMC/MIM conditioning → denoising U-Net → VAE decoder → generated video

- **Design tradeoffs**: Training-free DMC vs. training-based methods offers flexibility but may be less precise; optical flow discretization simplifies intensity representation but may lose fine-grained details; temporal smoothness regularization improves coherence but may constrain natural motion if too strong.

- **Failure signatures**: DMC failure - objects not following specified trajectories; MIM failure - incorrect motion intensity; Temporal coherence issues - flickering or inconsistent motion across frames.

- **First 3 experiments**: 1) Test DMC with simple trajectories to verify basic functionality; 2) Evaluate MIM by generating videos with varying intensity levels and comparing optical flow statistics; 3) Assess temporal coherence by generating videos with and without the temporal smoothness term and measuring attention map consistency across frames.

## Open Questions the Paper Calls Out

The paper does not explicitly call out open questions. However, based on the methodology and results presented, several implicit questions emerge regarding the framework's behavior in scenarios not fully explored in the paper.

## Limitations

- Training-free trajectory control precision may be less effective than training-based approaches for complex or long-range motion patterns.
- Optical flow discretization may oversimplify motion intensity representation, potentially losing fine-grained motion details.
- Computational overhead introduced by directional guidance mechanisms has not been thoroughly benchmarked.

## Confidence

- High confidence: Core architectural framework (VAE + Conv-Spatial-Temporal Transformer) and general approach to motion intensity control via optical flow.
- Medium confidence: Training-free DMC mechanism's effectiveness compared to training-based alternatives.
- Medium confidence: Temporal smoothness regularization's quantitative impact across diverse motion scenarios.

## Next Checks

1. **DMC Precision Test**: Generate videos with complex trajectories and quantitatively measure trajectory adherence using trajectory prediction metrics or object tracking algorithms.

2. **MIM Fidelity Analysis**: Create a controlled test set with videos of known motion intensities and validate whether MIM's intensity control produces statistically significant differences in generated motion patterns using optical flow analysis on outputs.

3. **Computational Overhead Benchmark**: Measure and compare inference times for Mojito against baseline diffusion models across different hardware configurations, specifically isolating the additional time cost introduced by DMC and MIM modules.