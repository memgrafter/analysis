---
ver: rpa2
title: Inference and Verbalization Functions During In-Context Learning
arxiv_id: '2410.09349'
source_url: https://arxiv.org/abs/2410.09349
tags:
- label
- words
- single-token
- flipped
- function
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This study investigates how large language models perform in-context\
  \ learning (ICL) with irrelevant or misleading label words by proposing that models\
  \ use two sequential functions: an inference function that derives a representation\
  \ of the answer, followed by a verbalization function that maps this representation\
  \ to the output label space. The key hypothesis is that the inference function is\
  \ invariant to remappings of the label space (e.g., \u201Ctrue\u201D/\u201Cfalse\u201D\
  \ to \u201Ccat\u201D/\u201Cdog\u201D)."
---

# Inference and Verbalization Functions During In-Context Learning

## Quick Facts
- **arXiv ID:** 2410.09349
- **Source URL:** https://arxiv.org/abs/2410.09349
- **Reference count:** 40
- **Primary result:** Layer-wise interchange interventions reveal sequential inference and verbalization functions in ICL, with high flip rates across multiple models and tasks

## Executive Summary
This study investigates how large language models perform in-context learning (ICL) when presented with irrelevant or misleading label words. The researchers propose a two-function mechanism: an inference function that derives a representation of the answer, followed by a verbalization function that maps this representation to the output label space. Through controlled layer-wise interchange intervention experiments, they demonstrate that the inference function is invariant to label space remappings (e.g., "true"/"false" to "cat"/"dog"), while the verbalization function adapts to the new label space. The findings are validated across multiple tasks (MultiNLI, RTE, ANLI, IMDb, AGNews) and model families (GEMMA-7B, MISTRAL-7B-V0.3, GEMMA-2-27B, LLAMA-3.1-70B).

## Method Summary
The researchers employed layer-wise interchange intervention experiments to test their hypothesis about sequential inference and verbalization functions during ICL. They trained models on tasks with different label words (e.g., "true"/"false" vs. "cat"/"dog") and then swapped representations between these models at various layers. High flip rates (70-90%) to counterfactual outputs were observed when interventions were performed on middle layers, confirming that the inference function is invariant to label remapping. The experiments were conducted across multiple model families and task types to ensure generalizability. The approach allowed for causal analysis of how representations are processed through different layers and how they map to output labels.

## Key Results
- Layer-wise interchange interventions achieved 70-90% flip rates to counterfactual outputs, validating the inference function's invariance to label remapping
- The two functions (inference and verbalization) are localized in consistent layers across different tasks and model families
- The mechanism operates consistently across GEMMA-7B, MISTRAL-7B-V0.3, GEMMA-2-27B, and LLAMA-3.1-70B models on tasks including MultiNLI, RTE, ANLI, IMDb, and AGNews

## Why This Works (Mechanism)
The mechanism works because the inference function operates on task-relevant features independently of the specific label words used, while the verbalization function maps these representations to the output space defined by the given labels. This separation allows the model to maintain ICL performance even when label words are irrelevant or misleading, as the core reasoning about the task remains intact. The interchange intervention results demonstrate that swapping representations between models trained on different label mappings can produce counterfactual outputs, confirming the modular nature of these functions and their sequential processing through the network.

## Foundational Learning
- **In-context learning (ICL):** The ability of language models to learn tasks from examples provided in the prompt, without parameter updates - needed because the study examines how models adapt to new label spaces during ICL
- **Layer-wise interchange intervention:** A causal analysis technique where representations are swapped between models at specific layers to test function localization - needed to empirically validate the sequential function hypothesis
- **Label space remapping:** The process of changing which words represent which categories (e.g., "true"/"false" to "cat"/"dog") - needed to test whether the inference function is invariant to label representations
- **Counterfactual outputs:** Outputs that would have been produced if the model had seen different label words during training - needed to measure the effectiveness of the intervention experiments
- **Function localization:** Identifying which layers in the network perform specific computational functions - needed to map where inference and verbalization occur in the model architecture
- **Flip rate:** The proportion of cases where an intervention successfully changes the output to a counterfactual result - needed to quantify the strength of evidence for the hypothesized mechanism

## Architecture Onboarding

**Component Map:** Input -> Middle Layers (Inference Function) -> Later Layers (Verbalization Function) -> Output

**Critical Path:** The critical path for ICL involves the middle layers where the inference function operates, extracting task-relevant representations that are then mapped to output labels by the verbalization function in later layers.

**Design Tradeoffs:** The model trades computational efficiency for modularity, with distinct inference and verbalization functions that can be independently manipulated. This separation enables robustness to label remapping but may limit flexibility in cases where label semantics are deeply integrated with reasoning.

**Failure Signatures:** The model would fail to maintain ICL performance when the inference function cannot extract task-relevant features (e.g., when label words contain critical semantic information for the task) or when the verbalization function cannot map representations to the output space (e.g., when label words are completely unrelated to the task domain).

**First 3 Experiments:** 1) Test the mechanism with partial label remappings where some labels are semantically related, 2) Conduct ablation studies where the verbalization function is selectively disrupted, 3) Investigate whether the same mechanism operates in smaller language models below 7B parameters

## Open Questions the Paper Calls Out
None

## Limitations
- Results may not generalize to all possible label remapping scenarios or to models outside the 7B-70B parameter range
- The study focuses on controlled, artificial label remapping experiments rather than naturally occurring label distributions, limiting ecological validity
- While function localization is consistent across tested conditions, the precise neural mechanisms and whether these functions are strictly modular or more distributed remains unclear

## Confidence
- **High**: The existence of sequential inference and verbalization functions in ICL, supported by consistent layer-wise intervention results across multiple models and tasks
- **Medium**: The exact layer localization of these functions and their invariance to label remapping, as this could vary with different architectural choices or training regimes
- **Medium**: The causal relationship between the hypothesized mechanism and ICL performance, as alternative explanations for the observed intervention effects cannot be fully ruled out

## Next Checks
1. Test the hypothesis with more diverse label remapping scenarios, including partial remappings and semantically related but task-incongruent labels, to assess the robustness of the inference function's invariance
2. Conduct ablation studies where the verbalization function is selectively disrupted to determine whether the inference function alone can support ICL performance
3. Investigate whether the same sequential function mechanism operates in smaller language models (below 7B parameters) and in models trained with different objectives or datasets