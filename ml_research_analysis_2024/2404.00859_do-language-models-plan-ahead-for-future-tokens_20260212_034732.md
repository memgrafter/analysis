---
ver: rpa2
title: Do language models plan ahead for future tokens?
arxiv_id: '2404.00859'
source_url: https://arxiv.org/abs/2404.00859
tags:
- myopic
- future
- transformer
- vanilla
- myopia
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper investigates whether transformers "think ahead" during
  inference, exploring two hypotheses: pre-caching (deliberately computing features
  for future tokens) and breadcrumbs (computing features useful for current tokens
  that happen to benefit future tokens). The authors introduce myopic training, where
  gradients from future tokens are not propagated to past positions, to test these
  hypotheses.'
---

# Do language models plan ahead for future tokens?

## Quick Facts
- arXiv ID: 2404.00859
- Source URL: https://arxiv.org/abs/2404.00859
- Authors: Wilson Wu; John X. Morris; Lionel Levine
- Reference count: 40
- Key outcome: Myopic training reveals that while small language models primarily use "breadcrumbs" (features useful for current prediction that happen to benefit future predictions), larger models increasingly employ "pre-caching" (deliberately computing features for future tokens).

## Executive Summary
This paper investigates whether transformers "think ahead" during inference by precomputing features for future tokens. The authors introduce myopic training, where gradients from future tokens are not propagated to past positions, to distinguish between two hypotheses: pre-caching (deliberate future planning) and breadcrumbs (present-focused computation that incidentally helps future predictions). In synthetic data requiring pre-caching, transformers learn to precompute future-relevant features. However, in natural language modeling with small models like GPT-2, experiments suggest breadcrumbs dominate, though pre-caching increases with model scale. The myopia gap (performance difference between vanilla and myopic models) is small for natural language but increases with model size.

## Method Summary
The authors develop myopic training by modifying standard transformer training to block gradient propagation from position j to parameters θ_i where i < j. They create a synthetic dataset (Dp task) with clear pre-caching requirements by sampling from a distribution involving sin(bxn) terms. They train both vanilla and myopic models on this synthetic data and natural language datasets (MS MARCO, The Pile) using GPT-2 and Pythia architectures. Performance is measured by comparing cross-entropy loss between models (myopia gap), with additional linear probing to detect pre-cached features in hidden states.

## Key Results
- Large myopia gaps in synthetic Dp tasks confirm transformers learn to precompute future-relevant features when necessary
- Small myopia gaps (0.01-0.07 nats) in natural language experiments suggest breadcrumbs dominate in small models
- Myopia gap increases with model scale, indicating larger models engage in more pre-caching behavior
- Pre-caching features can be detected through linear probing of hidden states

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Myopic training removes future-token gradients from past position updates
- Mechanism: During training, gradients from position j are not propagated back to update parameters θ_i where i < j, forcing each position to optimize only for its immediate next-token prediction
- Core assumption: The attention mechanism's dependence on past hidden states is primarily mediated through current parameters rather than being heavily influenced by past parameters
- Evidence anchors:
  - [abstract] "myopic training, where gradients from future tokens are not propagated to past positions"
  - [section 3.4] "myopic descent...without propagating gradients from the loss at the current position to hidden states from previous positions"
  - [corpus] Weak evidence - the corpus papers discuss planning but don't directly address the gradient isolation mechanism

### Mechanism 2
- Claim: Pre-caching vs breadcrumbs hypotheses explain why hidden states contain future information
- Mechanism: Pre-caching occurs when off-diagonal gradient terms cause models to compute features at position t that benefit future positions but aren't useful for current prediction; breadcrumbs occur when features most useful for current prediction happen to also benefit future predictions
- Core assumption: The presence of future-relevant information in hidden states can be distinguished by whether myopic models perform worse than vanilla models
- Evidence anchors:
  - [abstract] "pre-caching...features at t irrelevant to the present inference task but useful for the future...breadcrumbs...features most relevant to time step t are already the same as those that would most benefit inference at time t+τ"
  - [section 3] "We posit two explanations...pre-caching...breadcrumbs"
  - [corpus] Moderate evidence - "Transformers Can Navigate Mazes With Multi-Step Prediction" suggests transformers can plan ahead, supporting the idea that hidden states might contain future-relevant information

### Mechanism 3
- Claim: Model scale affects the prevalence of pre-caching behavior
- Mechanism: Larger models develop more sophisticated computation strategies that include deliberate future planning, while smaller models rely primarily on breadcrumbs
- Core assumption: The myopia gap increases with model scale because larger models have more capacity to compute and store future-relevant features
- Evidence anchors:
  - [abstract] "pre-caching increases with model scale...larger models are 'planning for the future' in a way that small models cannot"
  - [section 5.2] "the importance of pre-caching increases with scale, becoming non-negligible with larger models"
  - [corpus] Moderate evidence - "Scaling Recurrent Neural Networks to a Billion Parameters" suggests larger models develop different computational properties

## Foundational Learning

- Concept: Gradient propagation and backpropagation through time
  - Why needed here: Understanding how gradients flow from future positions to past parameters is crucial for grasping myopic training's mechanism
  - Quick check question: If we have a sequence of 4 tokens and compute loss at position 4, which parameters receive gradients from this loss in vanilla training vs myopic training?

- Concept: Attention mechanism and hidden state dependencies
  - Why needed here: The attention mechanism determines how past hidden states influence current computation, which affects whether myopic training successfully isolates present-focused computation
  - Quick check question: In a transformer with tied weights, does position i's forward pass depend more on parameters θ_i or on parameters θ_j for j < i?

- Concept: Forward bias and strong convexity in optimization
  - Why needed here: These mathematical properties ensure myopic descent converges to a solution that truly optimizes only for present prediction
  - Quick check question: Why does forward bias (H_y,y f + H_x,y f > 0) matter for myopic training to work correctly?

## Architecture Onboarding

- Component map:
  - Base transformer architecture with standard components (embedding, attention heads, feed-forward layers, unembedding)
  - Myopic attention mechanism: modified attention that uses separate key/value states for past positions vs current position
  - Training pipeline with two modes: vanilla (standard) and myopic (future gradients blocked)
  - Evaluation framework measuring myopia gap and local myopia bonus

- Critical path:
  1. Implement myopic attention mechanism that separates past vs current key/value states
  2. Modify training loop to block gradient propagation from position j to parameters θ_i where i < j
  3. Train both vanilla and myopic models on same dataset
  4. Measure performance difference (myopia gap) and local myopia bonus
  5. Analyze which features/models exhibit pre-caching vs breadcrumbs

- Design tradeoffs:
  - Computational overhead: Myopic training requires maintaining separate key/value states for past positions
  - Convergence stability: Forward bias assumption may not hold perfectly in all transformer architectures
  - Interpretability vs performance: Myopic models may be more interpretable but potentially less performant

- Failure signatures:
  - No myopia gap observed: Could indicate either no pre-caching occurs, or the forward-bias assumption is violated
  - Myopic models perform significantly worse: May indicate the dataset/task inherently requires pre-caching
  - Training instability: Could suggest the myopic gradient blocking interferes with necessary parameter updates

- First 3 experiments:
  1. Implement myopic attention mechanism on a small GPT-2 variant and verify it produces identical outputs to vanilla attention when past and current key/value states are the same
  2. Train myopic and vanilla models on the synthetic D_p task and verify the large myopia gap observed in the paper
  3. Measure the myopia gap on a natural language dataset (like MS MARCO) with GPT-2 to verify the small gap finding

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the myopia gap increase further for language models larger than Pythia 2.8B?
- Basis in paper: [explicit] The paper shows myopia gap increasing with model scale up to Pythia 2.8B, but doesn't explore larger models.
- Why unresolved: The paper only tested models up to 2.8B parameters, leaving open whether the trend continues for larger models.
- What evidence would resolve it: Testing myopic training on larger models (e.g., 10B+ parameters) and measuring the resulting myopia gap.

### Open Question 2
- Question: What specific architectural features enable larger models to develop more pre-caching capability?
- Basis in paper: [inferred] The paper shows pre-caching increases with model scale but doesn't analyze architectural differences.
- Why unresolved: While the paper demonstrates pre-caching scales with model size, it doesn't investigate which architectural components (depth, width, attention mechanisms) contribute to this scaling.
- What evidence would resolve it: Systematic ablation studies varying architectural parameters while measuring pre-caching capacity across different model scales.

### Open Question 3
- Question: How does myopic training affect interpretability and safety of language models?
- Basis in paper: [explicit] The paper suggests myopic transformers may have advantages for safety and interpretability but doesn't test this claim.
- Why unresolved: The authors propose potential applications of myopic training for interpretability but don't empirically investigate these claims.
- What evidence would resolve it: Comparative studies of interpretability techniques and safety evaluations between vanilla and myopic trained models.

### Open Question 4
- Question: Are there specific types of tasks or data distributions where breadcrumbs are more or less effective than pre-caching?
- Basis in paper: [inferred] The paper shows different results between synthetic and natural language tasks but doesn't systematically explore task-dependent differences.
- Why unresolved: The paper demonstrates task-dependent behavior but doesn't characterize which properties of tasks favor breadcrumbs vs pre-caching.
- What evidence would resolve it: Experiments comparing performance across diverse task families with varying temporal dependencies and computational requirements.

## Limitations

- Synthetic vs Natural Data Gap: The clearest evidence for pre-caching comes from synthetic data where pre-caching is strictly necessary, while natural language results show only small myopia gaps, raising questions about generalizability.
- Forward-Bias Assumption: Myopic training relies on the assumption that attention heads primarily depend on past hidden states rather than past parameters, which may not hold in all architectures.
- Scale Effects Ambiguity: While pre-caching increases with model scale, it's unclear whether larger models are actually planning ahead or simply have more capacity for both breadcrumbs and pre-caching.

## Confidence

- Pre-caching occurs in synthetic data: **High**
- Breadcrumbs dominate in small natural language models: **High**
- Pre-caching increases with model scale: **Medium**

## Next Checks

1. **Gradient Flow Verification**: Instrument the myopic training implementation to verify that gradients from position j truly do not flow to parameters θ_i where i < j by checking that the gradient accumulator for past parameters remains unchanged during forward passes that only depend on future positions.

2. **Forward-Bias Assumption Test**: Design an ablation study that systematically varies the dependence of attention heads on past parameters vs past hidden states, comparing models with untied vs tied parameters or architectures with different parameter-to-hidden-state relationships.

3. **Scale Continuum Analysis**: Train a continuous spectrum of models at different scales (not just discrete sizes) and measure how the myopia gap changes as a function of parameter count to reveal whether there's a sharp transition to pre-caching behavior or a gradual increase.