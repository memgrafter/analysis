---
ver: rpa2
title: 'ROER: Regularized Optimal Experience Replay'
arxiv_id: '2407.03995'
source_url: https://arxiv.org/abs/2407.03995
tags:
- clip
- value
- replay
- learning
- steps
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a new perspective on TD-error-based prioritization
  by connecting it to occupancy optimization. The authors derive a dual form of a
  regularized RL objective with an f-divergence regularizer, showing that an optimal
  solution is obtained by shifting the off-policy data distribution towards the on-policy
  optimal distribution using TD-error-based occupancy ratios.
---

# ROER: Regularized Optimal Experience Replay

## Quick Facts
- arXiv ID: 2407.03995
- Source URL: https://arxiv.org/abs/2407.03995
- Reference count: 40
- Primary result: ROER outperforms baselines in 6 out of 11 MuJoCo and DM Control tasks when combined with SAC

## Executive Summary
This paper proposes a new perspective on TD-error-based prioritization by connecting it to occupancy optimization. The authors derive a dual form of a regularized RL objective with an f-divergence regularizer, showing that an optimal solution is obtained by shifting the off-policy data distribution towards the on-policy optimal distribution using TD-error-based occupancy ratios. Specifically, using KL-divergence as the regularizer, they obtain a new prioritization scheme called regularized optimal experience replay (ROER). Experiments with Soft Actor-Critic on MuJoCo and DM Control tasks show that ROER outperforms baselines in 6 out of 11 tasks while matching or not deviating far from baselines in the rest. ROER also shows significant improvement on the difficult Antmaze environment when using pretraining, demonstrating its applicability to offline-to-online fine-tuning.

## Method Summary
ROER introduces a new prioritization scheme for experience replay by deriving TD-error-based occupancy ratios from a regularized RL objective with f-divergence regularizer. The method uses a separate value network trained with ExtremeV loss to compute TD errors, which are then used to calculate priorities via exponential weighting. A convergence parameter λ controls the stability of priority updates. The approach is integrated with SAC and evaluated on continuous control benchmarks, showing improved performance over uniform sampling and prioritized experience replay baselines.

## Key Results
- ROER outperforms baselines in 6 out of 11 MuJoCo and DM Control tasks
- Matches or closely matches baseline performance in the remaining 5 tasks
- Shows significant improvement on Antmaze environment with pretraining
- Maintains stability across different temperature (β) settings while improving performance

## Why This Works (Mechanism)

### Mechanism 1
- Claim: TD-error-based prioritization indirectly optimizes the distribution shift between off-policy replay buffer data and the current policy's on-policy distribution.
- Mechanism: The regularized objective with f-divergence regularizer encourages shifting the replay buffer distribution toward the optimal on-policy distribution. This is achieved by weighting experiences using TD-error-based occupancy ratios derived from the dual of the regularized RL objective.
- Core assumption: The f-divergence penalty (e.g., KL divergence) provides a stable and meaningful penalty that prevents over-prioritization of out-of-distribution experiences.
- Evidence anchors:
  - [abstract] "We show that an optimal solution to the objective is obtained by shifting the distribution of off-policy data in the replay buffer towards the on-policy optimal distribution using TD-error-based occupancy ratios."
  - [section] "Using this objective, the occupancy ratio has the form d∗/dD = f′(δQ/β) = eδQ/β which gives our proposed regularized optimal experience replay formulation."
  - [corpus] Weak; neighboring work focuses on uncertainty or correction methods, not the occupancy-ratio derivation.
- Break condition: If the f-divergence penalty is too weak, the method may over-prioritize stale or irrelevant transitions, causing instability or poor performance.

### Mechanism 2
- Claim: Introducing a separate value network trained with the regularized ExtremeV loss yields more accurate TD-error estimates for prioritization.
- Mechanism: The ExtremeV loss, derived from the KL-regularized dual objective, down-weights large TD errors from out-of-distribution states, mitigating overestimation bias and improving priority stability.
- Core assumption: A dedicated value network with ExtremeV loss provides TD-error estimates less prone to bias than those from the main critic, especially under distribution shift.
- Evidence anchors:
  - [section] "We propose to incorporate a separate value network using the regularized objective for TD error estimation and priority calculation."
  - [section] "Empirical results demonstrate that our proposed ROER outperforms state-of-the-arts on 6 out of 11 continuous control tasks..."
  - [corpus] Weak; no direct neighbor evidence, but related to ExtremeQ/ExtremeV work on stable Q-learning.
- Break condition: If the value network is poorly initialized or trained with incompatible hyperparameters, its TD estimates may be noisy, harming prioritization.

### Mechanism 3
- Claim: The convergence parameter λ controls the stability of the priority update, enabling gradual convergence toward the optimal distribution without destabilizing learning.
- Mechanism: By mixing the updated priority with the previous one via d′ = [λeδQ/β + (1−λ)]·dD, the method prevents abrupt shifts in sampling distribution that could destabilize Q-learning updates.
- Core assumption: Slow, stable updates to priority distribution are more effective than rapid, potentially volatile shifts, especially in online RL where data distribution changes constantly.
- Evidence anchors:
  - [section] "We introduce a convergence parameter λ and formulate the following priority update function d′ = [λeδQ/β + (1−λ)]·dD with λ ∈ (0, 1]."
  - [section] "This allows for obtaining a more accurate TD error for priority calculation."
  - [corpus] Weak; neighboring work does not explicitly discuss gradual priority convergence mechanisms.
- Break condition: If λ is too small, convergence to the optimal distribution is too slow; if too large, numerical instability or overfitting to recent experiences may occur.

## Foundational Learning

- Concept: f-divergence and its variational form
  - Why needed here: The regularization term in the objective is expressed as an f-divergence between the replay buffer distribution and the optimal on-policy distribution, and its dual form is key to deriving the TD-error-based priority formula.
  - Quick check question: What is the dual form of KL divergence and how does it relate to the occupancy ratio in ROER?

- Concept: Convex conjugate and Fenchel duality
  - Why needed here: The derivation relies on expressing the f-divergence in terms of its convex conjugate to obtain a tractable optimization objective independent of the unknown optimal distribution.
  - Quick check question: Given f(x) = x log x, what is f*(y) and how does it appear in the ROER dual objective?

- Concept: Temporal difference error and Bellman operator
  - Why needed here: TD error is the basis for prioritization in ROER; understanding its relation to the Bellman operator and value function updates is essential for implementing and debugging the method.
  - Quick check question: How is δQ defined in ROER, and why is a separate value network used to compute it rather than the critic?

## Architecture Onboarding

- Component map: SAC core (actor, critic Qθ, target critic, temperature network) -> ROER extension (separate value network Vϕ with ExtremeV loss, priority update module, replay buffer with dynamic priorities)
- Critical path: 1. Collect transition, store in replay buffer with initial priority = 1 2. Sample batch using current priorities 3. Train main critic Qθ on sampled batch 4. Train value network Vϕ on same batch using ExtremeV loss 5. Compute TD error from Vϕ, update priorities with λ-controlled mixing 6. Update actor and temperature
- Design tradeoffs:
  - Separate value network adds computation and memory but yields more stable priorities
  - KL regularizer vs. other f-divergences: KL gives exponential weighting, which can be sensitive to outliers
  - Convergence rate λ: slower updates are more stable but slower to adapt; faster updates risk instability
- Failure signatures:
  - Performance degrades sharply: check value network training stability and priority clipping ranges
  - Training diverges: verify λ is not too large, check for exploding TD errors, inspect ExtremeV loss values
  - No improvement over baselines: verify β and priority update parameters are tuned for the environment
- First 3 experiments:
  1. Run ROER on a simple MuJoCo task (e.g., HalfCheetah-v2) with default hyperparameters, compare average return to SAC+PER baseline
  2. Vary β (loss temperature) over [0.4, 1, 4], observe effect on early vs. late learning performance
  3. Set λ very small (e.g., 0.01) vs. larger (e.g., 0.1), compare stability and convergence speed

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the ROER prioritization scheme work equally well with other RL algorithms beyond SAC?
- Basis in paper: [explicit] The paper demonstrates ROER's effectiveness with SAC but notes that "the form of TD error prioritization is closely associated with the regularized objective which implies that using simple TD error alone may not work best for every RL objectives."
- Why unresolved: The paper only evaluates ROER with SAC and does not test its compatibility or performance with other RL algorithms like PPO, TD3, or DQN.
- What evidence would resolve it: Empirical results comparing ROER's performance across multiple RL algorithms on standard benchmark tasks.

### Open Question 2
- Question: How does ROER's performance scale with very large replay buffers or distributed learning settings?
- Basis in paper: [inferred] The paper uses standard replay buffer sizes (1M transitions) and evaluates single-agent performance. Distributed RL systems often use much larger replay buffers and benefit significantly from prioritization.
- Why unresolved: The paper doesn't explore performance at scale or in distributed settings where the benefits of prioritization could be amplified or degraded.
- What evidence would resolve it: Experiments scaling replay buffer size from 1M to 100M+ transitions and testing in distributed RL frameworks like IMPALA or Ape-X.

### Open Question 3
- Question: Can the loss temperature β be made adaptive to automatically adjust regularization strength?
- Basis in paper: [explicit] The paper notes that "we recognize the difficulty of deciding the value for β" and suggests "a solution to this can be using adaptive loss temperature."
- Why unresolved: The paper uses fixed β values for all experiments and doesn't explore adaptive schemes despite acknowledging this as a limitation.
- What evidence would resolve it: Implementation and evaluation of adaptive β schemes (e.g., based on KL divergence between replay buffer and on-policy distributions) showing improved performance or reduced hyperparameter tuning.

### Open Question 4
- Question: Does ROER provide benefits in purely offline RL settings without any online fine-tuning?
- Basis in paper: [inferred] While ROER shows promise in offline-to-online settings, the paper doesn't explore its effectiveness in purely offline scenarios where all data is collected before training begins.
- Why unresolved: The paper focuses on online and offline-to-online settings but doesn't test ROER's utility in purely offline RL where distribution shift is even more pronounced.
- What evidence would resolve it: Comparative results of ROER versus uniform sampling in standard offline RL benchmarks like D4RL without any online fine-tuning.

## Limitations

- Theoretical claims rely heavily on the derivation of f-divergence regularizer and its dual form
- Empirical validation limited to 11 tasks without statistical significance testing
- Lack of ablation studies on the necessity of separate value network versus using critic directly

## Confidence

- Mechanism 1 (Occupancy ratio derivation): Medium confidence
- Mechanism 2 (Separate value network): Low confidence
- Mechanism 3 (Convergence parameter λ): Medium confidence

## Next Checks

1. **Ablation study on value network**: Compare ROER with a variant that uses the critic's TD error for prioritization to isolate the benefit of the separate value network.
2. **Statistical significance testing**: Perform paired t-tests or bootstrap confidence intervals on the final evaluation returns across seeds to confirm that improvements are statistically significant.
3. **Hyperparameter sensitivity analysis**: Systematically vary λ and β over a wider range and plot performance heatmaps to identify stable and sensitive regions.