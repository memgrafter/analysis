---
ver: rpa2
title: Can ChatGPT Detect DeepFakes? A Study of Using Multimodal Large Language Models
  for Media Forensics
arxiv_id: '2403.14077'
source_url: https://arxiv.org/abs/2403.14077
tags:
- images
- llms
- deepfake
- detection
- face
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigates whether multimodal large language models
  (LLMs) can be used for DeepFake detection. The authors evaluate OpenAI's GPT4V and
  Google's Gemini 1.0 Pro on detecting AI-generated face images.
---

# Can ChatGPT Detect DeepFakes? A Study of Using Multimodal Large Language Models for Media Forensics

## Quick Facts
- arXiv ID: 2403.14077
- Source URL: https://arxiv.org/abs/2403.14077
- Reference count: 40
- Key outcome: GPT-4V achieves 75% AUC and 90% accuracy on AI-generated faces but only 50% on real faces, showing promise but significant limitations

## Executive Summary
This study investigates whether multimodal large language models can detect AI-generated face images (DeepFakes). The authors evaluate OpenAI's GPT-4V and Google's Gemini 1.0 Pro using a dataset of 1,000 real and 2,000 AI-generated faces with various text prompts. GPT-4V demonstrates reasonable performance with 75% AUC score and over 90% accuracy on AI-generated images, though it struggles significantly with real images at around 50% accuracy. The detection capability relies on semantic understanding of visual inconsistencies rather than signal-level artifacts, making it potentially robust to post-processing. While the performance is comparable to early detection methods, it lags behind state-of-the-art techniques, highlighting both the potential and current limitations of LLM-based media forensics.

## Method Summary
The study uses multimodal LLMs (GPT-4V and Gemini 1.0 Pro) to detect DeepFake images through text prompt engineering. Researchers collected 1,000 real face images from FFHQ and 2,000 AI-generated faces from DF3 dataset, including images from StyleGAN2 and Latent Diffusion models. They designed various text prompts ranging from simple yes/no questions to complex requests for identification and justification of synthesis signs. The LLMs process the images via API calls, and responses are evaluated using AUC scores, classification accuracies, and rejection rates. The method leverages the LLMs' semantic understanding rather than signal-level features, potentially making it more robust to post-processing operations.

## Key Results
- GPT-4V achieves 75% AUC score and over 90% accuracy on AI-generated face images
- Performance on real images is significantly lower at approximately 50% accuracy
- Detection capability relies on semantic-level understanding rather than signal-level artifacts
- Performance is comparable to early detection methods but lags behind state-of-the-art techniques

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Multimodal LLMs can detect AI-generated faces by leveraging semantic understanding of visual inconsistencies rather than signal-level artifacts.
- Mechanism: The LLM's vast training corpus on natural images enables it to recognize subtle semantic anomalies in AI-generated faces that deviate from typical human faces, such as irregular lighting, texture inconsistencies, or anatomical abnormalities.
- Core assumption: The LLM has been exposed to sufficient real face images during training to develop an implicit understanding of what constitutes a "normal" face appearance.
- Evidence anchors:
  - [abstract] "Multimodal LLMs demonstrate a certain capability to distinguish between authentic and AI-generated imagery, drawing on their semantic understanding."
  - [section 4.1] "The detection is less susceptible to post-processing operations that can disrupt signal-level features"
- Break condition: If the LLM's training corpus lacks sufficient diversity in real face images, or if AI generation techniques evolve to eliminate semantic inconsistencies, the detection capability would degrade significantly.

### Mechanism 2
- Claim: Well-crafted prompts with specific forensic context significantly improve LLM detection performance compared to simple binary prompts.
- Mechanism: Rich, context-aware prompts guide the LLM to focus on relevant forensic cues and reduce uncertainty or safety-related refusals, effectively channeling the model's knowledge toward the detection task.
- Core assumption: The LLM's response quality is highly sensitive to prompt structure and content, and can be optimized through prompt engineering.
- Evidence anchors:
  - [section 3] "Prompt #6 goes beyond simple binary answers: we ask the LLM to identify signs of synthesis and, in addition, request it to justify the answers."
  - [section 4.2] "Prompt #6 result in fewer rejections with comparable prediction accuracies"
- Break condition: If the LLM's architecture fundamentally limits its ability to process complex instructions, or if safety filters become too restrictive, even well-crafted prompts may fail to elicit accurate responses.

### Mechanism 3
- Claim: The LLM's independence from signal-level features allows detection to generalize across different AI generation models without retraining.
- Mechanism: Unlike traditional machine learning models that learn specific artifacts from training data, LLMs rely on semantic patterns that should be consistent across different generation approaches.
- Core assumption: Semantic inconsistencies in AI-generated faces are model-agnostic and can be identified through general visual reasoning.
- Evidence anchors:
  - [abstract] "Their independence from signal cues enables them to identify AI-created images regardless of the generation model used"
  - [section 4.1] "The detection is less susceptible to post-processing operations that can disrupt signal-level features"
- Break condition: If different AI generation models produce semantically consistent outputs that are indistinguishable from real faces, the generalization advantage would disappear.

## Foundational Learning

- Concept: Image forensics and deepfake detection principles
  - Why needed here: Understanding what makes deepfakes detectable helps in crafting effective prompts and interpreting results
  - Quick check question: What are the two main categories of artifacts used in traditional deepfake detection methods?

- Concept: Prompt engineering and chain-of-thought reasoning
  - Why needed here: The effectiveness of LLM-based detection heavily depends on how prompts are structured to elicit useful responses
  - Quick check question: How does chain-of-thought prompting differ from simple instruction prompting in terms of expected LLM behavior?

- Concept: Statistical evaluation metrics for classification tasks
  - Why needed here: Interpreting AUC scores, rejection rates, and accuracy metrics requires understanding their strengths and limitations
  - Quick check question: Why might AUC be a more reliable metric than accuracy for imbalanced datasets?

## Architecture Onboarding

- Component map: Input image → Face extraction → Prompt generation → LLM API call → Response parsing → Score aggregation → Performance evaluation
- Critical path: Input image → Prompt generation → LLM API call → Response parsing → Score aggregation → Performance evaluation
- Design tradeoffs: Simple prompts reduce API costs but increase rejection rates; complex prompts improve performance but increase costs and processing time
- Failure signatures: High rejection rates indicate prompt issues; low accuracy on real images suggests semantic bias; inconsistent responses across queries suggest probabilistic uncertainty
- First 3 experiments:
  1. Test different prompt variations (simple vs. context-rich) on a small dataset to measure rejection rates and accuracy
  2. Evaluate performance across multiple query attempts to assess response consistency and potential ensemble benefits
  3. Compare detection performance on raw vs. post-processed images to understand robustness to common manipulations

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can multimodal LLMs reliably detect post-processed DeepFake images across different types of processing (e.g., JPEG compression, Gaussian blur, face blending, adversarial attacks)?
- Basis in paper: [explicit] The paper mentions that current DeepFake detection methods struggle with post-processed images, but multimodal LLMs' performance improves with post-processing. The study tested post-processed data but did not extensively explore the robustness across various processing types.
- Why unresolved: The paper provides initial results but lacks comprehensive testing across diverse post-processing techniques. The improvement with post-processing suggests potential, but the extent and limitations are unclear.
- What evidence would resolve it: Extensive testing of multimodal LLMs on a wide range of post-processed DeepFake images with different processing techniques, comparing their performance to state-of-the-art methods.

### Open Question 2
- Question: What specific prompting strategies can maximize the detection accuracy of multimodal LLMs for DeepFake images, particularly for real images that are currently misidentified?
- Basis in paper: [explicit] The paper identifies that multimodal LLMs struggle with real images and suggests that refining prompts could improve performance. It mentions decomposition-based prompting and few-shot prompting as exploratory approaches but does not provide a comprehensive solution.
- Why unresolved: The paper explores basic prompting strategies but does not develop or test advanced techniques like chain-of-thought or few-shot prompting in a systematic way.
- What evidence would resolve it: Systematic testing of various prompting strategies on both real and AI-generated images, measuring improvements in accuracy and identifying the most effective approaches.

### Open Question 3
- Question: How can multimodal LLMs be integrated with signal-level or data-driven detection techniques to create a hybrid approach that leverages both semantic and signal-level cues for DeepFake detection?
- Basis in paper: [inferred] The paper notes that multimodal LLMs rely on semantic-level abnormalities, while state-of-the-art methods use signal-level features. It suggests future research could combine these approaches but does not explore this integration.
- Why unresolved: The paper does not investigate hybrid models or the potential benefits of combining multimodal LLMs with traditional detection methods.
- What evidence would resolve it: Development and testing of hybrid models that incorporate both multimodal LLMs and signal-level detection techniques, comparing their performance to standalone approaches.

## Limitations

- The study is limited to face images only, preventing generalization to other media types like audio, video, or full-body images
- The dataset size (3,000 total images) is relatively small compared to standard deepfake detection benchmarks, potentially missing real-world diversity
- Significant performance gap between AI-generated (90% accuracy) and real images (50% accuracy) indicates systematic bias with high false positive risks

## Confidence

- **High confidence**: Multimodal LLMs can perform basic deepfake detection through semantic reasoning, as directly demonstrated with measurable results
- **Medium confidence**: Prompt engineering significantly impacts performance, though optimal prompt structures require further investigation
- **Low confidence**: LLMs offer model-agnostic detection capabilities, as the study didn't systematically test across diverse generation architectures

## Next Checks

1. Test the LLM-based detection system on a much larger and more diverse dataset including multiple face datasets, different generation models (beyond StyleGAN2 and Latent Diffusion), and various post-processing operations to assess real-world robustness.

2. Evaluate detection performance across images generated by newer AI models not present in the original training corpus to determine if semantic understanding truly generalizes beyond known generation patterns.

3. Conduct systematic testing on real but challenging images (unusual lighting, uncommon facial features, artistic portraits) to quantify the false positive rate and assess practical deployment risks in real-world scenarios.