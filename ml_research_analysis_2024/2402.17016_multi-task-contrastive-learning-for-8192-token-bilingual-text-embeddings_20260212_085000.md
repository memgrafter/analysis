---
ver: rpa2
title: Multi-Task Contrastive Learning for 8192-Token Bilingual Text Embeddings
arxiv_id: '2402.17016'
source_url: https://arxiv.org/abs/2402.17016
tags:
- tasks
- training
- multilingual
- language
- bilingual
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a new family of bilingual text embedding
  models that support English and one target language, with a focus on improving performance
  on semantic textual similarity (STS) tasks. The models are designed to process long
  texts up to 8192 tokens and use a novel multi-task learning objective that combines
  retrieval, STS, and classification tasks.
---

# Multi-Task Contrastive Learning for 8192-Token Bilingual Text Embeddings

## Quick Facts
- arXiv ID: 2402.17016
- Source URL: https://arxiv.org/abs/2402.17016
- Reference count: 40
- This paper introduces bilingual text embedding models that support English and one target language, with improved performance on semantic textual similarity (STS) tasks compared to multilingual baselines.

## Executive Summary
This paper presents a new family of bilingual text embedding models designed for English paired with one target language (German or Spanish), with support for processing long texts up to 8192 tokens. The models use a three-stage training approach: pre-training bilingual backbone models using masked language modeling, fine-tuning for general embedding tasks using contrastive loss, and multi-task fine-tuning that combines retrieval, STS, and classification tasks. The authors demonstrate that these bilingual models outperform existing multilingual baselines on both target language and cross-lingual tasks while using fewer parameters. The work also expands the MTEB benchmark with new German and Spanish tasks to support evaluation in these languages.

## Method Summary
The approach consists of three training stages. First, bilingual BERT models (JinaBERT) are pre-trained using masked language modeling with 8192-token context and ALiBi for long context support. Second, the pre-trained models are fine-tuned on text pairs using bi-directional InfoNCE loss to learn general embedding capabilities. Third, a multi-task fine-tuning stage samples different task types (retrieval, STS, classification) per batch, using InfoNCE loss for retrieval tasks and Pearson correlation loss for STS tasks. The training uses large bilingual corpora (250M documents per language) with 1:1 English/target language ratio, and the models are evaluated on expanded MTEB benchmarks including newly added German and Spanish tasks.

## Key Results
- Bilingual models outperform multilingual baselines on STS tasks in both target language and cross-lingual evaluation
- The multi-task learning approach with task sampling improves STS performance compared to single-task training
- Pearson correlation loss is more effective than MSE loss for STS tasks in the multi-task setting
- Models support long contexts up to 8192 tokens while using fewer parameters than multilingual alternatives

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Bilingual models outperform multilingual models on target language tasks after embedding fine-tuning with the same amount of pair-wise training data.
- Mechanism: Bilingual models have dedicated vocabulary and model capacity for the language pair, avoiding the "curse of multilinguality" that dilutes model capacity across many languages.
- Core assumption: Language-specific capacity allocation in bilingual models leads to better representation learning for the target language.
- Evidence anchors:
  - [abstract]: "we have significantly improved the model performance on STS tasks, which outperforms the capabilities of existing multilingual models in both target language understanding and cross-lingual evaluation tasks"
  - [section]: "bilingual models perform clearly better on the German and cross-lingual tasks and also slightly outperform their multilingual counterparts on the English tasks"
  - [corpus]: Weak - no direct corpus evidence for this specific claim about bilingual vs multilingual performance
- Break condition: If training data distribution is heavily skewed toward English or target language is very similar to English.

### Mechanism 2
- Claim: Multi-task learning with task sampling per batch improves STS performance compared to single-task training.
- Mechanism: Diverse training signals from different tasks help the model develop more discriminative and transferable embeddings that perform well on STS tasks.
- Core assumption: Different task types provide complementary learning signals that enhance generalization.
- Evidence anchors:
  - [abstract]: "we introduce a novel multi-task learning objective that combines retrieval, STS, and classification tasks"
  - [section]: "we observe that the Pearson variant outperforms the other two models. The effect is especially strong for SICK-R [Marelli et al., 2014] and STS12 [Agirre et al., 2012]"
  - [corpus]: Weak - no direct corpus evidence for multi-task learning effectiveness
- Break condition: If tasks are too dissimilar or one task dominates the training signal.

### Mechanism 3
- Claim: Using Pearson correlation loss for STS tasks is more effective than MSE loss in the multi-task setting.
- Mechanism: Pearson correlation is invariant to the scale of similarity values, making it more stable when combined with InfoNCE loss for retrieval tasks.
- Core assumption: Scale invariance prevents instability when training with heterogeneous loss functions.
- Evidence anchors:
  - [section]: "we choose to use a Pearson correlation loss over the mean squared error (MSE) loss function...since the Pearson correlation is invariant to the scale of the similarity values"
  - [section]: "Table 6 shows the Spearman correlation...we observe that the Pearson variant outperforms the other two models"
  - [corpus]: Weak - no direct corpus evidence for Pearson vs MSE comparison
- Break condition: If similarity score ranges are already normalized or multi-task setting is not used.

## Foundational Learning

- Concept: Masked Language Modeling (MLM)
  - Why needed here: MLM is used in the pre-training phase to learn general language representations for the bilingual backbone models
  - Quick check question: What is the purpose of masking 30% of tokens during MLM training, and how does this help the model learn better representations?

- Concept: Contrastive Learning
  - Why needed here: Contrastive learning objectives (InfoNCE) are used in the fine-tuning phases to learn embeddings that bring similar texts closer and push dissimilar texts apart
  - Quick check question: How does the temperature parameter Ï„ in InfoNCE loss affect the learned embedding space, and what happens if it's set too high or too low?

- Concept: Multi-Task Learning
  - Why needed here: Multi-task learning is used in the final fine-tuning stage to improve performance on STS, retrieval, and classification tasks simultaneously
  - Quick check question: What are the potential benefits and risks of using different loss functions for different tasks in a multi-task learning setup?

## Architecture Onboarding

- Component map: Pre-training (MLM) -> General fine-tuning (InfoNCE) -> Multi-task fine-tuning (InfoNCE + Pearson correlation)
- Critical path: The critical path is from pre-training through to the final multi-task fine-tuning, with each stage building on the previous one
- Design tradeoffs: Using bilingual models instead of multilingual ones trades parameter efficiency for language-specific performance; multi-task learning trades simplicity for potentially better generalization
- Failure signatures: Poor cross-lingual performance may indicate imbalanced training data; training instability may indicate suboptimal loss function choices; lack of long context support may indicate issues with ALiBi implementation
- First 3 experiments:
  1. Evaluate pre-trained bilingual backbone on standard language understanding benchmark to verify basic language capabilities
  2. Fine-tune pre-trained model on text pairs and evaluate on retrieval task to verify fine-tuning pipeline works
  3. Add multi-task learning stage and compare STS performance with and without multi-task learning to verify approach effectiveness

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the bilingual model performance scale with the amount of parallel data available during pre-training, and what is the optimal balance between monolingual and parallel data?
- Basis in paper: [inferred] The paper mentions using parallel data for bilingual models but doesn't explore the impact of varying amounts of parallel data on model performance
- Why unresolved: The paper focuses on overall performance comparison but doesn't investigate sensitivity to parallel data amounts
- What evidence would resolve it: A study that systematically varies parallel data amounts during pre-training and evaluates performance on downstream tasks

### Open Question 2
- Question: Can the multi-task learning strategy be extended to include additional tasks beyond retrieval, STS, and classification, and how would this impact the model's performance?
- Basis in paper: [explicit] The paper mentions plans to include more tasks in the multi-task learning strategy in the future
- Why unresolved: The paper only evaluates the multi-task learning strategy on retrieval, STS, and classification tasks
- What evidence would resolve it: An experiment that extends the multi-task learning strategy to include additional tasks and evaluates the impact on performance

### Open Question 3
- Question: How does the model's performance on cross-lingual tasks compare to monolingual models trained specifically for those languages, and what factors contribute to any observed differences?
- Basis in paper: [inferred] The paper compares bilingual models to multilingual models on cross-lingual tasks but doesn't compare them to monolingual models
- Why unresolved: The paper focuses on advantages of bilingual models over multilingual models but doesn't investigate comparison to monolingual models
- What evidence would resolve it: A study that compares cross-lingual performance of bilingual models to monolingual models trained specifically for target languages

## Limitations

- Exact composition of training corpora remains unspecified, creating uncertainty about language balance and potential domain biases
- Critical hyperparameters for multi-task learning stage are not provided, raising questions about approach robustness
- Evaluation is limited to German-English and Spanish-English pairs, with no evidence for generalization to other language pairs

## Confidence

**High Confidence**: Bilingual models outperform multilingual baselines on target language STS tasks (well-supported by direct experimental comparisons)

**Medium Confidence**: Multi-task learning with task sampling improves STS performance (moderately supported but exact contributions not isolated)

**Low Confidence**: Pearson correlation loss is superior to MSE loss specifically in multi-task setting (based on limited evidence without theoretical justification)

## Next Checks

1. **Cross-Lingual Generalizability Test**: Train and evaluate bilingual models for language pairs with different typological distances (e.g., English-Chinese, English-Arabic) to verify whether performance gains extend to languages with different script systems and morphological complexity.

2. **Hyperparameter Robustness Analysis**: Systematically vary task sampling probabilities, batch sizes, and learning rates in the multi-task learning stage to determine the stability range of the approach and isolate the effect of loss function choice.

3. **Data Distribution Impact Study**: Create controlled experiments with varying proportions of target language data (0.25:1, 0.5:1, 1:1, 2:1 English:target language ratios) to quantify how sensitive bilingual model performance is to training data balance.