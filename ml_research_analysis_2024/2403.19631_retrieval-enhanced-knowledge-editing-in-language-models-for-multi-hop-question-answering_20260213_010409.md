---
ver: rpa2
title: Retrieval-enhanced Knowledge Editing in Language Models for Multi-Hop Question
  Answering
arxiv_id: '2403.19631'
source_url: https://arxiv.org/abs/2403.19631
tags:
- editing
- facts
- knowledge
- question
- fact
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of updating knowledge in large
  language models (LLMs) for multi-hop question answering. The authors propose the
  Retrieval-Augmented model Editing (RAE) framework, which first retrieves edited
  facts and then refines the language model through in-context learning.
---

# Retrieval-enhanced Knowledge Editing in Language Models for Multi-Hop Question Answering

## Quick Facts
- **arXiv ID**: 2403.19631
- **Source URL**: https://arxiv.org/abs/2403.19631
- **Authors**: Yucheng Shi; Qiaoyu Tan; Xuansheng Wu; Shaochen Zhong; Kaixiong Zhou; Ninghao Liu
- **Reference count**: 40
- **One-line primary result**: RAE achieves 14.5% average accuracy improvement across different LLMs and batch sizes for multi-hop question answering.

## Executive Summary
This paper addresses the challenge of updating knowledge in large language models for multi-hop question answering through a retrieval-augmented editing framework called RAE. The authors propose a two-step process that first retrieves relevant edited facts using mutual information maximization, then refines the LLM through in-context learning with pruned facts. RAE outperforms existing methods by leveraging the LLM's reasoning abilities to identify contextually relevant facts even when entities differ, while a pruning strategy using model output entropy eliminates redundant information and reduces hallucinations. The framework is theoretically justified and validated through comprehensive experiments across various LLMs, demonstrating significant improvements in editing accuracy while maintaining stability across different batch sizes.

## Method Summary
RAE employs a two-step approach to knowledge editing: retrieval and editing. During retrieval, the framework constructs a knowledge graph by integrating edited facts into an external KG, then uses mutual information maximization to identify chain facts relevant to the question. This is achieved by iteratively predicting the next logical relation using the LLM's next-token prediction capability. The pruning step then removes redundant facts based on model output entropy, selecting the subset that yields the lowest uncertainty. Finally, in-context learning is used to edit the LLM's output by feeding the pruned facts and question into the target model. The framework is validated across various LLMs including Vicuna, Llama2, and ChatGLM, demonstrating superior performance compared to baseline methods.

## Key Results
- RAE achieves an average accuracy improvement of 14.5% across different language models and batch sizes
- The framework maintains stable editing performance across different batch sizes while baseline methods like Mello degrade significantly
- RAE outperforms existing methods including KG Link, Question Reform, and Subgraph Retriever in edited accuracy for multi-hop questions

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Retrieval-augmented editing (RAE) outperforms traditional similarity-based search by using mutual information maximization to identify chain facts that are contextually relevant to the question, even when entities differ.
- Mechanism: The retrieval approach leverages the next-token prediction capability of LLMs to estimate conditional probabilities of relations and entities given the question and current head entity. It iteratively constructs a subgraph by selecting relations with the highest mutual information score, effectively capturing the "ripple effect" of edits across fact chains.
- Core assumption: The next-token prediction model can approximate the conditional probability of relations and entities in a fact chain given the question context.
- Evidence anchors:
  - [abstract]: "our retrieval approach, based on mutual information maximization, leverages the reasoning abilities of LLMs to identify chain facts that traditional similarity-based searches might miss."
  - [section 3.2]: "Our approach first feeds LLMs a sequence that includes the question, any preceding fact, and the relevant entity. Then, the model predicts this entity's next possible logical relation within the context of its input question."
  - [corpus]: Weak evidence; no direct citations found in the neighbor papers for mutual information-based retrieval in the context of model editing.
- Break condition: If the LLM's next-token prediction is biased towards frequently occurring words or fails to capture the logical relation needed to answer the question, the retrieval may miss critical facts.

### Mechanism 2
- Claim: The pruning strategy using model output entropy effectively removes redundant facts from the retrieved subgraph, improving editing accuracy and mitigating hallucination.
- Mechanism: The pruning method quantifies the LLM's prediction uncertainty using Shannon entropy. It iteratively evaluates subsets of the retrieved facts, starting from the smallest prefix and adding subsequent facts, to find the subset that yields the lowest entropy. This subset is assumed to contain the most relevant facts for answering the question.
- Core assumption: The LLM exhibits minimum entropy when provided with the correct fact chain needed to answer the question.
- Evidence anchors:
  - [section 3.3]: "We define editing uncertainty as the uncertainty of the output generated by large language models... A higher entropy value means less confidence in the answer, reflecting greater uncertainty."
  - [section 3.3.1]: "Our observations reveal a phenomenon: the language model produces answers with much lower entropy when the input facts are equal to the ground-truth fact chain."
  - [corpus]: No direct evidence in the neighbor papers; the pruning strategy appears to be a novel contribution.
- Break condition: If the LLM's output entropy is not a reliable indicator of the relevance of facts to the question, or if the pruning process removes critical facts needed for answering the question.

### Mechanism 3
- Claim: RAE maintains stable editing performance across different batch sizes, while other methods like Mello degrade significantly with increasing instances.
- Mechanism: RAE's retrieval strategy is independent of the number of editing instances, as it focuses on retrieving facts relevant to each individual question. The pruning strategy also helps by reducing noise in the retrieved facts, making the editing process more robust to larger batches.
- Core assumption: The retrieval and pruning strategies are effective in identifying and filtering relevant facts regardless of the number of editing instances.
- Evidence anchors:
  - [section 4.6]: "We can observe that, in both the Vicuna and Llama2 models, RAE's accuracy remains stable across different editing instances, whereas Mello's accuracy significantly declines with increasing instances."
  - [corpus]: No direct evidence in the neighbor papers; the stability across batch sizes is demonstrated in the experiments.
- Break condition: If the retrieval or pruning strategies become less effective as the number of editing instances increases, leading to degraded performance.

## Foundational Learning

- Concept: Mutual Information (MI)
  - Why needed here: MI is used as the retrieval objective to quantify the relevance of retrieved facts to the question. It helps identify facts that share the most information with the question, even when entities differ.
  - Quick check question: What is the formula for mutual information between two random variables X and Y?

- Concept: Shannon Entropy
  - Why needed here: Entropy is used to quantify the LLM's prediction uncertainty. It helps identify the subset of retrieved facts that yields the most confident (lowest entropy) answer.
  - Quick check question: What is the formula for Shannon entropy of a discrete random variable X?

- Concept: In-context Learning
  - Why needed here: RAE relies on the LLM's ability to learn from the provided facts and answer the question without modifying its parameters. Understanding in-context learning is crucial for designing effective prompts and evaluating the editing performance.
  - Quick check question: What is the key difference between in-context learning and fine-tuning in LLMs?

## Architecture Onboarding

- Component map: Knowledge Graph -> Retrieval Module -> Pruning Module -> Editing Module
- Critical path:
  1. Construct the knowledge graph by integrating edited facts into an external KG.
  2. Retrieve a subgraph from the KG using the mutual information-based retrieval strategy.
  3. Prune the retrieved subgraph using the entropy-based pruning strategy.
  4. Edit the LLM's output using in-context learning with the pruned facts.
- Design tradeoffs:
  - Retrieval strategy: Mutual information maximization vs. similarity-based search. MI is more effective at identifying contextually relevant facts but requires more computational resources.
  - Pruning strategy: Entropy-based pruning vs. simple fact count. Entropy is more effective at identifying redundant facts but requires running the LLM multiple times.
  - Batch size: RAE is more robust to larger batches but may require more computational resources.
- Failure signatures:
  - Retrieval failure: If the LLM's next-token prediction is biased or fails to capture the logical relation needed to answer the question, critical facts may be missed.
  - Pruning failure: If the LLM's output entropy is not a reliable indicator of fact relevance, or if the pruning process removes critical facts, editing accuracy may degrade.
  - In-context learning failure: If the LLM fails to learn from the provided facts, or if the prompt is not well-designed, the edited output may be incorrect.
- First 3 experiments:
  1. Evaluate the retrieval performance of RAE on a small subset of the MQUAKE-CF dataset using the P@1 and P@2 metrics.
  2. Compare the editing accuracy of RAE with and without the pruning strategy on a small subset of the MQUAKE-CF dataset.
  3. Evaluate the stability of RAE's editing performance across different batch sizes using the MQUAKE-CF dataset.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of RAE scale with increasingly larger knowledge graphs and more complex multi-hop questions beyond 4 hops?
- Basis in paper: [explicit] The paper mentions using beam search and setting the number of retrieval hops to 4, 5, and 6 for 2-hop, 3-hop, and 4-hop questions respectively, but does not explore scalability beyond this.
- Why unresolved: The paper only evaluates on 2-4 hop questions and does not provide theoretical or empirical analysis of scalability to larger knowledge graphs or more complex questions.
- What evidence would resolve it: Experiments evaluating RAE on knowledge graphs with 10x more facts and questions requiring 5-10 hops, along with analysis of retrieval precision and editing accuracy degradation rates.

### Open Question 2
- Question: What is the optimal balance between retrieval precision and editing accuracy when adjusting the beam width and number of retrieved facts in the knowledge pruning step?
- Basis in paper: [inferred] The paper uses a beam width of 2 and retrieves 2 additional facts beyond what the question needs, but does not explore how these parameters affect the trade-off between retrieval quality and editing performance.
- Why unresolved: The paper does not conduct ablation studies on beam width or number of retrieved facts, and the theoretical justification only addresses mutual information maximization without considering computational efficiency.
- What evidence would resolve it: Systematic experiments varying beam width (1-5) and number of retrieved facts (0-4 extra) while measuring both retrieval precision and final editing accuracy, plus analysis of computational costs.

### Open Question 3
- Question: How does RAE's mutual information-based retrieval compare to hybrid approaches that combine MI with traditional embedding similarity measures?
- Basis in paper: [explicit] The paper compares RAE to embedding-based methods (KG Link, Question Reform) and probability-based methods (Subgraph Retriever), showing MI-based retrieval outperforms both, but does not explore hybrid approaches.
- Why unresolved: The paper only evaluates pure MI-based retrieval against other single-method approaches, leaving open whether combining MI with similarity measures could yield better performance.
- What evidence would resolve it: Experiments comparing RAE against hybrid retrieval methods that weight MI scores with embedding similarity scores, along with ablation studies on different weighting schemes and their effects on editing accuracy.

## Limitations

- Scalability concerns with mutual information maximization approach for very large knowledge graphs
- Reliance on model entropy as proxy for fact relevance may not hold across all question types
- Limited evaluation on single-hop questions and non-MQUAKE datasets

## Confidence

**High Confidence**: The core retrieval-augmented editing mechanism (Mechanism 1) is well-supported by theoretical justification and experimental results, with consistent improvements across multiple LLM architectures and batch sizes.

**Medium Confidence**: The pruning strategy's effectiveness (Mechanism 2) is supported by entropy analysis and accuracy improvements, but the underlying assumption that minimum entropy correlates with optimal fact selection requires further validation across diverse question types.

**Low Confidence**: The computational complexity claims are insufficiently detailed, and the framework's performance on knowledge graphs significantly larger than those tested is not demonstrated.

## Next Checks

1. **Retrieval Robustness Test**: Evaluate RAE's retrieval precision on a systematically corrupted knowledge graph where random facts are removed or altered to assess the retrieval strategy's resilience to incomplete or noisy knowledge bases.

2. **Cross-Dataset Generalization**: Test RAE on multi-hop question answering datasets beyond MQUAKE-CF (such as HotpotQA or ComplexWebQuestions) to verify the framework's generalizability across different question distributions and knowledge graph structures.

3. **Computational Complexity Analysis**: Conduct a detailed time and memory complexity analysis comparing RAE with baseline methods across varying knowledge graph sizes (10K, 100K, 1M facts) to quantify the practical scalability limits of the mutual information-based retrieval approach.