---
ver: rpa2
title: Auditing Local Explanations is Hard
arxiv_id: '2407.13281'
source_url: https://arxiv.org/abs/2407.13281
tags:
- local
- explanations
- data
- have
- lemma
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates the challenge of auditing local explanations
  in machine learning, particularly when explanation providers may not be trusted.
  The authors propose a framework where a third-party auditor attempts to verify the
  trustworthiness of local explanations by querying model decisions and corresponding
  explanations.
---

# Auditing Local Explanations is Hard

## Quick Facts
- arXiv ID: 2407.13281
- Source URL: https://arxiv.org/abs/2407.13281
- Authors: Robi Bhattacharjee; Ulrike von Luxburg
- Reference count: 40
- Primary result: Auditing local explanations is practically impossible in high dimensions without additional model access

## Executive Summary
This paper investigates the fundamental challenge of auditing local explanations in machine learning when the explanation provider cannot be trusted. The authors establish that the locality of explanations - how small the local regions are - is the critical factor determining whether successful auditing is possible. Through theoretical analysis, they demonstrate that for high-dimensional settings, typical local explanations are so localized that auditing becomes practically impossible without additional access to the underlying machine learning model. This finding has significant implications for the use of local explanations in sensitive contexts where trust verification is crucial.

## Method Summary
The authors propose a framework where a third-party auditor attempts to verify the trustworthiness of local explanations by querying model decisions and corresponding explanations. They introduce an explainability loss function to measure how closely local explanations adhere to the original classifier. The auditing process involves querying the model with various inputs, receiving both predictions and explanations, and then determining whether the explanations are consistent with the model's behavior. The framework provides both lower and upper bounds on the amount of data needed for successful auditing, establishing theoretical limits on what can be verified through black-box queries alone.

## Key Results
- The locality of provided explanations is the main factor controlling the sample complexity of auditing
- For high-dimensional settings, typical local explanations are extremely local, making auditing practically impossible
- Without additional access to the machine learning model, black-box auditing of local explanations has fundamental limitations

## Why This Works (Mechanism)
The auditing framework works by measuring the consistency between provided explanations and the actual model behavior through carefully designed queries. The explainability loss function quantifies how well the explanations align with the classifier's decisions, allowing the auditor to detect discrepancies that might indicate untrustworthy explanations.

## Foundational Learning

- **Explainability Loss Function**: Measures consistency between explanations and model behavior. Needed to quantify auditing success. Quick check: Verify loss decreases as explanations become more aligned with model predictions.

- **Sample Complexity Bounds**: Theoretical limits on data needed for auditing. Needed to understand practical feasibility. Quick check: Calculate bounds for specific dimensions and locality parameters.

- **Black-box Auditing**: Verification without internal model access. Needed for realistic auditing scenarios. Quick check: Test auditing process with only input-output queries.

## Architecture Onboarding

Component Map: Auditor -> Query Generator -> Model/Explanation Provider -> Response Verifier

Critical Path: Query generation → Model prediction → Explanation retrieval → Consistency verification → Decision on trustworthiness

Design Tradeoffs: The framework trades completeness for practicality - it can detect major inconsistencies but may miss subtle manipulation. The choice of locality parameter significantly impacts both auditing power and computational requirements.

Failure Signatures: High explainability loss values indicate untrustworthy explanations. Inconsistent behavior across similar inputs suggests explanation manipulation. Inability to achieve low loss despite sufficient queries implies explanation locality is too high.

First Experiments:
1. Test auditing on synthetic data with controlled explanation locality
2. Validate bounds empirically on simple linear classifiers
3. Evaluate performance degradation as dimensionality increases

## Open Questions the Paper Calls Out
None

## Limitations
- Theoretical analysis relies on specific assumptions about explanation providers that may not capture all real-world scenarios
- Focus on black-box auditing may not generalize to all auditing contexts
- Mathematical bounds assume certain properties about classifiers that may not hold for complex models
- Practical implications may be overly pessimistic given real-world explanation methods often have additional structure

## Confidence

High confidence:
- Locality is the primary factor controlling sample complexity
- Theoretical bounds on auditing requirements are mathematically sound

Medium confidence:
- Practical applicability of theoretical findings to real explanation methods
- Pessimism about auditing feasibility in high dimensions
- Generalization to complex, non-linear models

## Next Checks

1. Empirical validation of theoretical bounds using popular local explanation methods (LIME, SHAP) on benchmark datasets
2. Investigation of whether relaxing certain theoretical assumptions changes the practical feasibility of auditing
3. Exploration of hybrid auditing approaches that combine black-box queries with limited white-box access to test if sample complexity requirements can be reduced in practice