---
ver: rpa2
title: Reinforcement Learning Policy as Macro Regulator Rather than Macro Placer
arxiv_id: '2412.07167'
source_url: https://arxiv.org/abs/2412.07167
tags:
- placement
- maskregulate
- chip
- should
- regularity
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work introduces MaskRegulate, a novel reinforcement learning
  approach for chip macro placement that operates as a regulator rather than a placer.
  The method focuses on refining existing placement layouts rather than generating
  them from scratch, addressing limitations of current RL-based placement methods
  including long training times, poor generalization, and PPA guarantee issues.
---

# Reinforcement Learning Policy as Macro Regulator Rather than Macro Placer

## Quick Facts
- arXiv ID: 2412.07167
- Source URL: https://arxiv.org/abs/2412.07167
- Authors: Ke Xue; Ruo-Tong Chen; Xi Lin; Yunqi Shi; Shixiong Kai; Siyuan Xu; Chao Qian
- Reference count: 40
- One-line primary result: MaskRegulate achieves 17.08% improvement in routing wirelength and 73.08% reduction in horizontal congestion overflow compared to competitive methods

## Executive Summary
This work introduces MaskRegulate, a novel reinforcement learning approach for chip macro placement that operates as a regulator rather than a placer. The method focuses on refining existing placement layouts rather than generating them from scratch, addressing limitations of current RL-based placement methods including long training times, poor generalization, and PPA guarantee issues. By shifting to a refinement stage, the approach gains access to comprehensive state information and more precise rewards during training.

The method integrates regularity as a key metric alongside traditional half-perimeter wirelength (HPWL), incorporating it into both the state representation and reward function. MaskRegulate uses visual representations of chip layouts through pixel-level masks, including PositionMask, WireMask, and newly introduced RegularMask to capture changes in regularity. Evaluated on ICCAD 2015 benchmarks, MaskRegulate achieves significant improvements in PPA metrics including 17.08% improvement in routing wirelength, 73.08% reduction in horizontal congestion overflow, and 46.17% reduction in violation points compared to competitive methods.

## Method Summary
MaskRegulate is a reinforcement learning approach for chip macro placement that functions as a regulator rather than a traditional placer. The method takes existing placement layouts (typically generated by DREAMPlace) and refines them through iterative adjustment. It uses pixel-level visual representations including PositionMask, WireMask, and RegularMask to encode chip layout information. The policy employs a neural network to process these masks and determine optimal macro adjustments. Training uses the PPO algorithm with a reward function that balances HPWL reduction and regularity improvement. The approach demonstrates superior generalization ability and maintains high performance when adjusting placements from various initial methods.

## Key Results
- 17.08% improvement in routing wirelength compared to competitive methods
- 73.08% reduction in horizontal congestion overflow
- 46.17% reduction in violation points
- Superior generalization ability across different chip designs
- Effective refinement from various initial placement methods

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Shifting from placer to regulator formulation provides more comprehensive state information during training
- Mechanism: The regulator operates on pre-existing placement layouts, allowing access to positions of all macros rather than just placed ones, enabling denser and more precise reward signals
- Core assumption: The pre-existing placement provides sufficient information about macro relationships to enable meaningful refinement
- Evidence anchors:
  - [abstract]: "which allows the RL policy to learn how to adjust existing placement layouts, thereby receiving sufficient information for the policy to act and obtain relatively dense and precise rewards"
  - [section 3.1]: "the regulator considers not only the macros that have already been placed but also the positions of all other macros"
- Break condition: If pre-existing placements are too poor or scattered, the regulator may lack useful structural information to build upon

### Mechanism 2
- Claim: Integrating regularity as both state representation and reward component improves PPA outcomes
- Mechanism: Regularity mask captures proximity to chip edges, and reward function balances wirelength reduction with regularity improvement, preventing macro blockage and congestion
- Core assumption: Macro blockage significantly impacts routing congestion and timing performance
- Evidence anchors:
  - [abstract]: "Additionally, we introduce the concept of regularity during training, which is considered an important metric in the chip design industry but is often overlooked in current RL placement methods"
  - [section 3.2]: "Regularity: We compute the regularity values for all macros, which serve as a measurement of the overall regularity of the placement result"
- Break condition: If regularity metric is poorly calibrated or if α trade-off coefficient is not properly tuned, the balance between objectives may fail

### Mechanism 3
- Claim: Visual pixel-level representation with PositionMask, WireMask, and RegularMask enables efficient learning
- Mechanism: The policy divides chip canvas into grids and uses image masks to encode placement information, allowing the neural network to process complex spatial relationships efficiently
- Core assumption: Visual representation captures sufficient information about macro placement configurations for effective policy learning
- Evidence anchors:
  - [section 3.1]: "The policy divides the chip canvas into several grids and utilizes visual information as inputs, converting chip information into pixel-level image masks"
  - [section 3.1]: "This approach has demonstrated superior efficiency and performance in RL placer policy learning [16; 15]"
- Break condition: If grid resolution is too coarse or masks don't capture relevant features, policy may not learn effective placement strategies

## Foundational Learning

- Concept: Markov Decision Process formulation for reinforcement learning
  - Why needed here: Provides the framework for the agent to learn placement refinement policies through state-action-reward interactions
  - Quick check question: What distinguishes the MDP formulation for regulator versus placer in this work?

- Concept: Half-perimeter wirelength (HPWL) as proxy metric
  - Why needed here: Serves as a computationally efficient approximation of routing wirelength that can be used during training and evaluation
  - Quick check question: How is HPWL calculated for a given net based on macro positions?

- Concept: Multi-objective optimization with trade-off coefficient α
  - Why needed here: Balances competing objectives of wirelength minimization and regularity improvement in the reward function
  - Quick check question: What happens to placement quality when α approaches 0 versus 1?

## Architecture Onboarding

- Component map:
  - PositionMask: Identifies valid positions for current macro adjustment
  - WireMask: Encodes approximate wirelength changes for potential placements
  - RegularMask: Captures regularity changes for potential placements
  - Global Mask Encoder/Decoder: Processes visual information through neural network
  - Policy Network: Outputs placement probabilities
  - Value Network: Estimates expected returns

- Critical path:
  1. Initialize with DREAMPlace-generated macro placement
  2. For each macro in adjustment sequence:
     - Generate PositionMask, WireMask, RegularMask
     - Pass through neural network to get action probabilities
     - Select action and update placement
     - Calculate reward based on HPWL and regularity changes
  3. Train using PPO algorithm

- Design tradeoffs:
  - Grid resolution vs. computational efficiency (224x224 chosen)
  - Trade-off coefficient α balancing wirelength vs. regularity
  - Mask normalization affecting reward scale
  - Episode length (1000 vs 2000) affecting training efficiency

- Failure signatures:
  - Poor convergence: Check learning rate, batch size, or reward normalization
  - Macro overlap: Verify PositionMask generation logic
  - Degraded regularity: Examine RegularMask calculation or α tuning
  - Overfitting: Test generalization on unseen chips

- First 3 experiments:
  1. Run baseline comparison: MaskRegulate vs DREAMPlace on single chip
  2. Test regularization impact: MaskRegulate vs MaskPlace + RegularMask
  3. Verify generalization: Pre-train on subset, test on unseen chips

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the integration of regularity in the reward function affect the overall wirelength optimization compared to traditional HPWL-only methods?
- Basis in paper: [explicit] The paper introduces regularity as a reward signal alongside HPWL, but the specific impact on wirelength optimization is not fully explored.
- Why unresolved: The paper focuses on the benefits of regularity in terms of PPA metrics but does not provide a detailed analysis of how regularity affects wirelength optimization specifically.
- What evidence would resolve it: Comparative experiments showing wirelength metrics with and without regularity integration, and analysis of the trade-offs between wirelength and regularity.

### Open Question 2
- Question: What are the computational trade-offs of using MaskRegulate compared to traditional placement methods, especially in terms of training time and resource usage?
- Basis in paper: [inferred] The paper mentions long training times for RL-based methods but does not provide a detailed comparison of computational resources required for MaskRegulate versus traditional methods.
- Why unresolved: The paper highlights the efficiency of MaskRegulate in terms of learning but lacks a comprehensive analysis of its computational demands.
- What evidence would resolve it: Detailed benchmarks comparing training and inference times, memory usage, and hardware requirements for MaskRegulate and traditional methods.

### Open Question 3
- Question: How does MaskRegulate perform on chip designs with varying aspect ratios and module sizes, and what are the limitations in handling such diversity?
- Basis in paper: [inferred] The paper does not address the impact of module aspect ratios and area factors on placement, suggesting a potential area for exploration.
- Why unresolved: The paper does not explore the adaptability of MaskRegulate to different chip design complexities, which could affect its generalization and performance.
- What evidence would resolve it: Experiments testing MaskRegulate on chips with diverse aspect ratios and module sizes, and analysis of performance metrics across these variations.

## Limitations
- The approach requires a pre-existing placement from traditional methods as starting point
- Training time remains significant despite regulator formulation
- Limited exploration of computational resource requirements compared to traditional methods
- Does not address chip designs with varying aspect ratios and module sizes

## Confidence

**Confidence levels:**
- **High confidence**: The mechanism of using visual pixel masks for state representation is well-established in prior RL literature and properly implemented
- **Medium confidence**: The effectiveness of shifting from placer to regulator formulation is supported by results but lacks theoretical guarantees about policy optimality
- **Medium confidence**: The regularity metric integration shows clear benefits in congestion reduction, but the choice of α=0.7 appears somewhat arbitrary without sensitivity analysis

## Next Checks

1. Conduct ablation studies removing RegularMask to quantify its specific contribution to PPA improvements
2. Test policy generalization by training on smaller chips and evaluating on larger, structurally different chips
3. Perform sensitivity analysis on α parameter to determine optimal trade-off between HPWL and regularity objectives