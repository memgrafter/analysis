---
ver: rpa2
title: 'JaFIn: Japanese Financial Instruction Dataset'
arxiv_id: '2404.09260'
source_url: https://arxiv.org/abs/2404.09260
tags:
- instruction
- response
- ideco
- financial
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces JaFIn, a Japanese financial instruction dataset
  for large language models (LLMs), aimed at improving domain adaptation through instruction
  tuning. The dataset is manually constructed from Japanese government websites and
  includes 1,490 samples covering diverse financial topics.
---

# JaFIn: Japanese Financial Instruction Dataset

## Quick Facts
- arXiv ID: 2404.09260
- Source URL: https://arxiv.org/abs/2404.09260
- Reference count: 40
- Primary result: JaFIn improves Japanese LLM financial domain adaptation via instruction tuning

## Executive Summary
This paper introduces JaFIn, a Japanese financial instruction dataset for large language models (LLMs), aimed at improving domain adaptation through instruction tuning. The dataset is manually constructed from Japanese government websites and includes 1,490 samples covering diverse financial topics. Using JaFIn, the authors applied instruction tuning to three Japanese LLMs (llm-jp-1.3b-v1.0, japanese-large-lm-1.7b, and japanese-large-lm-3.6b) using LoRA, resulting in improved performance on Japanese financial benchmark tasks such as security sales and cma basics. Qualitative evaluations also demonstrated enhanced responses to financial queries. The results indicate that JaFIn effectively enhances the financial domain knowledge of LLMs, though further improvements are needed for specialized tasks. The dataset will be made publicly available to accelerate the development of conversational AI in the financial domain.

## Method Summary
The authors constructed JaFIn by manually collecting 1,490 Japanese financial Q&A samples from government websites and other sources. They applied instruction tuning to three pre-trained Japanese LLMs using LoRA-based fine-tuning with specific hyperparameters (learning rates, batch sizes, epochs). The instruction tuning process used the JaFIn dataset split into training (85%) and validation (15%) sets. Model performance was evaluated on five Japanese financial benchmark tasks (chabsa, cma basics, cpa audit, fp2, security sales 1) to measure improvements in financial domain knowledge.

## Key Results
- JaFIn instruction tuning significantly improved performance on security sales 1 benchmark task
- Models showed enhanced ability to answer financial queries in qualitative evaluations
- Performance gains varied across different financial subdomains, with more improvement in some areas than others

## Why This Works (Mechanism)

### Mechanism 1
- Claim: JaFIn improves financial domain adaptation by instruction tuning LLMs with task-oriented Japanese financial data.
- Mechanism: The dataset pairs Japanese financial questions with accurate, government-sourced answers, enabling LLMs to learn domain-specific response patterns during LoRA-based instruction tuning.
- Core assumption: The instruction format (question-answer) aligns with how LLMs generalize task completion, so exposure to financial Q&A improves downstream financial task performance.
- Evidence anchors:
  - [abstract] "JaFIn is manually constructed based on multiple data sources, including Japanese government websites, which provide extensive financial knowledge."
  - [section III.A] "We constructed JaFIn... using frequently asked questions, explanatory materials related to finance, and text from Wikipedia... The names of the sources and the number of samples collected are as listed in Table I."
  - [corpus] Weak: Neighbor papers focus on similar finance-LLM work but do not directly evaluate JaFIn's mechanism.
- Break condition: If JaFIn's Q&A pairs are poorly aligned with actual user queries or contain domain bias, performance gains may not transfer.

### Mechanism 2
- Claim: LoRA tuning with JaFIn efficiently adapts LLMs without full fine-tuning overhead.
- Mechanism: LoRA freezes original LLM weights and injects low-rank adaptation matrices, updating only a small subset of parameters for financial specialization.
- Core assumption: Low-rank decomposition captures domain-specific features while preserving general capabilities, enabling faster, cheaper adaptation.
- Evidence anchors:
  - [section IV.C] "In LoRA, the weight parameters updated during pre-training are frozen, and only the differences in weights after fine-tuning are updated."
  - [section V.B] "Considering that JaFIn contains relatively long texts, the maximum token length provided to the model was set to 1024. Instruction tuning for each model was conducted over 20 epochs."
  - [corpus] Weak: Neighbor work mentions instruction tuning but not LoRA specifically.
- Break condition: If the rank or α hyperparameter is too small, adaptation may be insufficient; too large may cause overfitting or instability.

### Mechanism 3
- Claim: JaFIn's government-sourced, multi-topic coverage improves model robustness across financial domains.
- Mechanism: Diverse data sources (ministries, agencies, Wikipedia) ensure broad exposure to financial terminology and concepts, reducing domain gaps.
- Core assumption: Broader coverage during instruction tuning leads to better generalization on benchmark tasks spanning multiple financial subdomains.
- Evidence anchors:
  - [section III.A] "JaFIn includes 1,490 samples... collected from a variety of sources... data derived from government websites."
  - [section VI.A] "The improvement in performance due to instruction tuning was significant in security sales 1 compared to other tasks."
  - [corpus] Weak: Neighbor papers do not explicitly analyze source diversity effects.
- Break condition: If the dataset over-represents certain subdomains (e.g., pensions) and under-represents others (e.g., auditing), performance may be uneven.

## Foundational Learning

- Concept: Instruction tuning vs. continued pre-training
  - Why needed here: JaFIn is instruction data, not raw corpus; understanding how it shapes model behavior differently is key.
  - Quick check question: Does instruction tuning primarily improve response style or domain knowledge?

- Concept: LoRA low-rank adaptation
  - Why needed here: JaFIn uses LoRA; knowing how it modifies weights without full fine-tuning is essential for debugging and scaling.
  - Quick check question: What is the role of the rank (r) and scaling factor (α) in LoRA?

- Concept: Japanese NLP domain adaptation challenges
  - Why needed here: JaFIn is Japanese-specific; understanding language-specific tokenization and domain scarcity matters for evaluation.
  - Quick check question: Why might Japanese financial NLP lag behind English in available benchmarks?

## Architecture Onboarding

- Component map:
  Data source ingestion → Manual filtering & Q&A formatting → JaFIn dataset
  JaFIn + LoRA trainer → Instruction-tuned Japanese financial LLM
  Evaluation harness → Japanese financial benchmarks (chabsa, cma basics, etc.)

- Critical path:
  1. Construct/verify JaFIn dataset
  2. Configure LoRA hyperparameters (rank, α, lr, epochs)
  3. Run instruction tuning on target LLM
  4. Evaluate on Japanese financial benchmarks
  5. Qualitative review of responses

- Design tradeoffs:
  - Dataset size vs. domain coverage: Larger JaFIn may improve robustness but increase training cost.
  - LoRA rank vs. memory: Higher rank captures more nuance but needs more GPU.
  - Benchmark selection: Narrow benchmarks may overstate gains; broad ones dilute domain focus.

- Failure signatures:
  - No performance gain: Check LoRA rank/α, learning rate, data alignment.
  - Degraded performance: Likely overfitting or catastrophic forgetting; reduce rank or epochs.
  - Unnatural responses: Dataset filtering may be too strict or prompts poorly formatted.

- First 3 experiments:
  1. Run JaFIn on small subset of samples with LoRA rank=4, α=8; check training loss and qualitative output.
  2. Sweep learning rates (3e-4, 4e-4, 5e-4) for llm-jp-1.3b-v1.0; select best validation loss.
  3. Compare cma basics and security sales 1 scores before/after tuning to isolate domain gain.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of JaFIn-tuned LLMs compare to other domain-specific financial models like BloombergGPT or FinBERT when evaluated on the same benchmarks?
- Basis in paper: [explicit] The paper mentions BloombergGPT and FinBERT as existing financial models but does not compare JaFIn-tuned models to them on benchmarks
- Why unresolved: The paper only compares JaFIn-tuned models to their base versions, not to other established financial LLMs
- What evidence would resolve it: Direct benchmark comparisons between JaFIn-tuned models and BloombergGPT/FinBERT on the same financial tasks

### Open Question 2
- Question: What is the impact of instruction tuning on the model's ability to handle complex financial reasoning tasks beyond simple classification and multiple-choice questions?
- Basis in paper: [inferred] The paper shows improvements on simpler tasks like security sales 1 but notes limited improvement on more complex tasks like cma and fp2
- Why unresolved: The paper only evaluates on predefined benchmark tasks without exploring more complex financial reasoning scenarios
- What evidence would resolve it: Testing the models on complex financial analysis tasks, scenario-based reasoning, or open-ended financial planning problems

### Open Question 3
- Question: How does the size and diversity of the JaFIn dataset affect the performance of instruction tuning, and what is the optimal dataset size for financial domain adaptation?
- Basis in paper: [explicit] The paper uses 1,490 samples from various sources but doesn't explore how dataset size affects performance
- Why unresolved: The paper doesn't conduct experiments with different dataset sizes or diversity levels to determine optimal training data requirements
- What evidence would resolve it: Performance comparisons using different sizes of JaFIn (e.g., 500, 1,000, 2,000 samples) and analyses of which financial topics contribute most to performance gains

### Open Question 4
- Question: How does instruction tuning with JaFIn affect the model's general language capabilities and performance on non-financial tasks?
- Basis in paper: [inferred] The paper focuses solely on financial task performance without examining potential trade-offs in general language abilities
- Why unresolved: The paper doesn't test whether financial specialization through JaFIn instruction tuning impacts the model's performance on general NLP tasks
- What evidence would resolve it: Evaluations of JaFIn-tuned models on general language benchmarks like MMLU, commonsense reasoning, or general knowledge tasks compared to base models

## Limitations
- Dataset size of 1,490 samples is relatively small for comprehensive financial domain coverage
- Uneven performance improvements across different financial subdomains suggest incomplete coverage
- Evaluation benchmarks are specific to Japanese financial contexts, limiting broader generalization assessment

## Confidence
- **High confidence**: The mechanism that JaFIn improves financial domain adaptation through instruction tuning with government-sourced data is well-supported by the experimental results showing improved benchmark performance.
- **Medium confidence**: The effectiveness of LoRA-based instruction tuning for efficient domain adaptation is supported, though the specific hyperparameter choices and their sensitivity are not fully explored.
- **Medium confidence**: The claim that diverse government-sourced data improves model robustness is plausible given the performance improvements, but the uneven performance across different financial tasks suggests incomplete domain coverage.

## Next Checks
1. **Dataset coverage analysis**: Map JaFIn samples to financial subdomains and identify gaps where performance lags (e.g., auditing vs. sales tasks) to guide future data collection.
2. **Hyperparameter sensitivity testing**: Systematically vary LoRA rank (r) and scaling factor (α) to determine optimal values and identify overfitting thresholds with the current dataset size.
3. **Generalization evaluation**: Test instruction-tuned models on non-financial Japanese tasks to quantify catastrophic forgetting and ensure balanced capabilities are maintained.