---
ver: rpa2
title: 'Evaluating Large Language Models with Grid-Based Game Competitions: An Extensible
  LLM Benchmark and Leaderboard'
arxiv_id: '2407.07796'
source_url: https://arxiv.org/abs/2407.07796
tags:
- llms
- game
- player
- games
- prompt
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'The study introduces an extensible benchmark using grid-based
  games to evaluate large language models (LLMs). A web-based simulation platform
  allows LLMs to compete in Tic-Tac-Toe, Connect Four, and Gomoku using three prompt
  types: list, illustration, and image.'
---

# Evaluating Large Language Models with Grid-Based Game Competitions: An Extensible LLM Benchmark and Leaderboard

## Quick Facts
- **arXiv ID**: 2407.07796
- **Source URL**: https://arxiv.org/abs/2407.07796
- **Reference count**: 40
- **Primary result**: Benchmark reveals significant LLM performance variations across games and prompt types, with open-source code available for community contributions

## Executive Summary
This study introduces a novel benchmark using grid-based games to evaluate large language models' rule comprehension and strategic thinking. The researchers developed a web-based simulation platform where seven leading LLMs compete in Tic-Tac-Toe, Connect Four, and Gomoku using three distinct prompt formats: list, illustration, and image. Through 2,310 matches, the study revealed substantial performance differences across games and prompts, highlighting challenges with complex and visually-based prompts. The open-source nature of the code and data files enables ongoing testing and leaderboard submissions, creating an extensible framework for LLM evaluation.

## Method Summary
The benchmark employs a web-based simulation platform that dynamically updates game states and records detailed interactions between LLMs. Seven leading LLMs were tested across three games using three prompt types: list (text enumeration of moves), illustration (symbolic grid representation), and image (base64-encoded game board snapshots). The simulation software generates matches, validates moves, and records performance metrics including win rates, disqualification rates, missed opportunities, and invalid moves. Results are stored in JSON, CSV, TXT, and PNG formats and are available on GitHub for community access and leaderboard submissions.

## Key Results
- Significant performance variations across different games and prompt types
- List prompts consistently outperformed illustration and image prompts
- Disqualification rates varied substantially between LLMs, with some models showing high rates of invalid moves
- Missed opportunities to win or block opponent's win were more frequent with complex prompt types

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The benchmark works because it isolates LLM reasoning ability from domain-specific training by using games that are "solved" but not in LLM pretraining corpora.
- Mechanism: By selecting games with known optimal strategies but no explicit training exposure, the evaluation measures general rule comprehension and strategic planning rather than memorization.
- Core assumption: LLMs do not have embedded knowledge of optimal game strategies for Tic-Tac-Toe, Connect Four, or Gomoku.
- Evidence anchors:
  - [abstract] "This study enhances our understanding of LLMs' capabilities in playing games they were not specifically trained for, helping to assess their rule comprehension and strategic thinking."
  - [section] "We chose LLMs that are not specifically trained for the games used in the benchmark."
  - [corpus] Weak: No direct corpus evidence about game training data, but related work focuses on reasoning evaluation rather than memorized game strategies.
- Break condition: If LLMs are found to have been trained on game datasets or if the games are too simple to differentiate reasoning ability from pattern matching.

### Mechanism 2
- Claim: The three prompt types (list, illustration, image) create a gradient of cognitive load that reveals LLM limitations in processing different data formats.
- Mechanism: List prompts require parsing text coordinates, illustration prompts require interpreting symbolic grid representations, and image prompts require visual reasoning - each step increasing processing complexity.
- Core assumption: LLM performance degrades predictably as prompt format complexity increases, revealing specific capability gaps.
- Evidence anchors:
  - [abstract] "The results revealed significant variations in LLM performance across different games and prompt types"
  - [section] "The 'list' prompt enumerates previous moves for each player in a 'row, column' format. The 'illustration' prompt depicts the current state of the grid using specific symbols... The 'image' prompt visualizes the current state by providing a snapshot of the game board."
  - [corpus] Strong: Related work explicitly uses multiple prompt formats to evaluate LLM reasoning capabilities.
- Break condition: If performance variation across prompt types is minimal or inconsistent with expected complexity progression.

### Mechanism 3
- Claim: The leaderboard system creates competitive pressure that drives community contributions and continuous benchmark improvement.
- Mechanism: Open submission of results from new LLMs incentivizes ongoing testing and expansion of the benchmark, creating a living evaluation framework.
- Core assumption: Researchers and developers will actively contribute new results to maintain competitive positioning.
- Evidence anchors:
  - [abstract] "We also encourage submissions of results from other LLMs"
  - [section] "We provide game simulation software in the benchmark that generates submission files and encourage new submissions to the leaderboard."
  - [corpus] Weak: While the leaderboard concept is mentioned, there's limited evidence of actual community engagement or submission volume.
- Break condition: If submission rates are low or if the leaderboard becomes static with few contributors.

## Foundational Learning

- Concept: Grid-based game mechanics and rules
  - Why needed here: Understanding the fundamental rules of Tic-Tac-Toe, Connect Four, and Gomoku is essential for designing prompts and evaluating LLM performance
  - Quick check question: What is the winning condition for Connect Four, and how does it differ from Tic-Tac-Toe?
- Concept: Prompt engineering for structured outputs
- Concept: JSON data structure and API integration
  - Why needed here: The benchmark relies on LLMs providing moves in specific JSON format through web APIs
  - Quick check question: What JSON format is required for LLM move submissions, and why is this format important for evaluation?

## Architecture Onboarding

- Component map: Web-based simulation interface (JavaScript/HTML/CSS) -> AWS Lambda/Python backend -> LLM web APIs (OpenAI, Google, Amazon Bedrock) -> JSON/CSV/TXT/PNG data storage -> GitHub leaderboard
- Critical path: User selects game/LLMs -> Prompt generation -> API call to LLM -> Move validation -> Game state update -> Data recording -> Result aggregation
- Design tradeoffs: Text-based prompts vs. image prompts (simplicity vs. realism), synchronous vs. asynchronous API calls, local storage vs. cloud-based data management
- Failure signatures: LLM invalid move rates, API timeout errors, data format inconsistencies, browser compatibility issues
- First 3 experiments:
  1. Test single game with two LLMs using list prompt to verify basic functionality
  2. Test all three prompt types with one LLM to evaluate format impact
  3. Test random play against multiple LLMs to establish baseline performance

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do LLMs perform in more complex games like chess or Go compared to the simpler grid-based games tested in this study?
- Basis in paper: [explicit] The paper mentions the potential to expand the benchmark to include more complex games like chess or Go.
- Why unresolved: The study only evaluated LLMs on Tic-Tac-Toe, Connect Four, and Gomoku, which are simpler games.
- What evidence would resolve it: Testing LLMs on chess or Go using the same benchmarking framework and comparing their performance.

### Open Question 2
- Question: How does the performance of LLMs change with different prompt structures or more advanced prompt engineering techniques?
- Basis in paper: [explicit] The paper suggests that future research should investigate how variations in prompt structure might affect LLMs' understanding of game states and moves.
- Why unresolved: The study used a fixed set of prompt types (list, illustration, image) without exploring variations in structure.
- What evidence would resolve it: Conducting experiments with different prompt structures and analyzing their impact on LLM performance.

### Open Question 3
- Question: Can LLMs effectively collaborate with humans or other LLMs in multi-agent scenarios?
- Basis in paper: [inferred] The paper mentions the potential for future research on multi-agent collaboration scenarios.
- Why unresolved: The study focused on one-on-one games between LLMs and did not explore collaborative scenarios.
- What evidence would resolve it: Designing experiments where LLMs work together in team-based games or collaborative tasks and assessing their performance.

## Limitations

- The benchmark is based on only three grid-based games, representing a narrow slice of strategic reasoning tasks
- Limited evidence of actual community engagement or sustained contribution to the leaderboard system
- Methodology for handling image-based prompts lacks detailed documentation on processing and distinguishing failures

## Confidence

**High confidence**: The core mechanism that different prompt types create varying cognitive loads is well-supported by experimental design and consistent with related work on LLM evaluation.

**Medium confidence**: The claim that selecting games not explicitly trained in LLM corpora provides valid measure of general reasoning ability is reasonable but not fully validated without corpus analysis.

**Low confidence**: The leaderboard's effectiveness as a community-driven benchmark depends on future participation patterns that cannot be evaluated from current results.

## Next Checks

1. Analyze the pretraining data of evaluated LLMs to confirm Tic-Tac-Toe, Connect Four, and Gomoku are not explicitly represented, validating the benchmark's claim of measuring reasoning rather than memorization.

2. Track submission rates to the leaderboard over a 6-month period to assess whether the benchmark achieves sustained community engagement beyond the initial study.

3. Test whether LLMs that perform well on this grid-based benchmark also demonstrate superior performance on structurally different reasoning tasks (e.g., logical puzzles, planning problems) to validate generalizability of findings.