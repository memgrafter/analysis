---
ver: rpa2
title: Can Large Language Model Predict Employee Attrition?
arxiv_id: '2411.01353'
source_url: https://arxiv.org/abs/2411.01353
tags:
- employee
- attrition
- data
- dataset
- machine
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study evaluated large language models (LLMs) for predicting
  employee attrition using the IBM HR Analytics dataset. A fine-tuned GPT-3.5 model
  was compared against traditional ML classifiers including Logistic Regression, KNN,
  SVM, Decision Tree, Random Forest, AdaBoost, and XGBoost.
---

# Can Large Language Model Predict Employee Attrition?

## Quick Facts
- arXiv ID: 2411.01353
- Source URL: https://arxiv.org/abs/2411.01353
- Reference count: 23
- Primary result: Fine-tuned GPT-3.5 achieves 0.92 F1-score, outperforming traditional ML models for employee attrition prediction

## Executive Summary
This study evaluates large language models (LLMs) for predicting employee attrition using the IBM HR Analytics dataset. The researchers compared a fine-tuned GPT-3.5 model against traditional machine learning classifiers including Logistic Regression, KNN, SVM, Decision Tree, Random Forest, AdaBoost, and XGBoost. The fine-tuned GPT-3.5 achieved the highest performance with precision of 0.91, recall of 0.94, and F1-score of 0.92. The best traditional model, SVM, reached an F1-score of 0.82, while ensemble methods like Random Forest and XGBoost achieved 0.80. Results demonstrate that fine-tuned LLMs can outperform traditional ML approaches in capturing complex patterns in employee behavior and attrition risks, offering organizations improved insights for retention strategies.

## Method Summary
The study used the IBM HR Analytics Employee Attrition dataset with 1,470 samples and 35 attributes. Data preprocessing included removing constant columns (EmployeeCount, StandardHours, Over18, EmployeeNumber), handling skewness with log(1+x) transformation, creating composite features (WorkExperience, OverallSatisfaction), encoding categorical variables with LabelEncoder, and splitting data (80/20) with SMOTE for class imbalance. Traditional ML models were trained with specified parameters, and GPT-3.5 Turbo was fine-tuned using JSONL prompt-response pairs. All models were evaluated using precision, recall, and F1-score metrics.

## Key Results
- Fine-tuned GPT-3.5 achieved the highest performance with precision of 0.91, recall of 0.94, and F1-score of 0.92
- Best traditional model (SVM) reached an F1-score of 0.82, while ensemble methods (Random Forest, XGBoost) achieved 0.80
- Results demonstrate that fine-tuned LLMs can outperform traditional ML approaches in capturing complex patterns in employee behavior and attrition risks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Fine-tuning GPT-3.5 on structured HR data significantly improves its predictive performance for employee attrition.
- Mechanism: By training the LLM on labeled prompt-response pairs constructed from the IBM HR dataset, the model learns to map structured employee attributes into a classification decision (attrition yes/no).
- Core assumption: The LLM's underlying transformer architecture can effectively process structured tabular data when provided in natural language format via prompts.
- Evidence anchors:
  - [abstract] "Through our analysis, we aim to provide practical insights for organizations seeking to enhance their employee retention strategies with advanced predictive tools."
  - [section] "GPT-3.5 Turbo Fine-tuning. We used the OpenAI library and API to fine-tune the GPT model with our data."
  - [corpus] No direct evidence of LLM fine-tuning for structured prediction; corpus shows LLMs are primarily evaluated in NLP tasks.
- Break condition: If the prompt-response format fails to preserve the semantic relationships between features, or if the model overfits to the specific phrasing of training prompts.

### Mechanism 2
- Claim: Traditional ML models such as SVM and ensemble methods achieve high performance due to their ability to handle high-dimensional, structured numerical data with explicit feature engineering.
- Mechanism: These models learn explicit decision boundaries or weighted combinations of features that separate attrition vs. non-attrition cases.
- Core assumption: The IBM HR dataset contains informative, non-redundant features that can be effectively modeled by classical ML algorithms.
- Evidence anchors:
  - [abstract] "Our results show that the fine-tuned GPT-3.5 large language model (LLM) outperforms traditional machine learning approaches in terms of prediction accuracy..."
  - [section] "We used the IBM HR Analytics Employee Attrition dataset to compare the performance of a fine-tuned GPT-3.5 model with classic machine learning models..."
  - [corpus] Weak: The corpus neighbors focus on ML-based attrition prediction, but do not directly compare to LLM fine-tuning.
- Break condition: If feature engineering steps introduce bias or remove critical information, or if class imbalance is not adequately handled.

### Mechanism 3
- Claim: The superior F1-score of the fine-tuned GPT-3.5 (0.92) versus traditional models (SVM: 0.82) indicates that LLMs can capture more complex, non-linear relationships in the data.
- Mechanism: The transformer-based architecture can implicitly learn higher-order interactions between features that traditional models require explicit encoding for.
- Core assumption: The transformer's self-attention mechanism can effectively process the concatenated or formatted employee data even without explicit NLP input.
- Evidence anchors:
  - [abstract] "Our findings show that the fine-tuned GPT-3.5 model outperforms traditional machine learning approaches with a precision of 0.91, recall of 0.94, and an F1-score of 0.92..."
  - [section] "The fine-tuned GPT-3.5 achieved the highest performance with precision of 0.91, recall of 0.94, and F1-score of 0.92."
  - [corpus] No direct evidence in corpus for LLM performance on structured HR data; corpus neighbors focus on ML-only approaches.
- Break condition: If the model overfits to the training set due to the small size of the dataset (1470 samples) or if prompt formatting introduces spurious correlations.

## Foundational Learning

- Concept: Feature scaling and standardization
  - Why needed here: Ensures that numerical features contribute equally to model training, especially important for algorithms like SVM and logistic regression.
  - Quick check question: Why is it necessary to fit the scaler on the training data only and then apply it to both training and test data?

- Concept: Handling class imbalance with SMOTE
  - Why needed here: The attrition class is much smaller than non-attrition (16.1% vs 83.9%), which can bias models toward the majority class.
  - Quick check question: How does SMOTE generate synthetic samples, and why does this help balance the dataset without simply duplicating existing minority samples?

- Concept: Label encoding for categorical features
  - Why needed here: ML algorithms require numerical inputs; categorical features must be converted to numeric codes.
  - Quick check question: What is the difference between label encoding and one-hot encoding, and when might one be preferred over the other in this context?

## Architecture Onboarding

- Component map: Data preprocessing -> Model training (ML and LLM) -> Evaluation -> Comparison
- Critical path: Data preparation (cleaning, encoding, scaling, balancing) -> Model training -> Prediction on test set -> Metric calculation (precision, recall, F1)
- Design tradeoffs: Using a fine-tuned LLM trades interpretability and cost for potentially higher predictive accuracy; traditional models are faster and more transparent but may miss complex patterns.
- Failure signatures: Poor performance on minority class, overfitting due to small dataset, or degraded accuracy if feature engineering removes critical signals.
- First 3 experiments:
  1. Train and evaluate a logistic regression baseline on the original dataset (before scaling and SMOTE) to establish a performance floor.
  2. Apply SMOTE and scaling, retrain logistic regression and SVM, and compare F1-scores to assess the impact of preprocessing.
  3. Fine-tune GPT-3.5 on the preprocessed training set and evaluate on the test set; compare all model performances using the same metrics.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of GPT-3.5 for employee attrition prediction vary across different industry sectors and organizational sizes?
- Basis in paper: [explicit] The paper mentions "diverse fields" and "human resource management" but only tests on one dataset (IBM HR Analytics)
- Why unresolved: The study uses a single dataset from one organization, limiting generalizability across different industries and company sizes
- What evidence would resolve it: Testing GPT-3.5 on industry-specific HR datasets from different sectors (healthcare, tech, manufacturing) and organizations of varying sizes to compare performance metrics

### Open Question 2
- Question: What specific linguistic patterns and semantic features does GPT-3.5 identify that traditional ML models miss in employee attrition prediction?
- Basis in paper: [explicit] The paper states GPT-3.5 can "identify subtle linguistic cues and recurring themes" but doesn't specify what these are
- Why unresolved: The study reports superior performance but doesn't analyze or document the specific features that contribute to this advantage
- What evidence would resolve it: Detailed linguistic analysis comparing feature importance between GPT-3.5 and traditional models, identifying specific language patterns GPT-3.5 captures

### Open Question 3
- Question: How does the cost-benefit ratio of fine-tuning GPT-3.5 compare to implementing traditional ML solutions for employee attrition prediction in real organizations?
- Basis in paper: [inferred] The paper demonstrates GPT-3.5's superior performance but doesn't address implementation costs or resource requirements
- Why unresolved: The study focuses on predictive accuracy without considering practical deployment factors like computational costs, training time, and maintenance requirements
- What evidence would resolve it: Comprehensive cost analysis comparing infrastructure requirements, fine-tuning expenses, and ongoing operational costs between GPT-3.5 and traditional ML solutions across multiple organizations

## Limitations
- Small dataset size (1,470 samples) raises concerns about overfitting, particularly for the fine-tuned LLM
- Specific prompt-response format for LLM training is not detailed, making exact reproduction challenging
- Study lacks comparison with modern deep learning architectures designed for tabular data

## Confidence

- **High Confidence**: The methodology for traditional ML model training and evaluation is well-specified and follows standard practices. The dataset preprocessing steps (handling imbalance with SMOTE, feature engineering, and scaling) are clearly described and appropriate.
- **Medium Confidence**: The claim that GPT-3.5 outperforms traditional models is supported by the reported metrics, but the exact fine-tuning procedure and prompt design remain underspecified, limiting reproducibility and confidence in the mechanism.
- **Low Confidence**: The generalizability of LLM superiority across different HR datasets or organizational contexts is not established due to the single-dataset evaluation and lack of external validation.

## Next Checks

1. **Statistical Significance Testing**: Perform paired t-tests or McNemar's test on model predictions to determine if the performance differences between GPT-3.5 and the best traditional model (SVM) are statistically significant.
2. **External Dataset Validation**: Evaluate the fine-tuned GPT-3.5 and top-performing traditional models on a separate, independently sourced HR attrition dataset to assess generalizability beyond the IBM data.
3. **Prompt Format Ablation**: Systematically vary the prompt structure (e.g., different orderings of features, inclusion/exclusion of feature names) to determine how sensitive the LLM's performance is to prompt formatting and whether the results are robust to these changes.