---
ver: rpa2
title: Language Portability Strategies for Open-domain Dialogue with Pre-trained Language
  Models from High to Low Resource Languages
arxiv_id: '2407.01315'
source_url: https://arxiv.org/abs/2407.01315
tags:
- language
- dialogue
- bloom
- open-domain
- plms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper presents a comparative study of three strategies for
  porting open-domain dialogue models from English (high-resource) to French (simulated
  low-resource): TestOnSource (translate at inference), TrainOnTarget (translate dataset
  before fine-tuning), and TrainOnSourceAdaptOnTarget (use multilingual BLOOM with
  MAD-X adapters). Models were evaluated via human ratings on coherence, engagingness,
  and humanness across 140 conversations.'
---

# Language Portability Strategies for Open-domain Dialogue with Pre-trained Language Models from High to Low Resource Languages

## Quick Facts
- arXiv ID: 2407.01315
- Source URL: https://arxiv.org/abs/2407.01315
- Reference count: 29
- Key outcome: BLOOM-fr (TrainOnTarget) outperformed all other approaches in human evaluation, while automatic metrics showed weak or inverse correlation with human ratings

## Executive Summary
This paper presents a comparative study of three strategies for porting open-domain dialogue models from English to French: translating at inference (TestOnSource), translating dataset before fine-tuning (TrainOnTarget), and using multilingual BLOOM with MAD-X adapters (TrainOnSourceAdaptOnTarget). Human evaluation on 140 conversations showed BLOOM-fr (TrainOnTarget) achieved the highest scores for coherence, engagingness, and humanness. MAD-X models underperformed, possibly due to under-trained adapters. The study highlights that BLEU and perplexity poorly correlate with human judgments, emphasizing the need for human evaluation in dialogue quality assessment.

## Method Summary
The study compared three language portability strategies using the PersonaChat dataset translated from English to French. Three model variants were evaluated: GPT-fr (monolingual French), BLOOM (multilingual with French and English versions), and MAD-X-BLOOM (adapter-based multilingual approach). Models were fine-tuned using a double-head architecture and deployed on Rasa-X for human dialogue collection. Evaluation included human ratings on coherence, engagingness, and humanness, plus automatic metrics (BLEU, perplexity, Hits@1/3).

## Key Results
- BLOOM-fr (TrainOnTarget) achieved the highest human evaluation scores across all metrics
- MAD-X models underperformed, potentially due to under-trained adapters
- Automatic metrics (BLEU, perplexity) showed weak or inverse correlation with human ratings
- TrainOnTarget strategy with multilingual BLOOM proved most effective for cross-lingual dialogue transfer

## Why This Works (Mechanism)

### Mechanism 1
Training on target-language data (even if translated) allows models to internalize language-specific nuances that cannot be fully captured by inference-time translation. The multilingual BLOOM model leverages shared representations across languages, reducing translation noise impact during training. Core assumption: BLOOM's multilingual pretraining provides robust cross-lingual representations less sensitive to noisy translations. Evidence: BLOOM-fr (TrainOnTarget) outperformed all other models, likely due to multilingual pretraining robustness to translation noise.

### Mechanism 2
Human evaluation metrics are necessary because automatic metrics like BLEU and perplexity measure surface-level features that may not reflect meaningful dialogue quality. Dialogue quality is inherently subjective and context-dependent. Core assumption: The subjective nature of dialogue quality means automatic metrics cannot capture all aspects humans value. Evidence: BLEU and perplexity were weakly or inversely correlated with human ratings, confirming the need for human evaluation.

### Mechanism 3
MAD-X adapter architecture improves cross-lingual transfer but may underperform if adapters are under-trained. The sequential training of language adapters followed by task adapters is effective only if each stage receives adequate training. Core assumption: The MAD-X architecture works well when adapters are properly trained. Evidence: MAD-X models were the worst performers, possibly due to under-trained adapters.

## Foundational Learning

- Concept: Cross-lingual transfer learning
  - Why needed here: Understanding how knowledge transfers across languages is fundamental to evaluating the three approaches. Quick check: What are the key challenges in transferring NLP models across languages with different resource levels?

- Concept: Adapter-based fine-tuning
  - Why needed here: MAD-X uses adapters to adapt a multilingual PLM to a new language and task. Understanding how adapters work is crucial for interpreting results. Quick check: How do adapter-based methods reduce computational cost compared to full model fine-tuning?

- Concept: Human evaluation in dialogue systems
  - Why needed here: The paper emphasizes human evaluation over automatic metrics. Understanding human evaluation principles is essential for interpreting methodology. Quick check: What are the key considerations when designing human evaluation protocols for open-domain dialogue systems?

## Architecture Onboarding

- Component map: PersonaChat (English) → Google Translate API → French PersonaChat → Fine-tune models → Deploy on Rasa-X → Collect human dialogues → Human annotation → Analyze results
- Critical path: Translate PersonaChat → Fine-tune model on translated data → Deploy on Rasa-X → Collect human dialogues → Annotate dialogues → Analyze results
- Design tradeoffs:
  - Automatic translation vs. human translation: Cost and scalability vs. quality
  - Monolingual vs. multilingual models: Language-specific optimization vs. cross-lingual transfer
  - Adapter-based vs. full fine-tuning: Computational efficiency vs. potential performance
- Failure signatures:
  - Low human ratings across all models: Dataset or translation quality issues
  - Disparity between automatic metrics and human ratings: Need to reconsider evaluation approach
  - MAD-X models underperforming: Adapter training issues or insufficient multilingual representation
- First 3 experiments:
  1. Replicate TrainOnTarget with a larger translated dataset to test scalability
  2. Implement a fine-tuned version of the MAD-X approach with longer training for adapters
  3. Test TestOnSource with a different NMT system to evaluate dependency on translation quality

## Open Questions the Paper Calls Out
- Why did MAD-X adapter-based models underperform despite showing improved automatic metrics?
- What specific aspects of BLOOM's multilingual pretraining make it more robust to translation noise compared to monolingual models?
- How do human evaluation ratings correlate with actual dialogue quality metrics like task completion or user satisfaction?

## Limitations
- Translation quality from Google Translate API was not verified with human evaluation
- MAD-X adapter training parameters were not fully specified, making performance attribution uncertain
- Human evaluation protocol lacked complete transparency regarding guidelines and rater training

## Confidence
- High Confidence: BLOOM-fr (TrainOnTarget) outperforming English models with translation at inference
- Medium Confidence: BLEU and perplexity not correlating with human ratings
- Low Confidence: MAD-X underperformance attributed to under-trained adapters

## Next Checks
1. Replicate MAD-X training with extended language adapter training (2-3x duration) on the same Wikipedia corpus
2. Conduct controlled translation quality assessment by having bilingual human evaluators rate translated PersonaChat
3. Implement ablation study comparing full fine-tuning vs. adapter-based approaches with matched training compute