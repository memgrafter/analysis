---
ver: rpa2
title: Enhancing Traffic Prediction with Textual Data Using Large Language Models
arxiv_id: '2405.06719'
source_url: https://arxiv.org/abs/2405.06719
tags:
- large
- prediction
- language
- information
- traf
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes using LLM embeddings for traffic prediction.
  The method processes textual context (e.g., weather, holidays, events) through a
  large language model to obtain embeddings, which are then combined with historical
  traffic data and fed into traditional spatiotemporal forecasting models.
---

# Enhancing Traffic Prediction with Textual Data Using Large Language Models

## Quick Facts
- arXiv ID: 2405.06719
- Source URL: https://arxiv.org/abs/2405.06719
- Reference count: 40
- This paper proposes using LLM embeddings for traffic prediction, reducing prediction errors when combined with historical traffic data

## Executive Summary
This paper introduces a novel approach to enhance traffic prediction by incorporating textual context through large language model (LLM) embeddings. The method processes textual information such as weather, holidays, and events into numerical embeddings, which are then combined with historical traffic data and fed into traditional spatiotemporal forecasting models. Two scenario types are considered: regional-level (city-wide) and node-level (specific grid). Experiments on the New York Bike dataset demonstrate that incorporating LLM embeddings reduces prediction errors for both overall regions and specific grids, validating the effectiveness of the approach.

## Method Summary
The method processes textual context through a large language model to obtain embeddings, which are combined with historical traffic data and fed into traditional spatiotemporal forecasting models. Two scenario types are considered: regional-level (city-wide) and node-level (specific grid). Regional-level information is encoded as a connected auxiliary node for all grids, while node-level information is encoded as an auxiliary node only connected to the corresponding grid. Experiments on the New York Bike dataset show that incorporating LLM embeddings reduces prediction errors for both overall regions and specific grids, validating the effectiveness of the approach.

## Key Results
- Incorporating LLM embeddings reduces prediction errors for both regional-level and node-level traffic prediction scenarios
- The method demonstrates improved accuracy on the New York Bike dataset with 169 grids and 6-hour time windows
- Regional-level auxiliary nodes connect to all grids, while node-level auxiliary nodes connect only to the corresponding grid

## Why This Works (Mechanism)

### Mechanism 1
The method reduces prediction errors by converting textual context into numerical embeddings via a large language model, allowing integration with traditional spatiotemporal models. LLM embeddings capture semantic meaning of text and are concatenated with historical traffic data, preserving spatial relationships via auxiliary nodes. The core assumption is that textual information is sufficiently represented by LLM embeddings and retains predictive value when combined with numerical traffic features. If LLM embeddings lose critical semantic information during PCA or concatenation, the predictive benefit disappears.

### Mechanism 2
The auxiliary node design preserves spatial relationships while incorporating non-numerical context. Regional-level context is represented as a single node connected to all grids; node-level context is an additional node connected only to the corresponding grid. The core assumption is that the adjacency matrix structure can be extended with auxiliary nodes without disrupting existing spatial correlations. If the auxiliary node connections overwhelm or dilute the existing spatial graph structure, prediction accuracy may degrade.

### Mechanism 3
Using LLM embeddings avoids the high cost and non-determinism of directly querying LLMs for predictions. LLM embeddings are pre-computed once and reused, decoupling expensive model inference from prediction pipeline. The core assumption is that embeddings remain stable and sufficient for prediction tasks over the study period. If embeddings vary significantly across runs or LLM updates, consistency is lost and the advantage vanishes.

## Foundational Learning

- **Spatiotemporal graph neural networks**: Traffic prediction requires modeling both spatial relationships (between districts) and temporal dynamics (historical usage). Quick check: Can a plain RNN or CNN handle both spatial graphs and time series without extra structure?

- **Principal component analysis (PCA) for dimensionality reduction**: LLM embeddings are high-dimensional; PCA retains most variance while reducing computational load. Quick check: What happens if we keep 95% vs 99% variance—how does prediction error change?

- **Auxiliary node integration in graph models**: Textual context must be injected into graph structure without destroying existing edges. Quick check: How does adding a fully connected auxiliary node affect the adjacency matrix and subsequent graph convolutions?

## Architecture Onboarding

- **Component map**: Text preprocessing → LLM embedding → PCA → auxiliary node creation → extended adjacency matrix → spatiotemporal model → prediction
- **Critical path**: 
  1. Obtain embeddings from LLM for all relevant text features
  2. Apply PCA to reduce dimensionality while retaining ≥95% variance
  3. Construct auxiliary node(s) and update adjacency matrix
  4. Concatenate embeddings with historical traffic data
  5. Feed into chosen spatiotemporal model (e.g., STGCN, DCRNN)
- **Design tradeoffs**:
  - High embedding dimensionality → more expressive but slower
  - Aggressive PCA → faster but potential loss of nuance
  - Auxiliary node fully connected vs partial → simpler but may add noise
- **Failure signatures**:
  - Prediction error spikes after embedding step → embedding quality or PCA over-reduction
  - Overfitting to training set → too many auxiliary nodes or excessive embedding dimensions
  - Model crashes → adjacency matrix shape mismatch or incompatible input dimensions
- **First 3 experiments**:
  1. Baseline: Run without any textual embeddings; record MAE/RMSE
  2. Embedding only: Add LLM embeddings without PCA; compare error
  3. Full pipeline: Apply PCA, auxiliary nodes, and compare against baseline

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the dimensionality of the LLM embeddings affect the prediction accuracy of the traffic models?
- Basis in paper: The paper mentions using PCA to reduce the dimensionality of LLM embeddings, retaining 95% of the variance, but does not explore the impact of different dimensionality levels on model performance
- Why unresolved: The study does not investigate how varying the dimensionality of the embeddings influences the accuracy of traffic predictions, which could provide insights into optimizing the model
- What evidence would resolve it: Conducting experiments with different dimensionality levels of LLM embeddings and comparing their impact on prediction accuracy would clarify the optimal embedding size for traffic prediction tasks

### Open Question 2
- Question: How do LLM embeddings perform in predicting traffic patterns during unprecedented events not covered in the training data?
- Basis in paper: The paper suggests that LLMs can generalize to exceptional circumstances due to their inherent world knowledge, but does not test this capability with truly unprecedented events
- Why unresolved: The study does not include scenarios with completely novel events to assess the LLMs' ability to predict traffic patterns accurately without prior exposure
- What evidence would resolve it: Testing the model's performance on traffic prediction during events that are entirely new or rare would demonstrate the LLMs' generalization capabilities

## Limitations
- The paper lacks specific technical details about the LLM embedding extraction process, including which model was used and how embeddings were generated from text
- PCA dimensionality reduction parameters are unspecified, making it difficult to reproduce the exact embedding characteristics
- The paper does not address computational efficiency comparisons or potential overfitting risks from high-dimensional embeddings

## Confidence

**High Confidence**: The core mechanism of using LLM embeddings as auxiliary nodes in spatiotemporal models is technically sound and well-supported by the experimental results showing reduced MAE and RMSE on the New York Bike dataset.

**Medium Confidence**: The effectiveness of PCA for dimensionality reduction is supported by standard practices, but the specific variance threshold and its impact on prediction accuracy are not detailed.

**Low Confidence**: The paper does not provide sufficient detail about the LLM model selection, embedding extraction methodology, or sensitivity analysis for PCA parameters, making full reproducibility challenging.

## Next Checks

1. **Reproduce baseline results**: Implement the baseline spatiotemporal models without LLM embeddings to establish reference MAE/RMSE values for comparison.

2. **Sensitivity analysis for PCA**: Systematically vary the variance retention threshold (95%, 97.5%, 99%) and measure the impact on prediction accuracy and computational efficiency.

3. **Ablation study**: Remove the auxiliary node connections and compare prediction performance to isolate the contribution of the graph structure modification versus the embeddings themselves.