---
ver: rpa2
title: Conjugate-Gradient-like Based Adaptive Moment Estimation Optimization Algorithm
  for Deep Learning
arxiv_id: '2404.01714'
source_url: https://arxiv.org/abs/2404.01714
tags:
- uni00000003
- uni00000013
- uni00000010
- uni00000014
- uni00000048
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes CG-like-Adam, a new optimization algorithm
  for deep learning that combines conjugate gradient methods with Adam-style adaptive
  moment estimation. The key innovation is rectifying vanilla conjugate gradient by
  scaling the conjugate coefficient with a positive real monotonic decreasing sequence,
  creating a "conjugate-gradient-like" direction.
---

# Conjugate-Gradient-like Based Adaptive Moment Estimation Optimization Algorithm for Deep Learning

## Quick Facts
- arXiv ID: 2404.01714
- Source URL: https://arxiv.org/abs/2404.01714
- Authors: Jiawu Tian; Liwei Xu; Xiaowei Zhang; Yongqi Li
- Reference count: 40
- Key outcome: Proposes CG-like-Adam, an optimization algorithm combining conjugate gradient methods with Adam-style adaptive moment estimation, demonstrating faster convergence and better generalization on CIFAR-10 and CIFAR-100 datasets compared to Adam and CoBA.

## Executive Summary
This paper introduces CG-like-Adam, a novel optimization algorithm that addresses convergence issues in existing adaptive methods by incorporating conjugate-gradient-like directions into Adam's framework. The key innovation is rectifying vanilla conjugate gradient by scaling the conjugate coefficient with a positive real monotonic decreasing sequence, creating a direction that combines the benefits of conjugate gradient with Adam's adaptive learning rates. The algorithm provides theoretical convergence guarantees for challenging cases, including constant exponential moving average coefficients and unbiased moment estimations. Experiments on CIFAR-10 and CIFAR-100 using VGG-19 and ResNet-34 demonstrate superior performance, with faster convergence to 100% training accuracy and higher testing accuracy indicating better generalization ability.

## Method Summary
CG-like-Adam modifies the Adam optimizer by replacing both first-order and second-order moment estimations with conjugate-gradient-like approaches. The algorithm calculates a conjugate-gradient-like direction by scaling the conjugate coefficient with a parameter a > 1/2, then uses this direction in unbiased moment estimations. The first moment estimation is computed as mt = β1t·mt-1 + (1-β1t)·dt, with bias correction to ˆmt = mt/(1-βt). The second moment estimation maintains the maximum of all past squared conjugate gradient values, computed as vt = β2·vt-1 + (1-β2)·d²t, then ˆvt = max{vt/(1-β2), vt-1}. Parameters are updated using xt+1 = xt - αt·ˆV⁻¹/²t · ˆmt, where αt is the learning rate. The algorithm provides O(1/√T) convergence rate guarantees even with constant exponential moving average coefficients.

## Key Results
- Achieves faster convergence, reaching 100% training accuracy more quickly than Adam and CoBA
- Delivers higher testing accuracy on CIFAR-10 and CIFAR-100 datasets, indicating better generalization
- Provides theoretical convergence guarantees for challenging cases including constant β1t and unbiased moment estimations
- Maintains O(1/√T) convergence rate under the specified conditions

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The "conjugate-gradient-like" direction resolves Adam's convergence issues by incorporating curvature information from previous gradients
- Mechanism: The algorithm modifies vanilla conjugate gradient by scaling the conjugate coefficient with a positive real monotonic decreasing sequence a*t. This creates a direction that combines the benefits of conjugate gradient (using past gradient information for better descent directions) with Adam's adaptive learning rates. The key modification prevents the divergence that occurs when directly replacing Adam's gradient with vanilla conjugate gradient.
- Core assumption: The scaling factor a ∈ (1/2, +∞) ensures sufficient decay of the conjugate coefficient while maintaining enough momentum from previous directions
- Evidence anchors: [abstract] "rectify the vanilla conjugate gradient as conjugate-gradient-like and incorporate it into the generic Adam"; [section 3.1] "the conjugate coefficient is scaled by a positive real monotonic decreasing sequence depending on the number of iterations"
- Break condition: If a is too large (>1), the conjugate coefficient doesn't decay fast enough, potentially causing instability. If a is too close to 1/2, the algorithm loses the benefits of conjugate gradient direction.

### Mechanism 2
- Claim: The unbiased moment estimations with maximum operator ensure stable convergence even with constant exponential moving average coefficients
- Mechanism: Both first-order (mt) and second-order (vt) moment estimations are modified to be unbiased by dividing by (1-βt). The second-order moment also maintains the maximum of all past squared conjugate gradient values (vt = max{vt-1, vt}). This addresses convergence issues that arise when the exponential moving average coefficient β1t is constant, which is a challenging case for existing Adam-type methods.
- Core assumption: The maximum operator prevents the second moment estimation from decreasing too rapidly, maintaining adequate adaptive step sizes throughout training
- Evidence anchors: [abstract] "convergence analysis handles the cases where the exponential moving average coefficient of the first-order moment estimation is constant"; [section 3.1] "the maximum of all ˆvt until the current time step t is also maintained"
- Break condition: If the maximum operator is removed, the algorithm may fail to converge when β1t is constant, as demonstrated in convergence analysis for existing methods.

### Mechanism 3
- Claim: The algorithm achieves faster convergence and better generalization by finding more optimal parameters than standard Adam
- Mechanism: By combining conjugate-gradient-like directions with adaptive moment estimation, the algorithm reaches 100% training accuracy faster than Adam and CoBA. The convergence analysis shows the algorithm maintains O(1/√T) convergence rate even with unbiased moment estimations, and experiments demonstrate higher testing accuracy, indicating better generalization ability.
- Core assumption: The improved training efficiency translates to better generalization because the algorithm finds parameters that generalize better, not just fit the training data
- Evidence anchors: [abstract] "Experiments on CIFAR-10 and CIFAR-100 datasets... demonstrate superior performance compared to both Adam and CoBA" and "delivers higher testing accuracy, indicating better generalization ability"; [section 4] "CG-like-Adam achieves this goal faster and more stable" and "our algorithm performs better than CoBA on test dataset"
- Break condition: If the algorithm overfits (as seen in some ResNet-34 experiments), the testing accuracy advantage may disappear or reverse despite faster training convergence.

## Foundational Learning

- Concept: Conjugate gradient methods for unconstrained nonlinear optimization
  - Why needed here: The algorithm builds upon conjugate gradient theory to create the "conjugate-gradient-like" direction that replaces standard gradient descent
  - Quick check question: What are the key differences between Hestenes-Stiefel, Fletcher-Reeves, and Polak-Ribiere-Polyak conjugate coefficient formulas, and when might each be preferred?

- Concept: Adam optimizer and its convergence issues
  - Why needed here: Understanding Adam's moment estimation mechanism and why it fails to converge in certain cases is crucial for appreciating the modifications made
  - Quick check question: Why does Adam with constant β1t fail to converge in some cases, and how do AMSGrad and other variants attempt to address this?

- Concept: Stochastic optimization and empirical risk minimization
  - Why needed here: The algorithm operates within the stochastic optimization framework for training neural networks, minimizing expected loss over data distributions
  - Quick check question: How does the empirical risk minimization problem formulation differ from the general stochastic optimization setup, and why is this distinction important for deep learning?

## Architecture Onboarding

- Component map:
  - Input: Current parameters xt, noisy gradient gt, past direction dt-1
  - CG-like direction calculation: dt = gt - γt^a · dt-1 (with various conjugate coefficient formulas)
  - First moment estimation: mt = β1t·mt-1 + (1-β1t)·dt, then unbiased: ˆmt = mt/(1-β1t)
  - Second moment estimation: vt = β2·vt-1 + (1-β2)·d²t, then unbiased and max: ˆvt = max{vt/(1-β2), vt-1}
  - Parameter update: xt+1 = xt - αt·ˆV⁻¹/²t · ˆmt
  - Output: Updated parameters xt+1

- Critical path: The parameter update step depends on all previous components - the CG-like direction calculation feeds into both moment estimations, which then determine the adaptive step size and direction for the update

- Design tradeoffs: Using conjugate-gradient-like directions provides better descent directions but adds computational overhead compared to standard Adam. The unbiased moment estimations and maximum operator ensure convergence but may introduce additional variance. The choice of conjugate coefficient formula (HS, FR, PRP, DY, HZ) affects convergence behavior.

- Failure signatures: If training loss plateaus early or oscillates, the conjugate coefficient scaling (a) may be inappropriate. If training accuracy reaches 100% but testing accuracy is poor, the algorithm may be overfitting. If convergence is slow despite correct implementation, the learning rate schedule or β parameters may need adjustment.

- First 3 experiments:
  1. Implement CG-like-Adam with Hestenes-Stiefel conjugate coefficient and compare training loss curves against standard Adam on a simple CNN trained on CIFAR-10
  2. Test different values of the scaling parameter a (e.g., 1.0, 1.5, 2.0) to observe its effect on convergence speed and stability
  3. Verify the unbiased moment estimation by checking that E[ˆmt] ≈ E[∇f(xt)] and E[ˆvt] ≥ E[d²t] throughout training on a controlled synthetic dataset

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the choice of the scalar "a" in the conjugate-gradient-like direction (dt := gt - γt^ta * dt-1) affect convergence rate and generalization performance across different deep learning architectures?
- Basis in paper: [explicit] The paper mentions "a ∈ [1/2, +∞)" and uses "a = 1 + 10^-5" as default, but doesn't provide systematic analysis of how different values impact performance
- Why unresolved: The paper only uses a single value for all experiments without exploring the sensitivity to this hyperparameter
- What evidence would resolve it: Comparative experiments varying "a" across a range of values (e.g., 0.5, 1, 2, 10) on multiple architectures and datasets showing convergence speed and generalization differences

### Open Question 2
- Question: What is the theoretical convergence rate when using a constant learning rate (αt = α) rather than the time-decaying form (αt = α/t^b) as mentioned in the paper?
- Basis in paper: [explicit] The paper states "αt = α(∀t ∈ T ) is usually adopted in practice to mitigate the slowdown[16], yet it is still an open problem of convergence rate analysis in this case"
- Why unresolved: The convergence analysis in the paper focuses on the time-decaying learning rate case, leaving the constant learning rate case as an open problem
- What evidence would resolve it: Theoretical analysis providing convergence rate bounds for the constant learning rate case, followed by empirical validation showing whether the theoretical predictions match practical performance

### Open Question 3
- Question: How does CG-like-Adam perform on extremely deep networks (e.g., ResNet-101, DenseNet) and very large-scale datasets (e.g., ImageNet) compared to Adam and other state-of-the-art optimizers?
- Basis in paper: [inferred] The experiments are limited to VGG-19 on CIFAR-10 and ResNet-34 on CIFAR-100, which are relatively shallow networks and smaller datasets compared to modern deep learning applications
- Why unresolved: The paper doesn't test on the scale of models and datasets where adaptive methods like Adam typically show their greatest advantages
- What evidence would resolve it: Comprehensive experiments on large-scale architectures (ResNet-101, DenseNet-121) and datasets (ImageNet) comparing CG-like-Adam against Adam, AMSGrad, and other recent optimizers across training speed, final accuracy, and computational efficiency metrics

## Limitations

- The experimental validation is limited to relatively shallow architectures (VGG-19, ResNet-34) and smaller datasets (CIFAR-10, CIFAR-100), which may not fully represent the performance on modern deep learning applications
- The choice of conjugate coefficient formula and scaling parameter a is not systematically explored, leaving uncertainty about optimal configurations for different tasks
- The paper notes potential overfitting issues with ResNet-34, suggesting the algorithm may not always improve generalization across all architectures

## Confidence

- High confidence: The algorithm's ability to reach 100% training accuracy faster than Adam and CoBA, and the theoretical convergence guarantees under the specified conditions
- Medium confidence: The claim of better generalization ability (higher testing accuracy) compared to Adam and CoBA
- Low confidence: The claim that the algorithm resolves all convergence issues in existing adaptive methods

## Next Checks

1. Validate convergence across architectures: Test CG-like-Adam on additional architectures (e.g., DenseNet, EfficientNet) and datasets (e.g., ImageNet, SVHN) to assess the generalizability of the convergence and generalization claims beyond CIFAR-10/100 and VGG/ResNet.

2. Ablation study of conjugate coefficient impact: Systematically compare the five conjugate coefficient formulas (HS, FR, PRP, DY, HZ) and different scaling parameter values (a) to identify which combinations provide the best trade-off between convergence speed and stability across various tasks.

3. Investigate overfitting behavior: Analyze the training and testing loss curves in detail, particularly for ResNet-34 where the paper notes potential overfitting. Determine if the algorithm's faster convergence comes at the cost of increased overfitting tendency in certain architectures, and explore regularization techniques to mitigate this.