---
ver: rpa2
title: "DiffGaze: A Diffusion Model for Continuous Gaze Sequence Generation on 360\xB0\
  \ Images"
arxiv_id: '2403.17477'
source_url: https://arxiv.org/abs/2403.17477
tags:
- gaze
- human
- sequences
- diffgaze
- prediction
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "DiffGaze introduces a diffusion-based approach for generating\
  \ continuous human gaze sequences on 360\xB0 images, a task that goes beyond existing\
  \ scanpath and saliency prediction methods by modeling the full temporal and spatial\
  \ dynamics of eye movements. The method uses spherical image features as a condition\
  \ and employs two transformers to capture the temporal and spatial dependencies\
  \ of gaze data."
---

# DiffGaze: A Diffusion Model for Continuous Gaze Sequence Generation on 360° Images

## Quick Facts
- arXiv ID: 2403.17477
- Source URL: https://arxiv.org/abs/2403.17477
- Authors: Chuhan Jiao; Yao Wang; Guanhua Zhang; Mihai Bâce; Zhiming Hu; Andreas Bulling
- Reference count: 40
- Primary result: DiffGaze outperforms state-of-the-art methods on continuous gaze sequence generation for 360° images using a diffusion model with spherical image conditioning and dual transformers.

## Executive Summary
DiffGaze introduces a diffusion-based approach for generating continuous human gaze sequences on 360° images, modeling both fixations and saccades at high temporal resolution (30 Hz). The method uses spherical image features as conditioning and employs two transformers to capture temporal and spatial dependencies of gaze data. Evaluated on the Sitzmann and Salient360! datasets, DiffGaze outperforms state-of-the-art methods in continuous gaze sequence generation and scanpath prediction, and achieves strong results in saliency prediction. A user study with 21 participants found the generated sequences are practically indistinguishable from real human gaze data, while analysis shows the model reproduces natural fixation and saccade patterns.

## Method Summary
DiffGaze is a conditional diffusion model that generates continuous gaze sequences by denoising random noise into realistic eye movements guided by 360° image features. The model processes spherical images using a spherical CNN to create feature embeddings, then uses two transformers to capture temporal and spatial dependencies in the gaze data. The diffusion process is conditioned on both the spherical image features and side information (timestamp, speed, position). The model is trained on the Sitzmann dataset and evaluated on both Sitzmann and Salient360! benchmarks using time-series metrics and a user study.

## Key Results
- DiffGaze outperforms state-of-the-art methods on both Sitzmann and Salient360! datasets across all evaluation metrics
- Generated gaze sequences are practically indistinguishable from real human gaze data according to a user study with 21 participants
- The model reproduces natural eye movement statistics including mean saccade velocity, number of fixations, and fixation duration
- DiffGaze demonstrates strong cross-dataset generalization capabilities

## Why This Works (Mechanism)

### Mechanism 1
- Claim: DiffGaze outperforms scanpath and saliency prediction methods because it generates continuous gaze sequences rather than discrete fixations.
- Mechanism: Continuous gaze sequences capture both fixations and saccades at high temporal resolution (30 Hz), providing richer data than discrete fixation sequences. The diffusion model learns the full distribution of eye movements rather than predicting individual fixations.
- Core assumption: Human eye movements follow a complex but learnable distribution that can be modeled by a conditional diffusion process.
- Evidence anchors:
  - [abstract]: "DiffGaze outperforms state-of-the-art methods on all tasks on both benchmarks"
  - [section IV.C]: "DiffGaze demonstrates superior performance on the Sitzmann dataset across all metrics"
  - [corpus]: Weak - no direct corpus evidence found for continuous vs discrete performance comparison
- Break condition: If the diffusion process fails to capture the complex temporal dynamics of eye movements, the continuous sequences would not improve over discrete predictions.

### Mechanism 2
- Claim: The spherical image embedding as condition improves gaze sequence generation by providing spatially relevant context.
- Mechanism: The spherical convolution (S-CNN) processes 360° images while handling equirectangular distortions, creating feature embeddings that guide the diffusion process toward realistic gaze patterns based on image content.
- Core assumption: The spatial distribution and semantics of objects in 360° images strongly influence human gaze behavior.
- Evidence anchors:
  - [section III.C]: "we use the 360° image itself as a condition to the diffusion process"
  - [section III.D]: "we propose a spherical embedding of 360° image as the condition in the reverse diffusion process"
  - [corpus]: Missing - no direct corpus evidence for spherical vs planar image conditioning effectiveness
- Break condition: If the spherical embedding fails to capture relevant spatial information or the diffusion process ignores this conditioning, performance would degrade.

### Mechanism 3
- Claim: The dual transformer architecture captures both temporal and spatial dependencies in gaze sequences.
- Mechanism: One transformer learns temporal dependencies along the sequence length, while the other captures spatial correlations among the (x,y,z) gaze coordinates at each timestamp, enabling realistic generation of eye movement patterns.
- Core assumption: Human gaze sequences exhibit both temporal coherence (smooth transitions) and spatial correlations (coordinated eye movements).
- Evidence anchors:
  - [section III.D]: "two transformers to capture the temporal and spatial dependencies of continuous human gaze sequences"
  - [section IV.C]: "DiffGaze can generate gaze sequences with characteristics that are highly similar to real human gaze behaviour"
  - [corpus]: Weak - no direct corpus evidence for dual transformer architecture effectiveness in gaze generation
- Break condition: If the transformers fail to learn these dependencies, generated sequences would appear random or unnatural.

## Foundational Learning

- Concept: Diffusion models and score-based generative modeling
  - Why needed here: DiffGaze uses a conditional score-based denoising diffusion model to generate gaze sequences, requiring understanding of forward/reverse processes and noise schedules
  - Quick check question: What is the relationship between the noise schedule βt and the latent variable Xt in the forward diffusion process?

- Concept: Spherical image processing and equirectangular projections
  - Why needed here: DiffGaze uses spherical convolutions to process 360° images, requiring understanding of how to handle distortion from equirectangular projection
  - Quick check question: Why is it necessary to project 360° images onto a unit sphere rather than using raw equirectangular coordinates?

- Concept: Transformer architectures for sequential data
  - Why needed here: DiffGaze uses two transformers to model temporal and spatial dependencies in gaze sequences, requiring understanding of attention mechanisms and sequence modeling
  - Quick check question: How does the dual transformer architecture in DiffGaze differ from standard single-transformer approaches for sequence generation?

## Architecture Onboarding

- Component map: 360° image → Spherical CNN → Feature embedding → Diffusion embedding (time) + Side information → Dual transformers (temporal + spatial) → 1D CNN layers → Generated gaze sequence
- Critical path: Input 360° image through spherical CNN to generate conditioning features, which guide the diffusion process to denoise random noise into realistic gaze sequences
- Design tradeoffs: Continuous gaze generation vs. discrete scanpath prediction (more realistic but computationally heavier), spherical vs. planar image processing (handles distortion but more complex), dual transformers vs. single transformer (captures more dependencies but increases parameters)
- Failure signatures: Unrealistic gaze patterns (too many saccades or fixations), failure to capture spatial image features in gaze, inability to generate diverse sequences, poor cross-dataset generalization
- First 3 experiments:
  1. Test spherical CNN output quality on 360° images vs. standard 2D CNNs
  2. Validate diffusion model can generate reasonable noise distributions before training
  3. Verify dual transformer can learn simple temporal and spatial patterns on synthetic gaze data

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does DiffGaze's performance scale with higher sampling frequencies beyond 30 Hz, and can it accurately model finer eye movement events like microsaccades?
- Basis in paper: [explicit] The paper mentions this as a limitation, stating that while 30 Hz gaze trajectories were tested, future work will explore performance at higher sampling frequencies to model other eye events like microsaccades.
- Why unresolved: Current 360° image eye tracking datasets typically have a maximum sampling frequency of 120 Hz, which is insufficient for capturing high-frequency eye events like microsaccades that require up to 1,000 Hz.
- What evidence would resolve it: Empirical evaluation of DiffGaze on high-frequency eye tracking datasets (e.g., 1,000 Hz) to assess its ability to accurately model and generate microsaccades and other fine-grained eye movement events.

### Open Question 2
- Question: What are the key factors that make continuous gaze sequence generation more challenging than scanpath prediction, and how does DiffGaze specifically address these challenges?
- Basis in paper: [explicit] The paper discusses how continuous gaze sequence generation is more complex than scanpath prediction because it requires modeling both fixation and saccade events, generating numerous gaze samples for each fixation, and accounting for the variability in human eye movements.
- Why unresolved: While the paper mentions these challenges and DiffGaze's approach using diffusion models and transformers, it does not provide a detailed quantitative analysis of the specific factors that contribute to the increased difficulty and how DiffGaze's architecture effectively addresses them.
- What evidence would resolve it: Ablation studies comparing DiffGaze's performance with and without key components (e.g., diffusion model, transformers, conditioning mechanism) to identify the most critical factors for successful continuous gaze sequence generation.

### Open Question 3
- Question: How well does DiffGaze generalize to different types of visual tasks and environments beyond free-viewing 360° images, such as visual search, visual question answering, or natural images?
- Basis in paper: [explicit] The paper mentions that DiffGaze can be modified for other tasks like visual search, visual question answering, and natural images, but notes the limited availability of datasets with raw eye tracking data for these tasks.
- Why unresolved: The paper only evaluates DiffGaze on 360° images from two datasets, and it is unclear how well the model's learned representations and generation capabilities transfer to other visual tasks and environments with different characteristics and data distributions.
- What evidence would resolve it: Empirical evaluation of DiffGaze on diverse datasets for various visual tasks (e.g., visual search, visual question answering, natural images) to assess its generalization performance and identify potential limitations or areas for improvement.

## Limitations
- Performance evaluation is limited to only two datasets (Sitzmann and Salient360!) with different characteristics, raising questions about generalizability
- The user study validation has a small sample size (21 participants) and lacks detailed methodology description
- Spherical CNN implementation details are sparse, making exact reproduction challenging
- Claims about practical indistinguishability from real gaze data need more rigorous validation

## Confidence
- High confidence: The diffusion model architecture is technically sound and the dual transformer approach for temporal and spatial dependencies is well-motivated
- Medium confidence: Performance improvements over baselines are demonstrated but depend heavily on the specific datasets used
- Low confidence: Claims about practical indistinguishability from real gaze data based on user study, due to limited sample size (21 participants) and potential methodological gaps

## Next Checks
1. Reproduce the spherical CNN output quality comparison with standard 2D CNNs on the same 360° image datasets to verify the claimed advantage of spherical processing
2. Conduct an independent user study with larger sample size and more rigorous methodology to validate the perceptual indistinguishability claims
3. Test the model's cross-dataset generalization on additional 360° eye tracking datasets to assess robustness beyond the two evaluated datasets