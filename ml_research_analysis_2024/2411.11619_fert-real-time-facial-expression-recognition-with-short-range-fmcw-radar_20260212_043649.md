---
ver: rpa2
title: 'FERT: Real-Time Facial Expression Recognition with Short-Range FMCW Radar'
arxiv_id: '2411.11619'
source_url: https://arxiv.org/abs/2411.11619
tags:
- facial
- radar
- recognition
- expression
- fmcw
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper presents FERT, a novel real-time facial expression\
  \ recognition system using a short-range 60 GHz FMCW radar. The system processes\
  \ four distinct modalities\u2014Range-Doppler Images (RDIs), micro-RDIs, Range Azimuth\
  \ Images (RAIs), and Range Elevation Images (REIs)\u2014through a multi-block architecture\
  \ that includes feature extractors, intermediate feature extractors, and a ResNet34\
  \ classifier."
---

# FERT: Real-Time Facial Expression Recognition with Short-Range FMCW Radar

## Quick Facts
- arXiv ID: 2411.11619
- Source URL: https://arxiv.org/abs/2411.11619
- Reference count: 31
- Primary result: 98.91% average classification accuracy on four facial expressions (smile, anger, neutral, no-face) using 60 GHz FMCW radar

## Executive Summary
This paper presents FERT, a real-time facial expression recognition system using short-range 60 GHz FMCW radar. The system processes four distinct modalities—Range-Doppler Images (RDIs), micro-RDIs, Range Azimuth Images (RAIs), and Range Elevation Images (REIs)—through a multi-block architecture that includes feature extractors, intermediate feature extractors, and a ResNet34 classifier. The approach achieves 98.91% average classification accuracy on a dataset of four expressions, with real-time performance and person-independent operation. The system leverages low-cost radar technology for privacy-preserving FER, demonstrating high accuracy and robustness across different environmental conditions and participants.

## Method Summary
FERT employs a multi-block neural architecture trained simultaneously on four radar modalities generated from 60 GHz FMCW radar data. The preprocessing pipeline includes E-RESPD, which aggregates 200 consecutive frames to smooth noise and enhance movement capture. The architecture consists of four parallel feature extractors (3 convolutional layers each), two intermediate feature extractors that fuse paired modalities, and a ResNet34 classifier. The system is trained end-to-end for only three epochs using cross-entropy loss and SGD optimizer, achieving high accuracy while maintaining real-time performance.

## Key Results
- Achieved 98.91% average classification accuracy across four facial expressions
- Real-time performance with low computational overhead
- Person-independent operation with high robustness across participants
- Effective privacy preservation through non-visible sensing technology

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The multi-modality fusion of RDIs, micro-RDIs, RAIs, and REIs captures both static facial structure and dynamic muscle movement, enabling high classification accuracy.
- Mechanism: Each modality encodes complementary information: RDIs capture gross face shape, micro-RDIs capture fine muscle motion, RAIs and REIs provide angular resolution in different planes. Feature extractors learn modality-specific patterns, then intermediate extractors fuse similar modalities before a final ResNet classifier integrates all cues.
- Core assumption: Facial expressions produce distinct, measurable radar signatures across these four modalities, and their combined information is richer than any single modality alone.
- Evidence anchors:
  - [abstract] states the system leverages "four distinct modalities simultaneously: Range-Doppler images (RDIs), micro range-Doppler Images (micro-RDIs), range azimuth images (RAIs), and range elevation images (REIs)."
  - [section III] explains the pipeline architecture and training approach.
  - [corpus] evidence is limited; neighbor papers focus on radar-based FER but do not detail multi-modality fusion, so [corpus] anchor is weak.
- Break condition: If facial expressions do not produce distinct enough radar signatures in one or more modalities, or if modality fusion introduces noise rather than signal, accuracy would degrade.

### Mechanism 2
- Claim: The E-RESPD pre-processing method improves robustness by aggregating 200 consecutive frames, smoothing noise and enhancing movement capture.
- Mechanism: By stacking multiple frames and removing means from both fast and slow time signals, E-RESPD reduces high-frequency noise and highlights consistent motion patterns, which helps the network learn stable features across sessions and participants.
- Core assumption: Facial movements are relatively consistent within a 200-frame window and the aggregation preserves discriminative motion while suppressing noise.
- Evidence anchors:
  - [section II] describes the preprocessing and mentions E-RESPD is "inspired by [10]" and "positively affects the overall classification performance."
  - [section IV.B] shows an ablation study comparing accuracy with and without E-RESPD.
  - [corpus] does not provide external validation of E-RESPD; anchor is weak.
- Break condition: If facial movements are too fast or irregular relative to the frame window, or if the noise structure is not well addressed by mean removal, the method may fail.

### Mechanism 3
- Claim: Training with a small number of epochs (three) indicates the architecture is a "fast learner," suggesting efficient feature reuse and strong inductive bias from the ResNet backbone.
- Mechanism: The ResNet34 block, pre-trained on large image datasets, transfers robust feature extraction capabilities to radar image modalities, allowing rapid convergence with limited training data.
- Core assumption: Radar image features are sufficiently similar to natural image features for effective transfer learning, and the dataset size supports this approach.
- Evidence anchors:
  - [section III] states "Our architecture is a fast learner, so it is trained only three epochs to reach the final results."
  - [section IV.A] reports high accuracy (98.91%) with this approach.
  - [corpus] does not discuss transfer learning in this context; anchor is weak.
- Break condition: If radar images differ substantially from natural images, or if the dataset is too small to avoid overfitting in just three epochs, performance would degrade.

## Foundational Learning

- Concept: Radar signal processing fundamentals (FMCW chirp, range FFT, Doppler FFT, beamforming)
  - Why needed here: Understanding how raw radar data is converted into RDIs, RAIs, and REIs is critical to debug preprocessing and interpret model inputs.
  - Quick check question: What is the difference between Range-FFT and Doppler-FFT in the FMCW radar processing chain?

- Concept: Convolutional neural network design (2D convolutions, batch normalization, ReLU, max pooling)
  - Why needed here: The feature extractors use these layers to learn spatial hierarchies from radar images; understanding their role aids in tuning architecture.
  - Quick check question: How does batch normalization help during training of deep convolutional networks?

- Concept: Multi-modality fusion strategies (early vs late fusion, concatenation)
  - Why needed here: FERT merges modalities at two stages; understanding fusion impacts feature integration and classification performance.
  - Quick check question: What is the difference between early fusion and late fusion in multi-modal deep learning?

## Architecture Onboarding

- Component map: Input modalities (RDI, micro-RDI, RAI, REI) → four parallel feature extractors (3 conv layers each) → modality pair concatenation (RDI+micro-RDI, RAI+REI) → two intermediate feature extractors (2 conv + BN + ReLU + max pool) → modality pair concatenation → ResNet34 classifier → output classes (smile, anger, neutral, no-face)
- Critical path: Feature extractors → intermediate feature extractors → ResNet → classification. Any bottleneck or failure in early blocks propagates downstream.
- Design tradeoffs: Using four modalities increases robustness and accuracy but also computational cost and data collection complexity; early fusion simplifies architecture but may lose modality-specific detail.
- Failure signatures: Low accuracy in one modality’s feature extractor suggests poor preprocessing or misalignment; poor intermediate extractor performance may indicate modality fusion issues; ResNet failure points to insufficient or noisy combined features.
- First 3 experiments:
  1. Train and evaluate each feature extractor independently on its modality to confirm basic learning.
  2. Test intermediate feature extractor performance on paired modalities to check fusion quality.
  3. Train the full pipeline end-to-end with a small subset of data to validate integration before full-scale training.

## Open Questions the Paper Calls Out

The paper does not explicitly call out any open questions in the text.

## Limitations
- Limited dataset diversity with only two participants tested
- Dataset not publicly available, restricting independent validation
- Incomplete specification of E-RESPD preprocessing details
- High accuracy achieved with only three training epochs, raising overfitting concerns

## Confidence
- High: Core architecture design and data collection method using 60 GHz FMCW radar with four modalities
- Medium: Achieved accuracy and real-time performance claims
- Low: Generalizability to diverse populations and environmental conditions

## Next Checks
1. Conduct experiments with a larger, more diverse participant pool (minimum 20 individuals) to test person-independent generalization.
2. Perform cross-environment testing (different room sizes, background clutter, participant movement) to validate real-world robustness.
3. Compare E-RESPD preprocessing against standard radar preprocessing techniques on a held-out validation set to quantify its contribution.