---
ver: rpa2
title: 'MediQ: Question-Asking LLMs and a Benchmark for Reliable Interactive Clinical
  Reasoning'
arxiv_id: '2406.00922'
source_url: https://arxiv.org/abs/2406.00922
tags:
- patient
- information
- system
- question
- expert
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work addresses the limitation of static single-turn benchmarks
  in evaluating large language models (LLMs) for interactive clinical reasoning. The
  authors propose MEDI Q, a novel benchmark that simulates realistic clinical interactions
  where a patient system provides partial information and an expert system must proactively
  ask questions to gather missing details before making a diagnosis.
---

# MediQ: Question-Asking LLMs and a Benchmark for Reliable Interactive Clinical Reasoning

## Quick Facts
- **arXiv ID**: 2406.00922
- **Source URL**: https://arxiv.org/abs/2406.00922
- **Reference count**: 40
- **Primary result**: Direct prompting of LLMs to ask clinical questions degrades performance by 11.3% compared to abstention strategies

## Executive Summary
This work introduces MediQ, a novel benchmark for evaluating large language models in interactive clinical reasoning scenarios where information must be gathered through question-asking rather than provided upfront. The benchmark simulates realistic clinical interactions with a patient system providing partial information and an expert system that must proactively ask questions before diagnosis. The key finding reveals that standard prompting approaches actually harm LLM performance in these settings, with a 22.3% improvement observed when using abstention strategies for confidence estimation. Despite this improvement, a 10.3% gap remains compared to having complete information upfront, highlighting fundamental challenges in adapting LLMs to interactive information-seeking tasks.

## Method Summary
The authors developed MediQ to address limitations in static single-turn benchmarks for evaluating LLMs in clinical reasoning tasks. The benchmark consists of 148 clinical vignettes where a simulated patient system provides partial medical information, and the LLM must ask relevant questions to gather missing details before making a diagnosis. The evaluation framework compares three approaches: direct prompting where LLMs are asked to ask questions, abstention strategies that use confidence estimation techniques to decide when to ask versus diagnose, and an upper bound using complete information upfront. The study tested multiple state-of-the-art LLMs across these conditions to assess performance differences in interactive versus static information contexts.

## Key Results
- Direct prompting degrades LLM performance by 11.3% compared to abstention strategies
- Abstention strategies improve diagnostic accuracy by 22.3% in interactive settings
- A 10.3% gap remains between interactive performance and complete information scenarios

## Why This Works (Mechanism)
The mechanism behind these findings relates to how LLMs handle uncertainty in information-gathering contexts. When directly prompted to ask questions, LLMs may experience cognitive overload or misalignment between question-asking and diagnostic reasoning processes. Abstention strategies work by allowing models to leverage confidence estimation techniques to strategically decide when additional information is needed versus when to make a diagnosis, effectively separating the information-gathering and decision-making processes. This separation appears to reduce the cognitive burden and improve overall reasoning quality in clinical contexts.

## Foundational Learning
- **Interactive Clinical Reasoning**: Why needed - Clinical practice requires dynamic information gathering; quick check - Can the model adapt questioning based on patient responses?
- **Confidence Estimation**: Why needed - Determines when to seek more information vs. make decisions; quick check - Does abstention improve calibration metrics?
- **Information Completeness Trade-offs**: Why needed - Real clinical settings involve incomplete data; quick check - How does performance scale with information availability?

## Architecture Onboarding

**Component Map**: Patient System -> Question Module -> LLM Reasoning -> Diagnosis Output

**Critical Path**: The patient system provides partial information → LLM uses abstention strategy to decide whether to ask questions or diagnose → If asking questions, iterates through question module → Final diagnosis is made

**Design Tradeoffs**: Direct prompting vs. abstention represents a fundamental tradeoff between cognitive load management and diagnostic accuracy. The study shows that forcing simultaneous question-asking and reasoning degrades performance, suggesting these should be separate processes.

**Failure Signatures**: Performance degradation when LLMs are directly prompted to ask questions, inability to calibrate confidence appropriately for information-seeking decisions, and persistent gaps compared to complete information scenarios.

**First Experiments**:
1. Test abstention strategies across different confidence estimation techniques
2. Compare performance across different clinical specialties
3. Evaluate question quality and relevance in iterative information-gathering

## Open Questions the Paper Calls Out
None

## Limitations
- Reliance on single benchmark (MediQ) with 148 clinical vignettes may limit generalizability
- Study focuses on clinical reasoning domain without cross-domain validation
- Does not thoroughly investigate mechanisms behind abstention strategy improvements

## Confidence

**High Confidence**: The empirical finding that direct prompting degrades performance compared to abstention strategies is robust, with clear statistical significance and consistent results across multiple LLMs.

**Medium Confidence**: The claim that a 10.3% gap remains compared to complete information scenarios is methodologically sound but may overstate practical limitations, as real clinical reasoning often involves incomplete information by necessity.

**Medium Confidence**: The assertion that LLMs fundamentally struggle with interactive information-seeking requires further validation, as the study doesn't adequately distinguish between model limitations versus prompt engineering deficiencies.

## Next Checks

1. **Cross-Domain Generalization**: Test MediQ's findings on non-clinical information-seeking tasks (e.g., technical support, legal reasoning) to determine if the observed limitations are domain-specific or represent broader LLM deficiencies in interactive reasoning.

2. **Expert Clinician Validation**: Conduct blinded comparison studies where practicing clinicians evaluate MediQ cases alongside LLM responses to assess clinical validity and identify potential biases in the benchmark construction.

3. **Ablation Studies on Question Quality**: Implement detailed analysis of which types of questions LLMs struggle with most (factual vs. exploratory, closed vs. open-ended) and whether specific question templates or schemas could mitigate performance degradation.