---
ver: rpa2
title: Learning Fair Ranking Policies via Differentiable Optimization of Ordered Weighted
  Averages
arxiv_id: '2402.05252'
source_url: https://arxiv.org/abs/2402.05252
tags:
- fairness
- ranking
- optimization
- fair
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses fairness in learning to rank (LTR) systems,
  which can exhibit biases due to focusing solely on user relevance. It proposes SOFaiR,
  a framework that integrates fair ranking optimization into LTR training to balance
  fairness, utility, and runtime efficiency.
---

# Learning Fair Ranking Policies via Differentiable Optimization of Ordered Weighted Averages

## Quick Facts
- arXiv ID: 2402.05252
- Source URL: https://arxiv.org/abs/2402.05252
- Authors: My H. Dinh; James Kotary; Ferdinando Fioretto
- Reference count: 31
- Primary result: SOFaiR outperforms existing methods, achieving lower fairness violations with competitive utility and significantly faster runtime while naturally handling multi-group fairness

## Executive Summary
This paper addresses fairness in learning to rank (LTR) systems, which can exhibit biases due to focusing solely on user relevance. It proposes SOFaiR, a framework that integrates fair ranking optimization into LTR training to balance fairness, utility, and runtime efficiency. The key innovation is a differentiable optimization module that uses Ordered Weighted Averages (OWA) to enforce group fairness of exposure while maximizing relevance. Experiments show SOFaiR outperforms existing methods, achieving lower fairness violations with competitive utility and significantly faster runtime. It also naturally handles multi-group fairness scenarios.

## Method Summary
SOFaiR integrates fair ranking optimization into LTR training by incorporating OWA-based constrained optimization. The method predicts relevance scores using a neural network, then applies Frank-Wolfe optimization with Moreau envelope smoothing to compute ranking policies. Backpropagation uses SPO+ subgradients computed via LP reformulation. The framework handles multi-group fairness naturally by aggregating group exposures through OWA in the objective function. Training uses Adam optimizer with 100 iterations per query, evaluating DCG for utility and absolute difference in group exposure for fairness violation.

## Key Results
- Outperforms existing methods (DELTR, FULTR, SPOFR) on fairness violations while maintaining competitive utility
- Achieves significantly faster runtime compared to baseline methods
- Naturally handles multi-group fairness scenarios without requiring hard constraints
- Demonstrates effectiveness on both MSLR-Web10k and Yahoo LETOR datasets

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Backpropagation through the OWA optimization is enabled by recasting the problem as an LP with auxiliary variables and using the SPO+ loss.
- Mechanism: The OWA objective is rewritten as a max-min problem (22) which can be expressed as a linear program with z and r variables. The SPO+ subgradient (20) can then be computed as the difference of two optimal solutions of this LP, even though the LP itself is not solved.
- Core assumption: The SPO+ subgradient formula applies to the LP form of the OWA optimization, and the optimal solutions can be computed by the Frank-Wolfe method.
- Evidence anchors:
  - [abstract] "For the first time, it shows how to backpropagate gradients through the highly discontinuous optimization of OWA functions"
  - [section] "LSPO+(ˆc, c) = maxx (cTx − 2ˆcTx) + 2ˆcTx⋆(c) − cTx⋆(c)"
  - [corpus] Weak. No direct mention of SPO+ or LP reformulation for OWA in the 8 nearest neighbors.

### Mechanism 2
- Claim: The Frank-Wolfe method with Moreau envelope smoothing efficiently solves the constrained OWA optimization without needing Birkhoff-von Neumann decomposition.
- Mechanism: The linear constraint set B is optimized by linearizing the smoothed objective at each iteration. The linear subproblem is solved by sorting, which is O(n log n). The iterates naturally provide a decomposition of the policy matrix.
- Core assumption: The gradient of the smoothed OWA objective has the form required for the sorting-based solution.
- Evidence anchors:
  - [section] "the function yT Π b is maximized by the permutation matrix P ∈ P which sorts the relevance scores y decreasingly"
  - [section] "Algorithm 1 maintains O(n log n) complexity per iteration"
  - [corpus] Weak. The nearest neighbors do not discuss Frank-Wolfe methods or sorting-based solutions for OWA.

### Mechanism 3
- Claim: The OWA aggregation of group exposures provides a scalable way to enforce multi-group fairness without hard constraints.
- Mechanism: By including the OWA term in the objective with weight λ, the optimization trades off between utility and disparity in group exposures. Higher λ leads to lower disparities, and the method naturally handles any number of groups.
- Core assumption: The OWA weights w are chosen to have the fairness properties (impartiality, equitability, monotonicity).
- Evidence anchors:
  - [section] "When λ = 1, the optimization (11) returns a ranking policy that minimizes disparities in group exposure"
  - [section] "This allows SOFaiR to achieve faster runtime, and avoid the BVM decomposition at inference time, while naturally accommodating fairness over an arbitrary number of groups"
  - [corpus] Weak. The nearest neighbors focus on different aspects of fair LTR and do not discuss OWA or multi-group fairness in the context of optimization.

## Foundational Learning

- Concept: Linear programming and convex optimization
  - Why needed here: The fair ranking problem is formulated as a linear program over the Birkhoff polytope, and the SPO+ loss requires understanding of LP duality and subgradients.
  - Quick check question: What is the dual of the LP max z s.t. z ≤ wσ · r for all σ ∈ P?

- Concept: Ordered weighted averaging (OWA) operators
  - Why needed here: The OWA operator is used to aggregate group exposures in a way that promotes fairness. Understanding its properties (impartiality, equitability, monotonicity) is crucial for choosing the weights.
  - Quick check question: What is the OWA operator with weights w = (1/3, 1/3, 1/3) applied to the vector (2, 5, 1)?

- Concept: Predict-Then-Optimize (PtO) framework
  - Why needed here: The method learns to predict the relevance scores by minimizing the regret of the downstream optimization. This requires understanding of how to backpropagate through the optimization.
  - Quick check question: In the context of PtO, what is the difference between minimizing the regret and minimizing the prediction error?

## Architecture Onboarding

- Component map: Neural network (Mθ) predicting relevance scores → Fair ranking optimization module mapping to ranking policy using OWA → SPO+ loss computing regret and providing subgradients for backpropagation

- Critical path:
  1. Forward pass: ˆyq = Mθ(xq) → Π⋆(ˆyq) via Frank-Wolfe + Moreau envelope smoothing
  2. Compute regret: f(Π⋆(yq), yq) - f(Π⋆(ˆyq), yq)
  3. Backward pass: ∇LSPO+(ˆγ, γ) using LP reformulation and automatic differentiation

- Design tradeoffs:
  - Using OWA in the objective vs. hard fairness constraints: more scalable but requires careful weight selection
  - SPO+ loss vs. direct differentiation: works for non-differentiable objectives but introduces approximation error
  - Frank-Wolfe vs. other solvers: simple and efficient for this problem but may have slower convergence

- Failure signatures:
  - High regret even with good prediction accuracy: the optimization module is not learning the right policy
  - Slow convergence during training: the smoothing parameter schedule may need adjustment
  - Numerical instability in the LP reformulation: the auxiliary variables z and r may need regularization

- First 3 experiments:
  1. Verify that the Frank-Wolfe method with smoothing converges to the optimal solution of the OWA problem on a small synthetic dataset.
  2. Check that the SPO+ subgradient computed via the LP reformulation matches the true subgradient on a simple differentiable problem.
  3. Compare the fairness-utility tradeoff of SOFaiR with a baseline method on a real LTR dataset, varying the OWA weights and λ.

## Open Questions the Paper Calls Out

The paper does not explicitly call out open questions in the provided text. However, based on the content, potential open questions include:

- How does the choice of OWA weights affect the fairness-utility tradeoff in different ranking scenarios?
- Can the framework be extended to handle two-sided fairness considering both user utility and item exposure fairness?
- How does SOFaiR compare to post-processing fair ranking methods on real-world datasets with complex fairness requirements?

## Limitations

- The method's performance in extreme fairness scenarios or with very large item sets remains unverified
- The smoothing parameter schedule for Frank-Wolfe convergence is not explicitly detailed
- While claiming to handle multi-group fairness naturally, the choice of OWA weights for arbitrary group numbers requires careful consideration

## Confidence

- **High Confidence**: The theoretical framework connecting SPO+ loss to LP subgradients for OWA optimization is well-founded and mathematically rigorous.
- **Medium Confidence**: Empirical results show competitive performance against baselines, but the limited number of datasets and hyperparameter variations prevent strong generalization claims.
- **Low Confidence**: The paper does not provide comprehensive analysis of runtime behavior for very large-scale problems or detailed sensitivity analysis for OWA weight selection.

## Next Checks

1. **Numerical Stability Test**: Systematically evaluate the LP reformulation across a wide range of OWA weight configurations to identify potential numerical instability patterns.
2. **Extreme Fairness Scenario**: Test SOFaiR on datasets with severe group imbalance or extreme relevance distributions to verify robustness of the fairness-utility tradeoff.
3. **Runtime Scalability**: Benchmark runtime performance on progressively larger datasets (e.g., doubling query/item counts) to validate claimed scalability advantages over baseline methods.