---
ver: rpa2
title: 'Continual Pre-Training for Cross-Lingual LLM Adaptation: Enhancing Japanese
  Language Capabilities'
arxiv_id: '2404.17790'
source_url: https://arxiv.org/abs/2404.17790
tags:
- japanese
- language
- training
- pre-training
- vocabulary
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper presents Swallow, a Japanese-focused large language\
  \ model built via cross-lingual continual pre-training from Llama 2. The authors\
  \ extend Llama 2\u2019s vocabulary with Japanese characters and conduct continual\
  \ pre-training on a large Japanese web corpus."
---

# Continual Pre-Training for Cross-Lingual LLM Adaptation: Enhancing Japanese Language Capabilities

## Quick Facts
- arXiv ID: 2404.17790
- Source URL: https://arxiv.org/abs/2404.17790
- Reference count: 38
- Primary result: Swallow, a Japanese-focused LLM built via cross-lingual continual pre-training, significantly improves Japanese performance while maintaining English capabilities

## Executive Summary
This paper introduces Swallow, a Japanese-focused large language model created through cross-lingual continual pre-training of Llama 2. The authors expand Llama 2's vocabulary with Japanese characters and conduct pre-training on a large Japanese web corpus. The resulting model demonstrates substantial improvements in Japanese language performance, particularly on question answering tasks, while maintaining strong English capabilities. The study systematically investigates the effects of vocabulary expansion and parallel corpora usage, revealing that vocabulary expansion enhances efficiency without compromising performance (except in summarization) and that parallel corpora improve translation abilities. Swallow outperforms other Japanese LLMs trained from scratch and shows monotonic performance gains with increased training data, establishing continual pre-training as an efficient approach for adapting LLMs to non-English languages.

## Method Summary
The researchers created Swallow by extending Llama 2's vocabulary with Japanese characters and performing continual pre-training on a large Japanese web corpus. They systematically investigated two key aspects: vocabulary expansion (adding Japanese characters to the tokenizer) and the use of parallel corpora. The model was evaluated across multiple Japanese and English language tasks, comparing performance against baseline models including Llama 2 and other Japanese-specific LLMs trained from scratch. The experiments varied training data scales to assess the relationship between data size and performance improvements.

## Key Results
- Swallow significantly improves Japanese language performance, especially on question answering tasks, while maintaining strong English performance
- Vocabulary expansion improves pre-training efficiency without compromising performance (except in summarization tasks)
- Incorporating parallel corpora enhances translation ability
- Swallow outperforms other Japanese LLMs trained from scratch
- Performance gains show monotonic improvement with increased training data

## Why This Works (Mechanism)
Continual pre-training allows models to leverage existing knowledge from pre-trained LLMs while adapting to new language characteristics. By expanding the vocabulary with Japanese characters, the model gains finer-grained tokenization of Japanese text, reducing vocabulary collision and improving language-specific representation. The pre-training process on Japanese web data enables the model to learn language-specific patterns, idiomatic expressions, and cultural context. The use of parallel corpora provides explicit alignment between Japanese and English, enhancing translation capabilities while the cross-lingual pre-training framework ensures knowledge transfer without catastrophic forgetting of English capabilities.

## Foundational Learning
- Cross-lingual pre-training: Why needed - enables knowledge transfer between languages while adapting to new linguistic patterns; Quick check - compare performance on joint English-Japanese tasks
- Vocabulary expansion: Why needed - Japanese requires finer-grained tokenization than English; Quick check - measure vocabulary collision rates before and after expansion
- Continual pre-training: Why needed - builds on existing LLM knowledge without starting from scratch; Quick check - track performance on source language tasks during adaptation
- Parallel corpora utilization: Why needed - provides explicit language alignment for translation tasks; Quick check - evaluate translation quality improvements with and without parallel data
- Monotonic scaling: Why needed - demonstrates efficient resource utilization in adaptation; Quick check - plot performance curves against training data scale

## Architecture Onboarding
Component map: Llama 2 base -> Vocabulary expansion -> Japanese web corpus pre-training -> Parallel corpora fine-tuning -> Swallow model
Critical path: The sequence of vocabulary expansion followed by continual pre-training on Japanese data, with optional parallel corpora integration, represents the most effective path to achieving strong bilingual performance.
Design tradeoffs: The primary tradeoff involves balancing vocabulary size (finer granularity vs. computational efficiency) and the amount of parallel vs. monolingual data used during pre-training.
Failure signatures: Performance degradation in summarization tasks indicates potential issues with vocabulary expansion, while catastrophic forgetting would manifest as declining English performance during Japanese adaptation.
First experiments:
1. Evaluate base Llama 2 performance on Japanese tasks before any modifications
2. Test vocabulary expansion impact on tokenization quality and collision rates
3. Compare continual pre-training efficiency against training from scratch on Japanese data

## Open Questions the Paper Calls Out
None

## Limitations
- The study focuses primarily on Japanese and English, leaving unclear whether findings generalize to other languages or language families
- Performance degradation in summarization tasks after vocabulary expansion suggests potential trade-offs requiring deeper investigation
- The long-term scalability of performance improvements with increasing training data is not established beyond tested ranges
- The focus on web-crawled Japanese data may not fully represent the diversity of Japanese language use across different domains and contexts

## Confidence
High confidence: The core finding that continual pre-training effectively improves Japanese language performance while maintaining English capabilities is well-supported by experimental results.

Medium confidence: The claim about vocabulary expansion improving efficiency without compromising performance is mostly supported, though the summarization exception indicates this may not be universally applicable.

Low confidence: The generalizability of these findings to other languages beyond Japanese remains speculative without additional experiments.

## Next Checks
1. Conduct parallel experiments adapting Llama 2 to other non-English languages (e.g., Chinese, Arabic, or Indian languages) to verify whether observed benefits extend beyond Japanese.

2. Design controlled experiments specifically targeting the summarization task performance degradation, varying vocabulary sizes and pre-training strategies to identify optimal configurations.

3. Extend the training data scale experiments beyond current ranges to determine whether monotonic performance improvement continues, plateaus, or shows diminishing returns at larger scales.