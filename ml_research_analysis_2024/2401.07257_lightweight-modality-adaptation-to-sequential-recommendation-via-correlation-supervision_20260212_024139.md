---
ver: rpa2
title: Lightweight Modality Adaptation to Sequential Recommendation via Correlation
  Supervision
arxiv_id: '2401.07257'
source_url: https://arxiv.org/abs/2401.07257
tags:
- modality
- correlation
- learning
- training
- recommendation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses modality forgetting in sequential recommendation,
  where modality embeddings over-adapt to sequential correlations, losing original
  modality information. The authors propose a lightweight knowledge distillation framework
  that preserves both modality information and efficiency.
---

# Lightweight Modality Adaptation to Sequential Recommendation via Correlation Supervision

## Quick Facts
- arXiv ID: 2401.07257
- Source URL: https://arxiv.org/abs/2401.07257
- Authors: Hengchang Hu; Qijiong Liu; Chuang Li; Min-Yen Kan
- Reference count: 40
- Primary result: Outperforms top baselines by 6.8% on average across five datasets

## Executive Summary
This paper addresses the challenge of modality forgetting in sequential recommendation, where sequential objectives cause modality embeddings to over-adapt and lose original information. The authors propose a lightweight knowledge distillation framework that preserves modality information while maintaining efficiency. By supervising the learning process with both holistic and dissected correlation signals distilled from original modality representations, the framework enhances embedding learning without requiring heavy computational resources.

## Method Summary
The method employs knowledge distillation to transfer correlation information from original modality encoders to a sequential recommendation model. It uses two types of correlation supervision: holistic correlations that capture overall associations between modality representations, and dissected correlation codes that refine relationship facets through vector quantization. An asynchronous learning strategy gradually increases the learning rate for modality embedding parameters, allowing original information to be retained longer during training. The framework is compatible with various backbone architectures and demonstrates significant performance improvements across multiple datasets.

## Key Results
- Outperforms top baselines by 6.8% on average across five datasets
- Shows 8.7% improvement in HR@20 on the Beauty dataset compared to state-of-the-art methods
- Demonstrates that larger modality encoders require larger embedding dimensions to reach full potential
- Validates that preserving original feature associations significantly boosts recommendation performance

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The holistic correlation scores preserve overall modality relationships while preventing overfitting to sequential patterns.
- Mechanism: By distilling correlations between modality representations using scoring functions (cosine similarity, Euclidean distance), the framework maintains modality information that would otherwise be lost during sequential training.
- Core assumption: The original modality encoder captures meaningful feature correlations that are useful for recommendation.
- Evidence anchors:
  - [abstract] "holistic correlations, which quantify their overall associations"
  - [section] "For example, using cosine similarity, the function calculates the angle's cosine between vectors mi and mj, represented as ξ(mi, mj) = cos(θmi,mj)"

### Mechanism 2
- Claim: Dissected correlation codes capture fine-grained feature relationships through vector quantization.
- Mechanism: The framework splits modality vectors into sub-vectors, computes correlations for each pair, and uses vector quantization to map these correlation patterns to discrete codes, preserving granular relationships.
- Core assumption: Splitting modality vectors into meaningful sub-vectors preserves important feature distinctions.
- Evidence anchors:
  - [abstract] "dissected correlation types, which refine their relationship facets (honing in on specific aspects like color or shape consistency)"
  - [section] "Drawing inspiration from Product Quantization [18], we treat each sub-vector from ˜ mas an individual, meaningful segment"

### Mechanism 3
- Claim: Asynchronous training prevents early loss of modality information during embedding adaptation.
- Mechanism: By starting with a lower learning rate for modality embedding parameters and gradually increasing it, the framework retains original modality information longer during training.
- Core assumption: Modality embeddings need more gradual adaptation than other parameters to preserve useful information.
- Evidence anchors:
  - [abstract] "we propose an asynchronous learning step, allowing the original information to be retained longer for training the representation learning module"
  - [section] "Specifically, we use an asynchronous update training strategy [55] for modality embedding parameters and other representation learning parameters"

## Foundational Learning

- Concept: Knowledge Distillation
  - Why needed here: To transfer valuable correlation information from complex modality encoders to the more efficient sequential recommendation model.
  - Quick check question: What are the two main components of a knowledge distillation framework?

- Concept: Vector Quantization
  - Why needed here: To convert continuous correlation vectors into discrete codes that can be efficiently matched between teacher and student models.
  - Quick check question: How does vector quantization help in preserving fine-grained correlation patterns?

- Concept: Asynchronous Learning Rates
  - Why needed here: To prevent modality embeddings from adapting too quickly and losing useful original information during sequential training.
  - Quick check question: What problem does asynchronous learning address in the context of modality embeddings?

## Architecture Onboarding

- Component map: Teacher Model → Correlation Distillation → Student Model → Representation Learning → Prediction
- Critical path: Modality Encoder → Correlation Distillation → Student Model → Representation Learning → Prediction
- Design tradeoffs:
  - Complexity vs. performance: More sophisticated modality encoders require larger embedding dimensions to capture correlations
  - Granularity vs. efficiency: Fine-grained correlation supervision improves performance but increases computational cost
  - Preservation vs. adaptation: Balancing retention of original modality information with adaptation to sequential patterns
- Failure signatures:
  - Performance degradation when using larger modality encoders with small embedding dimensions
  - Overfitting to correlation supervision signals, especially in long sequences
  - Loss of modality information despite correlation supervision
- First 3 experiments:
  1. Compare performance with and without holistic correlation supervision on a small dataset
  2. Test different quantization methods (VQ vs k-Means) for dissected correlation codes
  3. Evaluate asynchronous learning effectiveness by comparing different learning rate schedules

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the effectiveness of the KDSR framework vary when using more sophisticated distillation formats, such as graphs, instead of triplets (vi, vj, rij)?
- Basis in paper: [inferred] The paper mentions that "adopting structured distillation formats, like graphs, may also be a compelling next step."
- Why unresolved: The paper focuses on using triplets for knowledge distillation and does not explore other structured formats like graphs.
- What evidence would resolve it: Conducting experiments to compare the performance of KDSR using different distillation formats, such as graphs, and analyzing the impact on recommendation accuracy and efficiency.

### Open Question 2
- Question: How does the performance of KDSR change when integrating deeper layers from the modality encoder, beyond just the last layer?
- Basis in paper: [inferred] The paper states that "integrating deeper encoder layers in future research" is a potential direction.
- Why unresolved: The current implementation only considers the output of the modality encoder's last layer for knowledge distillation.
- What evidence would resolve it: Experimenting with different layers of the modality encoder for knowledge distillation and comparing the performance of KDSR in terms of recommendation accuracy and efficiency.

### Open Question 3
- Question: How does the joint supervision of intra-modality and inter-modality correlations affect the performance of KDSR in multi-modal sequential recommendation tasks?
- Basis in paper: [inferred] The paper suggests that "studying joint supervision across varied modality types" could enhance multi-modal SR tasks.
- Why unresolved: The current implementation focuses on intra-modality supervision and does not explore the impact of joint supervision across different modalities.
- What evidence would resolve it: Conducting experiments to evaluate the performance of KDSR with joint supervision of intra-modality and inter-modality correlations, and comparing the results with the current implementation.

## Limitations

- Performance dependence on embedding dimension: Larger modality encoders require larger embedding dimensions to reach full potential, creating scalability challenges.
- Correlation supervision assumptions: Effectiveness depends on original modality encoders capturing meaningful feature correlations.
- Computational overhead: While described as "lightweight," the additional correlation distillation components add complexity beyond standard sequential recommendation models.

## Confidence

- **High Confidence**: The core mechanism of correlation supervision preserving modality information during sequential adaptation is well-supported by ablation studies and comparison experiments.
- **Medium Confidence**: The claim that asynchronous learning significantly improves performance requires further validation, as the learning rate scheduling effects are not fully isolated from other factors.
- **Medium Confidence**: The scalability concerns regarding larger modality encoders need more empirical validation across diverse datasets and architectures.

## Next Checks

1. **Ablation study on asynchronous learning**: Run experiments with different learning rate schedules (linear vs. exponential decay) to isolate the impact of asynchronous learning from other factors.

2. **Cross-dataset generalization test**: Validate the framework on datasets with different characteristics (e.g., shorter sequences, different modality types) to assess robustness beyond the five Amazon datasets.

3. **Efficiency analysis**: Measure actual computational overhead (inference time, memory usage) compared to baseline models to verify the "lightweight" claim in practical scenarios.