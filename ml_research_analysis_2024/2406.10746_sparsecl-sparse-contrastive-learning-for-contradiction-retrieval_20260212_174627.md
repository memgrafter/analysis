---
ver: rpa2
title: 'SparseCL: Sparse Contrastive Learning for Contradiction Retrieval'
arxiv_id: '2406.10746'
source_url: https://arxiv.org/abs/2406.10746
tags:
- retrieval
- contradiction
- cosine
- corpus
- sparsity
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces SPARSE CL, a method for efficient contradiction
  retrieval in large document corpora. The core idea is to train sentence embeddings
  that preserve sparsity of differences between contradicting sentences, measured
  using the Hoyer sparsity metric.
---

# SparseCL: Sparse Contrastive Learning for Contradiction Retrieval

## Quick Facts
- **arXiv ID**: 2406.10746
- **Source URL**: https://arxiv.org/abs/2406.10746
- **Reference count**: 40
- **Primary result**: SPARSE CL achieves over 30% improvement in NDCG@10 scores for contradiction retrieval across multiple datasets and model architectures

## Executive Summary
This paper introduces SPARSE CL, a method for efficient contradiction retrieval in large document corpora. The core innovation is training sentence embeddings that preserve sparsity of differences between contradicting sentences, measured using the Hoyer sparsity metric. By combining cosine similarity with Hoyer sparsity, the method can effectively identify contradictory passages while maintaining computational efficiency. The approach significantly outperforms existing methods on both MSMARCO and HotpotQA datasets, achieving substantial improvements in retrieval accuracy while being at least 200 times faster than cross-encoder models.

## Method Summary
SPARSE CL fine-tunes pretrained sentence embedding models using contrastive learning with Hoyer sparsity as a key component. During training, positive examples are contradictory passages while hard negatives are similar (paraphrased) passages. The method optimizes embeddings to preserve sparse differences between contradictory pairs while maintaining semantic similarity in other dimensions. For retrieval, SPARSE CL combines cosine similarity with the Hoyer sparsity measure to score potential contradictions, enabling efficient identification of contradicting passages in large corpora. The approach is validated on both real-world counterargument retrieval tasks and synthetic contradiction datasets, showing significant performance improvements over baseline methods.

## Key Results
- SPARSE CL achieves over 30% improvement in NDCG@10 scores on MSMARCO and HotpotQA datasets
- The method is effective across different model architectures including BGE, UAE, and GTE models
- Hoyer sparsity calculations are at least 200 times faster than cross-encoder models, enabling scalable contradiction retrieval
- SPARSE CL successfully recovers over 60% of performance loss in corpus cleaning applications with corrupted data

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Hoyer sparsity combined with cosine similarity captures contradictions better than cosine alone because it emphasizes differences in sparse semantic dimensions.
- **Core assumption**: Contradictions manifest as sparse differences in semantic embedding space, where only a few dimensions capture the disagreement.
- **Evidence anchors**: Abstract mentions combined metric of cosine similarity and sparsity function; section states key idea is to train embeddings to preserve sparsity of differences.
- **Break condition**: If contradictions are not sparse in semantic dimensions, the Hoyer sparsity metric will fail to capture the contradiction effectively.

### Mechanism 2
- **Claim**: Training embeddings with contrastive learning that rewards sparsity preserves contradictory nuances.
- **Core assumption**: The contrastive learning framework with Hoyer sparsity as a reward function can successfully train embeddings to preserve contradictory nuances.
- **Evidence anchors**: Section describes using contrastive learning to fine-tune pretrained sentence embedding models; sparsity function chosen is Hoyer sparsity with normalized output.
- **Break condition**: If contrastive learning cannot effectively optimize for Hoyer sparsity, or if Hoyer sparsity is not a good proxy for contradictory nuances, training will fail.

### Mechanism 3
- **Claim**: Hoyer sparsity is computationally efficient compared to cross-encoder models, making contradiction retrieval scalable.
- **Core assumption**: Hoyer sparsity calculation is significantly faster than cross-encoder models, and this speed difference is maintained in practice.
- **Evidence anchors**: Abstract states Hoyer sparsity calculations are at least 200 times faster than cross-encoder models; appendix shows cross-encoder is at least 200 times slower than standard vector computation.
- **Break condition**: If speed advantage of Hoyer sparsity calculation is not maintained in practice, or if computational efficiency is not a bottleneck, this mechanism will not provide significant benefit.

## Foundational Learning

- **Concept**: Hoyer sparsity metric
  - **Why needed here**: Provides measure of "spikiness" of difference vector between embeddings, crucial for capturing sparse contradictions
  - **Quick check question**: What is the range of the Hoyer sparsity metric, and what does a high value indicate about the difference vector between two embeddings?

- **Concept**: Contrastive learning
  - **Why needed here**: Used to train sentence embeddings to preserve sparsity of differences between contradictory passages
  - **Quick check question**: In context of this paper, what are positive and negative examples used in contrastive learning framework?

- **Concept**: Embedding similarity and dissimilarity
  - **Why needed here**: Contradiction retrieval requires combination of similarity (shared semantic dimensions) and dissimilarity (sparse disagreements)
  - **Quick check question**: Why is simple cosine similarity insufficient for contradiction retrieval, and how does combination with Hoyer sparsity address this limitation?

## Architecture Onboarding

- **Component map**: Pretrained sentence embedding model -> SPARSE CL training module -> Hoyer sparsity calculation module -> Cosine similarity calculation module -> Score function -> Retrieval module

- **Critical path**: 
  1. Fine-tune pretrained sentence embedding model using SPARSE CL training module
  2. Calculate embeddings for all passages in corpus using fine-tuned model
  3. For given query, calculate embeddings and Hoyer sparsity with all corpus passages
  4. Combine cosine similarity and Hoyer sparsity scores using score function
  5. Retrieve top contradicting passages based on combined score

- **Design tradeoffs**: 
  - Embedding model size vs. retrieval speed: Larger models may capture more nuanced contradictions but will be slower to calculate
  - Hoyer sparsity weight (α) vs. retrieval accuracy: Weight of Hoyer sparsity in score function needs tuning for optimal performance on different datasets
  - Corpus size vs. computational efficiency: Larger corpora will require more efficient data structures (e.g., FAISS) for fast retrieval

- **Failure signatures**:
  - Poor retrieval accuracy: Hoyer sparsity metric may not capture right kind of sparse differences, or embedding model may not be trained effectively
  - Slow retrieval: Computational efficiency of Hoyer sparsity calculation may not be maintained in practice, or retrieval module may not be optimized for large corpora
  - Unstable results: Score function may be too sensitive to choice of α, or embedding model may not be robust to different types of contradictions

- **First 3 experiments**:
  1. Verify Hoyer sparsity metric: Calculate Hoyer sparsity for known contradictory and non-contradictory pairs to ensure it captures expected patterns
  2. Test contrastive learning framework: Train embedding model on small dataset and evaluate Hoyer sparsity between positive and negative pairs to ensure it is being optimized correctly
  3. Evaluate retrieval accuracy: Test full pipeline on benchmark dataset (e.g., Arguana) and compare NDCG@10 score with baseline methods to ensure method is effective

## Open Questions the Paper Calls Out

- **Open Question 1**: Can sparse-aware sentence embeddings enable sublinear-time nearest neighbor search for contradiction retrieval, as opposed to current linear scan approach? The paper mentions this as interesting open question since it doesn't explore methods for sublinear-time search with Hoyer sparsity measure.

- **Open Question 2**: How does choice of sparsity function (other than Hoyer) affect performance of sparse-aware sentence embeddings in contradiction retrieval tasks? Paper experiments with different sparsity functions (l2/l1 and κ4) but doesn't explore other potential sparsity functions or provide comprehensive comparison.

- **Open Question 3**: How does sparsity-based contradiction retrieval method generalize to other languages and multilingual scenarios? Paper focuses on English datasets and doesn't explore generalization to other languages or multilingual settings.

## Limitations

- Claims about Hoyer sparsity being optimal sparsity measure lack ablation studies comparing alternative sparsity metrics
- Performance improvements may partly depend on specific datasets used and quality of GPT-4 generated synthetic contradictions
- Computational efficiency claims rely on relative comparisons without providing absolute performance benchmarks
- Method's generalization to other contradiction detection tasks beyond tested domains remains unverified

## Confidence

- **High Confidence**: Core mechanism of combining cosine similarity with Hoyer sparsity for contradiction retrieval is well-founded and experimentally validated; computational efficiency advantage over cross-encoders is strongly supported
- **Medium Confidence**: Effectiveness across different sentence embedding architectures is demonstrated but could benefit from testing on additional model families; corpus cleaning application shows promise but was only evaluated in limited synthetic setting
- **Low Confidence**: Claims about Hoyer sparsity being uniquely suited for contradiction detection versus other sparsity metrics lack comparative analysis; long-term robustness when applied to continuously evolving document corpora is not addressed

## Next Checks

1. Conduct ablation studies comparing Hoyer sparsity against alternative sparsity metrics (e.g., Gini index, coefficient of variation) to verify its superiority for contradiction detection

2. Test SPARSE CL on additional benchmark datasets with naturally occurring contradictions (not GPT-4 generated) to validate generalization beyond synthetic data

3. Implement long-term evaluation framework to assess model performance degradation over time when applied to dynamically changing document corpora