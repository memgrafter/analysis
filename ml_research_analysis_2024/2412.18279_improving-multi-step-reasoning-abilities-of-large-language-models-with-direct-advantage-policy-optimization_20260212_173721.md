---
ver: rpa2
title: Improving Multi-Step Reasoning Abilities of Large Language Models with Direct
  Advantage Policy Optimization
arxiv_id: '2412.18279'
source_url: https://arxiv.org/abs/2412.18279
tags:
- dapo
- policy
- training
- optimization
- reasoning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces Direct Advantage Policy Optimization (DAPO),
  a novel offline reinforcement learning method designed to improve the multi-step
  reasoning capabilities of large language models (LLMs). DAPO addresses two key challenges
  in applying RL to LLM reasoning: sparse rewards and unstable training processes.'
---

# Improving Multi-Step Reasoning Abilities of Large Language Models with Direct Advantage Policy Optimization

## Quick Facts
- arXiv ID: 2412.18279
- Source URL: https://arxiv.org/abs/2412.18279
- Reference count: 40
- Improves multi-step reasoning performance of LLMs through offline reinforcement learning with dense advantage signals

## Executive Summary
This paper introduces Direct Advantage Policy Optimization (DAPO), an offline reinforcement learning method designed to improve multi-step reasoning capabilities of large language models. DAPO addresses two key challenges in applying RL to LLM reasoning: sparse rewards and unstable training processes. The method employs a critic function to predict reasoning accuracy at each step, generating dense training signals, and trains the actor and critic components independently to avoid co-training instability. The authors demonstrate DAPO's effectiveness through extensive experiments on mathematical and coding benchmarks, showing significant improvements across multiple models and tasks.

## Method Summary
DAPO is a two-stage offline RL approach that improves multi-step reasoning in LLMs. First, a critic network is trained to predict the value (accuracy) of reasoning states using Monte Carlo rollouts from a reference policy. The critic is trained on pre-collected data where each state is completed multiple times to estimate true values. In the second stage, the actor (policy) is optimized using advantages computed by the pretrained critic, avoiding the instability of co-training. The method uses a KL constraint to prevent policy collapse and employs an objective function that maximizes expected advantage while maintaining similarity to the reference policy. DAPO operates entirely offline, generating advantage datasets from pre-collected solutions rather than requiring on-policy rollouts.

## Key Results
- Achieved up to 60.33% accuracy on MATH test set, a 7.60 percentage point improvement over base models
- Increased code generation performance by up to 9.2 percentage points on certain benchmarks
- Demonstrated strong generalization across in-domain and out-of-domain mathematical reasoning tasks
- Showed consistent improvements across multiple model sizes and architectures, including state-of-the-art models like Qwen2.5-Math-7B-Instruct

## Why This Works (Mechanism)

### Mechanism 1
- Claim: DAPO solves the sparse reward problem by generating dense training signals at each reasoning step
- Mechanism: Instead of only receiving rewards at the terminal state, DAPO trains a critic function to predict reasoning accuracy at each intermediate step, creating dense advantage signals that guide policy optimization throughout the reasoning process
- Core assumption: The critic can accurately approximate the true value function of the reasoning process
- Evidence anchors:
  - [abstract]: "DAPO employs a critic function to predict the reasoning accuracy at each step, thereby generating dense signals to refine the generation strategy"
  - [section 3.2]: "For each state s ∈ D_gen, DAPO uses the completer to sample multiple sub-trajectory from s and collects the MC value as the estimation of the true value. Then a critic network V_ϕ is trained to approximate the value function"
  - [corpus]: Weak - corpus neighbors don't directly address dense reward generation methods
- Break condition: If the critic fails to accurately predict step-level accuracy, the dense signals become misleading and optimization degrades

### Mechanism 2
- Claim: Independent training of actor and critic components eliminates co-training instability
- Mechanism: Unlike standard Actor-Critic methods that train actor and critic simultaneously with interdependent updates, DAPO trains the critic first to convergence, then optimizes the actor using the pretrained critic's advantages
- Core assumption: The critic can be trained to sufficient accuracy before actor training begins
- Evidence anchors:
  - [abstract]: "the Actor and Critic components in DAPO are trained independently, avoiding the co-training instability observed in standard AC algorithms like PPO"
  - [section 1]: "Another challenge stems from the inherent instability of RL, particularly when using Actor-Critic (AC) methods... which often leads to unstable training processes"
  - [section 3.1]: "Compared with standard actor-critic methods that train the actor and critic simultaneously, DAPO trains the critic firstly to ensure it's a good approximation of true value function then optimize the actor using the advantage dataset produced by the pretrained critic"
  - [corpus]: Weak - corpus neighbors don't directly discuss actor-critic co-training instability solutions
- Break condition: If the critic cannot achieve sufficient accuracy during pretraining, subsequent actor optimization will be ineffective

### Mechanism 3
- Claim: Offline training with pre-collected data eliminates the sampling efficiency problem of on-policy methods
- Mechanism: DAPO generates advantage datasets offline using a reference policy, then optimizes the actor entirely from this fixed dataset rather than requiring expensive on-policy rollouts
- Core assumption: The pre-collected data contains sufficient diversity to capture the reasoning space
- Evidence anchors:
  - [abstract]: "DAPO, an novel step-level offline RL algorithm. Unlike standard alignment that rely solely outcome rewards to optimize policies (such as DPO), DAPO employs a critic function to predict the reasoning accuracy at each step"
  - [section 1]: "Given the vast generation space, the successful training of LLMs requires extensive sampling to address exploration challenges, posing a significant challenge for on-policy methods like PPO [39], which have high sampling costs and low sample efficiency"
  - [section 3.1]: "We now to turn to the offline policy optimization problem setting for the sake of implementation simplicity and data efficiency"
  - [corpus]: Moderate - related work mentions sampling efficiency improvements in RLVR contexts
- Break condition: If the offline dataset is too narrow or unrepresentative, the learned policy will not generalize to new reasoning tasks

## Foundational Learning

- Concept: Reinforcement Learning from Human Feedback (RLHF)
  - Why needed here: DAPO is built on RLHF framework but extends it to step-level reasoning tasks with sparse rewards
  - Quick check question: What are the two main stages of standard RLHF, and how does DAPO modify this pipeline for reasoning tasks?

- Concept: Markov Decision Process (MDP) formulation of language generation
  - Why needed here: The paper models LLM reasoning as an MDP where states are intermediate reasoning steps and actions are next reasoning steps
  - Quick check question: How does defining actions as "reasoning steps" rather than "tokens" change the decision horizon and reward structure in DAPO?

- Concept: Value function approximation and Bellman operators
  - Why needed here: DAPO relies on training a critic to approximate the value function and uses Bellman optimality conditions for theoretical analysis
  - Quick check question: What is the relationship between the critic's predicted values and the true advantage function used for policy optimization in DAPO?

## Architecture Onboarding

- Component map: Generator -> Critic -> Actor -> Evaluation
- Critical path: Data generation → Critic training → Advantage computation → Actor optimization → Evaluation
- Design tradeoffs:
  - Dense vs sparse rewards: DAPO chooses dense step-level rewards at the cost of training a critic
  - Online vs offline training: DAPO uses offline training for efficiency vs. on-policy methods' better exploration
  - Independent vs co-training: DAPO trains components separately to avoid instability vs. standard AC methods' potential for faster convergence
- Failure signatures:
  - Critic underfitting: Low advantage gaps across states indicate critic cannot distinguish good from bad reasoning steps
  - Policy collapse: KL divergence between learned policy and reference policy grows too large
  - No improvement: Actor loss decreases but evaluation metrics remain flat
- First 3 experiments:
  1. Verify critic training: Check if MC estimation of values correlates with critic predictions on validation data
  2. Test advantage computation: Ensure advantages are positive for better actions and negative for worse actions
  3. Validate KL constraint: Monitor KL divergence between optimized policy and reference policy during actor training

## Open Questions the Paper Calls Out

- Question: Does DAPO's performance improvement scale with the size of the training dataset?
  - Basis in paper: [explicit] The paper notes that only 7,500 training problems from MATH were used and states "It's reasonable to anticipate a higher performance improvement of DAPO with a larger training query set."
  - Why unresolved: The authors did not conduct experiments with varying dataset sizes to establish a scaling law.
  - What evidence would resolve it: Experiments comparing DAPO performance using different quantities of training data (e.g., 1K, 7.5K, 75K, 750K problems) to determine if there's a logarithmic or power-law relationship between dataset size and performance gains.

- Question: What is the optimal sampling distribution νA(·|s) that maximizes the lower bound λs(νA) in Theorem 3.2?
  - Basis in paper: [explicit] The authors note in Remark 3.2 that "It remains as a great interest for us to derive a more precise expression of λs for general νA and figure out which νA should yield biggest lower bound."
  - Why unresolved: The theoretical analysis establishes that λs(νA) serves as a lower bound on performance improvement but doesn't provide the optimal distribution.
  - What evidence would resolve it: Analytical derivation of the optimal νA(·|s) that maximizes λs(νA), or empirical comparison of different sampling strategies (uniform, importance sampling, policy-based) to identify which yields the greatest performance gains.

- Question: What is the limit of DAPO's performance improvement on a fixed number of training problems?
  - Basis in paper: [explicit] The authors state they "would like to investigate what's the limitation of DAPO on a certain number of training problems in future work" and only conducted two iterative DAPO experiments due to resource constraints.
  - Why unresolved: Limited computational resources prevented exhaustive testing of iterative DAPO across multiple models and iterations.
  - What evidence would resolve it: Systematic study of DAPO performance through 5+ iterations on multiple base models, measuring whether improvements plateau and identifying the iteration count at which marginal gains become negligible.

## Limitations
- High computational cost for critic training requiring multiple completions per reasoning step
- Offline nature may limit exploration of new reasoning strategies beyond pre-collected data
- Effectiveness highly dependent on quality of reference policy used for data generation

## Confidence
- High confidence: DAPO improves multi-step reasoning performance across multiple benchmarks compared to base models
- Medium confidence: DAPO solves the sparse reward problem more effectively than outcome-only reward methods
- Medium confidence: Independent training of actor and critic eliminates co-training instability

## Next Checks
1. Ablation on critic pretraining: Compare DAPO performance with and without independent critic pretraining to directly measure the impact of avoiding co-training instability
2. Scaling analysis: Evaluate DAPO's effectiveness across different model sizes (beyond the 7B parameter models tested) to understand scaling properties
3. Generalization test: Assess DAPO's performance on completely unseen reasoning domains to verify true generalization beyond in-domain tasks