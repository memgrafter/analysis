---
ver: rpa2
title: 'Unleashing Artificial Cognition: Integrating Multiple AI Systems'
arxiv_id: '2408.04910'
source_url: https://arxiv.org/abs/2408.04910
tags:
- system
- chess
- cognitive
- reasoning
- cognition
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study presents an open-source AI system that integrates multiple
  AI models to achieve cognitive behaviors. The system combines a chess engine with
  language models, leveraging retrieval-augmented generation and vector databases
  to provide strategic explanations for moves.
---

# Unleashing Artificial Cognition: Integrating Multiple AI Systems

## Quick Facts
- arXiv ID: 2408.04910
- Source URL: https://arxiv.org/abs/2408.04910
- Authors: Muntasir Adnan; Buddhi Gamage; Zhiwei Xu; Damith Herath; Carlos C. N. Kuhn
- Reference count: 0
- Primary result: Achieves 100% accuracy in chess move prediction through integration of chess engine with language models

## Executive Summary
This study presents an open-source AI system that integrates multiple AI models to achieve cognitive behaviors, specifically combining a chess engine with language models, retrieval-augmented generation, and vector databases to provide strategic explanations for moves. The system evaluates cognitive abilities through five qualities: perception, memory, attention, reasoning, and anticipation. It demonstrates superior performance compared to individual language models in chess move prediction and strategic reasoning, with GPT-4o showing the highest reasoning scores among tested models. The framework provides a systematic approach to assess AI cognition and shows promise for applications beyond chess.

## Method Summary
The system integrates a chess engine (Stockfish) with language models using retrieval-augmented generation and vector databases to provide strategic explanations for chess moves. It employs a query analyzer to route requests to appropriate services, including a chess engine for move prediction and a fine-tuned Mistral 7B model with PEFT and LoRA for reasoning. The system uses a Faiss vector database for knowledge retrieval and evaluates performance across five cognitive qualities through human assessment using a 0-5 rubric.

## Key Results
- Achieves 100% accuracy in best-move prediction using the integrated chess engine
- GPT-4o demonstrates highest reasoning scores among tested language models
- System successfully integrates multiple AI components to provide both optimal move prediction and strategic explanations

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Integrating a chess engine with a language model provides perfect move prediction while the LLM adds strategic reasoning
- Mechanism: The chess engine (Stockfish) supplies optimal move predictions through game tree search, while the LLM generates human-understandable explanations. The query analyzer routes chess-specific queries to the engine, creating a hybrid system that achieves both computational precision and communicative clarity
- Core assumption: The LLM can accurately interpret and explain the chess engine's decisions without introducing significant errors
- Evidence anchors:
  - [abstract] "The system combines a chess engine with language models, leveraging retrieval-augmented generation and vector databases to provide strategic explanations for moves"
  - [section] "Particularly, Stockfish is a highly advanced and open-source chess engine renowned for its robust performance and ability to analyse positions and predict potential moves. In our system, the chess engine service interacts seamlessly with the LLM, providing real-time move suggestions"
  - [corpus] Weak - The corpus mentions similar chess-AI integration studies but doesn't directly address the explanation generation capability
- Break condition: If the LLM consistently misinterprets the chess engine's reasoning or introduces hallucinations when explaining moves, the integration fails to provide meaningful strategic insight

### Mechanism 2
- Claim: Retrieval-Augmented Generation (RAG) with vector databases enables the system to access and utilize external knowledge for enhanced reasoning
- Mechanism: The system encodes external documents (chess books) into a Faiss vector database, retrieves relevant contexts based on query similarity, and presents them to the LLM for reasoning. This creates a dynamic memory system that supplements the base model's knowledge
- Core assumption: The retrieved contexts are sufficiently relevant and accurate to improve the LLM's responses without causing confusion or hallucinations
- Evidence anchors:
  - [abstract] "Leveraging a vector database to achieve retrievable answer generation, our AI system elucidates its decision-making process"
  - [section] "We adopt RAG [16] in our framework to leverage external knowledge sources stored in a Faiss vector data store [6]. An embedding model plays a crucial role in transforming both textual information from the knowledge source and user queries into high-dimensional vectors"
  - [corpus] Weak - Corpus contains studies on chess AI but lacks specific evidence about RAG effectiveness in chess reasoning contexts
- Break condition: If similarity thresholds are too low and irrelevant information is retrieved, or too high and useful information is missed, the RAG system degrades reasoning quality

### Mechanism 3
- Claim: Fine-tuning with Parameter-Efficient Fine-Tuning (PEFT) and Low-Rank Adaptation (LoRA) creates a model specialized for chess reasoning while maintaining computational efficiency
- Mechanism: The Mistral 7B base model is quantized to 4-bit representation and fine-tuned using LoRA adapters with chess-specific datasets including FEN parsing, move analysis, and chain-of-thought reasoning. This creates a specialized student model that can handle chess tasks efficiently
- Core assumption: The fine-tuning process successfully transfers reasoning capabilities without overwhelming the smaller model's capacity
- Evidence anchors:
  - [abstract] "The system combines a chess engine with language models, leveraging retrieval-augmented generation and vector databases to provide strategic explanations for moves"
  - [section] "For fine-tuning, we leverage an instruction tuning [30, 31] methodology. The base model is Mistral 7B, chosen for its balance of performance, efficiency and size... To optimize computational efficiency during fine-tuning, we employ several techniques. The base Mistral 7B model is quantized to a 4-bit representation [4]"
  - [corpus] Moderate - The corpus includes studies on chess neural networks and AI cognition, supporting the general approach but not specifically validating this fine-tuning methodology
- Break condition: If the fine-tuned model develops severe hallucination tendencies when confronted with zero-shot tasks, as observed in related studies, the approach becomes unreliable

## Foundational Learning

- Concept: Vector embeddings and similarity search
  - Why needed here: The RAG system relies on transforming text into vector representations and finding similar contexts using distance metrics. Understanding this is crucial for implementing and troubleshooting the knowledge retrieval component
  - Quick check question: How does the Faiss vector database use cosine similarity to retrieve relevant chess strategy contexts from the vector store?

- Concept: Chain-of-Thought (CoT) reasoning in language models
  - Why needed here: The system uses CoT prompting to elicit step-by-step reasoning from the LLM when explaining chess moves. This is fundamental to understanding how the system generates strategic explanations
  - Quick check question: What role does the system prompt play in encouraging the fine-tuned model to produce deliberative reasoning rather than direct answers?

- Concept: Chess notation systems (FEN, Algebraic Notation)
  - Why needed here: The system must parse and generate chess positions using FEN notation and moves using Algebraic Notation. This is essential for the chess engine integration and evaluation metrics
  - Quick check question: How would you convert a chess position represented in FEN format into a sequence of moves in Algebraic Notation?

## Architecture Onboarding

- Component map: Query analyzer -> Service router -> Chess engine service (Stockfish) + LLM reasoning + Vector database (Faiss) + RAG framework + PEFT fine-tuned model + Vector database update service
- Critical path: User query -> Query analyzer -> Service selection -> Chess engine for moves OR RAG retrieval -> LLM reasoning -> Response generation
- Design tradeoffs: The system trades computational efficiency (using quantized models and LoRA) for specialized reasoning capability. It also balances perfect move prediction (from Stockfish) with human-understandable explanations (from LLM)
- Failure signatures: Query analyzer misrouting, LLM hallucinations in explanations, RAG retrieving irrelevant contexts, chess engine integration failures, vector database update delays
- First 3 experiments:
  1. Test the query analyzer with 20 diverse chess and non-chess queries to verify correct service routing (target: >90% accuracy)
  2. Validate RAG retrieval by querying known chess positions and checking if relevant strategy contexts are retrieved (target: >75% relevant retrieval)
  3. Benchmark the fine-tuned model on chess puzzles to measure reasoning quality using the provided scoring rubric (target: average score >3 on 1-5 scale)

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of the integrated system compare to specialized systems in domains beyond chess, such as medical diagnostics or financial forecasting?
- Basis in paper: [explicit] The paper mentions the system's potential for applications in medical diagnostics and financial forecasting but does not provide empirical evidence or comparative analysis
- Why unresolved: The paper focuses on demonstrating the system's capabilities within the chess domain and does not extend its evaluation to other domains. The authors acknowledge the need for further research to explore the system's applicability and performance in diverse real-world scenarios
- What evidence would resolve it: Empirical studies comparing the integrated system's performance against specialized systems in medical diagnostics, financial forecasting, or other domains would provide evidence of its generalizability and effectiveness

### Open Question 2
- Question: What are the specific limitations and challenges of using fine-tuned language models in conjunction with Retrieval-Augmented Generation (RAG) for complex reasoning tasks?
- Basis in paper: [explicit] The paper discusses the challenges faced by the fine-tuned Mistral 7B model when presented with novel contexts, leading to hallucinations. It also mentions the need for further fine-tuning to improve the model's ability to leverage retrieved contexts effectively
- Why unresolved: While the paper identifies some limitations, it does not provide a comprehensive analysis of the challenges and potential solutions for integrating fine-tuned models with RAG in complex reasoning tasks. The authors suggest that a dedicated study is warranted to address these concerns
- What evidence would resolve it: In-depth analysis of the performance of fine-tuned models in RAG frameworks across various reasoning tasks, along with proposed solutions to mitigate hallucinations and improve context utilization, would provide insights into the limitations and challenges of this approach

### Open Question 3
- Question: How can the query analyzer be improved to dynamically route user queries to appropriate services based on context rather than keyword detection?
- Basis in paper: [explicit] The paper acknowledges the limitations of the current keyword-based query analyzer and proposes transitioning to a fine-tuned language model or classification model for more accurate and context-aware routing of user queries
- Why unresolved: The paper presents the proposed improvement as future work and does not provide empirical evidence or comparative analysis of the current and improved query analyzer performance
- What evidence would resolve it: Comparative studies evaluating the performance of the current keyword-based query analyzer and the proposed context-aware query analyzer in terms of accuracy, efficiency, and scalability would demonstrate the effectiveness of the proposed improvement

## Limitations
- Evaluation framework relies heavily on human judgment, introducing potential subjectivity and reproducibility challenges
- System's performance with base models without fine-tuning is not clearly established
- Exact implementation details of critical components like the query analyzer are not fully specified

## Confidence

- **High confidence**: Core mechanism of integrating chess engine with LLM for move prediction and explanation generation, as this approach has strong theoretical grounding and results are unambiguous
- **Medium confidence**: RAG framework's contribution to reasoning quality, as methodology is sound but lacks detailed analysis of how retrieved contexts specifically improve outcomes
- **Low confidence**: Claims about fine-tuning process effectiveness, as study doesn't provide baseline comparisons with non-fine-tuned models or detailed hyperparameter information

## Next Checks

1. **Reproduce the integration pipeline**: Implement the full system with Stockfish, Mistral 7B, and RAG components to verify the 100% move prediction accuracy claim and assess the quality of generated explanations across diverse chess positions

2. **Conduct ablation studies**: Test the system with and without each major component (chess engine, RAG, fine-tuning) to quantify their individual contributions to overall performance and isolate which mechanisms drive the observed improvements

3. **Cross-domain validation**: Apply the integrated framework to a non-chess domain (e.g., medical diagnosis using symptom-to-condition mappings) to evaluate whether the cognitive architecture generalizes beyond the chess-specific implementation and maintains reasoning quality