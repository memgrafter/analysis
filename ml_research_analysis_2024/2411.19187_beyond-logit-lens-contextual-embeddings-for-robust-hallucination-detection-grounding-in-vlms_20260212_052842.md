---
ver: rpa2
title: 'Beyond Logit Lens: Contextual Embeddings for Robust Hallucination Detection
  & Grounding in VLMs'
arxiv_id: '2411.19187'
source_url: https://arxiv.org/abs/2411.19187
tags:
- arxiv
- image
- hallucination
- answer
- visual
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates the limitations of the logit lens technique
  for detecting hallucinations in vision-language models (VLMs). The authors propose
  ContextualLens, which leverages contextual token embeddings from intermediate layers
  of VLMs to improve hallucination detection across diverse categories such as actions,
  OCR, attributes, spatial relations, and comparisons.
---

# Beyond Logit Lens: Contextual Embeddings for Robust Hallucination Detection & Grounding in VLMs

## Quick Facts
- arXiv ID: 2411.19187
- Source URL: https://arxiv.org/abs/2411.19187
- Authors: Anirudh Phukan; Divyansh; Harshit Kumar Morj; Vaishnavi; Apoorv Saxena; Koustava Goswami
- Reference count: 14
- Key outcome: ContextualLens improves hallucination detection across diverse categories and introduces precise grounding technique with significant mAP improvements

## Executive Summary
This paper addresses the limitations of logit lens for detecting hallucinations in vision-language models (VLMs) by proposing ContextualLens, which leverages contextual token embeddings from intermediate layers. The method outperforms logit lens across hallucination categories requiring contextual understanding such as actions, OCR, attributes, spatial relations, and comparisons. Additionally, the authors introduce a novel grounding technique that produces highly precise bounding boxes, enabling a transition from zero-shot object segmentation to grounded visual question answering (GVQA). Experiments on HQH, TextVQA-X, and VizWiz-G benchmarks demonstrate significant improvements in both hallucination detection accuracy and grounding precision.

## Method Summary
The paper proposes ContextualLens, which extracts contextual embeddings from middle layers of VLMs rather than relying on output probabilities like logit lens. For hallucination detection, it computes cosine similarity between answer tokens and image patches using embeddings from specific layers (layer 13 for image patches and layer 27 for text embeddings in InternLM-VL). For grounding, the method computes cosine similarity between answer embeddings and bounding box embeddings, selecting the optimal bounding box. The approach is evaluated on three benchmarks: HQH (4,000 pairs), TextVQA-X (3,620 pairs), and VizWiz-G (1,131 pairs), demonstrating superior performance particularly in categories requiring contextual understanding.

## Key Results
- ContextualLens achieves higher mean average precision (mAP) than logit lens across hallucination detection tasks
- Significant improvements in grounding precision with the proposed bounding box technique on VizWiz-G benchmark
- Outperforms state-of-the-art in categories requiring contextual understanding (actions, OCR, attributes, spatial relations, comparisons)
- Effective transition from zero-shot object segmentation to grounded visual question answering (GVQA)

## Why This Works (Mechanism)
Contextual embeddings capture richer semantic relationships between image regions and answer tokens compared to static output probabilities used in logit lens. By leveraging intermediate layers, ContextualLens can better model the nuanced contextual dependencies required for detecting hallucinations in complex scenarios. The grounding technique exploits the spatial-semantic alignment encoded in contextual embeddings to produce more precise bounding boxes, particularly valuable for multimodal attribution where precision is prioritized over recall.

## Foundational Learning
- **Vision-Language Models (VLMs)**: Multimodal models that process both visual and textual inputs - needed to understand the foundation of hallucination detection in multimodal contexts; quick check: verify understanding of VLM architectures like Qwen2-VL-7B and InternLM-VL-7B
- **Contextual Embeddings**: Intermediate layer representations that capture semantic context - needed to grasp why middle-layer embeddings outperform output probabilities; quick check: understand difference between contextual and static embeddings
- **Cosine Similarity in Multimodal Space**: Metric for measuring semantic alignment between visual and textual features - needed to comprehend the core mechanism of both hallucination detection and grounding; quick check: verify how cosine similarity is computed between different modality embeddings
- **Bounding Box Grounding**: Technique for localizing answers in images through spatial attention - needed to understand the transition from segmentation to precise localization; quick check: understand how bounding boxes are generated from embedding similarity
- **Hallucination Detection Categories**: Different types of errors (actions, OCR, attributes, spatial relations, comparisons) - needed to appreciate the breadth of evaluation; quick check: distinguish between categories that require contextual understanding vs. those that don't
- **Mean Average Precision (mAP)**: Evaluation metric combining precision and recall across thresholds - needed to interpret the reported performance improvements; quick check: understand how mAP differs from simple accuracy metrics

## Architecture Onboarding

**Component Map**: VLM Encoder -> Intermediate Layer Extraction -> Contextual Embedding Generation -> Cosine Similarity Computation -> Hallucination Score/Optimal Bounding Box

**Critical Path**: Input Image+Question -> VLM Processing -> Layer 13 (image) + Layer 27 (text) Embeddings -> Cosine Similarity -> Grounding/Detection Decision

**Design Tradeoffs**: Contextual embeddings provide better semantic understanding but require careful layer selection; grounding prioritizes precision over recall, which may miss some evidence but ensures high-confidence localization

**Failure Signatures**: Poor performance in count category due to low semantic overlap between visual elements and numerical tokens; suboptimal layer selection leading to weak contextual signals

**3 First Experiments**:
1. Implement ContextualLens hallucination detection on HQH dataset using Qwen2-VL-7B with cosine similarity between layer 13 image embeddings and layer 27 text embeddings
2. Test different layer combinations (layers 10-15 for images, layers 25-30 for text) on validation set to identify optimal configuration
3. Implement grounding component on VizWiz-G using cosine similarity between answer embeddings and bounding box embeddings, comparing precision-recall curves with baseline segmentation

## Open Questions the Paper Calls Out
### Open Question 1
- Question: What is the optimal layer combination for hallucination detection across different VLM architectures and hallucination categories?
- Basis in paper: [explicit] The paper discusses layer selection for ContextualLens, showing that layer 13 for image patches and layer 27 for text embeddings worked well for InternLM-VL across most categories, with adversarial validation suggesting robustness.
- Why unresolved: Different VLM architectures may have different optimal layer combinations, and the optimal layers might vary depending on the specific hallucination category being evaluated.
- What evidence would resolve it: Systematic experiments across multiple VLM architectures (InternLM-VL, Qwen2VL-7B, etc.) testing various layer combinations for each hallucination category, with analysis of consistency and robustness across architectures.

### Open Question 2
- Question: How can ContextualLens be extended to handle abstractive scenarios like counting and comparison more effectively?
- Basis in paper: [explicit] The paper notes that ContextualLens is outperformed by output probabilities in the Count category due to low semantic overlap between visual elements and numerical tokens, suggesting limitations in handling abstractive scenarios.
- Why unresolved: The current method relies on cosine similarity between contextual embeddings, which may not capture the abstract relationships needed for counting and comparison tasks.
- What evidence would resolve it: Development and evaluation of extensions to ContextualLens that incorporate additional mechanisms for handling abstract concepts, such as integrating numerical reasoning modules or developing specialized embedding strategies for comparison tasks.

### Open Question 3
- Question: What evaluation metrics would better capture the trade-offs between precision and recall for multimodal attribution in real-world applications?
- Basis in paper: [explicit] The paper notes that for multimodal attribution, precision is more critical than recall, and their bounding box method provides higher precision but lower recall compared to the basic segmentation approach.
- Why unresolved: Current metrics like mAP and PR curves may not fully capture the practical requirements of multimodal attribution where users prioritize accurate identification of evidence locations over comprehensive coverage.
- What evidence would resolve it: Development of new evaluation metrics or frameworks that better reflect the practical needs of multimodal attribution, potentially incorporating user studies or task-specific performance measures that balance precision and recall according to application requirements.

## Limitations
- Implementation details for bounding box technique lack specificity, particularly how bounding box embeddings are computed from image patches
- Layer selection for contextual embeddings not clearly specified across different VLMs beyond examples for InternLM-VL
- Evaluation primarily focuses on 7B parameter VLMs, leaving uncertainty about performance scaling to larger models

## Confidence

**High Confidence**: The core hypothesis that contextual embeddings provide better hallucination detection than logit lens is well-supported by the experimental results across multiple benchmarks (HQH, TextVQA-X, VizWiz-G). The reported mAP improvements and PR curve comparisons are consistent and methodologically sound.

**Medium Confidence**: The claim about superior grounding precision is supported, but the lack of implementation details for the bounding box technique introduces uncertainty about reproducibility. The transition from zero-shot object segmentation to GVQA is promising but not fully validated across diverse real-world scenarios.

**Low Confidence**: The generalization of results to VLMs beyond the two tested models (Qwen2-VL-7B and InternLM-VL-7B) remains uncertain due to the absence of ablation studies on different model architectures.

## Next Checks

1. **Layer Selection Validation**: Conduct systematic experiments to identify optimal layer combinations for contextual embeddings across different VLMs, comparing performance against the fixed layer indices mentioned in the paper.

2. **Bounding Box Implementation**: Replicate the grounding component by implementing the cosine similarity-based bounding box selection and verify the reported precision improvements on the VizWiz-G benchmark.

3. **Cross-Model Generalization**: Test the ContextualLens approach on additional VLMs (e.g., LLaVA-Next, Chameleon) to validate whether the performance gains extend beyond the two models used in the original experiments.