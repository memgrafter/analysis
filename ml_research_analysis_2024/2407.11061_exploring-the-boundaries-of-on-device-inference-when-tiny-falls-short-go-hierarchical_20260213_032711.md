---
ver: rpa2
title: 'Exploring the Boundaries of On-Device Inference: When Tiny Falls Short, Go
  Hierarchical'
arxiv_id: '2407.11061'
source_url: https://arxiv.org/abs/2407.11061
tags:
- inference
- uni00000048
- latency
- energy
- accuracy
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the limitations of on-device machine learning
  inference on resource-constrained edge devices. The authors compare Hierarchical
  Inference (HI), which selectively offloads complex samples to a remote server, against
  on-device and remote inference strategies.
---

# Exploring the Boundaries of On-Device Inference: When Tiny Falls Short, Go Hierarchical

## Quick Facts
- arXiv ID: 2407.11061
- Source URL: https://arxiv.org/abs/2407.11061
- Authors: Adarsh Prasad Behera; Paulius Daubaris; Iñaki Bravo; José Gallego; Roberto Morabito; Joerg Widmer; Jaya Prakash Varma Champati
- Reference count: 40
- Primary result: Hierarchical Inference achieves up to 73% lower latency and 77% lower energy than on-device inference for a given accuracy requirement.

## Executive Summary
This paper systematically evaluates Hierarchical Inference (HI) as a strategy for edge machine learning systems where on-device models are too small to meet accuracy requirements. The authors compare HI against pure on-device and remote inference across five devices and three image classification datasets. Their measurements show that HI can achieve the same accuracy as remote-only inference while reducing latency by up to 73% and device energy consumption by up to 77%. They further improve these results with Early Exit with HI (EE-HI), achieving up to 59.7% latency reduction and 60.4% energy reduction compared to HI alone. The study concludes that smaller on-device models combined with HI often outperform larger models that meet accuracy requirements in isolation.

## Method Summary
The authors implemented Hierarchical Inference using binary logistic regression to decide whether to offload samples from an on-device model to a remote server based on confidence scores. They tested five devices (Arduino Nano, ESP32, Coral Micro, Raspberry Pi 4B, Jetson Orin) with three datasets (MNIST, CIFAR-10, ImageNet-1K) and various model architectures. On-device inference used TensorFlow Lite/TFLite Micro, while remote inference used TensorRT and ONNX Runtime on GPU servers. Early Exit with HI extended base models with early prediction branches at intermediate layers. The study measured accuracy, latency, and energy consumption across all configurations to determine optimal strategies for meeting accuracy requirements while minimizing latency and energy.

## Key Results
- HI achieves up to 73% lower latency and 77% lower device energy consumption than on-device inference for a given accuracy requirement.
- Using a smaller on-device model with HI can outperform a larger model that meets accuracy requirements alone.
- EE-HI reduces latency by up to 59.7% and energy consumption by up to 60.4% compared to HI.
- Smaller on-device models with HI are more effective than larger standalone models that meet accuracy requirements.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Hierarchical Inference (HI) reduces overall latency and energy by offloading only complex samples to a remote server while handling simple samples on-device.
- Mechanism: The HI system uses a binary logistic regression (LR) classifier to evaluate the confidence of on-device model predictions. If confidence is low, the sample is offloaded; if high, the on-device prediction is accepted. This avoids full offloading of all samples, saving energy and latency.
- Core assumption: The on-device model's confidence scores are reliable indicators of prediction correctness.
- Evidence anchors:
  - [abstract]: "The Hierarchical Inference (HI) system has emerged as a promising solution that augments the capabilities of the local ML by offloading selected samples to an edge server or cloud for remote ML inference."
  - [section]: "HI uses a HI decision algorithm that acts on the on-device (local ML) model output for each input sample and decides if the sample is simple or complex."
  - [corpus]: Weak; the cited corpus neighbors do not directly address hierarchical offloading mechanisms. They focus on TinyML deployment and energy optimization but not HI-specific confidence gating.
- Break condition: If the on-device model's confidence is poorly calibrated, the binary LR will offload too many or too few samples, increasing either latency or error rate.

### Mechanism 2
- Claim: Using a smaller on-device model with HI can outperform a larger model that meets accuracy requirements alone.
- Mechanism: A smaller model reduces per-sample on-device inference time and energy. Even if it doesn't meet accuracy QoS by itself, HI compensates by offloading uncertain samples to a more accurate remote model, achieving the required accuracy with lower total cost.
- Core assumption: The energy and latency savings from the smaller model outweigh the additional offloading overhead.
- Evidence anchors:
  - [abstract]: "We argue that it is beneficial to choose an HI system that uses a smaller on-device model, even if the model does not meet accuracy requirements alone, rather than an HI system with a larger state-of-the-art on-device model that does."
  - [section]: "Through measurements, we demonstrate that an HI system with a smaller on-device model can achieve the accuracy QoS requirement while delivering lower latency and energy consumption."
  - [corpus]: Weak; no corpus evidence directly supports the claim that smaller models in HI consistently beat larger standalone models in energy/latency terms.
- Break condition: If the binary LR offloading decision is inaccurate, the smaller model may fail to meet QoS because too many correct predictions are offloaded.

### Mechanism 3
- Claim: Early Exit with HI (EE-HI) further reduces latency and energy by enabling early predictions on the device before the full model runs.
- Mechanism: The base model is extended with early exit branches that allow classification at intermediate layers if confidence exceeds a threshold. This shortens the average inference path, reducing both device latency and energy, while still maintaining accuracy via HI offloading for samples that need deeper processing.
- Core assumption: Early exit branches can be trained to maintain accuracy while reducing computation.
- Evidence anchors:
  - [abstract]: "To address this, we propose using the early exit technique [24] to reduce the latency of inference on the device and consequently further improve the HI approach."
  - [section]: "We design a hybrid approach that combines Early Exit with HI (EE-HI) and demonstrate that, compared to HI, EE-HI reduces the latency up to 59.7% and device energy consumption up to 60.4%."
  - [corpus]: Weak; corpus neighbors focus on TinyML and model compression but not specifically on early-exit integration with HI.
- Break condition: If thresholds are set too low, many samples will take the full inference path, negating benefits; if too high, accuracy will drop.

## Foundational Learning

- Concept: Confidence calibration in neural networks
  - Why needed here: The binary LR decision in HI relies on softmax confidence values; if these are miscalibrated, offloading decisions will be poor.
  - Quick check question: What is the difference between a model's softmax output and its true probability of correctness?

- Concept: Model quantization and its effect on latency/energy
  - Why needed here: The study uses INT8 quantized models to reduce latency and energy; understanding quantization trade-offs is critical for reproducing results.
  - Quick check question: How does 8-bit quantization typically affect inference latency compared to FP32 on an MCU without a dedicated accelerator?

- Concept: Early exit neural network design
  - Why needed here: EE-HI relies on strategically placed exit branches; understanding how to train and tune them is necessary to achieve QoS goals.
  - Quick check question: What determines the optimal placement of early exit branches in a CNN?

## Architecture Onboarding

- Component map: On-device inference module (TFLite/TFLite Micro) -> Binary LR decision engine (scikit-learn-style) -> WiFi/Bluetooth communication stack -> Remote inference server (TensorRT/ONNX Runtime) -> Early exit branch network layers (optional)

- Critical path: Sample → On-device inference → Confidence scoring → Binary LR decision → (if low confidence) offload to server → Return prediction

- Design tradeoffs:
  - Model size vs. accuracy: Larger models meet QoS alone but cost more per-sample; smaller models save per-sample cost but require HI.
  - Threshold selection in EE: Higher thresholds improve accuracy but reduce latency/energy savings.
  - Network choice: WiFi is faster and more energy-efficient than BLE for offloading, despite higher power draw.

- Failure signatures:
  - Consistently high offloading rate (>70%) suggests binary LR is too conservative or on-device model is too weak.
  - Latency > remote-only indicates per-sample cost dominates even with offloading.
  - Accuracy < QoS with low offloading rate suggests confidence scores are miscalibrated.

- First 3 experiments:
  1. Measure baseline latency/energy of a small model (e.g., ResNet-8) on the target MCU to establish per-sample cost.
  2. Train and evaluate the binary LR on softmax values to determine offload threshold and F1 score.
  3. Implement EE-HI with a single early exit branch and sweep thresholds to find the QoS-optimal configuration.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal early exit branch placement and number of branches for different edge devices and model sizes to maximize accuracy while minimizing latency and energy consumption?
- Basis in paper: [explicit] The paper mentions that "the placement and composition of the EE branches are determined through trial-and-error" and that "larger models can accommodate more branches without introducing excessive overhead" but does not provide specific guidelines.
- Why unresolved: The paper only provides examples for specific models (ResNet-8, ResNet-56, AlexNet) and does not generalize to other architectures or device constraints.
- What evidence would resolve it: Systematic experiments testing different branch placements and numbers across various device types and model architectures, measuring accuracy, latency, and energy consumption.

### Open Question 2
- Question: How does device mobility and network interference impact the offloading times and overall efficiency of Hierarchical Inference (HI) systems in real-world scenarios?
- Basis in paper: [explicit] The paper states "Future work will focus on studying the impact of device mobility and network interference on offloading times and the overall efficiency of HI systems."
- Why unresolved: The current study was conducted in a static setup with stable WiFi access, not accounting for dynamic network conditions.
- What evidence would resolve it: Measurements of HI performance under varying network conditions, device mobility patterns, and interference scenarios, comparing static vs. dynamic environments.

### Open Question 3
- Question: What alternative offloading decision algorithms could reduce false positives and false negatives in HI systems compared to the binary logistic regression approach?
- Basis in paper: [explicit] The paper notes that "binary LR achieved an F1 score of 0.86 for ResNet-8 on CIFAR-10 and 0.83 for ResNet-18 on ImageNet-1K" and suggests exploring "alternative offloading decision algorithms that reduce false positives and false negatives."
- Why unresolved: The paper only tested one decision algorithm and acknowledges its limitations in classification performance.
- What evidence would resolve it: Comparative studies of different decision algorithms (e.g., neural networks, ensemble methods) measuring F1 scores, false positive/negative rates, and overall HI system performance.

## Limitations

- The confidence-calibration assumption in HI is weakly supported by corpus evidence, relying primarily on internal measurements.
- Exact configuration of early exit branches in EE-HI is not fully specified, making exact reproduction difficult.
- Binary LR threshold selection appears empirically derived without detailed methodology for optimal selection.

## Confidence

- **High confidence**: The overall performance improvements (73% latency reduction, 77% energy reduction for HI; 59.7% latency, 60.4% energy for EE-HI) are directly measured and internally consistent.
- **Medium confidence**: The claim that smaller models with HI outperform larger standalone models is supported by measurements but lacks corpus validation.
- **Low confidence**: The effectiveness of confidence-based offloading depends heavily on model calibration quality, which is not deeply examined.

## Next Checks

1. **Confidence calibration test**: Run temperature scaling or isotonic regression on the on-device model's softmax outputs and measure how much HI accuracy changes.

2. **Early exit branch placement sensitivity**: Systematically vary the placement of early exit branches in ResNet-8 and ResNet-18 and measure the trade-off between accuracy and latency/energy savings.

3. **Threshold sweep validation**: Reproduce the threshold optimization process for EE-HI by sweeping thresholds from 0.5 to 0.99 in 0.05 increments and plotting accuracy vs. latency/energy curves.