---
ver: rpa2
title: 'SceneMotion: From Agent-Centric Embeddings to Scene-Wide Forecasts'
arxiv_id: '2408.01537'
source_url: https://arxiv.org/abs/2408.01537
tags:
- motion
- forecasting
- scene-wide
- joint
- agents
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces SceneMotion, an attention-based model for
  forecasting scene-wide motion modes of multiple traffic agents in self-driving applications.
  The method transforms local agent-centric embeddings into scene-wide forecasts using
  a novel latent context module, which learns a scene-wide latent space from multiple
  agent-centric embeddings, enabling joint forecasting and interaction modeling.
---

# SceneMotion: From Agent-Centric Embeddings to Scene-Wide Forecasts

## Quick Facts
- arXiv ID: 2408.01537
- Source URL: https://arxiv.org/abs/2408.01537
- Reference count: 40
- Primary result: SceneMotion achieves state-of-the-art performance in the Waymo Open Interaction Prediction Challenge with 36.5 mAP, outperforming recent joint motion forecasting methods.

## Executive Summary
This paper introduces SceneMotion, an attention-based model that transforms local agent-centric embeddings into scene-wide motion forecasts for multiple traffic agents in self-driving applications. The key innovation is a novel latent context module that learns a scene-wide latent space from multiple agent-centric embeddings, enabling joint forecasting and interaction modeling. SceneMotion demonstrates competitive performance in the Waymo Open Interaction Prediction Challenge, achieving state-of-the-art results on the main challenge metric (mAP) and strong performance in marginal motion forecasting after fine-tuning.

## Method Summary
SceneMotion uses agent-centric representations where each agent is the focal point, generating one training sample per agent per scene rather than one per scene. The model consists of a local encoder that projects polyline representations to 256-dimensional vectors, a reduction decoder that transforms variable-length agent-centric views into fixed-size embeddings via cross-attention with RED tokens, and a latent context module that learns global context through self-attention layers. The motion decoder outputs 6 joint motion modes as mixture of Gaussians with 2D position means and covariances. The model is trained in two stages: first for joint forecasting using winner-takes-all assignment, then for marginal forecasting by fine-tuning with additional focal agents.

## Key Results
- Achieves 36.5 mAP on the Waymo Open Interaction Prediction Challenge, outperforming recent joint motion forecasting methods
- Demonstrates 36.5 mAP (interactive split) and 32.8 mAP (regular split) in joint motion forecasting
- Shows strong marginal motion forecasting performance after fine-tuning with additional focal agents
- Reduces training time compared to previous methods through larger batch sizes and efficient processing
- Quantifies interaction between agents by clustering future waypoints, showing effective conflict resolution

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Local agent-centric embeddings improve data efficiency by generating one training sample per agent per scene rather than one per scene.
- Mechanism: Agent-centric views produce more training samples by centering each agent as the focal point, enabling the model to learn local representations that generalize across agents and scenes. The reduction decoder then transforms variable-length agent-centric views into fixed-size embeddings via cross-attention with RED tokens.
- Core assumption: Agent-centric views capture sufficient local context for motion forecasting without requiring full scene-centric processing.
- Evidence anchors:
  - [abstract] "We use agent-centric representations since they allow for more data-efficient learning than scene-centric representations"
  - [section III] "Scene-centric methods encode all agent and map data in a common reference frame, generating only one sample per scene versus one sample for each agent"
  - [corpus] Weak evidence - no corpus papers directly address agent-centric vs scene-centric efficiency trade-offs
- Break condition: If local views miss critical interaction information beyond their coverage radius, the model cannot accurately forecast joint motion modes.

### Mechanism 2
- Claim: The latent context module enables joint forecasting by learning a scene-wide latent space from multiple agent-centric embeddings.
- Mechanism: After concatenating agent-centric embeddings with global reference tokens and rearranging to form a scene-wide embedding, the latent context module processes this through self-attention layers to capture global context. This learned global context enables the model to forecast joint distributions rather than independent marginal forecasts.
- Core assumption: Self-attention over the scene-wide embedding can effectively capture interactions between agents without explicit pairwise modeling.
- Evidence anchors:
  - [abstract] "Our model transforms local agent-centric embeddings into scene-wide forecasts using a novel latent context module"
  - [section III] "Our latent context module learns a scene-wide latent space from multiple agent-centric embeddings, enabling joint forecasting and interaction modeling"
  - [corpus] Weak evidence - no corpus papers directly validate the effectiveness of this specific latent context approach
- Break condition: If the latent context module cannot capture long-range interactions or complex multi-agent dynamics, the joint forecasting performance will degrade.

### Mechanism 3
- Claim: The winner-takes-all assignment for joint modes ensures consistency in scene-wide forecasts.
- Mechanism: During training, the model selects the joint mode with the lowest average displacement error across all agents and only backpropagates the loss for this mode. This encourages the model to learn coherent scene-wide motion modes rather than independent agent predictions.
- Core assumption: The mode with the lowest displacement error represents the most physically plausible joint motion scenario.
- Evidence anchors:
  - [section III] "To learn joint modes of scene-wide forecasts, we use a winner-takes-all assignment to select the best of 6 scene-wide trajectory sets"
  - [corpus] Weak evidence - no corpus papers directly validate this specific training strategy for joint motion forecasting
- Break condition: If the displacement error does not correlate with actual physical plausibility, the model may learn suboptimal joint modes.

## Foundational Learning

- Concept: Attention mechanisms in transformer architectures
  - Why needed here: The entire model architecture relies on self-attention and cross-attention to process both local agent-centric views and the global scene-wide embedding
  - Quick check question: What is the difference between self-attention and cross-attention, and how are they used in the local encoder and reduction decoder?

- Concept: Motion forecasting as probabilistic trajectory prediction
  - Why needed here: The model outputs trajectories represented as mixture of Gaussians with mean and covariance parameters for 2D positions
  - Quick check question: How does the mixture of Gaussians representation capture multimodal uncertainty in future motion?

- Concept: Data augmentation and sampling strategies for agent-centric representations
  - Why needed here: Agent-centric views generate multiple training samples per scene by centering on different agents, requiring proper sampling strategies
  - Quick check question: How does the choice of which agents to use as focal agents affect the model's ability to learn general motion patterns?

## Architecture Onboarding

- Component map:
  - Input processing: Agent-centric polyline representations with past motion, map data, and traffic light states
  - Local encoder: MLP that projects polylines to 256-dimensional vectors with positional embeddings
  - Reduction decoder: 4 blocks of alternating self-attention, cross-attention, LayerNorm, and FeedForward to create fixed-size agent-centric embeddings
  - Scene-wide modules: Concatenation with global reference tokens, rearrangement, latent context module with 6 self-attention blocks, motion decoder with MLP head
  - Output: Mixture of Gaussians for 6 motion modes with sampling frequency of 10 Hz

- Critical path: Input → Local encoder → Reduction decoder → Scene-wide embedding → Latent context module → Motion decoder → Output

- Design tradeoffs:
  - Agent-centric vs scene-centric: Agent-centric provides data efficiency but requires transformation to scene-wide space
  - Local vs full agent-centric views: Local views reduce redundancy but may miss distant interactions
  - Fixed-size vs variable-size embeddings: Fixed-size enables batch processing but requires reduction mechanism

- Failure signatures:
  - Poor joint forecasting performance: May indicate latent context module cannot capture interactions
  - Slow training: May indicate inefficient batch sizes or need for larger models
  - Mode collapse: May indicate winner-takes-all assignment is too restrictive

- First 3 experiments:
  1. Verify data loading and preprocessing pipeline by checking that agent-centric views are correctly generated and sampled
  2. Test the reduction decoder independently by feeding in simple agent-centric views and verifying the fixed-size embeddings
  3. Validate the latent context module by checking that the scene-wide embedding captures global context through attention visualization

## Open Questions the Paper Calls Out

The paper does not explicitly call out open questions, but based on the discussion section, several important questions emerge:

### Open Question 1
- Question: How does the performance of SceneMotion scale with an increased number of agents beyond the 8 focal agents used in the default configuration?
- Basis in paper: [inferred] The paper mentions inference latency for 2, 8, and 64 focal agents but does not discuss performance scaling.
- Why unresolved: The paper does not provide performance metrics for configurations with more than 8 focal agents, which is crucial for understanding scalability in complex scenarios.
- What evidence would resolve it: Performance metrics (e.g., mAP, minADE, minFDE) for configurations with 16, 32, or 64 focal agents would clarify how well SceneMotion scales with increasing scene complexity.

### Open Question 2
- Question: What is the impact of the learned assignment of trajectories to scene-wide modes compared to alternative assignment strategies, such as learned or heuristic-based methods?
- Basis in paper: [explicit] The paper discusses comparing learned assignment to a random assignment scheme but does not explore other strategies.
- Why unresolved: While the paper shows that learned assignment outperforms random assignment, it does not investigate whether other assignment strategies could further improve performance.
- What evidence would resolve it: Performance comparisons of SceneMotion using different assignment strategies (e.g., learned assignment, heuristic-based, or other learned methods) would reveal the optimal approach for trajectory assignment.

### Open Question 3
- Question: How does the SceneMotion model handle edge cases, such as scenarios with a high density of agents or rare traffic situations (e.g., construction zones)?
- Basis in paper: [inferred] The paper does not discuss the model's robustness to edge cases or rare scenarios, which are critical for real-world deployment.
- Why unresolved: The paper focuses on general performance metrics but does not address the model's behavior in challenging or uncommon scenarios.
- What evidence would resolve it: Testing SceneMotion on datasets or simulations containing edge cases and rare traffic situations would demonstrate its robustness and reliability in diverse environments.

## Limitations
- The reduction mechanism [33] that transforms variable-sized agent-centric views into fixed-size embeddings is not fully specified, making exact reproduction challenging
- The effectiveness of the latent context module in capturing complex multi-agent interactions is assumed but not directly validated through ablation studies
- The winner-takes-all assignment strategy may lead to mode collapse or suboptimal joint motion learning without proper regularization

## Confidence
- High confidence in the agent-centric to scene-wide transformation approach, as this is well-grounded in existing transformer architectures and the performance gains are demonstrated
- Medium confidence in the effectiveness of the latent context module for capturing multi-agent interactions, as the clustering analysis provides indirect evidence but lacks direct validation
- Low confidence in the winner-takes-all training strategy without comparison to alternative joint mode learning approaches

## Next Checks
1. **Ablation on reduction mechanism**: Test the model with and without the reduction decoder to quantify the impact of the fixed-size embedding transformation on joint forecasting performance
2. **Latent context module analysis**: Visualize attention weights in the latent context module to verify it captures relevant agent interactions, and compare performance when using different numbers of self-attention blocks
3. **Training strategy comparison**: Evaluate alternative joint mode learning strategies (e.g., multi-task loss weighting) against the winner-takes-all approach to assess robustness and prevent mode collapse