---
ver: rpa2
title: 'Physics of Language Models: Part 2.2, How to Learn From Mistakes on Grade-School
  Math Problems'
arxiv_id: '2408.16293'
source_url: https://arxiv.org/abs/2408.16293
tags:
- retry
- retryrate0
- data
- uni00000048
- each
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper investigates how to train language models to self-correct
  mistakes during generation rather than after, which is more efficient. Using synthetic
  math datasets, it explores pretraining with "retry" data (errors followed by corrections)
  and finds that this improves reasoning accuracy compared to pretraining on error-free
  data, especially for harder problems.
---

# Physics of Language Models: Part 2.2, How to Learn From Mistakes on Grade-School Math Problems

## Quick Facts
- arXiv ID: 2408.16293
- Source URL: https://arxiv.org/abs/2408.16293
- Reference count: 33
- Primary result: Pretraining language models with retry data (errors followed by corrections) improves reasoning accuracy on grade-school math problems compared to pretraining on error-free data.

## Executive Summary
This paper investigates how to train language models to self-correct mistakes during generation rather than after, which is more efficient. Using synthetic math datasets, it explores pretraining with "retry" data (errors followed by corrections) and finds that this improves reasoning accuracy compared to pretraining on error-free data, especially for harder problems. It shows that models can learn to correct mistakes without needing to mask errors during training, and that the model rarely generates errors even when trained with high error rates. In contrast, finetuning a model pretrained on error-free data with retry data using parameter-efficient methods like LoRA does not significantly improve accuracy, suggesting that error correction is a distinct skill requiring early exposure during pretraining. Simple methods to generate fake mistakes are also effective.

## Method Summary
The authors create synthetic math datasets (iGSM) with controllable error rates and retry data (errors followed by corrections). They pretrain language models (GPT2-12-12) on these datasets using standard autoregressive loss, varying the retry rate from 0.01 to 0.5. They evaluate models on in-distribution and out-of-distribution test sets, measuring reasoning accuracy. They also explore label masking (hiding errors during training) and parameter-efficient fine-tuning (LoRA) on retry data. Additionally, they propose methods to generate fake mistakes by inserting future steps as errors, which can teach the model not to skip reasoning steps.

## Key Results
- Pretraining with retry data improves reasoning accuracy compared to pretraining on error-free data, especially for harder problems.
- Higher retry rates (up to 0.5) improve accuracy, but masking errors during training provides little benefit.
- LoRA finetuning on retry data underperforms compared to pretraining with retry data, suggesting error correction is a distinct skill.
- Simple methods to generate fake mistakes (inserting future steps as errors) are effective at improving reasoning accuracy.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Language models can learn to correct reasoning errors immediately during generation, rather than only after full generation, by being exposed to retry data during pretraining.
- Mechanism: During pretraining, the model encounters sequences where a reasoning step is followed by an error marker and correction. The autoregressive loss on these tokens teaches the model the causal chain from error → correction, conditioning future generation on internal error-detection signals.
- Core assumption: The model's internal state after generating a step contains sufficient information to detect that the step is invalid, and the model can use this signal to backtrack and generate a correct step.
- Evidence anchors:
  - [abstract] "this type of pretrain data can help language models achieve higher reasoning accuracy directly (i.e., through simple auto-regression, without multi-round prompting)"
  - [section] "We discover that even when the model is pretrained entirely on retry data with high error rate (e.g., for the iGSM data, p = 20% or even p = 50%), it does not tend to produce erroneous steps during generation."
  - [corpus] Weak: Only 1 related paper directly addresses learning from mistakes; most focus on robustness or explanation rather than immediate correction during generation.
- Break condition: If the model cannot reliably detect its own errors internally, retry data will not improve accuracy; if the distribution of errors is too different from real mistakes, the correction skill will not transfer.

### Mechanism 2
- Claim: Error correction is a distinct reasoning skill that cannot be easily acquired through parameter-efficient fine-tuning on top of models pretrained only on error-free data.
- Mechanism: Error correction requires revising internal computations after a mistake is detected. This revision is a non-trivial change to the model's reasoning pathway, which LoRA (low-rank updates) cannot capture due to its limited capacity to modify existing internal representations.
- Core assumption: The revision process after detecting an error involves significant changes to the model's hidden state dynamics, beyond what a low-rank update can express.
- Evidence anchors:
  - [section] "LoRA finetuning falls short of pretraining directly with the retry data. When the LoRA rank is small, it even underperforms compared to pretraining with error-free data"
  - [abstract] "error correction is a distinct skill requiring early exposure during pretraining"
  - [corpus] Weak: Few papers directly compare LoRA vs full fine-tuning for error correction; most focus on verification or explanation generation.
- Break condition: If the model's error detection and correction mechanisms are simple enough to be captured by a low-rank update, LoRA could suffice; if full fine-tuning is computationally infeasible, this limits practical adoption.

### Mechanism 3
- Claim: Introducing "fake" mistakes (e.g., inserting a future step as an error and then correcting it) during pretraining can teach the model not to skip reasoning steps, improving accuracy without needing real error data.
- Mechanism: By training on sequences where a correct step is followed by an inserted future step (marked as an error) and then the correct step again, the model learns to avoid jumping ahead in reasoning. The autoregressive loss on these sequences reinforces the correct causal order of reasoning steps.
- Core assumption: The model's tendency to skip steps is a major source of errors, and training it to avoid skipping by seeing fake mistakes is sufficient to improve reasoning accuracy.
- Evidence anchors:
  - [section] "The most effective method we found is to introduce a random future step B as a 'fake error' at each step A in the solution, followed by A as its 'correction.'"
  - [abstract] "Simple methods to generate fake mistakes are also effective."
  - [corpus] Weak: No direct corpus evidence for fake mistake augmentation; related work focuses on robustness or explanation rather than step-skipping prevention.
- Break condition: If the model's errors are not primarily due to step-skipping, fake mistakes will not help; if the inserted fake errors are too dissimilar from real errors, the model may not learn the intended behavior.

## Foundational Learning

- Concept: Synthetic controllable datasets (e.g., iGSM)
  - Why needed here: Provides a setting where errors can be generated and corrected with 100% reliability, allowing controlled experiments on the effect of retry data.
  - Quick check question: Can you generate a math problem and its solution in the iGSM format, then insert a controlled error and correction?

- Concept: Autoregressive language model training with causal masking
  - Why needed here: The model learns to predict the next token conditioned on all previous tokens, which is the mechanism by which it can learn from retry data (error → correction sequences).
  - Quick check question: In a causal language model, can the model "peek ahead" to tokens it hasn't generated yet during training?

- Concept: Error detection via probing of hidden states
  - Why needed here: Determines whether the model's internal state contains signals indicating a generated step is invalid, which is necessary for it to correct mistakes during generation.
  - Quick check question: After generating a reasoning step, can you train a linear classifier on the model's hidden states to predict whether the next parameter can be computed?

## Architecture Onboarding

- Component map:
  - Pretraining data generator -> Language model -> Evaluation pipeline -> Probing module
  - (Pretraining data generator creates synthetic math problems with controlled error rates and retry data)
  - (Language model is trained autoregressively on this data)
  - (Evaluation pipeline generates solutions for test problems and checks correctness)
  - (Probing module trains a linear head on hidden states to detect errors)

- Critical path:
  1. Generate synthetic math dataset (iGSM) with controllable error rates.
  2. Train language model on this dataset using standard autoregressive loss.
  3. Evaluate model on held-out test problems, measuring reasoning accuracy.
  4. (Optional) Apply probing to detect errors and guide regeneration ("retry upon regret").

- Design tradeoffs:
  - Error rate in pretraining data: Higher rates may teach better correction but risk the model learning to make mistakes; masking errors may prevent learning but could reduce effectiveness.
  - Model size vs. accuracy: Larger models may capture more complex reasoning patterns but increase computational cost.
  - Full fine-tuning vs. LoRA: Full fine-tuning can learn error correction from retry data but is expensive; LoRA is cheap but may not capture the skill.

- Failure signatures:
  - Model generates correct final answers but with incorrect intermediate steps (logic errors).
  - Model fails to correct errors even when trained on retry data (error detection not learned).
  - Model's accuracy does not improve with retry data (skill not acquired or distribution mismatch).

- First 3 experiments:
  1. Train a baseline GPT-2 on error-free iGSM data, evaluate accuracy on test set.
  2. Train a GPT-2 on iGSM data with retry rate 0.2 (errors inserted), evaluate accuracy; compare to baseline.
  3. Apply "retry upon regret" to the baseline model using error detection probing, measure accuracy improvement.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the inclusion of retry data during pretraining affect the model's ability to handle different types of reasoning errors beyond simple step-skipping?
- Basis in paper: [explicit] The paper focuses on synthetic math problems with step-skipping errors and shows that retry data improves reasoning accuracy, but does not explore other types of reasoning errors.
- Why unresolved: The study uses a controlled synthetic dataset that primarily simulates step-skipping errors. Real-world reasoning errors may be more diverse and complex.
- What evidence would resolve it: Experiments on diverse datasets with various types of reasoning errors (e.g., arithmetic errors, logical inconsistencies) to compare models pretrained with and without retry data.

### Open Question 2
- Question: What is the optimal balance between error-free and retry data during pretraining to maximize reasoning accuracy without overfitting to errors?
- Basis in paper: [explicit] The paper shows that higher retry rates in pretraining data improve accuracy, but does not determine an optimal balance or investigate overfitting risks.
- Why unresolved: The study uses a fixed number of tokens for pretraining, varying only the retry rate. The impact of different ratios of error-free to retry data is not explored.
- What evidence would resolve it: Systematic experiments varying the ratio of error-free to retry data while keeping the total number of tokens constant, measuring accuracy and potential overfitting.

### Open Question 3
- Question: How do different methods of generating fake mistakes (e.g., retry weak vs. retry miss) compare in their effectiveness for pretraining models on real-world data?
- Basis in paper: [explicit] The paper introduces two methods for generating fake mistakes and shows that retry weak is effective, but does not compare these methods on real-world data or explore other potential methods.
- Why unresolved: The study uses synthetic data where the structure is known, making it easier to generate realistic fake mistakes. Real-world data may require more sophisticated methods.
- What evidence would resolve it: Experiments applying different fake mistake generation methods to real-world math datasets and comparing their impact on reasoning accuracy.

## Limitations

- The paper does not explore full fine-tuning on retry data as an alternative to LoRA, leaving open whether LoRA's failure is due to limited capacity or the fine-tuning paradigm itself.
- The mechanism by which models detect errors internally is not fully characterized; probing experiments suggest the model can detect errors, but the nature of the detection signal is unclear.
- The synthetic nature of the datasets means results may not transfer to real-world error distributions or more complex reasoning tasks.
- The paper does not address how the model balances error correction with computational efficiency during generation.

## Confidence

- High confidence in the experimental setup and the core result that retry data improves reasoning accuracy during pretraining.
- Medium confidence in the claim that error correction is a distinct skill requiring early exposure, due to limited exploration of alternative fine-tuning methods.
- Medium confidence in the effectiveness of fake mistakes, as the paper does not provide detailed analysis of their impact or ablation studies.

## Next Checks

1. **Probe the error detection mechanism**: Use the probing classifier to analyze which hidden states or token positions are most predictive of errors, and whether the model's error detection generalizes to out-of-distribution mistakes.
2. **Compare full fine-tuning vs. LoRA**: Fine-tune a model pretrained on error-free data with retry data using full parameter updates, and compare accuracy to LoRA and direct pretraining on retry data.
3. **Test fake mistakes on a new dataset**: Generate a small set of real math problems with step-skipping errors, and evaluate whether pretraining with fake mistakes reduces such errors compared to a baseline.