---
ver: rpa2
title: Knowledge Acquisition Disentanglement for Knowledge-based Visual Question Answering
  with Large Language Models
arxiv_id: '2407.15346'
source_url: https://arxiv.org/abs/2407.15346
tags:
- knowledge
- question
- image
- llms
- caption
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper tackles the challenge of Knowledge-based Visual Question
  Answering (KVQA), which requires both image and world knowledge to answer questions.
  The authors propose DKA (Disentangled Knowledge Acquisition from LLM feedback),
  a training-free framework that addresses the limitations of existing methods.
---

# Knowledge Acquisition Disentanglement for Knowledge-based Visual Question Answering with Large Language Models

## Quick Facts
- arXiv ID: 2407.15346
- Source URL: https://arxiv.org/abs/2407.15346
- Reference count: 15
- Primary result: DKA achieves 62.1% and 59.9% accuracy on OK-VQA and AOK-VQA benchmarks, respectively

## Executive Summary
This paper addresses the challenge of Knowledge-based Visual Question Answering (KVQA), where answering questions requires both image understanding and external world knowledge. The authors propose DKA, a training-free framework that disentangles knowledge acquisition by decomposing complex questions into image-based and knowledge-based sub-questions. This approach allows separate knowledge acquisition models to focus on their respective domains without interference, while LLM feedback explicitly specifies knowledge needs. Experiments on benchmark datasets demonstrate significant performance improvements over state-of-the-art models.

## Method Summary
DKA is a training-free framework that tackles KVQA by disentangling the knowledge acquisition process. It uses LLMs to decompose complex questions into two sub-questions: one focused on image content and another on external knowledge. The framework then generates question-aware captions using PromptCAP based on the image-based sub-question, retrieves relevant knowledge using the knowledge-based sub-question, and employs in-context learning with selected examples to help LLMs generate answers. The method leverages LLM feedback to specify knowledge needs and uses an ensemble of answers from multiple LLM prompts to improve final performance.

## Key Results
- DKA achieves 62.1% accuracy on OK-VQA benchmark, outperforming state-of-the-art models
- DKA achieves 59.9% accuracy on AOK-VQA benchmark, demonstrating consistent performance
- The framework successfully disentangles knowledge acquisition, reducing interference between image and knowledge retrieval

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Disentangling knowledge acquisition reduces interference between image and knowledge retrieval.
- Mechanism: By decomposing a complex question into two sub-questions—one focused on the image and one focused on external knowledge—the model avoids confusion caused by mixed content. This separation allows each retrieval model to focus on its specific domain without being distracted by irrelevant elements from the other domain.
- Core assumption: The original complex question contains elements that require knowledge from different sources, leading to confusion in retrieval models.
- Evidence anchors:
  - [abstract]: "acquiring different kinds of knowledge in a coupled manner may confuse models and hinder them from retrieving precise knowledge."
  - [section]: "retrieving different kinds of knowledge with the original complex question in a coupled manner can introduce much noise and confuse retrieval models"

### Mechanism 2
- Claim: LLM feedback explicitly specifies knowledge needs, improving knowledge alignment.
- Mechanism: The LLM is prompted to decompose the question and specify what knowledge it needs. This feedback helps align the retrieved knowledge with the LLM's actual needs, reducing mismatches between what is retrieved and what is needed.
- Core assumption: The "forward-only" answering process fails to explicitly capture the knowledge needs of LLMs.
- Evidence anchors:
  - [abstract]: "the 'forward-only' answering process fails to explicitly capture the knowledge needs of LLMs, which can further hurt answering quality."
  - [section]: "DKA requires LLMs to explicitly specify what knowledge they need to answer the question"

### Mechanism 3
- Claim: Question-aware caption generation improves visual information capture.
- Mechanism: The image-based sub-question guides the caption model to generate captions that capture visual details relevant to the question, avoiding interference from irrelevant elements in the original question.
- Core assumption: Generic image captions often miss details essential for LLMs to answer questions.
- Evidence anchors:
  - [section]: "we use the image-based sub-question qi to guide the question-aware caption model PromptCAP (Hu et al., 2022) to generate informative captions, which can capture visual details about the input question and avoid interference of irrelevant elements"

## Foundational Learning

- Concept: Question decomposition and its importance in knowledge-based VQA.
  - Why needed here: The paper's core innovation relies on decomposing complex questions into sub-questions for disentangled knowledge acquisition. Understanding this concept is crucial for implementing and improving the DKA framework.
  - Quick check question: What are the two types of sub-questions created in the DKA framework, and why are they important?

- Concept: In-context learning and its application in KVQA.
  - Why needed here: The paper uses in-context learning to select informative examples for the LLM. Understanding this concept is essential for implementing the in-context learning component of DKA.
  - Quick check question: How does the paper select in-context examples, and why is this selection important for KVQA performance?

- Concept: Knowledge retrieval and its challenges in multimodal contexts.
  - Why needed here: The paper addresses the challenge of retrieving precise knowledge from different sources (image and external knowledge base) for KVQA. Understanding knowledge retrieval concepts is crucial for improving the retrieval components of DKA.
  - Quick check question: What are the two main challenges in knowledge retrieval mentioned in the paper, and how does DKA address them?

## Architecture Onboarding

- Component map: Question Decomposition -> Caption Generation -> Knowledge Retrieval -> Answer Generation -> Answer Ensemble
- Critical path: Question Decomposition → Caption Generation → Knowledge Retrieval → Answer Generation → Answer Ensemble
- Design tradeoffs:
  - Using sub-questions vs. original complex question: Sub-questions reduce interference but require accurate decomposition
  - Number of retrieved knowledge items: More items provide more information but may introduce noise
  - Number of in-context examples: More examples improve performance but increase computation
- Failure signatures:
  - Poor question decomposition leading to irrelevant captions and knowledge
  - Caption model generating generic captions that miss question-specific details
  - Knowledge retriever failing to find relevant information despite accurate sub-questions
  - LLM not utilizing the provided information effectively in answer generation
- First 3 experiments:
  1. Test the accuracy of the question decomposition component by evaluating the relevance of generated sub-questions to their respective domains (image vs. knowledge)
  2. Evaluate the effectiveness of question-aware caption generation by comparing captions generated with and without sub-question guidance
  3. Assess the impact of knowledge disentanglement by comparing retrieval accuracy using original questions vs. decomposed sub-questions

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of DKA vary with different LLM architectures for question decomposition and answering?
- Basis in paper: [explicit] The paper mentions using LLaMA2-13B for both question decomposition and answering, but notes that the performance of using other open-source LLMs was not explored due to computational constraints.
- Why unresolved: The paper does not provide experimental results comparing DKA's performance with different LLM architectures. The authors only mention LLaMA2-13B without testing other models like GPT-3 or different LLaMA variants.
- What evidence would resolve it: Conducting experiments using different LLM architectures for question decomposition and answering, then comparing the performance metrics (accuracy, F1-score, etc.) across these models on benchmark datasets like OK-VQA and AOK-VQA.

### Open Question 2
- Question: What is the impact of training caption models and knowledge retrievers with LLM feedback on overall performance?
- Basis in paper: [inferred] The authors acknowledge that training component models (caption models, knowledge retrievers, and answering models) with LLM feedback may result in better performance, but they chose a training-free approach for efficiency.
- Why unresolved: The paper only presents results for the training-free approach and does not explore the potential benefits of training these components with LLM feedback. This leaves open the question of whether the performance gains observed in DKA could be further improved with additional training.
- What evidence would resolve it: Implementing and evaluating a version of DKA where the caption model and knowledge retriever are fine-tuned using LLM feedback, then comparing its performance against the training-free version on benchmark datasets.

### Open Question 3
- Question: How does DKA handle multimodal hallucinations in caption models, and what strategies can be employed to mitigate this issue?
- Basis in paper: [explicit] The authors identify multimodal hallucinations in caption models as a source of error in their error analysis section, noting that incorrect visual information can lead to wrong answers.
- Why unresolved: While the paper acknowledges this limitation, it does not propose or test specific strategies to address multimodal hallucinations. The authors mention this as a future research direction but do not provide concrete solutions or experimental results.
- What evidence would resolve it: Developing and testing methods to detect and correct multimodal hallucinations in caption models, such as incorporating consistency checks between generated captions and visual features, or using multiple caption models and aggregating their outputs. Then, evaluating the performance improvement of these methods on benchmark datasets.

## Limitations

- Question decomposition accuracy is critical to performance but not thoroughly validated
- Knowledge retrieval precision depends on external knowledge base coverage
- Framework evaluation limited to specific benchmark datasets without broader generalization testing

## Confidence

- High Confidence: The core hypothesis that disentangling knowledge acquisition improves KVQA performance is well-supported by experimental results (62.1% and 59.9% accuracy on benchmark datasets)
- Medium Confidence: The mechanism of using LLM feedback to specify knowledge needs is theoretically sound, but the practical effectiveness depends on the quality of LLM decomposition, which is not thoroughly validated
- Low Confidence: The scalability of the approach to more complex questions or domains beyond the evaluated benchmarks is uncertain

## Next Checks

1. **Decomposition Quality Analysis**: Conduct a systematic evaluation of the question decomposition component by having human annotators assess the accuracy and relevance of generated sub-questions compared to the original questions

2. **Ablation Study on Knowledge Retrieval**: Perform an ablation study to quantify the contribution of each knowledge source (image captions vs. external knowledge) and analyze cases where knowledge retrieval fails despite accurate question decomposition

3. **Cross-Dataset Generalization Test**: Evaluate the DKA framework on additional KVQA datasets with different characteristics to assess its generalizability and identify potential domain-specific limitations