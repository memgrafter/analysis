---
ver: rpa2
title: 'SparseGrad: A Selective Method for Efficient Fine-tuning of MLP Layers'
arxiv_id: '2410.07383'
source_url: https://arxiv.org/abs/2410.07383
tags:
- sparsegrad
- lora
- fine-tuning
- table
- parameters
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of efficient fine-tuning of
  large transformer models, particularly focusing on the often-neglected MLP layers
  which contain about half of the model parameters. The authors propose SparseGrad,
  a selective parameter-efficient fine-tuning method that identifies a space where
  gradient matrices are sparse and updates only their significant parts.
---

# SparseGrad: A Selective Method for Efficient Fine-tuning of MLP Layers

## Quick Facts
- arXiv ID: 2410.07383
- Source URL: https://arxiv.org/abs/2410.07383
- Reference count: 14
- Primary result: SparseGrad achieves superior performance to LoRA while updating only ~1% of parameters

## Executive Summary
SparseGrad introduces a novel parameter-efficient fine-tuning method that selectively updates MLP layer parameters by exploiting gradient sparsity. The approach uses Higher Order SVD to identify and update only the significant parts of gradient matrices during training. This selective update strategy achieves state-of-the-art performance on GLUE benchmarks while using approximately 1% of the parameters required by standard fine-tuning methods.

## Method Summary
SparseGrad operates by applying Higher Order SVD decomposition to gradient matrices, transforming them into a sparse space where only significant components are updated. The method identifies orthogonal transition matrices that map original weights into this sparse gradient space, allowing selective parameter updates. During training, SparseGrad tracks gradient sparsity patterns and updates only the most significant parameters, achieving computational efficiency while maintaining or improving performance compared to standard fine-tuning and existing PEF-T methods.

## Key Results
- On GLUE benchmark: SparseGrad achieved 82.6 average score for BERT base (vs LoRA's 81.6) and 83.6 for RoBERTa base (vs LoRA's 83.1)
- For LLaMa-2 fine-tuning on OpenAssistant dataset: GPT-4 score of 5.132 (vs LoRA's 5.025)
- Uses approximately 1% of parameters compared to standard fine-tuning methods

## Why This Works (Mechanism)
SparseGrad leverages the observation that gradient matrices often exhibit sparsity patterns that can be exploited for efficient parameter updates. By using Higher Order SVD to identify the most significant gradient components, the method focuses computational resources on parameters that contribute most to task performance. The orthogonal transition matrices ensure that the sparse gradient space maintains mathematical consistency while reducing the number of parameters that need updating. This selective approach is particularly effective for MLP layers, which contain about half of transformer model parameters but are often overlooked in parameter-efficient fine-tuning research.

## Foundational Learning

**Higher Order SVD (HOSVD)**: A generalization of matrix SVD to higher-dimensional tensors, used to decompose gradient matrices into orthogonal components. Needed to identify sparse gradient patterns across multiple dimensions. Quick check: Verify decomposition captures most variance in first few components.

**Gradient Sparsity**: The property that gradient matrices contain mostly small or zero values, with only a few significant components. Needed to identify which parameters to update selectively. Quick check: Compute percentage of gradient mass in top-k components.

**Orthogonal Transition Matrices**: Matrices that preserve inner products when transforming between spaces, ensuring mathematical consistency. Needed to map between original and sparse gradient spaces without information loss. Quick check: Verify orthogonality by checking if matrix multiplication equals identity.

**MLP Layer Structure**: Multi-Layer Perceptron layers in transformers that expand and project hidden states. Needed as primary target for selective updates since they contain ~50% of model parameters. Quick check: Count parameters in MLP vs attention layers.

**Parameter-Efficient Fine-tuning (PEF-T)**: Methods that update only a small subset of model parameters during adaptation. Needed to reduce computational cost while maintaining performance. Quick check: Compare parameter count between full and PEF-T methods.

## Architecture Onboarding

Component map: Input -> Gradient Computation -> HOSVD Decomposition -> Sparse Space Transformation -> Selective Parameter Update -> Output

Critical path: Forward pass through MLP layers → Gradient computation → HOSVD decomposition → Parameter update → Forward pass through remaining layers

Design tradeoffs: Computational overhead of HOSVD decomposition vs. parameter savings; sparsity threshold selection vs. performance; selective updating vs. full gradient propagation

Failure signatures: Degraded performance when sparsity threshold too high; increased training time due to decomposition overhead; instability when orthogonal matrices not properly maintained

First experiments:
1. Compare SparseGrad performance vs. LoRA on GLUE tasks with varying sparsity thresholds
2. Measure computational overhead of HOSVD decomposition during training
3. Ablation study on MLP vs. attention layer selective updating

## Open Questions the Paper Calls Out
None

## Limitations

The paper lacks comprehensive comparison with recent state-of-the-art PEF-T methods beyond LoRA and MeProp, particularly missing prefix tuning and adapter-based approaches. The computational overhead of HOSVD decomposition is not thoroughly analyzed, which could impact practical adoption. The method's generalization across different model architectures and tasks remains uncertain, as evaluation focuses primarily on GLUE tasks and one large-scale instruction tuning dataset.

## Confidence

High Confidence: Mathematical foundation of HOSVD for gradient sparsity is sound, and empirical results on GLUE tasks are reproducible and well-documented.

Medium Confidence: Superiority claims over LoRA and MeProp are supported by experiments, but comparison scope is limited and may not generalize to all PEF-T scenarios.

Medium Confidence: Theoretical justification for MLP layer focus is reasonable but could benefit from more extensive ablation studies.

## Next Checks

1. Runtime Overhead Analysis: Measure computational overhead of HOSVD decomposition during training, including GPU memory usage and wall-clock time compared to standard fine-tuning and LoRA.

2. Cross-Architecture Generalization: Test SparseGrad on additional model families beyond BERT, RoBERTa, and LLaMa-2, including Vision Transformers and other encoder-decoder architectures, to validate robustness.

3. Long Sequence Stability: Evaluate weight divergence and performance stability when fine-tuning with long input sequences (e.g., >512 tokens) to assess practical limitations.