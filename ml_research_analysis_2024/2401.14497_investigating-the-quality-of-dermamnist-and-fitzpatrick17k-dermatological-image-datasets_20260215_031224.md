---
ver: rpa2
title: Investigating the Quality of DermaMNIST and Fitzpatrick17k Dermatological Image
  Datasets
arxiv_id: '2401.14497'
source_url: https://arxiv.org/abs/2401.14497
tags:
- images
- image
- data
- pairs
- skin
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper systematically analyzed two large skin image datasets,
  DermaMNIST and Fitzpatrick17k, and uncovered significant data quality issues. For
  DermaMNIST, they found severe data leakage across train-valid-test partitions due
  to duplicate images of the same lesions.
---

# Investigating the Quality of DermaMNIST and Fitzpatrick17k Dermatological Image Datasets

## Quick Facts
- arXiv ID: 2401.14497
- Source URL: https://arxiv.org/abs/2401.14497
- Reference count: 40
- Authors found severe data quality issues in DermaMNIST and Fitzpatrick17k datasets, including data leakage, duplicate images, and mislabeled diagnoses

## Executive Summary
This paper systematically analyzes two popular dermatological image datasets, DermaMNIST and Fitzpatrick17k, uncovering significant data quality issues that compromise model reliability and evaluation. For DermaMNIST, the authors identified severe data leakage across train-valid-test partitions due to duplicate images of the same lesions, corrected this by moving duplicate lesion images to the training set (creating DermaMNIST-C), and proposed an extended version (DermaMNIST-E) using external ISIC 2018 data for testing. For Fitzpatrick17k, they discovered numerous duplicate image clusters, mislabeled diagnosis and Fitzpatrick skin tone (FST) labels, erroneous non-dermatological images, and the absence of a proper held-out test set. They cleaned the dataset to create Fitzpatrick17k-C with standardized train-valid-test splits and provided updated benchmarks. Their findings highlight the critical need for rigorous data quality assessment in medical image datasets to ensure reliable model training and evaluation.

## Method Summary
The authors employed a systematic approach to analyze and clean both datasets. For duplicate detection, they used fastdup and cleanvision with cosine similarity, applying thresholds of 0.90 and 0.95 for identifying highly similar image pairs. They manually verified duplicate pairs using a GUI interface and clustered images using a union-find algorithm. For DermaMNIST, they analyzed HAM10000 metadata to identify duplicate lesions across partitions, then moved all images of duplicate lesions to the training set. For Fitzpatrick17k, they identified erroneous non-dermatological images using outlier detection based on 5 nearest neighbors with lowest similarity scores. They created cleaned versions of both datasets with standardized train-valid-test splits (80-10-10) and evaluated ResNet-18 and ResNet-50 models using cross-entropy loss and Adam optimizer. All code and datasets are publicly available on GitHub.

## Key Results
- DermaMNIST had severe data leakage with 13.47% of lesions appearing in multiple partitions, corrected by moving duplicate lesions to training set
- Fitzpatrick17k contained 2,498 duplicate image pairs with different diagnoses and 4,030 pairs with different FST labels
- Cleaning process reduced DermaMNIST from 10,015 to 9,129 images and Fitzpatrick17k from 16,577 to 15,127 images

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Data leakage in DermaMNIST inflates model performance by allowing images of the same lesion to appear in both training and testing partitions.
- Mechanism: The dataset contains multiple images of the same lesion (due to different angles or magnifications), but the original partitioning did not account for this, causing images of the same lesion to leak across train-valid-test splits.
- Core assumption: Images of the same lesion represent the same underlying medical case, so their presence in both training and evaluation partitions leads to over-optimistic performance estimates.
- Evidence anchors:
  - [abstract] "severe data leakage across train-valid-test partitions due to duplicate images of the same lesions"
  - [section] "we found the following overlaps across partitions: train-test: 886 images (641 lesions), train-valid: 440 images (332 lesions); valid-test: 128 images (113 lesions); train-valid-test: 51 images (40 lesions)"
  - [corpus] Weak evidence - no corpus papers directly discuss this specific data leakage issue.
- Break condition: If lesions were truly independent despite being the same case, or if duplicates were removed before partitioning.

### Mechanism 2
- Claim: Mislabeled diagnosis and Fitzpatrick skin tone (FST) labels in Fitzpatrick17k introduce bias and reduce model reliability.
- Mechanism: The dataset was curated from online atlases without histopathology confirmation, leading to incorrect diagnosis labels and inconsistent FST annotations across duplicate images.
- Core assumption: Diagnosis labels should be consistent across duplicate images of the same lesion, and FST labels should reflect the actual skin tone of the subject in the image.
- Evidence anchors:
  - [abstract] "mislabeled diagnosis and Fitzpatrick skin tone (FST) labels"
  - [section] "3. 4% of the images were mislabeled" and "images with extremely high similarity varied in their FST labels, sometimes by as much as 4 tones"
  - [corpus] Weak evidence - no corpus papers directly discuss this specific labeling issue.
- Break condition: If the dataset had undergone expert review and correction of labels, or if duplicates were removed.

### Mechanism 3
- Claim: The absence of a held-out test partition in Fitzpatrick17k leads to overfit models and unreliable benchmarks.
- Mechanism: The original dataset used the same data partition for both validation (model selection) and testing (final evaluation), violating ML best practices and leading to over-optimistic performance estimates.
- Core assumption: A separate, unseen test set is necessary to accurately assess model generalization and avoid overfitting to validation data.
- Evidence anchors:
  - [abstract] "the absence of a proper held-out test set"
  - [section] "the authors only used a training and a validation set, and used the terms 'validation' and 'testing' interchangeably" and "This violates the fundamental rules of machine learning model training and evaluation"
  - [corpus] Weak evidence - no corpus papers directly discuss this specific partitioning issue.
- Break condition: If separate test sets were used for final evaluation, or if cross-validation was employed instead.

## Foundational Learning

- Concept: Data leakage and its impact on model evaluation
  - Why needed here: Understanding how data leakage (duplicate images across partitions) leads to inflated performance metrics is crucial for interpreting the results and proposed corrections.
  - Quick check question: If a model achieves 95% accuracy on a dataset with data leakage, what does this say about its true generalization ability on unseen data?

- Concept: Importance of proper dataset partitioning in ML
  - Why needed here: The paper highlights how incorrect partitioning (lack of a held-out test set) in Fitzpatrick17k leads to unreliable benchmarks, emphasizing the need for proper train/valid/test splits.
  - Quick check question: What is the purpose of a held-out test set, and why should it never be used during model training or hyperparameter selection?

- Concept: Challenges in medical image dataset curation
  - Why needed here: The paper discusses issues like mislabeled labels, duplicate images, and erroneous non-dermatological images in Fitzpatrick17k, highlighting the complexities of creating high-quality medical image datasets.
  - Quick check question: Why is expert review (e.g., by dermatologists) often necessary for confirming diagnosis labels in medical image datasets?

## Architecture Onboarding

- Component map:
  Fastdup/cleanvision -> Duplicate detection -> Manual verification GUI -> Union-find clustering -> Dataset cleaning -> Model training and evaluation

- Critical path:
  1. Detect duplicates using fastdup and cleanvision
  2. Manually verify duplicate pairs above similarity thresholds
  3. Merge duplicates into clusters and remove conflicting labels
  4. Remove erroneous non-dermatological images
  5. Create standardized train/valid/test splits
  6. Train and evaluate models on cleaned datasets

- Design tradeoffs:
  - Removing duplicates vs. retaining one representative image from each cluster
  - Using high similarity thresholds (0.90, 0.95) for duplicate detection vs. lower thresholds to catch more potential duplicates
  - Removing images with missing FST labels vs. retaining them for more data

- Failure signatures:
  - High similarity scores between images with different labels
  - Large clusters of duplicates with conflicting diagnosis or FST labels
  - Erroneous non-dermatological images in the dataset
  - Models performing significantly better on the original datasets compared to the cleaned versions

- First 3 experiments:
  1. Run fastdup on the Fitzpatrick17k dataset to detect duplicate image pairs above a similarity threshold of 0.90.
  2. Use the manual verification GUI to review the top 100 most similar duplicate pairs and confirm their status.
  3. Create a cleaned version of Fitzpatrick17k by removing duplicate clusters and erroneous images, then train a baseline model (e.g., VGG-16) on the cleaned dataset.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the exact extent of mislabeled images in Fitzpatrick17k, and can a fully verified ground truth dataset be established?
- Basis in paper: [explicit] The paper reports manual verification of 1,425 highly similar image pairs found 1.12% false positives and 0.49% ambiguous cases. For all image pairs with similarity above 0.90, they identify 2,498 pairs with different diagnoses and 4,030 pairs with different Fitzpatrick skin tone labels. The paper also notes that many labels do not map cleanly to ICD-11, complicating verification efforts.
- Why unresolved: Manual verification at scale is prohibitively expensive, and expert-verified ground truth is unavailable for the full dataset.
- What evidence would resolve it: Comprehensive expert annotation of a statistically representative sample of images, combined with ICD-11 mapping verification, would establish the true extent of mislabeling and enable creation of a verified ground truth.

### Open Question 2
- Question: How much do the data quality issues in DermaMNIST and Fitzpatrick17k affect model generalization performance on external test sets?
- Basis in paper: [explicit] The paper demonstrates data leakage in DermaMNIST where 13.47% of lesions appear in multiple partitions, and proposes DermaMNIST-E using external ISIC 2018 test data. For Fitzpatrick17k, they highlight the absence of a proper held-out test set and presence of duplicates, proposing Fitzpatrick17k-C with standardized partitions.
- Why unresolved: While corrected datasets are proposed, direct comparison of model performance on the original vs. corrected datasets is not possible due to different test partitions.
- What evidence would resolve it: Training and evaluating models on both original and corrected datasets using identical test sets would quantify the impact of data quality issues on generalization.

### Open Question 3
- Question: What are the most effective automated methods for detecting and resolving data quality issues in large medical image datasets?
- Basis in paper: [inferred] The paper employs fastdup and cleanvision for duplicate detection, but notes limitations including hash collisions and similarity threshold sensitivity. They also highlight challenges in resolving label conflicts and detecting erroneous images.
- Why unresolved: Automated methods have trade-offs between precision and recall, and no single approach captures all data quality issues comprehensively.
- What evidence would resolve it: Systematic evaluation of multiple duplicate detection algorithms, combined with error detection and label conflict resolution methods, on diverse medical image datasets would identify optimal approaches for different data quality issues.

## Limitations
- Dataset size reduction: Cleaning process significantly reduced dataset sizes, potentially limiting model generalization capacity
- Manual verification bottleneck: Duplicate detection relied on manual verification of 1,160 image pairs, introducing potential human error
- Missing metadata: Lesion IDs were inferred from filenames rather than directly provided, potentially leading to incomplete duplicate detection

## Confidence
- High confidence: Data leakage detection in DermaMNIST (supported by concrete overlap statistics)
- Medium confidence: Duplicate detection in Fitzpatrick17k (algorithm-based but requires manual verification)
- Low confidence: FST label correction (relies on majority voting within clusters, which may not always be correct)

## Next Checks
1. Verify the union-find clustering algorithm correctly groups all duplicate images by checking a random sample of clusters for internal consistency.
2. Re-run the outlier detection analysis on Fitzpatrick17k with different k-nearest neighbor values (k=3, k=7) to ensure the identified erroneous images are robust.
3. Train baseline models on the cleaned datasets using different random seeds to assess the stability of the reported performance improvements.