---
ver: rpa2
title: Mitigating Semantic Leakage in Cross-lingual Embeddings via Orthogonality Constraint
arxiv_id: '2409.15664'
source_url: https://arxiv.org/abs/2409.15664
tags:
- oracle
- language
- semantic
- lang
- sentence
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses semantic leakage in cross-lingual embeddings,
  where language-specific information unintentionally contaminates semantic representations.
  The proposed ORACLE method introduces orthogonality constraints via intra-class
  clustering and inter-class separation to enforce independence between semantic and
  language embeddings.
---

# Mitigating Semantic Leakage in Cross-lingual Embeddings via Orthogonality Constraint

## Quick Facts
- arXiv ID: 2409.15664
- Source URL: https://arxiv.org/abs/2409.15664
- Reference count: 18
- Key outcome: ORACLE reduces language embedding retrieval accuracy (indicating less leakage) while improving semantic retrieval accuracy and STS correlation, particularly for challenging low-resource and code-switched languages.

## Executive Summary
This paper addresses semantic leakage in cross-lingual embeddings, where language-specific information unintentionally contaminates semantic representations. The proposed ORACLE method introduces orthogonality constraints via intra-class clustering and inter-class separation to enforce independence between semantic and language embeddings. Experiments on cross-lingual retrieval and STS tasks show that ORACLE effectively reduces semantic leakage, particularly benefiting challenging language pairs like code-switched and low-resource languages.

## Method Summary
ORACLE mitigates semantic leakage by training separate semantic and language embeddings from a frozen multilingual encoder through orthogonality constraints. The method uses two MLP networks to generate semantic and language embeddings, then applies ORACLE loss combining intra-class clustering (LIC) to cluster language embeddings within each language and inter-class separation (LIS) to enforce orthogonality between semantic and language embeddings. ORACLE is integrated with existing DREAM or MEAT reconstruction losses and trained with Adam optimizer (lr=1e-5, batch size=512) for 10,000 iterations with early stopping.

## Key Results
- ORACLE reduces language embedding retrieval accuracy, demonstrating effective mitigation of semantic leakage
- Cross-lingual semantic retrieval accuracy improves significantly, particularly for challenging language pairs
- STS correlation improves, showing better semantic alignment between languages
- Performance gains are most pronounced for code-switched and low-resource languages

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Semantic leakage occurs when language-specific information contaminates semantic embeddings, degrading cross-lingual retrieval performance.
- Mechanism: Multilingual sentence encoders cluster sentences by language rather than meaning. When language-specific features are not properly separated, semantic embeddings still retain language identity signals, making retrieval accuracy dependent on language match rather than semantic similarity.
- Core assumption: Semantic and language representations can be effectively disentangled into orthogonal subspaces.
- Evidence anchors:
  - [abstract]: "semantic leakage - a term we introduce to describe when a substantial amount of language-specific information is unintentionally leaked into semantic representations"
  - [section]: "We discover that current disentangled representation learning methods suffer from semantic leakage... this issue as semantic leakage, which undermines the effectiveness of cross-lingual embeddings"
  - [corpus]: Weak - related work discusses leakage but lacks direct experimental validation of orthogonality enforcement
- Break condition: If semantic and language representations cannot be made orthogonal due to overlapping feature spaces or insufficient model capacity.

### Mechanism 2
- Claim: ORACLE's intra-class clustering (LIC) reduces semantic leakage by enforcing language-specific embeddings to cluster within each language.
- Mechanism: By minimizing pairwise cosine distances between language embeddings of the same language, ORACLE forces language representations to form tight clusters per language, making them more distinguishable from semantic content.
- Core assumption: Language identity is a distinct, clusterable feature that can be separated from semantic content.
- Evidence anchors:
  - [section]: "intra-class clustering aligns related components more closely" and "By minimizing LIC, we aim to cluster language-specific representation for each language"
  - [abstract]: "ORACLE builds upon two components: intra-class clustering and inter-class separation"
  - [corpus]: Moderate - modular approaches suggest separating language specialization from cross-lingual alignment
- Break condition: If language identity features are too distributed or if semantic content contains unavoidable language-specific markers.

### Mechanism 3
- Claim: ORACLE's inter-class separation (LIS) enforces orthogonality between semantic and language embeddings, directly addressing semantic leakage.
- Mechanism: By imposing a minimum cosine similarity constraint between semantic and language embeddings (forcing them to be orthogonal), ORACLE ensures these representations capture distinct aspects of the sentence, preventing language information from contaminating semantic embeddings.
- Core assumption: Semantic meaning and language identity can be represented in orthogonal subspaces without loss of information.
- Evidence anchors:
  - [section]: "LIS enforces irrelevant representations to be clearly separated" and "Minimizing LIS effectively disentangles semantics from language-specific representations by constraining them to be orthogonal"
  - [abstract]: "enforce orthogonality between semantic and language embeddings"
  - [corpus]: Strong - related work on modular encoders and privacy neurons suggests orthogonality can separate sensitive information
- Break condition: If orthogonality constraint is too restrictive and causes loss of semantic information or if the constraint cannot be satisfied given the data distribution.

## Foundational Learning

- Concept: Cosine similarity and distance metrics
  - Why needed here: ORACLE uses cosine similarity to measure distances between embeddings for both clustering and orthogonality enforcement
  - Quick check question: How would you compute the cosine distance between two 768-dimensional sentence embeddings?

- Concept: Multi-task learning objectives
  - Why needed here: ORACLE combines multiple loss components (LIC and LIS) in a single training objective to achieve both clustering and orthogonality
  - Quick check question: What are the advantages and potential drawbacks of combining reconstruction loss with orthogonality constraints in multi-task learning?

- Concept: Disentangled representation learning
  - Why needed here: The paper's core contribution relies on separating semantic meaning from language-specific features, which requires understanding how to train models to produce independent representations
  - Quick check question: In what ways does enforcing orthogonality between semantic and language representations differ from using adversarial training for disentanglement?

## Architecture Onboarding

- Component map:
  - Multilingual sentence encoder (frozen) → Semantic MLP → Semantic embeddings
  - Multilingual sentence encoder (frozen) → Language MLP → Language embeddings
  - ORACLE loss: LIC (intra-class clustering) + LIS (inter-class separation)
  - Integration: ORACLE loss combined with DREAM or MEAT reconstruction losses

- Critical path:
  1. Load pre-trained multilingual encoder (LASER, InfoXLM, or LaBSE)
  2. Initialize semantic and language MLPs with appropriate dimensions
  3. Implement ORACLE loss calculation (LIC + LIS)
  4. Integrate with existing DREAM/MEAT loss framework
  5. Train with Adam optimizer (lr=1e-5, batch size=512) for 10,000 iterations with early stopping
  6. Evaluate on cross-lingual retrieval and STS tasks

- Design tradeoffs:
  - Orthogonality constraint vs. semantic information preservation: Too strong orthogonality may degrade semantic alignment
  - Cluster tightness vs. representation capacity: Very tight language clusters may limit the model's ability to capture nuanced language features
  - Integration with existing methods: ORACLE works best when combined with DREAM/MEAT rather than as standalone

- Failure signatures:
  - Semantic retrieval accuracy drops significantly after applying ORACLE
  - Language embedding retrieval accuracy remains high (indicating insufficient separation)
  - Training instability or convergence issues due to conflicting loss terms
  - No improvement over baseline methods on cross-lingual tasks

- First 3 experiments:
  1. Baseline comparison: Run DREAM and MEAT on held-out test set without ORACLE, measure semantic and language retrieval accuracy
  2. ORACLE integration test: Apply ORACLE to DREAM baseline, compare semantic retrieval improvement and language retrieval reduction
  3. Component ablation: Test ORACLE with only LIC, only LIS, and both components to understand individual contributions to performance

## Open Questions the Paper Calls Out

The paper does not explicitly call out open questions, but several implications emerge from the analysis:

- Optimal balance between intra-class clustering and inter-class separation across different language pairs and encoder architectures
- How semantic leakage manifests in multilingual encoders trained with different objectives (masked language modeling vs. translation language modeling vs. contrastive learning)
- Whether ORACLE can be effectively applied to multilingual encoders trained on more than two languages simultaneously

## Limitations

- Limited ablation studies on the relative importance of LIC vs LIS components
- No comparison against recent modular encoder approaches that also separate language and semantic representations
- Claims about orthogonality being the optimal solution for all cross-lingual embedding scenarios lack theoretical bounds on semantic information preservation

## Confidence

- High confidence: ORACLE reduces language embedding retrieval accuracy (demonstrating effectiveness at mitigating semantic leakage)
- Medium confidence: ORACLE improves cross-lingual semantic retrieval accuracy and STS correlation
- Low confidence: Claims about orthogonality being the optimal solution for all cross-lingual embedding scenarios

## Next Checks

1. Conduct ablation studies comparing ORACLE with only LIC, only LIS, and both components to quantify their individual contributions to performance gains
2. Measure semantic information preservation by comparing downstream task performance (e.g., text classification) before and after ORACLE application
3. Test ORACLE on additional language pairs and domains (e.g., code-switched data, low-resource languages) to verify generalizability beyond the 12 language pairs studied