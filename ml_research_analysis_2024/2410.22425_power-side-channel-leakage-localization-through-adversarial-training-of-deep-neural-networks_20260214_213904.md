---
ver: rpa2
title: Power side-channel leakage localization through adversarial training of deep
  neural networks
arxiv_id: '2410.22425'
source_url: https://arxiv.org/abs/2410.22425
tags:
- power
- which
- leaking
- leakage
- trace
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a technique for identifying power trace timesteps
  that leak cryptographic keys, using an adversarial game between a deep learning-based
  classifier and a trainable noise generator. The classifier learns to predict a sensitive
  variable from power traces, while the noise generator learns to introduce noise
  to degrade classifier performance.
---

# Power side-channel leakage localization through adversarial training of deep neural networks

## Quick Facts
- arXiv ID: 2410.22425
- Source URL: https://arxiv.org/abs/2410.22425
- Authors: Jimmy Gammell; Anand Raghunathan; Kaushik Roy
- Reference count: 17
- Technique identifies power trace timesteps leaking cryptographic keys through adversarial training of DNNs and noise generators

## Executive Summary
This paper presents a novel technique for identifying power trace timesteps that leak cryptographic keys by using an adversarial game between a deep learning-based classifier and a trainable noise generator. The classifier learns to predict a sensitive variable from power traces, while the noise generator learns to introduce noise to degrade classifier performance. On synthetic datasets, the method outperforms existing techniques in the presence of countermeasures like Boolean masking and trace desynchronization. However, results on real datasets are weaker due to sensitivity to hyperparameters and lack of ground truth for model selection.

## Method Summary
The technique uses adversarial training where a DNN-based classifier and a trainable noise generator are jointly trained. The classifier learns to predict sensitive variables from power traces, while the noise generator learns to introduce noise to degrade classifier performance while adding minimal noise. An L1 norm penalty on the mask biases the noise generator toward producing temporally-sparse noise. The CNN architecture used in the classifier provides robustness to trace desynchronization through its convolution and pooling layers.

## Key Results
- Outperforms existing techniques on synthetic datasets with Boolean masking and trace desynchronization
- Demonstrates ability to identify dependencies between sensitive variables and multiple timesteps, even with pairwise-independent relationships
- Shows improved data efficiency when useful inductive biases are imposed through classifier architecture and objective function
- Real dataset results are weaker due to hyperparameter sensitivity and lack of ground truth for model selection

## Why This Works (Mechanism)

### Mechanism 1
The classifier learns to predict sensitive variables from power traces while the noise generator introduces noise to degrade classifier performance, forcing the classifier to use all leaking timesteps. An adversarial game is played between a DNN-based classifier and a trainable noise generator. The classifier learns to predict the value of some sensitive variable using power traces, while the noise generator learns to introduce noise into the power traces to optimize a compromise between degrading classifier performance and adding as little noise as possible. Over time, the classifier is forced to seek out new dependencies as those it presently relies on are progressively attenuated by noise.

### Mechanism 2
The L1 norm penalty on the mask biases the noise generator toward producing temporally-sparse noise, improving data efficiency when useful inductive biases can be imposed through choice of the classifier architecture and the objective function. The noise generator, represented by a data-independent vector m âˆˆ (0, 1)d, dictates a convex combination between power trace timesteps and i.i.d. d-dimensional standard normal noise. An L1-norm penalty on the output of the noise generator can bias it towards producing temporally-sparse noise, which can improve data efficiency when useful inductive biases can be imposed through choice of the classifier architecture and the objective function.

### Mechanism 3
The CNN architecture used in the classifier biases it toward functions which are invariant to temporal shifts of the input, making it robust to trace desynchronization. The CNN classifier architecture, composed of convolution-activation-pooling blocks followed by linear-activation blocks, can be modified to impose inductive biases towards functions which are expected a priori to generalize well. In this case, the convolution and pooling layers bias the classifier towards functions which are invariant to temporal shifts of the input, which makes it robust to trace desynchronization.

## Foundational Learning

- **Adversarial training**: Why needed here: Adversarial training is the core mechanism that forces the classifier to use all leaking timesteps by playing an adversarial game with the noise generator. Quick check question: How does adversarial training differ from standard supervised learning, and why is it necessary for leakage localization?

- **Deep learning for side-channel analysis**: Why needed here: Deep learning techniques have emerged as powerful tools for side-channel attacks, and this work leverages their ability to detect complex dependencies between power traces and sensitive variables. Quick check question: What are the key advantages of using deep learning for side-channel analysis compared to traditional techniques like template attacks and differential power analysis?

- **Neural network attribution techniques**: Why needed here: Neural network attribution techniques are used as a baseline for comparison, and understanding their limitations helps highlight the advantages of the adversarial masking approach. Quick check question: What are the main limitations of neural network attribution techniques for leakage localization, and how does the adversarial masking approach address these limitations?

## Architecture Onboarding

- **Component map**: Classifier -> Noise generator -> Adversarial game training
- **Critical path**: 1) Generate synthetic power traces using the provided algorithm 2) Initialize the classifier and noise generator 3) Train the classifier and noise generator simultaneously using alternating minibatch stochastic gradient descent 4) Monitor the classifier's loss and the mask's L1 norm during training 5) Stop training when the classifier's loss stops decreasing or the mask's L1 norm stops increasing 6) Use the final mask to identify the leaking timesteps in the power traces
- **Design tradeoffs**: Classifier architecture affects inductive biases and ability to detect complex dependencies. CNN with convolution and pooling layers imposes an inductive bias toward functions invariant to temporal shifts. Noise generator represented by data-independent vector simplifies training but may limit adaptation to individual datapoints. L1 norm penalty biases toward temporally-sparse noise, improving data efficiency but potentially limiting detection of distributed leakage.
- **Failure signatures**: Classifier overfitting to noise in dataset may identify false-positive leaking timesteps. Insufficient L1 norm penalty may prevent temporally-sparse noise production, reducing data efficiency. Inadequate classifier architecture may fail to detect complex dependencies or be sensitive to trace desynchronization.
- **First 3 experiments**: 1) Generate synthetic dataset with single leaking timestep and no countermeasures. Train using adversarial masking and verify final mask correctly identifies leaking timestep. 2) Generate synthetic dataset with multiple leaking timesteps and Boolean masking. Train using adversarial masking and verify final mask correctly identifies leaking timesteps despite Boolean masking. 3) Generate synthetic dataset with desynchronized traces. Train using adversarial masking with CNN classifier and verify final mask correctly identifies leaking timesteps despite desynchronization.

## Open Questions the Paper Calls Out

### Open Question 1
How can model selection be performed without oracle knowledge of leaking points, to prevent overfitting and false-positive leakage identifications? A major barrier to practical usage of our technique is that early stopping is essential to prevent false-positive leakage identification due to overfitting by the classifier, yet there is no clear performance metric to monitor which does not rely on unrealistic oracle knowledge of which points are leaking.

### Open Question 2
What is the impact of varying the CNN architecture (beyond the tested configuration) on AdvMask's performance in different leakage scenarios? The paper notes that "the CNN classifier architecture and optimizer hyperparameters used by both GradVis and AdvMask are inappropriate for small power traces" and suggests that "it would be necessary to explore a wider variety of hyperparameters and neural network architectures."

### Open Question 3
How does AdvMask perform on real-world devices with complex, non-linear power consumption models compared to synthetic datasets? The paper states that "results on real datasets are weak because the technique is highly sensitive to hyperparameters and early-stop point, and we lack a holdout dataset with ground truth knowledge of leaking points for model selection."

## Limitations

- Performance on real-world datasets is notably weaker than on synthetic data
- Technique is highly sensitive to hyperparameter choices and early stopping points
- Lack of ground truth for model selection creates significant practical limitations

## Confidence

**High Confidence**: The adversarial training mechanism for forcing classifiers to use all leaking timesteps is theoretically sound and well-supported by synthetic dataset results.

**Medium Confidence**: Claims about data efficiency improvements through L1 penalty and architectural inductive biases are supported by synthetic results but need more validation on real-world data.

**Low Confidence**: Generalizability of the technique to diverse real-world side-channel scenarios, particularly those with complex countermeasure combinations, is not well-established.

## Next Checks

1. Apply the technique to multiple real AES implementations with different countermeasure combinations and compare results against established leakage assessment methods.

2. Systematically vary the L1 penalty coefficient and classifier architecture parameters across multiple datasets to quantify their impact on detection accuracy and false positive rates.

3. Evaluate whether models trained on one device can successfully identify leakage in traces from a different device of the same type, testing the technique's practical applicability.