---
ver: rpa2
title: 'Optimizing Language Augmentation for Multilingual Large Language Models: A
  Case Study on Korean'
arxiv_id: '2403.10882'
source_url: https://arxiv.org/abs/2403.10882
tags:
- korean
- data
- language
- lima
- english
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper proposed three strategies to enhance the performance
  of less-resourced languages (LRLs) using multilingual large language models (MLLMs):
  vocabulary expansion by adding LRL-specific words, knowledge enrichment through
  bilingual pretraining on high- and LRL-parallel corpora, and usability improvement
  via high-quality LRL instruction tuning. The approach was evaluated on Korean using
  Llama2 as the base MLLM, with quantitative tests across eight tasks and qualitative
  comparisons via human and GPT-4 evaluations.'
---

# Optimizing Language Augmentation for Multilingual Large Language Models: A Case Study on Korean

## Quick Facts
- arXiv ID: 2403.10882
- Source URL: https://arxiv.org/abs/2403.10882
- Reference count: 0
- Key outcome: Proposed three strategies to enhance LRL performance: vocabulary expansion, bilingual pretraining, and instruction tuning. Evaluated on Korean using Llama2, showing up to 8% accuracy improvement and 93% win rate in qualitative comparisons.

## Executive Summary
This paper addresses the challenge of improving multilingual large language models (MLLMs) for less-resourced languages (LRLs) through a structured augmentation approach. Using Korean as a case study, the authors propose three complementary strategies: expanding the model's vocabulary with LRL-specific words, enriching knowledge through bilingual pretraining on parallel corpora, and enhancing usability via high-quality LRL instruction tuning. The approach is implemented on Llama2 and evaluated across eight diverse tasks, demonstrating both quantitative and qualitative performance gains. The work provides a practical framework for adapting MLLMs to better support LRLs, with broader implications for multilingual NLP development.

## Method Summary
The authors propose three strategies to enhance MLLMs for LRLs. First, they expand the vocabulary by merging Llama2's tokenizer with KoBERT's 8,002-word vocabulary, increasing expressiveness for Korean. Second, they perform bilingual pretraining on a 33GB Korean-English corpus for one epoch to align linguistic representations between high- and low-resource languages. Third, they instruction-tune the model using a refined 1,030-sample Korean LIMA dataset via a LoRA adapter to improve task-specific performance. The approach is evaluated on eight Korean NLP tasks, with results compared to baseline models through quantitative metrics and human/GPT-4 qualitative assessments.

## Key Results
- The Bllossom model outperformed baseline Korean models by up to 8% in accuracy across eight tasks.
- In head-to-head comparisons, Bllossom won 93% of qualitative assessments by human and GPT-4 evaluators.
- The three proposed strategies (vocabulary expansion, bilingual pretraining, and instruction tuning) collectively improved both task performance and usability for Korean.

## Why This Works (Mechanism)
The paper's approach works by addressing three core weaknesses of MLLMs when handling LRLs. Vocabulary expansion directly tackles the out-of-vocabulary problem by incorporating LRL-specific words, reducing reliance on subword tokenization artifacts. Bilingual pretraining aligns semantic representations between high- and low-resource languages, enabling better cross-lingual transfer. Instruction tuning with high-quality, language-specific data refines the model's ability to follow instructions and perform tasks in the target language, improving both accuracy and user experience.

## Foundational Learning
- **Vocabulary expansion**: Adding LRL-specific words to the tokenizer to reduce out-of-vocabulary issues. *Why needed*: Subword tokenizers often fail to represent unique LRL words, leading to encoding inefficiencies. *Quick check*: Compare tokenization length and OOV rate before and after expansion.
- **Bilingual pretraining**: Training on parallel corpora of high- and low-resource languages to align representations. *Why needed*: Ensures the model understands semantic equivalences across languages. *Quick check*: Evaluate cross-lingual transfer performance on parallel sentence matching.
- **Instruction tuning**: Fine-tuning on high-quality, task-specific instruction datasets. *Why needed*: Improves the model's ability to follow instructions and perform downstream tasks. *Quick check*: Measure instruction-following accuracy on held-out LRL instruction pairs.
- **LoRA adapters**: Low-rank adaptation for efficient fine-tuning. *Why needed*: Reduces computational cost while preserving base model capabilities. *Quick check*: Compare performance and memory usage with full fine-tuning.
- **Cross-lingual alignment**: Ensuring consistent semantic representations across languages. *Why needed*: Critical for multilingual models to generalize across languages. *Quick check*: Evaluate on cross-lingual semantic similarity tasks.

## Architecture Onboarding
- **Component map**: Llama2 -> Vocabulary Expansion -> Bilingual Pretraining -> Instruction Tuning (LoRA) -> Bllossom
- **Critical path**: The sequence of vocabulary expansion, bilingual pretraining, and instruction tuning is critical for achieving optimal performance. Each step builds on the previous one, with vocabulary expansion enabling better pretraining, and pretraining enabling better instruction tuning.
- **Design tradeoffs**: Vocabulary expansion increases tokenization time and model size; bilingual pretraining risks language bias if corpora are imbalanced; instruction tuning with a small dataset risks overfitting but maintains efficiency.
- **Failure signatures**: Vocabulary expansion may lead to longer token sequences; pretraining imbalance may bias responses toward the high-resource language; small instruction datasets may limit task diversity.
- **First experiments**:
  1. Evaluate tokenization efficiency and OOV reduction after vocabulary expansion.
  2. Measure cross-lingual transfer performance after bilingual pretraining.
  3. Assess instruction-following accuracy after LoRA-based instruction tuning.

## Open Questions the Paper Calls Out
### Open Question 1
- Question: How does vocabulary expansion impact the performance of multilingual LLMs across different language families beyond Korean?
- Basis in paper: [explicit] The paper discusses vocabulary expansion as a strategy to enhance expressiveness for less-resourced languages (LRLs) using the example of Korean.
- Why unresolved: The study focuses specifically on Korean as a case study, leaving the generalizability of vocabulary expansion to other language families unexplored.
- What evidence would resolve it: Conducting similar experiments with other LRLs from diverse language families (e.g., Slavic, Semitic, Dravidian) and comparing the performance gains from vocabulary expansion.

### Open Question 2
- Question: What is the optimal balance between pretraining data in high-resource and low-resource languages to maximize the performance of multilingual LLMs?
- Basis in paper: [inferred] The paper mentions bilingual pretraining to align high- and less-resourced languages but does not explore the optimal ratio of training data between these languages.
- Why unresolved: The study uses a specific ratio (7:3) for Korean and English pretraining data but does not investigate how different ratios affect model performance.
- What evidence would resolve it: Systematically varying the ratio of high-resource to low-resource language data in pretraining and measuring the impact on model performance across various tasks.

### Open Question 3
- Question: How does the quality of instruction data influence the performance of multilingual LLMs compared to the quantity of data?
- Basis in paper: [explicit] The paper emphasizes the importance of high-quality instruction data for improving usability, citing the LIMA approach of using a smaller amount of refined data.
- Why unresolved: While the paper constructs a Korean LIMA dataset, it does not directly compare the impact of data quality versus quantity on model performance.
- What evidence would resolve it: Training models with datasets of varying quality and size, then comparing their performance on relevant tasks to determine the relative importance of data quality and quantity.

### Open Question 4
- Question: How do different tokenization strategies affect the performance of multilingual LLMs on languages with non-Latin scripts?
- Basis in paper: [explicit] The paper discusses the limitations of the SentencePiece tokenizer's UTF-8 byte fallback mechanism for handling out-of-vocabulary words in Korean.
- Why unresolved: The study proposes vocabulary expansion as a solution but does not explore alternative tokenization strategies that might better handle non-Latin scripts.
- What evidence would resolve it: Comparing the performance of multilingual LLMs using different tokenization strategies (e.g., morphological tokenization, subword tokenization) on languages with non-Latin scripts across various tasks.

## Limitations
- Vocabulary expansion using KoBERT may introduce encoding inefficiencies and bias toward Korean-centric tokenization patterns.
- Pretraining corpus details (33GB) are not fully specified, raising concerns about potential English dominance and reproducibility.
- The instruction-tuning dataset of 1,030 samples may not fully represent the diversity of Korean language use cases.
- Evaluation relies on both quantitative metrics and subjective qualitative assessments, with limited cross-lingual generalization tests.

## Confidence
- **High confidence**: The core methodology (vocabulary expansion, bilingual pretraining, instruction tuning) is clearly defined and implemented.
- **Medium confidence**: The reported performance gains (up to 8% accuracy improvement) are supported by quantitative tests but depend on the quality and representativeness of evaluation datasets.
- **Low confidence**: Claims about usability improvements are primarily based on qualitative human and GPT-4 evaluations, which are inherently subjective and may not generalize.

## Next Checks
1. Conduct ablation studies to isolate the contribution of each proposed strategy (vocabulary expansion, bilingual pretraining, instruction tuning) to overall performance gains.
2. Test the approach on a different LRL (e.g., Vietnamese or Thai) to assess generalizability and identify language-specific challenges.
3. Perform a more detailed analysis of the pretraining corpus composition and its impact on model bias and cross-lingual capabilities.