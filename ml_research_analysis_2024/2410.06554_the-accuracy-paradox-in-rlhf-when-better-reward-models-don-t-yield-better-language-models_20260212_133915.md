---
ver: rpa2
title: 'The Accuracy Paradox in RLHF: When Better Reward Models Don''t Yield Better
  Language Models'
arxiv_id: '2410.06554'
source_url: https://arxiv.org/abs/2410.06554
tags:
- reward
- accurate
- training
- most
- steps
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This study challenges the widely held belief that stronger reward
  models always lead to better language model performance in Reinforcement Learning
  from Human Feedback (RLHF). Through experiments on relevance, factuality, and completeness
  tasks using the QA-FEEDBACK dataset and Longformer-based reward models, we uncover
  a paradox: language models trained with moderately accurate reward models outperform
  those guided by highly accurate ones.'
---

# The Accuracy Paradox in RLHF: When Better Reward Models Don't Yield Better Language Models

## Quick Facts
- arXiv ID: 2410.06554
- Source URL: https://arxiv.org/abs/2410.06554
- Reference count: 40
- This study reveals a paradox where language models trained with moderately accurate reward models outperform those guided by highly accurate ones in RLHF

## Executive Summary
This paper challenges the conventional wisdom that stronger reward models always lead to better language model performance in Reinforcement Learning from Human Feedback (RLHF). Through experiments on relevance, factuality, and completeness tasks using the QA-FEEDBACK dataset and Longformer-based reward models, the authors uncover a counterintuitive phenomenon: moderately accurate reward models can actually guide language models to better performance than highly accurate ones. The findings suggest that reward model accuracy alone is not sufficient for optimal RLHF training, and that the relationship between reward model performance and final language model quality is more complex than previously understood.

## Method Summary
The researchers conducted experiments using the QA-FEEDBACK dataset with Longformer-based reward models of varying accuracy levels. They trained language models using RLHF with different reward model accuracies and evaluated performance on three key dimensions: relevance, factuality, and completeness. The study systematically varied reward model accuracy to observe its impact on the final language model performance, comparing outcomes across different accuracy thresholds to identify optimal training conditions.

## Key Results
- Language models trained with moderately accurate reward models outperformed those trained with highly accurate reward models
- Moderately accurate reward models provided more task-appropriate feedback during training
- Highly accurate reward models were found to potentially mislead the optimization process

## Why This Works (Mechanism)
The mechanism behind this paradox remains unexplained in the paper, as the authors note that the counterintuitive behavior of highly accurate reward models could be an artifact of the synthetic feedback generation process. The study suggests that there may be a complex relationship between reward model accuracy and the quality of feedback provided during RLHF training, but the exact causal factors are not fully elucidated.

## Foundational Learning
- **Reinforcement Learning from Human Feedback (RLHF)**: Why needed - Core training methodology being studied; Quick check - Understanding the basic RLHF pipeline and how reward models guide language model training
- **Reward Model Accuracy**: Why needed - Central concept being challenged; Quick check - Ability to define and measure reward model accuracy metrics
- **Synthetic vs. Human Feedback**: Why needed - Key methodological distinction in the study; Quick check - Understanding the differences between synthetic and genuine human preference data
- **Optimization Dynamics**: Why needed - Relevant to understanding how reward models guide training; Quick check - Basic knowledge of how optimization processes work in RLHF

## Architecture Onboarding

**Component Map**: Language Model <- Reward Model -> Feedback Generation

**Critical Path**: Training Loop: Language Model receives feedback from Reward Model, which generates signals based on synthetic preference data from QA-FEEDBACK dataset

**Design Tradeoffs**: The study uses synthetic feedback instead of human preference data for controlled experiments, but this limits generalizability to real-world applications where genuine human feedback is essential.

**Failure Signatures**: The paradox itself serves as a failure signature - when highly accurate reward models produce worse language model performance than moderately accurate ones, indicating potential issues with optimization guidance.

**3 First Experiments**:
1. Replicate the original experiments with varying reward model accuracy levels
2. Test different base language model architectures beyond Longformer
3. Compare synthetic feedback results with preliminary human preference data

## Open Questions the Paper Calls Out
None

## Limitations
- The experimental setup relies on synthetic feedback rather than genuine human preference data, limiting real-world applicability
- The choice of Longformer-based reward models may not capture the full complexity of natural language tasks
- The mechanism behind the paradox remains unexplained, potentially being an artifact of the experimental design

## Confidence

**Moderately accurate reward models provide more task-appropriate feedback**: Medium confidence - due to artificial training data and limited task diversity

**Highly accurate reward models can mislead optimization**: Low confidence - mechanism unexplained and could be experimental artifact

## Next Checks

1. Replicate experiments using genuine human preference data across multiple task types and reward model architectures
2. Conduct ablation studies to isolate whether the paradox stems from reward model accuracy itself or from the relationship between reward accuracy and feedback generation process
3. Test whether the paradox persists when varying base language model size and architecture beyond the Longformer implementation used in this study