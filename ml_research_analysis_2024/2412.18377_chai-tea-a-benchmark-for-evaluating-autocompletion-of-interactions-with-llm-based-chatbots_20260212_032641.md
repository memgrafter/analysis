---
ver: rpa2
title: 'ChaI-TeA: A Benchmark for Evaluating Autocompletion of Interactions with LLM-based
  Chatbots'
arxiv_id: '2412.18377'
source_url: https://arxiv.org/abs/2412.18377
tags:
- completions
- saved
- user
- task
- completion
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces ChaI-TeA, a benchmark for evaluating autocompletion
  of interactions with LLM-based chatbots. The authors formally define the task, create
  suitable datasets and metrics, and evaluate 9 models on the task.
---

# ChaI-TeA: A Benchmark for Evaluating Autocompletion of Interactions with LLM-based Chatbots

## Quick Facts
- arXiv ID: 2412.18377
- Source URL: https://arxiv.org/abs/2412.18377
- Reference count: 40
- Primary result: Current models can save up to 45% of typing on average but struggle with ranking generated suggestions effectively

## Executive Summary
This paper introduces ChaI-TeA, a comprehensive benchmark for evaluating autocompletion of interactions with LLM-based chatbots. The authors formally define the task, create suitable datasets and metrics, and evaluate 9 different models on the task. The benchmark addresses a critical gap in evaluating chatbot autocompletion systems, which have been deployed widely but lack standardized evaluation frameworks. The results demonstrate that while current models perform reasonably well in generating completions, there remains significant room for improvement, particularly in ranking the generated suggestions to identify the most useful ones.

## Method Summary
The benchmark uses two main datasets (OpenAssistant and ShareGPT) to extract user-turn prefixes paired with full conversation history as context. Models generate k completions by sampling nc completions with nt tokens each, then ranking by perplexity. The primary metric is saved@k, which measures typing saved by accepting completions normalized by turn length minus acceptances. The evaluation also measures latency and acceptance rate@k. The method includes fine-tuning via LoRA on training data, which improves performance by 4-11%. The experiments systematically vary hyper-parameters like context length, number of completions, and completion lengths to study performance-latency tradeoffs.

## Key Results
- Current models can save up to 45% of typing on average in chatbot interactions
- Perplexity-based ranking shows significant limitations, with large performance gaps between small and large k values
- Fine-tuning models on the task improves performance by 4-11%
- Models can exploit distant history effectively, and allowing variable-length completions improves quality
- Latency reduction is better achieved by shortening context and completion lengths rather than generating fewer completions

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Current language models generate completion suggestions well but fail to rank them effectively
- Mechanism: Models produce a pool of completions but the ranking based on perplexity fails to prioritize the most useful suggestions
- Core assumption: Perplexity is insufficient for confidence ranking in this task
- Evidence anchors:
  - [abstract] "there is still much room for improvement, mainly in ranking of the generated suggestions"
  - [section] "there is a noticeably large performance gap between small, realistic, k values and larger values, suggesting that while in many cases models are able to generate the correct completion, their ranking of completions is far from perfect"
- Break condition: If a new ranking metric or fine-tuning approach can significantly improve the quality of top-k suggestions

### Mechanism 2
- Claim: Allowing completions of various lengths improves user experience compared to restricting to single words or full sentences
- Mechanism: Users benefit from flexible completion lengths that match their typing patterns and intent
- Core assumption: Different completion lengths serve different user needs during interaction
- Evidence anchors:
  - [abstract] "it is beneficial to enable completions of different lengths"
  - [section] "we compare completions of varying lengths to single word and full sentence completions to check whether allowing any-length completions improves quality"
- Break condition: If user studies show no preference for variable-length completions over simpler approaches

### Mechanism 3
- Claim: Reducing latency by shortening context length and completion length is more effective than generating fewer completions
- Mechanism: The task can trade off performance for latency by optimizing which parts of the input to truncate
- Core assumption: Users can tolerate some performance loss for faster response times
- Evidence anchors:
  - [abstract] "latency should be reduced by shortening context and completions rather than generating fewer completions"
  - [section] "it is preferable to generate more completions, while reducing the number of generated tokens and context length"
- Break condition: If users consistently prefer slower responses with better quality over faster but less accurate suggestions

## Foundational Learning

- Concept: Language modeling and perplexity
  - Why needed here: The task uses language models to generate completions, and perplexity is used for ranking
  - Quick check question: How does perplexity measure the quality of language model predictions, and why might it be insufficient for ranking autocomplete suggestions?

- Concept: Online vs offline evaluation
  - Why needed here: The paper uses offline metrics to simulate user interactions rather than running live user studies
  - Quick check question: What are the advantages and limitations of using exact match comparison versus user studies for evaluating autocomplete systems?

- Concept: Context window management in language models
  - Why needed here: The experiments vary context length to balance performance and latency
  - Quick check question: How does the context length affect language model performance and latency, and what are the practical implications for autocomplete systems?

## Architecture Onboarding

- Component map: Context Processor -> Language Model Generator -> Perplexity Ranker -> Completion Delivery
- Critical path: User input → context extraction → LM generation → ranking → suggestion delivery
- Design tradeoffs: Quality vs latency (longer context improves quality but increases latency), number of completions vs user cognitive load
- Failure signatures: High perplexity ranking leads to poor top-k suggestions, excessive latency causes user to type ahead, insufficient context misses relevant information
- First 3 experiments:
  1. Test different perplexity-based ranking methods to improve top-k suggestion quality
  2. Experiment with context length variations to find optimal performance-latency tradeoff
  3. Compare variable-length completions against single-word and full-sentence baselines to validate the benefits of flexible completion lengths

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What are the optimal hyper-parameters (nc, nt, and context length) for maximizing saved@k while maintaining practical latency constraints?
- Basis in paper: [explicit] The paper discusses a latency-performance trade-off study, showing that different configurations of nc (number of completions), nt (number of tokens), and context length affect both saved@k and latency.
- Why unresolved: The study shows that there is a trade-off, but it doesn't identify a single optimal configuration that balances performance and latency. The best configuration depends on the specific latency budget.
- What evidence would resolve it: A comprehensive evaluation of all possible hyper-parameter combinations across different latency budgets, identifying the configuration that consistently maximizes saved@k within each budget.

### Open Question 2
- Question: How does the performance of the chatbot interaction autocompletion task change when evaluated using semantic similarity metrics (e.g., BLEU, ROUGE) instead of exact match?
- Basis in paper: [inferred] The paper mentions that exact match is used to simulate acceptances, but notes that this may not fully represent real-world scenarios where users might accept semantically similar completions. It also states that generation metrics like BLEU or ROUGE fail to capture semantic similarity for partial completions.
- Why unresolved: The paper acknowledges the limitations of exact match but does not explore alternative evaluation methods that could better capture semantic similarity.
- What evidence would resolve it: An evaluation of the task using semantic similarity metrics, comparing the results to exact match and assessing whether the ranking of models changes.

### Open Question 3
- Question: How does the performance of chatbot interaction autocompletion change when the models are fine-tuned on datasets collected in the presence of an autocomplete solution?
- Basis in paper: [inferred] The paper mentions that the datasets used were collected without the presence of an autocomplete solution and that users might alter their behavior when completion suggestions are presented to them.
- Why unresolved: The paper acknowledges that the current datasets might not fully capture user behavior in the presence of an autocomplete solution, but it doesn't explore how fine-tuning on such datasets would affect performance.
- What evidence would resolve it: An evaluation of the task using models fine-tuned on datasets collected with an autocomplete solution, comparing the results to models fine-tuned on the current datasets.

## Limitations

- The evaluation relies entirely on offline metrics using exact match comparison rather than live user studies
- The benchmark datasets, while reasonably sized, may not fully capture the diversity of real-world chatbot interactions
- The perplexity-based ranking mechanism shows clear limitations in selecting the most useful suggestions

## Confidence

**High confidence**: The observation that current models can save up to 45% of typing on average is well-supported by experimental results across multiple models and datasets. The finding that fine-tuning models on the task improves performance by 4-11% is also strongly supported by ablation studies.

**Medium confidence**: The claim that models can exploit distant history is based on experiments showing that longer context improves performance, but practical implications remain unclear. The assertion that latency should be reduced by shortening context rather than generating fewer completions is supported by experiments but could vary based on specific deployment scenarios.

**Low confidence**: The recommendation that allowing completions of different lengths is beneficial relies on comparisons that may not fully account for user preferences or cognitive load. The assertion that perplexity is insufficient for ranking is demonstrated through performance gaps but lacks exploration of alternative ranking approaches.

## Next Checks

1. **Live user study validation**: Conduct a small-scale user study comparing the current autocomplete system against a baseline with no suggestions and against a random completion selection. Measure actual typing time saved, user satisfaction, and cognitive load.

2. **Alternative ranking mechanism comparison**: Implement and evaluate at least two alternative ranking methods beyond perplexity, such as confidence scores from fine-tuned reward models or learned ranking functions. Compare their performance against perplexity-based ranking on the same datasets.

3. **Context truncation sensitivity analysis**: Systematically test different context truncation strategies (beginning, middle, end, key phrases) across multiple models and conversation types. Measure the tradeoff between performance loss and latency reduction to identify optimal truncation policies for different use cases and latency requirements.