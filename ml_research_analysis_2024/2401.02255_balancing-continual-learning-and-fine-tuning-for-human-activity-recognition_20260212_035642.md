---
ver: rpa2
title: Balancing Continual Learning and Fine-tuning for Human Activity Recognition
arxiv_id: '2401.02255'
source_url: https://arxiv.org/abs/2401.02255
tags:
- learning
- continual
- performance
- task
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work explores continual self-supervised learning (CSSL) techniques
  for wearable-based human activity recognition (HAR), addressing the challenge of
  catastrophic forgetting in dynamic human behaviors. The authors adapt CaSSLe and
  Kaizen, two state-of-the-art CSSL frameworks, to HAR by modifying transformation
  functions and loss terms.
---

# Balancing Continual Learning and Fine-tuning for Human Activity Recognition

## Quick Facts
- arXiv ID: 2401.02255
- Source URL: https://arxiv.org/abs/2401.02255
- Reference count: 16
- One-line primary result: Kaizen achieves 58.8% average continual accuracy on WISDM2019, outperforming CaSSLe by 9.8%

## Executive Summary
This work addresses catastrophic forgetting in wearable-based human activity recognition (HAR) by adapting continual self-supervised learning (CSSL) frameworks to dynamic human behaviors. The authors modify CaSSLe and Kaizen frameworks with task-specific transformation functions and loss terms for HAR applications. Kaizen, which combines contrastive learning with self-training, demonstrates superior performance across multiple metrics on the WISDM2019 dataset. The study reveals that progressive importance coefficients reflecting the ratio of learned to new classes achieve optimal trade-offs between knowledge retention and new task learning.

## Method Summary
The authors adapt CSSL frameworks (CaSSLe and Kaizen) to HAR by implementing transformation functions (random 3D rotation, scaling, time warping) and modifying loss terms for feature extraction and classification. Kaizen extends CaSSLe by adding supervised and self-distillation components for classifier training, ensuring functional classifiers throughout the continual learning process. The method uses TPN architecture as feature extractor, BYOL or MoCoV2+ as SSL framework, and 1% replay data from previous tasks. An adaptive weighting coefficient λC scales the relative importance of knowledge distillation versus new task learning, with progressive coefficients showing better performance than constant values.

## Key Results
- Kaizen achieves 58.8% average continual accuracy and 48.1% final accuracy using BYOL, surpassing CaSSLe by 9.8% and 7.1% respectively
- Progressive importance coefficients (λC = a ⊕ b) outperform constant values for balancing knowledge retention and new task learning
- BYOL outperforms MoCoV2+ as the base SSL method in the Kaizen framework

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Kaizen's integration of knowledge distillation with fine-tuning enables continuous classifier availability during continual learning
- Mechanism: The Kaizen framework combines four loss components: contrastive learning for new tasks (LCT_FE), knowledge distillation for feature extraction (LKD_FE), supervised learning for classifier training (LCT_C), and self-distillation for classifier knowledge retention (LKD_C). This unified approach allows the model to maintain functional classifiers at every step.
- Core assumption: Maintaining a functional classifier throughout the continual learning process is critical for HAR applications.
- Evidence anchors:
  - [abstract] "Kaizen combines contrastive learning with self-training in a unified scheme that can leverage unlabelled and labelled data for continual learning"
  - [section 2.2] "The Kaizen framework (Tang et al. 2024) extends CaSSLe by proposing two additional components to handle classifier training, to ensure that a functional classifier is available at any step of the continual learning process"
  - [corpus] Weak evidence - no direct citations about classifier availability in HAR contexts
- Break condition: If labeled data becomes completely unavailable, the supervised learning component (LCT_C) cannot function, breaking the unified training scheme.

### Mechanism 2
- Claim: Progressive importance coefficients (λC = a ⊕ b) enable better balance between knowledge retention and new task learning.
- Mechanism: The importance coefficient λC scales the relative weight of knowledge distillation versus new task learning. Progressive coefficients increase this weight over time (λC(t) = a + b × (t - 1)), allowing the model to shift focus from learning new tasks to retaining knowledge as more classes are learned.
- Core assumption: The optimal balance between knowledge retention and new task learning changes as the number of learned classes increases.
- Evidence anchors:
  - [section 2.3] "we hypothesise that the relative importance of the knowledge distillation task compared to learning from new data in classification learning can have a direct impact on the performance of the classifier across time"
  - [section 3] "the use of a weighting factor that reflects the ratio between learned and new classes achieves the best overall trade-off in continual learning"
  - [corpus] Weak evidence - no direct citations about progressive weighting in HAR or CSSL contexts
- Break condition: If the progressive factor increases too rapidly (high b value), the model may over-prioritize knowledge retention too early, preventing adequate learning of new tasks.

### Mechanism 3
- Claim: Repurposing SSL methods (BYOL, MoCoV2+) for knowledge retention mitigates catastrophic forgetting in HAR.
- Mechanism: Contrastive learning frameworks are adapted to contrast representations from current and previous task feature extractors, forcing the model to maintain similar representations for the same activities across task boundaries.
- Core assumption: Contrastive learning's representation preservation capabilities can be effectively transferred from visual to sensor time-series domains.
- Evidence anchors:
  - [section 2.1] "The CaSSLe framework (Fini et al. 2022) re-purposed the Siamese/Contrastive learning setup and loss functions for tackling catastrophic forgetting in representation learning"
  - [section 3] "BYOL MoCoV2+ Self-supervised Learning Method 0.2 0.3 0.4 0.5 0.6 0.7 Top-1 Accuracy Average Continual Performance No Distill CaSSLe Kaizen"
  - [corpus] Moderate evidence - related works on contrastive learning for HAR (Haresamudram et al. 2020, 2021) but not specifically for continual learning

## Foundational Learning

- Concept: Catastrophic forgetting in neural networks
  - Why needed here: Understanding why standard fine-tuning causes performance degradation on previous tasks is fundamental to appreciating why CSSL approaches are necessary
  - Quick check question: What happens to a neural network's performance on previously learned tasks when it is trained on new, different data without any special mitigation?

- Concept: Self-supervised learning (SSL) frameworks
  - Why needed here: Kaizen and CaSSLe build upon SSL methods (BYOL, MoCoV2+) as the foundation for knowledge retention, so understanding these frameworks is essential
  - Quick check question: How do BYOL and MoCoV2+ create representations without using labels, and what role do transformation functions play?

- Concept: Contrastive learning and knowledge distillation
  - Why needed here: These are the core mechanisms for preventing forgetting in the proposed methods, so understanding how they work together is crucial
  - Quick check question: How does contrasting current and previous task representations help prevent forgetting, and how does self-distillation work in the classifier component?

## Architecture Onboarding

- Component map:
  - Feature extractor (TPN architecture adapted from Saeed et al.)
  - Transformation functions (random 3D rotation, scaling, time warping)
  - SSL framework (BYOL or MoCoV2+)
  - Classifier (separate neural network for downstream classification)
  - Replay buffer (1% of previous task data for exemplar-based learning)
  - Loss components (LCT_FE, LKD_FE, LCT_C, LKD_C with λC weighting)

- Critical path:
  1. Data preprocessing (z-normalization, windowing)
  2. Transformation function application to create views
  3. Feature extraction with contrastive learning loss
  4. Classifier training with supervised and self-distillation losses
  5. Knowledge distillation from previous task models
  6. Replay buffer management for exemplar-based learning

- Design tradeoffs:
  - BYOL vs MoCoV2+: BYOL shows better performance but may require more memory for momentum encoder
  - λC constant vs progressive: Constant values are simpler but progressive values better balance learning objectives over time
  - Replay buffer size: Larger buffers improve retention but increase memory requirements and computational cost

- Failure signatures:
  - Sudden accuracy drop on previous tasks after new task training → insufficient knowledge distillation
  - Consistently low performance on new tasks → excessive knowledge retention weighting (high λC)
  - Performance plateau early → transformation functions not diverse enough or SSL framework not effective

- First 3 experiments:
  1. Implement TPN feature extractor with WISDM2019 preprocessing and verify baseline accuracy on individual tasks
  2. Add BYOL with random transformations and evaluate representation quality without continual learning
  3. Implement Kaizen with λC = 1.0 and evaluate on WISDM2019 with 6-task split, comparing against CaSSLe and No Distill baselines

## Open Questions the Paper Calls Out

The paper does not explicitly call out open questions, but based on the discussion, several questions remain unaddressed:
- How does the performance of Kaizen and CaSSLe compare when applied to larger HAR datasets with more activities and users?
- What is the impact of different task orderings on catastrophic forgetting and knowledge retention in Kaizen?
- How does Kaizen perform in online continual learning settings where data arrives in streaming fashion rather than pre-segmented tasks?

## Limitations

- The paper's claims are based on experiments with a single dataset (WISDM2019) and a specific task split
- The adaptive weighting mechanism (λC = a ⊕ b) lacks extensive ablation studies to determine optimal parameters
- Transformation functions may not generalize well to other HAR datasets with different sensor modalities or sampling rates
- The study does not address computational overhead of maintaining functional classifiers throughout the continual learning process

## Confidence

- **High Confidence**: The general superiority of Kaizen over CaSSLe and No Distill baselines (58.8% vs 49.0% average continual accuracy)
- **Medium Confidence**: The effectiveness of progressive importance coefficients for balancing knowledge retention and new task learning
- **Medium Confidence**: The adaptation of SSL methods (BYOL, MoCoV2+) for HAR knowledge retention
- **Low Confidence**: The claim that these methods will generalize to other HAR datasets or sensor modalities

## Next Checks

1. Test Kaizen on multiple HAR datasets (e.g., PAMAP2, Opportunity) with varying class distributions and sensor modalities to assess generalization
2. Conduct extensive ablation studies on the adaptive weighting mechanism to determine optimal parameters and sensitivity to hyperparameter choices
3. Measure and report the computational overhead of maintaining functional classifiers throughout the continual learning process, including memory usage and inference latency