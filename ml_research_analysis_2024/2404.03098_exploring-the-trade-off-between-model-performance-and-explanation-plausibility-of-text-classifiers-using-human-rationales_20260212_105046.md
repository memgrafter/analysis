---
ver: rpa2
title: Exploring the Trade-off Between Model Performance and Explanation Plausibility
  of Text Classifiers Using Human Rationales
arxiv_id: '2404.03098'
source_url: https://arxiv.org/abs/2404.03098
tags:
- rationales
- hatexplain
- plausibility
- performance
- explanations
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work presents a method to enhance the plausibility of post-hoc
  explanations for text classification models by incorporating human rationales into
  the training process. The approach uses a contrastive-inspired loss function that
  encourages the model to base its predictions on the provided rationales, and explores
  the trade-off between model performance and explanation plausibility using a multi-objective
  optimization framework.
---

# Exploring the Trade-off Between Model Performance and Explanation Plausibility of Text Classifiers Using Human Rationales

## Quick Facts
- arXiv ID: 2404.03098
- Source URL: https://arxiv.org/abs/2404.03098
- Authors: Lucas E. Resck; Marcos M. Raimundo; Jorge Poco
- Reference count: 40
- Primary result: Incorporating human rationales into training significantly improves post-hoc explanation plausibility (AUPRC) without substantial performance degradation

## Executive Summary
This paper presents a method to enhance the plausibility of post-hoc explanations for text classification models by incorporating human rationales into the training process. The approach uses a contrastive-inspired loss function that encourages the model to base its predictions on the provided rationales, and explores the trade-off between model performance and explanation plausibility using a multi-objective optimization framework. Experiments across diverse models, datasets, and explainability methods show that the proposed approach significantly improves explanation plausibility without causing substantial degradation in model performance, particularly for samples with originally poor explanations.

## Method Summary
The method augments standard cross-entropy loss with a novel contrastive-inspired loss function using human rationales. During training, the model learns to focus on rationales by replacing full text samples with rationale samples in the loss calculation. Negative rationales (random tokens) are included to help the model distinguish relevant from irrelevant tokens. The trade-off between model performance and explanation plausibility is explored using a multi-objective optimization framework (NISE algorithm) that generates a Pareto-optimal frontier of models by varying the weights between cross-entropy and contrastive rationale losses.

## Key Results
- AUPRC (explanation plausibility) improved by 3.3-5.3% for samples with originally poor explanations
- Model performance degradation was negligible (less than 1% accuracy drop)
- The trade-off between performance and plausibility was effectively explored using NISE across multiple models and datasets
- Improvements were consistent across different explainability methods (LIME and SHAP) and model architectures (DistilBERT, BERT-Mini, TF-IDF + Logistic Regression)

## Why This Works (Mechanism)

### Mechanism 1
Incorporating human rationales as negative examples in contrastive learning reduces the model's reliance on irrelevant tokens. The contrastive rationale loss replaces full text samples with rationales during training, forcing the model to focus only on the provided rationales. By including negative rationales (random tokens) in the denominator, the model learns to distinguish between relevant and irrelevant tokens.

### Mechanism 2
Multi-objective optimization effectively balances model performance and explanation plausibility without requiring hyperparameter tuning. The weighted sum method combines cross-entropy and contrastive rationale loss with weights w1 and w2. NISE samples representative sets of these weights to generate a Pareto-optimal frontier of models.

### Mechanism 3
Using rationales improves explanation plausibility specifically for samples with originally poor explanations. By focusing the model's reasoning on rationales, the approach improves the quality of explanations for samples that initially had low plausibility scores (AUPRC < 1).

## Foundational Learning

- Concept: Contrastive learning
  - Why needed here: The method uses a contrastive-inspired loss function that compares rationales with negative examples to improve explanation plausibility.
  - Quick check question: How does contrastive learning differ from traditional supervised learning, and why is it suitable for incorporating rationales?

- Concept: Multi-objective optimization
  - Why needed here: The approach explores the trade-off between model performance and explanation plausibility using a weighted sum method and NISE solver.
  - Quick check question: What is the difference between Pareto-optimality and weighted sum optimization, and when is each appropriate?

- Concept: Explainability metrics (AUPRC, AOPC)
  - Why needed here: The method evaluates explanation quality using plausibility (AUPRC) and faithfulness (AOPC) metrics.
  - Quick check question: How do AUPRC and AOPC differ in measuring explanation quality, and what are their limitations?

## Architecture Onboarding

- Component map:
  Text classifier (DistilBERT, BERT-Mini, or TF-IDF + Logistic Regression) -> Explanation function (LIME or SHAP) -> Rationale dataset (HateXplain, TSE, or Movie Reviews) -> Multi-objective optimizer (NISE) -> Contrastive rationale loss function

- Critical path:
  1. Preprocess datasets and rationales
  2. Train baseline model with cross-entropy loss
  3. Implement contrastive rationale loss
  4. Run NISE to explore trade-off between losses
  5. Evaluate performance and plausibility on test data

- Design tradeoffs:
  Using only the classification layer vs. fine-tuning entire model
  Number of negative rationales to include
  Choice of explanation method (LIME vs. SHAP)

- Failure signatures:
  Performance degradation without plausibility improvement
  Instability in Pareto frontier shape
  Poor explanation quality despite rationale incorporation

- First 3 experiments:
  1. Implement baseline model (DistilBERT + cross-entropy) on HateXplain and evaluate AUPRC
  2. Add contrastive rationale loss with 2 negative rationales and run NISE
  3. Compare performance and plausibility trade-off with and without rationales

## Open Questions the Paper Calls Out

### Open Question 1
How does the model performance and explanation plausibility trade-off vary across different explainability methods beyond LIME and SHAP? The paper mentions that the approach is agnostic to explainability methods and shows results with LIME and SHAP, but acknowledges the need for further exploration.

### Open Question 2
What is the impact of the number of negative rationales on the trade-off between model performance and explanation plausibility? The paper tests with 2 and 5 negative rationales but notes the need for further investigation into the optimal number.

### Open Question 3
How does the proposed approach perform on out-of-distribution (OOD) data, and does it improve OOD generalization? The paper includes a brief experiment on OOD performance with HateXplain and HatEval datasets, showing some improvement in plausibility and OOD accuracy.

## Limitations
- Method's effectiveness depends on availability of human rationales, which may not exist for many real-world applications
- Assumes LIME/SHAP explanations are faithful representations of model decision-making, which may not hold in practice
- Requires careful tuning of loss weight parameters to achieve desired trade-off

## Confidence

- High confidence: The method significantly improves explanation plausibility for samples with originally poor explanations (AUPRC < 1)
- Medium confidence: The overall approach maintains model performance while improving explanation quality
- Medium confidence: The multi-objective optimization framework effectively explores the trade-off between performance and plausibility

## Next Checks

1. Conduct a human evaluation study to verify that improved explanations (higher AUPRC) are actually faithful to the model's decision-making process, not just plausible

2. Test the approach on datasets without human rationales by using automatically generated rationales to assess whether benefits transfer beyond human-annotated data

3. Implement an ablation study comparing the full method against baseline models in a realistic deployment scenario where users must act on the explanations, measuring both task performance and user trust/understanding