---
ver: rpa2
title: Learning Translations via Matrix Completion
arxiv_id: '2406.13195'
source_url: https://arxiv.org/abs/2406.13195
tags:
- translations
- word
- words
- translation
- bilingual
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "The paper addresses Bilingual Lexicon Induction (BLI), the task\
  \ of learning word translations without parallel corpora, by modeling it as a matrix\
  \ completion problem using Bayesian Personalized Ranking (BPR). The core method\
  \ integrates diverse bilingual and monolingual signals\u2014such as Wikipedia interlanguage\
  \ links, crowdsourced dictionaries, bilingually-informed word embeddings, and visual\
  \ representations\u2014within a matrix factorization framework to infer missing\
  \ translations."
---

# Learning Translations via Matrix Completion

## Quick Facts
- arXiv ID: 2406.13195
- Source URL: https://arxiv.org/abs/2406.13195
- Reference count: 40
- Primary result: Achieved top-10 accuracy (Acc10) of up to 87.2% on benchmark datasets, significantly outperforming state-of-the-art for bilingual lexicon induction across 27 languages.

## Executive Summary
This paper introduces a novel approach to Bilingual Lexicon Induction (BLI) by modeling it as a matrix completion problem using Bayesian Personalized Ranking (BPR). The method integrates diverse bilingual and monolingual signals—including Wikipedia interlanguage links, crowdsourced dictionaries, bilingually-informed word embeddings, and visual representations—within a matrix factorization framework to infer missing translations. Experiments on 27 languages show that the model significantly outperforms existing state-of-the-art methods, with accuracy improvements of up to 12.4% on the VULIC1000 benchmark. The approach is extensible and demonstrates consistent improvements as more and better signals are incorporated.

## Method Summary
The method frames BLI as matrix completion, using observed translations as a sparse matrix to learn low-rank latent representations via matrix factorization with Bayesian Personalized Ranking (BPR). For words with few or no observed translations ("cold words"), auxiliary signals like monolingual embeddings and visual features are used to predict translations. The model employs a back-off strategy: if a source word has sufficient observed translations, MF is used; otherwise, auxiliary features are leveraged. Third-language bridging (e.g., via Wikipedia interlanguage links) enriches the bilingual dictionary. The approach is evaluated on 27 languages using top-10 accuracy (Acc10) on benchmark datasets.

## Key Results
- Achieved Acc10 of up to 87.2% on VULIC1000, significantly outperforming the state-of-the-art.
- Incorporating auxiliary signals (embeddings, visuals) improved accuracy for low-resource languages.
- Third-language bridging (Wikipedia + crowdsourced dictionaries) consistently boosted performance across languages.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Matrix factorization with BPR learns translation probabilities from incomplete bilingual dictionaries.
- Mechanism: The observed translations form a sparse matrix; MF approximates it as a product of two low-rank matrices. BPR optimizes the difference between scores for observed (positive) translations and unobserved ones, treating the absence of a translation as "negative" in ranking.
- Core assumption: Positive-only training data can be used for ranking because unobserved entries are more likely negative than positive.
- Evidence anchors:
  - [abstract]: "model this task as a matrix completion problem... integrating diverse bilingual and monolingual signals."
  - [section]: "the training data in the matrix consists only of positive translations... BPR learns to optimize the difference between values assigned to the observed translations and values assigned to the unobserved translations."
  - [corpus]: No direct evidence; corpus signals are weak for this mechanism.
- Break condition: If negative examples are incorrectly sampled or too many unobserved pairs are actually valid translations, ranking accuracy will degrade.

### Mechanism 2
- Claim: Auxiliary features (bilingually-informed embeddings, visual representations) improve predictions for "cold" source words with few or no observed translations.
- Mechanism: For words lacking bilingual dictionary entries, translation scores are computed via auxiliary feature vectors. The model learns weights that map these features to likely translations, then blends these predictions with MF-based scores.
- Core assumption: Language-independent features (e.g., images, embeddings) are predictive of translation equivalence even without parallel data.
- Evidence anchors:
  - [section]: "additional information, e.g., monolingual signals of translation equivalence... must be used" and "auxiliary features about the words can be used to predict translations for cold words."
  - [corpus]: No direct evidence; corpus signals are weak for this mechanism.
- Break condition: If auxiliary features are noisy or not aligned across languages, the predictions for cold words become unreliable.

### Mechanism 3
- Claim: Integrating third-language signals (e.g., Wikipedia interlanguage links) enriches the bilingual dictionary and boosts translation accuracy.
- Mechanism: Translations from a source language to a third language, plus translations from that third language to English, are projected into the source-to-English matrix, filling in missing entries.
- Core assumption: Third-language translations are accurate enough to serve as a bridge, and the projection does not introduce too much noise.
- Evidence anchors:
  - [section]: "We use bilingual translations from the source to the target language, English, obtained from Wikipedia page titles with interlanguage links... we also use high accuracy, crowdsourced translations... from these third languages to English."
  - [corpus]: No direct evidence; corpus signals are weak for this mechanism.
- Break condition: If third-language dictionaries are sparse or inaccurate, the projected translations degrade model performance.

## Foundational Learning

- Concept: Matrix factorization (MF)
  - Why needed here: MF compresses the large, sparse translation matrix into low-rank latent feature matrices, making it tractable to infer missing translations.
  - Quick check question: If you have a 10,000 x 10,000 translation matrix with only 1% observed entries, how many latent dimensions would you choose for MF, and why?
- Concept: Bayesian Personalized Ranking (BPR)
  - Why needed here: BPR is designed for implicit feedback where only positive interactions are observed; it learns to rank likely translations higher than unobserved ones.
  - Quick check question: What is the difference between BPR's objective and a standard pairwise ranking loss in supervised learning?
- Concept: Cold-start problem
  - Why needed here: Many source words lack any dictionary translations, so standard MF cannot estimate their latent vectors; auxiliary signals are needed.
  - Quick check question: How does the cold-start problem in recommender systems relate to Bilingual Lexicon Induction?

## Architecture Onboarding

- Component map: Input (observed translations, monolingual embeddings, visual features, Wikipedia interlanguage links) -> Core (MF with BPR: target word features, source word features) -> Auxiliary (cold-word predictor using embeddings/visuals) -> Integration (back-off scheme) -> Output (ranked translations per source word)
- Critical path:
  1. Load observed translations and seed dictionary.
  2. Train MF with BPR on observed translations.
  3. For cold words, train auxiliary predictor on embeddings/visuals.
  4. Predict translations, applying back-off.
  5. Evaluate Acc10 on test sets.
- Design tradeoffs:
  - Using more auxiliary features improves coverage but risks noise.
  - Third-language bridging increases data but can introduce projection errors.
  - Higher rank in MF can capture more nuance but risks overfitting on sparse data.
- Failure signatures:
  - Low accuracy for cold words → auxiliary features are poor or missing.
  - Overall accuracy drops with third-language bridging → third-language translations are noisy or misaligned.
  - Training instability → learning rate or regularization in BPR is mis-tuned.
- First 3 experiments:
  1. Train BPR MF on WIKI translations only, evaluate Acc10 on VULIC1000.
  2. Add auxiliary embeddings for cold words, compare with baseline.
  3. Integrate third-language bridging (WIKI+CROWD), measure improvement on low-resource languages.

## Open Questions the Paper Calls Out

- Open Question 1: How does the choice of tokenization method influence the accuracy of bilingual lexicon induction across languages?
  - Basis in paper: [inferred] The paper notes that a simple regular-expression based tokenizer was used for many languages, and mentions potential influence on performance for languages like Vietnamese despite its large Wikipedia corpus.
  - Why unresolved: The paper does not systematically investigate the impact of different tokenization methods on BLI accuracy across various languages, especially those with complex morphology or orthography.
  - What evidence would resolve it: Conducting controlled experiments comparing BLI performance using different tokenization methods (e.g., language-specific, morphological) for a diverse set of languages, and correlating tokenization quality with BLI accuracy.

- Open Question 2: What is the optimal size and composition of seed translation lexicons for learning bilingual word embeddings, and how do these factors interact with the quality of the test set?
  - Basis in paper: [explicit] The paper finds that a seed lexicon size of 5K is sufficient for optimal performance, but also notes that test set size and seed translation quality may be relevant.
  - Why unresolved: The paper does not explore the relationship between seed lexicon size, seed translation quality, and test set characteristics (e.g., size, domain, POS distribution) in detail.
  - What evidence would resolve it: Designing experiments that vary seed lexicon size, composition (e.g., MNN vs. crowdsourced), and test set properties to determine their combined impact on BLI accuracy, potentially using statistical modeling to capture interactions.

- Open Question 3: How does the use of visual representations as auxiliary features affect the robustness of bilingual lexicon induction to noisy or sparse textual data, and what are the limitations of current visual feature extraction methods?
  - Basis in paper: [explicit] The paper incorporates visual representations as auxiliary features and shows improved performance, but notes that accuracies on the image corpus test set are lower overall and suggests using more images per word.
  - Why unresolved: The paper does not thoroughly analyze the conditions under which visual features are most beneficial (e.g., low-resource languages, noisy text) or investigate the limitations of current CNN-based visual feature extraction methods for BLI.
  - What evidence would resolve it: Conducting experiments that systematically vary the amount and quality of textual data while measuring the impact of visual features on BLI accuracy, and comparing the performance of different visual feature extraction methods (e.g., object recognition, scene understanding) for BLI.

## Limitations
- Performance drops significantly for truly low-resource languages due to sparse seed translations.
- Third-language bridging introduces projection noise that is not quantified.
- Quality of auxiliary features (embeddings, visuals) is assumed but not empirically validated for cold-start cases.

## Confidence
- Matrix completion + BPR framework: High (strong empirical results across 27 languages)
- Cold-start auxiliary signal integration: Medium (results show gains but noise sensitivity is unclear)
- Third-language bridging mechanism: Medium-Low (performance boost is reported but sensitivity to bridge quality is not explored)

## Next Checks
1. Benchmark model accuracy as a function of seed translation size to quantify low-resource limits.
2. Ablate third-language signals to measure the impact of bridge noise.
3. Evaluate cold-word predictions using only auxiliary features (no MF) to isolate their predictive power.