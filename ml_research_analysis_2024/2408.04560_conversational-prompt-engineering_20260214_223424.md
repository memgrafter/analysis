---
ver: rpa2
title: Conversational Prompt Engineering
arxiv_id: '2408.04560'
source_url: https://arxiv.org/abs/2408.04560
tags:
- user
- prompt
- prompts
- outputs
- instruction
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Conversational Prompt Engineering (CPE),
  a user-friendly system that helps users create personalized prompts for large language
  models through interactive chat. CPE eliminates the need for labeled data and initial
  prompts by analyzing unlabeled user data and engaging users in a brief conversation
  to understand their output preferences.
---

# Conversational Prompt Engineering

## Quick Facts
- arXiv ID: 2408.04560
- Source URL: https://arxiv.org/abs/2408.04560
- Authors: Liat Ein-Dor; Orith Toledo-Ronen; Artem Spector; Shai Gretz; Lena Dankin; Alon Halfon; Yoav Katz; Noam Slonim
- Reference count: 7
- Primary result: Conversational Prompt Engineering (CPE) enables users to create personalized prompts through interactive chat without requiring labeled data or initial prompts.

## Executive Summary
Conversational Prompt Engineering (CPE) is a novel system that allows users to create personalized prompts for large language models through interactive conversation. The system analyzes unlabeled user data to generate data-specific questions, then iteratively refines the instruction based on user feedback. CPE eliminates the need for labeled data and initial prompts, making prompt engineering accessible to non-expert users. A user study on summarization tasks demonstrated that CPE-generated prompts produce outputs that users prefer over baseline prompts, with the zero-shot prompt performing comparably to its longer few-shot counterpart.

## Method Summary
CPE implements a three-party chat system where a system model interacts with users and a model (Llama-3-70B) to create personalized prompts. The process begins with users uploading unlabeled data, which the system analyzes to generate data-specific questions. Through iterative conversation, the system refines the instruction based on user feedback on generated outputs. The conversation continues until the user approves all outputs, at which point the system creates a few-shot prompt with the approved outputs as examples. The system employs context filtering to manage conversation length and uses Chain-of-Thought reasoning for complex instruction refinement tasks.

## Key Results
- CPE successfully generates effective prompts without requiring labeled data or initial prompts
- Zero-shot prompts from CPE perform comparably to few-shot counterparts, suggesting significant efficiency gains
- Users preferred CPE-generated summaries over baseline prompts in evaluation study
- Average conversation required 32±9.6 turns to reach convergence

## Why This Works (Mechanism)

### Mechanism 1
- Claim: CPE achieves high-quality prompt generation without labeled data by leveraging user data analysis and conversational refinement.
- Mechanism: The system first analyzes unlabeled user data to generate data-specific questions, then iteratively refines the instruction through user feedback on generated outputs.
- Core assumption: User preferences can be effectively elicited through structured conversation rather than direct labeling.
- Evidence anchors:
  - [abstract] "uses user-provided unlabeled data to generate data-driven questions and utilize user responses to shape the initial instruction"
  - [section] "users frequently struggle to clearly define their task and their precise expectations from the model responses"
  - [corpus] Weak - corpus shows related work on prompt optimization but lacks direct evidence for unlabeled data approach
- Break condition: The mechanism breaks if users cannot articulate preferences even with guided questions, or if the unlabeled data is too sparse to generate meaningful insights.

### Mechanism 2
- Claim: Conversational refinement allows the system to capture nuanced user preferences that would be difficult to specify in a static prompt.
- Mechanism: The chat-based interface enables dynamic exploration of user requirements, with the system asking follow-up questions based on user responses and generating sample outputs for feedback.
- Core assumption: Users can better identify what they want when shown concrete examples rather than abstract descriptions.
- Evidence anchors:
  - [abstract] "user feedback on specific model-generated outputs can be leveraged not only to improve the outputs themselves, but also to refine the instruction"
  - [section] "The user provides feedback on the prompt outputs. If the user approves them as they are, no further refinements are needed"
  - [corpus] Moderate - related work on human-in-the-loop prompt refinement supports this approach
- Break condition: This mechanism fails when users lack clear preferences or when feedback becomes inconsistent across iterations.

### Mechanism 3
- Claim: CPE can generate effective prompts with minimal user input time by converging on instructions that produce acceptable outputs.
- Mechanism: The system iteratively refines both the instruction and outputs, with the process ending when the user approves all generated outputs, creating a few-shot prompt.
- Core assumption: Acceptable outputs can be generated with relatively few refinement cycles.
- Evidence anchors:
  - [abstract] "The results suggest that the zero-shot prompt obtained is comparable to its – much longer – few-shot counterpart"
  - [section] "On average, these chats required 32(±9.6) turns to reach convergence"
  - [corpus] Moderate - user study shows average 25 minutes to convergence, supporting efficiency claims
- Break condition: This mechanism breaks if convergence requires excessive iterations (making it impractical) or if the final prompt quality degrades significantly.

## Foundational Learning

- Concept: Few-shot prompting vs zero-shot prompting
  - Why needed here: CPE generates both zero-shot and few-shot versions, and understanding the difference is crucial for evaluating its effectiveness
  - Quick check question: What is the key difference between few-shot and zero-shot prompting, and why might few-shot prompts sometimes perform better?

- Concept: Context management in LLM interactions
  - Why needed here: CPE uses sophisticated context filtering to manage conversation flow and stay within token limits
  - Quick check question: Why is context filtering important in CPE, and how does it enable the system to handle longer conversations?

- Concept: Chain-of-Thought (CoT) prompting
  - Why needed here: CPE uses CoT in side-chats for complex reasoning tasks like instruction refinement
  - Quick check question: How does Chain-of-Thought prompting help CPE handle complex instruction refinement tasks?

## Architecture Onboarding

- Component map: User interface (chat, survey, evaluation modules) → System orchestrator → LLM model (Llama-3-70B) → Target model (user's choice)
- Critical path: User data upload → Initial instruction generation → Instruction refinement → Output generation → User feedback → Final prompt creation
- Design tradeoffs: Flexibility vs. guidance (open-ended chat vs. structured API calls), context length vs. detail (filtering mechanism), few-shot examples vs. token efficiency
- Failure signatures: User confusion about the process, excessive iteration cycles, model context overflow, inconsistent feedback patterns
- First 3 experiments:
  1. Test basic instruction generation with simple summarization task using 3 examples
  2. Evaluate context filtering by attempting to handle a longer conversation
  3. Test output refinement cycle with a user who provides contradictory feedback

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the prompts created by CPE be further improved by using them as initial prompts for existing automatic prompt optimization methods?
- Basis in paper: Explicit - Mentioned in the "Discussion" section as an interesting open question.
- Why unresolved: The paper suggests this as a potential direction for future work but does not explore it.
- What evidence would resolve it: Experiments comparing the performance of prompts optimized by CPE alone versus those further optimized using automatic PE methods.

### Open Question 2
- Question: How effective is CPE in tasks beyond summarization, such as topic generation, advertising content creation, or creative writing?
- Basis in paper: Explicit - Mentioned in the "Discussion" section as a potential direction for future work.
- Why unresolved: The user study only focused on summarization tasks due to limited annotators.
- What evidence would resolve it: User studies on CPE's effectiveness for various other tasks mentioned in the paper.

### Open Question 3
- Question: Can CPE assist users in planning and creating agentic workflows for LLM-empowered systems?
- Basis in paper: Explicit - Suggested in the "Discussion" section as a potential extension of CPE's application.
- Why unresolved: The paper does not explore this possibility and focuses solely on prompt generation.
- What evidence would resolve it: Experiments demonstrating CPE's ability to help users plan and create agentic workflows.

### Open Question 4
- Question: How can the convergence time of CPE be improved, especially for complex tasks with detailed user requirements?
- Basis in paper: Explicit - Mentioned in the "Results" section, noting that participants rated convergence time lower.
- Why unresolved: The paper acknowledges the issue but does not provide a solution.
- What evidence would resolve it: Comparative studies on convergence time with different task complexities and potential improvements to the CPE algorithm.

### Open Question 5
- Question: How does the quality of CPE-generated prompts compare to those created by expert prompt engineers?
- Basis in paper: Inferred - While the user study showed CPE's effectiveness, it doesn't compare it to expert-generated prompts.
- Why unresolved: The study focused on user preferences rather than expert benchmarks.
- What evidence would resolve it: A comparison between CPE-generated prompts and those created by expert prompt engineers using standard evaluation metrics.

## Limitations

- Evaluation limited to summarization tasks, may not generalize to other domains
- Reliance on a single LLM model (Llama-3-70B) limits generalizability
- Effectiveness of unlabeled data analysis for generating meaningful questions remains uncertain

## Confidence

| Claim | Confidence |
|-------|------------|
| CPE generates effective prompts without labeled data | Medium |
| Zero-shot prompts perform comparably to few-shot counterparts | Medium |
| Conversational refinement captures nuanced user preferences | Medium |

## Next Checks

1. **Cross-task validation**: Test CPE's effectiveness across diverse NLP tasks (e.g., translation, question answering, creative writing) to assess generalizability beyond summarization.

2. **Quantitative performance benchmarking**: Implement automated evaluation metrics (ROUGE, BLEU, human evaluation scales) to complement user preference studies and provide more objective quality measures.

3. **Scalability assessment**: Evaluate CPE's performance with varying dataset sizes and complexities, particularly testing the unlabeled data analysis mechanism's robustness with sparse or noisy inputs.