---
ver: rpa2
title: 'The AI Double Standard: Humans Judge All AIs for the Actions of One'
arxiv_id: '2412.06040'
source_url: https://arxiv.org/abs/2412.06040
tags:
- moral
- agent
- human
- agency
- humans
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces the concept of moral spillover in human-AI
  interaction, showing that immoral actions by one AI agent negatively affect perceptions
  of moral agency and patiency for all AIs. In two experiments (N = 720 and N = 684),
  participants read about either a chatbot or human assistant committing an immoral
  or morally neutral act.
---

# The AI Double Standard: Humans Judge All AIs for the Actions of One

## Quick Facts
- arXiv ID: 2412.06040
- Source URL: https://arxiv.org/abs/2412.06040
- Authors: Aikaterina Manoli; Janet V. T. Pauketat; Jacy Reese Anthis
- Reference count: 40
- Primary result: Immoral actions by one AI agent negatively affect perceptions of moral agency and patiency for all AIs, showing a double standard where AIs are judged more harshly than humans for individual transgressions

## Executive Summary
This paper introduces the concept of moral spillover in human-AI interaction, demonstrating that immoral actions by one AI agent negatively affect perceptions of moral agency and patiency for all AIs. In two experiments (N = 720 and N = 684), participants read about either a chatbot or human assistant committing an immoral or morally neutral act. Results showed that immoral actions increased attributions of negative moral agency and decreased attributions of positive moral agency and moral patiency to both the agent and their group. Importantly, in Study 2, spillover persisted for AIs but not for humans when groups were broadened from assistants to all AIs/humans, indicating that AIs are judged more harshly than humans for individual transgressions.

## Method Summary
Two online vignette experiments were conducted with U.S. participants recruited from Prolific. Study 1 used a 2x2 between-subjects design with agent type (human, AI) and action valence (immoral, morally neutral) as independent variables, measuring moral attributions to agent and congruent group. Study 2 added an individuating name "Ezal" to the agent and broadened group labels to "all humans" and "all AIs." Participants read vignettes describing workplace actions and completed scales measuring negative moral agency, positive moral agency, and moral patiency. Data were analyzed using preregistered 2x2 ANOVAs.

## Key Results
- Immoral actions increased attributions of negative moral agency and decreased attributions of positive moral agency and moral patiency to both the agent and their group
- In Study 2, spillover persisted in the AI context but not in the human context when groups were broadened to all AIs/humans
- When the agent was individuated with a name and described as an AI or human rather than specifically as a chatbot or personal assistant, spillover patterns differed

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Moral spillover occurs because people treat AIs as a homogeneous outgroup.
- Mechanism: People perceive AIs as a single, undifferentiated category, so one AI's action signals all AIs are similarly capable. This reduces threshold similarity needed for spillover.
- Core assumption: AIs are not differentiated in public perception as much as humans are.
- Evidence anchors:
  - [abstract] "spillover persisted in the AI context but not in the human context, possibly because AIs were perceived as more homogeneous due to their outgroup status relative to humans."
  - [section] "people might readily and persistently categorize all AIs as one group of similar, homogeneous entities—and thus all are affected by one AI's actions."
- Break condition: If AI agents become widely differentiated in public understanding, spillover effect may diminish.

### Mechanism 2
- Claim: Moral spillover is amplified by perceived competence expectations of AI.
- Mechanism: Humans expect AIs to be highly competent and less prone to moral error; a transgression violates this schema and is thus more shocking, increasing spillover to the group.
- Core assumption: Public expects AI to outperform humans morally.
- Evidence anchors:
  - [abstract] "people judge all AIs for the actions of one" implies harsher judgment on group.
  - [section] "people also expect AIs to be more competent and to act less immorally than humans."
- Break condition: If AI competence expectations decline or become more realistic, spillover magnitude may reduce.

### Mechanism 3
- Claim: Individualizing an AI agent reduces spillover from that agent to its group.
- Mechanism: Naming and describing an AI in general terms reduces perceived similarity between agent and group, limiting spillover.
- Core assumption: Similarity threshold is needed for spillover to occur.
- Evidence anchors:
  - [abstract] "when the agent was individuated with a name ('Ezal') may have further thus reduced the similarity between the human agent and the human group."
  - [section] "individuating the agent with a name ('Ezal') may have further thus reduced the similarity between the human agent and the human group."
- Break condition: If individuation cues are removed or minimized, spillover may reoccur.

## Foundational Learning

- Concept: Moral agency and patiency
  - Why needed here: Understanding how people attribute moral responsibility and moral consideration to AIs is essential to grasp spillover effects.
  - Quick check question: If an AI causes harm, what moral attribution is likely to increase first—negative moral agency or moral patiency?

- Concept: Outgroup homogeneity
  - Why needed here: Explains why AIs are treated as a single category, triggering spillover.
  - Quick check question: How does the perception of humans as a heterogeneous group reduce moral spillover among humans?

- Concept: Algorithmic aversion
  - Why needed here: Shows how AI failures generalize to distrust in all AI systems, paralleling moral spillover.
  - Quick check question: What behavioral bias explains people's tendency to distrust AI more after one error?

## Architecture Onboarding

- Component map: Vignette creation -> Participant randomization (AI/human × moral/immoral) -> Measurement (moral agency, patiency scales) -> Analysis (2x2 ANOVA, spillover tests) -> Interpretation (mechanistic reasoning)
- Critical path: Vignette -> Manipulation check -> Dependent variable measurement -> Statistical test -> Comparison across studies
- Design tradeoffs: Narrow vs. broad group labels (chatbots vs. AIs) balances ecological validity and control of similarity; adding agent names increases realism but may alter perceived similarity
- Failure signatures: No main effect of valence -> manipulation ineffective; no spillover interaction -> similarity threshold not met; agent-type valence interaction -> unexpected bias
- First 3 experiments:
  1. Replicate Study 1 with broader group labels to confirm spillover to more general categories
  2. Test individuating AI agents with different names to see if spillover reduces systematically
  3. Vary competence expectations in vignettes to measure effect on spillover magnitude

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What specific psychological mechanisms underlie moral spillover in human-AI interaction, and how do they differ from those in human-human interaction?
- Basis in paper: [inferred] The authors speculate about differences in similarity perception between humans and AIs, noting that humans may perceive AIs as more homogeneous due to outgroup status, while humans perceive their own group as heterogeneous.
- Why unresolved: The paper acknowledges this as speculation without directly testing the mechanisms through which moral spillover occurs.
- What evidence would resolve it: Experimental studies systematically manipulating factors like similarity, familiarity, and expectations for AI behavior, combined with psychological measures of group perception and attribution processes.

### Open Question 2
- Question: How can AI system designers mitigate the negative effects of moral spillover on user trust and interaction quality?
- Basis in paper: [explicit] The authors discuss the need for careful AI design to minimize negative perceptions, suggesting strategies like transparency about AI limitations, AI apologies for mistakes, and clear differentiation between AI types.
- Why unresolved: The paper presents these as suggestions rather than empirically tested interventions, and does not explore which strategies are most effective.
- What evidence would resolve it: Longitudinal studies testing various design interventions in real-world or controlled settings, measuring their impact on trust, user behavior, and spillover effects over time.

### Open Question 3
- Question: Does moral spillover from one AI to all AIs persist over time, or can positive interactions with other AIs reverse negative perceptions?
- Basis in paper: [inferred] The authors note the need for research on whether spillover effects persist and whether positive AI interactions can counteract negative ones, but do not address temporal dynamics.
- Why unresolved: The studies are cross-sectional and do not examine how perceptions evolve or whether they can be repaired after negative experiences.
- What evidence would resolve it: Longitudinal studies tracking user perceptions of AIs over extended periods, including exposure to both negative and positive AI interactions, to determine the durability of spillover effects and potential for perception reversal.

## Limitations

- The core spillover mechanism relies on AI outgroup homogeneity, but evidence for this is indirect without explicit similarity measurement
- The agent individuation manipulation may have reduced spillover through mechanisms other than similarity reduction, such as increased cognitive distance
- Competence expectation mechanism remains largely theoretical without direct measurement

## Confidence

- **High confidence**: Moral spillover exists and affects AI more than human groups (supported by robust 2x2 ANOVA interactions)
- **Medium confidence**: Spillover mechanism operates through outgroup homogeneity (supported by theoretical reasoning and Study 2 patterns, but lacks direct measurement)
- **Medium confidence**: AI competence expectations amplify spillover (plausible but untested directly)

## Next Checks

1. **Direct similarity measurement**: Add explicit pre- and post-vignette questions about perceived similarity between different AI agents to test the homogeneity assumption
2. **Competence expectation manipulation**: Design vignettes that systematically vary competence expectations (e.g., describing AI as "state-of-the-art" vs. "basic") to measure direct effects on spillover magnitude
3. **Longitudinal validation**: Track how spillover effects change over time with repeated AI transgressions in controlled experimental sequences to assess persistence and adaptation