---
ver: rpa2
title: Faster Q-Learning Algorithms for Restless Bandits
arxiv_id: '2409.05908'
source_url: https://arxiv.org/abs/2409.05908
tags:
- learning
- q-learning
- index
- algorithm
- convergence
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper studies Q-learning and its faster variants for restless
  multi-armed bandits (RMAB) with Whittle index learning. The authors present classical
  Q-learning, speedy Q-learning (SQL), generalized speedy Q-learning (GSQL), and phase
  Q-learning (PhaseQL), combined with exploration policies including epsilon-greedy
  and UCB.
---

# Faster Q-Learning Algorithms for Restless Bandits

## Quick Facts
- arXiv ID: 2409.05908
- Source URL: https://arxiv.org/abs/2409.05908
- Reference count: 36
- Key outcome: PhaseQL with UCB exploration achieves the fastest convergence rate among all tested algorithms for Whittle index learning in restless multi-armed bandits

## Executive Summary
This paper studies Q-learning and its faster variants for restless multi-armed bandits (RMAB) with Whittle index learning. The authors present classical Q-learning, speedy Q-learning (SQL), generalized speedy Q-learning (GSQL), and phase Q-learning (PhaseQL), combined with exploration policies including epsilon-greedy and UCB. They propose a two-timescale stochastic approximation algorithm for index learning, updating Q-values on the faster timescale and indices on the slower timescale. Numerical experiments show that PhaseQL with UCB exploration achieves the fastest convergence rate among all tested algorithms, outperforming standard Q-learning and other variants in both standalone Q-learning tasks and Whittle index learning for RMAB.

## Method Summary
The paper studies Q-learning variants (classical Q-learning, SQL, GSQL, PhaseQL) for restless multi-armed bandits with Whittle index learning. The main approach uses a two-timescale stochastic approximation algorithm where Q-values are updated on a faster timescale assuming fixed index values, while index parameters are updated on a slower timescale based on differences in Q-values between actions. The methods are tested with both epsilon-greedy and UCB exploration policies. The algorithm is evaluated on a numerical example with 5 states, comparing convergence rates across different Q-learning variants and exploration strategies.

## Key Results
- PhaseQL with UCB exploration achieves the fastest convergence rate among all tested algorithms
- The two-timescale stochastic approximation framework enables efficient index learning by decoupling Q-value updates from index parameter updates
- UCB exploration policy accelerates convergence of Q-learning variants by more effectively balancing exploration and exploitation than epsilon-greedy

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: PhaseQL with UCB exploration achieves faster convergence than standard Q-learning and other Q-learning variants due to its use of multiple samples per state-action pair to estimate the next-state distribution.
- **Mechanism**: PhaseQL computes Q-value updates using an empirical average over m samples of the next state, reducing variance compared to single-sample updates in classical Q-learning. This allows more accurate value estimates per iteration, leading to faster convergence.
- **Core assumption**: The reward and transition dynamics are stationary, and sufficient samples (m) are drawn to approximate the true transition probabilities.
- **Evidence anchors**:
  - [abstract] states that "PhaseQL with UCB have fastest convergence rate" and that numerical experiments show this outperforms standard Q-learning.
  - [section] explains: "Qn+1(s, a) = r(s, a) + β 1/m Σ_i [V^n(s^i_n)]" and "The intuition for faster convergence... there are m samples of future state available... used in next estimate to form Q value from preceding estimate of Q function."
- **Break condition**: If m is too small, the variance reduction benefit diminishes, and convergence speed may not improve over classical Q-learning.

### Mechanism 2
- **Claim**: The two-timescale stochastic approximation framework enables efficient index learning for RMABs by decoupling Q-value updates (fast timescale) from index parameter updates (slow timescale).
- **Mechanism**: Q-values are updated frequently using current index estimates, while indices are updated slowly based on the difference in Q-values between actions. This allows stable learning of both the Q-function and the index simultaneously.
- **Core assumption**: The step-sizes satisfy α_n >> γ_n, ensuring Q-values converge faster than indices.
- **Evidence anchors**:
  - [abstract] states: "The algorithm of index learning is two-timescale variant of stochastic approximation, on slower timescale we update index learning scheme and on faster timescale we update Q-learning assuming fixed index value."
  - [section] provides the update equations: Q-value update (12) and index update (13), with constant stepsizes α and γ.
- **Break condition**: If γ is too large relative to α, the index updates may destabilize Q-learning convergence.

### Mechanism 3
- **Claim**: UCB exploration policy accelerates convergence of Q-learning variants by balancing exploration and exploitation more effectively than ε-greedy, especially in the early learning phase.
- **Mechanism**: UCB selects actions based on an upper confidence bound that favors less-visited state-action pairs, ensuring all pairs are explored sufficiently while exploiting known good actions. This reduces the risk of suboptimal policies during learning.
- **Core assumption**: The reward and transition functions are bounded, ensuring the UCB term remains well-defined.
- **Evidence anchors**:
  - [abstract] states: "Q-learning with UCB exploration policy has faster convergence" and "PhaseQL with UCB have fastest convergence rate."
  - [section] gives the UCB action selection rule and compares convergence rates in Fig. 1 and Fig. 2, showing UCB variants outperform ε-greedy.
- **Break condition**: If the confidence parameter c is poorly tuned, UCB may explore too much (slowing convergence) or too little (risking suboptimal convergence).

## Foundational Learning

- **Concept**: Two-timescale stochastic approximation
  - Why needed here: Enables simultaneous learning of Q-values (fast) and Whittle indices (slow) in RMAB index learning without destabilizing either process.
  - Quick check question: In a two-timescale update, which parameter (Q or index) should have the larger step-size for stability?

- **Concept**: Restless Multi-Armed Bandit (RMAB) and Whittle index relaxation
  - Why needed here: The paper applies Q-learning variants to learn Whittle indices for RMABs, so understanding the problem structure and index definition is essential.
  - Quick check question: What is the Whittle index for a state-action pair in an RMAB?

- **Concept**: Exploration-exploitation tradeoff (ε-greedy vs UCB)
  - Why needed here: The paper compares different exploration policies and shows UCB often leads to faster convergence; knowing how each works is critical to interpreting results.
  - Quick check question: How does the UCB action selection rule differ from ε-greedy in terms of exploration?

## Architecture Onboarding

- **Component map**:
  - Q-learning core (QL, SQL, GSQL, PhaseQL) -> Exploration module (ε-greedy or UCB) -> Index learning loop (two-timescale updates) -> RMAB simulation environment

- **Critical path**:
  1. Initialize Q-values and indices.
  2. For each episode and state, select action using exploration policy.
  3. Observe reward and next state; update Q-values (fast timescale).
  4. Periodically update indices based on Q-value differences (slow timescale).
  5. Check convergence; terminate if max Q-difference < threshold.

- **Design tradeoffs**:
  - Exploration policy: ε-greedy is simpler but may be slower; UCB is more adaptive but requires tuning.
  - Sample size m in PhaseQL: Larger m improves accuracy but increases per-step computation.
  - Step-sizes α and γ: Must satisfy α >> γ for two-timescale stability; tuning impacts convergence speed.

- **Failure signatures**:
  - Slow or no convergence: Check if step-sizes are mismatched or exploration is too greedy.
  - Unstable indices: Likely γ is too large; reduce index step-size.
  - High variance in Q-updates: May need larger m in PhaseQL or switch to UCB.

- **First 3 experiments**:
  1. Run PhaseQL with UCB on the five-state example and plot convergence curves for Q-error vs iteration.
  2. Compare convergence of PhaseQL-UCB vs Q-learning-UCB and vs PhaseQL-ε-greedy on the same RMAB instance.
  3. Vary the index step-size γ in the two-timescale algorithm and observe impact on convergence stability and speed.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do PhaseQL with UCB and PhaseQL with epsilon-greedy compare in terms of computational complexity and sample efficiency for large-scale RMAB problems?
- Basis in paper: [explicit] The paper states "PhaseQL (with UCB and epsilon greedy) has the best convergence than other Q-learning algorithms" but does not provide a detailed comparison of computational complexity or sample efficiency between the two variants.
- Why unresolved: The paper only mentions the convergence rate but does not discuss computational or sample efficiency aspects of the two PhaseQL variants.
- What evidence would resolve it: A detailed analysis of the computational complexity and sample efficiency of PhaseQL with UCB versus PhaseQL with epsilon-greedy for various RMAB problem sizes and structures.

### Open Question 2
- Question: What is the impact of the number of samples (m) in PhaseQL on the convergence rate and stability of the Whittle index learning algorithm?
- Basis in paper: [inferred] The paper mentions that PhaseQL uses m samples to estimate transition probabilities but does not explore how varying m affects the convergence rate or stability of the index learning algorithm.
- Why unresolved: The paper uses a fixed value of m=20 in the numerical examples without exploring the sensitivity of the algorithm to this parameter.
- What evidence would resolve it: A systematic study of the convergence rate and stability of the Whittle index learning algorithm with PhaseQL as a function of different values of m for various RMAB problem instances.

### Open Question 3
- Question: How do the convergence properties of the two-timescale stochastic approximation algorithm with constant step sizes compare to those with decaying step sizes in the context of Whittle index learning?
- Basis in paper: [explicit] The paper states "We study constant stepsizes two timescale stochastic approximation algorithm" but does not compare it to the case of decaying step sizes.
- Why unresolved: The paper only considers constant step sizes and does not explore how the convergence properties might change with decaying step sizes.
- What evidence would resolve it: A comparative analysis of the convergence properties (e.g., rate, stability, finite-time performance) of the two-timescale stochastic approximation algorithm with constant versus decaying step sizes for Whittle index learning.

## Limitations

- The theoretical convergence guarantees for PhaseQL and GSQL are not rigorously established, particularly for the two-timescale index learning setting
- The experimental evaluation is limited to a single numerical example (5-state RMAB), which may not generalize to larger, more complex problems
- The impact of hyperparameter tuning (especially the confidence parameter c in UCB and step-sizes α and γ) is not systematically explored

## Confidence

- **High Confidence**: PhaseQL with UCB shows faster convergence than standard Q-learning in the tested numerical example
- **Medium Confidence**: The two-timescale stochastic approximation framework is theoretically sound and practically effective for index learning
- **Low Confidence**: The superiority of PhaseQL and UCB extends to more complex RMAB instances with larger state spaces

## Next Checks

1. Implement PhaseQL with UCB on a larger RMAB instance (e.g., 10+ states per arm) and compare convergence rates with standard Q-learning variants
2. Conduct a hyperparameter sensitivity analysis for the UCB confidence parameter c and the step-sizes α and γ in the two-timescale algorithm
3. Verify the theoretical assumptions underlying two-timescale stochastic approximation by monitoring the tracking error between Q-values and index updates during learning