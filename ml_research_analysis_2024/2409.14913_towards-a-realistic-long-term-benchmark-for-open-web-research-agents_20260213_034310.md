---
ver: rpa2
title: Towards a Realistic Long-Term Benchmark for Open-Web Research Agents
arxiv_id: '2409.14913'
source_url: https://arxiv.org/abs/2409.14913
tags:
- agent
- income
- data
- planning
- sources
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces an initial benchmark suite for evaluating
  LLM agents on economically valuable, real-world open-web research tasks, focusing
  on domains like finance, consulting, and forecasting. The benchmark emphasizes tasks
  that are messy, dynamic, and representative of white-collar work, with a design
  that enables smooth performance gains over successive model releases.
---

# Towards a Realistic Long-Term Benchmark for Open-Web Research Agents

## Quick Facts
- arXiv ID: 2409.14913
- Source URL: https://arxiv.org/abs/2409.14913
- Authors: Peter MÃ¼hlbacher; Nikos I. Bosse; Lawrence Phillips
- Reference count: 40
- Primary result: ReAct architecture with subtask delegation outperforms other architectures across different LLM models on economically valuable open-web research tasks

## Executive Summary
This paper introduces a benchmark suite for evaluating LLM agents on real-world open-web research tasks that mirror economically valuable white-collar work. The benchmark focuses on domains like finance, consulting, and forecasting, with tasks that are intentionally messy and dynamic to reflect actual analyst work. Across five carefully selected tasks, agents using Claude-3.5 Sonnet and o1-preview significantly outperformed those using GPT-4o, while Llama 3.1 (405b) and GPT-4o-mini lagged noticeably. The ReAct architecture with subtask delegation performed best across all models tested.

## Method Summary
The study evaluates LLM agents on five real-world research tasks derived from white-collar work domains, using a partial credit scoring system based on ideal agent traces. Agents are equipped with web search tools, Python REPL, and query capabilities, and are tested using four different architectures: ReAct, planning, delegating, and combinations thereof. Performance is assessed both quantitatively through scoring rubrics and qualitatively through trace inspection. The benchmark emphasizes tasks that allow for smooth performance improvements across successive model releases while maintaining economic relevance.

## Key Results
- Claude-3.5 Sonnet and o1-preview significantly outperformed GPT-4o on the benchmark tasks
- Llama 3.1 (405b) and GPT-4o-mini showed notably worse performance across all architectures
- ReAct architecture with subtask delegation consistently achieved the highest scores across different LLM models
- Agents frequently failed by getting stuck in loops or overlooking relevant information, particularly weaker models

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Performance improvements in LLM agents directly translate to economic value when tasks mirror real white-collar work
- Mechanism: The benchmark evaluates tasks that are directly tied to economically valuable analyst work (finance, consulting, forecasting). As agents improve on these tasks, they become capable of replacing or augmenting human labor in these domains, creating measurable economic impact.
- Core assumption: Tasks in the benchmark are representative of actual work performed in economically valuable roles
- Evidence anchors:
  - [abstract] "we lay the groundwork for an LLM agent evaluation suite where good performance directly corresponds to a large economic and societal impact"
  - [section] "The tasks we chose are derived from our experience as a startup providing AI-augmented research and forecasting services to clients in finance and consulting"
  - [corpus] Weak - no direct economic impact measurements found

### Mechanism 2
- Claim: ReAct architecture with subtask delegation consistently outperforms other architectures across different LLM models
- Mechanism: The delegation mechanism allows complex tasks to be broken down into manageable subtasks that can be solved in parallel, leveraging the strengths of different LLM models while maintaining task coherence through orchestration
- Core assumption: Subtask delegation can effectively decompose complex research tasks without losing context or introducing errors
- Evidence anchors:
  - [abstract] "Across LLMs, a ReAct architecture with the ability to delegate subtasks to subagents performed best"
  - [section] "Non-planning, delegating architecture...consists of an orchestratorReAct agent architecture that delegates tasks toReAct sub-agents"

### Mechanism 3
- Claim: The benchmark design with partial credit scoring enables meaningful performance tracking across model releases
- Mechanism: By defining multiple strategies with granular success criteria for each task, the benchmark can capture incremental improvements even when agents don't fully solve tasks, enabling smooth performance tracking
- Core assumption: Tasks can be decomposed into a small number of viable strategies with clear success criteria
- Evidence anchors:
  - [section] "For each task, we enumerate viable strategies. For each strategy, we define criteria for an ideal agent trace, such that the ideal trace would satisfyall of these criteria"
  - [section] "This is possible because the tasks we selected only allow for a relatively small number of plausible solution strategies"

## Foundational Learning

- Concept: Web interaction mechanics and tool usage patterns
  - Why needed here: Agents must effectively use web search, query tools, and data extraction capabilities to complete research tasks
  - Quick check question: Can you describe how an agent would extract specific information from a webpage using the provided query tools?

- Concept: Task decomposition and parallel execution
  - Why needed here: Understanding how to break complex research tasks into parallel subtasks that can be executed by subagents
  - Quick check question: How would you decompose the task of finding Chinese AI labs with training runs over 1e24 FLOPs into parallel subtasks?

- Concept: Partial credit evaluation methodology
  - Why needed here: Understanding how to design scoring criteria that capture incremental progress on complex tasks
  - Quick check question: How would you define partial credit criteria for a task where an agent finds most but not all relevant information?

## Architecture Onboarding

- Component map: LLM backend -> Agent architecture (ReAct/Planning/Delegating) -> Tool interface layer (Google search, Python REPL, Query link text) -> Evaluation framework (partial credit scoring, trace inspection)

- Critical path: 1. Agent receives task prompt 2. Architecture determines tool usage strategy 3. Tools execute and return results 4. LLM processes results and decides next action 5. Process repeats until task completion or termination 6. Evaluation against partial credit criteria

- Design tradeoffs:
  - Planning vs. ReAct: Planning provides explicit state management but adds complexity; ReAct is simpler but may miss opportunities for optimization
  - Delegation vs. direct execution: Delegation enables parallelism but introduces coordination overhead
  - Tool complexity: More tools provide more capabilities but increase learning curve and potential for errors

- Failure signatures:
  - Infinite loops in tool usage (common in weaker models)
  - Failure to update plans or maintain state
  - Inability to decompose tasks appropriately for delegation
  - Over-reliance on specific search strategies that fail

- First 3 experiments:
  1. Run basic ReAct agent on simple task (e.g., finding CEO of OpenAI) to verify tool functionality
  2. Test delegation architecture on parallelizable task (e.g., researching multiple AI labs) to validate subtask coordination
  3. Evaluate partial credit scoring on incomplete task completion to verify scoring logic

## Open Questions the Paper Calls Out

- Question: How do small differences in agent inputs affect task performance and what degree of variation exists between runs?
  - Basis in paper: [explicit] The paper states "our experience of working with LLM agents is that there may be a decent amount of variation from run to run" and "one should not necessarily read too much into an agent's performance on any given single task"
  - Why unresolved: The study only evaluated a single agent run per task, limiting understanding of performance variability and the impact of input variations
  - What evidence would resolve it: Multiple runs of each agent-task combination with different random seeds and input variations, followed by statistical analysis of performance distributions

- Question: Which specific architectural features most significantly impact agent performance across different task types?
  - Basis in paper: [inferred] The paper compares four architectures but doesn't deeply analyze which features drive performance differences. It notes that "a ReAct architecture with the ability to delegate subtasks to subagents performed best" but doesn't isolate why
  - Why unresolved: The study evaluates architecture-LLM combinations rather than systematically testing individual architectural features
  - What evidence would resolve it: Ablation studies testing individual architectural components (planning, delegation, etc.) across diverse task types

- Question: How do agent performance capabilities evolve over successive model releases and what is the relationship to economic impact?
  - Basis in paper: [explicit] The paper aims to create "a benchmark where good performance directly corresponds to a large economic and societal impact" and emphasizes "smooth gains across a large number of releases"
  - Why unresolved: The study only tested five current models without examining performance trends across multiple releases or measuring actual economic impact
  - What evidence would resolve it: Longitudinal studies tracking agent performance across multiple model generations and correlating improvements with measurable economic outcomes

## Limitations
- Benchmark covers only five tasks, which may not fully represent the diversity of real-world research work
- Task selection criteria and scoring rubrics are not fully specified, making independent validation challenging
- Study focuses on model family differences rather than exploring the full design space of agent architectures and tool configurations

## Confidence
- High confidence: Claude-3.5 Sonnet and o1-preview outperform GPT-4o on the benchmark tasks
- Medium confidence: ReAct with delegation is the optimal architecture
- Low confidence: Tasks are representative of economically valuable work

## Next Checks
1. Replicate the benchmark with at least 10 additional tasks covering diverse domains to test generalizability of architecture findings
2. Conduct ablation studies testing different tool configurations and prompting strategies within the ReAct framework
3. Compare benchmark task difficulty to actual white-collar work requirements through expert review and time-on-task analysis