---
ver: rpa2
title: On the Role of Attention Masks and LayerNorm in Transformers
arxiv_id: '2405.18781'
source_url: https://arxiv.org/abs/2405.18781
tags:
- attention
- rank
- layernorm
- token
- tokens
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper analyzes the role of attention masks and LayerNorm\
  \ in transformers, focusing on rank collapse\u2014a phenomenon where token representations\
  \ become increasingly similar as depth increases, limiting model expressivity. The\
  \ authors rigorously study how attention masks and LayerNorm affect rank collapse\
  \ by treating self-attention as a discrete-time dynamical system."
---

# On the Role of Attention Masks and LayerNorm in Transformers

## Quick Facts
- arXiv ID: 2405.18781
- Source URL: https://arxiv.org/abs/2405.18781
- Reference count: 40
- Primary result: Attention masks and LayerNorm significantly influence rank collapse in transformers, with LayerNorm enabling more expressive attention dynamics

## Executive Summary
This paper rigorously analyzes how attention masks and LayerNorm affect rank collapse in transformer models. Rank collapse is a phenomenon where token representations become increasingly similar as depth increases, limiting model expressivity. The authors treat self-attention as a discrete-time dynamical system and examine how different architectural components influence this behavior. Their analysis reveals that while attention masks primarily affect the rate of collapse, LayerNorm plays a crucial role in enabling more expressive attention dynamics and preventing collapse for certain value matrices.

## Method Summary
The authors approach the problem by modeling self-attention as a discrete-time dynamical system. They analyze pure masked attention without LayerNorm, studying how different attention patterns (causal masks, sliding windows, etc.) affect rank collapse. They then examine the impact of adding LayerNorm to the system, both theoretically and empirically. The theoretical analysis involves constructing counterexamples showing that with proper choice of value matrices, self-attention dynamics with LayerNorm can achieve equilibria of any possible rank between one and full. They validate their theoretical findings through numerical experiments on BERT, GPT2, T5, and ALBERT.

## Key Results
- Attention masks affect the rate but not the occurrence of rank collapse in pure self-attention
- LayerNorm plays a crucial role in preventing rank collapse for certain value matrices
- The self-attention dynamics with LayerNorm constitute a much more expressive nonlinear dynamical system than previously understood

## Why This Works (Mechanism)
The mechanism behind rank collapse involves the iterative application of self-attention layers causing token representations to become increasingly similar. In pure masked attention without LayerNorm, this collapse occurs exponentially as long as there exists a token that serves as common context for all others. LayerNorm fundamentally changes this behavior by normalizing the attention outputs, which alters the underlying dynamical system's properties. This normalization enables the system to reach equilibria of varying ranks depending on the value matrix configuration, making the attention dynamics much more expressive than previously thought.

## Foundational Learning

1. **Rank collapse**: The phenomenon where token representations become increasingly similar as depth increases in transformers, limiting model expressivity.
   - Why needed: Understanding this concept is crucial for analyzing transformer expressiveness
   - Quick check: Can be observed when attention matrices become increasingly rank-deficient across layers

2. **Discrete-time dynamical systems**: Mathematical framework used to model the iterative application of self-attention layers
   - Why needed: Provides rigorous tools to analyze the evolution of token representations
   - Quick check: Can be represented as x_{t+1} = f(x_t) where x_t is the representation at layer t

3. **Attention masks**: Mechanisms that restrict which tokens can attend to which others
   - Why needed: Different masking patterns affect the rate of rank collapse
   - Quick check: Causal masks prevent future tokens from attending to past tokens

## Architecture Onboarding

**Component Map**: Token Embeddings -> Self-Attention -> LayerNorm -> Feed-Forward -> Output

**Critical Path**: The self-attention mechanism with LayerNorm is the critical path for rank collapse dynamics

**Design Tradeoffs**: Pure self-attention without LayerNorm leads to inevitable rank collapse, while adding LayerNorm enables more expressive dynamics but may introduce computational overhead

**Failure Signatures**: 
- Rank-deficient attention matrices across layers
- Loss of token distinctiveness in representations
- Limited model expressivity despite increasing depth

**First Experiments**:
1. Measure rank of attention matrices across layers with and without LayerNorm
2. Compare token representation diversity across different attention masking patterns
3. Analyze the effect of value matrix initialization on rank collapse dynamics

## Open Questions the Paper Calls Out
None

## Limitations
- Analysis focuses primarily on pure self-attention layer without accounting for full transformer architecture
- Theoretical analysis may not fully capture practical training dynamics and initialization schemes
- Empirical validation demonstrates correlation rather than causal mechanisms

## Confidence

| Claim | Confidence Level |
|-------|------------------|
| Attention masks affect only the rate but not occurrence of rank collapse | Medium |
| LayerNorm plays a crucial role in preventing rank collapse for certain value matrices | High |
| Self-attention dynamics with LayerNorm can have equilibria of any possible rank | Medium |

## Next Checks
1. Analyze the combined effect of LayerNorm, skip connections, and residual connections on rank collapse dynamics through both theoretical analysis and empirical validation
2. Investigate how different initialization schemes for value matrices affect the theoretical bounds on rank collapse in practice
3. Extend the analysis to multi-head attention settings and examine how inter-head interactions influence rank collapse dynamics