---
ver: rpa2
title: 'TCE at Qur''an QA 2023 Shared Task: Low Resource Enhanced Transformer-based
  Ensemble Approach for Qur''anic QA'
arxiv_id: '2401.13060'
source_url: https://arxiv.org/abs/2401.13060
tags:
- task
- questions
- answer
- learning
- table
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents TCE's approaches for Qur'an QA 2023 shared
  tasks A and B, focusing on passage retrieval and reading comprehension over the
  Holy Qur'an. To address limited training data, the authors employ transfer learning
  with external resources and voting ensemble methods.
---

# TCE at Qur'an QA 2023 Shared Task: Low Resource Enhanced Transformer-based Ensemble Approach for Qur'anic QA

## Quick Facts
- arXiv ID: 2401.13060
- Source URL: https://arxiv.org/abs/2401.13060
- Authors: Mohammed Alaa Elkomy; Amany Sarhan
- Reference count: 27
- Primary result: MAP score of 25.05% for task A and pAP of 57.11% for task B, surpassing baseline performance

## Executive Summary
This paper presents TCE's approaches for Qur'an QA 2023 shared tasks A and B, focusing on passage retrieval and reading comprehension over the Holy Qur'an. To address limited training data, the authors employ transfer learning with external resources and voting ensemble methods. They explore Arabic pre-trained transformer models with different architectures for both tasks. A thresholding mechanism is proposed to identify unanswerable questions. Their top systems achieve a MAP score of 25.05% for task A and a pAP of 57.11% for task B, significantly surpassing the baseline performance on the hidden split.

## Method Summary
The authors employ a multi-stage fine-tuning approach using pre-trained Arabic language models (AraBERTv0.2-base, CAMeLBERT-CA, AraELECTRA) for both tasks. For task A (passage retrieval), they experiment with dual-encoder and cross-encoder architectures, incorporating external resources (TyDi QA and Tafseer) through pipelined fine-tuning. For task B (reading comprehension), they use span prediction with First Answer Loss (FAL) and Multi Answer Loss (MAL) methods. A voting ensemble combines multiple models trained with different seeds, and a thresholding mechanism identifies unanswerable questions by analyzing model confidence scores.

## Key Results
- Top system achieves MAP score of 25.05% for task A (passage retrieval)
- Top system achieves pAP score of 57.11% for task B (reading comprehension)
- Ensemble learning consistently outperforms single models for both tasks
- Thresholding mechanism effectively identifies zero-answer questions

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Ensemble learning improves stability by averaging out individual model variance.
- Mechanism: Voting self-ensemble aggregates predictions from models trained with different random seeds, reducing sensitivity to noisy predictions and small dataset size.
- Core assumption: Individual models make uncorrelated errors, so averaging improves overall performance.
- Evidence anchors:
  - [abstract]: "we rely on transfer learning together with a voting ensemble to improve prediction stability across multiple runs"
  - [section]: "We utilize a voting self-ensemble technique for a group of fine-tuned models trained with different seeds... Ensemble learning consistently outperforms single models for both tasks"
  - [corpus]: Weak evidence - no directly comparable studies on ensemble voting for Arabic QA found in corpus
- Break condition: If model errors become correlated (e.g., same biases in training data), ensemble averaging may not help.

### Mechanism 2
- Claim: Transfer learning with external resources improves model performance by providing richer training signals.
- Mechanism: Multi-stage fine-tuning uses task-specific data plus external QA and interpretation resources to progressively adapt pre-trained models to the Qur'anic domain.
- Core assumption: Knowledge from external Arabic QA datasets and tafseer (interpretation) resources is transferable to the specific task of Qur'anic QA.
- Evidence anchors:
  - [abstract]: "we rely on transfer learning together with a voting ensemble to improve prediction stability across multiple runs"
  - [section]: "we incorporate external QA and interpretation resources... External resources enhance our learning systems in general by leveraging transfer learning across multiple fine-tuning stages"
  - [corpus]: Corpus shows similar transfer learning approaches in low-resource settings (e.g., "Learning Beyond Limits: Multitask Learning and Synthetic Data for Low-Resource Canonical Morpheme Segmentation")
- Break condition: If external resources contain domain-specific biases that don't transfer well, or if fine-tuning stages overfit to external data.

### Mechanism 3
- Claim: Thresholding mechanism identifies zero-answer questions by leveraging model confidence scores.
- Mechanism: Normalizes model scores across all questions and applies a quantile-based threshold to classify unanswerable questions, with optimal threshold selection improving performance.
- Core assumption: Model confidence scores correlate with actual answerability - high scores indicate answerable questions, low scores indicate unanswerable ones.
- Evidence anchors:
  - [abstract]: "To identify unanswerable questions, we propose using a thresholding mechanism"
  - [section]: "We perform thresholding by a hyperparameter ζ to determine zero-answer samples... we make best use of the available data by employing a quantile method to determine the threshold ζ"
  - [corpus]: Weak evidence - no directly comparable thresholding approaches for Arabic QA in corpus
- Break condition: If model scores don't reliably reflect confidence (e.g., overconfident predictions), or if threshold selection doesn't generalize to hidden test data.

## Foundational Learning

- Concept: Transfer learning in low-resource NLP
  - Why needed here: Qur'anic QA has limited training data (174 training questions), making direct fine-tuning on small datasets prone to overfitting
  - Quick check question: What are the two key benefits of transfer learning when fine-tuning pre-trained models on small datasets?

- Concept: Ensemble methods for model stability
  - Why needed here: Small dataset size leads to high variance in individual model performance; ensemble averaging reduces this variance
  - Quick check question: How does a voting ensemble reduce the impact of individual model errors?

- Concept: Threshold-based classification
  - Why needed here: Task requires distinguishing between answerable and unanswerable questions; thresholding converts continuous model scores into binary predictions
  - Quick check question: What is the difference between using a fixed threshold versus a quantile-based threshold for classification?

## Architecture Onboarding

- Component map:
  Pre-trained Arabic LMs (AraBERTv0.2-base, CAMeLBERT-CA, AraELECTRA) -> Fine-tuning stages (External resources → Task data) -> Dual-encoder/Cross-encoder architecture (Task A) or Span prediction (Task B) -> Thresholding mechanism -> Voting ensemble

- Critical path:
  1. Load pre-trained Arabic LM
  2. Fine-tune on external QA resources (TyDi QA + Tafseer)
  3. Fine-tune on task-specific training data
  4. Generate predictions for development set
  5. Apply thresholding to identify zero-answer questions
  6. Ensemble multiple models if applicable
  7. Evaluate using MAP/pAP metrics

- Design tradeoffs:
  - Dual-encoder vs. cross-encoder: Dual-encoders are faster at inference but less accurate; cross-encoders are more accurate but slower
  - First Answer Loss vs. Multi Answer Loss: FAL is simpler but ignores multi-answer cases; MAL handles multi-answer but may over-distribute probability
  - External resources: More resources can improve performance but increase training time and complexity

- Failure signatures:
  - Low MAP/pAP scores with high variance across runs → Dataset size too small, ensemble needed
  - Poor zero-answer detection → Threshold ζ not well-calibrated, need better threshold selection
  - Overfitting on training data → Too many fine-tuning stages, need regularization or early stopping
  - Slow inference times → Using cross-encoders when dual-encoders would suffice

- First 3 experiments:
  1. Fine-tune AraBERTv0.2-base on task A training data only (direct fine-tuning baseline)
  2. Fine-tune AraBERTv0.2-base with external resources (TyDi QA + Tafseer) on task A (pipelined fine-tuning)
  3. Compare dual-encoder vs. cross-encoder performance on task A development set

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of the proposed ensemble method change when incorporating larger Arabic language models (e.g., beyond AraBERT, CAMeLBERT, AraELECTRA)?
- Basis in paper: [inferred] The paper explores ensemble learning and notes its consistent improvement over single models. However, it limits experiments to a specific set of Arabic pre-trained models without exploring the potential impact of larger or more recent models.
- Why unresolved: The study focuses on specific pre-trained models and their ensembles but does not test the effect of scaling up model size or using different transformer architectures.
- What evidence would resolve it: Performance comparisons of ensemble models using larger or more advanced Arabic transformers on the Qur'an QA tasks, showing improvements or limitations relative to the current models.

### Open Question 2
- Question: What is the impact of using different negative sampling strategies (beyond random and hard negatives) on the dual-encoder approach for task A?
- Basis in paper: [explicit] The paper experiments with random and hard negatives for dual-encoders but does not explore other negative sampling techniques, such as dynamic or curriculum-based negative mining.
- Why unresolved: While hard negatives improve performance, the paper does not investigate whether other negative sampling methods could further enhance retrieval accuracy.
- What evidence would resolve it: Empirical results comparing various negative sampling strategies (e.g., dynamic hard negatives, curriculum learning) on task A, showing their effects on MAP and MRR metrics.

### Open Question 3
- Question: How does the thresholding mechanism for identifying zero-answer questions generalize to other Arabic QA datasets or domains?
- Basis in paper: [inferred] The paper proposes a thresholding mechanism for zero-answer prediction, which is effective for the Qur'an QA tasks. However, it does not test the generalizability of this approach to other Arabic datasets or question types.
- Why unresolved: The thresholding mechanism is evaluated only within the context of the Qur'an QA dataset, leaving uncertainty about its effectiveness in broader Arabic NLP applications.
- What evidence would resolve it: Testing the thresholding mechanism on diverse Arabic QA datasets (e.g., Arabic SQuAD, TyDi QA) and comparing its performance to other zero-answer prediction methods.

## Limitations
- The thresholding mechanism's effectiveness may vary across different datasets and question types, with no systematic evaluation of threshold selection methods.
- The ensemble voting approach assumes uncorrelated errors across models, but with limited data, models may learn similar patterns leading to correlated errors.
- The external resources (TyDi QA and Tafseer) may introduce domain-specific biases that could affect model performance, though this is not extensively evaluated.

## Confidence

- **High confidence**: The basic transfer learning approach and ensemble voting methodology are well-established techniques in low-resource NLP settings. The use of pre-trained Arabic LMs for Qur'anic QA is a reasonable choice given the language-specific requirements.

- **Medium confidence**: The specific combination of multi-stage fine-tuning (external resources → task data) and the dual-encoder/cross-encoder architecture choices are supported by the results but would benefit from more ablation studies. The First Answer Loss versus Multi Answer Loss comparison provides useful insights but lacks extensive validation.

- **Low confidence**: The thresholding mechanism for zero-answer detection is the least validated component. The paper doesn't provide detailed analysis of threshold selection across different question types or demonstrate robustness to varying data distributions in the hidden test set.

## Next Checks

1. **Threshold Robustness Analysis**: Systematically vary the threshold ζ across different quantiles (e.g., 0.1, 0.25, 0.5, 0.75, 0.9) on the development set and evaluate performance stability to determine if the quantile method consistently selects optimal thresholds.

2. **Error Correlation Study**: Analyze the correlation between individual model errors in the ensemble by computing pairwise error agreement rates. This would reveal whether the voting ensemble is truly benefiting from error diversity or if models are making correlated mistakes.

3. **External Resource Ablation**: Train models with different combinations of external resources (TyDi QA only, Tafseer only, both, neither) to quantify the specific contribution of each resource and identify potential negative transfer effects.