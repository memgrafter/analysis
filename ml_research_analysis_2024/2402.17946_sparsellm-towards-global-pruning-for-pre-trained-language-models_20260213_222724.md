---
ver: rpa2
title: 'SparseLLM: Towards Global Pruning for Pre-trained Language Models'
arxiv_id: '2402.17946'
source_url: https://arxiv.org/abs/2402.17946
tags:
- pruning
- sparsellm
- sparsegpt
- sparsity
- global
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: SparseLLM introduces a novel global pruning framework for large
  language models that decomposes the global pruning objective into manageable subproblems
  while maintaining global optimality. The method achieves this by reformulating LLMs
  as composite functions and leveraging auxiliary variables for problem decomposition.
---

# SparseLLM: Towards Global Pruning for Pre-trained Language Models

## Quick Facts
- arXiv ID: 2402.17946
- Source URL: https://arxiv.org/abs/2402.17946
- Reference count: 32
- One-line primary result: SparseLLM achieves up to 80% reduction in perplexity compared to state-of-the-art local pruning methods in high-sparsity regimes (>60%)

## Executive Summary
SparseLLM introduces a novel global pruning framework for large language models that decomposes the intractable global pruning objective into manageable subproblems while maintaining global optimality. By reformulating LLMs as composite functions with auxiliary variables, SparseLLM achieves superior performance particularly in high-sparsity regimes where existing methods struggle. The framework demonstrates computational efficiency comparable to existing solvers while achieving significantly better performance across multiple model sizes and datasets.

## Method Summary
SparseLLM addresses the challenge of global pruning by decomposing the optimization problem into subproblems using auxiliary variables that separate densely parametric parts from non-parametric parts of the model. The framework reformulates LLMs as composite functions and leverages alternating optimization to solve for weight pruning, activation updates, and output updates in sequence. Each subproblem has a closed-form solution that enables fast convergence within 2-3 epochs while maintaining global dependencies through soft constraints. The method combines global pruning for FFN modules (which account for most parameters) with local pruning for MHA modules to balance computational feasibility with pruning effectiveness.

## Key Results
- SparseLLM achieves up to 80% reduction in perplexity compared to state-of-the-art local pruning methods in high-sparsity regimes (>60%)
- The framework demonstrates consistent performance improvements across multiple model sizes (OPT-1.3b to OPT-66b and LlaMA-2 models)
- SparseLLM maintains computational efficiency comparable to existing solvers while providing superior performance across WikiText2, PTB, and C4 datasets

## Why This Works (Mechanism)

### Mechanism 1
SparseLLM achieves global pruning optimality by reformulating LLMs as composite functions with auxiliary variables that decompose the global pruning objective into manageable subproblems. By introducing auxiliary variables (zℓ and aℓ) that separate densely parametric parts from non-parametric parts, SparseLLM converts the intractable global pruning problem into a series of coupled subproblems. Each subproblem has a closed-form solution that can be solved efficiently, while maintaining global dependencies through soft constraints.

### Mechanism 2
SparseLLM achieves superior performance in high-sparsity regimes (>60%) by prioritizing global pruning of FFN modules while maintaining local pruning for MHA modules. The FFN module accounts for more than two-thirds of parameters in each decoder layer. By globally pruning these dense modules while keeping MHA locally pruned, SparseLLM balances computational feasibility with pruning effectiveness, achieving better overall performance than purely local methods.

### Mechanism 3
SparseLLM achieves fast convergence (2-3 epochs) through alternating optimization of subproblems with closed-form solutions. The alternating optimization scheme updates each variable (weights, activations, outputs) while keeping others fixed. Because each subproblem has a closed-form solution, the algorithm converges rapidly without requiring extensive iterative computations.

## Foundational Learning

- Concept: Composite function formulation of neural networks
  - Why needed here: Understanding how LLMs can be decomposed into modular functions is essential for grasping SparseLLM's reformulation approach
  - Quick check question: Can you explain how a multi-layer neural network can be viewed as a composition of functions, where the output of one layer becomes the input to the next?

- Concept: Alternating optimization and ADMM (Alternating Direction Method of Multipliers)
  - Why needed here: SparseLLM uses alternating optimization to solve subproblems, and the global convergence argument references multiblock ADMM
  - Quick check question: What is the key difference between traditional gradient descent and alternating optimization when solving constrained optimization problems?

- Concept: Pruning techniques (structured vs unstructured, global vs local)
  - Why needed here: Understanding the landscape of pruning methods helps contextualize SparseLLM's contributions and trade-offs
  - Quick check question: What are the main computational and performance trade-offs between global pruning and local pruning approaches?

## Architecture Onboarding

- Component map: Pre-trained weights -> Auxiliary variables (zℓ, aℓ) -> Alternating optimization (weight pruning -> activation updates -> output updates) -> Pruned weights

- Critical path:
  1. Initialize auxiliary variables with pre-trained model values
  2. Pre-compute pseudo-inverses for weight updates
  3. Iteratively solve weight pruning, activation updates, and output updates
  4. Return pruned weight matrices

- Design tradeoffs:
  - Global vs local pruning: Global pruning provides better performance but higher memory costs; SparseLLM balances this with selective global pruning
  - Computational efficiency vs accuracy: Closed-form solutions enable fast convergence but may not always find the global optimum
  - Memory usage: Auxiliary variables increase memory requirements but enable the decomposition approach

- Failure signatures:
  - Poor convergence: Alternating optimization fails to reduce loss over epochs
  - Memory overflow: Auxiliary variables and intermediate computations exceed GPU memory
  - Performance degradation: Pruning results in significantly worse perplexity than expected

- First 3 experiments:
  1. Verify closed-form solutions: Implement a single layer update and verify it matches analytical expectations
  2. Test alternating optimization convergence: Run SparseLLM on a small model (OPT-125m) and plot training loss over epochs
  3. Compare pruning strategies: Run SparseLLM with different Ω configurations (global, local, mixed) on a small model to observe performance differences

## Open Questions the Paper Calls Out

- How can heterogeneous sparsity be achieved under the SparseLLM framework to fully realize the potential of global pruning? The paper mentions that SparseLLM assumes homogeneous sparsity, where each layer has the same pruning sparsity equal to the global sparsity, and notes this as a limitation.

- What is the generalizability of SparseLLM across different model architectures and tasks? The paper states that "the effectiveness of SparseLLM, like any pruning method, may vary across different models and tasks, and its generalizability to all scenarios remains an area for further exploration."

- What are the trade-offs between sparsity and performance when using SparseLLM, and how can they be optimally balanced? The paper mentions an "inevitable balance between sparsity and performance that requires careful calibration" as a limitation.

## Limitations

- Computational scalability concerns due to auxiliary variables doubling memory requirements during optimization, potentially creating bottlenecks for very large models (>70B parameters)
- Closed-form solutions rely on specific activation functions (ReLU, GeLU, SiLU) with entry-wise operations, limiting generalizability to other activation functions or complex layer types
- Theoretical global optimality claims rely on multiblock ADMM theory without providing the actual proof, leaving theoretical validation as an open question

## Confidence

- High Confidence: The superior performance of SparseLLM compared to local pruning methods in high-sparsity regimes (>60%) is well-supported by experimental results across multiple model sizes and datasets
- Medium Confidence: The computational efficiency claims are supported by comparison to SparseGPT solver but lack detailed complexity analysis
- Low Confidence: The global optimality claim relies on multiblock ADMM theory without providing the actual proof

## Next Checks

1. **Memory Overhead Validation**: Run SparseLLM on OPT-66B with different numbers of pruned layers (5, 10, 15, 20) while monitoring peak GPU memory usage. Compare this to SparseGPT baseline to quantify the actual memory overhead introduced by auxiliary variables.

2. **Convergence Robustness Test**: Implement a controlled experiment where SparseLLM is run on OPT-1.3B with corrupted initialization (add Gaussian noise to pre-trained weights). Measure whether the alternating optimization still converges to reasonable solutions and how performance degrades compared to clean initialization.

3. **Activation Function Generalization**: Extend SparseLLM to handle GELU activation (which was mentioned but not demonstrated in the paper) and compare performance against the ReLU case on the same model and dataset. This validates whether the framework truly generalizes beyond the specific activation functions tested.