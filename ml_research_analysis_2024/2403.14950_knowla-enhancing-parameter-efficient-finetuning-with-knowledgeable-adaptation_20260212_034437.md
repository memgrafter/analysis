---
ver: rpa2
title: 'KnowLA: Enhancing Parameter-efficient Finetuning with Knowledgeable Adaptation'
arxiv_id: '2403.14950'
source_url: https://arxiv.org/abs/2403.14950
tags:
- knowla
- knowledge
- entity
- llms
- alpaca2
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces KnowLA, a method to enhance parameter-efficient
  fine-tuning (PEFT) of large language models (LLMs) by incorporating knowledge graph
  (KG) embeddings. KnowLA injects entity embeddings from KGs into the LLM during PEFT,
  aligning the semantic spaces of the KG and LLM.
---

# KnowLA: Enhancing Parameter-efficient Finetuning with Knowledgeable Adaptation

## Quick Facts
- arXiv ID: 2403.14950
- Source URL: https://arxiv.org/abs/2403.14950
- Authors: Xindi Luo; Zequn Sun; Jing Zhao; Zhe Zhao; Wei Hu
- Reference count: 37
- Key outcome: KnowLA improves parameter-efficient fine-tuning (PEFT) performance by up to 1.47% accuracy on CommonsenseQA by injecting knowledge graph embeddings into LLMs

## Executive Summary
This paper introduces KnowLA, a method that enhances parameter-efficient fine-tuning (PEFT) of large language models (LLMs) by incorporating knowledge graph (KG) embeddings. The approach injects entity embeddings from KGs into the LLM during PEFT, aligning the semantic spaces of the KG and LLM through a knowledge mapping module. Experiments on six benchmarks demonstrate that KnowLA improves performance compared to standard PEFT methods like LoRA, with up to 1.47% accuracy gain on CommonsenseQA. The method is robust across different LLMs, instruction data, and KGs, and analysis shows KnowLA helps activate the LLM's internal knowledge, leading to better results.

## Method Summary
KnowLA enhances PEFT by inserting an adaptation layer into the LLM that integrates KG embeddings of entities appearing in the input text. The adaptation layer contains two components: knowledge mapping and knowledge fusion. Knowledge mapping aligns the semantic space of KG embeddings with the LLM's representation space through an RM-SNorm transformation and SwiGLU-based mapping. Knowledge fusion calculates attention weights between tokens and their linked entities, allowing the LLM to leverage its existing knowledge through the injected entity embeddings. The adaptation layer is trained alongside LoRA adapters on instruction data while freezing the LLM parameters, preserving computational efficiency while improving effectiveness.

## Key Results
- KnowLA achieves up to 1.47% accuracy gain on CommonsenseQA compared to standard LoRA
- KnowLA improves performance across six benchmarks: CommonsenseQA, SIQA, BIG-Bench Hard, WebQuestionSP, TriviaQA, and TruthfulQA
- The method demonstrates robustness across different LLMs (Llama 2, LLaMA 1), instruction data, and knowledge graphs (WordNet, ConceptNet, Wikidata)
- KnowLA preserves computational efficiency by working effectively with LoRA's low-rank adaptation

## Why This Works (Mechanism)

### Mechanism 1
- Claim: KnowLA aligns the semantic space of KG embeddings with the LLM's representation space through a knowledge mapping module
- Mechanism: The adaptation layer applies RM-SNorm to the LLM's hidden states and uses a SwiGLU-based transformation to map entity embeddings to the LLM's semantic space before knowledge fusion
- Core assumption: The LLM's hidden states encapsulate parameterized knowledge that can be meaningfully aligned with KG entity embeddings
- Evidence anchors:
  - [abstract]: "KnowLA injects entity embeddings from KGs into the LLM during PEFT, aligning the semantic spaces of the KG and LLM"
  - [section]: "The text representation of the l-th decoder layer in the LLM is denoted by hl... We apply RM-SNorm... to align with the pre-norm mode"
  - [corpus]: Weak - corpus papers focus on PEFT methods but don't directly discuss KG embedding alignment mechanisms
- Break condition: If the KG embedding space is too dissimilar from the LLM's semantic space, the mapping transformation may fail to create meaningful alignment

### Mechanism 2
- Claim: KnowLA activates the LLM's internal parameterized knowledge without changing its parameters
- Mechanism: The knowledge fusion module calculates attention weights between tokens and their linked entities, allowing the LLM to leverage its existing knowledge through the injected entity embeddings
- Core assumption: LLMs contain relevant parameterized knowledge that may not be activated by standard prompts but can be triggered through entity embeddings
- Evidence anchors:
  - [abstract]: "KnowLA can help activate the relevant parameterized knowledge in an LLM to answer a question without changing its parameters"
  - [section]: "We think the representations with greater changes capture more internal knowledge" and "KnowLA demonstrates enhanced knowledge activation capabilities"
  - [corpus]: Weak - corpus papers don't specifically discuss activation of parameterized knowledge through entity embeddings
- Break condition: If the LLM lacks the relevant parameterized knowledge for a task, injecting KG embeddings cannot create knowledge that doesn't exist

### Mechanism 3
- Claim: KnowLA works effectively with LoRA by preserving computational efficiency while improving effectiveness
- Mechanism: The adaptation layer is trained alongside LoRA adapters, adding minimal parameters while leveraging KG embeddings to enhance the knowledge captured during PEFT
- Core assumption: Combining knowledge injection with low-rank adaptation preserves the efficiency benefits of PEFT while adding semantic knowledge from KGs
- Evidence anchors:
  - [abstract]: "The adaptation layer is trained in combination with LoRA on instruction data" and "KnowLA improves performance compared to standard PEFT methods like LoRA"
  - [section]: "KnowLA achieves the alignment between KG knowledge and textual semantics by freezing the LLM during finetuning" and "KnowLA can also be used in conjunction with LoRA"
  - [corpus]: Moderate - corpus includes PEFT papers but none specifically discuss combining LoRA with knowledge injection
- Break condition: If the knowledge injection adds too many parameters or creates optimization conflicts with LoRA, the efficiency benefits may be compromised

## Foundational Learning

- Concept: Parameter-efficient fine-tuning (PEFT) methods like LoRA
  - Why needed here: KnowLA is built on top of LoRA and requires understanding how PEFT works with LLMs
  - Quick check question: How does LoRA achieve parameter efficiency compared to full fine-tuning?

- Concept: Knowledge graph embeddings and representation learning
  - Why needed here: KnowLA uses pre-trained entity embeddings from KGs as external knowledge
  - Quick check question: What are the key differences between TransE, RotatE, and RESCAL embedding methods?

- Concept: Transformer architecture and attention mechanisms
  - Why needed here: KnowLA modifies the transformer layers by inserting an adaptation layer that interacts with hidden states
  - Quick check question: How do the feed-forward network (FFN) layers in transformers capture and store knowledge?

## Architecture Onboarding

- Component map: LLM decoder layers → KnowLA adaptation layer (knowledge mapping + fusion) → LoRA adapters → output
- Critical path: Input text → BPE tokenization → LLM encoding → Entity linking → Knowledge mapping → Knowledge fusion → LoRA adaptation → Output generation
- Design tradeoffs: Knowledge injection vs. parameter efficiency (KnowLA adds parameters but fewer than full fine-tuning); KG quality vs. performance (better KGs improve results); KG embedding method vs. compatibility (some embeddings work better with LLMs)
- Failure signatures: Poor KG-entity linking reduces effectiveness; incompatible KG embedding spaces prevent alignment; over-parameterization of adaptation layer reduces efficiency; knowledge injection conflicts with LoRA optimization
- First 3 experiments:
  1. Test KnowLA with random entity embeddings vs. trained KG embeddings on a simple QA task to verify knowledge injection matters
  2. Compare KnowLA with different KG embedding methods (TransE vs. RotatE) on CommonsenseQA to find optimal embedding compatibility
  3. Measure parameter count and performance trade-off by varying LoRA rank while keeping KnowLA fixed on a benchmark task

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can KnowLA be effectively extended to integrate multiple knowledge graphs while avoiding performance degradation?
- Basis in paper: [explicit] The paper mentions that merging multiple KGs into a large graph using entity alignment and learning entity embeddings from this large KG results in slightly lower accuracy than using a single KG (KnowLA with ConceptNet).
- Why unresolved: The paper suggests that a more promising mechanism, such as Mixture of Experts, is necessary to combine multiple KGs effectively. However, it does not provide a concrete solution or implementation details.
- What evidence would resolve it: A study comparing the performance of KnowLA with different mechanisms for integrating multiple KGs, such as Mixture of Experts, against the current baseline of merging KGs, on various benchmarks.

### Open Question 2
- Question: Can KnowLA be adapted to work with other types of large language models, such as encoder-decoder models or smaller models with fewer parameters?
- Basis in paper: [inferred] The paper focuses on adapting KnowLA to decoder-based LLMs, specifically the LLaMA family. It does not explore the applicability of KnowLA to other types of LLMs or smaller models.
- Why unresolved: The paper does not provide any experiments or analysis on the performance of KnowLA with other types of LLMs or smaller models. It is unclear whether the knowledge injection mechanism would be as effective or if modifications would be necessary.
- What evidence would resolve it: Experiments comparing the performance of KnowLA with different types of LLMs (e.g., encoder-decoder models, smaller models) on various benchmarks, along with an analysis of the necessary modifications or adaptations for each type.

### Open Question 3
- Question: How does the choice of knowledge graph embedding learning model impact the performance of KnowLA, and are there more suitable models for different types of tasks or LLMs?
- Basis in paper: [explicit] The paper evaluates the impact of three KG embedding learning models (RESCAL, TransE, and RotatE) on KnowLA's performance using ConceptNet. It finds that TransE embeddings achieve the best results, while RotatE embeddings perform poorly.
- Why unresolved: The paper does not provide a comprehensive analysis of why certain embedding models work better than others for KnowLA. It also does not explore the potential of other embedding models or their suitability for different tasks or LLMs.
- What evidence would resolve it: A detailed analysis of the characteristics of different KG embedding learning models and their impact on KnowLA's performance, along with experiments comparing the performance of KnowLA with various embedding models on different tasks and LLMs.

## Limitations

- The paper lacks detailed implementation specifications for the adaptation layer architecture and its integration with LLM layers
- The evaluation focuses primarily on English-language benchmarks and knowledge graphs, limiting generalizability to multilingual contexts
- Computational overhead analysis lacks detailed measurements of memory usage and training time comparisons with other PEFT methods

## Confidence

**High Confidence:**
- KnowLA's ability to improve PEFT performance over standard LoRA on tested benchmarks (1.47% accuracy gain on CommonsenseQA is a concrete, measurable result)
- The mechanism of inserting an adaptation layer for knowledge injection is technically sound and implementable
- KnowLA's compatibility with LoRA preservation of computational efficiency

**Medium Confidence:**
- The claim that KnowLA activates internal LLM knowledge without changing parameters, as this requires deeper interpretability analysis beyond cosine similarity measurements
- The robustness across different LLMs and knowledge graphs, given the limited scope of tested models (Llama 2 and LLaMA 1) and KGs (WordNet, ConceptNet, Wikidata)
- The assertion that knowledge injection is the primary driver of performance improvements, as architectural changes could contribute

**Low Confidence:**
- Generalization claims to other task types beyond the six tested benchmarks
- Scalability to larger LLMs beyond the 7B parameter range tested
- Performance guarantees when using KGs with different entity densities or structural properties

## Next Checks

1. **Ablation Study on Adaptation Layer Components**: Test KnowLA variants with (a) knowledge mapping only, (b) knowledge fusion only, and (c) both components to isolate which mechanism drives performance improvements. This would validate the "knowledge activation" claim by demonstrating whether both components are necessary or if one suffices.

2. **Cross-Lingual Transfer Validation**: Apply KnowLA using a multilingual knowledge graph (e.g., Wikidata with multilingual entity labels) to evaluate performance on non-English benchmarks. This would test the generalizability claims and reveal whether the knowledge injection mechanism works across languages or is English-centric.

3. **Parameter Efficiency Stress Test**: Systematically vary the LoRA rank (r) from very low (r=4) to high (r=128) while keeping KnowLA fixed, measuring both performance and parameter count at each level. This would validate the efficiency claims by showing the actual trade-off curve between parameter efficiency and performance gains, particularly whether KnowLA's benefits persist at minimal parameter budgets.