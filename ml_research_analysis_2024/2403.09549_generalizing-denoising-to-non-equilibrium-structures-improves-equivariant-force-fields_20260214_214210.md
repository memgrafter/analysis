---
ver: rpa2
title: Generalizing Denoising to Non-Equilibrium Structures Improves Equivariant Force
  Fields
arxiv_id: '2403.09549'
source_url: https://arxiv.org/abs/2403.09549
tags:
- dens
- structures
- training
- energy
- forces
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of limited training data in
  3D atomistic systems by generalizing denoising techniques from equilibrium to non-equilibrium
  structures. The core idea is to use denoising non-equilibrium structures (DeNS)
  as an auxiliary task, which requires encoding the forces of the original non-equilibrium
  structures to specify the target structure.
---

# Generalizing Denoising to Non-Equilibrium Structures Improves Equivariant Force Fields

## Quick Facts
- arXiv ID: 2403.09549
- Source URL: https://arxiv.org/abs/2403.09549
- Reference count: 40
- One-line primary result: DeNS improves force field predictions by denoising non-equilibrium structures as an auxiliary task, achieving state-of-the-art results on OC20/OC22 and improving training efficiency on MD17.

## Executive Summary
This paper addresses the challenge of limited training data in 3D atomistic systems by generalizing denoising techniques from equilibrium to non-equilibrium structures. The core idea is to use denoising non-equilibrium structures (DeNS) as an auxiliary task, which requires encoding the forces of the original non-equilibrium structures to specify the target structure. This approach leverages the larger set of non-equilibrium structures available in datasets and improves the performance of equivariant networks on tasks like energy and force predictions. Experiments on OC20, OC22, and MD17 datasets demonstrate that DeNS achieves new state-of-the-art results on OC20 and OC22, significantly improves training efficiency on MD17, and can be applied to various equivariant networks. The method introduces a marginal increase in training time but offers substantial performance gains.

## Method Summary
The method generalizes denoising techniques to non-equilibrium structures in 3D atomistic systems. It corrupts atomic positions with noise and predicts the original structure, using forces as constraints to specify the target. DeNS is implemented as an auxiliary task during training, requiring equivariant networks to encode forces into node embeddings. The approach uses partially corrupted structures and multi-scale noise to improve performance across different datasets.

## Key Results
- DeNS achieves new state-of-the-art results on OC20 and OC22 datasets for energy and force predictions.
- Significant improvements in training efficiency and sample efficiency observed on MD17 dataset.
- Force encoding is critical for DeNS effectiveness, particularly on OC20 and MD17 datasets.
- Multi-scale noise improves performance compared to fixed noise scale across datasets.

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Denoising non-equilibrium structures as an auxiliary task improves performance on energy and force predictions.
- **Mechanism:** By corrupting atomic positions with noise and then predicting the original structure, the model learns to map between non-equilibrium structures and their corresponding forces. This auxiliary task acts as data augmentation, generating new structures with diverse geometries, and encourages learning the inverse relationship between forces and structures.
- **Core assumption:** The forces of the original non-equilibrium structure provide sufficient information to specify the target structure, making denoising a well-defined problem despite the existence of multiple possible structures.
- **Evidence anchors:**
  - [abstract]: "Our key insight is to additionally encode the forces of the original non-equilibrium structure to specify which non-equilibrium structure we are denoising."
  - [section]: "Since DeNS requires encoding forces, DeNS favors equivariant networks, which can easily incorporate forces and other higher-order tensors in node embeddings."
  - [corpus]: Weak evidence from related works; denoising is mentioned but not specifically for non-equilibrium structures.
- **Break condition:** If the force encoding is insufficient to uniquely determine the target structure, denoising becomes ill-posed and the auxiliary task fails to improve performance.

### Mechanism 2
- **Claim:** Force encoding is critical for denoising non-equilibrium structures.
- **Mechanism:** The forces of the original non-equilibrium structure act as constraints, specifying the target structure during denoising. Without force encoding, the model cannot uniquely determine the target structure, leading to poor performance.
- **Core assumption:** The forces of the original non-equilibrium structure are necessary and sufficient to specify the target structure.
- **Evidence anchors:**
  - [section]: "Empirically, we find that force encoding is critical to the effectiveness of DeNS on OC20 S2EF-2M dataset (Index 1 and Index 2 in Table 1e) and MD17 dataset (Index 2 and Index 3 in Table 6)."
  - [section]: "We also compare the performance of using a fixedσand multi-scale noise, and the comparison between Index 1 and Index 4 shows that multi-scale noise improves both energy and force predictions."
  - [corpus]: No direct evidence in related works; force encoding is not mentioned in the context of denoising non-equilibrium structures.
- **Break condition:** If the forces of the original non-equilibrium structure are not available or do not provide sufficient information to specify the target structure, force encoding becomes ineffective.

### Mechanism 3
- **Claim:** Multi-scale noise improves the performance of denoising non-equilibrium structures.
- **Mechanism:** By incorporating multiple noise scales, the model is exposed to a wider range of corrupted structures, spanning a more diverse distribution of non-equilibrium geometries. This improves the model's ability to generalize to new structures and predict their corresponding forces.
- **Core assumption:** Multi-scale noise spans a more diverse distribution of non-equilibrium geometries than a fixed noise scale.
- **Evidence anchors:**
  - [section]: "Since we sample standard deviationσwhen using multi-scale noise, we investigate whether we need to encodeσ. The comparison between Index 1 and Index 5 shows that DeNS without σencoding works better, and thus we can use the same approach when we use either a fixedσor multi-scale noise."
  - [section]: "Empirically, we find that incorporating multiple noise scales together for denoising improves energy and force predictions on OC20 and OC22 datasets."
  - [corpus]: No direct evidence in related works; multi-scale noise is not mentioned in the context of denoising non-equilibrium structures.
- **Break condition:** If multi-scale noise does not span a more diverse distribution of non-equilibrium geometries, it does not improve the model's ability to generalize and predict forces.

## Foundational Learning

- **Concept:** Equivariant neural networks
  - **Why needed here:** Denoising non-equilibrium structures requires encoding forces, which are higher-order tensors. Equivariant networks can easily incorporate forces into their node embeddings using vector spaces of irreducible representations.
  - **Quick check question:** What is the main advantage of using equivariant neural networks for denoising non-equilibrium structures?

- **Concept:** Density functional theory (DFT)
  - **Why needed here:** The datasets used in this paper (OC20, OC22, MD17) are generated using DFT calculations. Understanding DFT is crucial for interpreting the results and comparing them to other methods.
  - **Quick check question:** What is the main advantage of using DFT for generating datasets of atomistic systems?

- **Concept:** Self-supervised learning
  - **Why needed here:** Denoising non-equilibrium structures is a form of self-supervised learning. The model learns to reconstruct the original structure from a corrupted version without using any labels.
  - **Quick check question:** What is the main advantage of using self-supervised learning for pretraining on atomistic data?

## Architecture Onboarding

- **Component map:** EquiformerV2 (or equivariant network) -> Noise head -> Force head
- **Critical path:**
  1. Corrupt the input structure by adding noise to atomic positions.
  2. Encode the forces of the original non-equilibrium structure.
  3. Predict the noise added to the corrupted structure.
  4. Minimize the L2 difference between the predicted noise and the actual noise.
- **Design tradeoffs:**
  - Using multi-scale noise vs. fixed noise scale: Multi-scale noise improves performance but increases computational cost.
  - Encoding forces vs. not encoding forces: Encoding forces is critical for denoising non-equilibrium structures but requires additional computational overhead.
  - Using partially corrupted structures vs. fully corrupted structures: Partially corrupted structures improve performance on some datasets but may not be necessary on others.
- **Failure signatures:**
  - Poor performance on energy and force predictions: May indicate that the denoising auxiliary task is not effective or that the force encoding is insufficient.
  - Training instability: May indicate that the noise scale is too large or that the corruption ratio is too high.
- **First 3 experiments:**
  1. Train an equivariant graph attention network with denoising as an auxiliary task on the OC20 S2EF-2M dataset and compare the results to training without denoising.
  2. Vary the noise scale and corruption ratio to find the optimal settings for denoising non-equilibrium structures.
  3. Train an equivariant graph attention network with denoising as an auxiliary task on the MD17 dataset and compare the results to training without denoising.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of DeNS vary with different network architectures beyond Equiformer and eSCN, particularly invariant networks or other equivariant models?
- Basis in paper: [inferred] The paper primarily focuses on Equiformer and eSCN, but mentions that DeNS could be applicable to other equivariant networks like SEGNN-like networks on MD17.
- Why unresolved: The paper does not provide comprehensive experimental results on a wide range of network architectures, leaving the generalizability of DeNS to other architectures unclear.
- What evidence would resolve it: Extensive experiments comparing DeNS performance across diverse network architectures, including invariant networks and other equivariant models, would clarify its broad applicability.

### Open Question 2
- Question: What is the impact of DeNS on the computational efficiency of training large-scale models, particularly in terms of memory usage and scalability?
- Basis in paper: [explicit] The paper mentions a marginal increase in training time due to the additional noise head but does not delve into memory usage or scalability concerns for large-scale models.
- Why unresolved: The paper does not provide detailed analysis of how DeNS affects memory consumption or the ability to scale to larger datasets and more complex models.
- What evidence would resolve it: Empirical studies measuring memory usage and training scalability with DeNS on increasingly large models and datasets would provide insights into its practical limitations.

### Open Question 3
- Question: How does the choice of noise corruption strategy (e.g., partial vs. full corruption, multi-scale vs. fixed noise) influence the effectiveness of DeNS across different datasets?
- Basis in paper: [explicit] The paper discusses the use of partially corrupted structures and multi-scale noise, noting their effectiveness on specific datasets like OC22 and MD17.
- Why unresolved: The paper does not systematically explore how different noise corruption strategies impact DeNS performance across various datasets, leaving the optimal strategy dataset-dependent.
- What evidence would resolve it: Comparative experiments testing different noise corruption strategies across multiple datasets would reveal the most effective approaches for each dataset type.

## Limitations

- The method's effectiveness on tasks beyond energy and force predictions (e.g., electronic properties, reaction barriers) remains unexplored.
- Computational overhead, particularly memory usage and scalability for large-scale models, is not thoroughly quantified.
- Performance sensitivity to hyperparameters like noise scale and corruption ratio lacks comprehensive analysis.

## Confidence

- **High Confidence**: The core mechanism of using denoising non-equilibrium structures as an auxiliary task to improve force field predictions is well-supported by experimental results on multiple datasets.
- **Medium Confidence**: The claim that multi-scale noise improves performance is supported by experiments on OC20 and OC22, but the evidence is less conclusive for MD17.
- **Low Confidence**: The assertion that DeNS can be seamlessly integrated with various equivariant networks (e.g., NequIP, DimeNet++) is based on limited experimental evidence.

## Next Checks

1. **Reproducibility Test**: Implement DeNS on a different equivariant network architecture (e.g., NequIP) and validate the improvements on OC20 or OC22.
2. **Hyperparameter Sensitivity Analysis**: Conduct a systematic study of the impact of key hyperparameters (σhigh, pDeNS, λDeNS) on DeNS performance.
3. **Scalability Assessment**: Evaluate the computational overhead of DeNS on larger datasets or more complex molecular systems.