---
ver: rpa2
title: Recurrent Stochastic Configuration Networks with Hybrid Regularization for
  Nonlinear Dynamics Modelling
arxiv_id: '2412.00070'
source_url: https://arxiv.org/abs/2412.00070
tags:
- data
- order
- wout
- regularization
- output
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a hybrid regularized Recurrent Stochastic Configuration
  Network (RSCN) for nonlinear dynamic system modelling. The method integrates LASSO-based
  order variable selection with an improved RSCN incorporating L2 regularization to
  handle uncertainties in system orders and improve generalization.
---

# Recurrent Stochastic Configuration Networks with Hybrid Regularization for Nonlinear Dynamics Modelling

## Quick Facts
- arXiv ID: 2412.00070
- Source URL: https://arxiv.org/abs/2412.00070
- Authors: Gang Dang; Dianhui Wang
- Reference count: 32
- Key outcome: Proposed LASSO-RSCN-L2 model achieves NRMSE of 0.02345 on nonlinear system identification, 0.03926 on debutanizer column soft sensing, and 0.44771 on short-term power load forecasting.

## Executive Summary
This paper presents a hybrid regularized Recurrent Stochastic Configuration Network (RSCN) for nonlinear dynamic system modelling. The method integrates LASSO-based order variable selection with an improved RSCN incorporating L2 regularization to handle uncertainties in system orders and improve generalization. The proposed framework constructs the reservoir incrementally using a supervisory mechanism enhanced by regularization, while output weights are dynamically adjusted via a projection algorithm. Theoretical analysis confirms the universal approximation property and echo state property. Experimental results on a nonlinear system identification problem and two industrial predictive tasks demonstrate that the proposed LASSO-RSCN-L2 model achieves superior performance with structural compactness and reduced reservoir sizes compared to baseline methods.

## Method Summary
The hybrid regularized RSCN framework combines LASSO for order variable selection with an improved RSCN incorporating L2 regularization. The method first applies LASSO to identify significant order variables from high-dimensional temporal inputs, reducing dimensionality and eliminating noise. An improved RSCN with L2 regularization then builds a compact reservoir trained only on the residuals (difference between LASSO prediction and true target), avoiding direct prediction of complex dynamics and reducing overfitting risk. The reservoir is constructed incrementally using a supervisory mechanism enhanced by regularization, while output weights are computed using regularized least squares and dynamically adjusted via a projection algorithm for online adaptation.

## Key Results
- LASSO-RSCN-L2 model achieves testing NRMSE of 0.02345 on nonlinear system identification task
- Superior performance on industrial applications: 0.03926 on debutanizer column process soft sensing and 0.44771 on short-term power load forecasting
- Maintains structural compactness with reduced reservoir sizes compared to baseline methods
- Outperforms traditional RSCN and other baseline methods across all experimental scenarios

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The hybrid approach (LASSO + RSCN-L2) improves model generalization by first selecting relevant order variables and then approximating residuals with a regularized reservoir.
- Mechanism: LASSO performs sparse feature selection on high-order temporal inputs, reducing dimensionality and eliminating noise. RSCN-L2 then builds a compact reservoir trained only on the residuals, avoiding direct prediction of complex dynamics and reducing overfitting risk.
- Core assumption: Important order variables identified by LASSO remain relevant for residual approximation by RSCN-L2, and the residuals are smooth enough for the randomized reservoir to capture effectively.
- Evidence anchors:
  - [abstract] "the well-known least absolute shrinkage and selection operator (LASSO) is employed to identify the significant order variables. Subsequently, an improved RSCN with L2 regularization is introduced to approximate the residuals"
  - [section III.A] "By adjusting CL, the regression coefficients of redundant or irrelevant features can be compressed to zero, resulting in a sparser linear regression model that facilitates feature extraction"
  - [corpus] Weak - corpus does not discuss LASSO+residual approximation hybrids specifically.
- Break condition: If LASSO incorrectly excludes relevant order variables, the residual model loses key dynamic information; if residuals are too complex, RSCN-L2 cannot approximate them with a compact reservoir.

### Mechanism 2
- Claim: The improved supervisory mechanism with L2 regularization in RSCN ensures incremental reservoir growth while controlling overfitting and maintaining universal approximation.
- Mechanism: When adding new reservoir nodes, the mechanism enforces an inequality constraint that includes an L2 penalty term, biasing weight selection toward nodes that reduce training error with minimal complexity increase. Output weights are computed using regularized least squares (ridge regression), stabilizing learning.
- Core assumption: The L2 penalty term in the supervisory mechanism effectively balances node selection and generalization, and the regularized least squares solution remains stable as the reservoir grows.
- Evidence anchors:
  - [section III.B] "The L2 penalty term is introduced into the cost function of the original RSCN, and the output weights are evaluated by utilizing the regularized least squares method"
  - [section III.B] Theorem 1 formalizes the inequality constraint with regularization and proves convergence of residual error to zero.
  - [corpus] Weak - no corpus neighbors discuss regularized supervisory mechanisms for RSCN specifically.
- Break condition: If the regularization coefficient is too large, the model underfits and cannot capture dynamics; if too small, overfitting returns despite LASSO preprocessing.

### Mechanism 3
- Claim: Online projection-based weight updates enable rapid adaptation to dynamic changes in the system while preserving stability and convergence guarantees.
- Mechanism: At each time step, output weights are adjusted via a projection algorithm that minimizes change from the previous weights subject to matching the current target, ensuring smooth adaptation without destabilizing the reservoir dynamics.
- Core assumption: The projection update maintains echo state property and convergence because it only adjusts output weights, not reservoir weights, and the step size is implicitly controlled by the projection constraint.
- Evidence anchors:
  - [section III.C] "Determine Wout(n) to minimize the following cost function... By incorporating the Lagrange multiplier λp, we can obtain..."
  - [section III.C] "This online adjustment enables the network to quickly adapt to dynamic unknown data"
  - [corpus] Weak - no corpus neighbors discuss online projection updates in RSCN variants.
- Break condition: If the system changes too rapidly, the projection step may lag, causing prediction errors to accumulate; if the constraint is too tight, adaptation becomes too slow.

## Foundational Learning

- Concept: LASSO (Least Absolute Shrinkage and Selection Operator)
  - Why needed here: Identifies and selects the most important order variables from high-dimensional temporal inputs, reducing model complexity and noise before nonlinear modeling.
  - Quick check question: In a system with inputs u(n), u(n-1), u(n-2), if only u(n) has a large LASSO coefficient, what does that imply about the system's memory?

- Concept: Echo State Property (ESP)
  - Why needed here: Guarantees that the reservoir state depends only on recent inputs, not initial conditions, ensuring stable and reproducible dynamics for online learning.
  - Quick check question: If the spectral radius of the reservoir weight matrix exceeds 1, what happens to the echo state property?

- Concept: Universal Approximation Property (UAP)
  - Why needed here: Ensures the randomized reservoir can approximate any continuous function on a compact set given enough nodes, justifying the use of randomized learning for complex dynamics.
  - Quick check question: Why is UAP important when the reservoir is constructed incrementally rather than trained with backpropagation?

## Architecture Onboarding

- Component map:
  Input features -> LASSO selector -> Selected order variables -> RSCN-L2 core (incremental reservoir construction with L2 regularization) -> Online updater (projection algorithm) -> Residual approximation -> Output prediction

- Critical path: Feature selection → Reservoir initialization → Node addition loop (random generation → constraint check → ξ selection → output weight update) → Online projection update

- Design tradeoffs:
  - LASSO sparsity vs. completeness: Too aggressive selection loses dynamics; too loose selection adds noise
  - Reservoir size vs. overfitting: Larger reservoirs increase capacity but risk overfitting without regularization
  - Regularization strength vs. underfitting: High C stabilizes learning but may miss subtle dynamics

- Failure signatures:
  - High training NRMSE but low testing NRMSE: Underfitting (C too large, reservoir too small)
  - Low training NRMSE but high testing NRMSE: Overfitting (C too small, reservoir too large, poor LASSO selection)
  - Oscillating predictions: Echo state property violated (spectral radius too large, poor initialization)
  - Slow adaptation to input changes: Projection step too conservative or regularization too strong

- First 3 experiments:
  1. Run LASSO on synthetic NARMA data with known order variables; verify that selected variables match ground truth and observe effect of CL on sparsity.
  2. Build RSCN-L2 on residuals from LASSO prediction of a simple nonlinear system (e.g., y(n+1) = y(n) + sin(u(n))); test convergence and effect of C on generalization.
  3. Implement online projection update on a slowly drifting system (e.g., y(n+1) = 0.9y(n) + noise); measure adaptation speed and stability under different regularization strengths.

## Open Questions the Paper Calls Out

- What is the optimal data-dependent strategy for selecting the regularization coefficient C in the LASSO-RSCN-L2 model?
  - Basis in paper: The paper explicitly states that the proposed LASSO-RSCN-L2 framework lacks a theoretical basis for selecting the regularization coefficient C and suggests future research could focus on developing a data-dependent C to enhance adaptability and performance.
  - Why unresolved: While the paper acknowledges the importance of C selection, it does not provide a systematic method for determining this parameter, particularly in scenarios with varying noise levels and complexity.
  - What evidence would resolve it: A proposed algorithm or framework that dynamically adjusts C based on data characteristics (e.g., noise estimation, model complexity metrics) with empirical validation across multiple nonlinear dynamic system benchmarks.

- How does the performance of the LASSO-RSCN-L2 model compare when extended to block incremental RSCNs versus standard incremental construction?
  - Basis in paper: The paper mentions that subsequent studies may extend this strategy to block incremental RSCNs [30], suggesting this as a potential avenue for improving learning efficiency and effectiveness.
  - Why unresolved: The paper focuses on standard incremental construction and does not explore or compare the block incremental variant, leaving the potential benefits of this approach unexplored.
  - What evidence would resolve it: Comparative experimental results demonstrating the performance (e.g., training time, NRMSE) of block incremental LASSO-RSCN-L2 versus standard incremental LASSO-RSCN-L2 on nonlinear dynamic system identification tasks.

- What is the impact of different activation functions on the universal approximation property and echo state property of the RSCN framework?
  - Basis in paper: The paper discusses the universal approximation property and echo state property of RSCNs but does not explore how different activation functions might affect these properties or overall model performance.
  - Why unresolved: The paper uses a generic activation function g without analyzing the sensitivity of the model's theoretical properties and practical performance to the choice of activation function.
  - What evidence would resolve it: A systematic study comparing the performance and theoretical properties (universal approximation, echo state) of LASSO-RSCN-L2 models using different activation functions (e.g., tanh, sigmoid, ReLU) across various nonlinear dynamic system benchmarks.

## Limitations
- The core mechanisms rely heavily on theoretical properties (UAP, ESP) that are assumed but not empirically validated for the specific hybrid LASSO-RSCN-L2 configuration.
- Incremental reservoir construction with L2-regularized supervisory mechanism lacks empirical evidence from the corpus.
- Online projection updates are described but not validated for rapid system changes.
- The claim that LASSO residuals are "smooth enough" for randomized reservoir approximation is a key assumption without experimental support.

## Confidence
- LASSO feature selection effectiveness: High
- L2 regularization improving generalization: Medium
- Online projection algorithm stability: Low
- Universal approximation property preservation in hybrid model: Medium
- Superior performance on industrial tasks: Medium

## Next Checks
1. Test LASSO variable selection on systems with known orders; systematically vary CL to quantify false positive/negative rates in variable selection and measure impact on residual approximation quality.
2. Implement RSCN-L2 with synthetic residuals from known nonlinear functions; validate convergence properties and test sensitivity to regularization coefficient C by sweeping across orders of magnitude.
3. Create a slowly drifting nonlinear system and evaluate online projection update performance under different system change rates; measure tracking error and stability to identify lag thresholds.