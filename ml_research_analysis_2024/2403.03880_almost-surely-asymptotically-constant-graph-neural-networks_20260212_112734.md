---
ver: rpa2
title: Almost Surely Asymptotically Constant Graph Neural Networks
arxiv_id: '2403.03880'
source_url: https://arxiv.org/abs/2403.03880
tags:
- graph
- such
- have
- which
- graphs
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper analyzes how real-valued GNN classifiers behave as graph
  sizes grow within various random graph models. It introduces a flexible aggregate
  term language (AGG[WMEAN,RW]) that captures common GNN architectures and proves
  that for many random graph distributions, every term in this language converges
  almost surely to a constant function as graph size increases.
---

# Almost Surely Asymptotically Constant Graph Neural Networks

## Quick Facts
- arXiv ID: 2403.03880
- Source URL: https://arxiv.org/abs/2403.03880
- Authors: Sam Adam-Day; Michael Benedikt; İsmail İlkan Ceylan; Ben Finkelshtein
- Reference count: 40
- Primary result: Real-valued GNNs converge almost surely to constant functions as graph size increases in many random graph models.

## Executive Summary
This paper analyzes the asymptotic behavior of real-valued GNN classifiers as graph sizes grow within various random graph models. It introduces a flexible aggregate term language (AGG[WMEAN,RW]) that captures common GNN architectures and proves that for many random graph distributions, every term in this language converges almost surely to a constant function as graph size increases. This includes mean GNNs, GATs, and GPS+RW. The main consequence is that uniform expressivity of these GNNs is severely limited: they can only express asymptotically constant classifiers on large random graphs.

## Method Summary
The paper defines a term language AGG[WMEAN,RW] representing GNN operations and proves convergence results through induction on term construction. The proof strategy uses concentration inequalities and neighborhood convergence arguments, with different approaches for sparse and dense random graph models. Empirical validation involves training five independently initialized models per architecture (MeanGNN, GAT, GPS+RW) on synthetic random graphs of increasing size and measuring convergence of class probabilities.

## Key Results
- All AGG[WMEAN,RW] terms converge almost surely to constant functions on large random graphs
- Mean GNNs show rapid convergence, while attention-based models (GAT, GPS+RW) exhibit delayed convergence
- Sparse and dense random graph models require fundamentally different proof approaches
- Empirical results confirm theoretical predictions on both synthetic and real-world datasets

## Why This Works (Mechanism)

### Mechanism 1
- Claim: AGG[WMEAN,RW] terms converge almost surely to constant functions on large random graphs
- Mechanism: Induction on term construction shows each aggregate term collapses to a Lipschitz function of node features and graph type as graph size grows. The weighted mean operator normalizes contributions so each node's influence diminishes asymptotically.
- Core assumption: Node features are i.i.d. and bounded; graph features converge in distribution
- Evidence anchors: Abstract states output "converges to a constant function"; section explains induction step showing terms simplify to Lipschitz functions

### Mechanism 2
- Claim: Attention-based models (GAT, GPS+RW) exhibit delayed convergence compared to mean GNNs
- Mechanism: Attention weights vary significantly across different graphs of same size early in training, leading to higher variance. As training progresses, attention weights converge to stable values, then follow same path as mean aggregation.
- Core assumption: Attention mechanism eventually stabilizes because node representations converge
- Evidence anchors: Abstract notes result "applies to a very wide class of GNNs, including state of the art models"; section observes "attention-based models exhibit delayed convergence"

### Mechanism 3
- Claim: Sparse random graph models converge differently than dense models
- Mechanism: In sparse models, neighborhoods remain bounded with high probability, leading to many distinct local isomorphism types. In dense models, neighborhoods grow unbounded, causing most nodes to share a single local type and converge faster.
- Core assumption: Weak local convergence holds for the graph model
- Evidence anchors: Abstract lists supported models including "sparse and dense variants of the Erdős-Rényi model"; section contrasts sparse and non-sparse proofs

## Foundational Learning

- Concept: Random graph models and their growth regimes
  - Why needed here: Convergence behavior depends critically on whether model is sparse or dense, determining proof strategy and convergence rate
  - Quick check question: What distinguishes a sparse ER model from a dense one in terms of p(n) growth?

- Concept: Weighted mean aggregation and its normalization effect
  - Why needed here: Core mechanism by which AGG[WMEAN,RW] terms collapse to constants relies on weighted mean's normalization ensuring bounded influence from each node
  - Quick check question: How does the normalization in weighted mean prevent divergence of term values as graphs grow?

- Concept: Graph types and neighborhood isomorphism
  - Why needed here: In sparse models, convergence analyzed via neighborhood isomorphism types; in dense models, via graph types. Essential for following induction proofs
  - Quick check question: What is the difference between a graph type and a neighborhood isomorphism type?

## Architecture Onboarding

- Component map: Term language AGG[WMEAN,RW] -> formal representation of GNN operations -> Random graph model -> probabilistic input distribution -> Lipschitz controllers -> inductive proof machinery -> Attention mechanisms -> optional extensions affecting convergence speed -> Convergence analysis -> theoretical guarantee of constant output

- Critical path:
  1. Define the term language and its semantics
  2. Prove base cases (constants, node features, random walk embeddings)
  3. Inductively prove each aggregate collapses to a Lipschitz function
  4. Apply result to specific GNN architectures
  5. Validate empirically on synthetic and real datasets

- Design tradeoffs:
  - Using weighted mean ensures bounded outputs but may limit expressiveness compared to sum aggregation
  - Supporting random walk embeddings adds expressiveness but requires separate convergence analysis
  - Restricting to i.i.d. node features simplifies proofs but limits applicability to correlated feature settings

- Failure signatures:
  - Non-convergence: Node features not i.i.d., or graph model not covered by supported classes
  - Slow convergence: Attention mechanisms with unstable weights, or sparse models with many local types
  - Oscillatory behavior: Graph model growth rate oscillates between sparse and dense regimes

- First 3 experiments:
  1. Implement AGG[WMEAN,RW] term evaluator and verify it matches a simple MeanGNN on small graphs
  2. Generate synthetic ER graphs of increasing size, run a MeanGNN, and plot class probability convergence
  3. Compare convergence speed of MeanGNN vs GAT on ER graphs, confirming delayed convergence for attention models

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the precise relationship between convergence phenomenon observed in real-world graphs and theoretical convergence laws proven for random graphs?
- Basis in paper: [inferred] Paper shows empirical convergence on TIGER-Alaska dataset but doesn't establish theoretical conditions for real-world graphs
- Why unresolved: Demonstrates convergence on single real-world dataset without theoretical guarantees or identification of structural properties ensuring convergence in general real-world graphs
- What evidence would resolve it: Theoretical framework characterizing when real-world graphs satisfy convergence conditions, or empirical validation across diverse real-world datasets

### Open Question 2
- Question: How do GNNs with sum aggregation behave asymptotically compared to mean aggregation models?
- Basis in paper: [explicit] Paper states sum aggregation would lead to divergence but doesn't provide detailed analysis or proof
- Why unresolved: Mentions as contrast to main results but doesn't explore sum aggregation GNNs in depth or prove divergence results
- What evidence would resolve it: Rigorous mathematical proof showing sum aggregation GNNs either diverge or converge to non-constant functions, along with empirical validation

### Open Question 3
- Question: What architectural modifications could prevent or delay asymptotic convergence to constant functions in GNNs?
- Basis in paper: [inferred] Paper identifies convergence phenomenon as fundamental limitation but doesn't explore architectural modifications that could overcome it
- Why unresolved: Establishes convergence as fundamental limitation but doesn't investigate potential solutions or architectural modifications maintaining expressive power on large graphs
- What evidence would resolve it: Design and analysis of new GNN architectures that provably maintain non-constant asymptotic behavior on large random graphs, along with empirical validation

## Limitations
- Results rely heavily on i.i.d. node features, which may not hold in many real-world scenarios
- Theoretical analysis focuses on uniform convergence across all graphs of increasing size, but practical GNNs often operate on graphs of similar scale
- Applicability to GNNs with sum aggregation or non-i.i.d. features is not addressed

## Confidence
- **High**: Mathematical proofs of convergence for AGG[WMEAN,RW] terms are sound within model assumptions
- **Medium**: Empirical validation shows convergence trends, but real-world dataset (TIGER-Alaska) has limited size diversity
- **Low**: Applicability to GNNs with sum aggregation or non-i.i.d. features is not addressed

## Next Checks
1. **Feature Correlation Test**: Evaluate convergence when node features are correlated rather than i.i.d. to assess robustness of concentration inequalities
2. **Scale-Specific Analysis**: Analyze convergence rates on graphs of similar sizes (e.g., all ~100 nodes) to understand practical implications beyond asymptotic behavior
3. **Alternative Aggregators**: Test convergence for sum aggregation or other non-normalized operators to determine if weighted mean normalization is essential for the result