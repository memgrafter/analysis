---
ver: rpa2
title: 'Tokenization Falling Short: On Subword Robustness in Large Language Models'
arxiv_id: '2406.11687'
source_url: https://arxiv.org/abs/2406.11687
tags:
- tasks
- tokenization
- token
- performance
- word
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This study examines the limitations of tokenization in large language\
  \ models (LLMs), particularly their sensitivity to typographical errors and internal\
  \ token structure. Through three research questions\u2014complex problem solving,\
  \ token structure probing, and resilience to typographical variation\u2014the study\
  \ reveals that while scaling model parameters can mitigate some tokenization issues,\
  \ LLMs still struggle with typographical noise and token composition."
---

# Tokenization Falling Short: On Subword Robustness in Large Language Models

## Quick Facts
- arXiv ID: 2406.11687
- Source URL: https://arxiv.org/abs/2406.11687
- Reference count: 39
- Key outcome: This study examines the limitations of tokenization in large language models (LLMs), particularly their sensitivity to typographical errors and internal token structure. Through three research questions—complex problem solving, token structure probing, and resilience to typographical variation—the study reveals that while scaling model parameters can mitigate some tokenization issues, LLMs still struggle with typographical noise and token composition. The research demonstrates that subword regularization techniques like BPE-dropout with moderate drop rates can enhance model robustness. Experiments across various tasks and datasets show that LLMs are more sensitive to character-level variations than subword-level variations, and larger models generally perform better but remain susceptible to tokenization flaws.

## Executive Summary
This study investigates the robustness of large language models to tokenization errors, revealing significant sensitivity to typographical variations and internal token structure. Through comprehensive experiments across four diverse datasets and multiple model architectures, the research demonstrates that LLMs exhibit greater vulnerability to character-level perturbations compared to subword-level variations. The findings highlight that while scaling model parameters improves performance on tokenization-sensitive tasks, fundamental limitations persist due to the subword-based representation approach. The study also shows that BPE-dropout regularization can effectively mitigate these issues by introducing controlled variability during training.

## Method Summary
The research employs a systematic evaluation framework that tests LLM robustness through three research questions: complex problem solving with anagram tasks, token structure probing via few-shot classification, and resilience to typographical variations. The methodology involves creating corrupted versions of four benchmark datasets (MMLU, TruthfulQA, GSM8K, HumanEval) with character-level noise injection and token-level shuffling. Models tested include Llama3-8B/70B, Mistral-7B, Mixtral-8x7B, and GPT-4 Turbo, evaluated across 0-shot to 3-shot settings. Additionally, BPE-dropout post-training is applied to a Mistral-7B model using a synthesized dataset to assess regularization benefits. Performance is measured using exact match scores, pass@1 rates, and perplexity metrics.

## Key Results
- LLMs demonstrate significantly higher sensitivity to character-level variations than subword-level variations across all tested datasets and models
- Scaling model parameters improves performance on tokenization-sensitive tasks, but fundamental vulnerabilities remain even in 70B parameter models
- BPE-dropout with moderate dropout rates (p=0.2) consistently outperforms baseline models by introducing beneficial regularization
- Larger models show better robustness to tokenization errors, but the improvement is not sufficient to eliminate sensitivity to typographical noise

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Tokenization errors degrade LLM performance more on character-level variations than subword-level variations.
- Mechanism: LLMs are trained on subword-tokenized data, so they learn patterns at the subword level. Character-level permutations introduce noise that doesn't align with learned token boundaries, causing greater semantic drift.
- Core assumption: The subword tokenization scheme (e.g., BPE) creates a vocabulary that shapes how the model represents and processes language.
- Evidence anchors:
  - [abstract] "LLMs exhibit greater sensitivity to character-level variations compared to subword-level variations."
  - [section 5] "Across all datasets and models, there is a consistent trend showing that LLMs are much more sensitive to noise (solid lines) than to reordering (dashed lines)."
  - [corpus] Weak support; corpus contains related work but no direct evidence for this specific claim.
- Break condition: If the model is trained with noise injection at character level during pre-training, the sensitivity gap may shrink or reverse.

### Mechanism 2
- Claim: Larger models are more robust to tokenization errors but still vulnerable to certain failure modes.
- Mechanism: Scaling increases the model's capacity to generalize across subword representations, allowing it to infer correct outputs despite token boundary noise. However, deeper architectural constraints (e.g., decoupled embeddings) still limit surface-form understanding.
- Core assumption: Model capacity scales with robustness to tokenization issues, but not to the point of eliminating them.
- Evidence anchors:
  - [abstract] "scaling model parameters can mitigate the issue of tokenization; however, LLMs still suffer from biases induced by typos and other text format variations."
  - [section 3.1] "larger models demonstrate better performance on the anagram task, yet they remain susceptible to tokenization errors."
  - [corpus] No direct evidence; related works discuss subword composition but not scaling robustness.
- Break condition: If tokenization schemes evolve to preserve surface form (e.g., visual tokenization), the scaling benefit may plateau.

### Mechanism 3
- Claim: BPE-dropout with moderate dropout rates (p=0.2) improves model robustness to tokenization variations.
- Mechanism: By randomly dropping BPE merges during tokenization, the model is exposed to diverse subword segmentations during training. This regularizes the model's understanding of subword boundaries and enhances generalization.
- Core assumption: Training-time exposure to tokenization variability improves test-time robustness to unseen tokenizations.
- Evidence anchors:
  - [abstract] "Our experiments show that subword regularization such as BPE-dropout can mitigate this issue."
  - [section 6] "The introduction of a moderate BPE-dropout rate (p = 0.2) frequently surpasses the baseline, highlighting the benefits of inducing variability during tokenization."
  - [corpus] No direct evidence; related works discuss BPE-dropout but not its efficacy on this specific robustness problem.
- Break condition: If dropout rate is too high (p ≥ 0.6), tokenization becomes too unstable, leading to degradation in performance.

## Foundational Learning

- Concept: Subword tokenization (e.g., Byte Pair Encoding)
  - Why needed here: The entire study hinges on how tokenization affects model behavior. Understanding how subwords are generated and used is essential.
  - Quick check question: How does BPE decide which subword pairs to merge during training?

- Concept: Few-shot learning and in-context examples
  - Why needed here: The study evaluates models across 0-shot to 3-shot settings to understand how demonstration examples affect performance on tokenization-sensitive tasks.
  - Quick check question: What is the difference between few-shot learning and fine-tuning?

- Concept: Evaluation metrics (EM, edit distance, perplexity)
  - Why needed here: The study uses multiple metrics to assess model robustness and accuracy under different perturbations.
  - Quick check question: When would you use perplexity vs. exact match as an evaluation metric?

## Architecture Onboarding

- Component map: Tokenizer -> Embedding Lookup -> Transformer Layers -> Output Head
- Critical path: Input text -> Tokenizer -> Token IDs -> Embeddings -> Transformer -> Logits -> Loss
- Design tradeoffs: Fixed vocabulary vs. dynamic tokenization; subword granularity vs. out-of-vocabulary coverage
- Failure signatures: High perplexity on noisy inputs; poor exact match on anagram tasks; sensitivity to character permutations
- First 3 experiments:
  1. Run the TruthfulQA benchmark with character-level noise injection (n=2,3,5) and measure exact match degradation.
  2. Train a Mistral-7B model with BPE-dropout (p=0.2) and compare performance on token structure probing tasks vs. baseline.
  3. Evaluate GSM8K with token-level permutation (n=2,3,5) and analyze the correlation between original token length and post-corruption length.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal BPE-dropout rate for balancing regularization benefits and token integrity across different task complexities?
- Basis in paper: [explicit] The paper demonstrates that moderate BPE-dropout rates (0.2 to 0.4) improve generalization, while higher rates (0.6 and 0.8) lead to performance degradation.
- Why unresolved: The paper shows a range of effects but does not identify a precise optimal rate, and the effects vary across different task complexities.
- What evidence would resolve it: Systematic experiments varying BPE-dropout rates on a wide range of tasks and model architectures to identify the rate that maximizes performance while maintaining token integrity.

### Open Question 2
- Question: How do different tokenization strategies (e.g., WordPiece, Unigram, Byte-level) compare in handling typographical variations and internal token structure compared to BPE?
- Basis in paper: [inferred] The paper focuses on BPE and BPE-dropout but does not compare these strategies to other tokenization methods like WordPiece or Unigram.
- Why unresolved: The paper does not provide a comparative analysis of different tokenization strategies and their robustness to typographical variations and internal token structure.
- What evidence would resolve it: Comprehensive evaluation of multiple tokenization strategies on the same benchmarks used in the paper to assess their relative performance and robustness.

### Open Question 3
- Question: How does the sensitivity to typographical variations change with multilingual or cross-lingual tasks compared to monolingual English tasks?
- Basis in paper: [inferred] The paper evaluates tokenization robustness on English datasets but does not explore multilingual or cross-lingual scenarios.
- Why unresolved: The study focuses on English datasets, and the impact of typographical variations on multilingual or cross-lingual tasks is not addressed.
- What evidence would resolve it: Evaluation of tokenization robustness on multilingual datasets or cross-lingual benchmarks to determine if the sensitivity to typographical variations is consistent across languages.

## Limitations

- Temporal Generalization: The study evaluates models up to 70B parameters, but doesn't test how these findings scale to models with trillions of parameters.
- Task Specificity: Results are based on four datasets (MMLU, TruthfulQA, GSM8K, HumanEval) and may vary significantly across different task types.
- Tokenization Scheme Dependency: The study focuses on BPE tokenization and results may differ for models using SentencePiece, WordPiece, or other tokenization schemes.

## Confidence

**High Confidence**:
- LLMs are more sensitive to character-level variations than subword-level variations
- BPE-dropout with moderate rates (p=0.2) improves robustness
- Larger models show better performance on tokenization-sensitive tasks
- Tokenization errors significantly impact LLM performance on complex reasoning tasks

**Medium Confidence**:
- Scaling model parameters can mitigate tokenization issues (supported by evidence but limited to 70B parameter models)
- The proposed evaluation framework comprehensively captures tokenization robustness issues
- The sensitivity hierarchy (character > subword > clean) generalizes across all LLM architectures

**Low Confidence**:
- The specific dropout rate of p=0.2 is optimal for all scenarios
- These findings will remain consistent as models scale beyond 100B parameters
- The results will generalize identically to all tokenization schemes

## Next Checks

1. **Cross-tokenization validation**: Replicate the main experiments using models with different tokenization schemes (SentencePiece, WordPiece) to verify whether the sensitivity hierarchy and BPE-dropout benefits generalize beyond BPE.

2. **Scaling boundary test**: Evaluate the same corruption tasks on the largest available models (e.g., GPT-4, Claude, Gemini) to determine if the observed sensitivity trends continue to hold at extreme scales, or if they plateau.

3. **Domain expansion validation**: Apply the evaluation framework to domain-specific datasets (medical, legal, technical) and tasks requiring specialized vocabulary to determine if tokenization robustness correlates with vocabulary size or domain complexity.