---
ver: rpa2
title: 'HtmlRAG: HTML is Better Than Plain Text for Modeling Retrieved Knowledge in
  RAG Systems'
arxiv_id: '2411.02959'
source_url: https://arxiv.org/abs/2411.02959
tags:
- html
- block
- text
- tree
- computational
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces HtmlRAG, a retrieval-augmented generation
  (RAG) system that uses HTML documents directly instead of plain text. Traditional
  RAG systems lose structural and semantic information when converting HTML to plain
  text.
---

# HtmlRAG: HTML is Better Than Plain Text for Modeling Retrieved Knowledge in RAG Systems

## Quick Facts
- arXiv ID: 2411.02959
- Source URL: https://arxiv.org/abs/2411.02959
- Reference count: 40
- HtmlRAG outperforms existing RAG systems that rely on plain text by preserving HTML structure

## Executive Summary
HtmlRAG introduces a novel approach to retrieval-augmented generation by using HTML documents directly rather than converting them to plain text. Traditional RAG systems lose critical structural and semantic information during HTML-to-text conversion, including headings, tables, and tag-based context. HtmlRAG preserves this information by leveraging LLMs' inherent HTML understanding from pretraining. To handle the challenge of long HTML documents, the system implements an HTML cleaning module and a two-step block-tree-based pruning strategy that first uses text embeddings for coarse-grained pruning, then employs a generative model for finer-grained refinement. Experiments on six QA datasets demonstrate that HtmlRAG outperforms existing RAG systems.

## Method Summary
HtmlRAG processes HTML documents through a two-stage pipeline: HTML cleaning and block-tree-based pruning. The cleaning module removes CSS, JavaScript, comments, and redundant tags while preserving structural information. The block tree construction merges DOM nodes to fit within token limits. Embedding pruning then removes low-similarity coarse blocks using text embeddings. Finally, generative pruning refines the results by scoring fine-grained blocks through token-tree traversal and path generation probability. The system uses pre-trained LLMs (Llama-3.1-8B/70B-Instruct) as readers and retrieves HTML documents via Bing search API.

## Key Results
- HtmlRAG outperforms plain-text-based RAG systems on six QA datasets (ASQA, Hotpot-QA, NQ, Trivia-QA, MuSiQue, ELI5)
- The two-step pruning strategy effectively balances efficiency and accuracy
- HTML preservation provides richer semantic context than plain text conversion

## Why This Works (Mechanism)

### Mechanism 1
- Claim: HTML documents contain richer structural and semantic information than plain text, which helps LLMs generate better answers.
- Mechanism: HTML preserves headings, tables, and tag-based context that plain text loses during conversion, giving LLMs more context-aware cues.
- Core assumption: LLMs have sufficient HTML understanding from pretraining to leverage this structural information without additional fine-tuning.
- Evidence anchors: [abstract] "much of the structural and semantic information inherent in HTML, such as headings and table structures, is lost during this plain-text-based RAG process." [section 1] "During pre-training, LLMs have encountered HTML documents, which means that they inherently possess the ability to understand HTML without requiring further fine-tuning."
- Break condition: If the LLM's pretraining corpus lacked significant HTML exposure or the task requires precise table/column parsing beyond typical HTML understanding.

### Mechanism 2
- Claim: The two-step block-tree-based pruning strategy balances efficiency and effectiveness by first using embedding similarity and then refining with a generative model.
- Mechanism: Coarse embedding pruning removes obviously irrelevant blocks cheaply; generative pruning then re-evaluates remaining fine-grained blocks using the global context.
- Core assumption: Embedding models can handle coarse blocks but struggle with fine-grained ones; generative models can model longer contexts globally.
- Evidence anchors: [section 3.4.1] "The embedding model’s context window is limited to the scope of text within the block each time... The embedding model cannot handle block trees with finer granularity." [section 3.4.2] "The generative model has a long context to cover the whole block tree and is not limited to modeling one block at a time."
- Break condition: If embedding similarity is highly accurate even on fine blocks or if generative model context window is insufficient.

### Mechanism 3
- Claim: HTML cleaning and lossless structural compression significantly reduce token count without losing semantic information.
- Mechanism: Removing CSS, JS, comments, and merging redundant tags shrinks HTML while keeping semantic structure intact.
- Core assumption: HTML tags that provide structure are semantically valuable to LLMs, while embedded scripts/styles are not.
- Evidence anchors: [section 3.2.1] "Most of the HTML tags provide rich structural information that helps the LLM understand the HTML, while CSS and JavaScript content provide limited assistance." [section 3.2.2] "We merge multiple layers of single-nested tags... removed empty tags."
- Break condition: If downstream model relies on embedded scripts for computation or if tags are stripped that carry critical semantic meaning.

## Foundational Learning

- Concept: DOM tree vs. block tree granularity.
  - Why needed here: Pruning directly on DOM tree is too fine-grained and computationally expensive; block tree offers tunable granularity.
  - Quick check question: How does increasing maxWords affect the number of blocks and pruning accuracy?

- Concept: Embedding similarity for relevance scoring.
  - Why needed here: Embedding similarity is a lightweight proxy for relevance that works well on coarse blocks but degrades on small ones.
  - Quick check question: Why might embedding similarity fail on a single sentence block?

- Concept: Token tree construction for efficient generative pruning.
  - Why needed here: Mapping block paths to token sequences allows dynamic skipping of nodes with no siblings, reducing inference cost.
  - Quick check question: What is the benefit of reusing prefix sequences when traversing the token tree?

## Architecture Onboarding

- Component map: Retriever → Bing API → HTML documents → HTML Cleaning → Block Tree Construction → Embedding Pruning → Generative Pruning → LLM Reader
- Critical path: Retriever → HTML Cleaning → Block Tree → Embedding Pruning → Generative Pruning → LLM
- Design tradeoffs:
  - HTML vs. plain text: higher token count but richer semantics vs. lower token count but information loss
  - Embedding-only vs. generative pruning: speed vs. accuracy on fine-grained blocks
  - Block tree granularity: smaller blocks = finer pruning but more computation
- Failure signatures:
  - Embedding pruning removes all relevant blocks → no useful context left
  - Generative pruning fails to score blocks correctly → noisy final HTML
  - HTML cleaning over-prunes → loss of required structural tags
- First 3 experiments:
  1. Compare HTML vs. plain text retrieval accuracy on a small QA dataset to validate semantic retention claim
  2. Test embedding pruning alone vs. combined pruning on block granularity sweep to find sweet spot
  3. Benchmark token count reduction after each cleaning step to ensure lossless compression

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the block tree granularity parameter (maxWords) affect the trade-off between computational efficiency and retrieval accuracy across different types of questions (e.g., short-answer vs. long-form)?
- Basis in paper: [explicit] The paper discusses varying granularity (64 to 512 words) and observes that the generative model adapts to finer granularity than the embedding model, but doesn't provide detailed analysis on question-type-specific impacts.
- Why unresolved: The experiments show overall performance trends but don't isolate how granularity affects different question types differently, which would be crucial for optimizing the system for specific use cases.
- What evidence would resolve it: Systematic experiments varying maxWords specifically for short-answer QA datasets (like NQ, TriviaQA) versus long-form datasets (like ELI5), measuring both accuracy metrics and computational costs for each combination.

### Open Question 2
- Question: What is the optimal balance between HTML cleaning depth and information retention for different web page types (e.g., news articles vs. technical documentation vs. forums)?
- Basis in paper: [explicit] The paper describes HTML cleaning rules but doesn't explore how different cleaning intensities affect different content types, mentioning only that "real HTML document from the Web contains over 80K tokens on average."
- Why unresolved: The cleaning process removes CSS, JavaScript, and compresses structures, but different web page types have varying structural importance - aggressive cleaning might harm some content types more than others.
- What evidence would resolve it: Comparative experiments applying different cleaning intensities to various web page categories, measuring both information retention (through EM scores) and final QA performance across each category.

### Open Question 3
- Question: How does the two-stage pruning approach compare to end-to-end generative approaches in terms of computational cost and accuracy for extremely long documents (>100K tokens)?
- Basis in paper: [explicit] The paper compares its two-stage approach to various baselines but doesn't explore scenarios with extremely long documents where even cleaned HTML might exceed practical context windows.
- Why unresolved: The experiments focus on cleaned HTML around 60K tokens and pruned versions around 4-8K tokens, but don't test the approach's scalability to documents requiring multiple rounds of pruning or alternative strategies.
- What evidence would resolve it: Experiments with artificially extended documents (concatenated to exceed 100K tokens) comparing the two-stage approach against iterative pruning strategies and end-to-end generative compression methods, measuring both accuracy degradation and computational scaling.

## Limitations
- The paper lacks ablation studies comparing HTML vs. plain text retrieval directly
- Effectiveness depends heavily on the quality of both embedding and generative models
- HTML cleaning parameters and thresholds are not thoroughly explored

## Confidence

- **High Confidence**: The technical approach of using HTML directly and implementing pruning strategies is well-defined and reproducible. The improvement over existing RAG systems is demonstrated across multiple datasets.
- **Medium Confidence**: The mechanisms explaining why HTML preservation helps are plausible but not fully validated through controlled experiments. The assumption that LLMs can effectively leverage HTML structure without fine-tuning needs further empirical support.
- **Low Confidence**: The scalability and generalization of the approach to domains with complex HTML structures or non-standard tag usage is unclear, as the evaluation focuses on web documents and QA tasks.

## Next Checks

1. Conduct an ablation study comparing retrieval accuracy using original HTML vs. plain text versions of the same documents to quantify the structural information contribution.
2. Test the system on HTML documents with complex nested structures and non-standard tags to evaluate robustness and identify failure modes in HTML cleaning and parsing.
3. Evaluate the impact of different block tree granularities and pruning thresholds on both performance and computational efficiency to identify optimal configuration parameters.