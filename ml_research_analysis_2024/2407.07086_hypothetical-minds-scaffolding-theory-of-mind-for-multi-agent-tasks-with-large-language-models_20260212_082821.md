---
ver: rpa2
title: 'Hypothetical Minds: Scaffolding Theory of Mind for Multi-Agent Tasks with
  Large Language Models'
arxiv_id: '2407.07086'
source_url: https://arxiv.org/abs/2407.07086
tags:
- strategy
- opponent
- agent
- player
- inventory
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Hypothetical Minds integrates LLM-based perception, memory, and
  hierarchical planning with a Theory of Mind module that generates, evaluates, and
  refines hypotheses about other agents' strategies in natural language. It uses intrinsic
  rewards based on prediction accuracy and a validation threshold to determine when
  to commit to a hypothesis for planning.
---

# Hypothetical Minds: Scaffolding Theory of Mind for Multi-Agent Tasks with Large Language Models

## Quick Facts
- arXiv ID: 2407.07086
- Source URL: https://arxiv.org/abs/2407.07086
- Authors: Logan Cross; Violet Xiang; Agam Bhatia; Daniel LK Yamins; Nick Haber
- Reference count: 40
- Key outcome: Outperforms PPO, OPRE, and LLM-agent baselines across competitive, collaborative, mixed-motive, and large-population environments in the Melting Pot benchmark

## Executive Summary
Hypothetical Minds introduces a cognitively-inspired LLM-based agent architecture that uses a Theory of Mind module to generate, evaluate, and refine hypotheses about other agents' strategies in natural language. The system integrates perception, memory, and hierarchical planning with hypothesis-driven decision making, using intrinsic rewards based on prediction accuracy to guide learning. On the Melting Pot benchmark, it demonstrates significant performance improvements across various multi-agent scenarios including competitive (RWS: +40 reward), collaborative (Cooking: +400 reward), mixed-motive (Prisoner's Dilemma: +30 reward), and large-population (RWS Arena: +15 reward) environments.

## Method Summary
The method employs an LLM-based agent with a cognitively-inspired architecture featuring modular components for perception, memory, and hierarchical planning over two levels of abstraction. The core innovation is a Theory of Mind module that generates natural language hypotheses about other agents' strategies, evaluates them using prediction accuracy via intrinsic rewards, and refines them through iterative learning. The system uses a Rescorla-Wagner update rule to dynamically adjust hypothesis values based on whether predictions about opponent behavior match observed actions, with a validation threshold determining when to commit to a hypothesis for high-level planning.

## Key Results
- HM outperforms PPO, OPRE, and LLM-agent baselines across all tested Melting Pot environments
- Competitive environments: +40 reward in Running With Scissors Repeated
- Collaborative environments: +400 reward in Collaborative Cooking Asymmetric
- Mixed-motive environments: +30 reward in Prisoner's Dilemma Repeated
- Large-population environments: +15 reward in Running With Scissors Arena with 8 agents
- Computational cost scales linearly when hypotheses are not validated and sublinearly when frequently validated

## Why This Works (Mechanism)

### Mechanism 1
The Theory of Mind module generates, evaluates, and refines hypotheses about other agents' strategies, enabling adaptive behavior in multi-agent environments. The ToM module creates natural language hypotheses about latent variables (strategies, goals, competence) of other agents, scores them using prediction accuracy via intrinsic rewards, and selects validated hypotheses to condition high-level planning. This works because language models can effectively model and predict other agents' behavior through structured reasoning in natural language.

### Mechanism 2
Hierarchical planning over two levels of abstraction allows the agent to adapt its strategy based on inferred latent variables. The high-level planning module conditions its plans on validated hypotheses about opponent strategies, while the subgoal module decomposes these into actionable steps for the embodied agent. This separation enables better adaptation to opponent behaviors by maintaining strategic coherence while allowing tactical flexibility.

### Mechanism 3
Intrinsic rewards based on prediction accuracy provide a self-supervised learning signal for hypothesis refinement. The ToM module uses a Rescorla-Wagner update rule to dynamically adjust hypothesis values based on whether predictions about opponent behavior match observed actions, with a validation threshold determining when to commit to a hypothesis. This self-supervised approach effectively guides the learning of opponent models without external supervision.

## Foundational Learning

- Concept: Theory of Mind in cognitive science
  - Why needed here: Provides the conceptual foundation for modeling other agents' mental states and strategies
  - Quick check question: What is the difference between first-order and second-order Theory of Mind, and why is first-order sufficient for this work?

- Concept: Bayesian inference for belief updating
  - Why needed here: Underlies the hypothesis evaluation mechanism where beliefs about opponent strategies are updated based on observed evidence
  - Quick check question: How does the Rescorla-Wagner rule relate to Bayesian updating, and what are the trade-offs between these approaches?

- Concept: Intrinsic motivation in reinforcement learning
  - Why needed here: Provides the theoretical basis for using self-supervised prediction accuracy as a learning signal
  - Quick check question: What are the advantages and disadvantages of using intrinsic rewards versus extrinsic rewards for guiding agent behavior?

## Architecture Onboarding

- Component map: Perception module → Memory system → Theory of Mind module → High-level planning module → Subgoal module → Action planner
- Critical path: ToM → High-level planning → Subgoal → Action planner
- Design tradeoffs:
  - Top-k hypotheses parameter trades off computational cost against robustness to hypothesis failure
  - Validation threshold determines when to commit to a hypothesis versus continuing to generate alternatives
  - Separate hypothesis streams per opponent enable handling multiple opponents but increase memory/computation
- Failure signatures:
  - Poor performance on static strategies suggests hypothesis generation is not recognizing predictable patterns
  - Inconsistent performance across episodes suggests validation threshold is misconfigured
  - High computational costs suggest top-k parameter is set too high
- First 3 experiments:
  1. Run HM on RWS Repeated with top-k=0 (no hypothesis evaluation) to establish baseline performance
  2. Enable hypothesis evaluation with top-k=1 to test if maintaining any history improves performance
  3. Increase top-k to 5 and measure performance/cost trade-off curve

## Open Questions the Paper Calls Out

### Open Question 1
How does the hypothesis validation threshold Vthr impact performance across different environments and agent types? The paper mentions that Vthr = 0.7 was set apriori based on analysis of Rescorla-Wagner dynamics and shows results for this value, but only tests one threshold value. Systematic testing of multiple threshold values across all environments would show how performance varies with threshold choice.

### Open Question 2
How does the hypothesis evaluation mechanism scale to environments with more than 8 agents or in continuous state/action spaces? The paper demonstrates HM works in RWS Arena with 8 agents but doesn't test larger populations or continuous environments. Experiments with 16+ agents and/or continuous action spaces would reveal scalability limitations.

### Open Question 3
How much of HM's performance advantage comes from the hierarchical planning structure versus the specific Theory of Mind module? While PlanReAct performs worse than HM, the paper doesn't systematically test whether the hierarchical structure itself provides benefits independent of the ToM module. A controlled experiment comparing HM against a non-hierarchical baseline would isolate these contributions.

## Limitations
- Evaluation scope limited primarily to the Melting Pot benchmark, which may not generalize to real-world multi-agent scenarios
- Computational overhead of LLM-based reasoning may limit practical deployment in resource-constrained environments
- Performance appears sensitive to specific prompts and hyperparameters used in the Theory of Mind module

## Confidence
- High confidence: The core mechanism of using LLM-based hypothesis generation and evaluation to inform planning is technically sound and well-implemented
- Medium confidence: The comparative performance advantages over baselines are well-demonstrated on the specific benchmark, but may not generalize to other multi-agent environments
- Low confidence: The scalability claims regarding computational cost are based on theoretical analysis rather than extensive empirical validation across diverse scenarios

## Next Checks
1. Test the approach on additional multi-agent benchmarks beyond Melting Pot to assess generalization capabilities
2. Conduct ablation studies specifically isolating the impact of the validation threshold and top-k parameters on both performance and computational efficiency
3. Evaluate the approach's robustness to noisy observations and partial observability conditions to assess real-world applicability