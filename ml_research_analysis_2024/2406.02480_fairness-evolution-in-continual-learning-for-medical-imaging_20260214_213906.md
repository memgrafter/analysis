---
ver: rpa2
title: Fairness Evolution in Continual Learning for Medical Imaging
arxiv_id: '2406.02480'
source_url: https://arxiv.org/abs/2406.02480
tags:
- learning
- performance
- tasks
- fairness
- task
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study addresses fairness in continual learning for medical
  imaging, focusing on bias evolution across tasks. Using class-incremental scenarios
  on CheXpert and ChestX-ray14 datasets, the research evaluates how different CL strategies
  influence fairness over time.
---

# Fairness Evolution in Continual Learning for Medical Imaging

## Quick Facts
- **arXiv ID**: 2406.02480
- **Source URL**: https://arxiv.org/abs/2406.02480
- **Reference count**: 40
- **Primary result**: Pseudo-Label achieves optimal classification performance while maintaining better fairness across gender groups in continual learning for medical imaging.

## Executive Summary
This study addresses fairness in continual learning for medical imaging, focusing on bias evolution across tasks. Using class-incremental scenarios on CheXpert and ChestX-ray14 datasets, the research evaluates how different CL strategies influence fairness over time. The work introduces domain-specific fairness metrics alongside classification performance, examining true positive rate disparities across gender and age groups.

The core finding reveals that Pseudo-Label strategy achieves the best balance between classification accuracy and fairness, outperforming other CL strategies in both metrics. The study demonstrates that fairness considerations are crucial in CL settings, as different strategies exhibit varying degrees of bias evolution across demographic groups, with some strategies showing systematic bias against minority groups including females, Hispanic people, and those with lower socioeconomic status.

## Method Summary
The study compares five continual learning strategies (Fine-Tuning, Replay, Learning without Forgetting, Pseudo-Label, and LwF Replay) on class-incremental multi-label pathology classification tasks using CheXpert and ChestX-ray14 datasets. Each dataset is split into 5 tasks containing 2-3 pathologies each. The evaluation framework measures both classification performance using average ROC AUC and fairness using TPR disparities across gender and age groups. The Pseudo-Label strategy uses binarized predictions from the previous model as pseudo-ground truth labels for old tasks, while LwF applies knowledge distillation to retain old knowledge.

## Key Results
- Pseudo-Label achieves optimal classification performance (highest average AUC) while maintaining better fairness across gender groups
- LwF and Pseudo-Label demonstrate superior classification results compared to other CL strategies
- Fairness metrics reveal systematic bias evolution, with some strategies showing TPR disparities favoring majority groups
- Replay strategy shows limited performance improvement over Fine-Tuning due to label interference in multi-label scenarios

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Pseudo-Label strategy mitigates catastrophic forgetting better than Replay in multi-label continual learning.
- **Mechanism**: Pseudo-Label generates pseudo-ground truth labels for old classes using the previous model's predictions, allowing the model to retain knowledge without requiring stored old data. This avoids the label interference problem that occurs in Replay when the same images are associated with different labels across tasks.
- **Core assumption**: The pseudo-labels generated by the previous model are sufficiently accurate to guide learning without introducing significant noise.
- **Evidence anchors**: 
  - [section] "The evidence supporting this is illustrated in Figure 2, where the performance of the Replay method only marginally outperforms that of the Fine-Tuning approach... For this setting, it can happen that the same (or similar) images are seen in different tasks, each with different labels, causing interference in the learning of the model."
  - [corpus] "Incremental learning of object detectors without catastrophic forgetting" (cited as foundational for Pseudo-Label approach)
- **Break condition**: Pseudo-labels become unreliable due to significant distribution shift or when the previous model's performance degrades substantially.

### Mechanism 2
- **Claim**: Knowledge distillation in LwF helps retain old knowledge while learning new classes in continual learning.
- **Mechanism**: LwF adds a distillation loss term that encourages the current model to produce similar outputs for old tasks as the previous model did, preserving learned representations while adapting to new data.
- **Core assumption**: The distillation loss effectively constrains parameter updates to protect important weights for old tasks.
- **Evidence anchors**:
  - [section] "LwF is based on the idea of Knowledge Distillation proposed originally by [19]. In distillation, the knowledge learned by a model on previous tasks is distilled into a compact form (e.g., soft target probabilities) and used during the training of the new task to retain the old knowledge."
  - [corpus] "Learning without forgetting" (foundational paper for LwF approach)
- **Break condition**: When new task data is highly dissimilar from old task data, distillation may not provide sufficient constraint.

### Mechanism 3
- **Claim**: Fairness metrics reveal bias evolution that classification accuracy alone cannot detect in continual learning.
- **Mechanism**: By computing TPR disparities across demographic groups (gender, age) after each task, the study identifies how different CL strategies affect fairness over time, revealing that some strategies that perform well on accuracy may actually increase bias.
- **Core assumption**: TPR disparity is a meaningful fairness metric for medical diagnosis where underdiagnosis of minority groups has serious consequences.
- **Evidence anchors**:
  - [section] "Our main finding is that in state-of-the-art classifiers, there's a systematic bias that penalizes these minority groups: females, Hispanic people, people with Medicaid insurance, which is a proxy for low socioeconomic status, and people younger than 20."
  - [section] "The study of Fairness in medical AI is rather recent [10]. [9] discuss the importance of choosing the most appropriate definition of Fairness in the medical field."
- **Break condition**: When the dataset lacks sufficient representation of minority groups to compute meaningful TPR disparities.

## Foundational Learning

- **Concept**: Catastrophic forgetting
  - **Why needed here**: Understanding why continual learning strategies are necessary to prevent models from losing previously learned knowledge when trained on new tasks
  - **Quick check question**: What happens to a model's performance on old tasks when it's fine-tuned on new data without any continual learning strategy?

- **Concept**: Knowledge distillation
  - **Why needed here**: LwF and related strategies rely on distilling knowledge from the previous model to preserve old task performance
  - **Quick check question**: How does knowledge distillation help a model retain performance on previous tasks while learning new ones?

- **Concept**: Fairness metrics in medical AI
  - **Why needed here**: The study introduces domain-specific fairness metrics (TPR disparity) that are appropriate for medical diagnosis scenarios where underdiagnosis is a critical concern
  - **Quick check question**: Why is demographic parity not suitable for medical diagnosis fairness evaluation?

## Architecture Onboarding

- **Component map**: Backbone CNN for feature extraction → Task-specific heads for multi-label classification → Continual learning strategy module (Fine-Tuning, Replay, LwF, Pseudo-Label, or LwF Replay) → Fairness evaluation module that computes TPR disparities across demographic groups
- **Critical path**: Data → Model initialization → Task loop (for each task: load data, apply CL strategy, evaluate performance and fairness) → Final evaluation across all tasks
- **Design tradeoffs**: Using pseudo-labels trades off potential label noise for avoiding catastrophic forgetting, while knowledge distillation trades off some flexibility in learning new tasks for better retention of old knowledge. The choice of fairness metric (TPR disparity vs. other metrics) depends on the medical application context.
- **Failure signatures**: Poor AUC indicates catastrophic forgetting; large TPR disparities indicate fairness issues; inconsistent performance across demographic groups suggests bias evolution; poor performance on multi-label tasks suggests interference between labels.
- **First 3 experiments**:
  1. Implement Fine-Tuning baseline and verify catastrophic forgetting occurs as expected
  2. Implement LwF strategy and compare AUC retention vs Fine-Tuning
  3. Implement Pseudo-Label strategy and compare both AUC and fairness metrics (TPR disparity) vs LwF

## Open Questions the Paper Calls Out

- **Open Question 1**: How do fairness metrics evolve when using continual learning with non-medical datasets that have different demographic distributions?
  - **Basis in paper**: [explicit] The paper mentions that fairness metrics were evaluated on medical imaging datasets but does not explore other domains
  - **Why unresolved**: The study is limited to medical imaging datasets and does not investigate how fairness metrics behave in other domains
  - **What evidence would resolve it**: Experiments showing fairness evolution across different non-medical datasets with varying demographic distributions

- **Open Question 2**: What is the long-term impact of different continual learning strategies on fairness when the number of tasks significantly increases?
  - **Basis in paper**: [inferred] The paper evaluates five tasks but mentions that real-world applications may involve more complex scenarios
  - **Why unresolved**: The study only examines a limited number of tasks and does not explore scenarios with many more tasks
  - **What evidence would resolve it**: Long-term experiments with a large number of tasks showing how fairness metrics change over time

- **Open Question 3**: How do different data augmentation techniques affect fairness in continual learning scenarios?
  - **Basis in paper**: [inferred] The paper does not explore the impact of data augmentation on fairness metrics
  - **Why unresolved**: The study focuses on comparing different continual learning strategies but does not investigate data augmentation methods
  - **What evidence would resolve it**: Experiments comparing fairness metrics across different data augmentation techniques in continual learning scenarios

- **Open Question 4**: What is the optimal balance between classification performance and fairness metrics in continual learning?
  - **Basis in paper**: [explicit] The paper notes that Pseudo-Label performs better in both classification and fairness but mentions there is still a gap from optimal performance
  - **Why unresolved**: The study identifies a trade-off but does not determine the optimal balance point
  - **What evidence would resolve it**: Systematic experiments varying the balance between classification and fairness objectives to find optimal points

## Limitations

- The pseudo-label threshold value (τ) is not specified, which could affect fairness metric calculations
- Hyperparameter details (learning rate, batch size, distillation weight λ) are omitted, potentially impacting reproducibility
- The study focuses only on gender and age as fairness dimensions, potentially missing other important demographic factors
- Dataset imbalance across demographic groups may affect the reliability of fairness metrics

## Confidence

- **High**: Classification performance comparisons between CL strategies (AUC values, relative rankings)
- **Medium**: Fairness metric trends and conclusions about bias evolution across tasks
- **Low**: Causal claims about why specific strategies affect fairness metrics differently

## Next Checks

1. Verify pseudo-label threshold sensitivity by testing multiple τ values and measuring impact on both classification accuracy and TPR disparities
2. Conduct ablation studies on distillation weight λ in LwF and LwF Replay to determine optimal values for balancing performance and fairness
3. Analyze dataset demographic distributions task-by-task to confirm that fairness metric patterns aren't artifacts of representation imbalance