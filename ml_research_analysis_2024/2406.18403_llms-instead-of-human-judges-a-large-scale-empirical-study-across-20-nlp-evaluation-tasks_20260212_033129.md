---
ver: rpa2
title: LLMs instead of Human Judges? A Large Scale Empirical Study across 20 NLP Evaluation
  Tasks
arxiv_id: '2406.18403'
source_url: https://arxiv.org/abs/2406.18403
tags:
- human
- datasets
- computational
- dataset
- association
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study evaluated whether large language models (LLMs) can reliably
  replace human judges for assessing NLP outputs across 20 diverse datasets and 11
  different models. The researchers found that while LLMs sometimes align well with
  human judgments on specific tasks like instruction following, their performance
  is highly inconsistent across different datasets, properties, and types of language
  (human- vs.
---

# LLMs instead of Human Judges? A Large Scale Empirical Study across 20 NLP Evaluation Tasks

## Quick Facts
- arXiv ID: 2406.18403
- Source URL: https://arxiv.org/abs/2406.18403
- Reference count: 38
- LLMs sometimes align with human judgments on specific tasks but show high variability across datasets and require careful validation before use as evaluators

## Executive Summary
This large-scale empirical study investigates whether large language models (LLMs) can reliably replace human judges for evaluating natural language processing (NLP) outputs. The researchers conducted experiments across 20 diverse datasets and 11 different models, testing various prompting strategies including Chain-of-Thought. They found that while proprietary models like GPT-4o generally performed best, agreement with human judgments was highly variable depending on the task, property being evaluated, and whether the language was human- or model-generated. The study reveals that LLMs align better with non-expert human judges than experts, and that performance varies significantly across different evaluation scenarios.

## Method Summary
The researchers conducted a systematic evaluation of LLMs as judges for NLP tasks using 20 diverse datasets covering various properties (fluency, coherence, relevance, etc.) and 11 different language models including both proprietary and open-weight models. They employed Chain-of-Thought prompting and tested pairwise ranking as well as absolute scoring methods. The evaluation compared LLM judgments against human annotations from both expert and non-expert annotators, measuring agreement through various correlation metrics. The study also distinguished between human-generated and model-generated text to assess how the source of the language affects LLM evaluation performance.

## Key Results
- Proprietary models like GPT-4o generally outperformed open-weight models, though the margin was not always substantial
- LLMs showed better agreement with non-expert human judges than with expert annotators
- Performance was significantly better when evaluating human-generated text compared to model-generated outputs
- Chain-of-Thought prompting did not consistently improve agreement across all tasks and conditions

## Why This Works (Mechanism)
LLMs can serve as judges for NLP outputs because they have been trained on vast amounts of human language data, allowing them to develop implicit understanding of language quality, coherence, and relevance. Their ability to process and reason about text enables them to evaluate various linguistic properties systematically. The study demonstrates that LLMs can capture human judgment patterns in many cases, particularly when evaluating human-generated content where the language patterns align with their training distribution.

## Foundational Learning
- **Pairwise ranking evaluation**: Comparing two outputs to determine which is better; needed for controlled comparisons where relative quality matters more than absolute scores; quick check: examine correlation between LLM rankings and human preferences
- **Chain-of-Thought prompting**: A technique where models are encouraged to reason step-by-step before providing an answer; needed to potentially improve reasoning and reduce bias; quick check: compare performance with and without CoT across different task types
- **Human vs. model-generated text evaluation**: Different performance patterns when evaluating human versus AI-generated content; needed to understand domain generalization capabilities; quick check: analyze agreement differences across these text types
- **Expert vs. non-expert human agreement**: Variation in how well LLMs align with different types of human judges; needed to understand evaluation reliability across expertise levels; quick check: compare correlation metrics between expert and non-expert annotations
- **Proprietary vs. open-weight model comparison**: Performance differences between commercial and freely available models; needed to assess accessibility of high-quality evaluation; quick check: benchmark multiple models on identical tasks
- **Correlation metrics for evaluation**: Statistical measures (Kendall's tau, Spearman's rho) used to quantify agreement between LLM and human judgments; needed to provide objective performance assessment; quick check: verify metric calculations and interpretation

## Architecture Onboarding
**Component Map**: Human Annotations -> LLM Evaluations -> Correlation Analysis -> Task Performance Assessment
**Critical Path**: The evaluation pipeline follows: select dataset → generate LLM judgments → compare with human annotations → compute correlation metrics → analyze performance patterns
**Design Tradeoffs**: Proprietary models offer better performance but cost more and raise privacy concerns, while open-weight models are more accessible but may underperform on complex tasks; pairwise evaluation is more controlled but less scalable than absolute scoring
**Failure Signatures**: Poor performance on model-generated text suggests distributional shift issues; better alignment with non-experts indicates potential oversimplification of evaluation criteria; inconsistent CoT benefits suggest task-specific prompting needs
**First Experiments**: 1) Benchmark a new dataset with both pairwise and absolute scoring methods; 2) Compare performance of GPT-4o and a leading open-weight model on a medical text evaluation task; 3) Test different prompting strategies (including few-shot examples) beyond Chain-of-Thought on a coherence assessment task

## Open Questions the Paper Calls Out
None identified in the provided materials.

## Limitations
- The study's findings are based on a specific set of 20 datasets and 11 models, which may not generalize to other domains or newer model versions
- Evaluation focused primarily on pairwise comparisons and rank correlations, potentially missing aspects of evaluation quality for tasks requiring absolute scoring or nuanced qualitative assessment
- Limited number of expert datasets (only 3) constrains conclusions about LLM alignment with expert human judgments

## Confidence
- Overall claim that LLMs can sometimes replace human judges but require careful validation: **Medium**
- Proprietary models like GPT-4o perform best: **High**
- LLMs align better with non-expert than expert human judgments: **Medium**

## Next Checks
1. Test the same evaluation protocol on additional diverse datasets, particularly in specialized domains like medical or legal text where human expertise is critical
2. Evaluate newer model versions and different prompting strategies beyond Chain-of-Thought to identify optimal configurations for different task types
3. Conduct ablation studies to determine which specific aspects of LLM evaluations (absolute scores vs. pairwise rankings, different aggregation methods) drive the observed agreement patterns with human judges