---
ver: rpa2
title: SGD with Partial Hessian for Deep Neural Networks Optimization
arxiv_id: '2403.02681'
source_url: https://arxiv.org/abs/2403.02681
tags:
- sgd-ph
- hessian
- optimizers
- second-order
- sgdm
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a new optimizer called SGD with Partial Hessian
  (SGD-PH) for training deep neural networks. The key idea is to use second-order
  information from the Hessian matrix to update channel-wise parameters in normalization
  layers, while using first-order SGD for other parameters.
---

# SGD with Partial Hessian for Deep Neural Networks Optimization

## Quick Facts
- arXiv ID: 2403.02681
- Source URL: https://arxiv.org/abs/2403.02681
- Authors: Ying Sun; Hongwei Yong; Lei Zhang
- Reference count: 31
- Primary result: SGD-PH achieves higher test accuracies than popular optimizers on CIFAR-100, CIFAR-10, and Mini-ImageNet

## Executive Summary
This paper introduces SGD with Partial Hessian (SGD-PH), a novel optimizer that combines first-order SGD with precise second-order information for channel-wise parameters in normalization layers. The key innovation leverages the diagonal structure of Hessian matrices for channel-wise parameters, which can be computed exactly using Hessian-free methods. Experiments demonstrate that SGD-PH outperforms established optimizers like SGD with momentum, Adam, and second-order methods on image classification tasks.

## Method Summary
SGD-PH is a hybrid optimizer that uses second-order information from the Hessian matrix for channel-wise parameters in normalization layers while applying first-order SGD to other parameters. The method exploits the diagonal structure of Hessians for channel-wise parameters, allowing precise computation without the full computational burden of second-order methods. For layers without explicit channel-wise parameters, the approach decouples convolutional operations and applies weight normalization to create channel-wise parameters. The optimizer maintains momentum for both gradients and Hessian information.

## Key Results
- SGD-PH achieves best results across all tested DNN models on CIFAR-100 and CIFAR-10
- Outperforms SGDM, Adam, AdamW, Adabelief, Adahessian, and Apollo on ResNet18/50 and VGG architectures
- Maintains advantages of first-order optimizers while incorporating beneficial second-order information
- Validated effectiveness on Mini-ImageNet and ImageNet datasets

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Channel-wise parameters in normalization layers have diagonal Hessian matrices that can be computed precisely via Hessian-free methods
- Mechanism: Normalization layers operate independently on each channel, creating a block-diagonal Hessian where each block is a 1x1 diagonal matrix
- Core assumption: Independence of channel-wise normalization means no cross-channel curvature exists in the loss landscape
- Evidence anchors: Abstract states "Hessian matrices of channel-wise parameters are diagonal and can be extracted directly and precisely from Hessian-free methods"

### Mechanism 2
- Claim: Combining precise second-order information for channel-wise parameters with first-order SGD for other parameters achieves better performance
- Mechanism: Using exact second-order information where cheap to compute (channel-wise parameters) while maintaining first-order elsewhere
- Core assumption: Computational savings from partial Hessian outweigh performance loss from not using second-order everywhere
- Evidence anchors: Table 1 shows SGD-PH achieves best results on CIFAR100 and CIFAR10 across all DNN models

### Mechanism 3
- Claim: Hessian-free approach using second backpropagation can extract precise diagonal elements without storing full Hessian
- Mechanism: Computing gradient of gradient (second derivative) with respect to specific parameters using second backpropagation pass
- Core assumption: Computational overhead of second backpropagation is manageable and provides sufficient benefit
- Evidence anchors: Section describes obtaining precise partial Hessian through specific operations

## Foundational Learning

- Concept: Diagonal matrices and their properties
  - Why needed here: Understanding why diagonal structure of Hessians for channel-wise parameters matters for efficient computation
  - Quick check question: What is the computational complexity of inverting a diagonal matrix versus a full matrix?

- Concept: Backpropagation and higher-order derivatives
  - Why needed here: Method relies on computing second derivatives using backpropagation techniques
  - Quick check question: How does computing gradient of a gradient differ from computing gradient of a loss?

- Concept: Normalization layers in neural networks
  - Why needed here: Entire approach hinges on special structure of normalization layers like BatchNorm, LayerNorm
  - Quick check question: What parameters in a BatchNorm layer are considered "channel-wise"?

## Architecture Onboarding

- Component map: Forward pass -> Standard backpropagation -> Conditional second backpropagation -> Momentum computation -> Weight updates

- Critical path:
  1. Forward pass
  2. Standard backpropagation to get gradients
  3. Conditional second backpropagation for channel-wise parameters
  4. Compute momentum for gradients and Hessians
  5. Apply updates with appropriate learning rates

- Design tradeoffs:
  - Precision vs. computational cost: Exact Hessian diagonals provide better optimization but require extra computation
  - Memory vs. performance: Approach uses more memory than pure first-order but less than full second-order methods
  - Complexity vs. generality: Conditional logic adds implementation complexity but allows method to work on any network

- Failure signatures:
  - Performance worse than SGD: Likely due to incorrect Hessian computation or poor hyperparameter tuning
  - Excessive memory usage: May indicate second backpropagation is not properly optimized
  - Training instability: Could result from incorrect handling of non-convexity in Hessian

- First 3 experiments:
  1. Implement SGD-PH on simple CNN with BatchNorm on CIFAR-10, compare with standard SGD
  2. Remove BatchNorm layers and test WN generalization approach on same CNN
  3. Scale up to ResNet-18 on CIFAR-100, systematically tune learning rates and momentum parameters

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does performance of SGD-PH compare to other optimizers when applied to extremely deep networks beyond ResNet50, such as those with over 100 layers?
- Basis in paper: [inferred] Paper demonstrates SGD-PH's effectiveness on ResNet18, ResNet50, VGG11, and VGG19, but does not test it on extremely deep networks
- Why unresolved: Paper does not provide experimental results for networks deeper than ResNet50
- What evidence would resolve it: Conducting experiments with SGD-PH on extremely deep networks and comparing results with other optimizers

### Open Question 2
- Question: Can diagonal Hessian property for channel-wise parameters be extended to other types of layers beyond normalization layers and convolutional layers with weight normalization?
- Basis in paper: [explicit] Paper states diagonal Hessian property holds for channel-wise parameters in normalization layers and convolutional layers with weight normalization, but does not explore other layer types
- Why unresolved: Paper focuses on normalization layers and convolutional layers with weight normalization
- What evidence would resolve it: Investigating Hessian properties of other layer types to determine if they exhibit diagonal structure

### Open Question 3
- Question: How does choice of momentum factors (α and β) in SGD-PH affect its performance on different types of tasks, such as natural language processing or reinforcement learning?
- Basis in paper: [inferred] Paper sets α and β to 0.9 based on experience with SGDM, but does not explore their impact on different task types
- Why unresolved: Paper focuses on image classification tasks and does not investigate effects of momentum factors on other task types
- What evidence would resolve it: Conducting experiments with SGD-PH on various task types while varying α and β

## Limitations
- Computational overhead of second backpropagation for Hessian computation is not quantified
- Claim that channel-wise parameters are a "large portion" of total parameters is not empirically validated
- No comparison with newer adaptive optimizers like Lion that have emerged since experiments

## Confidence

- **High**: Mathematical derivation of diagonal Hessian structure for channel-wise parameters is sound and well-supported by theory
- **Medium**: Experimental results showing performance improvements are robust across multiple datasets and architectures, though limited to image classification
- **Low**: Claim about SGD-PH "inheriting advantages of both first-order and second-order optimizers" lacks systematic ablation studies

## Next Checks

1. **Scaling Analysis**: Measure wall-clock time per iteration for SGD-PH vs. Adam/Apollo on models ranging from ResNet-18 to Vision Transformers to quantify practical overhead

2. **Parameter Impact Study**: Quantify percentage of channel-wise parameters in various architectures (ResNet, EfficientNet, ConvNext) to verify assumption about their prevalence

3. **Robustness Testing**: Evaluate SGD-PH on non-image tasks like language modeling or recommendation systems where normalization layers behave differently