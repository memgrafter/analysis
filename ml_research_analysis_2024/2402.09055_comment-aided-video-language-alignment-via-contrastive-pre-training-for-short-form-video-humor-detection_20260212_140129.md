---
ver: rpa2
title: Comment-aided Video-Language Alignment via Contrastive Pre-training for Short-form
  Video Humor Detection
arxiv_id: '2402.09055'
source_url: https://arxiv.org/abs/2402.09055
tags:
- video
- pre-training
- humor
- multi-modal
- detection
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes CVLA, a novel two-branch hierarchical model
  for short-form video humor detection. CVLA addresses the challenges of multi-modal
  humor detection by aligning video and language components within a consistent semantic
  space.
---

# Comment-aided Video-Language Alignment via Contrastive Pre-training for Short-form Video Humor Detection

## Quick Facts
- arXiv ID: 2402.09055
- Source URL: https://arxiv.org/abs/2402.09055
- Reference count: 40
- Primary result: CVLA achieves 80.68% accuracy on DY11k and 56.42% on UR-FUNNY, outperforming state-of-the-art methods

## Executive Summary
This paper introduces CVLA, a two-branch hierarchical model for short-form video humor detection that aligns video and language modalities within a consistent semantic space. The model processes raw vision, audio, title, and comment signals through separate encoders before fusing them via cross-attention. A key innovation is the data-augmented contrastive pre-training strategy that leverages unlabeled videos to improve alignment between modalities.

CVLA demonstrates superior performance on two humor detection benchmarks, DY11k and UR-FUNNY, with accuracy gains of 2.42% and 4.25% respectively over previous state-of-the-art methods. The model's effectiveness is attributed to its ability to capture semantic complementarity between video and language components while maintaining modality-specific information through separate encoding.

## Method Summary
CVLA employs a two-branch hierarchical architecture that separately encodes video (vision + audio) and language (title + comments) before fusing them through a multi-modal encoder. The model uses patchified raw signals processed by Transformer encoders, with comments included to provide supplementary contextual information. A data-augmented contrastive pre-training strategy aligns semantically related samples while pushing apart unrelated ones, improving robustness before fine-tuning on labeled humor detection data.

## Key Results
- CVLA achieves 80.68% accuracy on DY11k dataset, outperforming state-of-the-art by 2.42%
- On UR-FUNNY dataset, CVLA reaches 56.42% accuracy, a 4.25% improvement over previous best
- Model demonstrates effectiveness across both short-form (DY11k) and long-form (UR-FUNNY) video contexts
- Data-augmented contrastive pre-training consistently improves performance across different dataset scales

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The two-branch hierarchical architecture aligns video and language semantics by encoding them separately before fusing, which reduces modality misalignment.
- Mechanism: Video branch processes raw vision and audio patches through a single Transformer encoder, while language branch processes title and comments via BERT; the multi-modal encoder fuses both branches with cross-attention, creating aligned multi-modal representations.
- Core assumption: Separate encoding before fusion preserves modality-specific semantics better than early fusion.
- Evidence anchors:
  - [abstract]: "CVLA not only operates on raw signals across various modal channels but also yields an appropriate multi-modal representation by aligning the video and language components within a consistent semantic space."
  - [section 3.2]: Describes separate video and language encoding, then multi-modal fusion.
- Break condition: If cross-attention in MME fails to capture complementarity, alignment degrades.

### Mechanism 2
- Claim: Data-augmented contrastive pre-training improves robustness by leveraging unlabeled data to align semantically related samples while pushing apart unrelated ones.
- Mechanism: Random augmentation (erase, jitter, noise) on vision/audio and token dropout on language creates positive pairs; contrastive loss pulls representations of same sample closer and pushes different samples apart.
- Core assumption: Unlabeled short-form videos contain sufficient semantic diversity for effective contrastive learning.
- Evidence anchors:
  - [abstract]: "data-augmented contrastive pre-training strategy."
  - [section 3.3]: Details augmentation pipeline and contrastive tuple construction.
- Break condition: If augmentation introduces too much noise, positive pairs become unreliable.

### Mechanism 3
- Claim: Including interactive comments as part of language branch enriches semantic context, helping disambiguate humor cues that video and title alone cannot capture.
- Mechanism: Comments are tokenized and encoded along with title in BERT, providing supplementary background knowledge that bridges semantic gaps between video and text.
- Core assumption: Comments contain relevant contextual information that correlates with video humor.
- Evidence anchors:
  - [abstract]: "the distinctive phrase pants all gone significantly contributes to humor recognition."
  - [section 1]: Discusses comments providing supplemental background knowledge.
- Break condition: If comments are noisy or irrelevant, they degrade representation quality.

## Foundational Learning

- Concept: Contrastive learning
  - Why needed here: Enables alignment of video and language modalities without requiring paired labels for every sample.
  - Quick check question: How does noise contrastive estimation encourage similar samples to have closer embeddings?

- Concept: Transformer-based multi-modal fusion
  - Why needed here: Cross-attention in MME allows video and language features to mutually refine each other for better alignment.
  - Quick check question: What role does the CLS token play in representing fused modality information?

- Concept: Patchifying continuous signals
  - Why needed here: Transforms raw video frames and audio spectrograms into sequences suitable for Transformer encoders.
  - Quick check question: Why are spatial and temporal positional embeddings added after linear projection?

## Architecture Onboarding

- Component map: Raw signals → Patchify → Vision/Audio Encoder → Video branch → Multi-modal Encoder → Fusion → Contrastive Pre-training → Humor Detection head.
- Critical path: Video/audio → VE → (CLS) → MME ← LE ← (CLS) ← Title/Comments → MME output → Humor classifier.
- Design tradeoffs: Separate encoding preserves semantics but adds complexity; large-scale pre-training improves robustness but requires more compute.
- Failure signatures: Poor alignment → low contrastive loss convergence; modality imbalance → skewed attention maps; overfitting → gap between dev/test accuracy.
- First 3 experiments:
  1. Train CVLA without pre-training on DY11k; measure accuracy and attention patterns.
  2. Add contrastive pre-training with small augmentation; compare alignment via similarity scores.
  3. Vary augmentation intensity; observe effect on loss stability and downstream humor detection accuracy.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the effectiveness of CVLA vary when applied to long-form videos compared to short-form videos?
- Basis in paper: [explicit] The paper mentions evaluating on the UR-FUNNY dataset, which is derived from long-form videos, and states that the findings suggest CVLA's applicability to long-form video contexts as well.
- Why unresolved: While the paper demonstrates CVLA's effectiveness on both short-form (DY11k) and long-form (UR-FUNNY) datasets, it does not provide a detailed comparative analysis of CVLA's performance across these different video formats.
- What evidence would resolve it: Conducting a comprehensive study comparing CVLA's performance on short-form versus long-form videos, including detailed metrics and analysis of the differences in effectiveness.

### Open Question 2
- Question: What is the impact of the scale of unlabeled data on the performance of CVLA in humor detection tasks?
- Basis in paper: [explicit] The paper discusses the expansion of the unlabeled data scale for pre-training in the DY24h dataset to DY11k and mentions that CVLA can consistently improve performance by feeding more unlabeled data.
- Why unresolved: The paper does not explore the upper limits of how much additional unlabeled data could further enhance CVLA's performance or whether there is a point of diminishing returns.
- What evidence would resolve it: Conducting experiments with varying scales of unlabeled data to determine the relationship between data size and model performance, and identifying any saturation points.

### Open Question 3
- Question: How does CVLA's performance compare to other state-of-the-art models when using the same computational resources?
- Basis in paper: [inferred] The paper notes that CVLA operates on raw signals across various modal channels, which enhances performance and streamlines the training and inference process. However, it does not provide a direct comparison of computational efficiency with other models.
- Why unresolved: While CVLA is shown to outperform other models in terms of accuracy, the paper does not discuss the computational efficiency or resource requirements of CVLA compared to other state-of-the-art models.
- What evidence would resolve it: Conducting a comparative analysis of CVLA's computational efficiency, including training time, inference speed, and resource utilization, against other state-of-the-art models under identical conditions.

## Limitations

- The paper lacks ablation studies to confirm whether performance gains come from the two-branch architecture, contrastive pre-training, or their combination.
- No empirical validation through controlled experiments that the comments branch actually contributes to humor detection accuracy.
- Does not address potential biases in humor detection across different cultural contexts or content types that could limit generalizability.

## Confidence

- **High Confidence**: The architectural design choices (two-branch hierarchy, multi-modal fusion via cross-attention) are technically sound and align with established multi-modal learning principles. The reported performance metrics on DY11k and UR-FUNNY datasets are specific and verifiable.
- **Medium Confidence**: The effectiveness of data-augmented contrastive pre-training is plausible given its success in other domains, but the paper lacks comparative analysis against simpler pre-training alternatives or different augmentation strategies.
- **Low Confidence**: The specific contribution of comments to humor detection accuracy is asserted based on qualitative examples but lacks quantitative validation through ablation experiments.

## Next Checks

1. **Ablation Study**: Systematically remove components (comments branch, contrastive pre-training, separate encoding) to quantify their individual contributions to performance gains on both datasets.

2. **Robustness Testing**: Evaluate CVLA on cross-dataset transfer tasks (e.g., train on DY11k, test on UR-FUNNY) to assess generalization beyond the specific training distributions.

3. **Attention Analysis**: Visualize cross-attention weights in the multi-modal encoder to verify that video and language components are genuinely aligning semantically rather than relying on spurious correlations.