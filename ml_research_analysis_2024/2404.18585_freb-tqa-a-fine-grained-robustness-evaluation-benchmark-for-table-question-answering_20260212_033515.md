---
ver: rpa2
title: 'FREB-TQA: A Fine-Grained Robustness Evaluation Benchmark for Table Question
  Answering'
arxiv_id: '2404.18585'
source_url: https://arxiv.org/abs/2404.18585
tags:
- table
- question
- systems
- answer
- questions
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces FREB-TQA, a fine-grained robustness evaluation
  benchmark for Table Question Answering (TQA) systems. The benchmark addresses three
  key aspects of robustness: retrieval robustness against table structure changes,
  attention to relevant cells, and aggregation/comparison robustness in case of value
  changes.'
---

# FREB-TQA: A Fine-Grained Robustness Evaluation Benchmark for Table Question Answering

## Quick Facts
- **arXiv ID**: 2404.18585
- **Source URL**: https://arxiv.org/abs/2404.18585
- **Reference count**: 40
- **Primary result**: FREB-TQA is a fine-grained robustness evaluation benchmark for TQA systems that reveals substantial robustness issues across different architectures

## Executive Summary
This paper introduces FREB-TQA, a fine-grained robustness evaluation benchmark for Table Question Answering (TQA) systems. The benchmark addresses three key aspects of robustness: retrieval robustness against table structure changes, attention to relevant cells, and aggregation/comparison robustness in case of value changes. Using seven novel perturbation methods and extensive manual annotations, the authors create a benchmark of 8,590 questions and tables, resulting in 75,205 instances.

Extensive experiments with state-of-the-art TQA systems and large language models reveal that all examined systems suffer from substantial robustness issues. End-to-end systems show better resilience to row/column arrangement changes but struggle with numerical reasoning. Large language models are more affected by table structure perturbations but perform better on numeric operations. Pipeline models demonstrate more robust attention to relevant cells and value changes due to their symbolic execution components.

## Method Summary
The authors developed FREB-TQA using seven novel perturbation methods to create a benchmark that evaluates TQA system robustness across three dimensions: retrieval, attention, and aggregation/comparison. The perturbation methods include row/column swaps, value changes, merging/splitting cells, and synthetic value generation. The benchmark comprises 8,590 questions and tables from four datasets, resulting in 75,205 instances after perturbation. Each instance was manually annotated for relevance and value correctness, with 5.5x more manual annotations than original questions.

## Key Results
- End-to-end systems demonstrate better resilience to row/column arrangement changes but struggle with numerical reasoning
- Large language models are more affected by table structure perturbations but perform better on numeric operations
- Pipeline models show more robust attention to relevant cells and value changes due to their symbolic execution components
- All examined TQA systems exhibit substantial robustness issues across different perturbation types

## Why This Works (Mechanism)
The benchmark works by systematically perturbing tables in controlled ways that isolate specific robustness vulnerabilities. By creating multiple perturbation types and requiring manual annotation of relevance and correctness, the benchmark can precisely identify where systems fail. The three-pronged approach (retrieval, attention, aggregation) provides comprehensive coverage of TQA system weaknesses.

## Foundational Learning

### Perturbation Methods
**Why needed**: To systematically evaluate how TQA systems handle structural changes to input tables
**Quick check**: Verify that perturbations maintain logical consistency while altering table structure

### Relevance Labeling
**Why needed**: To determine which table cells are actually used by systems in their answers
**Quick check**: Ensure inter-annotator agreement remains high across different perturbation types

### Value Correctness Annotation
**Why needed**: To assess whether systems can detect changes in table values
**Quick check**: Confirm that correctness judgments align with perturbation intent

## Architecture Onboarding

### Component Map
Original Tables -> Perturbation Methods -> Annotated Instances -> TQA System Evaluation

### Critical Path
Perturbation generation → Manual annotation → System evaluation → Performance analysis

### Design Tradeoffs
- Manual annotation provides accuracy but limits scalability
- Multiple perturbation types increase coverage but add complexity
- Three-dimensional evaluation captures nuances but requires more resources

### Failure Signatures
- End-to-end systems: Numerical reasoning failures
- LLMs: Structure perturbation sensitivity
- Pipeline models: Symbolic execution component dependencies

### First Experiments
1. Test perturbation methods on small subset to verify effectiveness
2. Validate manual annotation process with pilot study
3. Run baseline systems on unperturbed tables for reference

## Open Questions the Paper Calls Out
None

## Limitations
- Manual annotation process introduces potential subjectivity
- Benchmark focuses on English-language tables from Wikipedia and academic sources
- Does not explore full space of possible robustness dimensions
- Single in-context example may not fully capture LLM capabilities

## Confidence

**High confidence**: The characterization of robustness issues across different TQA architectures (end-to-end, pipeline, LLMs)

**Medium confidence**: The effectiveness of the seven novel perturbation methods in exposing robustness vulnerabilities

**Medium confidence**: The generalizability of findings to non-English tables and other domains

## Next Checks
1. Validate the benchmark's effectiveness by testing additional state-of-the-art TQA systems not included in the original evaluation
2. Conduct cross-linguistic validation by creating perturbed versions of the benchmark for non-English tables
3. Investigate the impact of varying in-context examples for LLM evaluation to determine if results hold with more extensive prompting