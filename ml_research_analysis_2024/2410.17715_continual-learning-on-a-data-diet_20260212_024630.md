---
ver: rpa2
title: Continual Learning on a Data Diet
arxiv_id: '2410.17715'
source_url: https://arxiv.org/abs/2410.17715
tags:
- learning
- coreset
- samples
- forgetting
- selection
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This study evaluates coreset selection methods for continual learning,
  aiming to improve performance by training models on carefully chosen, informative
  subsets of data rather than the full dataset. The approach involves two phases:
  a warm-up period to identify important samples, followed by learning from a reduced
  set selected via methods like Herding, Uncertainty, Forgetting, and GraphCut.'
---

# Continual Learning on a Data Diet

## Quick Facts
- arXiv ID: 2410.17715
- Source URL: https://arxiv.org/abs/2410.17715
- Reference count: 40
- Key outcome: Coreset selection consistently improves incremental accuracy and knowledge retention in class-incremental learning across multiple datasets and methods.

## Executive Summary
This paper explores coreset selection methods for continual learning, demonstrating that training models on carefully chosen subsets of data can significantly enhance performance compared to using full datasets. The approach involves a two-phase training process where models first undergo a warm-up period on full data to identify informative samples, then learn from a reduced coreset selected via methods like Herding, Uncertainty, Forgetting, and GraphCut. Results show consistent improvements in incremental accuracy and reduced forgetting across Split-CIFAR10, Split-CIFAR100, and Split-ImageNet100 datasets with various class-incremental learning methods.

## Method Summary
The method employs a two-phase training approach for class-incremental learning. During the warm-up phase (10% of training budget), the model trains on full task data to identify important samples. In the learning phase (90% of budget), the model trains on a reduced coreset selected using methods like Herding, Uncertainty, Forgetting, or GraphCut. The approach is evaluated across multiple datasets with 5-10 tasks each and various continual learning methods including DER, FOSTER, MEMO, iCaRL, ER, LwF, and CODA-Prompt, using ResNet18 and Vision Transformer architectures.

## Key Results
- Coreset selection consistently improves incremental accuracy across all tested datasets and methods
- Smaller coreset fractions (20%) often outperform larger ones (90%) for certain methods like DER
- Performance gains attributed to reduced forgetting and improved representation learning
- Effectiveness maintained even with pretrained backbones
- Different coreset selection methods show varying effectiveness depending on the continual learning method used

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Coreset selection reduces redundancy and focuses training on informative samples, leading to better generalization.
- Mechanism: By filtering out less informative data points, the model learns from a more diverse and representative subset, which enhances its ability to retain knowledge and avoid overfitting to noise.
- Core assumption: Not all data points contribute equally to model learning; some are redundant or less informative.
- Evidence anchors:
  - [abstract] "not all data points in a dataset have equal potential; some can be more informative than others."
  - [section] "learning from selectively chosen samples (i) enhances incremental accuracy, (ii) improves knowledge retention of previous tasks, and (iii) refines learned representations."
  - [corpus] Weak; related work focuses on efficient training but not coreset-specific improvements.

### Mechanism 2
- Claim: Coreset selection improves knowledge retention by enhancing the representativeness and diversity of training samples.
- Mechanism: Coreset samples are selected to approximate the distribution of the full dataset, ensuring that the model is exposed to a balanced representation of all classes and tasks. This prevents catastrophic forgetting by reinforcing previously learned concepts.
- Core assumption: Coreset samples can effectively approximate the full dataset distribution.
- Evidence anchors:
  - [section] "coreset selection provides an additional performance boost" (pretrained models section).
  - [section] "performance improvements... attributed to several factors: (i) First, coreset samples are carefully selected to represent the most informative subset of the data, thereby reducing redundancy and focusing on critical information."
  - [corpus] Weak; related work discusses data selection but not specifically for continual learning.

### Mechanism 3
- Claim: Coreset selection refines learned representations by forcing the model to focus on essential features.
- Mechanism: With fewer samples, the model is compelled to extract more meaningful features, leading to clearer distinctions between classes and better generalization.
- Core assumption: Fewer, more focused samples lead to better feature extraction and representation learning.
- Evidence anchors:
  - [section] "models trained with the coresets exhibit a greater ability to retain focus on the object itself, effectively capturing the essence of the image."
  - [section] "when using a smaller coreset, such as 20%, the model demonstrates distinct separations between classes, effectively preserving boundaries between different categories."
  - [corpus] Weak; related work does not directly address representation refinement through coreset selection.

## Foundational Learning

- Concept: Catastrophic forgetting
  - Why needed here: Understanding why continual learning is challenging and how coreset selection helps mitigate forgetting.
  - Quick check question: What is catastrophic forgetting, and why does it occur in continual learning?

- Concept: Coreset selection
  - Why needed here: Understanding how coreset selection works and its role in improving continual learning performance.
  - Quick check question: What is coreset selection, and how does it differ from traditional data sampling methods?

- Concept: Class-incremental learning
  - Why needed here: Understanding the specific scenario where new classes are added over time, and how coreset selection can be applied.
  - Quick check question: What is class-incremental learning, and what are its main challenges?

## Architecture Onboarding

- Component map: Data pipeline -> Coreset selection module -> Model architecture -> Training loop -> Evaluation module
- Critical path: Data preparation → Coreset selection → Model training → Evaluation → Repeat for each task
- Design tradeoffs:
  - Coreset size vs. performance: Smaller coresets may lead to better representation but risk underfitting
  - Warm-up duration vs. efficiency: Longer warm-up allows better coreset selection but increases training time
  - Selection method vs. task complexity: Some methods may work better for certain tasks or datasets
- Failure signatures:
  - Performance degradation: May indicate poor coreset selection or insufficient coreset size
  - Overfitting: Could suggest the coreset is too small or not diverse enough
  - Catastrophic forgetting: May indicate the coreset is not representative of the full dataset
- First 3 experiments:
  1. Train with random coreset selection to establish a baseline
  2. Compare different coreset selection methods (Herding, Uncertainty, Forgetting, GraphCut) on a single task
  3. Vary coreset size (10%, 20%, 50%, 80%, 90%) to find the optimal balance between performance and efficiency

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the effectiveness of coreset selection vary with different training budget allocations between the warm-up and learning phases?
- Basis in paper: [explicit] The paper mentions that the warm-up phase duration is much shorter than the learning phase, but doesn't explore how varying this ratio affects coreset selection effectiveness.
- Why unresolved: The paper fixes the warm-up fraction at 0.1, leaving the impact of different allocations unexplored.
- What evidence would resolve it: Experiments varying the warm-up fraction (e.g., 0.05, 0.2, 0.3) and measuring the impact on incremental accuracy and forgetting for different coreset selection methods.

### Open Question 2
- Question: How do different coreset selection methods perform in online continual learning scenarios where data arrives in a stream?
- Basis in paper: [inferred] The paper focuses on class-incremental learning with fixed task boundaries, while the broader CL field includes online settings with continuous data streams.
- Why unresolved: The paper explicitly states this is a limitation and suggests online CL as future work.
- What evidence would resolve it: Applying the studied coreset methods to online CL benchmarks and comparing performance with online-specific coreset approaches.

### Open Question 3
- Question: What is the optimal coreset size for different class-incremental learning methods, and how does this vary with dataset characteristics?
- Basis in paper: [explicit] The paper shows that smaller coresets (e.g., 20%) can outperform larger ones for some methods (like DER), but doesn't provide a systematic analysis of optimal sizes.
- Why unresolved: The paper tests multiple coreset fractions but doesn't determine optimal sizes for each method or analyze how dataset properties affect this.
- What evidence would resolve it: Systematic grid search over coreset sizes for each method-dataset combination, combined with analysis of dataset properties (e.g., class overlap, sample similarity) that predict optimal coreset sizes.

### Open Question 4
- Question: How do coreset selection methods affect the efficiency-accuracy tradeoff in continual learning?
- Basis in paper: [inferred] The paper demonstrates accuracy improvements with coreset selection but doesn't explicitly analyze computational efficiency gains.
- Why unresolved: While the paper mentions reduced memory/computation concerns, it doesn't quantify the actual efficiency improvements or analyze the accuracy-efficiency tradeoff.
- What evidence would resolve it: Detailed analysis of training time, memory usage, and computational complexity for full-dataset vs. coreset training, combined with accuracy measurements to quantify the efficiency-accuracy tradeoff curve.

## Limitations

- Computational overhead of coreset selection during warm-up phase is not thoroughly quantified
- Analysis primarily focused on image classification, generalizability to other domains unclear
- Limited exploration of coreset selection impact on complex, state-of-the-art continual learning methods

## Confidence

- **High confidence**: The claim that coreset selection improves incremental accuracy and knowledge retention is well-supported by experimental results across multiple datasets and methods.
- **Medium confidence**: The assertion that coreset selection refines learned representations is plausible but relies on qualitative observations rather than rigorous quantitative analysis.
- **Low confidence**: The claim that coreset selection is universally effective, even with pretrained backbones, is based on limited evidence and may not hold for all scenarios or architectures.

## Next Checks

1. **Quantitative Representation Analysis**: Conduct a detailed analysis of how coreset selection affects the learned representations, using metrics such as class separability (e.g., t-SNE visualizations) and feature diversity (e.g., intra-class and inter-class distances).
2. **Cross-Domain Generalization**: Test the effectiveness of coreset selection on non-image datasets (e.g., text or tabular data) to assess its applicability beyond image classification.
3. **Efficiency Benchmarking**: Measure the computational overhead of coreset selection methods during the warm-up phase and compare it to the efficiency gains during the learning phase to provide a holistic view of the trade-offs.