---
ver: rpa2
title: Language Models Don't Learn the Physical Manifestation of Language
arxiv_id: '2402.11349'
source_url: https://arxiv.org/abs/2402.11349
tags:
- label
- input
- language
- performance
- llms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Language models (LLMs) struggle with tasks requiring sensory understanding
  of language, such as visual and auditory aspects. This paper introduces H-TEST,
  a set of tasks designed to assess LLMs' ability to process language's physical manifestation.
---

# Language Models Don't Learn the Physical Manifestation of Language

## Quick Facts
- **arXiv ID**: 2402.11349
- **Source URL**: https://arxiv.org/abs/2402.11349
- **Reference count**: 10
- **One-line primary result**: Language models struggle with sensory-dependent language tasks, performing near random chance on H-TEST evaluations

## Executive Summary
This paper reveals a fundamental limitation in current language models: their inability to understand the physical manifestation of language, including visual and auditory aspects. Through the H-TEST evaluation framework, the authors demonstrate that even state-of-the-art LLMs perform at chance levels on tasks requiring sensory understanding of language, such as recognizing letter shapes, capitalization patterns, and phonetic structures. The findings suggest that language-only training, regardless of scale or sophistication, fails to capture the multimodal nature of human language comprehension.

## Method Summary
The study employs the H-TEST framework, a set of tasks designed to assess LLMs' ability to process the physical manifestation of language. The evaluation uses few-shot prompting and Chain-of-Thought (CoT) techniques to test various state-of-the-art language models on sensory-dependent tasks. Performance is measured against human baselines and analyzed across different model scales, training approaches, and prompting strategies to identify the limitations of current language modeling approaches.

## Key Results
- LLMs perform near random chance (~50%) on H-TEST sensory-dependent language tasks
- Scaling model size, adding few-shot examples, or using CoT reasoning techniques do not improve performance
- GPT-4 shows some improvement due to multi-modal training, but still exhibits significant limitations
- The gap between human and LLM performance on sensory tasks remains substantial and persistent

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Language-only models lack direct sensory experience of orthographic and phonological features of language.
- Mechanism: Training on text-only corpora prevents models from learning visual patterns (letter shapes, capitalization) and auditory patterns (rhyme, phonetic spelling) that humans acquire through multimodal experience.
- Core assumption: The distributional statistics of written text do not contain sufficient signal about the physical manifestation of language (e.g., "r" in "blueberry").
- Evidence anchors:
  - [abstract] "language-only models don't learn the physical manifestation of language"
  - [section 2] "supradiegetic information, which refers to the physical form of language, such as the shape of letters and sounds of syllables"
  - [corpus] weak - related works discuss multimodal grounding but do not directly test orthographic gaps
- Break condition: If models are trained on multimodal data (vision/audio) or on specially curated orthographic datasets, the gap may close.

### Mechanism 2
- Claim: Scaling model size or training data does not resolve sensory gaps in language understanding.
- Mechanism: Larger models improve diegetic (semantic) knowledge but do not acquire supradiegetic (sensory) knowledge without explicit multimodal input.
- Core assumption: The emergent abilities from scaling are domain-specific and do not generalize to sensory-dependent tasks.
- Evidence anchors:
  - [section 3] "stronger model in the same model family...does not bring meaningful improvement on the H-TEST performance"
  - [section 3] "GPT-4, the only model that was reported to have undergone multi-modal training...made a particular improvement on H-TEST"
  - [corpus] weak - scaling literature focuses on semantic tasks, not orthographic/visual tasks
- Break condition: If multi-modal training or MoE architectures are explicitly incorporated, performance may improve.

### Mechanism 3
- Claim: In-context learning and chain-of-thought reasoning are ineffective for sensory-dependent tasks.
- Mechanism: ICL relies on pattern matching in textual context, which cannot substitute for experiential learning of visual/auditory patterns.
- Core assumption: CoT reasoning improves performance by decomposing tasks into semantic steps, but cannot compensate for lack of sensory grounding.
- Evidence anchors:
  - [section 3] "CoT decreases performances in general"
  - [section 3] "CoT responses...often do not align with the sensory aspects required to solve H-TEST"
  - [corpus] weak - CoT literature focuses on reasoning tasks, not sensory-dependent tasks
- Break condition: If CoT is combined with multimodal input or if sensory grounding is explicitly provided, performance may improve.

## Foundational Learning

- Concept: Diegetic vs. Supradiegetic Information
  - Why needed here: Distinguishes semantic content from physical form of language, clarifying why text-only models fail on sensory tasks
  - Quick check question: What is an example of diegetic vs. supradiegetic information in the sentence "The cat jumped over the moon"?

- Concept: In-Context Learning (ICL)
  - Why needed here: Explains why few-shot prompting is used and why it fails for sensory tasks
  - Quick check question: How does ICL differ from fine-tuning, and why might it be insufficient for learning sensory patterns?

- Concept: Chain-of-Thought (CoT) Reasoning
  - Why needed here: Shows why step-by-step reasoning does not help with sensory-dependent tasks
  - Quick check question: Why might CoT reasoning decrease performance on tasks requiring visual or auditory understanding?

## Architecture Onboarding

- Component map: H-TEST task generator -> prompt formatter -> model interface -> result parser -> analysis module
- Critical path: Task generation -> prompt construction -> model API call -> response parsing -> accuracy calculation
- Design tradeoffs: Few-shot vs. fine-tuning (data efficiency vs. generalization); CoT vs. direct prompting (reasoning depth vs. task relevance)
- Failure signatures: Random chance performance (~50%) on binary classification tasks; inconsistent responses across model runs; failure to follow output format instructions
- First 3 experiments:
  1. Run H-TEST on a language-only model with k=50 few-shot examples; verify ~50% accuracy
  2. Run H-TEST on the same model with CoT prompting; verify performance decrease
  3. Run H-TEST on a multi-modal model; verify performance improvement (if available)

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do current LLMs process and understand the visual and auditory aspects of language that are typically learned through sensory experiences?
- Basis in paper: Explicit - The paper introduces H-TEST, a series of tasks designed to assess LLMs' ability to process the physical manifestation of language, including visual and auditory properties.
- Why unresolved: Despite extensive testing on various state-of-the-art LLMs, the paper shows that these models struggle with tasks that require sensory understanding of language, performing near random chance. The exact mechanisms and limitations of LLMs in processing these aspects remain unclear.
- What evidence would resolve it: Further experiments that delve into the internal workings of LLMs when processing sensory-dependent language tasks, potentially involving neuroscientific approaches or advanced interpretability techniques, could provide insights into how these models handle such information.

### Open Question 2
- Question: What specific architectural or training modifications could enable LLMs to better understand the physical manifestation of language?
- Basis in paper: Inferred - The paper discusses the limitations of current LLMs in handling tasks that require sensory understanding of language and mentions attempts to improve performance through scaling, additional examples, or reasoning techniques, which were unsuccessful.
- Why unresolved: The paper concludes that current language modeling approaches are insufficient for capturing the full essence of human language understanding, particularly its sensory components. However, it does not provide specific solutions or directions for overcoming these limitations.
- What evidence would resolve it: Development and testing of new LLM architectures or training methodologies that incorporate sensory data or alternative approaches to language representation could demonstrate improved performance on tasks like H-TEST.

### Open Question 3
- Question: To what extent can multi-modal training improve LLMs' understanding of the physical manifestation of language, and what are the limitations of this approach?
- Basis in paper: Explicit - The paper notes that GPT-4, which underwent multi-modal training, showed some improvement on H-TEST compared to its predecessors, but the exact reasons and extent of this improvement are not fully understood.
- Why unresolved: While the paper suggests that multi-modal training may help LLMs better understand certain aspects of language, it does not provide a comprehensive analysis of how much improvement can be expected or what specific limitations might still exist.
- What evidence would resolve it: Comparative studies of LLMs with varying degrees of multi-modal training on a wide range of sensory-dependent language tasks could quantify the improvements and identify remaining challenges. Additionally, detailed analysis of the training processes and data used in multi-modal models could shed light on effective strategies.

## Limitations
- The evaluation focuses on English language tasks, limiting generalizability to other languages with different orthographic and phonological systems
- H-TEST tasks may oversimplify the complexity of visual and auditory language processing through binary classification format
- The study does not explore all potential approaches, such as specialized training regimes or alternative model architectures

## Confidence

**High Confidence:** The core finding that language-only models perform near random chance on H-TEST tasks is well-supported by systematic experimentation across multiple model families and scales.

**Medium Confidence:** The claim that scaling model size does not resolve sensory gaps is supported but could benefit from testing on even larger models or different scaling approaches.

**Low Confidence:** The assertion that current language modeling approaches are fundamentally insufficient for capturing full human language understanding may be premature.

## Next Checks
1. **Cross-linguistic validation**: Test H-TEST tasks with models trained on languages with different orthographic systems (e.g., Chinese, Arabic) to verify if the sensory gap generalizes across writing systems.

2. **Controlled multimodal training**: Train a language model with limited multimodal input focused specifically on orthographic and phonological patterns, then test performance on H-TEST to isolate the effect of sensory grounding.

3. **Fine-grained error analysis**: Conduct detailed analysis of model failures on H-TEST tasks to determine whether errors stem from lack of sensory knowledge or other factors like task understanding or format compliance.