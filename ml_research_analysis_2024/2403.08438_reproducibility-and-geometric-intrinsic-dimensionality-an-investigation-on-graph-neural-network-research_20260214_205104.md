---
ver: rpa2
title: 'Reproducibility and Geometric Intrinsic Dimensionality: An Investigation on
  Graph Neural Network Research'
arxiv_id: '2403.08438'
source_url: https://arxiv.org/abs/2403.08438
tags:
- data
- reproducibility
- learning
- graph
- intrinsic
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a reproducibility ontology for machine learning
  research and applies it to graph neural networks, reproducing six major papers.
  It introduces a geometric intrinsic dimensionality concept and uses it for feature
  selection to analyze the impact of data set dimensionality on model performance.
---

# Reproducibility and Geometric Intrinsic Dimensionality: An Investigation on Graph Neural Network Research

## Quick Facts
- arXiv ID: 2403.08438
- Source URL: https://arxiv.org/abs/2403.08438
- Reference count: 40
- Reproduces six major GNN papers using a novel reproducibility ontology while investigating geometric intrinsic dimensionality for feature selection

## Executive Summary
This paper addresses two critical challenges in machine learning research: reproducibility and high-dimensional data handling. The authors develop a comprehensive reproducibility ontology for machine learning research and apply it to reproduce six major graph neural network papers. They introduce geometric intrinsic dimensionality as a metric for feature selection and demonstrate that discarding high-dimensional features can improve or maintain model performance, while discarding low-dimensional features leads to performance degradation. The study reveals that different GNN models exhibit varying robustness to dimensionality reduction, with some tolerating up to 70% feature reduction without significant accuracy loss.

## Method Summary
The research employs a two-pronged approach combining reproducibility analysis with dimensionality reduction experiments. The authors develop a reproducibility ontology that systematically evaluates research papers across five dimensions: methodology documentation, data accessibility, implementation availability, hyperparameter specification, and result reporting. This ontology is applied to reproduce six major GNN papers from different categories (graph classification, node classification, and graph regression). Concurrently, they introduce geometric intrinsic dimensionality as a feature selection metric, using it to analyze how different GNN models respond to progressive feature reduction. The experiments involve systematically removing features based on their intrinsic dimensionality scores and measuring the impact on model performance across multiple datasets and architectures.

## Key Results
- Different GNN models show varying robustness to feature reduction, with some tolerating up to 70% feature reduction without significant accuracy loss
- Discarding high-dimensional features can improve or maintain model performance, while discarding low-dimensional features first leads to performance drops
- The reproducibility ontology successfully identifies gaps in existing GNN research documentation and provides a framework for more reproducible ML research

## Why This Works (Mechanism)
The geometric intrinsic dimensionality captures the intrinsic complexity of feature subspaces by measuring how data points distribute in high-dimensional space. High-dimensional features often contain redundant or noisy information that doesn't contribute meaningfully to model performance, while low-dimensional features typically carry essential structural information. By quantifying this dimensionality, the method can identify and remove superfluous features while preserving critical information, explaining why performance sometimes improves after feature reduction.

## Foundational Learning
- **Reproducibility ontology**: A systematic framework for evaluating and ensuring research reproducibility across methodology, data, implementation, hyperparameters, and results documentation
- **Geometric intrinsic dimensionality**: A metric measuring the intrinsic dimensionality of feature subspaces in high-dimensional data, capturing how data points distribute in feature space
- **Graph neural networks**: Deep learning architectures that operate on graph-structured data, leveraging message passing between nodes to learn representations
- **Feature selection in high-dimensional spaces**: The process of identifying and retaining only the most informative features while discarding redundant or irrelevant ones
- **Dimensionality reduction impact on model performance**: How removing features affects the accuracy and generalization of machine learning models
- **Graph classification vs node classification vs graph regression**: Different GNN task types requiring distinct architectural approaches and evaluation metrics

## Architecture Onboarding
Component map: Data preprocessing -> Intrinsic dimensionality calculation -> Feature selection -> Model training -> Performance evaluation -> Reproducibility assessment

Critical path: The intrinsic dimensionality calculation and feature selection process is critical, as it directly impacts model performance. The reproducibility assessment runs parallel to model experiments but is independent of the dimensionality analysis.

Design tradeoffs: Balancing between feature reduction (improving efficiency and potentially reducing overfitting) versus preserving potentially useful information (maintaining model capacity and accuracy).

Failure signatures: Performance degradation when removing too many low-dimensional features, indicating the removal of critical structural information. Reproducibility failures when essential implementation details are missing from published papers.

First experiments:
1. Calculate geometric intrinsic dimensionality for all features in a test dataset
2. Perform ablation study removing high-dimensional features first
3. Perform control study removing low-dimensional features first

## Open Questions the Paper Calls Out
None identified in the provided content.

## Limitations
- The generalizability of geometric intrinsic dimensionality findings across different graph domains remains uncertain
- The reproducibility ontology may not fully capture all reproducibility concerns in ML subfields beyond GNNs
- The feature selection methodology assumes linear relationships in feature importance that may not hold for all graph datasets

## Confidence
High confidence: The core reproducibility analysis methodology and ontology design; the experimental results showing feature selection impacts on GNN performance; the observation that different GNN models exhibit varying robustness to dimensionality reduction.

Medium confidence: The generalizability of geometric intrinsic dimensionality as a universal feature selection metric across diverse graph datasets; the specific thresholds (e.g., 70% feature reduction) as optimal guidelines for practitioners.

Low confidence: The claim that intrinsic dimensionality alone sufficiently explains model behavior variations; the proposed recommendations for handling high-dimensional data in all graph learning scenarios.

## Next Checks
1. Test the feature selection methodology across diverse graph domains (social networks, molecular graphs, knowledge graphs) to validate domain generalizability
2. Apply the reproducibility ontology to non-GNN ML papers to assess its broader applicability
3. Conduct ablation studies comparing geometric intrinsic dimensionality with other feature importance metrics on the same datasets