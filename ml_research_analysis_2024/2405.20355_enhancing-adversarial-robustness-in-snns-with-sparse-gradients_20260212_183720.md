---
ver: rpa2
title: Enhancing Adversarial Robustness in SNNs with Sparse Gradients
arxiv_id: '2405.20355'
source_url: https://arxiv.org/abs/2405.20355
tags:
- adversarial
- snns
- robustness
- neural
- gradient
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a method to improve the robustness of spiking
  neural networks (SNNs) against adversarial attacks by incorporating gradient sparsity
  regularization into the training process. The key idea is to minimize the performance
  gap between SNNs under adversarial and random perturbations by encouraging sparser
  gradients of the probability associated with the true label with respect to the
  input image.
---

# Enhancing Adversarial Robustness in SNNs with Sparse Gradients

## Quick Facts
- arXiv ID: 2405.20355
- Source URL: https://arxiv.org/abs/2405.20355
- Reference count: 40
- Key outcome: Improves SNN robustness against adversarial attacks by incorporating gradient sparsity regularization, achieving significant gains over state-of-the-art methods

## Executive Summary
This paper addresses the vulnerability of Spiking Neural Networks (SNNs) to adversarial attacks by proposing a novel gradient sparsity regularization method. The authors theoretically prove that the performance gap between SNN robustness to random versus adversarial perturbations is upper bounded by the sparsity of the gradient of the true label probability. By incorporating this sparsity regularization into the loss function during training, the proposed method significantly enhances SNN robustness against various white-box and black-box attacks on both image-based and event-based datasets.

## Method Summary
The proposed method enhances SNN robustness by adding a gradient sparsity regularization term to the loss function during training. This term is approximated using finite differences to encourage sparser gradients of the true label probability with respect to the input. The approach is theoretically grounded in a proof that shows the performance gap between random and adversarial perturbation robustness is bounded by gradient sparsity. The method is designed to be compatible with any classification loss and can be combined with existing adversarial training strategies.

## Key Results
- Achieves 30.54% classification accuracy under white-box PGD10 attack on CIFAR-10, compared to 11.53% for RAT and 18.18% for AT
- Shows consistent improvements across black-box attack scenarios and on neuromorphic datasets like CIFAR10-DVS
- Outperforms state-of-the-art methods including Regularized Adversarial Training (RAT) and standard Adversarial Training (AT) across all tested datasets and architectures

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The performance gap between SNN robustness to random vs adversarial perturbations is upper bounded by the ℓ0 norm of the gradient of the true label probability with respect to the input.
- Mechanism: By proving this bound, the paper establishes that encouraging gradient sparsity directly reduces the vulnerability difference, making SNNs more uniformly robust across perturbation types.
- Core assumption: The surrogate gradient method used for training yields differentiable approximations that hold under small perturbations.
- Evidence anchors:
  - [abstract] "theoretically prove that this performance gap is upper bounded by the gradient sparsity of the probability associated with the true label concerning the input image"
  - [section 4.2] "Theorem 4.3... the ratio of ρadv(f, x, ϵ, ℓ∞) and ρrand(f, x, ϵ, ℓ∞) is upper bounded by the sparsity of∇xfy"
- Break condition: If the SNN training dynamics deviate significantly from the surrogate gradient assumption, or if the perturbation scale ϵ is too large, the bound may no longer hold.

### Mechanism 2
- Claim: Introducing a finite-difference approximation of ℓ1 gradient sparsity into the loss function trains SNNs to have sparser gradients, improving adversarial robustness.
- Mechanism: The finite difference term approximates the ℓ1 norm of the gradient of the true label probability, penalizing large gradient components and thereby reducing sensitivity to adversarial perturbations.
- Core assumption: The finite difference approximation (Equation 13) is sufficiently accurate to regularize sparsity without destabilizing training.
- Evidence anchors:
  - [abstract] "We propose to incorporate gradient sparsity regularization into the loss function during training"
  - [section 4.3] "we employ the ℓ1 norm as a substitute for the ℓ0 norm" and "adopt a finite difference approximation for this term"
- Break condition: If the finite difference step size h is too large or too small, the approximation error could negate the sparsity regularization benefit.

### Mechanism 3
- Claim: The gradient sparsity regularization is loss-function and attack-method independent, enabling combination with existing adversarial training strategies.
- Mechanism: Unlike gradient regularization that depends on the loss function, SR only regularizes the gradient of the true label probability, making it compatible with any classification loss and any adversarial attack used in training.
- Core assumption: The true label probability computation via softmax over top two logits preserves classification while enabling sparsity control.
- Evidence anchors:
  - [section 4.4] "the regularization term in SR is exclusively tied to the model itself, rather than being dependent on the multi-class calibrated loss used during the training phase"
  - [section 5.1] "Our proposed method consistently outperforms other State-Of-The-Art (SOTA) methods across all datasets and architectures"
- Break condition: If the top-two-logit approximation introduces instability or if the sparsity regularization conflicts with other training objectives, the independence claim may fail.

## Foundational Learning

- Concept: Surrogate gradient methods for SNNs
  - Why needed here: The paper relies on differentiable approximations of spiking neuron behavior to enable backpropagation through time.
  - Quick check question: What surrogate function is used for the Leaky Integrate-and-Fire neuron in this work?
- Concept: ℓ0 vs ℓ1 norm approximations
  - Why needed here: The theoretical bound involves ℓ0 sparsity, but optimization requires a tractable surrogate; the paper uses ℓ1 norm and finite differences.
  - Quick check question: Why does the paper approximate ∥∇xfy(x)∥0 with ∥∇xfy(x)∥1?
- Concept: Adversarial training and gradient obfuscation
  - Why needed here: The paper compares SR to adversarial training and discusses ensemble attacks to mitigate gradient obfuscation.
  - Quick check question: How does the paper evaluate robustness against gradient obfuscation?

## Architecture Onboarding

- Component map: Input encoder -> LIF neuron layers -> Output layer with two-stream probability computation -> Loss = CE + λ·gradient sparsity term
- Critical path:
  1. Forward pass through SNN with surrogate gradients
  2. Compute true label probability via top-two-logit softmax
  3. Approximate gradient sparsity using finite differences
  4. Backpropagate combined CE + sparsity loss
- Design tradeoffs:
  - Smaller h in finite difference improves approximation but may increase numerical instability
  - Larger λ increases sparsity and robustness but reduces clean accuracy
  - Two-stream probability computation adds minimal overhead while enabling sparsity control
- Failure signatures:
  - Training divergence or NaN gradients → likely h too large or λ too aggressive
  - No improvement in adversarial robustness → check if finite difference approximation is accurate
  - Clean accuracy drops significantly → λ may be too large or sparsity regularization conflicting with feature learning
- First 3 experiments:
  1. Verify that gradient sparsity increases with λ by plotting ℓ1/ℓ2 norms across λ values
  2. Compare robustness under FGSM vs PGD attacks with and without SR
  3. Test if SR improves robustness on CIFAR-10 vs CIFAR-100 to check dataset sensitivity

## Open Questions the Paper Calls Out

- Open Question 1: How does the gradient sparsity regularization affect the energy efficiency of SNNs during inference?
  - Basis in paper: [explicit] The paper discusses the energy-efficient operations of SNNs and the proposed method's impact on robustness, but does not explicitly address the energy efficiency during inference.
  - Why unresolved: The paper focuses on improving robustness through gradient sparsity regularization but does not provide empirical data or analysis on how this affects the energy consumption during inference.
  - What evidence would resolve it: Experimental results comparing the energy efficiency of SNNs with and without gradient sparsity regularization during inference would provide insights into this aspect.

- Open Question 2: Can the gradient sparsity regularization method be effectively combined with other robustness-enhancing techniques, such as model distillation or ensemble methods?
  - Basis in paper: [inferred] The paper mentions that the proposed SR strategy can be combined with adversarial training (SR*) to further boost robustness, suggesting potential for combining with other techniques.
  - Why unresolved: While the paper demonstrates the effectiveness of combining SR with adversarial training, it does not explore the combination with other robustness-enhancing techniques like model distillation or ensemble methods.
  - What evidence would resolve it: Empirical studies comparing the robustness of SNNs trained with SR in combination with various other techniques would help determine the potential benefits and limitations of such combinations.

- Open Question 3: How does the choice of surrogate gradient function affect the effectiveness of the gradient sparsity regularization in improving SNN robustness?
  - Basis in paper: [explicit] The paper mentions the use of different surrogate gradient functions in the ensemble attack evaluation but does not discuss their impact on the effectiveness of the SR method.
  - Why unresolved: The paper focuses on the overall effectiveness of the SR method but does not investigate how the choice of surrogate gradient function might influence its performance.
  - What evidence would resolve it: Comparative experiments evaluating the robustness of SNNs trained with SR using different surrogate gradient functions would provide insights into the impact of this choice on the method's effectiveness.

- Open Question 4: Can the gradient sparsity regularization be extended to other types of neural networks, such as convolutional neural networks (CNNs) or recurrent neural networks (RNNs)?
  - Basis in paper: [inferred] The paper's theoretical analysis of gradient sparsity's impact on robustness could potentially be applicable to other types of neural networks.
  - Why unresolved: While the paper focuses on SNNs, the underlying concept of gradient sparsity and its relationship to robustness might be relevant to other neural network architectures.
  - What evidence would resolve it: Experimental studies applying the gradient sparsity regularization to CNNs, RNNs, or other neural network types and evaluating their robustness against adversarial attacks would help determine the generalizability of the approach.

## Limitations
- Theoretical bounds rely on surrogate gradient assumptions that may not hold for large perturbations or complex SNN dynamics
- Finite difference approximation introduces potential numerical instability and approximation error
- Empirical validation focuses on image-based datasets, with limited testing on diverse SNN architectures

## Confidence
- High confidence: Theoretical mechanism (gradient sparsity bound) due to formal proof provided
- Medium confidence: Finite difference implementation effectiveness, as approximation accuracy depends on hyperparameters
- Medium confidence: General robustness improvements, as results show consistent gains but may be architecture-dependent

## Next Checks
1. Test SR with different surrogate gradient functions (not just triangle-shaped) to verify method generality
2. Evaluate robustness on non-image datasets (e.g., speech or sensor data) to assess domain transferability
3. Analyze the impact of finite difference step size h on both training stability and final robustness metrics