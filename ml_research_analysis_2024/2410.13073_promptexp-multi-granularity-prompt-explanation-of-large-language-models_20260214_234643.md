---
ver: rpa2
title: 'PromptExp: Multi-granularity Prompt Explanation of Large Language Models'
arxiv_id: '2410.13073'
source_url: https://arxiv.org/abs/2410.13073
tags:
- prompt
- explanation
- promptexp
- importance
- token
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'PromptExp addresses the challenge of explaining large language
  models (LLMs) by providing multi-granularity prompt explanations through token-level
  insights. The framework introduces two token-level explanation approaches: an aggregation-based
  approach combining local explanation techniques like Integrated Gradients across
  time steps, and a perturbation-based approach with novel techniques to evaluate
  token masking impact using semantic similarity and logit comparison.'
---

# PromptExp: Multi-granularity Prompt Explanation of Large Language Models

## Quick Facts
- arXiv ID: 2410.13073
- Source URL: https://arxiv.org/abs/2410.13073
- Reference count: 40
- Primary result: PromptExp provides multi-granularity explanations for LLMs through token-level insights, achieving 84% user satisfaction in accuracy assessments

## Executive Summary
PromptExp addresses the challenge of explaining large language models by providing multi-granularity prompt explanations through token-level insights. The framework introduces two approaches: an aggregation-based method that combines local explanation techniques like Integrated Gradients across time steps, and a perturbation-based approach that evaluates token masking impact using semantic similarity and logit comparison. A user study confirmed PromptExp's accuracy, with 84% of explanations rated as reasonable and accurate, demonstrating its practical value for tasks like prompt compression and understanding model behavior in misclassification cases.

## Method Summary
PromptExp introduces two token-level explanation approaches for LLMs: an aggregation-based method that computes token importance scores at each generation round using local explanation techniques like Integrated Gradients, then combines these scores across rounds; and a perturbation-based approach that masks tokens and measures output changes through semantic similarity and logit comparison. The framework supports both white-box and black-box explanations and extends explanations to higher granularity levels like words, sentences, or components by aggregating token-level scores.

## Key Results
- Perturbation-based approach using semantic similarity achieved best performance with flip rate of 0.68 in sentiment analysis case study
- User study confirmed 84% of explanations rated as reasonable and accurate
- Supports both white-box and black-box explanations with multi-granularity extension capability

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Token-level explanations can be aggregated across time steps to explain LLM prompts at higher granularity levels
- Mechanism: The aggregation-based approach computes token importance scores at each generation round using local explanation techniques (e.g., Integrated Gradients), then combines these scores across rounds using weighted averaging
- Core assumption: The importance of a token remains relatively consistent across different rounds of generation
- Evidence anchors:
  - [abstract]: "PromptExp introduces two token-level explanation approaches: 1. an aggregation-based approach combining local explanation techniques"
  - [section]: "Our aggregation-based approach is divided into two stages. Stage 1 uses existing local explanation techniques to compute the importance score of prompt tokens across all rounds of token generation. In Stage 2, the token importance scores from all rounds are aggregated"
  - [corpus]: No direct evidence; this is a novel adaptation not present in cited works

### Mechanism 2
- Claim: Perturbation-based approaches can evaluate token importance by masking tokens and measuring output changes
- Mechanism: Three techniques assess perturbation impact: PerbLog compares logits directly, PerbSim measures semantic similarity between outputs, and PerbDis counts token overlap
- Core assumption: Removing an important token should cause measurable changes in the model's output
- Evidence anchors:
  - [abstract]: "2. a perturbation-based approach with novel techniques to evaluate token masking impact using semantic similarity and logit comparison"
  - [section]: "We propose three approaches as below. PerbLog: Following prior studies [18], we compare the logits of outputs between original prompt and perturbed prompts"
  - [corpus]: Limited evidence; similar perturbation approaches exist but adaptation to token-level LLM explanations is novel

### Mechanism 3
- Claim: Multi-granularity explanations enable flexible analysis by aggregating token-level scores to word, sentence, or component levels
- Mechanism: Token-level importance scores are summed based on user-defined components (e.g., [instruction + query], [example1], [example2])
- Core assumption: The importance of higher-level components can be accurately represented by summing the importance of their constituent tokens
- Evidence anchors:
  - [abstract]: "PromptExp supports both white-box and black-box explanations and extends explanations to higher granularity levels (e.g., words, sentences, components), enabling flexible analysis"
  - [section]: "We offer the flexibility to extend prompt explanation to multiple granularities, such as word, sentence, or component by summing up the importance scores of corresponding tokens"
  - [corpus]: No direct evidence; this multi-granularity extension appears novel

## Foundational Learning

- Concept: Token-by-token generation in LLMs
  - Why needed here: Understanding that LLMs generate sequences one token at a time is crucial for grasping why existing single-output explanation techniques don't apply directly
  - Quick check question: Why can't standard feature attribution techniques be directly applied to LLMs without modification?

- Concept: Local explanation techniques (feature attribution)
  - Why needed here: The aggregation-based approach relies on these techniques to compute token importance scores at each generation round
  - Quick check question: What distinguishes local explanation techniques from global explanation approaches?

- Concept: Semantic similarity measurement
  - Why needed here: PerbSim uses semantic similarity to assess the impact of token perturbations when comparing perturbed and original outputs
  - Quick check question: How does cosine similarity between sentence embeddings capture semantic differences between outputs?

## Architecture Onboarding

- Component map:
  Input -> Token importance calculation -> Importance score aggregation -> Multi-granularity importance scores -> Browser-based UI visualization

- Critical path:
  1. User inputs prompt and selects model
  2. System computes token-level importance scores
  3. Scores are aggregated to user-defined granularity
  4. Results are displayed with color-coded importance visualization

- Design tradeoffs:
  - Aggregation vs. perturbation: Aggregation is faster but requires white-box access; perturbation is more flexible but computationally expensive
  - Granularity level: Higher granularity provides more detail but may overwhelm users; lower granularity is easier to interpret but may lose nuance
  - Explanation technique selection: Different techniques (IG, LIME, etc.) have varying effectiveness and computational costs

- Failure signatures:
  - Aggregation approach: Importance scores that don't align with human intuition across different generation rounds
  - Perturbation approach: Similar outputs despite token removal, or inconsistent importance scores
  - Multi-granularity extension: Aggregated scores that don't reflect true component importance when tokens interact

- First 3 experiments:
  1. Run sentiment analysis case study with both aggregation-based and perturbation-based approaches on Llama-2
  2. Compare flip rates between treatment and control groups for different explanation techniques
  3. Test parameter sensitivity by varying M (aggregation rounds) and K (vocabulary access size) values

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the impact of using different local explanation techniques (e.g., LIME, SHAP) instead of Integrated Gradients in the aggregation-based approach?
- Basis in paper: [explicit] The paper mentions that "any effective, efficient, and model-agnostic explanation technique could be used in our framework" but only evaluates Integrated Gradients.
- Why unresolved: The paper only evaluates one gradient-based technique (IG) and does not compare with other local explanation methods.
- What evidence would resolve it: A comparative study showing performance differences when using various local explanation techniques in the aggregation-based approach.

### Open Question 2
- Question: How does the effectiveness of PromptExp vary across different task types (e.g., text generation, summarization) beyond sentiment classification?
- Basis in paper: [inferred] The paper only evaluates PromptExp on sentiment classification and two related tasks, suggesting potential limitations in generalizability.
- Why unresolved: The evaluation is limited to a single task domain, leaving open questions about performance on other NLP tasks.
- What evidence would resolve it: Systematic evaluation of PromptExp across diverse NLP tasks including generation, summarization, question answering, and translation.

### Open Question 3
- Question: What is the optimal balance between explanation quality and computational overhead when selecting parameters M and K?
- Basis in paper: [explicit] The paper investigates parameter impacts but does not provide a systematic framework for balancing effectiveness and efficiency.
- Why unresolved: The paper shows parameter impacts separately but doesn't provide guidance on how to choose parameters based on user priorities or resource constraints.
- What evidence would resolve it: Empirical studies mapping parameter settings to specific use cases, along with recommendations for different operational contexts (e.g., real-time vs. offline analysis).

## Limitations
- The framework's performance is constrained by the assumption that token importance remains relatively consistent across generation rounds, which may not hold for all LLMs or prompt types
- The perturbation-based approach requires significant computational resources, particularly for long prompts with many tokens
- The semantic similarity measures used to evaluate perturbation impact may not fully capture nuanced semantic differences in outputs

## Confidence
- High Confidence: The core mechanism of using token-level explanations to understand LLM behavior is well-established, and the multi-granularity extension provides practical value for prompt engineering tasks
- Medium Confidence: The effectiveness of the aggregation-based approach depends on the consistency of token importance across rounds, which may vary by model architecture and task
- Medium Confidence: The perturbation-based approach's performance is sensitive to the choice of semantic similarity metric and perturbation magnitude

## Next Checks
1. Conduct ablation studies varying the number of aggregation rounds (M) and vocabulary access size (K) to identify optimal parameter settings for different model architectures and prompt lengths
2. Test the framework on diverse prompt types beyond sentiment analysis, including reasoning tasks and code generation, to evaluate generalizability
3. Implement a controlled user study comparing PromptExp's explanations against baseline approaches to quantify improvements in prompt engineering efficiency and accuracy