---
ver: rpa2
title: 'THaLLE: Text Hyperlocally Augmented Large Language Extension -- Technical
  Report'
arxiv_id: '2406.07505'
source_url: https://arxiv.org/abs/2406.07505
tags:
- exam
- llms
- performance
- training
- return
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work introduces THaLLE, an 8B-parameter Large Language Model
  fine-tuned for financial analysis, achieving the highest performance on mock Chartered
  Financial Analyst (CFA) exams among models of comparable size. The model was trained
  using supervised fine-tuning (SFT) and direct preference optimization (DPO) on 9,429
  internal CFA exam questions, with data augmentation techniques including zero-shot
  prompts, modified zero-shot prompts, and chain-of-thought reasoning.
---

# THaLLE: Text Hyperlocally Augmented Large Language Extension -- Technical Report

## Quick Facts
- arXiv ID: 2406.07505
- Source URL: https://arxiv.org/abs/2406.07505
- Reference count: 15
- Key outcome: THaLLE, an 8B-parameter LLM fine-tuned for financial analysis, achieves the highest performance on mock Chartered Financial Analyst (CFA) exams among models of comparable size.

## Executive Summary
This paper introduces THaLLE, an 8B-parameter Large Language Model fine-tuned specifically for financial analysis and CFA exam performance. The model was trained on 9,429 internal CFA exam questions using supervised fine-tuning and direct preference optimization with data augmentation techniques including zero-shot prompts, modified zero-shot prompts, and chain-of-thought reasoning. THaLLE surpasses both open-source and proprietary models like GPT-3.5-turbo, Gemini-1.5-Flash, and Gemini-1.5-Pro on CFA exam benchmarks, with some variants achieving passing scores (>70%) on CFA Level I and II exams.

## Method Summary
THaLLE was developed by fine-tuning Llama3-8B and Qwen2-7B base models using supervised fine-tuning (SFT) with reasoning-augmented data and direct preference optimization (DPO) with self-supervised data augmentation. The training pipeline incorporated prompt loss masking, LoRA for efficient fine-tuning, and various data augmentation strategies including zero-shot prompts, modified zero-shot prompts, and chain-of-thought reasoning. Models were evaluated on internal CFA exam questions from 2020 and 2024, as well as the external Flare CFA dataset.

## Key Results
- THaLLE surpasses both open-source and proprietary models on mock CFA exams
- Some THaLLE variants achieve passing scores (>70%) on CFA Level I and II exams
- Chain-of-thought reasoning and prompt loss masking significantly improve performance
- DPO provides better regularization and slower overfitting compared to SFT

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Training models to produce both the answer and reasoning leads to better CFA exam performance.
- Mechanism: Including reasoning steps during training forces the model to learn underlying concepts, not just memorize correct answers. Incorporating reasoning in the manner the model would naturally output enhanced its learning and performance.
- Core assumption: Models benefit from explicit reasoning during training rather than just learning answer patterns.
- Evidence anchors: Results show that THaLLE surpasses both open-source and proprietary models; incorporating reasoning is crucial for our CFA exam; incorporating Chain-of-Thought prompts enhanced the model's performance.

### Mechanism 2
- Claim: Prompt loss masking improves supervised fine-tuning performance on multiple-choice tasks.
- Mechanism: By excluding the loss associated with predicting the prompt tokens, the model focuses training on the target output (answer and reasoning). Applying prompt loss masking during SFT results in a notable enhancement in overall performance.
- Core assumption: The model can learn the task structure from examples without needing to perfectly reconstruct the prompt during training.
- Evidence anchors: Our investigation indicates that applying prompt loss masking during SFT results in a notable enhancement in overall performance.

### Mechanism 3
- Claim: DPO provides better regularization and slower overfitting compared to SFT for CFA exam training.
- Mechanism: The preference learning framework with reference model provides implicit regularization. Models trained using DPO exhibit a slower rate of overfitting compared to those trained with SFT.
- Core assumption: The reference model in DPO provides regularization that prevents the model from memorizing training data too closely.
- Evidence anchors: Models trained using DPO exhibit a slower rate of overfitting compared to those trained with SFT; DPO method consistently achieved a performance improvement without the same overfitting issues.

## Foundational Learning

- Concept: Multiple-choice reading comprehension task structure
  - Why needed here: The CFA exam is fundamentally an MRC task where models must select correct answers from options while understanding complex financial concepts
  - Quick check question: Can you explain the difference between "Zero Shot" and "Chain-of-Thought" prompting approaches for MRC tasks?

- Concept: Supervised fine-tuning vs direct preference optimization
  - Why needed here: The paper uses both SFT and DPO approaches, with different strengths and failure modes for each
  - Quick check question: What is the key difference in how SFT and DPO handle training data, and why might DPO be more robust to overfitting?

- Concept: LoRA (Low-Rank Adaptation) for efficient fine-tuning
  - Why needed here: The paper uses LoRA for efficient parameter-efficient fine-tuning, with specific recommendations about target modules and ranks
  - Quick check question: Why does using "all-linear target modules" in LoRA lead to faster convergence compared to using only basic q proj and v proj modules?

## Architecture Onboarding

- Component map: Base model (Llama3-8B/Qwen2-7B) → SFT with reasoning-augmented data → DPO with incorrect answer pairs → Evaluation on held-out CFA questions
- Critical path: Base model → SFT with reasoning → DPO with incorrect answer pairs → Evaluation on held-out CFA questions
- Design tradeoffs: The paper chose 8B-parameter models for cost-effectiveness vs larger models, used LoRA for efficient fine-tuning, and prioritized reasoning inclusion despite added complexity.
- Failure signatures: Overfitting within 1-3 epochs on SFT, inconsistent performance across base models with DPO, failure to meet evaluation criteria (must specify or recite answer choice).
- First 3 experiments:
  1. Fine-tune Llama3-8B with SFT using reasoning-augmented data, monitor for overfitting within 2-3 epochs
  2. Apply DPO to SFT model using self-supervised incorrect answer pairs, compare to SFT-only performance
  3. Fine-tune Qwen2-7B with modified "Zero Shot" prompt (answer + reasoning), evaluate on internal CFA test set

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal LoRA rank for financial domain fine-tuning tasks?
- Basis in paper: The paper reports that LoRA rank 32 provided optimal performance for CFA exam tasks, with no improvement beyond this value.
- Why unresolved: The optimal rank may vary depending on task complexity and domain specificity. The study only tested up to rank 32, leaving uncertainty about performance at higher ranks for more complex financial tasks.
- What evidence would resolve it: Systematic testing of LoRA ranks beyond 32 (e.g., 64, 128) across various financial tasks with different complexity levels would clarify the optimal rank for different scenarios.

### Open Question 2
- Question: How do different reasoning prompt types (Zero Shot vs Chain-of-Thought) impact model performance in financial reasoning tasks?
- Basis in paper: The paper found that incorporating Chain-of-Thought prompting consistently outperformed models trained solely under "Zero Shot" settings.
- Why unresolved: While the paper demonstrates superiority of Chain-of-Thought, it doesn't explore the full spectrum of reasoning prompt types or combinations that might yield even better results for financial reasoning.
- What evidence would resolve it: Comparative experiments testing various reasoning prompt types and combinations (e.g., Chain-of-Thought with few-shot examples, hybrid approaches) across diverse financial reasoning tasks would reveal optimal prompt strategies.

### Open Question 3
- Question: Can models fine-tuned on CFA exams generalize to real-world financial advisory scenarios?
- Basis in paper: The authors acknowledge that while models can pass CFA exams, "we cannot conclude that LLMs are ready to function as working finance advisors" and "requires more in-depth and detailed assessments in future work."
- Why unresolved: The study focuses on exam performance as a proxy for financial proficiency, but real-world financial advisory involves dynamic scenarios, client interaction, and ethical considerations not captured in standardized exams.
- What evidence would resolve it: Longitudinal studies evaluating model performance in simulated real-world financial advisory scenarios, including client interactions, ethical dilemmas, and dynamic market conditions, would determine practical readiness.

## Limitations
- Proprietary internal CFA exam questions cannot be publicly validated, preventing independent verification of performance claims
- Evaluation relies primarily on a single external dataset (Flare CFA) with only 1,032 questions
- Focus on multiple-choice question formats may not capture open-ended financial reasoning capabilities crucial for real-world applications

## Confidence
- High Confidence: The effectiveness of reasoning-based training and prompt loss masking are well-supported by experimental results with clear ablation studies
- Medium Confidence: The claim that DPO provides better regularization than SFT is supported by observed slower overfitting rates, but experimental design doesn't fully isolate the benefit source
- Low Confidence: The claim that THaLLE achieves "the highest performance among models of comparable size" cannot be independently verified due to proprietary evaluation data

## Next Checks
1. **External Benchmark Validation**: Test THaLLE on publicly available financial reasoning datasets (e.g., FLAME, FIN-Medical) to verify performance claims beyond proprietary internal data
2. **Real-World Application Testing**: Evaluate THaLLE on actual financial advisory tasks beyond exam questions, such as portfolio analysis or risk assessment scenarios
3. **Generalization Analysis**: Conduct experiments testing THaLLE's performance on CFA questions from years not included in training or validation sets to verify generalizable financial reasoning